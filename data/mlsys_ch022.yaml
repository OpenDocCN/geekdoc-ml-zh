- en: Robust AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鲁棒人工智能
- en: '*DALL·E 3 Prompt: Create an image featuring an advanced AI system symbolized
    by an intricate, glowing neural network, deeply nested within a series of progressively
    larger and more fortified shields. Each shield layer represents a layer of defense,
    showcasing the system’s robustness against external threats and internal errors.
    The neural network, at the heart of this fortress of shields, radiates with connections
    that signify the AI’s capacity for learning and adaptation. This visual metaphor
    emphasizes not only the technological sophistication of the AI but also its resilience
    and security, set against the backdrop of a state-of-the-art, secure server room
    filled with the latest in technological advancements. The image aims to convey
    the concept of ultimate protection and resilience in the field of artificial intelligence.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：创建一个图像，展示一个由复杂发光的神经网络象征的高级AI系统，该神经网络深深嵌套在一系列逐渐增大和更坚固的盾牌中。每一层盾牌代表一层防御，展示了系统对外部威胁和内部错误的鲁棒性。位于这些盾牌堡垒核心的神经网络，通过其表示AI学习和适应能力的连接辐射出光芒。这个视觉隐喻不仅强调了AI的技术复杂性，还强调了其弹性和安全性，背景是一个充满最新技术进步的先进、安全的服务器室。该图像旨在传达人工智能领域的终极保护和弹性概念。*'
- en: '![](../media/file246.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file246.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*How do we develop fault-tolerant and resilient machine learning systems for
    real-world deployment?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们如何开发出对现实世界部署具有容错性和弹性的机器学习系统？*'
- en: Machine learning systems in real-world applications require fault-tolerant execution
    across diverse operational conditions. These systems face multiple challenges
    degrading their capabilities, including hardware anomalies, adversarial attacks,
    and unpredictable real-world data distributions that diverge from training assumptions.
    These vulnerabilities require AI systems to prioritize robustness and trustworthiness
    throughout design and deployment phases. Building resilient machine learning systems
    requires safe and effective operation in dynamic and uncertain environments. Understanding
    robustness principles enables engineers to design systems withstanding hardware
    failures, resisting malicious attacks, and adapting to distribution shifts. This
    capability enables deploying ML systems in safety-critical applications where
    failures can have severe consequences, from autonomous vehicles to medical diagnosis
    systems operating in unpredictable real-world conditions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界应用中的机器学习系统需要在各种操作条件下实现容错执行。这些系统面临多个挑战，包括硬件异常、对抗性攻击以及与训练假设不符的不可预测的现实世界数据分布，这些挑战会降低其能力。这些漏洞要求AI系统在设计部署阶段优先考虑鲁棒性和可信度。构建具有弹性的机器学习系统需要在动态和不确定的环境中实现安全和有效的操作。理解鲁棒性原则使工程师能够设计出能够承受硬件故障、抵抗恶意攻击并适应分布变化的系统。这种能力使得能够在具有严重后果的安全关键应用中部署ML系统，例如在不可预测的现实世界条件下运行的自动驾驶汽车和医疗诊断系统。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Classify hardware faults affecting ML systems into transient, permanent, and
    intermittent categories with their distinctive characteristics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将影响机器学习系统的硬件故障分类为瞬态、永久和间歇性类别，并描述其独特的特征
- en: Analyze how bit flips, memory errors, and component failures propagate through
    neural network computations to degrade model performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析位翻转、内存错误和组件故障如何通过神经网络计算传播，从而降低模型性能
- en: Compare detection mechanisms for hardware faults including BIST, error detection
    codes, and redundancy voting systems
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较硬件故障检测机制，包括BIST、错误检测码和冗余投票系统
- en: Design fault tolerance strategies combining hardware-level protection with software-implemented
    monitoring for ML deployments
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计结合硬件级保护和软件实现监控的容错策略，以用于机器学习部署
- en: Evaluate adversarial attack vectors including gradient-based, optimization-based,
    and transfer-based techniques on neural networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估对抗性攻击向量，包括基于梯度的、基于优化的和基于迁移的技术在神经网络上的应用
- en: Implement defense strategies against data poisoning attacks through anomaly
    detection, sanitization, and robust training methods
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过异常检测、净化和鲁棒训练方法实施针对数据中毒攻击的防御策略
- en: Assess distribution shift impacts on model accuracy using monitoring techniques
    and statistical drift detection methods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用监控技术和统计漂移检测方法评估分布变化对模型准确性的影响
- en: Integrate robustness principles across the complete ML pipeline from data ingestion
    through model deployment and monitoring
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个机器学习管道中整合稳健性原则，从数据摄取到模型部署和监控。
- en: Introduction to Robust AI Systems
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性人工智能系统简介
- en: 'When traditional software fails, it often does so loudly: a server crashes,
    an application throws an error, users receive clear failure messages. When a machine
    learning system fails, it often fails silently. A self-driving car’s perception
    system doesn’t crash; it simply misclassifies a truck as the sky. A demand forecasting
    model doesn’t error out; it just starts making wildly inaccurate predictions.
    A medical diagnosis system doesn’t shut down; it quietly provides incorrect classifications
    that could endanger patient lives. This ‘silent failure’ mode makes robustness
    a unique and critical challenge in AI systems. Engineers must defend not just
    against bugs in code, but against a world that refuses to conform to training
    data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当传统软件失败时，它通常会大声失败：服务器崩溃，应用程序抛出错误，用户收到清晰的错误消息。当机器学习系统失败时，它通常是无声失败。自动驾驶汽车的感觉系统不会崩溃；它只是错误地将卡车分类为天空。需求预测模型不会出错；它只是开始做出极端不准确的预测。医疗诊断系统不会关闭；它安静地提供可能危及患者生命的错误分类。这种“无声失败”模式使得稳健性成为人工智能系统中的一个独特且关键的挑战。工程师必须不仅防御代码中的错误，还要防御一个拒绝符合训练数据的世界。
- en: This silent failure challenge is amplified as ML systems expand across diverse
    deployment contexts, from cloud-based services to edge devices and embedded systems,
    where hardware and software faults have pronounced impacts on performance and
    reliability. The increasing complexity of these systems and their deployment in
    safety-critical applications[1](#fn1) makes robust and fault-tolerant designs
    essential for maintaining system integrity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统在多样化的部署环境中扩展，从基于云的服务到边缘设备和嵌入式系统，硬件和软件故障对性能和可靠性有显著影响。这些系统的日益复杂性和其在安全关键应用中的部署[1](#fn1)使得稳健和容错设计对于维护系统完整性至关重要。
- en: Building on the adaptive deployment challenges introduced in [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    and the security vulnerabilities examined in [Chapter 15](ch021.xhtml#sec-security-privacy),
    we now turn to comprehensive system reliability. ML systems operate across diverse
    domains where systemic failures, including hardware and software faults, malicious
    inputs such as adversarial attacks and data poisoning, and environmental shifts,
    can have severe consequences ranging from economic disruption to life-threatening
    situations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第14章](ch020.xhtml#sec-ondevice-learning)中介绍的自适应部署挑战和[第15章](ch021.xhtml#sec-security-privacy)中考察的安全漏洞的基础上，我们现在转向全面系统可靠性。机器学习系统在多个领域运行，包括硬件和软件故障、恶意输入如对抗攻击和数据中毒，以及环境变化，这些都可能带来严重后果，从经济破坏到生命威胁。
- en: To address these risks, researchers and engineers must develop advanced techniques
    for fault detection, isolation, and recovery that go beyond security measures
    alone. While [Chapter 15](ch021.xhtml#sec-security-privacy) established how to
    protect against deliberate attacks, ensuring reliable operation requires addressing
    the full spectrum of potential failures, both intentional and unintentional, that
    can compromise system behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些风险，研究人员和工程师必须开发出超越单纯安全措施的先进技术，用于故障检测、隔离和恢复。虽然[第15章](ch021.xhtml#sec-security-privacy)确立了如何抵御有意的攻击，但确保可靠运行需要解决可能损害系统行为的所有潜在故障的全谱系，无论是故意的还是无意的。
- en: 'This imperative for fault tolerance establishes what we define as Robust AI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种容错的需求确立了我们所定义的**弹性人工智能**：
- en: '***Resilient AI*** describes machine learning systems designed to maintain
    *performance* and *reliability* despite *system errors*, *malicious inputs*, and
    *environmental changes* through systematic *fault detection*, *mitigation*, and
    *recovery*.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性人工智能**描述的是设计用来在系统错误、恶意输入和环境变化的情况下，通过系统的故障检测、缓解和恢复来维持**性能**和**可靠性**的机器学习系统。'
- en: This chapter examines robustness challenges through our unified three-category
    framework, building upon adaptive deployment challenges from [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    and security vulnerabilities from [Chapter 15](ch021.xhtml#sec-security-privacy).
    Our systematic approach ensures comprehensive system reliability before operational
    deployment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过我们统一的三类框架来考察鲁棒性挑战，建立在[第14章](ch020.xhtml#sec-ondevice-learning)中的自适应部署挑战和[第15章](ch021.xhtml#sec-security-privacy)中的安全漏洞之上。我们的系统方法确保在部署前系统可靠性全面。
- en: '**Positioning Within the Narrative Arc:** While [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    established adaptive deployment challenges in resource-constrained environments,
    and [Chapter 15](ch021.xhtml#sec-security-privacy) addressed the vulnerabilities
    these adaptations create, this chapter ensures system-wide reliability across
    all failure modes: intentional attacks, unintentional faults, and natural variations.
    This comprehensive reliability framework becomes essential for the operational
    workflows detailed in [Chapter 13](ch019.xhtml#sec-ml-operations).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**在叙事弧中的定位**：虽然[第14章](ch020.xhtml#sec-ondevice-learning)确立了资源受限环境中的自适应部署挑战，[第15章](ch021.xhtml#sec-security-privacy)则解决了这些适应带来的漏洞，但本章确保了在所有故障模式下的系统可靠性：故意攻击、意外故障和自然变化。这个全面的可靠性框架对于[第13章](ch019.xhtml#sec-ml-operations)中详细描述的操作工作流程至关重要。'
- en: The first category, systemic hardware failures, presents significant challenges
    across computing systems ([Chapter 2](ch008.xhtml#sec-ml-systems)). Whether transient[2](#fn2),
    permanent, or intermittent, these faults can corrupt computations and degrade
    system performance. The impact ranges from temporary glitches to complete component
    failures, requiring robust detection and mitigation strategies to maintain reliable
    operation. This hardware-centric perspective extends beyond the algorithmic optimizations
    of other chapters to address physical layer vulnerabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类，系统硬件故障，在计算系统中提出了重大挑战([第2章](ch008.xhtml#sec-ml-systems))。这些故障可能是瞬时的[2](#fn2)、永久的或间歇性的，它们可能会破坏计算并降低系统性能。影响范围从暂时的故障到完全的组件故障，需要强大的检测和缓解策略来维持可靠的运行。这种以硬件为中心的视角超越了其他章节中算法优化的范畴，以解决物理层漏洞。
- en: Malicious manipulation represents our second category, where we examine adversarial
    robustness from an engineering perspective rather than the security-first approach
    of [Chapter 15](ch021.xhtml#sec-security-privacy). While that chapter addresses
    authentication, access control, and privacy preservation, we focus on maintaining
    model performance when under attack. Adversarial attacks, data poisoning attempts,
    and prompt injection vulnerabilities can cause models to misclassify inputs or
    produce unreliable outputs, requiring specialized defensive mechanisms distinct
    from traditional security measures.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意操纵代表我们的第二类问题，我们从这个角度来考察对抗性鲁棒性，而不是像[第15章](ch021.xhtml#sec-security-privacy)中提到的以安全为首要考虑的方法。虽然那一章讨论了身份验证、访问控制和隐私保护，但我们专注于在受到攻击时保持模型性能。对抗性攻击、数据中毒尝试和提示注入漏洞可能导致模型错误分类输入或产生不可靠的输出，需要专门的防御机制，这些机制与传统安全措施不同。
- en: Complementing these deliberate threats, environmental changes introduce our
    third category of robustness challenges. Unlike the operational monitoring discussed
    in [Chapter 13](ch019.xhtml#sec-ml-operations), we examine how models maintain
    accuracy as data distributions shift naturally over time. Bugs, design flaws,
    and implementation errors within algorithms, libraries, and frameworks can propagate
    through the system, creating systemic vulnerabilities[3](#fn3) that transcend
    individual component failures. This systems-level view of robustness encompasses
    the entire ML pipeline from data ingestion through inference.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 补充这些故意威胁的是环境变化，这引入了我们的第三类鲁棒性挑战。与[第13章](ch019.xhtml#sec-ml-operations)中讨论的操作监控不同，我们考察模型如何随着数据分布随时间自然变化而保持准确性。算法、库和框架中的错误、设计缺陷和实现错误可能会在整个系统中传播，造成超越单个组件故障的系统级漏洞[3](#fn3)。这种鲁棒性的系统级视角涵盖了整个机器学习管道，从数据摄入到推理。
- en: The specific approaches to achieving robustness vary significantly based on
    deployment context and system constraints. While [Chapter 9](ch015.xhtml#sec-efficient-ai)
    establishes efficiency principles for optimization, large-scale cloud computing
    environments typically emphasize fault tolerance through redundancy and sophisticated
    error detection mechanisms. Edge devices from [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    must address robustness challenges within strict computational, memory, and energy
    limitations, requiring specialized hardening strategies appropriate for resource-constrained
    environments. These constraints require careful optimization and targeted hardening
    strategies[4](#fn4) appropriate for resource-constrained environments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实现鲁棒性的具体方法根据部署环境和系统限制而有很大差异。虽然[第9章](ch015.xhtml#sec-efficient-ai)确立了优化效率的原则，但大规模云计算环境通常通过冗余和复杂的错误检测机制来强调容错性。来自[第14章](ch020.xhtml#sec-ondevice-learning)的边缘设备必须在严格的计算、内存和能源限制内解决鲁棒性挑战，需要适用于资源受限环境的专用加固策略。这些限制要求进行仔细的优化和有针对性的加固策略[4](#fn4)，适用于资源受限的环境。
- en: Despite these contextual differences, the essential characteristics of a robust
    ML system include fault tolerance, error resilience, and sustained performance.
    By understanding and addressing these multifaceted challenges, engineers can develop
    reliable ML systems capable of operating effectively in real-world environments.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些情境差异，鲁棒机器学习系统的基本特征包括容错性、错误恢复性和持续性能。通过理解和解决这些多方面的挑战，工程师可以开发出可靠的机器学习系统，能够在现实世界环境中有效运行。
- en: Robust AI systems inevitably require additional computational resources compared
    to basic implementations, creating direct tensions with the sustainability principles
    established in [Chapter 18](ch024.xhtml#sec-sustainable-ai). Error correction
    mechanisms consume 12-25% additional memory bandwidth, redundant processing increases
    energy consumption by 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>,
    and continuous monitoring adds 5-15% computational overhead. These robustness
    measures also generate additional heat, exacerbating thermal management challenges
    that constrain deployment density and require enhanced cooling infrastructure.
    Understanding these sustainability trade-offs enables engineers to make informed
    decisions about where robustness investments provide the greatest value while
    minimizing environmental impact.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒的AI系统与基本实现相比，不可避免地需要更多的计算资源，这与[第18章](ch024.xhtml#sec-sustainable-ai)中确立的可持续性原则形成直接冲突。错误纠正机制消耗了12-25%的额外内存带宽，冗余处理使能耗增加了2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>，而持续监控增加了5-15%的计算开销。这些鲁棒性措施还产生了额外的热量，加剧了热管理挑战，限制了部署密度并需要增强的冷却基础设施。理解这些可持续性权衡使工程师能够做出明智的决定，在最小化环境影响的同时，确定鲁棒性投资提供最大价值的领域。
- en: This chapter systematically examines these multidimensional robustness challenges,
    exploring detection and mitigation techniques across hardware, algorithmic, and
    environmental domains. Building on the deployment strategies from edge systems
    ([Chapter 14](ch020.xhtml#sec-ondevice-learning)) and resource efficiency principles
    from [Chapter 18](ch024.xhtml#sec-sustainable-ai), we develop comprehensive approaches
    that address fault tolerance requirements across all computing environments while
    considering energy and thermal constraints. The systematic examination of robustness
    challenges provided here establishes the foundation for building reliable AI systems
    that maintain performance and safety in real-world deployments, transforming robustness
    from an afterthought into a core design principle for production machine learning
    systems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章系统地考察了这些多维鲁棒性挑战，探讨了硬件、算法和环境领域的检测和缓解技术。基于边缘系统([第14章](ch020.xhtml#sec-ondevice-learning))的部署策略和[第18章](ch024.xhtml#sec-sustainable-ai)中的资源效率原则，我们开发了全面的方法，这些方法在考虑能源和热约束的同时，解决了所有计算环境中的容错性要求。这里对鲁棒性挑战的系统性考察为构建可靠的人工智能系统奠定了基础，这些系统能在现实世界的部署中保持性能和安全，将鲁棒性从次要考虑转变为生产机器学习系统的核心设计原则。
- en: Real-World Robustness Failures
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实世界的鲁棒性失败
- en: Understanding the importance of robustness in machine learning systems requires
    examining how faults manifest in practice. Real-world case studies illustrate
    the consequences of hardware and software faults across cloud, edge, and embedded
    environments. These examples highlight the critical need for fault-tolerant design,
    rigorous testing, and robust system architectures to ensure reliable operation
    in diverse deployment scenarios.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 理解机器学习系统中鲁棒性的重要性需要检查故障在实际中的表现。现实世界的案例研究说明了硬件和软件故障在云、边缘和嵌入式环境中的后果。这些例子突出了容错设计、严格测试和强大系统架构的临界需求，以确保在多样化的部署场景中可靠运行。
- en: Cloud Infrastructure Failures
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云基础设施故障
- en: In February 2017, Amazon Web Services (AWS) experienced [a significant outage](https://aws.amazon.com/message/41926/)
    due to human error during routine maintenance. An engineer inadvertently entered
    an incorrect command, resulting in the shutdown of multiple servers across the
    US-East-1 region. This 4-hour outage disrupted over 150 AWS services, affecting
    approximately 54% of all internet traffic according to initial estimates and causing
    estimated losses of $150 million across affected businesses. Amazon’s AI-powered
    assistant, Alexa, serving over 40 million devices globally, became completely
    unresponsive during the outage. Voice recognition requests that normally process
    in 200-500 ms failed entirely, demonstrating the cascading impact of infrastructure
    failures on ML services. This incident underscores the impact of human error on
    cloud-based ML systems and the importance of robust maintenance protocols and
    failsafe mechanisms[5](#fn5).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年2月，由于例行维护中的人为错误，亚马逊网络服务（AWS）经历了[一次重大中断](https://aws.amazon.com/message/41926/)。一名工程师意外输入了错误的命令，导致美国东部-1区域的多台服务器关闭。这次4小时的故障中断了超过150项AWS服务，根据初步估计，影响了约54%的互联网流量，并导致受影响企业估计损失了1.5亿美元。亚马逊的AI助手Alexa，在全球为超过4000万台设备提供服务，在故障期间完全无响应。通常在200-500毫秒内处理的语音识别请求完全失败，这证明了基础设施故障对ML服务的影响级联效应。这一事件强调了人为错误对基于云的ML系统的影响以及强大维护协议和应急机制的重要性[5](#fn5)。
- en: In another case ([Vangal et al. 2021](ch058.xhtml#ref-dixit2021silent)), Facebook
    encountered a silent data corruption (SDC)[6](#fn6) issue in its distributed querying
    infrastructure, illustrated in [Figure 16.1](ch022.xhtml#fig-sdc-example). SDC
    refers to undetected errors during computation or data transfer that propagate
    silently through system layers. Facebook’s system processed SQL-like queries across
    datasets and supported a compression application designed to reduce data storage
    footprints. Files were compressed when not in use and decompressed upon read requests.
    A size check was performed before decompression to ensure the file was valid.
    However, an unexpected fault occasionally returned a file size of zero for valid
    files, leading to decompression failures and missing entries in the output database.
    The issue appeared sporadically, with some computations returning correct file
    sizes, making it particularly difficult to diagnose.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个案例([Vangal等人2021](ch058.xhtml#ref-dixit2021silent))中，Facebook在其分布式查询基础设施中遇到了静默数据损坏(SDC)[6](#fn6)问题，如图16.1所示。SDC是指在计算或数据传输过程中未检测到的错误，这些错误在系统层中静默传播。Facebook的系统处理跨数据集的类似SQL的查询，并支持一个旨在减少数据存储足迹的压缩应用程序。文件在未使用时进行压缩，在读取请求时解压缩。在解压缩之前进行大小检查，以确保文件有效。然而，偶尔一个意外的故障会返回有效文件的零大小，导致解压缩失败和输出数据库中的缺失条目。问题偶尔出现，有些计算返回正确的文件大小，这使得诊断特别困难。
- en: '![](../media/file247.svg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file247.svg)'
- en: 'Figure 16.1: **Silent Data Corruption**: Unexpected Faults Can Return Incorrect
    File Sizes, Leading to Data Loss During Decompression and Propagating Errors Through
    Distributed Querying Systems Despite Apparent Operational Success. This Example
    From Facebook Emphasizes the Challenge of Undetected Errors, silent Data Corruption,
    and the Importance of Robust Error Detection Mechanisms in Large-Scale Data Processing
    Pipelines. Source: [Facebook](https://arxiv.org/PDF/2102.11245).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：**静默数据损坏**：意外的故障可能导致文件大小错误，导致解压缩期间数据丢失，并通过分布式查询系统传播错误，尽管表面上操作成功。这个例子来自Facebook，强调了未检测到的错误、静默数据损坏以及在大规模数据处理管道中强大错误检测机制的重要性。来源：[Facebook](https://arxiv.org/PDF/2102.11245)。
- en: This case illustrates how silent data corruption can propagate across multiple
    layers of the application stack, resulting in data loss and application failures
    in large-scale distributed systems. Left unaddressed, such errors can degrade
    ML system performance, particularly affecting training processes ([Chapter 8](ch014.xhtml#sec-ai-training)).
    For example, corrupted training data or inconsistencies in data pipelines due
    to SDC may compromise model accuracy and reliability. The prevalence of such issues
    is confirmed by similar challenges reported across other major companies. As shown
    in [Figure 16.2](ch022.xhtml#fig-sdc-jeffdean), [Jeff Dean](https://en.wikipedia.org/wiki/Jeff_Dean),
    Chief Scientist at Google DeepMind and Google Research, highlighted these issues
    in AI hypercomputers[7](#fn7) during a keynote at [MLSys 2024](https://mlsys.org/)
    ([Jeff Dean 2024](ch058.xhtml#ref-dean2024mlsys)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例说明了静默数据损坏如何在应用程序堆栈的多个层次中传播，导致大规模分布式系统中的数据丢失和应用故障。如果未得到解决，此类错误可能会降低ML系统的性能，尤其是影响训练过程([第8章](ch014.xhtml#sec-ai-training))。例如，损坏的训练数据或由于SDC导致的数据管道中的不一致性可能会损害模型的准确性和可靠性。此类问题的普遍性得到了其他主要公司报告的类似挑战的证实。如图16.2[7](#fn7)所示，[杰夫·迪恩](https://en.wikipedia.org/wiki/Jeff_Dean)，Google
    DeepMind和Google Research的首席科学家，在[MLSys 2024](https://mlsys.org/)（[杰夫·迪恩 2024](ch058.xhtml#ref-dean2024mlsys)）的开幕式上强调了AI超计算机中的这些问题。
- en: '![](../media/file248.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file248.jpg)'
- en: 'Figure 16.2: **Silent Data Corruption**: Modern AI Systems, Particularly Those
    Employing Large-Scale Data Processing Like Spark, Are Vulnerable to Silent Data
    Corruption (SDC), Subtle Errors Accumulating During Data Transfer and Storage.
    SDC Manifests in a Shuffle and Merge Database, Highlighting Corrupted Data Blocks
    (Red) Amidst Healthy Data (Blue/Gray) and Emphasizing the Challenge of Detecting
    These Errors in Distributed Systems Using the Figure. Source: Jeff Dean at MLSys
    2024, Keynote (Google).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：**静默数据损坏**：现代AI系统，尤其是那些使用大规模数据处理如Spark的系统，容易受到静默数据损坏（SDC）的影响，这是在数据传输和存储过程中累积的微妙错误。SDC在洗牌和合并数据库中表现出来，突显了损坏的数据块（红色）在健康数据（蓝色/灰色）之间，并强调了使用此图在分布式系统中检测这些错误的挑战。来源：Jeff
    Dean在MLSys 2024的开幕式（Google）。
- en: Edge Device Vulnerabilities
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘设备漏洞
- en: Moving from centralized cloud environments to distributed edge deployments,
    self-driving vehicles provide prominent examples of how faults can critically
    affect ML systems in the edge computing domain[8](#fn8). These vehicles depend
    on machine learning for perception, decision-making, and control, making them
    particularly vulnerable to both hardware and software faults.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从集中式云环境迁移到分布式边缘部署，自动驾驶汽车是边缘计算领域中故障如何严重影响ML系统的突出例子[8](#fn8)。这些车辆依赖于机器学习进行感知、决策和控制，使它们特别容易受到硬件和软件故障的影响。
- en: '![](../media/file249.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file249.jpg)'
- en: 'Figure 16.3: **Autopilot Perception Failure**: This Crash Provides the Critical
    Safety Risks of Relying on Machine Learning for Perception in Autonomous Systems,
    Where Failures to Correctly Classify Objects Can Lead to Catastrophic Outcomes.
    The Incident Underscores the Need for Robust Validation, Redundancy, and Failsafe
    Mechanisms in Self-Driving Vehicle Designs to Mitigate the Impact of Imperfect
    AI Models. Source: BBC News.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：**自动驾驶感知失败**：此事故揭示了依赖机器学习进行自主系统感知的临界安全风险，未能正确分类对象可能导致灾难性后果。这一事件强调了在自动驾驶汽车设计中需要强大的验证、冗余和应急机制，以减轻不完善AI模型的影响。来源：BBC新闻。
- en: In May 2016, a fatal crash occurred when a Tesla Model S operating in Autopilot
    mode[9](#fn9) collided with a white semi-trailer truck. The system, relying on
    computer vision and ML algorithms, failed to distinguish the trailer against a
    bright sky, leading to a high-speed impact. The driver, reportedly distracted
    at the time, did not intervene, as shown in [Figure 16.3](ch022.xhtml#fig-tesla-example).
    This incident raised serious concerns about the reliability of AI-based perception
    systems and emphasized the need for robust failsafe mechanisms in autonomous vehicles.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年5月，一辆在自动驾驶模式下运行的特斯拉Model S发生了致命事故[9](#fn9)，与一辆白色半挂卡车相撞。系统依赖计算机视觉和ML算法，未能区分卡车与明亮的蓝天，导致高速碰撞。据报道，当时司机分心，没有干预，如图16.3[9](#fn9)所示。这一事件对基于AI的感知系统的可靠性提出了严重质疑，并强调了在自动驾驶汽车中需要强大的应急机制。
- en: Reinforcing these concerns, a similar case occurred in March 2018, when an Uber
    self-driving test vehicle [struck](https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL)
    and killed a pedestrian in Tempe, Arizona. The accident was attributed to a flaw
    in the vehicle’s object recognition software, which failed to classify the pedestrian
    as an obstacle requiring avoidance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 加强了这些担忧的是，2018年3月发生了一个类似的案例，当时一辆优步自动驾驶测试车辆在亚利桑那州图森市[撞击](https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL)并杀死了一名行人。事故被归因于车辆目标识别软件的缺陷，该软件未能将行人分类为需要避让的障碍物。
- en: Embedded System Constraints
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入式系统约束
- en: Extending beyond edge computing to even more constrained environments, embedded
    systems[10](#fn10) operate in resource-constrained and often safety-critical environments.
    As AI capabilities are increasingly integrated into these systems, the complexity
    and consequences of faults grow significantly.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 超出边缘计算，扩展到更加受限的环境，嵌入式系统[10](#fn10)在资源受限且通常安全至关重要的环境中运行。随着人工智能能力越来越多地集成到这些系统中，故障的复杂性和后果显著增加。
- en: One example comes from space exploration. In 1999, NASA’s Mars Polar Lander
    mission experienced [a catastrophic failure](https://spaceref.com/uncategorized/nasa-reveals-probable-cause-of-mars-polar-lander-and-deep-space-2-mission-failures/)
    due to a software error in its touchdown detection system ([Figure 16.4](ch022.xhtml#fig-nasa-example)).
    The lander’s software misinterpreted the vibrations from the deployment of its
    landing legs as a successful touchdown, prematurely shutting off its engines and
    causing a crash. This incident demonstrates the importance of rigorous software
    validation and robust system design, particularly for remote missions where recovery
    is impossible. As AI becomes more integral to space systems, ensuring robustness
    and reliability becomes necessary for mission success.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子来自太空探索。1999年，美国宇航局火星极地着陆器任务由于着陆检测系统中的软件错误[发生了一次灾难性故障](https://spaceref.com/uncategorized/nasa-reveals-probable-cause-of-mars-polar-lander-and-deep-space-2-mission-failures/)（[图16.4](ch022.xhtml#fig-nasa-example)）。着陆器的软件错误地将着陆腿展开时的振动解释为成功的着陆，过早地关闭了引擎，导致坠毁。这一事件证明了严格的软件验证和稳健的系统设计的重要性，尤其是在无法恢复的远程任务中。随着人工智能在太空系统中的重要性日益增加，确保稳健性和可靠性对于任务成功变得至关重要。
- en: '![](../media/file250.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file250.png)'
- en: 'Figure 16.4: **Touchdown Detection Failure**: Erroneous Sensor Readings During
    the Mars Polar Lander Mission Triggered a Premature Engine Shutdown, Demonstrating
    the Critical Need for Robust Failure Modes and Rigorous Validation of Embedded
    Systems, particularly Those Operating in Inaccessible Environments. This Incident
    Underscores How Software Errors Can Lead to Catastrophic Consequences in Safety-Critical
    Applications and Emphasizes the Growing Importance of Reliable AI Integration
    in Complex Systems. Source: Slashgear.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：**着陆检测失败**：火星极地着陆器任务期间的错误传感器读数触发了过早的引擎关闭，展示了在难以到达的环境中稳健的故障模式和严格的嵌入式系统验证的迫切需要。这一事件强调了软件错误如何在安全关键应用中导致灾难性后果，并强调了在复杂系统中可靠的人工智能集成日益重要。来源：Slashgear。
- en: The consequences of embedded system failures extend beyond space exploration
    to commercial aviation. In 2015, a Boeing 787 Dreamliner experienced a complete
    electrical shutdown mid-flight due to a software bug in its generator control
    units. This failure highlights the critical importance of safety-critical systems[11](#fn11)
    meeting stringent reliability requirements. The failure stemmed from a scenario
    in which powering up all four generator control units simultaneously after 248
    days of continuous power (approximately 8 months), caused them to enter failsafe
    mode, disabling all AC electrical power.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式系统故障的后果不仅限于太空探索，还扩展到商业航空。2015年，波音787梦幻客机在飞行中由于发电机控制单元中的软件错误导致完全断电。这一故障突出了安全关键系统[11](#fn11)满足严格可靠性要求的重要性。故障源于在连续供电248天后（大约8个月），同时启动所有四个发电机控制单元，导致它们进入安全模式，关闭了所有交流电。
- en: '*“If the four main generator control units (associated with the engine-mounted
    generators) were powered up at the same time, after 248 days of continuous power,
    all four GCUs will go into failsafe mode at the same time, resulting in a loss
    of all AC electrical power regardless of flight phase.” — [Federal Aviation Administration
    directive](https://s3.amazonaws.com/public-inspection.federalregister.gov/2015-10066.pdf)
    (2015)*'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “如果与发动机安装的发电机相关的四个主要发电机控制单元（GCUs）同时启动，在连续248天供电后，所有四个GCUs将同时进入安全模式，无论飞行阶段如何，都会导致所有交流电力的丧失。”
    —— [联邦航空管理局指令](https://s3.amazonaws.com/public-inspection.federalregister.gov/2015-10066.pdf)（2015）
- en: As AI is increasingly applied in aviation, including tasks such as autonomous
    flight control and predictive maintenance, the robustness of embedded systems
    affects passenger safety.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI在航空领域的应用越来越广泛，包括自主飞行控制和预测性维护等任务，嵌入式系统的鲁棒性影响乘客安全。
- en: The stakes become even higher when we consider implantable medical devices.
    For instance, a smart [pacemaker](https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors)
    that experiences a fault or unexpected behavior due to software or hardware failure
    could place a patient’s life at risk. As AI systems take on perception, decision-making,
    and control roles in such applications, new sources of vulnerability emerge, including
    data-related errors, model uncertainty[12](#fn12), and unpredictable behaviors
    in rare edge cases. The opaque nature of some AI models complicates fault diagnosis
    and recovery.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑植入式医疗设备时，风险等级变得更高。例如，一个由于软件或硬件故障而出现故障或意外行为的智能[起搏器](https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors)可能会危及患者的生命。随着AI系统在此类应用中承担感知、决策和控制角色，新的脆弱性来源出现，包括数据相关错误、模型不确定性[12](#fn12)以及在罕见边缘案例中的不可预测行为。某些AI模型的透明度不足使得故障诊断和恢复变得复杂。
- en: These real-world failure scenarios underscore the critical need for systematic
    approaches to robustness evaluation and mitigation. Each failure—whether the AWS
    outage affecting millions of voice interactions, autonomous vehicle perception
    errors leading to fatal crashes, or spacecraft software bugs causing mission loss—reveals
    common patterns that inform robust system design.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现实世界的故障场景强调了系统化方法对鲁棒性评估和缓解的迫切需求。每一次故障——无论是影响数百万次语音交互的AWS中断，自动驾驶汽车感知错误导致致命事故，还是航天器软件错误导致任务损失——都揭示了稳健系统设计的共同模式。
- en: Building on these concrete examples of system failures across deployment environments,
    we now establish a unified framework for understanding and addressing robustness
    challenges systematically.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些具体示例的基础上，我们现在建立了一个统一的框架，用于系统性地理解和解决鲁棒性挑战。
- en: A Unified Framework for Robust AI
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳健AI的统一框架
- en: The real-world failures examined above share common characteristics despite
    their diverse causes and contexts. Whether examining AWS outages that disable
    voice assistants, autonomous vehicle perception failures, or spacecraft software
    errors, these incidents reveal patterns that inform systematic approaches to building
    robust AI systems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述现实世界的故障原因和背景各不相同，但它们具有共同的特征。无论是检查使语音助手失效的AWS中断，自动驾驶汽车的感知故障，还是航天器软件错误，这些事件都揭示了构建稳健AI系统的系统化方法。
- en: Building on Previous Concepts
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立在先前概念之上
- en: Before establishing our robustness framework, we connect these challenges to
    foundational concepts from earlier chapters. Hardware acceleration architectures
    ([Chapter 11](ch017.xhtml#sec-ai-acceleration)) established how GPU memory hierarchies,
    interconnect fabrics, and specialized compute units create complex fault propagation
    paths that robustness systems must address. The security frameworks from [Chapter 15](ch021.xhtml#sec-security-privacy)
    introduced threat modeling principles that directly inform our understanding of
    adversarial attacks and defensive strategies. Operational monitoring systems from
    [Chapter 13](ch019.xhtml#sec-ml-operations) provide the infrastructure foundation
    for detecting and responding to robustness threats in production environments.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立我们的鲁棒性框架之前，我们将这些挑战与早期章节中的基础概念联系起来。硬件加速架构（[第11章](ch017.xhtml#sec-ai-acceleration)）阐述了GPU内存层次结构、互连布线和专用计算单元如何创建复杂的故障传播路径，这些路径是鲁棒性系统必须解决的。第15章（ch021.xhtml#sec-security-privacy）中的安全框架介绍了威胁建模原则，这些原则直接影响了我们对对抗攻击和防御策略的理解。第13章（ch019.xhtml#sec-ml-operations）中的操作监控系统为在生产环境中检测和响应鲁棒性威胁提供了基础设施基础。
- en: These earlier concepts converge in robust AI systems where GPU memory errors
    can corrupt model weights, adversarial inputs exploit learned vulnerabilities,
    and operational monitoring must detect anomalies across hardware, algorithmic,
    and environmental dimensions. The efficiency optimizations from [Chapter 9](ch015.xhtml#sec-efficient-ai)
    become critical constraints when implementing redundancy and error correction
    mechanisms within acceptable performance budgets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些早期概念在鲁棒的人工智能系统中汇聚，在这些系统中，GPU内存错误可能会损坏模型权重，对抗性输入会利用学习到的漏洞，并且操作监控必须在硬件、算法和环境维度上检测异常。来自[第9章](ch015.xhtml#sec-efficient-ai)的效率优化在实现可接受的性能预算内的冗余和错误纠正机制时成为关键约束。
- en: From ML Performance to System Reliability
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从机器学习性能到系统可靠性
- en: 'To understand these failure patterns systematically, we must bridge the gap
    between ML system performance concepts familiar from earlier chapters and the
    reliability engineering principles essential for robust deployment. In traditional
    ML development ([Chapter 2](ch008.xhtml#sec-ml-systems)), we focus on metrics
    like model accuracy, inference latency, and throughput. However, real-world deployment
    introduces an additional dimension: the reliability of the underlying computational
    substrate that executes our models.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统地理解这些故障模式，我们必须弥合早期章节中熟悉的机器学习系统性能概念与确保鲁棒部署的可靠性工程原则之间的差距。在传统的机器学习开发([第2章](ch008.xhtml#sec-ml-systems))中，我们关注模型准确度、推理延迟和吞吐量等指标。然而，现实世界的部署引入了一个额外的维度：执行我们模型的底层计算基质的可靠性。
- en: 'Consider how hardware reliability directly impacts ML performance: a single
    bit flip in a critical neural network weight can degrade ResNet-50 classification
    accuracy from 76.0% (top-1) to 11% on ImageNet, while memory subsystem failures
    during training corrupt gradient updates and prevent model convergence. Modern
    transformer models (such as GPT-3 with 175 B parameters) execute 10^15 floating-point
    operations per inference, creating over one million opportunities for hardware
    faults during a single forward pass. GPU memory systems operating at up to 900
    GB/s bandwidth (e.g., V100 HBM2) process 10^11 bits per second, where base error
    rates of 10^-17 errors per bit translate to multiple potential faults per hour
    of operation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑硬件可靠性如何直接影响机器学习性能：在关键神经网络权重中单个位翻转可以将ResNet-50在ImageNet上的分类准确率从76.0%（top-1）降低到11%，而在训练期间内存子系统故障会破坏梯度更新并阻止模型收敛。现代转换器模型（如具有175B参数的GPT-3）在推理时执行10^15次浮点运算，在单次前向传递中就创造了超过一百万个硬件故障的机会。运行在高达900
    GB/s带宽（例如，V100 HBM2）的GPU内存系统每秒处理10^11位，其中每位的基错误率为10^-17错误，这意味着每小时会有多个潜在的故障。
- en: This connection between hardware reliability and ML performance requires us
    to adopt concepts from reliability engineering[13](#fn13), including fault models
    that describe how failures occur, error detection mechanisms that identify problems
    before they impact results, and recovery strategies that restore system operation.
    These reliability concepts complement the performance optimization techniques
    covered in [Chapter 9](ch015.xhtml#sec-efficient-ai) by ensuring that optimized
    systems continue to operate correctly under real-world conditions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件可靠性与机器学习性能之间的这种联系要求我们采用可靠性工程[13](#fn13)的概念，包括描述故障如何发生的故障模型，在它们影响结果之前识别问题的错误检测机制，以及恢复策略以恢复系统操作。这些可靠性概念通过确保优化系统在现实世界条件下继续正确运行，补充了[第9章](ch015.xhtml#sec-efficient-ai)中涵盖的性能优化技术。
- en: Building on this conceptual bridge, we establish a unified framework for understanding
    robustness challenges across all dimensions of ML systems. This framework provides
    the conceptual foundation for understanding how different types of faults, whether
    originating from hardware, adversarial inputs, or software defects, share common
    characteristics and can be addressed through systematic approaches.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在概念桥梁的基础上，我们建立了一个统一的框架来理解机器学习系统所有维度的鲁棒性挑战。这个框架为理解不同类型的故障（无论起源于硬件、对抗性输入还是软件缺陷）共享的共同特征，并通过系统方法解决提供了概念基础。
- en: The Three Pillars of Robust AI
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鲁棒人工智能的三大支柱
- en: 'Robust AI systems must address three primary categories of challenges that
    can compromise system reliability and performance. [Figure 16.5](ch022.xhtml#fig-three-pillars-framework)
    illustrates this three-pillar framework, showing how system-level faults, input-level
    attacks, and environmental shifts each represent distinct but interconnected threats
    to ML system robustness:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健的AI系统必须解决可能损害系统可靠性和性能的三个主要挑战类别。[图16.5](ch022.xhtml#fig-three-pillars-framework)展示了这个三支柱框架，展示了系统级故障、输入级攻击和环境变化如何分别代表对机器学习系统稳健性的不同但相互关联的威胁：
- en: '![](../media/file251.svg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file251.svg)'
- en: 'Figure 16.5: **Three Pillars Framework**: The three core categories of robustness
    challenges that AI systems must address to ensure reliable operation in real-world
    deployments. A robust AI system is built upon effectively handling these three
    challenge areas.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：**三支柱框架**：AI系统必须解决以确保在实际部署中可靠运行的三种核心稳健性挑战。一个稳健的AI系统建立在有效处理这三个挑战领域的基础上。
- en: System-level faults encompass all failures originating from the underlying computing
    infrastructure. These include transient hardware errors from cosmic radiation,
    permanent component degradation, and intermittent faults that appear sporadically.
    System-level faults affect the physical substrate upon which ML computations execute,
    potentially corrupting calculations, memory access patterns, or communication
    between components.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级故障包括所有源自底层计算基础设施的故障。这包括来自宇宙辐射的短暂硬件错误、永久性组件退化以及偶尔出现的间歇性故障。系统级故障影响机器学习计算执行的物理基础，可能破坏计算、内存访问模式或组件间的通信。
- en: Input-level attacks comprise deliberate attempts to manipulate model behavior
    through carefully crafted inputs or training data. Adversarial attacks exploit
    model vulnerabilities by adding imperceptible perturbations to inputs, while data
    poisoning corrupts the training process itself. These threats target the information
    processing pipeline, subverting the model’s learned representations and decision
    boundaries.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输入级攻击包括通过精心设计的输入或训练数据来操纵模型行为的故意尝试。对抗性攻击通过向输入添加难以察觉的扰动来利用模型漏洞，而数据中毒则腐蚀了训练过程本身。这些威胁针对信息处理管道，颠覆了模型学习到的表示和决策边界。
- en: Environmental shifts represent the natural evolution of real-world conditions
    that can degrade model performance over time. Distribution shifts, concept drift,
    and changing operational contexts challenge the core assumptions underlying model
    training. Unlike deliberate attacks, these shifts reflect the dynamic nature of
    deployment environments and the inherent limitations of static training paradigms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变化代表了现实世界条件的自然演变，这些变化可能会随着时间的推移降低模型性能。分布变化、概念漂移和操作环境的变化挑战了模型训练的核心假设。与故意攻击不同，这些变化反映了部署环境的动态性质和静态训练范式的固有局限性。
- en: Common Robustness Principles
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见稳健性原则
- en: 'These three categories of challenges stem from different sources but share
    several key characteristics that inform our approach to building resilient systems:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种挑战类别源于不同的来源，但它们共享几个关键特征，这些特征指导我们构建弹性系统的方法：
- en: Detection and monitoring form the foundation of any robustness strategy. Hardware
    monitoring systems typically sample metrics at 1-10 Hz frequencies, detecting
    temperature anomalies (±5°C from baseline), voltage fluctuations (±5% from nominal),
    and memory error rates exceeding 10^-12 errors per bit per hour. Adversarial input
    detection leverages statistical tests with p-value thresholds of 0.01-0.05, achieving
    85-95% detection rates with false positive rates below 2%. Distribution monitoring
    using MMD tests processes 1,000-10,000 samples per evaluation, detecting shifts
    with Cohen’s d > 0.3 within 95% confidence intervals.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和监控是任何稳健性策略的基础。硬件监控系统通常以1-10 Hz的频率采样指标，检测温度异常（相对于基线±5°C）、电压波动（相对于标称值±5%）和每小时每比特超过10^-12个错误的内存错误率。对抗性输入检测利用p值阈值为0.01-0.05的统计测试，实现85-95%的检测率，误报率低于2%。使用MMD测试的分布监控在每个评估中处理1,000-10,000个样本，在95%的置信区间内检测到Cohen’s
    d > 0.3的变化。
- en: Building on this detection capability, graceful degradation ensures that systems
    maintain core functionality even when operating under stress. Rather than catastrophic
    failure, robust systems should exhibit predictable performance reduction that
    preserves critical capabilities. ECC memory systems recover from single-bit errors
    with 99.9% success rates while adding 12.5% bandwidth overhead. Model quantization
    from FP32 to INT8 reduces memory requirements by 75% and inference time by 2-4<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>, trading 1-3% accuracy
    for continued operation under resource constraints. Ensemble fallback systems
    maintain 85-90% of peak performance when primary models fail, with switchover
    latency under 10 ms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在检测能力的基础上，优雅降级确保系统即使在压力下运行也能保持核心功能。鲁棒系统应该表现出可预测的性能下降，以保留关键能力，而不是灾难性的故障。ECC内存系统以99.9%的成功率从单比特错误中恢复，同时增加了12.5%的带宽开销。从FP32到INT8的模型量化将内存需求减少了75%，推理时间减少了2-4<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>，以1-3%的精度换取在资源受限下的持续运行。集成回退系统在主模型失败时保持85-90%的峰值性能，切换延迟低于10 ms。
- en: Adaptive response enables systems to adjust their behavior based on detected
    threats or changing conditions. Adaptation might involve activating error correction
    mechanisms, applying input preprocessing techniques, or dynamically adjusting
    model parameters. The key principle is that robustness is not static but requires
    ongoing adjustment to maintain effectiveness.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应响应使系统能够根据检测到的威胁或变化条件调整其行为。自适应可能涉及激活错误纠正机制、应用输入预处理技术或动态调整模型参数。关键原则是鲁棒性不是静态的，而是需要持续调整以保持有效性。
- en: These principles extend beyond fault recovery to encompass comprehensive performance
    adaptation strategies that appear throughout ML system design. Detection strategies
    form the foundation for monitoring systems, graceful degradation guides fallback
    mechanisms when components fail, and adaptive response enables systems to evolve
    with changing conditions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则不仅超越了故障恢复，还包括了贯穿机器学习系统设计全过程的全面性能自适应策略。检测策略构成了监控系统的基础，优雅降级在组件失败时指导回退机制，自适应响应使系统能够随着条件的变化而发展。
- en: Integration Across the ML Pipeline
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在机器学习管道中的整合
- en: Robustness cannot be achieved through isolated techniques applied to individual
    components. Instead, it requires systematic integration across the entire ML pipeline,
    from data collection through deployment and monitoring. This integrated approach
    recognizes that vulnerabilities in one component can compromise the entire system,
    regardless of protective measures implemented elsewhere.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性不能通过应用于单个组件的孤立技术来实现。相反，它需要在整个机器学习管道中系统性地整合，从数据收集到部署和监控。这种整合方法认识到一个组件中的漏洞可能会损害整个系统，无论在其他地方实施了哪些保护措施。
- en: With this unified foundation established, the detection and mitigation strategies
    we explore in subsequent sections, whether for hardware faults, adversarial attacks,
    or software errors, all build upon these common principles while addressing the
    specific characteristics of each threat category. Understanding these shared foundations
    enables the development of more effective and efficient approaches to building
    robust AI systems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了这个统一的基础之后，我们在后续章节中探讨的检测和缓解策略，无论是针对硬件故障、对抗攻击还是软件错误，都是基于这些共同原则，同时解决每个威胁类别的特定特征。理解这些共享基础能够促进更有效、更高效的构建鲁棒人工智能系统的方法。
- en: The following sections examine each pillar systematically, providing the conceptual
    foundation necessary to understand specialized tools and frameworks used for robustness
    evaluation and improvement.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将系统地检查每个支柱，提供理解用于鲁棒性评估和改进的专业工具和框架所需的概念基础。
- en: Hardware Faults
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件故障
- en: Having established our unified framework, we now examine each pillar in detail,
    beginning with system-level faults. Hardware faults represent the foundational
    layer of robustness challenges because all ML computations ultimately execute
    on physical hardware that can fail in various ways.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了我们的统一框架之后，我们现在将详细检查每个支柱，从系统级故障开始。硬件故障代表了鲁棒性挑战的基础层，因为所有机器学习计算最终都是在可能以各种方式失败的物理硬件上执行的。
- en: Hardware Fault Impact on ML Systems
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件故障对机器学习系统的影响
- en: 'Understanding why hardware reliability particularly matters for machine learning
    workloads requires examining several key factors. ML systems differ from traditional
    applications in several ways that amplify the impact of hardware faults:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么硬件可靠性对机器学习工作负载尤其重要，需要考察几个关键因素。机器学习系统与传统应用在几个方面有所不同，这些差异放大了硬件故障的影响：
- en: '**Computational Intensity**: Modern ML workloads perform millions of operations
    per second, creating many opportunities for faults to corrupt results'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算强度**：现代机器学习工作负载每秒执行数百万次操作，为故障导致结果损坏提供了许多机会'
- en: '**Long-Running Training**: Training jobs may run for days or weeks, increasing
    the probability of encountering hardware faults'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长时间运行训练**：训练作业可能运行数天或数周，增加了遇到硬件故障的概率'
- en: '**Parameter Sensitivity**: Small corruptions in model weights can cause large
    changes in output predictions'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数敏感性**：模型权重中的微小错误可能导致输出预测结果发生较大变化'
- en: '**Distributed Dependencies**: Large-scale training depends on coordination
    across many processors, where single-point failures can disrupt entire workflows'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式依赖性**：大规模训练依赖于多个处理器之间的协调，单点故障可能会破坏整个工作流程'
- en: Building on these ML-specific considerations, hardware faults fall into three
    main categories based on their temporal characteristics and persistence, each
    presenting distinct challenges for ML system reliability.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些机器学习特定的考虑因素，硬件故障根据其时间特性和持久性分为三大类，每一类都为机器学习系统的可靠性带来了独特的挑战。
- en: To illustrate the direct impact of hardware faults on neural networks, consider
    a single bit-flip in a weight matrix. If a critical weight in a ResNet-50 model
    flips from `0.5` to `-0.5` due to a transient fault affecting the sign bit in
    the IEEE 754 floating-point representation, it changes the sign of a feature map,
    causing a cascade of errors through subsequent layers. Research has shown that
    a single, targeted bit-flip in a key layer can drop ImageNet accuracy from 76%
    to less than 10% ([Reagen et al. 2018](ch058.xhtml#ref-reagen2018ares)). This
    demonstrates why hardware reliability directly affects model performance, not
    merely infrastructure stability. Unlike traditional software where a single bit
    error might cause a crash or incorrect calculation, in neural networks it can
    silently corrupt the learned representations that determine system behavior.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明硬件故障对神经网络的直接影响，考虑权重矩阵中的一个单比特翻转。如果一个ResNet-50模型中的关键权重由于影响IEEE 754浮点表示中符号位的短暂故障而从`0.5`翻转至`-0.5`，它将改变特征图的符号，导致后续层级的错误级联。研究表明，关键层中的一个有针对性的单比特翻转可以将ImageNet的准确率从76%降至不到10%
    ([Reagen等人，2018](ch058.xhtml#ref-reagen2018ares))。这表明硬件可靠性直接影响模型性能，而不仅仅是基础设施的稳定性。与可能导致崩溃或计算错误的单个比特错误的传统软件不同，在神经网络中，它可能会静默地损坏决定系统行为的已学习表示。
- en: Transient faults are temporary disruptions caused by external factors such as
    cosmic rays or electromagnetic interference. These non-recurring events, exemplified
    by bit flips in memory, cause incorrect computations without permanent hardware
    damage. For ML systems, transient faults can corrupt gradient updates during training
    or alter model weights during inference, leading to temporary but potentially
    significant performance degradation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬时故障是由外部因素（如宇宙射线或电磁干扰）引起的暂时性中断。这些非重复事件，例如内存中的比特翻转，会导致错误的计算，但不会造成永久性的硬件损坏。对于机器学习系统，瞬时故障可能会在训练期间损坏梯度更新或在推理期间改变模型权重，导致暂时但可能显著的性能下降。
- en: Permanent faults represent irreversible damage from physical defects or component
    wear-out, such as stuck-at faults or device failures that require hardware replacement.
    These faults are particularly problematic for long-running ML training jobs, where
    hardware failure can result in days or weeks of lost computation and require complete
    job restart from the most recent checkpoint.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障表示由物理缺陷或组件磨损造成的不可逆损坏，例如卡在故障或需要硬件更换的设备故障。这些故障对于长时间运行的机器学习训练作业尤其有问题，硬件故障可能导致数天或数周的计算丢失，并需要从最近的检查点完全重新启动作业。
- en: Intermittent faults appear and disappear sporadically due to unstable conditions
    like loose connections or aging components, making them particularly challenging
    to diagnose and reproduce. These faults can cause non-deterministic behavior in
    ML systems, leading to inconsistent results that compromise model validation and
    reproducibility.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不稳定条件，如松散的连接或老化组件，间歇性故障会随机出现和消失，这使得它们特别难以诊断和重现。这些故障可能导致机器学习系统中的非确定性行为，导致结果不一致，从而损害模型验证和可重复性。
- en: Understanding this fault taxonomy provides the foundation for designing fault-tolerant
    ML systems that can detect, mitigate, and recover from hardware failures across
    different operational environments. The impact of these faults on ML systems extends
    beyond traditional computing applications due to the computational intensity,
    distributed nature, and long-running characteristics of modern AI workloads.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这种故障分类法为设计容错机器学习系统奠定了基础，这些系统能够检测、减轻和从不同操作环境中的硬件故障中恢复。这些故障对机器学习系统的影响超出了传统的计算应用，因为现代人工智能工作负载的计算强度、分布式特性和长时间运行特性。
- en: Transient Faults
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 瞬态故障
- en: Beginning our detailed examination with the most common category, transient
    faults in hardware can manifest in various forms, each with its own unique characteristics
    and causes. These faults are temporary in nature and do not result in permanent
    damage to the hardware components.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最常见的类别开始详细检查，硬件中的瞬态故障可以表现为各种形式，每种形式都有其独特的特性和原因。这些故障本质上是暂时的，不会对硬件组件造成永久性损害。
- en: Transient Fault Properties
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 瞬态故障特性
- en: Transient faults are characterized by their short duration and non-permanent
    nature. They do not persist or leave any lasting impact on the hardware. However,
    they can still lead to incorrect computations, data corruption, or system misbehavior
    if not properly handled. A classic example is shown in [Figure 16.6](ch022.xhtml#fig-bit-flip),
    where a single bit in memory unexpectedly changes state, potentially altering
    critical data or computations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬态故障的特点是持续时间短且非永久性。它们不会持续存在或对硬件留下任何持久的影响。然而，如果处理不当，它们仍然可能导致错误的计算、数据损坏或系统行为异常。一个典型的例子在[图16.6](ch022.xhtml#fig-bit-flip)中展示，其中内存中的一个位意外地改变状态，可能会改变关键数据或计算。
- en: These manifestations encompass several distinct categories. Common transient
    fault types include Single Event Upsets (SEUs)[14](#fn14) from cosmic rays and
    ionizing radiation, voltage fluctuations ([Reddi and Gupta 2013](ch058.xhtml#ref-reddi2013resilient))
    from power supply instability, Electromagnetic Interference (EMI)[15](#fn15) from
    external electromagnetic fields, Electrostatic Discharge (ESD) from sudden static
    electricity flow, crosstalk[16](#fn16) from unintended signal coupling, ground
    bounce from simultaneous switching of multiple outputs, timing violations from
    signal timing constraint breaches, and soft errors in combinational logic ([Mukherjee,
    Emer, and Reinhardt, n.d.](ch058.xhtml#ref-mukherjee2005soft)). Understanding
    these fault types enables designing robust hardware systems that can mitigate
    their impact and ensure reliable operation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表现包括几个不同的类别。常见的瞬态故障类型包括来自宇宙射线和电离辐射的单事件 upset (SEU)[14](#fn14)、来自电源不稳定性的电压波动
    ([Reddi 和 Gupta 2013](ch058.xhtml#ref-reddi2013resilient))、来自外部电磁场的电磁干扰 (EMI)[15](#fn15)、来自突然静电电流量静电放电
    (ESD)、来自意外信号耦合的串扰[16](#fn16)、来自多个输出同时切换的接地弹跳、来自信号时序约束违规的时序违规，以及组合逻辑中的软错误 ([Mukherjee,
    Emer, 和 Reinhardt, n.d.](ch058.xhtml#ref-mukherjee2005soft))。理解这些故障类型能够设计出能够减轻其影响并确保可靠运行的稳健硬件系统。
- en: Fault Analysis and Performance Impact
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 故障分析和性能影响
- en: Modern ML systems require precise understanding of fault rates and their performance
    implications to make informed engineering decisions. The quantitative analysis
    of transient faults reveals significant patterns that inform robust system design.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习系统需要精确理解故障率和它们对性能的影响，以便做出明智的工程决策。瞬态故障的定量分析揭示了重要的模式，这些模式为稳健的系统设计提供了信息。
- en: Advanced semiconductor processes exhibit dramatically higher soft error rates.
    Modern 7 nm processes experience approximately 1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> higher soft error
    rates compared to 65 nm nodes due to reduced node capacitance and charge collection
    efficiency ([Baumann 2005](ch058.xhtml#ref-baumann2005soft)). For ML accelerators
    fabricated on cutting-edge processes, this translates to base error rates of approximately
    1 error per 10^14 operations, requiring systematic error detection and correction
    strategies.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 先进半导体工艺表现出显著更高的软错误率。现代7 nm工艺相比65 nm节点，软错误率大约高1000倍，这是由于节点电容和电荷收集效率降低（[Baumann
    2005](ch058.xhtml#ref-baumann2005soft)）。对于采用尖端工艺制造的ML加速器，这相当于每10^14次操作大约有1个错误，需要系统性的错误检测和纠正策略。
- en: 'These theoretical fault rates translate into practical reliability metrics
    that vary significantly with deployment environment and workload characteristics.
    Typical AI accelerators demonstrate Mean Time Between Failures (MTBF)[17](#fn17)
    values that differ substantially across deployment contexts:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些理论故障率转化为实际可靠性指标，这些指标因部署环境和工作负载特征而显著不同。典型的AI加速器在不同部署环境中表现出平均故障间隔时间（MTBF）[17](#fn17)值差异很大：
- en: '**Cloud AI accelerators** (Tesla V100, A100): MTBF of 50,000-100,000 hours
    under controlled data center conditions'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云AI加速器**（Tesla V100, A100）：在受控数据中心条件下，平均故障间隔时间（MTBF）为50,000-100,000小时'
- en: '**Edge AI processors** (NVIDIA Jetson, Intel Movidius): MTBF of 20,000-40,000
    hours in uncontrolled environments'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘AI处理器**（NVIDIA Jetson, Intel Movidius）：在不受控制的环境中，平均故障间隔时间（MTBF）为20,000-40,000小时'
- en: '**Mobile AI chips** (Apple Neural Engine, Qualcomm Hexagon): MTBF of 30,000-60,000
    hours with thermal and power constraints'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动AI芯片**（Apple Neural Engine, Qualcomm Hexagon）：在热和功率限制下，平均故障间隔时间（MTBF）为30,000-60,000小时'
- en: These MTBF values compound significantly in distributed training scenarios.
    A cluster of 1,000 accelerators with individual MTBF of 50,000 hours experiences
    an expected failure every 50 hours, necessitating robust checkpointing and recovery
    mechanisms.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些MTBF值在分布式训练场景中会显著增加。一个由1,000个平均故障间隔时间为50,000小时的加速器组成的集群，预计每50小时就会发生一次故障，这需要强大的检查点和恢复机制。
- en: 'Beyond understanding failure rates, system designers must account for protection
    costs. Hardware fault tolerance mechanisms introduce measurable performance and
    energy penalties that must be considered in system design. [Table 16.1](ch022.xhtml#tbl-fault-tolerance-overhead)
    quantifies these trade-offs across different protection mechanisms:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解故障率外，系统设计者必须考虑保护成本。硬件容错机制引入了可测量的性能和能耗惩罚，这些必须在系统设计中考虑。 [表16.1](ch022.xhtml#tbl-fault-tolerance-overhead)量化了不同保护机制之间的权衡。
- en: 'Table 16.1: **Fault Tolerance Overhead Analysis**: Quantitative impact of different
    protection mechanisms on system performance, energy consumption, and hardware
    area requirements. These overheads must be balanced against fault rates and recovery
    costs to optimize system reliability per unit resource.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.1：**容错开销分析**：不同保护机制对系统性能、能耗和硬件面积需求的影响。这些开销必须与故障率和恢复成本相平衡，以优化每单位资源的系统可靠性。
- en: '| **Protection Mechanism** | **Performance** **Overhead** | **Energy Overhead**
    | **Area Overhead** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **保护机制** | **性能开销** | **能耗开销** | **面积开销** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Single-bit ECC** | 2-5% | 3-7% | 12-15% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **单位ECC** | 2-5% | 3-7% | 12-15% |'
- en: '| **Double-bit ECC** | 5-12% | 8-15% | 25-30% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **双位ECC** | 5-12% | 8-15% | 25-30% |'
- en: '| **Triple Modular Redundancy** | 200-300% | 200-300% | 200-300% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **三模冗余** | 200-300% | 200-300% | 200-300% |'
- en: '| **Checkpoint/Restart** | 10-25% | 15-30% | 5-10% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **检查点/重启** | 10-25% | 15-30% | 5-10% |'
- en: These overhead values have particularly significant impact on memory bandwidth
    utilization, a critical constraint in ML workloads. ECC memory[18](#fn18) reduces
    effective bandwidth by 12.5% due to additional storage requirements (8 ECC bits
    per 64 data bits). Memory scrubbing operations for error detection consume additional
    5-15% of available bandwidth depending on scrubbing frequency and memory configuration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些开销值对内存带宽利用率有特别显著的影响，这是ML工作负载中的一个关键限制。ECC内存[18](#fn18)由于额外的存储需求（每64位数据需要8位ECC），有效带宽降低了12.5%。错误检测的内存擦除操作消耗额外的5-15%可用带宽，具体取决于擦除频率和内存配置。
- en: These bandwidth overheads have direct performance implications. For typical
    transformer training workloads that are memory bandwidth-bound, these bandwidth
    reductions directly translate to proportional training time increases. A model
    requiring 900 GB/s of memory bandwidth with ECC protection effectively receives
    only 787 GB/s, extending training time by approximately 14%.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些带宽开销对性能有直接影响。对于典型的变压器训练工作负载，这些带宽减少直接导致训练时间成比例增加。一个需要900 GB/s内存带宽并带有ECC保护的模型实际上只能获得787
    GB/s，训练时间延长约14%。
- en: Memory Hierarchy and Bandwidth Impact
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存层次结构和带宽影响
- en: Memory subsystems represent the most vulnerability-prone components in modern
    ML systems, with fault tolerance mechanisms significantly impacting both bandwidth
    utilization and overall system performance. Understanding memory hierarchy robustness
    requires analyzing the interplay between different memory technologies, their
    error characteristics, and the bandwidth implications of protection mechanisms.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 内存子系统是现代机器学习系统中最易受攻击的组件，容错机制显著影响带宽利用率和整体系统性能。理解内存层次结构的鲁棒性需要分析不同内存技术、它们的错误特性和保护机制带宽影响之间的相互作用。
- en: 'This complexity stems from the diverse characteristics of memory technologies,
    which exhibit distinct fault patterns and protection requirements. [Table 16.2](ch022.xhtml#tbl-memory-bandwidth-protection)
    shows how ECC protection affects memory bandwidth across different technologies:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性源于内存技术的多样性，它们表现出不同的故障模式和保护需求。[表16.2](ch022.xhtml#tbl-memory-bandwidth-protection)展示了ECC保护如何影响不同技术中的内存带宽：
- en: '**DRAM**: Base error rate of 1 per 10^17 bits, dominated by single-bit soft
    errors. Requires refresh-based error detection and correction.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DRAM**：每10^17位有1个基本错误率，主要由单比特软错误主导。需要基于刷新的错误检测和纠正。'
- en: '**HBM (High Bandwidth Memory)**: 10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    higher error rates due to 3D stacking effects and thermal density. Advanced ECC
    required for reliable operation.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HBM（高带宽内存）**：由于3D堆叠效应和热密度，错误率比标准DRAM高10倍。需要高级ECC才能可靠运行。'
- en: '**SRAM (Cache)**: Lower soft error rates (1 per 10^19 bits) but higher vulnerability
    to voltage variations and process variations.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SRAM（缓存）**：软错误率较低（每10^19位1个），但更容易受到电压变化和工艺变化的影响。'
- en: '**NVM (Non-Volatile Memory)**: Emerging technologies like 3D XPoint with unique
    error patterns requiring specialized protection schemes[19](#fn19).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NVM（非易失性内存）**：如3D XPoint等新兴技术具有独特的错误模式，需要专门的保护方案[19](#fn19)。'
- en: '**GDDR**: Optimized for bandwidth over reliability, typically 2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> higher error rates
    than standard DRAM.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GDDR**：在带宽和可靠性之间进行优化，通常比标准DRAM的错误率高2-3倍。'
- en: 'The choice of memory technology and protection mechanism directly affects available
    bandwidth for ML workloads:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 内存技术和保护机制的选择直接影响到机器学习工作负载的可用带宽：
- en: 'Table 16.2: **Memory Bandwidth Protection Analysis**: Impact of ECC protection
    on effective memory bandwidth across different memory technologies used in ML
    accelerators. The bandwidth overhead directly affects training throughput for
    memory-bound workloads.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.2：**内存带宽保护分析**：ECC保护对不同机器学习加速器中使用的不同内存技术有效内存带宽的影响。带宽开销直接影响到内存受限工作负载的训练吞吐量。
- en: '| **Memory Technology** | **Base Bandwidth** **(GB/s)** | **ECC Overhead**
    **(%)** | **Effective** **Bandwidth (GB/s)** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **内存技术** | **基本带宽** **(GB/s)** | **ECC开销** **(%)** | **有效** **带宽 (GB/s)**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **DDR4-3200** | 51.2 | 12.5% | 44.8 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **DDR4-3200** | 51.2 | 12.5% | 44.8 |'
- en: '| **HBM2** | 900 | 12.5% | 787 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **HBM2** | 900 | 12.5% | 787 |'
- en: '| **HBM3** | 1,600 | 12.5% | 1,400 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **HBM3** | 1,600 | 12.5% | 1,400 |'
- en: '| **GDDR6X** | 760 | Typically none | 760 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **GDDR6X** | 760 | 通常无 | 760 |'
- en: 'Modern memory systems implement continuous background error detection through
    memory scrubbing, which periodically reads and rewrites memory locations to detect
    and correct accumulating soft errors. This background activity consumes memory
    bandwidth and creates interference with ML workloads:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现代内存系统通过内存擦洗实现连续的背景错误检测，定期读取和重写内存位置以检测和纠正累积的软错误。这种后台活动消耗内存带宽，并会对机器学习工作负载造成干扰：
- en: '**Scrubbing Rate**: Typical 24-hour full memory scan consumes 2-5% of total
    bandwidth'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**擦除率**：典型的24小时全内存扫描消耗2-5%的总带宽'
- en: '**Priority Arbitration**: ML memory requests must compete with scrubbing operations,
    increasing latency variance by 10-15%'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先仲裁**：ML内存请求必须与擦除操作竞争，增加10-15%的延迟变化率'
- en: '**Thermal Impact**: Scrubbing increases memory power consumption by 3-8%, affecting
    thermal design and cooling requirements'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热影响**：擦除增加了3-8%的内存功耗，影响热设计和冷却要求'
- en: 'Advanced ML systems implement hierarchical protection schemes that balance
    performance and reliability across the memory hierarchy:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的ML系统实施分层保护方案，在内存层次结构中平衡性能和可靠性：
- en: '**L1/L2 Cache**: Parity protection with immediate detection and replay capability'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L1/L2缓存**：带立即检测和重放功能的奇偶校验保护'
- en: '**L3 Cache**: Single-bit ECC with error logging and gradual cache line retirement'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L3缓存**：带错误记录和逐步缓存行退役的单位ECC'
- en: '**Main Memory**: Double-bit ECC with advanced syndrome analysis and predictive
    failure detection'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主存储器**：具有高级校验分析和预测故障检测的双位ECC'
- en: '**Persistent Storage**: Reed-Solomon codes with distributed redundancy across
    multiple devices'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**持久存储**：具有跨多个设备分布式冗余的里德-所罗门码'
- en: 'Modern AI accelerators integrate memory protection with compute pipeline design
    to minimize performance impact:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI加速器将内存保护与计算流水线设计集成，以最小化性能影响：
- en: '**Error Detection Pipelining**: Memory ECC checking overlapped with arithmetic
    operations to hide protection latency'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误检测流水线**：内存ECC检查与算术操作重叠，以隐藏保护延迟'
- en: '**Adaptive Protection Levels**: Dynamic adjustment of protection strength based
    on workload criticality and error rate monitoring'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应保护级别**：根据工作负载重要性和错误率监控动态调整保护强度'
- en: '**Bandwidth Allocation Policies**: Quality-of-service mechanisms that prioritize
    critical ML memory traffic over background protection operations'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带宽分配策略**：服务质量机制，优先处理关键ML内存流量，高于后台保护操作'
- en: '![](../media/file252.svg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file252.svg)'
- en: 'Figure 16.6: **Bit-Flip Error**: Transient faults can alter individual bits
    in memory, corrupting data or program instructions and potentially causing system
    malfunctions. These single-bit errors exemplify the vulnerability of hardware
    to transient faults like those induced by radiation or electromagnetic interference.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：**位翻转错误**：瞬态故障可以改变内存中的单个位，损坏数据或程序指令，并可能导致系统故障。这些单比特错误体现了硬件对辐射或电磁干扰等瞬态故障的脆弱性。
- en: Transient Fault Origins
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 瞬态故障起源
- en: External environmental factors represent the most significant source of the
    transient fault types described above. As illustrated in [Figure 16.7](ch022.xhtml#fig-transient-fault),
    cosmic rays, high-energy particles from outer space, strike sensitive hardware
    areas like memory cells or transistors, inducing charge disturbances that alter
    stored or transmitted data. [Electromagnetic interference (EMI)](https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference)
    from nearby devices creates voltage spikes or glitches that temporarily disrupt
    normal operation. Electrostatic discharge (ESD) events create temporary voltage
    surges that affect sensitive electronic components.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 外部环境因素是上述瞬态故障类型的最重要来源。如图16.7所示，宇宙射线、来自外太空的高能粒子撞击敏感的硬件区域，如存储单元或晶体管，引起电荷扰动，从而改变存储或传输的数据。[电磁干扰（EMI）](https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference)来自附近设备产生的电压尖峰或故障，暂时中断正常操作。静电放电（ESD）事件产生暂时电压激增，影响敏感的电子组件。
- en: '![](../media/file253.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file253.png)'
- en: 'Figure 16.7: **Transient Fault Mechanism**: Cosmic rays and electromagnetic
    interference induce bit flips within hardware by altering electrical charges in
    memory cells and transistors, potentially corrupting data and causing system errors.
    Understanding these fault sources is critical for building robust ai systems that
    can tolerate unpredictable hardware behavior. Source: [NTT](HTTPS://group.ntt/en/newsrelease/2018/11/22/181122a.HTML).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：**瞬态故障机制**：宇宙射线和电磁干扰通过改变存储单元和晶体管中的电荷，在硬件中引起位翻转，可能损坏数据并导致系统错误。理解这些故障来源对于构建能够容忍不可预测硬件行为的鲁棒AI系统至关重要。来源：[NTT](HTTPS://group.ntt/en/newsrelease/2018/11/22/181122a.HTML)。
- en: Complementing these external environmental factors, power and signal integrity
    issues constitute another major category of transient fault causes, affecting
    hardware systems ([Chapter 11](ch017.xhtml#sec-ai-acceleration)). Voltage fluctuations
    due to power supply noise or instability ([Reddi and Gupta 2013](ch058.xhtml#ref-reddi2013resilient))
    can cause logic circuits to operate outside their specified voltage ranges, leading
    to incorrect computations. Ground bounce, triggered by simultaneous switching
    of multiple outputs, creates temporary voltage variations in the ground reference
    that can affect signal integrity. Crosstalk, caused by unintended signal coupling
    between adjacent conductors, can induce noise that temporarily corrupts data or
    control signals, impacting training processes ([Chapter 8](ch014.xhtml#sec-ai-training)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 补充这些外部环境因素，电源和信号完整性问题构成了另一个主要的瞬态故障原因类别，影响硬件系统（[第11章](ch017.xhtml#sec-ai-acceleration)）。由于电源噪声或不稳定导致的电压波动（[Reddi和Gupta
    2013](ch058.xhtml#ref-reddi2013resilient)）可能导致逻辑电路在其指定的电压范围之外运行，从而导致错误的计算。由多个输出同时切换触发的地弹跳，会在地参考中产生暂时性的电压变化，这可能会影响信号完整性。由于相邻导体之间无意中的信号耦合引起的串扰，可以产生暂时性破坏数据或控制信号的噪声，影响训练过程（[第8章](ch014.xhtml#sec-ai-training)）。
- en: Timing and logic vulnerabilities create additional pathways for transient faults.
    Timing violations occur when signals fail to meet setup or hold time requirements
    due to process variations, temperature changes, or voltage fluctuations. These
    violations can cause incorrect data capture in sequential elements. Soft errors
    in combinational logic can affect circuit outputs even without memory involvement,
    particularly in deep logic paths where noise margins are reduced ([Mukherjee,
    Emer, and Reinhardt, n.d.](ch058.xhtml#ref-mukherjee2005soft)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 定时和逻辑漏洞为瞬态故障创造了额外的途径。当信号由于工艺变化、温度变化或电压波动而未能满足建立时间或保持时间要求时，就会发生定时违规。这些违规可能导致顺序元素中的数据捕获错误。组合逻辑中的软错误即使在没有内存参与的情况下也会影响电路输出，尤其是在噪声边缘减少的深层逻辑路径中（[Mukherjee，Emer和Reinhardt，n.d.](ch058.xhtml#ref-mukherjee2005soft)）。
- en: Transient Fault Propagation
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 瞬态故障的传播
- en: Building on these underlying causes, transient faults can manifest through different
    mechanisms depending on the affected hardware component. In memory devices like
    DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes
    its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions.
    In logic circuits, transient faults can cause glitches[20](#fn20) or voltage spikes
    propagating through the combinational logic[21](#fn21), resulting in incorrect
    outputs or control signals. Graphics Processing Units (GPUs)[22](#fn22) used extensively
    in ML workloads exhibit significantly higher error rates than traditional CPUs,
    with studies showing GPU error rates 10-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    higher than CPU errors due to their parallel architecture, higher transistor density,
    and aggressive voltage/frequency scaling. This disparity makes GPU-accelerated
    AI systems particularly vulnerable to transient faults during training and inference
    operations. Transient faults can also affect communication channels, causing bit
    errors or packet losses during data transmission. In distributed AI training systems,
    network partitions[23](#fn23) occur with measurable frequency - studies of large-scale
    clusters report partition events affecting 1-10% of nodes daily, with recovery
    times ranging from seconds to hours depending on the partition type and detection
    mechanisms.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在这些潜在原因的基础上，瞬态故障可以通过不同的机制表现出来，具体取决于受影响的硬件组件。在像DRAM或SRAM这样的存储设备中，瞬态故障通常会导致位翻转，即单个位从0变为1或相反。这可能会破坏存储的数据或指令。在逻辑电路中，瞬态故障可能导致[20](#fn20)毛刺或通过组合逻辑[21](#fn21)传播的电压尖峰，从而导致错误的输出或控制信号。在机器学习工作负载中广泛使用的图形处理单元（GPU）[22](#fn22]比传统的CPU表现出显著更高的错误率，研究表明GPU的错误率比CPU高10-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>，这是由于它们的并行架构、更高的晶体管密度和积极的电压/频率缩放。这种差异使得GPU加速的AI系统在训练和推理操作期间特别容易受到瞬态故障的影响。瞬态故障还可以影响通信通道，在数据传输期间导致位错误或数据包丢失。在分布式AI训练系统中，网络分区[23](#fn23)发生的频率可测
    - 大规模集群的研究报告显示，每天有1-10%的节点受到影响，恢复时间从秒到小时不等，具体取决于分区类型和检测机制。
- en: Transient Fault Effects on ML
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 瞬态故障对机器学习的影响
- en: A common example of a transient fault is a bit flip in the main memory. If an
    important data structure or critical instruction is stored in the affected memory
    location, it can lead to incorrect computations or program misbehavior. For instance,
    a bit flip in the memory storing a loop counter can cause the loop to execute
    indefinitely or terminate prematurely. Transient faults in control registers or
    flag bits can alter the flow of program execution, leading to unexpected jumps
    or incorrect branch decisions. In communication systems, transient faults can
    corrupt transmitted data packets, resulting in retransmissions or data loss.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的瞬态故障例子是主存储器中的位翻转。如果重要的数据结构或关键指令存储在受影响的内存位置，可能会导致计算错误或程序行为异常。例如，存储循环计数器的内存中的位翻转可能导致循环无限执行或提前终止。控制寄存器或标志位的瞬态故障可能会改变程序执行的流程，导致意外的跳转或错误的分支决策。在通信系统中，瞬态故障可能会损坏传输的数据包，导致重传或数据丢失。
- en: These general impacts become particularly pronounced in ML systems, where transient
    faults can have significant implications during the training phase ([Yi He et
    al. 2023](ch058.xhtml#ref-he2023understanding)). ML training involves iterative
    computations and updates to model parameters based on large datasets. If a transient
    fault occurs in the memory storing the model weights or gradients[24](#fn24),
    it can lead to incorrect updates and compromise the convergence and accuracy of
    the training process. For example, a bit flip in the weight matrix of a neural
    network can cause the model to learn incorrect patterns or associations, leading
    to degraded performance ([Wan et al. 2021](ch058.xhtml#ref-wan2021analyzing)).
    Transient faults in the data pipeline, such as corruption of training samples
    or labels, can also introduce noise and affect the quality of the learned model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些一般影响在机器学习系统中尤为明显，瞬态故障在训练阶段可能产生重大影响 ([Yi He 等人 2023](ch058.xhtml#ref-he2023understanding))。机器学习训练涉及基于大数据集的迭代计算和模型参数的更新。如果存储模型权重或梯度的内存中出现瞬态故障[24](#fn24)，可能会导致更新错误并损害训练过程的收敛性和准确性。例如，神经网络权重矩阵中的位翻转可能导致模型学习到错误的模式或关联，从而降低性能
    ([Wan 等人 2021](ch058.xhtml#ref-wan2021analyzing))。数据管道中的瞬态故障，如训练样本或标签的损坏，也可能引入噪声并影响学习到的模型质量。
- en: As shown in [Figure 16.8](ch022.xhtml#fig-sdc-training-fault), a real-world
    example from Google’s production fleet highlights how an SDC anomaly caused a
    significant deviation in the gradient norm, a measure of the magnitude of updates
    to the model parameters. Such deviations can disrupt the optimization process,
    leading to slower convergence or failure to reach an optimal solution.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图16.8](ch022.xhtml#fig-sdc-training-fault)所示，谷歌生产车队的真实案例突出了SDC异常如何导致梯度范数出现显著偏差，这是模型参数更新幅度的度量。这种偏差可能会干扰优化过程，导致收敛速度变慢或无法达到最优解。
- en: '![](../media/file254.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file254.jpg)'
- en: 'Figure 16.8: **Gradient Norm Deviation**: Transient hardware faults, such as
    single data corruption (SDC), disrupt optimization by causing abrupt changes in
    gradient norms during model training, potentially leading to convergence issues
    or inaccurate models. Real-world data from Google’s production fleet confirms
    that SDC anomalies manifest as visible spikes in gradient norm over time, indicating
    a disruption to the expected parameter update process. Source: jeff dean, mlsys
    2024 keynote (Google).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：**梯度范数偏差**：瞬态硬件故障，如单数据损坏（SDC），在模型训练期间通过引起梯度范数的突然变化来干扰优化，可能导致收敛问题或不准确的模型。来自谷歌生产车队的真实数据证实，SDC异常表现为梯度范数随时间出现的可见尖峰，表明预期的参数更新过程受到干扰。来源：jeff
    dean，mlsys 2024 大会演讲（谷歌）。
- en: During the inference phase, transient faults can impact the reliability and
    trustworthiness of ML predictions. If a transient fault occurs in the memory storing
    the trained model parameters or during the computation of inference results, it
    can lead to incorrect or inconsistent predictions. For instance, a bit flip in
    the activation values of a neural network can alter the final classification or
    regression output ([Mahmoud et al. 2020](ch058.xhtml#ref-mahmoud2020pytorchfi)).
    In safety-critical applications[25](#fn25), these faults can have severe consequences,
    resulting in incorrect decisions or actions that may compromise safety or lead
    to system failures ([G. Li et al. 2017](ch058.xhtml#ref-li2017understanding);
    [S. Jha et al. 2019](ch058.xhtml#ref-jha2019ml)).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，瞬时故障可能会影响机器学习预测的可靠性和可信度。如果在存储训练模型参数的内存或推理结果计算过程中发生瞬时故障，可能会导致预测结果错误或不一致。例如，神经网络激活值中的位翻转可能会改变最终的分类或回归输出（[Mahmoud
    等人 2020](ch058.xhtml#ref-mahmoud2020pytorchfi)）。在安全关键应用[25](#fn25)中，这些故障可能产生严重后果，导致错误的决策或行动，可能危害安全或导致系统故障（[G.
    李等人 2017](ch058.xhtml#ref-li2017understanding)；[S. Jha 等人 2019](ch058.xhtml#ref-jha2019ml)）。
- en: These vulnerabilities are particularly amplified in resource-constrained environments
    like TinyML, where limited computational and memory resources exacerbate their
    impact. One prominent example is Binarized Neural Networks (BNNs) ([Courbariaux
    et al. 2016](ch058.xhtml#ref-courbariaux2016binarized)), which represent network
    weights in single-bit precision to achieve computational efficiency and faster
    inference times. While this binary representation is advantageous for resource-constrained
    systems, it also makes BNNs particularly fragile to bit-flip errors. For instance,
    prior work ([Aygun, Gunes, and De Vleeschouwer 2021](ch058.xhtml#ref-Aygun2021BSBNN))
    has shown that a two-hidden-layer BNN architecture for a simple task such as MNIST
    classification suffers performance degradation from 98% test accuracy to 70% when
    random bit-flipping soft errors are inserted through model weights with a 10%
    probability. To address these vulnerabilities, techniques like flip-aware training
    and emerging approaches such as [stochastic computing](https://en.wikipedia.org/wiki/Stochastic_computing)[26](#fn26)
    are being explored to enhance fault tolerance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源受限的环境，如TinyML中，这些漏洞的影响尤其严重，有限的计算和内存资源加剧了它们的影响。一个突出的例子是二值化神经网络（BNNs）[Courbariaux
    等人 2016](ch058.xhtml#ref-courbariaux2016binarized)，它使用单比特精度来表示网络权重，以实现计算效率和更快的推理时间。虽然这种二进制表示对资源受限的系统有利，但它也使得BNNs特别容易受到位翻转错误的影响。例如，先前的研究([Aygun,
    Gunes, and De Vleeschouwer 2021](ch058.xhtml#ref-Aygun2021BSBNN))表明，一个用于简单任务如MNIST分类的两个隐藏层BNN架构，在通过模型权重以10%的概率插入随机位翻转软错误时，其性能从98%的测试准确率下降到70%。为了解决这些漏洞，正在探索诸如翻转感知训练和新兴方法如[随机计算](https://en.wikipedia.org/wiki/Stochastic_computing)[26](#fn26)等技术，以提高容错能力。
- en: Permanent Faults
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 永久性故障
- en: Transitioning from temporary disruptions to persistent issues, permanent faults
    are hardware defects that persist and cause irreversible damage to the affected
    components. These faults are characterized by their persistent nature and require
    repair or replacement of the faulty hardware to restore normal system functionality.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从暂时性中断过渡到持续性问题，永久性故障是持续存在并导致受影响组件不可逆损坏的硬件缺陷。这些故障以其持续性质为特征，需要修复或更换有故障的硬件才能恢复正常的系统功能。
- en: Permanent Fault Properties
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 永久性故障特性
- en: Permanent faults cause persistent and irreversible malfunctions in hardware
    components. The faulty component remains non-operational until it is repaired
    or replaced. These faults are consistent and reproducible, meaning the faulty
    behavior is observed every time the affected component is used. They can impact
    processors, memory modules, storage devices, or interconnects, potentially leading
    to system crashes, data corruption, or complete system failure.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障会导致硬件组件持续且不可逆的故障。有故障的组件将保持非工作状态，直到修复或更换。这些故障是一致的且可重复的，意味着每次使用受影响的组件时都会观察到故障行为。它们可能影响处理器、内存模块、存储设备或互连，可能导致系统崩溃、数据损坏或完全的系统故障。
- en: To illustrate the serious implications of permanent faults, a notable example
    is the [Intel FDIV bug](https://en.wikipedia.org/wiki/Pentium_FDIV_bug), discovered
    in 1994\. This flaw affected the floating-point division (FDIV) units of certain
    Intel Pentium processors, causing incorrect results for specific division operations
    and leading to inaccurate calculations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明永久性故障的严重后果，一个显著的例子是1994年发现的[英特尔FDIV错误](https://en.wikipedia.org/wiki/Pentium_FDIV_bug)，这个缺陷影响了某些英特尔奔腾处理器的浮点除法（FDIV）单元，导致特定除法操作的结果不正确，进而导致计算不准确。
- en: The FDIV bug occurred due to an error in the lookup table[27](#fn27) used by
    the division unit. In rare cases, the processor would fetch an incorrect value,
    resulting in a slightly less precise result than expected. For instance, [Figure 16.9](ch022.xhtml#fig-permanent-fault)
    shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV
    fault. The triangular regions highlight where erroneous calculations occurred.
    Ideally, all correct values would round to 1.3338, but the faulty results showed
    1.3337, indicating a mistake in the 5th digit.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: FDIV错误是由于除法单元使用的查找表[27](#fn27)中的错误引起的。在罕见的情况下，处理器会获取一个错误值，导致结果比预期略不精确。例如，[图16.9](ch022.xhtml#fig-permanent-fault)显示了在奔腾处理器上带有FDIV错误的4195835/3145727分数的绘图。三角形区域突出了错误计算发生的地方。理想情况下，所有正确的值都应该四舍五入到1.3338，但错误的结果显示为1.3337，表明第五位数字有错误。
- en: Although the error was small, it could compound across many operations, affecting
    results in precision-critical applications such as scientific simulations, financial
    calculations, and computer-aided design. The bug ultimately led to incorrect outcomes
    in these domains and underscored the severe consequences permanent faults can
    have.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然错误很小，但它可能在许多操作中累积，影响精度关键应用的结果，如科学模拟、金融计算和计算机辅助设计。这个错误最终导致这些领域的错误结果，并强调了永久性故障可能带来的严重后果。
- en: '![](../media/file255.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file255.png)'
- en: 'Figure 16.9: **FDIV Error Regions**: The triangular areas indicate where the
    pentium processor’s faulty division unit produced incorrect results when calculating
    4195835/3145727; ideally, all values should round to 1.3338, but the bug caused
    a slight inaccuracy in the fifth digit. Source: byte magazine.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：**FDIV错误区域**：三角形区域表示在计算4195835/3145727时，奔腾处理器的有缺陷的除法单元产生了错误结果；理想情况下，所有值都应该四舍五入到1.3338，但这个错误导致第五位数字有轻微的不准确。来源：byte杂志。
- en: The FDIV bug serves as a cautionary tale for ML systems. In such systems, permanent
    faults in hardware components can result in incorrect computations, impacting
    model accuracy and reliability. For example, if an ML system relies on a processor
    with a faulty floating-point unit, similar to the FDIV bug, it could introduce
    persistent errors during training or inference. These errors may propagate through
    the model, leading to inaccurate predictions or skewed learning outcomes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: FDIV错误是机器学习系统的警示故事。在这些系统中，硬件组件的永久性故障可能导致计算错误，影响模型准确性和可靠性。例如，如果一个机器学习系统依赖于一个带有有缺陷的浮点单元的处理器，类似于FDIV错误，它可能在训练或推理过程中引入持续的错误。这些错误可能会通过模型传播，导致预测不准确或学习结果偏差。
- en: This is especially critical in safety-sensitive applications[28](#fn28) explored
    in [Chapter 19](ch025.xhtml#sec-ai-good), where the consequences of incorrect
    computations can be severe. ML practitioners must be aware of these risks and
    incorporate fault-tolerant techniques, including hardware redundancy, error detection
    and correction, and robust algorithm design, to mitigate them. Thorough hardware
    validation and testing can help identify and resolve permanent faults before they
    affect system performance and reliability.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这在[第19章](ch025.xhtml#sec-ai-good)中探讨的安全敏感型应用[28](#fn28)中尤为重要，在这些应用中，计算错误可能带来严重后果。机器学习从业者必须意识到这些风险，并采用容错技术，包括硬件冗余、错误检测和纠正以及鲁棒算法设计，以减轻这些风险。彻底的硬件验证和测试有助于在影响系统性能和可靠性之前识别和解决永久性故障。
- en: Permanent Fault Origins
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 永久性故障起源
- en: 'Permanent faults can arise from two primary sources: manufacturing defects
    and wear-out mechanisms.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障可能源于两个主要来源：制造缺陷和磨损机制。
- en: The first category, [Manufacturing defects](https://www.sciencedirect.com/science/article/pii/B9780128181058000206),
    comprises flaws introduced during the fabrication process, including improper
    etching, incorrect doping, or contamination. These defects may result in non-functional
    or partially functional components. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/)
    occur over time due to prolonged use and operational stress. Phenomena like electromigration[29](#fn29),
    oxide breakdown[30](#fn30), and thermal stress[31](#fn31) degrade component integrity,
    eventually leading to permanent failure.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类，[制造缺陷](https://www.sciencedirect.com/science/article/pii/B9780128181058000206)，包括在制造过程中引入的缺陷，包括不正确的蚀刻、错误的掺杂或污染。这些缺陷可能导致非功能或部分功能的组件。相比之下，[磨损机制](https://semiengineering.com/what-causes-semiconductor-aging/)随着时间的推移由于长期使用和操作应力而发生。如电迁移[29](#fn29)、氧化物击穿[30](#fn30)和热应力[31](#fn31)等现象会降低组件的完整性，最终导致永久性故障。
- en: Permanent Fault Propagation
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 永久性故障传播
- en: Permanent faults manifest through several mechanisms, depending on their nature
    and location. A common example is the stuck-at fault ([Seong et al. 2010](ch058.xhtml#ref-seong2010safer)),
    where a signal or memory cell becomes permanently fixed at either 0 or 1, regardless
    of the intended input, as shown in [Figure 16.10](ch022.xhtml#fig-stuck-fault).
    This type of fault can occur in logic gates, memory cells, or interconnects and
    typically results in incorrect computations or persistent data corruption.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障通过多种机制表现出来，这取决于其性质和位置。一个常见的例子是固定故障([Seong等人2010](ch058.xhtml#ref-seong2010safer))，其中信号或存储单元永久固定在0或1，无论预期的输入如何，如图16.10所示([图16.10](ch022.xhtml#fig-stuck-fault))。此类故障可能发生在逻辑门、存储单元或互连中，通常会导致计算错误或持续的数据损坏。
- en: '![](../media/file256.svg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file256.svg)'
- en: 'Figure 16.10: **Stuck-at Fault Model**: Digital circuits can experience permanent
    faults where a signal line becomes fixed at a logical 0 or 1, regardless of input;
    this figure represents a simplified depiction of a stuck-at-0 fault, where a signal
    is persistently low, potentially leading to incorrect computations or system failures.
    *Source: [accendo reliability](HTTPS://accendoreliability.com/digital-circuits-stuck-fault-model/)*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：**固定故障模型**：数字电路可能会经历永久性故障，其中信号线固定在逻辑0或1，无论输入如何；此图表示固定在0的故障的简化表示，其中信号持续处于低电平，可能导致计算错误或系统故障。*来源：[accendo可靠性](HTTPS://accendoreliability.com/digital-circuits-stuck-fault-model/)*
- en: Other mechanisms include device failures, in which hardware components such
    as transistors or memory cells cease functioning entirely due to manufacturing
    defects or degradation over time. Bridging faults, which occur when two or more
    signal lines are unintentionally connected, can introduce short circuits or incorrect
    logic behaviors that are difficult to isolate.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其他机制包括设备故障，其中硬件组件如晶体管或存储单元由于制造缺陷或随时间退化而完全停止工作。桥接故障，当两个或多个信号线意外连接时发生，可能引入短路或难以隔离的错误逻辑行为。
- en: In more subtle cases, delay faults can arise when the propagation time of a
    signal exceeds the allowed timing constraints. The logical values may be correct,
    but the violation of timing expectations can still result in erroneous behavior.
    Similarly, interconnect faults, including open circuits caused by broken connections,
    high-resistance paths that impede current flow, and increased capacitance that
    distorts signal transitions, can significantly degrade circuit performance and
    reliability.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在更微妙的情况下，当信号的传播时间超过允许的时序约束时，可能会出现延迟故障。逻辑值可能是正确的，但时序预期的违反仍然可能导致错误的行为。同样，互连故障，包括由断开连接引起的开路、阻碍电流流动的高电阻路径以及扭曲信号转换的电容增加，可以显著降低电路性能和可靠性。
- en: Memory subsystems are particularly vulnerable to permanent faults. Transition
    faults can prevent a memory cell from successfully changing its state, while coupling
    faults result from unwanted interference between adjacent cells, leading to unintentional
    state changes. Neighborhood pattern sensitive faults occur when the state of a
    memory cell is incorrectly influenced by the data stored in nearby cells, reflecting
    a more complex interaction between circuit layout and logic behavior.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 存储子系统特别容易受到永久性故障的影响。转换故障可能阻止存储单元成功改变其状态，而耦合故障则源于相邻单元之间的不受欢迎的干扰，导致意外的状态变化。当存储单元的状态被附近存储的数据错误影响时，就会发生邻域模式敏感故障，这反映了电路布局和逻辑行为之间更复杂的相互作用。
- en: Permanent faults can also occur in critical infrastructure components such as
    the power supply network or clock distribution system. Failures in these subsystems
    can affect circuit-wide functionality, introduce timing errors, or cause widespread
    operational instability.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障也可能出现在关键基础设施组件中，如电源网络或时钟分配系统。这些子系统的故障可能会影响整个电路的功能，引入时间错误，或导致广泛的操作不稳定。
- en: Taken together, these mechanisms illustrate the varied and often complex ways
    in which permanent faults can undermine the behavior of computing systems. For
    ML applications in particular, where correctness and consistency are vital, understanding
    these fault modes is essential for developing resilient hardware and software
    solutions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些机制说明了永久性故障如何以多种和通常复杂的方式破坏计算系统的行为。对于机器学习应用尤其如此，在这些应用中，正确性和一致性至关重要，理解这些故障模式对于开发弹性硬件和软件解决方案至关重要。
- en: Permanent Fault Effects on ML
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 永久性故障对机器学习的影响
- en: Permanent faults can severely disrupt the behavior and reliability of computing
    systems. For example, a stuck-at fault in a processor’s arithmetic logic unit
    (ALU) can produce persistent computational errors, leading to incorrect program
    behavior or crashes. In memory modules, such faults may corrupt stored data, while
    in storage devices, they can result in bad sectors or total data loss. Interconnect
    faults may interfere with data transmission, leading to system hangs or corruption.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 永久性故障会严重破坏计算系统的行为和可靠性。例如，处理器算术逻辑单元（ALU）中的卡住故障可能会产生持续的计算错误，导致程序行为错误或崩溃。在内存模块中，此类故障可能会损坏存储的数据，而在存储设备中，它们可能导致坏扇区或数据完全丢失。互连故障可能会干扰数据传输，导致系统挂起或损坏。
- en: For ML systems, these faults pose significant risks in both training and inference
    phases. As with transient faults (Section X.X.X), permanent faults during training
    cause similar gradient calculation errors and parameter corruption, but persist
    until hardware replacement, requiring more comprehensive recovery strategies ([Yi
    He et al. 2023](ch058.xhtml#ref-he2023understanding)). Unlike transient faults
    that may only temporarily disrupt training, permanent faults in storage can compromise
    entire training datasets or saved models, affecting long-term consistency and
    reliability.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习系统来说，这些故障在训练和推理阶段都带来了显著的风险。与瞬态故障（第X.X.X节）一样，训练过程中的永久性故障会导致类似的梯度计算错误和参数损坏，但会持续到硬件更换，需要更全面的恢复策略（[Yi
    He 等人 2023](ch058.xhtml#ref-he2023understanding)）。与可能只是暂时中断训练的瞬态故障不同，存储中的永久性故障可能会损害整个训练数据集或保存的模型，影响长期的一致性和可靠性。
- en: In the inference phase, faults can distort prediction results or lead to runtime
    failures. For instance, errors in the hardware storing model weights might lead
    to outdated or corrupted models being used, while processor faults could yield
    incorrect outputs ([J. J. Zhang et al. 2018](ch058.xhtml#ref-zhang2018analyzing)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，故障可能会扭曲预测结果或导致运行时故障。例如，存储模型权重的硬件中的错误可能会导致使用过时或损坏的模型，而处理器故障可能会导致输出错误 ([J.
    J. Zhang 等人 2018](ch058.xhtml#ref-zhang2018analyzing))。
- en: Mitigating permanent faults requires comprehensive fault-tolerant design combining
    hardware redundancy and error-correcting codes ([J. Kim, Sullivan, and Erez 2015](ch058.xhtml#ref-kim2015bamboo))
    with software approaches like checkpoint and restart mechanisms[32](#fn32) ([Egwutuoha
    et al. 2013](ch058.xhtml#ref-egwutuoha2013survey)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解永久性故障需要综合的容错设计，结合硬件冗余和纠错码（[J. Kim, Sullivan, 和 Erez 2015](ch058.xhtml#ref-kim2015bamboo)）以及软件方法，如检查点和重启机制[32](#fn32)
    ([Egwutuoha 等人 2013](ch058.xhtml#ref-egwutuoha2013survey))。
- en: Regular monitoring, testing, and maintenance help detect and replace failing
    components before critical errors occur.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 定期监控、测试和维护有助于在发生关键错误之前检测和更换故障组件。
- en: Intermittent Faults
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 间歇性故障
- en: Intermittent faults are hardware faults that occur sporadically and unpredictably
    in a system. An example is illustrated in [Figure 16.11](ch022.xhtml#fig-intermittent-fault),
    where cracks in the material can introduce increased resistance in circuitry.
    These faults are particularly challenging to detect and diagnose because they
    appear and disappear intermittently, making it difficult to reproduce and isolate
    the root cause. Depending on their frequency and location, intermittent faults
    can lead to system instability, data corruption, and performance degradation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性故障是系统内偶然且不可预测发生的硬件故障。一个例子如图16.11所示，其中材料中的裂纹会在电路中引入增加的电阻。这些故障特别难以检测和诊断，因为它们是间歇性地出现和消失的，这使得重现和隔离根本原因变得困难。根据它们的频率和位置，间歇性故障可能导致系统不稳定、数据损坏和性能下降。
- en: '![](../media/file257.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file257.png)'
- en: 'Figure 16.11: **Intermittent Fault Mechanism**: Increased resistance from cracks
    between copper bumps and package solder represents a common source of intermittent
    faults, disrupting signal transmission and potentially causing unpredictable system
    behavior. Microscopic material defects like these highlight the vulnerability
    of hardware to latent failures that are difficult to detect during testing but
    can manifest during operation. Source: [constantinescu](HTTPS://ieeexplore.ieee.org/document/4925824).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11：**间歇性故障机制**：铜凸块与封装焊料之间的裂纹引起的电阻增加是间歇性故障的常见来源，这会干扰信号传输并可能导致不可预测的系统行为。这类微观材料缺陷凸显了硬件对难以在测试期间检测到的潜在故障的脆弱性，但这些故障可能在运行期间显现。来源：[constantinescu](HTTPS://ieeexplore.ieee.org/document/4925824)。
- en: Intermittent Fault Properties
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间歇性故障的特性
- en: Intermittent faults are defined by their sporadic and non-deterministic behavior.
    They occur irregularly and may manifest for short durations, disappearing without
    a consistent pattern. Unlike permanent faults, they do not appear every time the
    affected component is used, which makes them particularly difficult to detect
    and reproduce. These faults can affect a variety of hardware components, including
    processors, memory modules, storage devices, and interconnects. As a result, they
    may lead to transient errors, unpredictable system behavior, or data corruption.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性故障以其偶然性和非确定性行为为定义。它们不规则地发生，可能持续很短的时间，消失时没有一致的模式。与永久性故障不同，它们并非每次使用受影响的组件时都会出现，这使得它们特别难以检测和重现。这些故障可以影响各种硬件组件，包括处理器、内存模块、存储设备和互连。因此，它们可能导致瞬态错误、不可预测的系统行为或数据损坏。
- en: Their impact on system reliability can be significant. For instance, an intermittent
    fault in a processor’s control logic may disrupt the normal execution path, causing
    irregular program flow or unexpected system hangs. In memory modules, such faults
    can alter stored values inconsistently, leading to errors that are difficult to
    trace. Storage devices affected by intermittent faults may suffer from sporadic
    read/write errors or data loss, while intermittent faults in communication channels
    can cause data corruption, packet loss, or unstable connectivity. Over time, these
    failures can accumulate, degrading system performance and reliability ([Rashid,
    Pattabiraman, and Gopalakrishnan 2015](ch058.xhtml#ref-rashid2014characterizing)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 它们对系统可靠性的影响可能非常显著。例如，处理器控制逻辑中的间歇性故障可能会干扰正常的执行路径，导致程序流程不规则或系统意外挂起。在内存模块中，这类故障可能会不规律地改变存储值，导致难以追踪的错误。受间歇性故障影响的存储设备可能会出现间歇性的读写错误或数据丢失，而通信通道中的间歇性故障可能导致数据损坏、数据包丢失或不稳定的连接。随着时间的推移，这些故障可能会累积，降低系统性能和可靠性
    ([Rashid, Pattabiraman, and Gopalakrishnan 2015](ch058.xhtml#ref-rashid2014characterizing))。
- en: Intermittent Fault Origins
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间歇性故障的起源
- en: The causes of intermittent faults are diverse, ranging from physical degradation
    to environmental influences. One common cause is the aging and wear-out of electronic
    components. As hardware endures prolonged operation, thermal cycling, and mechanical
    stress, it may develop cracks, fractures, or fatigue that introduce intermittent
    faults. For instance, solder joints in ball grid arrays (BGAs) or flip-chip packages
    can degrade over time, leading to intermittent open circuits or short circuits.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性故障的原因多种多样，从物理退化到环境影响。一个常见的原因是电子元件的老化和磨损。随着硬件长时间运行、热循环和机械应力，它可能会出现裂纹、断裂或疲劳，从而引入间歇性故障。例如，球栅阵列（BGAs）或倒装芯片封装中的焊点可能会随时间退化，导致间歇性开路或短路。
- en: Manufacturing defects and process variations can also introduce marginal components
    that behave reliably under most circumstances but fail intermittently under stress
    or extreme conditions. For example, [Figure 16.12](ch022.xhtml#fig-intermittent-fault-dram)
    shows a residue-induced intermittent fault in a DRAM chip that leads to sporadic
    failures.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 制造缺陷和工艺变化也可能引入边缘组件，这些组件在大多数情况下表现可靠，但在压力或极端条件下会间歇性失效。例如，[图16.12](ch022.xhtml#fig-intermittent-fault-dram)
    展示了一个DRAM芯片中由残留物引起的间歇性故障，导致偶发故障。
- en: '![](../media/file258.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file258.png)'
- en: 'Figure 16.12: **DRAM Residue Fault**: Intermittent failures in DRAM chips commonly
    arise from microscopic residue accumulation, creating unreliable electrical connections.
    Physical defects can induce sporadic errors, highlighting the need for fault-tolerant
    system design and hardware testing via this figure. *Source: [hynix semiconductor](HTTPS://ieeexplore.ieee.org/document/4925824)*'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12：**DRAM残留故障**：DRAM芯片中的间歇性故障通常源于微观残留物的积累，造成不可靠的电气连接。物理缺陷可以引起偶发错误，突出了通过此图进行容错系统设计和硬件测试的必要性。*来源：[hynix半导体](HTTPS://ieeexplore.ieee.org/document/4925824)*
- en: Environmental factors such as thermal cycling, humidity, mechanical vibrations,
    or electrostatic discharge can exacerbate these weaknesses and trigger faults
    that would not otherwise appear. Loose or degrading physical connections, including
    those found in connectors or printed circuit boards, are also common sources of
    intermittent failures, particularly in systems exposed to movement or temperature
    variation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 环境因素，如热循环、湿度、机械振动或静电放电，可能会加剧这些弱点并触发本不会出现的故障。松散或退化的物理连接，包括在连接器或印制电路板上发现的连接，也是间歇性故障的常见来源，尤其是在暴露于运动或温度变化中的系统中。
- en: Intermittent Fault Propagation
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间歇性故障传播
- en: Intermittent faults can manifest through various physical and logical mechanisms
    depending on their root causes. One such mechanism is the intermittent open or
    short circuit, where physical discontinuities or partial connections cause signal
    paths to behave unpredictably. These faults may momentarily disrupt signal integrity,
    leading to glitches or unexpected logic transitions.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性故障可以通过各种物理和逻辑机制表现出来，具体取决于其根本原因。其中一种机制是间歇性开路或短路，物理不连续性或部分连接导致信号路径行为不可预测。这些故障可能会暂时破坏信号完整性，导致故障或意外的逻辑转换。
- en: Another common mechanism is the intermittent delay fault ([J. Zhang et al. 2018](ch058.xhtml#ref-zhang2018thundervolt)),
    where signal propagation times fluctuate due to marginal timing conditions, resulting
    in synchronization issues and incorrect computations. In memory cells or registers,
    intermittent faults can appear as transient bit flips or soft errors, corrupting
    data in ways that are difficult to detect or reproduce. Because these faults are
    often condition-dependent, they may only emerge under specific thermal, voltage,
    or workload conditions, adding further complexity to their diagnosis.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见机制是间歇性延迟故障（[张杰等，2018](ch058.xhtml#ref-zhang2018thundervolt)），由于边缘时序条件，信号传播时间波动，导致同步问题和计算错误。在存储单元或寄存器中，间歇性故障可能表现为瞬态位翻转或软错误，以难以检测或重现的方式损坏数据。由于这些故障通常是条件相关的，它们可能仅在特定的热、电压或工作负载条件下出现，从而增加了它们的诊断复杂性。
- en: Intermittent Fault Effects on ML
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间歇性故障对机器学习的影响
- en: Intermittent faults pose significant challenges for ML systems by undermining
    computational consistency and model reliability. During the training phase, such
    faults in processing units or memory can cause sporadic errors in the computation
    of gradients, weight updates, or loss values. These errors may not be persistent
    but can accumulate across iterations, degrading convergence and leading to unstable
    or suboptimal models. Intermittent faults in storage may corrupt input data or
    saved model checkpoints, further affecting the training pipeline ([Yi He et al.
    2023](ch058.xhtml#ref-he2023understanding)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 间歇性故障对机器学习系统构成了重大挑战，因为它破坏了计算一致性和模型可靠性。在训练阶段，处理单元或内存中的此类故障可能导致梯度计算、权重更新或损失值计算中的偶发错误。这些错误可能不是持续的，但可能会在迭代过程中累积，降低收敛性，导致不稳定或次优模型。存储中的间歇性故障可能会损坏输入数据或保存的模型检查点，进一步影响训练流程（[何毅等，2023](ch058.xhtml#ref-he2023understanding)）。
- en: In the inference phase, intermittent faults may result in inconsistent or erroneous
    predictions. Processing errors or memory corruption can distort activations, outputs,
    or intermediate representations of the model, particularly when faults affect
    model parameters or input data. Intermittent faults in data pipelines, such as
    unreliable sensors or storage systems, can introduce subtle input errors that
    degrade model robustness and output accuracy. In high-stakes applications like
    autonomous driving or medical diagnosis, these inconsistencies can result in dangerous
    decisions or failed operations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，间歇性故障可能导致不一致或错误的预测。处理错误或内存损坏可能会扭曲激活、输出或模型的中间表示，尤其是在故障影响模型参数或输入数据时。数据管道中的间歇性故障，如不可靠的传感器或存储系统，可能会引入微小的输入错误，降低模型的鲁棒性和输出精度。在自动驾驶或医疗诊断等高风险应用中，这些不一致性可能导致危险的决定或操作失败。
- en: Mitigating the effects of intermittent faults in ML systems requires a multi-layered
    approach ([Rashid, Pattabiraman, and Gopalakrishnan 2012](ch058.xhtml#ref-rashid2012intermittent)).
    At the hardware level, robust design practices, environmental controls, and the
    use of higher-quality or more reliable components can reduce susceptibility to
    fault conditions. Redundancy and error detection mechanisms can help identify
    and recover from transient manifestations of intermittent faults.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解机器学习中间歇性故障的影响需要多层次的方法([Rashid, Pattabiraman, and Gopalakrishnan 2012](ch058.xhtml#ref-rashid2012intermittent))。在硬件层面，稳健的设计实践、环境控制和使用更高质量或更可靠的组件可以减少对故障条件的敏感性。冗余和错误检测机制可以帮助识别和从间歇性故障的短暂表现中恢复。
- en: At the software level, techniques such as runtime monitoring, anomaly detection,
    and adaptive control strategies can provide resilience, integrating with the framework
    capabilities detailed in [Chapter 7](ch013.xhtml#sec-ai-frameworks) and deployment
    strategies from [Chapter 13](ch019.xhtml#sec-ml-operations). Data validation checks,
    outlier detection, model ensembling, and runtime model adaptation are examples
    of fault-tolerant methods that can be integrated into ML pipelines to improve
    reliability in the presence of sporadic errors.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件层面，运行时监控、异常检测和自适应控制策略等技术可以提供弹性，与第7章中详细说明的框架能力和第13章中的部署策略相结合。数据验证检查、异常值检测、模型集成和运行时模型自适应是可集成到机器学习管道中的容错方法，以提高存在间歇性错误时的可靠性。
- en: Designing ML systems that can gracefully handle intermittent faults maintains
    their accuracy, consistency, and dependability. This involves proactive fault
    detection, regular system monitoring, and ongoing maintenance to ensure early
    identification and remediation of issues. By embedding resilience into both the
    architecture and operational workflow detailed in [Chapter 13](ch019.xhtml#sec-ml-operations),
    ML systems can remain robust even in environments prone to sporadic hardware failures.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 设计能够优雅地处理间歇性故障的机器学习系统，可以保持其准确性、一致性和可靠性。这涉及到主动故障检测、定期系统监控和持续维护，以确保早期识别和修复问题。通过将弹性嵌入到第13章中详细说明的架构和操作流程中，机器学习系统即使在容易发生间歇性硬件故障的环境中也能保持稳健。
- en: Effective fault tolerance extends beyond detection to encompass adaptive performance
    management under varying system conditions. Comprehensive resource management
    strategies, including load balancing and dynamic scaling under fault conditions,
    are covered in [Chapter 13](ch019.xhtml#sec-ml-operations). For resource-constrained
    scenarios, adaptive model complexity reduction techniques, such as dynamic quantization
    and selective pruning in response to thermal or power constraints, are detailed
    in [Chapter 10](ch016.xhtml#sec-model-optimizations) and [Chapter 9](ch015.xhtml#sec-efficient-ai).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的容错不仅限于检测，还包括在变化系统条件下的自适应性能管理。第13章中涵盖了全面的资源管理策略，包括故障条件下的负载均衡和动态扩展。对于资源受限的场景，第10章中详细介绍了动态量化和响应于温度或电源限制的选择性剪枝等自适应模型复杂度降低技术，以及第9章中的高效人工智能。
- en: Hardware Fault Detection and Mitigation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件故障检测与缓解
- en: Fault detection techniques, including hardware-level and software-level approaches,
    and effective mitigation strategies enhance the resilience of ML systems. Resilient
    ML system design considerations, case studies, and research in fault-tolerant
    ML systems provide insights into building robust systems.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 故障检测技术，包括硬件级和软件级方法，以及有效的缓解策略，增强了机器学习系统的弹性。关于鲁棒机器学习系统设计考虑、案例研究和容错机器学习系统的研究，为构建稳健的系统提供了见解。
- en: Robust fault mitigation requires coordinated adaptation across the entire ML
    system stack. While the focus here is on fault detection and basic recovery mechanisms,
    comprehensive performance adaptation strategies are implemented through dynamic
    resource management ([Chapter 13](ch019.xhtml#sec-ml-operations)), fault-tolerant
    distributed training approaches ([Chapter 8](ch014.xhtml#sec-ai-training)), and
    adaptive model optimization techniques that maintain performance under resource
    constraints ([Chapter 10](ch016.xhtml#sec-model-optimizations), [Chapter 9](ch015.xhtml#sec-efficient-ai)).
    These adaptation strategies ensure that ML systems not only detect and recover
    from faults but also maintain optimal performance through intelligent resource
    allocation and model complexity adjustment. The future paradigms for more robust
    architectures that address fundamental vulnerabilities are explored in [Chapter 20](ch026.xhtml#sec-agi-systems).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现鲁棒的故障缓解，需要在整个机器学习系统堆栈中进行协调适应。虽然这里的重点是故障检测和基本恢复机制，但通过动态资源管理（[第13章](ch019.xhtml#sec-ml-operations)）、容错分布式训练方法（[第8章](ch014.xhtml#sec-ai-training)）以及适应资源约束下的性能保持的模型优化技术（[第10章](ch016.xhtml#sec-model-optimizations)，[第9章](ch015.xhtml#sec-efficient-ai)）实现了全面的性能适应策略。这些适应策略确保机器学习系统不仅能够检测和恢复故障，还能通过智能资源分配和模型复杂度调整保持最佳性能。在[第20章](ch026.xhtml#sec-agi-systems)中探讨了更鲁棒的架构的未来范式，这些架构旨在解决基本漏洞。
- en: Hardware Fault Detection Methods
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件故障检测方法
- en: Fault detection techniques identify and localize hardware faults in ML systems,
    building on the performance measurement principles from [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
    These techniques can be broadly categorized into hardware-level and software-level
    approaches, each offering unique capabilities and advantages.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 故障检测技术基于[第12章](ch018.xhtml#sec-benchmarking-ai)中的性能测量原则，在机器学习系统中识别和定位硬件故障。这些技术可以广泛地分为硬件级和软件级方法，每种方法都提供独特的功能和优势。
- en: Hardware-Level Detection
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 硬件级检测
- en: Hardware-level fault detection techniques are implemented at the physical level
    of the system and aim to identify faults in the underlying hardware components.
    Several hardware techniques exist, which can be categorized into the following
    groups.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件级故障检测技术是在系统的物理级别实现的，旨在识别底层硬件组件中的故障。存在几种硬件技术，可以归类为以下几组。
- en: Built-in self-test (BIST) Mechanisms
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内置自检（BIST）机制
- en: BIST is a powerful technique for detecting faults in hardware components ([Bushnell
    and Agrawal 2002](ch058.xhtml#ref-bushnell2002built)). It involves incorporating
    additional hardware circuitry into the system for self-testing and fault detection.
    BIST can be applied to various components, such as processors, memory modules,
    or application-specific integrated circuits (ASICs). For example, BIST can be
    implemented in a processor using scan chains[33](#fn33), which are dedicated paths
    that allow access to internal registers and logic for testing purposes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: BIST是一种强大的技术，用于检测硬件组件中的故障（[Bushnell和Agrawal 2002](ch058.xhtml#ref-bushnell2002built)）。它涉及将额外的硬件电路集成到系统中进行自检和故障检测。BIST可以应用于各种组件，例如处理器、内存模块或专用集成电路（ASIC）。例如，BIST可以通过使用扫描链[33](#fn33)在处理器中实现，这些是专门用于测试目的的路径，允许访问内部寄存器和逻辑。
- en: During the BIST process, predefined test patterns are applied to the processor’s
    internal circuitry, and the responses are compared against expected values. Any
    discrepancies indicate the presence of faults. Intel’s Xeon processors, for instance,
    include BIST mechanisms to test the CPU cores, cache memory, and other critical
    components during system startup.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在BIST过程中，预定义的测试模式被应用于处理器的内部电路，并将响应与预期值进行比较。任何差异都表明存在故障。例如，英特尔Xeon处理器包括BIST机制，在系统启动时测试CPU核心、缓存内存和其他关键组件。
- en: '![](../media/file259.svg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file259.svg)'
- en: 'Figure 16.13: **Parity Bit Error Detection**: This figure provides a simple
    error detection scheme where an extra bit (the parity bit) ensures the total number
    of 1s in a data sequence is either even or odd. The second sequence includes a
    flipped bit, triggering the parity check and indicating a data corruption event
    during transmission or storage. Source: computer hope.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：**奇偶校验位错误检测**：此图提供了一个简单的错误检测方案，其中额外的一位（奇偶校验位）确保数据序列中1的总数要么是偶数，要么是奇数。第二个序列包含一个翻转的位，触发奇偶校验并指示在传输或存储过程中发生的数据损坏事件。来源：电脑希望。
- en: Error Detection Codes
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 错误检测码
- en: 'Error detection codes are widely used to detect data storage and transmission
    errors ([Hamming 1950](ch058.xhtml#ref-hamming1950error))[34](#fn34). These codes
    add redundant bits to the original data, allowing the detection of bit errors.
    Example: Parity checks are a simple form of error detection code shown in [Figure 16.13](ch022.xhtml#fig-parity)[35](#fn35).
    In a single-bit parity scheme, an extra bit is appended to each data word, making
    the number of 1s in the word even (even parity) or odd (odd parity).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 错误检测码广泛用于检测数据存储和传输错误([Hamming 1950](ch058.xhtml#ref-hamming1950error))[34](#fn34)。这些码向原始数据添加冗余位，允许检测位错误。例如：奇偶校验是[图16.13](ch022.xhtml#fig-parity)[35](#fn35)中显示的简单错误检测码的一种形式。在单比特奇偶校验方案中，每个数据字附加一个额外的位，使得字中的1的数量为偶数（偶校验）或奇数（奇校验）。
- en: When reading the data, the parity is checked, and if it doesn’t match the expected
    value, an error is detected. More advanced error detection codes, such as cyclic
    redundancy checks (CRC)[36](#fn36), calculate a checksum based on the data and
    append it to the message.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取数据时，会检查奇偶校验，如果它与预期值不匹配，则会检测到错误。更高级的错误检测码，如循环冗余校验（CRC）[36](#fn36)，根据数据计算校验和并将其附加到消息中。
- en: Hardware redundancy and voting mechanisms
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 硬件冗余和投票机制
- en: Hardware redundancy involves duplicating critical components and comparing their
    outputs to detect and mask faults ([Sheaffer, Luebke, and Skadron 2007](ch058.xhtml#ref-sheaffer2007hardware)).
    Voting mechanisms, such as double modular redundancy (DMR)[37](#fn37) or triple
    modular redundancy (TMR)[38](#fn38), employ multiple instances of a component
    and compare their outputs to identify and mask faulty behavior ([Arifeen, Hassan,
    and Lee 2020](ch058.xhtml#ref-arifeen2020approximate)).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件冗余涉及复制关键组件，并比较它们的输出以检测和屏蔽故障([Sheaffer, Luebke, and Skadron 2007](ch058.xhtml#ref-sheaffer2007hardware))。投票机制，如双模块冗余（DMR）[37](#fn37)或三模块冗余（TMR）[38](#fn38)，使用多个组件实例，并比较它们的输出以识别和屏蔽故障行为([Arifeen,
    Hassan, and Lee 2020](ch058.xhtml#ref-arifeen2020approximate))。
- en: In a DMR or TMR system, two or three identical instances of a hardware component,
    such as a processor or a sensor, perform the same computation in parallel. The
    outputs of these instances are fed into a voting circuit, which compares the results
    and selects the majority value as the final output. If one of the instances produces
    an incorrect result due to a fault, the voting mechanism masks the error and maintains
    the correct output. TMR is commonly used in aerospace and aviation systems, where
    high reliability is critical. For instance, the Boeing 777 aircraft employs TMR
    in its primary flight computer system to ensure the availability and correctness
    of flight control functions ([Yeh, n.d.](ch058.xhtml#ref-yeh1996triple)).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在DMR或TMR系统中，两个或三个相同的硬件组件实例（如处理器或传感器）并行执行相同的计算。这些实例的输出被送入一个投票电路，该电路比较结果并选择多数值作为最终输出。如果一个实例由于故障产生错误的结果，投票机制将屏蔽错误并保持正确的输出。TMR在航空航天和航空系统中常用，因为这些系统对高可靠性至关重要。例如，波音777飞机在其主飞行计算机系统中采用TMR，以确保飞行控制功能的可用性和正确性([Yeh,
    n.d.](ch058.xhtml#ref-yeh1996triple))。
- en: Tesla’s self-driving computers, on the other hand, employ a DMR architecture
    to ensure the safety and reliability of critical functions such as perception,
    decision-making, and vehicle control, as shown in [Figure 16.14](ch022.xhtml#fig-tesla-dmr).
    In Tesla’s implementation, two identical hardware units, often called “redundant
    computers” or “redundant control units,” perform the same computations in parallel.
    Each unit independently processes sensor data, executes algorithms, and generates
    control commands for the vehicle’s actuators, such as steering, acceleration,
    and braking ([Bannon et al. 2019](ch058.xhtml#ref-bannon2019computer)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，特斯拉的自动驾驶计算机采用DMR架构以确保感知、决策和车辆控制等关键功能的安全性和可靠性，如图16.14所示。在特斯拉的实现中，两个相同的硬件单元（通常称为“冗余计算机”或“冗余控制单元”）并行执行相同的计算。每个单元独立处理传感器数据，执行算法，并为车辆的执行器（如转向、加速和制动）生成控制命令([Bannon等人2019](ch058.xhtml#ref-bannon2019computer))。
- en: '![](../media/file260.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file260.png)'
- en: 'Figure 16.14: **Dual Modular Redundancy**: Tesla’s full self-driving computer
    employs a DMR architecture, replicating critical computations across two independent
    system-on-chips (socs) to mitigate hardware faults and ensure continuous operation.
    This redundancy enables the system to mask errors: if one soc fails, the other
    continues functioning, maintaining safety-critical functions like perception and
    control. *Source: [Tesla](HTTPS://old.hotchips.org/hc31/HC31_2.3_tesla_hotchips_ppt_final_0817.PDF)*'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14：**双重模块冗余**：特斯拉的全自动驾驶计算机采用DMR架构，在两个独立的系统芯片（SoC）上复制关键计算，以减轻硬件故障并确保持续运行。这种冗余使得系统能够掩盖错误：如果一个SoC失败，另一个将继续运行，保持安全关键功能如感知和控制。*来源：[特斯拉](HTTPS://old.hotchips.org/hc31/HC31_2.3_tesla_hotchips_ppt_final_0817.PDF)*
- en: The outputs of these two redundant units are continuously compared to detect
    any discrepancies or faults. If the outputs match, the system assumes that both
    units function correctly, and the control commands are sent to the vehicle’s actuators.
    However, if a mismatch occurs between the outputs, the system identifies a potential
    fault in one of the units and takes appropriate action to ensure safe operation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个冗余单元的输出会持续进行比较，以检测任何差异或故障。如果输出匹配，系统假定这两个单元都正常工作，并将控制命令发送到车辆的执行器。然而，如果输出之间出现不匹配，系统会识别出其中一个单元可能存在的潜在故障，并采取适当的行动以确保安全运行。
- en: DMR in Tesla’s self-driving computer provides an extra safety and fault tolerance
    layer. By having two independent units performing the same computations, the system
    can detect and mitigate faults that may occur in one of the units. This redundancy
    helps prevent single points of failure and ensures that critical functions remain
    operational despite hardware faults.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉自动驾驶计算机中的DMR提供额外的安全性和容错层。通过有两个独立单元执行相同的计算，系统可以检测并减轻可能出现在其中一个单元中的故障。这种冗余有助于防止单点故障，并确保即使在硬件故障的情况下，关键功能也能保持运行。
- en: The system may employ additional mechanisms to determine which unit is faulty
    in a mismatch. This can involve using diagnostic algorithms, comparing the outputs
    with data from other sensors or subsystems, or analyzing the consistency of the
    outputs over time. Once the faulty unit is identified, the system can isolate
    it and continue operating using the output from the non-faulty unit.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 系统可能采用额外的机制来确定哪个单元在不匹配的情况下出现故障。这可能涉及使用诊断算法，将输出与其他传感器或子系统的数据进行比较，或分析输出随时间的一致性。一旦确定了故障单元，系统可以将其隔离，并继续使用非故障单元的输出运行。
- en: Tesla also incorporates redundancy mechanisms beyond DMR. For example, they
    use redundant power supplies, steering and braking systems, and diverse sensor
    suites[39](#fn39) (e.g., cameras, radar, and ultrasonic sensors) to provide multiple
    layers of fault tolerance.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉还采用了除DMR之外的冗余机制。例如，他们使用冗余电源、转向和制动系统，以及多样化的传感器套件[39](#fn39)（例如，摄像头、雷达和超声波传感器），以提供多层次的容错能力。
- en: While DMR provides fault detection and some level of fault tolerance, TMR may
    provide a different level of fault masking. In DMR, if both units experience simultaneous
    faults or the fault affects the comparison mechanism, the system may be unable
    to identify the fault. Therefore, Tesla’s SDCs rely on a combination of DMR and
    other redundancy mechanisms to achieve a high level of fault tolerance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DMR 提供了故障检测和一定程度的容错能力，但 TMR 可能提供不同级别的故障掩盖。在 DMR 中，如果两个单元同时发生故障或故障影响了比较机制，系统可能无法识别故障。因此，特斯拉的自动驾驶计算机依赖于
    DMR 和其他冗余机制的组合来实现高水平的容错能力。
- en: The use of DMR in Tesla’s self-driving computer highlights the importance of
    hardware redundancy in applications requiring high reliability. By employing redundant
    computing units and comparing their outputs, the system can detect and mitigate
    faults, enhancing the overall safety and reliability of the self-driving functionality.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在特斯拉的自动驾驶计算机中使用 DMR 强调了在需要高可靠性的应用中硬件冗余的重要性。通过使用冗余计算单元并比较它们的输出，系统可以检测和减轻故障，从而提高自动驾驶功能的整体安全和可靠性。
- en: Another approach to hardware redundancy is the use of hot spares[40](#fn40),
    as employed by Google in its data centers to address SDC during ML training. Unlike
    DMR and TMR, which rely on parallel processing and voting mechanisms to detect
    and mask faults, hot spares provide fault tolerance by maintaining backup hardware
    units that can seamlessly take over computations when a fault is detected. As
    illustrated in [Figure 16.15](ch022.xhtml#fig-sdc-controller), during normal ML
    training, multiple synchronous training workers process data in parallel. However,
    if a worker becomes defective and causes SDC, an SDC checker automatically identifies
    the issues. Upon detecting the SDC, the SDC checker moves the training to a hot
    spare and sends the defective machine for repair. This redundancy safeguards the
    continuity and reliability of ML training, effectively minimizing downtime and
    preserving data integrity.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种硬件冗余的方法是使用热备件[40](#fn40)，正如谷歌在其数据中心中用于解决机器学习训练期间的自动驾驶计算机问题所做的那样。与依赖于并行处理和投票机制来检测和掩盖故障的
    DMR 和 TMR 不同，热备件通过维护备份硬件单元来提供容错能力，当检测到故障时，这些单元可以无缝接管计算。如图16.15[图16.15](ch022.xhtml#fig-sdc-controller)所示，在正常的机器学习训练期间，多个同步训练工作者并行处理数据。然而，如果一个工作者出现故障并导致自动驾驶计算机问题，自动驾驶计算机检查器会自动识别问题。检测到自动驾驶计算机问题后，自动驾驶计算机检查器将训练转移到热备件，并将故障机器送修。这种冗余确保了机器学习训练的连续性和可靠性，有效地最小化了停机时间并保护了数据完整性。
- en: '![](../media/file261.svg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file261.svg)'
- en: 'Figure 16.15: **Hot Spare Redundancy**: Google’s data centers utilize hot spare
    cores to maintain uninterrupted ML training despite hardware failures, seamlessly
    transitioning workloads from defective machines to backup resources. This approach
    contrasts with parallel redundancy techniques like DMR/TMR by providing a reactive
    fault tolerance mechanism that minimizes downtime and preserves data integrity
    during ML training. Source: jeff dean, mlsys 2024 keynote (Google).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15：**热备冗余**：谷歌的数据中心利用热备核心来维持不间断的机器学习训练，即使在硬件故障的情况下也能无缝地将工作负载从故障机器转移到备份资源。这种方法与DMR/TMR等并行冗余技术形成对比，它提供了一种反应式容错机制，最小化停机时间并保护机器学习训练期间的数据完整性。来源：杰夫·迪恩，mlsys
    2024 大会演讲（谷歌）。
- en: Watchdog timers
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 看门狗定时器
- en: Watchdog timers are hardware components that monitor the execution of critical
    tasks or processes ([Pont and Ong 2002](ch058.xhtml#ref-pont2002using)). They
    are commonly used to detect and recover from software or hardware faults that
    cause a system to become unresponsive or stuck in an infinite loop. In an embedded
    system, a watchdog timer can be configured to monitor the execution of the main
    control loop, as illustrated in [Figure 16.16](ch022.xhtml#fig-watchdog). The
    software periodically resets the watchdog timer to indicate that it functions
    correctly. Suppose the software fails to reset the timer within a specified time
    limit (timeout period). In that case, the watchdog timer assumes that the system
    has encountered a fault and triggers a predefined recovery action, such as resetting
    the system or switching to a backup component. Watchdog timers are widely used
    in automotive electronics, industrial control systems, and other safety-critical
    applications to ensure the timely detection and recovery from faults.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 看门狗定时器是监控关键任务或进程的硬件组件（[Pont和Ong 2002](ch058.xhtml#ref-pont2002using)）。它们通常用于检测和从导致系统无响应或陷入无限循环的软件或硬件故障中恢复。在嵌入式系统中，看门狗定时器可以配置为监控主控制循环的执行，如图16.16（ch022.xhtml#fig-watchdog）所示。软件定期重置看门狗定时器以指示其正常工作。假设软件未能在一个指定的时间限制（超时期间）内重置定时器，那么看门狗定时器假定系统遇到了故障，并触发一个预定义的恢复操作，例如重置系统或切换到备用组件。看门狗定时器在汽车电子、工业控制系统和其他安全关键应用中得到广泛应用，以确保及时检测和从故障中恢复。
- en: '![](../media/file262.svg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file262.svg)'
- en: 'Figure 16.16: **Watchdog Timer Operation**: Embedded systems utilize watchdog
    timers to detect and recover from software or hardware faults by periodically
    resetting a timeout counter; failure to reset within the allotted time triggers
    a system reset or recovery action, ensuring continued operation. Source: [ablic](https://www.ablic.com/en/semicon/products/automotive/automotive-watchdog-timer/intro/)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16：**看门狗定时器操作**：嵌入式系统利用看门狗定时器通过定期重置超时计数器来检测和从软件或硬件故障中恢复；如果在规定时间内未能重置，则触发系统重置或恢复操作，确保持续运行。来源：[ablic](https://www.ablic.com/en/semicon/products/automotive/automotive-watchdog-timer/intro/)
- en: Software-Level Detection
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软件级检测
- en: Software-level fault detection techniques rely on software algorithms and monitoring
    mechanisms to identify system faults. These techniques can be implemented at various
    levels of the software stack, including the operating system, middleware, or application
    level.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 软件级故障检测技术依赖于软件算法和监控机制来识别系统故障。这些技术可以在软件堆栈的各个级别实现，包括操作系统、中间件或应用层。
- en: Runtime monitoring and anomaly detection
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 运行时监控和异常检测
- en: Runtime monitoring involves continuously observing the behavior of the system
    and its components during execution ([Francalanza et al. 2017](ch058.xhtml#ref-francalanza2017foundation)),
    extending the operational monitoring practices from [Chapter 13](ch019.xhtml#sec-ml-operations).
    It helps detect anomalies, errors, or unexpected behavior that may indicate the
    presence of faults. For example, consider an ML-based image classification system
    deployed in a self-driving car. Runtime monitoring can be implemented to track
    the classification model’s performance and behavior ([Mahmoud et al. 2021](ch058.xhtml#ref-mahmoud2021issre)).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时监控涉及在执行过程中持续观察系统和其组件的行为（[Francalanza等人2017](ch058.xhtml#ref-francalanza2017foundation)），扩展了第13章（ch019.xhtml#sec-ml-operations）中的操作监控实践。它有助于检测异常、错误或可能表明存在故障的意外行为。例如，考虑一个部署在自动驾驶汽车中的基于机器学习的图像分类系统。运行时监控可以实施以跟踪分类模型的性能和行为（[Mahmoud等人2021](ch058.xhtml#ref-mahmoud2021issre)）。
- en: Anomaly detection algorithms can be applied to the model’s predictions or intermediate
    layer activations, such as statistical outlier detection or machine learning-based
    approaches (e.g., One-Class SVM or Autoencoders) ([Chandola, Banerjee, and Kumar
    2009](ch058.xhtml#ref-chandola2009anomaly)). [Figure 16.17](ch022.xhtml#fig-ad)
    shows example of anomaly detection. Suppose the monitoring system detects a significant
    deviation from the expected patterns, such as a sudden drop in classification
    accuracy or out-of-distribution samples. In that case, it can raise an alert indicating
    a potential fault in the model or the input data pipeline. This early detection
    allows for timely intervention and fault mitigation strategies to be applied.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测算法可以应用于模型的预测或中间层激活，例如统计异常值检测或基于机器学习的方法（例如，单类SVM或自编码器）([Chandola, Banerjee,
    and Kumar 2009](ch058.xhtml#ref-chandola2009anomaly)）。图16.17展示了异常检测的示例。假设监控系统检测到与预期模式显著偏差的情况，例如分类准确性的突然下降或分布外的样本。在这种情况下，它可以发出警报，表明模型或输入数据管道可能存在潜在故障。这种早期检测允许及时采取干预措施和故障缓解策略。
- en: '![](../media/file263.svg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file263.svg)'
- en: 'Figure 16.17: **Anomaly Detection With SVM**: Support vector machines identify
    deviations from normal system behavior by mapping log data into a high-dimensional
    space and defining boundaries around expected values, enabling the detection of
    potential faults. Unsupervised anomaly detection techniques, like the one shown,
    are particularly valuable when labeled fault data is scarce, allowing systems
    to learn patterns from unlabeled operational data. Source: [Google](HTTPS://www.Google.com/url?sa=i&url=HTTP%3A%2F%2fresearch.Google%2fblog%2funsupervised-and-semi-supervised-)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17：**使用SVM进行异常检测**：支持向量机通过将日志数据映射到高维空间并定义预期值周围的边界来识别正常系统行为的偏差，从而实现潜在故障的检测。如图所示的无监督异常检测技术，在标记的故障数据稀缺时尤其有价值，允许系统从未标记的操作数据中学习模式。来源：[Google](https://www.google.com/url?sa=i&url=HTTP%3A%2F%2Fresearch.google.com%2fblog%2funsupervised-and-semi-supervised-)
- en: Consistency checks and data validation
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 一致性检查和数据验证
- en: 'Consistency checks and data validation techniques ensure data integrity and
    correctness at different processing stages in an ML system ([A. Lindholm et al.
    2019](ch058.xhtml#ref-lindholm2019data)). These checks help detect data corruption,
    inconsistencies, or errors that may propagate and affect the system’s behavior.
    Example: In a distributed ML system where multiple nodes collaborate to train
    a model, consistency checks can be implemented to validate the integrity of the
    shared model parameters. Each node can compute a checksum or hash of the model
    parameters before and after the training iteration, as shown in [Figure 16.17](ch022.xhtml#fig-ad).
    Any inconsistencies or data corruption can be detected by comparing the checksums
    across nodes. Range checks can be applied to the input data and model outputs
    to ensure they fall within expected bounds. For instance, if an autonomous vehicle’s
    perception system detects an object with unrealistic dimensions or velocities,
    it can indicate a fault in the sensor data or the perception algorithms ([Wan
    et al. 2023](ch058.xhtml#ref-wan2023vpp)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性检查和数据验证技术确保了在机器学习系统不同处理阶段的数据完整性和正确性（[A. Lindholm等人 2019](ch058.xhtml#ref-lindholm2019data)）。这些检查有助于检测数据损坏、不一致或错误，这些错误可能会传播并影响系统的行为。例如，在一个分布式机器学习系统中，多个节点协作训练模型时，可以实施一致性检查以验证共享模型参数的完整性。每个节点可以在训练迭代前后计算模型参数的校验和或哈希值，如图16.17所示。通过比较节点间的校验和，可以检测到任何不一致或数据损坏。范围检查可以应用于输入数据和模型输出，以确保它们在预期的范围内。例如，如果自动驾驶车辆的感觉系统检测到具有不切实际尺寸或速度的对象，这可能表明传感器数据或感知算法存在故障（[Wan等人
    2023](ch058.xhtml#ref-wan2023vpp)）。
- en: Heartbeat and timeout mechanisms
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 心跳和超时机制
- en: Heartbeat mechanisms and timeouts are commonly used to detect faults in distributed
    systems and ensure the liveness and responsiveness of components ([Kawazoe Aguilera,
    Chen, and Toueg 1997](ch058.xhtml#ref-kawazoe1997heartbeat)). These are quite
    similar to the watchdog timers found in hardware. For example, in a distributed
    ML system, where multiple nodes collaborate to perform tasks such as data preprocessing,
    model training, or inference, heartbeat mechanisms can be implemented to monitor
    the health and availability of each node. Each node periodically sends a heartbeat
    message to a central coordinator or its peer nodes, indicating its status and
    availability. Suppose a node fails to send a heartbeat within a specified timeout
    period, as shown in [Figure 16.18](ch022.xhtml#fig-heartbeat). In that case, it
    is considered faulty, and appropriate actions can be taken, such as redistributing
    the workload or initiating a failover mechanism. Given that network partitions
    affect 1-10% of nodes daily in large distributed training clusters, these heartbeat
    systems must distinguish between node failures and network connectivity issues
    to avoid unnecessary failover operations that could disrupt training progress.
    Timeouts can also be used to detect and handle hanging or unresponsive components.
    For example, if a data loading process exceeds a predefined timeout threshold,
    it may indicate a fault in the data pipeline, and the system can take corrective
    measures.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 心跳机制和超时通常用于检测分布式系统中的故障，并确保组件的活性和响应性（[Kawazoe Aguilera, Chen, and Toueg 1997](ch058.xhtml#ref-kawazoe1997heartbeat)）。这些与硬件中发现的看门狗定时器非常相似。例如，在一个分布式机器学习系统中，多个节点协作执行数据预处理、模型训练或推理等任务，可以实现心跳机制来监控每个节点的健康和可用性。每个节点定期向中央协调器或其对等节点发送心跳消息，表明其状态和可用性。假设一个节点在指定的超时时间内未能发送心跳，如图16.18所示。在这种情况下，它被认为是故障的，可以采取适当的措施，例如重新分配工作负载或启动故障转移机制。鉴于网络分区每天影响大型分布式训练集群中的1-10%的节点，这些心跳系统必须区分节点故障和网络连接问题，以避免不必要的故障转移操作，这可能会干扰训练进度。超时还可以用于检测和处理挂起或无响应的组件。例如，如果数据加载过程超过预定义的超时阈值，这可能表明数据管道存在故障，系统可以采取纠正措施。
- en: '![](../media/file264.svg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file264.svg)'
- en: 'Figure 16.18: **Heartbeat and Timeout**: Distributed Systems Employ Periodic
    Heartbeat Messages to Detect Node Failures; A Lack of Response Within a Defined
    Timeout Indicates a Fault, Triggering Corrective Actions Like Workload Redistribution
    or Failover. This Mechanism, Analogous to Watchdog Timers, Ensures System Robustness
    and Continuous Operation Despite Component Failures. Source: [geeksforgeeks](HTTPS://www.geeksforgeeks.org/what-are-heartbeat-messages/).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18：**心跳和超时**：分布式系统通过定期发送心跳消息来检测节点故障；在定义的超时时间内没有响应表明存在故障，从而触发纠正措施，如工作负载重新分配或故障转移。这种机制类似于看门狗定时器，即使在组件故障的情况下也能确保系统的健壮性和持续运行。来源：[geeksforgeeks](HTTPS://www.geeksforgeeks.org/what-are-heartbeat-messages/).
- en: Software-implemented fault tolerance (SIFT) techniques
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 软件实现的容错（SIFT）技术
- en: 'SIFT techniques introduce redundancy and fault detection mechanisms at the
    software level to improve the reliability and fault tolerance of the system ([Reis
    et al., n.d.](ch058.xhtml#ref-reis2005swift)). Example: N-version programming
    is a SIFT technique where multiple functionally equivalent software component
    versions are developed independently by different teams. This can be applied to
    critical components such as the model inference engine in an ML system. Multiple
    versions of the inference engine can be executed in parallel, and their outputs
    can be compared for consistency. It is considered the correct result if most versions
    produce the same output. A discrepancy indicates a potential fault in one or more
    versions, triggering appropriate error-handling mechanisms. Another example is
    using software-based error correction codes, such as Reed-Solomon codes ([Plank
    1997](ch058.xhtml#ref-plank1997tutorial)), to detect and correct errors in data
    storage or transmission, as shown in [Figure 16.19](ch022.xhtml#fig-Reed-Solomon).
    These codes add redundancy to the data, enabling detecting and correcting certain
    errors and enhancing the system’s fault tolerance.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT技术在软件层面引入冗余和故障检测机制，以提高系统的可靠性和容错性（[Reis等人，未发表](ch058.xhtml#ref-reis2005swift)）。例如：N版本编程是一种SIFT技术，其中多个功能等效的软件组件版本由不同的团队独立开发。这可以应用于关键组件，如机器学习系统中的模型推理引擎。推理引擎的多个版本可以并行执行，并比较它们的输出以检查一致性。如果大多数版本产生相同的输出，则被认为是正确的结果。差异表明一个或多个版本中可能存在潜在故障，从而触发适当的错误处理机制。另一个例子是使用基于软件的错误纠正码，如里德-所罗门码([Plank
    1997](ch058.xhtml#ref-plank1997tutorial))，来检测和纠正数据存储或传输中的错误，如图16.19所示。这些码向数据添加冗余，能够检测和纠正某些错误，并增强系统的容错性。
- en: '![](../media/file265.svg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file265.svg)'
- en: 'Figure 16.19: **Heartbeat Monitoring**: Redundant Node Connections and Periodic
    Heartbeat Messages Detect and Isolate Failing Components in Distributed Systems,
    Ensuring Continued Operation Despite Hardware Faults. These Mechanisms Enable
    Fault Tolerance by Allowing Nodes to Identify Unresponsive Peers and Reroute Communication
    Accordingly. Source: [geeksforgeeks](HTTPS://www.geeksforgeeks.org/what-is-reed-solomon-code/).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.19：**心跳监控**：冗余节点连接和周期性心跳消息检测和隔离分布式系统中的故障组件，确保在硬件故障的情况下继续运行。这些机制通过允许节点识别无响应的节点并相应地重新路由通信，从而实现容错。来源：[geeksforgeeks](HTTPS://www.geeksforgeeks.org/what-is-reed-solomon-code/).
- en: Hardware Fault Summary
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件故障摘要
- en: '[Table 16.3](ch022.xhtml#tbl-fault_types) provides a comparative analysis of
    transient, permanent, and intermittent faults. It outlines the primary characteristics
    or dimensions that distinguish these fault types. Here, we summarize the relevant
    dimensions we examined and explore the nuances that differentiate transient, permanent,
    and intermittent faults in greater detail.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[表16.3](ch022.xhtml#tbl-fault_types)提供了瞬态、永久性和间歇性故障的比较分析。它概述了区分这些故障类型的主要特征或维度。在此，我们总结了我们考察的相关维度，并更详细地探讨了区分瞬态、永久性和间歇性故障的细微差别。'
- en: While hardware faults represent one dimension of system vulnerability, they
    rarely occur in isolation. The physical failures we have examined often interact
    with and expose weaknesses in the algorithmic components of AI systems. This interconnection
    becomes particularly evident when we consider how adversaries might exploit model
    vulnerabilities through carefully crafted inputs—the focus of our next section
    on Input-Level Attacks.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然硬件故障代表了系统脆弱性的一个维度，但它们很少单独发生。我们考察的物理故障通常与人工智能系统的算法组件相互作用，并暴露出其弱点。当我们考虑对手如何通过精心设计的输入来利用模型漏洞时，这种互联性变得尤为明显——这是我们下一节关于输入级攻击的重点。
- en: 'Table 16.3: **Fault Characteristics**: Transient, permanent, and intermittent
    faults differ by duration, persistence, and recurrence, impacting system reliability
    and requiring distinct mitigation strategies for robust AI deployments. Understanding
    these distinctions guides the design of fault-tolerant systems capable of handling
    diverse hardware failures during operation.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.3：**故障特性**：瞬态故障、永久性故障和间歇性故障在持续时间、持续性和复发性方面有所不同，影响系统可靠性，并需要针对稳健人工智能部署采取不同的缓解策略。理解这些区别有助于设计能够处理操作期间各种硬件故障的容错系统。
- en: '| **Dimension** | **Transient Faults** | **Permanent Faults** | **Intermittent
    Faults** |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| **维度** | **瞬态故障** | **永久性故障** | **间歇性故障** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Duration** | Short-lived, temporary | Persistent, remains until repair
    or replacement | Sporadic, appears and disappears intermittently |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| **持续时间** | 短暂的、暂时的 | 持久的、直到修复或更换 | 偶然出现和消失 |'
- en: '| **Persistence** | Disappears after the fault condition passes | Consistently
    present until addressed | Recurs irregularly, not always present |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| **持续性** | 故障条件通过后消失 | 一致存在，直到问题解决 | 不规则地重复出现，不一定总是存在 |'
- en: '| **Causes** | External factors (e.g., electromagnetic interference cosmic
    rays) | Hardware defects, physical damage, wear-out | Unstable hardware conditions,
    loose connections, aging components |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| **原因** | 外部因素（例如，电磁干扰、宇宙射线） | 硬件缺陷、物理损坏、磨损 | 不稳定的硬件条件、松散的连接、老化组件 |'
- en: '| **Manifestation** | Bit flips, glitches, temporary data corruption | Stuck-at
    faults, broken components, complete device failures | Occasional bit flips, intermittent
    signal issues, sporadic malfunctions |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| **表现** | 比特翻转、故障、暂时性数据损坏 | 卡在故障、损坏的组件、完全设备故障 | 偶尔发生比特翻转、间歇性信号问题、偶然故障 |'
- en: '| **Impact on ML** | Introduces temporary errors | Causes consistent errors
    or | Leads to sporadic and unpredictable errors, |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| **对机器学习的影响** | 引入暂时性错误 | 导致持续性的错误或 | 导致偶然和不可预测的错误， |'
- en: '| **Systems** | or noise in computations | failures, affecting reliability
    | challenging to diagnose and mitigate |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| **系统** | 或计算中的噪声 | 故障，影响可靠性 | 诊断和缓解都很有挑战性 |'
- en: '| **Detection** | Error detection codes, comparison with expected values |
    Built-in self-tests, error detection codes, consistency checks | Monitoring for
    anomalies, analyzing error patterns and correlations |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| **检测** | 错误检测码，与预期值比较 | 内置自检，错误检测码，一致性检查 | 监测异常，分析错误模式和相关性 |'
- en: '| **Mitigation** | Error correction codes, redundancy, checkpoint and restart
    | Hardware repair or replacement, component redundancy, failover mechanisms |
    Robust design, environmental control, runtime monitoring, fault-tolerant techniques
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| **缓解措施** | 错误纠正码、冗余、检查点和重启 | 硬件维修或更换、组件冗余、故障转移机制 | 坚固的设计、环境控制、运行时监控、容错技术
    |'
- en: Intentional Input Manipulation
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故意输入操纵
- en: Input-level attacks represent a different threat model from unintentional hardware
    failures. Unlike random bit flips and component failures, these attacks involve
    deliberate manipulation of data to compromise system behavior. These sophisticated
    attempts manipulate ML model behavior through carefully crafted inputs or corrupted
    training data. These attack vectors can amplify the impact of hardware faults,
    for instance, when adversaries craft inputs specifically designed to trigger edge
    cases in fault-compromised hardware.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 输入级攻击代表了一种与无意硬件故障不同的威胁模型。与随机比特翻转和组件故障不同，这些攻击涉及对数据进行故意操纵，以损害系统行为。这些复杂的尝试通过精心设计的输入或损坏的训练数据来操纵机器学习模型的行为。这些攻击向量可以放大硬件故障的影响，例如，当对手精心设计输入以触发故障受损硬件的边缘情况时。
- en: Adversarial Attacks
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗性攻击
- en: Conceptual Foundation
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念基础
- en: 'At its core, an adversarial attack is surprisingly simple: add tiny, calculated
    changes to an input that fool a model while remaining invisible to humans. Imagine
    adjusting a few pixels in a photo of a cat, changes so subtle you cannot see them,
    yet the model suddenly classifies it as a toaster with 99% confidence. This counterintuitive
    vulnerability stems from how neural networks process information differently than
    humans do.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，对抗性攻击出奇地简单：向输入添加微小的、计算出的变化，以欺骗模型，同时对人来说却不可见。想象一下调整一张猫的照片中的几个像素，变化如此微妙以至于你看不见，但模型突然将其分类为烤面包机，置信度为99%。这种反直觉的漏洞源于神经网络处理信息的方式与人类不同。
- en: 'To understand the underlying mechanism through analogy, consider a person who
    has learned to identify cats by looking primarily for pointy ears. An adversarial
    attack is like showing this person a picture of a dog, but carefully drawing tiny,
    almost invisible pointy ears on top of the dog’s floppy ears. Because the person’s
    algorithm is overly reliant on the pointy ear feature, they confidently misclassify
    the dog as a cat. This is how adversarial attacks work: they find the specific,
    often superficial, features a model relies on and exploit them, even if the changes
    are meaningless to a human observer.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 通过类比来理解潜在机制，考虑一个通过主要寻找尖耳朵来识别猫的人。对抗攻击就像向这个人展示一张狗的照片，但仔细地在狗的耷拉耳朵上画上微小、几乎看不见的尖耳朵。因为这个人的算法过度依赖于尖耳朵特征，他们自信地将狗错误地分类为猫。这就是对抗攻击的工作方式：它们找到模型依赖的特定、通常是表面的特征并利用它们，即使这些变化对人类观察者来说没有意义。
- en: ML models learn statistical patterns rather than semantic understanding. They
    operate in high-dimensional spaces where decision boundaries can be surprisingly
    fragile. Small movements in this space, imperceptible in the input domain, can
    cross these boundaries and trigger misclassification.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型学习的是统计模式而不是语义理解。它们在高度维度的空间中运行，其中决策边界可能非常脆弱。在这个空间中的微小移动，在输入域中几乎不可察觉，可以跨越这些边界并触发错误分类。
- en: Technical Mechanisms
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术机制
- en: Adversarial attacks exploit ML models’ sensitivity to small input perturbations
    that are imperceptible to humans but cause dramatic changes in model outputs.
    These attacks reveal vulnerabilities in how models learn decision boundaries and
    generalize from training data. The mathematical foundation relies on the model’s
    gradient information to identify the most effective perturbation directions.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击利用了机器学习模型对人类几乎不可察觉的小输入扰动的敏感性，但会导致模型输出发生巨大变化。这些攻击揭示了模型学习决策边界和从训练数据泛化的脆弱性。其数学基础依赖于模型的梯度信息来识别最有效的扰动方向。
- en: '**Fast Gradient Sign Method (FGSM)** ([Goodfellow, Shlens, and Szegedy 2014b](ch058.xhtml#ref-goodfellow2014explaining))
    represents one of the earliest and most influential adversarial attack techniques.
    FGSM generates adversarial examples by adding small perturbations in the direction
    of the gradient with respect to the loss function, effectively “pushing” inputs
    toward misclassification boundaries. For ImageNet classifiers, FGSM attacks with
    ε = 8/255 (barely perceptible perturbations) can reduce accuracy from 76% to under
    10%, demonstrating the fragility of deep networks to small input modifications.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速梯度符号法（FGSM）**（[Goodfellow、Shlens和Szegedy 2014b](ch058.xhtml#ref-goodfellow2014explaining)）代表了一种最早且最具影响力的对抗攻击技术。FGSM通过在损失函数相对于梯度的方向添加小的扰动来生成对抗示例，有效地“推动”输入向错误分类边界移动。对于ImageNet分类器，ε
    = 8/255（几乎不可察觉的扰动）的FGSM攻击可以将准确率从76%降低到10%以下，展示了深度网络对微小输入修改的脆弱性。'
- en: Projected Gradient Descent (PGD) attacks ([Madry et al. 2017](ch058.xhtml#ref-madry2017towards))
    extend FGSM by iteratively applying small perturbations and projecting back to
    the allowed perturbation space. PGD attacks with 40 iterations and step size α
    = 2/255 achieve nearly 100% attack success rates against undefended models, dropping
    CIFAR-10 accuracy from 95% to under 5%. These attacks are considered among the
    strongest first-order adversaries and serve as benchmarks for evaluating defensive
    mechanisms.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 投影梯度下降（PGD）攻击（[Madry等人2017](ch058.xhtml#ref-madry2017towards)）通过迭代应用小的扰动并将其投影回允许的扰动空间来扩展FGSM。具有40次迭代和步长α
    = 2/255的PGD攻击在未受保护的模型上实现了近100%的攻击成功率，将CIFAR-10的准确率从95%降至5%以下。这些攻击被认为是最强的一阶对抗者之一，并作为评估防御机制的基准。
- en: Physical-world attacks pose particular challenges for deployed AI systems. Research
    has demonstrated that adversarial examples can be printed, photographed, or displayed
    on screens while maintaining their attack effectiveness ([Kurakin, Goodfellow,
    and Bengio 2016](ch058.xhtml#ref-kurakin2016adversarial)). Stop sign attacks achieve
    87% misclassification rates when physical patches are placed on traffic signs,
    causing autonomous vehicle classifiers to interpret “STOP” signs as “Speed Limit
    45” with potentially catastrophic consequences. Laboratory studies show that adversarial
    examples maintain effectiveness across different lighting conditions (2,000-10,000
    lux), viewing angles (±30 degrees), and camera distances (2-15 meters).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 物理世界攻击对部署的AI系统提出了特别的挑战。研究表明，对抗样本可以被打印、拍照或显示在屏幕上，同时保持其攻击有效性（[Kurakin, Goodfellow,
    和 Bengio 2016](ch058.xhtml#ref-kurakin2016adversarial)）。当在交通标志上放置物理补丁时，停止标志攻击的误分类率达到87%，导致自动驾驶车辆分类器将“STOP”标志解释为“限速45”，可能产生灾难性的后果。实验室研究表明，对抗样本在不同光照条件（2,000-10,000
    lux）、观察角度（±30度）和相机距离（2-15米）下仍保持有效性。
- en: Data Poisoning Attacks
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中毒攻击
- en: Data poisoning attacks target the training phase by injecting malicious samples
    into training datasets, causing models to learn incorrect associations or exhibit
    specific behaviors on targeted inputs. These attacks are particularly concerning
    in scenarios where training data is collected from untrusted sources or through
    crowdsourcing.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒攻击通过向训练数据集中注入恶意样本来针对训练阶段，导致模型学习错误的关联或在对准输入上表现出特定行为。这些攻击在从不受信任的来源或通过众包收集训练数据的情况下尤其令人担忧。
- en: Label flipping attacks modify the labels of training examples to introduce incorrect
    associations. Research demonstrates that flipping just 3% of labels in CIFAR-10
    reduces target class accuracy from 92% to 11%, while overall model accuracy drops
    only 2-4%, making detection difficult. For ImageNet, corrupting 0.5% of labels
    (6,500 images) can cause targeted misclassification rates above 90% for specific
    classes while maintaining 94% clean accuracy.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 标签翻转攻击通过修改训练样本的标签来引入错误关联。研究表明，在CIFAR-10中翻转3%的标签，将目标类准确率从92%降低到11%，而整体模型准确率仅下降2-4%，这使得检测变得困难。对于ImageNet，损坏0.5%的标签（6,500张图像）可以使特定类别的目标误分类率超过90%，同时保持94%的干净准确率。
- en: Backdoor attacks inject training samples with specific trigger patterns that
    cause models to exhibit attacker-controlled behavior when the trigger is present
    in test inputs ([T. Gu, Dolan-Gavitt, and Garg 2017](ch058.xhtml#ref-gu2017badnets)).
    Studies show that inserting backdoor triggers in just 1% of training data achieves
    99.5% attack success rates on trigger-bearing test inputs. The model performs
    normally on clean inputs but consistently misclassifies inputs containing the
    backdoor trigger, with clean accuracy typically dropping less than 1%.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 后门攻击通过在训练样本中注入特定的触发模式，使得模型在测试输入中存在触发器时表现出攻击者控制的行为（[T. Gu, Dolan-Gavitt, 和 Garg
    2017](ch058.xhtml#ref-gu2017badnets)）。研究表明，在1%的训练数据中插入后门触发器，对携带触发器的测试输入的攻击成功率可达99.5%。模型在干净输入上表现正常，但会持续错误地将包含后门触发器的输入分类，通常准确率下降不到1%。
- en: 'Gradient-based poisoning crafts training samples that appear benign but cause
    gradient updates during training to move the model toward attacker objectives
    ([Shafahi et al. 2018](ch058.xhtml#ref-shafahi2018poison)). These attacks require
    precise optimization but can be devastating: poisoning 50 crafted images in CIFAR-10
    (0.1% of training data) achieves target misclassification rates above 70%. The
    computational cost is significant, requiring 15-20<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> more training time
    to generate optimal poisoning samples, but the attack remains undetectable through
    visual inspection.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的中毒攻击通过创建看似无害的训练样本，但在训练过程中导致梯度更新将模型推向攻击者的目标（[Shafahi等人 2018](ch058.xhtml#ref-shafahi2018poison)）。这些攻击需要精确的优化，但可能具有破坏性：在CIFAR-10中中毒50个精心制作的图像（占训练数据的0.1%）可以实现超过70%的目标误分类率。计算成本很高，需要15-20倍更多的训练时间来生成最佳的中毒样本，但攻击仍然无法通过视觉检查检测到。
- en: Detection and Mitigation Strategies
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测和缓解策略
- en: Robust AI systems employ multiple defense mechanisms against input-level attacks,
    following the detection, graceful degradation, and adaptive response principles
    established in our unified framework.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒的人工智能系统采用多种防御机制来对抗输入级别的攻击，遵循我们在统一框架中建立的检测、优雅降级和自适应响应原则。
- en: Input sanitization applies preprocessing techniques to remove or reduce adversarial
    perturbations before they reach the model. JPEG compression with quality factor
    75% neutralizes 60-80% of adversarial examples while reducing clean accuracy by
    only 1-2%. Image denoising with Gaussian filters (σ = 0.5) blocks 45% of FGSM
    attacks but requires careful tuning to avoid degrading legitimate inputs. Geometric
    transformations like random rotations (±15°) and scaling (0.9-1.1<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>) provide 30-50% defense
    effectiveness with minimal clean accuracy loss.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 输入净化在对抗性扰动到达模型之前应用预处理技术来移除或减少它们。使用质量因子为75%的JPEG压缩可以中和60-80%的对抗样本，同时仅降低1-2%的干净数据精度。使用高斯滤波器（σ
    = 0.5）进行图像去噪可以阻止45%的FGSM攻击，但需要仔细调整以避免降低合法输入。几何变换，如随机旋转（±15°）和缩放（0.9-1.1<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>），在最小化干净数据精度损失的同时提供30-50%的防御效果。
- en: Adversarial training ([Madry et al. 2017](ch058.xhtml#ref-madry2017towards))
    incorporates adversarial examples into the training process, teaching models to
    maintain correct predictions in the presence of adversarial perturbations. PGD
    adversarial training on CIFAR-10 achieves 87% robust accuracy against ε = 8/255
    attacks compared to 0% for undefended models, though clean accuracy drops from
    95% to 84%. Training time increases 6-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    due to adversarial example generation during each epoch, requiring specialized
    hardware acceleration for practical implementation.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练（[Madry 等人 2017](ch058.xhtml#ref-madry2017towards)）将对抗样本纳入训练过程，教导模型在存在对抗性扰动的情况下保持正确的预测。在
    CIFAR-10 上进行的PGD对抗性训练对于 ε = 8/255 的攻击实现了87%的鲁棒精度，而未防御的模型为0%，尽管干净精度从95%下降到84%。由于每个epoch期间生成对抗样本，训练时间增加了6-10<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>，需要专门的硬件加速来实现实际应用。
- en: Certified defenses provide mathematical guarantees about model robustness within
    specified perturbation bounds ([J. Cohen, Rosenfeld, and Kolter 2019](ch058.xhtml#ref-cohen2019certified)).
    Randomized smoothing achieves 67% certified accuracy on ImageNet for ℓ2 perturbations
    with σ = 0.5, compared to 76% clean accuracy. The certification radius increases
    to ε = 1.0 for 54% of test inputs, providing provable robustness guarantees. However,
    inference time increases 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    due to Monte Carlo sampling requirements (typically 1,000 samples per prediction).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 认证防御在指定的扰动范围内为模型鲁棒性提供数学保证（[J. Cohen、Rosenfeld 和 Kolter 2019](ch058.xhtml#ref-cohen2019certified)）。随机平滑在
    ImageNet 上对于 ℓ2 扰动（σ = 0.5）实现了67%的认证精度，相比之下，干净数据的精度为76%。对于54%的测试输入，认证半径增加到 ε =
    1.0，提供了可证明的鲁棒性保证。然而，由于蒙特卡洛抽样的需求（通常每个预测1,000个样本），推理时间增加了100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>。
- en: Ensemble methods leverage multiple models or detection mechanisms to identify
    and filter adversarial inputs ([Tramèr et al. 2017](ch058.xhtml#ref-tramèr2017ensemble)).
    Ensembles of 5 independently trained models achieve 94% detection rates for adversarial
    examples using prediction entropy thresholds (τ = 1.5), with false positive rates
    below 2% on clean data. Computational overhead scales linearly with ensemble size,
    requiring <semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">5\times</annotation></semantics>
    inference time and memory for the 5-model ensemble, making real-time deployment
    challenging.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法利用多个模型或检测机制来识别和过滤对抗性输入（[Tramèr 等人 2017](ch058.xhtml#ref-tramèr2017ensemble)）。5个独立训练的模型集使用预测熵阈值（τ
    = 1.5）实现了94%的对抗样本检测率，在干净数据上的误报率低于2%。计算开销与集成规模线性相关，对于5模型集成，需要<semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">5\times</annotation></semantics>的推理时间和内存，使得实时部署变得具有挑战性。
- en: While input-level attacks represent intentional attempts to compromise model
    behavior, AI systems must also contend with natural variations in their operational
    environments that can be equally disruptive. These environmental challenges emerge
    organically from the evolving nature of real-world deployments.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输入级攻击代表了有意破坏模型行为的尝试，但AI系统还必须应对其操作环境中可能同样具有破坏性的自然变化。这些环境挑战自然地源于现实世界部署的演变性质。
- en: Environmental Shifts
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境变化
- en: The third pillar of robust AI addresses the natural evolution of real-world
    conditions that can degrade model performance over time. Unlike the deliberate
    manipulations of input-level attacks or the random failures of hardware faults,
    environmental shifts reflect the inherent challenge of deploying static models
    in dynamic environments where data distributions, user behavior, and operational
    contexts continuously evolve. These shifts can interact synergistically with other
    vulnerability types. For example, a model experiencing distribution shift becomes
    more susceptible to adversarial attacks, while hardware errors may manifest differently
    under changed environmental conditions.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 强健AI的第三个支柱解决了现实世界条件随时间自然演变的问题，这些问题可能会降低模型性能。与输入级攻击的有意操纵或硬件故障的随机故障不同，环境变化反映了在动态环境中部署静态模型的固有挑战，其中数据分布、用户行为和操作环境持续演变。这些变化可以与其他漏洞类型协同作用。例如，经历分布偏移的模型更容易受到对抗攻击，而硬件错误在改变的环境条件下可能表现出不同的形式。
- en: Distribution Shift and Concept Drift
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布偏移和概念漂移
- en: Intuitive Understanding
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直观理解
- en: 'Consider a medical diagnosis model trained on X-ray images from a modern hospital.
    When deployed in a rural clinic with older equipment, the model’s accuracy plummets
    not because the underlying medical conditions have changed, but because the image
    characteristics differ. This exemplifies distribution shift: the world the model
    encounters differs from the world it learned from.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个基于现代医院X光图像训练的医疗诊断模型。当该模型部署在设备较旧的乡村诊所时，其准确性大幅下降，并非因为潜在的医学状况发生了变化，而是因为图像特征不同。这体现了分布偏移：模型遇到的世界与它学习到的世界不同。
- en: Distribution shifts occur naturally as environments evolve. User preferences
    change seasonally, language evolves with new slang, and economic patterns shift
    with market conditions. Unlike adversarial attacks that require malicious intent,
    these shifts emerge organically from the dynamic nature of real-world systems.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 随着环境的演变，分布偏移自然发生。用户偏好会随季节变化，语言会随着新俚语的产生而演变，经济模式会随着市场条件的变化而变化。与需要恶意意图的对抗攻击不同，这些变化自然地源于现实世界系统的动态性质。
- en: Technical Categories
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术类别
- en: 'Covariate shift occurs when the input distribution changes while the relationship
    between inputs and outputs remains constant ([Quiñonero-Candela et al. 2008](ch058.xhtml#ref-quinonero2009dataset)).
    Autonomous vehicle perception models trained on daytime images (luminance 1,000-100,000
    lux) experience 15-30% accuracy degradation when deployed in nighttime conditions
    (0.1-10 lux), despite unchanged object recognition tasks. Weather conditions introduce
    additional covariate shift: rain reduces object detection mAP by 12%, snow by
    18%, and fog by 25% compared to clear conditions.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 协变量偏移发生在输入分布发生变化，而输入和输出之间的关系保持恒定时（[Quiñonero-Candela等人 2008](ch058.xhtml#ref-quinonero2009dataset)）。在白天图像（亮度为1,000-100,000勒克斯）上训练的自动驾驶车辆感知模型在夜间条件下（0.1-10勒克斯）部署时，准确性会下降15-30%，尽管物体识别任务没有变化。天气条件引入了额外的协变量偏移：与晴朗条件相比，雨会使物体检测mAP降低12%，雪降低18%，雾降低25%。
- en: Concept drift represents changes in the underlying relationship between inputs
    and outputs over time ([Widmer and Kubat 1996](ch058.xhtml#ref-widmer1996learning)).
    Credit card fraud detection systems experience concept drift with 6-month correlation
    decay rates of 0.2-0.4, requiring model retraining every 90-120 days to maintain
    performance above 85% precision. E-commerce recommendation systems show 15-20%
    accuracy degradation over 3-6 months due to seasonal preference changes and evolving
    user behavior patterns.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 概念漂移代表了输入和输出之间基本关系随时间的变化（[Widmer和Kubat 1996](ch058.xhtml#ref-widmer1996learning)）。信用卡欺诈检测系统会经历概念漂移，6个月的关联衰减率为0.2-0.4，需要每90-120天重新训练模型，以保持85%以上的精度。电子商务推荐系统在3-6个月内由于季节性偏好变化和用户行为模式的演变，准确性会下降15-20%。
- en: 'Label shift affects the distribution of output classes without changing the
    input-output relationship ([Lipton, Wang, and Smola 2018](ch058.xhtml#ref-lipton2018detecting)).
    COVID-19 caused dramatic label shift in medical imaging: pneumonia prevalence
    increased from 12% to 35% in some hospital systems, requiring recalibration of
    diagnostic thresholds. Seasonal label shift in agriculture monitoring shows crop
    disease prevalence varying by 40-60% between growing seasons, necessitating adaptive
    decision boundaries for accurate yield prediction.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 标签偏移会影响输出类别的分布，而不改变输入输出关系（[Lipton, Wang, and Smola 2018](ch058.xhtml#ref-lipton2018detecting)）。COVID-19导致医学影像中的标签偏移发生巨大变化：某些医院系统中肺炎的发病率从12%增加到35%，需要重新校准诊断阈值。农业监测中的季节性标签偏移显示作物疾病发病率在生长季节之间变化40-60%，需要适应性的决策边界以进行准确的产量预测。
- en: Monitoring and Adaptation Strategies
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控和适应策略
- en: Effective response to environmental shifts requires continuous monitoring of
    deployment conditions and adaptive mechanisms that maintain model performance
    as conditions change.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对环境变化的有效响应需要持续监控部署条件，并具有适应机制，以保持模型性能随条件变化。
- en: Statistical distance metrics quantify the degree of distribution shift by measuring
    differences between training and deployment data distributions. Maximum Mean Discrepancy
    (MMD) with RBF kernels (γ = 1.0) provides detection sensitivity of 0.85 for shifts
    with Cohen’s d > 0.5, processing 10,000 samples in 150 ms on modern hardware.
    Kolmogorov-Smirnov tests achieve 95% detection rates for univariate shifts with
    1,000+ samples, but scale poorly to high-dimensional data. Population Stability
    Index (PSI) thresholds of 0.1-0.25 indicate significant shift requiring model
    investigation.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 统计距离度量通过测量训练数据和部署数据分布之间的差异来量化分布变化的程度。使用径向基函数核（γ = 1.0）的最大均值差异（MMD）对于Cohen’s
    d > 0.5的偏移提供了0.85的检测灵敏度，在现代硬件上处理10,000个样本只需150 ms。Kolmogorov-Smirnov测试对于具有1,000+样本的单变量偏移达到95%的检测率，但扩展到高维数据时表现不佳。人口稳定性指数（PSI）阈值为0.1-0.25表明存在需要模型调查的显著偏移。
- en: Online learning enables models to continuously adapt to new data while maintaining
    performance on previously learned patterns ([Shalev-Shwartz 2011](ch058.xhtml#ref-shalev2012online)).
    Stochastic Gradient Descent with learning rates η = 0.001-0.01 achieves convergence
    within 100-500 samples for concept drift adaptation. Memory overhead typically
    requires 2-5 MB for maintaining sufficient historical context, while computation
    adds 15-25% inference latency for real-time adaptation. Techniques like Elastic
    Weight Consolidation prevent catastrophic forgetting with regularization coefficients
    λ = 400-40,000.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习使模型能够持续适应新数据，同时保持对先前学习模式的性能（[Shalev-Shwartz 2011](ch058.xhtml#ref-shalev2012online)）。学习率η
    = 0.001-0.01的随机梯度下降在100-500个样本内实现概念漂移适应的收敛。通常需要2-5 MB的内存开销来维持足够的历史上下文，而计算增加了15-25%的推理延迟以实现实时适应。像弹性权重巩固（Elastic
    Weight Consolidation）这样的技术通过正则化系数λ = 400-40,000防止灾难性遗忘。
- en: Model ensembles and selection maintain multiple models specialized for different
    environmental conditions, dynamically selecting the most appropriate model based
    on detected environmental characteristics ([Ross, Gordon, and Bagnell 2011](ch058.xhtml#ref-ross2013model)).
    Ensemble systems with 3-7 models achieve 8-15% better accuracy than single models
    under distribution shift, with selection overhead of 2-5 ms per prediction. Dynamic
    weighting based on recent performance (sliding windows of 500-2,000 samples) provides
    optimal adaptation to gradual drift.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 模型集成和选择维护多个针对不同环境条件专门化的模型，根据检测到的环境特征动态选择最合适的模型（[Ross, Gordon, and Bagnell 2011](ch058.xhtml#ref-ross2013model)）。具有3-7个模型的集成系统在分布偏移下比单个模型提高了8-15%的准确性，每次预测的选择开销为2-5 ms。基于最近性能的动态加权（滑动窗口为500-2,000个样本）提供了对渐进漂移的最佳适应。
- en: Federated learning enables distributed adaptation across multiple deployment
    environments while preserving privacy. FL systems with 50-1,000 participants achieve
    convergence in 10-50 communication rounds, each requiring 10-100 MB of parameter
    transmission depending on model size. Local training typically requires 5-20 epochs
    per round, with communication costs dominating when bandwidth falls below 1 Mbps.
    Differential privacy (ε = 1.0-8.0) adds noise but maintains model utility above
    90% for most applications.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习能够在保持隐私的同时，实现跨多个部署环境的分布式自适应。拥有50-1,000个参与者的FL系统在10-50轮通信中实现收敛，每轮通信需要10-100
    MB的参数传输，具体取决于模型大小。本地训练通常每轮需要5-20个epoch，当带宽低于1 Mbps时，通信成本占主导地位。差分隐私（ε = 1.0-8.0）添加噪声，但大多数应用中模型效用保持在90%以上。
- en: Robustness Evaluation Tools
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒性评估工具
- en: Having examined the three pillars of robust AI—hardware faults, input-level
    attacks, and environmental shifts—students now have the conceptual foundation
    to understand specialized tools and frameworks for robustness evaluation and improvement.
    These tools implement the detection, graceful degradation, and adaptive response
    principles across all three threat categories.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察了鲁棒人工智能的三个支柱——硬件故障、输入级攻击和环境变化后，学生们现在有了理解专门用于鲁棒性评估和改进的工具和框架的概念基础。这些工具在所有三个威胁类别中实现了检测、优雅降级和自适应响应原则。
- en: Hardware fault injection tools like PyTorchFI and TensorFI enable systematic
    testing of ML model resilience to the transient, permanent, and intermittent faults
    described earlier. Adversarial attack libraries implement FGSM, PGD, and certified
    defense techniques for evaluating input-level robustness. Distribution monitoring
    frameworks provide the statistical distance metrics and drift detection capabilities
    essential for environmental shift management.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于PyTorchFI和TensorFI的硬件故障注入工具能够系统地测试机器学习模型对之前描述的短暂、永久和间歇性故障的恢复能力。对抗攻击库实现了FGSM、PGD和认证防御技术，用于评估输入级鲁棒性。分布监控框架提供了环境变化管理所必需的统计距离指标和漂移检测能力。
- en: Modern robustness tools integrate directly with popular ML frameworks (PyTorch,
    TensorFlow, Keras), enabling seamless incorporation of robustness evaluation into
    development workflows established in [Chapter 13](ch019.xhtml#sec-ml-operations).
    The comprehensive examination of these tools and their practical applications
    appears in [Section 16.10](ch022.xhtml#sec-robust-ai-fault-injection-tools-frameworks-fc07),
    providing detailed implementation guidance for building robust AI systems.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现代鲁棒性工具可以直接集成到流行的机器学习框架（PyTorch、TensorFlow、Keras）中，使得将鲁棒性评估无缝融入[第13章](ch019.xhtml#sec-ml-operations)中建立的开发工作流程成为可能。这些工具及其实际应用的全面审查出现在[第16.10节](ch022.xhtml#sec-robust-ai-fault-injection-tools-frameworks-fc07)中，为构建鲁棒人工智能系统提供了详细的实施指南。
- en: Input-Level Attacks and Model Robustness
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入级攻击与模型鲁棒性
- en: While hardware faults represent unintentional disruptions to the underlying
    computing infrastructure, model robustness concerns extend to deliberate attacks
    targeting the AI system’s decision-making processes and natural variations in
    operational environments. The transition from hardware reliability to model robustness
    reflects a shift from protecting the physical substrate of computation to defending
    the learned representations and decision boundaries that define model behavior.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然硬件故障代表了底层计算基础设施的无意中断，但模型鲁棒性的担忧扩展到针对人工智能系统决策过程和操作环境中的自然变化的故意攻击。从硬件可靠性到模型鲁棒性的转变反映了从保护计算物理基础到防御定义模型行为的学到的表示和决策边界的转变。
- en: This shift requires a change in perspective. Hardware faults typically manifest
    as corrupted calculations, memory errors, or communication failures that propagate
    through the system in predictable ways guided by the underlying computational
    graph. In contrast, model robustness challenges exploit or expose core limitations
    in the model’s understanding of its problem domain. Adversarial attacks craft
    inputs specifically designed to trigger misclassifications, data poisoning corrupts
    the training process itself, and distribution shifts reveal the brittleness of
    models when deployed beyond their training assumptions.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转变需要改变视角。硬件故障通常表现为损坏的计算、内存错误或通信故障，这些故障以由底层计算图指导的、可预测的方式在系统中传播。相比之下，模型鲁棒性挑战利用或揭示了模型对其问题域理解的核心局限性。对抗攻击设计输入以触发错误分类，数据中毒会破坏训练过程本身，而分布偏移揭示了模型在超出其训练假设的情况下部署时的脆弱性。
- en: Following our three-category robustness framework from [Section 16.3](ch022.xhtml#sec-robust-ai-unified-framework-robust-ai-b25d),
    different challenge types require complementary defense strategies. While hardware
    fault mitigation often relies on redundancy, error detection codes, and graceful
    degradation, model robustness demands techniques like adversarial training, input
    sanitization, domain adaptation, and continuous monitoring of model behavior in
    deployment.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们从[第16.3节](ch022.xhtml#sec-robust-ai-unified-framework-robust-ai-b25d)提出的三个类别鲁棒性框架，不同的挑战类型需要互补的防御策略。虽然硬件故障缓解通常依赖于冗余、错误检测码和优雅降级，但模型鲁棒性需要像对抗训练、输入净化、领域适应以及持续监控模型在部署中的行为等技术。
- en: The importance of this dual perspective becomes clear when we consider that
    real-world AI systems face compound threats where hardware faults and model vulnerabilities
    can interact in complex ways. A hardware fault that corrupts model weights might
    create new adversarial vulnerabilities, while adversarial attacks might trigger
    error conditions that resemble hardware faults. Our unified framework from [Section 16.3](ch022.xhtml#sec-robust-ai-unified-framework-robust-ai-b25d)
    provides the conceptual foundation for addressing these interconnected challenges
    systematically.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到现实世界的AI系统面临复合威胁时，这种双重视角的重要性变得清晰，这些威胁中硬件故障和模型脆弱性可以以复杂的方式相互作用。一个损坏模型权重的硬件故障可能会创建新的对抗性脆弱性，而对抗攻击可能会触发类似于硬件故障的错误条件。我们从[第16.3节](ch022.xhtml#sec-robust-ai-unified-framework-robust-ai-b25d)提供的统一框架为系统地解决这些相互关联的挑战提供了概念基础。
- en: Adversarial Attacks
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗攻击
- en: Adversarial attacks represent counterintuitive vulnerabilities in modern machine
    learning systems. These attacks exploit core characteristics of how neural networks
    learn and represent information, revealing extreme model sensitivity to carefully
    crafted modifications that remain imperceptible to human observers. These attacks
    often involve adding small, carefully designed perturbations to input data, which
    can cause the model to misclassify it, as shown in [Figure 16.20](ch022.xhtml#fig-adversarial-attack-noise-example).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击代表了现代机器学习系统中反直觉的脆弱性。这些攻击利用了神经网络学习和表示信息的核心特征，揭示了模型对精心设计的、对人类观察者而言难以察觉的修改的极端敏感性。这些攻击通常涉及向输入数据添加小的、精心设计的扰动，这可能导致模型错误分类，如图[图16.20](ch022.xhtml#fig-adversarial-attack-noise-example)所示。
- en: '![](../media/file266.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file266.png)'
- en: 'Figure 16.20: **Adversarial Perturbation**: Subtle, Intentionally Crafted Noise
    Can Cause Neural Networks to Misclassify Images With High Confidence, Exposing
    a Vulnerability in Model Robustness. These Perturbations, Imperceptible to Humans,
    Alter the Input in a Way That Maximizes Prediction Error, Highlighting the Need
    for Defenses Against Adversarial Attacks. Source: Sutanto (2019).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '图16.20: **对抗扰动**: 微妙且故意设计的噪声可能导致神经网络以高置信度错误分类图像，暴露了模型鲁棒性的脆弱性。这些对人类而言难以察觉的扰动以最大化预测错误的方式改变输入，突显了对抗攻击防御的必要性。来源：Sutanto
    (2019)。'
- en: Understanding the Vulnerability
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解脆弱性
- en: Understanding why these attacks are so effective requires examining how they
    expose core limitations in neural network architectures. The existence of adversarial
    examples reveals a core mismatch between human and machine perception[41](#fn41).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么这些攻击如此有效需要检查它们如何揭示了神经网络架构中的核心局限性。对抗样本的存在揭示了人类和机器感知之间的核心不匹配[41](#fn41)。
- en: This vulnerability stems from several characteristics of neural network learning[42](#fn42).
    High-dimensional input spaces[43](#fn43) provide numerous dimensions that attackers
    can exploit simultaneously.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这种脆弱性源于神经网络学习的一些特性[42](#fn42)。高维输入空间[43](#fn43)提供了攻击者可以同时利用的多个维度。
- en: This deep understanding of why adversarial examples exist is crucial for developing
    effective defenses. The vulnerability reflects core properties of how neural networks
    represent and process information in high-dimensional spaces, rather than being
    merely a software bug or training artifact. The theoretical foundations explaining
    why neural networks[44](#fn44) are inherently vulnerable to adversarial perturbations
    are comprehensively detailed in [Chapter 3](ch009.xhtml#sec-dl-primer).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对为什么存在对抗样本的深入理解对于开发有效的防御措施至关重要。这种脆弱性反映了神经网络在高维空间中表示和处理信息的基本属性，而不仅仅是软件漏洞或训练伪象。解释为什么神经网络[44](#fn44)天生容易受到对抗性扰动的理论基础在[第3章](ch009.xhtml#sec-dl-primer)中得到了全面详细的阐述。
- en: Attack Categories and Mechanisms
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 攻击类别和机制
- en: Adversarial attacks can be organized into several categories based on their
    approach to crafting perturbations and the information available to the attacker.
    Each category exploits different aspects of model vulnerability and requires distinct
    defensive considerations.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击者构建扰动的方法和攻击者可用的信息，对抗攻击可以被组织成几个类别。每个类别利用模型脆弱性的不同方面，并需要不同的防御考虑。
- en: Gradient-based Attacks
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于梯度的攻击
- en: 'The most direct and widely studied category comprises gradient-based attacks,
    which exploit a core aspect of neural network training: the same gradient information
    used to train models can be weaponized to attack them. These attacks represent
    the most direct approach to adversarial example generation by leveraging the model’s
    own learning mechanism against itself.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接且研究最广泛的类别包括基于梯度的攻击，这些攻击利用了神经网络训练的核心方面：用于训练模型的相同梯度信息可以被武器化来攻击它们。这些攻击通过利用模型自身的学习机制来对抗自身，代表了生成对抗样本的最直接方法。
- en: '**Conceptual Foundation**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念基础**'
- en: The key insight behind gradient-based attacks is that neural networks compute
    gradients to understand how changes to their inputs affect their outputs. During
    training, gradients guide weight updates to minimize prediction errors. For attacks,
    these same gradients reveal which input modifications would maximize prediction
    errors—essentially running the training process in reverse.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度攻击背后的关键洞察是，神经网络通过计算梯度来理解其输入的变化如何影响其输出。在训练过程中，梯度引导权重更新以最小化预测误差。对于攻击，这些相同的梯度揭示了哪些输入修改会最大化预测误差——本质上是在反向运行训练过程。
- en: To illustrate this concept, consider an image classification model that correctly
    identifies a cat in a photo. The gradient with respect to the input image shows
    how sensitive the model’s prediction is to changes in each pixel. An attacker
    can use this gradient information to determine the most effective way to modify
    specific pixels to change the model’s prediction, perhaps causing it to misclassify
    the cat as a dog while keeping the changes imperceptible to human observers.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，可以考虑一个图像分类模型，该模型能够正确识别照片中的猫。相对于输入图像的梯度显示了模型预测对每个像素变化的敏感性。攻击者可以使用这些梯度信息来确定最有效地修改特定像素以改变模型预测的方法，可能使模型将猫误分类为狗，同时使变化对人类观察者不可察觉。
- en: '**Fast Gradient Sign Method (FGSM)**'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速梯度符号法 (FGSM)**'
- en: The Fast Gradient Sign Method[45](#fn45) exemplifies the elegance and danger
    of gradient-based attacks[46](#fn46). FGSM takes the conceptually simple approach
    of moving in the direction that most rapidly increases the model’s prediction
    error.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 快速梯度符号法[45](#fn45)展示了基于梯度的攻击[46](#fn46)的优雅和危险。FGSM采用了概念上简单的方法，即朝着最快增加模型预测误差的方向移动。
- en: 'The underlying mathematical formulation captures this intuitive process:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的数学公式捕捉了这个直观的过程：
- en: <semantics><mrow><msub><mi>x</mi><mtext mathvariant="normal">adv</mtext></msub><mo>=</mo><mi>x</mi><mo>+</mo><mi>ϵ</mi><mo>⋅</mo><mtext
    mathvariant="normal">sign</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow>
    <annotation encoding="application/x-tex">x_{\text{adv}} = x + \epsilon \cdot \text{sign}\big(\nabla_x
    J(\theta, x, y)\big)</annotation></semantics>
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msub><mi>x</mi><mtext mathvariant="normal">adv</mtext></msub><mo>=</mo><mi>x</mi><mo>+</mo><mi>ϵ</mi><mo>⋅</mo><mtext
    mathvariant="normal">sign</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow>
    <annotation encoding="application/x-tex">x_{\text{adv}} = x + \epsilon \cdot \text{sign}\big(\nabla_x
    J(\theta, x, y)\big)</annotation></semantics>
- en: 'Where the components represent:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 其中各部分代表：
- en: '<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>:
    the original input (e.g., an image of a cat)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>:
    原始输入（例如，一只猫的图像）'
- en: '<semantics><msub><mi>x</mi><mtext mathvariant="normal">adv</mtext></msub><annotation
    encoding="application/x-tex">x_{\text{adv}}</annotation></semantics>: the adversarial
    example that will fool the model'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><msub><mi>x</mi><mtext mathvariant="normal">adv</mtext></msub><annotation
    encoding="application/x-tex">x_{\text{adv}}</annotation></semantics>: 将欺骗模型的对抗性示例'
- en: '<semantics><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_x
    J(\theta, x, y)</annotation></semantics>: the gradient showing which input changes
    most increase prediction error'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_x
    J(\theta, x, y)</annotation></semantics>: 显示输入变化对预测误差影响最大的梯度'
- en: '<semantics><mrow><mtext mathvariant="normal">sign</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{sign}(\cdot)</annotation></semantics>: extracts
    only the direction of change, ignoring magnitude differences'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mtext mathvariant="normal">sign</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{sign}(\cdot)</annotation></semantics>: 仅提取变化方向，忽略大小差异'
- en: '<semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>:
    controls perturbation strength (typically 0.01-0.3 for normalized inputs)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>:
    控制扰动强度（对于归一化输入通常为0.01-0.3）'
- en: '<semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta,
    x, y)</annotation></semantics>: the loss function measuring prediction error'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mrow><mi>J</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(\theta,
    x, y)</annotation></semantics>: 测量预测误差的损失函数'
- en: The gradient <semantics><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_x
    J(\theta, x, y)</annotation></semantics> quantifies how the loss function changes
    with respect to each input feature, indicating which input modifications would
    most effectively increase the model’s prediction error. The <semantics><mrow><mtext
    mathvariant="normal">sign</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{sign}(\cdot)</annotation></semantics>
    function extracts the direction of steepest ascent, while the perturbation magnitude
    <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    controls the strength of the modification applied to each input dimension.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度<semantics><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>J</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_x
    J(\theta, x, y)</annotation></semantics>量化了损失函数相对于每个输入特征的改变，表明哪些输入修改能够最有效地增加模型的预测误差。函数<semantics><mrow><mtext
    mathvariant="normal">sign</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{sign}(\cdot)</annotation></semantics>提取了最陡上升的方向，而扰动幅度<semantics><mi>ϵ</mi><annotation
    encoding="application/x-tex">\epsilon</annotation></semantics>控制了应用于每个输入维度的修改强度。
- en: This approach generates adversarial examples by taking a single step in the
    direction that increases the loss most rapidly, as illustrated in [Figure 16.21](ch022.xhtml#fig-gradient-attack).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通过在增加损失最快的方向上迈出一步来生成对抗样本，如图16.21所示。[图16.21](ch022.xhtml#fig-gradient-attack)。
- en: '![](../media/file267.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file267.png)'
- en: 'Figure 16.21: **Adversarial Perturbations**: Gradient-based attacks generate
    subtle, intentionally crafted input noise – with magnitude controlled by <semantics><mi>ϵ</mi><annotation
    encoding="application/x-tex">\epsilon</annotation></semantics> – that maximizes
    the loss function <semantics><mrow><mi>j</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">j(\theta,
    x, y)</annotation></semantics> and causes misclassification by the model. These
    perturbations, imperceptible to humans, exploit model vulnerabilities by moving
    the input <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    across the decision boundary. Source: [ivezic](HTTPS://defence.AI/AI-security/gradient-based-attacks/)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.21：**对抗扰动**：基于梯度的攻击生成细微、有意设计的输入噪声（幅度由<semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>控制），以最大化损失函数<semantics><mrow><mi>j</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">j(\theta,
    x, y)</annotation></semantics>并导致模型误分类。这些对人类来说难以察觉的扰动通过将输入<semantics><mi>x</mi><annotation
    encoding="application/x-tex">x</annotation></semantics>移动到决策边界来利用模型漏洞。来源：[ivezic](HTTPS://defence.AI/AI-security/gradient-based-attacks/)
- en: Building on this foundation, the Projected Gradient Descent (PGD) attack ([Kurakin,
    Goodfellow, and Bengio 2016](ch058.xhtml#ref-kurakin2016adversarial)) extends
    FGSM by iteratively applying the gradient update step, allowing for more refined
    and powerful adversarial examples. PGD projects each perturbation step back into
    a constrained norm ball around the original input, ensuring that the adversarial
    example remains within a specified distortion limit. This makes PGD a stronger
    white-box attack and a benchmark for evaluating model robustness.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，投影梯度下降（Projected Gradient Descent，PGD）攻击([Kurakin, Goodfellow, and Bengio
    2016](ch058.xhtml#ref-kurakin2016adversarial))通过迭代应用梯度更新步骤扩展了FGSM，允许生成更精细和强大的对抗样本。PGD将每个扰动步骤投影回原始输入周围的约束范数球内，确保对抗样本保持在指定的扭曲限制内。这使得PGD成为一种更强的白盒攻击，并成为评估模型鲁棒性的基准。
- en: The Jacobian-based Saliency Map Attack (JSMA) ([Papernot, McDaniel, Jha, et
    al. 2016](ch058.xhtml#ref-papernot2016jsma)) is another gradient-based approach
    that identifies the most influential input features and perturbs them to create
    adversarial examples. By constructing a saliency map based on the Jacobian of
    the model’s outputs with respect to inputs, JSMA selectively alters a small number
    of input dimensions that are most likely to influence the target class. This makes
    JSMA more precise and targeted than FGSM or PGD, often requiring fewer perturbations
    to fool the model.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 基于雅可比的显著性图攻击 (JSMA) ([Papernot, McDaniel, Jha, et al. 2016](ch058.xhtml#ref-papernot2016jsma))
    是另一种基于梯度的方法，它识别最有影响力的输入特征，并通过扰动它们来创建对抗样本。通过构建基于模型输出相对于输入的雅可比的显著性图，JSMA 选择性地改变最有可能影响目标类的一小部分输入维度。这使得
    JSMA 比FGSM或PGD更精确和有针对性，通常需要更少的扰动来欺骗模型。
- en: Gradient-based attacks are particularly effective in white-box settings[47](#fn47),
    where the attacker has access to the model’s architecture and gradients. Their
    efficiency and relative simplicity have made them popular tools for both attacking
    and evaluating model robustness in research.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的攻击在白盒设置[47](#fn47)中特别有效，攻击者可以访问模型的架构和梯度。它们的效率和相对简单性使它们成为研究攻击和评估模型鲁棒性的流行工具。
- en: Optimization-based Attacks
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于优化的攻击
- en: While gradient-based methods offer speed and simplicity, optimization-based
    attacks formulate the generation of adversarial examples as a more sophisticated
    optimization problem. The Carlini and Wagner (C&W) attack ([Carlini and Wagner
    2017](ch058.xhtml#ref-carlini2017towards))[48](#fn48) is a prominent example in
    this category. It finds the smallest perturbation that can cause misclassification
    while maintaining the perceptual similarity to the original input. The C&W attack
    employs an iterative optimization process to minimize the perturbation while maximizing
    the model’s prediction error. It uses a customized loss function with a confidence
    term to generate more confident misclassifications.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于梯度的方法提供了速度和简单性，但基于优化的攻击将对抗样本的生成形式化为一个更复杂的优化问题。Carlini 和 Wagner (C&W) 攻击
    ([Carlini and Wagner 2017](ch058.xhtml#ref-carlini2017towards))[48](#fn48) 是这一类别中的突出例子。它找到最小的扰动，可以在保持与原始输入的感知相似性的同时导致误分类。C&W
    攻击采用迭代优化过程来最小化扰动，同时最大化模型的预测误差。它使用一个包含置信度项的定制损失函数来生成更自信的误分类。
- en: C&W attacks are especially difficult to detect because the perturbations are
    typically imperceptible to humans, and they often bypass many existing defenses.
    The attack can be formulated under various norm constraints (e.g., L2, L∞) depending
    on the desired properties of the adversarial perturbation.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: C&W 攻击特别难以检测，因为扰动通常对人类来说是不可察觉的，并且它们经常绕过许多现有的防御措施。攻击可以在各种范数约束下进行公式化（例如，L2，L∞），具体取决于对抗扰动的期望特性。
- en: Extending this optimization framework, the Elastic Net Attack to DNNs (EAD)
    incorporates elastic net regularization (a combination of L1 and L2 penalties)
    to generate adversarial examples with sparse perturbations. This can lead to minimal
    and localized changes in the input, which are harder to identify and filter. EAD
    is particularly useful in settings where perturbations need to be constrained
    in both magnitude and spatial extent.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在此优化框架的基础上，弹性网络攻击到深度神经网络 (EAD) 结合弹性网络正则化（L1和L2惩罚的组合）来生成具有稀疏扰动的对抗样本。这可能导致输入的最小和局部变化，这些变化更难识别和过滤。EAD
    在需要限制扰动的大小和空间范围的情况下特别有用。
- en: These attacks are more computationally intensive than gradient-based methods
    but offer finer control over the adversarial example’s properties, often requiring
    specialized optimization techniques detailed in [Chapter 10](ch016.xhtml#sec-model-optimizations).
    They are often used in high-stakes domains where stealth and precision are critical.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击比基于梯度的方法计算量更大，但可以更精细地控制对抗样本的特性，通常需要详细说明在[第10章](ch016.xhtml#sec-model-optimizations)中的专用优化技术。它们通常用于高风险领域，在这些领域中，隐蔽性和精确性至关重要。
- en: Transfer-based Attacks
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于迁移的攻击
- en: Moving from direct optimization to exploiting model similarities, transfer-based
    attacks exploit the transferability property[49](#fn49) of adversarial examples.
    Transferability refers to the phenomenon where adversarial examples crafted for
    one ML model can often fool other models, even if they have different architectures
    or were trained on different datasets. This enables attackers to generate adversarial
    examples using a surrogate model and then transfer them to the target model without
    requiring direct access to its parameters or gradients.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 从直接优化转向利用模型相似性，基于迁移的攻击利用了对抗性示例的可迁移性属性[49](#fn49)。可迁移性指的是为某个机器学习模型制作的对抗性示例往往可以欺骗其他模型，即使它们具有不同的架构或是在不同的数据集上训练。这使得攻击者可以使用代理模型生成对抗性示例，然后将它们转移到目标模型，而无需直接访问其参数或梯度。
- en: This transferability property underlies the feasibility of black-box attacks,
    where the adversary cannot query gradients but can still fool a model by crafting
    attacks on a publicly available or similar substitute model. Transfer-based attacks
    are particularly relevant in practical threat scenarios, such as attacking commercial
    ML APIs, where the attacker can observe inputs and outputs but not internal computations.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可迁移性属性是黑盒攻击可行性的基础，在这种攻击中，攻击者无法查询梯度，但可以通过对公开可用或类似的替代模型进行攻击来欺骗模型。基于迁移的攻击在攻击商业机器学习API等实际威胁场景中尤其相关，攻击者可以观察到输入和输出，但不能观察到内部计算。
- en: Attack success often depends on factors like similarity between models, alignment
    in training data, and the regularization techniques used. Techniques like input
    diversity (random resizing, cropping) and momentum during optimization can be
    used to increase transferability.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击的成功往往取决于模型之间的相似性、训练数据的一致性以及所使用的正则化技术。可以使用输入多样性（随机调整大小、裁剪）和优化过程中的动量等技术来增加可迁移性。
- en: Physical-world Attacks
  id: totrans-377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 物理世界攻击
- en: Physical-world attacks bring adversarial examples into real-world scenarios.
    These attacks involve creating physical objects or manipulations that can deceive
    ML models when captured by sensors or cameras. Adversarial patches, for example,
    are small, carefully designed patterns that can be placed on objects to fool object
    detection or classification models. These patches are designed to work under varying
    lighting conditions, viewing angles, and distances, making them robust in real-world
    environments.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 物理世界攻击将对抗性示例带入现实场景。这些攻击涉及创建物理对象或操作，当通过传感器或摄像头捕捉时，可以欺骗机器学习模型。例如，对抗性补丁是精心设计的小型图案，可以放置在物体上以欺骗目标检测或分类模型。这些补丁被设计成能够在不同的光照条件、观察角度和距离下工作，使它们在现实世界环境中具有鲁棒性。
- en: When attached to real-world objects, such as a stop sign or a piece of clothing,
    these patches can cause models to misclassify or fail to detect the objects accurately.
    Notably, the effectiveness of these attacks persists even after being printed
    out and viewed through a camera lens, bridging the digital and physical divide
    in adversarial ML.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些补丁附着在现实世界的物体上，例如停车标志或一件衣物上时，它们可能会导致模型误分类或无法准确检测到这些物体。值得注意的是，这些攻击的有效性即使在打印出来并通过相机镜头观察后仍然存在，这跨越了对抗性机器学习中的数字和物理界限。
- en: Adversarial objects, such as 3D-printed sculptures or modified road signs, can
    also be crafted to deceive ML systems in physical environments. For example, a
    3D turtle object was shown to be consistently classified as a rifle by an image
    classifier, even when viewed from different angles. These attacks underscore the
    risks facing AI systems deployed in physical spaces, such as autonomous vehicles,
    drones, and surveillance systems, raising critical considerations for responsible
    AI deployment covered in [Chapter 17](ch023.xhtml#sec-responsible-ai).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性物体，如3D打印雕塑或修改过的路标，也可以被制作出来以欺骗物理环境中的机器学习系统。例如，一个3D海龟物体被证明在从不同角度观察时，始终被图像分类器错误地分类为步枪。这些攻击突显了在物理空间中部署的AI系统（如自动驾驶汽车、无人机和监控系统）所面临的风险，并在[第17章](ch023.xhtml#sec-responsible-ai)中涵盖了关于负责任AI部署的临界考虑。
- en: Research into physical-world attacks also includes efforts to develop universal
    adversarial perturbations, perturbations that can fool a wide range of inputs
    and models. These threats raise serious questions about safety, robustness, and
    generalization in AI systems.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 物理世界攻击的研究还包括开发通用对抗性扰动，这些扰动可以欺骗广泛范围的输入和模型。这些威胁在AI系统的安全性、鲁棒性和泛化方面提出了严重的问题。
- en: Summary
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: '[Table 16.4](ch022.xhtml#tbl-attack_types) provides a concise overview of the
    different categories of adversarial attacks, including gradient-based attacks
    (FGSM, PGD, JSMA), optimization-based attacks (C&W, EAD), transfer-based attacks,
    and physical-world attacks (adversarial patches and objects). Each attack is briefly
    described, highlighting its key characteristics and mechanisms.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '[表16.4](ch022.xhtml#tbl-attack_types) 提供了对不同类别对抗性攻击的简要概述，包括基于梯度的攻击（FGSM、PGD、JSMA）、基于优化的攻击（C&W、EAD）、基于迁移的攻击和物理世界攻击（对抗性补丁和对象）。每种攻击都简要描述，突出其关键特性和机制。'
- en: The mechanisms of adversarial attacks reveal the intricate interplay between
    the ML model’s decision boundaries, the input data, and the attacker’s objectives.
    By carefully manipulating the input data, attackers can exploit the model’s sensitivities
    and blind spots, leading to incorrect predictions. The success of adversarial
    attacks highlights the need for a deeper understanding of ML models’ robustness
    and generalization properties.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击的机制揭示了机器学习模型决策边界、输入数据和攻击者目标之间的复杂相互作用。通过精心操作输入数据，攻击者可以利用模型的对敏感性和盲点，导致错误预测。对抗性攻击的成功突出了对机器学习模型鲁棒性和泛化特性的更深入理解的需求。
- en: 'Table 16.4: **Adversarial Attack Categories**: Machine learning model robustness
    relies on defending against attacks that intentionally perturb input data to cause
    misclassification; this table categorizes these attacks by their underlying mechanism,
    including gradient-based, optimization-based, transfer-based, and physical-world
    approaches, each exploiting different model vulnerabilities. Understanding these
    categories is crucial for developing effective defense strategies and evaluating
    model security.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.4：**对抗性攻击类别**：机器学习模型的鲁棒性依赖于防御有意扰动输入数据以导致误分类的攻击；此表根据其潜在机制对这些攻击进行分类，包括基于梯度、基于优化、基于迁移和物理世界方法，每种方法都利用不同的模型漏洞。理解这些类别对于开发有效的防御策略和评估模型安全性至关重要。
- en: '| **Attack Category** | **Attack Name** | **Description** |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| **攻击类别** | **攻击名称** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Gradient-based** | Fast Gradient Sign Method (FGSM) Projected Gradient
    Descent (PGD) Jacobian-based Saliency Map Attack (JSMA) | Perturbs input data
    by adding small noise in the gradient direction to maximize prediction error.
    Extends FGSM by iteratively applying the gradient update step for more refined
    adversarial examples. Identifies influential input features and perturbs them
    to create adversarial examples. |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| **基于梯度** | 快速梯度符号法（FGSM） 投影梯度下降（PGD）基于雅可比的显著性图攻击（JSMA） | 通过在梯度方向添加小噪声来扰动输入数据，以最大化预测错误。通过迭代应用梯度更新步骤扩展FGSM，以生成更精细的对抗性示例。识别有影响力的输入特征并对其进行扰动以创建对抗性示例。|'
- en: '| **Optimization-based** | Carlini and Wagner (C&W) Attack Elastic Net Attack
    to DNNs (EAD) | Finds the smallest perturbation that causes misclassification
    while maintaining perceptual similarity. Incorporates elastic net regularization
    to generate adversarial examples with sparse perturbations. |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| **基于优化** | 卡林尼和瓦格纳（C&W）攻击 弹性网络攻击到深度神经网络（EAD） | 寻找最小的扰动，导致误分类同时保持感知相似性。通过弹性网络正则化生成具有稀疏扰动的对抗性示例。|'
- en: '| **Transfer-based** | Transferability-based Attacks | Exploits the transferability
    of adversarial examples across different models, enabling black-box attacks. |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| **基于迁移** | 基于迁移性的攻击 | 利用对抗性示例在不同模型之间的可迁移性，实现黑盒攻击。|'
- en: '| **Physical-world** | Adversarial Patches Adversarial Objects | Small, carefully
    designed patches placed on objects to fool object detection or classification
    models. Physical objects (e.g., 3D-printed sculptures, modified road signs) crafted
    to deceive ML systems in real-world scenarios. |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| **物理世界** | 对抗性补丁 对抗性对象 | 在对象上放置的小型、精心设计的补丁，以欺骗对象检测或分类模型。物理对象（例如，3D打印雕塑、修改后的路标）在现实场景中精心制作，以欺骗机器学习系统。|'
- en: Defending against adversarial attacks requires the multifaceted defense strategies
    detailed in [Section 16.8.4.1.2](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d),
    including adversarial training, defensive distillation, input preprocessing, and
    ensemble methods.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 防御对抗性攻击需要详细在[第16.8.4.1.2节](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d)中描述的多方面防御策略，包括对抗性训练、防御蒸馏、输入预处理和集成方法。
- en: As adversarial machine learning evolves, researchers explore new attack mechanisms
    and develop more sophisticated defenses. The arms race between attackers and defenders
    drives constant innovation and vigilance in securing ML systems against adversarial
    threats. Understanding attack mechanisms is crucial for developing robust and
    reliable ML models that can withstand evolving adversarial examples.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对抗机器学习的不断发展，研究人员探索新的攻击机制并开发更复杂的防御措施。攻击者和防御者之间的军备竞赛推动了在对抗威胁下保护机器学习系统的不懈创新和警惕。理解攻击机制对于开发能够抵御不断发展的对抗样本的稳健和可靠的机器学习模型至关重要。
- en: Impact on ML
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对机器学习的影响
- en: The impact of adversarial attacks on ML systems extends far beyond simple misclassification,
    as demonstrated in [Figure 16.22](ch022.xhtml#fig-adversarial-googlenet). These
    vulnerabilities create systemic risks across deployment domains.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习系统的影响远远超出了简单的误分类，如[图16.22](ch022.xhtml#fig-adversarial-googlenet)所示。这些漏洞在部署领域造成了系统性风险。
- en: '![](../media/file268.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file268.png)'
- en: 'Figure 16.22: **Adversarial Perturbations**: Subtle, intentionally crafted
    noise added to an image can cause a trained deep neural network (googlenet) to
    misclassify it, even though the perturbed image remains visually indistinguishable
    to humans. This vulnerability underscores the lack of robustness in many machine
    learning models and motivates research into adversarial training and defense mechanisms.
    Source: goodfellow et al., 2014.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.22：**对抗扰动**：添加到图像中的微妙、有意设计的噪声可以使训练好的深度神经网络（googlenet）将其误分类，尽管被扰动的图像在视觉上对人类来说仍然无法区分。这种漏洞凸显了许多机器学习模型缺乏稳健性，并促使研究人员对对抗训练和防御机制进行研究。来源：goodfellow等人，2014。
- en: One striking example of the impact of adversarial attacks was demonstrated by
    researchers in 2017\. They experimented with small black and white stickers on
    stop signs ([Eykholt et al. 2017](ch058.xhtml#ref-eykholt2018robust)). To the
    human eye, these stickers did not obscure the sign or prevent its interpretability.
    However, when images of the sticker-modified stop signs were fed into standard
    traffic sign classification ML models, a shocking result emerged. The models misclassified
    the stop signs as speed limit signs over 85% of the time.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，研究人员展示了一个对抗攻击影响的显著例子。他们通过在停车标志上实验性地贴上小黑白色贴纸([Eykholt等人2017](ch058.xhtml#ref-eykholt2018robust))。对于人类眼睛来说，这些贴纸并没有遮挡标志或阻止其可解释性。然而，当修改后的停车标志图像被输入到标准的交通标志分类机器学习模型中时，出现了一个令人震惊的结果。模型有超过85%的时间将停车标志错误地分类为限速标志。
- en: This demonstration shed light on the alarming potential of simple adversarial
    stickers to trick ML systems into misreading critical road signs. The implications
    of such attacks in the real world are significant, particularly in the context
    of autonomous vehicles. If deployed on actual roads, these adversarial stickers
    could cause self-driving cars to misinterpret stop signs as speed limits, leading
    to dangerous situations, as shown in [Figure 16.23](ch022.xhtml#fig-graffiti).
    Researchers warned that this could result in rolling stops or unintended acceleration
    into intersections, endangering public safety.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示揭示了简单的对抗贴纸欺骗机器学习系统误读关键道路标志的令人担忧的潜力。此类攻击在现实世界中的影响是重大的，尤其是在自动驾驶汽车的情况下。如果实际部署在道路上，这些对抗贴纸可能导致自动驾驶汽车将停车标志误认为是限速标志，从而引发危险情况，如[图16.23](ch022.xhtml#fig-graffiti)所示。研究人员警告说，这可能导致滚动停车或意外加速进入交叉口，危及公共安全。
- en: Microsoft’s Tay chatbot provides a stark example of how adversarial users can
    exploit lack of robustness safeguards in deployed AI systems. Within 24 hours
    of launch, coordinated users manipulated Tay’s learning mechanisms to generate
    inappropriate and offensive content. The system lacked content filtering, user
    input validation, and behavioral monitoring safeguards that could have detected
    and prevented the exploitation. This incident highlights the critical need for
    comprehensive input validation, content filtering systems, and continuous behavioral
    monitoring in deployed AI systems, particularly those that learn from user interactions.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的Tay聊天机器人提供了一个鲜明的例子，说明了对抗用户如何利用部署的AI系统中的稳健性安全防护不足。在发布后的24小时内，协调一致的用户操纵Tay的学习机制，生成不适当和冒犯性的内容。该系统缺乏内容过滤、用户输入验证和行为监控安全防护，这些安全防护本可以检测并阻止利用。这一事件突显了在部署的AI系统中，特别是那些从用户交互中学习的系统中，进行综合输入验证、内容过滤系统和持续行为监控的迫切需要。
- en: '![](../media/file269.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file269.png)'
- en: 'Figure 16.23: **Adversarial Perturbation**: Subtle, physically realizable modifications
    to input data can cause machine learning models to make incorrect predictions,
    even when imperceptible to humans. This example shows how small stickers on a
    stop sign caused a traffic sign classifier to misidentify it as a 45 mph speed
    limit sign with over 85% accuracy, highlighting the vulnerability of ML systems
    to adversarial attacks. Source: [eykholt](https://arxiv.org/abs/1707.08945)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.23：**对抗性扰动**：对输入数据进行微妙的、物理上可实现的变化可以导致机器学习模型做出错误的预测，即使对人类来说不可察觉。这个例子展示了小型贴纸如何导致交通标志分类器错误地将它识别为超过
    85% 准确率的 45 英里/小时限速标志，突显了机器学习系统对对抗性攻击的脆弱性。来源：[eykholt](https://arxiv.org/abs/1707.08945)
- en: This demonstration illustrates how adversarial examples exploit fundamental
    vulnerabilities in ML pattern recognition. The attack’s simplicity—minor input
    modifications invisible to humans causing dramatic prediction changes—reveals
    deep architectural limitations rather than superficial bugs.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示说明了对抗性示例如何利用机器学习模式识别中的基本漏洞。攻击的简单性——对人类不可见的小输入修改导致预测发生巨大变化——揭示了深层次的架构限制，而不是表面上的错误。
- en: Beyond performance degradation, adversarial vulnerabilities create cascading
    systemic risks. In healthcare, attacks on medical imaging could enable misdiagnosis
    ([M.-J. Tsai, Lin, and Lee 2023](ch058.xhtml#ref-tsai2023adversarial)). Financial
    systems face manipulation of trading algorithms leading to economic losses. These
    vulnerabilities fundamentally undermine model trustworthiness by exposing reliance
    on superficial patterns rather than robust concept understanding ([Fursov et al.
    2021](ch058.xhtml#ref-fursov2021adversarial)).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能下降之外，对抗性漏洞还创造了连锁的系统风险。在医疗保健领域，对医学影像的攻击可能导致误诊 ([M.-J. Tsai, Lin, and Lee
    2023](ch058.xhtml#ref-tsai2023adversarial))。金融系统面临交易算法被操纵导致经济损失的风险。这些漏洞通过暴露对表面模式而非稳健概念理解的依赖，从根本上破坏了模型的可靠性
    ([Fursov 等人 2021](ch058.xhtml#ref-fursov2021adversarial))。
- en: Defending against adversarial attacks often requires additional computational
    resources and can impact the overall system performance. Techniques like adversarial
    training, where models are trained on adversarial examples to improve robustness,
    can significantly increase training time and computational requirements ([T. Bai
    et al. 2021](ch058.xhtml#ref-bai2021recent)). Runtime detection and mitigation
    mechanisms, such as input preprocessing ([Addepalli et al. 2020](ch058.xhtml#ref-addepalli2020towards))
    or prediction consistency checks, introduce latency and affect the real-time performance
    of ML systems.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 防御对抗性攻击通常需要额外的计算资源，并可能影响整体系统性能。像对抗性训练这样的技术，即通过在对抗性示例上训练模型来提高鲁棒性，可以显著增加训练时间和计算需求
    ([T. Bai 等人 2021](ch058.xhtml#ref-bai2021recent))。运行时检测和缓解机制，如输入预处理 ([Addepalli
    等人 2020](ch058.xhtml#ref-addepalli2020towards)) 或预测一致性检查，会引入延迟并影响机器学习系统的实时性能。
- en: The presence of adversarial vulnerabilities also complicates the deployment
    and maintenance of ML systems. System designers and operators must consider the
    potential for adversarial attacks and incorporate appropriate defenses and monitoring
    mechanisms. Regular updates and retraining of models become necessary to adapt
    to new adversarial techniques and maintain system security and performance over
    time.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性漏洞的存在也使得机器学习系统的部署和维护变得复杂。系统设计者和操作者必须考虑对抗性攻击的可能性，并纳入适当的防御和监控机制。定期更新和重新训练模型成为必要的，以适应新的对抗性技术，并保持系统安全性和性能。
- en: These vulnerabilities highlight the urgent need for the comprehensive defense
    strategies examined in [Section 16.8.4](ch022.xhtml#sec-robust-ai-input-attack-detection-defense-19d3).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这些漏洞突显了需要审查第 16.8.4 节中提到的全面防御策略的紧迫性 ([Section 16.8.4](ch022.xhtml#sec-robust-ai-input-attack-detection-defense-19d3))。
- en: Data Poisoning
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中毒
- en: Data poisoning presents a critical challenge to the integrity and reliability
    of machine learning systems. By introducing carefully crafted malicious data into
    the training pipeline, adversaries can subtly manipulate model behavior in ways
    that are difficult to detect through standard validation procedures.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒对机器学习系统的完整性和可靠性构成了重大挑战。通过将精心设计的恶意数据引入训练流程，攻击者可以以难以通过标准验证程序检测到的方式微妙地操纵模型行为。
- en: 'A key distinction from adversarial attacks emerges in their timing and targeting.
    While adversarial attacks happen *after* a model is trained (adding noise to test
    inputs), data poisoning happens *before* training (contaminating the training
    data itself). This difference is analogous to fooling a trained student during
    an exam versus giving a student wrong information while they’re learning. Both
    can cause incorrect answers, but they exploit different vulnerabilities at different
    stages:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 与对抗性攻击的关键区别在于它们的时机和目标。虽然对抗性攻击发生在模型训练后（向测试输入添加噪声），但数据中毒发生在训练前（污染训练数据本身）。这种差异类似于在考试中愚弄一个已经训练好的学生，与在学生学习时提供错误信息一样。两者都可能造成错误答案，但它们在不同的阶段利用了不同的漏洞：
- en: Adversarial attacks target deployed models, affecting inference, and can be
    detected by monitoring outputs
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性攻击针对部署的模型，影响推理，并且可以通过监控输出进行检测
- en: Data poisoning targets training data, affecting learning, and is much harder
    to detect because the model honestly learned wrong patterns
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中毒针对训练数据，影响学习，并且由于模型诚实地学习了错误的模式，因此更难检测
- en: Unlike adversarial examples, which target models at inference time, poisoning
    attacks exploit upstream components of the system, such as data collection, labeling,
    or ingestion. As ML systems are increasingly deployed in automated and high-stakes
    environments, understanding how poisoning occurs and how it propagates through
    the system is essential for developing effective defenses.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与针对推理时模型的对抗性示例不同，中毒攻击利用系统的上游组件，如数据收集、标记或摄取。随着机器学习系统越来越多地部署在自动化和高风险环境中，了解中毒如何发生以及它如何通过系统传播对于开发有效的防御措施至关重要。
- en: Data Poisoning Properties
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据中毒特性
- en: Data poisoning[50](#fn50) is an attack in which the training data is deliberately
    manipulated to compromise the performance or behavior of a machine learning model,
    as described in ([Biggio, Nelson, and Laskov 2012](ch058.xhtml#ref-biggio2012poisoning))
    and illustrated in [Figure 16.24](ch022.xhtml#fig-dirty-label-example). Attackers
    may alter existing training samples, introduce malicious examples, or interfere
    with the data collection pipeline. The result is a model that learns biased, inaccurate,
    or exploitable patterns.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒[50](#fn50)是一种攻击，其中训练数据被故意操纵以损害机器学习模型的性能或行为，如([Biggio, Nelson, 和 Laskov
    2012](ch058.xhtml#ref-biggio2012poisoning))所述，并在[图16.24](ch022.xhtml#fig-dirty-label-example)中展示。攻击者可能修改现有的训练样本，引入恶意示例，或干扰数据收集管道。结果是模型学习到有偏见、不准确或可利用的模式。
- en: '![](../media/file270.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file270.png)'
- en: 'Figure 16.24: **Data Poisoning Examples**: Mismatched image-text pairs represent
    a common data poisoning attack, where manipulated training data causes models
    to misclassify inputs. These adversarial examples can compromise model integrity
    and introduce vulnerabilities in real-world applications. Source: ([Shan et al.
    2023](ch058.xhtml#ref-shan2023prompt)).'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.24：**数据中毒示例**：图像与文本不匹配的成对表示一种常见的数据中毒攻击，其中被操纵的训练数据导致模型错误分类输入。这些对抗性示例可能损害模型完整性，并在实际应用中引入漏洞。来源：([Shan等人
    2023](ch058.xhtml#ref-shan2023prompt))。
- en: In most cases, data poisoning unfolds in three stages. In the injection stage,
    the attacker introduces poisoned samples into the training dataset. These samples
    may be altered versions of existing data or entirely new instances designed to
    blend in with clean examples. While they appear benign on the surface, these inputs
    are engineered to influence model behavior in subtle but deliberate ways. The
    attacker may target specific classes, insert malicious triggers, or craft outliers
    intended to distort the decision boundary.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，数据中毒分为三个阶段展开。在注入阶段，攻击者将中毒样本引入训练数据集。这些样本可能是现有数据的修改版本，或者是为了与干净示例混合而设计的全新实例。虽然它们表面上看似无害，但这些输入被设计成以微妙但故意的方影响模型行为。攻击者可能针对特定类别，插入恶意触发器，或者制造旨在扭曲决策边界的异常值。
- en: During the training phase, the machine learning model incorporates the poisoned
    data and learns spurious or misleading patterns. These learned associations may
    bias the model toward incorrect classifications, introduce vulnerabilities, or
    embed backdoors. Because the poisoned data is often statistically similar to clean
    data, the corruption process typically goes unnoticed during standard model training
    and evaluation.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，机器学习模型结合了中毒数据，并学习到虚假或误导性的模式。这些学习到的关联可能会使模型偏向于错误的分类，引入漏洞或嵌入后门。由于中毒数据通常与干净数据在统计上相似，因此在标准的模型训练和评估过程中，这种破坏过程通常不会被察觉。
- en: Finally, in the deployment stage, the attacker leverages the compromised model
    for malicious purposes. This could involve triggering specific behaviors, including
    the misclassification of an input that contains a hidden pattern, or simply exploiting
    the model’s degraded accuracy in production. In real-world systems, such attacks
    can be difficult to trace back to training data, especially if the system’s behavior
    appears erratic only in edge cases or under adversarial conditions.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，在部署阶段，攻击者利用受损的模型进行恶意目的。这可能包括触发特定的行为，包括对包含隐藏模式的输入进行错误分类，或者简单地利用模型在生产中的降低准确性。在现实世界的系统中，这种攻击可能很难追溯到训练数据，特别是如果系统的行为只在边缘情况或对抗条件下显得异常。 '
- en: The consequences of such manipulation are especially severe in high-stakes domains
    like healthcare, where even small disruptions to training data can lead to dangerous
    misdiagnoses or loss of trust in AI-based systems ([Marulli, Marrone, and Verde
    2022](ch058.xhtml#ref-marulli2022sensitivity)).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操纵的后果在高风险领域（如医疗保健）尤为严重，在这些领域，即使是训练数据的小幅中断也可能导致危险的误诊或对基于AI系统的信任丧失 ([Marulli,
    Marrone, and Verde 2022](ch058.xhtml#ref-marulli2022sensitivity))。
- en: Four main categories of poisoning attacks have been identified in the literature
    ([Oprea, Singhal, and Vassilev 2022](ch058.xhtml#ref-oprea2022poisoning)). In
    availability attacks, a substantial portion of the training data is poisoned with
    the aim of degrading overall model performance. A classic example involves flipping
    labels, for instance, systematically changing instances with true label <semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">y = 1</annotation></semantics> to <semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">y = 0</annotation></semantics> in a binary classification
    task. These attacks render the model unreliable across a wide range of inputs,
    effectively making it unusable.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中已经确定了四种主要的中毒攻击类型 ([Oprea, Singhal, and Vassilev 2022](ch058.xhtml#ref-oprea2022poisoning))。在可用性攻击中，大量训练数据被中毒，目的是降低整体模型性能。一个典型的例子是在二元分类任务中，系统地改变具有真实标签
    <semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y
    = 1</annotation></semantics> 的实例，将其改为 <semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">y = 0</annotation></semantics>。这些攻击使模型在广泛的输入上变得不可靠，实际上使其无法使用。
- en: In contrast, targeted poisoning attacks aim to compromise only specific classes
    or instances. Here, the attacker modifies just enough data to cause a small set
    of inputs to be misclassified, while overall accuracy remains relatively stable.
    This subtlety makes targeted attacks especially hard to detect.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，针对性中毒攻击旨在仅损害特定类别或实例。在这里，攻击者仅修改足够的数据，以使一小部分输入被错误分类，而整体准确率保持相对稳定。这种微妙之处使得针对性攻击特别难以检测。
- en: Backdoor poisoning[51](#fn51) introduces hidden triggers into training data,
    subtle patterns or features that the model learns to associate with a particular
    output. When the trigger appears at inference time, the model is manipulated into
    producing a predetermined response. These attacks are often effective even if
    the trigger pattern is imperceptible to human observers.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 后门中毒[51](#fn51) 在训练数据中引入隐藏触发器，这些触发器是模型学习与特定输出关联的微妙模式或特征。当触发器在推理时出现，模型会被操纵产生预定的响应。即使触发器模式对人类观察者不可见，这些攻击通常也相当有效。
- en: Subpopulation poisoning focuses on compromising a specific subset of the data
    population. While similar in intent to targeted attacks, subpopulation poisoning
    applies availability-style degradation to a localized group, for example, a particular
    demographic or feature cluster, while leaving the rest of the model’s performance
    intact. This distinction makes such attacks both highly effective and especially
    dangerous in fairness-sensitive applications.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 子群体中毒攻击专注于破坏数据集的一个特定子集。虽然与针对性攻击有相似的意图，但子群体中毒攻击将可用性风格的降级应用于局部化群体，例如特定的群体或特征簇，同时保持模型其余性能完好。这种区别使得此类攻击既非常有效，又特别危险，尤其是在对公平性敏感的应用中。
- en: A common thread across these poisoning strategies is their subtlety. Manipulated
    samples are typically indistinguishable from clean data, making them difficult
    to identify through casual inspection or standard data validation. These manipulations
    might involve small changes to numeric values, slight label inconsistencies, or
    embedded visual patterns, each designed to blend into the data distribution while
    still affecting model behavior.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这些中毒策略的共同点是它们的微妙。被操纵的样本通常与干净数据无法区分，这使得它们难以通过随意检查或标准数据验证来识别。这些操纵可能涉及对数值的小幅修改、轻微的标签不一致性或嵌入的视觉模式，每个都旨在融入数据分布，同时仍然影响模型行为。
- en: Such attacks may be carried out by internal actors, like data engineers or annotators
    with privileged access, or by external adversaries who exploit weak points in
    the data collection pipeline. In crowdsourced environments or open data collection
    scenarios, poisoning can be as simple as injecting malicious samples into a shared
    dataset or influencing user-generated content.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击可能由内部行为者执行，如具有特权访问权限的数据工程师或标注员，或由利用数据收集管道薄弱点的外部对手执行。在众包环境或公开数据收集场景中，中毒可能简单到向共享数据集中注入恶意样本或影响用户生成的内容。
- en: Crucially, poisoning attacks often target the early stages of the ML pipeline,
    such as collection and preprocessing, where there may be limited oversight. If
    data is pulled from unverified sources or lacks strong validation protocols, attackers
    can slip in poisoned data that appears statistically normal. The absence of integrity
    checks, robust outlier detection, or lineage tracking only heightens the risk.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，中毒攻击通常针对机器学习管道的早期阶段，如收集和预处理，在这些阶段可能存在有限的监督。如果数据来自未经验证的来源或缺乏强大的验证协议，攻击者可以悄悄地插入看似统计上正常的受毒数据。缺乏完整性检查、稳健的异常检测或血缘跟踪只会加剧风险。
- en: The goal of these attacks is to corrupt the learning process itself. A model
    trained on poisoned data may learn spurious correlations, overfit to false signals,
    or become vulnerable to highly specific exploit conditions. Whether the result
    is a degraded model or one with a hidden exploit path, the trustworthiness and
    safety of the system are severely compromised.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击的目标是破坏学习过程本身。在受毒数据上训练的模型可能会学习到虚假的相关性，过度拟合到虚假信号，或对高度特定的利用条件变得脆弱。无论是导致模型退化还是隐藏的利用路径，系统的可信度和安全性都受到了严重损害。
- en: Data Poisoning Attack Methods
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据中毒攻击方法
- en: Data poisoning can be implemented through a variety of mechanisms, depending
    on the attacker’s access to the system and understanding of the data pipeline.
    These mechanisms reflect different strategies for how the training data can be
    corrupted to achieve malicious outcomes.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒可以通过各种机制实现，这取决于攻击者对系统的访问权限和对数据管道的理解。这些机制反映了不同的策略，用于如何破坏训练数据以实现恶意结果。
- en: One of the most direct approaches involves modifying the labels of training
    data. In this method, an attacker selects a subset of training samples and alters
    their labels, flipping <semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">y = 1</annotation></semantics> to <semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">y = 0</annotation></semantics> or reassigning categories
    in multi-class settings. As shown in [Figure 16.25](ch022.xhtml#fig-distribution-shift-example),
    even small-scale label inconsistencies can lead to significant distributional
    shifts and learning disruptions.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法之一是修改训练数据的标签。在这种方法中，攻击者选择训练样本的一个子集并更改它们的标签，将<semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">y = 1</annotation></semantics>翻转成<semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">y = 0</annotation></semantics>或在多类设置中重新分配类别。如图[图16.25](ch022.xhtml#fig-distribution-shift-example)所示，即使是小规模的标签不一致也可能导致显著的分布变化和学习中断。
- en: '![](../media/file271.svg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file271.svg)'
- en: 'Figure 16.25: **Data Poisoning Impact**: Subtle perturbations to training data
    labels can induce significant distributional shifts, leading to model inaccuracies
    and compromised performance in machine learning systems. These shifts exemplify
    how even limited adversarial control over training data can disrupt model learning
    and highlight the vulnerability of data-driven approaches to malicious manipulation.
    Source: ([Shan et al. 2023](ch058.xhtml#ref-shan2023prompt)).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.25：**数据中毒影响**：对训练数据标签的微妙扰动可以引起显著的分布变化，导致机器学习系统中的模型不准确和性能受损。这些变化展示了即使是有限的对抗训练数据控制也能破坏模型学习，并突显了数据驱动方法易受恶意操纵的脆弱性。来源：([Shan等人2023](ch058.xhtml#ref-shan2023prompt))。
- en: Another mechanism involves modifying the input features of training examples
    without changing the labels. This might include imperceptible pixel-level changes
    in images, subtle perturbations in structured data, or embedding fixed patterns
    that act as triggers for backdoor attacks. These alterations are often designed
    using optimization techniques that maximize their influence on the model while
    minimizing detectability.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种机制涉及在不改变标签的情况下修改训练示例的输入特征。这可能包括图像中不可察觉的像素级变化、结构化数据中的微妙扰动，或者嵌入作为后门攻击触发器的固定模式。这些修改通常使用优化技术设计，以最大化其对模型的影响同时最小化可检测性。
- en: More sophisticated attacks generate entirely new, malicious training examples.
    These synthetic samples may be created using adversarial methods, generative models,
    or even data synthesis tools. The aim is to carefully craft inputs that will distort
    the decision boundary of the model when incorporated into the training set. Such
    inputs may appear natural and legitimate but are engineered to introduce vulnerabilities.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的攻击会生成全新的恶意训练示例。这些合成样本可能使用对抗方法、生成模型，甚至数据合成工具创建。目标是精心设计输入，当它们被纳入训练集时，将扭曲模型的决策边界。这种输入可能看起来自然和合法，但却是为了引入漏洞而设计的。
- en: Other attackers focus on weaknesses in data collection and preprocessing. If
    the training data is sourced from web scraping, social media, or untrusted user
    submissions, poisoned samples can be introduced upstream. These samples may pass
    through insufficient cleaning or validation checks, reaching the model in a “trusted”
    form. This is particularly dangerous in automated pipelines where human review
    is limited or absent.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 其他攻击者专注于数据收集和预处理中的弱点。如果训练数据来源于网络爬取、社交媒体或不可信的用户提交，则可以在上游引入中毒样本。这些样本可能通过不充分的清理或验证检查，以“可信”的形式到达模型。这在人类审查有限或缺失的自动化管道中尤其危险。
- en: In physically deployed systems, attackers may manipulate data at the source—for
    example, altering the environment captured by a sensor. A self-driving car might
    encounter poisoned data if visual markers on a road sign are subtly altered, causing
    the model to misclassify it during training. This kind of environmental poisoning
    blurs the line between adversarial attacks and data poisoning, but the mechanism,
    which involves compromising the training data, is the same.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理部署的系统中，攻击者可能操纵数据源——例如，改变传感器捕获的环境。自动驾驶汽车可能会遇到中毒数据，如果道路标志上的视觉标记被微妙地改变，导致模型在训练过程中错误分类它。这种环境中毒模糊了对抗攻击和数据中毒之间的界限，但涉及损害训练数据的机制是相同的。
- en: Online learning systems represent another unique attack surface. These systems
    continuously adapt to new data streams, making them particularly susceptible to
    gradual poisoning. An attacker may introduce malicious samples incrementally,
    causing slow but steady shifts in model behavior. This form of attack is illustrated
    in [Figure 16.26](ch022.xhtml#fig-poisoning-attack-example).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习系统代表了另一种独特的攻击面。这些系统持续适应新的数据流，因此特别容易受到渐进式中毒的影响。攻击者可能逐步引入恶意样本，导致模型行为缓慢但稳定地改变。这种攻击形式在[图16.26](ch022.xhtml#fig-poisoning-attack-example)中得到了说明。
- en: '![](../media/file272.svg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file272.svg)'
- en: 'Figure 16.26: **Data Poisoning Attack**: Adversarial manipulation of training
    data introduces subtle perturbations that compromise model integrity; incremental
    poisoning gradually shifts model behavior over time, making detection challenging
    in online learning systems. This attack surface differs from adversarial examples
    because it targets the model *during* training rather than at inference.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.26：**数据中毒攻击**：对抗性操纵训练数据引入了细微的扰动，损害了模型的完整性；渐进式中毒逐渐改变模型行为，使得在线学习系统中的检测变得困难。这种攻击面与对抗样本不同，因为它在训练期间而不是在推理期间针对模型。
- en: Insider collaboration adds a final layer of complexity. Malicious actors with
    legitimate access to training data, including annotators, researchers, or data
    vendors, can craft poisoning strategies that are more targeted and subtle than
    external attacks. These insiders may have knowledge of the model architecture
    or training procedures, giving them an advantage in designing effective poisoning
    schemes.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 内部合作增加了另一层复杂性。拥有合法访问训练数据的恶意行为者，包括标注者、研究人员或数据供应商，可以制定比外部攻击更具有针对性和隐蔽性的中毒策略。这些内部人员可能了解模型架构或训练过程，这使他们设计有效的中毒方案时具有优势。
- en: 'Defending against these diverse mechanisms requires a multi-pronged approach:
    secure data collection protocols, anomaly detection, robust preprocessing pipelines,
    and strong access control. Validation mechanisms must be sophisticated enough
    to detect not only outliers but also cleverly disguised poisoned samples that
    sit within the statistical norm.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 防御这些多样化的机制需要多管齐下的方法：安全的数据收集协议、异常检测、健壮的预处理管道和强大的访问控制。验证机制必须足够复杂，不仅能够检测异常值，还能够检测巧妙伪装的中毒样本，这些样本位于统计规范之内。
- en: Data Poisoning Effects on ML
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据中毒对机器学习的影响
- en: The effects of data poisoning extend far beyond simple accuracy degradation.
    In the most general sense, a poisoned dataset leads to a corrupted model. But
    the specific consequences depend on the attack vector and the adversary’s objective.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒的影响远不止简单的准确率下降。在最一般的意义上，一个中毒的数据集会导致模型被破坏。但具体的后果取决于攻击向量以及攻击者的目标。
- en: One common outcome is the degradation of overall model performance. When large
    portions of the training set are poisoned, often through label flipping or the
    introduction of noisy features, the model struggles to identify valid patterns,
    leading to lower accuracy, recall, or precision. In mission-critical applications
    like medical diagnosis or fraud detection, even small performance losses can result
    in significant real-world harm.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的后果是整体模型性能的下降。当训练集的大部分数据被中毒，通常是通过标签翻转或引入噪声特征，模型难以识别有效模式，导致准确率、召回率或精度的降低。在医疗诊断或欺诈检测等关键任务应用中，即使是微小的性能损失也可能导致重大的现实世界损害。
- en: Targeted poisoning presents a different kind of danger. Rather than undermining
    the model’s general performance, these attacks cause specific misclassifications.
    A malware detector, for instance, may be engineered to ignore one particular signature,
    allowing a single attack to bypass security. Similarly, a facial recognition model
    might be manipulated to misidentify a specific individual, while functioning normally
    for others.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 针对性中毒呈现了一种不同的危险。这些攻击不是破坏模型的总体性能，而是导致特定的错误分类。例如，一个恶意软件检测器可能被设计成忽略一个特定的签名，允许单个攻击绕过安全措施。同样，一个面部识别模型可能被操纵来错误识别特定个体，而对其他人则正常工作。
- en: Some poisoning attacks introduce hidden vulnerabilities in the form of backdoors
    or trojans. These poisoned models behave as expected during evaluation but respond
    in a malicious way when presented with specific triggers. In such cases, attackers
    can “activate” the exploit on demand, bypassing system protections without triggering
    alerts.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 一些中毒攻击以后门或特洛伊木马的形式引入隐藏漏洞。这些中毒模型在评估期间表现正常，但在遇到特定触发器时会以恶意方式响应。在这种情况下，攻击者可以按需“激活”漏洞，绕过系统保护而不触发警报。
- en: Bias is another insidious impact of data poisoning. If an attacker poisons samples
    tied to a specific demographic or feature group, they can skew the model’s outputs
    in biased or discriminatory ways. Such attacks threaten fairness, amplify existing
    societal inequities, and are difficult to diagnose if the overall model metrics
    remain high.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见是数据中毒的另一个隐蔽影响。如果攻击者中毒与特定人口或特征组相关的样本，他们可以通过有偏见或歧视的方式扭曲模型的输出。这种攻击威胁到公平性，放大了现有的社会不平等，如果整体模型指标仍然很高，则难以诊断。
- en: Ultimately, data poisoning undermines the trustworthiness of the system itself.
    A model trained on poisoned data cannot be considered reliable, even if it performs
    well in benchmark evaluations. This erosion of trust has profound implications,
    particularly in fields like autonomous systems, financial modeling, and public
    policy.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，数据中毒损害了系统的可信度。在中毒数据上训练的模型不能被认为是可靠的，即使它在基准评估中表现良好。这种信任的侵蚀具有深远的影响，尤其是在自主系统、金融建模和公共政策等领域。
- en: 'Case Study: Art Protection via Poisoning'
  id: totrans-451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究：通过中毒进行艺术保护
- en: Interestingly, not all data poisoning is malicious. Researchers have begun to
    explore its use as a defensive tool, particularly in the context of protecting
    creative work from unauthorized use by generative AI models.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，并非所有数据中毒都是恶意的。研究人员已经开始探索将其用作防御工具，特别是在保护创意作品免受生成AI模型未经授权使用的情况下。
- en: A compelling example is Nightshade, developed by researchers at the University
    of Chicago to help artists prevent their work from being scraped and used to train
    image generation models without consent ([Shan et al. 2023](ch058.xhtml#ref-shan2023prompt)).
    Nightshade allows artists to apply subtle perturbations to their images before
    publishing them online. These changes are invisible to human viewers but cause
    serious degradation in generative models that incorporate them into training.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人信服的例子是Nightshade，由芝加哥大学的研究人员开发，以帮助艺术家防止他们的作品被抓取并用于未经同意的训练图像生成模型([Shan等人2023](ch058.xhtml#ref-shan2023prompt))。Nightshade允许艺术家在将图像发布到网上之前对其应用微小的扰动。这些变化对人类观众来说是不可见的，但会对将它们纳入训练的生成模型造成严重的退化。
- en: When Stable Diffusion was trained on just 300 poisoned images, the model began
    producing bizarre outputs, such as cows when prompted with “car,” or cat-like
    creatures in response to “dog.” These results, visualized in [Figure 16.27](ch022.xhtml#fig-poisoning),
    show how effectively poisoned samples can distort a model’s conceptual associations.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 当Stable Diffusion仅在300个中毒图像上训练时，模型开始产生奇异的结果，例如在提示“汽车”时产生牛，或者在提示“狗”时产生类似猫的生物。这些结果如图[图16.27](ch022.xhtml#fig-poisoning)所示，展示了中毒样本如何有效地扭曲模型的概念关联。
- en: '![](../media/file273.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file273.png)'
- en: 'Figure 16.27: **Poisoning Attack**: An incremental process where malicious
    samples are introduced to gradually shift model behavior during online learning.
    Continuous data streams can be manipulated without immediate detection through
    this. Source: ([Shan et al. 2023](ch058.xhtml#ref-shan2023prompt)).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.27：**中毒攻击**：一个逐步的过程，在在线学习期间逐渐引入恶意样本以改变模型行为。通过这种方式，可以操纵连续数据流而不会立即被发现。来源：([Shan等人2023](ch058.xhtml#ref-shan2023prompt))。
- en: What makes Nightshade especially potent is the cascading effect of poisoned
    concepts. Because generative models rely on semantic relationships between categories,
    a poisoned “car” can bleed into related concepts like “truck,” “bus,” or “train,”
    leading to widespread hallucinations.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 夜幕降临特别有效的原因是中毒概念的级联效应。因为生成模型依赖于类别之间的语义关系，一个中毒的“汽车”可以渗透到相关的概念，如“卡车”、“公交车”或“火车”，导致广泛的幻觉。
- en: However, like any powerful tool, Nightshade also introduces risks. The same
    technique used to protect artistic content could be repurposed to sabotage legitimate
    training pipelines, highlighting the dual-use dilemma[52](#fn52) at the heart
    of modern machine learning security.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像任何强大的工具一样，Nightshade也带来了风险。用于保护艺术内容的相同技术可能被重新用于破坏合法的训练流程，突显了现代机器学习安全核心的二元使用困境[52](#fn52)。
- en: Distribution Shifts
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布偏移
- en: Distribution shifts represent one of the most prevalent and challenging robustness
    issues in deployed machine learning systems. Unlike adversarial attacks or data
    poisoning, distribution shifts often occur naturally as environments evolve, making
    them a core concern for system reliability. This section examines the characteristics
    of different types of distribution shifts, the mechanisms through which they occur,
    their impact on machine learning systems, and practical approaches for detection
    and mitigation.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移代表了在部署的机器学习系统中最普遍和最具挑战性的鲁棒性问题之一。与对抗攻击或数据中毒不同，分布偏移通常在环境演变过程中自然发生，这使得它们成为系统可靠性的核心关注点。本节将探讨不同类型分布偏移的特征、它们发生的机制、对机器学习系统的影响，以及检测和缓解的实际方法。
- en: Distribution Shift Properties
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布偏移属性
- en: Distribution shift refers to the phenomenon where the data distribution encountered
    by a machine learning model during deployment differs from the distribution it
    was trained on, challenging the generalization capabilities established through
    the training methodologies in [Chapter 8](ch014.xhtml#sec-ai-training) and architectural
    design choices from [Chapter 4](ch010.xhtml#sec-dnn-architectures), as shown in
    [Figure 16.28](ch022.xhtml#fig-distribution-shift). This change in distribution
    is not necessarily the result of a malicious attack. Rather, it often reflects
    the natural evolution of real-world environments over time. In essence, the statistical
    properties, patterns, or assumptions in the data may change between training and
    inference phases, which can lead to unexpected or degraded model performance.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移指的是机器学习模型在部署过程中遇到的数据分布与其训练时所用的分布不同，挑战了通过[第8章](ch014.xhtml#sec-ai-training)中的训练方法以及[第4章](ch010.xhtml#sec-dnn-architectures)中的架构设计选择建立的一般化能力，如图16.28[所示](ch022.xhtml#fig-distribution-shift)。这种分布的变化不一定是恶意攻击的结果。相反，它通常反映了现实世界环境随时间的自然演变。本质上，数据在训练和推理阶段之间的统计属性、模式或假设可能会发生变化，这可能导致模型性能出现意外或下降。
- en: '![](../media/file274.svg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file274.svg)'
- en: 'Figure 16.28: **Distribution Shift**: Small inconsistencies between training
    and deployment data (represented by differing distributions of spurious feature
    *z*) can significantly disrupt model performance, even without altering the true
    label *y*. This figure emphasizes how data poisoning attacks exploit distributional
    differences to induce model errors and emphasizes the vulnerability of machine
    learning systems to subtle data manipulations. Source: ([Shan et al. 2023](ch058.xhtml#ref-shan2023prompt)).'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.28：**分布偏移**：训练数据和部署数据之间（由虚假特征 *z* 的不同分布表示）的小不一致性可能会严重破坏模型性能，即使没有改变真实的标签
    *y*。此图强调了数据中毒攻击如何利用分布差异来诱导模型错误，并强调了机器学习系统对微妙数据操作的脆弱性。来源：([Shan 等人 2023](ch058.xhtml#ref-shan2023prompt))。
- en: 'A distribution shift typically takes one of several forms:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移通常采取以下几种形式：
- en: '**Covariate shift**, where the input distribution <semantics><mrow><mi>P</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(x)</annotation></semantics> changes while the conditional
    label distribution <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(y
    \mid x)</annotation></semantics> remains stable.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协变量偏移**，其中输入分布 <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics>
    发生变化，而条件标签分布 <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(y
    \mid x)</annotation></semantics> 保持稳定。'
- en: '**Label shift**, where the label distribution <semantics><mrow><mi>P</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(y)</annotation></semantics> changes while <semantics><mrow><mi>P</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(x
    \mid y)</annotation></semantics> stays the same.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签偏移**，其中标签分布 <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(y)</annotation></semantics>
    发生变化，而 <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(x
    \mid y)</annotation></semantics> 保持不变。'
- en: '**Concept drift**, where the relationship between inputs and outputs, <semantics><mrow><mi>P</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(y
    \mid x)</annotation></semantics>, evolves over time.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移**，其中输入和输出之间的关系，<semantics><mrow><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">P(y \mid x)</annotation></semantics>，随着时间的推移而演变。'
- en: These formal definitions help frame more intuitive examples of shift that are
    commonly encountered in practice.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 这些形式化的定义有助于构建更直观的例子，这些例子在实践中经常遇到。
- en: One of the most common causes is domain mismatch, where the model is deployed
    on data from a different domain than it was trained on. For example, a sentiment
    analysis model trained on movie reviews may perform poorly when applied to tweets,
    due to differences in language, tone, and structure. In this case, the model has
    learned domain-specific features that do not generalize well to new contexts.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的原因之一是领域不匹配，即模型在训练数据域之外的数据上部署。例如，在电影评论上训练的情感分析模型，当应用于推文时可能会表现不佳，因为语言、语气和结构存在差异。在这种情况下，模型已经学习了特定领域的特征，这些特征在新的上下文中泛化不良。
- en: Another major source is temporal drift, where the input distribution evolves
    gradually or suddenly over time. In production settings, data changes due to new
    trends, seasonal effects, or shifts in user behavior. For instance, in a fraud
    detection system, fraud patterns may evolve as adversaries adapt. Without ongoing
    monitoring or retraining, models become stale and ineffective. This form of shift
    is visualized in [Figure 16.29](ch022.xhtml#fig-drift-over-time).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要来源是时间漂移，其中输入分布随着时间的推移逐渐或突然演变。在生产环境中，数据因新趋势、季节性效应或用户行为的变化而变化。例如，在欺诈检测系统中，欺诈模式可能会随着对手的适应而演变。如果没有持续的监控或重新训练，模型就会过时，变得无效。这种变化形式在[图16.29](ch022.xhtml#fig-drift-over-time)中得到了可视化。
- en: Contextual changes arise when deployment environments differ from training conditions
    due to external factors such as lighting, sensor variation, or user behavior.
    For example, a vision model trained in a lab under controlled lighting may underperform
    when deployed in outdoor or dynamic environments.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署环境由于外部因素（如光照、传感器变化或用户行为）与训练条件不同时，会出现上下文变化。例如，在受控光照条件下在实验室训练的视觉模型，在户外或动态环境中部署时可能会表现不佳。
- en: Another subtle but critical factor is unrepresentative training data. If the
    training dataset fails to capture the full variability of the production environment,
    the model may generalize poorly. For example, a facial recognition model trained
    predominantly on one demographic group may produce biased or inaccurate predictions
    when deployed more broadly. In this case, the shift reflects missing diversity
    or structure in the training data.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个微妙但关键的因素是不具代表性的训练数据。如果训练数据集未能捕捉到生产环境的全部可变性，模型可能泛化不良。例如，主要在某一人口群体上训练的面部识别模型，在更广泛的应用中可能会产生有偏见或不准确的预测。在这种情况下，这种变化反映了训练数据中缺失的多样性和结构。
- en: '![](../media/file275.svg)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file275.svg)'
- en: 'Figure 16.29: **Temporal Drift**: Shifting data distributions over time degrade
    model performance unless systems adapt through continuous monitoring and retraining.
    Concept drift manifests as changes in input patterns—such as evolving fraud schemes
    or seasonal trends—that require models to learn new relationships and maintain
    accuracy in dynamic environments.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.29：**时间漂移**：随着时间的推移，数据分布的变化会降低模型性能，除非系统通过持续监控和重新训练来适应。概念漂移表现为输入模式的变化——例如演变的欺诈方案或季节性趋势——这要求模型学习新的关系并在动态环境中保持准确性。
- en: Distribution shifts like these can dramatically reduce the performance and reliability
    of ML models in production. Building robust systems requires not only understanding
    these shifts, but actively detecting and responding to them as they emerge.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分布偏移会显著降低生产中机器学习模型的性能和可靠性。构建稳健的系统不仅需要理解这些偏移，还需要积极检测和应对它们的出现。
- en: Tesla’s Autopilot system demonstrates how distribution shifts in real-world
    deployment can challenge even sophisticated ML systems. Vision systems trained
    primarily on highway driving data showed degraded performance in construction
    zones, unusual road configurations, and varying weather conditions that differed
    significantly from training scenarios. The system struggled with edge cases like
    construction barriers, unusual lane markings, and temporary traffic patterns not
    well-represented in training data. This highlights the critical importance of
    diverse training data collection and robust handling of distribution shift, particularly
    in safety-critical applications where edge cases can have severe consequences.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉的自动驾驶系统展示了在现实世界部署中分布偏移如何挑战甚至复杂的机器学习系统。主要在高速公路驾驶数据上训练的视觉系统在施工区域、不寻常的道路配置和与训练场景显著不同的各种天气条件下表现下降。系统在训练数据中不太充分表示的边缘案例，如施工障碍物、不寻常的车道标记和临时交通模式上遇到了困难。这突出了收集多样化训练数据和稳健处理分布偏移的关键重要性，尤其是在边缘案例可能产生严重后果的安全关键应用中。
- en: Distribution Shift Mechanisms
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布偏移机制
- en: Distribution shifts arise from a variety of underlying mechanisms—both natural
    and system-driven. Understanding these mechanisms helps practitioners detect,
    diagnose, and design mitigation strategies.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移源于各种潜在机制——既有自然的也有系统驱动的。理解这些机制有助于从业者检测、诊断并设计缓解策略。
- en: One common mechanism is a change in data sources. When data collected at inference
    time comes from different sensors, APIs, platforms, or hardware than the training
    data, even subtle differences in resolution, formatting, or noise can introduce
    significant shifts. For example, a speech recognition model trained on audio from
    one microphone type may struggle with data from a different device.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的机制是数据源的变化。当推理时收集的数据来自与训练数据不同的传感器、API、平台或硬件时，即使是细微的分辨率、格式或噪声差异也可能引入重大的偏移。例如，在一个麦克风类型上训练的语音识别模型可能会在来自不同设备的数据上遇到困难。
- en: Temporal evolution refers to changes in the underlying data over time. In recommendation
    systems, user preferences shift. In finance, market conditions change. These shifts
    may be slow and continuous or abrupt and disruptive. Without temporal awareness
    or continuous evaluation, models can become obsolete, frequently without prior
    indication. To illustrate this, [Figure 16.30](ch022.xhtml#fig-temporal-evolution)
    shows how selective breeding over generations has significantly changed the physical
    characteristics of a dog breed. The earlier version of the breed exhibits a lean,
    athletic build, while the modern version is stockier, with a distinctively different
    head shape and musculature. This transformation is analogous to how data distributions
    can shift in real-world systems—initial data used to train a model may differ
    substantially from the data encountered over time. Just as evolutionary pressures
    shape biological traits, dynamic user behavior, market forces, or changing environments
    can shift the distribution of data in machine learning applications. Without periodic
    retraining or adaptation, models exposed to these evolving distributions may underperform
    or become unreliable.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 时间演变指的是随时间变化的基础数据的变化。在推荐系统中，用户偏好会发生变化。在金融领域，市场状况会改变。这些变化可能是缓慢且连续的，也可能是突然且破坏性的。如果没有时间意识或持续评估，模型可能会变得过时，而且通常没有先前的迹象。为了说明这一点，[图16.30](ch022.xhtml#fig-temporal-evolution)展示了经过多代选择性育种后，狗的品种的物理特征发生了显著变化。该品种的早期版本表现出苗条、健壮的体型，而现代版本则更加粗壮，头部形状和肌肉结构有显著的不同。这种转变类似于现实世界系统中数据分布的变化——用于训练模型的初始数据可能与随时间遇到的数据有显著差异。正如进化压力塑造生物特征一样，动态用户行为、市场力量或环境变化可以改变机器学习应用中的数据分布。如果没有定期的重新训练或适应，暴露于这些演变分布的模型可能会表现不佳或变得不可靠。
- en: '![](../media/file276.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file276.png)'
- en: 'Figure 16.30: **Breed Evolution**: Selective breeding over generations produces
    substantial shifts in phenotypic characteristics, mirroring how data distributions
    change in machine learning systems over time. These temporal shifts necessitate
    model retraining or adaptation to maintain performance, as initial training data
    may no longer accurately represent current input distributions.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.30：**物种进化**：经过多代的选育，表型特征发生显著变化，这反映了机器学习系统中数据分布随时间的变化。这些时间上的变化需要模型重新训练或适应以保持性能，因为初始训练数据可能不再准确代表当前的输入分布。
- en: Domain-specific variation arises when a model trained on one setting is applied
    to another. A medical diagnosis model trained on data from one hospital may underperform
    in another due to differences in equipment, demographics, or clinical workflows.
    These variations often require explicit adaptation strategies, such as domain
    generalization or fine-tuning.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型在一个设置上训练后应用于另一个设置时，会出现领域特定的变化。一个在一家医院数据上训练的医疗诊断模型可能在另一家医院表现不佳，这可能是由于设备、人口统计或临床工作流程的差异。这些变化通常需要显式的适应策略，如领域泛化或微调。
- en: Selection bias occurs when the training data does not accurately reflect the
    target population. This may result from sampling strategies, data access constraints,
    or labeling choices. The result is a model that overfits to specific segments
    and fails to generalize. Addressing this requires thoughtful data collection and
    continuous validation.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据没有准确反映目标人群时，就会发生选择偏差。这可能是由于抽样策略、数据访问限制或标注选择造成的。结果是模型过度拟合到特定部分，无法泛化。解决这个问题需要深思熟虑的数据收集和持续验证。
- en: Feedback loops are a particularly subtle mechanism. In some systems, model predictions
    influence user behavior, which in turn affects future inputs. For instance, a
    dynamic pricing model might set prices that change buying patterns, which then
    distort the distribution of future training data. These loops can reinforce narrow
    patterns and make model behavior difficult to predict.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈循环是一种特别微妙的机制。在某些系统中，模型预测会影响用户行为，反过来又影响未来的输入。例如，一个动态定价模型可能会设定改变购买模式的价格，这进而扭曲未来训练数据的分布。这些循环可以加强狭窄的模式，使模型行为难以预测。
- en: Lastly, adversarial manipulation can induce distribution shifts deliberately.
    Attackers may introduce out-of-distribution samples or craft inputs that exploit
    weak spots in the model’s decision boundary. These inputs may lie far from the
    training distribution and can cause unexpected or unsafe predictions.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对抗性操作可以故意诱导分布偏移。攻击者可能会引入分布外的样本或构建利用模型决策边界弱点的输入。这些输入可能远离训练分布，并可能导致意外或不安全的预测。
- en: These mechanisms often interact, making real-world distribution shift detection
    and mitigation complex. From a systems perspective, this complexity necessitates
    ongoing monitoring, logging, and feedback pipelines—features often absent in early-stage
    or static ML deployments.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制通常相互作用，使得现实世界的分布偏移检测和缓解变得复杂。从系统角度来看，这种复杂性需要持续的监控、日志记录和反馈管道——这些功能通常在早期或静态机器学习部署中缺失。
- en: Distribution Shift Effects on ML
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布偏移对机器学习的影响
- en: Distribution shift can affect nearly every dimension of ML system performance,
    from prediction accuracy and latency to user trust and system maintainability.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移可以影响机器学习系统性能的几乎每个维度，从预测准确性和延迟到用户信任和系统可维护性。
- en: A common and immediate consequence is degraded predictive performance. When
    the data at inference time differs from training data, the model may produce systematically
    inaccurate or inconsistent predictions. This erosion of accuracy is particularly
    dangerous in high-stakes applications like fraud detection, autonomous vehicles,
    or clinical decision support.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 常见且直接的结果是预测性能下降。当推理时的数据与训练数据不同时，模型可能会产生系统性的不准确或不一致的预测。这种准确性的侵蚀在高风险应用中尤其危险，如欺诈检测、自动驾驶或临床决策支持。
- en: Another serious effect is loss of reliability and trustworthiness. As distribution
    shifts, users may notice inconsistent or erratic behavior. For example, a recommendation
    system might begin suggesting irrelevant or offensive content. Even if overall
    accuracy metrics remain acceptable, loss of user trust can undermine the system’s
    value.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个严重的影响是可靠性和可信度的丧失。随着分布的变化，用户可能会注意到不一致或异常的行为。例如，推荐系统可能会开始建议不相关或冒犯性的内容。即使整体准确率指标仍然可以接受，用户信任的丧失可能会损害系统的价值。
- en: Distribution shift also amplifies model bias. If certain groups or data segments
    are underrepresented in the training data, the model may fail more frequently
    on those groups. Under shifting conditions, these failures can become more pronounced,
    resulting in discriminatory outcomes or fairness violations.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移也放大了模型偏差。如果某些群体或数据片段在训练数据中代表性不足，模型在这些群体上可能会频繁失败。在偏移条件下，这些失败可能会变得更加明显，导致歧视性结果或公平性违规。
- en: ML models trained on data from specific hospitals frequently show degraded performance
    when deployed at different institutions, illustrating a classic distribution shift
    problem in healthcare. Models trained at academic medical centers with specific
    patient populations, equipment types, and clinical protocols failed to generalize
    to community hospitals with different demographics, imaging equipment, and clinical
    workflows. For example, diagnostic models trained on data from one hospital’s
    CT scanners showed reduced accuracy when applied to images from different scanner
    manufacturers or imaging protocols. This demonstrates how seemingly minor differences
    in data collection procedures and equipment can create significant distribution
    shifts that impact model performance and potentially patient safety.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定医院的数据上训练的机器学习模型，在部署到不同机构时通常表现出性能下降，这说明了医疗保健中经典的分布偏移问题。在具有特定患者群体、设备类型和临床方案的学术医疗中心训练的模型，未能推广到具有不同人口统计、成像设备和临床工作流程的社区医院。例如，基于一家医院CT扫描仪数据的诊断模型，在应用于不同扫描仪制造商或成像协议的图像时，准确性会降低。这表明，数据收集程序和设备看似微小的差异可以造成重大的分布偏移，从而影响模型性能和潜在的患者安全。
- en: Uncertainty and operational risk also increase. In many production settings,
    model decisions feed directly into business operations or automated actions. Under
    shift, these decisions become less predictable and harder to validate, increasing
    the risk of cascading failures or poor decisions downstream.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性和运营风险也增加。在许多生产环境中，模型决策直接进入业务运营或自动化操作。在偏移情况下，这些决策变得不那么可预测，更难验证，增加了级联故障或不良决策的风险。
- en: From a system maintenance perspective, distribution shifts complicate retraining
    and deployment workflows. Without robust mechanisms for drift detection and performance
    monitoring, shifts may go unnoticed until performance degrades significantly.
    Once detected, retraining may be required—raising challenges related to data collection,
    labeling, model rollback, and validation. This creates friction in continuous
    integration and deployment (CI/CD) workflows and can significantly slow down iteration
    cycles.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统维护的角度来看，分布偏移使重新培训和部署工作流程复杂化。如果没有强大的漂移检测和性能监控机制，偏移可能会在性能显著下降之前不被察觉。一旦检测到，可能需要进行重新培训，这会引发与数据收集、标注、模型回滚和验证相关的问题。这会在持续集成和部署（CI/CD）工作流程中产生摩擦，并可能显著减缓迭代周期。
- en: Distribution shift also increases vulnerability to adversarial attacks. Attackers
    can exploit the model’s poor calibration on unfamiliar data, using slight perturbations
    to push inputs outside the training distribution and cause failures. This is especially
    concerning when system feedback loops or automated decisioning pipelines are in
    place.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 分布偏移也增加了对抗攻击的脆弱性。攻击者可以利用模型在未知数据上的不良校准，通过轻微扰动将输入推离训练分布并导致故障。当系统反馈循环或自动化决策流程存在时，这一点尤其令人担忧。
- en: 'From a systems perspective, distribution shift is not just a modeling concern—it
    is a core operational challenge. It requires end-to-end system support: mechanisms
    for data logging, drift detection, automated alerts, model versioning, and scheduled
    retraining. ML systems must be designed to detect when performance degrades in
    production, diagnose whether a distribution shift is the cause, and trigger appropriate
    mitigation actions. This might include human-in-the-loop review, fallback strategies,
    model retraining pipelines, or staged deployment rollouts.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，分布偏移不仅是一个建模问题，它是一个核心的运营挑战。它需要端到端系统支持：数据记录、漂移检测、自动警报、模型版本控制和计划重新培训的机制。机器学习系统必须设计成能够在生产中检测到性能下降，诊断分布偏移是否是原因，并触发适当的缓解措施。这可能包括人工审查、回退策略、模型重新培训流程或分阶段部署推出。
- en: In mature ML systems, handling distribution shift becomes a matter of infrastructure,
    observability, and automation, not just modeling technique. Failing to account
    for it risks silent model failure in dynamic, real-world environments—precisely
    where ML systems are expected to deliver the most value.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在成熟的机器学习系统中，处理分布偏移已成为基础设施、可观察性和自动化的一个问题，而不仅仅是建模技术。未能考虑这一点，在动态、现实世界的环境中可能导致模型无声失败——这正是机器学习系统预期提供最大价值的地方。
- en: A summary of common types of distribution shifts, their effects on model performance,
    and potential system-level responses is shown in [Table 16.5](ch022.xhtml#tbl-distribution-shift-summary).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 常见分布偏移类型、其对模型性能的影响以及潜在的系统级响应的总结显示在[表16.5](ch022.xhtml#tbl-distribution-shift-summary)中。
- en: 'Table 16.5: **Distribution Shift Types**: Real-world ML systems encounter various
    forms of distribution shift—including covariate, concept, and prior shift—that
    degrade performance by altering the relationship between inputs and outputs, or
    the prevalence of different outcomes. Understanding these shifts and implementing
    system-level mitigations—such as monitoring, adaptive learning, and robust training—is
    crucial for maintaining reliable performance in dynamic environments.'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.5：**分布偏移类型**：现实世界的机器学习系统会遇到各种形式的分布偏移，包括协变量、概念和先验偏移，这些偏移通过改变输入和输出之间的关系或不同结果的出现频率来降低性能。理解这些偏移并实施系统级缓解措施——如监控、自适应学习和鲁棒训练——对于在动态环境中保持可靠性能至关重要。
- en: '| Type of Shift | Cause or Example | Consequence for Model | System-Level Response
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 偏移类型 | 原因或示例 | 对模型的影响 | 系统级响应 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Covariate Shift | Change in input features (e.g., sensor calibration drift)
    | Model misclassifies new inputs despite consistent labels | Monitor input distributions;
    retrain with updated features |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 协变量偏移 | 输入特征的变化（例如，传感器校准漂移） | 尽管标签一致，模型仍会错误分类新的输入 | 监控输入分布；使用更新特征重新训练 |'
- en: '| Label Shift | Change in label distribution (e.g., new class frequencies in
    usage) | Prediction probabilities become skewed | Track label priors; reweight
    or adapt output calibration |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 标签偏移 | 标签分布的变化（例如，使用中新类别的频率） | 预测概率变得倾斜 | 跟踪标签先验；重新加权或调整输出校准 |'
- en: '| Concept Drift | Evolving relationship between inputs and outputs (e.g. fraud
    tactics) | Model performance degrades over time | Retrain frequently; use continual
    or online learning |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 概念漂移 | 输入和输出之间关系的发展（例如，欺诈策略） | 模型性能随时间下降 | 频繁重新训练；使用持续或在线学习 |'
- en: '| Domain Mismatch | Train on reviews, deploy on tweets | Poor generalization
    due to different vocabularies or styles | Use domain adaptation or fine-tuning
    |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 领域不匹配 | 在评论上训练，在推文中部署 | 由于不同的词汇或风格导致泛化能力差 | 使用领域自适应或微调 |'
- en: '| Contextual Change | New deployment environment (e.g., lighting, user behavior)
    | Performance varies by context | Collect contextual data; monitor conditional
    accuracy |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 上下文变化 | 新的部署环境（例如，照明，用户行为） | 性能随上下文变化 | 收集上下文数据；监控条件准确性 |'
- en: '| Selection Bias | Underrepresentation during training | Biased predictions
    for unseen groups | Validate dataset balance; augment training data |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 选择偏差 | 训练过程中的代表性不足 | 对未见过的群体进行有偏预测 | 验证数据集平衡；增加训练数据 |'
- en: '| Feedback Loops | Model outputs affect future inputs (e.g., recommender systems)
    | Reinforced drift, unpredictable patterns | Monitor feedback effects; consider
    counterfactual logging |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 反馈循环 | 模型输出影响未来输入（例如，推荐系统） | 强化漂移，不可预测的模式 | 监控反馈效果；考虑反事实日志记录 |'
- en: '| Adversarial Shift | Attackers introduce OOD inputs or perturbations | Model
    becomes vulnerable to targeted failures | Use robust training; detect out-of-distribution
    inputs |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 对抗偏移 | 攻击者引入OOD输入或扰动 | 模型变得容易受到针对性故障的影响 | 使用鲁棒训练；检测分布外的输入 |'
- en: System Implications of Distribution Shifts
  id: totrans-512
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布偏移的系统影响
- en: Input Attack Detection and Defense
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入攻击检测和防御
- en: Building on the theoretical understanding of model vulnerabilities, we now examine
    practical defense strategies.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型脆弱性的理论理解的基础上，我们现在考察实际的防御策略。
- en: Adversarial Attack Defenses
  id: totrans-515
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗攻击防御
- en: Having established the mechanisms and impacts of adversarial attacks, we examine
    their detection and defense.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了对抗攻击的机制和影响之后，我们考察了它们的检测和防御。
- en: Detection Techniques
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检测技术
- en: Detecting adversarial examples is the first line of defense against adversarial
    attacks. Several techniques have been proposed to identify and flag suspicious
    inputs that may be adversarial.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 检测对抗样本是抵御对抗攻击的第一道防线。已经提出了几种技术来识别和标记可能为对抗性的可疑输入。
- en: Statistical methods represent one approach to detecting adversarial examples
    by analyzing the distributional properties of input data. These methods compare
    the input data distribution to a reference distribution, such as the training
    data distribution or a known benign distribution. Techniques like the [Kolmogorov-Smirnov](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm)
    ([Berger and Zhou 2014](ch058.xhtml#ref-berger2014kolmogorov)) test[53](#fn53)
    or the [Anderson-Darling](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm)
    test can measure the discrepancy between distributions and flag inputs that deviate
    significantly from the expected distribution.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 统计方法代表了一种通过分析输入数据的分布特性来检测对抗样本的方法。这些方法将输入数据分布与参考分布进行比较，例如训练数据分布或已知的良性分布。像[柯尔莫哥洛夫-斯米尔诺夫测试](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm)
    ([伯格和周 2014](ch058.xhtml#ref-berger2014kolmogorov)) 或 [安德森-达尔林测试](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm)
    这样的技术可以测量分布之间的差异，并标记出与预期分布显著偏离的输入。
- en: Beyond distributional analysis, input transformation methods offer an alternative
    detection strategy. Feature squeezing[54](#fn54) ([Panda, Chakraborty, and Roy
    2019](ch058.xhtml#ref-panda2019discretization)) reduces input space complexity
    through dimensionality reduction or discretization, eliminating the small, imperceptible
    perturbations that adversarial examples typically rely on.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分布分析之外，输入变换方法提供了一种替代的检测策略。特征压缩[54](#fn54) ([Panda, Chakraborty, and Roy 2019](ch058.xhtml#ref-panda2019discretization))
    通过降维或离散化来降低输入空间复杂性，消除了对抗样本通常依赖的微小、难以察觉的扰动。
- en: Model uncertainty estimation provides yet another detection paradigm by quantifying
    the confidence associated with predictions. Since adversarial examples often exploit
    regions of high uncertainty in the model’s decision boundary, inputs with elevated
    uncertainty can be flagged as suspicious. Several approaches exist for uncertainty
    estimation, each with distinct trade-offs between accuracy and computational cost.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不确定性估计通过量化预测的置信度提供了另一种检测范式。由于对抗样本通常利用模型决策边界中的高不确定性区域，因此具有较高不确定性的输入可以被标记为可疑。存在几种不确定性估计方法，每种方法在准确性和计算成本之间都有不同的权衡。
- en: Bayesian neural networks[55](#fn55) provide the most principled uncertainty
    estimates by treating model weights as probability distributions, capturing both
    aleatoric (data inherent) and epistemic (model) uncertainty through approximate
    inference methods. Ensemble methods (detailed further in [Section 16.8.4.1.2](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d))
    achieve uncertainty estimation by combining predictions from multiple independently
    trained models, using prediction variance as an uncertainty measure. While both
    approaches offer robust uncertainty quantification, they incur significant computational
    overhead.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络[55](#fn55) 通过将模型权重视为概率分布，通过近似推理方法捕捉了随机性（数据固有）和认知性（模型）不确定性，提供了最原则性的不确定性估计。集成方法（在[第16.8.4.1.2节](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d)中进一步详细说明）通过结合多个独立训练模型的预测来实现不确定性估计，使用预测方差作为不确定性度量。虽然这两种方法都提供了鲁棒的不确定性量化，但它们都带来了显著的计算开销。
- en: Dropout[56](#fn56), originally designed as a regularization technique to prevent
    overfitting during training ([G. E. Hinton et al. 2012](ch058.xhtml#ref-hinton2012improvingneuralnetworkspreventing)),
    works by randomly deactivating a fraction of neurons during each training iteration,
    forcing the network to avoid over-reliance on specific neurons and improving generalization.
    This mechanism can be repurposed for uncertainty estimation through Monte Carlo
    dropout at inference time, where multiple forward passes with different dropout
    masks approximate the uncertainty distribution. However, this approach provides
    less precise uncertainty estimates since dropout was not specifically designed
    for uncertainty quantification but rather for preventing overfitting through enforced
    redundancy. Hybrid approaches that combine dropout with lightweight ensemble methods
    or Bayesian approximations can balance computational efficiency with estimation
    quality, making uncertainty-based detection more practical for real-world deployment.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout[56](#fn56)，最初作为防止训练过程中过拟合的正则化技术而设计([G. E. Hinton等人 2012](ch058.xhtml#ref-hinton2012improvingneuralnetworkspreventing))，通过在每次训练迭代中随机停用一部分神经元来实现，迫使网络避免过度依赖特定神经元，从而提高泛化能力。这种机制可以通过推理时的蒙特卡洛dropout重新用于不确定性估计，其中多个带有不同dropout掩码的前向传递近似不确定性分布。然而，这种方法提供的不确定性估计不够精确，因为dropout并非专门设计用于不确定性量化，而是通过强制冗余来防止过拟合。结合dropout与轻量级集成方法或贝叶斯近似混合的方法可以在计算效率和估计质量之间取得平衡，使基于不确定性的检测在实际部署中更加实用。
- en: Defense Strategies
  id: totrans-524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 防御策略
- en: Once adversarial examples are detected, various defense strategies can be employed
    to mitigate their impact and improve the robustness of ML models.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检测到对抗性示例，可以采用各种防御策略来减轻其影响并提高机器学习模型的鲁棒性。
- en: Adversarial training is a technique that involves augmenting the training data
    with adversarial examples and retraining the model on this augmented dataset.
    Exposing the model to adversarial examples during training teaches it to classify
    them correctly and becomes more robust to adversarial attacks. [Listing 16.1](ch022.xhtml#lst-adversarial-training)
    demonstrates the core implementation pattern.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练是一种技术，它涉及通过添加对抗性示例来增强训练数据，并在增强的数据集上重新训练模型。在训练期间将模型暴露于对抗性示例教会它正确分类它们，并使其对对抗性攻击更加鲁棒。[列表16.1](ch022.xhtml#lst-adversarial-training)展示了核心实现模式。
- en: Adversarial training provides improved robustness but comes with significant
    computational overhead that must be carefully managed in production systems.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练提供了改进的鲁棒性，但伴随着显著的计算开销，必须在生产系统中谨慎管理。
- en: Training time increases 3-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    due to adversarial example generation during each training step. On-the-fly adversarial
    example generation requires additional forward and backward passes through the
    model, substantially increasing computational requirements. Memory requirements
    increase 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    for storing both clean and adversarial examples, along with gradients computed
    during attack generation. Specialized infrastructure may be needed for efficient
    adversarial example generation, particularly when using iterative attacks like
    PGD that require multiple optimization steps.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在每次训练步骤中生成对抗性示例，训练时间会增加3-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>。即时生成对抗性示例需要通过模型进行额外的正向和反向传递，这大大增加了计算需求。存储干净和对抗性示例以及攻击生成期间计算的梯度，内存需求增加2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>。在生成迭代攻击（如PGD）时，可能需要专门的硬件来高效地生成对抗性示例，尤其是当需要多个优化步骤时。
- en: Robust models typically sacrifice 2-8% clean accuracy for improved adversarial
    robustness, representing a fundamental trade-off in the robust optimization objective.
    Inference time may increase if ensemble methods or uncertainty estimation techniques
    are integrated with adversarial training. Model size often increases with robustness-enhancing
    architectural modifications, such as wider networks or additional normalization
    layers that improve gradient stability.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒模型通常牺牲2-8%的干净准确率以换取改进的对抗鲁棒性，这代表了鲁棒优化目标中的一个基本权衡。如果将集成方法或不确定性估计技术集成到对抗性训练中，推理时间可能会增加。模型大小通常随着增强鲁棒性的架构修改而增加，例如更宽的网络或额外的归一化层，这些修改可以改善梯度稳定性。
- en: Hyperparameter tuning becomes significantly more complex when balancing robustness
    and performance objectives. Validation procedures must evaluate both clean and
    adversarial performance using multiple attack methods to ensure comprehensive
    robustness assessment. Deployment infrastructure must support the additional computational
    requirements for adversarial training, including GPU memory for gradient computation
    and storage for adversarial example caches.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 当平衡鲁棒性和性能目标时，超参数调整变得显著更加复杂。验证程序必须使用多种攻击方法评估干净和对抗性性能，以确保全面的鲁棒性评估。部署基础设施必须支持对抗性训练的额外计算需求，包括用于梯度计算的GPU内存和存储对抗性示例缓存的存储空间。
- en: 'Listing 16.1: **Adversarial Training Implementation**: Practical adversarial
    training using FGSM to generate adversarial examples during training, mixing clean
    and perturbed data to improve model robustness against gradient-based attacks.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.1：**对抗性训练实现**：使用FGSM在训练期间生成对抗性示例的实用对抗性训练，通过混合干净和扰动数据来提高模型对基于梯度的攻击的鲁棒性。
- en: '[PRE0]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The implementation in [Listing 16.1](ch022.xhtml#lst-adversarial-training) generates
    adversarial examples on-the-fly during training by computing gradients with respect
    to input data (line 2190), applying the sign function to extract perturbation
    direction (line 2196), and mixing the resulting adversarial examples with clean
    training data (lines 2199-2200). The `torch.clamp()` operation ensures pixel values
    remain valid, while the final concatenation doubles the effective batch size by
    combining clean and adversarial examples. This approach requires careful tuning
    of the perturbation budget <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    and typically increases training time by 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    compared to standard training ([Shafahi et al. 2019](ch058.xhtml#ref-shafahi2019adversarial)).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表16.1](ch022.xhtml#lst-adversarial-training)中的实现通过计算输入数据的梯度（第2190行），应用符号函数以提取扰动方向（第2196行），并将生成的对抗性示例与干净的训练数据混合（第2199-2200行）来在训练过程中即时生成对抗性示例。`torch.clamp()`操作确保像素值保持有效，而最终的连接将干净和对抗性示例结合在一起，有效地将批处理大小加倍。这种方法需要仔细调整扰动预算
    <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>，通常比标准训练（[Shafahi
    等人 2019](ch058.xhtml#ref-shafahi2019adversarial)）增加2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的训练时间。'
- en: Production deployment patterns, MLOps pipeline integration, and monitoring strategies
    for robust ML systems are covered in detail in [Chapter 13](ch019.xhtml#sec-ml-operations),
    while distributed robustness coordination and fault tolerance at scale are addressed
    in distributed training contexts within [Chapter 8](ch014.xhtml#sec-ai-training).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章（[ch019.xhtml#sec-ml-operations](ch019.xhtml#sec-ml-operations)）详细介绍了生产部署模式、MLOps管道集成和鲁棒机器学习系统的监控策略，而分布式鲁棒性协调和大规模容错则在第8章（[ch014.xhtml#sec-ai-training](ch014.xhtml#sec-ai-training)）中分布式训练的上下文中得到解决。
- en: Defensive distillation ([Papernot, McDaniel, Wu, et al. 2016](ch058.xhtml#ref-papernot2016distillation))
    is a technique that trains a second model (the student model) to mimic the behavior
    of the original model (the teacher model). The student model is trained on the
    soft labels produced by the teacher model, which are less sensitive to small perturbations.
    Using the student model for inference can reduce the impact of adversarial perturbations,
    as the student model learns to generalize better and is less sensitive to adversarial
    noise.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 防御蒸馏（[Papernot, McDaniel, Wu, 等人 2016](ch058.xhtml#ref-papernot2016distillation)）是一种技术，它训练第二个模型（学生模型）来模仿原始模型（教师模型）的行为。学生模型在教师模型产生的软标签上进行训练，这些标签对小的扰动不太敏感。使用学生模型进行推理可以减少对抗性扰动的
    影响，因为学生模型学会了更好地泛化，并且对对抗性噪声不太敏感。
- en: Input preprocessing and transformation techniques try to remove or mitigate
    the effect of adversarial perturbations before feeding the input to the ML model.
    These techniques include image denoising, JPEG compression, random resizing, padding,
    or applying random transformations to the input data. By reducing the impact of
    adversarial perturbations, these preprocessing steps can help improve the model’s
    robustness to adversarial attacks.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 输入预处理和转换技术试图在将输入馈送到机器学习模型之前消除或减轻对抗性扰动的影响。这些技术包括图像去噪、JPEG压缩、随机调整大小、填充，或对输入数据应用随机转换。通过减少对抗性扰动的影响，这些预处理步骤可以帮助提高模型对对抗性攻击的鲁棒性。
- en: Ensemble methods combine multiple models to make more robust predictions. The
    ensemble can reduce the impact of adversarial attacks by using a diverse set of
    models with different architectures, training data, or hyperparameters. Adversarial
    examples that fool one model may not fool others in the ensemble, leading to more
    reliable and robust predictions. Model diversification techniques, such as using
    different preprocessing techniques or feature representations for each model in
    the ensemble, can further enhance the robustness.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法通过结合多个模型来做出更鲁棒的预测。集成可以通过使用具有不同架构、训练数据或超参数的多样化模型集来减少对抗性攻击的影响。欺骗一个模型的对抗性示例可能不会欺骗集成中的其他模型，从而产生更可靠和鲁棒的预测。通过为集成中的每个模型使用不同的预处理技术或特征表示等模型多样化技术，可以进一步增强鲁棒性。
- en: Evaluation and Testing
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估和测试
- en: Conduct thorough evaluation and testing to assess the effectiveness of adversarial
    defense techniques and measure the robustness of ML models.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 进行彻底的评估和测试，以评估对抗性防御技术的有效性并衡量机器学习模型的鲁棒性。
- en: Adversarial robustness metrics quantify the model’s resilience to adversarial
    attacks. These metrics can include the model’s accuracy on adversarial examples,
    the average distortion required to fool the model, or the model’s performance
    under different attack strengths. By comparing these metrics across different
    models or defense techniques, practitioners can assess and compare their robustness
    levels.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性鲁棒性指标量化了模型对对抗性攻击的韧性。这些指标可以包括模型在对抗性示例上的准确度、欺骗模型所需的平均扭曲量，或模型在不同攻击强度下的性能。通过比较不同模型或防御技术之间的这些指标，实践者可以评估和比较它们的鲁棒性水平。
- en: Standardized adversarial attack benchmarks and datasets provide a common ground
    for evaluating and comparing the robustness of ML models. These benchmarks include
    datasets with pre-generated adversarial examples and tools and frameworks for
    generating adversarial attacks. Examples of popular adversarial attack benchmarks
    include the [MNIST-C](https://github.com/google-research/mnist-c), [CIFAR-10-C](https://paperswithcode.com/dataset/cifar-10c),
    and ImageNet-C ([Hendrycks and Dietterich 2019](ch058.xhtml#ref-hendrycks2019benchmarking))
    datasets, which contain corrupted or perturbed versions of the original datasets.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的对抗性攻击基准和数据集为评估和比较机器学习模型的鲁棒性提供了一个共同基础。这些基准包括包含预生成对抗性示例的数据集和用于生成对抗性攻击的工具和框架。流行的对抗性攻击基准示例包括[MNIST-C](https://github.com/google-research/mnist-c)、[CIFAR-10-C](https://paperswithcode.com/dataset/cifar-10c)和ImageNet-C
    ([Hendrycks and Dietterich 2019](ch058.xhtml#ref-hendrycks2019benchmarking))数据集，它们包含原始数据集的损坏或扰动版本。
- en: Practitioners can develop more robust systems by leveraging the detection techniques
    and defense strategies outlined in this section. Adversarial robustness remains
    an ongoing research area requiring multi-layered approaches that combine multiple
    defense mechanisms and regular testing against evolving threats.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 实践者可以通过利用本节中概述的检测技术和防御策略来开发更鲁棒的系统。对抗性鲁棒性仍然是一个需要多层方法的研究领域，这些方法结合了多种防御机制，并定期针对不断发展的威胁进行测试。
- en: Data Poisoning Defenses
  id: totrans-543
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据投毒防御
- en: Data poisoning attacks aim to corrupt training data used to build ML models,
    targeting the data collection and preprocessing stages detailed in [Chapter 6](ch012.xhtml#sec-data-engineering),
    undermining their integrity. As illustrated in [Figure 16.31](ch022.xhtml#fig-adversarial-attack-injection),
    these attacks can manipulate or pollute the training data in ways that cause models
    to learn incorrect patterns, leading to erroneous predictions or undesirable behaviors
    when deployed. Given the foundational role of training data in ML system performance,
    detecting and mitigating data poisoning is critical for maintaining model trustworthiness
    and reliability.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 数据投毒攻击旨在破坏用于构建机器学习模型的训练数据，针对[第6章](ch012.xhtml#sec-data-engineering)中详细说明的数据收集和预处理阶段，破坏其完整性。如图[图16.31](ch022.xhtml#fig-adversarial-attack-injection)所示，这些攻击可以通过操纵或污染训练数据的方式，导致模型学习到错误的模式，当部署时产生错误的预测或不良行为。鉴于训练数据在机器学习系统性能中的基础作用，检测和减轻数据投毒对于保持模型的可信度和可靠性至关重要。
- en: '![](../media/file277.svg)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file277.svg)'
- en: 'Figure 16.31: **Data Poisoning Attack**: Adversaries inject malicious data
    into the training set to manipulate model behavior, potentially causing misclassification
    or performance degradation during deployment. This attack emphasizes the vulnerability
    of machine learning systems to compromised data integrity and the need for robust
    data validation techniques. *Source: [li](HTTPS://www.mdpi.com/2227-7390/12/2/247)*'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.31：**数据中毒攻击**：攻击者将恶意数据注入训练集以操纵模型行为，可能导致部署期间的误分类或性能下降。这种攻击强调了机器学习系统对数据完整性的脆弱性以及需要强大的数据验证技术。*来源：[li](HTTPS://www.mdpi.com/2227-7390/12/2/247)*
- en: Anomaly Detection Techniques
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 异常检测技术
- en: Statistical outlier detection methods identify data points that deviate significantly
    from most data. These methods assume that poisoned data instances are likely to
    be statistical outliers. Techniques such as the [Z-score method](https://ubalt.pressbooks.pub/mathstatsguides/chapter/z-score-basics/),
    [Tukey’s method](https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm),
    or the [Mahalanobis distance](https://www.statisticshowto.com/mahalanobis-distance/)
    can be used to measure the deviation of each data point from the central tendency
    of the dataset. Data points that exceed a predefined threshold are flagged as
    potential outliers and considered suspicious for data poisoning.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 统计异常检测方法识别出与大多数数据显著偏离的数据点。这些方法假设中毒数据实例很可能是统计异常。可以使用诸如[Z分数方法](https://ubalt.pressbooks.pub/mathstatsguides/chapter/z-score-basics/)、[Tukey的方法](https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm)或[马氏距离](https://www.statisticshowto.com/mahalanobis-distance/)等技术来衡量每个数据点与数据集中心趋势的偏差。超过预定义阈值的数据点被标记为潜在的异常值，并被认为是数据中毒的可疑候选。
- en: Clustering-based methods group similar data points together based on their features
    or attributes. The assumption is that poisoned data instances may form distinct
    clusters or lie far away from the normal data clusters. By applying clustering
    algorithms like [K-means](https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch12.html),
    [DBSCAN](https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/50efb27d-abbe-4855-ad81-a5357050161f.xhtml),
    or [hierarchical clustering](https://www.oreilly.com/library/view/cluster-analysis-5th/9780470978443/chapter04.html),
    anomalous clusters or data points that do not belong to any cluster can be identified.
    These anomalous instances are then treated as potentially poisoned data.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类的技术根据数据点的特征或属性将相似的数据点分组在一起。假设中毒数据实例可能形成独特的聚类或远离正常数据聚类。通过应用诸如[K-means](https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch12.html)、[DBSCAN](https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/50efb27d-abbe-4855-ad81-a5357050161f.xhtml)或[层次聚类](https://www.oreilly.com/library/view/cluster-analysis-5th/9780470978443/chapter04.html)等聚类算法，可以识别出异常聚类或不属于任何聚类的数据点。然后，将这些异常实例视为可能中毒的数据。
- en: Autoencoders[57](#fn57) are neural networks trained to reconstruct the input
    data from a compressed representation, as shown in [Figure 16.32](ch022.xhtml#fig-autoencoder).
    They can be used for anomaly detection by learning the normal patterns in the
    data and identifying instances that deviate from them. During training, the autoencoder
    is trained on clean, unpoisoned data. At inference time, the reconstruction error
    for each data point is computed. Data points with high reconstruction errors are
    considered abnormal and potentially poisoned, as they do not conform to the learned
    normal patterns.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器[57](#fn57)是一种经过训练的神经网络，用于从压缩表示中重建输入数据，如图16.32所示。它们可以通过学习数据中的正常模式并识别偏离这些模式的实例来用于异常检测。在训练期间，自编码器在干净、未中毒的数据上训练。在推理时间，计算每个数据点的重建误差。具有高重建误差的数据点被认为是异常的，并且可能是中毒的，因为它们不符合学习到的正常模式。
- en: '![](../media/file278.svg)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file278.svg)'
- en: 'Figure 16.32: **Autoencoder Architecture**: Autoencoders learn compressed data
    representations by minimizing reconstruction error, enabling anomaly detection
    by identifying inputs with high reconstruction loss. During training on normal
    data, the network learns efficient encoding and decoding, making it sensitive
    to deviations indicative of potential poisoning attacks. *Source: [dertat](HTTPS://medium.com/towards-data-science/applied-deep-learning-part-3-autoencoders-1c083af4d798)*'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.32：**自动编码器架构**：自动编码器通过最小化重建误差来学习压缩数据表示，通过识别具有高重建损失输入来启用异常检测。在正常数据上的训练过程中，网络学习有效的编码和解码，使其对潜在的攻击偏差指示敏感。*来源：[dertat](HTTPS://medium.com/towards-data-science/applied-deep-learning-part-3-autoencoders-1c083af4d798)*
- en: Sanitization and Preprocessing
  id: totrans-553
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清洗和预处理
- en: Data poisoning can be avoided by cleaning data, which involves identifying and
    removing or correcting noisy, incomplete, or inconsistent data points. Techniques
    such as data deduplication, missing value imputation, and outlier removal can
    be applied to improve the quality of the training data. By eliminating or filtering
    out suspicious or anomalous data points, the impact of poisoned instances can
    be reduced.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 通过清洗数据可以避免数据中毒，这涉及识别和删除或纠正噪声、不完整或不一致的数据点。可以应用数据去重、缺失值插补和异常值移除等技术来提高训练数据的质量。通过消除或过滤掉可疑或异常的数据点，可以减少中毒实例的影响。
- en: Data validation involves verifying the integrity and consistency of the training
    data. This can include checking for data type consistency, range validation, and
    cross-field dependencies. By defining and enforcing data validation rules, anomalous
    or inconsistent data points indicative of data poisoning can be identified and
    flagged for further investigation.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证涉及验证训练数据的完整性和一致性。这可以包括检查数据类型的一致性、范围验证以及跨字段依赖性。通过定义和执行数据验证规则，可以识别并标记出指示数据中毒的异常或不一致的数据点，以便进行进一步调查。
- en: Data provenance and lineage tracking involve maintaining a record of data’s
    origin, transformations, and movements throughout the ML pipeline. By documenting
    the data sources, preprocessing steps, and any modifications made to the data,
    practitioners can trace anomalies or suspicious patterns back to their origin.
    This helps identify potential points of data poisoning and facilitates the investigation
    and mitigation process.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 数据溯源和血缘追踪涉及在整个机器学习管道中维护数据的来源、转换和移动记录。通过记录数据来源、预处理步骤以及对数据进行任何修改，从业者可以将异常或可疑模式追溯到其起源。这有助于识别潜在的数据中毒点，并促进调查和缓解过程。
- en: Robust Training
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 鲁棒训练
- en: Robust optimization techniques can be used to modify the training objective
    to minimize the impact of outliers or poisoned instances. This can be achieved
    by using robust loss functions less sensitive to extreme values, such as the Huber
    loss or the modified Huber loss[58](#fn58). Regularization techniques[59](#fn59),
    such as [L1 or L2 regularization](https://medium.com/towards-data-science/l1-and-l2-regularization-methods-ce25e7fc831c),
    can also help in reducing the model’s sensitivity to poisoned data by constraining
    the model’s complexity and preventing overfitting.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用鲁棒优化技术来修改训练目标，以最小化异常值或中毒实例的影响。这可以通过使用对极端值不太敏感的鲁棒损失函数来实现，例如Huber损失或修改后的Huber损失[58](#fn58)。正则化技术[59](#fn59)，如[L1或L2正则化](https://medium.com/towards-data-science/l1-and-l2-regularization-methods-ce25e7fc831c)，也可以通过限制模型的复杂性和防止过拟合来帮助减少模型对中毒数据的敏感性。
- en: Robust loss functions are designed to be less sensitive to outliers or noisy
    data points. Examples include the modified [Huber loss](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html),
    the Tukey loss ([Beaton and Tukey 1974](ch058.xhtml#ref-beaton1974fitting)), and
    the trimmed mean loss. These loss functions down-weight or ignore the contribution
    of abnormal instances during training, reducing their impact on the model’s learning
    process. Robust objective functions, such as the minimax[60](#fn60) or distributionally
    robust objective, aim to optimize the model’s performance under worst-case scenarios
    or in the presence of adversarial perturbations.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒损失函数旨在对异常值或噪声数据点不太敏感。例如，包括修改后的[Huber损失](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html)、Tukey损失([Beaton和Tukey
    1974](ch058.xhtml#ref-beaton1974fitting))和修剪均值损失。这些损失函数在训练期间会降低或忽略异常实例的贡献，从而减少它们对模型学习过程的影响。鲁棒目标函数，如最小-最大[60](#fn60)或分布鲁棒目标，旨在在最坏情况或存在对抗性扰动的情况下优化模型性能。
- en: Data augmentation techniques involve generating additional training examples
    by applying random transformations or perturbations to the existing data [Figure 16.33](ch022.xhtml#fig-data-augmentation).
    This helps in increasing the diversity and robustness of the training dataset.
    By introducing controlled variations in the data, the model becomes less sensitive
    to specific patterns or artifacts that may be present in poisoned instances. Randomization
    techniques, such as random subsampling or bootstrap aggregating, can also help
    reduce the impact of poisoned data by training multiple models on different subsets
    of the data and combining their predictions.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强技术涉及通过应用随机变换或扰动到现有数据来生成额外的训练示例 [图16.33](ch022.xhtml#fig-data-augmentation)。这有助于增加训练数据集的多样性和鲁棒性。通过在数据中引入可控的变异，模型对可能存在于受污染实例中的特定模式或伪影变得不那么敏感。随机化技术，如随机子采样或自助聚合，还可以通过在不同的数据子集上训练多个模型并合并它们的预测来帮助减少受污染数据的影响。
- en: '![](../media/file279.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file279.png)'
- en: 'Figure 16.33: **Data Augmentation Techniques**: Applying transformations like
    horizontal flips, rotations, and cropping expands training datasets, improving
    model robustness to variations in input data and reducing overfitting. These techniques
    generate new training examples without requiring additional labeled data, effectively
    increasing dataset diversity and enhancing generalization performance.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.33：**数据增强技术**：应用变换如水平翻转、旋转和裁剪可以扩展训练数据集，提高模型对输入数据变化的鲁棒性，并减少过拟合。这些技术生成新的训练示例，而无需额外的标记数据，有效地增加了数据集的多样性和提高了泛化性能。
- en: Secure Data Sourcing
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安全数据来源
- en: Implementing the best data collection and curation practices can help mitigate
    the risk of data poisoning. This includes establishing clear data collection protocols,
    verifying the authenticity and reliability of data sources, and conducting regular
    data quality assessments. Sourcing data from trusted and reputable providers and
    following secure data handling practices can reduce the likelihood of introducing
    poisoned data into the training pipeline.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 实施最佳的数据收集和整理实践可以帮助减轻数据中毒的风险。这包括建立明确的数据收集协议，验证数据来源的真实性和可靠性，以及定期进行数据质量评估。从受信任和声誉良好的提供商处获取数据，并遵循安全的数据处理实践可以降低将受污染数据引入训练管道的可能性。
- en: Strong data governance and access control mechanisms are essential to prevent
    unauthorized modifications or tampering with the training data. This involves
    defining clear roles and responsibilities for data access, implementing access
    control policies based on the principle of least privilege,[61](#fn61) and monitoring
    and logging data access activities. By restricting access to the training data
    and maintaining an audit trail, potential data poisoning attempts can be detected
    and investigated.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 强有力的数据治理和访问控制机制对于防止未经授权修改或篡改训练数据至关重要。这包括明确数据访问的职责和角色，基于最小权限原则[61](#fn61) 实施访问控制策略，以及监控和记录数据访问活动。通过限制对训练数据的访问并保持审计跟踪，可以检测和调查潜在的数据中毒尝试。
- en: Detecting and mitigating data poisoning attacks requires a multifaceted approach
    that combines anomaly detection, data sanitization,[62](#fn62) robust training
    techniques, and secure data sourcing practices. Data poisoning remains an active
    research area requiring proactive and adaptive approaches to data security.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和缓解数据中毒攻击需要一种多方面的方法，结合异常检测、数据净化、[62](#fn62) 鲁棒的训练技术和安全的数据来源实践。数据中毒仍然是一个活跃的研究领域，需要积极和适应性的数据安全方法。
- en: Distribution Shift Adaptation
  id: totrans-567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布式偏移适应
- en: Distribution shifts pose ongoing challenges for deployed machine learning systems,
    requiring systematic approaches for both detection and mitigation. This subsection
    focuses on practical techniques for identifying when shifts occur and strategies
    for maintaining system performance despite these changes. We explore statistical
    methods for shift detection, algorithmic approaches for adaptation, and implementation
    considerations for production systems.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式偏移对部署的机器学习系统构成了持续的挑战，需要系统性的方法来检测和缓解。本节重点介绍识别偏移何时发生的技术和保持系统性能的策略。我们探讨了偏移检测的统计方法、适应的算法方法以及生产系统的实施考虑。
- en: Detection and Mitigation
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检测与缓解
- en: Recall that distribution shifts occur when the data distribution encountered
    by an ML model during deployment differs from the distribution it was trained
    on. These shifts can significantly impact the model’s performance and generalization
    ability, leading to suboptimal or incorrect predictions. Detecting and mitigating
    distribution shifts is crucial to ensure the robustness and reliability of ML
    systems in real-world scenarios.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当机器学习模型在部署期间遇到的数据分布与其训练时的分布不同，就会发生分布偏移。这些偏移可能会显著影响模型的表现和泛化能力，导致次优或错误的预测。检测和缓解分布偏移对于确保机器学习系统在实际场景中的鲁棒性和可靠性至关重要。
- en: Detection Techniques
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检测技术
- en: 'Statistical tests can be used to compare the distributions of the training
    and test data to identify significant differences. [Listing 16.2](ch022.xhtml#lst-distribution-shift)
    demonstrates a practical implementation for monitoring distribution shift in production:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 统计测试可以用来比较训练数据和测试数据的分布，以识别显著差异。[列表16.2](ch022.xhtml#lst-distribution-shift)
    展示了在生产环境中监控分布变化的实际实现：
- en: 'Listing 16.2: **Distribution Shift Detection**: Core statistical methods for
    monitoring data distribution changes in production, combining Kolmogorov-Smirnov
    tests for individual features with domain classifier approaches to detect when
    incoming data differs significantly from training distributions.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.2：**分布偏移检测**：监控生产中数据分布变化的核心理统计方法，结合单个特征的柯尔莫哥洛夫-斯米尔诺夫测试与领域分类器方法，以检测传入数据与训练分布显著不同的情况。
- en: '[PRE1]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Techniques such as the Kolmogorov-Smirnov test or the Anderson-Darling test
    measure the discrepancy between two distributions and provide a quantitative assessment
    of the presence of distribution shift. Applying these tests to the input features
    or the model’s predictions enables practitioners to detect statistically significant
    differences between the training and test distributions.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 像柯尔莫哥洛夫-斯米尔诺夫测试或安德森-达尔林测试这样的技术测量两个分布之间的差异，并提供了对分布变化的定量评估。将这些测试应用于输入特征或模型的预测，使从业者能够检测训练和测试分布之间的统计显著差异。
- en: Divergence metrics quantify the dissimilarity between two probability distributions.
    Commonly used divergence metrics include the [Kullback-Leibler (KL) divergence](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254)
    and the [Jensen-Shannon (JS) divergence](https://medium.com/towards-data-science/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6).
    By calculating the divergence between the training and test data distributions,
    practitioners can assess the extent of the distribution shift. High divergence
    values indicate a significant difference between the distributions, suggesting
    the presence of a distribution shift.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 散度度量量化了两个概率分布之间的差异。常用的散度度量包括 [Kullback-Leibler (KL) 散度](https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254)
    和 [Jensen-Shannon (JS) 散度](https://medium.com/towards-data-science/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6)。通过计算训练和测试数据分布之间的散度，从业者可以评估分布偏移的程度。高散度值表明分布之间存在显著差异，表明存在分布偏移。
- en: Uncertainty quantification techniques, such as Bayesian neural networks[63](#fn63)
    or ensemble methods[64](#fn64), can estimate the uncertainty associated with the
    model’s predictions. When a model is applied to data from a different distribution,
    its predictions may have higher uncertainty. By monitoring the uncertainty levels,
    practitioners can detect distribution shifts. If the uncertainty consistently
    exceeds a predetermined threshold for test samples, it suggests that the model
    is operating outside its trained distribution.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性量化技术，如贝叶斯神经网络[63](#fn63) 或集成方法[64](#fn64)，可以估计模型预测的不确定性。当模型应用于来自不同分布的数据时，其预测可能具有更高的不确定性。通过监控不确定性水平，从业者可以检测分布偏移。如果测试样本的不确定性持续超过预定的阈值，这表明模型正在其训练分布之外运行。
- en: In addition, domain classifiers are trained to distinguish between different
    domains or distributions. Practitioners can detect distribution shifts by training
    a classifier to differentiate between the training and test domains. If the domain
    classifier achieves high accuracy in distinguishing between the two domains, it
    indicates a significant difference in the underlying distributions. The performance
    of the domain classifier serves as a measure of the distribution shift.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，领域分类器被训练以区分不同的领域或分布。从业者可以通过训练一个分类器来区分训练域和测试域来检测分布变化。如果领域分类器在区分两个领域时达到高精度，这表明底层分布存在显著差异。领域分类器的性能作为分布变化的衡量标准。
- en: Mitigation Techniques
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缓解技术
- en: Transfer learning leverages knowledge gained from one domain to improve performance
    in another, as shown in [Figure 16.34](ch022.xhtml#fig-transfer-learning). By
    using pre-trained models or transferring learned features from a source domain
    to a target domain, transfer learning can help mitigate the impact of distribution
    shifts. The pre-trained model can be fine-tuned on a small amount of labeled data
    from the target domain, allowing it to adapt to the new distribution. Transfer
    learning is particularly effective when the source and target domains share similar
    characteristics or when labeled data in the target domain is scarce.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习利用一个领域获得的知识来提高另一个领域的性能，如图 16.34 所示。通过使用预训练模型或将源领域学习到的特征转移到目标领域，迁移学习可以帮助缓解分布变化的影响。预训练模型可以在来自目标域的一小部分标记数据上进行微调，使其能够适应新的分布。当源领域和目标领域具有相似特征或目标域的标记数据稀缺时，迁移学习特别有效。
- en: '![](../media/file280.svg)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file280.svg)'
- en: 'Figure 16.34: **Knowledge Transfer**: Pre-training on large datasets enables
    models to learn generalizable features, which can then be fine-tuned for specific
    target tasks with limited labeled data. This approach mitigates data scarcity
    and accelerates learning in new domains by leveraging previously acquired knowledge.
    *Source: [bhavsar](HTTPS://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f)*'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.34：**知识迁移**：在大数据集上预训练使模型能够学习可泛化的特征，然后可以通过有限的标记数据对特定目标任务进行微调。这种方法通过利用先前获得的知识来缓解数据稀缺问题并加速新领域的学习。*来源：[bhavsar](HTTPS://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f)*
- en: Continual learning, also known as lifelong learning, enables ML models to learn
    continuously from new data distributions while retaining knowledge from previous
    distributions. Techniques such as elastic weight consolidation (EWC) ([Kirkpatrick
    et al. 2017](ch058.xhtml#ref-kirkpatrick2017overcoming)) or gradient episodic
    memory (GEM) ([Lopez-Paz and Ranzato 2017](ch058.xhtml#ref-lopez2017gradient))
    allow models to adapt to evolving data distributions over time. These techniques
    aim to balance the plasticity of the model (ability to learn from new data) with
    the stability of the model (retaining previously learned knowledge). By incrementally
    updating the model with new data and mitigating catastrophic forgetting, continual
    learning helps models stay robust to distribution shifts.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习，也称为终身学习，使机器学习模型能够从新的数据分布中持续学习，同时保留先前分布中的知识。例如，弹性权重巩固（EWC）([Kirkpatrick
    等人 2017](ch058.xhtml#ref-kirkpatrick2017overcoming)) 或梯度片段记忆（GEM）([Lopez-Paz 和
    Ranzato 2017](ch058.xhtml#ref-lopez2017gradient)) 等技术允许模型随着时间的推移适应不断变化的数据分布。这些技术旨在平衡模型的可塑性（从新数据中学习的能力）与模型的稳定性（保留先前学习到的知识）。通过增量更新模型以包含新数据并减轻灾难性遗忘，持续学习有助于模型对分布变化保持鲁棒性。
- en: Data augmentation techniques, such as those we have seen previously, involve
    applying transformations or perturbations to the existing training data to increase
    its diversity and improve the model’s robustness to distribution shifts. By introducing
    variations in the data, such as rotations, translations, scaling, or adding noise,
    data augmentation helps the model learn invariant features and generalize better
    to unseen distributions. Data augmentation can be performed during training and
    inference to improve the model’s ability to handle distribution shifts.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强技术，如我们之前所见，涉及对现有训练数据进行变换或扰动，以增加其多样性并提高模型对分布变化的鲁棒性。通过引入数据中的变化，如旋转、平移、缩放或添加噪声，数据增强有助于模型学习不变特征并更好地泛化到未见过的分布。数据增强可以在训练和推理期间执行，以提高模型处理分布变化的能力。
- en: Ensemble methods, as described in [Section 16.8.4.1.2](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d)
    for adversarial defense, also provide robustness against distribution shifts.
    When presented with a shifted distribution, the ensemble can leverage the strengths
    of individual models to make more accurate and stable predictions.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 如第16.8.4.1.2节[16.8.4.1.2](ch022.xhtml#sec-robust-ai-defense-strategies-cb2d)中所述的集成方法，在对抗防御中也提供了对分布偏移的鲁棒性。当面对偏移的分布时，集成可以利用单个模型的优点来做出更准确和稳定的预测。
- en: Regularly updating models with new data from the target distribution is crucial
    to mitigate the impact of distribution shifts. As the data distribution evolves,
    models should be retrained or fine-tuned on the latest available data to adapt
    to the changing patterns, leveraging continuous learning approaches detailed in
    [Chapter 14](ch020.xhtml#sec-ondevice-learning). Monitoring model performance
    and data characteristics can help detect when an update is necessary. By keeping
    the models up to date, practitioners can ensure they remain relevant and accurate
    in the face of distribution shifts.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 定期用目标分布的新数据更新模型对于减轻分布偏移的影响至关重要。随着数据分布的发展，模型应该使用最新的可用数据进行重新训练或微调，以适应变化的模式，利用第14章中详细介绍的持续学习方法。监控模型性能和数据特征可以帮助检测何时需要更新。通过保持模型更新，从业者可以确保他们在面对分布偏移时保持相关性和准确性。
- en: Evaluating models using robust metrics less sensitive to distribution shifts
    can provide a more reliable assessment of model performance. Metrics such as the
    area under the precision-recall curve (AUPRC) or the F1 score[65](#fn65) are more
    robust to class imbalance and can better capture the model’s performance across
    different distributions. Using domain-specific evaluation metrics that align with
    the desired outcomes in the target domain can provide a more meaningful measure
    of the model’s effectiveness.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对分布偏移不太敏感的鲁棒性度量来评估模型可以提供对模型性能的更可靠评估。例如，精确召回曲线下的面积（AUPRC）或F1分数[65](#fn65)对类别不平衡更鲁棒，并能更好地捕捉模型在不同分布上的性能。使用与目标域中期望结果一致的领域特定评估度量可以提供对模型有效性的更有意义的衡量。
- en: Detecting and mitigating distribution shifts is an ongoing process that requires
    continuous monitoring, adaptation, and improvement. By employing the detection
    and mitigation techniques described in this section, practitioners can proactively
    address distribution shifts in real-world deployments.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和减轻分布偏移是一个持续的过程，需要持续的监控、适应和改进。通过采用本节中描述的检测和减轻技术，从业者可以主动解决现实部署中的分布偏移问题。
- en: Self-Supervised Learning for Robustness
  id: totrans-589
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自监督学习以鲁棒性为目标
- en: Self-supervised learning (SSL) approaches may provide a path toward more robust
    AI systems by learning from data structure rather than memorizing input-output
    mappings. Unlike supervised learning that relies on labeled examples, SSL methods
    discover representations by solving pretext tasks that require understanding underlying
    data patterns and relationships.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习（SSL）方法可能通过从数据结构中学习而不是记忆输入输出映射来提供通往更鲁棒人工智能系统的途径。与依赖于标记示例的监督学习不同，SSL方法通过解决需要理解底层数据模式和关系的预训练任务来发现表示。
- en: Self-supervised approaches potentially address several core limitations that
    contribute to neural network brittleness. SSL methods learn representations from
    environmental regularities and data structure, capturing invariant features that
    remain consistent across different conditions. Contrastive learning techniques
    encourage representations that capture invariant features across different views
    of the same data, promoting robustness to transformations and perturbations. Masked
    language modeling and similar techniques in vision learn to predict based on context
    rather than surface patterns, potentially developing more generalizable internal
    representations.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督方法可能解决导致神经网络脆弱性的几个核心限制。SSL方法从环境规律和数据结构中学习表示，捕捉在不同条件下保持一致的不变特征。对比学习技术鼓励捕捉不同视角下相同数据不变特征的表示，从而提高对变换和扰动的鲁棒性。掩码语言建模和视觉中的类似技术学习根据上下文而不是表面模式进行预测，可能发展出更具普遍性的内部表示。
- en: Self-supervised representations often demonstrate superior transfer capabilities
    compared to supervised learning representations, suggesting they capture more
    essential aspects of data structure. Learning from data structure rather than
    labels may be inherently more robust because it relies on consistent patterns
    present across domains and conditions. SSL approaches can leverage larger amounts
    of unlabeled data, potentially improving generalization by exposing models to
    broader ranges of natural variation. This exposure to diverse unlabeled data may
    help models develop representations that are more resilient to the distribution
    shifts commonly encountered in deployment.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习表示相比，自监督表示通常显示出更优越的迁移能力，这表明它们捕捉到了数据结构中更本质的方面。从数据结构而不是标签中学习可能本质上更鲁棒，因为它依赖于跨域和条件下存在的持续模式。自监督方法可以利用更多的未标记数据，通过使模型接触到更广泛的自然变异范围来提高泛化能力。这种对多样化未标记数据的接触可能有助于模型发展出对部署中常见的分布变化更具弹性的表示。
- en: Several strategies can incorporate self-supervised learning into robust system
    design. Pre-training models using self-supervised objectives before fine-tuning
    for specific tasks provides a robust foundation that may transfer better across
    domains. Multi-task approaches that combine self-supervised and supervised objectives
    during training can balance representation learning with task performance. SSL-learned
    representations can serve as the foundation for subsequent robust fine-tuning
    approaches, potentially requiring fewer labeled examples to achieve robustness.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 几种策略可以将自监督学习融入鲁棒系统设计中。在针对特定任务微调之前，使用自监督目标进行预训练模型提供了一种鲁棒的基础，这可能在跨域中更好地迁移。在训练期间结合自监督和监督目标的多元任务方法可以在表示学习和任务性能之间取得平衡。通过自监督学习获得的表示可以作为后续鲁棒微调方法的基础，可能需要更少的标记示例来实现鲁棒性。
- en: While promising, self-supervised learning for robustness remains an active research
    area with important limitations. Current SSL methods may still be vulnerable to
    adversarial attacks, particularly when attackers understand the pretext tasks.
    The theoretical understanding of why and when SSL improves robustness remains
    incomplete. Computational overhead for SSL pre-training can be substantial, requiring
    careful consideration of resource constraints.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前景广阔，但用于鲁棒性的自监督学习仍然是一个活跃的研究领域，存在重要的局限性。当前的SSL方法可能仍然容易受到对抗攻击的影响，尤其是当攻击者了解预置任务时。为什么以及何时SSL可以提高鲁棒性的理论理解仍然不完整。SSL预训练的计算开销可能很大，需要仔细考虑资源限制。
- en: This direction indicates an evolving research area that may change how we approach
    robust AI system development, moving beyond defensive techniques toward learning
    approaches that are inherently more reliable.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方向表明了一个不断发展的研究领域，它可能会改变我们处理鲁棒人工智能系统开发的方法，从防御技术转向本质上更可靠的学习方法。
- en: The three pillars we have examined—hardware faults, input-level attacks, and
    environmental shifts—each target different aspects of AI systems. Yet they all
    operate within and depend upon complex software infrastructures that present their
    own unique vulnerabilities.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察的三个支柱——硬件故障、输入级攻击和环境变化——每个都针对人工智能系统的不同方面。然而，它们都在复杂的软件基础设施中运行并依赖于它，这些基础设施本身也具有独特的漏洞。
- en: Software Faults
  id: totrans-597
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件故障
- en: 'The robustness challenges we have examined so far—hardware faults, input-level
    attacks, and environmental shifts—each compromise different system layers. Hardware
    faults corrupt physical computation, adversarial attacks exploit algorithmic boundaries,
    and environmental shifts challenge model generalization. Software faults introduce
    a fourth dimension that can amplify all three: bugs and implementation errors
    in the complex software ecosystems that support modern AI deployments.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止考察的鲁棒性挑战——硬件故障、输入级攻击和环境变化——每个都损害了不同的系统层。硬件故障破坏了物理计算，对抗攻击利用算法边界，环境变化挑战模型泛化。软件故障引入了第四个维度，它可以放大所有三个：支持现代人工智能部署的复杂软件生态系统中的错误和实现错误。
- en: This third category differs from the previous two. Unlike hardware faults, which
    typically arise from physical phenomena, or model robustness issues, which stem
    from core limitations in learning algorithms, software faults result from human
    errors in system design and implementation. These faults can corrupt any aspect
    of the AI pipeline, from data preprocessing and model training to inference and
    result interpretation, often in subtle ways that may not be immediately apparent.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 这第三个类别与前面两个不同。与通常源于物理现象的硬件故障不同，或与源于学习算法核心限制的模型鲁棒性问题不同，软件故障源于系统设计和实现中的人类错误。这些故障可以破坏人工智能管道的任何方面，从数据预处理和模型训练到推理和结果解释，通常以微妙的方式出现，可能不会立即明显。
- en: Software faults in AI systems are particularly challenging because they can
    interact with and amplify the other robustness threats we have discussed. A bug
    in data preprocessing might create distribution shifts that expose model vulnerabilities.
    Implementation errors in numerical computations might manifest similarly to hardware
    faults but without the benefit of hardware-level error detection mechanisms. Race
    conditions in distributed training might cause model corruption that resembles
    adversarial attacks on the learned representations.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统中的软件故障特别具有挑战性，因为它们可以与并放大我们讨论过的其他稳健性威胁相互作用。数据预处理中的错误可能会导致分布变化，从而暴露模型漏洞。数值计算中的实现错误可能类似于硬件故障，但没有硬件级别的错误检测机制的好处。分布式训练中的竞争条件可能导致模型损坏，类似于对学习表示的对抗性攻击。
- en: These interactions arise from the inherent complexity of modern AI software
    stacks—spanning frameworks, libraries, runtime environments, distributed systems,
    and deployment infrastructure—which creates numerous opportunities for faults
    to emerge and propagate. Understanding and mitigating these software-level threats
    is essential for building truly robust AI systems that can operate reliably in
    production environments despite the inherent complexity of their supporting software
    infrastructure.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 这些交互源于现代人工智能软件堆栈固有的复杂性——包括框架、库、运行时环境、分布式系统和部署基础设施，这为故障的出现和传播创造了众多机会。理解和缓解这些软件层面的威胁对于构建真正稳健的人工智能系统至关重要，这些系统能够在生产环境中可靠地运行，尽管其支持软件基础设施具有固有的复杂性。
- en: Machine learning systems rely on complex software infrastructures that extend
    far beyond the models themselves. These systems are built on top of frameworks
    detailed in [Chapter 7](ch013.xhtml#sec-ai-frameworks), libraries, and runtime
    environments that facilitate model training, evaluation, and deployment. As with
    any large-scale software system, the components that support ML workflows are
    susceptible to faults—unintended behaviors resulting from defects, bugs, or design
    oversights in the software, creating operational challenges beyond the standard
    practices detailed in [Chapter 13](ch019.xhtml#sec-ml-operations). These faults
    can manifest across all stages of an ML pipeline and, if not identified and addressed,
    may impair performance, compromise security, or even invalidate results. This
    section examines the nature, causes, and consequences of software faults in ML
    systems, as well as strategies for their detection and mitigation.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统依赖于复杂的软件基础设施，这些基础设施远远超出了模型本身。这些系统建立在第七章中详细介绍的框架、库和运行时环境之上，这些环境促进了模型训练、评估和部署。与任何大型软件系统一样，支持机器学习工作流程的组件容易受到故障的影响——这些故障是由软件中的缺陷、错误或设计疏忽引起的意外行为，在[第13章](ch019.xhtml#sec-ml-operations)中详细说明的标准实践之外，创造了运营挑战。这些故障可能出现在机器学习管道的所有阶段，如果未被发现和解决，可能会损害性能、损害安全性，甚至使结果无效。本节探讨了机器学习系统中软件故障的性质、原因和后果，以及它们的检测和缓解策略。
- en: Software Fault Properties
  id: totrans-603
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件故障属性
- en: Understanding how software faults impact ML systems requires examining their
    distinctive characteristics. Software faults in ML frameworks originate from various
    sources, including programming errors, architectural misalignments, and version
    incompatibilities. These faults exhibit several important characteristics that
    influence how they arise and propagate in practice.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 理解软件故障如何影响机器学习系统需要考察它们的独特特征。机器学习框架中的软件故障源于各种来源，包括编程错误、架构不匹配和版本不兼容。这些故障表现出几个重要特征，这些特征影响了它们在实际中的产生和传播。
- en: One defining feature of software faults is their diversity. Faults can range
    from syntactic and logical errors to more complex manifestations such as memory
    leaks, concurrency bugs, or failures in integration logic. The broad variety of
    potential fault types complicates both their identification and resolution, as
    they often surface in non-obvious ways.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 软件错误的定义特征之一是其多样性。错误可能从语法和逻辑错误到更复杂的体现，如内存泄漏、并发错误或集成逻辑失败。潜在的故障类型广泛，使得它们的识别和解决都变得复杂，因为它们往往以不明显的方式出现。
- en: Complicating this diversity, a second key characteristic is their tendency to
    propagate across system boundaries. An error introduced in a low-level module,
    such as a tensor allocation routine or a preprocessing function, can produce cascading
    effects that disrupt model training, inference, or evaluation. Because ML frameworks
    are often composed of interconnected components, a fault in one part of the pipeline
    can introduce failures in seemingly unrelated modules.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性使得第二个关键特征是它们倾向于跨越系统边界传播。在底层模块中引入的错误，例如张量分配例程或预处理函数，可能会产生级联效应，破坏模型训练、推理或评估。由于机器学习框架通常由相互连接的组件组成，管道某一部分的故障可能会在看似无关的模块中引入故障。
- en: Some faults are intermittent, manifesting only under specific conditions such
    as high system load, particular hardware configurations, or rare data inputs.
    These transient faults are notoriously difficult to reproduce and diagnose, as
    they may not consistently appear during standard testing procedures.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 一些错误是间歇性的，仅在特定条件下才会出现，例如高系统负载、特定的硬件配置或罕见的数据输入。这些短暂的错误因其可能在标准测试程序中不一致地出现而闻名，因此难以重现和诊断。
- en: Perhaps most concerning for ML systems, software faults may subtly interact
    with ML models themselves. For example, a bug in a data transformation script
    might introduce systematic noise or shift the distribution of inputs, leading
    to biased or inaccurate predictions. Similarly, faults in the serving infrastructure
    may result in discrepancies between training-time and inference-time behaviors,
    undermining deployment consistency.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习系统来说，最令人担忧的可能就是软件错误可能会微妙地与机器学习模型本身相互作用。例如，数据转换脚本中的错误可能会引入系统性的噪声或改变输入的分布，导致偏差或不准确的预测。同样，服务基础设施中的错误可能会导致训练时和推理时行为之间的差异，破坏部署一致性。
- en: The consequences of software faults extend to a range of system properties.
    Faults may impair performance by introducing latency or inefficient memory usage;
    they may reduce scalability by limiting parallelism; or they may compromise reliability
    and security by exposing the system to unexpected behaviors or malicious exploitation.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 软件错误的后果涉及一系列系统属性。错误可能通过引入延迟或低效的内存使用来损害性能；它们可能通过限制并行性来降低可扩展性；或者它们可能通过使系统暴露于意外行为或恶意利用来损害可靠性和安全性。
- en: Adding another layer of complexity, the manifestation of software faults is
    often shaped by external dependencies, such as hardware platforms, operating systems,
    or third-party libraries. Incompatibilities arising from version mismatches or
    hardware-specific behavior may result in subtle, hard-to-trace bugs that only
    appear under certain runtime conditions.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，软件错误的体现通常受到外部依赖的影响，例如硬件平台、操作系统或第三方库。由于版本不匹配或特定硬件行为导致的兼容性问题可能会导致微妙且难以追踪的错误，这些错误仅在特定的运行时条件下才会出现。
- en: A thorough understanding of these characteristics is essential for developing
    robust software engineering practices in ML. It also provides the foundation for
    the detection and mitigation strategies described later in this section.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些特性的深入了解对于在机器学习中开发稳健的软件工程实践至关重要。它也为本节后面描述的检测和缓解策略提供了基础。
- en: Software Fault Propagation
  id: totrans-612
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件错误传播
- en: These characteristics illustrate how software faults in ML frameworks arise
    through a variety of mechanisms, reflecting the complexity of modern ML pipelines
    and the layered architecture of supporting tools. These mechanisms correspond
    to specific classes of software failures that commonly occur in practice.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性说明了机器学习框架中的软件错误是如何通过各种机制出现的，反映了现代机器学习管道的复杂性和支持工具分层架构。这些机制对应于实践中常见的特定软件故障类别。
- en: One prominent class involves resource mismanagement, particularly with respect
    to memory. Improper memory allocation, including the failure to release buffers
    or file handles, can lead to memory leaks and, eventually, to resource exhaustion.
    This is especially detrimental in deep learning applications, where large tensors
    and GPU memory allocations are common. As shown in [Figure 16.35](ch022.xhtml#fig-gpu-out-of-memory),
    inefficient memory usage or the failure to release GPU resources can cause training
    procedures to halt or significantly degrade runtime performance.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个突出的类别涉及资源管理不当，尤其是与内存相关。不恰当的内存分配，包括未能释放缓冲区或文件句柄，可能导致内存泄漏，最终导致资源耗尽。这在深度学习应用中尤其有害，因为大型张量和GPU内存分配很常见。如图16.35所示，[图16.35](ch022.xhtml#fig-gpu-out-of-memory)，不高效的内存使用或未能释放GPU资源可能导致训练过程停止或显著降低运行性能。
- en: '![](../media/file281.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file281.png)'
- en: 'Figure 16.35: **GPU Resource Management**: Inefficient memory usage or failure
    to release GPU resources can lead to out-of-memory errors and suboptimal performance
    during training.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.35：**GPU资源管理**：不高效的内存使用或未能释放GPU资源可能导致内存不足错误和训练过程中的次优性能。
- en: Beyond resource management issues, another recurring fault mechanism stems from
    concurrency and synchronization errors. In distributed or multi-threaded environments,
    incorrect coordination among parallel processes can lead to race conditions, deadlocks,
    or inconsistent states. These issues are often tied to the improper use of [asynchronous
    operations](https://odsc.medium.com/optimizing-ml-serving-with-asynchronous-architectures-1071fc1be8e2),
    such as non-blocking I/O or parallel data ingestion. Synchronization bugs can
    corrupt the consistency of training states or produce unreliable model checkpoints.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 除了资源管理问题之外，另一个常见的故障机制源于并发和同步错误。在分布式或多线程环境中，并行进程之间不正确的协调可能导致竞争条件、死锁或不一致状态。这些问题通常与[异步操作](https://odsc.medium.com/optimizing-ml-serving-with-asynchronous-architectures-1071fc1be8e2)的不当使用有关，例如非阻塞I/O或并行数据摄取。同步错误可能会破坏训练状态的一致性或产生不可靠的模型检查点。
- en: Compatibility problems frequently arise from changes to the software environment,
    extending the framework compatibility issues discussed in [Chapter 7](ch013.xhtml#sec-ai-frameworks).
    For example, upgrading a third-party library without validating downstream effects
    may introduce subtle behavioral changes or break existing functionality. These
    issues are exacerbated when the training and inference environments differ in
    hardware, operating system, or dependency versions. Reproducibility in ML experiments
    often hinges on managing these environmental inconsistencies.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性问题通常源于软件环境的变化，这扩展了第7章中讨论的框架兼容性问题。[第7章](ch013.xhtml#sec-ai-frameworks)。例如，在不验证下游影响的情况下升级第三方库可能会引入微妙的行为变化或破坏现有功能。当训练和推理环境在硬件、操作系统或依赖版本上存在差异时，这些问题会变得更加严重。在机器学习实验中，可重复性往往取决于管理这些环境的不一致性。
- en: Faults related to numerical instability are also common in ML systems, particularly
    in optimization routines. Improper handling of floating-point precision, division
    by zero, or underflow/overflow conditions can introduce instability into gradient
    computations and convergence procedures. As described in [this resource](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter22.04-Numerical-Error-and-Instability.html),
    the accumulation of rounding errors across many layers of computation can distort
    learned parameters or delay convergence.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 与数值不稳定性相关的故障在机器学习系统中也很常见，尤其是在优化过程中。不恰当处理浮点精度、除以零或下溢/上溢条件可能会将不稳定性引入梯度计算和收敛过程。正如[这个资源](https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter22.04-Numerical-Error-and-Instability.html)中所述，在许多计算层中累积舍入误差可能会扭曲学习到的参数或延迟收敛。
- en: Exception handling, though often overlooked, plays a crucial role in the stability
    of ML pipelines. Inadequate or overly generic exception management can cause systems
    to fail silently or crash under non-critical errors. Ambiguous error messages
    and poor logging practices impede diagnosis and prolong resolution times.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然异常处理通常被忽视，但它对机器学习管道的稳定性起着至关重要的作用。不充分或过于通用的异常管理可能导致系统在非关键错误下静默失败或崩溃。模糊的错误信息和糟糕的日志记录实践会阻碍诊断并延长解决时间。
- en: These fault mechanisms, while diverse in origin, share the potential to significantly
    impair ML systems. Understanding how they arise provides the basis for effective
    system-level safeguards.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 这些故障机制虽然来源多样，但都具有显著损害机器学习系统的潜力。了解它们是如何产生的为有效的系统级安全措施提供了基础。
- en: Software Fault Effects on ML
  id: totrans-622
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件故障对机器学习的影响
- en: The mechanisms through which software faults arise inform their impact on ML
    systems. The consequences of software faults can be profound, affecting not only
    the correctness of model outputs but also the broader usability and reliability
    of an ML system in production.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 软件故障产生的机制揭示了其对机器学习系统的影响。软件故障的后果可能非常严重，不仅会影响模型输出的正确性，还会影响机器学习系统在生产中的广泛可用性和可靠性。
- en: The most immediately visible impact is performance degradation, a common symptom
    often resulting from memory leaks, inefficient resource scheduling, or contention
    between concurrent threads. These issues tend to accumulate over time, leading
    to increased latency, reduced throughput, or even system crashes. As noted by
    ([Maas et al. 2024](ch058.xhtml#ref-maas2008combining)), the accumulation of performance
    regressions across components can severely restrict the operational capacity of
    ML systems deployed at scale.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的影响是性能下降，这是一个常见的症状，通常由内存泄漏、资源调度效率低下或并发线程之间的竞争引起。这些问题往往会随着时间的推移而积累，导致延迟增加、吞吐量减少，甚至系统崩溃。正如([Maas等人2024](ch058.xhtml#ref-maas2008combining))所指出的，组件间性能退化的累积可能会严重限制大规模部署的机器学习系统的运行能力。
- en: In addition to slowing system performance, faults can lead to inaccurate predictions.
    For example, preprocessing errors or inconsistencies in feature encoding can subtly
    alter the input distribution seen by the model, producing biased or unreliable
    outputs. These kinds of faults are particularly insidious, as they may not trigger
    any obvious failure but still compromise downstream decisions. Over time, rounding
    errors and precision loss can amplify inaccuracies, particularly in deep architectures
    with many layers or long training durations.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减慢系统性能外，故障还可能导致预测不准确。例如，预处理错误或特征编码的不一致性可能会微妙地改变模型看到的输入分布，产生有偏或不可靠的输出。这类故障尤其隐蔽，因为它们可能不会触发任何明显的故障，但仍然会损害下游决策。随着时间的推移，舍入误差和精度损失会放大不准确度，尤其是在具有许多层或长期训练周期的深度架构中。
- en: Beyond accuracy concerns, reliability is also undermined by software faults.
    Systems may crash unexpectedly, fail to recover from errors, or behave inconsistently
    across repeated executions. Intermittent faults are especially problematic in
    this context, as they erode user trust while eluding conventional debugging efforts.
    In distributed settings, faults in checkpointing or model serialization can cause
    training interruptions or data loss, reducing the resilience of long-running training
    pipelines.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确性问题之外，软件故障还会破坏可靠性。系统可能会意外崩溃，无法从错误中恢复，或者在重复执行中表现不一致。间歇性故障在此背景下尤其成问题，因为它们会侵蚀用户信任，同时逃避传统的调试努力。在分布式环境中，检查点或模型序列化中的故障可能导致训练中断或数据丢失，降低长期运行训练管道的弹性。
- en: Security vulnerabilities frequently arise from overlooked software faults. Buffer
    overflows, improper validation, or unguarded inputs can open the system to manipulation
    or unauthorized access. Attackers may exploit these weaknesses to alter the behavior
    of models, extract private data, or induce denial-of-service conditions. As described
    by ([Q. Li et al. 2023](ch058.xhtml#ref-li2021survey)), such vulnerabilities pose
    serious risks, particularly when ML systems are integrated into critical infrastructure
    or handle sensitive user data.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 安全漏洞通常源于被忽视的软件故障。缓冲区溢出、验证不当或未受保护的输入可能会使系统容易受到操纵或未经授权的访问。攻击者可能会利用这些弱点来改变模型的行为、提取私有数据或引发拒绝服务条件。正如([Q.
    李等人2023](ch058.xhtml#ref-li2021survey))所描述的，这些漏洞带来了严重风险，尤其是在机器学习系统集成到关键基础设施或处理敏感用户数据时。
- en: Finally, the presence of faults complicates development and maintenance. Debugging
    becomes more time-consuming, especially when fault behavior is non-deterministic
    or dependent on external configurations. Frequent software updates or library
    patches may introduce regressions that require repeated testing. This increased
    engineering overhead can slow iteration, inhibit experimentation, and divert resources
    from model development.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，故障的存在使开发和维护变得复杂。调试变得更加耗时，尤其是在故障行为是非确定性的或依赖于外部配置时。频繁的软件更新或库补丁可能引入回归，需要重复测试。这种增加的工程开销可能会减慢迭代速度，阻碍实验，并将资源从模型开发中转移出来。
- en: Taken together, these impacts underscore the importance of systematic software
    engineering practices in ML—practices that anticipate, detect, and mitigate the
    diverse failure modes introduced by software faults.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 一起来看，这些影响突出了在机器学习中系统软件工程实践的重要性——这些实践可以预测、检测和缓解由软件故障引入的多种故障模式。
- en: Software Fault Detection and Prevention
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件故障检测和预防
- en: Given the significant impact of software faults on ML systems, addressing these
    issues requires an integrated strategy that spans development, testing, deployment,
    and monitoring, building upon the operational best practices from [Chapter 13](ch019.xhtml#sec-ml-operations).
    An effective mitigation framework should combine proactive detection methods with
    robust design patterns and operational safeguards.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到软件故障对机器学习系统的影响重大，解决这些问题需要跨越开发、测试、部署和监控的集成策略，基于第13章（ch019.xhtml#sec-ml-operations）中的运营最佳实践。一个有效的缓解框架应结合主动检测方法、稳健的设计模式和操作保障。
- en: To help summarize these techniques and clarify where each strategy fits in the
    ML lifecycle, [Table 16.6](ch022.xhtml#tbl-software-faults-summary) below categorizes
    detection and mitigation approaches by phase and objective. This table provides
    a high-level overview that complements the detailed explanations that follow.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助总结这些技术并阐明每种策略在机器学习生命周期中的位置，下表 [表16.6](ch022.xhtml#tbl-software-faults-summary)
    按阶段和目标对检测和缓解方法进行了分类。此表提供了一个高级概述，补充了随后的详细解释。
- en: 'Table 16.6: **Fault Mitigation Strategies**: Software faults in ML systems
    require layered detection and mitigation techniques applied throughout the development
    lifecycle—from initial testing to ongoing monitoring—to ensure reliability and
    robustness. This table categorizes these strategies by phase and objective, providing
    a framework for building comprehensive fault tolerance into machine learning deployments.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.6：**故障缓解策略**：机器学习系统中的软件故障需要分层检测和缓解技术，这些技术应用于整个开发生命周期——从初始测试到持续监控，以确保可靠性和鲁棒性。此表按阶段和目标对这些策略进行了分类，为构建全面故障容忍的机器学习部署提供了一个框架。
- en: '| **Category** | **Technique** | **Purpose** | **When to Apply** |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **技术** | **目的** | **何时应用** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Testing and Validation** | Unit testing, integration testing, regression
    testing | Verify correctness and identify regressions | During development |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| **测试和验证** | 单元测试、集成测试、回归测试 | 验证正确性并识别回归 | 开发期间 |'
- en: '| **Static Analysis and Linting** | Static analyzers, linters, code reviews
    | Detect syntax errors, unsafe operations, enforce best practices | Before integration
    |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| **静态分析和代码审查** | 静态分析器、代码审查工具 | 检测语法错误、不安全操作、强制最佳实践 | 集成前 |'
- en: '| **Runtime Monitoring & Logging** | Metric collection, error logging, profiling
    | Observe system behavior, detect anomalies | During training and deployment |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| **运行时监控与日志记录** | 指标收集、错误日志、分析 | 观察系统行为，检测异常 | 训练和部署期间 |'
- en: '| **Fault-Tolerant Design** | Exception handling, modular architecture, checkpointing
    | Minimize impact of failures, support recovery | Design and implementation phase
    |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| **容错设计** | 异常处理、模块化架构、检查点 | 最小化故障影响，支持恢复 | 设计和实现阶段 |'
- en: '| **Update Management** | Dependency auditing, test staging, version tracking
    | Prevent regressions and compatibility issues | Before system upgrades or deployment
    |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| **更新管理** | 依赖审计、测试阶段、版本跟踪 | 预防回归和兼容性问题 | 在系统升级或部署之前 |'
- en: '| **Environment Isolation** | Containerization (e.g., Docker, Kubernetes),
    virtual environments | Ensure reproducibility, avoid environment-specific bugs
    | Development, testing, deployment |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| **环境隔离** | 容器化（例如，Docker、Kubernetes）、虚拟环境 | 确保可重复性，避免环境特定错误 | 开发、测试、部署 |'
- en: '| **CI/CD and Automation** | Automated test pipelines, monitoring hooks, deployment
    gates | Enforce quality assurance and catch faults early | Continuously throughout
    development |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| **CI/CD和自动化** | 自动化测试管道、监控钩子、部署门 | 强制质量保证并尽早捕获错误 | 开发过程中的持续进行 |'
- en: The first line of defense involves systematic testing. Unit testing verifies
    that individual components behave as expected under normal and edge-case conditions.
    Integration testing ensures that modules interact correctly across boundaries,
    while regression testing detects errors introduced by code changes. Continuous
    testing is essential in fast-moving ML environments, where pipelines evolve rapidly
    and small modifications may have system-wide consequences. As shown in [Figure 16.36](ch022.xhtml#fig-regression-testing),
    automated regression tests help preserve functional correctness over time.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 第一道防线涉及系统性的测试。单元测试验证单个组件在正常和边缘情况下的行为是否符合预期。集成测试确保模块在边界之间正确交互，而回归测试检测由代码更改引入的错误。在快速发展的机器学习环境中，持续测试至关重要，因为管道迅速演变，小的修改可能产生系统级的影响。如图16.36[图](ch022.xhtml#fig-regression-testing)所示，自动回归测试有助于随着时间的推移保持功能正确性。
- en: '![](../media/file282.png)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file282.png)'
- en: 'Figure 16.36: **Regression Test Automation**: Automated regression tests verify
    that new code changes do not introduce unintended errors into existing functionality,
    preserving system reliability throughout the development lifecycle. Continuous
    execution of these tests is crucial in rapidly evolving machine learning systems
    where even small modifications can have widespread consequences. *Source: [UTOR](HTTPS://u-tor.com/topic/regression-vs-integration)*'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.36：**回归测试自动化**：自动回归测试验证新代码更改不会向现有功能引入意外的错误，从而在整个开发生命周期中保持系统可靠性。在快速发展的机器学习系统中，这些测试的持续执行至关重要，因为即使是微小的修改也可能产生广泛的影响。*来源：[UTOR](HTTPS://u-tor.com/topic/regression-vs-integration)*
- en: Static code analysis tools complement dynamic tests by identifying potential
    issues at compile time. These tools catch common errors such as variable misuse,
    unsafe operations, or violation of language-specific best practices. Combined
    with code reviews and consistent style enforcement, static analysis reduces the
    incidence of avoidable programming faults.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 静态代码分析工具通过在编译时识别潜在问题来补充动态测试。这些工具可以捕获常见的错误，例如变量误用、不安全操作或违反特定语言的最佳实践。结合代码审查和一致的样式执行，静态分析可以减少可避免的编程错误的发病率。
- en: Runtime monitoring is critical for observing system behavior under real-world
    conditions. Logging frameworks should capture key signals such as memory usage,
    input/output traces, and exception events. Monitoring tools can track model throughput,
    latency, and failure rates, providing early warnings of software faults. Profiling,
    as illustrated in this [Microsoft resource](https://microsoft.github.io/code-with-engineering-playbook/machine-learning/profiling-ml-and-mlops-code/),
    helps identify performance bottlenecks and inefficiencies indicative of deeper
    architectural issues.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时监控对于观察系统在实际环境下的行为至关重要。日志框架应捕获关键信号，如内存使用、输入/输出跟踪和异常事件。监控工具可以跟踪模型吞吐量、延迟和故障率，提供软件错误的早期警告。如图所示，[Microsoft资源](https://microsoft.github.io/code-with-engineering-playbook/machine-learning/profiling-ml-and-mlops-code/)中的性能分析有助于识别性能瓶颈和表明更深层次架构问题的低效。
- en: Robust system design further improves fault tolerance. Structured exception
    handling and assertion checks prevent small errors from cascading into system-wide
    failures. Redundant computations, fallback models, and failover mechanisms improve
    availability in the presence of component failures. Modular architectures that
    encapsulate state and isolate side effects make it easier to diagnose and contain
    faults. Checkpointing techniques, such as those discussed in ([Eisenman et al.
    2022](ch058.xhtml#ref-eisenman2022check)), enable recovery from mid-training interruptions
    without data loss.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 顽强的系统设计进一步提高了容错能力。结构化异常处理和断言检查防止小错误级联成系统级故障。冗余计算、回退模型和故障转移机制在组件故障的情况下提高了可用性。封装状态和隔离副作用的结构化架构使得诊断和包含故障更加容易。如([Eisenman等人2022年](ch058.xhtml#ref-eisenman2022check))中讨论的检查点技术，可以在不丢失数据的情况下从训练中断中恢复。
- en: Keeping ML software up to date is another key strategy. Applying regular updates
    and security patches helps address known bugs and vulnerabilities. However, updates
    must be validated through test staging environments to avoid regressions. Reviewing
    [release notes](https://github.com/pytorch/pytorch/releases) and change logs ensures
    teams are aware of any behavioral changes introduced in new versions.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 保持机器学习软件的更新是另一项关键策略。应用定期的更新和安全补丁有助于解决已知的错误和漏洞。然而，更新必须通过测试阶段环境进行验证，以避免回归。审查[发布说明](https://github.com/pytorch/pytorch/releases)和变更日志确保团队了解新版本中引入的任何行为变化。
- en: Containerization technologies like [Docker](https://www.docker.com) and [Kubernetes](https://kubernetes.io)
    allow teams to define reproducible runtime environments that mitigate compatibility
    issues. By isolating system dependencies, containers prevent faults introduced
    by system-level discrepancies across development, testing, and production.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[Docker](https://www.docker.com)和[Kubernetes](https://kubernetes.io)这样的容器化技术允许团队定义可重复的运行环境，从而减轻兼容性问题。通过隔离系统依赖，容器防止了由于开发、测试和生产中系统级差异引入的故障。
- en: Finally, automated pipelines built around continuous integration and continuous
    deployment (CI/CD) provide an infrastructure for enforcing fault-aware development.
    Testing, validation, and monitoring can be embedded directly into the CI/CD flow.
    As shown in [Figure 16.37](ch022.xhtml#fig-CI-CD-procedure), such pipelines reduce
    the risk of unnoticed regressions and ensure only tested code reaches deployment
    environments.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，围绕持续集成和持续部署（CI/CD）构建的自动化流水线为强制执行故障感知开发提供了基础设施。测试、验证和监控可以直接嵌入到CI/CD流程中。如图16.37[图](ch022.xhtml#fig-CI-CD-procedure)所示，这样的流水线降低了未注意到的回归风险，并确保只有经过测试的代码达到部署环境。
- en: '![](../media/file283.svg)'
  id: totrans-652
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file283.svg)'
- en: 'Figure 16.37: **CI/CD Pipeline**: Automated CI/CD pipelines enforce fault-aware
    development by integrating testing and validation directly into the software delivery
    process, reducing the risk of regressions and ensuring only tested code reaches
    production. Containerization technologies, such as Docker and Kubernetes, further
    enhance reliability by providing reproducible runtime environments across these
    pipeline stages. *Source: [geeksforgeeks](HTTPS://www.geeksforgeeks.org/ci-cd-continuous-integration-and-continuous-delivery/)*'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '图16.37: **CI/CD流水线**: 自动化的CI/CD流水线通过将测试和验证直接集成到软件交付过程中，强制执行故障感知开发，降低回归风险，并确保只有经过测试的代码达到生产环境。容器化技术，如Docker和Kubernetes，通过在这些流水线阶段提供可重复的运行环境来进一步增强可靠性。*来源：[geeksforgeeks](HTTPS://www.geeksforgeeks.org/ci-cd-continuous-integration-and-continuous-delivery/)*'
- en: Together, these practices form a complete approach to software fault management
    in ML systems. When adopted systematically, they reduce the likelihood of system
    failures, improve long-term maintainability, and foster trust in model performance
    and reproducibility.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实践共同构成了机器学习系统中软件故障管理的完整方法。当系统性地采用时，它们降低了系统故障的可能性，提高了长期的可维护性，并促进了模型性能和可重复性的信任。
- en: Fault Injection Tools and Frameworks
  id: totrans-655
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障注入工具和框架
- en: Given the importance of developing robust AI systems, in recent years, researchers
    and practitioners have developed a wide range of tools and frameworks building
    on the software infrastructure from [Chapter 7](ch013.xhtml#sec-ai-frameworks)
    to understand how hardware faults manifest and propagate to impact ML systems.
    These tools and frameworks play a crucial role in evaluating the resilience of
    ML systems to hardware faults by simulating various fault scenarios and analyzing
    their impact on the system’s performance, complementing the evaluation methodologies
    described in [Chapter 12](ch018.xhtml#sec-benchmarking-ai). This enables designers
    to identify potential vulnerabilities and develop effective mitigation strategies,
    ultimately creating more robust and reliable ML systems that can operate safely
    despite hardware faults, supporting the deployment strategies detailed in [Chapter 13](ch019.xhtml#sec-ml-operations).
    This section provides an overview of widely used fault models[66](#fn66) in the
    literature and the tools and frameworks developed to evaluate the impact of such
    faults on ML systems.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开发稳健的人工智能系统的重要性，近年来，研究人员和从业者已经开发了一系列工具和框架，这些工具和框架建立在第7章（[Chapter 7](ch013.xhtml#sec-ai-frameworks)）中描述的软件基础设施之上，以了解硬件故障如何表现和传播以影响机器学习系统。这些工具和框架在通过模拟各种故障场景并分析其对系统性能的影响方面发挥着关键作用，从而补充了第12章（[Chapter 12](ch018.xhtml#sec-benchmarking-ai)）中描述的评估方法。这使得设计人员能够识别潜在的安全漏洞并制定有效的缓解策略，最终创建出更稳健、更可靠且能够在硬件故障下安全运行的人工智能系统，支持第13章（[Chapter 13](ch019.xhtml#sec-ml-operations)）中详细说明的部署策略。本节概述了文献中广泛使用的故障模型[66](#fn66)以及为评估此类故障对机器学习系统的影响而开发的工具和框架。
- en: Fault and Error Models
  id: totrans-657
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障和错误模型
- en: As discussed previously, hardware faults can manifest in various ways, including
    transient, permanent, and intermittent faults. In addition to the type of fault
    under study, how the fault manifests is also important. For example, does the
    fault happen in a memory cell or during the computation of a functional unit?
    Is the impact on a single bit, or does it impact multiple bits? Does the fault
    propagate all the way and impact the application (causing an error), or does it
    get masked quickly and is considered benign? All these details impact what is
    known as the fault model, which plays a major role in simulating and measuring
    what happens to a system when a fault occurs.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，硬件故障可以以多种方式表现出来，包括瞬态、永久和间歇性故障。除了正在研究的故障类型外，故障的表现方式也同样重要。例如，故障是在存储单元中发生还是在功能单元的计算过程中发生？它的影响是针对单个比特，还是影响多个比特？故障是否传播到整个系统并影响应用程序（导致错误），或者它是否迅速被掩盖并被认为是无害的？所有这些细节都会影响所谓的故障模型，这在模拟和测量故障发生时系统所发生的情况中起着重要作用。
- en: To study and understand the impact of hardware faults on ML systems, understanding
    the concepts of fault models and error models is essential. A fault model describes
    how a hardware fault manifests itself in the system, while an error model represents
    how the fault propagates and affects the system’s behavior.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究和理解硬件故障对机器学习系统的影响，理解故障模型和错误模型的概念是至关重要的。故障模型描述了硬件故障如何在系统中表现，而错误模型则表示故障如何传播并影响系统的行为。
- en: 'Fault models are often classified by several key properties. First, they can
    be defined by their duration: transient faults are temporary and vanish quickly;
    permanent faults persist indefinitely; and intermittent faults occur sporadically,
    making them particularly difficult to identify or predict. Another dimension is
    fault location, with faults arising in hardware components such as memory cells,
    functional units, or interconnects. Faults can also be characterized by their
    granularity—some faults affect only a single bit (e.g., a bitflip), while others
    impact multiple bits simultaneously, as in burst errors.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 故障模型通常根据几个关键属性进行分类。首先，它们可以根据其持续时间来定义：瞬态故障是暂时的，很快就会消失；永久故障无限期地持续存在；间歇性故障偶尔发生，这使得它们特别难以识别或预测。另一个维度是故障位置，故障可能出现在硬件组件中，如存储单元、功能单元或互连。故障还可以通过其粒度来表征——一些故障仅影响单个比特（例如，比特翻转），而其他故障则同时影响多个比特，如突发错误。
- en: Error models, in contrast, describe the behavioral effects of faults as they
    propagate through the system. These models help researchers understand how initial
    hardware-level disturbances might manifest in the system’s behavior, such as through
    corrupted weights or miscomputed activations in an ML model. These models may
    operate at various abstraction levels, from low-level hardware errors to higher-level
    logical errors in ML frameworks.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，错误模型描述了故障在系统内传播时的行为效应。这些模型帮助研究人员理解初始硬件级别的扰动可能如何体现在系统的行为中，例如通过机器学习模型中损坏的权重或错误的激活。这些模型可以在各种抽象级别上运行，从低级别的硬件错误到机器学习框架中的高级逻辑错误。
- en: The choice of fault or error model is central to robustness evaluation. For
    example, a system built to study single-bit transient faults ([Sangchoolie, Pattabiraman,
    and Karlsson 2017](ch058.xhtml#ref-sangchoolie2017one)) will not offer meaningful
    insight into the effects of permanent multi-bit faults ([Wilkening et al. 2014](ch058.xhtml#ref-wilkening2014calculating)),
    since its design and assumptions are grounded in a different fault model entirely.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 故障或错误模型的选择对于鲁棒性评估至关重要。例如，一个旨在研究单比特瞬态故障的系统（[Sangchoolie, Pattabiraman, 和 Karlsson
    2017](ch058.xhtml#ref-sangchoolie2017one)）将不会对永久性多比特故障的影响提供有意义的见解（[Wilkening 等人
    2014](ch058.xhtml#ref-wilkening2014calculating)），因为其设计和假设基于完全不同的故障模型。
- en: It’s also important to consider how and where an error model is implemented.
    A single-bit flip at the architectural register level, modeled using simulators
    like gem5 ([Binkert et al. 2011](ch058.xhtml#ref-binkert2011gem5)), differs meaningfully
    from a similar bit flip in a PyTorch model’s weight tensor. While both simulate
    value-level perturbations, the lower-level model captures microarchitectural effects
    that are often abstracted away in software frameworks.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑错误模型如何以及在哪里实现也很重要。在架构寄存器级别发生的单比特翻转，使用gem5等模拟器建模（[Binkert 等人 2011](ch058.xhtml#ref-binkert2011gem5)），与PyTorch模型权重张量中的类似比特翻转有明显的区别。虽然两者都模拟了值级别的扰动，但低级别模型捕捉了通常在软件框架中抽象掉的微架构效应。
- en: Interestingly, certain fault behavior patterns remain consistent regardless
    of abstraction level. For example, research has consistently demonstrated that
    single-bit faults cause more disruption than multi-bit faults, whether examining
    hardware-level effects or software-visible impacts ([Sangchoolie, Pattabiraman,
    and Karlsson 2017](ch058.xhtml#ref-sangchoolie2017one); [Papadimitriou and Gizopoulos
    2021](ch058.xhtml#ref-papadimitriou2021demystifying)). However, other important
    behaviors like error masking ([Mohanram and Touba, n.d.](ch058.xhtml#ref-mohanram2003partial))
    may only be observable at lower abstraction levels. As illustrated in [Figure 16.38](ch022.xhtml#fig-error-masking),
    this masking phenomenon can cause faults to be filtered out before they propagate
    to higher levels, meaning software-based tools may miss these effects entirely.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，某些故障行为模式在抽象级别上保持一致。例如，研究一直表明，单比特故障比多比特故障造成的破坏更大，无论是考察硬件级别的效应还是软件可见的影响（[Sangchoolie,
    Pattabiraman, 和 Karlsson 2017](ch058.xhtml#ref-sangchoolie2017one)；[Papadimitriou
    和 Gizopoulos 2021](ch058.xhtml#ref-papadimitriou2021demystifying)）。然而，其他重要的行为，如错误掩码（[Mohanram
    和 Touba，未注明年份](ch058.xhtml#ref-mohanram2003partial)），可能只能在较低的抽象级别上观察到。如图16.38所示，这种掩码现象可能导致故障在传播到更高级别之前被过滤掉，这意味着基于软件的工具可能会完全错过这些效应。
- en: '![](../media/file284.svg)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file284.svg)'
- en: 'Figure 16.38: **Error Masking**: Microarchitectural redundancy can absorb single-bit
    faults before they propagate to observable system errors, highlighting a discrepancy
    between hardware-level and software-level fault models. This figure details how
    fault masking occurs within microarchitectural components, demonstrating that
    software-based error detection tools may underestimate the true resilience of
    a system to transient errors. *([Ko 2021](ch058.xhtml#ref-ko2021characterizing))*'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.38：**错误掩码**：微架构冗余可以在故障传播到可观察的系统错误之前吸收单比特故障，突显了硬件级别和软件级别故障模型之间的差异。此图详细说明了故障掩码在微架构组件中的发生过程，表明基于软件的错误检测工具可能会低估系统对瞬态错误的真正弹性。*([Ko
    2021](ch058.xhtml#ref-ko2021characterizing))*
- en: To address these discrepancies, tools like Fidelity ([Yi He, Balaprakash, and
    Li 2020](ch058.xhtml#ref-he2020fidelity)) have been developed to align fault models
    across abstraction layers. By mapping software-observed fault behaviors to corresponding
    hardware-level patterns ([E. Cheng et al. 2016](ch058.xhtml#ref-cheng2016clear)),
    Fidelity offers a more accurate means of simulating hardware faults at the software
    level. While lower-level tools capture the true propagation of errors through
    a hardware system, they are generally slower and more complex. Software-level
    tools, such as those implemented in PyTorch or TensorFlow, are faster and easier
    to use for large-scale robustness testing, albeit with less precision.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些差异，开发了像Fidelity ([Yi He, Balaprakash, and Li 2020](ch058.xhtml#ref-he2020fidelity))这样的工具，以对抽象层之间的故障模型进行对齐。通过将软件观察到的故障行为映射到相应的硬件级模式([E.
    Cheng et al. 2016](ch058.xhtml#ref-cheng2016clear))，Fidelity提供了一种更精确的软件级别模拟硬件故障的方法。虽然底层工具捕获了错误通过硬件系统的真实传播，但它们通常速度较慢且更复杂。软件级别的工具，如PyTorch或TensorFlow中实现的工具，对于大规模鲁棒性测试来说更快、更易用，尽管精度较低。
- en: Hardware-Based Fault Injection
  id: totrans-668
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于硬件的故障注入
- en: Hardware-based fault injection methods allow researchers to directly introduce
    faults into physical systems and observe their effects on ML models. These approaches
    are essential for validating assumptions made in software-level fault injection
    tools and for studying how real-world hardware faults influence system behavior.
    While most error injection tools used in ML robustness research are software-based,
    because of their speed and scalability, hardware-based approaches remain critical
    for grounding higher-level error models. They are considered the most accurate
    means of studying the impact of faults on ML systems by manipulating the hardware
    directly to introduce errors.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 基于硬件的故障注入方法允许研究人员直接将故障引入物理系统，并观察它们对机器学习模型的影响。这些方法对于验证软件级别故障注入工具中做出的假设以及研究实际硬件故障如何影响系统行为至关重要。虽然大多数用于机器学习鲁棒性研究中的错误注入工具都是基于软件的，但由于它们的速度和可扩展性，基于硬件的方法对于建立高级错误模型仍然至关重要。通过直接操作硬件来引入错误，它们被认为是研究故障对机器学习系统影响的最准确方法。
- en: As illustrated in [Figure 16.39](ch022.xhtml#fig-hardware-errors), hardware
    faults can arise at various points within a deep neural network (DNN) processing
    pipeline. These faults may affect the control unit, on-chip memory (SRAM), off-chip
    memory (DRAM), processing elements, and accumulators, leading to erroneous results.
    In the depicted example, a DNN tasked with recognizing traffic signals correctly
    identifies a red light under normal conditions. However, hardware-induced faults,
    caused by phenomena such as aging, electromigration, soft errors, process variations,
    and manufacturing defects, can introduce errors that cause the DNN to misclassify
    the signal as a green light, potentially leading to catastrophic consequences
    in real-world applications.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图16.39](ch022.xhtml#fig-hardware-errors)所示，硬件故障可以在深度神经网络（DNN）处理管道的各个点出现。这些故障可能影响控制单元、片上内存（SRAM）、片外内存（DRAM）、处理元素和累加器，导致错误结果。在所描述的示例中，一个被赋予正确识别交通信号的DNN在正常条件下正确地识别了红灯。然而，由老化、电迁移、软错误、工艺变化和制造缺陷等现象引起的硬件引起的故障可能会引入错误，导致DNN将信号误分类为绿灯，这可能导致实际应用中的灾难性后果。
- en: '![](../media/file285.png)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file285.png)'
- en: 'Figure 16.39: **Hardware Faults**: This figure enables where hardware-induced
    errors can occur within a DNN processing pipeline, highlighting potential points
    of failure such as control units and memory modules that can lead to misclassifications
    in real-world applications.'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.39：**硬件故障**：此图展示了在深度神经网络（DNN）处理管道中可能发生的硬件引起的错误，突出了可能导致实际应用中误分类的潜在故障点，如控制单元和内存模块。
- en: These methods enable researchers to observe the system’s behavior under real-world
    fault conditions. Both software-based and hardware-based error injection tools
    are described in this section in more detail.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使研究人员能够观察系统在实际世界故障条件下的行为。本节更详细地描述了基于软件和硬件的错误注入工具。
- en: Hardware Injection Methods
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件注入方法
- en: Two of the most common hardware-based fault injection methods are FPGA-based
    fault injection and radiation or beam testing.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的两种基于硬件的故障注入方法是FPGA基于的故障注入和辐射或束测试。
- en: '**FPGA-based Fault Injection.** Field-Programmable Gate Arrays (FPGAs)[67](#fn67)
    are reconfigurable integrated circuits that can be programmed to implement various
    hardware designs. In the context of fault injection, FPGAs offer high precision
    and accuracy, as researchers can target specific bits or sets of bits within the
    hardware. By modifying the FPGA configuration, faults can be introduced at specific
    locations and times during the execution of an ML model. FPGA-based fault injection
    allows for fine-grained control over the fault model, enabling researchers to
    study the impact of different types of faults, such as single-bit flips or multi-bit
    errors. This level of control makes FPGA-based fault injection a valuable tool
    for understanding the resilience of ML systems to hardware faults.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于FPGA的故障注入**。现场可编程门阵列（FPGAs）[67](#fn67)是可重新配置的集成电路，可以编程以实现各种硬件设计。在故障注入的背景下，FPGAs提供了高精度和准确性，因为研究人员可以在硬件中针对特定的位或位集。通过修改FPGA配置，可以在机器学习模型的执行过程中在特定位置和时间引入故障。基于FPGA的故障注入允许对故障模型进行细粒度控制，使研究人员能够研究不同类型故障的影响，例如单比特翻转或多比特错误。这种控制水平使得基于FPGA的故障注入成为了解机器学习系统对硬件故障的恢复力的宝贵工具。'
- en: While FPGA-based methods allow precise, controlled fault injection, other approaches
    aim to replicate fault conditions found in natural environments.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于FPGA的方法允许精确、可控的故障注入，但其他方法旨在复制自然环境中发现的故障条件。
- en: '**Radiation or Beam Testing.** Radiation or beam testing ([Velazco, Foucard,
    and Peronnard 2010](ch058.xhtml#ref-velazco2010combining)) exposes hardware running
    ML models to high-energy particles like protons or neutrons. As shown in [Figure 16.40](ch022.xhtml#fig-beam-testing),
    specialized test facilities enable controlled radiation exposure to induce bitflips
    and other hardware-level faults. This approach is widely regarded as one of the
    most accurate methods for measuring error rates from particle strikes during application
    execution. Beam testing provides highly realistic fault scenarios that mirror
    conditions in radiation-rich environments, making it particularly valuable for
    validating systems destined for space missions or particle physics experiments.
    However, while beam testing offers exceptional realism, it lacks the precise targeting
    capabilities of FPGA-based injection - particle beams cannot be aimed at specific
    hardware bits or components with high precision. Despite this limitation and its
    significant operational complexity and cost, beam testing remains a trusted industry
    practice for rigorously evaluating hardware reliability under real-world radiation
    effects.'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '**辐射或束测试**。辐射或束测试（[Velazco, Foucard, 和 Peronnard 2010](ch058.xhtml#ref-velazco2010combining)）将运行机器学习模型的硬件暴露于高能粒子，如质子或中子。如图16.40（ch022.xhtml#fig-beam-testing）所示，专用测试设施能够实现可控的辐射暴露，以诱导位翻转和其他硬件级别的故障。这种方法被广泛认为是在应用执行期间测量粒子打击错误率的最准确方法之一。束测试提供了高度逼真的故障场景，反映了辐射丰富环境中的条件，这使得它对于验证旨在太空任务或粒子物理实验的系统特别有价值。然而，尽管束测试提供了非凡的现实感，但它缺乏基于FPGA注入的精确目标能力——粒子束不能以高精度指向特定的硬件位或组件。尽管存在这种限制以及其显著的操作复杂性和成本，束测试仍然是严格评估实际辐射效应下硬件可靠性的行业公认实践。'
- en: '![](../media/file286.png)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file286.png)'
- en: 'Figure 16.40: **Radiation Testing Setup**: Beam testing facilities induce hardware
    faults by exposing semiconductor components to high-energy particles, simulating
    realistic radiation environments encountered in space or particle physics experiments.
    This controlled fault injection method provides valuable data for assessing hardware
    reliability and error rates under extreme conditions, though it lacks the precise
    targeting capabilities of FPGA-based fault injection. *Source: JD instruments
    [HTTPS://jdinstruments.net/tester-capabilities-radiation-test/]*'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.40：**辐射测试设置**：辐射测试设施通过将半导体元件暴露于高能粒子中，模拟在太空或粒子物理实验中遇到的现实辐射环境，从而诱导硬件故障。这种可控的故障注入方法为评估硬件在极端条件下的可靠性和错误率提供了宝贵的数据，尽管它缺乏基于FPGA的故障注入的精确目标能力。*来源：JD仪器[HTTPS://jdinstruments.net/tester-capabilities-radiation-test/]*
- en: Hardware Injection Limitations
  id: totrans-681
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件注入限制
- en: Despite their high accuracy, hardware-based fault injection methods have several
    limitations that can hinder their widespread adoption.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于硬件的故障注入方法具有很高的准确性，但它们存在一些限制，可能会阻碍其广泛应用。
- en: First, cost is a major barrier. Both FPGA-based and beam testing[68](#fn68)
    approaches require specialized hardware and facilities, which can be expensive
    to set up and maintain. This makes them less accessible to research groups with
    limited funding or infrastructure.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，成本是一个主要障碍。基于FPGA和光束测试[68](#fn68)的方法都需要专用硬件和设施，这可能导致高昂的设置和维护成本。这使得资金或基础设施有限的研究小组难以获得。
- en: Second, these methods face challenges in scalability. Injecting faults and collecting
    data directly on hardware is time-consuming, which limits the number of experiments
    that can be run in a reasonable timeframe. This is especially restrictive when
    analyzing large ML systems or performing statistical evaluations across many fault
    scenarios.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这些方法面临着可扩展性的挑战。在硬件上直接注入故障和收集数据是耗时的，这限制了在合理时间内可以运行的实验数量。当分析大型机器学习系统或对许多故障场景进行统计分析时，这一点尤其受限。
- en: Third, flexibility limitations exist. Hardware-based methods may not be as adaptable
    as software-based alternatives when modeling a wide variety of fault and error
    types. Changing the experimental setup to accommodate a new fault model often
    requires time-intensive hardware reconfiguration.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，存在灵活性限制。在模拟广泛的故障和错误类型时，基于硬件的方法可能不如基于软件的替代方案适应性强。为了适应新的故障模型而更改实验设置通常需要耗时的硬件重新配置。
- en: Despite these limitations, hardware-based fault injection remains essential
    for validating the accuracy of software-based tools and for studying system behavior
    under real-world fault conditions. By combining the high fidelity of hardware-based
    methods with the scalability and flexibility of software-based tools, researchers
    can develop a more complete understanding of ML systems’ resilience to hardware
    faults and craft effective mitigation strategies.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，基于硬件的故障注入对于验证基于软件工具的准确性以及研究在真实世界故障条件下的系统行为仍然是必不可少的。通过结合基于硬件方法的精确性和基于软件工具的可扩展性和灵活性，研究人员可以更全面地了解机器学习系统对硬件故障的弹性，并制定有效的缓解策略。
- en: Software-Based Fault Injection
  id: totrans-687
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于软件的故障注入
- en: As machine learning frameworks like TensorFlow, PyTorch, and Keras have become
    the dominant platforms for developing and deploying ML models, software-based
    fault injection tools have emerged as a flexible and scalable way to evaluate
    the robustness of these systems to hardware faults. Unlike hardware-based approaches,
    which operate directly on physical systems, software-based methods simulate the
    effects of hardware faults by modifying a model’s underlying computational graph,
    tensor values, or intermediate computations.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 随着TensorFlow、PyTorch和Keras等机器学习框架成为开发和部署ML模型的主导平台，基于软件的故障注入工具已成为评估这些系统对硬件故障鲁棒性的灵活且可扩展的方法。与直接在物理系统上运行的基于硬件的方法不同，基于软件的方法通过修改模型的底层计算图、张量值或中间计算来模拟硬件故障的影响。
- en: These tools have become increasingly popular in recent years because they integrate
    directly with ML development pipelines, require no specialized hardware, and allow
    researchers to conduct large-scale fault injection experiments quickly and cost-effectively.
    By simulating hardware-level faults, including bit flips in weights, activations,
    or gradients, at the software level, these tools enable efficient testing of fault
    tolerance mechanisms and provide valuable insight into model vulnerabilities.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，这些工具越来越受欢迎，因为它们可以直接集成到机器学习（ML）开发管道中，无需专用硬件，并允许研究人员快速且经济高效地进行大规模故障注入实验。通过在软件层面模拟硬件级故障，包括权重、激活或梯度的位翻转，这些工具能够有效地测试容错机制，并为模型漏洞提供宝贵的见解。
- en: In the remainder of this section, we will examine the advantages and limitations
    of software-based fault injection methods, introduce major classes of tools (both
    general-purpose and domain-specific), and discuss how they contribute to building
    resilient ML systems.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将探讨基于软件的故障注入方法的优缺点，介绍主要工具类别（通用和特定领域），并讨论它们如何有助于构建具有弹性的机器学习系统。
- en: Software Injection Trade-offs
  id: totrans-691
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 软件注入权衡
- en: Software-based fault injection tools offer several advantages that make them
    attractive for studying the resilience of ML systems.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 基于软件的故障注入工具提供了几个优势，使它们对研究机器学习系统的弹性具有吸引力。
- en: One of the primary benefits is speed. Since these tools operate entirely within
    the software stack, they avoid the overhead associated with modifying physical
    hardware or configuring specialized test environments. This efficiency enables
    researchers to perform a large number of fault injection experiments in significantly
    less time. The ability to simulate a wide range of faults quickly makes these
    tools particularly useful for stress-testing large-scale ML models or conducting
    statistical analyses that require thousands of injections.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个主要的好处是速度。由于这些工具完全在软件堆栈中运行，它们避免了修改物理硬件或配置专用测试环境的相关开销。这种效率使得研究人员能够在更短的时间内进行大量的故障注入实验。快速模拟广泛故障的能力使得这些工具特别适用于对大规模机器学习模型进行压力测试或进行需要数千次注入的统计分析。
- en: These tools also offer flexibility. Software-based fault injectors can be easily
    adapted to model various types of faults. Researchers can simulate single-bit
    flips, multi-bit corruptions, or even more complex behaviors such as burst errors
    or partial tensor corruption. Software tools allow faults to be injected at different
    stages of the ML pipeline, at the stages of training, inference, or gradient computation,
    enabling precise targeting of different system components or layers.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具还提供了灵活性。基于软件的故障注入器可以轻松地适应模拟各种类型的故障。研究人员可以模拟单比特翻转、多比特损坏，甚至更复杂的如突发错误或部分张量损坏的行为。软件工具允许在机器学习管道的不同阶段注入故障，在训练、推理或梯度计算阶段，从而能够精确地针对不同的系统组件或层。
- en: These tools are also highly accessible, as they require only standard ML development
    environments. Unlike hardware-based methods, software tools require no costly
    experimental setups, custom circuitry, or radiation testing facilities. This accessibility
    opens up fault injection research to a broader range of institutions and developers,
    including those working in academia, startups, or resource-constrained environments.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具也高度可访问，因为它们只需要标准的机器学习开发环境。与基于硬件的方法不同，软件工具不需要昂贵的实验设置、定制电路或辐射测试设施。这种可访问性使得故障注入研究对更广泛的机构和发展者开放，包括那些在学术界、初创公司或资源受限环境中工作的人。
- en: However, these advantages come with certain trade-offs. Chief among them is
    accuracy. Because software-based tools model faults at a higher level of abstraction,
    they may not fully capture the low-level hardware interactions that influence
    how faults actually propagate. For example, a simulated bit flip in an ML framework
    may not account for how data is buffered, cached, or manipulated at the hardware
    level, potentially leading to oversimplified conclusions.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些优势也伴随着某些权衡。其中最重要的是准确性。由于基于软件的工具在更高层次上抽象地模拟故障，它们可能无法完全捕捉影响故障实际传播的低级硬件交互。例如，在机器学习框架中模拟的位翻转可能没有考虑到数据在硬件级别的缓冲、缓存或操作方式，这可能导致结论过于简化。
- en: Closely related is the issue of fidelity. While it is possible to approximate
    real-world fault behaviors, software-based tools may diverge from true hardware
    behavior, particularly when it comes to subtle interactions like masking, timing,
    or data movement. The results of such simulations depend heavily on the underlying
    assumptions of the error model and may require validation against real hardware
    measurements to be reliable.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 与此密切相关的是保真度问题。虽然可以近似现实世界的故障行为，但基于软件的工具可能与真实硬件行为有所偏差，尤其是在掩码、时序或数据移动等微妙交互方面。此类模拟的结果高度依赖于错误模型的潜在假设，并且可能需要与真实硬件测量结果进行验证才能可靠。
- en: Despite these limitations, software-based fault injection tools play an indispensable
    role in the study of ML robustness. Their speed, flexibility, and accessibility
    allow researchers to perform wide-ranging evaluations and inform the development
    of fault-tolerant ML architectures. In subsequent sections, we explore the major
    tools in this space, highlighting their capabilities and use cases.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，基于软件的故障注入工具在机器学习鲁棒性研究中发挥着不可或缺的作用。它们的速度、灵活性和可访问性允许研究人员进行广泛的评估，并指导容错机器学习架构的开发。在接下来的章节中，我们将探讨这个领域的主要工具，突出它们的功能和用例。
- en: Software Injection Limitations
  id: totrans-699
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 软件注入限制
- en: While software-based fault injection tools offer significant advantages in terms
    of speed, flexibility, and accessibility, they are not without limitations. These
    constraints can impact the accuracy and realism of fault injection experiments,
    particularly when assessing the robustness of ML systems to real-world hardware
    faults.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于软件的故障注入工具在速度、灵活性和可访问性方面提供了显著的优势，但它们并非没有局限性。这些限制可能会影响故障注入实验的准确性和真实性，尤其是在评估机器学习系统对现实世界硬件故障的鲁棒性时。
- en: One major concern is accuracy. Because software-based tools operate at higher
    levels of abstraction, they may not always capture the full spectrum of effects
    that hardware faults can produce. Low-level hardware interactions, including subtle
    timing errors, voltage fluctuations, and architectural side effects, can be missed
    entirely in high-level simulations. As a result, fault injection studies that
    rely solely on software models may under- or overestimate a system’s true vulnerability
    to certain classes of faults.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的问题是准确性。由于基于软件的工具在更高的抽象级别上运行，它们可能无法始终捕捉到硬件故障可以产生的全部影响。包括微妙的时序错误、电压波动和架构副作用在内的低级硬件交互可能会在高级模拟中完全被忽略。因此，仅依赖于软件模型的故障注入研究可能会低估或高估系统对某些类别故障的真实脆弱性。
- en: Closely related is the issue of fidelity. While software-based methods are often
    designed to emulate specific fault behaviors, the extent to which they reflect
    real-world hardware conditions can vary. For example, simulating a single-bit
    flip in the value of a neural network weight may not fully replicate how that
    same bit error would propagate through memory hierarchies or affect computation
    units on an actual chip. The more abstract the tool, the greater the risk that
    the simulated behavior will diverge from physical behavior under fault conditions.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 与之密切相关的是保真度问题。虽然基于软件的方法通常被设计来模拟特定的故障行为，但它们反映现实世界硬件条件的程度可能会有所不同。例如，模拟神经网络权重值中的单个位翻转可能无法完全复制该位错误如何在内存层次结构中传播或影响实际芯片上的计算单元。工具越抽象，在故障条件下模拟行为与物理行为偏离的风险就越大。
- en: Because software-based tools are easier to modify, they risk unintentionally
    deviating from realistic fault assumptions. This can occur if the chosen fault
    model is overly simplified or not grounded in empirical data from actual hardware
    behavior. As discussed later in the section on bridging the hardware-software
    gap, tools like Fidelity ([Yi He, Balaprakash, and Li 2020](ch058.xhtml#ref-he2020fidelity))
    attempt to address these concerns by aligning software-level models with known
    hardware-level fault characteristics.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于软件的工具更容易修改，它们可能会无意中偏离现实世界的故障假设。这可能发生在所选择的故障模型过于简化或没有基于实际硬件行为的经验数据的情况下。正如在讨论硬件-软件差距的章节中所述，像Fidelity
    ([Yi He, Balaprakash, and Li 2020](ch058.xhtml#ref-he2020fidelity)) 这样的工具试图通过将软件级模型与已知的硬件级故障特征对齐来解决这些担忧。
- en: Despite these limitations, software-based fault injection remains a critical
    part of the ML robustness research toolkit. When used appropriately, particularly
    when used in conjunction with hardware-based validation, these tools provide a
    scalable and efficient way to explore large design spaces, identify vulnerable
    components, and develop mitigation strategies. As fault modeling techniques continue
    to evolve, the integration of hardware-aware insights into software-based tools
    will be key to improving their realism and impact.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些局限性，基于软件的故障注入仍然是机器学习鲁棒性研究工具箱的一个关键部分。当适当使用时，尤其是与基于硬件的验证结合使用时，这些工具提供了一种可扩展且高效的方式来探索大型设计空间，识别易受攻击的组件，并开发缓解策略。随着故障建模技术的不断发展，将硬件感知见解集成到基于软件的工具中将是提高其真实性和影响力的关键。
- en: Software Injection Tool Categories
  id: totrans-705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 软件注入工具分类
- en: Over the past several years, software-based fault injection tools have been
    developed for a wide range of ML frameworks and use cases. These tools vary in
    their level of abstraction, target platforms, and the types of faults they can
    simulate. Many are built to integrate with popular machine learning libraries
    such as PyTorch and TensorFlow, making them accessible to researchers and practitioners
    already working within those ecosystems.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，基于软件的故障注入工具已被开发用于广泛的机器学习框架和用例。这些工具在抽象级别、目标平台和可以模拟的故障类型方面各不相同。许多工具被构建为与流行的机器学习库（如PyTorch和TensorFlow）集成，使得那些已经在这些生态系统中工作的研究人员和实践者能够使用它们。
- en: One of the earliest and most influential tools is Ares ([Reagen et al. 2018](ch058.xhtml#ref-reagen2018ares)),
    initially designed for the Keras framework. Developed at a time when deep neural
    networks (DNNs) were growing in popularity, Ares was one of the first tools to
    systematically explore the effects of hardware faults on DNNs. It provided support
    for injecting single-bit flips and evaluating bit-error rates (BER) across weights
    and activation values. Importantly, Ares was validated against a physical DNN
    accelerator implemented in silicon, demonstrating its relevance for hardware-level
    fault modeling. As the field matured, Ares was extended to support PyTorch, allowing
    researchers to analyze fault behavior in more modern ML settings.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 最早且最具影响力的工具之一是 Ares ([Reagen 等人 2018](ch058.xhtml#ref-reagen2018ares))，最初是为
    Keras 框架设计的。在深度神经网络（DNNs）日益流行的时期开发，Ares 是最早系统地探索硬件故障对 DNNs 影响的工具之一。它提供了支持注入单比特翻转和评估权重和激活值之间的比特错误率（BER）。重要的是，Ares
    通过与在硅中实现的物理 DNN 加速器进行验证，证明了其在硬件级故障建模中的相关性。随着该领域的发展，Ares 被扩展以支持 PyTorch，使研究人员能够在更现代的机器学习环境中分析故障行为。
- en: Building on this foundation, PyTorchFI ([Mahmoud et al. 2020](ch058.xhtml#ref-mahmoud2020pytorchfi))
    was introduced as a dedicated fault injection library for PyTorch. Developed in
    collaboration with Nvidia Research, PyTorchFI allows fault injection into key
    components of ML models, including weights, activations, and gradients. Its native
    support for GPU acceleration makes it especially well-suited for evaluating large
    models efficiently. As shown in [Figure 16.41](ch022.xhtml#fig-phantom-objects),
    even simple bit-level faults can cause severe visual and classification errors,
    including the appearance of ‘phantom’ objects in images, which could have downstream
    safety implications in domains like autonomous driving.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，PyTorchFI ([Mahmoud 等人 2020](ch058.xhtml#ref-mahmoud2020pytorchfi)) 被引入作为一个为
    PyTorch 定制的故障注入库。与 Nvidia 研究团队合作开发，PyTorchFI 允许对机器学习模型的关键组件进行故障注入，包括权重、激活和梯度。它对
    GPU 加速的原生支持使其特别适合高效评估大型模型。如图 [图 16.41](ch022.xhtml#fig-phantom-objects) 所示，即使是简单的位级故障也可能导致严重的视觉和分类错误，包括图像中出现“幽灵”对象，这可能在自动驾驶等领域的下游安全方面产生影响。
- en: '![](../media/file287.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file287.png)'
- en: 'Figure 16.41: **Fault Injection Effects**: Bit-level hardware faults can induce
    phantom objects and misclassifications in machine learning models, potentially
    leading to safety-critical errors in applications like autonomous driving; the
    left image represents correct classification, while the right image presents a
    false positive detection resulting from a single bit flip injected using pytorchfi.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.41：**故障注入效果**：位级硬件故障可能导致机器学习模型中出现“幽灵”对象和误分类，这可能导致自动驾驶等应用中的安全关键错误；左图表示正确分类，而右图展示了一个由
    pytorchfi 注入的单比特翻转导致的假阳性检测。
- en: The modular and accessible design of PyTorchFI has led to its adoption in several
    follow-on projects. For example, PyTorchALFI (developed by Intel xColabs) extends
    PyTorchFI’s capabilities to evaluate system-level safety in automotive applications.
    Similarly, Dr. DNA ([D. Ma et al. 2024](ch058.xhtml#ref-ma2024dr)) from Meta introduces
    a more streamlined, Pythonic API to simplify fault injection workflows. Another
    notable extension is GoldenEye ([Mahmoud et al. 2022](ch058.xhtml#ref-mahmoud2022dsn)),
    which incorporates alternative numeric datatypes, including AdaptivFloat ([Tambe
    et al. 2020](ch058.xhtml#ref-tambe2020algorithm)) and BlockFloat, with bfloat16
    as a specific example, to study the fault tolerance of non-traditional number
    formats under hardware-induced bit errors.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchFI 的模块化和易于访问的设计使其在多个后续项目中得到应用。例如，PyTorchALFI（由 Intel xColabs 开发）扩展了 PyTorchFI
    的功能，以评估汽车应用中的系统级安全性。同样，Meta 的 Dr. DNA ([D. Ma 等人 2024](ch058.xhtml#ref-ma2024dr))
    引入了一个更简洁、Pythonic 的 API，以简化故障注入工作流程。另一个值得注意的扩展是 GoldenEye ([Mahmoud 等人 2022](ch058.xhtml#ref-mahmoud2022dsn))，它结合了替代的数值数据类型，包括
    AdaptivFloat ([Tambe 等人 2020](ch058.xhtml#ref-tambe2020algorithm)) 和 BlockFloat，以
    bfloat16 为例，来研究在硬件引起的位错误下非传统数字格式的容错性。
- en: For researchers working within the TensorFlow ecosystem, TensorFI ([Z. Chen
    et al. 2020](ch058.xhtml#ref-chen2020tensorfi)) provides a parallel solution.
    Like PyTorchFI, TensorFI enables fault injection into the TensorFlow computational
    graph and supports a variety of fault models. One of TensorFI’s strengths is its
    broad applicability—it can be used to evaluate many types of ML models beyond
    DNNs. Additional extensions such as BinFi ([Z. Chen et al. 2019](ch058.xhtml#ref-chen2019sc))
    aim to accelerate the fault injection process by focusing on the most critical
    bits in a model. This prioritization can help reduce simulation time while still
    capturing the most meaningful error patterns.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在 TensorFlow 生态系统内工作的研究人员，TensorFI ([Z. Chen 等人 2020](ch058.xhtml#ref-chen2020tensorfi))
    提供了一种并行解决方案。与 PyTorchFI 类似，TensorFI 允许对 TensorFlow 计算图进行故障注入，并支持多种故障模型。TensorFI
    的一个优势是其广泛的适用性——它可以用于评估许多类型的机器学习模型，而不仅仅是深度神经网络（DNNs）。如 BinFi ([Z. Chen 等人 2019](ch058.xhtml#ref-chen2019sc))
    这样的附加扩展旨在通过关注模型中最关键的位来加速故障注入过程。这种优先级设置可以帮助减少仿真时间，同时仍然捕捉到最有意义的错误模式。
- en: At a lower level of the software stack, NVBitFI ([T. Tsai et al. 2021](ch058.xhtml#ref-tsai2021nvbitfi))
    offers a platform-independent tool for injecting faults directly into GPU assembly
    code. Developed by Nvidia, NVBitFI is capable of performing fault injection on
    any GPU-accelerated application, not just ML workloads. This makes it an especially
    powerful tool for studying resilience at the instruction level, where errors can
    propagate in subtle and complex ways. NVBitFI represents an important complement
    to higher-level tools like PyTorchFI and TensorFI, offering fine-grained control
    over GPU-level behavior and supporting a broader class of applications beyond
    machine learning.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件堆栈的较低层次，NVBitFI ([T. Tsai 等人 2021](ch058.xhtml#ref-tsai2021nvbitfi)) 提供了一个平台无关的工具，可以直接将故障注入到
    GPU 汇编代码中。由 Nvidia 开发的 NVBitFI 能够对任何 GPU 加速的应用程序进行故障注入，而不仅仅是机器学习工作负载。这使得它成为研究指令级别弹性的特别强大的工具，因为错误可以以微妙和复杂的方式传播。NVBitFI
    是 PyTorchFI 和 TensorFI 等高级工具的重要补充，它提供了对 GPU 级别行为的精细控制，并支持机器学习之外的更广泛的应用程序类别。
- en: Together, these tools offer a wide spectrum of fault injection capabilities.
    While some are tightly integrated with high-level ML frameworks for ease of use,
    others enable lower-level fault modeling with higher fidelity. By choosing the
    appropriate tool based on the level of abstraction, performance needs, and target
    application, researchers can tailor their studies to gain more actionable insights
    into the robustness of ML systems. The next section focuses on how these tools
    are being applied in domain-specific contexts, particularly in safety-critical
    systems such as autonomous vehicles and robotics.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具共同提供了广泛的故障注入能力。虽然一些工具与高级机器学习框架紧密集成，以便于使用，但其他工具则允许进行更精细的故障建模，具有更高的保真度。通过根据抽象级别、性能需求和目标应用程序选择适当的工具，研究人员可以根据自己的研究需求定制他们的研究，以获得更多关于机器学习系统鲁棒性的可操作见解。下一节将重点介绍这些工具如何在特定领域环境中应用，特别是在自动驾驶汽车和机器人等关键安全系统。
- en: ML-Specific Injection Tools
  id: totrans-715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机器学习特定注入工具
- en: 'To address the unique challenges posed by specific application domains, researchers
    have developed specialized fault injection tools tailored to different ML systems.
    In high-stakes environments such as autonomous vehicles and robotics, domain-specific
    tools play a crucial role in evaluating system safety and reliability under hardware
    fault conditions. This section highlights three such tools: DriveFI and PyTorchALFI,
    which focus on autonomous vehicles, and MAVFI, which targets uncrewed aerial vehicles
    (UAVs). Each tool enables the injection of faults into mission-critical components,
    including perception, control, and sensor systems, providing researchers with
    insights into how hardware errors may propagate through real-world ML pipelines.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决特定应用领域提出的独特挑战，研究人员已经开发了针对不同机器学习系统的专用故障注入工具。在自动驾驶汽车和机器人等高风险环境中，特定领域的工具在评估系统在硬件故障条件下的安全性和可靠性方面发挥着至关重要的作用。本节重点介绍了三种此类工具：DriveFI
    和 PyTorchALFI，它们专注于自动驾驶汽车，以及 MAVFI，它针对的是无人驾驶飞行器（UAVs）。每个工具都允许将故障注入到任务关键组件中，包括感知、控制和传感器系统，为研究人员提供了关于硬件错误如何通过现实世界的机器学习管道传播的见解。
- en: DriveFI ([S. Jha et al. 2019](ch058.xhtml#ref-jha2019ml)) is a fault injection
    tool developed for autonomous vehicle systems. It facilitates the injection of
    hardware faults into the perception and control pipelines, enabling researchers
    to study how such faults affect system behavior and safety. Notably, DriveFI integrates
    with industry-standard platforms like Nvidia DriveAV and Baidu Apollo, offering
    a realistic environment for testing. Through this integration, DriveFI enables
    practitioners to evaluate the end-to-end resilience of autonomous vehicle architectures
    in the presence of fault conditions.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: DriveFI ([S. Jha et al. 2019](ch058.xhtml#ref-jha2019ml)) 是为自动驾驶系统开发的故障注入工具。它促进了硬件故障注入到感知和控制管道中，使研究人员能够研究这些故障如何影响系统行为和安全。值得注意的是，DriveFI
    与行业标准平台（如 Nvidia DriveAV 和百度 Apollo）集成，提供了一个真实的测试环境。通过这种集成，DriveFI 使从业者能够在存在故障条件的情况下评估自动驾驶架构的端到端弹性。
- en: PyTorchALFI ([Gräfe et al. 2023](ch058.xhtml#ref-grafe2023large)) extends the
    capabilities of PyTorchFI for use in the autonomous vehicle domain. Developed
    by Intel xColabs, PyTorchALFI enhances the underlying fault injection framework
    with domain-specific features. These include the ability to inject faults into
    multimodal sensor data[69](#fn69), such as inputs from cameras and LiDAR systems.
    This allows for a deeper examination of how perception systems in autonomous vehicles
    respond to underlying hardware faults, further refining our understanding of system
    vulnerabilities and potential failure modes.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchALFI ([Gräfe et al. 2023](ch058.xhtml#ref-grafe2023large)) 扩展了 PyTorchFI
    的功能，使其适用于自动驾驶领域。由英特尔 xColabs 开发的 PyTorchALFI 通过添加特定领域的功能增强了底层故障注入框架。这些功能包括将故障注入到多模态传感器数据中[69](#fn69)，例如来自摄像头和激光雷达系统的输入。这允许我们更深入地研究自动驾驶车辆的感知系统如何对底层硬件故障做出反应，进一步细化我们对系统脆弱性和潜在故障模式的了解。
- en: MAVFI ([Hsiao et al. 2023](ch058.xhtml#ref-hsiao2023mavfi)) is a domain-specific
    fault injection framework tailored for robotics applications, particularly uncrewed
    aerial vehicles. Built atop the Robot Operating System (ROS), MAVFI provides a
    modular and extensible platform for injecting faults into various UAV subsystems,
    including sensors, actuators, and flight control algorithms. By assessing how
    injected faults impact flight stability and mission success, MAVFI offers a practical
    means for developing and validating fault-tolerant UAV architectures.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: MAVFI ([Hsiao et al. 2023](ch058.xhtml#ref-hsiao2023mavfi)) 是一个针对机器人应用，尤其是无人机的特定领域故障注入框架。基于机器人操作系统
    (ROS) 构建，MAVFI 为将故障注入各种无人机子系统（包括传感器、执行器和飞行控制算法）提供了一个模块化和可扩展的平台。通过评估注入的故障如何影响飞行稳定性和任务成功，MAVFI
    为开发和验证容错无人机架构提供了一种实用的方法。
- en: Together, these tools demonstrate the growing sophistication of fault injection
    research across application domains. By enabling fine-grained control over where
    and how faults are introduced, domain-specific tools provide actionable insights
    that general-purpose frameworks may overlook. Their development has greatly expanded
    the ML community’s capacity to design and evaluate resilient systems—particularly
    in contexts where reliability, safety, and real-time performance are critical.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具共同展示了故障注入研究在各个应用领域的日益复杂化。通过允许对故障引入的位置和方式进行精细控制，特定领域的工具提供了通用框架可能忽略的可操作见解。它们的发展极大地扩大了机器学习社区设计和管理弹性系统的能力——特别是在可靠性、安全和实时性能至关重要的环境中。
- en: Bridging Hardware-Software Gap
  id: totrans-721
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 桥接软硬件差距
- en: 'While software-based fault injection tools offer many advantages in speed,
    flexibility, and accessibility, they do not always capture the full range of effects
    that hardware faults can impose on a system. This is largely due to the abstraction
    gap: software-based tools operate at a higher level and may overlook low-level
    hardware interactions or nuanced error propagation mechanisms that influence the
    behavior of ML systems in critical ways.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于软件的故障注入工具在速度、灵活性和可访问性方面具有许多优势，但它们并不总是能够捕捉到硬件故障对系统可能产生的影响的全部范围。这主要是因为抽象差距：基于软件的工具在更高的层面上运行，可能会忽略低级硬件交互或影响机器学习系统行为的微妙错误传播机制。
- en: 'As discussed in the work by ([Bolchini et al. 2023](ch058.xhtml#ref-bolchini2022fast)),
    hardware faults can exhibit complex spatial distribution patterns that are difficult
    to replicate using purely software-based fault models. They identify four characteristic
    fault propagation patterns: single point, where the fault corrupts a single value
    in a feature map; same row, where a partial or entire row in a feature map is
    corrupted; bullet wake, where the same location across multiple feature maps is
    affected; and shatter glass, a more complex combination of both same row and bullet
    wake behaviors. These diverse patterns, visualized in [Figure 16.42](ch022.xhtml#fig-hardware-errors-bolchini),
    highlight the limits of simplistic injection strategies and emphasize the need
    for hardware-aware modeling when evaluating ML system robustness.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 如([Bolchini 等人 2023](ch058.xhtml#ref-bolchini2022fast))的工作所述，硬件故障可以表现出复杂的空间分布模式，这些模式难以仅使用基于软件的故障模型来复制。他们确定了四种特征故障传播模式：单点，其中故障会破坏特征图中的一个值；同一行，其中特征图中的一行部分或全部被破坏；子弹尾波，其中多个特征图中相同的位置受到影响；以及破碎玻璃，这是同一行和子弹尾波行为的更复杂组合。这些不同的模式，如图[图16.42](ch022.xhtml#fig-hardware-errors-bolchini)所示，突出了简单注入策略的局限性，并强调了在评估机器学习系统鲁棒性时进行硬件感知建模的必要性。
- en: '![](../media/file288.svg)'
  id: totrans-724
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file288.svg)'
- en: 'Figure 16.42: **Hardware Fault Patterns**: Dnns exhibit distinct error manifestations
    from hardware faults, categorized by their spatial distribution across feature
    maps and layers. These patterns—single point, same row, bullet wake, and shatter
    glass—represent localized versus widespread corruption, impacting model predictions
    and highlighting the need for fault-tolerant system design. Source: ([Bolchini
    et al. 2023](ch058.xhtml#ref-bolchini2022fast)).'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.42：**硬件故障模式**：Dnns表现出从硬件故障中独特的错误表现，按其在特征图和层之间的空间分布进行分类。这些模式——单点、同一行、子弹尾波和破碎玻璃——代表了局部与广泛破坏，影响模型预测并强调了容错系统设计的必要性。来源：([Bolchini
    等人 2023](ch058.xhtml#ref-bolchini2022fast))。
- en: To address this abstraction gap, researchers have developed tools that explicitly
    aim to map low-level hardware error behavior to software-visible effects. One
    such tool is Fidelity, which bridges this gap by studying how hardware-level faults
    propagate and become observable at higher software layers. The next section discusses
    Fidelity in more detail.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个抽象差距，研究人员开发了旨在明确地将底层硬件错误行为映射到软件可见效果的工具。其中一个这样的工具是Fidelity，它通过研究硬件级别的故障如何传播并在更高软件层变得可观察来弥合这一差距。下一节将更详细地讨论Fidelity。
- en: Simulation Fidelity Challenges
  id: totrans-727
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模拟忠实度挑战
- en: Fidelity ([Yi He, Balaprakash, and Li 2020](ch058.xhtml#ref-he2020fidelity))
    is a tool designed to model hardware faults more accurately within software-based
    fault injection experiments. Its core goal is to bridge the gap between low-level
    hardware fault behavior and the higher-level effects observed in machine learning
    systems by simulating how faults propagate through the compute stack.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: Fidelity（[叶，Balaprakash 和 李 2020](ch058.xhtml#ref-he2020fidelity)）是一个工具，旨在在基于软件的故障注入实验中更准确地模拟硬件故障。其核心目标是通过模拟故障如何在计算堆栈中传播，来弥合底层硬件故障行为与机器学习系统中观察到的更高层次效应之间的差距。
- en: 'The central insight behind Fidelity is that not all faults need to be modeled
    individually at the hardware level to yield meaningful results. Instead, Fidelity
    focuses on how faults manifest at the software-visible state and identifies equivalence
    relationships that allow representative modeling of entire fault classes. To accomplish
    this, it relies on several key principles:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: Fidelity（忠实度）背后的核心洞察是，并非所有故障都需要在硬件级别单独建模才能产生有意义的结果。相反，Fidelity关注故障如何在软件可见状态下表现，并识别出允许对整个故障类进行代表性建模的等价关系。为此，它依赖于几个关键原则：
- en: First, fault propagation is studied to understand how a fault originating in
    hardware can move through various layers, including architectural registers, memory
    hierarchies, and numerical operations, eventually altering values in software.
    Fidelity captures these pathways to ensure that injected faults in software reflect
    the way faults would actually manifest in a real system.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，研究故障传播以了解一个起源于硬件的故障如何通过各种层移动，包括架构寄存器、内存层次结构和数值运算，最终改变软件中的值。Fidelity捕获这些路径以确保注入到软件中的故障反映了故障在实际系统中实际表现的方式。
- en: Second, the tool identifies fault equivalence, which refers to grouping hardware
    faults that lead to similar observable outcomes in software. By focusing on representative
    examples rather than modeling every possible hardware bit flip individually, Fidelity
    allows more efficient simulations without sacrificing accuracy.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，该工具识别了故障等价性，这指的是将导致软件中类似可观察结果的硬件故障分组。通过关注代表性示例而不是单独对每个可能的硬件位翻转进行建模，Fidelity允许更高效的模拟，同时不牺牲准确性。
- en: Finally, Fidelity uses a layered modeling approach, capturing the system’s behavior
    at various abstraction levels—from hardware fault origin to its effect in the
    ML model’s weights, activations, or predictions. This layering ensures that the
    impact of hardware faults is realistically simulated in the context of the ML
    system.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Fidelity采用分层建模方法，捕捉系统在各个抽象层次上的行为——从硬件故障的起源到其在机器学习模型权重、激活或预测中的影响。这种分层确保了在机器学习系统的背景下，硬件故障的影响得到真实模拟。
- en: By combining these techniques, Fidelity allows researchers to run fault injection
    experiments that closely mirror the behavior of real hardware systems, but with
    the efficiency and flexibility of software-based tools. This makes Fidelity especially
    valuable in safety-critical settings, where the cost of failure is high and an
    accurate understanding of hardware-induced faults is essential.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些技术，Fidelity允许研究人员运行与真实硬件系统行为紧密相似的故障注入实验，但具有基于软件工具的效率和灵活性。这使得Fidelity在成本高昂且对硬件引起的故障有准确理解至关重要的关键安全设置中特别有价值。
- en: Hardware Behavior Modeling
  id: totrans-734
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件行为建模
- en: Capturing the true behavior of hardware faults in software-based fault injection
    tools is critical for advancing the reliability and robustness of ML systems.
    This fidelity becomes especially important when hardware faults have subtle but
    significant effects that may not be evident when modeled at a high level of abstraction.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于软件的故障注入工具中捕捉硬件故障的真实行为对于提高机器学习系统的可靠性和鲁棒性至关重要。当硬件故障具有微妙但重大的影响，而这些影响在高级抽象层次上建模时可能不明显时，这种精确度变得尤为重要。
- en: Several reasons explain why accurately reflecting hardware behavior is essential.
    First, accuracy is paramount. Software-based tools that mirror the actual propagation
    and manifestation of hardware faults provide more dependable insights into how
    faults influence model behavior. These insights are crucial for designing and
    validating fault-tolerant architectures and ensuring that mitigation strategies
    are grounded in realistic system behavior.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 几个原因解释了为什么准确反映硬件行为是至关重要的。首先，准确性至关重要。基于软件的工具，能够反映硬件故障的实际传播和表现，提供了对故障如何影响模型行为的更可靠的见解。这些见解对于设计和验证容错架构以及确保缓解策略基于现实系统行为至关重要。
- en: Second, reproducibility is improved when hardware effects are faithfully captured.
    This allows fault injection results to be reliably reproduced across different
    systems and environments, which is a cornerstone of rigorous scientific research.
    Researchers can better compare results, validate findings, and ensure consistency
    across studies.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，当硬件效果得到忠实捕捉时，可重复性得到提高。这允许在不同系统和环境中可靠地重现故障注入结果，这是严格科学研究的基石。研究人员可以更好地比较结果，验证发现，并确保研究之间的一致性。
- en: Third, efficiency is enhanced when fault models focus on the most representative
    and impactful fault scenarios. Rather than exhaustively simulating every possible
    bit flip, tools can target a subset of faults that are known, through accurate
    modeling, to affect the system in meaningful ways. This selective approach saves
    computational resources while still providing thorough insights.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，当故障模型专注于最具代表性和影响力的故障场景时，效率会得到提升。而不是全面模拟每个可能的位翻转，工具可以针对已知通过精确建模影响系统的故障子集。这种选择性方法在节省计算资源的同时，仍然提供深入的见解。
- en: Finally, understanding how hardware faults appear at the software level is essential
    for designing effective mitigation strategies. When researchers know how specific
    hardware-level issues affect different components of an ML system, they can develop
    more targeted hardening techniques—such as retraining specific layers, applying
    redundancy selectively, or improving architectural resilience in bottleneck components.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，理解硬件故障在软件层面的出现方式对于设计有效的缓解策略至关重要。当研究人员知道特定硬件级别的问题如何影响机器学习系统的不同组件时，他们可以开发更具有针对性的加固技术——例如重新训练特定层、选择性应用冗余或提高瓶颈组件的架构弹性。
- en: Tools like Fidelity are central to this effort. By establishing mappings between
    low-level hardware behavior and higher-level software effects, Fidelity and similar
    tools empower researchers to conduct fault injection experiments that are not
    only faster and more scalable, but also grounded in real-world system behavior.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如Fidelity是这项工作的核心。通过在低级硬件行为和高级软件效果之间建立映射，Fidelity和类似工具使研究人员能够进行故障注入实验，这些实验不仅更快、更可扩展，而且基于现实世界的系统行为。
- en: As ML systems continue to increase in scale and are deployed in increasingly
    safety-critical environments, this kind of hardware-aware modeling will become
    even more important. Ongoing research in this space aims to further refine the
    translation between hardware and software fault models and to develop tools that
    offer both efficiency and realism in evaluating ML system resilience. These advances
    will provide the community with more powerful, reliable methods for understanding
    and defending against the effects of hardware faults.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统规模的不断扩大，并在越来越关键的安全生产环境中部署，这种硬件感知建模将变得更加重要。该领域的研究持续进行，旨在进一步细化硬件和软件故障模型之间的翻译，并开发出在评估机器学习系统弹性方面既高效又真实的工具。这些进步将为社区提供更强大、更可靠的方法来理解和防御硬件故障的影响。
- en: Fallacies and Pitfalls
  id: totrans-742
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fallacies and Pitfalls
- en: The complexity and interconnected nature of robustness threats often leads to
    misconceptions about effective defense strategies, particularly around the assumption
    that robustness techniques provide universal protection without trade-offs or
    limitations.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性威胁的复杂性和相互关联性往往导致对有效防御策略的误解，尤其是在鲁棒性技术提供无权衡或限制的普遍保护的假设方面。
- en: '**Fallacy:** *Adversarial robustness can be achieved through defensive techniques
    without trade-offs.*'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *通过防御技术可以实现对抗鲁棒性，而不需要权衡。*'
- en: This misconception leads teams to believe that robustness techniques like adversarial
    training or input preprocessing provide complete protection without costs. Adversarial
    defenses often introduce significant trade-offs including reduced clean accuracy,
    increased computational overhead, or brittleness to new attack methods. Many defensive
    techniques that appear effective against specific attacks fail when evaluated
    against stronger or adaptive adversaries. The arms race between attacks and defenses
    means that robustness is not a solved problem but an ongoing engineering challenge
    that requires continuous adaptation and evaluation against evolving threats.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解使团队相信，像对抗性训练或输入预处理这样的鲁棒性技术可以提供无成本的完全保护。对抗性防御通常会引入重大的权衡，包括降低清洁精度、增加计算开销或对新攻击方法的脆弱性。许多在特定攻击中看似有效的防御技术，在对抗更强或自适应的对手时却失败了。攻击和防御之间的军备竞赛意味着鲁棒性不是一个已解决的问题，而是一个持续的工程挑战，需要不断适应和评估不断发展的威胁。
- en: '**Pitfall:** *Testing robustness only against known attack methods rather than
    comprehensive threat modeling.*'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *仅针对已知攻击方法测试鲁棒性，而不是进行全面的威胁建模。*'
- en: Many practitioners evaluate model robustness by testing against a few standard
    adversarial attacks without considering the full spectrum of potential threats.
    This approach provides false confidence when models perform well against limited
    test cases but fail catastrophically against novel attack vectors. Real-world
    threats include not only sophisticated adversarial examples but also hardware
    faults, data corruption, distribution shifts, and software vulnerabilities that
    may not resemble academic attack scenarios. Comprehensive robustness evaluation
    requires systematic threat modeling that considers the full attack surface rather
    than focusing on a narrow set of known vulnerabilities.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者通过针对少数标准对抗性攻击进行测试来评估模型的鲁棒性，而没有考虑到潜在威胁的全谱。这种方法在模型对有限的测试案例表现良好时提供了虚假的信心，但在面对新颖的攻击向量时却会灾难性地失败。现实世界的威胁不仅包括复杂的对抗性示例，还包括硬件故障、数据损坏、分布偏移和可能不像学术攻击场景的软件漏洞。全面的鲁棒性评估需要系统性的威胁建模，考虑完整的攻击面，而不是仅仅关注已知的一组漏洞。
- en: '**Fallacy:** *Distribution shift can be solved by collecting more diverse training
    data.*'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *通过收集更多样化的训练数据可以解决分布偏移问题。*'
- en: This belief assumes that dataset diversity alone ensures robustness to distribution
    shifts encountered in deployment. While diverse training data helps, it cannot
    anticipate all possible distribution changes that occur in dynamic real-world
    environments. Training datasets remain inherently limited compared to the infinite
    variety of deployment conditions. Some distribution shifts are inherently unpredictable,
    emerging from changing user behavior, evolving data sources, or external environmental
    factors. Effective robustness requires adaptive systems with monitoring, detection,
    and response capabilities rather than relying solely on comprehensive training
    data.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设仅凭数据集多样性就能确保对部署中遇到的分布偏移的鲁棒性。虽然多样化的训练数据有所帮助，但它无法预测动态现实环境中可能发生的所有可能的分布变化。与无限的部署条件相比，训练数据集本质上仍然是有限的。一些分布偏移本质上是不可预测的，它们可能源于用户行为的变化、数据源的演变或外部环境因素。有效的鲁棒性需要具有监控、检测和响应能力的自适应系统，而不是仅仅依赖于全面训练数据。
- en: '**Pitfall:** *Assuming that robustness techniques designed for one threat category
    protect against all failure modes.*'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *假设为某一威胁类别设计的鲁棒性技术可以保护所有故障模式。*'
- en: Teams often apply robustness techniques developed for specific threats without
    understanding their limitations against other failure modes. Adversarial training
    designed for gradient-based attacks may not improve robustness against hardware
    faults or data poisoning. Similarly, techniques that handle benign distribution
    shifts might fail against adversarial distribution shifts designed to exploit
    model weaknesses. Each threat category requires specialized defenses, and effective
    robustness necessitates layered protection strategies that address the full spectrum
    of potential failures rather than assuming cross-domain effectiveness.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 团队通常在没有理解它们对其他故障模式限制的情况下，应用针对特定威胁开发的鲁棒性技术。针对基于梯度的攻击设计的对抗性训练可能不会提高对硬件故障或数据中毒的鲁棒性。同样，处理良性分布偏移的技术可能无法应对旨在利用模型弱点的对抗性分布偏移。每个威胁类别都需要专门的防御措施，有效的鲁棒性需要分层保护策略，以解决潜在故障的全谱系，而不是假设跨域的有效性。
- en: '**Fallacy:** *Different failure modes operate independently and can be addressed
    in isolation.*'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *不同的故障模式独立运作，可以单独解决。*'
- en: 'This assumption overlooks the complex interactions between different fault
    types that can create compound vulnerabilities exceeding the sum of individual
    threats. Real-world failures often involve cascading effects where one vulnerability
    enables or amplifies others. Consider these compound scenarios:'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 这种假设忽略了不同故障类型之间可能产生的复杂交互，这些交互可以创建超过单个威胁总和的复合漏洞。现实世界的故障往往涉及级联效应，其中一种漏洞使其他漏洞得以启用或放大。考虑以下复合场景：
- en: Hardware-adversarial interactions illustrate how bit flips in model weights
    can inadvertently create adversarial vulnerabilities not present in the original
    model. An attacker discovering these corruptions could craft targeted adversarial
    examples that exploit the specific weight perturbations, achieving 95% attack
    success rates compared to 20% on uncorrupted models. Conversely, adversarial training
    meant to improve robustness increases model complexity by 2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>, raising the probability
    of hardware faults due to increased memory and computation requirements.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件对抗性交互说明了模型权重中的位翻转如何意外地创建出原始模型中不存在的对抗性漏洞。攻击者发现这些破坏后，可以构建针对特定权重扰动的针对性对抗示例，实现95%的攻击成功率，而未受破坏的模型攻击成功率仅为20%。相反，旨在提高鲁棒性的对抗性训练通过增加2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>模型复杂性，提高了由于内存和计算需求增加而导致的硬件故障的概率。
- en: Environmental-software cascades occur when gradual distribution shift may go
    undetected due to bugs in monitoring software that fail to log outlier samples.
    As the shift progresses over 3-6 months, the model’s accuracy degrades by 40%,
    but the faulty monitoring system reports normal operation. When finally discovered,
    the compounded data drift and delayed detection require complete model retraining
    rather than incremental adaptation, incurring 10<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> higher recovery costs.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 环境软件级联发生在由于监控软件中的错误导致无法记录异常样本，从而可能未被发现逐渐的分布变化时。随着变化在3-6个月内进展，模型的准确性下降40%，但故障监控系统报告正常操作。当最终被发现时，累积的数据漂移和延迟检测需要重新训练整个模型，而不是增量适应，导致10<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>更高的恢复成本。
- en: Attack-enabled distribution exploitation involves an adversary observing natural
    distribution shift in a deployed system and crafting poisoning attacks that accelerate
    the drift in specific directions. By injecting just 0.1% poisoned samples that
    align with natural drift patterns, attackers can cause 5<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> faster performance
    degradation while evading detection systems calibrated for either pure adversarial
    or pure drift scenarios.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击性分布利用涉及攻击者观察部署系统中自然分布的变化，并构建加速特定方向漂移的中毒攻击。通过注入仅与自然漂移模式一致的0.1%中毒样本，攻击者可以导致5<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>更快的性能退化，同时避开针对纯对抗或纯漂移场景校准的检测系统。
- en: Triple-threat scenarios demonstrate the most severe compound vulnerabilities.
    Consider an autonomous vehicle where cosmic ray-induced bit flips corrupt perception
    model weights, adversarial road markings exploit these corruptions, and seasonal
    weather changes create distribution shift. The combination results in 85% misclassification
    of stop signs under specific conditions, while each individual threat would cause
    only 15-20% degradation.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 三重威胁场景展示了最严重的复合漏洞。考虑一个自动驾驶汽车，其中宇宙射线引起的位翻转破坏了感知模型权重，对抗性道路标记利用这些破坏，季节性天气变化造成分布漂移。这种组合在特定条件下导致85%的停车标志误分类，而每个单独的威胁只会造成15-20%的退化。
- en: These compound scenarios demonstrate that robust AI systems must consider threat
    interactions through comprehensive failure mode analysis, cross-domain testing
    that evaluates combined vulnerabilities, and defense strategies that account for
    cascading failures rather than treating each threat in isolation.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 这些复合场景表明，鲁棒的人工智能系统必须通过全面的故障模式分析、跨领域测试来评估综合漏洞，以及考虑级联故障的防御策略，而不是孤立地处理每个威胁。
- en: Summary
  id: totrans-759
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter established robust AI as a core requirement for reliable machine
    learning systems operating in real-world environments. Through examination of
    concrete failures across cloud, edge, and embedded deployments, we demonstrated
    that robustness challenges span multiple dimensions and require systematic approaches
    to detection, mitigation, and recovery.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将鲁棒人工智能确立为在现实世界环境中运行的可靠机器学习系统的核心要求。通过检查云、边缘和嵌入式部署中的具体故障，我们证明了鲁棒性挑战跨越多个维度，需要系统性的检测、缓解和恢复方法。
- en: The unified framework developed here organizes robustness challenges into three
    interconnected pillars that share common principles while requiring specialized
    approaches. System-level faults address the physical substrate reliability that
    underlies all ML computations, from transient cosmic ray effects to permanent
    hardware degradation. Input-level attacks encompass deliberate attempts to manipulate
    model behavior through adversarial examples and data poisoning techniques. Environmental
    shifts represent the natural evolution of deployment conditions that challenge
    static model assumptions through distribution drift and concept changes.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里开发的统一框架将鲁棒性挑战组织成三个相互关联的支柱，这些支柱共享共同的原则，同时需要专门的解决方案。系统级故障解决所有机器学习计算背后的物理基础可靠性问题，从瞬态宇宙射线效应到永久性硬件退化。输入级攻击包括通过对抗性示例和数据中毒技术故意操纵模型行为的尝试。环境变化代表部署条件的自然演变，通过分布漂移和概念变化挑战静态模型假设。
- en: Across these three pillars, robust AI systems implement common principles of
    detection and monitoring to identify threats before they impact system behavior,
    graceful degradation to maintain core functionality under stress, and adaptive
    response to adjust system behavior based on detected conditions. These principles
    manifest differently across pillar types but provide a unified foundation for
    building comprehensive robustness solutions.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个支柱中，稳健人工智能系统实施检测和监控的通用原则，以在影响系统行为之前识别威胁，优雅降级以在压力下保持核心功能，以及根据检测到的条件调整系统行为的自适应响应。这些原则在支柱类型之间表现不同，但为构建全面的稳健性解决方案提供了一个统一的基础。
- en: The practical implementation of robust AI requires integration across the entire
    ML pipeline, from data collection through deployment and monitoring. Hardware
    fault tolerance mechanisms must coordinate with adversarial defenses and drift
    detection systems to provide comprehensive protection. This robustness foundation
    establishes the reliability guarantees necessary for the operational frameworks
    detailed in [Chapter 13](ch019.xhtml#sec-ml-operations), where these fault-tolerant
    systems will be deployed, monitored, and maintained at scale. Without the comprehensive
    reliability mechanisms developed here, the operational workflows in the next chapter
    would lack the fundamental resilience required for production deployment.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健人工智能的实用实施需要在整个机器学习（ML）管道中实现集成，从数据收集到部署和监控。硬件容错机制必须与对抗性防御和漂移检测系统协调一致，以提供全面保护。这个稳健性基础为[第13章](ch019.xhtml#sec-ml-operations)中详细说明的操作框架提供了必要的可靠性保证，在这些框架中，这些容错系统将得到部署、监控和维护。如果没有在这里开发的综合可靠性机制，下一章中的操作工作流程将缺乏生产部署所需的基本弹性。
- en: '[Table 16.7](ch022.xhtml#tbl-robustness-summary) provides a practical reference
    mapping each of the three main fault categories to their primary detection and
    mitigation strategies, serving as an engineering guide for implementing comprehensive
    robustness solutions:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '[表16.7](ch022.xhtml#tbl-robustness-summary)提供了一个实用的参考，将三个主要故障类别映射到它们的主要检测和缓解策略，作为实施全面稳健性解决方案的工程指南：'
- en: 'Table 16.7: **Robustness Strategy Reference**: A practical mapping of fault
    categories to their primary detection and mitigation approaches, providing engineers
    with a systematic framework for implementing comprehensive robustness solutions
    across the three pillars of robust AI.'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.7：**稳健性策略参考**：将故障类别映射到它们的主要检测和缓解方法的一个实用映射，为工程师提供了一个系统框架，用于在稳健人工智能的三个支柱上实施全面的稳健性解决方案。
- en: '| **Fault Category** | **Detection Methods** | **Mitigation Strategies** |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| **故障类别** | **检测方法** | **缓解策略** |'
- en: '| --- | --- | --- |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **System-Level** | ECC Memory | Redundancy (TMR/DMR) |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| **系统级别** | ECC 内存 | 冗余（TMR/DMR） |'
- en: '| **Faults** | BIST (Built-In Self-Test) Watchdog Timers Voltage/Temperature
    Monitoring | Checkpointing Hardware Redundancy Error Correction Codes |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| **故障** | BIST（内置自检）看门狗定时器电压/温度监控 | 检查点硬件冗余错误纠正码 |'
- en: '| **Input-Level** | Input Sanitization | Adversarial Training |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| **输入级别** | 输入净化 | 对抗性训练 |'
- en: '| **Attacks** | Anomaly Detection Statistical Testing Behavioral Analysis |
    Defensive Distillation Input Preprocessing Model Ensembles |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| **攻击** | 异常检测统计测试行为分析 | 防御蒸馏输入预处理模型集成 |'
- en: '| **Environmental** | Statistical Monitoring (MMD, PSI) | Continuous Learning
    |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| **环境** | 统计监控（MMD，PSI） | 持续学习 |'
- en: '| **Shifts** | Distribution Comparison Performance Degradation Tracking Concept
    Drift Detection | Model Retraining Adaptive Thresholds Ensemble Methods |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| **变化** | 分布比较性能退化跟踪概念漂移检测 | 模型重新训练自适应阈值集成方法 |'
- en: '**Key Takeaways**'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: 'Robust AI systems must address three interconnected threat categories: system-level
    faults, input-level attacks, and environmental shifts'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳健人工智能系统必须解决三个相互关联的威胁类别：系统级故障、输入级攻击和环境变化。
- en: Common principles of detection, graceful degradation, and adaptive response
    apply across all threat types while requiring specialized implementations
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测、优雅降级和自适应响应的通用原则适用于所有威胁类型，同时需要专门的实现。
- en: Hardware reliability directly impacts ML performance, with single-bit errors
    capable of degrading model accuracy by 10-50%
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件可靠性直接影响机器学习性能，单比特错误能够使模型精度降低10-50%。
- en: Real-world robustness requires integration across the entire ML pipeline rather
    than isolated protection mechanisms
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的稳健性需要在整个机器学习管道中实现集成，而不是孤立的保护机制。
- en: Modern AI deployments require systematic approaches to robustness evaluation
    and mitigation
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代人工智能部署需要系统性的稳健性评估和缓解方法
- en: Building on these robustness foundations, the following chapters examine complementary
    aspects of trustworthy AI systems. Privacy and security considerations ([Chapter 15](ch021.xhtml#sec-security-privacy))
    layer additional operational requirements onto robust deployment infrastructure,
    requiring specialized techniques for protecting sensitive data while maintaining
    system reliability. The principles developed here for detecting and responding
    to threats provide foundational patterns that extend to privacy-preserving and
    secure AI system design, creating comprehensive frameworks for trustworthy AI
    deployment across diverse environments and applications.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些稳健性基础上，以下章节将探讨可信赖人工智能系统的互补方面。隐私和安全考虑因素([第15章](ch021.xhtml#sec-security-privacy))在稳健部署基础设施上增加了额外的运营要求，需要专门的技术来保护敏感数据同时保持系统可靠性。这里开发的检测和应对威胁的原则为隐私保护和安全人工智能系统设计提供了基础模式，创建了适用于不同环境和应用的全面框架，以实现可信赖人工智能的部署。
- en: Building robust AI systems requires embedding robustness considerations throughout
    the development process, from initial design through deployment and maintenance,
    validated through systematic evaluation methods detailed in [Chapter 12](ch018.xhtml#sec-benchmarking-ai)
    and aligned with responsible AI principles from [Chapter 17](ch023.xhtml#sec-responsible-ai).
    Critical applications in autonomous vehicles, medical devices, and infrastructure
    systems demand proactive approaches that anticipate failure modes and implement
    extensive safeguards. The challenge extends beyond individual components to encompass
    system-level interactions, requiring comprehensive approaches that ensure reliable
    operation under diverse and evolving conditions encountered in real-world deployments
    while considering the sustainability implications of robust system design covered
    in [Chapter 18](ch024.xhtml#sec-sustainable-ai).
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 构建稳健的人工智能系统需要在整个开发过程中嵌入稳健性考虑，从初始设计到部署和维护，通过[第12章](ch018.xhtml#sec-benchmarking-ai)中详细说明的系统评估方法进行验证，并与[第17章](ch023.xhtml#sec-responsible-ai)中提到的负责任人工智能原则保持一致。在自动驾驶汽车、医疗设备和基础设施系统等关键应用中，需要采取主动方法来预测故障模式并实施广泛的安全保障措施。这一挑战不仅涉及单个组件，还包括系统级交互，需要确保在现实世界部署中遇到的各种和不断变化条件下可靠运行的综合方法，同时考虑[第18章](ch024.xhtml#sec-sustainable-ai)中涵盖的稳健系统设计的可持续性影响。
- en: '* * *'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
