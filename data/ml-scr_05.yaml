- en: Concept
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c1/concept.html](https://dafriedman97.github.io/mlbook/content/c1/concept.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/c1/concept.html](https://dafriedman97.github.io/mlbook/content/c1/concept.html)
- en: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\btheta}{\boldsymbol{\theta}}
    \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}} \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
    \newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\by}{\mathbf{y}}
    \newcommand{\bX}{\mathbf{X}} \newcommand{\bphi}{\boldsymbol{\phi}} \newcommand{\bPhi}{\boldsymbol{\Phi}}
    \]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\btheta}{\boldsymbol{\theta}}
    \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}} \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
    \newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\by}{\mathbf{y}}
    \newcommand{\bX}{\mathbf{X}} \newcommand{\bphi}{\boldsymbol{\phi}} \newcommand{\bPhi}{\boldsymbol{\Phi}}
    \]'
- en: Model Structure
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型结构
- en: '*Linear regression* is a relatively simple method that is extremely widely-used.
    It is also a great stepping stone for more sophisticated methods, making it a
    natural algorithm to study first.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归* 是一种相对简单的方法，它被极其广泛地使用。它也是更复杂方法的良好基石，使其成为首先研究的自然算法。'
- en: In linear regression, the target variable \(y\) is assumed to follow a linear
    function of one or more predictor variables, \(x_1, \dots, x_D\), plus some random
    error. Specifically, we assume the model for the \(n^\text{th}\) observation in
    our sample is of the form
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，目标变量 \(y\) 被假设为遵循一个或多个预测变量 \(x_1, \dots, x_D\) 的线性函数，加上一些随机误差。具体来说，我们假设样本中第
    \(n^\text{th}\) 个观察的模型形式如下
- en: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
- en: Here \(\beta_0\) is the intercept term, \(\beta_1\) through \(\beta_D\) are
    the coefficients on our feature variables, and \(\epsilon\) is an error term that
    represents the difference between the true \(y\) value and the linear function
    of the predictors. Note that the terms with an \(n\) in the subscript differ between
    observations while the terms without (namely the \(\beta\text{s}\)) do not.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(\beta_0\) 是截距项，\(\beta_1\) 到 \(\beta_D\) 是特征变量的系数，\(\epsilon\) 是一个误差项，表示真实
    \(y\) 值与预测变量的线性函数之间的差异。注意，下标带有 \(n\) 的项在不同观察之间是不同的，而下标不带 \(n\) 的项（即 \(\beta\)）是不变的。
- en: 'The math behind linear regression often becomes easier when we use vectors
    to represent our predictors and coefficients. Let’s define \(\bx_n\) and \(\bbeta\)
    as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们使用向量来表示我们的预测变量和系数时，线性回归背后的数学往往变得更简单。让我们如下定义 \(\bx_n\) 和 \(\bbeta\):'
- en: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
- en: Note that \(\bx_n\) includes a leading 1, corresponding to the intercept term
    \(\beta_0\). Using these definitions, we can equivalently express \(y_n\) as
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，\(\bx_n\) 包含一个前导的 1，对应于截距项 \(\beta_0\)。使用这些定义，我们可以等价地表示 \(y_n\) 为
- en: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
- en: Below is an example of a dataset designed for linear regression. The input variable
    is generated randomly and the target variable is generated as a linear combination
    of that input variable plus an error term.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个为线性回归设计的示例数据集。输入变量是随机生成的，目标变量是输入变量的线性组合加上一个误差项。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
- en: Parameter Estimation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数估计
- en: The previous section covers the entire structure we assume our data follows
    in linear regression. The machine learning task is then to estimate the parameters
    in \(\bbeta\). These estimates are represented by \(\hat{\beta}_0, \dots, \hat{\beta}_D\)
    or \(\bbetahat\). The estimates give us *fitted values* for our target variable,
    represented by \(\hat{y}_n\).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节涵盖了我们在线性回归中假设数据遵循的整个结构。然后，机器学习任务就是估计 \(\bbeta\) 中的参数。这些估计由 \(\hat{\beta}_0,
    \dots, \hat{\beta}_D\) 或 \(\bbetahat\) 表示。这些估计为我们目标变量的*拟合值*提供了，表示为 \(\hat{y}_n\)。
- en: 'This task can be accomplished in two ways which, though slightly different
    conceptually, are identical mathematically. The first approach is through the
    lens of *minimizing loss*. A common practice in machine learning is to choose
    a loss function that defines how well a model with a given set of parameter estimates
    the observed data. The most common loss function for linear regression is squared
    error loss. This says the *loss* of our model is proportional to the sum of squared
    differences between the true \(y_n\) values and the fitted values, \(\hat{y}_n\).
    We then *fit* the model by finding the estimates \(\bbetahat\) that minimize this
    loss function. This approach is covered in the subsection [Approach 1: Minimizing
    Loss](s1/loss_minimization.html).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务可以通过两种方式完成，尽管在概念上略有不同，但在数学上是相同的。第一种方法是通过 *最小化损失* 的视角。在机器学习中，选择一个损失函数来定义具有给定参数估计值的模型如何好地估计观察到的数据是一种常见做法。线性回归中最常见的损失函数是平方误差损失。这意味着我们的模型
    *损失* 与真实 \(y_n\) 值和拟合值 \(\hat{y}_n\) 之间平方差的和成比例。然后我们通过找到最小化此损失函数的估计 \(\bbetahat\)
    来 *拟合* 模型。这种方法在子节 [方法 1：最小化损失](s1/loss_minimization.html) 中进行了介绍。
- en: The second approach is through the lens of *maximizing likelihood*. Another
    common practice in machine learning is to model the target as a random variable
    whose distribution depends on one or more parameters, and then find the parameters
    that maximize its likelihood. Under this approach, we will represent the target
    with \(Y_n\) since we are treating it as a random variable. The most common model
    for \(Y_n\) in linear regression is a Normal random variable with mean \(E(Y_n)
    = \bbeta^\top \bx_n\). That is, we assume
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是通过 *最大化似然* 的视角。在机器学习中，另一种常见的做法是将目标建模为一个随机变量，其分布依赖于一个或多个参数，然后找到最大化其似然的参数。在这种方法下，我们将目标表示为
    \(Y_n\)，因为我们将其视为一个随机变量。在线性回归中，\(Y_n\) 最常见的模型是具有均值 \(E(Y_n) = \bbeta^\top \bx_n\)
    的正态随机变量。也就是说，我们假设
- en: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
- en: 'and we find the values of \(\bbetahat\) to maximize the likelihood. This approach
    is covered in subsection [Approach 2: Maximizing Likelihood](s1/likelihood_maximization.html).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到 \(\bbetahat\) 的值以最大化似然。这种方法在子节 [方法 2：最大化似然](s1/likelihood_maximization.html)
    中进行了介绍。
- en: Once we’ve estimated \(\bbeta\), our model is *fit* and we can make predictions.
    The below graph is the same as the one above but includes our estimated line-of-best-fit,
    obtained by calculating \(\hat{\beta}_0\) and \(\hat{\beta}_1\).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们估计了 \(\bbeta\)，我们的模型就 *拟合* 好了，我们可以进行预测。下面的图表与上面的图表相同，但包括我们通过计算 \(\hat{\beta}_0\)
    和 \(\hat{\beta}_1\) 得到的最佳拟合线。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
- en: Extensions of Ordinary Linear Regression
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通线性回归的扩展
- en: There are many important extensions to linear regression which make the model
    more flexible. Those include [Regularized Regression](../c2/s1/regularized.html)—which
    balances the bias-variance tradeoff for high-dimensional regression models—[Bayesian
    Regression](../c2/s1/bayesian.html)—which allows for prior distributions on the
    coefficients—and [GLMs](../c2/s1/GLMs.html)—which introduce non-linearity to regression
    models. These extensions are discussed in the next chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归有许多重要的扩展，使模型更加灵活。这些包括 [正则化回归](../c2/s1/regularized.html)——它平衡了高维回归模型中的偏差-方差权衡——[贝叶斯回归](../c2/s1/bayesian.html)——它允许对系数进行先验分布——以及
    [广义线性模型](../c2/s1/GLMs.html)——它向回归模型引入非线性。这些扩展将在下一章中讨论。
- en: Model Structure
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型结构
- en: '*Linear regression* is a relatively simple method that is extremely widely-used.
    It is also a great stepping stone for more sophisticated methods, making it a
    natural algorithm to study first.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归* 是一种相对简单的方法，它被极其广泛地使用。它也是更复杂方法的良好基石，使其成为首先研究的自然算法。'
- en: In linear regression, the target variable \(y\) is assumed to follow a linear
    function of one or more predictor variables, \(x_1, \dots, x_D\), plus some random
    error. Specifically, we assume the model for the \(n^\text{th}\) observation in
    our sample is of the form
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，目标变量 \(y\) 被假设遵循一个或多个预测变量 \(x_1, \dots, x_D\) 的线性函数，以及一些随机误差。具体来说，我们假设样本中第
    \(n^\text{th}\) 个观测值的模型形式如下
- en: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
- en: Here \(\beta_0\) is the intercept term, \(\beta_1\) through \(\beta_D\) are
    the coefficients on our feature variables, and \(\epsilon\) is an error term that
    represents the difference between the true \(y\) value and the linear function
    of the predictors. Note that the terms with an \(n\) in the subscript differ between
    observations while the terms without (namely the \(\beta\text{s}\)) do not.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 \(\beta_0\) 是截距项，\(\beta_1\) 到 \(\beta_D\) 是特征变量的系数，\(\epsilon\) 是一个误差项，它表示真实
    \(y\) 值与预测变量的线性函数之间的差异。请注意，下标带有 \(n\) 的项在不同观测之间是不同的，而下标不带 \(n\) 的项（即 \(\beta\)
    项）是不变的。
- en: 'The math behind linear regression often becomes easier when we use vectors
    to represent our predictors and coefficients. Let’s define \(\bx_n\) and \(\bbeta\)
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用向量来表示我们的预测变量和系数时，线性回归背后的数学通常会更简单。让我们定义 \(\bx_n\) 和 \(\bbeta\) 如下：
- en: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
- en: Note that \(\bx_n\) includes a leading 1, corresponding to the intercept term
    \(\beta_0\). Using these definitions, we can equivalently express \(y_n\) as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，\(\bx_n\) 包含一个前导的 1，对应于截距项 \(\beta_0\)。使用这些定义，我们可以等价地表示 \(y_n\) 为
- en: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
- en: Below is an example of a dataset designed for linear regression. The input variable
    is generated randomly and the target variable is generated as a linear combination
    of that input variable plus an error term.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个为线性回归设计的示例数据集。输入变量是随机生成的，目标变量是输入变量的线性组合加上一个误差项。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
- en: Parameter Estimation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数估计
- en: The previous section covers the entire structure we assume our data follows
    in linear regression. The machine learning task is then to estimate the parameters
    in \(\bbeta\). These estimates are represented by \(\hat{\beta}_0, \dots, \hat{\beta}_D\)
    or \(\bbetahat\). The estimates give us *fitted values* for our target variable,
    represented by \(\hat{y}_n\).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节涵盖了我们在线性回归中假设数据遵循的整个结构。那么，机器学习任务就是估计 \(\bbeta\) 中的参数。这些估计值由 \(\hat{\beta}_0,
    \dots, \hat{\beta}_D\) 或 \(\bbetahat\) 表示。这些估计值为我们目标变量的*拟合值*提供了信息，表示为 \(\hat{y}_n\)。
- en: 'This task can be accomplished in two ways which, though slightly different
    conceptually, are identical mathematically. The first approach is through the
    lens of *minimizing loss*. A common practice in machine learning is to choose
    a loss function that defines how well a model with a given set of parameter estimates
    the observed data. The most common loss function for linear regression is squared
    error loss. This says the *loss* of our model is proportional to the sum of squared
    differences between the true \(y_n\) values and the fitted values, \(\hat{y}_n\).
    We then *fit* the model by finding the estimates \(\bbetahat\) that minimize this
    loss function. This approach is covered in the subsection [Approach 1: Minimizing
    Loss](s1/loss_minimization.html).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务可以通过两种方式完成，虽然概念上略有不同，但数学上是相同的。第一种方法是通过*最小化损失*的视角。在机器学习中，选择一个损失函数来定义具有给定参数估计值的模型如何拟合观察到的数据是一种常见做法。线性回归中最常见的损失函数是平方误差损失。这意味着我们的模型*损失*与真实
    \(y_n\) 值和拟合值 \(\hat{y}_n\) 之间平方差的和成正比。然后，我们通过找到最小化这个损失函数的估计 \(\bbetahat\) 来*拟合*模型。这种方法在子节[方法1：最小化损失](s1/loss_minimization.html)中有详细说明。
- en: The second approach is through the lens of *maximizing likelihood*. Another
    common practice in machine learning is to model the target as a random variable
    whose distribution depends on one or more parameters, and then find the parameters
    that maximize its likelihood. Under this approach, we will represent the target
    with \(Y_n\) since we are treating it as a random variable. The most common model
    for \(Y_n\) in linear regression is a Normal random variable with mean \(E(Y_n)
    = \bbeta^\top \bx_n\). That is, we assume
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是通过*最大化似然*的视角。在机器学习中，将目标建模为依赖于一个或多个参数的随机变量，然后找到最大化其似然的参数是另一种常见做法。在这种方法下，我们将目标表示为
    \(Y_n\)，因为我们将其视为一个随机变量。线性回归中 \(Y_n\) 的最常见模型是具有均值 \(E(Y_n) = \bbeta^\top \bx_n\)
    的正态随机变量。也就是说，我们假设
- en: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
- en: 'and we find the values of \(\bbetahat\) to maximize the likelihood. This approach
    is covered in subsection [Approach 2: Maximizing Likelihood](s1/likelihood_maximization.html).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到 \(\bbetahat\) 的值以最大化似然。这种方法在子节[方法2：最大化似然](s1/likelihood_maximization.html)中有详细说明。
- en: Once we’ve estimated \(\bbeta\), our model is *fit* and we can make predictions.
    The below graph is the same as the one above but includes our estimated line-of-best-fit,
    obtained by calculating \(\hat{\beta}_0\) and \(\hat{\beta}_1\).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们估计了 \(\bbeta\)，我们的模型就*拟合*好了，我们可以进行预测。下面的图表与上面的图表相同，但包括了我们的最佳拟合线估计，通过计算 \(\hat{\beta}_0\)
    和 \(\hat{\beta}_1\) 得到。
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
- en: Extensions of Ordinary Linear Regression
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通线性回归的扩展
- en: There are many important extensions to linear regression which make the model
    more flexible. Those include [Regularized Regression](../c2/s1/regularized.html)—which
    balances the bias-variance tradeoff for high-dimensional regression models—[Bayesian
    Regression](../c2/s1/bayesian.html)—which allows for prior distributions on the
    coefficients—and [GLMs](../c2/s1/GLMs.html)—which introduce non-linearity to regression
    models. These extensions are discussed in the next chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归有许多重要的扩展，使模型更加灵活。这些扩展包括[正则化回归](../c2/s1/regularized.html)—它平衡了高维回归模型的偏差-方差权衡—[贝叶斯回归](../c2/s1/bayesian.html)—它允许对系数进行先验分布—以及[广义线性模型](../c2/s1/GLMs.html)—它向回归模型引入非线性。这些扩展将在下一章中讨论。
