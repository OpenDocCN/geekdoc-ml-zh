- en: 8.2.3 Reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2.3 å¼ºåŒ–å­¦ä¹ 
- en: åŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html](https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html](https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html)
- en: ğŸŒ³ **Tip** ğŸŒ³
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸŒ³ **æç¤º** ğŸŒ³
- en: To refresh your knowledge on deep RL, checkout [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)
    (OpenAI)
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ·æ–°ä½ å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ï¼Œè¯·æŸ¥çœ‹[Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)
    (OpenAI)
- en: '[E] Explain the explore vs exploit tradeoff with examples.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ç”¨ä¾‹å­è§£é‡Šæ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚'
- en: '[E] How would a finite or infinite horizon affect our algorithms?'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æœ‰é™æˆ–æ— é™çš„æ—¶é—´èŒƒå›´ä¼šå¦‚ä½•å½±å“æˆ‘ä»¬çš„ç®—æ³•ï¼Ÿ'
- en: '[E] Why do we need the discount term for objective functions?'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä¸ºä»€ä¹ˆç›®æ ‡å‡½æ•°éœ€è¦æŠ˜æ‰£é¡¹ï¼Ÿ'
- en: '[E] Fill in the empty circles using the minimax algorithm.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä½¿ç”¨æœ€å°-æœ€å¤§ç®—æ³•å¡«å†™ç©ºåœ†åœˆã€‚'
- en: '![Minimax algorithm](../Images/24e7be9d8e9ad4ff02ee1ee2e821bff8.png "image_tooltip")'
  id: totrans-8
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![æœ€å°-æœ€å¤§ç®—æ³•](../Images/24e7be9d8e9ad4ff02ee1ee2e821bff8.png "image_tooltip")'
- en: '[M] Fill in the alpha and beta values as you traverse the minimax tree from
    left to right.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åœ¨ä»å·¦åˆ°å³éå†æœ€å°-æœ€å¤§æ ‘æ—¶ï¼Œå¡«å†™ alpha å’Œ beta çš„å€¼ã€‚'
- en: '![Alpha-beta pruning](../Images/e5c5778c32bf07f69195be40a13b8d14.png "image_tooltip")'
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Alpha-beta å‰ªæ](../Images/e5c5778c32bf07f69195be40a13b8d14.png "image_tooltip")'
- en: '[E] Given a policy, derive the reward function.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ç»™å®šä¸€ä¸ªç­–ç•¥ï¼Œæ¨å¯¼å¥–åŠ±å‡½æ•°ã€‚'
- en: '[M] Pros and cons of on-policy vs. off-policy.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åŸºäºç­–ç•¥å’Œç¦»ç­–ç•¥çš„ä¼˜ç¼ºç‚¹ã€‚'
- en: '[M] Whatâ€™s the difference between model-based and model-free? Which one is
    more data-efficient?'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå“ªä¸€ä¸ªæ›´æ•°æ®é«˜æ•ˆï¼Ÿ'
