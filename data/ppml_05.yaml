- en: Chapter 3 Variable Types and Data Structures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 变量类型和数据结构
- en: 原文：[https://ppml.dev/types-structures.html](https://ppml.dev/types-structures.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ppml.dev/types-structures.html](https://ppml.dev/types-structures.html)
- en: In exploring the components that machine learning systems are built on, we stressed
    that different types of hardware are optimised to work with data and models stored
    in specific formats. Both are complex entities comprising a variety of elements
    that are organised into *data structures* such as data frames (representing tabular
    data) or standardised model formats like ONNX (ONNX [2021](#ref-onnx)). The individual
    elements inside these data structures are *variables* of specific *types* such
    as integer numbers, floating point numbers and strings.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索机器学习系统构建的基础组件时，我们强调不同类型的硬件被优化以与存储在特定格式的数据和模型一起工作。两者都是复杂的实体，由各种元素组成，这些元素被组织成*数据结构*，如数据框（表示表格数据）或标准化的模型格式如ONNX（ONNX
    [2021](#ref-onnx)）。这些数据结构内部的各个元素是特定*类型*的*变量*，例如整数、浮点数和字符串。
- en: In this chapter we revisit the variable types we mentioned in Chapter [2](hardware.html#hardware),
    as well as string representations, and we discuss possible reasons to choose one
    over another for different classes of data (Section [3.1](types-structures.html#variable-types)).
    We then give some notable examples of how variables are organised in data structures
    such as vectors and lists (Section [3.2.1](types-structures.html#vectors-lists)),
    data frames (Section [3.2.2](types-structures.html#dataframes)) and matrices (Section
    [3.2.3](types-structures.html#matrices)). Different choices of variable types
    (Section [3.3](types-structures.html#right-variables)) and data structures (Section
    [3.4](types-structures.html#right-data-structures)) represent different trade-offs
    both in terms of hardware support, as we saw in Chapter [2](hardware.html#hardware),
    and in terms of the computational complexity of the machine learning algorithms
    that will operate on them, as we will discuss in Chapter [4](algorithms.html#algorithms).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了第[2](hardware.html#hardware)章中提到的变量类型，以及字符串表示，并讨论了为什么对于不同类型的数据选择一个而不是另一个的可能原因（第[3.1](types-structures.html#variable-types)节）。然后我们给出了一些变量在数据结构中组织的显著例子，如向量（第[3.2.1](types-structures.html#vectors-lists)节）、列表（第[3.2.2](types-structures.html#dataframes)节）和矩阵（第[3.2.3](types-structures.html#matrices)节）。不同的变量类型选择（第[3.3](types-structures.html#right-variables)节）和数据结构选择（第[3.4](types-structures.html#right-data-structures)节）在硬件支持方面以及机器学习算法在它们上操作的计算复杂度方面都代表了不同的权衡，正如我们在第[2](hardware.html#hardware)章中看到的，我们将在第[4](algorithms.html#algorithms)章中讨论。
- en: 3.1 Variable Types
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 变量类型
- en: Machine learning software primarily deals with numbers that are the mathematical
    representation of the data we are modelling. Images can be represented using the
    values of the colour channels of each of their pixels; text can be encoded into
    strings, which are then converted to frequencies for particular words; sensor
    data are recorded as a time series. We can store each of them with different types
    of variables, each with pros and cons that we will discuss in Section [3.3](types-structures.html#right-variables).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习软件主要处理的是我们建模的数据的数学表示。图像可以用每个像素的颜色通道的值来表示；文本可以编码成字符串，然后转换成特定单词的频率；传感器数据记录为时间序列。我们可以用不同类型的变量存储它们，每种变量都有其优缺点，我们将在第[3.3](types-structures.html#right-variables)节中讨论。
- en: 3.1.1 Integers
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 整数
- en: Integer variables can be used to represent natural (\(\mathbb{N}\)) or integer
    numbers (\(\mathbb{Z}\)). They are often used to represent Boolean variables as
    *indicators* (also known as *dummy variables*) as follows.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 整数变量可以用来表示自然数（\(\mathbb{N}\)）或整数数（\(\mathbb{Z}\)）。它们通常用来表示布尔变量作为*指示器*（也称为*虚拟变量*）如下。
- en: '![](../Images/4fb7610c15d4c942edbb576525b2157e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fb7610c15d4c942edbb576525b2157e.png)'
- en: More generally, they can be used to give a numeric representation to finite
    sets by mapping each element to a different integer number.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地，它们可以用来给有限集提供数值表示，通过将每个元素映射到不同的整数。
- en: '![](../Images/c1e62830c60b87282430c1bc8318fba5.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1e62830c60b87282430c1bc8318fba5.png)'
- en: Enumerations in C or factors in R are constructed exactly in this way to minimise
    memory usage. However, this numeric representation is not suitable for modelling
    discrete variables because it makes parameter estimates dependent on the specific
    mapping between elements and integer numbers.[⁷](#fn7) Instead, we map discrete
    variables to their *one-hot encoding*:[⁸](#fn8) each element in the set is assigned
    an indicator variable that takes a value of 1 if the element is observed, and
    0 otherwise.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在C中的枚举或在R中的因子正是以这种方式构建的，以最小化内存使用。然而，这种数字表示不适合对离散变量进行建模，因为它使得参数估计依赖于元素和整数之间的特定映射。[⁷](#fn7)
    相反，我们将离散变量映射到它们的*单热编码*：[⁸](#fn8) 集合中的每个元素都被分配一个指示变量，如果观察到该元素，则该变量取值为1，否则为0。
- en: '![](../Images/5d2db59281f543994669c64e9b01050d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5d2db59281f543994669c64e9b01050d.png)'
- en: 'The number of elements we want to represent determines how many bits of memory
    each integer will use: \(n\) bits allow for \(2^n\) distinct values. A single
    bit is enough for a Boolean value, although, in practice, it is usually stored
    using at least 1 byte. A finite set with \(k\) elements can be represented with
    \(\log_2 k\) bits: one-hot encoding side-steps this limitation by using indicator
    variables at the cost of using more memory.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要表示的元素数量决定了每个整数将使用多少位内存：\(n\)位允许有 \(2^n\) 个不同的值。一个位就足以表示布尔值，尽管在实践中，它通常至少使用1个字节来存储。一个包含\(k\)个元素的有限集可以用\(\log_2
    k\)位表示：单热编码通过使用指示变量来绕过这个限制，但代价是使用更多的内存。
- en: 'Natural and integer numbers cannot be completely represented by integer variables:
    that would require an infinite number of bits. For this reason, programming languages
    provide integer variables of different *sizes* such as 8, 16, 32 and 64 bits.
    These sizes are all multiples of 8 bits because processors are optimised to work
    on bytes. The size of an integer variable determines the largest number it can
    represent. For instance, the largest (*unsigned*) natural number we can represent
    in 32 bits is \(2^{32} - 1 \approx 4.29 \times 10^9\), and the largest (*signed*)
    integer number is \(\pm 2^{31} - 1 \approx \pm 2.14 \times 10^{9}\) because 1
    bit is reserved for encoding the sign. (If that bit is set to zero, the number
    is considered to be positive.) Numbers that are outside of this range are said
    to *overflow*, meaning that their bit representation is larger than the size of
    the integer variable and thus overflows the memory reserved for that variable.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然数和整数不能完全由整数变量表示：那将需要无限数量的位。因此，编程语言提供了不同大小的整数变量，例如8位、16位、32位和64位。这些大小都是8位的倍数，因为处理器被优化以处理字节。整数变量的大小决定了它可以表示的最大数字。例如，在32位中我们可以表示的最大（*无符号*）自然数是
    \(2^{32} - 1 \approx 4.29 \times 10^9\)，而最大的（*有符号*）整数是 \(\pm 2^{31} - 1 \approx
    \pm 2.14 \times 10^{9}\)，因为1位被保留用于编码符号。（如果该位设置为0，则该数被认为是正数。）超出此范围的数字被称为*溢出*，意味着它们的位表示大于整数变量的大小，因此超出了为该变量保留的内存。
- en: For the sake of the example, consider the natural number 3134 represented as
    a 16-bit unsigned integer variable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例，考虑将自然数3134表示为16位无符号整数变量。
- en: '![](../Images/0928beb318cbdbc3da5ed86cbb81d2ce.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/0928beb318cbdbc3da5ed86cbb81d2ce.png)'
- en: 'It’s easy to check that this representation is equivalent to the “natural”
    one, they just use different bases: \[\begin{equation*} 2^1 + 2^2 + 2^3 + 2^4
    + 2^5 + 2^{10} + 2^{11} = 3134. \end{equation*}\] The largest number that can
    be represented in 16 bits is \(2^{16} - 1 = 65535\), and 3134 is comfortably smaller
    than that: we can easily see that the 4 *most-significant bits* that represent
    the 4 largest powers of 2 (\(2^{15}\), \(2^{14}\), \(2^{13}\) and \(2^{12}\))
    are all equal to zero. Now consider a much larger number: 247586.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易验证这种表示与“自然”表示等价，它们只是使用了不同的基数：\[\begin{equation*} 2^1 + 2^2 + 2^3 + 2^4 +
    2^5 + 2^{10} + 2^{11} = 3134. \end{equation*}\] 16位可以表示的最大数字是 \(2^{16} - 1 = 65535\)，而3134远小于这个数字：我们可以很容易地看到表示4个最大的2的幂（\(2^{15}\)，\(2^{14}\)，\(2^{13}\)和\(2^{12}\)）的4个最高有效位都是零。现在考虑一个更大的数字：247586。
- en: '![](../Images/70be4fcf3e6f53c305e5cdb47849431d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/70be4fcf3e6f53c305e5cdb47849431d.png)'
- en: Again, we can easily check that \[\begin{equation*} 2^1 + 2^5 + 2^8 + 2^9 +
    2^{10} + 2^{14} + 2^{15} + 2^{16} + 2^{17} = 247586. \end{equation*}\]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以很容易地验证 \[\begin{equation*} 2^1 + 2^5 + 2^8 + 2^9 + 2^{10} + 2^{14} +
    2^{15} + 2^{16} + 2^{17} = 247586. \end{equation*}\]
- en: 'Unfortunately, 247586 is larger than 65535 and cannot be represented in 16
    bits. If we try to store it in 16 bits, we overflow: the integer variables will
    contain only the 16 *least-significant bits* representing the powers from \(2^0\)
    to \(2^{15}\). The bits corresponding to \(2^{16}\) and \(2^{17}\) will be silently
    dropped, and the integer variable will contain the number \[\begin{equation*}
    2^1 + 2^5 + 2^8 + 2^9 + 2^{10} + 2^{14} + 2^{15} = 50978. \end{equation*}\]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，247586大于65535，无法用16位表示。如果我们尝试将其存储在16位中，会发生溢出：整数变量将只包含16个**最低有效位**，代表从\(2^0\)到\(2^{15}\)的幂。对应于\(2^{16}\)和\(2^{17}\)的位将被静默丢弃，整数变量将包含数字
    \[\begin{equation*} 2^1 + 2^5 + 2^8 + 2^9 + 2^{10} + 2^{14} + 2^{15} = 50978.
    \end{equation*}\]
- en: In the case of signed integers, the bits that overflow will overwrite the sign
    bit, and result in the integer variable storing a number that may be incorrect
    in both sign and absolute value. Consider again the number 247586, this time represented
    as a 16-bit signed integer variable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在有符号整数的情况下，溢出的位将覆盖符号位，导致整数变量存储的数字可能在符号和绝对值上都是不正确的。再次考虑数字247586，这次表示为一个16位有符号整数变量。
- en: '![](../Images/1bd802543062fb519ba05462ed1f1c34.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bd802543062fb519ba05462ed1f1c34.png)'
- en: The first 15 bits of the binary representation are stored correctly, the bit
    corresponding to \(2^{15}\) overwrites the sign bit, and the last two bits corresponding
    to \(2^{16}\) and \(2^{17}\) are again silently dropped. As a result, the integer
    variable will contain the number \[\begin{equation*} - (2^1 + 2^5 + 2^8 + 2^9
    + 2^{10} + 2^{14}) = -18210. \end{equation*}\]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制表示的前15位被正确存储，对应于\(2^{15}\)的位覆盖了符号位，而对应于\(2^{16}\)和\(2^{17}\)的最后两位再次被静默丢弃。因此，整数变量将包含数字
    \[\begin{equation*} - (2^1 + 2^5 + 2^8 + 2^9 + 2^{10} + 2^{14}) = -18210. \end{equation*}\]
- en: As an exception to the rule, R represents a missing integer value (`NA`) with
    the largest negative signed integer for a given precision. In Python, Pandas uses
    masked arrays for the same purpose and keeps a separate Boolean variable that
    indicates whether the integer is a missing value (denoted `pandas.NA`). NumPy
    does not support missing values for integer variables.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规则的例外，R使用给定精度的最大负有符号整数来表示缺失的整数值（`NA`）。在Python中，Pandas使用掩码数组来达到相同的目的，并保留一个单独的布尔变量来指示整数是否为缺失值（表示为`pandas.NA`）。NumPy不支持整数变量的缺失值。
- en: 'Range limitations aside, integer variables allow *exact computer arithmetic*:
    their bit representation coincides with the mathematical representation of integer
    and natural numbers in base 2, so there is no rounding or loss of precision.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了范围限制之外，整数变量允许**精确的计算机算术**：它们的位表示与二进制下整数和自然数的数学表示相一致，因此没有舍入或精度损失。
- en: 3.1.2 Floating Point
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 浮点数
- en: 'Floating point variables are used to represent real (\(\mathbb{R}\)) and complex
    numbers (\(\mathbb{C}\)), the former with a single variable and the latter with
    two (one for the real part, one for the imaginary part). Each variable is composed
    of four parts: the *sign* \(S\), the *bias* or *offset* \(O\), an *exponent* \(E\)
    and a *mantissa* \(M\). The floating point representation of a real number \(x\)
    is then \[\begin{equation*} x = (-1)^S * (1 + M) 2^{E + O}. \end{equation*}\]
    The overall size of the variable in bits is typically one of 16, 32, and 64 bits,
    often called *half-precision*, *single-precision*, *double-precision*. The number
    of bits assigned to each of the exponent (after adding the offset) and the mantissa
    depends on what encoding is used; the sign is always stored in a single bit. The
    variables defined in the IEEE 754 standard (Overton [2001](#ref-overton)) reserve:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点变量用于表示实数（\(\mathbb{R}\)）和复数（\(\mathbb{C}\)），前者用一个变量表示，后者用两个（一个表示实部，一个表示虚部）。每个变量由四个部分组成：**符号**
    \(S\)、**偏移量**或**偏移** \(O\)、**指数** \(E\) 和**尾数** \(M\)。实数 \(x\) 的浮点表示为 \[\begin{equation*}
    x = (-1)^S * (1 + M) 2^{E + O}. \end{equation*}\] 变量的总位数通常是16、32或64位，通常称为**半精度**、**单精度**、**双精度**。分配给指数（加上偏移量后）和尾数的位数取决于所使用的编码；符号始终存储在一个位中。在IEEE
    754标准中定义的变量（Overton [2001](#ref-overton)）保留：
- en: '| **precision** | **exponent** | **mantissa** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **精度** | **指数** | **尾数** |'
- en: '| --- | --: | --: |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --: | --: |'
- en: '| half (16 bit) | 5 bits | 10 bits |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 半精度（16位） | 5位 | 10位 |'
- en: '| single (32 bit) | 8 bits | 23 bits |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 单精度（32位） | 8位 | 23位 |'
- en: '| double (64 bit) | 11 bits | 52 bits |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 双精度（64位） | 11位 | 52位 |'
- en: The value of the offset is determined as \(O = 2^{|E| - 1} - 1\) where \(|E|\)
    is the size of the exponent in bits.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移值的计算公式为 \(O = 2^{|E| - 1} - 1\)，其中 \(|E|\) 是指数的位数。
- en: The alternative “brain” format devised by Google in the process of developing
    TPUs (see Section [2.1.1](hardware.html#hardware-compute)) typically has size
    16 bits and is known as “bfloat16”. It works in the same way as the IEEE 754 floating
    point, so we will not discuss it further; the only difference is that it allocates
    8 bits to the exponent and 7 bits to the mantissa.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发 TPUs 的过程中，谷歌提出的替代“大脑”格式通常大小为 16 位，被称为“bfloat16”。它的工作方式与 IEEE 754 浮点数相同，因此我们不再进一步讨论；唯一的区别是它为指数分配了
    8 位，为尾数分配了 7 位。
- en: 'What does this mean in terms of binary representation? Consider the number
    435.25\. In the usual scientific notation, which is in base 10, we can write it
    as \(4.3525 \times 10^2\). If we do the same in base 2, the scientific notation
    becomes \(1.7001953125 \times 2^8\). The exponent is \(8\), and the mantissa is
    \(0.7001953125\). As a half-precision floating point variable, 435.25 then has
    the following binary representation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这在二进制表示中意味着什么？以数字 435.25 为例。在通常的科学记数法中，以 10 为基数，我们可以将其写成 \(4.3525 \times 10^2\)。如果我们以
    2 为基数做同样的事情，科学记数法变为 \(1.7001953125 \times 2^8\)。指数是 \(8\)，尾数是 \(0.7001953125\)。作为一个半精度浮点变量，435.25
    的二进制表示如下：
- en: '![](../Images/9a2d64045472ae45c5aeea4e31b91cf5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a2d64045472ae45c5aeea4e31b91cf5.png)'
- en: 'The exponent is stored after adding the offset: \[\begin{equation*} 8 + (2^{5
    - 1} - 1) = 23 = 2^0 + 2^1 + 2^2 + 2^4. \end{equation*}\] The resulting number
    is treated as unsigned, regardless of whether the original exponent was positive
    or negative. If the sign is stored in the most significant bit of the variable,
    the exponent is adjusted with the offset, and the mantissa is stored in the least
    significant bits, we can compare floating point numbers just by ranking their
    binary representations, which can be done efficiently with hardware instructions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移值添加后存储指数：\[\begin{equation*} 8 + (2^{5 - 1} - 1) = 23 = 2^0 + 2^1 + 2^2 +
    2^4. \end{equation*}\] 结果数字被视为无符号数，无论原始指数是正数还是负数。如果符号存储在变量的最高有效位中，则使用偏移调整指数，并将尾数存储在最低有效位中，我们只需通过比较它们的二进制表示来比较浮点数，这可以通过硬件指令高效完成。
- en: The mantissa is \[\begin{equation*} 2^{-1} + 2^{-3} + 2^{-4} + 2^{-7} + 2^{-8}
    + 2^{-10} = 0.7001953, \end{equation*}\] which differs from \(0.7001953125\) by
    \(1.25 \times 10^{-8}\). This difference is known as the *floating point error*
    arising from the limits in the precision that can be achieved with the number
    of bits of the mantissa. The only numbers that can be represented exactly are
    those that factorise into the powers of 2 available in the exponent and in the
    mantissa. This obviously excludes all numbers with an infinite number of digits,
    such as \(\pi\), \(e\) or \(1/3\).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尾数 \[\begin{equation*} 2^{-1} + 2^{-3} + 2^{-4} + 2^{-7} + 2^{-8} + 2^{-10}
    = 0.7001953, \end{equation*}\] 与 \(0.7001953125\) 相差 \(1.25 \times 10^{-8}\)。这种差异被称为由尾数位数精度限制产生的
    *浮点误差*。唯一可以精确表示的数字是那些可以分解为指数和尾数中可用的 2 的幂的数字。这显然排除了所有具有无限位数的数字，例如 \(\pi\)、\(e\)
    或 \(1/3\)。
- en: 'What is the range of floating point numbers? The largest number (positive or
    negative) that we can represent is limited by the size of the exponent: with \(|E|\)
    bits we can represent up to \(2^{|E|}\) exponents. The offset ensures that the
    available exponents are equally divided between positive and negative numbers
    ranging from \(-2^{|E| - 1} - 2\) to \(2^{|E| - 1} - 1\) due to the offset.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数的范围是多少？我们可以表示的最大数（正数或负数）受指数大小的限制：使用 \(|E|\) 位，我们可以表示多达 \(2^{|E|}\) 个指数。偏移确保可用的指数在从
    \(-2^{|E| - 1} - 2\) 到 \(2^{|E| - 1} - 1\) 的正负数之间平均分配，这是由于偏移的存在。
- en: '| **precision** | **smallest exponent** | **largest exponent** |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **精度** | **最小指数** | **最大指数** |'
- en: '| --- | --: | --: |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --: | --: |'
- en: '| half | \(-(2^{4} - 2) = -14\) | \(2^{4} - 1 = 15\) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| half | \(-(2^{4} - 2) = -14\) | \(2^{4} - 1 = 15\) |'
- en: '| single | \(-(2^{7} - 2) = -126\) | \(2^{7} - 1 = 127\) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| single | \(-(2^{7} - 2) = -126\) | \(2^{7} - 1 = 127\) |'
- en: '| double | \(-(2^{10} - 2) = -1022\) | \(2^{10} - 1 = 1023\) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| double | \(-(2^{10} - 2) = -1022\) | \(2^{10} - 1 = 1023\) |'
- en: 'This range is smaller than the theoretical \(2^{|E|}\) values it could potentially
    contain because some combinations of bits are reserved for special classes of
    numbers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范围比理论上可能包含的 \(2^{|E|}\) 值要小，因为一些位组合被保留用于特殊类别的数字：
- en: Zero is encoded with the exponent field and the mantissa filled with 0s.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零使用指数字段和尾数填充0位来编码。
- en: Positive and negative infinity (`+Inf`, `-Inf`) are encoded with the exponent
    field filled with 1s and a mantissa filled with 0s.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正无穷和负无穷（`+Inf`，`-Inf`）使用指数字段填充1位和尾数填充0位来编码。
- en: 'Irrepresentable numbers (usually denoted `NaN`) are encoded with the exponent
    field filled with 1s and at least one non-zero bit in the mantissa. Different
    patterns of bits in the mantissa are used for different types of irrepresentable
    numbers: the most common is the missing value identifier `NA`. Typically, `NaN`
    arises from dividing a number by zero or by trying to apply a mathematical function
    to a value outside its domain, for instance, taking the logarithm of a negative
    number.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可表示的数字（通常表示为 `NaN`）使用指数字段填充1位和至少一个非零位来编码。在尾数中使用的不同位模式用于不同类型的不可表示数字：最常见的是缺失值标识符
    `NA`。通常，`NaN` 是由于除以零或尝试将数学函数应用于其域外的值而产生的，例如，对负数取对数。
- en: 'Subnormal numbers, that is, numbers that are too small to be written in binary
    scientific notation with the available exponents. In other words, their leading
    exponent is smaller than the smallest available exponent. They are encoded with
    the exponent field filled with 0s. These numbers have reduced precision because
    they effectively use only part of the mantissa. As an example, \(2^{-10} \times
    2^{-14} \approx 5.96 \times 10^{-8}\) is represented as follows in half precision:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚正常数，即太小而不能用可用指数的二进制科学记数法表示的数字。换句话说，它们的起始指数小于最小可用指数。它们使用指数字段填充0位来编码。这些数字的精度降低，因为它们实际上只使用了部分尾数。例如，\(2^{-10}
    \times 2^{-14} \approx 5.96 \times 10^{-8}\) 在半精度中表示如下：
- en: '![](../Images/5e99bc399ccb5d3c2411b7aea644d999.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5e99bc399ccb5d3c2411b7aea644d999.png)'
- en: 'How coarse is floating point rounding? For any given precision, that depends
    on the magnitude of the number. As we saw in the example above, the mantissa can
    encode only so many significant decimal digits: \(\log_{10}(2^{10}) \approx 3\)
    digits for half-precision, \(\log_{10}(2^{23}) \approx 7\) for single precision,
    \(\log_{10}(2^{51}) \approx 16\) for double precision. This effectively creates
    a grid of values that can be represented exactly, and any other number is rounded
    to the nearest number that can be represented exactly or to `+Inf`/`-Inf`. The
    grid becomes coarser, in absolute terms, as the exponent becomes larger. Consider
    a number like 0.0002 that is small for a half-precision variable:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点数舍入有多粗糙？对于任何给定的精度，这取决于数字的大小。正如我们在上面的例子中所看到的，尾数只能编码这么多有效数字：半精度的 \(\log_{10}(2^{10})
    \approx 3\) 位数字，单精度的 \(\log_{10}(2^{23}) \approx 7\) 位，双精度的 \(\log_{10}(2^{51})
    \approx 16\) 位。这实际上创建了一个可以精确表示的值网格，任何其他数字都四舍五入到最接近的可以精确表示的数字或 `+Inf`/`-Inf`。随着指数的增大，网格在绝对值上变得更加粗糙。考虑一个像
    0.0002 这样对于半精度变量来说很小的数字：
- en: '![](../Images/b9bb7a7170667c44853d8921c3a78e29.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b9bb7a7170667c44853d8921c3a78e29.png)'
- en: The exponent is \[\begin{equation*} -13 + (2^{5 - 1} - 1) = 2^1 \end{equation*}\]
    and the mantissa is \[\begin{equation*} 2^{-1} + 2^{-3} + 2^{-7} + 2^{-8} + 2^{-9}
    = 0.638671875, \end{equation*}\] which gives \[\begin{equation*} 1.638671875 \times
    2^{-13} \approx 0.000200033. \end{equation*}\] Increasing this number by the least
    possible amount by adding \(2^{-10}\) and decreasing it by the same amount shows
    that the nearest numbers that can be represented in half precision are \(\approx
    0.000200033 \pm 1.19 \times 10^{-7}\).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 指数是 \[\begin{equation*} -13 + (2^{5 - 1} - 1) = 2^1 \end{equation*}\]，尾数是 \[\begin{equation*}
    2^{-1} + 2^{-3} + 2^{-7} + 2^{-8} + 2^{-9} = 0.638671875, \end{equation*}\]，这给出
    \[\begin{equation*} 1.638671875 \times 2^{-13} \approx 0.000200033. \end{equation*}\]
    通过添加 \(2^{-10}\) 来增加这个数的最小可能量，并通过相同数量的减少来减少它，这表明可以用半精度表示的最近数字是 \(\approx 0.000200033
    \pm 1.19 \times 10^{-7}\)。
- en: 'Now consider a relatively large number (for half precision) like 10002:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑一个相对较大的数字（对于半精度）如 10002：
- en: '![](../Images/6d21e052a7bdf6dde92199a309492390.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6d21e052a7bdf6dde92199a309492390.png)'
- en: 'The exponent is \[\begin{equation*} 13 + (2^{5 - 1} - 1) = 28 = 2^2 + 2^3 +
    2^4 \end{equation*}\] and the mantissa is \[\begin{equation*} 2^{-3} + 2^{-4}
    + 2^{-5} + 2^{-9} = 0.220703125, \end{equation*}\] which gives \(1.220703125 \times
    2^{13} = 10000\). The closest numbers that can be represented in half precision
    are 9992 and 10008: all the numbers in between are rounded. This leaves an interval
    of \(\pm 8\) around 10000\. For large enough numbers, floating point variables
    cannot even represent integer numbers without rounding!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 指数是 \[\begin{equation*} 13 + (2^{5 - 1} - 1) = 28 = 2^2 + 2^3 + 2^4 \end{equation*}\]
    并且尾数是 \[\begin{equation*} 2^{-3} + 2^{-4} + 2^{-5} + 2^{-9} = 0.220703125, \end{equation*}\]
    这给出 \(1.220703125 \times 2^{13} = 10000\)。可以表示为半精度的最接近的数字是9992和10008：介于它们之间的所有数字都被舍入。这给10000周围留下了一个\(\pm
    8\)的区间。对于足够大的数字，浮点变量甚至无法不进行舍入就表示整数！
- en: 'How can we keep the errors introduced by floating point rounding in check?
    Errors compound across operations, and machine learning models typically perform
    large numbers of operations compared to the size of their inputs. (More on that
    in Chapter [4](algorithms.html#algorithms).) Fortunately, probability theory and
    statistics have historically standardised computations to work on the logarithmic
    scale to make closed-form mathematical derivations easier. Working with the logarithmic
    transforms of floating point numbers reduces the chances that large numbers will
    overflow to `+Inf`/`-Inf` or that small numbers will be rounded down to zero.
    In the case of numbers with subnormal floating point representations, we also
    retain better precision because their logarithm will not be subnormal. This is
    particularly important in the common case of summing up large numbers of log-probabilities.
    Working with numbers on the same scale (that is, they have similar exponents)
    also helps in avoiding catastrophic losses in precision. When operations involve
    numbers on very different scales, the difference in the granularity of the floating
    point rounding may cause the result to have unacceptably large errors even though
    all the operands are accurate. As an extreme example, consider adding 10002 and
    0.0002: the result would be 10000, the closest floating point number in half precision!
    A similar issue is *catastrophic cancellation*, which may happen when subtracting
    two floating point numbers that are very close to each other.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何控制由浮点数舍入引入的错误？错误在操作过程中会累积，与输入大小相比，机器学习模型通常执行大量的操作。（更多内容请参阅第[4](algorithms.html#algorithms)章。）幸运的是，概率论和统计学在历史上已经标准化了计算，使其在对数尺度上工作，从而使闭式数学推导更容易。使用浮点数的对数变换可以降低大数溢出到`+Inf`/`-Inf`或小数四舍五入到零的可能性。在具有非正常浮点数表示的数字的情况下，我们也保留了更好的精度，因为它们的对数不会是非正常的。这在将大量对数概率相加的常见情况下尤为重要。使用相同尺度（即，它们有相似的指数）的数字也有助于避免精度灾难性的损失。当操作涉及尺度非常不同的数字时，浮点数舍入的粒度差异可能导致结果具有无法接受的大误差，尽管所有操作数都是准确的。作为一个极端的例子，考虑将10002和0.0002相加：结果将是10000，这是半精度中最接近的浮点数！类似的问题还有*灾难性消去*，这可能在减去两个非常接近的浮点数时发生。
- en: 'Unlike integer arithmetic, floating point arithmetic is not exact because of
    the impact of floating point rounding. The results of operations involving floating
    point variables can differ in many ways from the mathematical operations they
    implement, even in common scenarios. It is easy to demonstrate with a simple recurrence
    such as \[\begin{align*} x_0 = 4, x_1 = 4.25, x_{n + 1} = 108 - \left(815 - \frac{1500}{x_{n
    - 1}}\right) \frac{1}{x_n}, \end{align*}\] which converges to 100 in double precision
    even though the true limit in \(\mathbb{R}\) is 5 (Rump [2020](#ref-wrongfp)[b](#ref-wrongfp)).
    This can happen even if all the operands are exactly representable, as proved
    in (Rump [2020](#ref-wrongfp2)[a](#ref-wrongfp2)). Some effects of this discrepancy
    are:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与整数算术不同，浮点数算术由于浮点数舍入的影响而不精确。涉及浮点变量的操作结果可能与它们实现的数学操作以多种方式不同，即使在常见场景中也是如此。可以通过一个简单的递归来说明这一点，例如
    \[\begin{align*} x_0 = 4, x_1 = 4.25, x_{n + 1} = 108 - \left(815 - \frac{1500}{x_{n
    - 1}}\right) \frac{1}{x_n}, \end{align*}\] 在双精度下收敛到100，尽管在\(\mathbb{R}\)中的真正极限是5（Rump
    [2020](#ref-wrongfp)[b](#ref-wrongfp))。即使所有操作数都可以精确表示，这种情况也可能发生，如(Rump [2020](#ref-wrongfp2)[a](#ref-wrongfp2))中证明的那样。这种差异的一些影响包括：
- en: Numbers that should be equal are not equal. We should always compare numbers
    with a tolerance that is a function of the floating point precision we are using.
    The default in R is the square root of the smallest representable number, obtainable
    as `sqrt(.Machine$double.eps)`.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该相等的数字并不相等。我们应该始终使用一个与我们所使用的浮点精度相关的容差来比较数字。在 R 中，默认值是最小可表示数的平方根，可以通过 `sqrt(.Machine$double.eps)`
    获取。
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Conversely, numbers that should not be equal may end up being equal.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，不应该相等的数字最终可能会相等。
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The order in which operations are performed matters, even when the mathematical
    operations or functions they implement are commutative and/or associative. Structuring
    code so that key computations are implemented only once and therefore ensuring
    that operations are always performed in the same sequences is the best way to
    prevent this issue.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数学运算或函数是可交换的或/和结合的，操作执行的顺序也很重要。将代码结构化，以便关键计算只实现一次，从而确保操作始终以相同的顺序执行，是防止此类问题的最佳方式。
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The order in which operations are performed matters also because intermediate
    results may underflow to zero or overflow to `+Inf`/`-Inf` unless we reorder operations
    to prevent that from happening.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作执行的顺序也很重要，因为中间结果可能会下溢到零或上溢到 `+Inf`/`-Inf`，除非我们重新排序操作以防止这种情况发生。
- en: Working on a log-scale is the best option when dealing with the small probabilities
    that often arise from multivariate distributions or from a large number of data
    points. Otherwise, the final result is likely to underflow to zero.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理多变量分布或大量数据点经常出现的小概率时，以对数尺度工作是最好的选择。否则，最终结果可能会下溢到零。
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 3.1.3 Strings
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 字符串
- en: Strings are sequences of characters encoded in binary form and stored into variables.
    Their binary format varies, but it typically is UTF-8 on Linux and MacOS X and
    UTF-16 on Windows. Both are Unicode standards (Unicode [2021](#ref-unicode)) that
    use between 1 and 4 bytes to encode each character and support many alphabets,
    mathematical symbols and pictograms such as emoji.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串是以二进制形式编码的字符序列，并存储到变量中。它们的二进制格式各不相同，但在 Linux 和 MacOS X 上通常是 UTF-8，在 Windows
    上通常是 UTF-16。两者都是 Unicode 标准（Unicode [2021](#ref-unicode)），它们使用 1 到 4 个字节来编码每个字符，并支持许多字母表、数学符号和象形文字，如表情符号。
- en: 'In the context of machine learning software, character strings are typically
    only encountered as input data in natural language processing (NLP) applications.
    In other settings, they are used as human-readable labels for the items in a set
    and can be represented using integers as we saw in Section [3.1.1](types-structures.html#integers).
    In fact, they are eventually given a numerical representation even in NLP in order
    to feed them to algorithms such as word2vec (Rong [2014](#ref-word2vec)), GLOVE
    (Pennington, Socher, and Manning [2014](#ref-glove)) and BERT (Devlin et al. [2019](#ref-bert)).
    In NLP, strings are also preprocessed taking into account their meaning and their
    grammatical and syntactical properties to facilitate later analyses. For instance:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习软件的背景下，字符串通常仅在自然语言处理（NLP）应用中作为输入数据出现。在其他环境中，它们被用作集合中项目的可读性标签，并且可以使用整数来表示，正如我们在第
    [3.1.1](types-structures.html#integers) 节中看到的。实际上，即使在 NLP 中，它们最终也会被赋予数值表示，以便将它们输入到诸如
    word2vec (Rong [2014](#ref-word2vec))、GLOVE (Pennington, Socher, and Manning [2014](#ref-glove))
    和 BERT (Devlin et al. [2019](#ref-bert)) 这样的算法中。在 NLP 中，字符串也会进行预处理，考虑到它们的含义以及它们的语法和句法属性，以方便后续分析。例如：
- en: Common words that do not add meaning to a sentence, often called *stopwords*,
    are removed to reduce the dimensionality of the data.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不增加句子意义的常见单词，通常称为 *stopwords*，被移除以减少数据的维度。
- en: Words may be *stemmed*, that is, different words may be reduced to their common
    stem after removing suffixes and prefixes to identify which are in fact the same
    word.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词可以被 *stemmed*，也就是说，通过去除后缀和前缀将不同的单词还原到它们的共同词干，以识别哪些实际上是相同的单词。
- en: Words may be *tagged* with their syntactic role.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词可以被 *tagged* 以表示它们的句法角色。
- en: Words may be *normalised* by making all characters lower-case and sometimes
    by removing accents and diacritics as well. Complex, composite characters can
    be encoded in different ways in both UTF-8 and UTF-16, and transforming them into
    their *canonical form* is essential to identify unique words correctly.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词可以通过将所有字符转换为小写，有时通过去除重音和变音符号来 *normalised*。在 UTF-8 和 UTF-16 中，复杂、组合的字符可以以不同的方式编码，将它们转换为它们的
    *canonical form* 对于正确识别唯一的单词是至关重要的。
- en: Extraneous characters such as punctuation, hyphenation and numbers may be removed
    as non-informative. Abbreviations and acronyms may be expanded to make explicit
    the words they correspond to. Similarly, emoji may be replaced by a textual description.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以移除非信息性的额外字符，如标点符号、连字符和数字。缩写词和首字母缩略词可以扩展，以明确它们对应的单词。同样，表情符号可以用文本描述来替换。
- en: A detailed treatment of these topics is beyond the scope of this book, and we
    refer the reader to monographs such as (Aggarwal [2018](#ref-mlfortext)) and to
    the documentation of relevant software libraries such as Spacy (Explosion [2021](#ref-spacy))
    and NLTK (NLTK Team [2021](#ref-nltk)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些主题的详细讨论超出了本书的范围，我们建议读者参考如（Aggarwal [2018](#ref-mlfortext)）等专著，以及Spacy（Explosion
    [2021](#ref-spacy)）和NLTK（NLTK Team [2021](#ref-nltk)）等相关软件库的文档。
- en: 3.2 Data Structures
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 数据结构
- en: Data structures are particular ways of organising variables of one or more types
    for effective and efficient processing. Different data structures will be most
    effective for different operations or different algorithms. We will discuss both
    aspects further in Section [3.4](types-structures.html#right-data-structures)
    and later in Chapter [4](algorithms.html#algorithms), characterising memory and
    computational efficiency in terms of space and time complexity. Here we will only
    cover those data structures that are foundational for machine learning software,
    referring the reader to other excellent resources (Brass [2008](#ref-brass); Cormen
    [2013](#ref-cormen)) for a broader coverage of the topic.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构是组织一个或多个类型变量以有效和高效处理的一种特定方式。不同的数据结构将适用于不同的操作或不同的算法。我们将在第[3.4](types-structures.html#right-data-structures)节和后面的第[4](algorithms.html#algorithms)章中进一步讨论这两个方面，用空间和时间复杂度来表征内存和计算效率。在这里，我们只涵盖那些对机器学习软件基础性的数据结构，并建议读者参考其他优秀资源（Brass
    [2008](#ref-brass)；Cormen [2013](#ref-cormen)）以获得对该主题更广泛的了解。
- en: 'Why use data structures? Firstly, they make code more compact by allowing us
    to abstract away basic variable manipulations that would otherwise be repeatedly
    implemented in different places. Our code will be clearer and most likely have
    fewer bugs as a result. Secondly, data structures tell the software how particular
    groups of variables belong together, both in terms of how they are laid out in
    memory and how we operate on them. This makes it possible for the software we
    write to be compiled or interpreted (see Section [6.1](writing-code.html#programming-language))
    to operate efficiently on the variables contained in the data structures. Thirdly,
    the information that particular groups of variables belong together will be useful
    to developers working on our code. Those variables may describe the parts of a
    single mathematical object or real-world entity, they may have the same semantic
    meaning or they may have attached metadata that can be used for interpretation
    and debugging purposes: all facts that are useful to know when reading and developing
    code.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用数据结构？首先，它们通过允许我们抽象出基本变量操作（否则这些操作将在不同的地方反复实现）来使代码更加紧凑。因此，我们的代码将更加清晰，并且很可能出现更少的错误。其次，数据结构告诉软件特定组变量如何组合在一起，无论是从它们在内存中的布局，还是从我们对它们的操作来看。这使得我们编写的软件能够编译或解释（见第[6.1](writing-code.html#programming-language)节）以高效地处理数据结构中的变量。第三，特定组变量属于一起的信息将对开发我们代码的开发者有用。这些变量可能描述单个数学对象或现实世界实体的部分，它们可能具有相同的语义意义，或者它们可能附加了可用于解释和调试目的的元数据：所有这些都是在阅读和开发代码时需要了解的有用事实。
- en: 3.2.1 Vectors and Lists
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 向量和列表
- en: The most fundamental data structures in machine learning software are *vectors*
    and *lists*. Both can contain any type of variable, and are defined by their *length*
    (the number of elements they contain). Their conceptual structure is shown in
    Figure [3.1](types-structures.html#fig:arrays).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习软件中最基本的数据结构是**向量**和**列表**。两者都可以包含任何类型的变量，并且由它们的**长度**（它们包含的元素数量）定义。它们的概念结构如图[3.1](types-structures.html#fig:arrays)所示。
- en: '![A schematic view of the logical structure and the memory layout of vectors
    (left) and lists (right).](../Images/21f8ba5543c0ba4d0732b529111bfe86.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![向量（左）和列表（右）的逻辑结构和内存布局的示意图。](../Images/21f8ba5543c0ba4d0732b529111bfe86.png)'
- en: 'Figure 3.1: A schematic view of the logical structure and the memory layout
    of vectors (left) and lists (right).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：向量（左）和列表（右）的逻辑结构和内存布局的示意图。
- en: 'Vectors are *homogeneous* data structures holding sequences of variables of
    the same type. The variables are stored sequentially in a single block of memory,
    so vectors can be accessed with a single memory access using a *pointer* to their
    first element. (A pointer is itself a variable that contains a memory address.)
    Reading the variables stored in a vector is trivial because all elements occupy
    the same number of bytes in memory: the \(i\)th element is located at a memory
    address that is that of the first element plus \(i\) times the variable type size.
    Copying the whole vector is also trivial, since it is stored as a single block
    of memory. The same is true for subsets of variables that are adjacent to each
    other within a vector.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是**同构**的数据结构，它包含相同类型变量的序列。变量按顺序存储在单个内存块中，因此可以使用指向其第一个元素的**指针**来访问向量（指针本身是一个包含内存地址的变量）。读取向量中存储的变量是微不足道的，因为所有元素在内存中占用相同数量的字节：第\(i\)个元素位于第一个元素的内存地址加上\(i\)乘以变量类型大小。整个向量的复制也是微不足道的，因为它存储为一个单独的内存块。对于向量中相邻的变量子集也是如此。
- en: 'Lists, on the other hand, are *heterogeneous* data structures that can contain
    different kinds of elements. They essentially act as vectors of pointers to arbitrary
    data structures or variable types. Therefore, each element in a list can be anything:
    a single variable of some type, a vector of any length, a second list, a matrix,
    etc. However, this means that accessing the elements of a list is less trivial
    since we need to locate each element and access it separately. However, copying
    the list and subsetting it can be easier: if we do not need to modify the contents
    of its elements, we can just copy (all or a subset of) the pointers to the elements
    to create a new list. This is called a *shallow copy*, and can significantly reduce
    memory use. However, we must duplicate the elements as well if we need to modify
    them later in the new list in order to avoid altering the original list they are
    attached to. Copying both the list and its elements is called a *deep copy*. In
    contrast, subsetting vectors requires a deep copy in the general case. Shallow
    copies are only possible when copying a whole vector or when subsetting a slice
    of adjacent elements.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，列表是**异构**的数据结构，可以包含不同类型的元素。它们本质上充当指向任意数据结构或变量类型的指针的向量。因此，列表中的每个元素可以是任何东西：某种类型的单个变量、任意长度的向量、第二个列表、矩阵等。然而，这意味着访问列表的元素不太简单，因为我们需要定位每个元素并单独访问它。然而，复制列表和子集它可能更容易：如果我们不需要修改其元素的内容，我们只需复制（全部或部分）指向元素的指针以创建一个新的列表。这被称为**浅拷贝**，可以显著减少内存使用。然而，如果我们需要在新的列表中稍后修改这些元素，我们必须复制元素本身，以避免更改它们所附加的原始列表。同时复制列表及其元素称为**深拷贝**。相比之下，在一般情况下，子集向量需要深拷贝。浅拷贝仅在复制整个向量或子集相邻元素切片时才可行。
- en: 'Storing variables into vectors makes *vectorised* computations possible: a
    function can be applied independently to each element of the vector, potentially
    leveraging hardware’s SIMD and FMA instructions to achieve instruction- and data-level
    parallelism as we discussed in Section [2.2](hardware.html#hardware-using). If
    the return value of the function is a scalar, the results can be saved in a second
    vector of the same length. Otherwise, the results can be saved in a dense matrix
    (Section [3.2.3](types-structures.html#matrices)) or in a data frame (Section
    [3.2.2](types-structures.html#dataframes)) in which each row or column contains
    the return values for a single input element. Vectorised computations are also
    possible for lists using thread-level parallelism, assuming that the function
    can handle all the types of variables stored in the list. Its outputs would then
    be stored in a second list regardless of whether each of them is a scalar or not.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将变量存储到向量中使得**向量化**计算成为可能：一个函数可以独立应用于向量的每个元素，可能利用硬件的SIMD和FMA指令来实现指令级和数据级并行性，正如我们在第[2.2](hardware.html#hardware-using)节中讨论的那样。如果函数的返回值是标量，结果可以保存在长度相同的第二个向量中。否则，结果可以保存在密集矩阵（第[3.2.3](types-structures.html#matrices)节）或数据框（第[3.2.2](types-structures.html#dataframes)节）中，其中每一行或每一列都包含单个输入元素的返回值。使用线程级并行性，列表也可以进行向量化计算，假设函数可以处理列表中存储的所有变量类型。其输出将存储在第二个列表中，无论每个输出是否为标量。
- en: 3.2.2 Representing Data with Data Frames
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 使用数据框表示数据
- en: 'A data frame is a *heterogeneous* two-dimensional data structure with columns
    of potentially different types. Its primary task is storing tabular data and the
    associated metadata, such as column- and row-names. The implementations in Julia
    (DataFrame.jl) and Python (Pandas and Scikit-Learn (Scikit-learn Developers [2022](#ref-sklearn)))
    have been heavily inspired by R data frames: they only have minor differences
    in their semantics. The most notable is that operations on two data frames will
    match cells by position in R and Julia (regardless of row- and column-names) and
    by row- and column-names in Python (regardless of the cell positions).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框是一种*异构*的二维数据结构，具有可能不同类型的列。其主要任务是存储表格数据和相关的元数据，例如列名和行名。在Julia（DataFrame.jl）和Python（Pandas和Scikit-Learn（Scikit-learn
    Developers [2022](#ref-sklearn)））中的实现受到了R数据框的强烈启发：它们在语义上只有细微的差异。最显著的是，在R和Julia中对两个数据框的操作将按位置匹配单元格（无论行名和列名），而在Python中则按行名和列名匹配（无论单元格位置）。
- en: '![A schematic view of tabular data (bottom right) encoded as a data frame (left,
    top).](../Images/5f584445c8c6e64e3d25eec8eb938cc9.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![表格数据（右下角）编码为数据框（左，上）的示意图。](../Images/5f584445c8c6e64e3d25eec8eb938cc9.png)'
- en: 'Figure 3.2: A schematic view of tabular data (bottom right) encoded as a data
    frame (left, top).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：表格数据的示意图（右下角）编码为数据框（左，上）。
- en: 'The fundamental structure of a data frame is that of a list: each column in
    the tabular data is a vector that is stored as an element along with its own metadata
    as shown in Figure [3.2](types-structures.html#fig:dataframes). Therefore, each
    column is stored in a separate block of memory, and there are no constraints on
    the types of variables that can be stored in different columns. In addition, a
    data frame typically contains its dimensions and the labels of the rows and of
    the columns as metadata, making it possible to access its contents as we would
    with a table. The dimensions are the number of rows and columns of the data frame.
    The labels of the columns (called “column names” in R) can be used to access them
    by their names instead of by their positions in the data frame, which improves
    the readability of code and thus our ability to debug it. It makes the code invariant
    to the data layout as well. The labels of the rows (“row names” in R) serve the
    same function but are not used as often: they usually have no practical use in
    the common case in which we assume that data points are independent and identically
    distributed.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的基本结构是列表：表格数据中的每一列都是一个向量，它作为元素存储，并带有其自己的元数据，如图[3.2](types-structures.html#fig:dataframes)所示。因此，每一列都存储在单独的内存块中，不同列中可以存储的变量类型没有限制。此外，数据框通常包含其维度和行标签以及列标签作为元数据，这使得我们可以像访问表格一样访问其内容。维度是数据框的行数和列数。列的标签（在R中称为“列名”）可以用来通过名称而不是通过数据框中的位置访问它们，这提高了代码的可读性，从而提高了我们的调试能力。它还使代码对数据布局不变。行的标签（“行名”在R中）具有相同的功能，但使用得不太频繁：在常见的假设数据点是独立且同分布的情况下，通常没有实际用途。
- en: 'Data frames make it efficient to operate on columns. Creating a new data frame
    with a subset of columns is like subsetting a list, with the additional step of
    carrying over row and column labels as needed. Copying it can be done efficiently
    with a shallow copy. Adding a column to a data frame is similar: we perform a
    shallow copy into a new data frame with an empty slot in which we can insert the
    vector storing the column’s values. Applying a function to each column of a data
    frame can be vectorised and performed in parallel, and the appropriate method
    can be called for each column in the case of generic functions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框使得对列的操作变得高效。创建一个包含列子集的新数据框就像子集化一个列表，额外步骤是按需携带行和列标签。可以通过浅拷贝高效地复制它。向数据框中添加列的过程类似：我们执行一个浅拷贝到一个新的数据框中，该数据框有一个空槽，我们可以插入存储列值的向量。对数据框的每一列应用函数可以矢量化并并行执行，对于通用函数，可以为每一列调用适当的方法。
- en: However, operating on rows is not efficient in most cases. Adding or removing
    data points involves modifying the length of each column, which will likely involve
    copying the chosen data points to newly-allocated vectors of the appropriate length.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，对行的操作效率不高。添加或删除数据点需要修改每一列的长度，这很可能会涉及将选择的数据点复制到新分配的适当长度的向量中。
- en: 3.2.3 Dense and Sparse Matrices
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 稠密和稀疏矩阵
- en: A matrix is a *homogeneous* two-dimensional data structure holding a grid of
    variables of the same type arranged into rows and columns. It is the programming
    construct that represents the mathematical objects of the same name studied in
    linear algebra. Matrices can be *dense* or *sparse*. Most of the elements of dense
    matrices are informative (that is, non-zero cells) and therefore must be stored
    in the data structure. On the other hand, most of the elements of sparse matrices
    are equal to zero so we can save considerable amounts of memory by storing only
    the locations and the values of the few non-zero elements. We will cover the trade-off
    between speed and memory use for these two types of matrices in Section [4.5.2](algorithms.html#bigO-sparsem)
    while discussing computational complexity.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵是一种**同质**的二维数据结构，它包含一个由相同类型的变量组成的网格，这些变量按行和列排列。它是表示线性代数中同名数学对象的编程结构。矩阵可以是**密集**的或**稀疏**的。密集矩阵的大部分元素都是信息性的（即非零单元格），因此必须存储在数据结构中。另一方面，稀疏矩阵的大部分元素都等于零，因此我们可以通过只存储少数非零元素的位置和值来节省大量的内存。在讨论计算复杂性的同时，我们将在第[4.5.2](algorithms.html#bigO-sparsem)节中介绍这两种类型矩阵在速度和内存使用之间的权衡。
- en: 'In Python, dense matrices are implemented in NumPy as a special case of multidimensional
    arrays along with vectors (Harris et al. [2020](#ref-numpy)). The same is true
    in R. In both cases, the data structure encoding a multidimensional array comprises
    the *pointer* to the first element of the array; the *variable type* of the elements;
    and the *dimensions of the array*, which determine its shape (Figure [3.3](types-structures.html#fig:multidimensional-arrays)).
    The dimensions and the variable type of the elements determine the *strides*:
    the number of bytes skipped in memory to proceed to the next element along a given
    dimension. They are pre-calculated and stored in NumPy but not in R. On the other
    hand, R arrays contain labels for their dimensions (row and column names in the
    case of matrices). The elements are stored as a vector, typically in *column-major
    order*: the columns of the matrix are concatenated starting from the left-most
    one.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，密集矩阵作为多维数组的特例以及向量（Harris等人[2020](#ref-numpy)）在NumPy中实现。在R中也是如此。在这两种情况下，编码多维数组的结构包括数组的第一个元素的**指针**；元素的**变量类型**；以及数组的**维度**，这决定了其形状（图[3.3](types-structures.html#fig:multidimensional-arrays)）。元素的维度和变量类型决定了**步长**：在给定维度上前进到下一个元素时在内存中跳过的字节数。它们在NumPy中预先计算并存储，但在R中不是这样。另一方面，R数组包含其维度的标签（在矩阵的情况下是行和列的名称）。元素作为向量存储，通常在**列主序**中：矩阵的列从最左边开始连接。
- en: '![A schematic view of a dense matrix (left) encoded as a multidimensional array
    with variables stored in column-major format (right).](../Images/1e7a82e4cfe3f8a0799d3e4ca549f856.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![一个密集矩阵的示意图（左），编码为以列主序格式存储变量的多维数组（右）](../Images/1e7a82e4cfe3f8a0799d3e4ca549f856.png)'
- en: 'Figure 3.3: A schematic view of a dense matrix (left) encoded as a multidimensional
    array with variables stored in column-major format (right).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：一个密集矩阵的示意图（左），编码为以列主序格式存储变量的多维数组（右）。
- en: Storing dense matrices as a list of columns, or as a list of rows, is typical
    in C and but it is less common in higher-level languages, where we can use data
    frames for the same purpose.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在C语言中，将密集矩阵存储为列的列表，或存储为行的列表是常见的，但在高级语言中则较少见，在这些语言中，我们可以使用数据框来达到相同的目的。
- en: 'The fact that multidimensional arrays store their dimensions as metadata allows
    three types of operations on their elements. The first is vectorised operations
    in which a function is applied individually to each element. The second is what
    is called *broadcasting* in Python and Julia and *recycling* in R: when a function
    operates on two arrays with different dimensions, the shorter array is repeated
    (that is, virtually concatenated to itself) to make the shapes of the operands
    match. The third is *marginalisation* or *reduction*: aggregating elements across
    one or more dimensions of an array, for instance, by summing or averaging them,
    to produce a second array with fewer dimensions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 多维数组将它们的维度作为元数据存储的事实允许对其元素执行三种类型的操作。第一种是向量化操作，其中函数单独应用于每个元素。第二种是在Python和Julia中称为**广播**，在R中称为**回收**：当函数在具有不同维度的两个数组上操作时，较短的数组被重复（即，虚拟连接到自身）以使操作数的形状相匹配。第三种是**边缘化**或**缩减**：通过求和或平均等方式聚合数组的一个或多个维度的元素，从而产生一个维度更少的第二个数组。
- en: 'Sparse matrices are supported in R through the Matrix package (Bates and Maechler
    [2021](#ref-matrixpkg)) and in Python through the SciPy package (Virtanen et al.
    [2020](#ref-scipy)). For brevity, we will only illustrate in detail the *compressed
    sparse column* data structure that is the most widely used in both packages. Consider
    the sparse matrix shown in Figure [3.4](types-structures.html#fig:matrices) along
    with its compressed representation from the Matrix package. The three vectors
    in the data structure contain, from top to bottom: the *start* and *end indexes*
    of each column (\(C\)), the *row* of each non-zero cell in the matrix (\(R\))
    and its *value* (\(V\)). This representation assumes that the non-zero cells are
    stored in position order, starting from the top-left cell, moving down within
    each column, and considering columns from left to right.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，稀疏矩阵通过 Matrix 包（Bates 和 Maechler [2021](#ref-matrixpkg)）支持，而在 Python 中通过
    SciPy 包（Virtanen 等人 [2020](#ref-scipy)）支持。为了简洁，我们将仅详细说明两种包中最广泛使用的 *压缩稀疏列* 数据结构。考虑图
    [3.4](types-structures.html#fig:matrices) 中所示的稀疏矩阵及其来自 Matrix 包的压缩表示。数据结构中的三个向量从上到下包含：每列的
    *起始* 和 *结束索引* (\(C\))，矩阵中每个非零单元格的 *行* (\(R\)) 和其 *值* (\(V\))。这种表示假设非零单元格按位置顺序存储，从左上角单元格开始，在同一列内向下移动，并从左到右考虑列。
- en: '![A schematic view of a sparse matrix (left) and its compressed sparse column
    (right) format representation.](../Images/dd1cef84f415d779187ba250d16f5dc9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏矩阵（左）及其压缩稀疏列（右）格式表示的示意图。](../Images/dd1cef84f415d779187ba250d16f5dc9.png)'
- en: 'Figure 3.4: A schematic view of a sparse matrix (left) and its compressed sparse
    column (right) format representation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：稀疏矩阵（左）及其压缩稀疏列（右）格式表示的示意图。
- en: 'Say, for instance, that we would like to read the value of the cell (2, 3)
    in the matrix from the data structure. The required steps are:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，我们想从数据结构中读取矩阵中单元格 (2, 3) 的值。所需的步骤是：
- en: Use the column delimiters in \(C\) to find which subset of \(R\) and \(V\) to
    read. The \(i\)th column of the matrix starts at the index stored in the \(i\)th
    element of \(C\) (\(C[3] = 3\)) and ends by the index stored in the \((i + 1)\)th
    element of \(C\) (\(C[4] = 5\)), where the next column starts. This implies that
    there are \(5 - 3 = 2\) non-zero elements in the third column. If the start and
    end indexes are identical, there are no non-zero cells in the column.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 \(C\) 中的列分隔符来找到要读取的 \(R\) 和 \(V\) 的哪个子集。矩阵的第 \(i\) 列从存储在 \(C\) 的第 \(i\) 个元素中的索引开始（\(C[3]
    = 3\)），并在存储在 \((i + 1)\) 个元素中的索引结束（\(C[4] = 5\)），这是下一列开始的地方。这意味着第三列中有 \(5 - 3
    = 2\) 个非零元素。如果起始和结束索引相同，则该列中没有非零单元格。
- en: We read the two row coordinates stored in \(R[3]\) and \(R[4]\).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取存储在 \(R[3]\) 和 \(R[4]\) 中的两个行坐标。
- en: If we do not find the row coordinate we are looking for, the cell has value
    zero.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们没有找到我们正在寻找的行坐标，则该单元格的值为零。
- en: Otherwise, the value of the cell will be stored in the element of \(V\) that
    has the same index as the row coordinate. In our case, the row coordinates of
    the non-zero elements of the third column are \(R[3] = 2\) and \(R[4] = 3\). Row
    coordinate \(2\) is in \(R[3]\), so we can read the corresponding cell value from
    \(V[3]\).
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，单元格的值将存储在具有与行坐标相同索引的 \(V\) 元素中。在我们的例子中，第三列非零元素的行坐标是 \(R[3] = 2\) 和 \(R[4]
    = 3\)。行坐标 \(2\) 在 \(R[3]\) 中，因此我们可以从 \(V[3]\) 读取相应的单元格值。
- en: Other data structures for sparse matrices include the *compressed row column*,
    which is identical to the above save that the roles of \(R\) and \(C\) are reversed
    and the *coordinate list* (called the “triplet format”), which stores row and
    column coordinates directly in \(R\) and \(C\).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵的其他数据结构包括 *压缩行列*，它与上述结构相同，只是 \(R\) 和 \(C\) 的角色相反，以及 *坐标列表*（称为“三元组格式”），它直接在
    \(R\) 和 \(C\) 中存储行和列坐标。
- en: 3.3 Choosing the Right Variable Types for the Job
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 为工作选择正确的变量类型
- en: 'The floating point format used to represent real numbers in Section [3.1.2](types-structures.html#floating-point)
    has a more complex structure than the fixed point format for integer variables
    in Section [3.1.1](types-structures.html#integers). Intuitively, this might suggest
    that the same operation is more efficient on integer variables than on floating
    point variables. This is not usually the case for two reasons. Firstly, high-level
    languages like Python and R have additional checks to deal with integer overflow.
    In R, integers are stored using 32 bits and they are either replaced with `NaN`
    or transformed into double-precision floating point variables when they overflow.
    In base Python, integers are stored with *arbitrary precision*: their size is
    extended as needed to prevent them from overflowing. Pandas (McKinney [2017](#ref-pandas))
    integer variables have size 64 bits, and NumPy (Harris et al. [2020](#ref-numpy))
    provides integer variables in sizes 8, 16, 32 and 64 bits: both can overflow and,
    unlike in R, are not replaced with `NaN`. Consider the following vector inner
    product benchmark in R:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3.1.2](types-structures.html#floating-point) 节中用于表示实数的浮点格式比第 [3.1.1](types-structures.html#integers)
    节中用于整数变量的定点格式结构更复杂。直观上，这可能会让人想到相同的操作在整数变量上比在浮点变量上更有效率。但通常并非如此，原因有两个。首先，像 Python
    和 R 这样的高级语言有额外的检查来处理整数溢出。在 R 中，整数使用 32 位存储，当它们溢出时，要么被替换为 `NaN`，要么转换为双精度浮点变量。在基本的
    Python 中，整数使用 *任意精度* 存储：它们的大小会根据需要扩展，以防止溢出。Pandas (McKinney [2017](#ref-pandas))
    的整数变量大小为 64 位，而 NumPy (Harris et al. [2020](#ref-numpy)) 提供了 8、16、32 和 64 位的整数变量：两者都可以溢出，并且与
    R 不同，不会用 `NaN` 替换。以下是在 R 中的向量内积基准测试示例：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'On average, the inner product takes 196.7% longer with integer vectors than
    it does with double-precision floating point vectors on a 7th-generation Intel
    Core processor. The same benchmark in Python and NumPy is shown below, and the
    results are similar: the inner product takes 40.5% longer with integer vectors.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在第七代英特尔酷睿处理器上，平均而言，使用整数向量的内积比使用双精度浮点向量的内积慢 196.7%。Python 和 NumPy 中的相同基准测试结果如下，结果相似：使用整数向量的内积慢
    40.5%。
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Secondly, it should be apparent from Section [2.1.1](hardware.html#hardware-compute)
    that in recent years much effort has been put into improving hardware support
    for floating point numbers. CPUs, GPUs and TPUs have all been optimised to handle
    single- and double-precision floating point variables with SIMD and FMA instructions
    as much as they have been optimised to handle integer variables, if not more.
    Therefore, depending on the available hardware and on the ability of compilers
    to leverage it, using floating point variables may lead to faster code when using
    low-level languages. However, whether that will be the case for a specific machine
    learning software depends on the exact combination of hardware and software used
    and can only be ascertained by benchmarking it. Matching software and hardware
    was a key point in Section [2.2](hardware.html#hardware-using) and Section [2.4](hardware.html#hardware-choice).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，从第 [2.1.1](hardware.html#hardware-compute) 节中可以看出，近年来，人们投入了大量努力来提高对浮点数的硬件支持。CPU、GPU
    和 TPU 都已经优化，以尽可能多地使用 SIMD 和 FMA 指令来处理单精度和双精度浮点变量，就像它们已经优化来处理整数变量一样，如果不是更多。因此，根据可用的硬件和编译器利用这些硬件的能力，使用浮点变量在低级语言中可能会导致代码运行更快。然而，对于特定的机器学习软件来说，这是否会成为现实取决于所使用的硬件和软件的确切组合，并且只能通过基准测试来确认。匹配软件和硬件在第
    [2.2](hardware.html#hardware-using) 节和第 [2.4](hardware.html#hardware-choice) 节中被视为一个关键点。
- en: 'The size of the variables also matters: we saw in Section [2.1.2](hardware.html#hardware-memory)
    how faster forms of memory are smaller, and how copying data between different
    types of memory can impact operational intensity. We should always choose the
    smallest size of integer or floating point variables that we can handle with SIMD
    and FMA hardware instructions, and that has a large enough range to represent
    the numbers we are working with. The effect on performance is noticeable even
    in the simple Python benchmark above: reducing the size of the floating point
    and integer variables in the vectors from 64 bits (the default) to 32 bits and
    then to 16 bits produces interesting patterns in the normalised running times.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的尺寸也很重要：我们在第 [2.1.2](hardware.html#hardware-memory) 节中看到了更快的内存形式通常更小，以及在不同类型的内存之间复制数据如何影响操作强度。我们应该始终选择我们能够用
    SIMD 和 FMA 硬件指令处理的整数或浮点变量中最小尺寸，并且这个范围足够大，可以表示我们正在处理的数据。这种影响在简单的 Python 基准测试中也很明显：将向量的浮点数和整数变量的尺寸从
    64 位（默认值）减少到 32 位，然后减少到 16 位，在标准化运行时间中产生了有趣的模式。
- en: '| **variable type** | **64 bits** | **32 bits** | **16 bits** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **变量类型** | **64 位** | **32 位** | **16 位** |'
- en: '| --- | --: | --: | --: |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --: | --: | --: |'
- en: '| floating point | 100% | 54.1% | 887.1% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 浮点数 | 100% | 54.1% | 887.1% |'
- en: '| integer | 100% | 62.2% | 61.6% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 整数 | 100% | 62.2% | 61.6% |'
- en: Reducing the size of both floating point and integer variables from 64 to 32
    bits improves the speed of the inner product by a factor of 1.5–2, as we would
    expect. Reducing the size of the variables further to 16 bits provides only marginal
    benefits for integer variables but, surprisingly, slows floating point variables
    by a factor of nearly 9. That is a strong indication that we are unable to leverage
    SIMD and FMA instructions as we do for integers of the same size!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将浮点数和整数变量的尺寸从 64 位减少到 32 位，可以按预期将内积的速度提高 1.5-2 倍。将变量的尺寸进一步减少到 16 位，对于整数变量来说只提供了微小的收益，但出人意料的是，将浮点变量的速度减慢了近
    9 倍。这是一个强烈的迹象，表明我们无法像处理相同尺寸的整数一样利用 SIMD 和 FMA 指令！
- en: 'Size matters even more in the case of floating point variables. As we noted
    in Section [3.1.2](types-structures.html#floating-point), the smaller the precision,
    the larger the floating point errors are likely to be. They also propagate with
    each operation and compound each other. This can become a critical issue with
    variables that are involved in most of the steps of an algorithm, such as the
    *accumulator variables* used to calculate a sum or product of a series of values,
    and with those that are rescaled to predefined ranges with other computed quantities.
    An example from classical statistics is computing the empirical correlation between
    two vectors:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在浮点变量的情况下，尺寸的重要性更大。正如我们在第 [3.1.2](types-structures.html#floating-point) 节中提到的，精度越小，浮点误差可能越大。它们还会随着每个操作而传播，并相互累积。这对于参与算法大多数步骤的变量来说可能成为一个关键问题，例如用于计算一系列值之和或积的
    *累加变量*，以及那些与其他计算量一起缩放到预定义范围的变量。经典统计学中的一个例子是计算两个向量之间的经验相关系数：
- en: We compute the average of each vector, which we store in two accumulator variables.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算每个向量的平均值，并将其存储在两个累加变量中。
- en: We compute the variance of each vector by summing up the squared differences
    from its average, which we store in two more accumulator variables.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将每个向量的平均值的平方差相加来计算每个向量的方差，这些平均值存储在另外两个累加变量中。
- en: We do the same with the cross-product of the differences to compute the covariance,
    which is an additional accumulator variable.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对差异的叉积做同样的处理来计算协方差，这是一个额外的累加变量。
- en: We divide the covariance by the square root of the product of the variances.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将协方差除以方差乘积的平方根。
- en: 'Each of these steps can potentially build up an error large enough to produce
    a correlation coefficient that is either greater than 1 or less than -1\. Floating
    point errors compound and propagate from the means to the variances and the covariances,
    affecting the final division through the accumulator variables that store them.
    In such a situation, we should store accumulator variables with a higher precision
    than the variables they are accumulating to limit the magnitude of the errors
    of the individual operations: for instance, we should store the average of numbers
    stored in single precision as a double precision variable. Using FMA instructions
    may also help because, as we noted in Section [2.1.1](hardware.html#hardware-compute),
    they operate at higher precision and only round their final result. Choosing the
    scale of the numbers being accumulated may also help by keeping all variables
    in a range that is not prone to overflow or underflow. Keeping all variables on
    the same scale also prevents catastrophic loss of precision, particularly in multiplications
    and divisions. This is the reason why so much numeric software works with quantities
    on log-scales: large numbers are reduced in magnitude and do not overflow or lose
    precision easily, and small numbers become large negative numbers instead of underflowing
    or losing precision.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些步骤都可能累积足够大的误差，从而产生一个相关系数，要么大于1，要么小于-1。浮点误差会累积并从均值传播到方差和协方差，影响最终通过存储这些值的累加变量进行的除法操作。在这种情况下，我们应该用比它们所累积的变量更高的精度来存储累加变量，以限制单个操作误差的幅度：例如，我们应该将存储在单精度中的数字的平均值存储为双精度变量。使用FMA指令也可能有所帮助，因为我们已在第[2.1.1](hardware.html#hardware-compute)节中提到，它们在更高的精度上操作，并且只对最终结果进行舍入。选择累加数字的尺度也可能有所帮助，因为它可以保持所有变量在一个不太可能溢出或下溢的范围内。保持所有变量在同一尺度上还可以防止精度灾难性的损失，尤其是在乘法和除法中。这就是为什么许多数值软件都在对数尺度上工作：大数减小幅度，不太容易溢出或丢失精度，而小数则变成大负数，而不是下溢或丢失精度。
- en: Last but not least, floating point rounding may be unacceptable for legal reasons
    in some applications, particularly in finance and accounting. Fixed point integers
    may be used instead, with the convention that the smallest possible amount of
    currency (say, 1/100th of 0.01) is taken as the unit value. A rare open-source
    example of this approach in the commercial world is Oanda’s libfixed (Oanda [2018](#ref-libfixed))
    library.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，在某些应用中，由于法律原因，浮点舍入可能不可接受，尤其是在金融和会计领域。可以使用定点整数代替，按照惯例，最小的货币单位（比如说，0.01的1/100）被用作单位值。在商业世界中，这种方法的罕见开源示例是Oanda的libfixed（Oanda
    [2018](#ref-libfixed)）库。
- en: 3.4 Choosing the Right Data Structures for the Job
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 为任务选择合适的数据结构
- en: The choice of data structures can have an even larger impact than that of variable
    types because it determines memory access patterns. Operational intensity depends
    crucially on efficient memory use and access, as we argued in Sections [2.1.2](hardware.html#hardware-memory)
    and [2.2](hardware.html#hardware-using).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构的选择可能比变量类型的选择影响更大，因为它决定了内存访问模式。操作强度关键取决于有效的内存使用和访问，正如我们在第[2.1.2](hardware.html#hardware-memory)节和第[2.2](hardware.html#hardware-using)节中讨论的那样。
- en: 'Lists can be more memory efficient than vectors if we need to repeatedly subset
    them, for example, because we need to work on combinations or permutations of
    their values. If shallow copies are acceptable, lists are faster to duplicate
    as well because we do not have to allocate memory for and copy their elements.
    If shallow copies are not acceptable, then vectors are faster to copy because
    all their elements are stored as a single block of memory and can be copied with
    a single operation. And they are not less memory efficient, since their size is
    determined by their length. In fact, lists use more memory than vectors because
    they contain pointers to each element in addition to the elements themselves:
    the difference can be significant if the elements are small overall. These considerations
    are important for optimising performance given the effects on memory latency discussed
    in Sections [2.1.2](hardware.html#hardware-memory) and [2.2](hardware.html#hardware-using).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要反复对列表进行子集操作，例如，因为我们需要处理它们的值的组合或排列，列表可能比向量更节省内存。如果浅拷贝是可以接受的，列表复制起来也更快，因为我们不需要为它们的元素分配内存并复制它们。如果浅拷贝不可接受，那么向量复制起来更快，因为所有元素都存储为单个内存块，并且可以单个操作进行复制。而且它们并不比向量内存效率低，因为它们的大小由它们的长度决定。实际上，列表比向量使用更多的内存，因为除了包含元素本身外，它们还包含指向每个元素的指针：如果元素总体较小，这种差异可能是显著的。这些考虑对于优化性能很重要，因为它们会影响内存延迟，这在第
    [2.1.2](hardware.html#hardware-memory) 和 [2.2](hardware.html#hardware-using) 节中讨论过。
- en: We can make similar considerations for data frames and matrices, since data
    frames essentially behave as lists whose elements are the columns storing the
    variables in the tabular data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对数据框和矩阵进行类似的考虑，因为数据框本质上表现为列表，其元素是存储在表格数据中的变量列。
- en: Ideally, we should choose which data structures to use in our code taking into
    account what data structures are used in the libraries and in the other software
    that are part of the machine learning pipeline. If different parts of the pipeline
    encode data and models in different ways, we will be forced to convert between
    them, which is inefficient and increases memory use. For instance, R typically
    imports data as data frames. However, the underlying BLAS and LAPACK code that
    powers many models (all linear regressions among them) requires data to be stored
    as dense matrices in column-major format. Converting a data frame into a matrix
    requires copying all the data into a single memory block, column by column, which
    doubles memory use and wastes processor time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们应该根据机器学习管道中使用的库和其他软件中使用的哪些数据结构来选择我们在代码中使用的哪种数据结构。如果管道的不同部分以不同的方式编码数据和模型，我们将被迫在它们之间进行转换，这既低效又增加了内存使用。例如，R通常将数据导入为数据框。然而，为许多模型提供动力的底层BLAS和LAPACK代码（其中所有线性回归都属于此类）要求数据以列主序格式存储为密集矩阵。将数据框转换为矩阵需要按列将所有数据复制到单个内存块中，这加倍了内存使用并浪费了处理器时间。
- en: We should also choose data structures based on how the algorithms that use them
    access their contents. Data that are processed together should be stored together
    to allow algorithms to perform as few separate memory accesses as possible. For
    instance, if we mostly process whole columns in tabular data, then a data frame
    is ideal because a single column can be efficiently read from memory as a single
    memory block. However, a data frame makes memory access very inefficient if we
    need to process individual rows in various combinations because each variable
    in a row is stored in a separate memory block and because we need to access all
    variables to read each row. If the data are homogeneous, storing them in a dense
    matrix with cells stored in row-major order is a better choice.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在选择数据结构时，也应该基于使用它们的算法如何访问其内容。应该将一起处理的数据存储在一起，以便算法尽可能少地进行单独的内存访问。例如，如果我们主要处理表格数据的整个列，那么数据框是理想的，因为单个列可以作为一个单独的内存块高效地从内存中读取。然而，如果我们需要以各种组合处理单个行，数据框会使内存访问非常低效，因为每一行中的每个变量都存储在单独的内存块中，并且我们需要访问所有变量来读取每一行。如果数据是同质的，将它们存储在以行主序存储单元格的密集矩阵中是更好的选择。
- en: References
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aggarwal, C. C. 2018\. *Machine Learning for Text*. Springer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Aggarwal, C. C. 2018\. *文本机器学习*. Springer.
- en: 'Bates, D., and M. Maechler. 2021\. *Matrix: Sparse and Dense Matrix Classes
    and Methods*. [https://cran.r-project.org/web/packages/Matrix/](https://cran.r-project.org/web/packages/Matrix/).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Bates, D. 和 M. Maechler. 2021\. *矩阵：稀疏和密集矩阵类和方法*. [https://cran.r-project.org/web/packages/Matrix/](https://cran.r-project.org/web/packages/Matrix/).
- en: Brass, P. 2008\. *Advanced Data Structures*. Cambridge University Press.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Brass, P. 2008. *高级数据结构*。剑桥大学出版社。
- en: Cormen, T. H. 2013\. *Algorithms Unlocked*. The MIT Press.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Cormen, T. H. 2013. *算法解锁*。麻省理工学院出版社。
- en: 'Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019\. “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding.” In *Proceedings
    of the Annual Conference of the North American Chapter of the Association for
    Computational Linguistics (NNACL-HLT)*, 4171–86.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. “BERT：用于语言理解的预训练深度双向变换器。”
    在 *北美计算语言学协会（NAACL-HLT）年度会议论文集* 中，4171–86.
- en: 'Explosion. 2021\. *Spacy: Industrial-Strength Natural Language Processing*.
    [https://spacy.io/](https://spacy.io/).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Explosion. 2021. *Spacy：工业级自然语言处理*。[https://spacy.io/](https://spacy.io/).
- en: 'Harris, C. R., K. J. Millman, Stéfan J. van der Walt, R. Gommers, P. Virtanen,
    D. Cournapeau, E. Wieser, et al. 2020\. “Array Programming with NumPy.” *Nature*
    585 (7285): 357–62.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'Harris, C. R., K. J. Millman, Stéfan J. van der Walt, R. Gommers, P. Virtanen,
    D. Cournapeau, E. Wieser, 等人. 2020. “使用NumPy进行数组编程。” *自然* 585 (7285): 357–62.'
- en: McKinney, W. 2017\. *Python for Data Analysis*. 2nd ed. O’Reilly.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: McKinney, W. 2017. *Python数据分析*。第2版。O’Reilly。
- en: 'NLTK Team. 2021\. *NLTK: A Natural Language Toolkit*. [https://www.nltk.org/](https://www.nltk.org/).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK团队. 2021. *NLTK：自然语言工具包*。[https://www.nltk.org/](https://www.nltk.org/).
- en: Oanda. 2018\. *A C++ Fixed Point Math Library Suitable for Financial Applications*.
    [https://github.com/oanda/libfixed](https://github.com/oanda/libfixed).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Oanda. 2018. *适用于金融应用的C++定点数数学库*。[https://github.com/oanda/libfixed](https://github.com/oanda/libfixed).
- en: ONNX. 2021\. *Open Neural Network Exchange*. [https://github.com/onnx/onnx](https://github.com/onnx/onnx).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX. 2021. *开放神经网络交换*。[https://github.com/onnx/onnx](https://github.com/onnx/onnx).
- en: Overton, M. L. 2001\. *Numerical Computing with IEEE Floating Point Arithmetic*.
    SIAM.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Overton, M. L. 2001. *使用IEEE浮点算术进行数值计算*。SIAM。
- en: 'Pennington, J., R. Socher, and C. Manning. 2014\. “Glove: Global Vectors for
    Word Representation.” In *Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, 1532–43.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Pennington, J., R. Socher, and C. Manning. 2014. “Glove：全局词向量表示。” 在 *2014年实证自然语言处理会议（EMNLP）论文集*
    中，1532–43.
- en: Rong, X. 2014\. “Word2vec Parameter Learning Explained.” *arXiv Preprint arXiv:1411.2738*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Rong, X. 2014. “Word2vec参数学习解释。” *arXiv预印本arXiv:1411.2738*.
- en: 'Rump, S. M. 2020a. “Addendum to ’On Recurrences Converging to the Wrong Limit
    in Finite Precision’.” *Electronic Transactions on Numerical Analysis* 52: 571–75.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rump, S. M. 2020a. “关于有限精度中收敛到错误极限的递归的补充。” *电子数值分析交易* 52: 571–75.'
- en: 'Rump, S. 2020b. “On Recurrences Converging to the Wrong Limit in Finite Precision.”
    *Electronic Transactions on Numerical Analysis* 52: 358–69.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rump, S. 2020b. “关于有限精度中收敛到错误极限的递归。” *电子数值分析交易* 52: 358–69.'
- en: 'Scikit-learn Developers. 2022\. *Scikit-learn: Machine Learning in Python*.
    [https://scikit-learn.org/](https://scikit-learn.org/).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn开发者. 2022. *Scikit-learn：Python机器学习*。[https://scikit-learn.org/](https://scikit-learn.org/).
- en: Unicode. 2021\. *Unicode Technical Documentation*. [https://www.unicode.org/main.html](https://www.unicode.org/main.html).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Unicode. 2021. *Unicode技术文档*。[https://www.unicode.org/main.html](https://www.unicode.org/main.html).
- en: 'Virtanen, P., R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
    E. Burovski, et al. 2020\. “SciPy 1.0: Fundamental Algorithms for Scientific Computing
    in Python.” *Nature Methods* 17: 261–72.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'Virtanen, P., R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
    E. Burovski, 等人. 2020. “SciPy 1.0：Python科学计算的基石算法。” *自然方法* 17: 261–72.'
- en: '* * *'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: If we use the colour in a regression model, the effect of “green” on the response
    will be twice that of “red”, which clearly does not make any sense since the numbers
    associated with the colours are arbitrary.[↩︎](types-structures.html#fnref7)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在回归模型中使用颜色，"绿色"对响应的影响将是"红色"的两倍，这显然没有意义，因为与颜色相关的数字是任意的。[↩︎](types-structures.html#fnref7)
- en: One-hot encoding is a particular case of what are known as *contrasts* in statistics.
    Since they are collinear, we usually drop one before using them in a model.[↩︎](types-structures.html#fnref8)
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: One-hot编码是统计学中所谓的*对比*的一个特例。由于它们是共线的，我们通常在使用模型之前删除一个。[↩︎](types-structures.html#fnref8)
