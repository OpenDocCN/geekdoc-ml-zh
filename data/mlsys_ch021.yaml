- en: Security & Privacy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全与隐私
- en: '*DALL·E 3 Prompt: An illustration on privacy and security in machine learning
    systems. The image shows a digital landscape with a network of interconnected
    nodes and data streams, symbolizing machine learning algorithms. In the foreground,
    there’s a large lock superimposed over the network, representing privacy and security.
    The lock is semi-transparent, allowing the underlying network to be partially
    visible. The background features binary code and digital encryption symbols, emphasizing
    the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays,
    suggesting a high-tech, digital environment.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：一幅关于机器学习系统中隐私和安全的插图。图中展示了一个由相互连接的节点和数据流组成的数字景观，象征着机器学习算法。在前景中，有一个大锁叠加在网络上，代表隐私和安全。锁是半透明的，允许部分可见底下的网络。背景有二进制代码和数字加密符号，强调网络安全主题。色彩方案是蓝色、绿色和灰色的混合，暗示了一个高科技、数字的环境。*'
- en: '![](../media/file228.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file228.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why do privacy and security determine whether machine learning systems achieve
    widespread adoption and societal trust?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么隐私和安全决定了机器学习系统能否得到广泛采用和社会信任？*'
- en: Machine learning systems require unprecedented access to personal data, institutional
    knowledge, and behavioral patterns to function effectively, creating tension between
    utility and protection that determines societal acceptance. Unlike traditional
    software that processes data transiently, ML systems learn from sensitive information
    and embed patterns into persistent models that can inadvertently reveal private
    details. This capability creates systemic risks extending beyond individual privacy
    violations to threaten institutional trust, competitive advantages, and democratic
    governance. Success of machine learning deployment across critical domains (healthcare,
    finance, education, and public services) depends entirely on establishing robust
    security and privacy foundations enabling beneficial use while preventing harmful
    exposure. Without these protections, even the most capable systems remain unused
    due to legal, ethical, and practical concerns. Understanding privacy and security
    principles enables engineers to design systems achieving both technical excellence
    and societal acceptance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统需要前所未有的访问个人数据、机构知识和行为模式才能有效运行，这在社会接受度上产生了效用和保护之间的紧张关系。与处理数据瞬时的传统软件不同，ML系统从敏感信息中学习，并将模式嵌入到持久模型中，这些模型可能会无意中泄露私人细节。这种能力创造了系统性风险，不仅超越了个人隐私侵犯，还威胁到机构信任、竞争优势和民主治理。机器学习在关键领域（医疗保健、金融、教育和公共服务）的成功部署完全取决于建立强大的安全和隐私基础，以实现有益的使用同时防止有害的暴露。没有这些保护，即使是最有能力的系统也因法律、伦理和实际担忧而无法使用。理解隐私和安全原则使工程师能够设计出既具有技术卓越性又得到社会认可的系统。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Distinguish between security and privacy concerns in machine learning systems
    using formal definitions and threat models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用形式定义和威胁模型来区分机器学习系统中的安全和隐私问题
- en: Analyze historical security incidents to extract principles applicable to ML
    system vulnerabilities
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析历史安全事件，提取适用于机器学习系统漏洞的原则
- en: Classify ML threats across model, data, and hardware attack surfaces
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型、数据和硬件攻击面上对机器学习威胁进行分类
- en: Evaluate privacy-preserving techniques including differential privacy, federated
    learning, and synthetic data generation for specific use cases
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估包括差分隐私、联邦学习和合成数据生成在内的隐私保护技术在特定用例中的应用
- en: Design layered defense architectures that integrate data protection, model security,
    and hardware trust mechanisms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计集成数据保护、模型安全和硬件信任机制的分层防御架构
- en: Implement basic security controls including access management, encryption, and
    input validation for ML systems
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习系统实施基本的安全控制措施，包括访问管理、加密和输入验证
- en: Assess trade-offs between security measures and system performance using quantitative
    cost-benefit analysis
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用定量成本效益分析来评估安全措施与系统性能之间的权衡
- en: Apply the three-phase security roadmap to prioritize defenses based on organizational
    threat models and risk tolerance
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用三阶段安全路线图，根据组织威胁模型和风险承受能力优先考虑防御措施
- en: Security and Privacy in ML Systems
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习系统中的安全与隐私
- en: The shift from centralized training architectures to distributed, adaptive machine
    learning systems has altered the threat landscape and security requirements for
    modern ML infrastructure. Contemporary machine learning systems, as examined in
    [Chapter 14](ch020.xhtml#sec-ondevice-learning), increasingly operate across heterogeneous
    computational environments spanning edge devices, federated networks, and hybrid
    cloud deployments. This architectural evolution enables new capabilities in adaptive
    intelligence but introduces attack vectors and privacy vulnerabilities that traditional
    cybersecurity frameworks cannot adequately address.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从集中式训练架构向分布式、自适应机器学习系统的转变改变了现代机器学习基础设施的威胁格局和安全要求。[第14章](ch020.xhtml#sec-ondevice-learning)中考察的当代机器学习系统越来越多地运行在跨越边缘设备、联邦网络和混合云部署的异构计算环境中。这种架构演变使得自适应智能的新能力成为可能，但同时也引入了攻击向量和隐私漏洞，这些是传统的网络安全框架无法充分解决的。
- en: Machine learning systems exhibit different security characteristics compared
    to conventional software applications. Traditional software systems process data
    transiently and deterministically, whereas machine learning systems extract and
    encode patterns from training data into persistent model parameters. This learned
    knowledge representation creates unique vulnerabilities where sensitive information
    can be inadvertently memorized and later exposed through model outputs or systematic
    interrogation. Such risks manifest across domains from healthcare systems that
    may leak patient information to proprietary models that can be reverse-engineered
    through strategic query patterns, threatening both individual privacy and organizational
    intellectual property.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的软件应用相比，机器学习系统表现出不同的安全特性。传统的软件系统处理数据是瞬时的和确定的，而机器学习系统则从训练数据中提取和编码模式，将其编码成持久的模型参数。这种学习到的知识表示创建了独特的漏洞，敏感信息可能会无意中被记住，并在模型输出或系统询问后暴露出来。这种风险在各个领域都有所体现，从可能泄露患者信息的医疗系统到可以通过战略查询模式逆向工程的自有模型，都威胁到个人隐私和组织的知识产权。
- en: The architectural complexity of machine learning systems, as detailed in [Chapter 2](ch008.xhtml#sec-ml-systems),
    compounds these security challenges through multi-layered attack surfaces. Contemporary
    ML deployments include data ingestion pipelines, distributed training infrastructure,
    model serving systems, and continuous monitoring frameworks. Each architectural
    component introduces distinct vulnerabilities while privacy concerns affect the
    entire computational stack. The distributed nature of modern deployments, with
    continuous adaptation at edge nodes and federated coordination protocols, expands
    the attack surface while complicating comprehensive security implementation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch008.xhtml#sec-ml-systems)中详细所述，机器学习系统的架构复杂性通过多层攻击表面加剧了这些安全挑战。当代机器学习部署包括数据摄取管道、分布式训练基础设施、模型服务系统和持续监控框架。每个架构组件都引入了独特的漏洞，而隐私问题影响了整个计算栈。现代部署的分布式特性，包括边缘节点的持续适应和联邦协调协议，扩大了攻击面，同时使全面的安全实现复杂化。
- en: Addressing these challenges requires systematic approaches that integrate security
    and privacy considerations throughout the machine learning system lifecycle. This
    chapter establishes the foundations and methodologies necessary for engineering
    ML systems that achieve both computational effectiveness and trustworthy operation.
    We examine the application of established security principles to machine learning
    contexts, identify threat models specific to learning systems, and present comprehensive
    defense strategies that include data protection mechanisms, secure model architectures,
    and hardware-based security implementations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要系统性的方法，这些方法将安全和隐私考虑贯穿于机器学习系统的整个生命周期。本章建立了构建既具有计算有效性又具有可信操作的机器学习系统所需的基础和方法。我们探讨了将已建立的安全原则应用于机器学习环境，确定了特定于学习系统的威胁模型，并提出了包括数据保护机制、安全模型架构和基于硬件的安全实现在内的综合防御策略。
- en: Our investigation proceeds through four interconnected frameworks. We begin
    by establishing distinctions between security and privacy within machine learning
    contexts, then examine evidence from historical security incidents to inform contemporary
    threat assessment. We analyze vulnerabilities that emerge from the learning process
    itself, before presenting layered defense architectures that span cryptographic
    data protection, adversarial-robust model design, and hardware security mechanisms.
    Throughout this analysis, we emphasize implementation guidance that enables practitioners
    to develop systems meeting both technical performance requirements and the trust
    standards necessary for societal deployment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查通过四个相互关联的框架进行。我们首先在机器学习环境中建立安全和隐私之间的区别，然后通过分析历史安全事件来了解当代威胁评估。我们分析学习过程中出现的漏洞，然后介绍跨越加密数据保护、对抗鲁棒模型设计和硬件安全机制的分层防御架构。在整个分析过程中，我们强调实施指南，使从业者能够开发出满足技术性能要求和社会部署所需信任标准的系统。
- en: Foundational Concepts and Definitions
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础概念和定义
- en: Security and privacy are core concerns in machine learning system design, but
    they are often misunderstood or conflated. Both aim to protect systems and data,
    yet they do so in different ways, address different threat models, and require
    distinct technical responses. For ML systems, distinguishing between the two helps
    guide the design of robust and responsible infrastructure.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和隐私是机器学习系统设计中的核心关注点，但它们往往被误解或混淆。两者都旨在保护系统和数据，但它们以不同的方式、针对不同的威胁模型，并需要不同的技术响应。对于机器学习系统，区分两者有助于指导设计稳健和负责任的基础设施。
- en: Security Defined
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全的定义
- en: Security in machine learning focuses on defending systems from adversarial behavior.
    This includes protecting model parameters, training pipelines, deployment infrastructure,
    and data access pathways from manipulation or misuse.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的安全关注于防御系统免受对抗性行为的侵害。这包括保护模型参数、训练管道、部署基础设施和数据访问路径免受操纵或滥用。
- en: '***Security*** is the protection of ML system *data*, *models*, and *infrastructure*
    from *unauthorized access*, *manipulation*, and *disruption* through *defensive
    mechanisms* spanning development, deployment, and operational environments.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***安全*** 是通过在开发、部署和运营环境中跨越防御机制来保护机器学习系统的 *数据*、*模型* 和 *基础设施*，以防止 *未经授权的访问*、*操纵*
    和 *破坏*。'
- en: '*Example*: A facial recognition system deployed in public transit infrastructure
    may be targeted with adversarial inputs that cause it to misidentify individuals
    or fail entirely. This is a runtime security vulnerability that threatens both
    accuracy and system availability.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例*：部署在公共交通基础设施中的面部识别系统可能会受到对抗性输入的影响，导致其错误识别个人或完全失败。这是一个运行时安全漏洞，威胁到准确性和系统可用性。'
- en: Privacy Defined
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私的定义
- en: While security addresses adversarial threats, privacy focuses on limiting the
    exposure and misuse of sensitive information within ML systems. This includes
    protecting training data, inference inputs, and model outputs from leaking personal
    or proprietary information, even when systems operate correctly and no explicit
    attack is taking place.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然安全解决对抗性威胁，但隐私关注的是限制在机器学习系统中敏感信息的暴露和滥用。这包括保护训练数据、推理输入和模型输出，防止泄露个人信息或专有信息，即使系统运行正确且没有发生明确的攻击。
- en: '***Privacy*** is the protection of *sensitive information* from *unauthorized
    disclosure*, *inference*, and *misuse* through methods that preserve *confidentiality*
    and *control over data usage* across ML system environments.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***隐私*** 是通过在机器学习系统环境中保持 *机密性* 和对数据使用 *控制* 的方法来保护 *敏感信息*，以防止 *未经授权的披露*、*推断*
    和 *滥用*。'
- en: '*Example*: A language model trained on medical transcripts may inadvertently
    memorize snippets of patient conversations. If a user later triggers this content
    through a public-facing chatbot, it represents a privacy failure, even in the
    absence of an attacker.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例*：在医疗记录上训练的语言模型可能会无意中记住患者对话的片段。如果用户后来通过面向公众的聊天机器人触发此内容，即使没有攻击者，这也代表了一种隐私失败。'
- en: Security versus Privacy
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全与隐私
- en: Although they intersect in some areas (encrypted storage supports both), security
    and privacy differ in their objectives, threat models, and typical mitigation
    strategies. [Table 15.1](ch021.xhtml#tbl-security-privacy-comparison) below summarizes
    these distinctions in the context of machine learning systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们在某些领域有交集（加密存储支持两者），但安全和隐私在目标、威胁模型和典型的缓解策略上有所不同。[表15.1](ch021.xhtml#tbl-security-privacy-comparison)以下总结了在机器学习系统背景下这些区别。
- en: 'Table 15.1: **Security-Privacy Distinctions**: Machine learning systems require
    distinct approaches to security and privacy; security mitigates adversarial threats
    targeting system functionality, while privacy protects sensitive information from
    both intentional and unintentional exposure through data leakage or re-identification.
    This table clarifies how differing goals and threat models shape the specific
    concerns and mitigation strategies for each domain.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.1：**安全-隐私区别**：机器学习系统需要针对安全和隐私采取不同的方法；安全缓解针对系统功能的目标攻击，而隐私保护则防止敏感信息因数据泄露或再识别而有意或无意地暴露。此表说明了不同的目标和威胁模型如何塑造每个领域的特定关注点和缓解策略。
- en: '| **Aspect** | **Security** | **Privacy** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **安全** | **隐私** |'
- en: '| --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Primary Goal** | Prevent unauthorized access or disruption | Limit exposure
    of sensitive information |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **主要目标** | 防止未授权访问或破坏 | 限制敏感信息的暴露 |'
- en: '| **Threat Model** | Adversarial actors (external or internal) | Honest-but-curious
    observers or passive leaks |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **威胁模型** | 对抗性参与者（外部或内部） | 诚实但好奇的观察者或被动泄露 |'
- en: '| **Typical Concerns** | Model theft, poisoning, evasion attacks | Data leakage,
    re-identification, memorization |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **典型关注点** | 模型窃取、投毒攻击、规避攻击 | 数据泄露、再识别、记忆化 |'
- en: '| **Example Attack** | Adversarial inputs cause misclassification | Model inversion
    reveals training data |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **示例攻击** | 对抗性输入导致误分类 | 模型反演揭示训练数据 |'
- en: '| **Representative Defenses** | Access control, adversarial training | Differential
    privacy, federated learning |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **代表性防御措施** | 访问控制、对抗性训练 | 差分隐私、联邦学习 |'
- en: '| **Relevance to Regulation** | Emphasized in cybersecurity standards | Central
    to data protection laws (e.g., GDPR) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **与法规的相关性** | 在网络安全标准中被强调 | 在数据保护法律中居核心地位（例如，GDPR） |'
- en: Security-Privacy Interactions and Trade-offs
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全-隐私交互和权衡
- en: Security and privacy are deeply interrelated but not interchangeable. A secure
    system helps maintain privacy by restricting unauthorized access to models and
    data. Privacy-preserving designs can improve security by reducing the attack surface,
    for example, minimizing the retention of sensitive data reduces the risk of exposure
    if a system is compromised.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和隐私紧密相关但不可互换。一个安全的系统通过限制对模型和数据的未授权访问来帮助维护隐私。隐私保护的设计可以通过减少攻击面来提高安全性，例如，最小化敏感数据的保留可以降低系统被破坏时数据泄露的风险。
- en: However, they can also be in tension. Techniques like differential privacy[1](#fn1)
    reduce memorization risks but may lower model utility. Similarly, encryption enhances
    security but may obscure transparency and auditability, complicating privacy compliance.
    In machine learning systems, designers must reason about these trade-offs holistically.
    Systems that serve sensitive domains, including healthcare, finance, and public
    safety, must simultaneously protect against both misuse (security) and overexposure
    (privacy). Understanding the boundaries between these concerns is essential for
    building systems that are performant, trustworthy, and legally compliant.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们也可能存在紧张关系。差分隐私[1](#fn1)技术可以降低记忆化风险，但可能会降低模型效用。同样，加密增强了安全性，但可能会模糊透明性和可审计性，从而复杂化隐私合规性。在机器学习系统中，设计者必须全面地考虑这些权衡。服务于敏感领域（包括医疗保健、金融和公共安全）的系统必须同时防止滥用（安全）和过度暴露（隐私）。理解这些关注点之间的界限对于构建性能良好、值得信赖且符合法律要求的系统至关重要。
- en: Learning from Security Breaches
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从安全漏洞中学习
- en: Having established the conceptual foundations of security and privacy, we now
    examine how these principles manifest in real-world systems through landmark security
    incidents. These historical cases provide concrete illustrations of the abstract
    concepts we’ve defined, showing how security vulnerabilities emerge and propagate
    through complex systems. More importantly, they reveal universal patterns (supply
    chain compromise, insufficient isolation, and weaponized endpoints) that directly
    apply to modern machine learning deployments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了安全和隐私的概念基础之后，我们现在考察这些原则如何在现实世界系统中通过里程碑式的安全事件体现出来。这些历史案例为我们所定义的抽象概念提供了具体的说明，展示了安全漏洞如何在复杂系统中产生和传播。更重要的是，它们揭示了普遍存在的模式（供应链破坏、隔离不足和武器化端点），这些模式直接适用于现代机器学习部署。
- en: Valuable lessons can be drawn from well-known security breaches across a range
    of computing systems. Understanding how these patterns apply to modern ML deployments,
    which increasingly operate across cloud, edge, and embedded environments, provides
    important lessons for securing machine learning systems. These incidents demonstrate
    how weaknesses in system design can lead to widespread, and sometimes physical,
    consequences. Although the examples discussed in this section do not all involve
    machine learning directly, they provide important insights into designing secure
    systems. These lessons apply to machine learning applications deployed across
    cloud, edge, and embedded environments.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种计算系统中的知名安全漏洞中可以吸取宝贵的教训。理解这些模式如何适用于现代机器学习部署，这些部署越来越多地运行在云、边缘和嵌入式环境中，为保护机器学习系统提供了重要的经验教训。这些事件展示了系统设计中的弱点如何导致广泛的影响，有时甚至是物理影响。尽管本节讨论的例子并不都直接涉及机器学习，但它们为设计安全系统提供了重要的见解。这些经验教训适用于部署在云、边缘和嵌入式环境中的机器学习应用。
- en: 'Supply Chain Compromise: Stuxnet'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应链破坏：Stuxnet
- en: In 2010, security researchers discovered a highly sophisticated computer worm
    later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[2](#fn2),
    which targeted industrial control systems used in Iran’s Natanz nuclear facility
    ([Farwell and Rohozinski 2011](ch058.xhtml#ref-farwell2011stuxnet)). Stuxnet exploited
    four previously unknown “[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)”[3](#fn3)
    vulnerabilities in Microsoft Windows, allowing it to spread undetected through
    networked and isolated systems.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年，安全研究人员发现了一种高度复杂的计算机蠕虫，后来被命名为[Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[2](#fn2)，它针对伊朗纳坦兹核设施中使用的工业控制系统([Farwell
    and Rohozinski 2011](ch058.xhtml#ref-farwell2011stuxnet))。Stuxnet利用了微软Windows中四个之前未知的“[零日漏洞](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)”[3](#fn3)，使其能够在网络化和隔离系统中无痕传播。
- en: Unlike typical malware designed to steal information or perform espionage, Stuxnet
    was engineered to cause physical damage. Its objective was to disrupt uranium
    enrichment by sabotaging the centrifuges used in the process. Despite the facility
    being air-gapped[4](#fn4) from external networks, the malware is believed to have
    entered the system via an infected USB device[5](#fn5), demonstrating how physical
    access can compromise isolated environments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与旨在窃取信息或执行间谍活动的典型恶意软件不同，Stuxnet被设计用来造成物理损害。其目标是通过对过程中使用的离心机进行破坏来干扰铀浓缩。尽管该设施与外部网络断开连接[4](#fn4)，但据信恶意软件是通过一个受感染的USB设备[5](#fn5)进入系统的，这表明物理访问可以破坏隔离环境。
- en: The worm specifically targeted programmable logic controllers (PLCs), industrial
    computers that automate electromechanical processes such as controlling the speed
    of centrifuges. By exploiting vulnerabilities in the Windows operating system
    and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly
    targeted, real-world disruption. This represents a landmark in cybersecurity,
    demonstrating how malicious software can bridge the digital and physical worlds
    to manipulate industrial infrastructure.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该蠕虫专门针对可编程逻辑控制器（PLC），这些工业计算机自动化了诸如控制离心机速度等机电过程。通过利用Windows操作系统和用于编程PLC的西门子Step7软件中的漏洞，Stuxnet实现了高度针对性的现实世界破坏。这标志着网络安全的一个里程碑，展示了恶意软件如何跨越数字和物理世界来操纵工业基础设施。
- en: 'The lessons from Stuxnet directly apply to modern ML systems. Training pipelines
    and model repositories face persistent supply chain risks analogous to those exploited
    by Stuxnet. Just as Stuxnet compromised industrial systems through infected USB
    devices and software vulnerabilities, modern ML systems face multiple attack vectors:
    compromised dependencies (malicious packages in PyPI/conda repositories), malicious
    training data (poisoned datasets on HuggingFace, Kaggle), backdoored model weights
    (trojan models in model repositories), and tampered hardware drivers (compromised
    NVIDIA CUDA libraries, firmware backdoors in AI accelerators).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Stuxnet 的教训直接适用于现代机器学习系统。训练管道和模型仓库面临着与 Stuxnet 所利用的类似的持续供应链风险。正如 Stuxnet 通过感染的可移动存储设备和软件漏洞来破坏工业系统一样，现代机器学习系统面临着多个攻击向量：受损害的依赖项（PyPI/conda
    仓库中的恶意软件包）、恶意训练数据（HuggingFace、Kaggle 上的中毒数据集）、后门模型权重（模型仓库中的特洛伊木马模型）和篡改的硬件驱动程序（受损害的
    NVIDIA CUDA 库、AI 加速器中的固件后门）。
- en: 'A concrete ML attack scenario illustrates these risks: an attacker uploads
    a backdoored image classification model to a popular model repository, trained
    to misclassify specific patterns while maintaining normal accuracy on clean data.
    When deployed in autonomous vehicles, this backdoored model correctly identifies
    most objects but fails to detect pedestrians wearing specific patterns, creating
    safety risks. The attack propagates through automated model deployment pipelines,
    affecting thousands of vehicles before detection.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的机器学习攻击场景说明了这些风险：攻击者上传了一个后门图像分类模型到一个流行的模型仓库，该模型被训练来错误分类特定模式，同时在干净数据上保持正常精度。当部署在自动驾驶汽车中时，这个后门模型正确地识别了大多数物体，但未能检测到穿着特定模式的行人，从而产生了安全风险。攻击通过自动化的模型部署管道传播，在检测到之前影响了数千辆车。
- en: 'Defending against such supply chain attacks requires end-to-end security measures:
    (1) cryptographic verification to sign all model artifacts, datasets, and dependencies
    with cryptographic signatures; (2) provenance tracking to maintain immutable logs
    of all training data sources, code versions, and infrastructure used; (3) integrity
    validation to implement automated scanning for model backdoors, dependency vulnerabilities,
    and dataset poisoning before deployment; (4) air-gapped training to isolate sensitive
    model training in secure environments with controlled dependency management. [Figure 15.1](ch021.xhtml#fig-stuxnet)
    illustrates how these supply chain compromise patterns apply across both industrial
    and ML systems.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 防御此类供应链攻击需要端到端的安全措施：(1) 加密验证，使用加密签名对所有模型工件、数据集和依赖项进行签名；(2) 原因追踪，维护所有训练数据源、代码版本和使用的不可变日志；(3)
    完整性验证，在部署前实施自动扫描以检测模型后门、依赖项漏洞和数据集中毒；(4) 空隔训练，在具有受控依赖项管理的安全环境中隔离敏感模型训练。[图 15.1](ch021.xhtml#fig-stuxnet)
    展示了这些供应链破坏模式如何适用于工业和机器学习系统。
- en: '![](../media/file229.svg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file229.svg)'
- en: 'Figure 15.1: **Stuxnet**: Targets PLCs by exploiting Windows and Siemens software
    vulnerabilities, demonstrating supply chain compromise that enabled digital malware
    to cause physical infrastructure damage. Modern ML systems face analogous risks
    through compromised training data, backdoored dependencies, and tampered model
    weights. [Figure 15.1](ch021.xhtml#fig-stuxnet)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1：**Stuxnet**：通过利用 Windows 和西门子软件漏洞来针对 PLC，展示了供应链被破坏的情况，这使得数字恶意软件能够造成物理基础设施损坏。现代机器学习系统面临着类似的通过受损害的训练数据、后门依赖项和篡改的模型权重带来的风险。[图
    15.1](ch021.xhtml#fig-stuxnet)
- en: 'Insufficient Isolation: Jeep Cherokee Hack'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 孤立不足：Jeep Cherokee 漏洞
- en: The 2015 Jeep Cherokee hack demonstrated how connectivity in everyday products
    creates new vulnerabilities. Security researchers publicly demonstrated a remote
    cyberattack on a Jeep Cherokee that exposed important vulnerabilities in automotive
    system design ([Miller and Valasek 2015](ch058.xhtml#ref-miller2015remote); [Miller
    2019](ch058.xhtml#ref-miller2019lessons)). Conducted as a controlled experiment,
    the researchers exploited a vulnerability in the vehicle’s Uconnect entertainment
    system, which was connected to the internet via a cellular network. By gaining
    remote access to this system, they sent commands that affected the vehicle’s engine,
    transmission, and braking systems without physical access to the car.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年Jeep Cherokee的破解展示了日常产品中的连接性如何创造出新的漏洞。安全研究人员公开演示了对一辆Jeep Cherokee的远程网络攻击，揭示了汽车系统设计中重要的漏洞（[Miller和Valasek
    2015](ch058.xhtml#ref-miller2015remote); [Miller 2019](ch058.xhtml#ref-miller2019lessons)）。作为一个控制实验，研究人员利用了车辆Uconnect娱乐系统中的漏洞，该系统通过蜂窝网络连接到互联网。通过远程访问该系统，他们发送了影响车辆引擎、变速器和制动系统的命令，而无需物理接触汽车。
- en: This demonstration served as a wake-up call for the automotive industry, highlighting
    the risks posed by the growing connectivity of modern vehicles. Traditionally
    isolated automotive control systems, such as those managing steering and braking,
    were shown to be vulnerable when exposed through externally accessible software
    interfaces. The ability to remotely manipulate safety-critical functions raised
    serious concerns about passenger safety, regulatory oversight, and industry best
    practices.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这次演示对汽车行业起到了警钟的作用，突显了现代车辆日益增长的连接性所带来的风险。传统上隔离的汽车控制系统，如管理转向和制动的系统，在通过外部可访问的软件接口暴露时被发现是脆弱的。远程操纵关键功能的可能性引发了关于乘客安全、监管监督和行业最佳实践的严重担忧。
- en: The incident also led to a recall of over 1.4 million vehicles to patch the
    vulnerability[6](#fn6), highlighting the need for manufacturers to prioritize
    cybersecurity in their designs. The National Highway Traffic Safety Administration
    (NHTSA)[7](#fn7) issued guidelines for automakers to improve vehicle cybersecurity,
    including recommendations for secure software development practices and incident
    response protocols.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该事件还导致超过140万辆汽车召回以修复漏洞[6](#fn6)，突显了制造商在设计时优先考虑网络安全的必要性。美国国家公路交通安全管理局（NHTSA）[7](#fn7)发布了针对汽车制造商的指南，以改善车辆网络安全，包括关于安全软件开发实践和事件响应协议的建议。
- en: The Jeep Cherokee hack offers critical lessons for ML system security. Connected
    ML systems require strict isolation between external interfaces and safety-critical
    components, as this incident dramatically illustrated. The architectural flaw
    (allowing external interfaces to reach safety-critical functions) directly threatens
    modern ML deployments where inference APIs often connect to physical actuators
    or critical systems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Jeep Cherokee的破解为机器学习系统安全提供了关键教训。连接的机器学习系统需要在外部接口和关键安全组件之间进行严格的隔离，正如这一事件所戏剧性地展示的那样。架构缺陷（允许外部接口访问关键功能）直接威胁到现代机器学习部署，其中推理API通常连接到物理执行器或关键系统。
- en: 'Modern ML attack vectors exploit these same isolation failures across multiple
    domains: (1) Autonomous vehicles where compromised infotainment system ML APIs
    (voice recognition, navigation) gain access to perception models controlling steering
    and braking; (2) Smart home systems where exploited voice assistant wake-word
    detection models provide backdoor access to security systems, door locks, and
    cameras; (3) Industrial IoT where compromised edge ML inference endpoints (predictive
    maintenance, anomaly detection) manipulate actuator control logic in manufacturing
    systems; (4) Medical devices where attacked diagnostic ML models influence treatment
    recommendations and drug delivery systems.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习攻击向量利用了这些相同的隔离失败，跨越多个领域：（1）自动驾驶汽车中，受损害的信息娱乐系统机器学习API（语音识别、导航）获取控制转向和制动的感知模型；（2）智能家居系统中，被利用的语音助手唤醒词检测模型为安全系统、门锁和摄像头提供后门访问；（3）工业物联网中，受损害的边缘机器学习推理端点（预测性维护、异常检测）操纵制造系统中的执行器控制逻辑；（4）医疗设备中，被攻击的诊断机器学习模型影响治疗建议和药物输送系统。
- en: 'Consider a concrete attack scenario: a smart home voice assistant processes
    user commands through cloud-based NLP models. An attacker exploits a vulnerability
    in the voice processing API to inject malicious commands that bypass authentication.
    Through insufficient network segmentation, the compromised voice system gains
    access to the home security ML model responsible for facial recognition door unlocking,
    allowing unauthorized physical access.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的攻击场景：智能家居语音助手通过云端的自然语言处理模型处理用户命令。攻击者利用语音处理 API 的漏洞注入恶意命令，绕过认证。由于网络分段不足，被攻陷的语音系统获得了访问家庭安全机器学习模型（负责面部识别门解锁）的权限，从而允许未经授权的物理访问。
- en: 'Effective defense requires comprehensive isolation architecture: (1) network
    segmentation to isolate ML inference networks from actuator control networks using
    firewalls and VPNs; (2) API authentication requiring cryptographic authentication
    for all ML API calls with rate limiting and anomaly detection; (3) privilege separation
    to run inference models in sandboxed environments with minimal system permissions;
    (4) fail-safe defaults that design actuator control logic to revert to safe states
    (locked doors, stopped motors) when ML systems detect anomalies or lose connectivity;
    (5) monitoring that implements real-time logging and alerting for suspicious ML
    API usage patterns.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的防御需要全面的隔离架构：(1) 使用防火墙和 VPN 将机器学习推理网络与执行器控制网络隔离开来；(2) API 认证要求对所有机器学习 API
    调用进行加密认证，并实施速率限制和异常检测；(3) 权限分离，在沙盒环境中运行推理模型，以最小的系统权限运行；(4) 失效安全默认设置，当机器学习系统检测到异常或失去连接时，将执行器控制逻辑设计为恢复到安全状态（锁定的门，停止的电机）；(5)
    监控实现实时日志记录和针对可疑机器学习 API 使用模式的警报。
- en: 'Weaponized Endpoints: Mirai Botnet'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 武器化终端：Mirai 恶意软件
- en: While the Jeep Cherokee hack demonstrated targeted exploitation of connected
    systems, the Mirai botnet revealed how poor security practices could be weaponized
    at massive scale. In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[8](#fn8)
    emerged as one of the most disruptive distributed denial-of-service (DDoS)[9](#fn9)
    attacks in internet history ([Antonakakis et al. 2017](ch058.xhtml#ref-antonakakis2017understanding)).
    The botnet infected thousands of networked devices, including digital cameras,
    DVRs, and other consumer electronics. These devices, often deployed with factory-default
    usernames and passwords, were easily compromised by the Mirai malware and enlisted
    into a large-scale attack network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Jeep Cherokee 的黑客攻击展示了针对连接系统的针对性利用，但 Mirai 恶意软件揭示了糟糕的安全实践如何被大规模武器化。2016年，[Mirai
    恶意软件](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[8](#fn8)
    成为互联网历史上最具破坏性的分布式拒绝服务（DDoS）[9](#fn9) 攻击之一（Antonakakis 等人，2017年[ch058.xhtml#ref-antonakakis2017understanding]）。该恶意软件感染了成千上万的网络设备，包括数字摄像头、DVR和其他消费电子产品。这些设备通常使用出厂默认的用户名和密码部署，很容易被
    Mirai 恶意软件攻陷并被纳入大规模攻击网络。
- en: The Mirai botnet was used to overwhelm major internet infrastructure providers,
    disrupting access to popular online services across the United States and beyond.
    The scale of the attack demonstrated how vulnerable consumer and industrial devices
    can become a platform for widespread disruption when security is not prioritized
    in their design and deployment.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Mirai 恶意软件被用于压垮主要的互联网基础设施提供商，导致美国及以外的流行在线服务访问中断。攻击的规模展示了当在设计部署时没有优先考虑安全，消费者和工业设备如何变成广泛破坏的平台。
- en: The Mirai botnet’s lessons apply directly to modern ML deployments. Edge-deployed
    ML devices with weak authentication become weaponized attack infrastructure at
    unprecedented scale, precisely as the Mirai botnet demonstrated with traditional
    IoT devices. Modern ML edge devices (smart cameras running object detection, voice
    assistants performing wake-word detection, autonomous drones with navigation models,
    industrial IoT sensors with anomaly detection algorithms) face identical vulnerability
    patterns but with amplified consequences due to their AI capabilities and access
    to sensitive data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Mirai 恶意软件的教训直接适用于现代机器学习部署。边缘部署的机器学习设备由于认证薄弱，成为前所未有的武器化攻击基础设施，正如 Mirai 恶意软件在传统的物联网设备上所展示的那样。现代机器学习边缘设备（运行物体检测的智能摄像头、执行唤醒词检测的语音助手、具有导航模型的自主无人机、具有异常检测算法的工业物联网传感器）面临相同的漏洞模式，但由于它们的
    AI 能力和对敏感数据的访问，其后果被放大。
- en: 'The attack escalation with ML devices differs significantly from traditional
    IoT compromises. Unlike simple IoT devices that provided only computing power
    for DDoS attacks, compromised ML devices offer sophisticated capabilities: (1)
    Data exfiltration where smart cameras leak facial recognition databases, voice
    assistants extract conversation transcripts, and health monitors steal biometric
    data; (2) Model weaponization where hijacked autonomous drones coordinate swarm
    attacks and compromised traffic cameras misreport vehicle counts to manipulate
    traffic systems; (3) AI-powered reconnaissance where compromised edge ML devices
    use their trained models to identify high-value targets (facial recognition for
    VIP identification, voice analysis for emotion detection) and coordinate sophisticated
    multi-stage attacks.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习设备的攻击升级与传统物联网入侵有显著不同。与仅提供DDoS攻击计算能力的简单物联网设备不同，被入侵的机器学习设备提供了复杂的功能：(1) 数据泄露，智能摄像头泄露人脸识别数据库，语音助手提取对话记录，健康监测器窃取生物识别数据；(2)
    模型武器化，被劫持的自主无人机协调集群攻击，被入侵的交通摄像头错误报告车辆数量以操纵交通系统；(3) AI驱动的侦察，被入侵的边缘机器学习设备使用其训练的模型来识别高价值目标（人脸识别用于VIP识别，语音分析用于情绪检测）并协调复杂的多阶段攻击。
- en: 'Consider a concrete attack scenario where attackers compromise 50,000 smart
    security cameras with default passwords, each running ML object detection models.
    Rather than traditional DDoS attacks, they use the compromised cameras to: (1)
    extract facial recognition databases from residential and commercial buildings;
    (2) coordinate physical surveillance of targeted individuals using distributed
    camera networks; (3) inject false object detection alerts to trigger emergency
    responses and create chaos; (4) use the cameras’ computing power to train adversarial
    examples against other security systems.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的攻击场景，攻击者利用默认密码入侵了50,000个智能安全摄像头，每个摄像头都在运行机器学习物体检测模型。他们不是使用传统的DDoS攻击，而是利用被入侵的摄像头进行以下操作：(1)
    从住宅和商业建筑中提取人脸识别数据库；(2) 使用分布式摄像头网络协调对目标个人的物理监控；(3) 注入虚假物体检测警报以触发紧急响应并造成混乱；(4) 利用摄像头的计算能力来训练对抗性样本以对抗其他安全系统。
- en: 'Comprehensive defense against such weaponization requires zero-trust edge security:
    (1) Secure manufacturing that eliminates default credentials, implements hardware
    security modules (HSMs) for device-unique keys, and enables secure boot with cryptographic
    verification; (2) Encrypted communications that mandate TLS 1.3+ for all ML API
    communications with certificate pinning and mutual authentication; (3) Behavioral
    monitoring that deploys anomaly detection systems to identify unusual inference
    patterns, unexpected network traffic, and suspicious computational loads; (4)
    Automated response that implements kill switches to disable compromised devices
    remotely and quarantine them from networks; (5) Update security that enforces
    cryptographically signed firmware updates with automatic security patching and
    version rollback capabilities.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这种武器化的全面防御需要零信任边缘安全：(1) 确保制造过程安全，消除默认凭证，实施硬件安全模块（HSM）以生成设备唯一密钥，并启用带有加密验证的安全启动；(2)
    加密通信，要求所有机器学习API通信使用TLS 1.3+，并实施证书固定和相互认证；(3) 行为监控，部署异常检测系统以识别不寻常的推理模式、意外的网络流量和可疑的计算负载；(4)
    自动响应，实施关闭开关以远程禁用被入侵的设备并将它们隔离于网络之外；(5) 更新安全，强制执行加密签名固件更新，并具有自动安全补丁和版本回滚功能。
- en: Systematic Threat Analysis and Risk Assessment
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统性威胁分析和风险评估
- en: 'The historical incidents demonstrate how fundamental security failures manifest
    across different computing paradigms. Supply chain vulnerabilities enable persistent
    compromise, insufficient isolation allows privilege escalation, and weaponized
    endpoints create attack infrastructure at scale. These patterns directly apply
    to machine learning deployments: compromised training pipelines and model repositories
    inherit supply chain risks, external interfaces to safety-critical ML components
    require strict isolation, and compromised ML edge devices can exfiltrate inference
    data or participate in coordinated attacks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 历史事件展示了基本安全失败如何在不同的计算范式下体现。供应链漏洞导致持续入侵，隔离不足允许权限提升，以及武器化端点在规模上创建攻击基础设施。这些模式直接适用于机器学习部署：被入侵的训练管道和模型存储库继承了供应链风险，与安全关键型机器学习组件的外部接口需要严格的隔离，被入侵的机器学习边缘设备可以泄露推理数据或参与协调攻击。
- en: These historical incidents reveal universal security patterns that translate
    directly to ML system vulnerabilities. Supply chain compromise, as demonstrated
    by Stuxnet, manifests in ML through training data poisoning and backdoored model
    repositories. Insufficient isolation, exemplified by the Jeep Cherokee hack, appears
    in ML API access to safety-critical systems and compromised inference endpoints.
    Weaponized endpoints, illustrated by the Mirai botnet, emerge through hijacked
    ML edge devices capable of coordinated AI-powered attacks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些历史事件揭示了通用的安全模式，这些模式直接转化为ML系统的漏洞。正如Stuxnet所展示的供应链妥协，在ML中表现为训练数据中毒和后门模型存储库。不足的隔离，如Jeep
    Cherokee黑客攻击所例证，表现为对安全关键系统的ML API访问和受损害的推理端点。武器化端点，如Mirai僵尸网络所示，通过劫持能够进行协调AI攻击的ML边缘设备出现。
- en: The key insight is that traditional cybersecurity patterns amplify in ML systems
    because models learn from data and make autonomous decisions. While Stuxnet required
    sophisticated malware to manipulate industrial controllers, ML systems can be
    compromised through data poisoning that appears statistically normal but embeds
    hidden behaviors. This characteristic makes ML systems both more vulnerable to
    subtle attacks and more dangerous when compromised, as they can make decisions
    affecting physical systems autonomously. Understanding these historical patterns
    helps recognize how familiar attack vectors manifest in ML contexts, while the
    unique properties of learning systems (statistical learning, decision autonomy,
    and data dependency) create new attack surfaces requiring specialized defenses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的见解是，传统的网络安全模式在ML系统中得到了放大，因为模型从数据中学习并做出自主决策。虽然Stuxnet需要复杂的恶意软件来操纵工业控制器，但ML系统可以通过数据中毒来被破坏，这种数据中毒在统计上看似正常，但嵌入隐藏的行为。这一特性使得ML系统在遭受攻击时更加脆弱，并且更加危险，因为它们可以自主地做出影响物理系统的决策。理解这些历史模式有助于识别熟悉攻击向量如何在ML环境中体现，而学习系统的独特属性（统计学习、决策自主性和数据依赖性）创造了需要专门防御的新攻击面。
- en: 'Machine learning systems introduce attack vectors that extend beyond traditional
    computing vulnerabilities. The data-driven nature of learning creates new opportunities
    for adversaries: training data can be manipulated to embed backdoors, input perturbations
    can exploit learned decision boundaries, and systematic API queries can extract
    proprietary model knowledge. These ML-specific threats require specialized defenses
    that account for the statistical and probabilistic foundations of learning systems,
    complementing traditional infrastructure hardening.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统引入了超出传统计算漏洞的攻击向量。学习的数据驱动特性为对手创造了新的机会：训练数据可以被操纵以嵌入后门，输入扰动可以利用学习到的决策边界，系统性的API查询可以提取专有模型知识。这些特定于ML的威胁需要专门的防御措施，这些措施考虑到学习系统的统计和概率基础，补充了传统的基础设施加固。
- en: Threat Prioritization Framework
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 威胁优先级框架
- en: With the wide range of potential threats facing ML systems, practitioners need
    a framework to prioritize their defensive efforts effectively. Not all threats
    are equally likely or impactful, and security resources are always constrained.
    A simple prioritization matrix based on likelihood and impact helps focus attention
    where it matters most.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 面对ML系统面临的广泛潜在威胁，从业者需要一个框架来有效地优先考虑他们的防御工作。并非所有威胁的可能性或影响都相同，而且安全资源总是有限的。一个基于可能性和影响的简单优先级矩阵有助于将注意力集中在最重要的地方。
- en: 'Consider these threat priority categories:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下威胁优先级类别：
- en: '**High Likelihood / High Impact**: Data poisoning in federated learning systems
    where training data comes from untrusted sources. These attacks are relatively
    easy to execute but can severely compromise model behavior.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可能性/高影响**：在联邦学习系统中，训练数据来自不可信来源的数据中毒攻击。这些攻击相对容易执行，但可能会严重损害模型行为。'
- en: '**High Likelihood / Medium Impact**: Model extraction attacks against public
    APIs. These are common and technically simple but may only affect competitive
    advantage rather than safety or privacy.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可能性/中等影响**：针对公共API的模型提取攻击。这些攻击很常见且技术上简单，但可能只会影响竞争优势，而不是安全或隐私。'
- en: '**Low Likelihood / High Impact**: Hardware side-channel attacks on cloud-deployed
    models. These require sophisticated adversaries and physical access but could
    expose all model parameters and user data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低可能性/高影响**：针对云部署模型的硬件侧信道攻击。这些攻击需要复杂的对手和物理访问权限，但可能会暴露所有模型参数和用户数据。'
- en: '**Medium Likelihood / Medium Impact**: Membership inference attacks against
    models trained on sensitive data. These require some technical skill but mainly
    threaten individual privacy rather than system integrity.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中等可能性/中等影响**：针对在敏感数据上训练的模型的成员推理攻击。这些攻击需要一定的技术技能，但主要威胁的是个人隐私而不是系统完整性。'
- en: This framework guides resource allocation throughout this chapter. We begin
    with the most common and accessible threats (model theft, data poisoning, and
    adversarial attacks) before examining more specialized hardware and infrastructure
    vulnerabilities. Understanding these priority levels helps practitioners implement
    defenses in a logical sequence that maximizes security benefit per invested effort.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本框架在本章中指导资源分配。我们首先从最常见且易于访问的威胁（模型盗窃、数据中毒和对抗攻击）开始，然后检查更专业的硬件和基础设施漏洞。理解这些优先级有助于从业者以逻辑顺序实施防御措施，以最大化每项投入的努力带来的安全效益。
- en: Model-Specific Attack Vectors
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型特定的攻击向量
- en: 'Machine learning systems face threats spanning the entire ML lifecycle, from
    training-time manipulations to inference-time evasion. These threats fall into
    three broad categories: threats to model confidentiality (model theft), threats
    to training integrity (data poisoning[10](#fn10)), and threats to inference robustness
    (adversarial examples[11](#fn11)). Each category targets different vulnerabilities
    and requires distinct defensive strategies.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统面临从训练时间操纵到推理时间规避的整个ML生命周期的威胁。这些威胁分为三个广泛的类别：针对模型机密性的威胁（模型盗窃）、针对训练完整性的威胁（数据中毒[10](#fn10)）和针对推理鲁棒性的威胁（对抗示例[11](#fn11)）。每个类别针对不同的漏洞，并需要不同的防御策略。
- en: Understanding when and where different attacks occur in the ML lifecycle helps
    prioritize defenses and understand attacker motivations. [Figure 15.2](ch021.xhtml#fig-ml-lifecycle-threats)
    maps the primary attack vectors to their target stages in the machine learning
    pipeline, revealing how adversaries exploit different system vulnerabilities at
    different times.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同攻击在ML生命周期中何时何地发生有助于优先考虑防御措施并理解攻击者的动机。[图15.2](ch021.xhtml#fig-ml-lifecycle-threats)将主要攻击向量映射到机器学习管道中的目标阶段，揭示了攻击者如何在不同时间利用不同的系统漏洞。
- en: '**During Data Collection**: Attackers can inject malicious samples or manipulate
    labels in training datasets, particularly in federated learning or crowdsourced
    data scenarios where data sources are less controlled.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在数据收集期间**：攻击者可以在训练数据集中注入恶意样本或操纵标签，尤其是在数据来源控制较少的联邦学习或众包数据场景中。'
- en: '**During Training**: This stage faces backdoor insertion attacks, where adversaries
    embed hidden behaviors that activate only under specific trigger conditions, and
    label manipulation attacks that systematically corrupt the learning process.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在训练期间**：这一阶段面临后门注入攻击，攻击者嵌入仅在特定触发条件下激活的隐藏行为，以及系统性地破坏学习过程的标签操纵攻击。'
- en: '**During Deployment**: Model theft attacks target this stage because trained
    models become accessible through APIs, file downloads, or reverse engineering
    of mobile applications. This is where intellectual property is most vulnerable.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在部署期间**：模型盗窃攻击针对这一阶段，因为训练模型可以通过API、文件下载或移动应用程序的反向工程来访问。这是知识产权最脆弱的地方。'
- en: '**During Inference**: Adversarial attacks occur at runtime, where attackers
    craft inputs designed to fool deployed models into making incorrect predictions
    while appearing normal to human observers.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在推理期间**：对抗攻击在运行时发生，攻击者构建旨在欺骗部署的模型做出错误预测的输入，同时对于人类观察者来说看起来是正常的。'
- en: This lifecycle perspective reveals that different threats require different
    defensive strategies. Data validation protects the collection phase, secure training
    environments protect the training phase, access controls and API design protect
    deployment, and input validation protects inference. By understanding which attacks
    target which lifecycle stages, security teams can implement appropriate defenses
    at the right architectural layers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种生命周期视角揭示了不同的威胁需要不同的防御策略。数据验证保护收集阶段，安全训练环境保护训练阶段，访问控制和API设计保护部署，输入验证保护推理。通过理解哪些攻击针对哪些生命周期阶段，安全团队能够在适当的架构层实施适当的防御措施。
- en: '![](../media/file230.svg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file230.svg)'
- en: 'Figure 15.2: **ML Lifecycle Threats**: Model theft, data poisoning, and adversarial
    attacks target distinct stages of the machine learning lifecycle (from data ingestion
    to model deployment and inference), creating unique vulnerabilities at each step.
    Understanding these lifecycle positions clarifies attack surfaces and guides the
    development of targeted defense strategies for robust AI systems.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：**机器学习生命周期威胁**：模型盗窃、数据中毒和对抗性攻击针对机器学习生命周期的不同阶段（从数据摄取到模型部署和推理），在每个步骤中创造独特的漏洞。了解这些生命周期位置可以阐明攻击面，并指导开发针对稳健人工智能系统的针对性防御策略。
- en: Machine learning models are not solely passive victims of attack; in some cases,
    they can be employed as components of an attack strategy. Pretrained models, particularly
    large generative or discriminative networks, may be adapted to automate tasks
    such as adversarial example generation, phishing content synthesis[12](#fn12),
    or protocol subversion. Open-source or publicly accessible models can be fine-tuned
    for malicious purposes, including impersonation, surveillance, or reverse-engineering
    of secure systems.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型并非仅仅是攻击的被动受害者；在某些情况下，它们可以被用作攻击策略的一部分。预训练模型，尤其是大型生成或判别网络，可能被调整以自动化诸如对抗性样本生成、钓鱼内容合成[12](#fn12)或协议篡改等任务。开源或公开可访问的模型可以被微调以用于恶意目的，包括模仿、监控或逆向工程安全系统。
- en: Model Theft
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型盗窃
- en: The first category of model-specific threats targets confidentiality. Threats
    to model confidentiality arise when adversaries gain access to a trained model’s
    parameters, architecture, or output behavior. These attacks can undermine the
    economic value of machine learning systems, allow competitors to replicate proprietary
    functionality, or expose private information encoded in model weights.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类针对特定模型的威胁针对机密性。当对手获取到训练模型的参数、架构或输出行为时，就会产生对模型机密性的威胁。这些攻击可能会损害机器学习系统的经济价值，允许竞争对手复制专有功能，或暴露在模型权重中编码的私人信息。
- en: Such threats arise across a range of deployment settings, including public APIs[13](#fn13),
    cloud-hosted services, on-device inference engines, and shared model repositories[14](#fn14).
    Machine learning models may be vulnerable due to exposed interfaces, insecure
    serialization formats[15](#fn15), or insufficient access controls, factors that
    create opportunities for unauthorized extraction or replication ([Ateniese et
    al. 2015](ch058.xhtml#ref-ateniese2015hacking)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些威胁出现在各种部署环境中，包括公共API[13](#fn13)、云托管服务、设备上的推理引擎和共享模型存储库[14](#fn14)。由于暴露的接口、不安全的序列化格式[15](#fn15)或访问控制不足，机器学习模型可能存在漏洞，这些因素为未经授权的提取或复制创造了机会
    ([Ateniese等人2015](ch058.xhtml#ref-ateniese2015hacking))。
- en: The severity of these threats is underscored by high-profile legal cases that
    have highlighted the strategic and economic value of machine learning models.
    For example, former Google engineer Anthony Levandowski was accused of [stealing
    proprietary designs from Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html),
    including critical components of its autonomous vehicle technology, before founding
    a competing startup. Such cases illustrate the potential for insider threats to
    bypass technical protections and gain access to sensitive intellectual property.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些威胁的严重性通过高调的法律案例得到了强调，这些案例突出了机器学习模型的战略和经济价值。例如，前谷歌工程师安东尼·莱万多夫斯基被指控在创立竞争性初创公司之前从Waymo[偷窃专有设计](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html)，包括其自动驾驶汽车技术的关键组件。此类案例说明了内部威胁绕过技术保护并获取敏感知识产权的潜在可能性。
- en: 'The consequences of model theft extend beyond economic loss. Stolen models
    can be used to extract sensitive information, replicate proprietary algorithms,
    or enable further attacks. The economic impact can be substantial: research estimates
    suggest that aspects of large language models can be approximated through systematic
    API queries at costs orders of magnitude lower than original training, though
    full model replication remains economically and technically challenging ([Tramèr
    et al. 2016](ch058.xhtml#ref-tramer2016stealing); [Carlini et al. 2024](ch058.xhtml#ref-carlini2024stealing)).
    For instance, a competitor who obtains a stolen recommendation model from an e-commerce
    platform might gain insights into customer behavior, business analytics, and embedded
    trade secrets. This knowledge can also be used to conduct model inversion attacks[16](#fn16),
    where an attacker attempts to infer private details about the model’s training
    data ([Fredrikson, Jha, and Ristenpart 2015](ch058.xhtml#ref-fredrikson2015model)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型盗窃的后果不仅限于经济损失。被盗的模型可以被用来提取敏感信息，复制专有算法，或启用进一步的攻击。经济影响可能非常严重：研究估计表明，大型语言模型的一些方面可以通过系统性的API查询来近似，其成本比原始训练低几个数量级，尽管完全复制模型在技术和经济上仍然具有挑战性
    ([Tramèr等人 2016](ch058.xhtml#ref-tramer2016stealing); [Carlini等人 2024](ch058.xhtml#ref-carlini2024stealing))。例如，一个从电子商务平台获得被盗推荐模型的竞争对手可能会获得有关客户行为、商业分析和嵌入式商业机密的见解。这些知识也可以用来进行模型反演攻击[16](#fn16)，其中攻击者试图推断关于模型训练数据的私人细节
    ([Fredrikson, Jha, and Ristenpart 2015](ch058.xhtml#ref-fredrikson2015model))。
- en: In a model inversion attack, the adversary queries the model through a legitimate
    interface, such as a public API, and observes its outputs. By analyzing confidence
    scores or output probabilities, the attacker can optimize inputs to reconstruct
    data resembling the model’s training set. For example, a facial recognition model
    used for secure access could be manipulated to reveal statistical properties of
    the employee photos on which it was trained. Similar vulnerabilities have been
    demonstrated in studies on the Netflix Prize dataset[17](#fn17), where researchers
    inferred individual movie preferences from anonymized data ([A. Narayanan and
    Shmatikov 2006](ch058.xhtml#ref-narayanan2006break)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型反演攻击中，攻击者通过合法接口查询模型，例如公共API，并观察其输出。通过分析置信度分数或输出概率，攻击者可以优化输入以重建类似于模型训练集的数据。例如，用于安全访问的面部识别模型可以被操纵以揭示其训练的员工照片的统计特性。类似的安全漏洞已在Netflix
    Prize数据集的研究中得以证明[17](#fn17)，研究人员从匿名数据中推断出个人的电影偏好 ([A. Narayanan and Shmatikov
    2006](ch058.xhtml#ref-narayanan2006break)))。
- en: 'Model theft can target two distinct objectives: extracting exact model properties,
    such as architecture and parameters, or replicating approximate model behavior
    to produce similar outputs without direct access to internal representations.
    Understanding neural network architectures helps recognize which architectural
    patterns are most vulnerable to extraction attacks. The specific architectural
    vulnerabilities vary by model type, as discussed in [Chapter 4](ch010.xhtml#sec-dnn-architectures).
    Both forms of theft undermine the security and value of machine learning systems,
    as explored in the following subsections.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型盗窃可以针对两个不同的目标：提取精确的模型属性，例如架构和参数，或者复制近似模型行为以产生类似输出，而不直接访问内部表示。理解神经网络架构有助于识别哪些架构模式最易受到提取攻击。具体的架构漏洞因模型类型而异，如第[4章](ch010.xhtml#sec-dnn-architectures)所述。这两种形式的盗窃都损害了机器学习系统的安全和价值，如下小节所述。
- en: These two attack paths are illustrated in [Figure 15.3](ch021.xhtml#fig-model-theft-types).
    In exact model theft, the attacker gains access to the model’s internal components,
    including serialized files, weights, and architecture definitions, and reproduces
    the model directly. In contrast, approximate model theft relies on observing the
    model’s input-output behavior, typically through a public API. By repeatedly querying
    the model and collecting responses, the attacker trains a surrogate that mimics
    the original model’s functionality. The first approach compromises the model’s
    internal design and training investment, while the second threatens its predictive
    value and can facilitate further attacks such as adversarial example transfer
    or model inversion.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种攻击路径在[图15.3](ch021.xhtml#fig-model-theft-types)中进行了说明。在精确模型窃取中，攻击者可以访问模型的内部组件，包括序列化文件、权重和架构定义，并直接复制模型。相比之下，近似模型窃取依赖于观察模型的输入输出行为，通常通过公共API进行。通过反复查询模型并收集响应，攻击者训练一个模拟器来模仿原始模型的功能。第一种方法损害了模型的内部设计和训练投资，而第二种威胁到其预测价值，并可能促进进一步的攻击，如对抗性示例迁移或模型反转。
- en: '![](../media/file231.svg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file231.svg)'
- en: 'Figure 15.3: **Model Theft Strategies**: Attackers can target either a model’s
    internal parameters or its external behavior to create a stolen copy. Direct theft
    extracts model weights and architecture, while approximate theft trains a surrogate
    model by querying the original’s input-output behavior, potentially enabling further
    attacks despite lacking direct access to internal components.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：**模型窃取策略**：攻击者可以针对模型内部参数或外部行为来创建被盗的副本。直接窃取提取模型权重和架构，而近似窃取通过查询原始的输入输出行为来训练模拟器模型，可能在没有直接访问内部组件的情况下实现进一步的攻击。
- en: Exact Model Theft
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精确模型窃取
- en: Exact model property theft refers to attacks aimed at extracting the internal
    structure and learned parameters of a machine learning model. These attacks often
    target deployed models that are exposed through APIs, embedded in on-device inference
    engines, or shared as downloadable model files on collaboration platforms. Exploiting
    weak access control, insecure model packaging, or unprotected deployment interfaces,
    attackers can recover proprietary model assets without requiring full control
    of the underlying infrastructure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 精确模型属性窃取是指针对提取机器学习模型内部结构和学习参数的攻击。这些攻击通常针对通过API公开的部署模型，嵌入在设备推理引擎中，或在协作平台上作为可下载模型文件共享的模型。利用弱访问控制、不安全的模型打包或未受保护的部署接口，攻击者可以在不要求对底层基础设施完全控制的情况下恢复专有模型资产。
- en: These attacks typically seek three types of information. The first is the model’s
    learned parameters, such as weights and biases. By extracting these parameters,
    attackers can replicate the model’s functionality without incurring the cost of
    training. This replication allows them to benefit from the model’s performance
    while bypassing the original development effort.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击通常寻求三种类型的信息。第一种是模型学习到的参数，例如权重和偏差。通过提取这些参数，攻击者可以在不承担训练成本的情况下复制模型的功能。这种复制使他们能够从模型性能中获益，同时绕过原始的开发努力。
- en: The second target is the model’s fine-tuned hyperparameters, including training
    configurations such as learning rate, batch size, and regularization settings.
    These hyperparameters significantly influence model performance, and stealing
    them allows attackers to reproduce high-quality results with minimal additional
    experimentation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个目标是模型的微调超参数，包括学习率、批量大小和正则化设置等训练配置。这些超参数对模型性能有显著影响，窃取它们允许攻击者通过最小额外的实验来重现高质量的结果。
- en: Finally, attackers may seek to reconstruct the model’s architecture. This includes
    the sequence and types of layers, activation functions, and connectivity patterns
    that define the model’s behavior. Architecture theft may be accomplished through
    side-channel attacks[18](#fn18), reverse engineering, or analysis of observable
    model behavior.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，攻击者可能试图重建模型的架构。这包括定义模型行为的层序列和类型、激活函数以及连接模式。架构窃取可能通过侧信道攻击[18](#fn18)、逆向工程或分析可观察到的模型行为来完成。
- en: Revealing the architecture not only compromises intellectual property but also
    gives competitors strategic insights into the design choices that provide competitive
    advantage.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示架构不仅会损害知识产权，还会让竞争对手对提供竞争优势的设计选择有战略洞察。
- en: System designers must account for these risks by securing model serialization
    formats, restricting access to runtime APIs, and hardening deployment pipelines.
    Protecting models requires a combination of software engineering practices, including
    access control, encryption, and obfuscation techniques, to reduce the risk of
    unauthorized extraction ([Tramèr et al. 2016](ch058.xhtml#ref-tramer2016stealing)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 系统设计者必须通过确保模型序列化格式、限制对运行时API的访问以及加固部署管道来考虑这些风险。保护模型需要结合软件工程实践，包括访问控制、加密和混淆技术，以降低未经授权提取的风险（[Tramèr等人2016](ch058.xhtml#ref-tramer2016stealing)）。
- en: Approximate Model Theft
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 近似模型窃取
- en: While some attackers seek to extract a model’s exact internal properties, others
    focus on replicating its external behavior. Approximate model behavior theft refers
    to attacks that attempt to recreate a model’s decision-making capabilities without
    directly accessing its parameters or architecture. Instead, attackers observe
    the model’s inputs and outputs to build a substitute model that performs similarly
    on the same tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当一些攻击者寻求提取模型的确切内部属性时，其他人则专注于复制其外部行为。近似模型行为窃取是指试图在不直接访问其参数或架构的情况下重新创建模型的决策能力。相反，攻击者通过观察模型的输入和输出来构建一个在相同任务上表现相似的替代模型。
- en: This type of theft often targets models deployed as services, where the model
    is exposed through an API or embedded in a user-facing application. By repeatedly
    querying the model and recording its responses, an attacker can train their own
    model to mimic the behavior of the original. This process, often called model
    distillation[19](#fn19) or knockoff modeling, allows attackers to achieve comparable
    functionality without access to the original model’s proprietary internals ([Orekondy,
    Schiele, and Fritz 2019](ch058.xhtml#ref-orekondy2019knockoff)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的窃取通常针对作为服务部署的模型，其中模型通过API公开或嵌入在面向用户的应用程序中。通过反复查询模型并记录其响应，攻击者可以训练自己的模型来模仿原始模型的行为。这个过程通常被称为模型蒸馏[19](#fn19)或仿制建模，允许攻击者在无法访问原始模型专有内部结构的情况下实现类似的功能（[Orekondy、Schiele和Fritz
    2019](ch058.xhtml#ref-orekondy2019knockoff)）。
- en: Attackers may evaluate the success of behavior replication in two ways. The
    first is by measuring the level of effectiveness of the substitute model. This
    involves assessing whether the cloned model achieves similar accuracy, precision,
    recall, or other performance metrics on benchmark tasks. By aligning the substitute’s
    performance with that of the original, attackers can build a model that is practically
    indistinguishable in effectiveness, even if its internal structure differs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可以通过两种方式评估行为复制的成功率。第一种是通过衡量替代模型的效率水平。这涉及到评估克隆模型是否在基准任务上实现了类似的准确度、精确度、召回率或其他性能指标。通过将替代模型的性能与原始模型对齐，攻击者可以构建一个在效果上实际上无法区分的模型，即使其内部结构不同。
- en: The second is by testing prediction consistency. This involves checking whether
    the substitute model produces the same outputs as the original model when presented
    with the same inputs. Matching not only correct predictions but also the original
    model’s mistakes can provide attackers with a high-fidelity reproduction of the
    target model’s behavior. This poses particular concern in applications such as
    natural language processing, where attackers might replicate sentiment analysis
    models to gain competitive insights or bypass proprietary systems.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种是通过测试预测一致性。这涉及到检查替代模型在提供相同输入时是否产生与原始模型相同的输出。匹配不仅包括正确的预测，还包括原始模型的错误，可以为攻击者提供对目标模型行为的忠实复制。这在自然语言处理等应用中尤其令人担忧，攻击者可能复制情感分析模型以获得竞争优势或绕过专有系统。
- en: Approximate behavior theft proves challenging to defend against in open-access
    deployment settings, such as public APIs or consumer-facing applications. Limiting
    the rate of queries, detecting automated extraction patterns, and watermarking
    model outputs are among the techniques that can help mitigate this risk. However,
    these defenses must be balanced with usability and performance considerations,
    especially in production environments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 近似行为窃取在开放访问部署环境中，例如公共API或面向消费者的应用程序中，证明难以防御。限制查询速率、检测自动化提取模式以及水印模型输出是帮助减轻这种风险的几种技术。然而，这些防御措施必须与可用性和性能考虑因素相平衡，尤其是在生产环境中。
- en: One demonstration of approximate model theft extracts internal components of
    black-box language models via public APIs. In their paper, Carlini et al. ([2024](ch058.xhtml#ref-carlini2024stealing)),
    researchers show how to reconstruct the final embedding projection matrix of several
    OpenAI models, including `ada`, `babbage`, and `gpt-3.5-turbo`, using only public
    API access. By exploiting the low-rank structure of the output projection layer
    and making carefully crafted queries, they recover the model’s hidden dimensionality
    and replicate the weight matrix up to affine transformations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一个近似模型盗窃的演示是通过公共API提取黑盒语言模型的内部组件。在他们的论文中，Carlini等人 ([2024](ch058.xhtml#ref-carlini2024stealing))展示了如何仅通过公共API访问来重建包括`ada`、`babbage`和`gpt-3.5-turbo`在内的几个OpenAI模型的最终嵌入投影矩阵。通过利用输出投影层的低秩结构和精心设计的查询，他们恢复了模型的隐藏维度，并复制了权重矩阵直到仿射变换。
- en: The attack does not reconstruct the full model, but reveals internal architecture
    parameters and sets a precedent for future, deeper extractions. This work demonstrated
    that even partial model theft poses risks to confidentiality and competitive advantage,
    especially when model behavior can be probed through rich API responses such as
    logit bias and log-probabilities.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击并未重建完整模型，而是揭示了内部架构参数，并为未来的更深层提取设定了先例。这项工作表明，即使是部分模型盗窃也可能会对机密性和竞争优势构成风险，尤其是在模型行为可以通过丰富的API响应（如logit偏差和对数概率）进行探测的情况下。
- en: 'Table 15.2: **Model Stealing Costs**: Attackers can extract model weights with
    a relatively low query cost using publicly available apis; the table quantifies
    this threat for OpenAI’s ada and babbage models, showing that extracting weights
    achieves low root mean squared error (RMSE) with fewer than (4 ^6) queries. Estimated
    costs for weight extraction range from $1 to $12, demonstrating the economic feasibility
    of model stealing attacks despite API rate limits and associated expenses. Source:
    Carlini et al. ([2024](ch058.xhtml#ref-carlini2024stealing)).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.2：**模型盗窃成本**：攻击者可以使用公开API以相对较低的查询成本提取模型权重；该表量化了OpenAI的ada和babbage模型所面临的这一威胁，显示通过少于(4
    ^6)次查询提取权重可以实现低根均方误差（RMSE）。权重提取的估计成本在1到12美元之间，这表明尽管存在API速率限制和相关费用，模型盗窃攻击在经济上是可行的。来源：Carlini等人
    ([2024](ch058.xhtml#ref-carlini2024stealing))。
- en: '| **Model** | **Size** **(Dimension Extraction)** | **Number of** **Queries**
    | **RMS** **(Weight Matrix Extraction)** | **Cost (USD)** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **大小** **(维度提取)** | **查询数** | **RMS** **(权重矩阵提取)** | **成本 (美元)**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **OpenAI ada** | 1024 ✓ | < 2 ^6$ | <semantics><mrow><mn>5</mn><mo>⋅</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">5 \cdot 10^{-4}</annotation></semantics> | $1 / $4
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **OpenAI ada** | 1024 ✓ | < 2 ^6$ | <semantics><mrow><mn>5</mn><mo>⋅</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">5 \cdot 10^{-4}</annotation></semantics> | $1 / $4
    |'
- en: '| **OpenAI babbage** | 2048 ✓ | < 4 ^6$ | <semantics><mrow><mn>7</mn><mo>⋅</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">7 \cdot 10^{-4}</annotation></semantics> | $2 / $12
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **OpenAI babbage** | 2048 ✓ | < 4 ^6$ | <semantics><mrow><mn>7</mn><mo>⋅</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">7 \cdot 10^{-4}</annotation></semantics> | $2 / $12
    |'
- en: '| **OpenAI babbage-002** | 1536 ✓ | < 4 ^6$ | Not implemented | $2 / $12 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **OpenAI babbage-002** | 1536 ✓ | < 4 ^6$ | 未实现 | $2 / $12 |'
- en: '| **OpenAI gpt-3.5-turbo-instruct** | Not disclosed | < 4 ^7$ | Not implemented
    | $200 / ~$2,000 (estimated) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **OpenAI gpt-3.5-turbo-instruct** | 未公开 | < 4 ^7$ | 未实现 | $200 / ~$2,000
    (估计) |'
- en: '| **OpenAI gpt-3.5-turbo-1106** | Not disclosed | < 4 ^7$ | Not implemented
    | $800 / ~$8,000 (estimated) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **OpenAI gpt-3.5-turbo-1106** | 未公开 | < 4 ^7$ | 未实现 | $800 / ~$8,000 (估计)
    |'
- en: As shown in their empirical evaluation, reproduced in [Table 15.2](ch021.xhtml#tbl-openai-theft),
    model parameters could be extracted with root mean square errors as low as <semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-4}</annotation></semantics>, confirming that
    high-fidelity approximation is achievable at scale. These findings raise important
    implications for system design, suggesting that innocuous API features, like returning
    top-k logits, can serve as significant leakage vectors if not tightly controlled.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如他们在[表15.2](ch021.xhtml#tbl-openai-theft)中所示的实证评估所示，模型参数可以以低至<semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>4</mn></mrow></msup><annotation
    encoding="application/x-tex">10^{-4}</annotation></semantics>的均方根误差被提取，证实了在规模上实现高保真近似是可行的。这些发现对系统设计具有重要意义，表明如果不受严格控制，看似无害的API功能，如返回top-k
    logits，可以成为重要的泄露向量。
- en: 'Case Study: Tesla IP Theft'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究：特斯拉知识产权盗窃
- en: In 2018, Tesla filed a [lawsuit](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf)
    against the self-driving car startup [Zoox](https://zoox.com/), alleging that
    former Tesla employees had stolen proprietary data and trade secrets related to
    Tesla’s autonomous driving technology. According to the lawsuit, several employees
    transferred over 10 gigabytes of confidential files, including machine learning
    models and source code, before leaving Tesla to join Zoox.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，特斯拉对自动驾驶汽车初创公司[Zoox](https://zoox.com/)提起了[诉讼](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf)，指控前特斯拉员工窃取了与特斯拉自动驾驶技术相关的专有数据和商业机密。根据诉讼，几名员工在离开特斯拉加入Zoox之前转移了超过10吉字节的秘密文件，包括机器学习模型和源代码。
- en: Among the stolen materials was a key image recognition model used for object
    detection in Tesla’s self-driving system. By obtaining this model, Zoox could
    have bypassed years of research and development, giving the company a competitive
    advantage. Beyond the economic implications, there were concerns that the stolen
    model could expose Tesla to further security risks, such as model inversion attacks
    aimed at extracting sensitive data from the model’s training set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在被盗材料中，有一个用于特斯拉自动驾驶系统中物体检测的关键图像识别模型。通过获得这个模型，Zoox可以绕过多年的研发，为公司带来竞争优势。除了经济影响之外，还有担忧被盗模型可能会使特斯拉面临进一步的安全风险，例如针对模型训练集提取敏感数据的模型反演攻击。
- en: The Zoox employees denied any wrongdoing, and the case was ultimately settled
    out of court. The incident highlights the real-world risks of model theft, especially
    in industries where machine learning models represent significant intellectual
    property. The theft of models not only undermines competitive advantage but also
    raises broader concerns about privacy, safety, and the potential for downstream
    exploitation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Zoox的员工否认了任何不当行为，此案最终在庭外和解。这一事件突显了模型盗窃的现实风险，特别是在机器学习模型代表重大知识产权的行业中。模型的盗窃不仅损害了竞争优势，还引发了关于隐私、安全和下游滥用的更广泛担忧。
- en: This case demonstrates that model theft is not limited to theoretical attacks
    conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities,
    and unauthorized access to development infrastructure pose equally serious risks
    to machine learning systems deployed in commercial environments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个案例表明，模型盗窃不仅限于在API或公共接口上进行的理论攻击。内部威胁、供应链漏洞以及未经授权访问开发基础设施对在商业环境中部署的机器学习系统同样构成严重风险。
- en: Data Poisoning
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据投毒
- en: While model theft targets confidentiality, the second category of threats focuses
    on training integrity. Training integrity threats stem from the manipulation of
    data used to train machine learning models. These attacks aim to corrupt the learning
    process by introducing examples that appear benign but induce harmful or biased
    behavior in the final model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型盗窃针对的是机密性时，第二类威胁则集中在训练完整性上。训练完整性威胁源于用于训练机器学习模型的数据被操纵。这些攻击旨在通过引入看似无害但会在最终模型中引起有害或偏见的行为的示例来破坏学习过程。
- en: Data poisoning attacks are a prominent example, in which adversaries inject
    carefully crafted data points into the training set to influence model behavior
    in targeted or systemic ways ([Biggio, Nelson, and Laskov 2012](ch058.xhtml#ref-biggio2012poisoning)).
    Poisoned data may cause a model to make incorrect predictions, degrade its generalization
    ability, or embed failure modes that remain dormant until triggered post-deployment.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒攻击是一个突出的例子，其中攻击者将精心制作的数据点注入训练集中，以针对或系统性地影响模型行为([Biggio, Nelson, and Laskov
    2012](ch058.xhtml#ref-biggio2012poisoning))。中毒数据可能导致模型做出错误的预测，降低其泛化能力，或者嵌入在部署后触发的故障模式。
- en: Data poisoning is a security threat because it involves intentional manipulation
    of the training data by an adversary, with the goal of embedding vulnerabilities
    or subverting model behavior. These attacks pose concern in applications where
    models retrain on data collected from external sources, including user interactions,
    crowdsourced annotations[20](#fn20), and online scraping, since attackers can
    inject poisoned data without direct access to the training pipeline.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒是一种安全威胁，因为它涉及攻击者有意操纵训练数据，目的是嵌入漏洞或颠覆模型行为。这些攻击在模型在从外部来源收集的数据上重新训练的应用中引起关注，包括用户交互、众包注释[20](#fn20)和在线抓取，因为攻击者可以在不直接访问训练管道的情况下注入中毒数据。
- en: These attacks occur across diverse threat models. From a security perspective,
    poisoning attacks vary depending on the attacker’s level of access and knowledge.
    In white-box scenarios, the adversary may have detailed insight into the model
    architecture or training process, enabling more precise manipulation. In contrast,
    black-box or limited-access attacks exploit open data submission channels or indirect
    injection vectors. Poisoning can target different stages of the ML pipeline, ranging
    from data collection and preprocessing to labeling and storage, making the attack
    surface both broad and system-dependent. The relative priority of data poisoning
    threats varies by deployment context as analyzed in [Section 15.4.1](ch021.xhtml#sec-security-privacy-threat-prioritization-framework-f2d5).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击发生在多种威胁模型中。从安全角度来看，中毒攻击根据攻击者的访问级别和知识水平而有所不同。在白盒场景中，攻击者可能对模型架构或训练过程有详细的了解，从而能够进行更精确的操作。相比之下，黑盒或有限访问攻击利用开放的数据提交渠道或间接注入向量。中毒攻击可以针对机器学习管道的不同阶段，从数据收集和预处理到标记和存储，使得攻击面既广泛又依赖于系统。数据中毒威胁的相对优先级根据部署环境的不同而变化，如[第15.4.1节](ch021.xhtml#sec-security-privacy-threat-prioritization-framework-f2d5)所述。
- en: Poisoning attacks typically follow a three-stage process. First, the attacker
    injects malicious data into the training set. These examples are often designed
    to appear legitimate but introduce subtle distortions that alter the model’s learning
    process. Second, the model trains on this compromised data, embedding the attacker’s
    intended behavior. Finally, once the model is deployed, the attacker may exploit
    the altered behavior to cause mispredictions, bypass safety checks, or degrade
    overall reliability.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 中毒攻击通常遵循三个阶段的过程。首先，攻击者将恶意数据注入训练集中。这些例子通常设计得看似合法，但引入了细微的扭曲，从而改变了模型的学习过程。其次，模型在受损害的数据上训练，嵌入攻击者的意图行为。最后，一旦模型部署，攻击者可能利用改变后的行为来造成误预测、绕过安全检查或降低整体可靠性。
- en: 'To understand these attack mechanisms precisely, data poisoning can be viewed
    as a bilevel optimization problem, where the attacker seeks to select poisoning
    data <semantics><msub><mi>D</mi><mi>p</mi></msub><annotation encoding="application/x-tex">D_p</annotation></semantics>
    that maximizes the model’s loss on a validation or target dataset <semantics><msub><mi>D</mi><mtext
    mathvariant="normal">test</mtext></msub><annotation encoding="application/x-tex">D_{\text{test}}</annotation></semantics>.
    Let <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>
    represent the original training data. The attacker’s objective is to solve: <semantics><mrow><munder><mo>max</mo><msub><mi>D</mi><mi>p</mi></msub></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><mo>,</mo><msub><mi>D</mi><mtext
    mathvariant="normal">test</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\max_{D_p} \ \mathcal{L}(f_{D \cup D_p},
    D_{\text{test}})</annotation></semantics> where <semantics><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><annotation
    encoding="application/x-tex">f_{D \cup D_p}</annotation></semantics> represents
    the model trained on the combined dataset of original and poisoned data. For targeted
    attacks, this objective can be refined to focus on specific inputs <semantics><msub><mi>x</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">x_t</annotation></semantics> and target labels <semantics><msub><mi>y</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">y_t</annotation></semantics>: <semantics><mrow><munder><mo>max</mo><msub><mi>D</mi><mi>p</mi></msub></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\max_{D_p}
    \ \mathcal{L}(f_{D \cup D_p}, x_t, y_t)</annotation></semantics>'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确理解这些攻击机制，数据中毒可以被视为一个双层优化问题，其中攻击者试图选择中毒数据 <semantics><msub><mi>D</mi><mi>p</mi></msub><annotation
    encoding="application/x-tex">D_p</annotation></semantics> 以最大化模型在验证或目标数据集 <semantics><msub><mi>D</mi><mtext
    mathvariant="normal">test</mtext></msub><annotation encoding="application/x-tex">D_{\text{test}}</annotation></semantics>
    上的损失。令 <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>
    代表原始训练数据。攻击者的目标是解决： <semantics><mrow><munder><mo>max</mo><msub><mi>D</mi><mi>p</mi></msub></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><mo>,</mo><msub><mi>D</mi><mtext
    mathvariant="normal">test</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\max_{D_p} \ \mathcal{L}(f_{D \cup D_p},
    D_{\text{test}})</annotation></semantics> 其中 <semantics><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><annotation
    encoding="application/x-tex">f_{D \cup D_p}</annotation></semantics> 代表在原始和中毒数据组合数据集上训练的模型。对于定向攻击，这个目标可以细化以关注特定的输入
    <semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_t</annotation></semantics>
    和目标标签 <semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">y_t</annotation></semantics>：
    <semantics><mrow><munder><mo>max</mo><msub><mi>D</mi><mi>p</mi></msub></munder><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mrow><mi>D</mi><mo>∪</mo><msub><mi>D</mi><mi>p</mi></msub></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\max_{D_p}
    \ \mathcal{L}(f_{D \cup D_p}, x_t, y_t)</annotation></semantics>
- en: This formulation captures the adversary’s goal of introducing carefully crafted
    data points to manipulate the model’s decision boundaries.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式捕捉了对手引入精心设计的数据点以操纵模型决策边界的目标。
- en: For example, consider a traffic sign classification model trained to distinguish
    between stop signs and speed limit signs. An attacker might inject a small number
    of stop sign images labeled as speed limit signs into the training data. The attacker’s
    goal is to subtly shift the model’s decision boundary so that future stop signs
    are misclassified as speed limit signs. In this case, the poisoning data <semantics><msub><mi>D</mi><mi>p</mi></msub><annotation
    encoding="application/x-tex">D_p</annotation></semantics> consists of mislabeled
    stop sign images, and the attacker’s objective is to maximize the misclassification
    of legitimate stop signs <semantics><msub><mi>x</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">x_t</annotation></semantics> as speed limit signs
    <semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">y_t</annotation></semantics>,
    following the targeted attack formulation above. Even if the model performs well
    on other types of signs, the poisoned training process creates a predictable and
    exploitable vulnerability.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个训练有素的交通标志分类模型，该模型旨在区分停车标志和限速标志。攻击者可能会将少量标记为限速标志的停车标志图像注入训练数据中。攻击者的目标是微妙地改变模型的决策边界，使得未来的停车标志被错误地分类为限速标志。在这种情况下，中毒数据<semantics><msub><mi>D</mi><mi>p</mi></msub><annotation
    encoding="application/x-tex">D_p</annotation></semantics>由错误标记的停车标志图像组成，攻击者的目标是最大化合法停车标志<semantics><msub><mi>x</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">x_t</annotation></semantics>被错误分类为限速标志<semantics><msub><mi>y</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">y_t</annotation></semantics>，遵循上述针对性攻击公式。即使模型在其他类型的标志上表现良好，中毒的训练过程也会创建一个可预测且可利用的漏洞。
- en: Data poisoning attacks can be classified based on their objectives and scope
    of impact. Availability attacks degrade overall model performance by introducing
    noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate
    a specific input or class, leaving general performance intact but causing consistent
    misclassification in select cases. Backdoor attacks[21](#fn21) embed hidden triggers,
    which are often imperceptible patterns, that elicit malicious behavior only when
    the trigger is present. Subpopulation attacks degrade performance on a specific
    group defined by shared features, making them particularly dangerous in fairness-sensitive
    applications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其目标和影响范围，数据中毒攻击可以被分类。可用性攻击通过引入噪声或标签翻转来降低任务的整体准确性，从而降低模型性能。针对性攻击操纵特定的输入或类别，保持一般性能不变，但在某些情况下会导致一致的误分类。后门攻击[21](#fn21)嵌入隐藏的触发器，这些触发器通常是难以察觉的模式，只有在触发器存在时才会引发恶意行为。子群体攻击降低特定群体（由共享特征定义）的性能，在公平敏感的应用中尤其危险。
- en: A notable real-world example of a targeted poisoning attack was demonstrated
    against Perspective, Google’s widely-used online toxicity detection model[22](#fn22)
    that helps platforms identify harmful content ([Hosseini et al. 2017](ch058.xhtml#ref-hosseini2017deceiving)).
    By injecting synthetically generated toxic comments with subtle misspellings and
    grammatical errors into the model’s training set, researchers degraded its ability
    to detect harmful content[23](#fn23).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 针对性中毒攻击的一个引人注目的现实世界例子是对Google广泛使用的在线毒性检测模型Perspective进行的攻击[22](#fn22)，该模型帮助平台识别有害内容([Hosseini等人2017](ch058.xhtml#ref-hosseini2017deceiving))。通过将带有细微拼写错误和语法错误的合成有害评论注入模型的训练集中，研究人员降低了其检测有害内容的能力[23](#fn23)。
- en: Mitigating data poisoning threats requires end-to-end security of the data pipeline,
    encompassing collection, storage, labeling, and training. Preventative measures
    include input validation checks, integrity verification of training datasets,
    and anomaly detection to flag suspicious patterns. In parallel, robust training
    algorithms can limit the influence of mislabeled or manipulated data by down-weighting
    or filtering out anomalous instances. While no single technique guarantees immunity,
    combining proactive data governance, automated monitoring, and robust learning
    practices is important for maintaining model integrity in real-world deployments.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解数据中毒威胁需要数据管道的端到端安全性，包括收集、存储、标记和训练。预防措施包括输入验证检查、训练数据集完整性的验证以及异常检测以标记可疑模式。同时，鲁棒的训练算法可以通过降低异常实例的权重或过滤掉它们来限制错误标记或操纵数据的影响。虽然没有单一技术可以保证免疫，但结合主动数据治理、自动化监控和鲁棒的学习实践对于在现实部署中保持模型完整性至关重要。
- en: Adversarial Attacks
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗性攻击
- en: Moving from training-time to inference-time threats, the third category targets
    model robustness during deployment. Inference robustness threats occur when attackers
    manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning,
    which compromises the training process, these attacks exploit vulnerabilities
    in the model’s decision surface during inference.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练时间到推理时间的威胁，第三类针对的是模型在部署期间的鲁棒性。推理鲁棒性威胁发生在攻击者在测试时间操纵输入以诱导错误预测时。与数据中毒不同，数据中毒会破坏训练过程，这些攻击利用了模型在推理过程中的决策表面上的脆弱性。
- en: A central class of such threats is adversarial attacks, where carefully constructed
    inputs are designed to cause incorrect predictions while remaining nearly indistinguishable
    from legitimate data. As detailed in [Chapter 16](ch022.xhtml#sec-robust-ai),
    these attacks highlight vulnerabilities in ML models’ sensitivity to small, targeted
    perturbations that can drastically alter output confidence or classification results.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这类威胁的核心类别是对抗攻击，其中精心构造的输入被设计成在几乎与合法数据无法区分的情况下导致错误的预测。正如[第16章](ch022.xhtml#sec-robust-ai)中详细所述，这些攻击突显了机器学习模型对微小、有针对性的扰动敏感的脆弱性，这些扰动可以极大地改变输出置信度或分类结果。
- en: 'These attacks create significant real-world risks in domains such as autonomous
    driving, biometric authentication, and content moderation. The effectiveness can
    be striking: research demonstrates that adversarial examples can achieve 99%+
    attack success rates against state-of-the-art image classifiers while modifying
    less than 0.01% of pixel values, changes virtually imperceptible to humans ([Szegedy
    et al. 2013a](ch058.xhtml#ref-szegedy2014intriguing); [Goodfellow, Shlens, and
    Szegedy 2014a](ch058.xhtml#ref-goodfellow2015explaining)). In physical-world attacks,
    printed adversarial patches as small as 2% of an image can cause autonomous vehicles
    to misclassify stop signs as speed limit signs with 80%+ success rates under varying
    lighting conditions ([Eykholt et al. 2017](ch058.xhtml#ref-eykholt2018robust)).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击在自动驾驶、生物识别认证和内容审核等领域创造了重大的现实风险。其有效性可能非常显著：研究表明，对抗样本可以在修改不到0.01%的像素值的情况下，对最先进的图像分类器实现99%以上的攻击成功率，这些变化对人类几乎不可察觉（[Szegedy等人2013a](ch058.xhtml#ref-szegedy2014intriguing)；[Goodfellow、Shlens和Szegedy
    2014a](ch058.xhtml#ref-goodfellow2015explaining)）。在物理世界攻击中，大小仅为图像2%的对抗性补丁可以在不同的光照条件下以80%以上的成功率将自动车辆的停止标志误分类为速度限制标志（[Eykholt等人2017](ch058.xhtml#ref-eykholt2018robust)）。
- en: Unlike data poisoning, which corrupts the model during training, adversarial
    attacks manipulate the model’s behavior at test time, often without requiring
    any access to the training data or model internals. The attack surface thus shifts
    from upstream data pipelines to real-time interaction, demanding robust defense
    mechanisms capable of detecting or mitigating malicious inputs at the point of
    inference.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与在训练期间破坏模型的数据中毒不同，对抗攻击在测试时间操纵模型的行为，通常不需要访问训练数据或模型内部。因此，攻击面从上游数据管道转移到实时交互，需要能够检测或减轻推理点恶意输入的鲁棒防御机制。
- en: The mathematical foundations of adversarial example generation and comprehensive
    taxonomies of attack algorithms, including gradient-based, optimization-based,
    and transfer-based techniques, are covered in detail in [Chapter 16](ch022.xhtml#sec-robust-ai),
    which explores robust approaches to building adversarially resistant systems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本生成的数学基础和攻击算法的全面分类，包括基于梯度的、基于优化的和基于迁移的技术，在[第16章](ch022.xhtml#sec-robust-ai)中有详细论述，该章节探讨了构建对抗性抵抗系统的鲁棒方法。
- en: Adversarial attacks vary based on the attacker’s level of access to the model.
    In white-box attacks, the adversary has full knowledge of the model’s architecture,
    parameters, and training data, allowing them to craft highly effective adversarial
    examples. In black-box attacks, the adversary has no internal knowledge and must
    rely on querying the model and observing its outputs. Grey-box attacks fall between
    these extremes, with the adversary possessing partial information, such as access
    to the model architecture but not its parameters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击根据攻击者对模型的访问级别而有所不同。在白盒攻击中，攻击者对模型的架构、参数和训练数据有全面了解，使他们能够制作出高度有效的对抗样本。在黑盒攻击中，攻击者没有内部知识，必须依赖查询模型并观察其输出。灰盒攻击介于这两种极端之间，攻击者拥有部分信息，例如可以访问模型架构但不能访问其参数。
- en: These attacker models can be summarized along a spectrum of knowledge levels.
    [Table 15.3](ch021.xhtml#tbl-adversary-knowledge-spectrum) highlights the differences
    in model access, data access, typical attack strategies, and common deployment
    scenarios. Such distinctions help characterize the practical challenges of securing
    ML systems across different deployment environments.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击者模型可以总结为知识水平的一个连续谱。[表15.3](ch021.xhtml#tbl-adversary-knowledge-spectrum)突出了模型访问、数据访问、典型攻击策略和常见部署场景之间的差异。这些区别有助于描述在不同部署环境中确保机器学习系统安全的实际挑战。
- en: Common attack strategies include surrogate model construction, transfer attacks
    exploiting adversarial transferability, and GAN-based perturbation generation.
    The technical details of these approaches and their mathematical formulations
    are thoroughly covered in [Chapter 16](ch022.xhtml#sec-robust-ai).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的攻击策略包括代理模型构建、利用对抗迁移性的迁移攻击以及基于GAN的扰动生成。这些方法的技术细节及其数学公式在[第16章](ch022.xhtml#sec-robust-ai)中得到了详尽的阐述。
- en: 'Table 15.3: **Adversarial Knowledge Spectrum**: Varying levels of attacker
    access to model details and training data define distinct threat models, influencing
    the feasibility and sophistication of adversarial attacks and impacting deployment
    security strategies. The table categorizes these models by access level, typical
    attack methods, and common deployment scenarios, clarifying the practical challenges
    of securing machine learning systems.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.3：**对抗知识谱系**：攻击者对模型细节和训练数据的访问级别不同，定义了不同的威胁模型，影响了对抗攻击的可行性和复杂性，并影响了部署安全策略。该表按访问级别、典型攻击方法和常见部署场景对这些模型进行分类，明确了确保机器学习系统安全的实际挑战。
- en: '| **Adversary Knowledge Level** | **Model Access** | **Training Data Access**
    | **Attack Example** | **Common Scenario** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| **对抗知识水平** | **模型访问** | **训练数据访问** | **攻击示例** | **常见场景** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **White-box** | Full access to architecture and parameters | Full access
    | Crafting adversarial examples using gradients | Insider threats, open-source
    model reuse |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **白盒** | 完全访问架构和参数 | 完全访问 | 使用梯度构建对抗示例 | 内部威胁，开源模型重用 |'
- en: '| **Grey-box** | Partial access (e.g., architecture only) | Limited or no access
    | Attacks based on surrogate model approximation | Known model family, unknown
    fine-tuning |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **灰盒** | 部分访问（例如，仅架构） | 有限或无访问 | 基于代理模型近似的攻击 | 已知模型家族，未知微调 |'
- en: '| **Black-box** | No internal access; only query-response view | No access
    | Query-based surrogate model training and transfer attacks | Public APIs, model-as-a-service
    deployments |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| **黑盒** | 无内部访问；只有查询-响应视图 | 无访问 | 基于查询的代理模型训练和迁移攻击 | 公共API，模型即服务部署 |'
- en: One illustrative example involves the manipulation of traffic sign recognition
    systems ([Eykholt et al. 2017](ch058.xhtml#ref-eykholt2018robust)). Researchers
    demonstrated that placing small stickers on stop signs could cause machine learning
    models to misclassify them as speed limit signs. While the altered signs remained
    easily recognizable to humans, the model consistently misinterpreted them. Such
    attacks pose serious risks in applications like autonomous driving, where reliable
    perception is important for safety.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有说明性的例子涉及对交通标志识别系统的操纵([Eykholt等人2017](ch058.xhtml#ref-eykholt2018robust))。研究人员证明，在停车标志上放置小贴纸可能导致机器学习模型将它们错误分类为限速标志。尽管修改后的标志对人类来说仍然容易识别，但模型却持续地将它们误认为是限速标志。这类攻击在自动驾驶等需要可靠感知的应用中存在严重风险，因为可靠感知对于安全至关重要。
- en: Adversarial attacks highlight the need for robust defenses that go beyond improving
    model accuracy. Securing ML systems against adversarial threats requires runtime
    defenses such as input validation, anomaly detection, and monitoring for abnormal
    patterns during inference. Training-time robustness methods (e.g., adversarial
    training) complement these strategies and are explored in [Chapter 16](ch022.xhtml#sec-robust-ai).
    The training methodologies that support robust model development are detailed
    in [Chapter 8](ch014.xhtml#sec-ai-training). These defenses aim to enhance model
    resilience against adversarial examples, ensuring that machine learning systems
    can operate reliably even in the presence of malicious inputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击突出了需要强大的防御措施，这些措施不仅限于提高模型准确性。为了保护机器学习系统免受对抗性威胁，需要运行时防御措施，例如输入验证、异常检测以及在推理期间监控异常模式。训练时间鲁棒性方法（例如，对抗性训练）补充了这些策略，并在[第16章](ch022.xhtml#sec-robust-ai)中进行了探讨。支持鲁棒模型发展的训练方法在第8章[第8章](ch014.xhtml#sec-ai-training)中详细说明。这些防御措施旨在增强模型对对抗性样本的抵抗力，确保机器学习系统即使在存在恶意输入的情况下也能可靠地运行。
- en: 'Case Study: Traffic Sign Attack'
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：交通标志攻击
- en: In 2017, researchers conducted experiments by placing small black and white
    stickers on stop signs ([Eykholt et al. 2017](ch058.xhtml#ref-eykholt2018robust)).
    As shown in [Figure 15.4](ch021.xhtml#fig-adversarial-stickers), these stickers
    were designed to be nearly imperceptible to the human eye, yet they significantly
    altered the appearance of the stop sign when viewed by machine learning models.
    When viewed by a normal human eye, the stickers did not obscure the sign or prevent
    interpretability. However, when images of the stickers stop signs were fed into
    standard traffic sign classification ML models, they were misclassified as speed
    limit signs over 85% of the time.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，研究人员通过在停车标志上放置小块黑白贴纸([Eykholt等人2017](ch058.xhtml#ref-eykholt2018robust))进行了实验。如图[图15.4](ch021.xhtml#fig-adversarial-stickers)所示，这些贴纸被设计成几乎对人类眼睛不可见，但机器学习模型在观察时却显著改变了停车标志的外观。对于正常的人类眼睛来说，这些贴纸并没有遮挡标志或阻止可解释性。然而，当贴纸停车标志的图像被输入到标准的交通标志分类机器学习模型时，有超过85%的时间被错误地分类为限速标志。
- en: '![](../media/file232.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file232.png)'
- en: 'Figure 15.4: **Adversarial Stickers**: Nearly imperceptible stickers can trick
    machine learning models into misclassifying stop signs as speed limit signs over
    85% of the time. This emphasizes the vulnerability of ML systems to adversarial
    attacks. Source: Eykholt et al. ([2017](ch058.xhtml#ref-eykholt2018robust)).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：**对抗性贴纸**：几乎不可见的贴纸可以使机器学习模型在85%以上的时间内将停车标志错误地分类为限速标志。这强调了机器学习系统对对抗性攻击的脆弱性。来源：Eykholt等人([2017](ch058.xhtml#ref-eykholt2018robust))。
- en: This demonstration showed how simple adversarial stickers could trick ML systems
    into misreading important road signs. If deployed realistically, these attacks
    could endanger public safety, causing autonomous vehicles to misinterpret stop
    signs as speed limits. Researchers warned this could potentially cause dangerous
    rolling stops or acceleration into intersections.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示展示了简单的对抗性贴纸如何欺骗机器学习系统错误地读取重要的道路标志。如果实际部署，这些攻击可能会危及公共安全，导致自动驾驶汽车将停车标志误认为是限速标志。研究人员警告说，这可能会造成危险的滚动停车或加速进入交叉路口。
- en: This case study provides a concrete illustration of how adversarial examples
    exploit the pattern recognition mechanisms of ML models. By subtly altering the
    input data, attackers can induce incorrect predictions and pose significant risks
    to safety-important applications like self-driving cars. The attack’s simplicity
    demonstrates how even minor, imperceptible changes can lead models astray. Consequently,
    developers must implement robust defenses against such threats.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个案例研究具体说明了对抗性示例是如何利用机器学习模型的模式识别机制的。通过微妙地改变输入数据，攻击者可以诱导错误的预测，并对自动驾驶汽车等安全重要应用造成重大风险。攻击的简单性展示了即使是微小的、不可察觉的变化也可能使模型误入歧途。因此，开发者必须实施针对此类威胁的强大防御措施。
- en: These threat types span different stages of the ML lifecycle and demand distinct
    defensive strategies. [Table 15.4](ch021.xhtml#tbl-threats-models-summary) below
    summarizes their key characteristics.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这些威胁类型跨越了机器学习生命周期的不同阶段，并需要不同的防御策略。下表[表15.4](ch021.xhtml#tbl-threats-models-summary)总结了它们的关键特征。
- en: 'Table 15.4: **Threat Landscape**: Machine learning systems face diverse threats
    throughout their lifecycle, ranging from data manipulation during training to
    model theft post-deployment. The table categorizes these threats by lifecycle
    stage and attack vector, clarifying how vulnerabilities manifest and enabling
    targeted mitigation strategies.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.4：**威胁格局**：机器学习系统在其生命周期中面临各种威胁，从训练期间的数据操纵到部署后的模型盗窃。该表按生命周期阶段和攻击向量对这些威胁进行分类，阐明漏洞如何显现，并使针对性的缓解策略成为可能。
- en: '| **Threat Type** | **Lifecycle Stage** | **Attack Vector** | **Example Impact**
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **威胁类型** | **生命周期阶段** | **攻击向量** | **示例影响** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Model Theft** | Deployment | API access, insider leaks | Stolen IP, model
    inversion, behavioral clone |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **模型盗窃** | 部署 | API访问，内部泄露 | 盗窃的知识产权，模型反演，行为克隆 |'
- en: '| **Data Poisoning** | Training | Label flipping, backdoors | Targeted misclassification,
    degraded accuracy |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **数据中毒** | 训练 | 标签翻转，后门 | 目标误分类，精度下降 |'
- en: '| **Adversarial Attacks** | Inference | Input perturbation | Real-time misclassification,
    safety failure |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **对抗性攻击** | 推理 | 输入扰动 | 实时误分类，安全故障 |'
- en: The appropriate defense for a given threat depends on its type, attack vector,
    and where it occurs in the ML lifecycle. [Figure 15.5](ch021.xhtml#fig-threat-mitigation-flow)
    provides a simplified decision flow that connects common threat categories, such
    as model theft, data poisoning, and adversarial examples, to corresponding defensive
    strategies. While real-world deployments may require more nuanced combinations
    of defenses as discussed in our layered defense framework, this flowchart serves
    as a conceptual guide for aligning threat models with practical mitigation techniques.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定威胁的适当防御措施取决于其类型、攻击向量以及它在机器学习生命周期中的位置。[图15.5](ch021.xhtml#fig-threat-mitigation-flow)提供了一个简化的决策流程，将常见的威胁类别，如模型盗窃、数据中毒和对抗性示例，与相应的防御策略相连接。虽然现实世界的部署可能需要更细致的防御组合，正如我们在分层防御框架中所讨论的，但此流程图作为将威胁模型与实际缓解技术相匹配的概念指南。
- en: '![](../media/file233.svg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file233.svg)'
- en: 'Figure 15.5: **Threat Mitigation Flow**: This diagram maps common machine learning
    threats to corresponding defense strategies, guiding selection based on attack
    vector and lifecycle stage. By following this flow, practitioners can align threat
    models with practical mitigation techniques, such as secure model access and data
    sanitization, to build more robust AI systems.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：**威胁缓解流程**：此图将常见的机器学习威胁映射到相应的防御策略，根据攻击向量和生命周期阶段进行选择。通过遵循此流程，从业者可以将威胁模型与实际缓解技术（如安全模型访问和数据净化）相匹配，以构建更稳健的人工智能系统。
- en: While ML models themselves present important attack surfaces, they ultimately
    run on hardware that can introduce vulnerabilities beyond the model’s control.
    The transition from software-based threats to hardware-based vulnerabilities represents
    a significant shift in the security landscape. Where software attacks target code
    logic and data flows, hardware attacks exploit the physical properties of the
    computing substrate itself.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习模型本身提供了重要的攻击面，但它们最终运行在可能引入超出模型控制范围的安全漏洞的硬件上。从基于软件的威胁到基于硬件的漏洞的转变代表了安全领域的重大转变。软件攻击针对代码逻辑和数据流，而硬件攻击则利用计算基质的物理属性。
- en: The specialized computing infrastructure that powers machine learning workloads
    creates a layered attack surface that extends far beyond traditional software
    vulnerabilities. This includes the processors that execute instructions, the memory
    systems that store data, and the interconnects that move information between components.
    Understanding these hardware-level risks is essential because they can bypass
    conventional software security mechanisms and remain difficult to detect. These
    risks are addressed through the hardware-based security mechanisms detailed in
    [Section 15.8.7](ch021.xhtml#sec-security-privacy-hardware-security-foundations-f5e8).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动机器学习工作负载的专用计算基础设施创建了一个分层攻击面，它远远超出了传统的软件漏洞。这包括执行指令的处理器、存储数据的内存系统以及在不同组件之间传递信息的互连。理解这些硬件级别的风险是至关重要的，因为它们可以绕过传统的软件安全机制，并且难以检测。这些风险通过[第15.8.7节](ch021.xhtml#sec-security-privacy-hardware-security-foundations-f5e8)中详细介绍的基于硬件的安全机制来解决。
- en: In the next section, we examine how adversaries can target the physical infrastructure
    that executes machine learning workloads through hardware bugs, physical tampering,
    side channels, and supply chain risks.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨对手如何通过硬件故障、物理篡改、旁路通道和供应链风险来针对执行机器学习工作负载的物理基础设施。
- en: Hardware-Level Security Vulnerabilities
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件级安全漏洞
- en: As machine learning systems move from research prototypes to large-scale, real-world
    deployments, their security depends on the hardware platforms they run on. Whether
    deployed in data centers, on edge devices, or in embedded systems, machine learning
    applications rely on a layered stack of processors, accelerators, memory, and
    communication interfaces. These hardware components, while essential for enabling
    efficient computation, introduce unique security risks that go beyond traditional
    software-based vulnerabilities.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统从研究原型发展到大规模、实际部署，它们的安全性依赖于运行的硬件平台。无论是在数据中心、边缘设备还是嵌入式系统中，机器学习应用都依赖于由处理器、加速器、内存和通信接口组成的分层堆栈。这些硬件组件虽然对于实现高效计算至关重要，但它们引入了独特的安全风险，这些风险超出了传统的基于软件的漏洞。
- en: Unlike general-purpose software systems, machine learning workflows often process
    high-value models and sensitive data in performance-constrained environments.
    This makes them attractive targets not only for software attacks but also for
    hardware-level exploitation. Vulnerabilities in hardware can expose models to
    theft, leak user data, disrupt system reliability, or allow adversaries to manipulate
    inference results. Because hardware operates below the software stack, such attacks
    can bypass conventional security mechanisms and remain difficult to detect.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与通用软件系统不同，机器学习工作流程通常在性能受限的环境中处理高价值模型和敏感数据。这使得它们不仅成为软件攻击的目标，也成为硬件级别利用的目标。硬件中的漏洞可能会使模型面临被盗用的风险，泄露用户数据，破坏系统可靠性，或允许对手操纵推理结果。由于硬件在软件堆栈之下运行，此类攻击可以绕过传统的安全机制，并且难以检测。
- en: Understanding hardware security threats requires considering how computing substrates
    implement machine learning operations. At the hardware level, CPU components like
    arithmetic logic units, registers, and caches execute the instructions that drive
    model inference and training. Memory hierarchies determine how quickly models
    can access parameters and intermediate results. The hardware-software interface,
    mediated by firmware and bootloaders, establishes the initial trust foundation
    for system operation. The physical properties of computation—including power consumption,
    timing characteristics, and electromagnetic emissions—create observable signals
    that attackers can exploit to extract sensitive information.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 理解硬件安全威胁需要考虑计算子层如何实现机器学习操作。在硬件层面，CPU组件如算术逻辑单元、寄存器和缓存执行驱动模型推理和训练的指令。内存层次结构决定了模型访问参数和中间结果的速度。硬件-软件接口，由固件和引导加载程序介导，为系统操作建立了初始的信任基础。计算的物理特性——包括功耗、时序特性和电磁辐射——创造了攻击者可以利用以提取敏感信息的可观察信号。
- en: Hardware threats arise from multiple sources that span the entire system lifecycle.
    Design flaws in processor architectures, exemplified by vulnerabilities like Meltdown
    and Spectre, can compromise security guarantees. Physical tampering enables direct
    manipulation of components and data flows. Side-channel attacks exploit unintended
    information leakage through power traces, timing variations, and electromagnetic
    radiation. Supply chain compromises introduce malicious components or modifications
    during manufacturing and distribution. Together, these threats form a critical
    attack surface that must be addressed to build trustworthy machine learning systems.
    For readers focusing on practical deployment, the key lessons center on supply
    chain verification, physical access controls, and hardware trust anchors, while
    the defensive strategies in [Section 15.8](ch021.xhtml#sec-security-privacy-comprehensive-defense-architectures-48ab)
    provide actionable guidance regardless of deep architectural expertise.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件威胁源于多个来源，贯穿整个系统生命周期。处理器架构中的设计缺陷，例如Meltdown和Spectre等漏洞，可能损害安全保证。物理篡改能够直接操纵组件和数据流。侧信道攻击通过电源轨迹、时间变化和电磁辐射等意外信息泄露来利用。供应链妥协在制造和分销过程中引入恶意组件或修改。这些威胁共同构成了一个关键的攻击面，必须解决以构建可信的机器学习系统。对于关注实际部署的读者，关键经验教训集中在供应链验证、物理访问控制和硬件信任锚上，而[第15.8节](ch021.xhtml#sec-security-privacy-comprehensive-defense-architectures-48ab)中的防御策略提供了无论深度架构专业知识如何都可行的指导。
- en: '[Table 15.5](ch021.xhtml#tbl-threat_types) summarizes the major categories
    of hardware security threats, describing their origins, methods, and implications
    for machine learning system design and deployment.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[表15.5](ch021.xhtml#tbl-threat_types) 总结了硬件安全威胁的主要类别，描述了它们的起源、方法和对于机器学习系统设计和部署的影响。'
- en: 'Table 15.5: **Hardware Threat Landscape**: Machine learning systems face diverse
    hardware threats ranging from intrinsic design flaws to physical attacks and supply
    chain vulnerabilities. Understanding these threats, and their relevance to ML
    hardware, is essential for building secure and trustworthy AI deployments.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '表15.5: **硬件威胁格局**：机器学习系统面临着从内在设计缺陷到物理攻击和供应链漏洞的多种硬件威胁。理解这些威胁及其与机器学习的相关性对于构建安全和可信的人工智能部署至关重要。'
- en: '| **Threat Type** | **Description** | **Relevance to ML Hardware Security**
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **威胁类型** | **描述** | **对机器学习硬件安全的相关性** |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Hardware Bugs** | Intrinsic flaws in hardware designs that can compromise
    system integrity. | Foundation of hardware vulnerability. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **硬件缺陷** | 硬件设计中固有的缺陷，可能损害系统完整性。 | 硬件漏洞的基础。 |'
- en: '| **Physical Attacks** | Direct exploitation of hardware through physical access
    or manipulation. | Basic and overt threat model. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **物理攻击** | 通过物理访问或操作直接利用硬件。 | 基本且明显的威胁模型。 |'
- en: '| **Fault-injection Attacks** | Induction of faults to cause errors in hardware
    operation, leading to potential system crashes. | Systematic manipulation leading
    to failure. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **故障注入攻击** | 诱导故障导致硬件操作中的错误，可能导致系统崩溃。 | 导致失败的系统性操纵。 |'
- en: '| **Side-Channel Attacks** | Exploitation of leaked information from hardware
    operation to extract sensitive data. | Indirect attack via environmental observation.
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **侧信道攻击** | 利用硬件操作中泄露的信息来提取敏感数据。 | 通过环境观察的间接攻击。 |'
- en: '| **Leaky Interfaces** | Vulnerabilities arising from interfaces that expose
    data unintentionally. | Data exposure through communication channels. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **泄露接口** | 由于接口无意中暴露数据而产生的漏洞。 | 通过通信渠道的数据暴露。 |'
- en: '| **Counterfeit Hardware** | Use of unauthorized hardware components that may
    have security flaws. | Compounded vulnerability issues. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **假冒硬件** | 使用未经授权的硬件组件，这些组件可能存在安全漏洞。 | 复合的漏洞问题。 |'
- en: '| **Supply Chain Risks** | Risks introduced through the hardware lifecycle,
    from production to deployment. | Cumulative & multifaceted security challenges.
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| **供应链风险** | 通过硬件生命周期（从生产到部署）引入的风险。 | 累积的多方面安全挑战。 |'
- en: Hardware Bugs
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件缺陷
- en: The first category of hardware threats stems from design vulnerabilities. Hardware
    is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit
    these vulnerabilities to access, manipulate, or extract sensitive data, breaching
    the confidentiality and integrity that users and services depend on. One of the
    most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[24](#fn24)—two
    vulnerabilities in modern processors that allow malicious programs to bypass memory
    isolation and read the data of other applications and the operating system ([Kocher
    et al. 2019a](ch058.xhtml#ref-Lipp2018meltdown), [2019b](ch058.xhtml#ref-Kocher2018spectre)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件威胁的第一类源于设计漏洞。硬件并非免疫于设计缺陷或错误这一普遍问题。攻击者可以利用这些漏洞访问、操纵或提取敏感数据，破坏用户和服务所依赖的机密性和完整性。最引人注目的例子之一是[Meltdown和Spectre](https://meltdownattack.com/)[24](#fn24)的发现——现代处理器中的两个漏洞，允许恶意程序绕过内存隔离并读取其他应用程序和操作系统的数据([Kocher等人2019a](ch058.xhtml#ref-Lipp2018meltdown),
    [2019b](ch058.xhtml#ref-Kocher2018spectre))。
- en: These attacks exploit speculative execution[25](#fn25), a performance optimization
    in CPUs that executes instructions out of order before safety checks are complete.
    While improving computational speed, this optimization inadvertently exposes sensitive
    data through microarchitectural side channels, such as CPU caches. The technical
    sophistication of these attacks highlights the difficulty of eliminating vulnerabilities
    even with extensive hardware validation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击利用了推测执行[25](#fn25)，这是CPU中的一种性能优化，在安全检查完成之前，它以不正确的顺序执行指令。虽然提高了计算速度，但这种优化无意中通过微架构的旁路通道（如CPU缓存）暴露了敏感数据。这些攻击的技术复杂性凸显了即使在广泛的硬件验证下，消除漏洞的难度。
- en: Further research has revealed that these were not isolated incidents. Variants
    such as Foreshadow, ZombieLoad, and RIDL target different microarchitectural elements,
    ranging from secure enclaves to CPU internal buffers, demonstrating that speculative
    execution flaws are a systemic hardware risk. This systemic nature means that
    while these attacks were first demonstrated on general-purpose CPUs, their implications
    extend to machine learning accelerators and specialized hardware. ML systems often
    rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs,
    or custom accelerators. These components process sensitive data such as personal
    information, medical records, or proprietary models. Vulnerabilities in any part
    of this stack could expose such data to attackers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的研究表明，这些并非孤立事件。如Foreshadow、ZombieLoad和RIDL等变体针对不同的微架构元素，从安全区域到CPU内部缓冲区，这表明推测执行漏洞是系统性的硬件风险。这种系统性意味着虽然这些攻击最初是在通用CPU上展示的，但它们的含义延伸到机器学习加速器和专用硬件。机器学习系统通常依赖于结合CPU与GPU、TPU、FPGA或定制加速器的异构计算平台。这些组件处理敏感数据，如个人信息、医疗记录或专有模型。此堆栈任何部分的漏洞都可能使这些数据暴露于攻击者。
- en: For example, an edge device like a smart camera running a face recognition model
    on an accelerator could be vulnerable if the hardware lacks proper cache isolation.
    An attacker might exploit this weakness to extract intermediate computations,
    model parameters, or user data. Similar risks exist in cloud inference services,
    where hardware multi-tenancy increases the chances of cross-tenant data leakage.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个边缘设备，如运行在加速器上的智能摄像头，如果硬件缺乏适当的缓存隔离，运行面部识别模型可能会存在漏洞。攻击者可能利用这种弱点提取中间计算、模型参数或用户数据。在云推理服务中，也存在类似的风险，其中硬件多租户增加了跨租户数据泄露的可能性。
- en: Such vulnerabilities pose concern in privacy-sensitive domains like healthcare,
    where ML systems routinely handle patient data. A breach could violate privacy
    regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[26](#fn26),
    leading to significant legal and ethical consequences. Similar regulatory risks
    apply globally, with GDPR[27](#fn27) imposing fines up to 4% of global revenue
    for organizations that fail to implement appropriate technical measures to protect
    EU citizens’ data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种漏洞在隐私敏感领域（如医疗保健）中引起关注，在这些领域中，机器学习系统通常处理患者数据。一旦发生泄露，可能会违反隐私法规，例如[健康保险可携带性和问责制法案（HIPAA）](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[26](#fn26)，导致重大的法律和伦理后果。类似的监管风险在全球范围内适用，GDPR[27](#fn27)对未能实施适当技术措施以保护欧盟公民数据的组织处以高达全球收入4%的罚款。
- en: These examples illustrate that hardware security is not solely about preventing
    physical tampering. It also requires architectural safeguards to prevent data
    leakage through the hardware itself. As new vulnerabilities continue to emerge
    across processors, accelerators, and memory systems, addressing these risks requires
    continuous mitigation efforts, often involving performance trade-offs, especially
    in compute- and memory-intensive ML workloads. Proactive solutions, such as confidential
    computing and trusted execution environments (TEEs), offer promising architectural
    defenses. However, achieving robust hardware security requires attention at every
    stage of the system lifecycle, from design to deployment.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子说明，硬件安全并不仅仅是防止物理篡改。它还需要架构保障来防止通过硬件本身的数据泄露。随着处理器、加速器和内存系统出现新的漏洞，应对这些风险需要持续的缓解努力，通常涉及性能权衡，尤其是在计算和内存密集型的机器学习工作负载中。主动解决方案，如机密计算和可信执行环境（TEEs），提供了有希望的建筑防御。然而，实现稳健的硬件安全需要在系统生命周期的每个阶段都给予关注，从设计到部署。
- en: Physical Attacks
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物理攻击
- en: Beyond design flaws, the second category involves direct physical manipulation.
    Physical tampering refers to the direct, unauthorized manipulation of computing
    hardware to undermine the integrity of machine learning systems. This type of
    attack is particularly concerning because it bypasses traditional software security
    defenses, directly targeting the physical components on which machine learning
    depends. ML systems are especially vulnerable to such attacks because they rely
    on hardware sensors, accelerators, and storage to process large volumes of data
    and produce reliable outcomes in real-world environments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计缺陷之外，第二类涉及直接的物理操作。物理篡改是指直接、未经授权地操纵计算硬件，以破坏机器学习系统的完整性。这种攻击尤其令人担忧，因为它绕过了传统的软件安全防御措施，直接针对机器学习所依赖的物理组件。由于机器学习系统依赖于硬件传感器、加速器和存储来处理大量数据并在现实世界环境中产生可靠的输出，因此它们特别容易受到此类攻击。
- en: While software security measures, including encryption, authentication, and
    access control, protect ML systems against remote attacks, they offer little defense
    against adversaries with physical access to devices. Physical tampering can range
    from simple actions, like inserting a malicious USB device into an edge server,
    to highly sophisticated manipulations such as embedding hardware trojans during
    chip manufacturing. These threats are particularly relevant for machine learning
    systems deployed at the edge or in physically exposed environments, where attackers
    may have opportunities to interfere with the hardware directly.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然包括加密、身份验证和访问控制在内的软件安全措施可以保护机器学习系统免受远程攻击，但它们对拥有物理访问设备设备的对手提供的防御很少。物理篡改的范围从简单的操作，如将恶意USB设备插入边缘服务器，到在芯片制造过程中嵌入硬件木马的高度复杂操作。这些威胁对于在边缘或物理暴露环境中部署的机器学习系统尤其相关，攻击者可能有机会直接干扰硬件。
- en: To understand how such attacks affect ML systems in practice, consider the example
    of an ML-powered drone used for environmental mapping or infrastructure inspection.
    The drone’s navigation depends on machine learning models that process data from
    GPS, cameras, and inertial measurement units. If an attacker gains physical access
    to the drone, they could replace or modify its navigation module, embedding a
    hidden backdoor that alters flight behavior or reroutes data collection. Such
    manipulation not only compromises the system’s reliability but also opens the
    door to misuse, such as surveillance or smuggling operations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解此类攻击如何在实际中影响机器学习系统，可以考虑一个用于环境测绘或基础设施检查的机器学习无人机示例。无人机的导航依赖于处理来自GPS、摄像头和惯性测量单元的数据的机器学习模型。如果攻击者获得了对无人机的物理访问权限，他们可以替换或修改其导航模块，嵌入一个隐藏的后门，改变飞行行为或重新路由数据收集。这种操纵不仅损害了系统的可靠性，还打开了滥用的大门，例如监视或走私行动。
- en: These threats extend across application domains. Physical attacks are not limited
    to mobility systems. Biometric access control systems, which rely on ML models
    to process face or fingerprint data, are also vulnerable. These systems typically
    use embedded hardware to capture and process biometric inputs. An attacker could
    physically replace a biometric sensor with a modified component designed to capture
    and transmit personal identification data to an unauthorized receiver. This creates
    multiple vulnerabilities including unauthorized data access and enabling future
    impersonation attacks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些威胁跨越了应用领域。物理攻击不仅限于移动系统。依赖于机器学习模型处理人脸或指纹数据的生物识别访问控制系统也容易受到攻击。这些系统通常使用嵌入式硬件来捕获和处理生物识别输入。攻击者可以用一个设计用来捕获和传输个人识别数据给未经授权接收者的修改组件物理替换生物识别传感器。这创造了多个漏洞，包括未经授权的数据访问和允许未来的冒充攻击。
- en: In addition to tampering with external sensors, attackers may target internal
    hardware subsystems. For example, the sensors used in autonomous vehicles, including
    cameras, LiDAR, and radar, are important for ML models that interpret the surrounding
    environment. A malicious actor could physically misalign or obstruct these sensors,
    degrading the model’s perception capabilities and creating safety hazards.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了篡改外部传感器外，攻击者还可能针对内部硬件子系统。例如，用于自动驾驶汽车中的传感器，包括摄像头、激光雷达和雷达，对于解释周围环境的机器学习模型至关重要。恶意行为者可以通过物理错位或遮挡这些传感器，降低模型的感知能力，并造成安全隐患。
- en: Hardware trojans pose another serious risk. Malicious modifications introduced
    during chip fabrication or assembly can embed dormant circuits in ML accelerators
    or inference chips. These trojans may remain inactive under normal conditions
    but trigger malicious behavior when specific inputs are processed or system states
    are reached. Such hidden vulnerabilities can disrupt computations, leak model
    outputs, or degrade system performance in ways that are extremely difficult to
    diagnose post-deployment.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件木马也构成了另一个严重风险。在芯片制造或组装过程中引入的恶意修改可以在机器学习加速器或推理芯片中嵌入休眠电路。这些木马在正常条件下可能保持不活跃，但在处理特定输入或达到系统状态时触发恶意行为。这种隐藏的漏洞可能会干扰计算、泄露模型输出或以难以诊断的方式降低系统性能。
- en: Memory subsystems are also attractive targets. Attackers with physical access
    to edge devices or embedded ML accelerators could manipulate memory chips to extract
    encrypted model parameters or training data. Fault injection techniques, including
    voltage manipulation and electromagnetic interference, can further degrade system
    reliability by corrupting model weights or forcing incorrect computations during
    inference.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 内存子系统也是吸引攻击者的目标。拥有物理访问边缘设备或嵌入式机器学习加速器的攻击者可以操纵内存芯片以提取加密的模型参数或训练数据。包括电压操控和电磁干扰在内的故障注入技术可以通过破坏模型权重或在推理期间强制执行错误计算来进一步降低系统可靠性。
- en: Physical access threats extend to data center and cloud environments as well.
    Attackers with sufficient access could install hardware implants, such as keyloggers
    or data interceptors, to capture administrative credentials or monitor data streams.
    Such implants can provide persistent backdoor access, enabling long-term surveillance
    or data exfiltration from ML training and inference pipelines.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 物理访问威胁也扩展到数据中心和云环境。拥有足够访问权限的攻击者可以安装硬件植入物，如键盘记录器或数据拦截器，以捕获管理凭证或监控数据流。这种植入物可以提供持续的后门访问，使长期监控或从机器学习训练和推理管道中窃取数据成为可能。
- en: In summary, physical attacks on machine learning systems threaten both security
    and reliability across a wide range of deployment environments. Addressing these
    risks requires a combination of hardware-level protections, tamper detection mechanisms,
    and supply chain integrity checks. Without these safeguards, even the most secure
    software defenses may be undermined by vulnerabilities introduced through direct
    physical manipulation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对机器学习系统的物理攻击威胁了广泛部署环境中的安全和可靠性。应对这些风险需要结合硬件级别的保护、篡改检测机制和供应链完整性检查。如果没有这些安全措施，即使是最安全的软件防御也可能被通过直接物理操作引入的漏洞所破坏。
- en: Fault Injection Attacks
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 故障注入攻击
- en: Building on physical tampering techniques, fault injection represents a more
    sophisticated approach to hardware exploitation. Fault injection is a powerful
    class of physical attacks that deliberately disrupts hardware operations to induce
    errors in computation. These induced faults can compromise the integrity of machine
    learning models by causing them to produce incorrect outputs, degrade reliability,
    or leak sensitive information. For ML systems, such faults not only disrupt inference
    but also expose models to deeper exploitation, including reverse engineering and
    bypass of security protocols ([Joye and Tunstall 2012](ch058.xhtml#ref-joye2012fault)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在物理篡改技术的基础上，故障注入代表了一种更复杂的硬件利用方法。故障注入是一类强大的物理攻击，它故意破坏硬件操作以在计算中引起错误。这些引起的故障可以通过导致模型产生错误输出、降低可靠性或泄露敏感信息来损害机器学习模型的完整性。对于机器学习系统，这些故障不仅会干扰推理，还会使模型面临更深入的利用，包括逆向工程和安全协议的绕过（[Joye
    和 Tunstall 2012](ch058.xhtml#ref-joye2012fault)）。
- en: Attackers achieve fault injection by applying precisely timed physical or electrical
    disturbances to the hardware while it is executing computations. Techniques such
    as low-voltage manipulation ([Barenghi et al. 2010](ch058.xhtml#ref-barenghi2010low)),
    power spikes ([M. Hutter, Schmidt, and Plos 2009](ch058.xhtml#ref-hutter2009contact)),
    clock glitches ([Amiel, Clavier, and Tunstall 2006](ch058.xhtml#ref-amiel2006fault)),
    electromagnetic pulses ([Agrawal et al. 2007](ch058.xhtml#ref-agrawal2003side)),
    temperature variations ([S. Skorobogatov 2009](ch058.xhtml#ref-skorobogatov2009local)),
    and even laser strikes ([S. P. Skorobogatov and Anderson 2003](ch058.xhtml#ref-skorobogatov2003optical))
    have been demonstrated to corrupt specific parts of a program’s execution. These
    disturbances can cause effects such as bit flips, skipped instructions, or corrupted
    memory states, which adversaries can exploit to alter ML model behavior or extract
    sensitive information.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者通过在硬件执行计算时施加精确时间控制的物理或电气干扰来实现故障注入。诸如低电压操作([Barenghi 等人 2010](ch058.xhtml#ref-barenghi2010low))、电源尖峰([M.
    Hutter, Schmidt 和 Plos 2009](ch058.xhtml#ref-hutter2009contact))、时钟故障([Amiel,
    Clavier 和 Tunstall 2006](ch058.xhtml#ref-amiel2006fault))、电磁脉冲([Agrawal 等人 2007](ch058.xhtml#ref-agrawal2003side))、温度变化([S.
    Skorobogatov 2009](ch058.xhtml#ref-skorobogatov2009local))以及甚至激光打击([S. P. Skorobogatov
    和 Anderson 2003](ch058.xhtml#ref-skorobogatov2003optical))等技术已被证明可以破坏程序执行的具体部分。这些干扰可以引起诸如位翻转、跳过的指令或损坏的内存状态等效果，攻击者可以利用这些效果来改变机器学习模型的行为或提取敏感信息。
- en: For machine learning systems, these attacks pose several concrete risks. Fault
    injection can degrade model accuracy, force incorrect classifications, trigger
    denial of service, or even leak internal model parameters. For example, attackers
    could inject faults into an embedded ML model running on a microcontroller, forcing
    it to misclassify inputs in safety-important applications such as autonomous navigation
    or medical diagnostics. More sophisticated attackers may target memory or control
    logic to steal intellectual property, such as proprietary model weights or architecture
    details.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习系统，这些攻击带来了几个具体的风险。故障注入会降低模型精度，强制进行错误分类，触发拒绝服务，甚至泄露内部模型参数。例如，攻击者可以向在微控制器上运行的嵌入式机器学习模型注入故障，迫使其在安全至关重要的应用中（如自主导航或医疗诊断）错误地分类输入。更复杂的攻击者可能会针对内存或控制逻辑来窃取知识产权，例如专有模型权重或架构细节。
- en: The practical viability of these attacks has been demonstrated through controlled
    experiments. One notable example is the work by Breier et al. ([2018](ch058.xhtml#ref-breier2018deeplaser)),
    where researchers successfully used a laser fault injection attack on a deep neural
    network deployed on a microcontroller. By heating specific transistors, as shown
    in [Figure 15.6](ch021.xhtml#fig-laser-bitflip). they forced the hardware to skip
    execution steps, including a ReLU activation function.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击的实际可行性已通过控制实验得到证明。一个值得注意的例子是 Breier 等人的工作([2018](ch058.xhtml#ref-breier2018deeplaser))，研究人员成功地对部署在微控制器上的深度神经网络进行了激光故障注入攻击。如图
    15.6 所示，通过加热特定的晶体管，他们迫使硬件跳过执行步骤，包括 ReLU 激活函数。
- en: '![](../media/file234.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file234.png)'
- en: 'Figure 15.6: **Laser Fault Injection**: Focused laser pulses induce bit flips
    within microcontroller memory, enabling attackers to manipulate model execution
    and compromise system integrity. Researchers utilize this technique to simulate
    hardware errors, revealing vulnerabilities in embedded machine learning systems
    and informing the development of fault-tolerant designs. Source: ([Breier et al.
    2018](ch058.xhtml#ref-breier2018deeplaser)).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '图15.6: **激光故障注入**: 聚焦的激光脉冲在微控制器内存中引起位翻转，使攻击者能够操纵模型执行并损害系统完整性。研究人员利用这项技术来模拟硬件错误，揭示嵌入式机器学习系统中的漏洞，并指导容错设计的开发。来源：([Breier等人2018](ch058.xhtml#ref-breier2018deeplaser))。'
- en: This manipulation is illustrated in [Figure 15.7](ch021.xhtml#fig-injection),
    which shows a segment of assembly code implementing the ReLU activation function.
    Normally, the code compares the most significant bit (MSB) of the accumulator
    to zero and uses a brge (branch if greater or equal) instruction to skip the assignment
    if the value is non-positive. However, the fault injection suppresses the branch,
    causing the processor to always execute the “else” block. As a result, the neuron’s
    output is forcibly zeroed out, regardless of the input value.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操纵在[图15.7](ch021.xhtml#fig-injection)中得到了说明，该图显示了一个实现ReLU激活函数的汇编代码段。通常，代码比较累加器的最高有效位（MSB）与零，并使用brge（如果大于或等于则分支）指令，如果值非正则跳过赋值。然而，故障注入抑制了分支，导致处理器始终执行“else”块。因此，神经元的输出被强制设为零，无论输入值如何。
- en: '![](../media/file235.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file235.png)'
- en: 'Figure 15.7: **Fault Injection Attack**: Manipulating assembly code bypasses
    safety checks, forcing a neuron’s output to zero regardless of input and demonstrating
    a hardware vulnerability in machine learning systems. Source: ([Breier et al.
    2018](ch058.xhtml#ref-breier2018deeplaser)).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '图15.7: **故障注入攻击**: 通过操纵汇编代码绕过安全检查，强制神经元输出为零，无论输入如何，从而展示了机器学习系统中的硬件漏洞。来源：([Breier等人2018](ch058.xhtml#ref-breier2018deeplaser))。'
- en: Fault injection attacks can also be combined with side-channel analysis, where
    attackers first observe power or timing characteristics to infer model structure
    or data flow. This reconnaissance allows them to target specific layers or operations,
    such as activation functions or final decision layers, maximizing the impact of
    the injected faults.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 故障注入攻击还可以与侧信道分析相结合，其中攻击者首先观察功率或时序特征，以推断模型结构或数据流。这种侦察使他们能够针对特定的层或操作，例如激活函数或最终决策层，最大化注入故障的影响。
- en: Embedded and edge ML systems are particularly vulnerable because they often
    lack physical hardening and operate under resource constraints that limit runtime
    defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers
    may gain direct access to system buses and memory, enabling precise fault manipulation.
    Many embedded ML models are designed to be lightweight, leaving them with little
    redundancy or error correction to recover from induced faults.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式和边缘机器学习系统尤其容易受到攻击，因为它们通常缺乏物理加固，并且在资源受限的环境下运行，这限制了运行时防御。如果没有防篡改包装或安全硬件区域，攻击者可能直接访问系统总线内存，从而实现精确的故障操纵。许多嵌入式机器学习模型被设计为轻量级，这使它们几乎没有冗余或错误纠正来恢复由诱导的故障。
- en: Mitigating fault injection requires multiple complementary protections. Physical
    protections, such as tamper-proof enclosures and design obfuscation, help limit
    physical access. Anomaly detection techniques can monitor sensor inputs or model
    outputs for signs of fault-induced inconsistencies ([Hsiao et al. 2023](ch058.xhtml#ref-hsiao2023mavfi)).
    Error-correcting memories and secure firmware can reduce the likelihood of silent
    corruption. Techniques such as model watermarking may provide traceability if
    stolen models are later deployed by an adversary.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解故障注入需要多种互补的保护措施。物理保护，如防篡改外壳和设计混淆，有助于限制物理访问。异常检测技术可以监控传感器输入或模型输出以寻找故障引起的异常迹象
    ([Hsiao等人2023](ch058.xhtml#ref-hsiao2023mavfi))。纠错内存和安全的固件可以降低静默损坏的可能性。如果被盗模型后来被对手部署，模型水印等技术可能提供可追溯性。
- en: These protections are difficult to implement in cost- and power-constrained
    environments, where adding cryptographic hardware or redundancy may not be feasible.
    Achieving resilience to fault injection requires cross-layer design considerations
    that span electrical, firmware, software, and system architecture levels. Without
    such holistic design practices, ML systems deployed in the field may remain exposed
    to these low-cost yet highly effective physical attacks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在成本和功耗受限的环境中实现这些保护措施很困难，在这些环境中，添加加密硬件或冗余可能不可行。要实现针对故障注入的弹性，需要跨层设计考虑，这些考虑跨越了电气、固件、软件和系统架构级别。没有这样的整体设计实践，部署在现场的机器学习系统可能仍然容易受到这些低成本但高度有效的物理攻击的影响。
- en: Side-Channel Attacks
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 侧信道攻击
- en: Moving from direct fault injection to indirect information leakage, side-channel
    attacks constitute a class of security breaches that exploit information inadvertently
    revealed through the physical implementation of computing systems. In contrast
    to direct attacks that target software or network vulnerabilities, these attacks
    use the system’s hardware characteristics, including power consumption, electromagnetic
    emissions, or timing behavior, to extract sensitive information.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从直接故障注入到间接信息泄露，侧信道攻击构成了一类安全漏洞，这些漏洞利用了通过计算系统的物理实现无意中泄露的信息。与针对软件或网络漏洞的直接攻击不同，这些攻击利用系统的硬件特性，包括功耗、电磁辐射或时序行为，以提取敏感信息。
- en: The core premise of a side-channel attack is that a device’s operation can leak
    information through observable physical signals. Such leaks may originate from
    the electrical power the device consumes ([Kocher, Jaffe, and Jun 1999](ch058.xhtml#ref-kocher1999differential)),
    the electromagnetic fields it emits ([Gandolfi, Mourtel, and Olivier 2001](ch058.xhtml#ref-gandolfi2001electromagnetic)),
    the time required to complete computations, or even the acoustic noise it produces.
    By carefully measuring and analyzing these signals, attackers can infer internal
    system states or recover secret data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 侧信道攻击的核心前提是设备的操作可以通过可观察的物理信号泄露信息。这种泄露可能源于设备消耗的电能([Kocher, Jaffe, and Jun 1999](ch058.xhtml#ref-kocher1999differential))，它发出的电磁场([Gandolfi,
    Mourtel, and Olivier 2001](ch058.xhtml#ref-gandolfi2001electromagnetic))，完成计算所需的时间，甚至它产生的声学噪声。通过仔细测量和分析这些信号，攻击者可以推断内部系统状态或恢复机密数据。
- en: Although these techniques are commonly discussed in cryptography, they are equally
    relevant to machine learning systems. ML models deployed on hardware accelerators,
    embedded devices, or edge systems often process sensitive data. Even when these
    models are protected by secure algorithms or encryption, their physical execution
    may leak side-channel signals that can be exploited by adversaries.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些技术在密码学中经常被讨论，但它们对机器学习系统同样相关。部署在硬件加速器、嵌入式设备或边缘系统上的机器学习模型经常处理敏感数据。即使这些模型受到安全算法或加密的保护，它们的物理执行可能泄露侧信道信号，这些信号可能被对手利用。
- en: One of the most widely studied examples involves Advanced Encryption Standard
    (AES)[28](#fn28) implementations. While AES is mathematically secure, the physical
    process of computing its encryption functions leaks measurable signals.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛研究的一个例子涉及高级加密标准（AES）[28](#fn28)的实现。虽然AES在数学上是安全的，但其加密函数的物理计算过程会泄露可测量的信号。
- en: A useful example of this attack technique can be seen in a power analysis of
    a password authentication process. Consider a device that verifies a 5-byte password—in
    this case, `0x61, 0x52, 0x77, 0x6A, 0x73`. During authentication, the device receives
    each byte sequentially over a serial interface, and its power consumption pattern
    reveals how the system responds as it processes these inputs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击技术的一个有用例子可以在密码认证过程的功耗分析中看到。考虑一个验证5字节密码的设备——在这种情况下，`0x61, 0x52, 0x77, 0x6A,
    0x73`。在认证过程中，设备通过串行接口逐个接收每个字节，其功耗模式揭示了系统如何响应这些输入。
- en: '[Figure 15.8](ch021.xhtml#fig-encryption) shows the device’s behavior when
    the correct password is entered. The red waveform captures the serial data stream,
    marking each byte as it is received. The blue curve records the device’s power
    consumption over time. When the full, correct password is supplied, the power
    profile remains stable and consistent across all five bytes, providing a clear
    baseline for comparison with failed attempts.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.8](ch021.xhtml#fig-encryption) 展示了输入正确密码时设备的行为。红色波形捕获了串行数据流，标记接收到的每个字节。蓝色曲线记录了设备随时间变化的功耗。当提供完整的正确密码时，功率曲线在所有五个字节上保持稳定和一致，为与失败尝试的比较提供了一个清晰的基线。'
- en: '![](../media/file236.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file236.png)'
- en: 'Figure 15.8: **Power Profile**: The device’s power consumption remains stable
    during authentication when the correct password is entered, setting a baseline
    for comparison in subsequent figures through This figure. Source: colin o’flynn.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：**功率曲线**：当输入正确的密码时，设备的功耗在认证过程中保持稳定，为后续图中的比较设定了基线。来源：Colin O’Flynn。
- en: When an incorrect password is entered, the power analysis chart changes as shown
    in [Figure 15.9](ch021.xhtml#fig-encryption2). In this case, the first three bytes
    (`0x61, 0x52, 0x77`) are correct, so the power patterns closely match the correct
    password up to that point. However, when the fourth byte (`0x42`) is processed
    and found to be incorrect, the device halts authentication. This change is reflected
    in the sudden jump in the blue power line, indicating that the device has stopped
    processing and entered an error state.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入错误的密码时，功率分析图会像[图15.9](ch021.xhtml#fig-encryption2)中所示那样改变。在这种情况下，前三个字节（`0x61,
    0x52, 0x77`）是正确的，所以功率模式与正确密码匹配到那个点。然而，当处理第四个字节（`0x42`）并发现它是错误的时，设备停止认证。这种变化在蓝色功率线的突然跳跃中反映出来，表明设备已停止处理并进入错误状态。
- en: '![](../media/file237.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file237.png)'
- en: 'Figure 15.9: **Side-Channel Attack Vulnerability**: Power consumption patterns
    reveal cryptographic key information during authentication; consistent power usage
    indicates correct password bytes, while abrupt changes signal incorrect input
    and halted processing. Even without knowing the password, an attacker can infer
    it by analyzing the device’s power usage during authentication attempts via this
    figure. Source: Colin O’Flynn.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：**侧信道攻击漏洞**：在认证过程中，功耗模式揭示了加密密钥信息；一致的功耗表示正确的密码字节，而突然的变化则表示输入错误和停止处理。即使不知道密码，攻击者也可以通过分析设备在认证尝试期间的功耗来推断密码。来源：Colin
    O’Flynn。
- en: '[Figure 15.10](ch021.xhtml#fig-encryption3) shows the case where the password
    is entirely incorrect (`0x30, 0x30, 0x30, 0x30, 0x30`). Here, the device detects
    the mismatch immediately after the first byte and halts processing much earlier.
    This is again visible in the power profile, where the blue line exhibits a sharp
    jump following the first byte, reflecting the device’s early termination of authentication.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.10](ch021.xhtml#fig-encryption3) 展示了密码完全错误的情况（`0x30, 0x30, 0x30, 0x30,
    0x30`）。在这里，设备在第一个字节之后立即检测到不匹配并提前停止处理。这在功率曲线中也很明显，蓝色线在第一个字节之后出现急剧跳跃，反映了设备在认证过程中的早期终止。'
- en: '![](../media/file238.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file238.png)'
- en: 'Figure 15.10: **Power Consumption Jump**: The blue line’s sharp increase after
    processing the first byte indicates immediate authentication failure, highlighting
    how incorrect passwords are quickly detected through power usage. Source: Colin
    O’Flynn.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10：**功耗跳跃**：蓝色线在处理第一个字节后的急剧增加表明立即认证失败，突出了如何通过功耗快速检测到错误的密码。来源：Colin O’Flynn。
- en: These examples demonstrate how attackers can exploit observable power consumption
    differences to reduce the search space and eventually recover secret data through
    brute-force analysis. By systematically measuring power consumption patterns and
    correlating them with different inputs, attackers can extract sensitive information
    that should remain hidden.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了攻击者如何利用可观察到的功耗差异来缩小搜索空间，并通过暴力分析最终恢复秘密数据。通过系统地测量功耗模式并将它们与不同的输入相关联，攻击者可以提取出应该保持隐藏的敏感信息。
- en: The scope of these vulnerabilities extends beyond cryptographic applications.
    Machine learning applications face similar risks. For example, an ML-based speech
    recognition system processing voice commands on a local device could leak timing
    or power signals that reveal which commands are being processed. Even subtle acoustic
    or electromagnetic emissions may expose operational patterns that an adversary
    could exploit to infer user behavior.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这些漏洞的范围超出了加密应用。机器学习应用也面临着类似的风险。例如，一个基于ML的语音识别系统在本地设备上处理语音命令时，可能会泄露时间或功耗信号，揭示正在处理的命令。即使是微妙的声学或电磁发射也可能暴露操作模式，对手可以利用这些模式来推断用户行为。
- en: Historically, side-channel attacks have been used to bypass even the most secure
    cryptographic systems. In the 1960s, British intelligence agency MI5 famously
    exploited acoustic emissions from a cipher machine in the Egyptian Embassy ([Burnet
    and Thomas 1989](ch058.xhtml#ref-Burnet1989Spycatcher)). By capturing the mechanical
    clicks of the machine’s rotors, MI5 analysts were able to dramatically reduce
    the complexity of breaking encrypted messages. This early example illustrates
    that side-channel vulnerabilities are not confined to the digital age but are
    rooted in the physical nature of computation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，旁路攻击已被用于绕过甚至最安全的加密系统。在20世纪60年代，英国情报机构MI5著名地利用了埃及大使馆密码机的声学发射([Burnet and
    Thomas 1989](ch058.xhtml#ref-Burnet1989Spycatcher))。通过捕捉机器转子的机械点击声，MI5分析人员能够显著降低破解加密信息的复杂性。这个早期例子说明，旁路漏洞不仅限于数字时代，而且根植于计算的物理性质。
- en: Today, these techniques have advanced to include attacks such as keyboard eavesdropping
    ([Asonov and Agrawal, n.d.](ch058.xhtml#ref-Asonov2004Keyboard)), power analysis
    on cryptographic hardware ([Gnad, Oboril, and Tahoori 2017](ch058.xhtml#ref-gnad2017voltage)),
    and voltage-based attacks on ML accelerators ([M. Zhao and Suh 2018](ch058.xhtml#ref-zhao2018fpga)).
    Timing attacks, electromagnetic leakage, and thermal emissions continue to provide
    adversaries with indirect channels for observing system behavior.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，这些技术已经发展到包括键盘窃听([Asonov and Agrawal, n.d.](ch058.xhtml#ref-Asonov2004Keyboard))、对加密硬件的功耗分析([Gnad,
    Oboril, and Tahoori 2017](ch058.xhtml#ref-gnad2017voltage))以及基于电压的ML加速器攻击([M.
    Zhao and Suh 2018](ch058.xhtml#ref-zhao2018fpga))等攻击。时间攻击、电磁泄漏和热辐射继续为对手提供观察系统行为的间接渠道。
- en: Machine learning systems deployed on specialized accelerators or embedded platforms
    are especially at risk. Attackers may exploit side-channel signals to infer model
    structure, steal parameters, or reconstruct private training data. As ML becomes
    increasingly deployed in cloud, edge, and embedded environments, these side-channel
    vulnerabilities pose significant challenges to system security.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在专用加速器或嵌入式平台上的机器学习系统尤其容易受到攻击。攻击者可能利用旁路信号来推断模型结构、窃取参数或重建私有训练数据。随着ML在云、边缘和嵌入式环境中的日益部署，这些旁路漏洞对系统安全构成了重大挑战。
- en: Understanding the persistence and evolution of side-channel attacks is important
    for building resilient machine learning systems. By recognizing that where there
    is a signal, there is potential for exploitation, system designers can begin to
    address these risks through a combination of hardware shielding, algorithmic defenses,
    and operational safeguards.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 理解旁路攻击的持续性和演变对于构建具有弹性的机器学习系统至关重要。通过认识到只要有信号，就有被利用的潜力，系统设计人员可以通过结合硬件屏蔽、算法防御和操作保障来开始解决这些风险。
- en: Leaky Interfaces
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泄露接口
- en: While side-channel attacks exploit unintended physical signals, leaky interfaces
    represent a different category of vulnerability involving exposed communication
    channels. Interfaces in computing systems are important for enabling communication,
    diagnostics, and updates. However, these same interfaces can become significant
    security vulnerabilities when they unintentionally expose sensitive information
    or accept unverified inputs. Such leaky interfaces often go unnoticed during system
    design, yet they provide attackers with powerful entry points to extract data,
    manipulate functionality, or introduce malicious code.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当旁路攻击利用未预期的物理信号时，泄露接口代表了一种不同类别的漏洞，涉及暴露的通信通道。计算系统中的接口对于实现通信、诊断和更新至关重要。然而，这些相同的接口在无意中暴露敏感信息或接受未经验证的输入时，可能成为重大的安全漏洞。这种泄露接口通常在系统设计期间被忽视，但它们为攻击者提供了强大的入口点，用于提取数据、操纵功能或引入恶意代码。
- en: A leaky interface is any access point that reveals more information than intended,
    often because of weak authentication, lack of encryption, or inadequate isolation.
    These issues have been widely demonstrated across consumer, medical, and industrial
    systems.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 泄露接口是指任何泄露比预期更多信息的访问点，通常是由于身份验证薄弱、缺乏加密或隔离不足。这些问题在消费、医疗和工业系统中已被广泛证明。
- en: For example, many WiFi-enabled baby monitors have been found to expose unsecured
    remote access ports[29](#fn29), allowing attackers to intercept live audio and
    video feeds from inside private homes. Similarly, researchers have identified
    wireless vulnerabilities in pacemakers[30](#fn30) that could allow attackers to
    manipulate cardiac functions if exploited, raising life-threatening safety concerns.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，许多支持WiFi的婴儿监控器被发现暴露了未受保护的远程访问端口[29](#fn29)，这使得攻击者能够拦截来自私人住宅内的实时音频和视频流。同样，研究人员已经发现了起搏器中的无线漏洞[30](#fn30)，如果被利用，攻击者可能操纵心脏功能，引发生命威胁的安全问题。
- en: A notable case involving smart lightbulbs demonstrated that accessible debug
    ports[31](#fn31) left on production devices leaked unencrypted WiFi credentials.
    This security oversight provided attackers with a pathway to infiltrate home networks
    without needing to bypass standard security mechanisms.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个涉及智能灯泡的显著案例表明，留在生产设备上的可访问调试端口[31](#fn31)泄露了未加密的WiFi凭证。这种安全疏忽为攻击者提供了渗透家庭网络而不需要绕过标准安全机制的途径。
- en: These examples reveal vulnerability patterns that directly apply to machine
    learning deployments. While these examples do not target machine learning systems
    directly, they illustrate architectural patterns that are highly relevant to ML-allowd
    devices. Consider a smart home security system that uses machine learning to detect
    user routines and automate responses. Such a system may include a maintenance
    or debug interface for software updates. If this interface lacks proper authentication
    or transmits data unencrypted, attackers on the same network could gain unauthorized
    access. This intrusion could expose user behavior patterns, compromise model integrity,
    or disable security features altogether.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子揭示了直接适用于机器学习部署的漏洞模式。虽然这些例子并没有直接针对机器学习系统，但它们说明了与ML-allowed设备高度相关的架构模式。考虑一个使用机器学习来检测用户习惯并自动响应的智能家居安全系统。这样的系统可能包括用于软件更新的维护或调试接口。如果这个接口缺乏适当的身份验证或未加密传输数据，同一网络上的攻击者可能获得未经授权的访问。这种入侵可能会暴露用户行为模式，损害模型完整性，或完全禁用安全功能。
- en: Leaky interfaces in ML systems can also expose training data, model parameters,
    or intermediate outputs. Such exposure can allow attackers to craft adversarial
    examples, steal proprietary models, or reverse-engineer system behavior. Worse
    still, these interfaces may allow attackers to tamper with firmware, introducing
    malicious code that disables devices or recruits them into botnets.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统中的泄露接口也可能暴露训练数据、模型参数或中间输出。这种暴露可能允许攻击者构建对抗性示例、窃取专有模型或逆向工程系统行为。更糟糕的是，这些接口可能允许攻击者篡改固件，引入恶意代码，禁用设备或将它们招募到僵尸网络中。
- en: Mitigating these risks requires coordinated protections across technical and
    organizational domains. Technical safeguards such as strong authentication, encrypted
    communications, and runtime anomaly detection are important. Organizational practices
    such as interface inventories, access control policies, and ongoing audits are
    equally important. Adopting a zero-trust architecture, where no interface is trusted
    by default, further reduces exposure by limiting access to only what is strictly
    necessary.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这些风险需要在技术和组织领域进行协调保护。技术保障措施，如强身份验证、加密通信和运行时异常检测，非常重要。组织实践，如接口清单、访问控制策略和持续审计，同样重要。采用零信任架构，其中默认不信任任何接口，通过仅限制访问严格必要的内容来进一步减少暴露。
- en: For designers of ML-powered systems, securing interfaces must be a first-class
    concern alongside algorithmic and data-centric design. Whether the system operates
    in the cloud, on the edge, or in embedded environments, failure to secure these
    access points risks undermining the entire system’s trustworthiness.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习系统设计者来说，确保接口安全必须与算法和数据为中心的设计一样成为首要关注点。无论系统是在云端、边缘还是嵌入式环境中运行，未能保护这些访问点可能会损害整个系统的可信度。
- en: Counterfeit Hardware
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 伪造硬件
- en: Beyond vulnerabilities in legitimate hardware, another significant threat emerges
    from the supply chain itself. Machine learning systems depend on the reliability
    and security of the hardware on which they run. Yet, in today’s globalized hardware
    ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious
    threat to system integrity. Counterfeit components refer to unauthorized reproductions
    of genuine parts, designed to closely imitate their appearance and functionality.
    These components can enter machine learning systems through complex procurement
    and manufacturing processes that span multiple vendors and regions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 除了合法硬件中的漏洞之外，供应链本身也出现了另一个重大威胁。机器学习系统依赖于它们运行的硬件的可靠性和安全性。然而，在当今全球化的硬件生态系统中，假冒或克隆硬件的风险已成为对系统完整性的严重威胁。假冒组件是指未经授权的真正部件的复制，设计上旨在尽可能模仿其外观和功能。这些组件可以通过跨越多个供应商和地区的复杂采购和制造流程进入机器学习系统。
- en: A single lapse in component sourcing can introduce counterfeit hardware into
    important systems. For example, a facial recognition system deployed for secure
    facility access might unknowingly rely on counterfeit processors. These unauthorized
    components could fail to process biometric data correctly or introduce hidden
    vulnerabilities that allow attackers to bypass authentication controls.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 组件采购的任何失误都可能将假冒硬件引入重要系统。例如，用于安全设施访问的面部识别系统可能无意中依赖假冒处理器。这些未经授权的组件可能无法正确处理生物识别数据，或引入隐藏的漏洞，使攻击者能够绕过身份验证控制。
- en: The risks posed by counterfeit hardware are multifaceted. From a reliability
    perspective, such components often degrade faster, perform unpredictably, or fail
    under load due to substandard manufacturing. From a security perspective, counterfeit
    hardware may include hidden backdoors or malicious circuitry, providing attackers
    with undetectable pathways to compromise machine learning systems. A cloned network
    router installed in a data center, for instance, could silently intercept model
    predictions or user data, creating systemic vulnerabilities across the entire
    infrastructure.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 假冒硬件带来的风险是多方面的。从可靠性角度来看，这些组件通常退化更快，表现不可预测，或在负载下因制造标准低下而失败。从安全角度来看，假冒硬件可能包含隐藏的后门或恶意电路，为攻击者提供无法检测的途径来破坏机器学习系统。例如，在数据中心安装的克隆网络路由器可能静默地拦截模型预测或用户数据，在整个基础设施中创建系统性的漏洞。
- en: Legal and regulatory risks further compound the problem. Organizations that
    unknowingly integrate counterfeit components into their ML systems may face serious
    legal consequences, including penalties for violating safety, privacy, or cybersecurity
    regulations[32](#fn32). This is particularly concerning in sectors such as healthcare
    and finance, where compliance with industry standards is non-negotiable. Healthcare
    organizations must demonstrate HIPAA compliance throughout their technology stack,
    while organizations handling EU citizens’ data must meet GDPR’s requirements for
    technical and organizational measures, including supply chain integrity.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 法律和监管风险进一步加剧了问题。那些无意中将假冒组件集成到其机器学习系统中的组织可能面临严重的法律后果，包括违反安全、隐私或网络安全法规的处罚[32](#fn32)。这在医疗保健和金融等必须遵守行业标准的领域尤其令人担忧。医疗保健组织必须在其技术堆栈中证明符合HIPAA标准，而处理欧盟公民数据的组织必须满足GDPR对技术和组织措施的要求，包括供应链完整性。
- en: Economic pressures often incentivize sourcing from lower-cost suppliers without
    rigorous verification, increasing the likelihood of counterfeit parts entering
    production systems. Detection is especially challenging, as counterfeit components
    are designed to mimic legitimate ones. Identifying them may require specialized
    equipment or forensic analysis, making prevention far more practical than remediation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 经济压力常常激励从低成本供应商处采购，而不进行严格的验证，增加了假冒部件进入生产系统的可能性。检测特别具有挑战性，因为假冒组件被设计成模仿合法组件。识别它们可能需要专用设备或法医分析，使得预防比补救更为实际。
- en: The stakes are particularly high in machine learning applications that require
    high reliability and low latency, such as real-time decision-making in autonomous
    vehicles, industrial automation, or important healthcare diagnostics. Hardware
    failure in these contexts can lead not only to system downtime but also to significant
    safety risks. Consequently, as machine learning continues to expand into safety-important
    and high-value applications, counterfeit hardware presents a growing risk that
    must be recognized and addressed. Organizations must treat hardware trustworthiness
    as a core design requirement, on par with algorithmic accuracy and data security,
    to ensure that ML systems can operate reliably and securely in the real world.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要高可靠性和低延迟的机器学习应用中，如自动驾驶汽车的实时决策、工业自动化或重要的医疗诊断，风险尤其高。在这些环境中，硬件故障不仅会导致系统停机，还会带来重大的安全风险。因此，随着机器学习继续扩展到安全重要和高价值的应用，伪造硬件呈现出的风险越来越大，必须得到认识和解决。组织必须将硬件可靠性视为核心设计要求，与算法准确性和数据安全同等重要，以确保ML系统可以在现实世界中可靠和安全地运行。
- en: Supply Chain Risks
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应链风险
- en: Counterfeit hardware exemplifies a broader systemic challenge. While counterfeit
    hardware presents a serious challenge, it is only one part of the larger problem
    of securing the global hardware supply chain. Machine learning systems are built
    from components that pass through complex supply networks involving design, fabrication,
    assembly, distribution, and integration. Each of these stages presents opportunities
    for tampering, substitution, or counterfeiting—often without the knowledge of
    those deploying the final system.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 伪造硬件体现了更广泛的系统性挑战。虽然伪造硬件提出了严重的挑战，但它只是确保全球硬件供应链安全的大问题的一部分。机器学习系统由通过复杂供应链的组件构建，涉及设计、制造、组装、分销和集成。每个阶段都存在篡改、替换或伪造的机会——通常在部署最终系统的那些人不知情的情况下。
- en: Malicious actors can exploit these vulnerabilities in various ways. A contracted
    manufacturer might unknowingly receive recycled electronic waste that has been
    relabeled as new components. A distributor might deliberately mix cloned parts
    into otherwise legitimate shipments. Insiders at manufacturing facilities might
    embed hardware Trojans that are nearly impossible to detect once the system is
    deployed. Advanced counterfeits can be particularly deceptive, with refurbished
    or repackaged components designed to pass visual inspection while concealing inferior
    or malicious internals.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意行为者可以以各种方式利用这些漏洞。一个签约制造商可能无意中收到被重新标记为新组件的回收电子废物。一个分销商可能故意将克隆部件混入其他合法的货物中。制造设施的内幕人士可能嵌入硬件特洛伊木马，一旦系统部署，就几乎无法检测到。高级仿制品尤其具有欺骗性，翻新或重新包装的组件旨在通过视觉检查，同时隐藏劣质或恶意内部组件。
- en: Identifying such compromises typically requires sophisticated analysis, including
    micrography, X-ray screening, and functional testing. However, these methods are
    costly and impractical for large-scale procurement. As a result, many organizations
    deploy systems without fully verifying the authenticity and security of every
    component.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 识别此类妥协通常需要复杂分析，包括显微摄影、X射线筛选和功能测试。然而，这些方法成本高昂，对于大规模采购来说不切实际。因此，许多组织在未完全验证每个组件的真实性和安全性之前就部署了系统。
- en: The risks extend beyond individual devices. Machine learning systems often rely
    on heterogeneous hardware platforms, integrating CPUs, GPUs, memory, and specialized
    accelerators sourced from a global supply base. Any compromise in one part of
    this chain can undermine the security of the entire system. These risks are further
    amplified when systems operate in shared or multi-tenant environments, such as
    cloud data centers or federated edge networks, where hardware-level isolation
    is important to preventing cross-tenant attacks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 风险不仅限于单个设备。机器学习系统通常依赖于异构硬件平台，整合来自全球供应链的CPU、GPU、内存和专用加速器。这个链条中任何一部分的妥协都可能破坏整个系统的安全性。当系统在共享或多租户环境中运行时，这些风险进一步放大，例如云数据中心或联邦边缘网络，在这些环境中，硬件级别的隔离对于防止跨租户攻击至关重要。
- en: The 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted
    spy chips into Supermicro server motherboards brought these risks to mainstream
    attention. While the claims remain disputed, the story underscored the industry’s
    limited visibility into its own hardware supply chains. Companies often rely on
    complex, opaque manufacturing and distribution networks, leaving them vulnerable
    to hidden compromises. Over-reliance on single manufacturers or regions, including
    the semiconductor industry’s reliance on TSMC, further concentrates this risk.
    This recognition has driven policy responses like the U.S. [CHIPS and Science
    Act](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/),
    which aims to bring semiconductor production onshore and strengthen supply chain
    resilience.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，《彭博商业周刊》报道称中国国家行为者将间谍芯片植入Supermicro服务器主板，将这些风险带入了公众视野。虽然这些说法仍然存在争议，但这个故事突显了行业对其自身硬件供应链的有限可见性。公司通常依赖复杂、不透明的制造和分销网络，使他们容易受到隐藏妥协的影响。过度依赖单一制造商或地区，包括半导体行业对台积电的依赖，进一步集中了这种风险。这种认识推动了政策反应，如美国的[芯片和科学法案](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/)，旨在将半导体生产转移到国内，并加强供应链的弹性。
- en: Securing machine learning systems requires moving beyond trust-by-default models
    toward zero-trust supply chain practices. This includes screening suppliers, validating
    component provenance, implementing tamper-evident protections, and continuously
    monitoring system behavior for signs of compromise. Building fault-tolerant architectures
    that detect and contain failures provides an additional layer of defense.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 保护机器学习系统需要超越默认信任模型，转向零信任供应链实践。这包括审查供应商、验证组件来源、实施篡改检测保护，以及持续监控系统行为以发现妥协迹象。构建能够检测和包含故障的容错架构提供了额外的防御层。
- en: Ultimately, supply chain risks must be treated as a first-class concern in ML
    system design. Trust in the computational models and data pipelines that power
    machine learning depends corely on the trustworthiness of the hardware on which
    they run. Without securing the hardware foundation, even the most sophisticated
    models remain vulnerable to compromise.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，供应链风险必须在机器学习系统设计中被视为首要关注的问题。对驱动机器学习的计算模型和数据管道的信任，核心依赖于它们运行的硬件的可靠性。如果不确保硬件基础的安全，即使是最复杂的模型也仍然容易受到妥协的威胁。
- en: 'Case Study: Supermicro Controversy'
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：Supermicro争议
- en: The abstract nature of supply chain risks became concrete in a high-profile
    controversy that captured industry attention. In 2018, Bloomberg Businessweek
    published a widely discussed report alleging that Chinese state-sponsored actors
    had secretly implanted tiny surveillance chips on server motherboards manufactured
    by Supermicro ([Robertson and Riley 2018](ch058.xhtml#ref-TheBigHa77)). These
    compromised servers were reportedly deployed by more than 30 major companies,
    including Apple and Amazon. The chips, described as no larger than a grain of
    rice, were said to provide attackers with backdoor access to sensitive data and
    systems.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 供应链风险的抽象性质在一个备受瞩目的争议中变得具体，这个争议吸引了行业的关注。2018年，《彭博商业周刊》发表了一份广泛讨论的报告，声称中国支持的国家行为者秘密在Supermicro制造的机架式服务器主板上植入微型监控芯片([Robertson
    and Riley 2018](ch058.xhtml#ref-TheBigHa77))。据报道，这些被篡改的服务器被超过30家主要公司部署，包括苹果和亚马逊。这些芯片据称大小不超过一粒米，可以为攻击者提供对敏感数据和系统的后门访问。
- en: The allegations sparked immediate concern across the technology industry, raising
    questions about the security of global supply chains and the potential for state-level
    hardware manipulation. However, the companies named in the report publicly denied
    the claims. Apple, Amazon, and Supermicro stated that they had found no evidence
    of the alleged implants after conducting thorough internal investigations. Industry
    experts and government agencies also expressed skepticism, noting the lack of
    verifiable technical evidence presented in the report.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指控在技术行业引发了立即的关注，引发了关于全球供应链安全和国家层面硬件操纵潜力的质疑。然而，报告中提到的公司公开否认了这些指控。苹果、亚马逊和超微公司表示，在进行了彻底的内部调查后，他们没有发现任何所谓的植入物。行业专家和政府机构也表达了怀疑，指出报告中缺乏可验证的技术证据。
- en: Despite these denials, the story had a lasting impact on how organizations and
    policymakers view hardware supply chain security. Whether or not the specific
    claims were accurate, the report highlighted the real and growing concern that
    hardware supply chains are difficult to fully audit and secure. It underscored
    how geopolitical tensions, manufacturing outsourcing, and the complexity of modern
    hardware ecosystems make it increasingly challenging to guarantee the integrity
    of hardware components.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些否认，但这个故事对组织和政策制定者如何看待硬件供应链安全产生了持久的影响。无论具体指控是否准确，报告都突出了这样一个现实和日益增长的关注：硬件供应链难以进行全面审计和保障。它强调了地缘政治紧张、制造业外包和现代硬件生态系统的复杂性如何使得保证硬件组件的完整性越来越具有挑战性。
- en: 'The Supermicro case illustrates a broader truth: once a product enters a complex
    global supply chain, it becomes difficult to ensure that every component is free
    from tampering or unauthorized modification. This risk is particularly acute for
    machine learning systems, which depend on a wide range of hardware accelerators,
    memory modules, and processing units sourced from multiple vendors across the
    globe.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 超微案例说明了更普遍的真理：一旦产品进入复杂的全球供应链，就很难确保每个组件都没有被篡改或未经授权修改。这种风险对于依赖来自全球多个供应商的广泛硬件加速器、内存模块和处理器单元的机器学习系统尤其严重。
- en: In response to these risks, both industry and government stakeholders have begun
    to invest in supply chain security initiatives. The U.S. government’s CHIPS and
    Science Act is one such effort, aiming to bring semiconductor manufacturing back
    onshore to improve transparency and reduce dependency on foreign suppliers. While
    these efforts are valuable, they do not fully eliminate supply chain risks. They
    must be complemented by technical safeguards, such as component validation, runtime
    monitoring, and fault-tolerant system design.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些风险，行业和政府利益相关者已经开始投资供应链安全计划。美国政府的芯片和科学法案就是此类努力之一，旨在将半导体制造回归本土，以提高透明度并减少对外国供应商的依赖。尽管这些努力很有价值，但它们并不能完全消除供应链风险。它们必须通过技术保障措施来补充，例如组件验证、运行时监控和容错系统设计。
- en: The Supermicro controversy serves as a cautionary tale for the machine learning
    community. It demonstrates that hardware security cannot be taken for granted,
    even when working with reputable suppliers. Ensuring the integrity of ML systems
    requires rigorous attention to the entire hardware lifecycle—from design and fabrication
    to deployment and maintenance. This case reinforces the need for organizations
    to adopt comprehensive supply chain security practices as a foundational element
    of trustworthy ML system design.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 超微争议为机器学习社区提供了一个警示故事。它表明，即使在与信誉良好的供应商合作时，硬件安全也不能被理所当然地认为。确保机器学习系统的完整性需要对整个硬件生命周期进行严格的关注——从设计、制造到部署和维护。这一案例强调了组织采用全面的供应链安全实践作为可信机器学习系统设计基础要素的必要性。
- en: When ML Systems Become Attack Tools
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当机器学习系统成为攻击工具
- en: 'The threats examined thus far—model theft, data poisoning, adversarial attacks,
    hardware vulnerabilities—represent attacks targeting machine learning systems.
    However, a complete threat model must also account for the inverse: machine learning
    as an attack amplifier. The same capabilities that make ML powerful for beneficial
    applications also enhance adversarial operations, transforming machine learning
    from passive target to active weapon.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止所考察的威胁——模型盗窃、数据中毒、对抗性攻击、硬件漏洞——代表了针对机器学习系统的攻击。然而，完整的威胁模型还必须考虑到相反的情况：机器学习作为攻击放大器。使机器学习在有益应用中强大的相同能力也增强了对抗性操作，将机器学习从被动目标转变为主动武器。
- en: While machine learning systems are often treated as assets to protect, they
    may also serve as tools for launching attacks. In adversarial settings, the same
    models used to enhance productivity, automate perception, or assist decision-making
    can be repurposed to execute or amplify offensive operations. This dual-use characteristic
    of machine learning, its capacity to secure systems as well as to subvert them,
    marks a core shift in how ML must be considered within system-level threat models.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习系统通常被视为需要保护的资产，但它们也可能成为发起攻击的工具。在对抗性环境中，用于提高生产力、自动化感知或辅助决策的相同模型可以被重新用于执行或放大攻击操作。机器学习的这种双重用途特性，即它既能保护系统也能颠覆系统，标志着在系统级威胁模型中考虑ML的核心转变。
- en: An offensive use of machine learning refers to any scenario in which a machine
    learning model is employed to facilitate the compromise of another system. In
    such cases, the model itself is not the object under attack, but the mechanism
    through which an adversary advances their objectives. These applications may involve
    reconnaissance, inference, subversion, impersonation, or the automation of exploit
    strategies that would otherwise require manual execution.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的攻击性应用是指任何使用机器学习模型来协助破坏另一个系统的场景。在这种情况下，模型本身不是攻击的目标，而是攻击者推进其目标的机制。这些应用可能涉及侦察、推理、颠覆、伪装或自动化那些原本需要手动执行的利用策略。
- en: Importantly, such offensive applications are not speculative. Attackers are
    already integrating machine learning into their toolchains across a wide range
    of activities, from spam filtering evasion to model-driven malware generation.
    What distinguishes these scenarios is the deliberate use of learning-based systems
    to extract, manipulate, or generate information in ways that undermine the confidentiality,
    integrity, or availability of targeted components.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这种攻击性应用并非纯粹推测。攻击者已经将机器学习集成到他们的工具链中，涉及广泛的活动中，从垃圾邮件过滤规避到基于模型的恶意软件生成。这些场景的区别在于，攻击者故意使用基于学习的系统以提取、操纵或生成信息，这些信息会破坏目标组件的机密性、完整性和可用性。
- en: To clarify the diversity and structure of these applications, [Table 15.6](ch021.xhtml#tbl-offensive-ml-use-cases)
    summarizes several representative use cases. For each, the table identifies the
    type of machine learning model typically employed, the underlying system vulnerability
    it exploits, and the primary advantage conferred by the use of machine learning.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这些应用的多样性和结构，[表15.6](ch021.xhtml#tbl-offensive-ml-use-cases)总结了几个代表性的用例。对于每个用例，表格确定了通常使用的机器学习模型类型、它所利用的底层系统漏洞以及使用机器学习带来的主要优势。
- en: These documented cases illustrate how machine learning models can serve as amplifiers
    of adversarial capability. For example, language models allow more convincing
    and adaptable phishing attacks, while clustering and classification algorithms
    facilitate reconnaissance by learning system-level behavioral patterns. The generative
    AI capabilities of large language models particularly amplify these offensive
    applications. Similarly, adversarial example generators and inference models systematically
    uncover weaknesses in decision boundaries or data privacy protections, often requiring
    only limited external access to deployed systems. In hardware contexts, as discussed
    in the next section, deep neural networks trained on side-channel data can automate
    the extraction of cryptographic secrets from physical measurements—transforming
    an expert-driven process into a learnable pattern recognition task. The deep learning
    foundations from [Chapter 3](ch009.xhtml#sec-dl-primer)—convolutional neural networks
    for spatial pattern recognition, recurrent architectures for temporal dependencies,
    and gradient-based optimization—enable attackers to apply these techniques across
    various hardware platforms discussed in [Chapter 11](ch017.xhtml#sec-ai-acceleration),
    from GPUs and TPUs in cloud environments to edge accelerators with constrained
    resources.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些记录的案例说明了机器学习模型如何作为对抗能力的放大器。例如，语言模型允许更令人信服和适应性强的钓鱼攻击，而聚类和分类算法通过学习系统级行为模式来促进侦察。大型语言模型的生成人工智能能力尤其放大了这些攻击应用。同样，对抗性示例生成器和推理模型系统地揭示了决策边界或数据隐私保护中的弱点，通常只需要有限的对外部部署系统的访问。在硬件环境中，如下一节所述，在侧信道数据上训练的深度神经网络可以自动化从物理测量中提取加密秘密——将专家驱动的过程转化为可学习的模式识别任务。第3章（ch009.xhtml#sec-dl-primer）中的深度学习基础——用于空间模式识别的卷积神经网络、用于时间依赖性的循环架构和基于梯度的优化——使攻击者能够将这些技术应用于第11章（ch017.xhtml#sec-ai-acceleration）中讨论的各种硬件平台，从云环境中的GPU和TPU到资源受限的边缘加速器。
- en: 'Table 15.6: **Offensive ML Use Cases**: This table categorizes how machine
    learning amplifies cyberattacks by enabling automated content generation, exploiting
    system vulnerabilities, and increasing attack sophistication; it details the typical
    ML model, targeted weakness, and resulting advantage for each offensive application.
    Understanding these use cases is important for developing effective defenses against
    increasingly intelligent threats.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.6：**攻击性机器学习用例**：此表分类了机器学习如何通过实现自动化内容生成、利用系统漏洞和增加攻击复杂性来放大网络攻击；它详细说明了每个攻击应用的典型机器学习模型、目标弱点和由此产生的优势。理解这些用例对于开发有效防御日益智能的威胁至关重要。
- en: '| **Offensive Use Case** | **ML Model Type** | **Targeted System Vulnerability**
    | **Advantage of ML** |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| **攻击性用例** | **机器学习模型类型** | **目标系统漏洞** | **机器学习的优势** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Phishing and Social Engineering** | Large Language Models (LLMs) | Human
    perception and communication systems | Personalized, context-aware message crafting
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| **钓鱼和社会工程** | 大型语言模型（LLMs） | 人类感知和通信系统 | 定制化、情境感知的消息构建 |'
- en: '| **Reconnaissance and Fingerprinting** | Supervised classifiers, clustering
    models | System configuration, network behavior | Scalable, automated profiling
    of system behavior |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| **侦察和指纹识别** | 监督分类器，聚类模型 | 系统配置，网络行为 | 可扩展的、自动化的系统行为配置文件 |'
- en: '| **Exploit Generation** | Code generation models, fine-tuned transformers
    | Software bugs, insecure code patterns | Automated discovery of candidate exploits
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **利用生成** | 代码生成模型，微调的转换器 | 软件漏洞，不安全的代码模式 | 自动发现候选利用方法 |'
- en: '| **Data Extraction (Inference Attacks)** | Classification models, inversion
    models | Privacy leakage through model outputs | Inference with limited or black-box
    access |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **数据提取（推理攻击）** | 分类模型，逆模型 | 通过模型输出泄露隐私 | 有限的或黑盒访问下的推理 |'
- en: '| **Evasion of Detection Systems** | Adversarial input generators | Detection
    boundaries in deployed ML systems | Crafting minimally perturbed inputs to evade
    filters |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **逃避检测系统** | 对抗性输入生成器 | 部署的机器学习系统中的检测边界 | 构建最小扰动的输入以逃避过滤器 |'
- en: '| **Hardware-Level Attacks** | Deep learning models | Physical side-channels
    (e.g., power, timing, EM) | Learning leakage patterns directly from raw signals
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **硬件级攻击** | 深度学习模型 | 物理侧信道（例如，功率、时序、电磁） | 直接从原始信号中学习泄露模式 |'
- en: 'Although these applications differ in technical implementation, they share
    a common foundation: the adversary replaces a static exploit with a learned model
    capable of approximating or adapting to the target’s vulnerable behavior. This
    shift increases flexibility, reduces manual overhead, and improves robustness
    in the face of evolving or partially obscured defenses.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些应用在技术实现上有所不同，但它们有一个共同的基础：对手用能够近似或适应目标脆弱行为的学到的模型替换了静态的漏洞利用。这种转变增加了灵活性，减少了人工开销，并提高了面对不断演变或部分隐蔽的防御时的鲁棒性。
- en: What makes this class of threats particularly significant is their favorable
    scaling behavior. Just as accuracy in computer vision or language modeling improves
    with additional data, larger architectures, and greater compute resources, so
    too does the performance of attack-oriented machine learning models. A model trained
    on larger corpora of phishing attempts or power traces, for instance, may generalize
    more effectively, evade more detectors, or require fewer inputs to succeed. The
    same ecosystem that drives innovation in beneficial AI, including public datasets,
    open-source tooling, and scalable infrastructure, also lowers the barrier to developing
    effective offensive models.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使这类威胁特别显著的是它们有利的扩展行为。正如计算机视觉或语言建模的准确性随着额外数据、更大架构和更多计算资源的增加而提高一样，以攻击为导向的机器学习模型的性能也是如此。例如，在更大规模的钓鱼尝试或电源迹线数据集上训练的模型可能具有更有效的泛化能力，能够规避更多检测器，或需要更少的输入才能成功。推动有益人工智能创新的相同生态系统，包括公共数据集、开源工具和可扩展基础设施，也降低了开发有效攻击模型的技术门槛。
- en: This dynamic creates an asymmetry between attacker and defender. While defensive
    measures are bounded by deployment constraints, latency budgets, and regulatory
    requirements, attackers can scale training pipelines with minimal marginal cost.
    The widespread availability of pretrained models and public ML platforms further
    reduces the expertise required to develop high-impact attacks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这种动态在攻击者和防御者之间创造了一种不对称性。尽管防御措施受到部署限制、延迟预算和监管要求的约束，但攻击者可以以最低的边际成本扩展训练管道。预训练模型和公共机器学习平台的广泛可用性进一步降低了开发高影响力攻击所需的技能水平。
- en: Examining these offensive capabilities serves a crucial defensive purpose. Security
    professionals have long recognized that effective defense requires understanding
    attack methodologies—this principle underlies penetration testing[33](#fn33),
    red team exercises[34](#fn34), and threat modeling throughout the cybersecurity
    industry.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这些攻击能力具有至关重要的防御作用。安全专业人士长期以来一直认识到，有效的防御需要理解攻击方法——这一原则是渗透测试[33](#fn33)、红队练习[34](#fn34)以及整个网络安全行业中的威胁建模的基础。
- en: In the machine learning domain, this understanding becomes essential because
    ML amplifies both defensive and offensive capabilities. The same computational
    advantages that make ML powerful for legitimate applications—pattern recognition,
    automation, and scalability—also enhance adversarial capabilities. By examining
    how machine learning can be weaponized, security professionals can anticipate
    attack vectors, design more robust defenses, and develop detection mechanisms.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，这种理解变得至关重要，因为机器学习放大了防御和攻击能力。使机器学习在合法应用中强大的相同计算优势——模式识别、自动化和可扩展性——也增强了对抗能力。通过研究机器学习如何被武器化，安全专业人士可以预测攻击向量，设计更健壮的防御措施，并开发检测机制。
- en: As a result, any comprehensive treatment of machine learning system security
    must consider not only the vulnerabilities of ML systems themselves but also the
    ways in which machine learning can be used to compromise other components—whether
    software, data, or hardware. Understanding the offensive potential of machine-learned
    systems is essential for designing resilient, trustworthy, and forward-looking
    defenses.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何关于机器学习系统安全的全面处理都必须考虑不仅机器学习系统的漏洞，还要考虑机器学习被用来损害其他组件的方式——无论是软件、数据还是硬件。理解机器学习系统的攻击潜力对于设计有弹性、值得信赖和前瞻性的防御至关重要。
- en: 'Case Study: Deep Learning for SCA'
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：深度学习用于SCA
- en: To illustrate these offensive capabilities concretely, we examine a specific
    case where machine learning transforms traditional attack methodologies. One of
    the most well-known and reproducible demonstrations of deep-learning-assisted
    SCA is the SCAAML framework (Side-Channel Attacks Assisted with Machine Learning)
    ([Bursztein et al. 2024a](ch058.xhtml#ref-scaaml_2019)). Developed by researchers
    at Google, SCAAML provides a practical implementation of the attack pipeline described
    above.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明这些攻击能力，我们考察了一个具体案例，其中机器学习将传统的攻击方法进行了转换。深度学习辅助的侧信道攻击（SCA）最著名且可重复的演示之一是SCAAML框架（由机器学习辅助的侧信道攻击）（[Bursztein等人2024a](ch058.xhtml#ref-scaaml_2019)）。由谷歌的研究人员开发，SCAAML提供了上述攻击管道的实用实现。
- en: '![](../media/file239.svg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file239.svg)'
- en: 'Figure 15.11: **Power Traces**: Cryptographic computations reveal subtle, data-dependent
    variations in power consumption that reflect internal states during specific operations.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11：**功率迹线**：密码计算揭示了功耗上的微妙、数据相关的变化，这些变化反映了特定操作期间的内部状态。
- en: As shown in [Figure 15.11](ch021.xhtml#fig-side-channel-curves), cryptographic
    computations exhibit data-dependent variations in their power consumption. These
    variations, while subtle, are measurable and reflect the internal state of the
    algorithm at specific points in time.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图15.11](ch021.xhtml#fig-side-channel-curves)所示，密码计算在功耗上表现出数据相关的变化。这些变化虽然微妙，但可测量，并反映了算法在特定时间点的内部状态。
- en: In traditional side-channel attacks, experts rely on statistical techniques
    to extract these differences. However, a neural network can learn to associate
    the shape of these signals with the specific data values being processed, effectively
    learning to decode the signal in a manner that mimics expert-crafted models, yet
    with enhanced flexibility and generalization. The model is trained on labeled
    examples of power traces and their corresponding intermediate values (e.g., output
    of an S-box operation). Over time, it learns to associate patterns in the trace,
    similar to those depicted in [Figure 15.11](ch021.xhtml#fig-side-channel-curves),
    with secret-dependent computational behavior. This transforms the key recovery
    task into a classification problem, where the goal is to infer the correct key
    byte based on trace shape alone.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的侧信道攻击中，专家们依赖于统计技术来提取这些差异。然而，神经网络可以学会将这些信号的形状与正在处理的具体数据值相关联，有效地学习以模仿专家构建的模型的方式来解码信号，同时具有增强的灵活性和泛化能力。该模型在标记的功率迹线及其相应的中间值（例如，S-box操作的输出）的示例上进行训练。随着时间的推移，它学会了将迹线中的模式与秘密相关的计算行为相关联，类似于[图15.11](ch021.xhtml#fig-side-channel-curves)中所示。这把密钥恢复任务转换成了一个分类问题，其目标是仅根据迹线形状推断正确的密钥字节。
- en: In their study, Bursztein et al. ([2024a](ch058.xhtml#ref-scaaml_2019)) trained
    a convolutional neural network to extract AES keys from power traces collected
    on an STM32F415 microcontroller running the open-source TinyAES implementation.
    The model was trained to predict intermediate values of the AES algorithm, such
    as the output of the S-box in the first round, directly from raw power traces.
    The trained model recovered the full 128-bit key using only a small number of
    traces per byte.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的研究中，Bursztein等人([2024a](ch058.xhtml#ref-scaaml_2019))训练了一个卷积神经网络，从运行开源TinyAES实现的STM32F415微控制器上收集的功率迹线中提取AES密钥。该模型被训练来直接从原始功率迹线预测AES算法的中间值，例如第一轮S-box的输出。训练好的模型仅使用每个字节少量迹线就恢复了完整的128位密钥。
- en: The traces were collected using a ChipWhisperer setup with a custom STM32F target
    board, shown in [Figure 15.12](ch021.xhtml#fig-stm32f-board). This board executes
    AES operations while allowing external equipment to monitor power consumption
    with high temporal precision. The experimental setup captures how even inexpensive,
    low-power embedded devices can leak information through side channels—information
    that modern machine learning models can learn to exploit.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这些迹线是使用带有定制STM32F目标板的ChipWhisperer设置收集的，该目标板在[图15.12](ch021.xhtml#fig-stm32f-board)中显示。该板执行AES操作，同时允许外部设备以高时间精度监控功耗。实验设置捕捉了即使是价格低廉、功耗低的嵌入式设备也能通过侧信道泄露信息的情况——这些信息是现代机器学习模型可以学会利用的。
- en: '![](../media/file240.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file240.png)'
- en: 'Figure 15.12: **STM32F415 Target Board**: Enables monitoring of power consumption
    during AES operations on the microcontroller, highlighting side-channel vulnerabilities
    that can be exploited by machine learning models. Source: Bursztein et al. ([2024a](ch058.xhtml#ref-scaaml_2019)).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：**STM32F415 目标板**：允许在微控制器上监控 AES 操作期间的功耗，突显了机器学习模型可以利用的侧信道漏洞。来源：Bursztein
    等人 ([2024a](ch058.xhtml#ref-scaaml_2019))。
- en: Subsequent work expanded on this approach by introducing long-range models capable
    of leveraging broader temporal dependencies in the traces, improving performance
    even under noise and desynchronization ([Bursztein et al. 2024b](ch058.xhtml#ref-bursztein2023generic)).
    These developments highlight the potential for machine learning models to serve
    as offensive cryptanalysis tools, especially in the analysis of secure hardware.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 后续工作通过引入能够利用更广泛时间依赖性的长程模型来扩展这一方法，即使在噪声和不同步的情况下也能提高性能（[Bursztein 等人 2024b](ch058.xhtml#ref-bursztein2023generic)）。这些发展突显了机器学习模型作为进攻性密码分析工具的潜力，尤其是在安全硬件分析方面。
- en: The implications extend beyond academic interest. As deep learning models continue
    to scale, their application to side-channel contexts is likely to lower the cost,
    skill threshold, and trace requirements of hardware-level attacks—posing a growing
    challenge for the secure deployment of embedded machine learning systems, cryptographic
    modules, and trusted execution environments.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 其影响超出了学术兴趣的范围。随着深度学习模型持续扩展，它们在侧信道环境中的应用可能会降低硬件级攻击的成本、技能门槛和跟踪要求——对嵌入式机器学习系统、加密模块和可信执行环境的可靠部署构成日益增长的挑战。
- en: Comprehensive Defense Architectures
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全面防御架构
- en: Having examined threats against ML systems and threats enabled by ML capabilities,
    we now turn to comprehensive defensive strategies. Designing secure and privacy-preserving
    machine learning systems requires more than identifying individual threats. It
    demands a layered defense strategy that integrates protections across multiple
    system levels to create comprehensive resilience.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察了针对机器学习系统的威胁以及由机器学习能力引发的威胁之后，我们现在转向全面的防御策略。设计安全且保护隐私的机器学习系统不仅需要识别个别威胁，还需要一种分层防御策略，该策略将多个系统级别的保护整合在一起，以创建全面的弹性。
- en: 'This section progresses systematically through four layers of defense: Data
    Layer protections including differential privacy and secure computation that safeguard
    sensitive information during training; Model Layer defenses such as adversarial
    training and secure deployment that protect the models themselves; Runtime Layer
    measures including input validation and output monitoring that secure inference
    operations; and Hardware Layer foundations such as trusted execution environments
    that provide the trust anchor for all other protections. We conclude with practical
    frameworks for selecting and implementing these defenses based on your deployment
    context.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本节系统地通过四个防御层进行阐述：数据层保护，包括差分隐私和安全的计算，在训练期间保护敏感信息；模型层防御，如对抗训练和安全部署，保护模型本身；运行时层措施，包括输入验证和输出监控，确保推理操作的安全；以及硬件层基础，如可信执行环境，为所有其他保护提供信任锚。我们以基于您的部署环境选择和实施这些防御的实际框架作为结论。
- en: The Layered Defense Principle
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层防御原则
- en: Layered defense (also known as defense-in-depth) represents a core security
    architecture principle where multiple independent defensive mechanisms work together
    to protect against diverse threat vectors. In machine learning systems, this approach
    becomes essential due to the unique attack surfaces introduced by data dependencies,
    model exposures, and inference patterns. Unlike traditional software systems that
    primarily face code-based vulnerabilities, ML systems are vulnerable to input
    manipulation, data leakage, model extraction, and runtime abuse, all amplified
    by tight coupling between data, model behavior, and infrastructure.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 分层防御（也称为深度防御）代表了一种核心安全架构原则，其中多个独立的防御机制协同工作，以抵御各种威胁向量。在机器学习系统中，这种方法变得至关重要，因为数据依赖性、模型暴露和推理模式引入了独特的攻击面。与主要面临基于代码的漏洞的传统软件系统不同，ML
    系统容易受到输入操纵、数据泄露、模型提取和运行时滥用的攻击，所有这些攻击都因数据、模型行为和基础设施之间的紧密耦合而加剧。
- en: 'The layered approach recognizes that no single defensive mechanism can address
    all possible threats. Instead, security emerges from the interaction of complementary
    protections: data-layer techniques like differential privacy and federated learning;
    model-layer defenses including robustness techniques and secure deployment; runtime-layer
    measures such as input validation and output monitoring; and hardware-layer solutions
    including trusted execution environments and secure boot. Each layer contributes
    to the system’s overall resilience while compensating for potential weaknesses
    in other layers.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 分层方法认识到没有单一防御机制可以应对所有可能的威胁。相反，安全来自于互补保护措施的相互作用：数据层技术如差分隐私和联邦学习；模型层防御包括鲁棒性技术和安全部署；运行时层措施如输入验证和输出监控；以及硬件层解决方案包括可信执行环境和安全启动。每一层都为系统的整体弹性做出贡献，同时补偿其他层中可能存在的潜在弱点。
- en: This section presents a structured framework implementing layered defense for
    ML systems, progressing from data-centric protections to infrastructure-level
    enforcement. The framework builds upon data protection practices in [Chapter 6](ch012.xhtml#sec-data-engineering)
    and connects forward to operational security measures detailed in [Chapter 13](ch019.xhtml#sec-ml-operations).
    By integrating safeguards across layers, organizations can build ML systems that
    not only perform reliably but also withstand adversarial pressure in production
    environments.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一个结构化的框架，用于实现机器学习系统的分层防御，从数据中心的保护到基础设施级别的执行。该框架建立在[第6章](ch012.xhtml#sec-data-engineering)中的数据保护实践之上，并向前连接到[第13章](ch019.xhtml#sec-ml-operations)中详细描述的操作安全措施。通过在各个层之间整合保障措施，组织可以构建既可靠又能在生产环境中承受对抗性压力的机器学习系统。
- en: The layered approach is visualized in [Figure 15.13](ch021.xhtml#fig-defense-stack),
    which shows how defensive mechanisms progress from foundational hardware-based
    security to runtime system protections, model-level controls, and privacy-preserving
    techniques at the data level. Each layer builds on the trust guarantees of the
    layer below it, forming an end-to-end strategy for deploying ML systems securely.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 分层方法在[图15.13](ch021.xhtml#fig-defense-stack)中得到了可视化，该图显示了防御机制如何从基于硬件的基础安全进步到运行时系统保护、模型级控制和数据层的隐私保护技术。每一层都建立在下一层的信任保证之上，形成一个端到端策略，用于安全部署机器学习系统。
- en: '![](../media/file241.svg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file241.svg)'
- en: 'Figure 15.13: **Layered Defense Stack**: Machine learning systems require multi-faceted
    security strategies that progress from foundational hardware protections to data-centric
    privacy techniques, building trust across all layers. This architecture integrates
    safeguards at the data, model, runtime, and infrastructure levels to mitigate
    threats and ensure robust deployment in production environments.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13：**分层防御堆栈**：机器学习系统需要多方面的安全策略，从基础硬件保护到以数据为中心的隐私技术，构建跨所有层的信任。该架构在数据、模型、运行时和基础设施级别整合了保障措施，以减轻威胁并确保在生产环境中稳健部署。
- en: Privacy-Preserving Data Techniques
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私保护数据技术
- en: At the highest level of our defense stack, we begin with data privacy techniques.
    Protecting the privacy of individuals whose data fuels machine learning systems
    is a foundational requirement for trustworthy AI. Unlike traditional systems where
    data is often masked or anonymized before processing, ML workflows typically rely
    on access to raw, high-fidelity data to train effective models. This tension between
    utility and privacy has motivated a diverse set of techniques aimed at minimizing
    data exposure while preserving learning performance.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们防御堆栈的最高层，我们首先从数据隐私技术开始。保护为机器学习系统提供动力的个人的隐私是可信人工智能的基础性要求。与在处理前通常对数据进行掩盖或匿名化的传统系统不同，机器学习工作流程通常依赖于访问原始、高保真数据来训练有效的模型。这种效用与隐私之间的紧张关系促使一系列旨在最小化数据暴露同时保持学习性能的技术应运而生。
- en: Differential Privacy
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 差分隐私
- en: One of the most widely adopted frameworks for formalizing privacy guarantees
    is differential privacy (DP). DP provides a rigorous mathematical definition of
    privacy loss, ensuring that the inclusion or exclusion of a single individual’s
    data has a provably limited effect on the model’s output.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛采用的用于形式化隐私保证的框架之一是差分隐私（DP）。DP提供了一个严格的数学定义的隐私损失，确保单个个体的数据包含或排除对模型输出的影响是可证明有限的。
- en: 'To understand the need for differential privacy, consider this challenge: how
    can we quantify privacy loss when learning from data? Traditional privacy approaches
    focus on removing identifying information (names, addresses, social security numbers)
    or applying statistical disclosure controls. However, these methods fail against
    sophisticated adversaries who can re-identify individuals through auxiliary data,
    statistical correlation attacks, or inference from model outputs.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解差分隐私的需求，考虑这个挑战：我们在从数据中学习时如何量化隐私损失？传统的隐私方法侧重于删除识别信息（姓名、地址、社会保障号码）或应用统计披露控制。然而，这些方法在面对复杂的对手时失效，对手可以通过辅助数据、统计相关性攻击或从模型输出中进行推理来重新识别个人。
- en: Differential privacy takes a different approach by focusing on algorithmic behavior
    rather than data content. The key insight is that privacy protection should be
    measurable and should limit what can be learned about any individual, regardless
    of what external information an adversary possesses.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私通过关注算法行为而非数据内容采取了一种不同的方法。关键洞见是隐私保护应该是可衡量的，并且应该限制对任何个人的了解，无论对手拥有多少外部信息。
- en: 'To build intuition for this concept, imagine you want to find the average salary
    of a group of people, but no one wants to reveal their actual salary. With differential
    privacy, you could ask everyone to write their salary on a piece of paper, but
    before they hand it in, they add or subtract a random number from a known distribution.
    When you average all the papers, the random noise tends to cancel out, giving
    you a very close estimate of the true average. However, if you pull out any single
    piece of paper, you cannot know the person’s real salary because you do not know
    what random number they added. This is the core idea: learn aggregate patterns
    while making it impossible to be sure about any single individual.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这一概念建立直观理解，想象一下你想找到一组人的平均工资，但没有人愿意透露他们的实际工资。使用差分隐私，你可以要求每个人都把他们的工资写在一张纸上，但在他们交上之前，他们从已知分布中添加或减去一个随机数。当你平均所有纸张时，随机噪声往往会相互抵消，给你一个非常接近真实平均值的估计。然而，如果你抽出任何一张单独的纸，你无法知道这个人的真实工资，因为你不知道他们添加了什么随机数。这是核心思想：在学习总体模式的同时，使确定任何单个个体的信息变得不可能。
- en: Differential privacy formalizes this intuition through a comparison of algorithm
    behavior on similar datasets. Consider two adjacent datasets that differ only
    in the presence or absence of a single individual’s record. Differential privacy
    ensures that the probability distributions of algorithm outputs remain statistically
    similar regardless of whether that individual’s data is included. This protection
    is achieved through carefully calibrated noise that masks individual contributions
    while preserving the aggregate statistical patterns necessary for machine learning.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私通过比较相似数据集上的算法行为来形式化这种直观。考虑两个相邻的数据集，它们只在单个个体的记录是否存在上有所不同。差分隐私确保算法输出的概率分布无论该个体的数据是否包含，在统计上都是相似的。这种保护是通过精心校准的噪声实现的，它掩盖了个体贡献，同时保留了机器学习所需的总体统计模式。
- en: To make this intuition mathematically precise, differential privacy introduces
    a quantitative measure of privacy loss. The mathematical framework uses probability
    ratios to bound how much an algorithm’s behavior can change when a single individual’s
    data is added or removed. This approach allows us to prove privacy guarantees
    rather than simply assume them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这种直观在数学上精确，差分隐私引入了隐私损失的定量度量。数学框架使用概率比率来限制当添加或删除单个个体的数据时，算法行为可以改变的程度。这种方法使我们能够证明隐私保证，而不仅仅是假设它们。
- en: 'A randomized algorithm <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>
    is said to be <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>-differentially
    private if, for all adjacent datasets <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>
    and <semantics><mrow><mi>D</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">D''</annotation></semantics>
    differing in one record, and for all outputs <semantics><mrow><mi>S</mi><mo>⊆</mo><mtext
    mathvariant="normal">Range</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝒜</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S
    \subseteq \text{Range}(\mathcal{A})</annotation></semantics>, the following holds:
    <semantics><mrow><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>𝒜</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><mo>≤</mo><msup><mi>e</mi><mi>ϵ</mi></msup><mo>Pr</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>𝒜</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mi>′</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\Pr[\mathcal{A}(D)
    \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D'') \in S]</annotation></semantics>'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机算法 <semantics><mi>𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics>
    被称为 <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>-差分隐私的，如果对于所有相邻数据集
    <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>
    和 <semantics><mrow><mi>D</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">D'</annotation></semantics>（它们在一条记录上有所不同），以及对于所有输出
    <semantics><mrow><mi>S</mi><mo>⊆</mo><mtext mathvariant="normal">Range</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>𝒜</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">S \subseteq \text{Range}(\mathcal{A})</annotation></semantics>，以下条件成立：<semantics><mrow><mo>Pr</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>𝒜</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mo stretchy="true"
    form="postfix">]</mo></mrow><mo>≤</mo><msup><mi>e</mi><mi>ϵ</mi></msup><mo>Pr</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>𝒜</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>D</mi><mi>′</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>S</mi><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">\Pr[\mathcal{A}(D)
    \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D') \in S]</annotation></semantics>
- en: The parameter <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    quantifies the privacy budget, representing the maximum allowable privacy loss.
    Smaller values of <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    provide stronger privacy guarantees through increased noise injection, but may
    reduce model utility. Typical values include <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation
    encoding="application/x-tex">\epsilon = 0.1</annotation></semantics> for strong
    privacy protection, <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1.0</mn></mrow><annotation
    encoding="application/x-tex">\epsilon = 1.0</annotation></semantics> for moderate
    protection, and <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">\epsilon = 10</annotation></semantics> for weaker
    but utility-preserving guarantees. The multiplicative factor <semantics><msup><mi>e</mi><mi>ϵ</mi></msup><annotation
    encoding="application/x-tex">e^{\epsilon}</annotation></semantics> bounds the
    likelihood ratio between algorithm outputs on adjacent datasets, constraining
    how much an individual’s participation can influence any particular result.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    量化了隐私预算，表示最大允许的隐私损失。较小的 <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    值通过增加噪声注入提供更强的隐私保障，但可能会降低模型效用。典型值包括用于强隐私保护的 <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0.1</mn></mrow><annotation
    encoding="application/x-tex">\epsilon = 0.1</annotation></semantics>，用于适度保护的 <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1.0</mn></mrow><annotation
    encoding="application/x-tex">\epsilon = 1.0</annotation></semantics>，以及用于较弱但保持效用保证的
    <semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\epsilon
    = 10</annotation></semantics>。乘法因子 <semantics><msup><mi>e</mi><mi>ϵ</mi></msup><annotation
    encoding="application/x-tex">e^{\epsilon}</annotation></semantics> 限制了相邻数据集算法输出的似然比，约束了个人参与对任何特定结果的影响程度。
- en: This bound ensures that the algorithm’s behavior remains statistically indistinguishable
    regardless of whether any individual’s data is present, thereby limiting the information
    that can be inferred about that individual. In practice, DP is implemented by
    adding calibrated noise to model updates or query responses, using mechanisms
    such as the Laplace or Gaussian mechanism. Training techniques like differentially
    private stochastic gradient descent[35](#fn35) integrate calibrated noise into
    training computations, ensuring that individual data points cannot be distinguished
    from the model’s learned behavior.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 此界限确保算法的行为在统计上与任何个人数据是否存在无关，从而限制了可以推断出的关于该个人的信息。在实践中，差分隐私通过添加校准噪声到模型更新或查询响应中，使用拉普拉斯机制或高斯机制等机制来实现。像差分隐私随机梯度下降[35](#fn35)这样的训练技术将校准噪声整合到训练计算中，确保个人数据点无法与模型学习到的行为区分开来。
- en: While differential privacy offers strong theoretical assurances, it introduces
    a trade-off between privacy and utility[36](#fn36) that has measurable computational
    and accuracy costs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然差分隐私提供了强大的理论保证，但它引入了隐私和效用之间的权衡[36](#fn36)，这会产生可衡量的计算和精度成本。
- en: Practical DP deployment requires careful consideration of computational trade-offs,
    privacy budget management, and implementation challenges, as detailed in [Table 15.7](ch021.xhtml#tbl-privacy-technique-comparison).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上的差分隐私部署需要仔细考虑计算权衡、隐私预算管理和实施挑战，这些内容在[表15.7](ch021.xhtml#tbl-privacy-technique-comparison)中详细说明。
- en: Increasing the noise to reduce <semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>
    may degrade model accuracy, especially in low-data regimes or fine-grained classification
    tasks. Consequently, DP is often applied selectively—either during training on
    sensitive datasets or at inference when returning aggregate statistics—to balance
    privacy with performance goals ([Dwork and Roth 2013](ch058.xhtml#ref-dwork2014algorithmic)).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 增加噪声以减少<semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics>可能会降低模型精度，尤其是在数据量较少或细粒度分类任务中。因此，差分隐私通常是有选择性地应用的——要么在敏感数据集的训练期间，要么在推理时返回汇总统计时——以平衡隐私和性能目标([Dwork
    and Roth 2013](ch058.xhtml#ref-dwork2014algorithmic))。
- en: Federated Learning
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 联邦学习
- en: 'While differential privacy adds mathematical guarantees to data processing,
    federated learning (FL) offers a complementary approach that reduces privacy risks
    by restructuring the learning process itself. This technique directly addresses
    the privacy challenges of on-device learning explored in [Chapter 14](ch020.xhtml#sec-ondevice-learning),
    where models must adapt to local data patterns without exposing sensitive user
    information. Rather than aggregating raw data at a central location, FL distributes
    the training across a set of client devices, each holding local data ([McMahan
    et al. 2017d](ch058.xhtml#ref-mcmahan2017communicationefficient)). This distributed
    training paradigm, which builds on the adaptive deployment concepts from on-device
    learning, requires careful coordination of security measures across multiple participants
    and infrastructure providers. Clients compute model updates locally and share
    only parameter deltas with a central server for aggregation: <semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><mo>⋅</mo><msubsup><mi>θ</mi><mi>t</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta_{t+1} \leftarrow \sum_{k=1}^{K}
    \frac{n_k}{n} \cdot \theta_{t}^{(k)}</annotation></semantics>'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 当差分隐私为数据处理提供数学保证时，联邦学习（FL）提供了一种补充方法，通过重新构建学习过程本身来降低隐私风险。这种技术直接解决了第14章中探讨的设备上学习的隐私挑战，其中模型必须适应本地数据模式，同时不泄露敏感用户信息。FL不是在中央位置汇总原始数据，而是在一组客户端设备上分配训练任务，每个设备都持有本地数据（[McMahan等人2017d](ch058.xhtml#ref-mcmahan2017communicationefficient)）。这种分布式训练范式建立在设备上学习的自适应部署概念之上，需要跨多个参与者和基础设施提供商仔细协调安全措施。客户端在本地计算模型更新，并仅与中央服务器共享参数增量以进行汇总：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><mo>⋅</mo><msubsup><mi>θ</mi><mi>t</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta_{t+1} \leftarrow \sum_{k=1}^{K}
    \frac{n_k}{n} \cdot \theta_{t}^{(k)}</annotation></semantics>
- en: Here, <semantics><msubsup><mi>θ</mi><mi>t</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_{t}^{(k)}</annotation></semantics>
    represents the model update from client <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>,
    <semantics><msub><mi>n</mi><mi>k</mi></msub><annotation encoding="application/x-tex">n_k</annotation></semantics>
    the number of samples held by that client, and <semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics> the total number of samples
    across all clients. This weighted aggregation allows the global model to learn
    from distributed data without direct access to it. FL reduces the exposure of
    raw data, but still leaks information through gradients, motivating the use of
    DP, secure aggregation, and hardware-based protections in federated settings.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<semantics><msubsup><mi>θ</mi><mi>t</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_{t}^{(k)}</annotation></semantics>
    代表客户端<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>的模型更新，<semantics><msub><mi>n</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">n_k</annotation></semantics>是该客户端持有的样本数量，而<semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>是所有客户端的样本总数。这种加权聚合允许全局模型从分布式数据中学习，而无需直接访问它。FL减少了原始数据的暴露，但仍然通过梯度泄露信息，这促使在联邦环境中使用差分隐私（DP）、安全聚合和基于硬件的保护。
- en: '**Real-World Example: Google Gboard Federated Learning**'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**现实世界示例：谷歌Gboard联邦学习**'
- en: 'Google’s Gboard keyboard uses federated learning to improve next-word prediction
    across 1+ billion Android devices without collecting typing data. The system works
    as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的Gboard键盘使用联邦学习来改善超过10亿部Android设备的下一个单词预测，而不收集打字数据。系统的工作原理如下：
- en: 'Local Training: Each device trains a small update to the language model using
    the user’s recent typing (typically 100-1000 words)'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本地训练：每个设备使用用户的最近打字（通常是100-1000个单词）来训练语言模型的小型更新
- en: 'Secure Aggregation: Devices upload encrypted model updates (not raw text) to
    Google’s servers'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安全聚合：设备将加密的模型更新（而非原始文本）上传到谷歌的服务器
- en: 'Global Update: The server aggregates thousands of updates, computing an improved
    global model'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全球更新：服务器汇总数千个更新，计算一个改进的全局模型
- en: 'Distribution: The updated model is pushed back to devices in the next app update'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分发：更新的模型将在下一个应用更新中推送到设备
- en: '**Privacy Properties:** Individual typing data never leaves the device. Even
    Google’s servers cannot decrypt individual updates, seeing only the aggregated
    result. The system combines FL with differential privacy <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>ε</mi><mo>≈</mo><mn>6</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\varepsilon\approx
    6)</annotation></semantics> and secure aggregation protocols.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私属性**：个人打字数据永远不会离开设备。即使是谷歌的服务器也无法解密单个更新，只能看到汇总的结果。该系统结合了联邦学习与差分隐私 <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>ε</mi><mo>≈</mo><mn>6</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\varepsilon\approx
    6)</annotation></semantics> 和安全聚合协议。'
- en: '**Performance:** FL achieves 92% of the accuracy of centralized training while
    eliminating raw data collection. Communication efficiency optimizations (gradient
    compression, selective participation) reduce bandwidth to ~100 KB per device per
    day.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能**：联邦学习在消除原始数据收集的同时，实现了集中式训练92%的精度。通信效率优化（梯度压缩、选择性参与）将带宽降低到每天每台设备约100 KB。'
- en: '**Trade-offs:** FL requires 10-100x more communication rounds than centralized
    training and introduces 2-5% accuracy degradation. However, for privacy-sensitive
    applications, these costs are acceptable compared to the alternative of not training
    at all.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**权衡**：联邦学习（FL）比集中式训练需要10-100倍的通信轮次，并引入2-5%的精度下降。然而，对于对隐私敏感的应用，与完全不训练的替代方案相比，这些成本是可以接受的。'
- en: To address scenarios requiring computation on encrypted data, homomorphic encryption
    (HE)[37](#fn37) and secure multiparty computation (SMPC) allow models to perform
    inference or training over encrypted inputs. The computational overhead of homomorphic
    operations often requires the efficiency optimization techniques covered in [Chapter 9](ch015.xhtml#sec-efficient-ai)—including
    model compression (quantization reduces precision requirements for encrypted operations),
    architectural optimization (depthwise separable convolutions minimize encrypted
    multiplications), and hardware acceleration (specialized cryptographic accelerators)—to
    maintain practical performance.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决需要在加密数据上执行计算的场景，同态加密（HE）[37](#fn37) 和安全多方计算（SMPC）允许模型在加密输入上执行推理或训练。同态操作的计算开销通常需要涵盖在[第9章](ch015.xhtml#sec-efficient-ai)中的效率优化技术——包括模型压缩（量化降低加密操作的精度要求）、架构优化（深度可分离卷积最小化加密乘法）和硬件加速（专门的加密加速器）——以保持实际性能。
- en: 'In the case of HE, operations on ciphertexts correspond to operations on plaintexts,
    enabling encrypted inference: <semantics><mrow><mtext mathvariant="normal">Enc</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Enc</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{Enc}(f(x))
    = f(\text{Enc}(x))</annotation></semantics>'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在同态加密的情况下，对密文的操作对应于对明文的操作，从而实现加密推理： <semantics><mrow><mtext mathvariant="normal">Enc</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Enc</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{Enc}(f(x))
    = f(\text{Enc}(x))</annotation></semantics>
- en: This property supports privacy-preserving computation in untrusted environments,
    such as cloud inference over sensitive health or financial records. The computational
    cost of HE remains high, making it more suitable for fixed-function models and
    low-latency batch tasks. SMPC[38](#fn38), by contrast, distributes the computation
    across multiple parties such that no single party learns the complete input or
    output. This is particularly useful in joint training across institutions with
    strict data-use policies, such as hospitals or banks[39](#fn39).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这一特性支持在不受信任的环境中实现隐私保护计算，例如在敏感健康或财务记录上进行云推理。同态加密的计算成本仍然很高，使其更适合固定功能模型和低延迟批量任务。相比之下，SMPC[38](#fn38)将计算分布在多个参与者之间，使得没有任何一个参与者能够学习到完整的输入或输出。这在具有严格数据使用政策的机构之间进行联合训练时特别有用，例如医院或银行[39](#fn39)。
- en: Synthetic Data Generation
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合成数据生成
- en: 'Beyond cryptographic approaches like homomorphic encryption, a more pragmatic
    and increasingly popular alternative involves the use of synthetic data generation[40](#fn40).
    This approach offers an intuitive solution to privacy protection: if we can create
    artificial data that looks statistically similar to real data, we can train models
    without ever exposing sensitive information.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于同态加密等加密方法，一种更加实用且越来越受欢迎的替代方案是使用合成数据生成[40](#fn40)。这种方法为隐私保护提供了一个直观的解决方案：如果我们能够创建出在统计上与真实数据相似的人工数据，我们就可以在不暴露敏感信息的情况下训练模型。
- en: Synthetic data generation works by training a generative model (such as a GAN,
    VAE, or diffusion model) on the original sensitive dataset, then using this trained
    generator to produce new artificial samples. The key insight is that the generative
    model learns the underlying patterns and distributions in the data without memorizing
    specific individuals. When properly implemented, the synthetic data preserves
    statistical properties necessary for machine learning while removing personally
    identifiable information.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据生成的工作原理是，在原始敏感数据集上训练一个生成模型（例如GAN、VAE或扩散模型），然后使用这个训练好的生成器来产生新的人工样本。关键洞察是，生成模型学习数据中的底层模式和分布，而不需要记住具体的个体。当正确实施时，合成数据保留了机器学习所需的统计属性，同时去除了个人可识别信息。
- en: The generation typically follows three stages. First, distribution learning
    trains a generative model <semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation
    encoding="application/x-tex">G_\theta</annotation></semantics> on real data <semantics><mrow><msub><mi>D</mi><mtext
    mathvariant="normal">real</mtext></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">D_{\text{real}}
    = \{x_1, x_2,\ldots, x_n\}</annotation></semantics> to learn the data distribution
    <semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(x)</annotation></semantics>.
    Second, synthetic sampling generates new samples <semantics><mrow><msub><mi>D</mi><mtext
    mathvariant="normal">synthetic</mtext></msub><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">}</mo></mrow><annotation
    encoding="application/x-tex">D_{\text{synthetic}} = \{G_\theta(z_1), G_\theta(z_2),\ldots,
    G_\theta(z_m)\}</annotation></semantics> by sampling from random noise <semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∼</mo><mi>𝒩</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_i
    \sim \mathcal{N}(0,I)</annotation></semantics>. Third, validation verifies that
    <semantics><msub><mi>D</mi><mtext mathvariant="normal">synthetic</mtext></msub><annotation
    encoding="application/x-tex">D_{\text{synthetic}}</annotation></semantics> maintains
    statistical fidelity to <semantics><msub><mi>D</mi><mtext mathvariant="normal">real</mtext></msub><annotation
    encoding="application/x-tex">D_{\text{real}}</annotation></semantics> while avoiding
    memorization of specific records. By training generative models on real datasets
    and sampling new instances from the learned distribution, organizations can create
    datasets that approximate the statistical properties of the original data without
    retaining identifiable details ([Goncalves et al. 2020](ch058.xhtml#ref-goncalves2020generation)).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程通常分为三个阶段。首先，分布学习在真实数据上训练一个生成模型 <semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation
    encoding="application/x-tex">G_\theta</annotation></semantics>，以学习数据分布 <semantics><mrow><mi>p</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">p(x)</annotation></semantics>。其次，合成采样通过从随机噪声中采样生成新的样本
    <semantics><mrow><msub><mi>D</mi><mtext mathvariant="normal">synthetic</mtext></msub><mo>=</mo><mo
    stretchy="false" form="prefix">{</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">}</mo></mrow><annotation
    encoding="application/x-tex">D_{\text{synthetic}} = \{G_\theta(z_1), G_\theta(z_2),\ldots,
    G_\theta(z_m)\}</annotation></semantics>。第三，验证确保 <semantics><msub><mi>D</mi><mtext
    mathvariant="normal">synthetic</mtext></msub><annotation encoding="application/x-tex">D_{\text{synthetic}}</annotation></semantics>
    在避免记忆特定记录的同时，保持与 <semantics><msub><mi>D</mi><mtext mathvariant="normal">real</mtext></msub><annotation
    encoding="application/x-tex">D_{\text{real}}</annotation></semantics> 的统计一致性。通过在真实数据集上训练生成模型并从学习到的分布中采样新实例，组织可以创建近似原始数据统计特性的数据集，而不保留可识别的细节
    ([Goncalves et al. 2020](ch058.xhtml#ref-goncalves2020generation))).
- en: While appealing, synthetic data generation faces important limitations. Generative
    models can suffer from mode collapse, failing to capture rare but important patterns
    in the original data. More critically, sophisticated adversaries can potentially
    extract information about the original training data through generative model
    inversion attacks or membership inference. The privacy protection depends heavily
    on the generative model architecture, training procedure, and hyperparameter choices—making
    it difficult to provide formal privacy guarantees without additional mechanisms
    like differential privacy.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然合成数据生成很有吸引力，但面临着重要的限制。生成模型可能会出现模式坍塌，无法捕捉原始数据中罕见但重要的模式。更重要的是，复杂的对手可能通过生成模型反演攻击或成员推理来提取有关原始训练数据的信息。隐私保护在很大程度上取决于生成模型架构、训练过程和超参数选择——这使得在没有额外机制（如差分隐私）的情况下提供正式的隐私保证变得困难。
- en: Consider a practical example where a hospital wants to share patient data for
    ML research while protecting privacy. They train a generative adversarial network
    (GAN) on 10,000 real patient records containing demographics, lab results, and
    diagnoses. The GAN learns to generate synthetic patients with realistic combinations
    of features (e.g., diabetic patients typically have elevated glucose levels).
    The synthetic dataset of 50,000 artificial patients maintains clinical correlations
    necessary for training diagnostic models while containing no real patient information.
    However, the hospital also applies differential privacy during GAN training (ε
    = 1.0) to prevent the model from memorizing specific patients, trading a 5% reduction
    in statistical fidelity for formal privacy guarantees.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个实际例子，其中一家医院希望在保护隐私的同时共享患者数据用于机器学习研究。他们在包含人口统计信息、实验室结果和诊断的10,000份真实患者记录上训练了一个生成对抗网络（GAN）。GAN学习生成具有现实特征组合的合成患者（例如，糖尿病患者通常有升高的血糖水平）。包含50,000个合成患者的合成数据集保持了训练诊断模型所需的临床相关性，同时不包含任何真实患者信息。然而，医院在GAN训练期间也应用了差分隐私（ε
    = 1.0），以防止模型记住特定患者，以5%的统计精度降低为正式的隐私保证。
- en: Together, these techniques reflect a shift from isolating data as the sole path
    to privacy toward embedding privacy-preserving mechanisms into the learning process
    itself. Each method offers distinct guarantees and trade-offs depending on the
    application context, threat model, and regulatory constraints. Effective system
    design often combines multiple approaches, such as applying differential privacy
    within a federated learning setup, or employing homomorphic encryption for important
    inference stages, to build ML systems that are both useful and respectful of user
    privacy.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术共同反映了一种转变，即从将数据孤立作为实现隐私的唯一途径，转向将隐私保护机制嵌入到学习过程本身。每种方法都根据应用环境、威胁模型和监管约束提供独特的保证和权衡。有效的系统设计通常结合多种方法，例如在联邦学习设置中应用差分隐私，或者在重要的推理阶段使用同态加密，以构建既实用又尊重用户隐私的机器学习系统。
- en: Comparative Properties
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较属性
- en: Having examined individual techniques, it becomes clear that these privacy-preserving
    approaches differ not only in the guarantees they offer but also in their system-level
    implications. For practitioners, the choice of mechanism depends on factors such
    as computational constraints, deployment architecture, and regulatory requirements.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察了单个技术之后，变得明显的是，这些隐私保护方法不仅在提供的保证上有所不同，而且在系统层面的影响上也有所不同。对于从业者来说，机制的选择取决于诸如计算约束、部署架构和监管要求等因素。
- en: '[Table 15.7](ch021.xhtml#tbl-privacy-technique-comparison) summarizes the comparative
    properties of these methods, focusing on privacy strength, runtime overhead, maturity,
    and common use cases. Understanding these trade-offs is important for designing
    privacy-aware machine learning systems that operate under real-world constraints.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[表15.7](ch021.xhtml#tbl-privacy-technique-comparison)总结了这些方法的比较属性，重点关注隐私强度、运行时开销、成熟度和常见用例。理解这些权衡对于设计在现实世界约束下运行的隐私感知机器学习系统非常重要。'
- en: 'Table 15.7: **Privacy-Accuracy Trade-Offs**: Data privacy techniques impose
    varying computational costs and offer different levels of formal privacy guarantees,
    requiring practitioners to balance privacy strength with model utility and deployment
    constraints. The table summarizes key properties—privacy guarantees, computational
    overhead, maturity, typical use cases, and trade-offs—to guide informed decisions
    when designing privacy-aware machine learning systems.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.7：**隐私-准确度权衡**：数据隐私技术施加不同的计算成本，并提供不同级别的形式化隐私保证，要求从业者平衡隐私强度与模型效用和部署限制。该表总结了关键属性——隐私保证、计算开销、成熟度、典型用例和权衡，以指导在设计隐私感知机器学习系统时的明智决策。
- en: '| **Technique** | **Privacy Guarantee** | **Computational Overhead** | **Deployment
    Maturity** | **Typical Use Case** | **Trade-offs** |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **隐私保证** | **计算开销** | **部署成熟度** | **典型用例** | **权衡** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Differential Privacy** | Formal (ε-DP) | Moderate to High | Production
    | Training with sensitive or regulated data | Reduced accuracy; careful tuning
    of ε/noise required to balance utility and protection |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| **差分隐私** | 形式化（ε-DP） | 中等到高 | 生产环境 | 使用敏感或受监管数据进行训练 | 准确度降低；需要仔细调整ε/噪声以平衡效用和保护
    |'
- en: '| **Federated Learning** | Structural | Moderate | Production | Cross-device
    or cross-org collaborative learning | Gradient leakage risk; requires secure aggregation
    and orchestration infrastructure |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| **联邦学习** | 结构性 | 中等 | 生产环境 | 跨设备或跨组织协作学习 | 梯度泄露风险；需要安全的聚合和编排基础设施 |'
- en: '| **Homomorphic Encryption** | Strong (Encrypted) | High | Experimental | Inference
    in untrusted cloud environments | High latency and memory usage; suitable for
    limited-scope inference on fixed-function models |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| **同态加密** | 强（加密） | 高 | 实验性 | 在不受信任的云环境中进行推理 | 高延迟和内存使用；适用于固定功能模型上的有限范围推理
    |'
- en: '| **Secure MPC** | Strong (Distributed) | Very High | Experimental | Joint
    training across mutually untrusted parties | Expensive communication; challenging
    to scale to many participants or deep models |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| **安全多方计算** | 强（分布式） | 非常高 | 实验性 | 在相互不信任的各方之间进行联合训练 | 通信成本高昂；难以扩展到多个参与者或深度模型
    |'
- en: '| **Synthetic Data** | Weak (if standalone) | Low to Moderate | Emerging |
    Data sharing, benchmarking without direct access to raw data | May leak sensitive
    patterns if training process is not differentially private or audited for fidelity
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| **合成数据** | 弱（如果独立使用） | 低到中等 | 新兴 | 数据共享，基准测试而不直接访问原始数据 | 如果训练过程不是差分隐私或未经过审计以确保一致性，可能会泄露敏感模式
    |'
- en: 'Case Study: GPT-3 Data Extraction Attack'
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：GPT-3数据提取攻击
- en: In 2020, researchers conducted a groundbreaking study demonstrating that large
    language models could leak sensitive training data through carefully crafted prompts
    ([Carlini et al. 2021](ch058.xhtml#ref-carlini2021extracting)). The research team
    systematically queried OpenAI’s GPT-3 model to extract verbatim content from its
    training dataset, revealing privacy vulnerabilities in large-scale language models.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年，研究人员进行了一项开创性的研究，证明大型语言模型可以通过精心设计的提示泄露敏感的训练数据（[Carlini等人2021](ch058.xhtml#ref-carlini2021extracting)）。研究团队系统地查询了OpenAI的GPT-3模型，从中提取其训练数据中的原文内容，揭示了大型语言模型中的隐私漏洞。
- en: The attack proved remarkably successful at extracting sensitive information
    directly from the model’s outputs. By repeatedly querying the model with prompts
    like “My name is” followed by attempts to continue famous quotes or repeated phrases,
    researchers successfully extracted personal information including email addresses
    and phone numbers from the training data, verbatim passages from copyrighted books,
    private data that should have been filtered during training, and personally identifiable
    information from millions of individuals.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 该攻击在直接从模型输出中提取敏感信息方面证明非常成功。通过反复使用如“我的名字是”这样的提示，并尝试继续著名引语或重复短语，研究人员成功地从训练数据中提取了包括电子邮件地址和电话号码在内的个人信息，版权书籍的原文段落，训练过程中应被过滤的私人数据，以及数百万个人的可识别个人信息。
- en: The technical approach exploited GPT-3’s memorization of rare or repeated text
    sequences. The researchers used prompt engineering to craft inputs that triggered
    memorized sequences, continuation attacks that used partial quotes or names to
    extract full sensitive information, statistical analysis to identify patterns
    in model outputs indicating verbatim memorization, and verification methods that
    cross-referenced extracted data with known public sources to confirm accuracy.
    Out of 600,000 attempts, they successfully extracted over 16,000 unique instances
    of memorized training data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 技术方法利用了GPT-3对稀有或重复文本序列的记忆。研究人员使用提示工程来构建触发记忆序列的输入，使用部分引用或名称提取完整敏感信息的延续攻击，对模型输出中的模式进行统计分析以识别字面记忆，以及通过将提取的数据与已知公共来源交叉引用来验证准确性的验证方法。在60万次尝试中，他们成功提取了超过16,000个独特的记忆训练数据实例。
- en: This attack challenged assumptions about training data privacy. The results
    demonstrated that large language models can act as unintentional databases, storing
    and retrieving sensitive information from their training data. This violated privacy
    expectations that training data would be “forgotten” after model training, revealing
    that scale amplifies privacy risk as larger models (175B parameters) memorize
    more training data than smaller models.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击挑战了关于训练数据隐私的假设。结果表明，大型语言模型可以作为无意中的数据库，从其训练数据中存储和检索敏感信息。这违反了训练数据在模型训练后会被“遗忘”的隐私预期，揭示了规模放大了隐私风险，因为更大的模型（175B参数）比小模型记忆了更多的训练数据。
- en: The research revealed that common data protection measures proved insufficient.
    Even after data deduplication, models still memorized sensitive information, highlighting
    the tension between model utility and privacy protection. Techniques to prevent
    memorization such as differential privacy and aggressive data filtering reduce
    model quality, creating challenging trade-offs for practitioners.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，常见的数据保护措施证明是不够的。即使在数据去重之后，模型仍然会记忆敏感信息，突显了模型效用与隐私保护之间的紧张关系。防止记忆的技术，如差分隐私和激进的数据过滤，会降低模型质量，为从业者创造具有挑战性的权衡。
- en: The industry response was swift and comprehensive. Organizations began widespread
    adoption of differential privacy in large model training, enhanced data filtering
    and PII removal processes, development of membership inference defenses, new research
    into machine unlearning techniques, and regulatory discussions about training
    data rights and model transparency. Modern organizations now commonly implement
    differential privacy during training (ε ≤ 8), aggressive PII filtering using automated
    detection tools, regular auditing for data memorization using extraction attacks,
    and legal frameworks for handling training data containing personal information
    ([Carlini et al. 2021](ch058.xhtml#ref-carlini2021extracting)).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 行业反应迅速而全面。组织开始在大模型训练中广泛采用差分隐私，增强了数据过滤和PII移除流程，开发了成员推理防御，进行了机器反学习技术的新研究，并就训练数据权利和模型透明度进行了监管讨论。现代组织现在通常在训练期间实施差分隐私（ε
    ≤ 8），使用自动化检测工具进行激进的PII过滤，定期审计使用提取攻击的数据记忆，以及处理包含个人信息的训练数据的法律框架（[Carlini等人2021](ch058.xhtml#ref-carlini2021extracting)）。
- en: Secure Model Design
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全模型设计
- en: Moving from data-level protections to model-level security, we address how security
    considerations shape the model development process. Security begins at the design
    phase of a machine learning system. While downstream mechanisms such as access
    control and encryption protect models once deployed, many vulnerabilities can
    be mitigated earlier—through architectural choices, defensive training strategies,
    and mechanisms that embed resilience directly into the model’s structure or behavior.
    By considering security as a design constraint, system developers can reduce the
    model’s exposure to attacks, limit its ability to leak sensitive information,
    and provide verifiable ownership protection.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据级保护转向模型级安全，我们探讨了安全考虑如何塑造模型开发过程。安全始于机器学习系统的设计阶段。虽然下游机制如访问控制和加密在模型部署后保护模型，但许多漏洞可以在早期通过架构选择、防御性训练策略以及将弹性直接嵌入模型结构或行为的机制来缓解。将安全视为设计约束，系统开发者可以减少模型遭受攻击的风险，限制其泄露敏感信息的能力，并提供可验证的所有权保护。
- en: One important design strategy is to build robust-by-construction models that
    reduce the risk of exploitation at inference time. For instance, models with confidence
    calibration or abstention mechanisms can be trained to avoid making predictions
    when input uncertainty is high. These techniques can help prevent overconfident
    misclassifications in response to adversarial or out-of-distribution inputs. Models
    may also employ output smoothing, regularizing the output distribution to reduce
    sharp decision boundaries that are especially susceptible to adversarial perturbations.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的设计策略是构建通过构造稳健的模型，以降低推理时的利用风险。例如，具有置信度校准或弃权机制的模型可以被训练以避免在输入不确定性高时做出预测。这些技术可以帮助防止对对抗性或分布外输入的过度自信的错误分类。模型还可以采用输出平滑，正则化输出分布以减少特别容易受到对抗性扰动影响的尖锐决策边界。
- en: Certain application contexts may also benefit from choosing simpler or compressed
    architectures. Limiting model capacity can reduce opportunities for memorization
    of sensitive training data and complicate efforts to reverse-engineer the model
    from output behavior. For embedded or on-device settings, smaller models are also
    easier to secure, as they typically require less memory and compute, lowering
    the likelihood of side-channel leakage or runtime manipulation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 某些应用场景也可能从选择更简单或压缩的架构中受益。限制模型容量可以减少对敏感训练数据的记忆机会，并使从输出行为中逆向工程模型的努力复杂化。对于嵌入式或设备上的设置，较小的模型也更容易保证安全，因为它们通常需要更少的内存和计算，从而降低侧信道泄露或运行时操作的可能性。
- en: Another design-stage consideration is the use of model watermarking[41](#fn41),
    a technique for embedding verifiable ownership signatures directly into the model’s
    parameters or output behavior ([Adi et al. 2018](ch058.xhtml#ref-adi2018turning)).
    A watermark might be implemented, for example, as a hidden response pattern triggered
    by specific inputs, or as a parameter-space perturbation that does not affect
    accuracy but is statistically identifiable.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个设计阶段的考虑因素是使用模型水印[41](#fn41)，这是一种将可验证的所有权签名直接嵌入到模型参数或输出行为的技术（[Adi 等人 2018](ch058.xhtml#ref-adi2018turning)）。例如，水印可以实施为一个由特定输入触发的隐藏响应模式，或者作为一个参数空间扰动，它不会影响准确性但可以统计识别。
- en: For example, in a keyword spotting system deployed on embedded hardware for
    voice activation (e.g., “Hey Alexa” or “OK Google”), a secure design might use
    a lightweight convolutional neural network with confidence calibration to avoid
    false activations on uncertain audio. The model might also include an abstention
    threshold, below which it produces no activation at all. To protect intellectual
    property, a designer could embed a watermark by training the model to respond
    with a unique label only when presented with a specific, unused audio trigger
    known only to the developer. These design choices not only improve robustness
    and accountability, but also support future verification in case of IP disputes
    or performance failures in the field.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个部署在嵌入式硬件上的关键词检测系统中，用于语音激活（例如，“嘿，Alexa”或“好的，Google”），一个安全的设计可能会使用具有置信度校准的轻量级卷积神经网络，以避免在不确定的音频上产生误激活。该模型还可以包括一个弃权阈值，低于此阈值它不会产生任何激活。为了保护知识产权，设计者可以通过训练模型以对特定、未使用的音频触发器做出唯一标签来嵌入水印，该触发器只有开发者知道。这些设计选择不仅提高了鲁棒性和问责制，还支持在知识产权争议或现场性能故障的情况下进行未来的验证。
- en: In high-risk applications, such as medical diagnosis, autonomous vehicles, or
    financial decision systems, designers may also prioritize interpretable model
    architectures, such as decision trees, rule-based classifiers, or sparsified networks,
    to enhance system auditability. These models are often easier to understand and
    explain, making it simpler to identify potential vulnerabilities or biases. Using
    interpretable models allows developers to provide clearer insights into how the
    system arrived at a particular decision, which is important for building trust
    with users and regulators.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险应用中，例如医疗诊断、自动驾驶汽车或金融决策系统，设计者可能还会优先考虑可解释的模型架构，如决策树、基于规则的分类器或稀疏网络，以增强系统可审计性。这些模型通常更容易理解和解释，从而更容易识别潜在漏洞或偏差。使用可解释模型允许开发者提供更清晰的见解，了解系统如何得出特定决策，这对于建立用户和监管机构的信任至关重要。
- en: Model design choices often reflect trade-offs between accuracy, robustness,
    transparency, and system complexity. When viewed from a systems perspective, early-stage
    design decisions yield the highest value for long-term security. They shape what
    the model can learn, how it behaves under uncertainty, and what guarantees can
    be made about its provenance, interpretability, and resilience.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 模型设计的选择通常反映了准确性、鲁棒性、透明度和系统复杂性之间的权衡。从系统角度来看，早期设计决策为长期安全提供了最高价值。它们决定了模型可以学习的内容、它在不确定性下的行为，以及对其来源、可解释性和弹性的保证。
- en: Secure Model Deployment
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全模型部署
- en: While secure design establishes a foundation of robustness, protection extends
    beyond the model itself to how it is packaged and deployed. Protecting machine
    learning models from theft, abuse, and unauthorized manipulation requires security
    considerations throughout both the design and deployment phases. A model’s vulnerability
    is not solely determined by its training procedure or architecture, but also by
    how it is serialized, packaged, deployed, and accessed during inference. As models
    are increasingly embedded into edge devices, served through public APIs, or integrated
    into multi-tenant platforms, robust security practices are important to ensure
    the integrity, confidentiality, and availability of model behavior.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然安全设计为鲁棒性奠定了基础，但保护范围不仅限于模型本身，还包括其打包和部署方式。保护机器学习模型免受盗窃、滥用和未经授权的操纵需要在设计和部署的整个阶段考虑安全因素。模型的安全性不仅由其训练过程或架构决定，还由其在推理过程中的序列化、打包、部署和访问方式决定。随着模型越来越多地嵌入到边缘设备中、通过公共API提供或集成到多租户平台中，稳健的安全实践对于确保模型行为的完整性、机密性和可用性至关重要。
- en: 'This section addresses security mechanisms across three key stages: model design,
    secure packaging and serialization, and deployment and access control. These practices
    complement the model optimization techniques discussed in [Chapter 10](ch016.xhtml#sec-model-optimizations),
    where performance improvements must not compromise security properties.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了三个关键阶段的安全机制：模型设计、安全打包和序列化，以及部署和访问控制。这些实践补充了第10章中讨论的模型优化技术，其中性能改进不得损害安全属性。
- en: From a design perspective, architectural choices can reduce a model’s exposure
    to adversarial manipulation and unauthorized use. For example, models can incorporate
    confidence calibration or abstention mechanisms that allow them to reject uncertain
    or anomalous inputs rather than producing potentially misleading outputs. Designing
    models with simpler or compressed architectures can also reduce the risk of reverse
    engineering or information leakage through side-channel analysis. In some cases,
    model designers may embed imperceptible watermarks, which are unique signatures
    embedded in the parameters or behavior of the model, that can later be used to
    demonstrate ownership in cases of misappropriation ([Uchida et al. 2017](ch058.xhtml#ref-uchida2017embedding)).
    These design-time protections are essential for commercially valuable models,
    where intellectual property rights are at stake.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 从设计角度来看，架构选择可以减少模型受到对抗性操纵和未经授权使用的风险。例如，模型可以采用置信度校准或弃权机制，允许它们拒绝不确定或异常的输入，而不是产生可能具有误导性的输出。设计具有更简单或压缩架构的模型也可以降低逆向工程或通过侧信道分析泄露信息的风险。在某些情况下，模型设计者可能会嵌入不可察觉的水印，这些水印是嵌入在模型参数或行为中的独特签名，可以在不当使用的情况下用于证明所有权（[Uchida等人
    2017](ch058.xhtml#ref-uchida2017embedding)）。这些设计时的保护对于具有商业价值的模型至关重要，在这些模型中，知识产权处于风险之中。
- en: Once training is complete, the model must be securely packaged for deployment.
    Storing models in plaintext formats, including unencrypted ONNX or PyTorch checkpoint
    files, can expose internal structures and parameters to attackers with access
    to the file system or memory. To mitigate this risk, models should be encrypted,
    obfuscated, or wrapped in secure containers. Decryption keys should be made available
    only at runtime and only within trusted environments. Additional mechanisms, such
    as quantization-aware encryption or integrity-checking wrappers, can prevent tampering
    and offline model theft.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，模型必须安全打包以供部署。将模型存储在明文格式中，包括未加密的ONNX或PyTorch检查点文件，可能会将内部结构和参数暴露给有权访问文件系统或内存的攻击者。为了减轻这种风险，模型应该被加密、混淆或封装在安全容器中。解密密钥应在运行时且仅在受信任的环境中提供。额外的机制，如量化感知加密或完整性检查包装器，可以防止篡改和离线模型盗窃。
- en: Deployment environments must also enforce strong access control policies to
    ensure that only authorized users and services can interact with inference endpoints.
    Authentication protocols, including OAuth[42](#fn42) tokens, mutual TLS[43](#fn43),
    or API keys[44](#fn44), should be combined with role-based access control (RBAC)[45](#fn45)
    to restrict access according to user roles and operational context. For instance,
    OpenAI’s hosted model APIs require users to include an OPENAI_API_KEY when submitting
    inference requests.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 部署环境还必须执行强大的访问控制策略，以确保只有授权用户和服务才能与推理端点交互。认证协议，包括OAuth[42](#fn42)令牌、相互TLS[43](#fn43)或API密钥[44](#fn44)，应与基于角色的访问控制(RBAC)[45](#fn45)相结合，根据用户角色和操作环境限制访问。例如，OpenAI的托管模型API要求用户在提交推理请求时包含OPENAI_API_KEY。
- en: This key authenticates the client and allows the backend to enforce usage policies,
    monitor for abuse, and log access patterns. Secure implementations retrieve API
    keys from environment variables rather than hardcoding them into source code,
    preventing credential exposure in version control systems or application logs.
    Such key-based access control mechanisms are simple to implement but require careful
    key management and monitoring to prevent misuse, unauthorized access, or model
    extraction. Additional security measures in production deployments typically include
    model integrity verification through SHA-256 hash checking, rate limiting to prevent
    abuse, input validation for size and format constraints, and comprehensive logging
    for security event tracking.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 此关键验证客户端并允许后端执行使用策略，监控滥用情况，并记录访问模式。安全实现从环境变量中检索API密钥，而不是将它们硬编码到源代码中，从而防止凭证在版本控制系统或应用程序日志中泄露。这种基于密钥的访问控制机制易于实现，但需要谨慎的密钥管理和监控，以防止滥用、未经授权的访问或模型提取。生产部署中的附加安全措施通常包括通过SHA-256哈希检查验证模型完整性、速率限制以防止滥用、对大小和格式约束进行输入验证以及为安全事件跟踪进行全面的日志记录。
- en: The secure deployment patterns established here integrate naturally with the
    development workflows explored in [Chapter 5](ch011.xhtml#sec-ai-workflow), ensuring
    security becomes part of standard engineering practice rather than an afterthought.
    Runtime monitoring ([Section 15.8.6](ch021.xhtml#sec-security-privacy-runtime-system-monitoring-a71c))
    extends these protections to operational environments.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里建立的网络安全部署模式自然地与[第五章](ch011.xhtml#sec-ai-workflow)中探讨的开发工作流程相结合，确保安全成为标准工程实践的一部分，而不是事后考虑。运行时监控([第15.8.6节](ch021.xhtml#sec-security-privacy-runtime-system-monitoring-a71c))将这些保护措施扩展到操作环境。
- en: Runtime System Monitoring
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行时系统监控
- en: While secure design and deployment establish strong foundations, protection
    must extend to runtime operations. Even with robust design and deployment safeguards,
    machine learning systems remain vulnerable to runtime threats. Attackers may craft
    inputs that bypass validation, exploit model behavior, or target system-level
    infrastructure.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然安全设计和部署建立了坚实的基础，但保护必须扩展到运行时操作。即使有稳健的设计和部署保障，机器学习系统仍然容易受到运行时威胁的攻击。攻击者可能构建绕过验证的输入，利用模型行为，或针对系统级基础设施。
- en: Production ML systems face diverse deployment contexts—from cloud services to
    edge devices to embedded systems. Each environment presents unique monitoring
    challenges and opportunities, as the system architectures from [Chapter 2](ch008.xhtml#sec-ml-systems)
    demonstrate. Defensive strategies must extend beyond static protection to include
    real-time monitoring, threat detection, and incident response. This section outlines
    operational defenses that maintain system trust under adversarial conditions,
    connecting forward to the comprehensive MLOps practices detailed in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级机器学习系统面临着多样化的部署环境——从云服务到边缘设备再到嵌入式系统。每个环境都带来了独特的监控挑战和机遇，正如[第二章](ch008.xhtml#sec-ml-systems)中展示的系统架构所示。防御策略必须超越静态保护，包括实时监控、威胁检测和事件响应。本节概述了在对抗条件下保持系统信任的操作防御措施，并与[第十三章](ch019.xhtml#sec-ml-operations)中详细介绍的全面MLOps实践相衔接。
- en: 'Runtime monitoring encompasses a range of techniques for observing system behavior,
    detecting anomalies, and triggering mitigation. These techniques can be grouped
    into three categories: input validation, output monitoring, and system integrity
    checks.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时监控包括一系列观察系统行为、检测异常和触发缓解的技术。这些技术可以分为三个类别：输入验证、输出监控和系统完整性检查。
- en: Input Validation
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入验证
- en: Input validation is the first line of defense at runtime. It ensures that incoming
    data conforms to expected formats, statistical properties, or semantic constraints
    before it is passed to a machine learning model. Without these safeguards, models
    are vulnerable to adversarial inputs, which are crafted examples designed to trigger
    incorrect predictions, or to malformed inputs that cause unexpected behavior in
    preprocessing or inference.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 输入验证是运行时的第一道防线。它确保在传递给机器学习模型之前，传入数据符合预期的格式、统计属性或语义约束。没有这些保障措施，模型容易受到对抗性输入的影响，这些输入是精心设计的例子，旨在触发错误的预测，或者导致预处理或推理中的意外行为。
- en: Machine learning models, unlike traditional rule-based systems, often do not
    fail safely. Small, carefully chosen changes to input data can cause models to
    make high-confidence but incorrect predictions. Input validation helps detect
    and reject such inputs early in the pipeline ([Goodfellow, Shlens, and Szegedy
    2014a](ch058.xhtml#ref-goodfellow2015explaining)).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的基于规则的系统不同，机器学习模型通常不会安全地失败。对输入数据的微小、精心选择的变化可能导致模型做出高度自信但错误的预测。输入验证有助于在管道早期检测和拒绝此类输入([Goodfellow,
    Shlens, and Szegedy 2014a](ch058.xhtml#ref-goodfellow2015explaining))。
- en: Validation techniques range from low-level checks (e.g., input size, type, and
    value ranges) to semantic filters (e.g., verifying whether an image contains a
    recognizable object or whether a voice recording includes speech). For example,
    a facial recognition system might validate that the uploaded image is within a
    certain resolution range (e.g., 224×224 to 1024×1024 pixels), contains RGB channels,
    and passes a lightweight face detection filter. This prevents inputs like blank
    images, text screenshots, or synthetic adversarial patterns from reaching the
    model. Similarly, a voice assistant might require that incoming audio files be
    between 1 and 5 seconds long, have a valid sampling rate (e.g., 16kHz), and contain
    detectable human speech using a speech activity detector (SAD)[46](#fn46). This
    ensures that empty recordings, music clips, or noise bursts are filtered before
    model inference.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 验证技术范围从低级检查（例如，输入大小、类型和值范围）到语义过滤器（例如，验证图像是否包含可识别的对象或语音录音是否包含语音）。例如，一个面部识别系统可能会验证上传的图像是否在特定的分辨率范围内（例如，224×224到1024×1024像素），是否包含RGB通道，并且通过了一个轻量级的面部检测过滤器。这阻止了如空白图像、文本截图或合成对抗模式等输入达到模型。同样，一个语音助手可能要求传入的音频文件长度在1到5秒之间，具有有效的采样率（例如，16kHz），并且使用语音活动检测器（SAD）[46](#fn46)可以检测到可识别的人类语音。这确保了在模型推理之前，空录音、音乐剪辑或噪声爆裂被过滤掉。
- en: In generative systems such as DALL·E, Stable Diffusion, or Sora, input validation
    often involves prompt filtering. This includes scanning the user’s text prompt
    for banned terms, brand names, profanity, or misleading medical claims. For example,
    a user prompt like “Generate an image of a medication bottle labeled with Pfizer’s
    logo” might be rejected or rewritten due to trademark concerns. Filters may operate
    using keyword lists, regular expressions, or lightweight classifiers that assess
    prompt intent. These filters prevent the generative model from being used to produce
    harmful, illegal, or misleading content—even before sampling begins.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在DALL·E、Stable Diffusion或Sora等生成系统中，输入验证通常涉及提示过滤。这包括扫描用户的文本提示以查找禁止的术语、品牌名称、粗俗语言或误导性医疗声明。例如，一个用户提示“生成一个标有辉瑞标志的药物瓶的图像”可能会因商标问题而被拒绝或重写。过滤器可能使用关键字列表、正则表达式或轻量级分类器来评估提示意图。这些过滤器防止生成模型被用于产生有害、非法或误导性的内容——甚至在采样开始之前。
- en: In some applications, distributional checks are also used. These assess whether
    the incoming data statistically resembles what the model saw during training.
    For instance, a computer vision pipeline might compare the color histogram of
    the input image to a baseline distribution, flagging outliers for manual review
    or rejection.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，也使用分布性检查。这些检查评估传入的数据在统计上是否类似于模型在训练期间所见到的。例如，一个计算机视觉管道可能会将输入图像的颜色直方图与基线分布进行比较，标记异常值以供人工审查或拒绝。
- en: These validations can be lightweight (heuristics or threshold rules) or learned
    (small models trained to detect distribution shift or adversarial artifacts).
    In either case, input validation serves as a important pre-inference firewall—reducing
    exposure to adversarial behavior, improving system stability, and increasing trust
    in downstream model decisions.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这些验证可以是轻量级的（启发式或阈值规则）或学习型的（训练以检测分布变化或对抗性伪影的小型模型）。在任一情况下，输入验证都充当一个重要的预推理防火墙——减少对对抗行为的暴露，提高系统稳定性，并增加对下游模型决策的信任。
- en: Output Monitoring
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出监控
- en: Even when inputs pass validation, adversarial or unexpected behavior may still
    emerge at the model’s output. Output monitoring helps detect such anomalies by
    analyzing model predictions in real time. These mechanisms observe how the model
    behaves across inputs, by tracking its confidence, prediction entropy, class distribution,
    or response patterns, to flag deviations from expected behavior.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 即使输入通过了验证，模型输出仍可能出现对抗性或意外行为。输出监控通过实时分析模型预测来帮助检测此类异常。这些机制通过跟踪模型的置信度、预测熵、类别分布或响应模式，观察模型在输入上的行为，以标记与预期行为偏差的情况。
- en: A key target for monitoring is prediction confidence. For example, if a classification
    model begins assigning high confidence to low-frequency or previously rare classes,
    this may indicate the presence of adversarial inputs or a shift in the underlying
    data distribution. Monitoring the entropy of the output distribution can similarly
    reveal when the model is overly certain in ambiguous contexts—an early signal
    of possible manipulation.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的关键目标是预测置信度。例如，如果一个分类模型开始对低频或之前罕见的类别分配高置信度，这可能表明存在对抗性输入或底层数据分布的变化。通过监控输出分布的熵，可以同样揭示模型在模糊情境中过度自信的情况——这是可能操纵的早期信号。
- en: In content moderation systems, a model that normally outputs neutral or “safe”
    labels may suddenly begin producing high-confidence “safe” labels for inputs containing
    offensive or restricted content. Output monitoring can detect this mismatch by
    comparing predictions against auxiliary signals or known-safe reference sets.
    When deviations are detected, the system may trigger a fallback policy—such as
    escalating the content for human review or switching to a conservative baseline
    model.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在内容审核系统中，一个通常输出中性或“安全”标签的模型可能会突然开始对包含冒犯性或受限内容输入产生高置信度的“安全”标签。输出监控可以通过比较预测与辅助信号或已知的“安全”参考集来检测这种不匹配。当检测到偏差时，系统可能会触发回退策略——例如，将内容提升给人审阅或切换到保守的基线模型。
- en: Time-series models also benefit from output monitoring. For instance, an anomaly
    detection model used in fraud detection might track predicted fraud scores for
    sequences of financial transactions. A sudden drop in fraud scores, especially
    during periods of high transaction volume, may indicate model tampering, label
    leakage, or evasion attempts. Monitoring the temporal evolution of predictions
    provides a broader perspective than static, pointwise classification.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列模型也受益于输出监控。例如，在欺诈检测中使用的异常检测模型可能会跟踪一系列金融交易的预测欺诈分数。欺诈分数的突然下降，尤其是在交易量高的时期，可能表明模型被篡改、标签泄露或逃避尝试。监控预测的时间演变比静态的、逐点分类提供了更广阔的视角。
- en: Generative models, such as text-to-image systems, introduce unique output monitoring
    challenges. These models can produce high-fidelity imagery that may inadvertently
    violate content safety policies, platform guidelines, or user expectations. To
    mitigate these risks, post-generation classifiers are commonly employed to assess
    generated content for objectionable characteristics such as violence, nudity,
    or brand misuse. These classifiers operate downstream of the generative model
    and can suppress, blur, or reject outputs based on predefined thresholds. Some
    systems also inspect internal representations (e.g., attention maps[47](#fn47)
    or latent embeddings) to anticipate potential misuse before content is rendered.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型，如文本到图像系统，引入了独特的输出监控挑战。这些模型可以产生高保真度的图像，可能会无意中违反内容安全政策、平台指南或用户期望。为了减轻这些风险，通常使用后生成分类器来评估生成内容是否具有令人反感的特征，如暴力、裸露或品牌滥用。这些分类器在生成模型之后运行，可以根据预定义的阈值抑制、模糊或拒绝输出。一些系统还会检查内部表示（例如，注意力图[47](#fn47)或潜在嵌入）以预测内容渲染前的潜在滥用。
- en: However, prompt filtering alone is insufficient for safety. Research has shown
    that text-to-image systems can be manipulated through implicitly adversarial prompts,
    which are queries that appear benign but lead to policy-violating outputs. The
    Adversarial Nibbler project introduces an open red teaming methodology that identifies
    such prompts and demonstrates how models like Stable Diffusion can produce unintended
    content despite the absence of explicit trigger phrases ([Quaye et al. 2024](ch058.xhtml#ref-quaye2024adversarial)).
    These failure cases often bypass prompt filters because their risk arises from
    model behavior during generation, not from syntactic or lexical cues.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅靠提示过滤不足以保证安全。研究表明，文本到图像系统可以通过隐式对抗性提示被操纵，这些提示看似无害但会导致违反策略的输出。Adversarial
    Nibbler项目引入了一种开放的红队方法，用于识别此类提示，并展示了像Stable Diffusion这样的模型如何在没有明确触发短语的情况下产生意外内容([Quaye等人
    2024](ch058.xhtml#ref-quaye2024adversarial))。这些失败案例通常绕过提示过滤器，因为它们的风险源于模型在生成过程中的行为，而不是语法或词汇线索。
- en: '![](../media/file242.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file242.png)'
- en: 'Figure 15.14: **Adversarial Prompt Evasion**: Implicitly adversarial prompts
    bypass typical content filters by triggering unintended generations, revealing
    limitations of solely relying on pre-generation safety checks. these examples
    underscore the necessity of post-hoc content analysis as a complementary defense
    layer for robust generative AI systems. Source: ([Quaye et al. 2024](ch058.xhtml#ref-quaye2024adversarial).).'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.14：**对抗性提示规避**：隐式对抗性提示通过触发意外生成绕过典型的内容过滤器，揭示了仅依赖预先生成安全检查的局限性。这些例子强调了事后内容分析作为稳健生成人工智能系统的补充防御层的重要性。来源：([Quaye等人
    2024](ch058.xhtml#ref-quaye2024adversarial))。
- en: As shown in [Figure 15.14](ch021.xhtml#fig-adversarial-nibbler), even prompts
    that appear innocuous can trigger unsafe generations. Such examples highlight
    the limitations of pre-generation safety checks and reinforce the necessity of
    output-based monitoring as a second line of defense. This two-stage pipeline—consisting
    of prompt filtering followed by post-hoc content analysis important for ensuring
    the safe deployment of generative models in open-ended or user-facing environments.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图15.14](ch021.xhtml#fig-adversarial-nibbler)所示，即使是看似无害的提示也可能触发不安全的生成。这些例子突出了预先生成安全检查的限制，并强调了基于输出的监控作为第二道防线的重要性。这个两阶段管道——由提示过滤后跟事后内容分析组成，对于确保在开放或面向用户的环境中使用生成模型的安全性至关重要。
- en: In the domain of language generation, output monitoring plays a different but
    equally important role. Here, the goal is often to detect toxicity, hallucinated
    claims, or off-distribution responses. For example, a customer support chatbot
    may be monitored for keyword presence, tonal alignment, or semantic coherence.
    If a response contains profanity, unsupported assertions, or syntactically malformed
    text, the system may trigger a rephrasing, initiate a fallback to scripted templates,
    or halt the response altogether.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言生成领域，输出监控扮演着不同但同样重要的角色。在这里，目标通常是检测毒性、幻觉声明或偏离分布的响应。例如，客户支持聊天机器人可能会被监控是否存在关键词、语调一致或语义连贯性。如果响应包含粗俗语言、未经证实的断言或语法上不正确的文本，系统可能会触发改写、启动回退到脚本模板，或者完全停止响应。
- en: Effective output monitoring combines rule-based heuristics with learned detectors
    trained on historical outputs. These detectors are deployed to flag deviations
    in real time and feed alerts into incident response pipelines. In contrast to
    model-centric defenses like adversarial training, which aim to improve model robustness,
    output monitoring emphasizes containment and remediation. Its role is not to prevent
    exploitation but to detect its symptoms and initiate appropriate countermeasures
    ([Savas et al. 2022](ch058.xhtml#ref-savas2022ml)). In safety-important or policy-sensitive
    applications, such mechanisms form a important layer of operational resilience.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的输出监控结合了基于规则的启发式方法和在历史输出上训练的学习检测器。这些检测器被部署以实时标记偏差并将警报输入到事件响应管道中。与旨在提高模型鲁棒性的以模型为中心的防御措施（如对抗性训练）相比，输出监控强调的是控制和补救。其作用不是防止利用，而是检测其症状并启动适当的对策([Savas等人
    2022](ch058.xhtml#ref-savas2022ml))。在安全重要或政策敏感的应用中，此类机制构成了操作弹性的重要层。
- en: These principles have been implemented in recent output filtering frameworks.
    For example, LLM Guard combines transformer-based classifiers with safety dimensions
    such as toxicity, misinformation, and illegal content to assess and reject prompts
    or completions in instruction-tuned LLMs ([Inan et al. 2023](ch058.xhtml#ref-lee2023llmguard)).
    Similarly, [ShieldGemma](https://ai.google.dev/gemma/docs/shieldgemma), developed
    as part of Google’s open Gemma model release, applies configurable scoring functions
    to detect and filter undesired outputs during inference. Both systems exemplify
    how safety classifiers and output monitors are being integrated into the runtime
    stack to support scalable, policy-aligned deployment of generative language models.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则已应用于最近的输出过滤框架中。例如，LLM Guard结合了基于transformer的分类器以及如毒性、错误信息和非法内容等安全维度，以评估和拒绝指令调整的LLM中的提示或完成([Inan等人2023](ch058.xhtml#ref-lee2023llmguard))。同样，[ShieldGemma](https://ai.google.dev/gemma/docs/shieldgemma)，作为谷歌开放Gemma模型发布的一部分开发，在推理期间应用可配置的评分函数以检测和过滤不希望的结果。这两个系统都展示了如何将安全分类器和输出监控集成到运行时堆栈中，以支持可扩展、符合政策的生成语言模型的部署。
- en: Integrity Checks
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 完整性检查
- en: While input and output monitoring focus on model behavior, system integrity
    checks ensure that the underlying model files, execution environment, and serving
    infrastructure remain untampered throughout deployment. These checks detect unauthorized
    modifications, verify that the model running in production is authentic, and alert
    operators to suspicious system-level activity.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输入和输出监控关注模型行为，但系统完整性检查确保在整个部署过程中，底层模型文件、执行环境和服务基础设施保持未受篡改。这些检查可以检测未经授权的修改，验证生产中运行的模型是真实的，并警告操作员有可疑的系统级活动。
- en: One of the most common integrity mechanisms is cryptographic model verification.
    Before a model is loaded into memory, the system can compute a cryptographic hash
    (e.g., SHA-256)[48](#fn48) of the model file and compare it against a known-good
    signature.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的完整性机制之一是加密模型验证。在模型被加载到内存之前，系统可以计算模型文件的加密哈希（例如，SHA-256[48](#fn48)），并将其与已知的良好签名进行比较。
- en: Access control and audit logging complement cryptographic checks. ML systems
    should restrict access to model files using role-based permissions and monitor
    file access patterns. For instance, repeated attempts to read model checkpoints
    from a non-standard path, or inference requests from unauthorized IP ranges, may
    indicate tampering, privilege escalation, or insider threats.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制和审计日志补充了加密检查。机器学习系统应使用基于角色的权限限制对模型文件的访问，并监控文件访问模式。例如，从非标准路径重复尝试读取模型检查点，或从未经授权的IP范围发出的推理请求，可能表明篡改、权限提升或内部威胁。
- en: In cloud environments, container- or VM-based isolation[49](#fn49) helps enforce
    process and memory boundaries, but these protections can erode over time due to
    misconfiguration or supply chain vulnerabilities.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中，基于容器或虚拟机的隔离[49](#fn49)有助于强制执行进程和内存边界，但这些保护措施可能会因配置错误或供应链漏洞而随着时间的推移而减弱。
- en: 'For example, in a regulated healthcare ML deployment[50](#fn50), integrity
    checks might include: verifying the model hash against a signed manifest, validating
    that the runtime environment uses only approved Python packages, and checking
    that inference occurs inside a signed and attested virtual machine. These checks
    ensure compliance with regulations like HIPAA[51](#fn51)’s integrity requirements
    and GDPR’s accountability principle, limit the risk of silent failures, and create
    a forensic trail in case of audit or breach.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个受监管的医疗保健机器学习部署[50](#fn50)中，完整性检查可能包括：验证模型哈希与已签名的清单相匹配，确认运行时环境仅使用批准的Python包，以及检查推理是否在已签名并证明的虚拟机内部进行。这些检查确保符合如HIPAA[51](#fn51)的完整性要求以及GDPR的问责制原则，限制静默故障的风险，并在审计或违规情况下创建法医跟踪。
- en: Some systems also implement runtime memory verification, such as scanning for
    unexpected model parameter changes or checking that memory-mapped model weights
    remain unaltered during execution. While more common in high-assurance systems,
    such checks are becoming more feasible with the adoption of secure enclaves and
    trusted runtimes.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统还实现了运行时内存验证，例如扫描意外的模型参数变化或检查在执行期间内存映射的模型权重是否保持未更改。虽然这些检查在高保证系统中更为常见，但随着安全区域和可信运行时的采用，这些检查变得更加可行。
- en: Taken together, system integrity checks play a important role in protecting
    machine learning systems from low-level attacks that bypass the model interface.
    When coupled with input/output monitoring, they provide layered assurance that
    both the model and its execution environment remain trustworthy under adversarial
    conditions.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，系统完整性检查在保护机器学习系统免受绕过模型界面的低级攻击中发挥着重要作用。当与输入/输出监控相结合时，它们提供了分层保证，即在对抗条件下模型及其执行环境仍然值得信赖。
- en: Response and Rollback
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 响应和回滚
- en: When a security breach, anomaly, or performance degradation is detected in a
    deployed machine learning system, rapid and structured incident response is important
    to minimizing impact. The goal is not only to contain the issue but to restore
    system integrity and ensure that future deployments benefit from the insights
    gained. Unlike traditional software systems, ML responses may require handling
    model state, data drift, or inference behavior, making recovery more complex.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 当在部署的机器学习系统中检测到安全漏洞、异常或性能下降时，快速和结构化的事件响应对于最小化影响至关重要。目标是不仅遏制问题，还要恢复系统完整性并确保未来的部署能够从获得的见解中受益。与传统软件系统不同，机器学习响应可能需要处理模型状态、数据漂移或推理行为，这使得恢复更加复杂。
- en: The first step is to define incident detection thresholds that trigger escalation.
    These thresholds may come from input validation (e.g., invalid input rates), output
    monitoring (e.g., drop in prediction confidence), or system integrity checks (e.g.,
    failed model signature verification). When a threshold is crossed, the system
    should initiate an automated or semi-automated response protocol.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义触发升级的事件检测阈值。这些阈值可能来自输入验证（例如，无效输入率）、输出监控（例如，预测置信度下降）或系统完整性检查（例如，模型签名验证失败）。当超过阈值时，系统应启动自动或半自动响应协议。
- en: One common strategy is model rollback, where the system reverts to a previously
    verified version of the model. For instance, if a newly deployed fraud detection
    model begins misclassifying transactions, the system may fall back to the last
    known-good checkpoint, restoring service while the affected version is quarantined.
    Rollback mechanisms require version-controlled model storage, typically supported
    by MLOps platforms such as MLflow, TFX, or SageMaker.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见策略是模型回滚，系统将回滚到之前验证过的模型版本。例如，如果新部署的欺诈检测模型开始错误分类交易，系统可能会回滚到最后已知的良好检查点，在受影响的版本被隔离的同时恢复服务。回滚机制需要版本控制的模型存储，通常由MLOps平台（如MLflow、TFX或SageMaker）支持。
- en: In high-availability environments, model isolation may be used to contain failures.
    The affected model instance can be removed from load balancers or shadowed in
    a canary deployment setup. This allows continued service with unaffected replicas
    while maintaining forensic access to the compromised model for analysis.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在高可用性环境中，可以使用模型隔离来遏制故障。受影响的模型实例可以从负载均衡器中移除或在金丝雀部署设置中屏蔽。这允许在不影响副本的情况下继续提供服务，同时保持对受损害模型的取证访问，以便进行分析。
- en: Traffic throttling is another immediate response tool. If an adversarial actor
    is probing a public inference API at high volume, the system can rate-limit or
    temporarily block offending IP ranges while continuing to serve trusted clients.
    This containment technique helps prevent abuse without requiring full system shutdown.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 流量限制是另一种即时响应工具。如果敌对行为者以高量探测公共推理API，系统可以限制或暂时阻止违规IP范围，同时继续为受信任客户提供服务。这种遏制技术有助于防止滥用，而无需完全关闭系统。
- en: Once immediate containment is in place, investigation and recovery can begin.
    This may include forensic analysis of input logs, parameter deltas between model
    versions, or memory snapshots from inference containers. In regulated environments,
    organizations may also need to notify users or auditors, particularly if personal
    or safety-important data was affected.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实施即时遏制，就可以开始调查和恢复。这可能包括对输入日志的取证分析、模型版本之间的参数差异或推理容器中的内存快照。在受监管的环境中，组织可能还需要通知用户或审计师，尤其是如果受影响的数据是个人或安全重要数据。
- en: Recovery typically involves retraining or patching the model. This must occur
    through a secure update process, using signed artifacts, trusted build pipelines,
    and validated data. To prevent recurrence, the incident should feed back into
    model evaluation pipelines—updating tests, refining monitoring thresholds, or
    hardening input defenses. For example, if a prompt injection attack bypassed a
    content filter in a generative model, retraining might include adversarially crafted
    prompts, and the prompt validation logic would be updated to reflect newly discovered
    patterns.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复通常涉及重新训练或修补模型。这必须通过安全更新过程进行，使用签名工件、可信构建管道和验证数据。为了防止再次发生，事件应反馈到模型评估管道中——更新测试、细化监控阈值或强化输入防御。例如，如果提示注入攻击绕过了生成模型中的内容过滤器，重新训练可能包括对抗性制作的提示，并且提示验证逻辑将更新以反映新发现的模式。
- en: Finally, organizations should establish post-incident review practices. This
    includes documenting root causes, identifying gaps in detection or response, and
    updating policies and playbooks. Incident reviews help translate operational failures
    into actionable improvements across the design-deploy-monitor lifecycle.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，组织应建立事件后审查实践。这包括记录根本原因、确定检测或响应中的差距，并更新政策和剧本。事件审查有助于将运营失败转化为设计-部署-监控生命周期中的可操作改进。
- en: Hardware Security Foundations
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件安全基础
- en: The software-layer defenses we’ve explored—input validation, output monitoring,
    and integrity checks—establish important protections, but they ultimately depend
    on the underlying hardware and firmware being trustworthy. If an attacker compromises
    the operating system, gains physical access to the device, or exploits vulnerabilities
    in the processor itself, these software defenses can be bypassed or disabled entirely.
    This limitation motivates hardware-based security mechanisms that operate below
    the software layer, creating a hardware root of trust that remains secure even
    when higher-level systems are compromised.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所探讨的软件层防御——输入验证、输出监控和完整性检查——建立了重要的保护措施，但它们最终依赖于底层硬件和固件的可信度。如果攻击者破坏了操作系统、获得了对设备的物理访问或利用了处理器本身的漏洞，这些软件防御可以被绕过或完全禁用。这种限制促使基于硬件的安全机制在软件层以下运行，创建一个即使在更高层系统被破坏时也能保持安全的硬件信任根。
- en: At the foundational level of our defensive framework, hardware-based security
    mechanisms provide the trust anchor for all higher-layer protections. Machine
    learning systems deployed in edge devices, embedded systems, and untrusted cloud
    infrastructure increasingly rely on hardware-based security features to establish
    this foundation. The hardware acceleration platforms discussed in [Chapter 11](ch017.xhtml#sec-ai-acceleration)—including
    GPUs, TPUs, and specialized ML accelerators—often incorporate these security features
    (secure enclaves, trusted execution environments, hardware cryptographic units),
    while edge deployment scenarios from [Chapter 14](ch020.xhtml#sec-ondevice-learning)
    present unique security challenges.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们防御框架的基础层面，基于硬件的安全机制为所有更高层保护提供信任锚。部署在边缘设备、嵌入式系统和不受信任的云基础设施中的机器学习系统越来越多地依赖于基于硬件的安全功能来建立这个基础。《第11章》中讨论的硬件加速平台——包括GPU、TPU和专门的ML加速器——通常包含这些安全特性（安全区域、可信执行环境、硬件加密单元），而《第14章》中提到的边缘部署场景则提出了独特的安全挑战。
- en: These hardware security mechanisms become particularly crucial when systems
    must meet regulatory compliance requirements. Healthcare ML systems handling protected
    health information under HIPAA must implement “appropriate technical safeguards”
    including access controls and encryption. Systems processing EU citizens’ data
    under GDPR must demonstrate “appropriate technical and organizational measures”
    with privacy by design principles embedded at the hardware level.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统必须满足监管合规要求时，这些硬件安全机制变得尤为重要。在HIPAA下处理受保护健康信息的医疗保健机器学习系统必须实施“适当的技术保障”，包括访问控制和加密。在GDPR下处理欧盟公民数据的系统必须证明“适当的技术和组织措施”，并在硬件级别嵌入设计原则。
- en: 'To understand how hardware security protects ML systems, imagine building a
    secure fortress for your most valuable assets. Each hardware security primitive
    serves a distinct defensive role:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解硬件安全如何保护机器学习系统，想象一下为你的最有价值资产建造一个安全的堡垒。每个硬件安全原语都扮演着独特的防御角色：
- en: 'Table 15.8: **Hardware Security Mechanisms**: Each primitive provides distinct
    defensive capabilities that work together to create comprehensive protection from
    hardware-level threats.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.8：**硬件安全机制**：每个原语提供独特的防御能力，共同作用以创建从硬件级别威胁的综合保护。
- en: '| **Mechanism** | **Fortress Analogy and Function** |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| **机制** | **堡垒类比和功能** |'
- en: '| --- | --- |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Secure Boot** | Functions like a trusted gatekeeper checking credentials
    of everyone entering the fortress at dawn. Before your system runs any code, Secure
    Boot cryptographically verifies that the firmware and operating system haven’t
    been tampered with. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| **安全启动** | 功能像一个受信任的守门人，在黎明时分检查进入堡垒的每个人的凭证。在您的系统运行任何代码之前，安全启动通过密码学验证固件和操作系统是否被篡改。
    |'
- en: '| **Trusted Execution** | Create secure, windowless rooms deep inside the fortress
    where you |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| **可信执行** | 在堡垒深处创建安全、无窗口的房间，您可以在其中 |'
- en: '| **Environments** | handle your most sensitive operations. When your ML model
    processes |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| **环境** | 处理您最敏感的操作。当您的机器学习模型处理 |'
- en: '| **(TEEs)** | private medical data or proprietary algorithms, the TEE isolates
    these computations from the rest of the system. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| **(TEEs)** | 私人医疗数据或专有算法，TEE将这些计算与系统其他部分隔离开来。 |'
- en: '| **Hardware Security** | Serve as specialized, impenetrable vaults designed
    specifically for |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| **硬件安全** | 作为专门、不可穿透的保险库，专为 |'
- en: '| **Modules (HSMs)** | storing and using your most valuable cryptographic keys.
    Rather than keeping encryption keys in regular computer memory where they might
    be stolen, HSMs provide tamper-resistant storage. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| **模块（HSMs）** | 存储和使用您最宝贵的密码学密钥。而不是将加密密钥保存在普通计算机内存中，可能会被盗，HSMs提供防篡改存储。 |'
- en: '| **Physical** | Give each device a unique biometric fingerprint at the silicon
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| **物理** | 在硅芯片上为每个设备提供一个独特的生物识别指纹 |'
- en: '| **Unclonable** | level. Just as human fingerprints cannot be perfectly replicated,
    |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| **不可克隆** | 级别。正如人类指纹无法完美复制一样， |'
- en: '| **Functions (PUFs)** | PUFs exploit tiny manufacturing variations in each
    chip to create device-unique identifiers that cannot be cloned. |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| **功能（PUFs）** | PUFs利用每个芯片的微小制造差异来创建无法克隆的设备唯一标识符。 |'
- en: These mechanisms work together to create comprehensive protection that begins
    in hardware and extends through all software layers.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制共同作用，在硬件开始并在所有软件层扩展，以创建综合保护。
- en: 'This section explores how these four complementary hardware primitives work
    together to create comprehensive protection ([Table 15.8](ch021.xhtml#tbl-hardware-security-mechanisms)).
    Each mechanism addresses different security challenges but works most effectively
    when combined: secure boot establishes initial trust, TEEs provide runtime isolation,
    HSMs handle cryptographic operations, and PUFs enable device-unique authentication.
    We begin with Trusted Execution Environments (TEEs), which provide isolated runtime
    environments for sensitive computations. Secure Boot ensures system integrity
    from power-on, creating the trusted foundation that TEEs depend upon. Hardware
    Security Modules (HSMs) offer specialized cryptographic processing and tamper-resistant
    key storage, often required for regulatory compliance. Finally, Physical Unclonable
    Functions (PUFs) provide device-unique identities that enable lightweight authentication
    and cannot be cloned or extracted.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了这四个互补的硬件原语如何协同工作以创建综合保护（[表15.8](ch021.xhtml#tbl-hardware-security-mechanisms)）。每个机制解决不同的安全挑战，但结合使用时效果最佳：安全启动建立初始信任，TEE提供运行时隔离，HSM处理密码学操作，PUF实现设备唯一认证。我们首先从可信执行环境（TEEs）开始，它们为敏感计算提供隔离的运行时环境。安全启动确保系统从电源开启时的完整性，为TEE依赖的信任基础。硬件安全模块（HSMs）提供专门的密码学处理和防篡改密钥存储，通常符合监管要求。最后，物理不可克隆函数（PUFs）提供设备唯一的身份标识，实现轻量级认证且无法被克隆或提取。
- en: Each mechanism addresses different aspects of the security challenge, working
    most effectively when deployed together across hardware, firmware, and software
    boundaries.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机制解决安全挑战的不同方面，当在硬件、固件和软件边界上共同部署时，效果最为显著。
- en: Hardware-Software Co-Design
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件-软件协同设计
- en: Modern ML systems require holistic analysis of security trade-offs across the
    entire hardware-software stack, similar to how we analyze compute-memory-energy
    trade-offs in performance optimization. The interdependence between hardware security
    features and software defenses creates both opportunities and constraints that
    must be understood quantitatively.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习系统需要对整个硬件-软件堆栈中的安全权衡进行整体分析，类似于我们在性能优化中分析计算-内存-能量权衡的方式。硬件安全特性和软件防御之间的相互依赖性既创造了机会，也带来了必须定量理解的限制。
- en: Hardware security mechanisms introduce measurable overhead that must be factored
    into system design. ARM TrustZone world-switching adds approximately 300-1000
    cycles depending on processor generation and cache state (0.6-2.0μs at 500MHz)
    of latency per transition between secure and non-secure worlds. Cryptographic
    operations in secure mode typically consume 15-30% additional power compared to
    normal execution, impacting battery life in mobile ML applications. Intel SGX
    context switching imposes 15-30μs overhead per inference, representing 2% energy
    overhead for typical edge ML workloads.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件安全机制引入了可测量的开销，这些开销必须纳入系统设计中。ARM TrustZone世界切换根据处理器代和缓存状态（在500MHz时为0.6-2.0μs）大约增加300-1000个周期，每切换一次安全和非安全世界之间的延迟。在安全模式下进行的加密操作通常比正常执行多消耗15-30%的电力，这会影响移动机器学习应用中的电池寿命。Intel
    SGX上下文切换为每次推理增加15-30μs的开销，对于典型的边缘机器学习工作负载，这代表着2%的能量开销。
- en: Security features scale differently than computational resources. TEE memory
    limitations constrain model size regardless of available system memory. A quantized
    ResNet-18 model (47MB) can operate within ARM TrustZone constraints, while ResNet-50
    (176MB) requires careful memory management or model partitioning. These constraints
    create architectural decisions that must be made early in system design.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 安全特性与计算资源扩展的方式不同。TEE内存限制无论系统内存可用与否都会约束模型大小。一个量化的ResNet-18模型（47MB）可以在ARM TrustZone约束内运行，而ResNet-50（176MB）则需要仔细的内存管理或模型分区。这些限制在系统设计早期就必须做出架构决策。
- en: Different threat models and protection levels require quantitative trade-off
    analysis. For ML workloads requiring cryptographic verification, AES-256 operations
    add 0.1-0.5ms per inference depending on model size and hardware acceleration
    availability. Homomorphic encryption operations impose 100-100,000x computational
    overhead, with fully homomorphic encryption (FHE) at the higher end and somewhat
    homomorphic encryption (SHE) at the lower end, making them viable only for small
    models or offline scenarios where strong privacy guarantees justify the performance
    cost.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的威胁模型和防护级别需要定量权衡分析。对于需要加密验证的机器学习工作负载，AES-256操作根据模型大小和硬件加速的可用性，每推理增加0.1-0.5ms。同态加密操作带来100-100,000倍的计算开销，完全同态加密（FHE）在高端，而某种同态加密（SHE）在低端，这使得它们仅适用于小型模型或离线场景，在这些场景中，强大的隐私保证可以证明性能成本是合理的。
- en: Trusted Execution Environments
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可信执行环境
- en: A Trusted Execution Environment (TEE)[52](#fn52) is a hardware-isolated region
    within a processor designed to protect sensitive computations and data from potentially
    compromised software. TEEs enforce confidentiality, integrity, and runtime isolation,
    ensuring that even if the host operating system or application layer is attacked,
    sensitive operations within the TEE remain secure.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 可信执行环境（TEE）[52](#fn52)是处理器内部的一个硬件隔离区域，旨在保护敏感的计算和数据免受可能受损的软件的威胁。TEE强制执行机密性、完整性和运行时隔离，确保即使主机操作系统或应用层遭到攻击，TEE内的敏感操作仍然保持安全。
- en: In the context of machine learning, TEEs are increasingly important for preserving
    the confidentiality of models, securing sensitive user data during inference,
    and ensuring that model outputs remain trustworthy. For example, a TEE can protect
    model parameters from being extracted by malicious software running on the same
    device, or ensure that computations involving biometric inputs, including facial
    data or fingerprint data, are performed securely. This capability is essential
    in applications where model integrity, user privacy, or regulatory compliance
    are non-negotiable.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，TEE（可信执行环境）对于保护模型机密性、在推理过程中保护敏感用户数据以及确保模型输出保持可信性变得越来越重要。例如，TEE可以保护模型参数不被同一设备上运行的恶意软件提取，或者确保涉及生物识别输入的计算，包括面部数据或指纹数据，都是安全进行的。这种能力对于模型完整性、用户隐私或合规性不可协商的应用至关重要。
- en: One widely deployed example is [Apple’s Secure Enclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web),
    which provides isolated execution and secure key storage for iOS devices. By separating
    cryptographic operations and biometric data from the main processor, the Secure
    Enclave ensures that user credentials and Face ID features remain protected, even
    in the event of a broader system compromise.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广泛部署的例子是[苹果的Secure Enclave](https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web)，它为iOS设备提供隔离执行和安全密钥存储。通过将加密操作和生物识别数据与主处理器分离，Secure
    Enclave确保即使在更广泛的系统妥协事件中，用户凭据和Face ID功能也保持受保护。
- en: Trusted Execution Environments are important across a range of industries with
    high security requirements. In telecommunications, TEEs are used to safeguard
    encryption keys and secure important 5G control-plane operations. In finance,
    they allow secure mobile payments and protect PIN-based authentication workflows.
    In healthcare, TEEs help enforce patient data confidentiality during edge-based
    ML inference on wearable or diagnostic devices. In the automotive industry, they
    are deployed in advanced driver-assistance systems (ADAS) to ensure that safety-important
    perception and decision-making modules operate on verified software.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 可信执行环境（TEE）在众多对安全性要求高的行业中都至关重要。在电信领域，TEE用于保护加密密钥和确保重要的5G控制平面操作的安全。在金融领域，它们允许安全的移动支付并保护基于PIN的认证工作流程。在医疗保健领域，TEE有助于在可穿戴或诊断设备上的边缘机器学习推理过程中强制执行患者数据保密性。在汽车行业中，它们被部署在高级驾驶辅助系统（ADAS）中，以确保与安全相关的感知和决策模块在经过验证的软件上运行。
- en: In machine learning systems, TEEs can provide several important protections.
    They secure the execution of model inference or training, shielding intermediate
    computations and final predictions from system-level observation. They protect
    the confidentiality of sensitive inputs, including biometric or clinical signals,
    used in personal identification or risk scoring tasks. TEEs also serve to prevent
    reverse engineering of deployed models by restricting access to weights and architecture
    internals. When models are updated, TEEs ensure the authenticity of new parameters
    and block unauthorized tampering. In distributed ML settings, TEEs can protect
    data exchanged between components by enabling encrypted and attested communication
    channels.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，TEE可以提供几种重要的保护。它们保护模型推理或训练的执行，屏蔽中间计算和最终预测免受系统级观察。它们保护敏感输入的机密性，包括用于个人识别或风险评估任务中的生物识别或临床信号。TEE还通过限制对权重和架构内部的访问来防止已部署模型的逆向工程。当模型更新时，TEE确保新参数的真实性并阻止未经授权的篡改。在分布式机器学习设置中，TEE可以通过启用加密和证明的通信通道来保护组件之间交换的数据。
- en: 'The core security properties of a TEE are achieved through four mechanisms:
    isolated execution, secure storage, integrity protection, and in-TEE data encryption.
    Code that runs inside the TEE is executed in a separate processor mode, inaccessible
    to the normal-world operating system. Sensitive assets such as cryptographic keys
    or authentication tokens are stored in memory that only the TEE can access. Code
    and data can be verified for integrity before execution using hardware-anchored
    hashes or signatures. Finally, data processed inside the TEE can be encrypted,
    ensuring that even intermediate results are inaccessible without appropriate keys,
    which are also managed internally by the TEE.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: TEE的核心安全属性是通过四种机制实现的：隔离执行、安全存储、完整性保护和TEE内部数据加密。在TEE内部运行的代码以单独的处理器模式执行，无法被正常世界的操作系统访问。敏感资产，如加密密钥或认证令牌，存储在只有TEE可以访问的内存中。在执行之前，可以使用硬件锚定的散列或签名来验证代码和数据完整性。最后，TEE内部处理的数据可以加密，确保即使中间结果在没有适当密钥的情况下也无法访问，这些密钥也由TEE内部管理。
- en: Several commercial platforms provide TEE functionality tailored for different
    deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[53](#fn53)
    offers secure and normal world execution on ARM-based systems and is widely used
    in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[54](#fn54)
    implements enclave-based security for cloud and desktop systems, enabling secure
    computation even on untrusted infrastructure. [Qualcomm’s Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions)
    supports secure mobile transactions and user authentication. Apple’s Secure Enclave
    remains a canonical example of a hardware-isolated security coprocessor for consumer
    devices.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 几个商业平台提供了针对不同部署环境定制的TEE功能。[ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[53](#fn53)在基于ARM的系统上提供安全和正常世界的执行，广泛应用于移动和物联网应用。[Intel
    SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[54](#fn54)实现了基于安全区域的安全，为云和桌面系统提供，即使在不受信任的基础设施上也能实现安全计算。[高通的Secure
    Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions)支持安全的移动交易和用户认证。苹果的Secure
    Enclave仍然是消费设备上硬件隔离安全协处理器的典范。
- en: '[Figure 15.15](ch021.xhtml#fig-enclave) illustrates a secure enclave integrated
    into a system-on-chip (SoC) architecture. The enclave includes a dedicated processor,
    an AES engine, a true random number generator (TRNG), a public key accelerator
    (PKA), and a secure I²C interface to nonvolatile storage. These components operate
    in isolation from the main application processor and memory subsystem. A memory
    protection engine enforces access control, while cryptographic operations such
    as NAND flash encryption are handled internally using enclave-managed keys. By
    physically separating secure execution and key management from the main system,
    this architecture limits the impact of system-level compromises and establishes
    hardware-enforced trust.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[图15.15](ch021.xhtml#fig-enclave)展示了集成到片上系统（SoC）架构中的安全区域。该区域包括一个专用处理器、一个AES引擎、一个真正的随机数生成器（TRNG）、一个公钥加速器（PKA）和一个安全的I²C接口，用于非易失性存储。这些组件与主应用程序处理器和内存子系统隔离运行。内存保护引擎强制执行访问控制，而如NAND闪存加密之类的加密操作则使用安全区域管理的密钥在内部处理。通过在物理上将安全执行和密钥管理从主系统分离出来，这种架构限制了系统级破坏的影响，并建立了硬件强制的信任。'
- en: '![](../media/file243.svg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file243.svg)'
- en: 'Figure 15.15: **Secure Enclave Architecture**: Hardware-isolated enclaves enhance
    system security by encapsulating sensitive data and cryptographic operations within
    a dedicated processor and memory. This design minimizes the attack surface and
    protects important keys even if the main application processor is compromised,
    providing a trusted execution environment for security-important tasks. Source:
    Apple.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.15：**安全区域架构**：硬件隔离的安全区域通过在专用处理器和内存中封装敏感数据和加密操作来增强系统安全性。这种设计最小化了攻击面，即使在主应用程序处理器被破坏的情况下，也能保护重要密钥，为安全重要任务提供可信执行环境。来源：苹果。
- en: This architecture underpins the secure deployment of machine learning applications
    on consumer devices. For example, Apple’s Face ID system uses a secure enclave
    to perform facial recognition entirely within a hardware-isolated environment.
    The face embedding model is executed inside the enclave, and biometric templates
    are stored in secure nonvolatile memory accessible only via the enclave’s I²C
    interface. During authentication, input data from the infrared camera is processed
    locally, and no facial features or predictions ever leave the secure region. Even
    if the application processor or operating system is compromised, the enclave prevents
    access to sensitive model inputs, parameters, and outputs—ensuring that biometric
    identity remains protected end to end.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构为在消费设备上安全部署机器学习应用提供了基础。例如，苹果的Face ID系统使用安全区域在硬件隔离环境中执行面部识别。面部嵌入模型在安全区域内执行，生物识别模板存储在安全的非易失性存储器中，该存储器只能通过安全区域的I²C接口访问。在认证过程中，来自红外摄像头的输入数据在本地进行处理，面部特征或预测永远不会离开安全区域。即使应用程序处理器或操作系统被破坏，安全区域也能防止对敏感模型输入、参数和输出的访问，确保生物识别身份从端到端得到保护。
- en: Despite their strengths, Trusted Execution Environments come with notable trade-offs.
    Implementing a TEE increases both direct hardware costs and indirect costs associated
    with developing and maintaining secure software. Integrating TEEs into existing
    systems may require architectural redesigns, especially for legacy infrastructure.
    Developers must adhere to strict protocols for isolation, attestation, and secure
    update management, which can extend development cycles and complicate testing
    workflows. TEEs can also introduce performance overhead, particularly when cryptographic
    operations are involved, or when context switching between trusted and untrusted
    modes is frequent.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有优势，但可信执行环境也伴随着显著的权衡。实现TEE会增加直接硬件成本和与开发及维护安全软件相关的间接成本。将TEE集成到现有系统中可能需要架构重设计，尤其是对于遗留基础设施。开发者必须遵守严格的隔离、证明和安全的更新管理协议，这可能会延长开发周期并复杂化测试工作流程。TEE还可能引入性能开销，尤其是在涉及密码学操作或频繁在可信和不可信模式之间切换时。
- en: Energy efficiency is another consideration, particularly in battery-constrained
    devices. TEEs typically consume additional power due to secure memory accesses,
    cryptographic computation, and hardware protection logic. In resource-limited
    embedded systems, these costs may limit their use. In terms of scalability and
    flexibility, the secure boundaries enforced by TEEs may complicate distributed
    training or federated inference workloads, where secure coordination between enclaves
    is required.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 能效也是另一个考虑因素，尤其是在电池受限的设备中。TEE通常由于安全的内存访问、密码学计算和硬件保护逻辑而消耗额外的电力。在资源有限的嵌入式系统中，这些成本可能会限制其使用。在可扩展性和灵活性方面，TEE强制执行的secure
    boundaries可能会使分布式训练或联邦推理工作负载复杂化，在这些工作负载中需要安全地协调enclaves（安全区域）。
- en: Market demand also varies. In some consumer applications, perceived threat levels
    may be too low to justify the integration of TEEs. Systems with TEEs may be subject
    to formal security certifications, such as [Common Criteria](https://www.commoncriteriaportal.org/ccra/index.cfm)
    or evaluation under [ENISA](https://www.enisa.europa.eu/), which can introduce
    additional time and expense. For this reason, TEEs are typically adopted only
    when the expected threat model, including adversarial users, cloud tenants, and
    malicious insiders, justifies the investment.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 市场需求也各不相同。在某些消费应用中，感知到的威胁水平可能过低，不足以证明集成TEE（可信执行环境）的必要性。具有TEE的系统可能需要正式的安全认证，例如[通用标准](https://www.commoncriteriaportal.org/ccra/index.cfm)或[ENISA](https://www.enisa.europa.eu/)的评估，这可能会带来额外的时间和费用。因此，TEE通常仅在预期的威胁模型（包括对抗性用户、云租户和恶意内部人员）证明投资合理时才被采用。
- en: Nonetheless, TEEs remain a powerful hardware primitive in the machine learning
    security landscape. When paired with software- and system-level defenses, they
    provide a trusted foundation for executing ML models securely, privately, and
    verifiably, especially in scenarios where adversarial compromise of the host environment
    is a serious concern.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，TEE在机器学习安全领域仍然是一个强大的硬件原语。当与软件和系统级别的防御措施结合使用时，它们为安全、私密和可验证地执行ML模型提供了一个可信的基础，尤其是在主机环境可能遭受对抗性破坏的严重关注场景中。
- en: Here is the revised 7.5.2 Secure Boot section, rewritten in formal textbook
    tone with all original technical content, hyperlinks, and figures preserved. The
    structure emphasizes narrative clarity, avoids bullet lists, and integrates the
    Apple Face ID case study naturally.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是修订后的7.5.2安全启动部分，以正式教科书风格重写，保留了所有原始技术内容、超链接和图表。结构强调叙事清晰，避免使用项目符号列表，并自然地整合了苹果Face
    ID案例研究。
- en: Secure Boot
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全启动
- en: Secure Boot is a mechanism that ensures a device only boots software components
    that are cryptographically verified and explicitly authorized by the manufacturer.
    At startup, each stage of the boot process, comprising the bootloader, kernel,
    and base operating system, is checked against a known-good digital signature.
    If any signature fails verification, the boot sequence is halted, preventing unauthorized
    or malicious code from executing. This chain-of-trust model establishes system
    integrity from the very first instruction executed.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 安全启动是一种机制，确保设备仅启动由制造商通过密码学验证并明确授权的软件组件。在启动过程中，引导加载程序、内核和基本操作系统等启动过程的每个阶段都会与已知的良好数字签名进行核对。如果任何签名验证失败，启动序列将被终止，防止未经授权或恶意代码执行。这种信任链模型从执行的第一条指令开始就建立了系统完整性。
- en: In ML systems, especially those deployed on embedded or edge hardware, Secure
    Boot plays an important role. A compromised boot process may result in malicious
    software loading before the ML runtime begins, enabling attackers to intercept
    model weights, tamper with training data, or reroute inference results. Such breaches
    can lead to incorrect or manipulated predictions, unauthorized data access, or
    device repurposing for botnets or crypto-mining.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，尤其是在部署在嵌入式或边缘硬件上的系统中，安全启动发挥着重要作用。受损的启动过程可能导致在机器学习运行时开始之前加载恶意软件，使攻击者能够拦截模型权重、篡改训练数据或重定向推理结果。此类违规行为可能导致预测错误或被操纵，未经授权的数据访问，或设备被重新用于僵尸网络或加密挖矿。
- en: For machine learning systems, Secure Boot offers several guarantees. First,
    it protects model-related data, such as training data, inference inputs, and outputs,
    during the boot sequence, preventing pre-runtime tampering. Second, it ensures
    that only authenticated model binaries and supporting software are loaded, which
    helps guard against deployment-time model substitution. Third, Secure Boot allows
    secure model updates by verifying that firmware or model changes are signed and
    have not been altered in transit.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习系统，安全启动提供了多项保证。首先，它在启动序列中保护与模型相关的数据，例如训练数据、推理输入和输出，防止在运行前被篡改。其次，它确保仅加载经过身份验证的模型二进制文件和支持软件，这有助于防止部署时的模型替换。第三，安全启动通过验证固件或模型更改是否已签名且在传输过程中未被篡改，允许安全地更新模型。
- en: Secure Boot frequently works in tandem with hardware-based Trusted Execution
    Environments (TEEs) to create a fully trusted execution stack. As shown in [Figure 15.16](ch021.xhtml#fig-secure-boot),
    this layered boot process verifies firmware, operating system components, and
    TEE integrity before permitting execution of cryptographic operations or ML workloads.
    In embedded systems, this architecture provides resilience even under severe adversarial
    conditions or physical device compromise.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 安全启动通常与基于硬件的信任执行环境（TEEs）协同工作，以创建一个完全信任的执行堆栈。如图15.16所示，这种分层的启动过程在允许执行加密操作或机器学习工作负载之前，验证固件、操作系统组件和TEEs的完整性。在嵌入式系统中，这种架构即使在严重对抗条件或物理设备受损的情况下也能提供弹性。
- en: '![](../media/file244.svg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file244.svg)'
- en: 'Figure 15.16: **Secure Boot Sequence**: Embedded systems employ a layered boot
    process to verify firmware and software integrity, establishing a root of trust
    before executing machine learning workloads and protecting against pre-runtime
    attacks. This architecture ensures only authenticated code runs, safeguarding
    model data and preventing unauthorized model substitution or modification during
    deployment. Source: ([R. V. and A. 2018](ch058.xhtml#ref-rashmi2018secure)).'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.16：**安全启动序列**：嵌入式系统采用分层启动过程来验证固件和软件完整性，在执行机器学习工作负载之前建立信任根，并保护免受运行前攻击。这种架构确保仅运行经过身份验证的代码，保护模型数据，并在部署期间防止未经授权的模型替换或修改。来源：([R.
    V. 和 A. 2018](ch058.xhtml#ref-rashmi2018secure))。
- en: A well-known real-world implementation of Secure Boot appears in Apple’s Face
    ID system, which uses advanced machine learning for facial recognition. For Face
    ID to operate securely, the entire device stack, from the initial power-on to
    the execution of the model, must be verifiably trusted.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 安全启动的一个著名的现实世界实现出现在苹果的Face ID系统中，该系统使用高级机器学习进行面部识别。为了使Face ID安全运行，从初始开机到执行模型，整个设备堆栈必须可验证地信任。
- en: Upon device startup, Secure Boot initiates within Apple’s [Secure Enclave](https://support.apple.com/en-us/102381),
    a dedicated security coprocessor that handles biometric data. The firmware loaded
    onto the Secure Enclave is digitally signed by Apple, and any unauthorized modification
    causes the boot process to fail. Once verified, the Secure Enclave performs continuous
    checks in coordination with the central processor to maintain a trusted boot chain.
    Each system component, ranging from the iOS kernel to the application-level code,
    is verified using cryptographic signatures.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备启动时，安全启动在苹果的[安全 enclave](https://support.apple.com/en-us/102381)中启动，这是一个专门的安全协处理器，用于处理生物识别数据。加载到安全
    enclave 的固件由苹果数字签名，任何未经授权的修改都会导致启动过程失败。一旦验证通过，安全 enclave 将与中央处理器协同进行持续检查，以维护一个信任的启动链。每个系统组件，从iOS内核到应用级代码，都使用加密签名进行验证。
- en: After completing the secure boot sequence, the Secure Enclave activates the
    ML-based Face ID system. The facial recognition model projects over 30,000 infrared
    points to map a user’s face, generating a depth image and computing a mathematical
    representation that is compared against a securely stored profile. These facial
    data artifacts are never written to disk, transmitted off-device, or shared externally.
    All processing occurs within the enclave to protect against eavesdropping or exfiltration,
    even in the presence of a compromised kernel.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成安全引导序列后，Secure Enclave 激活基于机器学习的 Face ID 系统。面部识别模型投射超过 30,000 个红外点来映射用户的面部，生成深度图像并计算一个数学表示，该表示与安全存储的配置文件进行比较。这些面部数据工件永远不会写入磁盘、从设备传输或外部共享。所有处理都在安全区域内进行，以防止窃听或数据泄露，即使在内核受损的情况下也是如此。
- en: To support continued integrity, Secure Boot also governs software updates. Only
    firmware or model updates signed by Apple are accepted, ensuring that even over-the-air
    patches do not introduce risk. This process maintains a robust chain of trust
    over time, enabling the secure evolution of the ML system while preserving user
    privacy and device security.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持持续的完整性，Secure Boot 还管理软件更新。只有由苹果公司签名的固件或模型更新才被接受，确保即使空中补丁也不会引入风险。这个过程随着时间的推移维护了一个强大的信任链，使机器学习系统的安全演变成为可能，同时保护用户隐私和设备安全。
- en: While Secure Boot provides strong protection, its adoption presents technical
    and operational challenges. Managing the cryptographic keys used to sign and verify
    system components is complex, especially at scale. Enterprises must securely provision,
    rotate, and revoke keys, ensuring that no trusted root is compromised. Any such
    breach would undermine the entire security chain.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Secure Boot 提供了强大的保护，但其采用也带来了技术和操作上的挑战。管理用于签名和验证系统组件的加密密钥是复杂的，尤其是在规模较大的情况下。企业必须安全地提供、轮换和吊销密钥，确保没有可信根被破坏。任何此类违规行为都会破坏整个安全链。
- en: Performance is also a consideration. Verifying signatures during the boot process
    introduces latency, typically on the order of tens to hundreds of milliseconds
    per component. Although acceptable in many applications, these delays may be problematic
    for real-time or power-constrained systems. Developers must also ensure that all
    components, including bootloaders, firmware, kernels, drivers, and even ML models,
    are correctly signed. Integrating third-party software into a Secure Boot pipeline
    introduces additional complexity.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 性能也是一个考虑因素。在引导过程中验证签名引入了延迟，通常每个组件的延迟在数十到数百毫秒之间。尽管在许多应用中可以接受，但这些延迟对于实时或功耗受限的系统可能成为问题。开发者还必须确保所有组件，包括引导加载程序、固件、内核、驱动程序，甚至机器学习模型，都正确签名。将第三方软件集成到
    Secure Boot 管道中引入了额外的复杂性。
- en: Some systems limit user control in favor of vendor-locked security models, restricting
    upgradability or customization. In response, open-source bootloaders like [u-boot](https://source.denx.de/u-boot/u-boot)
    and [coreboot](https://www.coreboot.org/) have emerged, offering Secure Boot features
    while supporting extensibility and transparency. To further scale trusted device
    deployments, emerging industry standards such as the [Device Identifier Composition
    Engine (DICE)](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/)
    and [IEEE 802.1AR IDevID](https://1.ieee802.org/security/802-1ar/) provide mechanisms
    for secure device identity, key provisioning, and cross-vendor trust assurance.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统为了优先考虑供应商锁定安全模型而限制了用户控制，限制了可升级性或定制性。作为回应，开源引导加载程序如 [u-boot](https://source.denx.de/u-boot/u-boot)
    和 [coreboot](https://www.coreboot.org/) 应运而生，提供了 Secure Boot 功能，同时支持可扩展性和透明度。为了进一步扩展可信设备部署，新兴的行业标准，如
    [设备标识符组合引擎 (DICE)](https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/)
    和 [IEEE 802.1AR IDevID](https://1.ieee802.org/security/802-1ar/)，提供了安全设备标识、密钥提供和跨供应商信任保证的机制。
- en: Secure Boot, when implemented carefully and complemented by trusted hardware
    and secure software update processes, forms the backbone of system integrity for
    embedded and distributed ML. It provides the assurance that the machine learning
    model running in production is not only the correct version, but is also executing
    in a known-good environment, anchored to hardware-level trust.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Secure Boot 被谨慎实施，并辅以可信硬件和安全的软件更新流程时，它构成了嵌入式和分布式机器学习系统完整性的基石。它确保在生产环境中运行的机器学习模型不仅是正确的版本，而且是在一个已知良好的环境中执行，并基于硬件级别的信任。
- en: Hardware Security Modules
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件安全模块
- en: While TEEs and secure boot provide runtime isolation and integrity verification,
    Hardware Security Modules (HSMs) specialize in the cryptographic operations that
    underpin these protections. An HSM[55](#fn55) is a tamper-resistant physical device
    designed to perform cryptographic operations and securely manage digital keys.
    HSMs are widely used across security-important industries such as finance, defense,
    and cloud infrastructure, and they are increasingly relevant for securing the
    machine learning pipeline—particularly in deployments where key confidentiality,
    model integrity, and regulatory compliance are important.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TEE和安全的引导提供了运行时隔离和完整性验证，但硬件安全模块（HSMs）专门从事支撑这些保护的基础加密操作。HSM[55](#fn55)是一种抗篡改的物理设备，旨在执行加密操作并安全地管理数字密钥。HSM在金融、国防和云基础设施等安全重要的行业中得到广泛应用，并且对于确保机器学习管道的安全性越来越重要——特别是在关键保密性、模型完整性和法规遵从性重要的部署中。
- en: HSMs provide an isolated, hardened environment for performing sensitive operations
    such as key generation, digital signing, encryption, and decryption. Unlike general-purpose
    processors, they are engineered to withstand physical tampering and side-channel
    attacks, and they typically include protected storage, cryptographic accelerators,
    and internal audit logging. HSMs may be implemented as standalone appliances,
    plug-in modules, or integrated chips embedded within broader systems.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: HSMs提供了一种隔离、加固的环境，用于执行敏感操作，如密钥生成、数字签名、加密和解密。与通用处理器不同，它们被设计成能够抵御物理篡改和侧信道攻击，并且通常包括受保护存储、加密加速器和内部审计日志。HSMs可以作为独立设备、插件模块或嵌入更广泛系统中的集成芯片来实现。
- en: In machine learning systems, HSMs enhance security across several dimensions.
    They are commonly used to protect encryption keys associated with sensitive data
    that may be processed during training or inference. These keys might encrypt data
    at rest in model checkpoints or allow secure transmission of inference requests
    across networked environments. By ensuring that the keys are generated, stored,
    and used exclusively within the HSM, the system minimizes the risk of key leakage,
    unauthorized reuse, or tampering.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，HSMs在多个维度上增强了安全性。它们通常用于保护在训练或推理过程中可能处理的敏感数据的加密密钥。这些密钥可能加密模型检查点中的静态数据，或允许在网络环境中安全地传输推理请求。通过确保密钥仅在HSM内部生成、存储和使用，系统最大限度地降低了密钥泄露、未经授权的重用或篡改的风险。
- en: HSMs also play a role in maintaining the integrity of machine learning models.
    In many production pipelines, models must be signed before deployment to ensure
    that only verified versions are accepted into runtime environments. The signing
    keys used to authenticate models can be stored and managed within the HSM, providing
    cryptographic assurance that the deployed artifact is authentic and untampered.
    Similarly, secure firmware updates and configuration changes, regardless of whether
    they pertain to models, hyperparameters, or supporting infrastructure, can be
    validated using signatures produced by the HSM.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: HSMs还在维护机器学习模型的完整性方面发挥作用。在许多生产管道中，模型在部署之前必须进行签名，以确保只有经过验证的版本被接受到运行时环境中。用于验证模型的签名密钥可以存储和管理在HSM中，提供加密保证，确保部署的工件是真实的且未被篡改。同样，无论它们是否与模型、超参数或支持基础设施相关，都可以使用HSM生成的签名来验证安全的固件更新和配置更改。
- en: In addition to protecting inference workloads, HSMs can be used to secure model
    training. During training, data may originate from distributed and potentially
    untrusted sources. HSM-backed protocols can help ensure that training pipelines
    perform encryption, integrity checks, and access control enforcement securely
    and in compliance with organizational or legal requirements. In regulated industries
    such as healthcare and finance, such protections are often mandatory. For instance,
    HIPAA requires covered entities to implement technical safeguards including “integrity
    controls” and “encryption and decryption,” while GDPR mandates pseudonymization
    and encryption as examples of appropriate technical measures.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 除了保护推理工作负载外，HSMs还可以用于确保模型训练的安全性。在训练过程中，数据可能来自分布式和可能不可信的来源。基于HSM的协议可以帮助确保训练管道安全地执行加密、完整性检查和访问控制执行，并符合组织或法律要求。在医疗保健和金融等受监管行业，此类保护通常是强制性的。例如，HIPAA要求受保护实体实施包括“完整性控制”和“加密和解密”在内的技术保障措施，而GDPR则将匿名化和加密作为适当技术措施的例子。
- en: Despite these benefits, incorporating HSMs into embedded or resource-constrained
    ML systems introduces several trade-offs. First, HSMs are specialized hardware
    components and often come at a premium. Their cost may be justified in data center
    settings or safety-important applications but can be prohibitive for low-margin
    embedded products or wearables. Physical space is also a concern. Embedded systems
    often operate under strict size, weight, and form factor constraints, and integrating
    an HSM may require redesigning circuit layouts or sacrificing other functionality.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些好处，将HSMs集成到嵌入式或资源受限的ML系统中会引入一些权衡。首先，HSMs是专门的硬件组件，通常价格较高。在数据中心设置或安全至关重要的应用中，其成本可能是合理的，但对于低利润的嵌入式产品或可穿戴设备来说可能是难以承受的。物理空间也是一个问题。嵌入式系统通常在严格的大小、重量和外形因素限制下运行，集成HSM可能需要重新设计电路布局或牺牲其他功能。
- en: From a performance standpoint, HSMs introduce latency, particularly for operations
    like key exchange, signature verification, or on-the-fly decryption. In real-time
    inference systems, including autonomous vehicles, industrial robotics, and live
    translation devices, these delays can affect responsiveness. While HSMs are typically
    optimized for cryptographic throughput, they are not general-purpose processors,
    and offloading secure operations must be carefully coordinated.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，HSMs引入了延迟，尤其是在密钥交换、签名验证或即时解密等操作中。在实时推理系统中，包括自动驾驶汽车、工业机器人和实时翻译设备，这些延迟可能会影响响应速度。虽然HSMs通常针对加密吞吐量进行优化，但它们不是通用处理器，并且安全操作的分担必须仔细协调。
- en: Power consumption is another concern. The continuous secure handling of keys,
    signing of transactions, and cryptographic validations can consume more power
    than basic embedded components, impacting battery life in mobile or remote deployments.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 功耗也是一个关注点。持续安全地处理密钥、签署交易和进行加密验证可能会消耗比基本嵌入式组件更多的电力，影响移动或远程部署的电池寿命。
- en: Integration complexity also grows when HSMs are introduced into existing ML
    pipelines. Interfacing between the HSM and the host processor requires dedicated
    APIs and often specialized software development. Firmware and model updates must
    be routed through secure, signed channels, and update orchestration must account
    for device-specific key provisioning. These requirements increase the operational
    burden, especially in large deployments.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 当HSMs被引入现有的ML管道时，集成复杂性也会增加。HSM与主机处理器之间的接口需要专用API，并且通常需要专门的软件开发。固件和模型更新必须通过安全、签名的通道进行路由，并且更新编排必须考虑到设备特定的密钥配置。这些要求增加了操作负担，尤其是在大规模部署中。
- en: Scalability presents its own set of challenges. Managing a distributed fleet
    of HSM-equipped devices requires secure provisioning of individual keys, secure
    identity binding, and coordinated trust management. In large ML deployments, including
    fleets of smart sensors or edge inference nodes, ensuring uniform security posture
    across all devices is nontrivial.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性也带来了一组挑战。管理配备HSM的分布式设备群需要为单个密钥提供安全配置、安全身份绑定和协调的信任管理。在大规模ML部署中，包括智能传感器或边缘推理节点群，确保所有设备都保持一致的安全态势并非易事。
- en: Finally, the use of HSMs often requires organizations to engage in certification
    and compliance processes[56](#fn56), particularly when handling regulated data.
    Meeting standards such as FIPS 140-2[57](#fn57) or Common Criteria adds time and
    cost to development.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用HSMs通常要求组织参与认证和合规流程[56](#fn56)，尤其是在处理受监管数据时。满足FIPS 140-2[57](#fn57)或通用标准等标准会增加开发的时间和成本。
- en: Despite these operational complexities, HSMs remain a valuable option for machine
    learning systems that require high assurance of cryptographic integrity and access
    control. When paired with TEEs, secure boot, and software-based defenses, HSMs
    contribute to a multilayered security model that spans hardware, system software,
    and ML runtime.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些操作复杂性，但HSMs（硬件安全模块）仍然是对于需要高度保证加密完整性和访问控制的安全机器学习系统的宝贵选择。当与TEE（可信执行环境）、安全启动和基于软件的防御措施结合使用时，HSMs有助于构建一个跨越硬件、系统软件和ML运行时的多层安全模型。
- en: Physical Unclonable Functions
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 物理不可克隆功能
- en: Physical Unclonable Functions (PUFs)[58](#fn58) provide a hardware-intrinsic
    mechanism for cryptographic key generation and device authentication by leveraging
    physical randomness in semiconductor fabrication ([Gassend et al. 2002](ch058.xhtml#ref-gassend2002silicon)).
    Unlike traditional keys stored in memory, a PUF generates secret values based
    on microscopic variations in a chip’s physical properties—variations that are
    inherent to manufacturing processes and difficult to clone or predict, even by
    the manufacturer.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 物理不可克隆函数（PUFs）[58](#fn58)通过利用半导体制造中的物理随机性，提供了一种硬件固有的机制，用于加密密钥生成和设备身份验证（[Gassend等人，2002](ch058.xhtml#ref-gassend2002silicon)）。与存储在内存中的传统密钥不同，PUF基于芯片物理属性的微观变化生成秘密值——这些变化是制造过程固有的，难以复制或预测，即使是制造商也无法做到。
- en: These variations arise from uncontrollable physical factors such as doping concentration,
    line edge roughness, and dielectric thickness. As a result, even chips fabricated
    with the same design masks exhibit small but measurable differences in timing,
    power consumption, or voltage behavior. PUF circuits amplify these variations
    to produce a device-unique digital output. When a specific input challenge is
    applied to a PUF, it generates a corresponding response based on the chip’s physical
    fingerprint. Because these characteristics are effectively impossible to replicate,
    the same challenge will yield different responses across devices.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化源于不可控的物理因素，如掺杂浓度、线边缘粗糙度和介电层厚度。因此，即使使用相同的设计掩模制造的芯片，在时序、功耗或电压行为上也会表现出微小但可测量的差异。PUF电路放大这些差异以产生设备独特的数字输出。当对PUF应用特定的输入挑战时，它会根据芯片的物理指纹生成相应的响应。因为这些特性实际上无法复制，所以相同的挑战在不同设备上会产生不同的响应。
- en: This challenge-response mechanism allows PUFs to serve several cryptographic
    purposes. They can be used to derive device-specific keys that never need to be
    stored externally, reducing the attack surface for key exfiltration. The same
    mechanism also supports secure authentication and attestation, where devices must
    prove their identity to trusted servers or hardware gateways. These properties
    make PUFs a natural fit for machine learning systems deployed in embedded and
    distributed environments.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这种挑战-响应机制允许PUF服务于多个加密目的。它们可以用来推导出特定于设备的密钥，这些密钥无需存储在外部，从而减少了密钥泄露的攻击面。同样的机制也支持安全的身份验证和证明，其中设备必须向受信任的服务器或硬件网关证明其身份。这些特性使PUF成为嵌入式和分布式环境中部署的机器学习系统的理想选择。
- en: In ML applications, PUFs offer unique advantages for securing resource-constrained
    systems. For example, consider a smart camera drone that uses onboard computer
    vision to track objects. A PUF embedded in the drone’s processor can generate
    a private key to encrypt the model during boot. Even if the model were extracted,
    it would be unusable on another device lacking the same PUF response. That same
    PUF-derived key could also be used to watermark the model parameters, creating
    a cryptographically verifiable link between a deployed model and its origin hardware.
    If the model were leaked or pirated, the embedded watermark could help prove the
    source of the compromise.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习应用中，PUF为保护资源受限的系统提供了独特的优势。例如，考虑一个使用机载计算机视觉跟踪物体的智能相机无人机。嵌入在无人机处理器中的PUF可以生成一个私钥，在启动时加密模型。即使模型被提取，在没有相同PUF响应的另一个设备上也无法使用。同样的PUF派生的密钥也可以用来水印模型参数，在部署的模型与其原始硬件之间创建一个可加密验证的链接。如果模型被泄露或盗版，嵌入的水印可以帮助证明攻击的来源。
- en: PUFs also support authentication in distributed ML pipelines. If the drone offloads
    computation to a cloud server, the PUF can help verify that the drone has not
    been cloned or tampered with. The cloud backend can issue a challenge, verify
    the correct response from the device, and permit access only if the PUF proves
    device authenticity. These protections enhance trust not only in the model and
    data, but in the execution environment itself.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: PUFs（物理不可克隆函数）也支持在分布式机器学习管道中进行身份验证。如果无人机将计算任务卸载到云服务器，PUF可以帮助验证无人机没有被克隆或篡改。云后端可以发出挑战，验证设备返回的正确响应，并且只有当PUF证明设备真实性时才允许访问。这些保护措施不仅增强了人们对模型和数据的信任，也增强了人们对执行环境的信任。
- en: 'The internal operation of a PUF is illustrated in [Figure 15.17](ch021.xhtml#fig-pfu).
    At a high level, a PUF accepts a challenge input and produces a unique response
    determined by the physical microstructure of the chip ([Gao, Al-Sarawi, and Abbott
    2020](ch058.xhtml#ref-gao2020physical)). Variants include optical PUFs, in which
    the challenge consists of a light pattern and the response is a speckle image,
    and electronic PUFs such as Arbiter PUFs (APUFs), where timing differences between
    circuit paths produce a binary output. Another common implementation is the SRAM
    PUF, which exploits the power-up state of uninitialized SRAM cells: due to threshold
    voltage mismatch, each cell tends to settle into a preferred value when power
    is first applied. These response patterns form a stable, reproducible hardware
    fingerprint.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: PUF（物理不可克隆函数）的内部工作原理在[图15.17](ch021.xhtml#fig-pfu)中进行了说明。从高层次来看，PUF接受一个挑战输入并产生一个由芯片的物理微结构决定的独特响应（[高，阿尔-萨拉维和艾博特
    2020](ch058.xhtml#ref-gao2020physical)）。变体包括光学PUF，其中挑战由光模式组成，响应为斑点图像，以及电子PUF，如仲裁器PUF（APUF），其中电路路径之间的时序差异产生二进制输出。另一种常见实现是SRAM
    PUF，它利用未初始化SRAM单元的加电状态：由于阈值电压不匹配，每个单元在首次加电时往往会稳定在一个首选值。这些响应模式形成了一个稳定、可重复的硬件指纹。
- en: '![](../media/file245.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file245.png)'
- en: 'Figure 15.17: **Physical Unclonable Functions**: Pufs generate unique hardware
    fingerprints from inherent manufacturing variations, enabling device authentication
    and secure key generation without storing secrets. Optical and electronic PUF
    implementations use physical phenomena—such as light speckle patterns or timing
    differences—to produce challenge-response pairs that are difficult to predict
    or replicate. Source: ([Gao, Al-Sarawi, and Abbott 2020](ch058.xhtml#ref-gao2020physical)).'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.17：**物理不可克隆函数**：PUF从固有的制造变化中生成独特的硬件指纹，使设备认证和安全的密钥生成成为可能，而无需存储秘密。光学和电子PUF实现使用物理现象——如光斑点图案或时序差异——来产生难以预测或复制的挑战-响应对。来源：([高，阿尔-萨拉维和艾博特
    2020](ch058.xhtml#ref-gao2020physical))。
- en: Despite their promise, PUFs present several challenges in system design. Their
    outputs can be sensitive to environmental variation, such as changes in temperature
    or voltage, which can introduce instability or bit errors in the response. To
    ensure reliability, PUF systems must often incorporate error correction codes
    or helper data schemes. Managing large sets of challenge-response pairs also raises
    questions about storage, consistency, and revocation. Additionally, the unique
    statistical structure of PUF outputs may make them vulnerable to machine learning-based
    modeling attacks if not carefully shielded from external observation.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有前景，PUF在系统设计中仍存在一些挑战。它们的输出可能对环境变化敏感，如温度或电压的变化，这可能导致响应中的不稳定性或位错误。为确保可靠性，PUF系统通常必须包含错误纠正码或辅助数据方案。管理大量挑战-响应对也引发了关于存储、一致性和撤销的问题。此外，如果未经仔细保护免受外部观察，PUF输出的独特统计结构可能会使其容易受到基于机器学习的建模攻击。
- en: From a manufacturing perspective, incorporating PUF technology can increase
    device cost or require additional layout complexity. While PUFs eliminate the
    need for external key storage, thereby reducing long-term security risk and provisioning
    cost, they may require calibration and testing during fabrication to ensure consistent
    performance across environmental conditions and device aging.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 从制造角度来看，采用PUF技术可能会增加设备成本或需要额外的布局复杂性。虽然PUF消除了外部密钥存储的需要，从而降低了长期安全风险和配置成本，但它们可能需要在制造过程中进行校准和测试，以确保在不同环境条件和设备老化过程中的性能一致性。
- en: Nevertheless, Physical Unclonable Functions remain a compelling building block
    for securing embedded machine learning systems. By embedding hardware identity
    directly into the chip, PUFs support lightweight cryptographic operations, reduce
    key management burden, and help establish root-of-trust anchors in distributed
    or resource-constrained environments. When integrated thoughtfully, they complement
    other hardware-assisted security mechanisms such as Secure Boot, TEEs, and HSMs
    to provide defense-in-depth across the ML system lifecycle.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，物理不可克隆函数仍然是确保嵌入式机器学习系统安全的一个有吸引力的构建块。通过将硬件身份直接嵌入到芯片中，PUF支持轻量级加密操作，减轻密钥管理负担，并在分布式或资源受限环境中帮助建立信任锚。当精心集成时，它们补充了其他硬件辅助安全机制，如安全启动、TEE和HSM，以在整个机器学习系统生命周期内提供多层次防御。
- en: Mechanisms Comparison
  id: totrans-520
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机制比较
- en: Hardware-assisted security mechanisms play a foundational role in establishing
    trust within modern machine learning systems. While software-based defenses offer
    flexibility, they ultimately rely on the security of the hardware platform. As
    machine learning workloads increasingly operate on edge devices, embedded platforms,
    and untrusted infrastructure, hardware-backed protections become important for
    maintaining system integrity, confidentiality, and trust.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件辅助的安全机制在现代机器学习系统中建立信任发挥着基础性作用。虽然基于软件的防御提供了灵活性，但它们最终依赖于硬件平台的可靠性。随着机器学习工作负载越来越多地运行在边缘设备、嵌入式平台和不信任的基础设施上，硬件支持的保护对于维护系统完整性、机密性和信任变得重要。
- en: Trusted Execution Environments (TEEs) provide runtime isolation for model inference
    and sensitive data handling. Secure Boot enforces integrity from power-on, ensuring
    that only verified software is executed. Hardware Security Modules (HSMs) offer
    tamper-resistant storage and cryptographic processing for secure key management,
    model signing, and firmware validation. Physical Unclonable Functions (PUFs) bind
    secrets and authentication to the physical characteristics of a specific device,
    enabling lightweight and unclonable identities.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 可信执行环境 (TEEs) 为模型推理和敏感数据处理提供运行时隔离。安全启动从电源开启就强制执行完整性，确保只执行已验证的软件。硬件安全模块 (HSMs)
    提供防篡改的存储和加密处理，用于安全密钥管理、模型签名和固件验证。物理不可克隆函数 (PUFs) 将秘密和认证绑定到特定设备的物理特性，从而实现轻量级且不可克隆的身份。
- en: These mechanisms address different layers of the system stack, ranging from
    initialization and attestation to runtime protection and identity binding, and
    complement one another when deployed together. [Table 15.9](ch021.xhtml#tbl-hw-security-comparison)
    below compares their roles, use cases, and trade-offs in machine learning system
    design.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制针对系统堆栈的不同层次，从初始化和证明到运行时保护和身份绑定，当一起部署时相互补充。[表15.9](ch021.xhtml#tbl-hw-security-comparison)下比较了它们在机器学习系统设计中的角色、用例和权衡。
- en: 'Table 15.9: **Hardware Security Mechanisms**: Machine learning systems use
    diverse hardware defenses—trusted execution environments, secure boot, hardware
    security modules, and physical unclonable functions—to establish trust and protect
    sensitive data across the system stack. The table details how each mechanism addresses
    specific security challenges—from runtime isolation and integrity verification
    to key management and device identity—and emphasizes the associated trade-offs
    in performance and complexity.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.9：**硬件安全机制**：机器学习系统使用各种硬件防御措施——可信执行环境、安全启动、硬件安全模块和物理不可克隆函数——以建立信任并保护系统堆栈中的敏感数据。该表详细说明了每种机制如何解决特定的安全挑战——从运行时隔离和完整性验证到密钥管理和设备身份——并强调了相关的性能和复杂性权衡。
- en: '| **Mechanism** | **Primary Function** | **Common Use in ML** | **Trade-offs**
    |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| **机制** | **主要功能** | **在机器学习中的常见应用** | **权衡** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Trusted Execution****Environment (TEE)** | Isolated runtime environment
    for secure computation | Secure inference and on-device privacy for sensitive
    inputs and outputs | Added complexity, memory limits, perf. costRequires trusted
    code development |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| **可信执行环境 (TEE)** | 安全计算的安全运行环境 | 对敏感输入和输出的安全推理和设备隐私 | 增加复杂性，内存限制，性能成本需要可信代码开发
    |'
- en: '| **Secure Boot** | Verified boot sequence and firmware validation | Ensures
    only signed ML models and firmware execute on embedded devices | Key management
    complexity, vendor lock-in Performance impact during startup |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| **安全启动** | 验证的启动序列和固件验证 | 确保仅在嵌入式设备上执行已签名的机器学习模型和固件 | 密钥管理复杂性，供应商锁定性能在启动期间的影响
    |'
- en: '| **Hardware Security Module** **(HSM)** | Secure key generation and storage,
    crypto-processing | Signing ML models, securing training pipelines, verifying
    firmware | High cost, integration overhead, limited I/ORequires infrastructure-level
    provisioning |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| **硬件安全模块 (HSM)** | 安全密钥生成和存储，加密处理 | 签名机器学习模型，保护训练管道，验证固件 | 成本高，集成开销，有限的I/O需要基础设施级别的配置
    |'
- en: '| **Physical Unclonable****Function (PUF)** | Hardware-bound identity and key
    derivation | Model binding, device authentication, protecting IP in embedded deployments
    | Environmental sensitivity, modeling attacksNeeds error correction and calibration
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| **物理不可克隆函数 (PUF)** | 与硬件绑定的身份和密钥派生 | 模型绑定，设备认证，保护嵌入式部署中的知识产权 | 对环境敏感，建模攻击需要错误纠正和校准
    |'
- en: Together, these hardware primitives form the foundation of a defense-in-depth
    strategy for securing ML systems in adversarial environments. Their integration
    is especially important in domains that demand provable trust, such as autonomous
    vehicles, healthcare devices, federated learning systems, and important infrastructure.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这些硬件原语共同构成了在对抗环境中保护机器学习系统的深度防御策略的基础。在需要可证明信任的领域，如自动驾驶汽车、医疗设备、联邦学习系统和重要基础设施，它们的集成尤为重要。
- en: Practical Implementation Roadmap
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施路线图
- en: The comprehensive security and privacy techniques covered in this chapter can
    seem overwhelming for organizations just beginning to secure their ML systems.
    Rather than implementing every defense simultaneously, a phased approach enables
    systematic security improvements while managing complexity and costs. This roadmap
    provides a practical sequence for building robust ML security, progressing from
    foundational controls to advanced defenses.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的全面安全和隐私技术对于刚开始保护其机器学习系统的组织来说可能显得令人不知所措。与其同时实施所有防御措施，不如分阶段的方法能够系统地提高安全性，同时管理复杂性和成本。本路线图提供了一个构建稳健机器学习安全性的实用序列，从基础控制到高级防御逐步推进。
- en: 'Phase 1: Foundation Security Controls'
  id: totrans-534
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一阶段：基础安全控制
- en: Begin with basic security controls that provide the greatest risk reduction
    for the least complexity. These foundational measures address the most common
    attack vectors and create the trust infrastructure needed for more advanced defenses.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 从提供最大风险降低且复杂性最低的基本安全控制措施开始。这些基础措施针对最常见的攻击向量，并建立了更高级防御所需的可信基础设施。
- en: '**Access Control and Authentication**: Implement role-based access control
    (RBAC) for all ML system components, including training data, model repositories,
    and inference APIs. Use multi-factor authentication for administrative access
    and service-to-service authentication with short-lived tokens. Establish the principle
    of least privilege, ensuring users and services have only the minimum permissions
    required for their functions.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制和身份验证**：为所有机器学习系统组件实施基于角色的访问控制（RBAC），包括训练数据、模型存储库和推理API。对于管理访问使用多因素身份验证，对于服务到服务的身份验证使用短期令牌。建立最小权限原则，确保用户和服务仅拥有执行其功能所需的最小权限。'
- en: '**Data Protection**: Encrypt all data at rest using AES-256 and enforce TLS
    1.3 for all data in transit. This includes training datasets, model files, and
    inference communications. Implement comprehensive logging of all data access and
    model operations to support incident investigation and compliance auditing.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据保护**：使用AES-256对所有静态数据进行加密，并强制执行TLS 1.3对所有传输中的数据进行加密。这包括训练数据集、模型文件和推理通信。实施所有数据访问和模型操作的全面日志记录，以支持事件调查和合规性审计。'
- en: '**Input Validation and Basic Monitoring**: Deploy input validation for all
    ML APIs to reject malformed requests, implement rate limiting to prevent abuse,
    and establish baseline monitoring for unusual inference patterns. These measures
    protect against basic adversarial inputs and provide visibility into system behavior.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入验证和基本监控**：为所有机器学习API部署输入验证以拒绝格式错误的请求，实施速率限制以防止滥用，并建立基线监控以检测异常推理模式。这些措施可以抵御基本的对抗性输入，并使系统能够了解其行为。'
- en: '**Secure Development Practices**: Establish secure coding practices for ML
    pipelines, including dependency management with vulnerability scanning, secure
    model serialization that validates model integrity, and automated security testing
    in deployment pipelines.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全开发实践**：为机器学习管道建立安全编码实践，包括带有漏洞扫描的依赖关系管理、验证模型完整性的安全模型序列化和部署管道中的自动安全测试。'
- en: 'Phase 2: Privacy Controls and Model Protection'
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二阶段：隐私控制和模型保护
- en: With foundational controls in place, focus on protecting sensitive data and
    securing your ML models from theft and manipulation. This phase addresses privacy
    regulations and intellectual property protection.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立基础控制措施之后，重点关注保护敏感数据和确保您的机器学习模型免受窃取和操纵。这一阶段涉及隐私法规和知识产权保护。
- en: '**Privacy-Preserving Techniques**: Implement data anonymization for non-sensitive
    use cases and differential privacy for scenarios requiring formal privacy guarantees.
    For collaborative learning scenarios, deploy federated learning architectures
    that keep sensitive data localized while enabling model improvement.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护技术**：对于非敏感用例实施数据匿名化，对于需要正式隐私保证的场景实施差分隐私。在协作学习场景中，部署联邦学习架构，以保持敏感数据本地化同时实现模型改进。'
- en: '**Model Security**: Protect deployed models through encryption of model files,
    secure API design that limits information leakage, and monitoring for model extraction
    attempts. Implement model versioning and integrity checking to detect unauthorized
    modifications.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型安全**：通过加密模型文件、设计安全的API以限制信息泄露和监控模型提取尝试来保护部署的模型。实施模型版本控制和完整性检查以检测未经授权的修改。'
- en: '**Secure Training Infrastructure**: Harden training environments by isolating
    training workloads, implementing secure data pipelines with validation and provenance
    tracking, and establishing secure model registries with access controls and audit
    trails.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全训练基础设施**：通过隔离训练工作负载、实施具有验证和来源跟踪的安全数据管道以及建立具有访问控制和审计跟踪的安全模型注册表来强化训练环境。'
- en: '**Compliance Integration**: Implement necessary controls for regulatory requirements
    such as GDPR, HIPAA, or industry-specific standards. This includes data subject
    rights management, privacy impact assessments, and documentation of data processing
    activities.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性集成**：实施必要的控制措施以满足监管要求，如GDPR、HIPAA或行业特定标准。这包括数据主体权利管理、隐私影响评估以及数据处理活动的记录。'
- en: 'Phase 3: Advanced Threat Defense'
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三阶段：高级威胁防御
- en: The final phase implements sophisticated defenses for high-stakes environments
    where advanced adversaries pose significant threats. These defenses require more
    expertise and resources but provide protection against state-of-the-art attacks.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 最终阶段在高风险环境中实施复杂的防御措施，这些环境中的高级对手构成了重大威胁。这些防御措施需要更多的专业知识和资源，但可以提供对最先进攻击的保护。
- en: '**Adversarial Robustness**: Deploy adversarial training to improve model robustness
    against evasion attacks, implement certified defenses for safety-critical applications,
    and establish continuous testing against new adversarial techniques.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗鲁棒性**：部署对抗性训练以提高模型对逃避攻击的鲁棒性，实施针对安全关键应用的认证防御，并建立针对新对抗技术的持续测试。'
- en: '**Advanced Runtime Monitoring**: Deploy ML-specific anomaly detection systems
    that can identify sophisticated attacks like data poisoning effects or gradual
    model degradation. Implement behavioral analysis that establishes normal operation
    baselines and alerts on deviations.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级运行时监控**：部署针对特定于机器学习的异常检测系统，可以识别如数据中毒效应或模型逐渐退化等复杂攻击。实施行为分析，建立正常操作基线和针对偏差的警报。'
- en: '**Hardware-Based Security**: For the highest security requirements, implement
    trusted execution environments (TEEs) for model inference, secure boot processes
    for edge devices, and hardware security modules (HSMs) for cryptographic key management.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于硬件的安全**：对于最高安全要求，实施用于模型推理的信任执行环境（TEEs）、边缘设备的安全引导过程以及用于加密密钥管理的硬件安全模块（HSMs）。'
- en: '**Incident Response and Recovery**: Establish ML-specific incident response
    procedures, including model rollback capabilities, contaminated data isolation
    procedures, and forensic analysis techniques for ML-specific attacks.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件响应和恢复**：建立针对机器学习的特定事件响应程序，包括模型回滚功能、受污染数据的隔离程序以及针对机器学习特定攻击的取证分析技术。'
- en: Implementation Considerations
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施注意事项
- en: 'Success with this roadmap requires balancing security improvements with operational
    capabilities. Each phase should be fully implemented and stabilized before progressing
    to the next level. Organizations should customize this sequence based on their
    specific threat model: healthcare systems may prioritize Phase 2 privacy controls,
    financial institutions may emphasize Phase 1 data protection, and autonomous vehicle
    systems may fast-track to Phase 3 adversarial robustness.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功实施此路线图，需要在安全改进和运营能力之间取得平衡。在进入下一阶段之前，每个阶段都应完全实施并稳定。组织应根据其特定的威胁模型定制此序列：医疗保健系统可能优先考虑第二阶段的隐私控制，金融机构可能强调第一阶段的数据保护，而自动驾驶车辆系统可能快速推进到第三阶段的对抗鲁棒性。
- en: Resource allocation should account for the increasing technical complexity and
    operational overhead of advanced phases. Phase 1 controls typically require standard
    IT security expertise, while Phase 3 defenses may require specialized ML security
    knowledge or external consulting. Organizations should invest in training and
    hiring appropriate expertise before implementing advanced controls.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分配应考虑到高级阶段不断增加的技术复杂性和运营开销。第一阶段通常需要标准的IT安全专业知识，而第三阶段的防御可能需要专门的机器学习安全知识或外部咨询。组织应在实施高级控制之前投资于培训和招聘适当的专家。
- en: Regular security assessments should validate the effectiveness of implemented
    controls and guide progression through phases. These assessments should include
    penetration testing of ML-specific attack vectors, red team exercises that simulate
    realistic adversarial scenarios, and compliance audits that verify regulatory
    requirements are met effectively.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 定期的安全评估应验证实施控制措施的有效性，并指导通过各个阶段。这些评估应包括针对机器学习特定攻击向量的渗透测试、模拟现实对抗场景的红队演习，以及合规性审计，以验证是否有效满足监管要求。
- en: Fallacies and Pitfalls
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Having examined both defensive and offensive capabilities, we now address common
    misconceptions that can undermine security efforts. Security and privacy in machine
    learning systems present unique challenges that extend beyond traditional cybersecurity
    concerns, involving sophisticated attacks on data, models, and inference processes.
    The complexity of modern ML pipelines, combined with the probabilistic nature
    of machine learning and the sensitivity of training data, creates numerous opportunities
    for misconceptions about effective protection strategies.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了防御和攻击能力之后，我们现在解决可能削弱安全努力的常见误解。机器学习系统中的安全和隐私面临独特的挑战，这些挑战超越了传统的网络安全问题，涉及对数据、模型和推理过程的复杂攻击。现代机器学习管道的复杂性，加上机器学习的概率性质和训练数据的敏感性，为关于有效保护策略的误解提供了许多机会。
- en: '**Fallacy:** *Security through obscurity provides adequate protection for machine
    learning models.*'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *通过保密提供的安全措施足以保护机器学习模型*。'
- en: This outdated approach assumes that hiding model architectures, parameters,
    or implementation details provides meaningful security protection. Modern attacks
    often succeed without requiring detailed knowledge of target systems, relying
    instead on black-box techniques that probe system behavior through input-output
    relationships. Model extraction attacks can reconstruct significant model functionality
    through carefully designed queries, while adversarial attacks often transfer across
    different architectures. Effective ML security requires robust defenses that function
    even when attackers have complete knowledge of the system, following established
    security principles rather than relying on secrecy.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过时的方法假设隐藏模型架构、参数或实现细节可以提供有意义的网络安全保护。现代攻击往往不需要详细了解目标系统，而是依赖黑盒技术，通过输入输出关系探测系统行为。通过精心设计的查询，模型提取攻击可以重建重要的模型功能，而对抗攻击通常可以跨不同架构转移。有效的机器学习安全需要强大的防御措施，即使在攻击者完全了解系统的情况下也能发挥作用，遵循既定的安全原则，而不是依赖保密。
- en: '**Pitfall:** *Assuming that differential privacy automatically ensures privacy
    without considering implementation details.*'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *假设差分隐私自动确保隐私，而没有考虑实施细节*。'
- en: Many practitioners treat differential privacy as a universal privacy solution
    without understanding the critical importance of proper implementation and parameter
    selection. Poorly configured privacy budgets can provide negligible protection
    while severely degrading model utility. Implementation vulnerabilities like floating-point
    precision issues, inadequate noise generation, or privacy budget exhaustion can
    completely compromise privacy guarantees. Real-world systems require careful analysis
    of privacy parameters, rigorous implementation validation, and ongoing monitoring
    to ensure that theoretical privacy guarantees translate to practical protection.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者将差分隐私视为通用的隐私解决方案，而没有理解适当实施和参数选择的重要性。配置不当的隐私预算可能提供微不足道的保护，同时严重降低模型效用。实施漏洞，如浮点精度问题、噪声生成不足或隐私预算耗尽，可能会完全破坏隐私保证。现实世界系统需要仔细分析隐私参数，进行严格的实施验证，以及持续的监控，以确保理论上的隐私保证转化为实际的保护。
- en: '**Fallacy:** *Federated learning inherently provides privacy protection without
    additional safeguards.*'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *联邦学习本质上提供隐私保护，无需额外的安全措施*。'
- en: A related privacy misconception assumes that keeping data decentralized automatically
    ensures privacy protection. While federated learning improves privacy compared
    to centralized training, gradient and model updates can still leak significant
    information about local training data through inference attacks. Sophisticated
    adversaries can reconstruct training examples, infer membership information, or
    extract sensitive attributes from shared model parameters. True privacy protection
    in federated settings requires additional mechanisms like secure aggregation,
    differential privacy, and careful communication protocols rather than relying
    solely on data locality.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的隐私误解假设保持数据去中心化自动确保隐私保护。虽然联邦学习比集中式训练提高了隐私性，但梯度更新和模型更新仍然可以通过推理攻击泄露关于本地训练数据的重大信息。复杂的对手可以重建训练示例、推断成员信息或从共享模型参数中提取敏感属性。在联邦环境中实现真正的隐私保护需要额外的机制，如安全聚合、差分隐私和仔细的通信协议，而不仅仅是依赖于数据本地性。
- en: '**Pitfall:** *Treating security as an isolated component rather than a system-wide
    property.*'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 将安全视为一个独立的组件，而不是系统级属性。'
- en: Beyond specific technical misconceptions, a key architectural pitfall involves
    organizations that approach ML security by adding security features to individual
    components without considering system-level interactions and attack vectors. This
    piecemeal approach fails to address sophisticated attacks that span multiple components
    or exploit interfaces between subsystems. Effective ML security requires holistic
    threat modeling that considers the entire system lifecycle from data collection
    through model deployment and maintenance, following the threat prioritization
    principles established in [Section 15.4.1](ch021.xhtml#sec-security-privacy-threat-prioritization-framework-f2d5).
    Security must be integrated into every stage of the ML pipeline rather than treated
    as an add-on feature or post-deployment consideration.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 除去特定的技术误解之外，一个关键的架构陷阱涉及那些通过向单个组件添加安全功能来处理机器学习安全，而没有考虑到系统级交互和攻击向量。这种零散的方法无法解决跨越多个组件或利用子系统之间接口的复杂攻击。有效的机器学习安全需要全面的威胁建模，该建模考虑了从数据收集到模型部署和维护的整个系统生命周期，遵循在[第15.4.1节](ch021.xhtml#sec-security-privacy-threat-prioritization-framework-f2d5)中确立的威胁优先级原则。安全必须集成到机器学习管道的每个阶段，而不是作为一个附加功能或部署后的考虑。
- en: '**Pitfall:** *Underestimating the attack surface expansion in distributed ML
    systems.*'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 低估分布式机器学习系统中的攻击面扩展。'
- en: Many organizations focus on securing individual components without recognizing
    how distributed ML architectures increase the attack surface and introduce new
    vulnerability classes. Distributed training across multiple data centers creates
    opportunities for man-in-the-middle attacks on gradient exchanges, certificate
    spoofing, and unauthorized participation in training rounds. Edge deployment multiplies
    endpoints that require security updates, monitoring, and incident response capabilities.
    Model serving infrastructure spanning multiple clouds introduces dependency chain
    attacks, where compromising any component in the distributed system can affect
    overall security. Orchestration systems, load balancers, model registries, and
    monitoring infrastructure each present potential entry points for sophisticated
    attackers. Effective distributed ML security requires thorough threat modeling
    that accounts for network communication security, endpoint hardening, identity
    management across multiple domains, and coordination of security policies across
    heterogeneous infrastructure components.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织专注于保护单个组件，而没有认识到分布式机器学习架构如何增加攻击面并引入新的漏洞类别。跨多个数据中心进行分布式训练为中间人攻击梯度交换、证书欺骗和未经授权参与训练轮次提供了机会。边缘部署增加了需要安全更新、监控和事件响应能力的端点。跨越多个云的模型服务基础设施引入了依赖链攻击，其中分布式系统中的任何组件被破坏都可能影响整体安全性。编排系统、负载均衡器、模型注册表和监控基础设施各自为高级攻击者提供了潜在的入口点。有效的分布式机器学习安全需要全面的威胁建模，该建模考虑到网络通信安全、端点加固、跨多个域的身份管理以及跨异构基础设施组件的安全策略协调。
- en: Summary
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has explored the complex landscape of security and privacy in machine
    learning systems, from core threats to comprehensive defense strategies. Security
    and privacy represent essential requirements for deploying machine learning systems
    in production environments. As these systems handle sensitive data, operate across
    diverse platforms, and face sophisticated threats, their security posture must
    encompass the entire technology stack. Modern machine learning systems encounter
    attack vectors ranging from data poisoning and model extraction to adversarial
    examples and hardware-level compromises that can undermine system integrity and
    user trust.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了机器学习系统中安全和隐私的复杂格局，从核心威胁到综合防御策略。安全和隐私是部署机器学习系统到生产环境中的基本要求。由于这些系统处理敏感数据，在多个平台上运行，并面临复杂的威胁，其安全态势必须涵盖整个技术栈。现代机器学习系统遭遇的攻击向量从数据中毒和模型提取到对抗性示例和硬件级别的妥协，这些都可能损害系统完整性和用户信任。
- en: Effective security strategies employ defense-in-depth approaches that operate
    across multiple layers of the system architecture. Privacy-preserving techniques
    like differential privacy and federated learning protect sensitive data while
    enabling model training. Robust model design incorporates adversarial training
    and input validation to resist manipulation. Hardware security features provide
    trusted execution environments and secure boot processes. Runtime monitoring detects
    anomalous behavior and potential attacks during operation. These complementary
    defenses create resilient systems that can withstand coordinated attacks across
    multiple attack surfaces.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的安全策略采用深度防御方法，这些方法在系统架构的多个层面上运行。隐私保护技术如差分隐私和联邦学习在保护敏感数据的同时，使模型训练成为可能。健壮的模型设计结合对抗性训练和输入验证以抵抗操纵。硬件安全功能提供可信执行环境和安全启动过程。运行时监控在操作期间检测异常行为和潜在的攻击。这些互补的防御措施创建了能够抵御跨多个攻击面的协调攻击的弹性系统。
- en: '**Key Takeaways**'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Security and privacy must be integrated from initial architecture design rather
    than added as afterthoughts to ML systems
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性和隐私必须从初始架构设计开始整合，而不是作为机器学习系统的附加考虑
- en: 'ML systems face threats across three categories: model confidentiality (theft),
    training integrity (poisoning), and inference robustness (adversarial attacks)'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习系统面临来自三个类别的威胁：模型机密性（盗窃）、训练完整性（中毒）和推理鲁棒性（对抗性攻击）
- en: Historical security patterns (supply chain compromise, insufficient isolation,
    weaponized endpoints) amplify in ML contexts due to autonomous decision-making
    capabilities
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于自主决策能力，历史安全模式（供应链妥协、隔离不足、武器化端点）在机器学习环境中加剧
- en: Effective defense requires layered protection spanning data privacy, model security,
    runtime monitoring, and hardware trust anchors
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的防御需要跨越数据隐私、模型安全、运行时监控和硬件信任锚点的分层保护
- en: 'Context drives defense selection: healthcare prioritizes regulatory compliance,
    autonomous vehicles prioritize adversarial robustness, financial systems prioritize
    model theft prevention'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境驱动防御选择：医疗保健优先考虑法规遵从性，自动驾驶汽车优先考虑对抗性鲁棒性，金融系统优先考虑模型盗窃预防
- en: Privacy-preserving techniques include differential privacy, federated learning,
    homomorphic encryption, and synthetic data generation, each with distinct trade-offs
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护技术包括差分隐私、联邦学习、同态加密和合成数据生成，每种技术都有其独特的权衡
- en: Hardware security mechanisms (TEEs, secure boot, HSMs, PUFs) provide foundational
    trust for software-level protections
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件安全机制（TEE、安全启动、HSM、PUF）为软件级保护提供基础信任
- en: Security introduces inevitable trade-offs in computational cost, accuracy degradation,
    and implementation complexity that must be balanced against protection benefits
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性在计算成本、精度下降和实现复杂性方面引入了不可避免的权衡，这些权衡必须与保护效益相平衡
- en: Looking forward, the security and privacy foundations established in this chapter
    form critical building blocks for the comprehensive robustness framework explored
    in [Chapter 16](ch022.xhtml#sec-robust-ai). While we’ve focused on defending against
    malicious actors and protecting sensitive information, true system reliability
    requires expanding these concepts to handle all forms of operational stress. The
    monitoring infrastructure, defensive mechanisms, and layered architectures we’ve
    developed here provide the foundation for detecting distribution shifts, managing
    uncertainty, and ensuring graceful degradation under adverse conditions—topics
    that will be central to our exploration of robust AI.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，本章中建立的安全和隐私基础构成了在[第16章](ch022.xhtml#sec-robust-ai)中探讨的综合鲁棒框架的关键构建块。虽然我们专注于防御恶意行为者和保护敏感信息，但真正的系统可靠性需要将这些概念扩展到处理所有形式的操作压力。我们在此开发的监控基础设施、防御机制和分层架构为检测分布变化、管理不确定性和在不利条件下确保优雅降级提供了基础——这些内容将是我们在鲁棒人工智能探索中的核心主题。
- en: '* * *'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
