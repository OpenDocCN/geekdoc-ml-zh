- en: 5.1.3 Dimensionality reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.1.3 维度约简
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html](https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html](https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html)
- en: '*If some characters seem to be missing, it''s because MathJax is not loaded
    correctly. Refreshing the page should fix it.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果某些字符似乎缺失，那是因为MathJax没有正确加载。刷新页面应该可以解决这个问题。*'
- en: '[E] Why do we need dimensionality reduction?'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 我们为什么需要维度约简？'
- en: '[E] Eigendecomposition is a common factorization technique used for dimensionality
    reduction. Is the eigendecomposition of a matrix always unique?'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 特征值分解是一种常用的因子分解技术，用于维度约简。矩阵的特征值分解总是唯一的吗？'
- en: '[M] Name some applications of eigenvalues and eigenvectors.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 列举一些特征值和特征向量的应用。'
- en: '[M] We want to do PCA on a dataset of multiple features in different ranges.
    For example, one is in the range 0-1 and one is in the range 10 - 1000\. Will
    PCA work on this dataset?'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 我们想在具有不同范围的多个特征的数据集上执行PCA。例如，一个是0-1的范围，另一个是10-1000的范围。PCA在这个数据集上会起作用吗？'
- en: '[H] Under what conditions can one apply eigendecomposition? What about SVD?'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 在什么条件下可以应用特征值分解？SVD呢？'
- en: What is the relationship between SVD and eigendecomposition?
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVD和特征值分解之间有什么关系？
- en: What’s the relationship between PCA and SVD?
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA和SVD之间有什么关系？
- en: '[H] How does t-SNE (T-distributed Stochastic Neighbor Embedding) work? Why
    do we need it?'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] t-SNE（T-分布随机邻域嵌入）是如何工作的？为什么我们需要它？'
- en: '**In case you need a refresh on PCA, here''s an explanation without any math.**'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**如果您需要PCA的复习，这里有一个没有数学解释的解释。**'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Assume that your grandma likes wine and would like to find characteristics
    that best describe wine bottles sitting in her cellar. There are many characteristics
    we can use to describe a bottle of wine including age, price, color, alcoholic
    content, sweetness, acidity, etc. Many of these characteristics are related and
    therefore redundant. Is there a way we can choose fewer characteristics to describe
    our wine and answer questions such as: which two bottles of wine differ the most?'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设您的祖母喜欢葡萄酒，并希望找到最能描述她地窖中葡萄酒瓶的特征。我们可以使用许多特征来描述一瓶葡萄酒，包括年龄、价格、颜色、酒精含量、甜度、酸度等。许多这些特征是相关的，因此是冗余的。我们有没有办法选择更少的特征来描述我们的葡萄酒，并回答像：哪两瓶葡萄酒差异最大这样的问题？
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PCA is a technique to construct new characteristics out of the existing characteristics.
    For example, a new characteristic might be computed as `age - acidity + price`
    or something like that, which we call a linear combination.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PCA是一种从现有特征中构建新特征的技术。例如，一个新特征可能被计算为`年龄 - 酸度 + 价格`或类似的东西，我们称之为线性组合。
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To differentiate our wines, we'd like to find characteristics that strongly
    differ across wines. If we find a new characteristic that is the same for most
    of the wines, then it wouldn't be very useful. PCA looks for characteristics that
    show as much variation across wines as possible, out of all linear combinations
    of existing characteristics. These constructed characteristics are principal components
    of our wines.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了区分我们的葡萄酒，我们希望找到在葡萄酒之间强烈差异的特征。如果我们找到一个对所有葡萄酒都相同的特征，那么它就不会很有用。PCA寻找的是在所有现有特征的线性组合中，尽可能多地显示葡萄酒之间差异的特征。这些构建的特征是我们葡萄酒的主成分。
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to see a more detailed, intuitive explanation of PCA with visualization,
    check out [amoeba's answer on StackOverflow](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).
    This is possibly the best PCA explanation I've ever read.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想看到一个更详细、直观的PCA解释，包括可视化，请查看[amoeba在StackOverflow上的答案](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)。这可能是我读过的最好的PCA解释。
