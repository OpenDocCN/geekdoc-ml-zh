- en: 12  Uncertainty
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 不确定性
- en: 原文：[https://ml-science-book.com/uncertainty.html](https://ml-science-book.com/uncertainty.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ml-science-book.com/uncertainty.html](https://ml-science-book.com/uncertainty.html)
- en: '[Integrating Machine Learning Into Science](./part-two.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[将机器学习融入科学](./part-two.html)'
- en: '[12  Uncertainty](./uncertainty.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12 不确定性](./uncertainty.html)'
- en: The Western United States has a problem with water – they don’t have enough
    of it. They even call it *The Dry West*, and if you have ever been to Utah, you
    know why. Because of this, water management is done for things that require planning,
    like water storage and agriculture. The Bureau of Reclamation, among others, is
    concerned with forecasting for each year how much water will be available in the
    season (from April to July) as measured at certain gauges in rivers and intakes
    of dams. Since the main water supply in the rivers comes from snowmelt in the
    mountain ranges and from precipitation, these are the most important features
    used in forecasting. In the past, hydrologists relied on traditional statistical
    models, but recently they dipped their toes into machine learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 西部美国有一个问题——他们没有足够的水。他们甚至称之为*干旱的西部*，如果你曾经去过犹他州，你就知道为什么了。正因为如此，水管理是为了那些需要规划的事情，比如水储存和农业。除了其他机构外，水利重整局还关注每年预测在河流和水库进水口测量的水量（从4月到7月）的情况。由于河流的主要供水来自山脉的融雪和降水，这些是在预测中使用的最重要的特征。在过去，水文学家依赖于传统的统计模型，但最近他们开始涉足机器学习。
- en: 'Running out of drinking water or losing crops is not an option. That’s why
    decision-makers not only need a forecast, they need to know how certain that forecast
    is. For example, hydrologists often communicate uncertainty using quantile forecasts
    (see [Figure 12.1](#fig-quantiles)): A (correct) 90% quantile forecast means that
    there is a 90% chance that the actual streamflow will be below the forecast and
    a 10% chance that it will exceed the forecast. Forecasting the 10, 30, 50 (median),
    70, and 90 percent quantiles is common.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 没有饮用水或失去农作物都不是一个选择。这就是为什么决策者不仅需要一个预测，他们还需要知道这个预测有多确定。例如，水文学家经常使用分位数预测来传达不确定性（参见[图12.1](#fig-quantiles)）：一个（正确）的90%分位数预测意味着实际流量有90%的可能性低于预测值，有10%的可能性超过预测值。预测10%、30%、50%（中位数）、70%和90%的分位数是常见的。
- en: '![](../Images/2bb00f0d60f8aa4604f6199f8bcab8ec.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bb00f0d60f8aa4604f6199f8bcab8ec.png)'
- en: 'Figure 12.1: Beaver River water supply forecasts of different “exceedances”,
    which are inverted quantiles (e.g., 90% exceedance = 10% quantile). The February
    2024 report was provided by the natural resources conservation service of the
    U.S. Department of Agriculture, CC-BY (https://creativecommons.org/licenses/by/4.0/).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：不同“超过”情况的比佛河供水预测，这些是逆分位数（例如，90%超过 = 10%分位数）。2024年2月的报告由美国农业部自然资源保护服务提供，CC-BY（https://creativecommons.org/licenses/by/4.0/）。
- en: In fact, in a world with perfect predictions, all quantiles would “collapse”
    to the same point, but nature does not allow you to peek into her cards so easily.
    You have to deal with unforeseen weather changes, measurement errors in mountain
    ranges, and inadequate spatial coverage. This chapter shows you how to integrate
    these factors into your machine learning pipeline and qualify your predictions
    with uncertainty estimates.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在一个完美预测的世界里，所有分位数都会“坍缩”到同一点，但自然界不允许你那么容易地窥视她的牌。你必须处理不可预见的天气变化、山脉中的测量误差以及空间覆盖不足。本章将向您展示如何将这些因素整合到您的机器学习流程中，并使用不确定性估计来验证您的预测。
- en: For a more technical dive into machine learning uncertainty quantification,
    we recommend [[1]](references.html#ref-gruber2023sources), [[2]](references.html#ref-hoffmann2021multiplicity),
    [[3]](references.html#ref-begoli2019need), and [[4]](references.html#ref-hullermeier2021aleatoric),
    all of which partially inspired our chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 若要深入了解机器学习不确定性量化，我们推荐[1](references.html#ref-gruber2023sources)、[2](references.html#ref-hoffmann2021multiplicity)、[3](references.html#ref-begoli2019need)和[4](references.html#ref-hullermeier2021aleatoric)，这些文献部分启发了我们的章节。
- en: 'All berries are tasty, but some are dangerous. As a result, berry studies has
    always been one of the best-funded sciences. So asking Rattle to automate berry
    classification seemed like a natural step. And indeed, after two weeks, Rattle
    delivered the world’s best berry classifier. The only problem was that nobody
    trusted the results: The classifier may be right on average, but is it also right
    for this berry?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有浆果都美味，但有些是有毒的。因此，浆果研究一直是资金最充足的科学之一。所以让Rattle自动化浆果分类似乎是自然而然的一步。确实，两周后，Rattle交付了世界上最好的浆果分类器。唯一的问题是没有人相信结果：这个分类器可能在平均意义上是正确的，但它对这个浆果也是正确的吗？
- en: '![](../Images/8c792672e4800f3fea466e314a428c45.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c792672e4800f3fea466e314a428c45.png)'
- en: 12.1 Frequentist vs Bayesian interpretation of uncertainty
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 频率解释与贝叶斯解释的不确定性
- en: 'Say a hydrologist tells you that there is a 10% probability that the April
    streamflow in the Beaver River will be less than 6.3 thousand acre-feet. How do
    you interpret that statement? The possible answers describe one of the oldest
    discussions within statistics: What is a probability?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一位水文学家告诉你，有10%的概率表明比弗河4月份的流量将低于6.3千英亩英尺。你如何解释这个陈述？可能的答案描述了统计学中最古老的讨论之一：什么是概率？
- en: '**Frequentist interpretation**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**频率解释**'
- en: Imagine that you examine the water supply (infinitely) many times under similar
    conditions, then 10% of the time, you would observe water supplies below 6.3 thousand
    acre-feet. Frequentists view statements about uncertainty as statements about
    the relative frequency of events in the long run under similar conditions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在相似条件下（无限次）检查水资源，那么10%的时候，你会观察到水资源低于6.3千英亩英尺。频率主义者将关于不确定性的陈述视为在相似条件下长期事件相对频率的陈述。
- en: 'The frequentist interpretation of uncertainty has two central conceptual problems:
    1) You simply cannot observe an event with an infinite number of repetitions under
    similar observational conditions, and it is even unclear whether their relative
    frequencies are well-defined. 2) You are faced with the so-called *reference class
    problem* – what defines relevant similar conditions?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 频率解释的不确定性有两个核心概念问题：1) 在相似的观测条件下，你无法观察到无限重复的事件，而且它们相对频率的定义是否明确也不清楚。2) 你面临所谓的*参考类问题*——什么定义了相关的相似条件？
- en: '**Bayesian interpretation**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯解释**'
- en: The hydrologist believes with 10% confidence that the water supply will be less
    than 6.3 thousand acre-feet. This belief can be expressed in terms of bets she
    would be willing to make. For example, the hydrologist would be willing to bet
    up to 10 cents that the water supply will be below 6.3 thousand acre-feet, receiving
    1 Dollar if the water supply is actually below 6.3 and nothing if it’s above.
    To a Bayesian, uncertainty describes subjective degrees of belief. In fact, this
    subjective belief is constantly updated as new data is observed using the Bayes
    formula[¹](#fn1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 水文学家有10%的信心认为水资源将少于6.3千英亩英尺。这种信念可以通过她愿意做出的赌注来表示。例如，水文学家愿意赌上最多10美分，如果水资源实际上低于6.3千英亩英尺，她将获得1美元，如果高于这个数值则什么也得不到。对于贝叶斯来说，不确定性描述的是主观信念的程度。实际上，这种主观信念会随着新数据的观察而不断更新，使用贝叶斯公式[¹](#fn1)。
- en: 'While the Bayesian approach seems to have solved the limitations of the frequentist
    approach, it faces its own problems: 1) What should you believe if you have not
    seen any evidence on the matter? This is the so-called problem of *prior belief*.
    2) If uncertainty is a subjective statement about the world, can probabilities
    be correct? 3) Bayesian probabilities rely on calculating posterior probabilities
    with the Bayes formula, which is often computationally intractable.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然贝叶斯方法似乎解决了频率方法的局限性，但它也面临着自己的问题：1) 如果你没有看到任何关于这个问题的证据，你该相信什么？这就是所谓的*先验信念*问题。2)
    如果不确定性是对世界的客观陈述，概率是否可以正确？3) 贝叶斯概率依赖于使用贝叶斯公式计算后验概率，这通常在计算上是不可行的。
- en: “Probability is the most important concept in modern science, especially as
    nobody has the slightest notion of what it means.” (Attributed to Bertrand Russell,
    1929)
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “概率是现代科学中最重要的概念，特别是没有人有丝毫的概念它意味着什么。”（据说是伯特兰·罗素在1929年所说）
- en: There are also other less established interpretations of uncertainty, such as
    propensities, likelihoodism, objective Bayesianism, and many others. But whatever
    understanding of uncertainty you choose, you have to live with its limitations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些不太成熟的关于不确定性的解释，例如倾向性、可能性论、客观贝叶斯主义以及许多其他理论。但无论你选择哪种对不确定性的理解，你都必须接受其局限性。
- en: '**Laplace’s Demon: Uncertainties in a deterministic world**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**拉普拉斯恶魔：决定性世界中的不确定性**'
- en: Does it matter for the frequentist or Bayesian interpretation of uncertainty
    whether the world is deterministic or not?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '对于频率主义或贝叶斯主义的不确定性解释来说，世界是否是决定性的是否重要？ '
- en: Imagine a hypothetical being with infinite computational resources who knows
    the location and momentum of every particle in the world. Knowing the laws of
    nature, could it perfectly determine every past and future state of the world?
    If so, the world would be deterministic, there would be no uncertainty for that
    being. This thought experiment of a being with infinite knowledge and infinite
    computational resources is called the *Laplacian Demon*. It would be able to precisely
    calculate the water supply for April.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个拥有无限计算资源、知道世界上每个粒子的位置和动量的假设存在生物。如果知道自然法则，它能否完美地确定世界的过去和未来状态？如果是这样，世界将是决定性的，对于那个生物来说将没有不确定性。这个拥有无限知识和无限计算资源的生物的思想实验被称为**拉普拉斯恶魔**。它将能够精确计算出四月份的水资源供应。
- en: Does it matter for the interpretation of uncertainty if such a Laplacian Demon
    exists? Not really! For frequentists, uncertainties are always defined relative
    to a reference class of similar conditions specified in a particular language.
    As long as these similar conditions do not determine the event, the uncertainty
    remains well-defined. Similarly, for Bayesians, uncertainties arise from *human*
    computational limitations, incomplete language for the system, insufficient knowledge
    of the laws, or lack of information about the state of the world. Therefore, both
    Bayesians and frequentists can reasonably speak of uncertainty at a higher level
    of description, regardless of whether there is no uncertainty at a lower level
    for Laplace’s hypothetical demon [[5]](references.html#ref-list2019levels). This
    chapter deals with uncertainties that arise from the restriction of the prediction
    task to certain features, inconclusive reference classes, or simply limited human
    capacities. We lack the expertise to make sophisticated speculations about potential
    irreducible uncertainties in the quantum world.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在这样的拉普拉斯恶魔，这会对不确定性的解释产生影响吗？实际上并没有！对于频率主义者来说，不确定性总是相对于在特定语言中指定的类似条件参考类来定义的。只要这些类似条件不决定事件，不确定性就保持良好定义。同样，对于贝叶斯主义者来说，不确定性源于**人类**的计算限制、系统的语言不完整、对法则的不充分了解或对世界状态的缺乏信息。因此，无论在拉普拉斯假设的恶魔的较低层次是否存在不确定性，贝叶斯主义者和频率主义者都可以在较高层次上合理地谈论不确定性。本章处理的是由于预测任务限制在特定特征、不明确的参考类或简单的人类能力有限而产生的不确定性。我们缺乏在量子世界中关于潜在不可还原不确定性的复杂推测的专业知识。
- en: 12.2 The shared language of uncertainty
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 不确定性的共同语言
- en: While there are great fights about what uncertainty means, there is a relatively
    broad consensus about the language used to describe uncertainty.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人们对不确定性的含义存在很大争议，但对于描述不确定性的语言，相对而言有较广泛的共识。
- en: '**Probability space**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率空间**'
- en: 'The most central concept for describing uncertainty is standard probability
    theory, which involves:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 描述不确定性的最核心概念是标准概率理论，它包括：
- en: A *sample space* \(\Omega\) that describes all possible outcomes. In our example,
    an outcome could contain all kinds of information, such as the amount of snow
    in the U.S., global winds, and the water supply.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**样本空间** \(\Omega\)，它描述了所有可能的结果。在我们的例子中，一个结果可能包含各种信息，例如美国降雪量、全球风力和水资源供应。
- en: An *event space* \(\mathcal{F}\) that describes relevant sets of possible outcomes.
    One such set might be all outcomes where the water supply is exactly 6.3 thousand
    acre-feet.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**事件空间** \(\mathcal{F}\)，它描述了相关可能结果集合。这样一个集合可能包括所有水资源供应恰好为6.3千英亩英尺的结果。
- en: A *probability measure* \(\mathbb{P}\) that assigns a non-negative real value
    to each element in the event space. For example, the event that the water supply
    is less than 6.3 thousand acre-feet can be assigned a probability of 0.1 (10%).
    \(\mathbb{P}\) should satisfy the Kolmogorov axioms.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *概率测度* \(\mathbb{P}\)，它将非负实值分配给事件空间中的每个元素。例如，供水低于6.3千英亩英尺的事件可以分配一个概率为0.1（10%）。\(\mathbb{P}\)
    应满足柯尔莫哥洛夫公理。
- en: '*Kolmogorov Axioms* *The Kolmogorov axioms consist of three components: 1)
    Every event \(E\) gets a probability assigned of greater or equal to zero, i.e. \(\mathbb{P}(E)\geq
    0\). 2) \(\Omega\) is in the event space and its probability is 1, i.e. \(\mathbb{P}(\Omega)=1\).
    This means that one of the outcomes in \(\Omega\) has to occur for sure. 3) If
    you unite a countable infinite sequence of events \((E_1, E_2,\dots)\) that are
    pairwise disjoint, then the union of the events has the same probability as the
    sum of the individual events, i.e. \(\mathbb{P}\left(\bigcup_{i=1}^{\infty} E_i\right)
    = \sum_{i=1}^{\infty} \mathbb{P}(E_i)\). For example, the event that the water
    supply is below 6.3 thousand acre-feet (with probability 0.1) and the event that
    the water supply is strictly above 6.3 thousand acre-feet (with probability 0.9)
    have no common outcomes, so their joint probability is the sum of the individual
    probabilities (=1).*  *Together these three components form a probability space
    \((\Omega,\mathcal{F},\mathbb{P})\).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*柯尔莫哥洛夫公理* *柯尔莫哥洛夫公理由三个部分组成：1) 每个事件 \(E\) 被分配一个大于或等于零的概率，即 \(\mathbb{P}(E)\geq
    0\)。2) \(\Omega\) 在事件空间中，其概率为1，即 \(\mathbb{P}(\Omega)=1\)。这意味着 \(\Omega\) 中的某个结果一定会发生。3)
    如果将可数无限序列的事件 \((E_1, E_2,\dots)\) 合并，那么这些事件的并集的概率与单个事件的概率之和相同，即 \(\mathbb{P}\left(\bigcup_{i=1}^{\infty}
    E_i\right) = \sum_{i=1}^{\infty} \mathbb{P}(E_i)\)。例如，供水低于6.3千英亩英尺（概率为0.1）的事件和供水严格高于6.3千英亩英尺（概率为0.9）的事件没有共同的结果，因此它们的联合概率是各自概率之和（=1）。*  *这三个部分共同构成了一个概率空间
    \((\Omega,\mathcal{F},\mathbb{P})\)。'
- en: '**Random variables**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机变量**'
- en: '*Random variables* describe quantities based on random events from the event
    space \(\mathcal{F}\). Say you are interested in water supply. Random variables
    allow you to define a function \(Y\) that maps each possible outcome to its corresponding
    water supply, i.e. \(Y: \Omega\rightarrow \mathbb{R}\) with \(\omega\mapsto \text{water
    supply in } \omega\). This gives you an efficient way to describe events related
    to water supply, for example, \(Y=6.3\) describes the event where the water supply
    is 6.3 thousand acre-feet. Throughout this book, we have used random variables
    to describe the features and the target variable.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机变量* 描述基于事件空间 \(\mathcal{F}\) 中的随机事件的数量。比如说，你对供水感兴趣。随机变量允许你定义一个函数 \(Y\)，它将每个可能的结果映射到其对应的水供应，即
    \(Y: \Omega\rightarrow \mathbb{R}\) with \(\omega\mapsto \text{water supply in
    } \omega\)。这为你描述与供水相关的事件提供了一个有效的方法，例如，\(Y=6.3\) 描述了供水为6.3千英亩英尺的事件。在这本书的整个过程中，我们使用了随机变量来描述特征和目标变量。'
- en: '**Confidence vs credible intervals**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**置信区间与可信区间**'
- en: '*Confidence intervals* and *credible intervals* quantify uncertainties in a
    compact way. Say you are not satisfied with a point prediction, and instead you
    want interval predictions that are likely to cover the true label. Such intervals
    can indeed be computed. Frequentists will give you confidence intervals, and Bayesians
    will give you credible intervals. But be careful, they come with different interpretations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*置信区间* 和 *可信区间* 以紧凑的方式量化不确定性。比如说，你对一个点预测不满意，而希望得到可能覆盖真实标签的区间预测。这样的区间确实可以计算出来。频率主义者会给你提供置信区间，而贝叶斯主义者会给你提供可信区间。但请注意，它们有不同的解释：'
- en: Frequentist 95% confidence intervals say that with a relative frequency of 95%
    the true label is contained in an infinite sequence of confidence intervals computed
    for different data samples. Thus, frequentists see the true label as fixed and
    the intervals as random variables.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频率主义者的95%置信区间表示，在95%的相对频率下，真实标签包含在为不同数据样本计算出的无限序列的置信区间中。因此，频率主义者认为真实标签是固定的，而区间是随机变量。
- en: Bayesian 95% credible intervals say that there is a 95% certainty that the outcome
    falls into the given interval. Thus, Bayesians see the interval as fixed and the
    true label as a random variable.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯主义者的95%可信区间表示，有95%的确定性认为结果落在给定的区间内。因此，贝叶斯主义者认为区间是固定的，而真实标签是随机变量。
- en: '**Quantiles**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**分位数**'
- en: 'The *q-quantiles* are the cutoff points that divide the probability distribution
    of a random variable into q equally sized intervals. In practice, the quantiles
    are relatively easy to estimate:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*q 分位数*是将随机变量的概率分布划分为 q 个等大小区间的截断点。在实践中，分位数相对容易估计：'
- en: Arrange your data along the variable of interest.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 沿着感兴趣的变量排列你的数据。
- en: Put \(\frac{100}{q}\)-th of the data with the lowest variable value in the first
    bin, and so on until all the data is in one of the bins.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 \(\frac{100}{q}\) 的数据中变量值最低的数据放入第一个分箱，依此类推，直到所有数据都位于一个分箱中。
- en: Compute the average between the highest variable value in the k-th bin and the
    lowest value in the k+1-th bin. This average is your estimate of the k-th q-quantile.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算第 k 个分箱中最高变量值和第 k+1 个分箱中最低值的平均值。这个平均值就是你对第 k 个 q 分位数的估计。
- en: For example, the first 10% quantile describes the water supply volume where
    10% of your observed volumes lie below. This can give you information about the
    distribution of the data beyond the mean value.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第一个 10% 分位数描述了 10% 的观测值低于的水供应量。这可以给你关于数据分布超过平均值的信息。
- en: '**Imprecise probabilities**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**不精确概率**'
- en: 'Standard probability theory assigns each event in the event space \(\mathcal{F}\)
    a value between 0 and 1\. This can sometimes be difficult: Think of a hydrologist
    who is asked for her certainty that the water supply will exceed 12 thousand acre-feet.
    Instead of giving a point probability of 70% certainty, she might say that her
    certainty lies between 65-75%. Notice how the information is different from a
    certainty judgment of 50-90%, even though the average is the same.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 标准概率论将事件空间 \(\mathcal{F}\) 中的每个事件分配一个介于 0 和 1 之间的值。这有时可能很困难：想想一个被要求给出其供水量将超过
    12 千亩英尺的确定性的水文学家。她可能不会给出 70% 确定的点概率，而是说她有 65-75% 的确定性。注意，即使平均值相同，这种信息与 50-90%
    的确定性判断的信息是不同的。
- en: 'Such scenarios are precisely what theories of *imprecise probabilities* are
    designed for. They allow you to express uncertainty about the probability of a
    concrete event:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的场景正是 *不精确概率* 理论设计的目的。它们允许你表达对具体事件概率的不确定性：
- en: '*Credal sets*, for instance, describe sets of probability distributions rather
    than a single distribution.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，*可信集*描述的是概率分布的集合，而不是单个分布。
- en: '*Upper and lower probabilities* describe the upper and lower limits of sets
    of probabilities assigned to a certain event.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上界和下界概率*描述了分配给某个事件的概率集合的上界和下界。'
- en: '*Interval probabilities* assign each event an interval \([a,b]\) with \(a,b\in[0,1]\)
    and \(a<b\).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*区间概率*将每个事件分配一个区间 \([a,b]\)，其中 \(a,b\in[0,1]\) 且 \(a<b\)。'
- en: There are tons of books on uncertainty quantification, and this small chapter
    cannot provide the same depth. Instead of providing an exhaustive list of all
    the concepts and languages used to describe uncertainty, our selection focuses
    on the most common ones.*  *## 12.3 Uncertainty in predictions, performance &
    properties
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不确定性量化有很多书籍，而这小章节无法提供同样的深度。我们不是提供描述不确定性的所有概念和语言的详尽列表，而是专注于最常见的一些。*## 12.3
    预测、性能和属性中的不确定性
- en: Uncertainty matters whenever you estimate things. Scientists particularly care
    about the uncertainties in the estimations of predictions, performance, and properties.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在你估计事物时，不确定性很重要。科学家特别关注预测、性能和属性估计中的不确定性。
- en: '**Predictions**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**'
- en: In the hydrology example, we were concerned with estimating the expected water
    supply. In this case, we care about the uncertainty in *predictions*. What is
    the probability that the water supply will be less than 6.3 thousand acre-feet?
    What predicted values cover the true label with 95% certainty? What is the variance
    in predictions from similarly performing models? Prediction uncertainty is the
    most common concern of researchers using machine learning. Whenever predictions
    are the basis for action, investigating prediction uncertainties is a key requirement.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在水文示例中，我们关注的是估计预期供水。在这种情况下，我们关心的是 *预测* 中的不确定性。水供应量少于 6.3 千亩英尺的概率是多少？哪些预测值以 95%
    的确定性覆盖了真实标签？从表现相似的模型中预测的方差是多少？预测不确定性是使用机器学习的学者最常见的问题。每当预测是行动的基础时，调查预测不确定性是一个关键要求。
- en: '**Performance**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能**'
- en: A hydrologist may wish to compare her model with those of a competitor or with
    the state-of-the-art. This can be done by evaluating the error on historical/future
    data that was not used to train the model (i.e., the holdout set). However, this
    provides only a single estimate of the expected error on other unseen data. Therefore,
    it is often useful to estimate the test error repeatedly and to define confidence
    intervals. Whenever scientists want to fairly compare the performance of their
    model with others, they should be transparent about performance uncertainties.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 水文学家可能希望将她自己的模型与竞争对手的模型或最先进的模型进行比较。这可以通过评估历史/未来数据上的误差来完成，这些数据并未用于训练模型（即保留集）。然而，这只能提供一个关于其他未见数据的预期误差的单个估计。因此，通常需要反复估计测试误差并定义置信区间。每当科学家想要公平地比较他们的模型与其他模型的性能时，他们应该对性能不确定性保持透明。
- en: '**Properties**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**特性**'
- en: In a classic statistical modeling context, hydrologists are interested in the
    uncertainties of model parameters, assuming that they reflect general properties
    of interest such as (causal) feature effect sizes. In machine learning, parameters
    such as weights in neural networks often do not lend themselves to such an intuitive
    interpretation. There are often neither uniquely optimal nor robust parameter
    settings. To analyze the properties of interest, such as feature effects and importance,
    you can generalize the notion of parameters to general properties of interest
    of the data distribution. These properties and their uncertainties can be efficiently
    estimated, for example, using targeted learning [[6]](references.html#ref-van2011targeted).
    Alternatively, the same properties and uncertainties can be estimated using post-hoc
    interpretability techniques [[7]](references.html#ref-freiesleben2022scientific).
    This allows hydrologists to answer questions about the most effective or important
    feature of water supply.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的统计建模背景下，水文学家对模型参数的不确定性感兴趣，假设它们反映了感兴趣的通用特性，如（因果）特征效应大小。在机器学习中，神经网络中的权重等参数通常不适用于这种直观的解释。通常既没有唯一最优的参数设置，也没有鲁棒的参数设置。为了分析感兴趣的特性，如特征效应和重要性，可以将参数的概念推广到数据分布的感兴趣的一般特性。这些特性和它们的不确定性可以有效地估计，例如，使用针对性学习
    [[6]](references.html#ref-van2011targeted)。或者，可以使用事后可解释性技术 [[7]](references.html#ref-freiesleben2022scientific)来估计相同的特性和不确定性。这允许水文学家回答关于水资源供应最有效或最重要的特征的问题。
- en: 12.4 Uncertainty quantifies the expected error
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 不确定性量化预期误差
- en: When we talk about uncertainty, we are implicitly talking about errors. How
    close is the predicted water supply to the true future water supply? How close
    is your test error to the true generalization error? Does the estimated effect
    of precipitation on water supply match the true effect?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论不确定性时，我们隐含地谈论误差。预测的水资源供应与真实未来水资源供应有多接近？你的测试误差与真实泛化误差有多接近？估计的降水对水供应的影响与真实影响相匹配吗？
- en: 'In general terms, you always have a target quantity of interest \(T\) (e.g.,
    true water supply), and your estimate \(\hat{T}\) (e.g., predicted water supply)
    and you want to know how large the error is, measured by some distance function
    \(L\): \[ \epsilon:=L(T,\hat{T}) \]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般意义上讲，你总是有一个感兴趣的目标量 \(T\)（例如，真实水资源供应），以及你的估计 \(\hat{T}\)（例如，预测水资源供应），并且你想要知道误差有多大，误差是通过某个距离函数
    \(L\) 来衡量的：\[ \epsilon:=L(T,\hat{T}) \]
- en: Unfortunately, you don’t have access to \(T\). If you did, you wouldn’t have
    to estimate anything at all. Therefore, you usually look at the expected error
    in your estimation, such as the expectation over datasets used in the estimation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你无法访问 \(T\)。如果你能，你就不需要估计任何东西了。因此，你通常查看估计中的预期误差，例如在估计中使用的数据集的期望。
- en: 'The reason the expected error can be quantified is the so-called bias-variance
    decomposition, which works for many known distance functions such as the mean
    squared error or the 0-1 loss [[8]](references.html#ref-domingos2000unified):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 预期误差可以被量化的原因在于所谓的偏差-方差分解，它适用于许多已知的距离函数，例如均方误差或0-1损失 [[8]](references.html#ref-domingos2000unified)：
- en: \[ \underbrace{\mathbb{E}[L(T,\hat{T})]}_{\text{Expected error}} = \underbrace{L(\mathbb{E}[\hat{T}],T)}_{\text{Bias}}
    + \underbrace{\mathbb{E}[(L(\hat{T},\mathbb{E}[\hat{T}])]}_{\text{Variance}} \]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \underbrace{\mathbb{E}[L(T,\hat{T})]}_{\text{预期误差}} = \underbrace{L(\mathbb{E}[\hat{T}],T)}_{\text{偏差}}
    + \underbrace{\mathbb{E}[(L(\hat{T},\mathbb{E}[\hat{T}])]}_{\text{方差}} \]
- en: '**Bias** describes how the expected estimate differs from the true target value
    \(T\). In a frequentist interpretation, the idea is that if you repeatedly estimate
    your target you will be correct on average. A biased estimator systematically
    underestimates or overestimates the target quantity.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**描述了预期估计值与真实目标值 \(T\) 的差异。在频率主义解释中，其思想是如果你反复估计你的目标，你将平均上是正确的。有偏估计量系统地低估或高估目标量。'
- en: '**Variance** describes how the estimates vary, for example, for a different
    data sample. An estimator has high variance if the estimates are highly sensitive
    to the dataset it receives.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**描述了估计值的变化情况，例如，对于不同的数据样本。如果估计值对所接收的数据集高度敏感，则估计量具有高方差。'
- en: '![](../Images/8ce63bb678d2af6d97aa3e18e7193d52.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8ce63bb678d2af6d97aa3e18e7193d52.png)'
- en: 'Figure 12.2: Bias and variance explained intuitively on a dartboard, CC-BY
    (https://creativecommons.org/licenses/by/4.0/)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12.2]：在飞镖盘上直观地解释偏差和方差，CC-BY (https://creativecommons.org/licenses/by/4.0/)
- en: But how does bias-variance decomposition help? For many estimators you can prove
    unbiasedness, meaning the bias term vanishes. Therefore, the expected error can
    be fully determined from the variance of the estimate. And the variance can estimated
    empirically or theoretically.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但偏差-方差分解如何帮助呢？对于许多估计量，你可以证明无偏性，这意味着偏差项消失。因此，预期误差可以完全由估计值的方差来确定。方差可以经验或理论地估计。
- en: 12.5 Sources of uncertainty
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 不确定性的来源
- en: Predictions, performance, and properties are the juicy lemonade you get from
    our whole machine learning pipeline. Whether the lemonade tastes good depends
    on the water, the lemons, the squeezing technique, and the ice cubes. A lack of
    quality in any of these has a direct impact on the quality of your lemonade. Similarly,
    the expected errors in predictions, performance, and properties are due to errors
    in our task, modeling setup, and data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 预测、性能和属性是我们整个机器学习流程中得到的香甜柠檬汁。柠檬汁是否美味取决于水、柠檬、挤压技巧和冰块。这些中的任何一项质量不足都会直接影响柠檬汁的质量。同样，预测、性能和属性中的预期误差是由于我们的任务、建模设置和数据中的误差。
- en: Machine learning modeling can be described as a series of steps. Each of these
    steps can introduce errors that propagate uncertainty in our estimate. The examples
    and errors shown in the [Figure 12.3](#fig-pipeline) focus on *prediction uncertainty*,
    but performance and property uncertainties can be captured in the same way.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习建模可以描述为一系列步骤。这些步骤中的每一个都可能引入错误，从而传播我们估计中的不确定性。图 [12.3](#fig-pipeline) 中显示的示例和错误主要集中在*预测不确定性*上，但性能和属性不确定性也可以以相同的方式捕捉。
- en: '![](../Images/03e06da2e94baf6026e8b2a354a66691.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03e06da2e94baf6026e8b2a354a66691.png)'
- en: 'Figure 12.3: The machine learning pipeline from the perspective of uncertainty.
    Each step describes an approximation of the true target value, CC-BY (https://creativecommons.org/licenses/by/4.0/)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12.3]：从不确定性的角度看待机器学习流程。每一步描述了对真实目标值的近似，CC-BY (https://creativecommons.org/licenses/by/4.0/)
- en: 12.5.1 Task uncertainty
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.1 任务不确定性
- en: 'Specifying a machine learning task means specifying three things: 1\. the prediction
    target \(Y\); 2\. a notion of loss \(L\); 3\. the input features \(X=(X_1,\dots,X_p)\).
    In machine learning theory, these three objects are usually considered fixed.
    In most practical settings, however, they must be chosen carefully.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 指定机器学习任务意味着指定三件事：1. 预测目标 \(Y\)；2. 损失的概念 \(L\)；3. 输入特征 \(X=(X_1,\dots,X_p)\)。在机器学习理论中，这三个对象通常被认为是固定的。然而，在大多数实际设置中，它们必须被仔细选择。
- en: '**Prediction target**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测目标**'
- en: Often, what you want to predict is vague, making it difficult to operationalize.
    How do you define the flow of a river? Is one measurement enough? Where to place
    the sensor? Or should you place multiple sensors and average over many measurements?
    Vague target variables are especially common in the social sciences. Social scientists
    are often interested in latent variables that cannot be measured directly. Think
    of concepts like intelligence, happiness, or motivation. For empirical research,
    these variables need to be operationalized by some measurable proxy. For example,
    using grades as a proxy for intelligence, money as a proxy for utility, or hours
    spent as a proxy for motivation. Operationalizations of latent variables are often
    debatable, to say the least.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你想要预测的东西是模糊的，这使得操作化变得困难。你如何定义河流的流量？一个测量是否足够？传感器应该放在哪里？或者应该放置多个传感器并对多个测量值进行平均？模糊的目标变量在社会科学中尤为常见。社会科学家通常对无法直接测量的潜在变量感兴趣。想想像智力、幸福或动机这样的概念。对于实证研究，这些变量需要通过某些可测量的代理变量进行操作化。例如，使用成绩作为智力的代理变量，金钱作为效用的代理变量，或花费的时间作为动机的代理变量。潜在变量的操作化至少是有争议的。
- en: 'The *operationalization error* describes the difference between the true prediction
    target \(Y_{true}\) and its operationalization \(Y\) according to some distance
    function \(d\): \[ \epsilon_{operationalize}:=d(Y_{true},Y) \]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*操作化误差* 描述了根据某些距离函数 \(d\) 真实预测目标 \(Y_{true}\) 与其操作化 \(Y\) 之间的差异：\[ \epsilon_{operationalize}:=d(Y_{true},Y)
    \]'
- en: The operationalization error is a big topic in measurement theory [[9]](references.html#ref-diamantopoulos2008advancing),
    but it still needs to be better bridged to scientific practice [[10]](references.html#ref-carpentras2024we).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 操作化误差是测量理论中的一个重要话题 [[9]](references.html#ref-diamantopoulos2008advancing)，但它仍然需要更好地与科学实践相衔接
    [[10]](references.html#ref-carpentras2024we)。
- en: '**Loss**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失**'
- en: Uncertainty in the target variable leads to uncertainty in the choice of loss
    function. The loss function is defined over the co-domain of \(Y\) and if \(Y\)
    changes, so does the loss \(L\) most of the time. An appropriate loss on the money
    domain \(]-\infty,\infty[\) differs from the personal happiness evaluation domain
    \((1,2,\dots,10)\). Even if there is no uncertainty about \(Y\), there can still
    be uncertainty about the appropriate notion of loss in a given context. Is it
    better to define prediction error in water supply in terms of absolute distance
    or should particularly bad predictions be penalized more using the mean squared
    error?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量的不确定性会导致损失函数选择的不确定性。损失函数定义在 \(Y\) 的陪域上，如果 \(Y\) 发生变化，那么损失 \(L\) 大多数情况下也会随之变化。货币域
    \([-∞,∞]\) 上的适当损失与个人幸福评价域 \((1,2,\dots,10)\) 不同。即使对 \(Y\) 没有不确定性，在特定情境中关于适当损失概念的不确定性仍然可能存在。是用水供应的绝对距离来定义预测误差更好，还是应该使用均方误差对特别差的预测进行更多惩罚？
- en: '**Input features**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入特征**'
- en: You could use many different features to predict a certain target variable,
    and each combination of features would result in a different prediction. For example,
    the snow stations could be placed in different locations. But which is the best
    prediction? It is complicated! When you have multiple models making predictions
    based on different features, there is not necessarily one prediction model that
    is always superior. Each of the models may have its merits in different situations.
    Constraining the prediction task to a subset of input features \(X\) may lead
    to a feature selection error.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用许多不同的特征来预测某个目标变量，并且每个特征组合都会产生不同的预测。例如，雪站可以放置在不同的位置。但哪个是最佳预测？这很复杂！当你有多个模型基于不同的特征进行预测时，并不一定有一个预测模型在所有情况下都是优越的。每个模型在不同的情境下可能都有其优点。将预测任务限制在输入特征子集
    \(X\) 中可能会导致特征选择误差。
- en: 'The *feature selection error* describes the difference between the operationalized
    target \(Y\) and the Bayes-optimal prediction \(f_{dep}(X)\) according to \(L\)
    in the deployment distribution (see below in [Section 12.5.2](#sec-Bayes) our
    explanation of Bayes-optimal predictors): \[ \epsilon_{feature}:=L(Y,f_{dep}(X)).
    \] Adding features with predictive value tends to reduce the error. Thus, one
    might be interested in the difference between the optimal prediction of \(Y\)
    based on feature set \(X\) (snow-water equivalent at certain landmarks) and the
    optimal prediction based on \(X\) plus \(Z\) (weather forecasts). The *omitted
    features error* relative to \(X\) describes the difference between the optimal
    prediction based on \(X\) and the optimal prediction based on \(X\) and \(Z\):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征选择误差*描述了在部署分布中根据 \(L\) 的操作化目标 \(Y\) 与贝叶斯最优预测 \(f_{dep}(X)\) 之间的差异（参见下文[第12.5.2节](#sec-Bayes)中我们对贝叶斯最优预测器的解释）：\[
    \epsilon_{feature}:=L(Y,f_{dep}(X)). \] 添加具有预测价值的特征往往会减少误差。因此，人们可能对基于特征集 \(X\)（例如，某些地标处的雪水当量）和基于
    \(X\) 加 \(Z\)（天气预报）的最优预测之间的差异感兴趣。相对于 \(X\) 的*遗漏特征误差*描述了基于 \(X\) 的最优预测与基于 \(X\)
    和 \(Z\) 的最优预测之间的差异：'
- en: \[ \epsilon_{omitted}:= L(f_{dep}(X,Z),f_{dep}(X)). \]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \epsilon_{omitted}:= L(f_{dep}(X,Z),f_{dep}(X)). \]
- en: '12.5.2 Interlude: The Bayes-optimal predictor, epistemic, and aleatoric uncertainty'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.2 间奏：贝叶斯最优预测器、认知不确定性和随机不确定性
- en: Given a task \((Y,L,X)\), we can define a central concept when it comes to uncertainty
    – the Bayes-optimal predictor. It describes a prediction function that takes input
    features \(X\) (e.g., snow-water equivalent, weather forecasts, etc.) and always
    outputs the best prediction for \(Y\) (i.e., water supply). This does not mean
    that the Bayes-optimal predictor always predicts the correct amount of water supply.
    It just gives the best possible guess based on \(X\) only. As we discussed above,
    the features we have access to, such as snow-water equivalent and weather forecasts,
    even if perfectly accurate, will usually not completely determine the water supply.
    Other factors such as the rock layers below the river, the form of the riverbed,
    or the lakes connected to the river also affect the water supply. The error made
    by the Bayes-optimal predictor is the feature selection error \(\epsilon_{feature}\)
    we defined above.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个任务 \((Y,L,X)\)，我们可以定义一个关于不确定性的核心概念——贝叶斯最优预测器。它描述了一个预测函数，该函数接受输入特征 \(X\)（例如，雪水当量、天气预报等）并始终输出
    \(Y\)（即，供水）的最佳预测。这并不意味着贝叶斯最优预测器总是预测正确的供水量。它只是基于 \(X\) 给出的最佳猜测。正如我们上面讨论的，我们能够访问的特征，如雪水当量和天气预报，即使完全准确，通常也不会完全决定供水。其他因素，如河流下面的岩石层、河床的形状或与河流相连的湖泊，也会影响供水。贝叶斯最优预测器犯的错误是我们上面定义的特征选择误差
    \(\epsilon_{feature}\)。
- en: 'Let’s define the Bayes-optimal predictor more formally. It describes the function
    \(f: \mathcal{X}\rightarrow \mathcal{Y}\) that minimizes the generalization error
    (see [Chapter 7](generalization.html) or [[11]](references.html#ref-hastie2009elements))
    and is defined pointwise [²](#fn2) for \(x\in\mathcal{X}\) by:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更正式地定义贝叶斯最优预测器。它描述了函数 \(f: \mathcal{X}\rightarrow \mathcal{Y}\)，该函数最小化泛化误差（参见[第7章](generalization.html)或[[11]](references.html#ref-hastie2009elements)），并且对于
    \(x\in\mathcal{X}\) 在点wise上定义为 [²](#fn2)：'
- en: \[f(x)=\underset{c}{\mathrm{argmin}}\;\;\mathbb{E}_{Y|X}[L(Y,c)\mid X=x].\]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \[f(x)=\underset{c}{\mathrm{argmin}}\;\;\mathbb{E}_{Y|X}[L(Y,c)\mid X=x].\]
- en: For many loss functions, you can theoretically derive the Bayes-optimal predictor.
    For example, if you face a regression task and measure loss with the mean-squared
    error, the optimal predictor is the conditional expectation of \(Y\) given \(X\),
    i.e. \(f=\mathbb{E}_{Y\mid X}[Y \mid X]\). Alternatively, if you face a classification
    task and use the \(0-1\) loss, the optimal predictor predicts the class with the
    highest conditional probability, i.e. \(\mathrm{argmax}_{y\in\mathcal{Y}}\;\;\mathbb{P}(y
    \mid X)\).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多损失函数，理论上可以推导出贝叶斯最优预测器。例如，如果你面对一个回归任务，并且用均方误差来衡量损失，最优预测器是 \(X\) 给定 \(Y\)
    的条件期望，即 \(f=\mathbb{E}_{Y\mid X}[Y \mid X]\)。或者，如果你面对一个分类任务，并使用 \(0-1\) 损失，最优预测器预测具有最高条件概率的类别，即
    \(\mathrm{argmax}_{y\in\mathcal{Y}}\;\;\mathbb{P}(y \mid X)\)。
- en: '**Aleatoric uncertainty, epistemic uncertainty, and why we don’t like the idea
    of irreducible uncertainties**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机不确定性、认知不确定性以及我们为什么不喜欢不可减少的不确定性概念**'
- en: In the discussion of uncertainty, a common taxonomy is to distinguish between
    aleatoric and epistemic uncertainty [[4]](references.html#ref-hullermeier2021aleatoric).
    Aleatoric uncertainty is said to be irreducible. Even with infinite data on snow-water
    equivalent, weather, and water supply from the past, and fantastic modeling skills,
    you might still not be able to forecast water supply perfectly – the remaining
    error is subject to aleatoric uncertainty. Commonly, aleatoric uncertainty is
    identified with the feature selection error \(\epsilon_{feature}\). Epistemic
    uncertainty is seen as reducible and stems from your limited access to data, skill
    in modeling, or computation. For example, if you only have one month of historical
    data on snow-water equivalent, weather, and water supply, then you have high epistemic
    uncertainty – collecting more data can reduce this uncertainty.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在不确定性的讨论中，一个常见的分类法是区分随机不确定性和认知不确定性 [[4]](references.html#ref-hullermeier2021aleatoric)。随机不确定性被认为是不可减少的。即使有关于过去雪水当量、天气和供水的无限数据，以及出色的建模技能，你可能仍然无法完美预测供水
    – 剩余的错误属于随机不确定性。通常，随机不确定性与特征选择误差 \(\epsilon_{feature}\) 相关联。认知不确定性被视为可减少的，并源于你对数据的有限访问、建模技能或计算。例如，如果你只有一个月关于雪水当量、天气和供水的历史数据，那么你就有很高的认知不确定性
    – 收集更多数据可以减少这种不确定性。
- en: While we agree that the feature selection error \(\epsilon_{feature}\) is of
    interest, we disagree with calling it irreducible. As mentioned above, including
    more features allows you to reduce the error in practice. Uncertainty must always
    be considered reducible or irreducible only relative to the assumptions that are
    considered fixed, such as the features, the model, or the data. Whether there
    is a truly irreducible uncertainty is for physicists to decide, not statisticians.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们同意特征选择误差 \(\epsilon_{feature}\) 是有意义的，但我们不同意将其称为不可减少的。如上所述，包括更多特征可以在实践中减少错误。不确定性必须始终相对于被认为是固定的假设来考虑，例如特征、模型或数据。是否存在真正的不可减少的不确定性，应由物理学家来决定，而不是统计学家。
- en: 12.5.3 Distribution uncertainty
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.3 分布不确定性
- en: 'The next source of uncertainty is the deployment environment. You simply do
    not know whether the environment in which you will make your predictions will
    look exactly like the one you have observed. Over the past 100 years, climate
    change has significantly altered patterns important to water flow: The best water
    flow predictions in a given setting in 1940 might look different from the best
    predictions in 2024\. It remains uncertain what environmental conditions will
    prevail on the Beaver River in the coming years: Will Timpanogos Glacier still
    be there? How much precipitation can be expected in this area? How many extreme
    weather events such as droughts or floods will occur?'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个不确定性的来源是部署环境。你根本不知道你将做出预测的环境是否会与你观察到的环境完全一样。在过去100年里，气候变化已经显著改变了与水流重要性的模式：1940年在给定环境中的最佳水流预测可能与2024年的最佳预测不同。关于未来几年贝弗河的环境条件仍然存在不确定性：蒂帕诺格斯冰川是否仍然存在？这个地区可以预期多少降水？会有多少极端天气事件，如干旱或洪水发生？
- en: 'The *distribution error* describes the difference in prediction for a given
    input \(x\) between the Bayes-optimal predictor \(f\) with respect to the training
    distribution \((X_{train},Y_{train})\) and the Bayes-optimal predictor \(f_{dep}(x)\)
    with respect to the deployment distribution \((X_{dep},Y_{dep})\):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布误差*描述了在给定输入 \(x\) 的情况下，贝叶斯最优预测器 \(f\)（相对于训练分布 \((X_{train},Y_{train})\)）和贝叶斯最优预测器
    \(f_{dep}(x)\)（相对于部署分布 \((X_{dep},Y_{dep})\)）之间的预测差异：'
- en: \[ \epsilon_{distribution}:= L(f_{dep}(x),f(x)) \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \epsilon_{distribution}:= L(f_{dep}(x),f(x)) \]
- en: We use \(X\), \(Y\), and \(f\) instead of \(X_{train},Y_{train}, and f_{train}\)
    to keep the notation easy to read.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 \(X\)、\(Y\) 和 \(f\) 而不是 \(X_{train}\)、\(Y_{train}\) 和 \(f_{train}\)，以保持符号易于阅读。
- en: 12.5.4 Model uncertainty
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.4 模型不确定性
- en: The Bayes-optimal predictor is a theoretical construct. The best you can do
    to get it is to approximate it with a machine learning model. To do this, you
    need to specify what the relationship between \(X\) (e.g. snow-water equivalent)
    and your target \(Y\) (e.g. water supply) might be. The constraints you place
    on the model will potentially lead to a model misspecification error.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯最优预测器是一个理论构造。要获得它，你最好的办法是用机器学习模型来近似它。为此，你需要指定 \(X\)（例如，雪水当量）和你的目标 \(Y\)（例如，供水）之间可能存在的关系。你对模型施加的约束可能会导致模型设定错误。
- en: '**Model misspecification**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型设定错误**'
- en: Choosing the right *model class* for a problem is seen as essential, but what
    is the right class? What kind of relationship should you expect between snow-water
    equivalent and water supply? How can snow-water equivalent interact with the weather
    forecast features? This is difficult! Ideally, you want to choose a model class
    that contains models that are close to or even contain the Bayes-optimal predictor.
    Since you usually don’t know what the Bayes-optimal predictor looks like, this
    incentivizes choosing a large model class that allows you to express many possible
    functions. At the same time, finding the best-fitting model in a large model class
    is much harder than finding it in a small model class.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个问题的正确**模型类别**选择被视为至关重要，但正确的类别是什么？你期望雪水当量与供水之间存在什么样的关系？雪水当量如何与天气预报特征相互作用？这是困难的！理想情况下，你希望选择一个包含接近或甚至包含贝叶斯最优预测器的模型的模型类别。由于你通常不知道贝叶斯最优预测器是什么样子，这促使你选择一个允许你表达许多可能函数的大模型类别。同时，在一个大模型类别中找到最佳拟合模型比在小模型类别中更难。
- en: Deep neural networks and tree-based ensembles describe highly expressive model
    classes. They can approximate arbitrary functions well [[12]](references.html#ref-cybenko1989approximation),
    [[13]](references.html#ref-hornik1991approximation), [[14]](references.html#ref-halmos2013measure)
    including the Bayes-optimal predictor. Simpler models like linear models or k-nearest
    neighbors are more constrained in their modeling capabilities.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络和基于树的集成描述了高度表达力的模型类别。它们可以很好地逼近任意函数 [[12]](references.html#ref-cybenko1989approximation),
    [[13]](references.html#ref-hornik1991approximation), [[14]](references.html#ref-halmos2013measure)，包括贝叶斯最优预测器。像线性模型或k近邻这样的简单模型在建模能力上更加受限。
- en: The bias-variance decomposition provides a good perspective on the uncertainties
    that can arise from model misspecification. Choosing an expressive model class
    will reduce the bias in your estimation of the Bayes-optimal predictor. However,
    it will increase the variance in the estimation process. Conversely, choosing
    a model class with low expressivity will generally result in a higher bias but
    lower variance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差分解提供了对模型误指定可能产生的不确定性的良好视角。选择一个表达力强的模型类别将减少你对贝叶斯最优预测器估计中的偏差。然而，它将增加估计过程中的方差。相反，选择一个低表达力的模型类别通常会导致更高的偏差但更低的方差。
- en: '**Randomness in the hyperparameters selection**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数选择中的随机性**'
- en: One constraint you place on the search for the optimal model within the model
    class is the *hyperparameters*. The learning rate in stochastic gradient descent,
    regularizers like dropout, or the number of allowed splits in random forests,
    all constrain the models that can *effectively* be learned. While the search for
    suitable hyperparameters can in some cases be automated using AutoML methods [[15]](references.html#ref-hutter2019automated),
    there remains a random element, as trying out all options always remains computationally
    intractable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型类别内寻找最优模型时，你施加的一个约束是**超参数**。在随机梯度下降中的学习率、正则化器如dropout，或者在随机森林中允许的分割数，都限制了可以**有效**学习的模型。虽然在某些情况下可以使用AutoML方法
    [[15]](references.html#ref-hutter2019automated) 自动化地寻找合适的超参数，但仍然存在随机元素，因为尝试所有选项在计算上总是不可行的。
- en: '**Random seeds and implementation errors**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机种子和实现错误**'
- en: Most complex learning algorithms contain random elements, for example, bootstrapping
    in random forests and batch gradient descent in deep learning. This randomness
    is made reproducible with a seed. Running the same non-deterministic algorithm
    twice with different seeds will produce different predictions. And, there is generally
    no theoretical justification for choosing one seed over another. Also, implementation
    errors in machine learning pipelines can affect the learned model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数复杂的学习算法都包含随机元素，例如随机森林中的自助法和深度学习中的批量梯度下降。这种随机性可以通过种子使其可重现。使用不同的种子运行相同的非确定性算法两次将产生不同的预测。而且，通常没有理论依据来证明选择一个种子比另一个种子更好。此外，机器学习管道中的实现错误可能会影响学习到的模型。
- en: '**Model class, hyperparameters, and seeds constrain the set of effectively
    learnable models**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型类别、超参数和种子限制了可学习模型的集合**'
- en: Together, the choices of model class, hyperparameters, and random seeds constrain
    the effective model class – i.e. the class of models that can be learned under
    those choices. We call the learnable model closest to the Bayes-optimal predictor
    the *class-optimal predictor* and denote it by \(f^*\).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一起，模型类别、超参数和随机种子的选择限制了有效模型类——即在这些选择下可以学习的模型类。我们称与贝叶斯最优预测器最接近的可学习模型为*类别最优预测器*，并用
    \(f^*\) 表示。
- en: 'The *model error* describes the difference between the Bayes-optimal predictor
    \(f\) within the training distribution \((X,Y)\) and the class-optimal predictor
    \(f^*\):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型误差**描述了在训练分布 \((X,Y)\) 内的贝叶斯最优预测器 \(f\) 与类别最优预测器 \(f^*\) 之间的差异：'
- en: \[ \epsilon_{model}:= L(f(X),f^*(X)). \]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \epsilon_{model}:= L(f(X),f^*(X)). \]
- en: 12.5.5 Data uncertainty
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5.5 数据不确定性
- en: 'Finally, let us look at the ultimate source of uncertainty – the data. You
    have made measurements in the past and obtained data describing the snow-water
    equivalent, the weather conditions, and the corresponding water supply. Several
    things can go wrong:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看不确定性的最终来源——数据。你过去已经进行了测量，并获得了描述雪水当量、天气条件和相应供水的数据。可能出现几个问题：
- en: '**Sampling**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽样**'
- en: The data you have obtained is only a small sample of the underlying population.
    With a different sample or more data, your estimate of the label, performance,
    or property of interest may have looked entirely different.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你获得的数据只是潜在总体中很小的一部分样本。如果样本不同或数据更多，你对标签、性能或真正感兴趣的属性的估计可能会有很大的不同。
- en: 'Uncertainty arising from the data sample can have a large impact on your estimate:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 来自数据样本的不确定性可能对你的估计有重大影响：
- en: '**Randomness:** You may have been unlucky and taken a sample that contains
    many outliers. For example, you took measurements on random days, but they happened
    to be very rainy days.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机性**：你可能运气不好，抽取了一个包含许多异常值的样本。例如，你随机选择了一些日子进行测量，但恰好是雨天。'
- en: '**Data size:** Your data sample may be too small to be representative of the
    underlying probability distribution. For example, one measurement per month is
    not enough for your model to learn the relevant dependencies.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据量**：你的数据样本可能太小，无法代表潜在的概率分布。例如，每月一次的测量对于你的模型学习相关依赖关系来说是不够的。'
- en: '**Selection bias:** Your sample may be biased, resulting in a non-representative
    sample. For example, you may have taken measurements only on dry days to avoid
    getting wet.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择偏差**：你的样本可能存在偏差，导致样本不具有代表性。例如，你可能只选择在干燥的日子里进行测量，以避免弄湿。'
- en: 'The *sample error* describes the difference between the prediction of the class-optimal
    model \(f^*\) and the learned model \(\hat{f}_D\) based on the complete dataset
    \(D\): \[ \epsilon_{sample}:= L(f^*(X),\hat{f}_D(X)) \]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**样本误差**描述了基于完整数据集 \(D\) 的类别最优模型 \(f^*\) 的预测与学习模型 \(\hat{f}_D\) 之间的差异：\[ \epsilon_{sample}:=
    L(f^*(X),\hat{f}_D(X)) \]'
- en: In the literature, this error is sometimes referred to as the *approximation
    error* [[4]](references.html#ref-hullermeier2021aleatoric), [[16]](references.html#ref-jalaian2019uncertain),
    but we found the term *sample error* more informative about the nature of this
    error.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这种误差有时被称为*近似误差* [[4]](references.html#ref-hullermeier2021aleatoric), [[16]](references.html#ref-jalaian2019uncertain)，但我们发现术语*样本误差*更能说明这种误差的本质。
- en: '**Measurement errors**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**测量误差**'
- en: All data are derived from measurements, or at least it is useful to think of
    it that way. Your water flow sensors make a measurement. Tracking the amount of
    snow is a measurement. Even just taking a picture of that snow is a measurement.
    A measurement error is the deviation of the result of a measurement from the (unknown)
    true property. It is inevitable to make measurement errors.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都是通过测量得到的，或者至少这样考虑是有用的。你的水流传感器进行测量。追踪雪量也是一种测量。甚至只是拍摄那场雪也是一种测量。测量误差是测量结果与（未知的）真正属性之间的偏差。做出测量误差是不可避免的。
- en: '**Measurement error in the target** introduces a bias. Think of human transcription
    errors in water flow data.[³](#fn3) There are two directions for this bias: it
    could be above or below the true property of interest. You might get lucky and
    biases cancel out on average but that is unlikely, especially if the error is
    systematic.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标测量误差**引入了偏差。想想人类在流水数据转录中的错误。[³](#fn3) 这种偏差有两个方向：它可能高于或低于真正感兴趣的属性。你可能很幸运，平均来看偏差会相互抵消，但这不太可能，尤其是如果误差是系统性的。'
- en: '**Measurement errors in the features** may or may not introduce a bias in the
    predictions. If there is a random error in the measurement of the snow-water equivalent,
    it can be washed out during training. Systematic measurement errors in the input
    features can have more serious consequences: If the snow-water equivalent measurement
    contains large errors, the model may rely on a proxy feature such as streamflow
    in the previous month.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征中的测量误差**可能或可能不会在预测中引入偏差。如果雪水当量的测量中存在随机误差，它可以在训练过程中被消除。输入特征中的系统测量误差可能后果更严重：如果雪水当量测量包含大误差，模型可能会依赖于前一个月的流量等代理特征。'
- en: '**Missing data**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失数据**'
- en: 'In an ideal dataset, no cell is empty. Scientific reality is often less tidy.
    Some values in your rows will look odd: The water flow on March 22 was *infinite*?
    The current snow-water equivalent is *not a number (NaN)*? The date human-entered
    record is *November 32 in 2102*? What should we do if some values are missing
    or undefined?[⁴](#fn4)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的数据库中，没有单元格是空的。科学现实往往不那么整洁。你行中的某些值看起来会很奇怪：3月22日的水流量是*无限的*？当前的积雪水当量是*不是一个数字
    (NaN)*？人为输入的记录日期是*2102年11月32日*？如果某些值缺失或未定义，我们应该怎么办？[⁴](#fn4)
- en: 'There are several reasons why data may be missing:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能缺失有几个原因：
- en: '**Missing completely at random (MCAR):** The missing value mechanism is independent
    of \(X\) and \(Y\). For example, the company that tracks the snow-water equivalent
    has a server problem and therefore the value is missing.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全随机缺失 (MCAR)：** 缺失值机制与 \(X\) 和 \(Y\) 无关。例如，跟踪雪水当量的公司服务器出现问题，因此值缺失。'
- en: '**Missing at random (MAR):** The probability of missingness depends on the
    observed data. For example, if the guy who sends you the snow-water equivalent
    sometimes misses it on Thursdays because it is his date night.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机缺失 (MAR)：** 缺失的概率取决于观察到的数据。例如，如果那个人在星期四发送给你雪水当量时有时会错过，因为那是他的约会之夜。'
- en: '**Missing not at random (MNAR):** The probability of missingness depends on
    the missing information. For example, if you don’t get a snow-water equivalent
    measurement during a snowstorm because the staff can’t reach the sensors.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非随机缺失 (MNAR)：** 缺失的概率取决于缺失的信息。例如，如果你在暴风雪期间没有收到雪水当量测量值，因为工作人员无法到达传感器。'
- en: '**Measurement errors and missing values lead to another estimation error**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**测量误差和缺失值导致另一个估计误差**'
- en: 'Measurement errors and missing values make the data set you deal with in practice
    look different from the complete data set with unbiased values \(D\) you consider
    in theory. The *data error* describes the difference between the learned model
    \(\hat{f}_D\) based on the complete and accurate dataset \(D\) and the learned
    model \(\hat{f}_{cl(\tilde{D})}\) on the dataset \(\tilde{D}\) that contains measurement
    errors and missing values that need to be cleaned up using operations \(cl\):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 测量误差和缺失值使得你在实践中处理的数据集与你在理论上考虑的具有无偏值的完整数据集 \(D\) 看起来不同。*数据误差*描述了基于完整和准确数据集 \(D\)
    学习得到的模型 \(\hat{f}_D\) 与在包含测量误差和缺失值的数据集 \(\tilde{D}\) 上学习得到的模型 \(\hat{f}_{cl(\tilde{D})}\)
    之间的差异，这些缺失值需要通过操作 \(cl\) 进行清理：
- en: \[ \epsilon_{data}:= L(\hat{f}_D(X),\hat{f}_{cl(\tilde{D})}(X)). \]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \epsilon_{data}:= L(\hat{f}_D(X),\hat{f}_{cl(\tilde{D})}(X)). \]
- en: 12.6 Quantifying uncertainties
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 量化不确定性
- en: Now we have fancy names for all kinds of sources of uncertainty and the errors
    they represent. But how can we quantify these uncertainties?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为所有各种不确定性的来源和它们所代表的误差取了花哨的名字。但我们如何量化这些不确定性呢？
- en: '*Uncertainties are hard to disentangle in practice* *Above we tried to disentangle
    all kinds of uncertainties: What is the difference between the true target and
    its operationalization? What is the difference between the Bayes-optimal predictor
    and the class-optimal model? What is the difference between the class-optimal
    model and the model learned on an imperfect dataset? In practice, however, you
    do not have access to the true target, the Bayes-optimal predictor, or the class-optimal
    model. You always have to run the whole process. The different errors are theoretically
    well-defined and useful for thinking about minimizing uncertainties, but in practice,
    the uncertainties are mixed together.*  *### 12.6.1 Frequentist vs Bayesian uncertainty
    quantification'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*在实践中，不确定性很难区分* *以上我们试图区分各种不确定性：真实目标与其操作化的区别是什么？贝叶斯最优预测器与类最优模型的区别是什么？类最优模型与在不完美数据集上学习的模型的区别是什么？然而，在实践中，你无法访问真实目标、贝叶斯最优预测器或类最优模型。你总是必须运行整个过程。不同的误差在理论上定义良好且对思考最小化不确定性是有用的，但在实践中，不确定性是混合在一起的。*  *###
    12.6.1 经验主义与贝叶斯不确定性量化'
- en: Frequentists and Bayesians approach uncertainty quantification differently.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 经验主义者和贝叶斯主义者对不确定性量化有不同的方法。
- en: 'Frequentists have a simple default recipe – they like repetition. Say you want
    to quantify the uncertainty in your estimate:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 经验主义者有一个简单的默认方法——他们喜欢重复。比如说，你想量化你的估计中的不确定性：
- en: Assume, or better *prove* that your estimation is unbiased, i.e. in expectation
    it will measure the right target quantity.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设，或者更好的是*证明*你的估计是无偏的，即期望上它将测量正确的目标量。
- en: Run the estimation process of the target quantity (i.e., predictions, performance,
    or properties) multiple times under different plausible conditions (e.g., task
    conceptualization, modeling setup, or data).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同的可能条件下（例如任务概念化、建模设置或数据）多次运行目标量的估计过程（即预测、性能或属性）。
- en: Because of the bias-variance decomposition and the unbiasedness from Step 1,
    the uncertainty comes exclusively from the variance of the estimates.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于偏差-方差分解和第一步的无偏性，不确定性完全来自估计的方差。
- en: This approach has limitations. For example, you cannot prove that your task
    conceptualization is unbiased. And it can be difficult to come up with multiple
    plausible conditions for estimation. For frequentist uncertainty quantification,
    you need confidence in your domain knowledge. But then it is a feasible approach
    that can be applied post-hoc to all kinds of tasks, models, and data settings.
    Note, however, that some methods, such as confidence intervals, require additional
    assumptions that the errors are IID, homoscedastic (i.e., remain the same across
    different data instances), or Gaussian.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有其局限性。例如，你不能证明你的任务概念化是无偏的。而且，提出多个可能的估计条件可能很困难。对于经验主义的不确定性量化，你需要对你的领域知识有信心。但然后它是一种可行的方法，可以事后应用于各种任务、模型和数据设置。然而，请注意，某些方法，如置信区间，需要额外的假设，即误差是独立同分布的（即在不同数据实例中保持相同），或高斯分布的。
- en: 'Like frequentists, Bayesians have a standard recipe – they just love their
    posteriors. Say you want to quantify the uncertainty in your estimate:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 和经验主义者一样，贝叶斯主义者有一个标准的食谱——他们非常喜欢他们的后验。比如说，你想量化你的估计中的不确定性：
- en: Model the source of uncertainty (e.g. distribution, model, or data) explicitly
    with a random variable and a prior distribution over that variable.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 明确地用随机变量和该变量的先验分布来建模不确定性的来源（例如分布、模型或数据）。
- en: Examine how the uncertainty propagates from the source to the target quantity
    (i.e., predictions, performance, or properties).
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查不确定性是如何从源头传播到目标量（即预测、性能或属性）的。
- en: Update the source of uncertainty, and consequently the uncertainty in the target,
    based on new evidence (e.g., data) using Bayes’ formula.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据新的证据（例如数据）使用贝叶斯公式更新不确定性的来源，以及因此目标中的不确定性。
- en: In terms of uncertainty quantification strategies, we focus mainly on frequentist
    approaches. Bayesian uncertainty often needs to be baked in from the start, including
    a refinement of the optimization problem, while frequentist uncertainty is often
    an easy add-on. But honestly, another reason is that we are just more familiar
    with frequentist approaches. For a detailed overview of Bayesian uncertainty quantification
    strategies in machine learning, check out [[17]](references.html#ref-gal2016uncertainty)
    or [[18]](references.html#ref-kendall2017uncertainties).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在不确定性量化策略方面，我们主要关注频率主义方法。贝叶斯不确定性通常需要从一开始就嵌入，包括优化问题的细化，而频率主义不确定性通常是一个容易添加的附加项。但说实话，另一个原因是我们更熟悉频率主义方法。要详细了解机器学习中贝叶斯不确定性量化策略的概述，请参阅[[17]](references.html#ref-gal2016uncertainty)或[[18]](references.html#ref-kendall2017uncertainties)。
- en: 12.6.2 Directly optimize for uncertainty
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.2 直接优化不确定性
- en: The simplest approach to uncertainty quantification is to predict uncertainties
    rather than labels. This is often done in classification tasks. Instead of simply
    predicting the model class with the highest output value, the output values are
    transformed with a softmax function. The softmax function transforms arbitrary
    positive and negative values into values that look like probabilities; they are
    non-negative and sum up to one. When predicting probabilities, you also need different
    loss functions – the default choices here are cross-entropy and Kullbach-Leibler
    divergence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性量化的最简单方法是对不确定性进行预测，而不是标签。这在分类任务中经常这样做。不是简单地预测输出值最高的模型类别，而是使用softmax函数转换输出值。softmax函数将任意正负值转换为看起来像概率的值；它们是非负的，并且总和为1。在预测概率时，你还需要不同的损失函数——这里的默认选择是交叉熵和Kullbach-Leibler散度。
- en: For regression tasks, optimizing directly for uncertainty is less straightforward.
    Typically, the probability of any individual target value is zero. However, you
    can estimate the density function directly. This is usually done with Bayesian
    approaches such as Bayesian regression, Gaussian processes, and variational inference
    [[17]](references.html#ref-gal2016uncertainty). Here, you explicitly model the
    uncertainty about the optimal model, either via a Gaussian distribution over functions
    or parameters, and update the model in light of new data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务，直接优化不确定性不太直接。通常，任何单个目标值的概率为零。然而，你可以直接估计密度函数。这通常使用贝叶斯方法，如贝叶斯回归、高斯过程和变分推理[[17]](references.html#ref-gal2016uncertainty)来完成。在这里，你明确地通过函数或参数上的高斯分布来建模关于最优模型的不确定性，并根据新数据更新模型。
- en: More technically, this means that you define a prior distribution \(p(\hat{f})\)
    over the set of models \(\hat{f}\in F\). Based on this, you can compute the posterior
    distribution of the models given your data \(p(\hat{f}\mid D)=p(D\mid \hat{f})p(\hat{f})/p(D)\).
    Finally, this allows us to estimate the target in a Bayesian manner by \[ p(y\mid
    x, D)=\int_{\hat{f}} p(y\mid \hat{f}) p(\hat{f}\mid x,D)\;d\hat{f}. \] This integral
    has to be solved numerically again [[17]](references.html#ref-gal2016uncertainty).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术地说，这意味着你定义一个先验分布 \(p(\hat{f})\) 在模型集合 \(\hat{f}\in F\) 上。基于此，你可以计算给定你的数据
    \(p(\hat{f}\mid D)=p(D\mid \hat{f})p(\hat{f})/p(D)\) 的模型后验分布。最后，这使我们能够通过 \[ p(y\mid
    x, D)=\int_{\hat{f}} p(y\mid \hat{f}) p(\hat{f}\mid x,D)\;d\hat{f}. \] 来以贝叶斯方式估计目标。这个积分需要再次数值求解[[17]](references.html#ref-gal2016uncertainty)。
- en: While you get numbers from direct estimation that look like probabilities, they
    are often difficult to interpret as such. Especially because they are not calibrated
    – i.e., they do not match the probabilities in the true outcomes. We discuss how
    to calibrate probabilities in [Section 12.8](#sec-calibration).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你从直接估计中得到的数据看起来像概率，但它们往往很难被解释为概率。特别是，因为它们没有校准——也就是说，它们不匹配真实结果中的概率。我们将在[第12.8节](#sec-calibration)中讨论如何校准概率。
- en: 12.6.3 Task uncertainty is hard to quantify
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.3 任务不确定性难以量化
- en: 'As a reminder, operationalization error describes the difference between the
    true target (e.g., intelligence) and its operationalization (e.g., IQ test). Quantifying
    this difference is difficult. There are several challenges: First, the true target
    is sometimes not directly measurable. Second, even if you had access to the true
    target and the operationalization, they wouldn’t be on the same scale, making
    it hard to define a distance function.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，操作化误差描述的是真实目标（例如，智力）与其操作化（例如，智商测试）之间的差异。量化这个差异是困难的。有几个挑战：首先，真实目标有时不能直接测量。其次，即使你能够访问真实目标和操作化，它们也不会在同一个尺度上，这使得定义距离函数变得困难。
- en: What can be done, however, is to look at the coherence or correlations between
    different operationalizations [[2]](references.html#ref-hoffmann2021multiplicity).
    For example, in intelligence research, the correlation between different operationalizations
    of intelligence led to the so-called *G-factor*, which became the real target
    that researchers wanted to measure with intelligence tests [[19]](references.html#ref-gottfredson1998general).
    Operationalizing the G-factor through multiple specific operationalizations and
    tests allowed researchers to quantify the operationalization error of specific
    intelligence tests.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以做到的是查看不同操作化之间的连贯性或相关性 [[2]](references.html#ref-hoffmann2021multiplicity)。例如，在智力研究中，不同智力操作化之间的相关性导致了所谓的
    *G因子*，这成为了研究人员想要通过智力测试来衡量的真正目标 [[19]](references.html#ref-gottfredson1998general)。通过多个具体操作化和测试来操作G因子允许研究人员量化特定智力测试的操作化误差。
- en: Remember that the feature selection error describes the difference between the
    operationalized target \(Y\) (like water supply) and the Bayes-optimal predictor
    given a feature set \(X\) (e.g., snow-water equivalent). It is more tangible to
    quantify than the operationalization error but it is still difficult. The best
    proxy for the expected overall feature selection error is the test performance
    of a well-trained model. For example, if you squeeze the highly engineered water
    supply prediction model down to a certain test error, the remaining error may
    arguably be the feature selection error. If you have squeezed out all the predictive
    value contained in the available features, techniques like cross-validation allow
    you to additionally estimate the variance in this error.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，特征选择误差描述的是操作化的目标 \(Y\)（如供水）和给定特征集 \(X\)（例如，积雪水量）的贝叶斯最优预测器之间的差异。量化这个差异比量化操作化误差更具体，但仍很困难。预期整体特征选择误差的最佳代理是训练良好的模型的测试性能。例如，如果你将高度工程化的供水预测模型压缩到一定的测试误差，剩余的误差可能就是特征选择误差。如果你已经从可用特征中挤压出了所有的预测价值，交叉验证等技术允许你额外估计这个误差的方差。
- en: Quantifying the expected feature selection error for individual instances \(x\)
    (e.g., the snow-water equivalent and weather forecasts for a given day) is even
    more difficult. Even if the prediction model as a whole is close to the Bayes-optimal
    predictor, it may be far off for individual instances \(x\). To see how it might
    still be possible to quantify the error, check out our discussion of Rashomon
    sets below.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对单个实例 \(x\)（例如，给定一天的积雪水量和天气预报）的预期特征选择误差进行量化甚至更加困难。即使预测模型整体上接近贝叶斯最优预测器，它对于单个实例
    \(x\) 可能仍然相差甚远。要了解如何仍然可能量化误差，请参阅我们下面关于罗生门集的讨论。
- en: The omitted feature error can be quantified using interpretability techniques
    such as *leave-one-covariate-out (LOCO)* [[20]](references.html#ref-lei2018distributionfree)
    or *conditional feature importance* [[21]](references.html#ref-strobl2008conditional).
    These techniques also allow to quantify uncertainty but always require labeled
    data of \((X,Z)\).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用如 *留一协变量法（LOCO）* [[20]](references.html#ref-lei2018distributionfree) 或 *条件特征重要性*
    [[21]](references.html#ref-strobl2008conditional) 这样的可解释性技术来量化遗漏特征误差。这些技术也允许量化不确定性，但始终需要
    \((X,Z)\) 的标记数据。
- en: 12.6.4 No distribution uncertainty without deployment data
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.4 无部署数据无分布不确定性
- en: To estimate distribution uncertainty, we need to know how the training distribution
    differs from the deployment distribution, or at least have some data from both
    distributions. If we have access to data, we can compute the respective errors
    directly. Say one researcher only has access to Beaver River data from 1970 to
    1980 while the other has data from July 2014 to July 2024\. Both use machine learning
    algorithms to get the best possible models. Then, they can compare their predictions,
    performance, or properties on the most recent data to assess the expected distribution
    error.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计分布的不确定性，我们需要知道训练分布与部署分布之间的差异，或者至少从这两个分布中获取一些数据。如果我们能够访问数据，我们可以直接计算相应的误差。比如说，一位研究者只能访问1970年到1980年之间的比弗河数据，而另一位研究者则有2014年7月到2024年7月的数据。他们都使用机器学习算法来获取最佳模型。然后，他们可以比较在最新数据上的预测、性能或属性，以评估预期的分布误差。
- en: Often, we are interested in deployment uncertainty but do not have deployment
    distribution data. Physical models can the be used to simulate data from different
    potential distribution shifts. A more detailed discussion on data simulation and
    augmentation can be found in [Chapter 11](robustness.html).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们感兴趣的是部署不确定性，但我们没有部署分布数据。物理模型可以用来模拟不同潜在分布变化的数据。关于数据模拟和增强的更详细讨论可以在[第11章](robustness.html)找到。
- en: 12.6.5 Rashomon sets – many models are better than one
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.5 罗生门集——多个模型胜过单一模型
- en: Imagine meeting with ten water supply forecasting experts. Each knows the historical
    data and has insights into snow-water equivalent and weather forecasts. But they
    come from different modeling schools, there are physicists, statisticians, machine
    learners, and so on. Each of them gives you their honest estimate of the water
    supply in Beaver River for June – but their predictions differ. Not much, but
    enough to matter. What should you do? There is no good reason to think that any
    of the ten experts will stand out. The intuitive strategy would be to predict
    the average (or the median if there are outliers) and analyze the variance in
    the experts’ opinions to assess the uncertainty in your prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下与十个供水预测专家会面。每个人都了解历史数据，并对积雪水当量和天气预报有见解。但他们来自不同的建模学派，有物理学家、统计学家、机器学习者等等。他们每个人都给出了自己对比弗河6月份供水量的诚实估计——但他们的预测不同。虽然差异不大，但足以产生影响。你应该怎么办？没有充分的理由认为这十个专家中的任何一个会脱颖而出。直观的策略是预测平均值（如果有异常值，则为中位数）并分析专家意见的方差，以评估你预测的不确定性。
- en: Why not just apply this reasoning to machine learning? Each of the ten experts
    can be thought of as a different learning algorithm, turning past data into models
    and current data into predictions. Say you have trained ten models from different
    model classes, using different hyperparameters and random seeds, all of which
    perform similarly well. Then this set of models is called a *Rashomon set*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不将这种推理应用于机器学习呢？这十个专家可以被视为不同的学习算法，将过去的数据转换为模型，将当前的数据转换为预测。假设你从不同的模型类别中训练了十个模型，使用了不同的超参数和随机种子，它们的表现都相当好。那么，这个模型集就被称为*罗生门集*。
- en: Say there is a well-trained model \(\hat{f}_{ref}\) as a reference point and
    you fix a certain admitted error \(\delta\). Then, \(S:=\lbrace \hat{f}_1,\dots,\hat{f}_k\rbrace\)
    is a Rashomon set to \(\hat{f}_{ref}\) if for all \(i\in\lbrace 1,\dots,k\rbrace\)
    holds that the performance of \(\hat{f}_i\) is no worse than \(\delta\), i.e.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设存在一个训练良好的模型 \(\hat{f}_{ref}\) 作为参考点，并且你设定了一个允许的误差 \(\delta\)。那么，\(S:=\lbrace
    \hat{f}_1,\dots,\hat{f}_k\rbrace\) 是 \(\hat{f}_{ref}\) 的一个罗生门集，如果对于所有 \(i\in\lbrace
    1,\dots,k\rbrace\) 都成立，即 \(\hat{f}_i\) 的性能不差于 \(\delta\)，即
- en: \[\mathbb{E}[L(\hat{f}_i(X),Y)] - \mathbb{E}[L(\hat{f}_{ref}(X),Y)] \leq \delta\].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: \[\mathbb{E}[L(\hat{f}_i(X),Y)] - \mathbb{E}[L(\hat{f}_{ref}(X),Y)] \leq \delta\].
- en: If you assume that your collection of models in the Rashomon set comes from
    an unbiased estimation process of the optimal model, you can estimate the expected
    model error by the variance of the predictions in the Rashomon set, i.e.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你假设罗生门集中的模型集合来自对最优模型的无偏估计过程，你可以通过罗生门集中预测的方差来估计预期的模型误差，即
- en: \[\widehat{\mathbb{V}}_{f^*}[f^*(x)]=\frac{1}{k-1}\sum_{\hat{f}_i\in S}L(\overline{\hat{f}}(x),\hat{f}_i(x)).\]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[\widehat{\mathbb{V}}_{f^*}[f^*(x)]=\frac{1}{k-1}\sum_{\hat{f}_i\in S}L(\overline{\hat{f}}(x),\hat{f}_i(x)).\]
- en: 'Assuming a certain distribution of errors (e.g., t-distribution) and IID errors
    in the Rashomon set, you can use the variance to define confidence intervals.
    For example, the \(\alpha\) confidence interval for the predictions would be:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设Rashomon集中存在一定的误差分布（例如，t分布）和独立同分布的误差，你可以使用方差来定义置信区间。例如，预测的\(\alpha\)置信区间会是：
- en: \[ CI_{\hat{Y}}=[\hat{f}_{ref}(x)\pm t_{1-\alpha/2}\sqrt{\widehat{\mathbb{V}}_{f^*}[f^*(x)]}]
    \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ CI_{\hat{Y}}=[\hat{f}_{ref}(x)\pm t_{1-\alpha/2}\sqrt{\widehat{\mathbb{V}}_{f^*}[f^*(x)]}]
    \]
- en: As with all strategies presented here, Rashomon sets can be used to quantify
    not only the uncertainty in predictions but also the uncertainty in estimated
    performance and properties. For example, Rashomon sets have been used to quantify
    the uncertainty in estimating feature importance [[22]](references.html#ref-fisher2019all)
    and feature effects [[23]](references.html#ref-molnar2023relating). Similarly,
    even if the predictions are already probabilities, as is common in classification
    models, Rashomon sets can be used to quantify higher-order uncertainties. For
    example, you can obtain imprecise probabilities in the form of intervals by taking
    the highest and lowest predicted probabilities in the Rashomon set as interval
    bounds.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与这里提出的所有策略一样，Rashomon集可以用来量化预测的不确定性，以及估计的性能和属性的不确定性。例如，Rashomon集已被用来量化估计特征重要性的不确定性
    [[22]](references.html#ref-fisher2019all) 和特征效应 [[23]](references.html#ref-molnar2023relating)。同样，即使预测已经是概率，这在分类模型中很常见，Rashomon集也可以用来量化更高阶的不确定性。例如，你可以通过取Rashomon集中最高和最低预测概率作为区间界限，以区间形式获得不精确的概率。
- en: Interestingly, in a very Rashomon-set fashion, ensemble methods like random
    forests implicitly provide an uncertainty quantification of the model error [[24]](references.html#ref-mentch2016quantifying).
    Each decision tree in the forest can be seen as one prediction model. Similarly,
    dropout in neural networks can be described as an ensemble method with implicit
    uncertainty quantification [[25]](references.html#ref-gal2016dropout).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，以一种非常类似《罗生门》风格的方式，集成方法如随机森林隐式地提供了模型误差的不确定性量化 [[24]](references.html#ref-mentch2016quantifying)。森林中的每一棵决策树都可以看作是一个预测模型。同样，神经网络中的dropout可以描述为一种具有隐式不确定性量化的集成方法
    [[25]](references.html#ref-gal2016dropout)。
- en: '12.6.6 The key to data uncertainty: sampling, resampling, repeated imputation'
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6.6 数据不确定性的关键：采样、重采样、重复插补
- en: 'The first error we mentioned in the context of data uncertainty is the sample
    error. In an ideal world, it would be easy to quantify: 1\. Take many independent,
    unbiased samples of different sizes from the same distribution. 2\. Quantify the
    variance in the resulting estimates (i.e., predictions, performance, or properties).
    This is nice in theory or in simulation studies. But in real life, data is usually
    both valuable and scarce. You want to use all available data to make your estimates
    as accurate as possible, not just waste 90% on uncertainty quantification.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据不确定性的背景下提到的第一个误差是样本误差。在理想世界中，它很容易量化：1. 从同一分布中取出许多独立、无偏的、不同大小的样本。2. 量化结果估计中的方差（即预测、性能或属性）。这在理论上是美好的，或者在模拟研究中是如此。但在现实生活中，数据通常既宝贵又稀缺。你希望使用所有可用的数据来尽可能准确地做出估计，而不仅仅是浪费90%在不确定性量化上。
- en: '**Resampling can quantify sample uncertainty, but it can systematically underestimate
    it**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**重采样可以量化样本不确定性，但它可能会系统地低估它**'
- en: 'Statisticians have therefore developed smart strategies to avoid wasting data
    under the umbrella term *resampling*. The idea: make your best estimate based
    on the entire dataset. To quantify the sampling uncertainty of that estimate,
    you study the variance when you repeatedly re-estimate the quantity of interest
    on different subsets of the full dataset.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，统计学家们开发了智能策略来避免在*重采样*的框架下浪费数据。其思路：基于整个数据集做出最佳估计。为了量化该估计的采样不确定性，你研究在重复对全数据集的不同子集重新估计感兴趣的数量时的方差。
- en: 'There are several approaches to resampling:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种重采样的方法：
- en: '*Bootstrapping* involves repeatedly drawing samples with replacement.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bootstrapping* 涉及重复地有放回地抽取样本。'
- en: In *subsampling*, you repeatedly draw samples without replacement.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*子采样*中，你反复抽取样本而不进行替换。
- en: With *cross-validation*, you split your data into k-junks of equal size. *Cross-validation*
    is particularly useful when you need separate data splits for training and estimation,
    as in performance estimation (see [Chapter 7](generalization.html)).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**交叉验证**，你将数据分成k-junks（k个大小相等的部分）。交叉验证在需要为训练和估计分别划分数据时特别有用，例如在性能估计中（见第7章[generalization.html]）。
- en: But be careful, with resampling approaches you run the danger of underestimating
    the variance and consequently the true uncertainty. The reason is that the samples
    you draw are not really independent. They come from the same overall dataset and
    share many instances. There are several strategies to deal with the underestimation,
    such as variance correction strategies like the *bias-corrected and accelerated
    bootstrap* [[26]](references.html#ref-diciccio1988review). For performance estimation,
    Nadeau and Bengio [[27]](references.html#ref-nadeau1999inference) suggest several
    correction factors to tackle underestimation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 但要小心，使用重采样方法，你可能会低估方差和真正的不确定性。原因是抽取的样本实际上并不独立。它们来自同一个总体数据集，并且共享许多实例。有几种策略可以处理低估，例如像**偏差校正和加速重抽样**（[26](references.html#ref-diciccio1988review)）这样的方差校正策略。对于性能估计，Nadeau和Bengio
    [[27](references.html#ref-nadeau1999inference)]建议几个校正因子来应对低估。
- en: '**Strategies for noisy or missing data**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理噪声或缺失数据的策略**'
- en: 'Measurement errors and missing data may look different from the outside, but
    when you think about it, they are similar. A very noisy feature value can just
    as well be seen as missing. And, a missing value that is imputed can also be considered
    noisy. This similarity is reflected in the shared data-cleaning strategies for
    dealing with them:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 测量误差和缺失数据在外观上可能不同，但当你仔细考虑时，它们是相似的。一个非常嘈杂的特征值可以被视为缺失。同样，被填补的缺失值也可以被视为噪声。这种相似性反映在处理它们的共享数据清理策略中：
- en: '**Remove the feature:** If a feature is generally super noisy and often missing,
    we recommend removing it. This way, you can ignore the data error but potentially
    have a higher feature selection error. Therefore, removing features may be a bad
    idea if the feature is highly informative about the target. For example, removing
    the snow-water equivalent from your forecast model will decrease forecast performance.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除特征：** 如果一个特征通常非常嘈杂且经常缺失，我们建议删除它。这样，你可以忽略数据错误，但可能会有更高的特征选择错误。因此，如果特征对目标非常具有信息量，删除特征可能不是一个好主意。例如，从你的预测模型中删除雪水当量将降低预测性能。'
- en: '**Remove individual data points:** If the values of a feature are missing *completely
    at random* in only a few cases, it may be fine to remove the affected data points.
    This way, no data uncertainty is introduced. Note that removing data reduces your
    data size and therefore increases the sampling error. Also, if the missingness
    or noise is not random, you may introduce selection bias into your dataset.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除单个数据点：** 如果一个特征的值在只有少数情况下完全随机缺失，删除受影响的数据点可能没问题。这样，不会引入数据不确定性。请注意，删除数据会减少你的数据量，从而增加抽样误差。此外，如果缺失或噪声不是随机的，你可能会将选择偏差引入你的数据集中。'
- en: '**Impute missing values:** If the missingness is random (MAR), we recommend
    using data imputation strategies. Ideally, the imputation process takes into account
    your domain knowledge and data dependencies. Note that imputing missing values
    based on domain knowledge is itself only a guess. The imputation will sometimes
    be incorrect.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填补缺失值：** 如果缺失是随机的（MNAR），我们建议使用数据填补策略。理想情况下，填补过程应考虑你的领域知识和数据依赖性。请注意，基于领域知识填补缺失值本身只是一个猜测。填补有时可能是不正确的。'
- en: '**Explicitly model uncertainty:** If the values of a feature are moderately
    noisy and you have a guess about how the noise is distributed, we recommend that
    you model the noise explicitly. This way, the uncertainty can be quantified by
    propagating the noise forward to the target estimate.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式地建模不确定性：** 如果一个特征的值适度嘈杂，并且你对噪声的分布有一个猜测，我们建议你显式地建模噪声。这样，可以通过将噪声传播到目标估计中来量化不确定性。'
- en: '**Improve the measurement:** If the missingness is not at random (MNAR), the
    only good strategy is to get rid of the reason for the missingness and collect
    new data.*  *## 12.7 Minimizing uncertainty'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进测量：** 如果缺失不是随机的（MNAR），唯一的良好策略是消除缺失的原因并收集新的数据。'
- en: 'Great, so now you have identified uncertainties and found ways to quantify
    them – but how do you minimize them? In most contexts, you want to reduce uncertainty:
    For example, when the water supply model tells you that it is uncertain whether
    there will be a drought or a flood in the Beaver River. How should you take precautions
    if the uncertainty is high?'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在你已经确定了不确定性并找到了量化它们的方法——但如何最小化它们呢？在大多数情况下，你希望减少不确定性：例如，当水资源模型告诉你贝弗河可能会发生干旱或洪水时，不确定性很高，你应该如何采取预防措施？
- en: 'Most, if not all, uncertainties can, in principle, be minimized. Some are just
    harder to minimize than others. While the goal is to minimize that total uncertainty,
    the only strategy for doing so is to minimize the individual uncertainties that
    arise from different sources:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，大多数，如果不是所有的不确定性都可以最小化。有些比其他一些更难最小化。虽然目标是减少总的不确定性，但实现这一目标的唯一策略是减少来自不同来源的个别不确定性：
- en: '**Task uncertainty**'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务不确定性**'
- en: To minimize the expected operationalization error, put considerable effort into
    an appropriate operationalization. For example, placing only a single sensor in
    the river might be a bad idea if the water flows vary along the river.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化预期的操作化误差，投入相当大的努力进行适当的操作化。例如，如果河流中的水流变化，只在河流中放置一个传感器可能不是一个好主意。
- en: To minimize the expected feature selection error, select features that are highly
    predictive and contain little noise. Omit features that do not add new information
    to your model.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化预期的特征选择误差，选择高度预测性和噪声含量小的特征。省略那些不会为你的模型增加新信息的特征。
- en: '**Distribution uncertainty**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布不确定性**'
- en: To minimize the expected distribution error, collect or simulate training data
    that is representative of the deployment distribution. For example, training your
    model only on data from the 1970s may be a bad idea given the changed distribution
    of extreme weather events.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化预期的分布误差，收集或模拟代表部署分布的训练数据。例如，仅使用1970年代的数据来训练模型，考虑到极端天气事件分布的变化，可能不是一个好主意。
- en: '**Model uncertainty**'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型不确定性**'
- en: To minimize model error, choose an appropriate inductive bias in modeling. Strongly
    constraining the model class and testing a few hyperparameters will result in
    a high bias. Choosing too broad a model class and unconstrained hyperparameter
    search will result in high variance. To minimize uncertainty, use your domain
    knowledge to constrain your model search to a minimal but sufficiently expressive
    model class to capture the dependency in the data and plausible hyperparameter
    settings [[28]](references.html#ref-semenova2024path). In water supply forecasting,
    it is recommended to include physical constraints because they constrain the appropriate
    model class and consequently reduce model uncertainty [[29]](references.html#ref-fleming2021augmenting).
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化模型误差，在建模时选择合适的归纳偏差。严格限制模型类别并测试几个超参数会导致高偏差。选择过于宽泛的模型类别和不加约束的超参数搜索会导致高方差。为了最小化不确定性，使用你的领域知识将模型搜索限制在最小但足够表达性的模型类别，以捕捉数据中的依赖关系和合理的超参数设置
    [[28]](references.html#ref-semenova2024path)。在水资源预测中，建议包括物理约束，因为它们限制了适当的模型类别，从而减少了模型不确定性
    [[29]](references.html#ref-fleming2021augmenting)。
- en: 'To find an appropriate model class and hyperparameters, two strategies can
    be helpful: Either start with simple models and increase complexity until there
    is no performance gain or start with a complex model and sequentially decrease
    complexity until there is a performance drop.'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了找到一个合适的模型类别和超参数，有两种策略可能会有帮助：要么从简单的模型开始，逐渐增加复杂性，直到没有性能提升，要么从复杂的模型开始，逐次降低复杂性，直到性能下降。
- en: '**Data uncertainty**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不确定性**'
- en: To minimize the expected sample error, the best strategy is to collect more
    data. Active learning can help you select the data that will reduce uncertainty
    the most (see the active learning in [Chapter 11](robustness.html)). However,
    active learning is impossible in domains like water supply forecasting, where
    control over the data-generating mechanism is limited.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化预期的样本误差，最佳策略是收集更多的数据。主动学习可以帮助你选择最能减少不确定性的数据（参见第11章中的主动学习[robustness.html]）。然而，在像水资源预测这样的领域，由于对数据生成机制的控制有限，主动学习是不可能的。
- en: Collect multiple labels for an instance to minimize the expected data error
    in the face of label noise. If there is feature noise, either reduce the noise
    through improved measurement or replace the feature with an equally predictive
    but less noisy feature. For example, move a snow sensor from a mountain area with
    a lot of skiers to a quieter area.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最小化面对标签噪声时的预期数据误差，为实例收集多个标签。如果有特征噪声，可以通过改进测量来减少噪声，或者用同样具有预测性但噪声更小的特征来替换该特征。例如，将一个雪传感器从有很多滑雪者的山区移至一个更安静的区域。
- en: 'If the problem is missing data, investigate the cause of the missingness: in
    many cases, the missingness can be counteracted by a careful measurement, like
    replacing an unreliable sensor. If the missingness cannot be counteracted, consider
    removing the feature.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题是缺失数据，调查缺失的原因：在许多情况下，可以通过仔细的测量来对抗缺失，例如更换一个不可靠的传感器。如果缺失无法对抗，考虑删除该特征。
- en: 12.8 Calibrating uncertainty measures
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.8 校准不确定性度量
- en: Say you have studied the sources of uncertainty, quantified them, and perhaps
    even minimized them. Of course, you want to interpret the values in your uncertainty
    estimates as true probabilities. But there is often a problem with these “probabilities”.
    The fact that your uncertainty estimate has a 90% confidence does not necessarily
    mean that the actual probability is 90%. Probabilities can be miscalibrated –
    the estimated probabilities don’t match the true outcome probabilities.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经研究了不确定性的来源，量化了它们，甚至可能最小化了它们。当然，你希望将你的不确定性估计中的值解释为真实概率。但“概率”通常存在问题。你的不确定性估计有90%的置信度并不意味着实际概率是90%。概率可能被校准不当——估计的概率与真实结果概率不匹配。
- en: 'What does calibration mean in more formal terms? Say you are in a regression
    setting, such as the water supply forecasting problem. We denote the prediction
    by \(\hat{Y}\) and the uncertainty estimate for \(\hat{Y}\) with confidence \(c\)
    by \(u_c(\hat{Y})\). We say that the uncertainties are perfectly calibrated if
    for all \(c\in[0,1]\) holds:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在更正式的术语中，校准意味着什么？假设你处于回归设置中，例如供水预测问题。我们用 \(\hat{Y}\) 表示预测，用置信度 \(c\) 表示 \(\hat{Y}\)
    的不确定性估计，记为 \(u_c(\hat{Y})\)。我们说不确定性完全校准，如果对于所有 \(c\in[0,1]\) 都成立：
- en: \[ \mathbb{P}(Y\in u_c(\hat{Y}))=c. \]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}(Y\in u_c(\hat{Y}))=c. \]
- en: For example, the hydrologist’s certainty of 10% that the water supply in the
    Beaver River will be less than 6.3 thousand acre-feet is calibrated if the actual
    water supply in the Beaver River is less than 6.3 thousand acre-feet only 10%
    of the time. Calibration can be similarly defined for classification tasks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果实际供水在比弗河中只有10%的时间少于6.3千英亩英尺，那么水文学家对比弗河水供应将少于6.3千英亩英尺的10%的确定性就是校准的。对于分类任务，校准可以类似地定义。
- en: '*Calibration plots* allow you to assess whether probabilities of a classifier
    are calibrated or not. You compare the predicted and quantified uncertainty with
    the empirical frequency. For this you pick an outcome class and bin the data points
    by output score, for example into 0-10%, 10-20%, …, 90-100%. On the x-axes, you
    plot the confidence bins and on the y-axes the empirical frequencies with which
    the class matches the chosen class. [Figure 12.4](#fig-calibration-plot) shows
    an example of a calibration plot, where we compare the mean probabilities with
    the actual probabilities. If the plot shows a boring linear curve with a slope
    of 1, congratulations, your model is perfectly calibrated. Evaluating calibration
    using relative frequencies shows the close relationship between calibration and
    a frequentist interpretation of probability.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*校准图* 允许你评估分类器的概率是否校准。你将预测和量化的不确定性与经验频率进行比较。为此，你选择一个结果类别，并将数据点按输出分数进行分箱，例如0-10%，10-20%，…，90-100%。在x轴上，你绘制置信度分箱，在y轴上绘制匹配所选类别的经验频率。[图12.4](#fig-calibration-plot)
    展示了一个校准图的示例，其中我们比较了平均概率与实际概率。如果图表显示一个无聊的线性曲线，斜率为1，恭喜你，你的模型已经完全校准。使用相对频率评估校准显示了校准与频率主义概率解释之间的密切关系。'
- en: '![](../Images/8e4ecacdf175750c756c1e3a190333ba.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8e4ecacdf175750c756c1e3a190333ba.png)'
- en: 'Figure 12.4: Example of a calibration plot: The logistic regression model is
    not perfectly calibrated, especially for predicted probabilities between 0.5 and
    0.7\. But it could be worse.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：校准图的示例：逻辑回归模型并未完全校准，尤其是在预测概率在0.5到0.7之间时。但它本可以更糟。
- en: 'Miscalibration is not just an occasional problem. It is a fair default assumption
    that any uncertainty estimate is miscalibrated:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 校准错误不仅仅是一个偶尔的问题。任何不确定性估计都是校准错误的合理默认假设：
- en: '**Too short prediction intervals:** For the water flow prediction example,
    we were interested in prediction intervals that cover 80% of the output. The interval
    was created by predicting the 10% and 90% quantiles. The interval width expresses
    uncertainty: The larger the interval, the more uncertain the water flow. However,
    quantile regression has the problem that quantiles are often drawn toward the
    median [[30]](references.html#ref-takeuchi2006nonparametric), an effect that becomes
    stronger the closer the desired quantile levels are to 0% and 100%. As a result,
    intervals based on quantile regression tend to be too short and underestimate
    uncertainty.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测区间太短：** 对于水流预测的例子，我们感兴趣的是覆盖80%输出的预测区间。该区间是通过预测10%和90%分位数创建的。区间宽度表示不确定性：区间越大，水流的不确定性就越大。然而，分位数回归有一个问题，即分位数往往趋向于中位数
    [[30]](references.html#ref-takeuchi2006nonparametric)，当期望的分位数水平接近0%和100%时，这种效应会变得更强烈。因此，基于分位数回归的区间往往太短，低估了不确定性。'
- en: '**Bootstrapping also undercovers:** Remember that in bootstrapping you repeatedly
    sample data with replacement from the training data. Because the bootstrapped
    datasets are highly correlated, they underestimate the true uncertainty unless
    you correct for this bias.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自举也低估了：** 记住，在自举过程中，你反复从训练数据中用放回抽样方式获取数据。由于自举数据集高度相关，除非你纠正这种偏差，否则它们会低估真实的不确定性。'
- en: '**Bayesian models rely on assumptions:** In theory, Bayesian models propagate
    uncertainty perfectly. You could look at the predictive posterior distribution
    and, for example, output credible intervals. But they are calibrated (in the frequentist
    sense) only if your assumptions about likelihood, priors, and so on are correct.
    Hint: they are unlikely to be correct.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯模型依赖于假设：** 理论上，贝叶斯模型完美地传播不确定性。你可以查看预测后验分布，例如，输出可信区间。但它们只有在你的似然、先验等假设正确时才会校准（在频率主义意义上）。提示：它们很可能是不正确的。'
- en: For each miscalibrated uncertainty quantification method, there are several
    suggestions on how to fix it. For example, to fix miscalibrated probability outputs,
    you could use post-processing methods such as Platt’s logistic model [[31]](references.html#ref-platt1999probabilistic)
    and Isotonic Calibration. A more general framework for dealing with all these
    problems is conformal prediction, which can make any uncertainty measure “conformal”
    – with coverage guarantees.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种校准错误的不确定性量化方法，都有几种修复建议。例如，为了修复校准错误的概率输出，你可以使用后处理方法，如Platt的逻辑模型 [[31]](references.html#ref-platt1999probabilistic)
    和等调校准。处理所有这些问题的更通用框架是正规化预测，它可以使任何不确定性度量“正规化” – 带有覆盖保证。
- en: '**Conformal prediction, a set-based approach to uncertainty**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**正规化预测，一种基于集合的不确定性方法**'
- en: 'Standard uncertainty quantification treats models and predictions as fixed
    and returns the uncertainties. Conformal prediction takes the opposite approach:
    The modeler must specify the desired confidence level (related to uncertainty),
    and a conformal prediction procedure changes the model output to conform to the
    confidence level. Consider water supply forecasting: Normally, you would simply
    predict a scalar value and provide the quantified uncertainty estimate in some
    form. With conformal prediction, you first set a confidence level and then modify
    your confidence interval to cover the true value with the desired confidence.
    Thus, conformal prediction comes with a *coverage guarantee* – at least for data
    that is exchangeable, the prediction set corresponds to the desired confidence
    level. But beware, the coverage guarantee is often marginal, i.e. it only applies
    to the average of the data.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 标准不确定性量化将模型和预测视为固定，并返回不确定性。正规化预测采取相反的方法：模型师必须指定所需的置信水平（与不确定性相关），然后正规化预测过程会改变模型输出以符合置信水平。考虑水资源预测：通常，你只会预测一个标量值，并以某种形式提供量化的不确定性估计。使用正规化预测，你首先设置置信水平，然后修改你的置信区间以覆盖期望的真实值。因此，正规化预测带有*覆盖保证*
    – 至少对于可交换的数据，预测集对应于期望的置信水平。但请注意，覆盖保证通常是边际的，即它仅适用于数据的平均值。
- en: 'Conformal prediction is more than a simple algorithm, it is a whole framework
    that allows to conformalize different uncertainty scores:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正规化预测不仅仅是简单的算法，而是一个完整的框架，允许将不同的不确定性分数正规化：
- en: Turn a quantile interval into a conformalized quantile interval [[32]](references.html#ref-romano2019conformalized)).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分位数区间转换为一致性分位数区间 [[32]](references.html#ref-romano2019conformalized)).
- en: Turn a class probability vector into a set of classes [[33]](references.html#ref-romano2020classification).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将类别概率向量转换为类别集合 [[33]](references.html#ref-romano2020classification)。
- en: Turn a probability score into a probability range (Venn-ABERS predictor) [[34]](references.html#ref-vovk2003self).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将概率分数转换为概率范围（Venn-ABERS预测器） [[34]](references.html#ref-vovk2003self)。
- en: There is a simple recipe for conformal prediction – *training*, *calibration*,
    and *prediction*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性预测有一个简单的配方——*训练*、*校准*和*预测*。
- en: '**Training:**'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练：**'
- en: You split the training data into training and calibration data.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将训练数据分为训练数据和校准数据。
- en: Train the model using the training data.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练数据训练模型。
- en: '**Calibration:**'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**校准：**'
- en: Compute uncertainty scores (also called nonconformity scores) for the calibration
    data.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为校准数据计算不确定性分数（也称为非一致性分数）。
- en: Sort the scores from certain to uncertain (low to high).
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照从确定到不确定（低到高）的顺序排列分数。
- en: Decide on a confidence level.
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定置信水平。
- en: Find the quantile where \(1-\alpha\) of the nonconformity scores are smaller.
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到使 \(1-\alpha\) 的非一致性分数较小的分位数。
- en: '**Prediction:**'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测：**'
- en: Compute nonconformity scores for the new data.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为新数据计算非一致性分数。
- en: Select all predictions that produce a score below \(\hat{q}\).
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择所有产生分数低于 \(\hat{q}\) 的预测。
- en: These predictions form the prediction set/interval.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些预测形成预测集/区间。
- en: Nothing comes for free and neither does conformal prediction. The “payment”
    is the additional calibration data you need, which can complicate the training
    process. Small calibration sets will lead to large prediction sets, possibly to
    the point where they are no longer useful. Furthermore, the calibration data must
    be interchangeable with the training data, which can be a problem with time series
    data, for example. However, there are also conformal procedures for time series
    that rely on a few more assumptions. Otherwise, conformal prediction is a very
    versatile approach.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 任何东西都不是免费的，一致性预测也不例外。“代价”是您需要的额外校准数据，这可能会使训练过程复杂化。小的校准集会导致大的预测集，可能到它们不再有用的程度。此外，校准数据必须与训练数据可互换，例如，对于时间序列数据可能会出现问题。然而，也存在适用于时间序列的一致性程序，这些程序基于更多的假设。否则，一致性预测是一个非常灵活的方法。
- en: '*Introduction to Conformal Prediction* *![](../Images/5e261fd75e8c61ad52e2a4b7b3fb8536.png)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*《一致性预测入门》* *![](../Images/5e261fd75e8c61ad52e2a4b7b3fb8536.png)*'
- en: To learn more about conformal prediction, check out the book [Introduction to
    Conformal Prediction with Python](https://christophmolnar.com/books/conformal-prediction/).*  *##
    12.9 Understand uncertainty to gain knowledge
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于一致性预测的信息，请参阅书籍[《Python一致性预测入门》](https://christophmolnar.com/books/conformal-prediction/).*  *##
    12.9 理解不确定性以获得知识
- en: In this chapter, you learned about the many different sources of error in machine
    learning pipelines and how they introduce uncertainty into your estimates. While
    uncertainty can be disentangled in theory, it is difficult to disentangle in practice
    – you can usually only quantify the overall uncertainty. Nevertheless, it is helpful
    to separate uncertainties conceptually, especially if you are looking for strategies
    to minimize uncertainty.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了机器学习管道中许多不同的错误来源以及它们如何将不确定性引入您的估计。虽然理论上可以分解不确定性，但在实践中很难分解——您通常只能量化整体不确定性。尽管如此，从概念上分离不确定性是有帮助的，尤其是如果您正在寻找最小化不确定性的策略。
- en: Uncertainty should not be seen as just an add-on. Uncertainty quantification
    is essential, especially if you want to gain new knowledge with machine learning
    or act on machine learning predictions. To get a realistic picture of the true
    uncertainty, all sources of uncertainty should be considered
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性不应被视为附加的。不确定性量化是必不可少的，尤其是如果您想通过机器学习获得新知识或对机器学习预测采取行动。为了获得真实不确定性的现实图景，应考虑所有不确定性来源
- en: When it comes to quantifying uncertainty, we recommend that scientists be pragmatists.
    You can use Bayesian techniques if you have domain knowledge that allows you to
    set a reasonable prior. Otherwise, frequentist approaches to uncertainty quantification
    can provide you with (calibrated) uncertainties under modest assumptions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到量化不确定性时，我们建议科学家们采取实用主义的态度。如果您有领域知识，允许您设置合理的先验，则可以使用贝叶斯技术。否则，在适度假设下，频率主义的不确定性量化方法可以为您提供（校准的）不确定性。
- en: 'Uncertainty is closely related to many chapters in this book:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性与此书中的许多章节密切相关：
- en: Theoretically, the roots of uncertainty quantification lie in our theory of
    generalization from [Chapter 7](generalization.html). Concepts such as generalization
    error, cross-validation, or biased sampling can be found in both chapters.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从理论上讲，不确定性量化的根源在于我们从[第7章](generalization.html)中推广的通用理论。例如，泛化误差、交叉验证或有偏抽样等概念可以在这两章中找到。
- en: Many approaches to minimizing uncertainty rely on domain knowledge (see [Chapter
    8](domain.html)). Reliable domain knowledge can significantly minimize task, distribution,
    model, and data uncertainty.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多最小化不确定性的方法依赖于领域知识（参见[第8章](domain.html)）。可靠的领域知识可以显著减少任务、分布、模型和数据的不确定性。
- en: The uncertainty framework presented here is quite general. It can be applied
    to interpretability techniques (see [Chapter 9](interpretability.html)) and even
    to causal effect estimation (see [Chapter 10](causality.html)).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里提出的不确定性框架相当通用。它可以应用于可解释性技术（参见[第9章](interpretability.html)），甚至可以应用于因果效应估计（参见[第10章](causality.html)）。
- en: Uncertainty has close ties to the robustness in [Chapter 11](robustness.html),
    especially when it comes to distribution uncertainty and strategies to reduce
    data uncertainty like active learning. Conceptually, the link is that robustness
    gives guarantees for worst-case error, while uncertainty gives guarantees in expectation
    [[35]](references.html#ref-freiesleben2023beyond).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性紧密关联到[第11章](robustness.html)中的鲁棒性，尤其是在分布不确定性和减少数据不确定性的策略（如主动学习）方面。从概念上讲，这种联系是鲁棒性为最坏情况下的误差提供保证，而不确定性在期望中提供保证
    [[35]](references.html#ref-freiesleben2023beyond)。
- en: We hope we have minimized your uncertainty about uncertainty with this chapter…
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这一章能最大限度地减少你对不确定性的不确定性…
- en: '[1]C. Gruber, P. O. Schenk, M. Schierholz, F. Kreuter, and G. Kauermann, “Sources
    of Uncertainty in Machine Learning – A Statisticians’ View.” arXiv, May 2023\.
    doi: [10.48550/arXiv.2305.16703](https://doi.org/10.48550/arXiv.2305.16703).[2]S.
    Hoffmann, F. Schönbrodt, R. Elsas, R. Wilson, U. Strasser, and A.-L. Boulesteix,
    “The multiplicity of analysis strategies jeopardizes replicability: Lessons learned
    across disciplines,” *Royal Society Open Science*, vol. 8, no. 4, p. 201925, 2021,
    doi: [10.1098/rsos.201925](https://doi.org/10.1098/rsos.201925).[3]E. Begoli,
    T. Bhattacharya, and D. Kusnezov, “The need for uncertainty quantification in
    machine-assisted medical decision making,” *Nature Machine Intelligence*, vol.
    1, no. 1, pp. 20–23, 2019, doi: [10.1038/s42256-018-0004-1](https://doi.org/10.1038/s42256-018-0004-1).[4]E.
    Hüllermeier and W. Waegeman, “Aleatoric and epistemic uncertainty in machine learning:
    An introduction to concepts and methods,” *Machine Learning*, vol. 110, no. 3,
    pp. 457–506, Mar. 2021, doi: [10.1007/s10994-021-05946-3](https://doi.org/10.1007/s10994-021-05946-3).[5]C.
    List, “Levels: Descriptive, explanatory, and ontological,” *Noûs*, vol. 53, no.
    4, pp. 852–883, 2019.[6]M. J. Van der Laan and S. Rose, *Targeted learning*, vol.
    1\. Springer, 2011\. doi: [10.1007/978-1-4419-9782-1](https://doi.org/10.1007/978-1-4419-9782-1).[7]T.
    Freiesleben, G. König, C. Molnar, and Á. Tejero-Cantero, “Scientific inference
    with interpretable machine learning: Analyzing models to learn about real-world
    phenomena,” *Minds and Machines*, vol. 34, no. 3, p. 32, 2024, doi: [10.1007/s11023-024-09691-z](https://doi.org/10.1007/s11023-024-09691-z).[8]P.
    Domingos, “A unified bias-variance decomposition,” in *Proceedings of 17th international
    conference on machine learning*, Morgan Kaufmann Stanford, 2000, pp. 231–238.[9]A.
    Diamantopoulos, P. Riefler, and K. P. Roth, “Advancing formative measurement models,”
    *Journal of business research*, vol. 61, no. 12, pp. 1203–1218, 2008, doi: [10.1016/j.jbusres.2008.01.009](https://doi.org/10.1016/j.jbusres.2008.01.009).[10]D.
    Carpentras, “We urgently need a culture of multi-operationalization in psychological
    research,” *Communications Psychology*, vol. 2, no. 1, p. 32, 2024.[11]T. Hastie,
    R. Tibshirani, J. H. Friedman, and J. H. Friedman, *The elements of statistical
    learning: Data mining, inference, and prediction*, vol. 2\. Springer, 2009.[12]G.
    Cybenko, “Approximation by superpositions of a sigmoidal function,” *Mathematics
    of control, signals and systems*, vol. 2, no. 4, pp. 303–314, 1989, doi: [10.1007/BF02551274](https://doi.org/10.1007/BF02551274).[13]K.
    Hornik, “Approximation capabilities of multilayer feedforward networks,” *Neural
    networks*, vol. 4, no. 2, pp. 251–257, 1991, doi: [10.1016/0893-6080(91)90009-T](https://doi.org/10.1016/0893-6080(91)90009-T).[14]P.
    R. Halmos, *Measure theory*, vol. 18\. Springer, 2013\. doi: [10.1007/978-1-4684-9440-2](https://doi.org/10.1007/978-1-4684-9440-2).[15]F.
    Hutter, L. Kotthoff, and J. Vanschoren, *Automated machine learning: Methods,
    systems, challenges*. Springer Nature, 2019\. doi: [10.1007/978-3-030-05318-5](https://doi.org/10.1007/978-3-030-05318-5).[16]B.
    Jalaian, M. Lee, and S. Russell, “Uncertain context: Uncertainty quantification
    in machine learning,” *AI Magazine*, vol. 40, no. 4, pp. 40–49, 2019, doi: [10.1609/aimag.v40i4.4812](https://doi.org/10.1609/aimag.v40i4.4812
    ) .[17]Y. Gal, “Uncertainty in deep learning,” 2016.[18]A. Kendall and Y. Gal,
    “What uncertainties do we need in bayesian deep learning for computer vision?”
    *Advances in neural information processing systems*, vol. 30, 2017.[19]L. S. Gottfredson,
    “The general intelligence factor.” Scientific American, Incorporated, 1998.[20]J.
    Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman, “Distribution-Free
    Predictive Inference for Regression,” *Journal of the American Statistical Association*,
    vol. 113, no. 523, pp. 1094–1111, Jul. 2018, doi: [10.1080/01621459.2017.1307116](https://doi.org/10.1080/01621459.2017.1307116).[21]C.
    Strobl, A.-L. Boulesteix, T. Kneib, T. Augustin, and A. Zeileis, “Conditional
    variable importance for random forests,” *BMC bioinformatics*, vol. 9, pp. 1–11,
    2008.[22]A. Fisher, C. Rudin, and F. Dominici, “All Models are Wrong, but Many
    are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction
    Models Simultaneously,” *Journal of machine learning research : JMLR*, vol. 20,
    p. 177, 2019, Accessed: Jan. 16, 2024\. [Online]. Available: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/)[23]C.
    Molnar *et al.*, “Relating the Partial Dependence Plot and Permutation Feature
    Importance to the Data Generating Process,” in *Explainable Artificial Intelligence*,
    L. Longo, Ed., in Communications in Computer and Information Science. Cham: Springer
    Nature Switzerland, 2023, pp. 456–479\. doi: [10.1007/978-3-031-44064-9_24](https://doi.org/10.1007/978-3-031-44064-9_24).[24]L.
    Mentch and G. Hooker, “Quantifying uncertainty in random forests via confidence
    intervals and hypothesis tests,” *Journal of Machine Learning Research*, vol.
    17, no. 26, pp. 1–41, 2016.[25]Y. Gal and Z. Ghahramani, “Dropout as a bayesian
    approximation: Representing model uncertainty in deep learning,” in *International
    conference on machine learning*, PMLR, 2016, pp. 1050–1059.[26]T. J. Diciccio
    and J. P. Romano, “A review of bootstrap confidence intervals,” *Journal of the
    Royal Statistical Society Series B: Statistical Methodology*, vol. 50, no. 3,
    pp. 338–354, 1988.[27]C. Nadeau and Y. Bengio, “Inference for the generalization
    error,” *Advances in neural information processing systems*, vol. 12, 1999.[28]L.
    Semenova, H. Chen, R. Parr, and C. Rudin, “A path to simpler models starts with
    noise,” *Advances in neural information processing systems*, vol. 36, 2024.[29]S.
    W. Fleming, V. V. Vesselinov, and A. G. Goodbody, “Augmenting geophysical interpretation
    of data-driven operational water supply forecast modeling for a western US river
    using a hybrid machine learning approach,” *Journal of Hydrology*, vol. 597, p.
    126327, 2021.[30]I. Takeuchi, Q. V. Le, T. D. Sears, A. J. Smola, and C. Williams,
    “Nonparametric quantile estimation.” *Journal of machine learning research*, vol.
    7, no. 7, 2006.[31]J. Platt *et al.*, “Probabilistic outputs for support vector
    machines and comparisons to regularized likelihood methods,” *Advances in large
    margin classifiers*, vol. 10, no. 3, pp. 61–74, 1999.[32]Y. Romano, E. Patterson,
    and E. Candes, “Conformalized quantile regression,” *Advances in neural information
    processing systems*, vol. 32, 2019.[33]Y. Romano, M. Sesia, and E. Candes, “Classification
    with valid and adaptive coverage,” *Advances in Neural Information Processing
    Systems*, vol. 33, pp. 3581–3591, 2020.[34]V. Vovk, G. Shafer, and I. Nouretdinov,
    “Self-calibrating probability forecasting,” *Advances in neural information processing
    systems*, vol. 16, 2003.[35]T. Freiesleben and T. Grote, “Beyond generalization:
    A theory of robustness in machine learning,” *Synthese*, vol. 202, no. 4, p. 109,
    2023, doi: [10.1007/s11229-023-04334-9](https://doi.org/10.1007/s11229-023-04334-9).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]C. Gruber, P. O. Schenk, M. Schierholz, F. Kreuter, 和 G. Kauermann, “机器学习中不确定性的来源
    – 统计学家的观点.” arXiv, 五月 2023\. doi: [10.48550/arXiv.2305.16703](https://doi.org/10.48550/arXiv.2305.16703).[2]S.
    Hoffmann, F. Schönbrodt, R. Elsas, R. Wilson, U. Strasser, 和 A.-L. Boulesteix,
    “分析策略的多样性威胁了可重复性：跨学科的经验教训,” *皇家学会开放科学*, 第 8 卷，第 4 期，p. 201925，2021，doi: [10.1098/rsos.201925](https://doi.org/10.1098/rsos.201925).[3]E.
    Begoli, T. Bhattacharya, 和 D. Kusnezov, “在机器辅助医疗决策中量化不确定性的必要性,” *自然机器智能*, 第 1
    卷，第 1 期，pp. 20–23，2019，doi: [10.1038/s42256-018-0004-1](https://doi.org/10.1038/s42256-018-0004-1).[4]E.
    Hüllermeier 和 W. Waegeman, “机器学习中的随机性和认知不确定性：概念和方法简介,” *机器学习*, 第 110 卷，第 3 期，pp.
    457–506，2021 年 3 月，doi: [10.1007/s10994-021-05946-3](https://doi.org/10.1007/s10994-021-05946-3).[5]C.
    List, “层次：描述性、解释性和本体论,” *诺斯*, 第 53 卷，第 4 期，pp. 852–883，2019。[6]M. J. Van der Laan
    和 S. Rose, *目标学习*, 第 1 卷\. Springer, 2011\. doi: [10.1007/978-1-4419-9782-1](https://doi.org/10.1007/978-1-4419-9782-1).[7]T.
    Freiesleben, G. König, C. Molnar, 和 Á. Tejero-Cantero, “可解释机器学习中的科学推理：分析模型以了解现实世界现象,”
    *思维与机器*, 第 34 卷，第 3 期，p. 32，2024，doi: [10.1007/s11023-024-09691-z](https://doi.org/10.1007/s11023-024-09691-z).[8]P.
    Domingos, “统一的偏差-方差分解,” 在 *第 17 届国际机器学习会议论文集*，Morgan Kaufmann Stanford, 2000，pp.
    231–238.[9]A. Diamantopoulos, P. Riefler, 和 K. P. Roth, “推进形成性测量模型,” *商业研究杂志*,
    第 61 卷，第 12 期，pp. 1203–1218，2008，doi: [10.1016/j.jbusres.2008.01.009](https://doi.org/10.1016/j.jbusres.2008.01.009).[10]D.
    Carpentras, “我们迫切需要心理学研究中多操作化的文化，” *沟通心理学*, 第 2 卷，第 1 期，p. 32，2024。[11]T. Hastie,
    R. Tibshirani, J. H. Friedman, 和 J. H. Friedman, *统计学习的要素：数据挖掘、推理和预测*, 第 2 卷\.
    Springer, 2009。[12]G. Cybenko, “通过 sigmoid 函数的叠加进行逼近,” *控制、信号和系统数学*, 第 2 卷，第 4
    期，pp. 303–314，1989，doi: [10.1007/BF02551274](https://doi.org/10.1007/BF02551274).[13]K.
    Hornik, “多层前馈网络的逼近能力,” *神经网络*, 第 4 卷，第 2 期，pp. 251–257，1991，doi: [10.1016/0893-6080(91)90009-T](https://doi.org/10.1016/0893-6080(91)90009-T).[14]P.
    R. Halmos, *测度论*, 第 18 卷\. Springer, 2013\. doi: [10.1007/978-1-4684-9440-2](https://doi.org/10.1007/978-1-4684-9440-2).[15]F.
    Hutter, L. Kotthoff, 和 J. Vanschoren, *自动机器学习：方法、系统、挑战*. Springer Nature, 2019\.
    doi: [10.1007/978-3-030-05318-5](https://doi.org/10.1007/978-3-030-05318-5).[16]B.
    Jalaian, M. Lee, 和 S. Russell, “不确定的上下文：机器学习中的不确定性量化,” *人工智能杂志*, 第 40 卷，第 4 期，pp.
    40–49，2019，doi: [10.1609/aimag.v40i4.4812](https://doi.org/10.1609/aimag.v40i4.4812)
    .[17]Y. Gal, “深度学习中的不确定性，” 2016。[18]A. Kendall 和 Y. Gal, “在计算机视觉的贝叶斯深度学习中我们需要哪些不确定性？”
    *神经信息处理系统进展*, 第 30 卷，2017。[19]L. S. Gottfredson, “一般智力因素.” 科学美国人公司，1998。[20]J.
    Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, 和 L. Wasserman, “回归的无分布预测推断，” *美国统计学会杂志*,
    第 113 卷，第 523 期，pp. 1094–1111，2018 年 7 月，doi: [10.1080/01621459.2017.1307116](https://doi.org/10.1080/01621459.2017.1307116).[21]C.
    Strobl, A.-L. Boulesteix, T. Kneib, T. Augustin, 和 A. Zeileis, “随机森林的条件变量重要性，”
    *生物信息学杂志*, 第 9 卷，pp. 1–11，2008。[22]A. Fisher, C. Rudin, 和 F. Dominici, “所有模型都是错误的，但许多是有用的：通过同时研究整个预测模型类别来学习变量的重要性，”
    *机器学习研究杂志: JMLR*, 第 20 卷，p. 177，2019，访问日期：2024 年 1 月 16 日\. [在线]. 可用: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/)[23]C.
    Molnar 等人， “部分依赖图和置换特征重要性与数据生成过程的关系，” 在 *可解释人工智能*，L. Longo 编，在计算机与信息科学通信中. Cham:
    Springer Nature Switzerland, 2023，pp. 456–479\. doi: [10.1007/978-3-031-44064-9_24](https://doi.org/10.1007/978-3-031-44064-9_24).[24]L.
    Mentch 和 G. Hooker， “通过置信区间和假设检验量化随机森林的不确定性，” *机器学习研究杂志*, 第 17 卷，第 26 期，pp. 1–41，2016。[25]Y.
    Gal 和 Z. Ghahramani， “Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性，” 在 *国际机器学习会议*，PMLR，2016，pp.
    1050–1059。[26]T. J. Diciccio 和 J. P. Romano， “自举置信区间的回顾，” *皇家统计学会系列 B：统计方法论*,
    第 50 卷，第 3 期，pp. 338–354，1988。[27]C. Nadeau 和 Y. Bengio， “对泛化错误的推断，” *神经信息处理系统进展*,
    第 12 卷，1999。[28]L. Semenova, H. Chen, R. Parr, 和 C. Rudin， “从噪声开始简化模型的道路，” *神经信息处理系统进展*,
    第 36 卷，2024。[29]S. W. Fleming, V. V. Vesselinov, 和 A. G. Goodbody， “使用混合机器学习方法增强美国西部河流数据驱动操作供水预测建模的地球物理解释，”
    *水文杂志*, 第 597 卷，p. 126327，2021。[30]I. Takeuchi, Q. V. Le, T. D. Sears, A. J. Smola,
    和 C. Williams， “非参数分位数估计.” *机器学习研究杂志*, 第 7 卷，第 7 期，2006。[31]J. Platt 等人， “支持向量机的概率输出及其与正则化似然方法的比较，”
    *大间隔分类器进展*, 第 10 卷，第 3 期，pp. 61–74，1999。[32]Y. Romano, E. Patterson, 和 E. Candes，
    “正则化分位数回归，” *神经信息处理系统进展*, 第 32 卷，2019。[33]Y. Romano, M. Sesia, 和 E. Candes， “具有有效和自适应覆盖的分类，”
    *神经信息处理系统进展*, 第 33 卷，pp. 3581–3591，2020。[34]V. Vovk, G. Shafer, 和 I. Nouretdinov，
    “自我校准的概率预测，” *神经信息处理系统进展*, 第 16 卷，2003。[35]T. Freiesleben 和 T. Grote， “超越泛化：机器学习中的鲁棒性理论，”
    *综合*, 第 202 卷，第 4 期，p. 109，2023，doi: [10.1007/s11229-023-04334-9](https://doi.org/10.1007/s11229-023-04334-9).'
- en: '* * *'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Bayes formula describes how to update your prior beliefs in light of new evidence.
    Namely, if \(\hat{f}\) is your hypothesis and \(D\) is your evidence, you should
    update your belief with: \(\mathbb{P}(\hat{f}\mid D)=\frac{\mathbb{P}(D\mid \hat{f})\mathbb{P}(\hat{f})}{\mathbb{P}(D)}.\)[↩︎](#fnref1)'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯公式描述了如何根据新的证据更新你的先验信念。具体来说，如果 \(\hat{f}\) 是你的假设，\(D\) 是你的证据，你应该用以下方式更新你的信念：\(\mathbb{P}(\hat{f}\mid
    D)=\frac{\mathbb{P}(D\mid \hat{f})\mathbb{P}(\hat{f})}{\mathbb{P}(D)}.\)[↩︎](#fnref1)
- en: 'Technical detail: For \(X=x\) with a probability of zero, the term on the right
    is undefined. Therefore, the Bayes-optimal predictor does not describe a unique
    function, but an equivalence class of functions that are aligned at \(x\) with
    positive density.[↩︎](#fnref2)'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 技术细节：对于 \(X=x\) 的概率为零，右侧的项是未定义的。因此，贝叶斯最优预测器并不描述一个独特的函数，而是一系列在 \(x\) 处具有正密度的函数等价类。[↩︎](#fnref2)
- en: Many of these measurements from gauges and snow stations are fully automated.
    But for the sake of illustration, let’s assume they are recorded by humans.[↩︎](#fnref3)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多这些来自仪表和雪站的测量都是完全自动化的。但为了说明，让我们假设它们是由人类记录的。[↩︎](#fnref3)
- en: We do not talk about whole entries missing here, because this typically leads
    to problems of representativeness and is covered in [Chapter 7](generalization.html).[↩︎](#fnref4)***
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里不讨论整个条目缺失的情况，因为这通常会导致代表性问题，并在[第7章](generalization.html)中有所涉及。[↩︎](#fnref4)***
