- en: '8.1.1 Overview: Basic algorithms'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.1.1 概述：基本算法
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html](https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html](https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html)
- en: 8.1.1.1 k-nearest neighbor (k-NN)
  id: totrans-2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.1 k-最近邻（k-NN）
- en: k-NN is a non-parametric method used for classification and regression. Given
    an object, the algorithm’s output is computed from its k closest training examples
    in the feature space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN是一种非参数方法，用于分类和回归。给定一个对象，算法的输出是从其特征空间中最近的k个训练示例计算得出的。
- en: In k-NN classification, each object is classified into the class most common
    among its k nearest neighbors.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在k-NN分类中，每个对象被分类为其k个最近邻中最常见的类别。
- en: In k-NN regression, each object’s value is calculated as the average of the
    values of its k nearest neighbors.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在k-NN回归中，每个对象的价值是计算为其k个最近邻的平均值。
- en: '**Applications**: anomaly detection, search, recommender system'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：异常检测、搜索、推荐系统'
- en: 8.1.1.2 k-means clustering
  id: totrans-7
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.2 k-means聚类
- en: k-means clustering aims to partition observations into k clusters in which each
    observation belongs to the cluster with the nearest mean. k-means minimizes within-cluster
    variances (squared Euclidean distances), but not regular Euclidean distances.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类旨在将观测值划分为k个簇，其中每个观测值属于最近的均值所在的簇。k-means最小化簇内方差（平方欧几里得距离），但不最小化常规的欧几里得距离。
- en: The algorithm doesn’t guarantee convergence to the global optimum. The result
    may depend on the initial clusters. As the algorithm is usually fast, it is common
    to run it multiple times with different starting conditions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法不保证收敛到全局最优解。结果可能取决于初始簇。由于该算法通常很快，因此通常多次运行它，以不同的起始条件。
- en: The problem is computationally difficult (NP-hard); however, efficient heuristic
    algorithms converge quickly to a local optimum.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题在计算上很困难（NP-hard）；然而，有效的启发式算法可以快速收敛到局部最优解。
- en: The algorithm has a loose relationship to the k-nearest neighbor classifier.
    After obtaining clusters using k-means clustering, we can classify new data into
    those clusters by applying the 1-nearest neighbor classifier to the cluster centers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法与k-最近邻分类器有松散的关系。在用k-means聚类获得簇之后，我们可以通过将1-最近邻分类器应用于簇中心来将这些新数据分类到这些簇中。
- en: '**Applications**: Vector quantization for signal processing (where k-means
    clustering was originally developed), cluster analysis, feature learning, topic
    modeling.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：信号处理中的矢量量化（k-means聚类最初开发的地方）、聚类分析、特征学习、主题建模。'
- en: 8.1.1.3 EM (expectation-maximization) algorithm
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.3 EM（期望最大化）算法
- en: EM algorithm is an iterative method to find maximum likelihood (MLE) or maximum
    a posteriori (MAP) estimates of parameters. It’s useful when the model depends
    on unobserved latent variables and equations can’t be solved directly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法是一种迭代方法，用于找到参数的最大似然（MLE）或最大后验（MAP）估计。当模型依赖于未观察到的潜在变量且方程无法直接求解时，它非常有用。
- en: 'The iteration alternates between performing:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代在执行以下操作之间交替：
- en: an expectation (E) step, which creates a function for the expectation of the
    log-likelihood evaluated using the current estimate for the parameters
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个期望（E）步骤，它创建一个函数来评估当前参数估计的似然对数的期望。
- en: a maximization (M) step, which computes parameters maximizing the expected log-likelihood
    found on the E step. These parameter-estimates are then used to determine the
    distribution of the latent variables in the next E step.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最大化（M）步骤，它计算在E步骤上找到的期望对数似然的最大化参数。然后使用这些参数估计值来确定下一E步骤中潜在变量的分布。
- en: EM algorithm is guaranteed to return a local optimum of the sample likelihood
    function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法保证返回样本似然函数的局部最优解。
- en: '**Example**: Gaussian mixture models (GMM)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：高斯混合模型（GMM）'
- en: '**Applications**: Data clustering, collaborative filtering.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：数据聚类、协同过滤。'
- en: 8.1.1.4 Tree-based methods
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.4 基于树的方法
- en: '**Decision tree** is a tree-based method that goes from observations about
    an object (represented in the branches) to conclusions about its target value
    (represented in the leaves). At its core, decision trees are nest if-else conditions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**是一种基于树的方法，它从关于对象（在分支中表示）的观察结果到关于其目标值（在叶子中表示）的结论。在其核心，决策树是嵌套的if-else条件。'
- en: In **classification trees**, the target value is discrete and each leaf represents
    a class. In **regression trees**, the target value is continuous and each leaf
    represents the mean of the target values of all objects that end up with that
    leaf.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分类树**中，目标值是离散的，每个叶子代表一个类别。在**回归树**中，目标值是连续的，每个叶子代表最终到达该叶子的所有对象的目标值的平均值。
- en: Decision trees are easy to interpret and can be used to visualize decisions.
    However, they are overfit to the data they are trained on -- small changes to
    the training set can result in significantly different tree structures, which
    lead to significantly different outputs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树易于解释，可以用于可视化决策。然而，它们对训练数据过拟合——训练集的微小变化可能导致显著不同的树结构，从而导致显著不同的输出。
- en: 8.1.1.5 Bagging and boosting
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.5 袋装法和提升法
- en: Bagging and boosting are two popular ensembling methods commonly used with tree-based
    algorithms that can also be used for other algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法和提升法是两种常用的集成方法，通常与基于树的算法一起使用，也可以用于其他算法。
- en: 8.1.1.5.1 Bagging
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 8.1.1.5 袋装法和提升法
- en: Bagging, shortened for **b**ootstrap **agg**regat**ing**, is designed to improve
    the stability and accuracy of ML algorithms. It reduces variance and helps to
    avoid overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法，简称**b**ootstrap **agg**regat**ing**，旨在提高机器学习算法的稳定性和准确性。它减少了方差，有助于避免过拟合。
- en: Given a dataset, instead of training one classifier on the entire dataset, you
    sample **with** replacement to create different datasets, called bootstraps, and
    train a classification or regression model on each of these bootstraps. Sampling
    with replacement ensures each bootstrap is independent of its peers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个数据集，而不是在整个数据集上训练一个分类器，你通过**替换**来采样以创建不同的数据集，称为自助法数据集，并在每个这些自助法数据集上训练一个分类或回归模型。替换采样确保每个自助法数据集与其同伴独立。
- en: If the problem is classification, the final prediction is decided by the majority
    vote of all models. For example, if 10 classifiers vote SPAM and 6 models vote
    NOT SPAM, the final prediction is SPAM.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题是分类，最终的预测由所有模型的多数投票决定。例如，如果有10个分类器投票为SPAM，而6个模型投票为NOT SPAM，则最终的预测为SPAM。
- en: If the problem is regression, the final prediction is the average of all models’
    predictions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题是回归，最终的预测是所有模型预测的平均值。
- en: Bagging generally improves unstable methods, such as neural networks, classification
    and regression trees, and subset selection in linear regression. However, it can
    mildly degrade the performance of stable methods such as k-nearest neighbors^([2](#fn_2)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法通常可以改善不稳定的方法，如神经网络、分类和回归树以及线性回归中的子集选择。然而，它可能会略微降低稳定方法（如k-最近邻[2](#fn_2)）的性能。
- en: '![Bagging](../Images/5ad61a7b073fce0fb5f0bfa41c3585bc.png "image_tooltip")'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![袋装法](../Images/5ad61a7b073fce0fb5f0bfa41c3585bc.png "image_tooltip")'
- en: Illustration by [Sirakorn](https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Sirakorn](https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg)
    提供的插图
- en: A **random forest** is an example of bagging. A random forest is a collection
    of decision trees constructed by both **bagging** and **feature randomness**,
    each tree can pick only from a random subset of features to use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是袋装法的例子。随机森林是由**袋装法**和**特征随机性**构建的决策树集合，每棵树只能从随机特征子集中选择使用。'
- en: Due to its ensembling nature, random forests correct for decision trees’ overfitting
    to their training set.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其集成特性，随机森林可以纠正决策树对其训练集的过拟合。
- en: '**Applications**: Random forests are among the most widely used machine learning
    algorithms in the real world. They are used in banking for fraud detection, medicine
    for disease prediction, stock market analysis, etc.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**：随机森林是现实世界中应用最广泛的机器学习算法之一。它们在银行用于欺诈检测，在医学用于疾病预测，在股市分析等。'
- en: For more information on random forests, see [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)
    by Tony Yiu.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林的更多信息，请参阅Tony Yiu的[理解随机森林](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)。
- en: 8.1.1.5.2 Boosting
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 8.1.1.5.2 提升法
- en: Boosting is a family of iterative ensemble algorithms that convert weak learners
    to strong ones. Each learner in this ensemble is trained on the same set of samples
    but the samples are weighted differently among iterations. Thus, future weak learners
    focus more on the examples that previous weak learners misclassified.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法是一系列迭代集成算法，将弱学习器转换为强学习器。这个集成中的每个学习器都在相同的样本集上训练，但在迭代中样本的权重不同。因此，未来的弱学习器会更关注先前弱学习器错误分类的示例。
- en: You start by training the first weak classifier on the original dataset.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你首先在原始数据集上训练第一个弱分类器。
- en: Samples are reweighted based on how well the first classifier classifies them,
    e.g. misclassified samples are given higher weight.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本根据第一个分类器对它们的分类效果进行重新加权，例如，错误分类的样本被赋予更高的权重。
- en: Train the second classifier on this reweighted dataset. Your ensemble now consists
    of the first and the second classifiers.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个重新加权的数据集上训练第二个分类器。你的集成现在由第一个和第二个分类器组成。
- en: Samples are weighted based on how well the ensemble classifies them.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本根据集成对它们的分类效果进行加权。
- en: Train the third classifier on this reweighted dataset. Add the third classifier
    to the ensemble.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个重新加权的数据集上训练第三个分类器。将第三个分类器添加到集成中。
- en: Repeat for as many iterations as needed.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要重复进行多次迭代。
- en: Form the final strong classifier as a weighted combination of the existing classifiers
    -- classifiers with smaller training errors have higher weights.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最终强分类器形成为现有分类器的加权组合 -- 训练误差较小的分类器具有更高的权重。
- en: '![Boosting](../Images/e5e31b937fa248d597f8d95cffc55f4b.png "image_tooltip")'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting](../Images/e5e31b937fa248d597f8d95cffc55f4b.png "image_tooltip")'
- en: Illustration by [Sirakorn](https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Sirakorn](https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg)
    提供插图。
- en: An example of a boosting algorithm is Gradient Boosting Machine which produces
    a prediction model typically from weak decision trees. It builds the model in
    a stage-wise fashion as other boosting methods do, and it generalizes them by
    allowing optimization of an arbitrary differentiable loss function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法的一个例子是梯度提升机，它通常从弱决策树生成预测模型。它以分阶段的方式构建模型，就像其他提升方法一样，并通过允许优化任意可微分的损失函数来推广它们。
- en: XGBoost, a variant of GBM, used to be [the algorithm of choice for many winning
    teams of machine learning competitions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
    It’s been used in a wide range of tasks from classification, ranking, to the discovery
    of the Higgs Boson^([3](#fn_3)). However, many teams have been opting for [LightGBM](https://github.com/microsoft/LightGBM),
    a distributed gradient boosting framework that allows parallel learning which
    generally allows faster training on large datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，GBM的一个变体，曾经是[许多机器学习竞赛获胜团队的算法选择](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)。它被用于广泛的任务中，从分类、排序到希格斯玻色子的发现^([3](#fn_3))。然而，许多团队已经开始选择[LightGBM](https://github.com/microsoft/LightGBM)，这是一个允许并行学习的分布式梯度提升框架，通常允许在大数据集上更快地训练。
- en: 8.1.1.6 Kernel methods
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 8.1.1.6 内核方法
- en: 'In machine learning, kernel methods are a class of algorithms for pattern analysis,
    whose best-known member is the support vector machine (SVM). The general task
    of pattern analysis is to find and study general types of relations (for example
    clusters, rankings, principal components, correlations, classifications) in datasets.
    For many algorithms that solve these tasks, the data in raw representation have
    to be explicitly transformed into feature vector representations via a user-specified
    feature map: in contrast, kernel methods require only a user-specified kernel,
    i.e., a similarity function over pairs of data points in raw representation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，内核方法是用于模式分析的一类算法，其中最著名的成员是支持向量机（SVM）。模式分析的一般任务是找到并研究数据集中的一般关系类型（例如聚类、排序、主成分、相关性、分类）。对于解决这些任务的大多数算法，原始表示中的数据必须通过用户指定的特征映射显式地转换为特征向量表示：相比之下，内核方法只需要用户指定的核，即原始表示中数据点对的相似度函数。
- en: Kernel methods owe their name to the use of kernel functions, which enable them
    to operate in a high-dimensional, implicit feature space without ever computing
    the coordinates of the data in that space, but rather by simply computing the
    inner products between the images of all pairs of data in the feature space. This
    operation is often computationally cheaper than the explicit computation of the
    coordinates. This approach is called the "kernel trick".[1] Kernel functions have
    been introduced for sequence data, graphs, text, images, as well as vectors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 内核方法的名字来源于对核函数的使用，这使得它们能够在高维隐式特征空间中操作，而无需计算该空间中数据的坐标，而是通过简单地计算特征空间中所有数据对图像之间的内积。这种操作通常比显式计算坐标的计算成本更低。这种方法被称为“核技巧”。[1]
    核函数已经被用于序列数据、图、文本、图像以及向量。
- en: 'Algorithms capable of operating with kernels include the kernel perceptron,
    support vector machines (SVM), Gaussian processes, principal components analysis
    (PCA), canonical correlation analysis, ridge regression, spectral clustering,
    linear adaptive filters, and many others. Any linear model can be turned into
    a non-linear model by applying the kernel trick to the model: replacing its features
    (predictors) with a kernel function.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 能够与核函数一起操作的算法包括核感知器、支持向量机（SVM）、高斯过程、主成分分析（PCA）、典型相关分析、岭回归、谱聚类、线性自适应滤波器以及许多其他算法。任何线性模型都可以通过应用核技巧转换为非线性模型：用核函数替换其特征（预测器）。
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright ©2021 Chip Huyen.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是由[Chip Huyen](https://huyenchip.com)在众多朋友的帮助下创作的。对于反馈、勘误和建议，作者可以通过[这里](https://huyenchip.com/communication/)联系。版权©2021
    Chip Huyen。
