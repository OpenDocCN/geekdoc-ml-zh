- en: Vision-Language Models (VLM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉语言模型（VLM）
- en: '![](../media/file941.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file941.jpg)'
- en: '*DALL·E prompt - A Raspberry Pi setup featuring vision tasks. The image shows
    a Raspberry Pi connected to a camera, with various computer vision tasks displayed
    visually around it, including object detection, image captioning, segmentation,
    and visual grounding. The Raspberry Pi is placed on a desk, with a display showing
    bounding boxes and annotations related to these tasks. The background should be
    a home workspace, with tools and devices typically used by developers and hobbyists.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 提示 - 一个配备视觉任务的树莓派设置。图中显示了一个连接到摄像头的树莓派，周围以视觉方式显示了各种计算机视觉任务，包括目标检测、图像标题、分割和视觉定位。树莓派放置在桌子上，显示器显示了与这些任务相关的边界框和注释。背景应为一个家庭工作空间，其中包含开发人员和爱好者通常使用的工具和设备。*'
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: In this hands-on lab, we will continuously explore AI applications at the Edge,
    from the basic setup of the Florence-2, Microsoft’s state-of-the-art vision foundation
    model, to advanced implementations on devices like the Raspberry Pi. We will learn
    to use Vision Language Models (VLMs) for tasks such as captioning, object detection,
    grounding, segmentation, and OCR on a Raspberry Pi.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手实验中，我们将持续探索边缘的 AI 应用，从 Florence-2 的基本设置，即微软最先进的视觉基础模型，到在树莓派等设备上的高级实现。我们将学习在树莓派上使用视觉语言模型（VLMs）进行标题、目标检测、定位、分割和
    OCR 等任务。
- en: Why Florence-2 at the Edge?
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么在边缘使用 Florence-2？
- en: '[Florence-2](https://arxiv.org/abs/2311.06242) is a vision-language model open-sourced
    by Microsoft under the MIT license, which significantly advances vision-language
    models by combining a lightweight architecture with robust capabilities. Thanks
    to its training on the massive FLD-5B dataset, which contains 126 million images
    and 5.4 billion visual annotations, it achieves performance comparable to larger
    models. This makes Florence-2 ideal for deployment at the edge, where power and
    computational resources are limited.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Florence-2](https://arxiv.org/abs/2311.06242) 是微软开源的视觉语言模型，在 MIT 许可证下，通过结合轻量级架构和强大的功能，显著推进了视觉语言模型。得益于其在包含
    1.26 亿张图像和 54 亿视觉注释的庞大 FLD-5B 数据集上的训练，它实现了与更大模型相当的性能。这使得 Florence-2 成为边缘部署的理想选择，因为在边缘，电力和计算资源有限。'
- en: 'In this tutorial, we will explore how to use Florence-2 for real-time computer
    vision applications, such as:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨如何使用 Florence-2 进行实时计算机视觉应用，例如：
- en: Image captioning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像标题
- en: Object detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测
- en: Segmentation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割
- en: Visual grounding
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉定位
- en: '**Visual grounding** involves linking textual descriptions to specific regions
    within an image. This enables the model to understand where particular objects
    or entities described in a prompt are in the image. For example, if the prompt
    is “a red car,” the model will identify and highlight the region where the red
    car is found in the image. Visual grounding is helpful for applications where
    precise alignment between text and visual content is needed, such as human-computer
    interaction, image annotation, and interactive AI systems.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**视觉定位**涉及将文本描述与图像中的特定区域相关联。这使得模型能够理解在提示中描述的特定对象或实体在图像中的位置。例如，如果提示是“一辆红色的车”，模型将识别并突出显示图像中红色车所在的位置。视觉定位对于需要文本和视觉内容之间精确对齐的应用程序很有帮助，例如人机交互、图像注释和交互式人工智能系统。'
- en: 'In the tutorial, we will walk through:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在教程中，我们将逐步介绍：
- en: Setting up Florence-2 on the Raspberry Pi
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在树莓派上设置 Florence-2
- en: Running inference tasks such as object detection and captioning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行推理任务，如目标检测和标题
- en: Optimizing the model to get the best performance from the edge device
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化模型以从边缘设备获得最佳性能
- en: Exploring practical, real-world applications with fine-tuning.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过微调探索实际、现实世界的应用。
- en: Florence-2 Model Architecture
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Florence-2 模型架构
- en: 'Florence-2 utilizes a unified, prompt-based representation to handle various
    vision-language tasks. The model architecture consists of two main components:
    an **image encoder** and a **multi-modal transformer encoder-decoder**.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 利用统一的基于提示的表示来处理各种视觉语言任务。模型架构由两个主要组件组成：一个**图像编码器**和一个**多模态 Transformer
    编码器-解码器**。
- en: '![](../media/file942.svg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file942.svg)'
- en: '**Image Encoder**: The image encoder is based on the [DaViT (Dual Attention
    Vision Transformers) architecture](https://arxiv.org/abs/2204.03645). It converts
    input images into a series of visual token embeddings. These embeddings serve
    as the foundational representations of the visual content, capturing both spatial
    and contextual information about the image.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像编码器**：图像编码器基于 [DaViT (双注意力视觉Transformer) 架构](https://arxiv.org/abs/2204.03645)。它将输入图像转换为一系列视觉标记嵌入。这些嵌入作为视觉内容的基元表示，捕捉图像的空间和上下文信息。'
- en: '**Multi-Modal Transformer Encoder-Decoder**: Florence-2’s core is the multi-modal
    transformer encoder-decoder, which combines visual token embeddings from the image
    encoder with textual embeddings generated by a BERT-like model. This combination
    allows the model to simultaneously process visual and textual inputs, enabling
    a unified approach to tasks such as image captioning, object detection, and segmentation.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态 Transformer 编码器-解码器**：Florence-2 的核心是多模态 Transformer 编码器-解码器，它将图像编码器中的视觉标记嵌入与类似
    BERT 的模型生成的文本嵌入相结合。这种组合使模型能够同时处理视觉和文本输入，实现图像描述、目标检测和分割等任务的统一方法。'
- en: The model’s training on the extensive FLD-5B dataset ensures it can effectively
    handle diverse vision tasks without requiring task-specific modifications. Florence-2
    uses textual prompts to activate specific tasks, making it highly flexible and
    capable of zero-shot generalization. For tasks like object detection or visual
    grounding, the model incorporates additional location tokens to represent regions
    within the image, ensuring a precise understanding of spatial relationships.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在广泛的 FLD-5B 数据集上的训练确保它能够有效处理各种视觉任务，而无需进行特定任务的修改。Florence-2 使用文本提示来激活特定任务，使其非常灵活，并能够实现零样本泛化。对于诸如目标检测或视觉定位等任务，模型采用额外的位置标记来表示图像中的区域，确保对空间关系的精确理解。
- en: Florence-2’s compact architecture and innovative training approach allow it
    to perform computer vision tasks accurately, even on resource-constrained devices
    like the Raspberry Pi.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Florence-2 的紧凑架构和创新训练方法使其能够在资源受限的设备（如树莓派）上准确执行计算机视觉任务。
- en: Technical Overview
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术概述
- en: 'Florence-2 introduces several innovative features that set it apart:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 引入了一些创新特性，使其与众不同：
- en: Architecture
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构
- en: '![](../media/file943.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file943.png)'
- en: '**Lightweight Design**: Two variants available'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻量级设计**：提供两种变体'
- en: 'Florence-2-Base: 232 million parameters'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence-2-Base：2.32 亿个参数
- en: 'Florence-2-Large: 771 million parameters'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence-2-Large：7.71 亿个参数
- en: '**Unified Representation**: Handles multiple vision tasks through a single
    architecture'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一表示**：通过单个架构处理多个视觉任务'
- en: '**DaViT Vision Encoder**: Converts images into visual token embeddings'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DaViT 视觉编码器**：将图像转换为视觉标记嵌入'
- en: '**Transformer-based Multi-modal Encoder-Decoder**: Processes combined visual
    and text embeddings'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于 Transformer 的多模态编码器-解码器**：处理结合视觉和文本嵌入'
- en: Training Dataset (FLD-5B)
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据集 (FLD-5B)
- en: '![](../media/file944.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file944.png)'
- en: 126 million unique images
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1.26 亿个独特图像
- en: '5.4 billion comprehensive annotations, including:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 54 亿个综合注释，包括：
- en: 500M text annotations
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 亿个文本注释
- en: 1.3B region-text annotations
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 13 亿个区域-文本注释
- en: 3.6B text-phrase-region annotations
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 36 亿个文本短语-区域注释
- en: Automated annotation pipeline using specialist models
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用专业模型自动化的注释管道
- en: Iterative refinement process for high-quality labels
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高质量标签的迭代优化过程
- en: Key Capabilities
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键能力
- en: 'Florence-2 excels in multiple vision tasks:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 在多个视觉任务中表现出色：
- en: Zero-shot Performance
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零样本性能
- en: 'Image Captioning: Achieves 135.6 CIDEr score on COCO'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述：在 COCO 上达到 135.6 CIDEr 分数
- en: 'Visual Grounding: 84.4% recall@1 on Flickr30k'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉定位：在 Flickr30k 上达到 84.4% 的 recall@1
- en: 'Object Detection: 37.5 mAP on COCO val2017'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测：在 COCO val2017 上达到 37.5 mAP
- en: 'Referring Expression: 67.0% accuracy on RefCOCO'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指代表达式：在 RefCOCO 上达到 67.0% 的准确率
- en: Fine-tuned Performance
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调性能
- en: Competitive with specialist models despite the smaller size
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管规模较小，但与专业模型具有竞争力
- en: Outperforms larger models in specific benchmarks
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定基准测试中优于更大的模型
- en: Efficient adaptation to new tasks
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效适应新任务
- en: Practical Applications
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际应用
- en: 'Florence-2 can be applied across various domains:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 可应用于各个领域：
- en: '**Content Understanding**'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内容理解**'
- en: Automated image captioning for accessibility
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动图像描述以实现无障碍访问
- en: Visual content moderation
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉内容审核
- en: Media asset management
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 媒体资产管理
- en: '**E-commerce**'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**电子商务**'
- en: Product image analysis
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品图像分析
- en: Visual search
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉搜索
- en: Automated product tagging
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动产品标记
- en: '**Healthcare**'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**医疗保健**'
- en: Medical image analysis
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学图像分析
- en: Diagnostic assistance
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断辅助
- en: Research data processing
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究数据处理
- en: '**Security & Surveillance**'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安全与监控**'
- en: Object detection and tracking
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测与跟踪
- en: Anomaly detection
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Scene understanding
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景理解
- en: Comparing Florence-2 with other VLMs
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较Florence-2与其他VLMs（视觉语言模型）
- en: Florence-2 stands out from other visual language models due to its impressive
    zero-shot capabilities. Unlike models like [Google PaliGemma](https://huggingface.co/blog/paligemma),
    which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works
    right out of the box, as we will see in this lab. It can also compete with larger
    models like GPT-4V and Flamingo, which often have many more parameters but only
    sometimes match Florence-2’s performance. For example, Florence-2 achieves better
    zero-shot results than Kosmos-2 despite having over twice the parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2因其令人印象深刻的零样本能力而优于其他视觉语言模型。与像[Google PaliGemma](https://huggingface.co/blog/paligemma)这样的模型不同，后者需要广泛的微调来适应各种任务，Florence-2可以直接使用，正如我们将在本实验中看到的那样。它还可以与GPT-4V和Flamingo等更大的模型竞争，这些模型通常具有更多的参数，但有时只能匹配Florence-2的性能。例如，尽管参数多出两倍以上，Florence-2在零样本结果上优于Kosmos-2。
- en: In benchmark tests, Florence-2 has shown remarkable performance in tasks like
    COCO captioning and referring expression comprehension. It outperformed models
    like PolyFormer and UNINEXT in object detection and segmentation tasks on the
    [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/). It is a highly
    competitive choice for real-world applications where both performance and resource
    efficiency are crucial.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试中，Florence-2在COCO描述和指代表达理解等任务上表现出色。它在COCO数据集（[COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/)）上的目标检测和分割任务中优于PolyFormer和UNINEXT等模型。它是一个高度竞争的选择，对于既需要性能又需要资源效率的实际应用来说，这是一个很好的选择。
- en: Setup and Installation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置与安装
- en: Our choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform
    is equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76
    CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts
    a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder.
    Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8 GB being
    our choice to run Florence-2\. It also features expandable storage via a microSD
    card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid
    State Drives).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的边缘设备是Raspberry Pi 5（Raspi-5）。其坚固的平台配备了Broadcom BCM2712，这是一款2.4 GHz的四核64位Arm
    Cortex-A76 CPU，具有加密扩展和增强的缓存能力。它拥有VideoCore VII GPU，双4Kp60 HDMI®输出带HDR，以及4Kp60
    HEVC解码器。内存选项包括4 GB和8 GB的高速LPDDR4X SDRAM，我们选择8 GB来运行Florence-2。它还通过microSD卡槽和PCIe
    2.0接口提供可扩展的存储，用于快速的外设，如M.2 SSD（固态硬盘）。
- en: For real applications, SSDs are a better option than SD cards.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于实际应用来说，SSD（固态硬盘）比SD卡是一个更好的选择。
- en: We suggest installing an Active Cooler, a dedicated clip-on cooling solution
    for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink
    with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably
    under heavy loads, such as running Florense-2.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议为这个实验安装一个Active Cooler，这是为Raspberry Pi 5（Raspi-5）设计的专用夹式冷却解决方案。它结合了铝制散热器和温度控制的吹风风扇，以保持Raspi-5在重负载下（如运行Florense-2）舒适运行。
- en: '![](../media/file945.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file945.jpg)'
- en: Environment configuration
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境配置
- en: 'To run [Microsoft Florense-2](https://huggingface.co/microsoft/Florence-2-base)
    on the Raspberry Pi 5, we’ll need a few libraries:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Raspberry Pi 5上运行[Microsoft Florense-2](https://huggingface.co/microsoft/Florence-2-base)，我们需要几个库：
- en: '**[Transformers](https://huggingface.co/docs/transformers/en/index)**:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[Transformers](https://huggingface.co/docs/transformers/en/index)**：'
- en: Florence-2 uses the `transformers` library from Hugging Face for model loading
    and inference. This library provides the architecture for working with pre-trained
    vision-language models, making it easy to perform tasks like image captioning,
    object detection, and more. Essentially, `transformers` helps in interacting with
    the model, processing input prompts, and obtaining outputs.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence-2使用Hugging Face的`transformers`库进行模型加载和推理。这个库提供了与预训练视觉语言模型一起工作的架构，使得执行图像描述、目标检测等任务变得容易。本质上，`transformers`帮助与模型交互，处理输入提示，并获得输出。
- en: '**PyTorch**:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PyTorch**：'
- en: PyTorch is a deep learning framework that provides the infrastructure needed
    to run the Florence-2 model, which includes tensor operations, GPU acceleration
    (if a GPU is available), and model training/inference functionalities. The Florence-2
    model is trained in PyTorch, and we need it to leverage its functions, layers,
    and computation capabilities to perform inferences on the Raspberry Pi.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 是一个深度学习框架，它提供了运行 Florence-2 模型所需的基础设施，包括张量操作、GPU 加速（如果可用）以及模型训练/推理功能。Florence-2
    模型是在 PyTorch 中训练的，我们需要利用其功能、层和计算能力，在 Raspberry Pi 上执行推理。
- en: '**Timm** (PyTorch Image Models):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Timm** (PyTorch Image Models):'
- en: Florence-2 uses `timm` to access efficient implementations of vision models
    and pre-trained weights. Specifically, the `timm` library is utilized for the
    **image encoder** part of Florence-2, particularly for managing the DaViT architecture.
    It provides model definitions and optimized code for common vision tasks and allows
    the easy integration of different backbones that are lightweight and suitable
    for edge devices.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florence-2 使用 `timm` 访问视觉模型的高效实现和预训练权重。具体来说，`timm` 库被用于 Florence-2 的 **图像编码器**
    部分，特别是用于管理 DaViT 架构。它提供了常见视觉任务的模型定义和优化代码，并允许轻松集成不同类型的轻量级骨干网络，这些网络适合边缘设备。
- en: '**Einops**:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Einops**:'
- en: '`Einops` is a library for flexible and powerful tensor operations. It makes
    it easy to reshape and manipulate tensor dimensions, which is especially important
    for the multi-modal processing done in Florence-2\. Vision-language models like
    Florence-2 often need to rearrange image data, text embeddings, and visual embeddings
    to align correctly for the transformer blocks, and `einops` simplifies these complex
    operations, making the code more readable and concise.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Einops` 是一个用于灵活和强大的张量操作的库。它使得重塑和操作张量维度变得容易，这对于 Florence-2 中进行的多模态处理尤为重要。像
    Florence-2 这样的视觉语言模型通常需要重新排列图像数据、文本嵌入和视觉嵌入，以便正确地对齐 transformer 块，而 `einops` 简化了这些复杂的操作，使得代码更易于阅读和简洁。'
- en: 'In short, these libraries enable different essential components of Florence-2:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些库使 Florence-2 能够启用不同的基本组件：
- en: '**Transformers** and **PyTorch** are needed to load the model and run the inference.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers** 和 **PyTorch** 用于加载模型和运行推理。'
- en: '**Timm** is used to access and efficiently implement the vision encoder.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Timm** 用于访问和高效实现视觉编码器。'
- en: '**Einops** helps reshape data, facilitating the integration of visual and text
    features.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Einops** 帮助重塑数据，便于视觉和文本特征的集成。'
- en: All these components work together to help Florence-2 run seamlessly on our
    Raspberry Pi, allowing it to perform complex vision-language tasks relatively
    quickly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组件协同工作，帮助 Florence-2 在我们的 Raspberry Pi 上无缝运行，使其能够相对快速地执行复杂的视觉语言任务。
- en: 'Considering that the Raspberry Pi already has its OS installed, let’s use `SSH`
    to reach it from another computer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 Raspberry Pi 已经安装了其操作系统，让我们使用 `SSH` 从另一台计算机访问它：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And check the IP allocated to it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 并检查分配给它的 IP：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`192.168.4.209`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`192.168.4.209`'
- en: '![](../media/file946.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file946.png)'
- en: '**Updating the Raspberry Pi**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新 Raspberry Pi**'
- en: 'First, ensure your Raspberry Pi is up to date:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保您的 Raspberry Pi 已更新：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Initial setup for using PIP**:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 PIP 的初始设置**:'
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Install Dependencies**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装依赖项**'
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s set up and activate a **Virtual Environment** for working with Florence-2:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置并激活一个用于与 Florence-2 一起工作的 **虚拟环境**：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Install PyTorch**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PyTorch**'
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s verify that PyTorch is correctly installed:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证 PyTorch 是否正确安装：
- en: '![](../media/file947.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file947.png)'
- en: '**Install Transformers, Timm and Einops**:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 Transformers、Timm 和 Einops**:'
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Install the model**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装模型**:'
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Jupyter Notebook and Python libraries**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook 和 Python 库**'
- en: Installing a Jupyter Notebook to run and test our Python scripts is possible.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Jupyter Notebook 以运行和测试我们的 Python 脚本是可能的。
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Testing the installation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试安装
- en: Running the Jupyter Notebook on the remote computer
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在远程计算机上运行 Jupyter Notebook
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the above command on the SSH terminal, we can see the local URL address
    to open the notebook:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SSH 终端运行上述命令，我们可以看到打开笔记本的本地 URL 地址：
- en: '![](../media/file948.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file948.png)'
- en: 'The notebook with the code used on this initial test can be found on the Lab
    GitHub:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在此初始测试中使用的代码笔记本可以在 Lab GitHub 上找到：
- en: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
- en: We can access it on the remote computer by entering the Raspberry Pi’s IP address
    and the provided token in a web browser (copy the entire URL from the terminal).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在网页浏览器中输入树莓派的IP地址和提供的令牌来访问远程计算机（从终端复制整个URL）。
- en: From the Home page, create a new notebook [`Python 3 (ipykernel)` ] and copy
    and paste the [example code](https://huggingface.co/microsoft/Florence-2-base#how-to-get-started-with-the-model)
    from Hugging Face Hub.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从主页创建一个新的笔记本[`Python 3 (ipykernel)`]并将从Hugging Face Hub的[示例代码](https://huggingface.co/microsoft/Florence-2-base#how-to-get-started-with-the-model)复制粘贴过来。
- en: The code is designed to run Florence-2 on a given image to perform **object
    detection**. It loads the model, processes an image and a prompt, and then generates
    a response to identify and describe the objects in the image.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 代码旨在在给定的图像上运行Florence-2以执行**目标检测**。它加载模型，处理图像和提示，然后生成响应以识别和描述图像中的对象。
- en: The **processor** helps prepare text and image inputs.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器**帮助准备文本和图像输入。'
- en: The **model** takes the processed inputs to generate a meaningful response.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**接收处理后的输入以生成有意义的响应。'
- en: The **post-processing** step refines the generated output into a more interpretable
    form, like bounding boxes for detected objects.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**步骤将生成的输出细化成更易解释的形式，例如检测到的对象的边界框。'
- en: This workflow leverages the versatility of Florence-2 to handle **vision-language
    tasks** and is implemented efficiently using PyTorch, Transformers, and related
    image-processing tools.
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此工作流程利用Florence-2的灵活性来处理**视觉-语言任务**，并使用PyTorch、Transformers和相关图像处理工具高效实现。
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s break down the provided code step by step:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分解提供的代码：
- en: Importing Required Libraries
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导入所需库
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**requests**: Used to make HTTP requests. In this case, it downloads an image
    from a URL.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**requests**：用于发送HTTP请求。在这种情况下，它从URL下载图像。'
- en: '**PIL (Pillow)**: Provides tools for manipulating images. Here, it’s used to
    open the downloaded image.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PIL (Pillow)**：提供用于操作图像的工具。在这里，它用于打开下载的图像。'
- en: '**torch**: PyTorch is imported to handle tensor operations and determine the
    hardware availability (CPU or GPU).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**torch**：PyTorch被导入以处理张量操作并确定硬件可用性（CPU或GPU）。'
- en: '**transformers**: This module provides easy access to Florence-2 by using `AutoProcessor`
    and `AutoModelForCausalLM` to load pre-trained models and process inputs.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transformers**：此模块通过使用`AutoProcessor`和`AutoModelForCausalLM`来加载预训练模型并处理输入，提供对Florence-2的简单访问。'
- en: Determining the Device and Data Type
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确定设备和数据类型
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Device Setup**: The code checks if a CUDA-enabled GPU is available (`torch.cuda.is_available()`).
    The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to
    `"cpu"` (our case here).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备设置**：代码检查是否有可用的CUDA启用GPU（`torch.cuda.is_available()`）。如果有GPU可用，设备设置为“cuda:0”。否则，默认为
    `"cpu"`（我们这里的案例）。'
- en: '**Data Type Setup**: If a GPU is available, `torch.float16` is chosen, which
    uses half-precision floats to speed up processing and reduce memory usage. On
    the CPU, it defaults to `torch.float32` to maintain compatibility.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据类型设置**：如果可用GPU，则选择`torch.float16`，它使用半精度浮点数以加快处理速度并减少内存使用。在CPU上，默认为`torch.float32`以保持兼容性。'
- en: Loading the Model and Processor
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载模型和处理器
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Model Initialization**:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型初始化**：'
- en: '**`AutoModelForCausalLM.from_pretrained()`** loads the pre-trained Florence-2
    model from Microsoft’s repository on Hugging Face. The `torch_dtype` is set according
    to the available hardware (GPU/CPU), and `trust_remote_code=True` allows the use
    of any custom code that might be provided with the model.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`AutoModelForCausalLM.from_pretrained()`** 从Hugging Face上微软的存储库加载预训练的Florence-2模型。`torch_dtype`根据可用的硬件（GPU/CPU）设置，`trust_remote_code=True`允许使用可能与模型一起提供的任何自定义代码。'
- en: '**`.to(device)`** moves the model to the appropriate device (either CPU or
    GPU). In our case, it will be set to `CPU`.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`.to(device)`** 将模型移动到适当的设备（CPU或GPU）。在我们的案例中，它将被设置为`CPU`。'
- en: '**Processor Initialization**:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理器初始化**：'
- en: '**`AutoProcessor.from_pretrained()`** loads the processor for Florence-2\.
    The processor is responsible for transforming text and image inputs into a format
    the model can work with (e.g., encoding text, normalizing images, etc.).'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`AutoProcessor.from_pretrained()`** 加载Florence-2的处理器。处理器负责将文本和图像输入转换为模型可以处理的形式（例如，编码文本、归一化图像等）。'
- en: Defining the Prompt
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**定义提示**'
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Prompt Definition**: The string `"<OD>"` is used as a prompt. This refers
    to “Object Detection”, instructing the model to detect objects on the image.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示定义**：字符串 `"<OD>"` 被用作提示。这指的是“目标检测”，指示模型在图像上检测对象。'
- en: Downloading and Loading the Image
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下载和加载图像
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Downloading the Image**: The **`requests.get()`** function fetches the image
    from the specified URL. The `stream=True` parameter ensures the image is streamed
    rather than downloaded completely at once.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载图像**：**`requests.get()`** 函数从指定的 URL 获取图像。`stream=True` 参数确保图像是流式传输的，而不是一次性完全下载。'
- en: '**Opening the Image**: **`Image.open()`** opens the image so the model can
    process it.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打开图像**：**`Image.open()`** 打开图像，以便模型可以处理它。'
- en: Processing Inputs
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理输入
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Processing Input Data**: The **`processor()`** function processes the text
    (`prompt`) and the image (`image`). The `return_tensors="pt"` argument converts
    the processed data into PyTorch tensors, which are necessary for inputting data
    into the model.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理输入数据**：**`processor()`** 函数处理文本（提示）和图像（图像）。`return_tensors="pt"` 参数将处理后的数据转换为
    PyTorch 张量，这对于将数据输入模型是必要的。'
- en: '**Moving Inputs to Device**: **`.to(device, torch_dtype)`** moves the inputs
    to the correct device (CPU or GPU) and assigns the appropriate data type.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动输入到设备**：**`.to(device, torch_dtype)`** 将输入移动到正确的设备（CPU 或 GPU）并分配适当的数据类型。'
- en: Generating the Output
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成输出
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Model Generation**: **`model.generate()`** is used to generate the output
    based on the input data.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型生成**：使用 **`model.generate()`** 根据输入数据生成输出。'
- en: '**`input_ids`**: Represents the tokenized form of the prompt.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`input_ids`**：表示提示的标记化形式。'
- en: '**`pixel_values`**: Contains the processed image data.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pixel_values`**：包含处理后的图像数据。'
- en: '**`max_new_tokens=1024`**: Specifies the maximum number of new tokens to be
    generated in the response. This limits the response length.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`max_new_tokens=1024`**：指定响应中要生成的最大新标记数。这限制了响应的长度。'
- en: '**`do_sample=False`**: Disables sampling; instead, the generation uses deterministic
    methods (beam search).'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`do_sample=False`**：禁用采样；相反，生成使用确定性方法（束搜索）。'
- en: '**`num_beams=3`**: Enables beam search with three beams, which improves output
    quality by considering multiple possibilities during generation.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`num_beams=3`**：启用具有三个束的束搜索，通过在生成过程中考虑多个可能性来提高输出质量。'
- en: Decoding the Generated Text
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解码生成的文本
- en: '[PRE19]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Batch Decode**: **`processor.batch_decode()`** decodes the generated IDs
    (tokens) into readable text. The `skip_special_tokens=False` parameter means that
    the output will include any special tokens that may be part of the response.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量解码**：**`processor.batch_decode()`** 将生成的 ID（标记）解码为可读文本。`skip_special_tokens=False`
    参数意味着输出将包括任何可能是响应一部分的特殊标记。'
- en: Post-processing the Generation
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成后的后处理
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Post-Processing**: **`processor.post_process_generation()`** is called to
    process the generated text further, interpreting it based on the task (`"<OD>"`
    for object detection) and the size of the image.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**：调用 **`processor.post_process_generation()`** 来进一步处理生成的文本，根据任务（`"<OD>"`
    用于目标检测）和图像的大小进行解释。'
- en: This function extracts specific information from the generated text, such as
    bounding boxes for detected objects, making the output more useful for visual
    tasks.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此函数从生成的文本中提取特定信息，例如检测到的对象的边界框，使输出对视觉任务更有用。
- en: Printing the Output
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打印输出
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finally, **`print(parsed_answer)`** displays the output, which could include
    object detection results, such as bounding box coordinates and labels for the
    detected objects in the image.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，**`print(parsed_answer)`** 显示输出，这可能包括目标检测结果，例如图像中检测到的对象的边界框坐标和标签。
- en: Result
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Running the code, we get as the Parsed Answer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码，我们得到以下解析答案：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'First, let’s inspect the image:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查一下图像：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../media/file949.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file949.png)'
- en: 'By the Object Detection result, we can see that:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过目标检测的结果，我们可以看到：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It seems that at least a few objects were detected. We can also implement a
    code to draw the bounding boxes in the find objects:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来至少检测到了几个对象。我们还可以实现一个代码来在找到的对象中绘制边界框：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Box (x0, y0, x1, y1)**: Location tokens correspond to the top-left and bottom-right
    corners of a box.'
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**框（x0, y0, x1, y1）**：位置标记对应于框的左上角和右下角。'
- en: And running
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 并运行
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We get:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到：
- en: '![](../media/file950.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file950.png)'
- en: Florence-2 Tasks
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Florence-2 任务
- en: Florence-2 is designed to perform a variety of computer vision and vision-language
    tasks through `prompts`. These tasks can be activated by providing a specific
    textual prompt to the model, as we saw with `<OD>` (Object Detection).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 是通过 `prompts` 来执行各种计算机视觉和视觉-语言任务的。这些任务可以通过向模型提供特定的文本提示来激活，正如我们通过
    `<OD>` (目标检测) 看到的那样。
- en: Florence-2’s versatility comes from combining these prompts, allowing us to
    guide the model’s behavior to perform specific vision tasks. Changing the prompt
    allows us to adapt Florence-2 to different tasks without needing task-specific
    modifications in the architecture. This capability directly results from Florence-2’s
    unified model architecture and large-scale multi-task training on the FLD-5B dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 的多功能性来自于结合这些提示，使我们能够引导模型的行为以执行特定的视觉任务。改变提示允许我们适应 Florence-2 的不同任务，而无需对架构进行特定任务的修改。这种能力直接源于
    Florence-2 的统一模型架构和基于 FLD-5B 数据集的大规模多任务训练。
- en: 'Here are some of the key tasks that Florence-2 can perform, along with example
    prompts:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 Florence-2 可以执行的一些关键任务，以及示例提示：
- en: Object Detection (OD)
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物体检测（OD）
- en: '**Prompt**: `"<OD>"`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<OD>"`'
- en: '**Description**: Identifies objects in an image and provides bounding boxes
    for each detected object. This task is helpful for applications like visual inspection,
    surveillance, and general object recognition.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：在图像中识别对象并为每个检测到的对象提供边界框。这项任务对于视觉检查、监控和一般对象识别等应用很有帮助。'
- en: Image Captioning
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像字幕
- en: '**Prompt**: `"<CAPTION>"`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<CAPTION>"`'
- en: '**Description**: Generates a textual description for an input image. This task
    helps the model describe what is happening in the image, providing a human-readable
    caption for content understanding.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：为输入图像生成文本描述。这项任务帮助模型描述图像中正在发生的事情，为内容理解提供可读性强的字幕。'
- en: Detailed Captioning
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 详细字幕
- en: '**Prompt**: `"<DETAILED_CAPTION>"`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<DETAILED_CAPTION>"`'
- en: '**Description**: Generates a more detailed caption with more nuanced information
    about the scene, such as the objects present and their relationships.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：生成包含更多细微信息的更详细字幕，例如场景中的对象及其关系。'
- en: Visual Grounding
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉定位
- en: '**Prompt**: `"<CAPTION_TO_PHRASE_GROUNDING>"`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<CAPTION_TO_PHRASE_GROUNDING>"`'
- en: '**Description**: Links a textual description to specific regions in an image.
    For example, given a prompt like “a green car,” the model highlights where the
    green car is in the image. This is useful for human-computer interaction, where
    you must find specific objects based on text.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：将文本描述与图像中的特定区域关联起来。例如，给定一个如“一辆绿色的车”的提示，模型会在图像中突出显示绿色车的位置。这在需要根据文本找到特定对象的人机交互中非常有用。'
- en: Segmentation
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割
- en: '**Prompt**: `"<REFERRING_EXPRESSION_SEGMENTATION>"`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<REFERRING_EXPRESSION_SEGMENTATION>"`'
- en: '**Description**: Performs segmentation based on a referring expression, such
    as “the blue cup.” The model identifies and segments the specific region containing
    the object mentioned in the prompt (all related pixels).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：基于指称表达式（如“蓝色的杯子”）进行分割。模型会识别并分割出提示中提到的对象所在的特定区域（所有相关像素）。'
- en: Dense Region Captioning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集区域字幕
- en: '**Prompt**: `"<DENSE_REGION_CAPTION>"`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<DENSE_REGION_CAPTION>"`'
- en: '**Description**: Provides captions for multiple regions within an image, offering
    a detailed breakdown of all visible areas, including different objects and their
    relationships.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：为图像中的多个区域提供字幕，提供所有可见区域的详细分解，包括不同的对象及其关系。'
- en: OCR with Region
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带区域的 OCR
- en: '**Prompt**: `"<OCR_WITH_REGION>"`'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<OCR_WITH_REGION>"`'
- en: '**Description**: Performs Optical Character Recognition (OCR) on an image and
    provides bounding boxes for the detected text. This is useful for extracting and
    locating textual information in images, such as reading signs, labels, or other
    forms of text in images.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：在图像上执行光学字符识别（OCR），并为检测到的文本提供边界框。这在从图像中提取和定位文本信息（如读取标志、标签或其他形式的文本）时非常有用。'
- en: Phrase Grounding for Specific Expressions
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特定表达式的短语定位
- en: '**Prompt**: `"<CAPTION_TO_PHRASE_GROUNDING>"` along with a specific expression,
    such as `"a wine glass"`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<CAPTION_TO_PHRASE_GROUNDING>"` 以及特定的表达，例如 `"a wine glass"`。'
- en: '**Description**: Locates the area in the image that corresponds to a specific
    textual phrase. This task allows for identifying particular objects or elements
    when prompted with a word or keyword.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：定位图像中与特定文本短语相对应的区域。这项任务允许在给出一个词或关键词时识别特定的对象或元素。'
- en: Open Vocabulary Object Detection
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开放词汇物体检测
- en: '**Prompt**: `"<OPEN_VOCABULARY_OD>"`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**：`"<OPEN_VOCABULARY_OD>"`'
- en: '**Description**: The model can detect objects without being restricted to a
    predefined list of classes, making it helpful in recognizing a broader range of
    items based on general visual understanding.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述**：模型可以检测对象，而不受预定义类别列表的限制，这使得它在根据一般视觉理解识别更广泛的物品时非常有用。'
- en: Exploring computer vision and vision-language tasks
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索计算机视觉和视觉语言任务
- en: 'For exploration, all codes can be found on the GitHub:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于探索，所有代码都可以在 GitHub 上找到：
- en: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
- en: 'Let’s use a couple of images created by Dall-E and upload them to the Rasp-5
    (FileZilla can be used for that). The images will be saved on a sub-folder named
    `images` :'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用由 Dall-E 创建的几张图片并将它们上传到 Rasp-5（可以使用 FileZilla 来完成）。图片将被保存在名为 `images` 的子文件夹中：
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../media/file951.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file951.png)'
- en: 'Let’s create a function to facilitate our exploration and to keep track of
    the latency of the model for different tasks:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个函数来方便我们的探索，并跟踪模型在不同任务中的延迟：
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Caption
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标题
- en: '**1\. Dogs and Cats**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 狗和猫**'
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**2\. Table**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 表格**'
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Detailed Caption
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 详细标题
- en: '**1\. Dogs and Cats**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 狗和猫**'
- en: '[PRE33]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**2\. Table**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 表格**'
- en: '[PRE35]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: More Detailed Caption
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更详细的标题
- en: '**1\. Dogs and Cats**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 狗和猫**'
- en: '[PRE37]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**2\. Table**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 表格**'
- en: '[PRE39]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can note that the more detailed the caption task, the longer the latency
    and the possibility of mistakes (like “The image shows a group of four cats and
    a dog in a garden”, instead of two dogs and three cats).
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以注意到，标题任务越详细，延迟越长，出错的可能性也越大（例如，“图像显示了一个花园中的四只猫和一只狗”，而不是两只狗和三只猫）。
- en: Object Detection
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测
- en: We can run the same previous function for object detection using the prompt
    `<OD>`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `<OD>` 提示运行之前用于对象检测的相同函数。
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s see the result:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '[PRE42]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Only by the labels `[''cat,'' ''cat,'' ''cat,'' ''dog,'' ''dog'']` is it possible
    to see that the main objects in the image were captured. Let’s apply the function
    used before to draw the bounding boxes:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 只有通过标签 `['cat,' 'cat,' 'cat,' 'dog,' 'dog']` 才能看出图像中的主要对象已被捕捉。让我们应用之前使用的函数来绘制边界框：
- en: '[PRE43]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](../media/file952.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file952.png)'
- en: 'Let’s also do it with the Table image:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也用表格图片来做：
- en: '[PRE44]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../media/file953.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file953.png)'
- en: Dense Region Caption
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集区域标题
- en: 'It is possible to mix the classic Object Detection with the Caption task in
    specific sub-regions of the image:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像的特定子区域中，可以将经典的目标检测与标题任务混合：
- en: '[PRE46]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](../media/file954.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file954.png)'
- en: Caption to Phrase Grounding
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标题到短语定位
- en: 'With this task, we can enter with a caption, such as “a wine glass”, “a wine
    bottle,” or “a half orange,” and Florence-2 will localize the object in the image:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，我们可以输入一个标题，例如“一个酒杯”，“一个酒瓶”，或者“半个橙子”，Florence-2 将在图像中定位该对象：
- en: '[PRE47]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](../media/file955.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file955.png)'
- en: '[PRE48]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Cascade Tasks
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 级联任务
- en: 'We can also enter the image caption as the input text to push Florence-2 to
    find more objects:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将图像标题作为输入文本输入，以推动 Florence-2 找到更多对象：
- en: '[PRE49]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Changing the task_prompt among `<CAPTION,>` `<DETAILED_CAPTION>` and `<MORE_DETAILED_CAPTION>`,
    we will get more objects in the image.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `<CAPTION,>` `<DETAILED_CAPTION>` 和 `<MORE_DETAILED_CAPTION>` 之间更改 task_prompt，我们将在图像中获得更多的对象。
- en: '![](../media/file956.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file956.png)'
- en: Open Vocabulary Detection
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开放词汇检测
- en: '`<OPEN_VOCABULARY_DETECTION>` allows Florence-2 to detect recognizable objects
    in an image without relying on a predefined list of categories, making it a versatile
    tool for identifying various items that may not have been explicitly labeled during
    training. Unlike `<CAPTION_TO_PHRASE_GROUNDING>`, which requires a specific text
    phrase to locate and highlight a particular object in an image, `<OPEN_VOCABULARY_DETECTION>`
    performs a broad scan to find and classify all objects present.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`<OPEN_VOCABULARY_DETECTION>` 允许 Florence-2 在不依赖于预定义类别列表的情况下检测图像中的可识别对象，使其成为识别各种可能未在训练期间明确标记的项目的一种多用途工具。与
    `<CAPTION_TO_PHRASE_GROUNDING>` 不同，后者需要特定的文本短语来定位和突出显示图像中的特定对象，`<OPEN_VOCABULARY_DETECTION>`
    执行广泛的扫描以找到和分类所有存在的对象。'
- en: 'This makes `<OPEN_VOCABULARY_DETECTION>` particularly useful for applications
    where you need a comprehensive overview of everything in an image without prior
    knowledge of what to expect. Enter with a text describing specific objects not
    previously detected, resulting in their detection. For example:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 `<OPEN_VOCABULARY_DETECTION>` 在需要在没有事先了解预期内容的情况下对图像中的所有内容有一个全面概述的应用中特别有用。输入描述特定未检测对象的文本，从而实现其检测。例如：
- en: '[PRE50]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](../media/file957.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file957.png)'
- en: '[PRE51]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Note: Trying to use Florence-2 to find objects that were not found can leads
    to mistakes (see examples on the Notebook).'
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：尝试使用 Florence-2 找到未找到的对象可能会导致错误（请参阅笔记本中的示例）。
- en: Referring expression segmentation
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指示表达式分割
- en: We can also segment a specific object in the image and give its description
    (caption), such as “a wine bottle” on the table image or “a German Sheppard” on
    the dogs_cats.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在图像中分割特定对象并给出其描述（标题），例如在桌子图像上为“一瓶红酒”或在 dogs_cats 上的“德国牧羊犬”。
- en: 'Referring expression segmentation results format: `{''<REFERRING_EXPRESSION_SEGMENTATION>'':
    {''Polygons'': [[[polygon]], ...], ''labels'': ['''', '''', ...]}}`, one object
    is represented by a list of polygons. each polygon is `[x1, y1, x2, y2, ..., xn,
    yn]`.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '指示表达式分割结果格式：`{''<REFERRING_EXPRESSION_SEGMENTATION>'': {''Polygons'': [[[polygon]],
    ...], ''labels'': ['''', '''', ...]}}`，一个对象由一系列多边形表示。每个多边形是 `[x1, y1, x2, y2,
    ..., xn, yn]`。'
- en: '**Polygon (x1, y1, …, xn, yn)**: Location tokens represent the vertices of
    a polygon in clockwise order.'
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**多边形（x1, y1, …, xn, yn）**：位置标记表示多边形的顺时针顺序顶点。'
- en: 'So, let’s first create a function to plot the segmentation:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们首先创建一个函数来绘制分割：
- en: '[PRE52]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we can run the functions:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行函数：
- en: '[PRE53]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](../media/file958.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file958.png)'
- en: '[PRE54]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Region to Segmentation
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域到分割
- en: With this task, it is also possible to give the object coordinates in the image
    to segment it. The input format is `'<loc_x1><loc_y1><loc_x2><loc_y2>', [x1, y1,
    x2, y2]` , which is the quantized coordinates in [0, 999].
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，也可以给出图像中要分割的对象坐标。输入格式是 `'<loc_x1><loc_y1><loc_x2><loc_y2>', [x1, y1,
    x2, y2]` ，这是 [0, 999] 中的量化坐标。
- en: 'For example, when running the code:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当运行代码时：
- en: '[PRE55]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The results were:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE56]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Using the bboxes rounded coordinates:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用四舍五入的边界框坐标：
- en: '[PRE57]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We got the segmentation of the object on those coordinates (Latency: 83 seconds):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这些坐标上获得了对象的分割（延迟：83秒）：
- en: '![](../media/file959.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file959.png)'
- en: Region to Texts
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域到文本
- en: 'We can also give the region (coordinates and ask for a caption):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以给出区域（坐标）并请求一个标题：
- en: '[PRE58]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The model identified an orange in that region. Let’s ask for a description:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在那个区域识别出一个橙子。让我们请求一个描述：
- en: '[PRE60]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In this case, the description did not provide more details, but it could. Try
    another example.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，描述没有提供更多细节，但它可以。尝试另一个例子。
- en: OCR
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OCR
- en: With Florence-2, we can perform Optical Character Recognition (OCR) on an image,
    getting what is written on it (`task_prompt = '<OCR>'` and also get the bounding
    boxes (location) for the detected text (`ask_prompt = '<OCR_WITH_REGION>'`). Those
    tasks can help extract and locate textual information in images, such as reading
    signs, labels, or other forms of text in images.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Florence-2，我们可以在图像上执行光学字符识别（OCR），获取其上所写的内容（`task_prompt = '<OCR>'`）以及检测到的文本的边界框（位置）（`ask_prompt
    = '<OCR_WITH_REGION>'`）。这些任务可以帮助提取和定位图像中的文本信息，例如阅读标志、标签或其他形式的文本。
- en: 'Let’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another
    language, here Portuguese):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从巴西的演讲中上传一张传单到 Raspi。让我们测试另一种语言的工作，这里是用葡萄牙语）：
- en: '[PRE62]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](../media/file960.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file960.png)'
- en: 'Let’s examine the image with `''<MORE_DETAILED_CAPTION>''` :'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `'<MORE_DETAILED_CAPTION>'` 检查图像：
- en: '[PRE63]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The description is very accurate. Let’s get to the more important words with
    the task OCR:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 描述非常准确。让我们使用 OCR 任务获取更重要的单词：
- en: '[PRE64]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let’s locate the words in the flyer:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定位传单中的单词：
- en: '[PRE66]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let’s also create a function to draw bounding boxes around the detected words:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个函数来绘制检测到的单词周围的边界框：
- en: '[PRE67]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](../media/file961.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file961.png)'
- en: 'We can inspect the detected words:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查检测到的单词：
- en: '[PRE69]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Latency Summary
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟摘要
- en: 'The latency observed for different tasks using Florence-2 on the Raspberry
    Pi (Raspi-5) varied depending on the complexity of the task:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Florence-2 在 Raspberry Pi (Raspi-5) 上执行不同任务时观察到的延迟因任务的复杂度而异：
- en: '**Image Captioning**: It took approximately 16-17 seconds to generate a caption
    for an image.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像标题**：为图像生成标题大约需要 16-17 秒。'
- en: '**Detailed Captioning**: Increased latency to around 25-27 seconds, requiring
    generating more nuanced scene descriptions.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**详细标题**：延迟增加到约 25-27 秒，需要生成更细致的场景描述。'
- en: '**More Detailed Captioning**: It took about 32-50 seconds, and the latency
    increased as the description grew more complex.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更详细的标题**：大约需要 32-50 秒，随着描述变得更加复杂，延迟增加。'
- en: '**Object Detection**: It took approximately 20-41 seconds, depending on the
    image’s complexity and the number of detected objects.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象检测**：根据图像的复杂性和检测到的对象数量，大约需要 20-41 秒。'
- en: '**Visual Grounding**: Approximately 15-16 seconds to localize specific objects
    based on textual prompts.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视觉定位**：根据文本提示定位特定对象，大约需要 15-16 秒。'
- en: '**OCR (Optical Character Recognition)**: Extracting text from an image took
    around 37-38 seconds.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**光学字符识别（OCR）**：从图像中提取文本大约需要37-38秒。'
- en: '**Segmentation and Region to Segmentation**: Segmentation tasks took considerably
    longer, with a latency of around 83-207 seconds, depending on the complexity and
    the number of regions to be segmented.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割与区域分割**：分割任务耗时较长，大约为83-207秒，具体取决于复杂性和需要分割的区域数量。'
- en: These latency times highlight the resource constraints of edge devices like
    the Raspberry Pi and emphasize the need to optimize the model and the environment
    to achieve real-time performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这些延迟时间突出了边缘设备（如Raspberry Pi）的资源限制，并强调了优化模型和环境以实现实时性能的必要性。
- en: '![](../media/file962.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file962.png)'
- en: Running complex tasks can use all 8 GB of the Raspi-5’s memory. For example,
    the above screenshot during the Florence OD task shows 4 CPUs at full speed and
    over 5 GB of memory in use. Consider increasing the SWAP memory to 2 GB.
  id: totrans-351
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 运行复杂任务可能会使用Raspi-5的8 GB内存。例如，上述截图显示在Florence OD任务期间，有4个CPU全速运行，并使用了超过5 GB的内存。考虑将SWAP内存增加到2
    GB。
- en: Checking the CPU temperature with `vcgencmd measure_temp` , showed that temperature
    can go up to +80oC.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`vcgencmd measure_temp`检查CPU温度，显示温度可升至+80°C。
- en: Fine-Tuning
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: As explored in this lab, Florence supports many tasks out of the box, including
    captioning, object detection, OCR, and more. However, like other pre-trained foundational
    models, Florence-2 may need domain-specific knowledge. For example, it may need
    to improve with medical or satellite imagery. In such cases, **fine-tuning** with
    a custom dataset is necessary. The Roboflow tutorial, [How to Fine-tune Florence-2
    for Object Detection Tasks](https://blog.roboflow.com/fine-tune-florence-2-object-detection/),
    shows how to fine-tune Florence-2 on object detection datasets to improve model
    performance for our specific use case.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如本实验室所探讨的，Florence支持许多开箱即用的任务，包括描述、目标检测、OCR等。然而，与其他预训练的基础模型一样，Florence-2可能需要特定领域的知识。例如，它可能需要通过医学或卫星图像来改进。在这种情况下，**微调**与自定义数据集是必要的。Roboflow教程[如何微调Florence-2以进行目标检测任务](https://blog.roboflow.com/fine-tune-florence-2-object-detection/)展示了如何在目标检测数据集上微调Florence-2以提高模型性能以适应我们的特定用例。
- en: 'Based on the above tutorial, it is possible to fine-tune the Florence-2 model
    to detect boxes and wheels used in previous labs:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述教程，可以微调Florence-2模型以检测之前实验室中使用的箱子和轮子：
- en: '![](../media/file963.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file963.png)'
- en: It is important to note that after fine-tuning, the model can still detect classes
    that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen
    before).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在微调后，模型仍然可以检测到不属于我们自定义数据集的类别，如猫、狗、葡萄等，正如之前所见）。
- en: 'The complete fine-tuning project using a previously annotated dataset in Roboflow
    and executed on CoLab can be found in the notebook:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在Roboflow中使用先前标注的数据集并在CoLab上执行的全局微调项目可以在以下笔记本中找到：
- en: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
- en: In another example, in the post, [Fine-tuning Florence-2 - Microsoft’s Cutting-edge
    Vision Language Models](https://huggingface.co/blog/finetune-florence2), the authors
    show an example of fine-tuning Florence on `DocVQA`. The authors report that Florence
    2 can perform visual question answering (VQA), but the released models don’t include
    VQA capability.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个例子中，在文章[微调Florence-2 - 微软的尖端视觉语言模型](https://huggingface.co/blog/finetune-florence2)中，作者展示了在`DocVQA`上微调Florence的示例。作者报告称，Florence
    2可以执行视觉问答（VQA），但发布的模型不包括VQA功能。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Florence-2 offers a versatile and powerful approach to vision-language tasks
    at the edge, providing performance that rivals larger, task-specific models, such
    as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized
    OCR models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2为边缘的视觉语言任务提供了一种灵活且强大的方法，其性能可与更大的、特定于任务的模型相媲美，例如用于目标检测的YOLO，用于文本分析的BERT/RoBERTa，以及专门的OCR模型。
- en: Thanks to its multi-modal transformer architecture, Florence-2 is more flexible
    than YOLO in terms of the tasks it can handle. These include object detection,
    image captioning, and visual grounding.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了其多模态转换器架构，Florence-2在处理任务方面比YOLO更灵活。这些任务包括目标检测、图像描述和视觉定位。
- en: Unlike **BERT**, which focuses purely on language, Florence-2 integrates vision
    and language, allowing it to excel in applications that require both modalities,
    such as image captioning and visual grounding.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅关注语言的**BERT**不同，Florence-2整合了视觉和语言，使其在需要这两种模态的应用中表现出色，例如图像标题和视觉定位。
- en: Moreover, while traditional **OCR models** such as Tesseract and EasyOCR are
    designed solely for recognizing and extracting text from images, Florence-2’s
    OCR capabilities are part of a broader framework that includes contextual understanding
    and visual-text alignment. This makes it particularly useful for scenarios that
    require both reading text and interpreting its context within images.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然传统的**OCR模型**如Tesseract和EasyOCR仅设计用于从图像中识别和提取文本，但Florence-2的OCR能力是更广泛框架的一部分，该框架包括上下文理解和视觉-文本对齐。这使得它在需要读取文本并解释图像中其上下文的场景中特别有用。
- en: Overall, Florence-2 stands out for its ability to seamlessly integrate various
    vision-language tasks into a unified model that is efficient enough to run on
    edge devices like the Raspberry Pi. This makes it a compelling choice for developers
    and researchers exploring AI applications at the edge.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Florence-2因其能够将各种视觉-语言任务无缝集成到一个统一模型中而脱颖而出，该模型足够高效，可以在树莓派等边缘设备上运行。这使得它成为探索边缘AI应用的开发者和研究人员的一个有吸引力的选择。
- en: Key Advantages of Florence-2
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Florence-2的关键优势
- en: '**Unified Architecture**'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**统一架构**'
- en: Single model handles multiple vision tasks vs. specialized models (YOLO, BERT,
    Tesseract)
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个模型处理多个视觉任务与专用模型（YOLO，BERT，Tesseract）相比
- en: Eliminates the need for multiple model deployments and integrations
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除了需要多个模型部署和集成的需求
- en: Consistent API and interface across tasks
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务间一致的API和界面
- en: '**Performance Comparison**'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能比较**'
- en: 'Object Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs. YOLOv8’s ~39.7
    mAP) despite being general-purpose'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测：尽管是通用型，但与YOLOv8相当（在COCO上约为37.5 mAP，与YOLOv8的约39.7 mAP相当）
- en: 'Text Recognition: Handles multiple languages effectively like specialized OCR
    models (Tesseract, EasyOCR)'
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本识别：有效处理多种语言，如专门的OCR模型（Tesseract，EasyOCR）
- en: 'Language Understanding: Integrates BERT-like capabilities for text processing
    while adding visual context'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言理解：集成类似BERT的文本处理能力，同时添加视觉上下文
- en: '**Resource Efficiency**'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源效率**'
- en: The Base model (232M parameters) achieves strong results despite smaller size
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型（232M参数）尽管规模较小，但取得了良好的效果
- en: Runs effectively on edge devices (Raspberry Pi)
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘设备（树莓派）上运行有效
- en: Single model deployment vs. multiple specialized models
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个模型部署与多个专用模型
- en: Trade-offs
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权衡
- en: '**Performance vs. Specialized Models**'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能与专用模型比较**'
- en: YOLO series may offer faster inference for pure object detection
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO系列可能在纯目标检测方面提供更快的推理
- en: Specialized OCR models might handle complex document layouts better
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用OCR模型可能更好地处理复杂的文档布局
- en: BERT/RoBERTa provide deeper language understanding for text-only tasks
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT/RoBERTa为纯文本任务提供更深入的语言理解
- en: '**Resource Requirements**'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源需求**'
- en: Higher latency on edge devices (15-200s depending on task)
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘设备上的延迟更高（15-200秒，取决于任务）
- en: Requires careful memory management on Raspberry Pi
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在树莓派上仔细管理内存
- en: It may need optimization for real-time applications
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要针对实时应用进行优化
- en: '**Deployment Considerations**'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署考虑**'
- en: Initial setup is more complex than single-purpose models
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始设置比专用模型更复杂
- en: Requires understanding of multiple task types and prompts
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要理解多种任务类型和提示
- en: The learning curve for optimal prompt engineering
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化提示工程的学习曲线
- en: Best Use Cases
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳用例
- en: '**Resource-Constrained Environments**'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**资源受限环境**'
- en: Edge devices requiring multiple vision capabilities
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要多种视觉能力的边缘设备
- en: Systems with limited storage/deployment capacity
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储或部署容量有限的系统
- en: Applications needing flexible vision processing
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要灵活视觉处理的程序
- en: '**Multi-modal Applications**'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多模态应用**'
- en: Content moderation systems
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容审查系统
- en: Accessibility tools
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无障碍工具
- en: Document analysis workflows
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分析工作流程
- en: '**Rapid Prototyping**'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**快速原型设计**'
- en: Quick deployment of vision capabilities
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速部署视觉能力
- en: Testing multiple vision tasks without separate models
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不使用单独模型的情况下测试多个视觉任务
- en: Proof-of-concept development
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念验证开发
- en: Future Implications
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 未来影响
- en: Florence-2 represents a shift toward unified vision models that could eventually
    replace task-specific architectures in many applications. While specialized models
    maintain advantages in specific scenarios, the convenience and efficiency of unified
    models like Florence-2 make them increasingly attractive for real-world deployments.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 代表了向统一视觉模型转变的趋势，这些模型最终可能取代许多应用中的特定任务架构。虽然专用模型在特定场景中保持优势，但像 Florence-2
    这样的统一模型在便利性和效率方面的优势使它们在现实世界的部署中越来越有吸引力。
- en: The lab demonstrates Florence-2’s viability on edge devices, suggesting future
    IoT, mobile computing, and embedded systems applications where deploying multiple
    specialized models would be impractical.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室展示了 Florence-2 在边缘设备上的可行性，这表明未来在物联网、移动计算和嵌入式系统中部署多个专用模型将是不切实际的。
- en: Resources
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10-florence2_test.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb)'
- en: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20-florence_2.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb)'
- en: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb)'
