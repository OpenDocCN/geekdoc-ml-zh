- en: Small Language Models (SLM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰
- en: '![](../media/file908.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file908.jpg)'
- en: '*DALLÂ·E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi
    running a small language model at the edge. The Raspberry Pi is stylized in a
    retro-futuristic way with rounded edges and chrome accents, connected to playful
    cartoonish sensors and devices. Speech bubbles are floating around, representing
    language processing, and the background has a whimsical landscape of interconnected
    devices with wires and small gadgets, all drawn in a vintage cartoon style. The
    color palette uses soft pastel colors and bold outlines typical of 1950s cartoons,
    giving a fun and nostalgic vibe to the scene.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALLÂ·Eæç¤º - ä¸€å¹…1950å¹´ä»£é£æ ¼çš„å¡é€šæ’å›¾ï¼Œå±•ç¤ºä¸€ä¸ªRaspberry Piåœ¨è¾¹ç¼˜è¿è¡Œä¸€ä¸ªå°å‹è¯­è¨€æ¨¡å‹ã€‚Raspberry Piä»¥å¤å¤æœªæ¥ä¸»ä¹‰çš„æ–¹å¼è®¾è®¡ï¼Œè¾¹ç¼˜åœ†æ¶¦ï¼Œé“¬è‰²è£…é¥°ï¼Œè¿æ¥åˆ°æœ‰è¶£çš„å¡é€šå¼ä¼ æ„Ÿå™¨å’Œè®¾å¤‡ã€‚æ°”æ³¡åœ¨å‘¨å›´é£˜æµ®ï¼Œä»£è¡¨è¯­è¨€å¤„ç†ï¼ŒèƒŒæ™¯æ˜¯ä¸€ä¸ªå¥‡å¦™çš„äº’è”è®¾å¤‡æ™¯è§‚ï¼Œæœ‰ç”µçº¿å’Œå°å‹å°ç©æ„ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä»¥å¤å¤å¡é€šé£æ ¼ç»˜åˆ¶ã€‚è‰²å½©è°ƒè‰²æ¿ä½¿ç”¨æŸ”å’Œçš„ç²‰å½©é¢œè‰²å’Œå…¸å‹çš„1950å¹´ä»£å¡é€šçš„ç²—è½®å»“ï¼Œç»™åœºæ™¯å¢æ·»äº†ä¹è¶£å’Œæ€€æ—§æ„Ÿã€‚*'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: In the fast-growing area of artificial intelligence, edge computing presents
    an opportunity to decentralize capabilities traditionally reserved for powerful,
    centralized servers. This lab explores the practical integration of small versions
    of traditional large language models (LLMs) into a Raspberry Pi 5, transforming
    this edge device into an AI hub capable of real-time, on-site data processing.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äººå·¥æ™ºèƒ½å¿«é€Ÿå¢é•¿çš„é¢†åŸŸï¼Œè¾¹ç¼˜è®¡ç®—ä¸ºå°†ä¼ ç»Ÿä¸Šä¿ç•™ç»™å¼ºå¤§ã€é›†ä¸­å¼æœåŠ¡å™¨çš„åŠŸèƒ½å»ä¸­å¿ƒåŒ–æä¾›äº†æœºä¼šã€‚æœ¬å®éªŒæ¢è®¨äº†å°†ä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å°å‹ç‰ˆæœ¬é›†æˆåˆ°Raspberry
    Pi 5ä¸­çš„å®é™…åº”ç”¨ï¼Œå°†è¿™ä¸ªè¾¹ç¼˜è®¾å¤‡è½¬å˜ä¸ºä¸€ä¸ªèƒ½å¤Ÿå®æ—¶ã€ç°åœºå¤„ç†æ•°æ®çš„AIä¸­å¿ƒã€‚
- en: As large language models grow in size and complexity, Small Language Models
    (SLMs) offer a compelling alternative for edge devices, striking a balance between
    performance and resource efficiency. By running these models directly on Raspberry
    Pi, we can create responsive, privacy-preserving applications that operate even
    in environments with limited or no internet connectivity.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„æ¨¡å’Œå¤æ‚æ€§ä¸Šçš„å¢é•¿ï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸ºè¾¹ç¼˜è®¾å¤‡æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚é€šè¿‡ç›´æ¥åœ¨Raspberry
    Piä¸Šè¿è¡Œè¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå“åº”å¼ã€ä¿æŠ¤éšç§çš„åº”ç”¨ç¨‹åºï¼Œå³ä½¿åœ¨æœ‰é™æˆ–æ²¡æœ‰äº’è”ç½‘è¿æ¥çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½è¿è¡Œã€‚
- en: This lab will guide you through setting up, optimizing, and leveraging SLMs
    on Raspberry Pi. We will explore the installation and utilization of [Ollama](https://ollama.com/).
    This open-source framework allows us to run LLMs locally on our machines (our
    desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama
    is designed to be efficient, scalable, and easy to use, making it a good option
    for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa
    (Multimodal). We will integrate some of those models into projects using Pythonâ€™s
    ecosystem, exploring their potential in real-world scenarios (or at least point
    in this direction).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å®éªŒå°†æŒ‡å¯¼æ‚¨åœ¨Raspberry Piä¸Šè®¾ç½®ã€ä¼˜åŒ–å’Œåˆ©ç”¨SLMsã€‚æˆ‘ä»¬å°†æ¢è®¨[Ollama](https://ollama.com/)çš„å®‰è£…å’Œä½¿ç”¨ã€‚è¿™ä¸ªå¼€æºæ¡†æ¶å…è®¸æˆ‘ä»¬åœ¨è‡ªå·±çš„æœºå™¨ä¸Šï¼ˆæˆ‘ä»¬çš„å°å¼æœºæˆ–è¾¹ç¼˜è®¾å¤‡ï¼Œå¦‚Raspberry
    Piæˆ–NVidia Jetsonsï¼‰æœ¬åœ°è¿è¡ŒLLMsã€‚Ollamaè¢«è®¾è®¡æˆé«˜æ•ˆã€å¯æ‰©å±•ä¸”æ˜“äºä½¿ç”¨ï¼Œä½¿å…¶æˆä¸ºéƒ¨ç½²AIæ¨¡å‹ï¼ˆå¦‚Microsoft Phiã€Google
    Gemmaã€Meta Llamaå’ŒLLaVaï¼ˆå¤šæ¨¡æ€ï¼‰ï¼‰çš„å¥½é€‰æ‹©ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Pythonç”Ÿæ€ç³»ç»Ÿå°†è¿™äº›æ¨¡å‹ä¸­çš„æŸäº›é›†æˆåˆ°é¡¹ç›®ä¸­ï¼Œæ¢ç´¢å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ½œåŠ›ï¼ˆæˆ–è€…è‡³å°‘æŒ‡å‘è¿™ä¸ªæ–¹å‘ï¼‰ã€‚
- en: '![](../media/file909.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file909.jpg)'
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: We could use any Raspi model in the previous labs, but here, the choice must
    be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades
    the last version 4, equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit
    Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities.
    It boasts a VideoCore VII GPU, dual 4Kp60 HDMIÂ® outputs with HDR, and a 4Kp60
    HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM,
    with 8GB being our choice to run SLMs. It also features expandable storage via
    a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2
    SSDs (Solid State Drives).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰å®éªŒä¸­çš„ä»»ä½•Raspiå‹å·ï¼Œä½†åœ¨è¿™é‡Œï¼Œé€‰æ‹©å¿…é¡»æ˜¯Raspberry Pi 5ï¼ˆRaspi-5ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªåšå›ºçš„å¹³å°ï¼Œå®è´¨æ€§åœ°å‡çº§äº†ä¸Šä¸€ç‰ˆæœ¬çš„4ï¼Œé…å¤‡äº†Broadcom
    BCM2712ï¼Œä¸€ä¸ª2.4 GHzå››æ ¸64ä½Arm Cortex-A76 CPUï¼Œå…·æœ‰åŠ å¯†æ‰©å±•å’Œå¢å¼ºçš„ç¼“å­˜åŠŸèƒ½ã€‚å®ƒæ‹¥æœ‰VideoCore VII GPUï¼ŒåŒ4Kp60
    HDMIÂ®è¾“å‡ºå¸¦HDRï¼Œä»¥åŠ4Kp60 HEVCè§£ç å™¨ã€‚å†…å­˜é€‰é¡¹åŒ…æ‹¬4 GBå’Œ8 GBçš„é«˜é€ŸLPDDR4X SDRAMï¼Œæˆ‘ä»¬é€‰æ‹©8 GBæ¥è¿è¡ŒSLMsã€‚å®ƒè¿˜é€šè¿‡microSDå¡æ§½å’ŒPCIe
    2.0æ¥å£æä¾›å¯æ‰©å±•çš„å­˜å‚¨ï¼Œç”¨äºå¿«é€Ÿå¤–å›´è®¾å¤‡ï¼Œå¦‚M.2 SSDsï¼ˆå›ºæ€ç¡¬ç›˜ï¼‰ã€‚
- en: For real SSL applications, SSDs are a better option than SD cards.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸæ­£çš„SSLåº”ç”¨ï¼ŒSSDæ¯”SDå¡æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚
- en: By the way, as [Alasdair Allan](https://www.hackster.io/aallan) discussed, inferencing
    directly on the Raspberry Pi 5 CPUâ€”with no GPU accelerationâ€”is now on par with
    the performance of the Coral TPU.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿æä¸€ä¸‹ï¼Œæ­£å¦‚[é˜¿æ‹‰æ–¯ä»£å°”Â·è‰¾ä¼¦](https://www.hackster.io/aallan)æ‰€è®¨è®ºçš„ï¼Œç›´æ¥åœ¨Raspberry Pi 5 CPUä¸Šè¿›è¡Œæ¨ç†â€”â€”æ²¡æœ‰GPUåŠ é€Ÿâ€”â€”ç°åœ¨çš„æ€§èƒ½å·²ç»ä¸Coral
    TPUç›¸å½“ã€‚
- en: '![](../media/file910.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file910.png)'
- en: 'For more info, please see the complete article: [Benchmarking TensorFlow and
    TensorFlow Lite on Raspberry Pi 5](https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2?mc_cid=0cab3d08f4&mc_eid=e96256ccba).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å®Œæ•´æ–‡ç« ï¼š[åœ¨Raspberry Pi 5ä¸ŠåŸºå‡†æµ‹è¯•TensorFlowå’ŒTensorFlow Lite](https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2?mc_cid=0cab3d08f4&mc_eid=e96256ccba)ã€‚
- en: Raspberry Pi Active Cooler
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Raspberry Piæ´»æ€§æ•£çƒ­å™¨
- en: We suggest installing an Active Cooler, a dedicated clip-on cooling solution
    for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heat sink
    with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably
    under heavy loads, such as running SLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®ä¸ºè¿™ä¸ªå®éªŒå®¤å®‰è£…ä¸€ä¸ªæ´»æ€§æ•£çƒ­å™¨ï¼Œè¿™æ˜¯ä¸ºRaspberry Pi 5ï¼ˆRaspi-5ï¼‰è®¾è®¡çš„ä¸“ç”¨å¤¹å…·å¼å†·å´è§£å†³æ–¹æ¡ˆã€‚å®ƒç»“åˆäº†é“åˆ¶æ•£çƒ­å™¨å’Œæ¸©åº¦æ§åˆ¶çš„å¹é£é£æ‰‡ï¼Œä»¥ä¿æŒRaspi-5åœ¨é‡è´Ÿè½½ä¸‹ï¼ˆå¦‚è¿è¡ŒSLMsï¼‰èˆ’é€‚è¿è¡Œã€‚
- en: '![](../media/file911.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file911.png)'
- en: 'The Active Cooler has pre-applied thermal pads for heat transfer and is mounted
    directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry
    Pi firmware actively manages it: at 60Â°C, the blowerâ€™s fan will be turned on;
    at 67.5Â°C, the fan speed will be increased; and finally, at 75Â°C, the fan increases
    to full speed. The blowerâ€™s fan will spin down automatically when the temperature
    drops below these limits.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ´»æ€§æ•£çƒ­å™¨é¢„å…ˆæ¶‚æœ‰çƒ­å«ä»¥è¿›è¡Œçƒ­ä¼ é€’ï¼Œå¹¶ä½¿ç”¨å¼¹ç°§åŠ è½½çš„æ¨é’‰ç›´æ¥å®‰è£…åˆ°Raspberry Pi 5æ¿ä¸Šã€‚Raspberry Piå›ºä»¶ä¼šä¸»åŠ¨ç®¡ç†å®ƒï¼šåœ¨60Â°Cæ—¶ï¼Œé£æ‰‡å°†å¼€å¯ï¼›åœ¨67.5Â°Cæ—¶ï¼Œé£æ‰‡é€Ÿåº¦å°†æé«˜ï¼›æœ€åï¼Œåœ¨75Â°Cæ—¶ï¼Œé£æ‰‡å°†å…¨é€Ÿè¿è½¬ã€‚å½“æ¸©åº¦é™è‡³è¿™äº›é™åˆ¶ä»¥ä¸‹æ—¶ï¼Œé£æ‰‡å°†è‡ªåŠ¨å‡é€Ÿã€‚
- en: '![](../media/file912.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file912.png)'
- en: To prevent overheating, all Raspberry Pi boards begin to throttle the processor
    when the temperature reaches 80Â°Cand throttle even further when it reaches the
    maximum temperature of 85Â°C (more detail [here](https://www.raspberrypi.com/news/heating-and-cooling-raspberry-pi-5/)).
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†é˜²æ­¢è¿‡çƒ­ï¼Œæ‰€æœ‰Raspberry Piæ¿åœ¨æ¸©åº¦è¾¾åˆ°80Â°Cæ—¶å¼€å§‹é™ä½å¤„ç†å™¨é€Ÿåº¦ï¼Œå½“æ¸©åº¦è¾¾åˆ°æœ€å¤§æ¸©åº¦85Â°Cæ—¶è¿›ä¸€æ­¥é™ä½é€Ÿåº¦ï¼ˆæ›´å¤šè¯¦æƒ…[è¿™é‡Œ](https://www.raspberrypi.com/news/heating-and-cooling-raspberry-pi-5/))ã€‚
- en: Generative AI (GenAI)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼AIï¼ˆGenAIï¼‰
- en: Generative AI is an artificial intelligence system capable of creating new,
    original content across various mediums such as **text, images, audio, and video**.
    These systems learn patterns from existing data and use that knowledge to generate
    novel outputs that didnâ€™t previously exist. **Large Language Models (LLMs)**,
    **Small Language Models (SLMs)**, and **multimodal models** can all be considered
    types of GenAI when used for generative tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼AIæ˜¯ä¸€ç§èƒ½å¤Ÿåœ¨å„ç§åª’ä»‹å¦‚**æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘**ä¸­åˆ›å»ºæ–°ã€åŸåˆ›å†…å®¹çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¿™äº›ç³»ç»Ÿä»ç°æœ‰æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†ç”Ÿæˆä¹‹å‰ä¸å­˜åœ¨çš„æ–°é¢–è¾“å‡ºã€‚**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**ã€**å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰**å’Œ**å¤šæ¨¡æ€æ¨¡å‹**éƒ½å¯ä»¥è¢«è®¤ä¸ºæ˜¯ç”¨äºç”Ÿæˆä»»åŠ¡çš„ç”Ÿæˆå¼AIçš„ç±»å‹ã€‚
- en: GenAI provides the conceptual framework for AI-driven content creation, with
    LLMs serving as powerful general-purpose text generators. SLMs adapt this technology
    for edge computing, while multimodal models extend GenAI capabilities across different
    data types. Together, they represent a spectrum of generative AI technologies,
    each with its strengths and applications, collectively driving AI-powered content
    creation and understanding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¼AIä¸ºAIé©±åŠ¨çš„å†…å®¹åˆ›ä½œæä¾›äº†æ¦‚å¿µæ¡†æ¶ï¼Œå…¶ä¸­LLMsä½œä¸ºå¼ºå¤§çš„é€šç”¨æ–‡æœ¬ç”Ÿæˆå™¨ã€‚SLMså°†è¿™é¡¹æŠ€æœ¯åº”ç”¨äºè¾¹ç¼˜è®¡ç®—ï¼Œè€Œå¤šæ¨¡æ€æ¨¡å‹æ‰©å±•äº†ç”Ÿæˆå¼AIåœ¨ä¸åŒæ•°æ®ç±»å‹ä¸Šçš„èƒ½åŠ›ã€‚å®ƒä»¬å…±åŒä»£è¡¨äº†ä¸€ç»„ç”Ÿæˆå¼AIæŠ€æœ¯ï¼Œæ¯ç§æŠ€æœ¯éƒ½æœ‰å…¶ä¼˜åŠ¿å’Œç”¨é€”ï¼Œå…±åŒæ¨åŠ¨AIé©±åŠ¨çš„åˆ›ä½œå’Œç†è§£ã€‚
- en: Large Language Models (LLMs)
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰
- en: 'Large Language Models (LLMs) are advanced artificial intelligence systems that
    understand, process, and generate human-like text. These models are characterized
    by their massive scale in terms of the amount of data they are trained on and
    the number of parameters they contain. Critical aspects of LLMs include:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯é«˜çº§äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿç†è§£ã€å¤„ç†å’Œç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚è¿™äº›æ¨¡å‹çš„ç‰¹ç‚¹æ˜¯å®ƒä»¬åœ¨è®­ç»ƒæ•°æ®é‡å’Œå‚æ•°æ•°é‡ä¸Šçš„å·¨å¤§è§„æ¨¡ã€‚LLMsçš„å…³é”®æ–¹é¢åŒ…æ‹¬ï¼š
- en: '**Size**: LLMs typically contain billions of parameters. For example, GPT-3
    has 175 billion parameters, while some newer models exceed a trillion parameters.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è§„æ¨¡**ï¼šLLMsé€šå¸¸åŒ…å«æ•°åäº¿ä¸ªå‚æ•°ã€‚ä¾‹å¦‚ï¼ŒGPT-3æœ‰1750äº¿ä¸ªå‚æ•°ï¼Œè€Œä¸€äº›è¾ƒæ–°çš„æ¨¡å‹è¶…è¿‡äº†ä¸‡äº¿ä¸ªå‚æ•°ã€‚'
- en: '**Training Data**: They are trained on vast amounts of text data, often including
    books, websites, and other diverse sources, amounting to hundreds of gigabytes
    or even terabytes of text.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ•°æ®**ï¼šå®ƒä»¬åœ¨å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œé€šå¸¸åŒ…æ‹¬ä¹¦ç±ã€ç½‘ç«™å’Œå…¶ä»–å¤šæ ·åŒ–çš„æ¥æºï¼Œæ€»è®¡è¾¾åˆ°æ•°ç™¾ç”šè‡³æ•°åƒå‰å­—èŠ‚ï¼ˆGBï¼‰æˆ–ç”šè‡³å¤ªå­—èŠ‚ï¼ˆTBï¼‰çš„æ–‡æœ¬ã€‚'
- en: '**Architecture**: Most LLMs use [transformer-based architectures](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)),
    which allow them to process and generate text by paying attention to different
    parts of the input simultaneously.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¶æ„**ï¼šå¤§å¤šæ•°LLMsä½¿ç”¨åŸºäº[transformeræ¶æ„](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿé€šè¿‡åŒæ—¶å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†æ¥å¤„ç†å’Œç”Ÿæˆæ–‡æœ¬ã€‚'
- en: '**Capabilities**: LLMs can perform a wide range of language tasks without specific
    fine-tuning, including:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŠŸèƒ½**ï¼šLLMså¯ä»¥åœ¨ä¸è¿›è¡Œç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¹¿æ³›çš„è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š'
- en: Text generation
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ç”Ÿæˆ
- en: Translation
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¿»è¯‘
- en: Summarization
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Question answering
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é—®ç­”
- en: Code generation
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç ç”Ÿæˆ
- en: Logical reasoning
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€»è¾‘æ¨ç†
- en: '**Few-shot Learning**: They can often understand and perform new tasks with
    minimal examples or instructions.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å°‘æ ·æœ¬å­¦ä¹ **ï¼šå®ƒä»¬é€šå¸¸å¯ä»¥é€šè¿‡æœ€å°‘çš„ç¤ºä¾‹æˆ–æŒ‡ä»¤æ¥ç†è§£å’Œæ‰§è¡Œæ–°ä»»åŠ¡ã€‚'
- en: '**Resource-Intensive**: Due to their size, LLMs typically require significant
    computational resources to run, often needing powerful GPUs or TPUs.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**èµ„æºå¯†é›†å‹**ï¼šç”±äºå®ƒä»¬çš„è§„æ¨¡ï¼ŒLLMsé€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºæ¥è¿è¡Œï¼Œé€šå¸¸éœ€è¦å¼ºå¤§çš„GPUæˆ–TPUã€‚'
- en: '**Continual Development**: The field of LLMs is rapidly evolving, with new
    models and techniques constantly emerging.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æŒç»­å‘å±•**ï¼šLLMsé¢†åŸŸæ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œæ–°çš„æ¨¡å‹å’ŒæŠ€æœ¯ä¸æ–­æ¶Œç°ã€‚'
- en: '**Ethical Considerations**: The use of LLMs raises important questions about
    bias, misinformation, and the environmental impact of training such large models.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¼¦ç†è€ƒé‡**ï¼šLLMsçš„ä½¿ç”¨å¼•å‘äº†å…³äºåè§ã€é”™è¯¯ä¿¡æ¯å’Œè®­ç»ƒå¦‚æ­¤å¤§å‹æ¨¡å‹çš„ç¯å¢ƒå½±å“çš„é‡å¤§é—®é¢˜ã€‚'
- en: '**Applications**: LLMs are used in various fields, including content creation,
    customer service, research assistance, and software development.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åº”ç”¨é¢†åŸŸ**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬å†…å®¹åˆ›ä½œã€å®¢æˆ·æœåŠ¡ã€ç ”ç©¶è¾…åŠ©å’Œè½¯ä»¶å¼€å‘ã€‚'
- en: '**Limitations**: Despite their power, LLMs can produce incorrect or biased
    information and lack true understanding or reasoning capabilities.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å±€é™æ€§**ï¼šå°½ç®¡åŠŸèƒ½å¼ºå¤§ï¼ŒLLMsä»ç„¶å¯èƒ½äº§ç”Ÿé”™è¯¯æˆ–å¸¦æœ‰åè§çš„ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹çœŸæ­£çš„ç†è§£æˆ–æ¨ç†èƒ½åŠ›ã€‚'
- en: We must note that we use large models beyond text, calling them *multi-modal
    models*. These models integrate and process information from multiple types of
    input simultaneously. They are designed to understand and generate content across
    various forms of data, such as text, images, audio, and video.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿…é¡»æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è¶…è¶Šæ–‡æœ¬çš„å¤§å‹æ¨¡å‹ï¼Œæˆ‘ä»¬ç§°å®ƒä»¬ä¸º**å¤šæ¨¡æ€æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹èƒ½å¤ŸåŒæ—¶æ•´åˆå’Œå¤„ç†æ¥è‡ªå¤šç§ç±»å‹è¾“å…¥çš„ä¿¡æ¯ã€‚å®ƒä»¬è¢«è®¾è®¡æˆèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè·¨å„ç§æ•°æ®å½¢å¼çš„å†…å®¹ï¼Œä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚
- en: 'Closed vs Open Models:'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°é—­æ¨¡å‹ä¸å¼€æ”¾æ¨¡å‹ï¼š
- en: '**Closed models**, also called proprietary models, are AI models whose internal
    workings, code, and training data are not publicly disclosed. Examples: GPT-4
    (by OpenAI), Claude (by Anthropic), Gemini (by Google).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°é—­æ¨¡å‹**ï¼Œä¹Ÿç§°ä¸ºä¸“æœ‰æ¨¡å‹ï¼Œæ˜¯æŒ‡å…¶å†…éƒ¨å·¥ä½œåŸç†ã€ä»£ç å’Œè®­ç»ƒæ•°æ®ä¸å…¬å¼€çš„AIæ¨¡å‹ã€‚ä¾‹å¦‚ï¼šGPT-4ï¼ˆç”±OpenAIå¼€å‘ï¼‰ã€Claudeï¼ˆç”±Anthropicå¼€å‘ï¼‰ã€Geminiï¼ˆç”±Googleå¼€å‘ï¼‰ã€‚'
- en: '**Open models**, also known as open-source models, are AI models whose underlying
    code, architecture, and often training data are publicly available and accessible.
    Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼€æ”¾æ¨¡å‹**ï¼Œä¹Ÿç§°ä¸ºå¼€æºæ¨¡å‹ï¼Œæ˜¯æŒ‡å…¶åº•å±‚ä»£ç ã€æ¶æ„ä»¥åŠé€šå¸¸çš„è®­ç»ƒæ•°æ®éƒ½æ˜¯å…¬å¼€å¯ç”¨å’Œå¯è®¿é—®çš„AIæ¨¡å‹ã€‚ä¾‹å¦‚ï¼šGemmaï¼ˆç”±Googleå¼€å‘ï¼‰ã€LLaMAï¼ˆç”±Metaå¼€å‘ï¼‰å’ŒPhiï¼ˆç”±Microsoftå¼€å‘ï¼‰ã€‚'
- en: Open models are particularly relevant for running models on edge devices like
    Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained
    environments. Still, it is crucial to verify their Licenses. Open models come
    with various open-source licenses that may affect their use in commercial applications,
    while closed models have clear, albeit restrictive, terms of service.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€æ”¾æ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºåœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚Raspberry Piï¼‰ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬æ›´å®¹æ˜“é€‚åº”ã€ä¼˜åŒ–å’Œéƒ¨ç½²åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼ŒéªŒè¯å®ƒä»¬çš„è®¸å¯è¯è‡³å…³é‡è¦ã€‚å¼€æ”¾æ¨¡å‹é™„å¸¦å„ç§å¼€æºè®¸å¯è¯ï¼Œå¯èƒ½ä¼šå½±å“å®ƒä»¬åœ¨å•†ä¸šåº”ç”¨ä¸­çš„ä½¿ç”¨ï¼Œè€Œå°é—­æ¨¡å‹åˆ™å…·æœ‰æ˜ç¡®ã€å°½ç®¡æœ‰äº›é™åˆ¶çš„æœåŠ¡æ¡æ¬¾ã€‚
- en: '![](../media/file913.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file913.png)'
- en: Adapted from [arXiv](images/2304.13712)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®æ”¹è‡ª [arXiv](images/2304.13712)
- en: Small Language Models (SLMs)
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰
- en: In the context of edge computing on devices like Raspberry Pi, full-scale LLMs
    are typically too large and resource-intensive to run directly. This limitation
    has driven the development of smaller, more efficient models, such as the Small
    Language Models (SLMs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾¹ç¼˜è®¡ç®—è®¾å¤‡å¦‚æ ‘è“æ´¾ï¼ˆRaspberry Piï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œå…¨è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸å¤ªå¤§ä¸”èµ„æºå¯†é›†ï¼Œæ— æ³•ç›´æ¥è¿è¡Œã€‚è¿™ç§é™åˆ¶æ¨åŠ¨äº†æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹çš„å‘å±•ï¼Œä¾‹å¦‚å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚
- en: SLMs are compact versions of LLMs designed to run efficiently on resource-constrained
    devices such as smartphones, IoT devices, and single-board computers like the
    Raspberry Pi. These models are significantly smaller in size and computational
    requirements than their larger counterparts while still retaining impressive language
    understanding and generation capabilities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SLMsæ˜¯LLMsçš„ç´§å‡‘ç‰ˆæœ¬ï¼Œæ—¨åœ¨åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šï¼ˆå¦‚æ™ºèƒ½æ‰‹æœºã€ç‰©è”ç½‘è®¾å¤‡å’Œæ ‘è“æ´¾ç­‰å•æ¿è®¡ç®—æœºï¼‰é«˜æ•ˆè¿è¡Œã€‚è¿™äº›æ¨¡å‹åœ¨å°ºå¯¸å’Œè®¡ç®—éœ€æ±‚æ–¹é¢æ¯”å…¶å¤§å‹å¯¹åº”ç‰©å°å¾—å¤šï¼ŒåŒæ—¶ä»ç„¶ä¿ç•™äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚
- en: 'Key characteristics of SLMs include:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SLMsçš„å…³é”®ç‰¹æ€§åŒ…æ‹¬ï¼š
- en: '**Reduced parameter count**: Typically ranging from a few hundred million to
    a few billion parameters, compared to two-digit billions in larger models.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‚æ•°æ•°é‡å‡å°‘**ï¼šé€šå¸¸ä»å‡ äº¿åˆ°å‡ åäº¿å‚æ•°ä¸ç­‰ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹æ¨¡å‹æœ‰ä¸¤ä½æ•°çš„åäº¿å‚æ•°ã€‚'
- en: '**Lower memory footprint**: Requiring, at most, a few gigabytes of memory rather
    than tens or hundreds of gigabytes.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´ä½çš„å†…å­˜å ç”¨**ï¼šæœ€å¤šåªéœ€è¦å‡ GBçš„å†…å­˜ï¼Œè€Œä¸æ˜¯æ•°åæˆ–æ•°ç™¾GBã€‚'
- en: '**Faster inference time**: Can generate responses in milliseconds to seconds
    on edge devices.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´å¿«çš„æ¨ç†æ—¶é—´**ï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå¯ä»¥ä»¥æ¯«ç§’åˆ°ç§’çš„é€Ÿåº¦ç”Ÿæˆå“åº”ã€‚'
- en: '**Energy efficiency**: Consuming less power, making them suitable for battery-powered
    devices.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**èƒ½æºæ•ˆç‡**ï¼šæ¶ˆè€—æ›´å°‘çš„ç”µåŠ›ï¼Œä½¿å…¶é€‚åˆç”µæ± ä¾›ç”µçš„è®¾å¤‡ã€‚'
- en: '**Privacy-preserving**: Enabling on-device processing without sending data
    to cloud servers.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éšç§ä¿æŠ¤**ï¼šå…è®¸åœ¨è®¾å¤‡ä¸Šå¤„ç†æ•°æ®ï¼Œè€Œæ— éœ€å‘é€åˆ°äº‘ç«¯æœåŠ¡å™¨ã€‚'
- en: '**Offline functionality**: Operating without an internet connection.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¦»çº¿åŠŸèƒ½**ï¼šæ— éœ€äº’è”ç½‘è¿æ¥å³å¯è¿è¡Œã€‚'
- en: SLMs achieve their compact size through various techniques such as knowledge
    distillation, model pruning, and quantization. While they may not match the broad
    capabilities of larger models, SLMs excel in specific tasks and domains, making
    them ideal for targeted applications on edge devices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SLMsé€šè¿‡çŸ¥è¯†è’¸é¦ã€æ¨¡å‹å‰ªæå’Œé‡åŒ–ç­‰å„ç§æŠ€æœ¯å®ç°å…¶ç´§å‡‘çš„å°ºå¯¸ã€‚è™½ç„¶å®ƒä»¬å¯èƒ½æ— æ³•ä¸æ›´å¤§æ¨¡å‹çš„å¹¿æ³›èƒ½åŠ›ç›¸åŒ¹é…ï¼Œä½†SLMsåœ¨ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½¿å®ƒä»¬éå¸¸é€‚åˆè¾¹ç¼˜è®¾å¤‡ä¸Šçš„é’ˆå¯¹æ€§åº”ç”¨ã€‚
- en: We will generally consider SLMs, language models with less than 5 billion parameters
    quantized to 4 bits.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šå¸¸å°†SLMsè§†ä¸ºå‚æ•°é‡å°‘äº50äº¿ä¸”é‡åŒ–ä¸º4ä½çš„è¯­è¨€æ¨¡å‹ã€‚
- en: Examples of SLMs include compressed versions of models like Meta Llama, Microsoft
    PHI, and Google Gemma. These models enable a wide range of natural language processing
    tasks directly on edge devices, from text classification and sentiment analysis
    to question answering and limited text generation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SLMsçš„ä¾‹å­åŒ…æ‹¬Meta Llamaã€Microsoft PHIå’ŒGoogle Gemmaç­‰æ¨¡å‹çš„å‹ç¼©ç‰ˆæœ¬ã€‚è¿™äº›æ¨¡å‹ä½¿å¾—åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šç›´æ¥æ‰§è¡Œå¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä»æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æåˆ°é—®ç­”å’Œæœ‰é™çš„æ–‡æœ¬ç”Ÿæˆã€‚
- en: 'For more information on SLMs, the paper, [LLM Pruning and Distillation in Practice:
    The Minitron Approach](https://arxiv.org/pdf/2408.11796), provides an approach
    applying pruning and distillation to obtain SLMs from LLMs. And, [SMALL LANGUAGE
    MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS](https://arxiv.org/pdf/2409.15790),
    presents a comprehensive survey and analysis of Small Language Models (SLMs),
    which are language models with 100 million to 5 billion parameters designed for
    resource-constrained devices.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…³äºSLMsçš„æ›´å¤šä¿¡æ¯ï¼Œè®ºæ–‡ã€ŠLLM Pruning and Distillation in Practice: The Minitron Approachã€‹ï¼ˆ[https://arxiv.org/pdf/2408.11796](https://arxiv.org/pdf/2408.11796)ï¼‰æä¾›äº†ä¸€ç§å°†å‰ªæå’Œè’¸é¦åº”ç”¨äºä»LLMsè·å¾—SLMsçš„æ–¹æ³•ã€‚è€Œã€ŠSMALL
    LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTSã€‹ï¼ˆ[https://arxiv.org/pdf/2409.15790](https://arxiv.org/pdf/2409.15790)ï¼‰åˆ™å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥å’Œåˆ†æï¼Œè¿™äº›æ¨¡å‹å…·æœ‰1äº¿åˆ°50äº¿å‚æ•°ï¼Œæ—¨åœ¨ä¸ºèµ„æºå—é™çš„è®¾å¤‡è®¾è®¡ã€‚'
- en: Ollama
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama
- en: '![](../media/file914.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file914.png)'
- en: ollama logo
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ollama logo
- en: '[Ollama](https://ollama.com/) is an open-source framework that allows us to
    run language models (LMs), large or small, locally on our machines. Here are some
    critical points about Ollama:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama](https://ollama.com/) æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œå…è®¸æˆ‘ä»¬åœ¨è‡ªå·±çš„æœºå™¨ä¸Šæœ¬åœ°è¿è¡Œè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ï¼Œæ— è®ºæ˜¯å¤§å‹è¿˜æ˜¯å°å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºOllamaçš„ä¸€äº›å…³é”®ç‚¹ï¼š'
- en: '**Local Model Execution**: Ollama enables running LMs on personal computers
    or edge devices such as the Raspi-5, eliminating the need for cloud-based API
    calls.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ¬åœ°æ¨¡å‹æ‰§è¡Œ**ï¼šOllamaä½¿å¾—åœ¨ä¸ªäººç”µè„‘æˆ–è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚Raspi-5ï¼‰ä¸Šè¿è¡ŒLMsæˆä¸ºå¯èƒ½ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹åŸºäºäº‘çš„APIè°ƒç”¨çš„éœ€æ±‚ã€‚'
- en: '**Ease of Use**: It provides a simple command-line interface for downloading,
    running, and managing different language models.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ˜“äºä½¿ç”¨**ï¼šå®ƒæä¾›äº†ä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œç•Œé¢ï¼Œç”¨äºä¸‹è½½ã€è¿è¡Œå’Œç®¡ç†ä¸åŒçš„è¯­è¨€æ¨¡å‹ã€‚'
- en: '**Model Variety**: Ollama supports various LLMs, including Phi, Gemma, Llama,
    Mistral, and other open-source models.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å¤šæ ·æ€§**ï¼šOllamaæ”¯æŒå„ç§LLMï¼ŒåŒ…æ‹¬Phiã€Gemmaã€Llamaã€Mistralå’Œå…¶ä»–å¼€æºæ¨¡å‹ã€‚'
- en: '**Customization**: Users can create and share custom models tailored to specific
    needs or domains.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šåˆ¶**ï¼šç”¨æˆ·å¯ä»¥åˆ›å»ºå’Œåˆ†äº«å®šåˆ¶æ¨¡å‹ï¼Œä»¥æ»¡è¶³ç‰¹å®šéœ€æ±‚æˆ–é¢†åŸŸã€‚'
- en: '**Lightweight**: Designed to be efficient and run on consumer-grade hardware.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è½»é‡çº§**ï¼šè®¾è®¡ä¸ºé«˜æ•ˆä¸”èƒ½åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œã€‚'
- en: '**API Integration**: Offers an API that allows integration with other applications
    and services.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**APIé›†æˆ**ï¼šæä¾›APIï¼Œå…è®¸ä¸å…¶ä»–åº”ç”¨ç¨‹åºå’ŒæœåŠ¡é›†æˆã€‚'
- en: '**Privacy-Focused**: By running models locally, it addresses privacy concerns
    associated with sending data to external servers.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éšç§ä¼˜å…ˆ**ï¼šé€šè¿‡æœ¬åœ°è¿è¡Œæ¨¡å‹ï¼Œå®ƒè§£å†³äº†ä¸å‘å¤–éƒ¨æœåŠ¡å™¨å‘é€æ•°æ®ç›¸å…³çš„éšç§é—®é¢˜ã€‚'
- en: '**Cross-Platform**: Available for macOS, Windows, and Linux systems (our case,
    here).'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è·¨å¹³å°**ï¼šé€‚ç”¨äºmacOSã€Windowså’ŒLinuxç³»ç»Ÿï¼ˆæˆ‘ä»¬çš„æ¡ˆä¾‹ï¼Œåœ¨è¿™é‡Œï¼‰ã€‚'
- en: '**Active Development**: Regularly updated with new features and model support.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ´»è·ƒå¼€å‘**ï¼šå®šæœŸæ›´æ–°æ–°åŠŸèƒ½å’Œæ¨¡å‹æ”¯æŒã€‚'
- en: '**Community-Driven**: Benefits from community contributions and model sharing.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¤¾åŒºé©±åŠ¨**ï¼šå—ç›Šäºç¤¾åŒºè´¡çŒ®å’Œæ¨¡å‹å…±äº«ã€‚'
- en: 'To learn more about what Ollama is and how it works under the hood, you should
    see this short video from [Matt Williams](https://www.youtube.com/@technovangelist),
    one of the founders of Ollama:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šå…³äºOllamaæ˜¯ä»€ä¹ˆä»¥åŠå®ƒçš„å·¥ä½œåŸç†ï¼Œä½ åº”è¯¥çœ‹çœ‹æ¥è‡ª[Matt Williams](https://www.youtube.com/@technovangelist)ï¼ŒOllamaåˆ›å§‹äººä¹‹ä¸€çš„è¿™ä¸ªç®€çŸ­è§†é¢‘ï¼š
- en: '[https://www.youtube.com/embed/90ozfdsQOKo](https://www.youtube.com/embed/90ozfdsQOKo)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/90ozfdsQOKo](https://www.youtube.com/embed/90ozfdsQOKo)'
- en: 'Matt has an entirely free course about Ollama that we recommend: [https://youtu.be/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy](https://youtu.be/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy)'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Mattæœ‰ä¸€ä¸ªå…³äºOllamaçš„å®Œå…¨å…è´¹çš„è¯¾ç¨‹ï¼Œæˆ‘ä»¬æ¨èï¼š[https://youtu.be/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy](https://youtu.be/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy)
- en: Installing Ollama
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®‰è£…Ollama
- en: 'Letâ€™s set up and activate a Virtual Environment for working with Ollama:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¾ç½®å¹¶æ¿€æ´»ä¸€ä¸ªç”¨äºä¸Ollamaä¸€èµ·å·¥ä½œçš„è™šæ‹Ÿç¯å¢ƒï¼š
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And run the command to install Ollama:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…Ollamaï¼š
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As a result, an API will run in the background on `127.0.0.1:11434`. From now
    on, we can run Ollama via the terminal. For starting, letâ€™s verify the Ollama
    version, which will also tell us that it is correctly installed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒAPIå°†åœ¨åå°è¿è¡Œåœ¨`127.0.0.1:11434`ã€‚ä»ç°åœ¨èµ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»ˆç«¯è¿è¡ŒOllamaã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬éªŒè¯Ollamaçš„ç‰ˆæœ¬ï¼Œè¿™ä¹Ÿä¼šå‘Šè¯‰æˆ‘ä»¬å®ƒæ˜¯å¦æ­£ç¡®å®‰è£…ï¼š
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../media/file915.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file915.png)'
- en: On the [Ollama Library page](https://ollama.com/library), we can find the models
    Ollama supports. For example, by filtering by `Most popular`, we can see Meta
    Llama, Google Gemma, Microsoft Phi, LLaVa, etc.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Ollamaåº“é¡µé¢](https://ollama.com/library)ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°Ollamaæ”¯æŒçš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡æŒ‰`æœ€å—æ¬¢è¿`ç­›é€‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°Meta
    Llamaã€Google Gemmaã€Microsoft Phiã€LLaVaç­‰ã€‚
- en: Meta Llama 3.2 1B/3B
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Meta Llama 3.2 1B/3B
- en: '![](../media/file916.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file916.png)'
- en: Letâ€™s install and run our first small language model, [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    1B (and 3B). The Meta Llama 3.2 series comprises a set of multilingual generative
    language models available in 1 billion and 3 billion parameter sizes. These models
    are designed to process text input and generate text output. The instruction-tuned
    variants within this collection are specifically optimized for multilingual conversational
    applications, including tasks involving information retrieval and summarization
    with an agentic approach. When compared to many existing open-source and proprietary
    chat models, the Llama 3.2 instruction-tuned models demonstrate superior performance
    on widely-used industry benchmarks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®‰è£…å¹¶è¿è¡Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå°å‹è¯­è¨€æ¨¡å‹[Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    1Bï¼ˆå’Œ3Bï¼‰ã€‚Meta Llama 3.2ç³»åˆ—åŒ…æ‹¬ä¸€ç³»åˆ—å¯åœ¨1äº¿å’Œ30äº¿å‚æ•°å¤§å°ä¸­ä½¿ç”¨çš„å¤šè¯­è¨€ç”Ÿæˆè¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹æ—¨åœ¨å¤„ç†æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚æ­¤ç³»åˆ—ä¸­çš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ä¸“é—¨é’ˆå¯¹å¤šè¯­è¨€å¯¹è¯åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ¶‰åŠä¿¡æ¯æ£€ç´¢å’Œæ‘˜è¦çš„ä»£ç†æ–¹æ³•ã€‚ä¸è®¸å¤šç°æœ‰çš„å¼€æºå’Œä¸“æœ‰èŠå¤©æ¨¡å‹ç›¸æ¯”ï¼ŒLlama
    3.2æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨å¹¿æ³›ä½¿ç”¨çš„è¡Œä¸šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚
- en: The 1B and 3B models were pruned from the Llama 8B, and then logits from the
    8B and 70B models were used as token-level targets (token-level distillation).
    Knowledge distillation was used to recover performance (they were trained with
    9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the
    3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3
    GB and 2 GB, respectively. Its context window is 131,072 tokens.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 1B å’Œ 3B æ¨¡å‹æ˜¯ä» Llama 8B ä¸­å‰ªæå¾—åˆ°çš„ï¼Œç„¶åä½¿ç”¨ 8B å’Œ 70B æ¨¡å‹çš„ logits ä½œä¸ºæ ‡è®°çº§ç›®æ ‡ï¼ˆæ ‡è®°çº§è’¸é¦ï¼‰ã€‚ä½¿ç”¨çŸ¥è¯†è’¸é¦æ¥æ¢å¤æ€§èƒ½ï¼ˆå®ƒä»¬æ˜¯ç”¨
    9 ä¸‡äº¿ä¸ªæ ‡è®°è®­ç»ƒçš„ï¼‰ã€‚1B æ¨¡å‹æœ‰ 1,24B ä¸ªå‚æ•°ï¼Œé‡åŒ–ä¸ºæ•´æ•°ï¼ˆQ8_0ï¼‰ï¼Œ3B æ¨¡å‹æœ‰ 3.12B ä¸ªå‚æ•°ï¼Œä½¿ç”¨ Q4_0 é‡åŒ–ï¼Œæœ€ç»ˆå¤§å°åˆ†åˆ«ä¸º 1.3
    GB å’Œ 2 GBã€‚å…¶ä¸Šä¸‹æ–‡çª—å£ä¸º 131,072 ä¸ªæ ‡è®°ã€‚
- en: '![](../media/file917.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file917.jpg)'
- en: '**Install and run the** **Model**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®‰è£…å¹¶è¿è¡Œ** **æ¨¡å‹**'
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running the model with the command before, we should have the Ollama prompt
    available for us to input a question and start chatting with the LLM model; for
    example,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¹‹å‰çš„å‘½ä»¤è¿è¡Œæ¨¡å‹ï¼Œæˆ‘ä»¬åº”è¯¥æœ‰ Ollama æç¤ºå¯ç”¨ï¼Œä»¥ä¾¿æˆ‘ä»¬è¾“å…¥é—®é¢˜å¹¶å¼€å§‹ä¸ LLM æ¨¡å‹èŠå¤©ï¼›ä¾‹å¦‚ï¼Œ
- en: '`>>> What is the capital of France?`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`>>> æ³•å›½é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ`'
- en: 'Almost immediately, we get the correct answer:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹ç«‹å³ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼š
- en: '`The capital of France is Paris.`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`The capital of France is Paris.`'
- en: Using the option `--verbose` when calling the model will generate several statistics
    about its performance (The model will be polling only the first time we run the
    command).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä½¿ç”¨ `--verbose` é€‰é¡¹å°†ç”Ÿæˆå…³äºå…¶æ€§èƒ½çš„å‡ ä¸ªç»Ÿè®¡æ•°æ®ï¼ˆæ¨¡å‹å°†åœ¨æˆ‘ä»¬ç¬¬ä¸€æ¬¡è¿è¡Œå‘½ä»¤æ—¶è¿›è¡Œè½®è¯¢ï¼‰ã€‚
- en: '![](../media/file918.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file918.png)'
- en: 'Each metric gives insights into how the model processes inputs and generates
    outputs. Hereâ€™s a breakdown of what each metric means:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæŒ‡æ ‡éƒ½èƒ½æ­ç¤ºæ¨¡å‹å¤„ç†è¾“å…¥å’Œç”Ÿæˆè¾“å‡ºçš„æ–¹å¼ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªæŒ‡æ ‡çš„å«ä¹‰åˆ†è§£ï¼š
- en: '**Total Duration (2.620170326 s)**: This is the complete time taken from the
    start of the command to the completion of the response. It encompasses loading
    the model, processing the input prompt, and generating the response.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€»æŒç»­æ—¶é—´ï¼ˆ2.620170326 ç§’ï¼‰**ï¼šè¿™æ˜¯ä»å‘½ä»¤å¼€å§‹åˆ°å“åº”å®Œæˆæ‰€èŠ±è´¹çš„å®Œæ•´æ—¶é—´ã€‚å®ƒåŒ…æ‹¬åŠ è½½æ¨¡å‹ã€å¤„ç†è¾“å…¥æç¤ºå’Œç”Ÿæˆå“åº”ã€‚'
- en: '**Load Duration (39.947908 ms)**: This duration indicates the time to load
    the model or necessary components into memory. If this value is minimal, it can
    suggest that the model was preloaded or that only a minimal setup was required.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ è½½æŒç»­æ—¶é—´ï¼ˆ39.947908 æ¯«ç§’ï¼‰**ï¼šè¿™ä¸ªæŒç»­æ—¶é—´è¡¨ç¤ºå°†æ¨¡å‹æˆ–å¿…è¦ç»„ä»¶åŠ è½½åˆ°å†…å­˜ä¸­çš„æ—¶é—´ã€‚å¦‚æœè¿™ä¸ªå€¼å¾ˆå°ï¼Œå¯ä»¥è¡¨æ˜æ¨¡å‹æ˜¯é¢„å…ˆåŠ è½½çš„ï¼Œæˆ–è€…åªéœ€è¦æœ€å°çš„è®¾ç½®ã€‚'
- en: '**Prompt Eval Count (32 tokens)**: The number of tokens in the input prompt.
    In NLP, tokens are typically words or subwords, so this count includes all the
    tokens that the model evaluated to understand and respond to the query.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤ºè¯„ä¼°è®¡æ•°ï¼ˆ32 ä¸ªæ ‡è®°ï¼‰**ï¼šè¾“å…¥æç¤ºä¸­çš„æ ‡è®°æ•°é‡ã€‚åœ¨ NLP ä¸­ï¼Œæ ‡è®°é€šå¸¸æ˜¯å•è¯æˆ–å­è¯ï¼Œå› æ­¤è¿™ä¸ªè®¡æ•°åŒ…æ‹¬æ¨¡å‹è¯„ä¼°ä»¥ç†è§£å’Œå“åº”æŸ¥è¯¢çš„æ‰€æœ‰æ ‡è®°ã€‚'
- en: '**Prompt Eval Duration (1.644773 s)**: This measures the modelâ€™s time to evaluate
    or process the input prompt. It accounts for the bulk of the total duration, implying
    that understanding the query and preparing a response is the most time-consuming
    part of the process.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤ºè¯„ä¼°æŒç»­æ—¶é—´ï¼ˆ1.644773 ç§’ï¼‰**ï¼šè¿™è¡¡é‡æ¨¡å‹è¯„ä¼°æˆ–å¤„ç†è¾“å…¥æç¤ºçš„æ—¶é—´ã€‚å®ƒå ç”¨äº†æ€»æŒç»­æ—¶é—´çš„å¤§éƒ¨åˆ†ï¼Œè¿™æ„å‘³ç€ç†è§£æŸ¥è¯¢å’Œå‡†å¤‡å“åº”æ˜¯æ•´ä¸ªè¿‡ç¨‹ä¸­æœ€è€—æ—¶çš„éƒ¨åˆ†ã€‚'
- en: '**Prompt Eval Rate (19.46 tokens/s)**: This rate indicates how quickly the
    model processes tokens from the input prompt. It reflects the modelâ€™s speed in
    terms of natural language comprehension.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤ºè¯„ä¼°é€Ÿç‡ï¼ˆ19.46 ä¸ªæ ‡è®°/ç§’ï¼‰**ï¼šè¿™ä¸ªé€Ÿç‡è¡¨æ˜æ¨¡å‹å¤„ç†è¾“å…¥æç¤ºä¸­çš„æ ‡è®°æœ‰å¤šå¿«ã€‚å®ƒåæ˜ äº†æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢çš„é€Ÿåº¦ã€‚'
- en: '**Eval Count (8 token(s))**: This is the number of tokens in the modelâ€™s response,
    which in this case was, â€œThe capital of France is Paris.â€'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°è®¡æ•°ï¼ˆ8 ä¸ªæ ‡è®°ï¼‰**ï¼šè¿™æ˜¯æ¨¡å‹å“åº”ä¸­çš„æ ‡è®°æ•°é‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ï¼Œâ€œæ³•å›½çš„é¦–éƒ½æ˜¯å·´é»ã€‚â€'
- en: '**Eval Duration (889.941 ms)**: This is the time taken to generate the output
    based on the evaluated input. Itâ€™s much shorter than the prompt evaluation, suggesting
    that generating the response is less complex or computationally intensive than
    understanding the prompt.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°æŒç»­æ—¶é—´ï¼ˆ889.941 æ¯«ç§’ï¼‰**ï¼šè¿™æ˜¯æ ¹æ®è¯„ä¼°è¾“å…¥ç”Ÿæˆè¾“å‡ºæ‰€éœ€çš„æ—¶é—´ã€‚å®ƒæ¯”æç¤ºè¯„ä¼°è¦çŸ­å¾—å¤šï¼Œè¿™è¡¨æ˜ç”Ÿæˆå“åº”æ¯”ç†è§£æç¤ºæ›´ç®€å•æˆ–è®¡ç®—å¯†é›†åº¦æ›´ä½ã€‚'
- en: '**Eval Rate (8.99 tokens/s)**: Similar to the prompt eval rate, this indicates
    the speed at which the model generates output tokens. Itâ€™s a crucial metric for
    understanding the modelâ€™s efficiency in output generation.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¯„ä¼°é€Ÿç‡ï¼ˆ8.99 ä¸ªæ ‡è®°/ç§’ï¼‰**ï¼šç±»ä¼¼äºæç¤ºè¯„ä¼°é€Ÿç‡ï¼Œè¿™è¡¨æ˜æ¨¡å‹ç”Ÿæˆè¾“å‡ºæ ‡è®°çš„é€Ÿåº¦ã€‚è¿™æ˜¯ç†è§£æ¨¡å‹åœ¨è¾“å‡ºç”Ÿæˆæ•ˆç‡æ–¹é¢çš„ä¸€ä¸ªå…³é”®æŒ‡æ ‡ã€‚'
- en: This detailed breakdown can help understand the computational demands and performance
    characteristics of running SLMs like Llama on edge devices like the Raspberry
    Pi 5\. It shows that while prompt evaluation is more time-consuming, the actual
    generation of responses is relatively quicker. This analysis is crucial for optimizing
    performance and diagnosing potential bottlenecks in real-time applications.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¯¦ç»†çš„åˆ†è§£å¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£åœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚ Raspberry Pi 5ï¼‰ä¸Šè¿è¡Œ SLMsï¼ˆå¦‚ Llamaï¼‰çš„è®¡ç®—éœ€æ±‚å’Œæ€§èƒ½ç‰¹å¾ã€‚è¿™è¡¨æ˜ï¼Œè™½ç„¶æç¤ºè¯„ä¼°è€—æ—¶æ›´é•¿ï¼Œä½†å®é™…ç”Ÿæˆå“åº”ç›¸å¯¹è¾ƒå¿«ã€‚è¿™ç§åˆ†æå¯¹äºä¼˜åŒ–æ€§èƒ½å’Œè¯Šæ–­å®æ—¶åº”ç”¨ä¸­çš„æ½œåœ¨ç“¶é¢ˆè‡³å…³é‡è¦ã€‚
- en: Loading and running the 3B model, we can see the difference in performance for
    the same prompt;
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å¹¶è¿è¡Œ 3B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç›¸åŒæç¤ºä¸‹çš„æ€§èƒ½å·®å¼‚ï¼›
- en: '![](../media/file919.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file919.png)'
- en: The eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç‡è¾ƒä½ï¼Œ5.3 ä¸ª token/s ä¸è¾ƒå°æ¨¡å‹ç›¸æ¯”ä¸º 9 ä¸ª token/sã€‚
- en: When question about
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æå‡ºé—®é¢˜æ—¶
- en: '`>>> What is the distance between Paris and Santiago, Chile?`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`>>> å·´é»å’Œæ™ºåˆ©åœ£åœ°äºšå“¥ä¹‹é—´çš„è·ç¦»æ˜¯å¤šå°‘ï¼Ÿ`'
- en: The 1B model answered `9,841 kilometers (6,093 miles)`, which is inaccurate,
    and the 3B model answered `7,300 miles (11,700 km)`, which is close to the correct
    (11,642 km).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 1B æ¨¡å‹å›ç­”äº† `9,841 å…¬é‡Œï¼ˆ6,093 è‹±é‡Œï¼‰`ï¼Œè¿™æ˜¯ä¸å‡†ç¡®çš„ï¼Œè€Œ 3B æ¨¡å‹å›ç­”äº† `7,300 è‹±é‡Œï¼ˆ11,700 å…¬é‡Œï¼‰`ï¼Œæ¥è¿‘æ­£ç¡®ç­”æ¡ˆï¼ˆ11,642
    å…¬é‡Œï¼‰ã€‚
- en: 'Letâ€™s ask for the Parisâ€™s coordinates:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯¢é—®å·´é»çš„åæ ‡ï¼š
- en: '`>>> what is the latitude and longitude of Paris?`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`>>> å·´é»çš„ç»çº¬åº¦æ˜¯å¤šå°‘ï¼Ÿ`'
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../media/file920.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file920.png)'
- en: Both 1B and 3B models gave correct answers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 1B å’Œ 3B æ¨¡å‹éƒ½ç»™å‡ºäº†æ­£ç¡®ç­”æ¡ˆã€‚
- en: Google Gemma 2 2B
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Gemma 2 2B
- en: 'Letâ€™s install [Gemma 2](https://ollama.com/library/gemma2:2b), a high-performing
    and efficient model available in three sizes: 2B, 9B, and 27B. We will install
    [**Gemma 2 2B**](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f),
    a lightweight model trained with 2 trillion tokens that produces outsized results
    by learning from larger models through distillation. The model has 2.6 billion
    parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context
    window is 8,192 tokens.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®‰è£… [Gemma 2](https://ollama.com/library/gemma2:2b)ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ä¸”é«˜æ•ˆçš„æ¨¡å‹ï¼Œæœ‰ä¸‰ç§å°ºå¯¸ï¼š2Bã€9B
    å’Œ 27Bã€‚æˆ‘ä»¬å°†å®‰è£… [**Gemma 2 2B**](https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡
    2 ä¸‡äº¿ token è®­ç»ƒçš„è½»é‡çº§æ¨¡å‹ï¼Œé€šè¿‡è’¸é¦ä»æ›´å¤§æ¨¡å‹ä¸­å­¦ä¹ ï¼Œäº§ç”Ÿäº†è¶…å‡ºé¢„æœŸçš„ç»“æœã€‚è¯¥æ¨¡å‹æœ‰ 26 äº¿ä¸ªå‚æ•°å’Œä¸€ä¸ª Q4_0 é‡åŒ–ï¼Œæœ€ç»ˆå¤§å°ä¸º 1.6
    GBã€‚å…¶ä¸Šä¸‹æ–‡çª—å£ä¸º 8,192 ä¸ª tokenã€‚
- en: '![](../media/file921.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file921.png)'
- en: '**Install and run the** **Model**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®‰è£…å’Œè¿è¡Œ** **æ¨¡å‹**'
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Running the model with the command before, we should have the Ollama prompt
    available for us to input a question and start chatting with the LLM model; for
    example,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¹‹å‰çš„å‘½ä»¤è¿è¡Œæ¨¡å‹ï¼Œæˆ‘ä»¬åº”è¯¥æœ‰ Ollama æç¤ºå¯ç”¨ï¼Œä»¥ä¾¿æˆ‘ä»¬è¾“å…¥é—®é¢˜å¹¶å¼€å§‹ä¸ LLM æ¨¡å‹èŠå¤©ï¼›ä¾‹å¦‚ï¼Œ
- en: '`>>> What is the capital of France?`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`>>> æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ`'
- en: 'Almost immediately, we get the correct answer:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹ç«‹å³ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼š
- en: '`The capital of France is **Paris**. ğŸ—¼`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`æ³•å›½çš„é¦–éƒ½æ˜¯**å·´é»**ã€‚ ğŸ—¼`'
- en: And itâ€™ statistics.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠå®ƒçš„ç»Ÿè®¡æ•°æ®ã€‚
- en: '![](../media/file922.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file922.png)'
- en: We can see that Gemma 2:2B has around the same performance as Llama 3.2:3B,
    but having less parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ° Gemma 2:2B çš„æ€§èƒ½ä¸ Llama 3.2:3B å¤§è‡´ç›¸åŒï¼Œä½†å‚æ•°æ›´å°‘ã€‚
- en: '**Other examples**:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¶ä»–ç¤ºä¾‹**:'
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Also, a good response but less accurate than Llama3.2:3B.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å›ç­”å¾ˆå¥½ï¼Œä½†ä¸å¦‚ Llama3.2:3B å‡†ç¡®ã€‚
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A good and accurate answer (a little more verbose than the Llama answers).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥½ä¸”å‡†ç¡®çš„ç­”æ¡ˆï¼ˆæ¯” Llama çš„ç­”æ¡ˆç¨å¾®å†—é•¿ä¸€äº›ï¼‰ã€‚
- en: Microsoft Phi3.5 3.8B
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¾®è½¯ Phi3.5 3.8B
- en: 'Letâ€™s pull a bigger (but still tiny) model, the [PHI3.5,](https://ollama.com/library/phi3.5)
    a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs
    to the Phi-3 model family and supports `128K token` context length and the languages:
    Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew,
    Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian,
    Spanish, Swedish, Thai, Turkish and Ukrainian.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‹‰å–ä¸€ä¸ªæ›´å¤§çš„ï¼ˆä½†ä»ç„¶å¾ˆå°ï¼‰æ¨¡å‹ï¼Œ[PHI3.5](https://ollama.com/library/phi3.5)ï¼Œè¿™æ˜¯å¾®è½¯çš„ä¸€ä¸ª 3.8B
    è½»é‡çº§æœ€å…ˆè¿›å¼€æºæ¨¡å‹ã€‚è¯¥æ¨¡å‹å±äº Phi-3 æ¨¡å‹ç³»åˆ—ï¼Œæ”¯æŒ `128K token` çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥åŠä»¥ä¸‹è¯­è¨€ï¼šé˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€æ·å…‹è¯­ã€ä¸¹éº¦è¯­ã€è·å…°è¯­ã€è‹±è¯­ã€èŠ¬å…°è¯­ã€æ³•è¯­ã€å¾·è¯­ã€å¸Œä¼¯æ¥è¯­ã€åŒˆç‰™åˆ©è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€æŒªå¨è¯­ã€æ³¢å…°è¯­ã€è‘¡è„ç‰™è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€ç‘å…¸è¯­ã€æ³°è¯­ã€åœŸè€³å…¶è¯­å’Œä¹Œå…‹å…°è¯­ã€‚
- en: The model size, in terms of bytes, will depend on the specific quantization
    format used. The size can go from 2-bit quantization (`q2_k`) of 1.4 GB (higher
    performance/lower quality) to 16-bit quantization (fp-16) of 7.6 GB (lower performance/higher
    quality).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰å°†å–å†³äºæ‰€ä½¿ç”¨çš„ç‰¹å®šé‡åŒ–æ ¼å¼ã€‚å¤§å°å¯ä»¥ä» 2 ä½é‡åŒ–ï¼ˆ`q2_k`ï¼‰çš„ 1.4 GBï¼ˆé«˜æ€§èƒ½/ä½è´¨é‡ï¼‰åˆ° 16 ä½é‡åŒ–ï¼ˆfp-16ï¼‰çš„
    7.6 GBï¼ˆä½æ€§èƒ½/é«˜è´¨é‡ï¼‰ä¸ç­‰ã€‚
- en: Letâ€™s run the 4-bit quantization (`Q4_0`), which will need 2.2 GB of RAM, with
    an intermediary trade-off regarding output quality and performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿è¡Œ4ä½é‡åŒ–ï¼ˆ`Q4_0`ï¼‰ï¼Œè¿™å°†éœ€è¦2.2GBçš„RAMï¼Œåœ¨è¾“å‡ºè´¨é‡å’Œæ€§èƒ½ä¹‹é—´è¿›è¡Œä¸­é—´æƒè¡¡ã€‚
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can use `run` or `pull` to download the model. What happens is that Ollama
    keeps note of the pulled models, and once the PHI3 does not exist, before running
    it, Ollama pulls it.
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`run`æˆ–`pull`æ¥ä¸‹è½½æ¨¡å‹ã€‚å‘ç”Ÿçš„æƒ…å†µæ˜¯Ollamaä¼šè®°å½•æ‹‰å–çš„æ¨¡å‹ï¼Œä¸€æ—¦PHI3ä¸å­˜åœ¨ï¼Œåœ¨è¿è¡Œä¹‹å‰ï¼ŒOllamaä¼šæ‹‰å–å®ƒã€‚
- en: 'Letâ€™s enter with the same prompt used before:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰ä½¿ç”¨çš„ç›¸åŒæç¤ºè¿›å…¥ï¼š
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The answer was very â€œverboseâ€, letâ€™s specify a better prompt:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆéå¸¸â€œå†—é•¿â€ï¼Œè®©æˆ‘ä»¬æŒ‡å®šä¸€ä¸ªæ›´å¥½çš„æç¤ºï¼š
- en: '![](../media/file923.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file923.png)'
- en: In this case, the answer was still longer than we expected, with an eval rate
    of 2.25 tokens/s, more than double that of Gemma and Llama.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç­”æ¡ˆä»ç„¶æ¯”æˆ‘ä»¬é¢„æœŸçš„è¦é•¿ï¼Œè¯„ä¼°é€Ÿç‡ä¸º2.25ä¸ªtoken/ç§’ï¼Œæ˜¯Gemmaå’ŒLlamaçš„ä¸¤å€å¤šã€‚
- en: Choosing the most appropriate prompt is one of the most important skills to
    be used with LLMs, no matter its size.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€‰æ‹©æœ€åˆé€‚çš„æç¤ºæ˜¯ä¸LLMä¸€èµ·ä½¿ç”¨çš„é‡è¦æŠ€èƒ½ä¹‹ä¸€ï¼Œæ— è®ºå…¶å¤§å°å¦‚ä½•ã€‚
- en: When we asked the same questions about distance and Latitude/Longitude, we did
    not get a good answer for a distance of `13,507 kilometers (8,429 miles)`, but
    it was OK for coordinates. Again, it could have been less verbose (more than 200
    tokens for each answer).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è¯¢é—®å…³äºè·ç¦»å’Œç»çº¬åº¦çš„ç›¸åŒé—®é¢˜æ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰å¾—åˆ°å…³äº`13,507å…¬é‡Œï¼ˆ8,429è‹±é‡Œï¼‰`è·ç¦»çš„è‰¯å¥½ç­”æ¡ˆï¼Œä½†å¯¹äºåæ ‡æ¥è¯´è¿˜å¯ä»¥ã€‚å†æ¬¡å¼ºè°ƒï¼Œå®ƒå¯èƒ½æ›´åŠ ç®€æ´ï¼ˆæ¯ä¸ªç­”æ¡ˆè¶…è¿‡200ä¸ªtokenï¼‰ã€‚
- en: We can use any model as an assistant since their speed is relatively decent,
    but on September 24 (2023), the Llama2:3B is a better choice. You should try other
    models, depending on your needs. [ğŸ¤— Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    can give you an idea about the best models in size, benchmark, license, etc.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒä»¬çš„é€Ÿåº¦ç›¸å¯¹é€‚ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æ¨¡å‹ä½œä¸ºåŠ©æ‰‹ï¼Œä½†æˆªè‡³2023å¹´9æœˆ24æ—¥ï¼ŒLlama2:3Bæ˜¯ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©ã€‚æ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„éœ€æ±‚å°è¯•å…¶ä»–æ¨¡å‹ã€‚[ğŸ¤—
    Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)å¯ä»¥ç»™æ‚¨ä¸€ä¸ªå…³äºæœ€ä½³æ¨¡å‹åœ¨å¤§å°ã€åŸºå‡†ã€è®¸å¯ç­‰æ–¹é¢çš„æƒ³æ³•ã€‚
- en: The best model to use is the one fit for your specific necessity. Also, take
    into consideration that this field evolves with new models everyday.
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ€å¥½çš„æ¨¡å‹æ˜¯é€‚åˆæ‚¨ç‰¹å®šéœ€æ±‚çš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°è¿™ä¸ªé¢†åŸŸæ¯å¤©éƒ½åœ¨éšç€æ–°æ¨¡å‹çš„å‘å±•è€Œæ¼”å˜ã€‚
- en: Multimodal Models
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€æ¨¡å‹
- en: Multimodal models are artificial intelligence (AI) systems that can process
    and understand information from multiple sources, such as images, text, audio,
    and video. In our context, multimodal LLMs can process various inputs, including
    text, images, and audio, as prompts and convert those prompts into various outputs,
    not just the source type.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€æ¨¡å‹æ˜¯èƒ½å¤Ÿå¤„ç†å’Œç†è§£æ¥è‡ªå¤šä¸ªæ¥æºçš„ä¿¡æ¯çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿï¼Œå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚åœ¨æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå¤šæ¨¡æ€LLMå¯ä»¥å¤„ç†å„ç§è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ï¼Œå¹¶å°†è¿™äº›æç¤ºè½¬æ¢ä¸ºå„ç§è¾“å‡ºï¼Œè€Œä¸ä»…ä»…æ˜¯æºç±»å‹ã€‚
- en: We will work here with [LLaVA-Phi-3](https://ollama.com/library/llava-phi3:3.8b),
    a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks
    that are on par with the original [LLaVA](https://llava-vl.github.io/) (Large
    Language and Vision Assistant) model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨[LLaVA-Phi-3](https://ollama.com/library/llava-phi3:3.8b)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¥è‡ªPhi
    3 Mini 4kçš„å¾®è°ƒè¿‡çš„LLaVAæ¨¡å‹ã€‚å®ƒå…·æœ‰ä¸åŸå§‹[LLaVA](https://llava-vl.github.io/)ï¼ˆå¤§å‹è¯­è¨€å’Œè§†è§‰åŠ©æ‰‹ï¼‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½åŸºå‡†ã€‚
- en: The LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to
    understand and generate content based on visual inputs (images) and textual instructions.
    It combines the capabilities of a visual encoder and a language model to process
    and respond to multimodal inputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: LLaVA-Phi-3æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ ¹æ®è§†è§‰è¾“å…¥ï¼ˆå›¾åƒï¼‰å’Œæ–‡æœ¬æŒ‡ä»¤ç†è§£å’Œç”Ÿæˆå†…å®¹ã€‚å®ƒç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥å¤„ç†å’Œå“åº”ç”¨æˆ·çš„å¤šæ¨¡æ€è¾“å…¥ã€‚
- en: 'Letâ€™s install the model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®‰è£…æ¨¡å‹ï¼š
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Letâ€™s start with a text input:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»æ–‡æœ¬è¾“å…¥å¼€å§‹ï¼š
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The response took around 30 s, with an eval rate of 3.93 tokens/s! Not bad!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å“åº”å¤§çº¦éœ€è¦30ç§’ï¼Œè¯„ä¼°é€Ÿç‡ä¸º3.93ä¸ªtoken/ç§’ï¼è¿˜ä¸é”™ï¼
- en: 'But let us know to enter with an image as input. For that, letâ€™s create a directory
    for working:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è®©æˆ‘ä»¬çŸ¥é“ä»¥å›¾åƒä½œä¸ºè¾“å…¥è¿›å…¥ã€‚ä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå·¥ä½œç›®å½•ï¼š
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Letâ€™s download a <semantics><mrow><mn>640</mn><mo>Ã—</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">640\times 320</annotation></semantics> image from
    the internet, for example (Wikipedia: [Paris, France)](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/640px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»äº’è”ç½‘ä¸‹è½½ä¸€å¼  <semantics><mrow><mn>640</mn><mo>Ã—</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">640\times 320</annotation></semantics> å›¾ç‰‡ï¼Œä¾‹å¦‚ï¼ˆç»´åŸºç™¾ç§‘ï¼š[æ³•å›½å·´é»](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/640px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg)ï¼‰ï¼š
- en: '![](../media/file924.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file924.jpg)'
- en: Using FileZilla, for example, letâ€™s upload the image to the OLLAMA folder at
    the Raspi-5 and name it `image_test_1.jpg`. We should have the whole image path
    (we can use `pwd` to get it).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä½¿ç”¨ FileZilla å°†å›¾ç‰‡ä¸Šä¼ åˆ° Raspi-5 çš„ OLLAMA æ–‡ä»¶å¤¹ï¼Œå¹¶å‘½åä¸º `image_test_1.jpg`ã€‚æˆ‘ä»¬åº”è¯¥æœ‰å®Œæ•´çš„å›¾ç‰‡è·¯å¾„ï¼ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨
    `pwd` æ¥è·å–å®ƒï¼‰ã€‚
- en: '`/home/mjrovai/Documents/OLLAMA/image_test_1.jpg`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`/home/mjrovai/Documents/OLLAMA/image_test_1.jpg`'
- en: If you use a desktop, you can copy the image path by clicking the image with
    the mouseâ€™s right button.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ¡Œé¢ï¼Œå¯ä»¥é€šè¿‡å³é”®ç‚¹å‡»å›¾ç‰‡æ¥å¤åˆ¶å›¾ç‰‡è·¯å¾„ã€‚
- en: '![](../media/file925.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file925.png)'
- en: 'Letâ€™s enter with this prompt:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»¥ä¸‹æç¤ºè¿›å…¥ï¼š
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result was great, but the overall latency was significant; almost 4 minutes
    to perform the inference.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå¾ˆæ£’ï¼Œä½†æ•´ä½“å»¶è¿Ÿå¾ˆå¤§ï¼›æ¨ç†å‡ ä¹éœ€è¦4åˆ†é’Ÿã€‚
- en: '![](../media/file926.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file926.png)'
- en: Inspecting local resources
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æœ¬åœ°èµ„æº
- en: Using htop, we can monitor the resources running on our device.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ htopï¼Œæˆ‘ä»¬å¯ä»¥ç›‘æ§è®¾å¤‡ä¸Šè¿è¡Œçš„èµ„æºã€‚
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'During the time that the model is running, we can inspect the resources:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹è¿è¡ŒæœŸé—´ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥èµ„æºï¼š
- en: '![](../media/file927.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file927.png)'
- en: All four CPUs run at almost 100% of their capacity, and the memory used with
    the model loaded is `3.24 GB`. Exiting Ollama, the memory goes down to around
    `377 MB` (with no desktop).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å››ä¸ª CPU éƒ½å‡ ä¹ä»¥ 100% çš„å®¹é‡è¿è¡Œï¼ŒåŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨çš„å†…å­˜ä¸º `3.24 GB`ã€‚é€€å‡º Ollama åï¼Œå†…å­˜ä¸‹é™åˆ°å¤§çº¦ `377 MB`ï¼ˆæ²¡æœ‰æ¡Œé¢ï¼‰ã€‚
- en: 'It is also essential to monitor the temperature. When running the Raspberry
    with a desktop, you can have the temperature shown on the taskbar:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç›‘æ§æ¸©åº¦ä¹Ÿéå¸¸é‡è¦ã€‚å½“ä½¿ç”¨æ¡Œé¢è¿è¡Œ Raspberry æ—¶ï¼Œä½ å¯ä»¥åœ¨ä»»åŠ¡æ ä¸Šçœ‹åˆ°æ¸©åº¦ï¼š
- en: '![](../media/file928.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file928.png)'
- en: 'If you are â€œheadlessâ€, the temperature can be monitored with the command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¤„äºâ€œæ— å¤´â€çŠ¶æ€ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›‘æ§æ¸©åº¦ï¼š
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you are doing nothing, the temperature is around `50Â°C` for CPUs running
    at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost
    `70Â°C`. This is OK and means the active cooler is working, keeping the temperature
    below 80Â°C / 85Â°C (its limit).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»€ä¹ˆä¹Ÿä¸åšï¼ŒCPU åœ¨ 1% è¿è¡Œæ—¶ï¼Œæ¸©åº¦å¤§çº¦ä¸º `50Â°C`ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå½“ CPU è¾¾åˆ° 100% æ—¶ï¼Œæ¸©åº¦å¯ä»¥ä¸Šå‡åˆ°å‡ ä¹ `70Â°C`ã€‚è¿™æ˜¯æ­£å¸¸çš„ï¼Œæ„å‘³ç€æ´»åŠ¨æ•£çƒ­å™¨æ­£åœ¨å·¥ä½œï¼Œå°†æ¸©åº¦ä¿æŒåœ¨
    80Â°C / 85Â°Cï¼ˆå…¶é™åˆ¶ï¼‰ä»¥ä¸‹ã€‚
- en: Ollama Python Library
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ollama Python åº“
- en: So far, we have explored SLMsâ€™ chat capability using the command line on a terminal.
    However, we want to integrate those models into our projects, so Python seems
    to be the right path. The good news is that Ollama has such a library.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨ç»ˆç«¯ä¸Šçš„å‘½ä»¤æ¢ç´¢äº† SLMs çš„èŠå¤©åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¸Œæœ›å°†è¿™äº›æ¨¡å‹é›†æˆåˆ°æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼Œå› æ­¤ Python ä¼¼ä¹æ˜¯ä¸€æ¡æ­£ç¡®çš„è·¯å¾„ã€‚å¥½æ¶ˆæ¯æ˜¯
    Ollama æœ‰è¿™æ ·çš„åº“ã€‚
- en: The [Ollama Python library](https://github.com/ollama/ollama-python) simplifies
    interaction with advanced LLM models, enabling more sophisticated responses and
    capabilities, besides providing the easiest way to integrate Python 3.8+ projects
    with [Ollama.](https://github.com/ollama/ollama)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ollama Python åº“](https://github.com/ollama/ollama-python) ç®€åŒ–äº†ä¸é«˜çº§ LLM æ¨¡å‹çš„äº¤äº’ï¼Œé™¤äº†æä¾›å°†
    Python 3.8+ é¡¹ç›®ä¸ [Ollama](https://github.com/ollama/ollama) é›†æˆçš„æœ€ç®€å•æ–¹å¼å¤–ï¼Œè¿˜ä½¿æ›´å¤æ‚çš„å“åº”å’ŒåŠŸèƒ½æˆä¸ºå¯èƒ½ã€‚'
- en: 'For a better understanding of how to create apps using Ollama with Python,
    we can follow [Matt Williamsâ€™s videos](https://www.youtube.com/@technovangelist),
    as the one below:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨ Python å’Œ Ollama åˆ›å»ºåº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ [Matt Williams çš„è§†é¢‘](https://www.youtube.com/@technovangelist)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[https://www.youtube.com/embed/_4K20tOsXK8](https://www.youtube.com/embed/_4K20tOsXK8)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/_4K20tOsXK8](https://www.youtube.com/embed/_4K20tOsXK8)'
- en: '**Installation**:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®‰è£…**ï¼š'
- en: 'In the terminal, run the command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will need a text editor or an IDE to create a Python script. If you run the
    Raspberry OS on a desktop, several options, such as Thonny and Geany, have already
    been installed by default (accessed by `[Menu][Programming]`). You can download
    other IDEs, such as Visual Studio Code, from `[Menu][Recommended Software]`. When
    the window pops up, go to `[Programming]`, select the option of your choice, and
    press `[Apply]`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘å™¨æˆ–IDEæ¥åˆ›å»ºPythonè„šæœ¬ã€‚å¦‚æœä½ åœ¨æ¡Œé¢ä¸Šè¿è¡ŒRaspberry OSï¼ŒThonnyå’ŒGeanyç­‰å‡ ä¸ªé€‰é¡¹å·²ç»é»˜è®¤å®‰è£…ï¼ˆé€šè¿‡
    `[Menu][Programming]` è®¿é—®ï¼‰ã€‚ä½ å¯ä»¥ä» `[Menu][Recommended Software]` ä¸‹è½½å…¶ä»–IDEï¼Œä¾‹å¦‚Visual
    Studio Codeã€‚å½“çª—å£å¼¹å‡ºæ—¶ï¼Œè½¬åˆ° `[Programming]`ï¼Œé€‰æ‹©ä½ çš„é€‰é¡¹ï¼Œç„¶åæŒ‰ `[Apply]`ã€‚
- en: '![](../media/file929.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file929.png)'
- en: 'If you prefer using Jupyter Notebook for development:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ›´å–œæ¬¢ä½¿ç”¨Jupyter Notebookè¿›è¡Œå¼€å‘ï¼š
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To run Jupyter Notebook, run the command (change the IP address for yours):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿è¡ŒJupyter Notebookï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆæ›´æ”¹IPåœ°å€ä¸ºä½ çš„ï¼‰ï¼š
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'On the terminal, you can see the local URL address to open the notebook:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»ˆç«¯ä¸Šï¼Œä½ å¯ä»¥çœ‹åˆ°æ‰“å¼€notebookçš„æœ¬åœ°URLåœ°å€ï¼š
- en: '![](../media/file930.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file930.png)'
- en: We can access it from another computer by entering the Raspberry Piâ€™s IP address
    and the provided token in a web browser (we should copy it from the terminal).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ç½‘é¡µæµè§ˆå™¨ä¸­è¾“å…¥æ ‘è“æ´¾çš„IPåœ°å€å’Œæä¾›çš„ä»¤ç‰Œæ¥ä»å¦ä¸€å°è®¡ç®—æœºè®¿é—®å®ƒï¼ˆæˆ‘ä»¬åº”è¯¥ä»ç»ˆç«¯å¤åˆ¶å®ƒï¼‰ã€‚
- en: In our working directory in the Raspi, we will create a new Python 3 notebook.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Raspiçš„å·¥ä½œç›®å½•ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ–°çš„Python 3ç¬”è®°æœ¬ã€‚
- en: 'Letâ€™s enter with a very simple script to verify the installed models:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„è„šæœ¬æ¥éªŒè¯å·²å®‰è£…çš„æ¨¡å‹ï¼š
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'All the models will be printed as a dictionary, for example:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹éƒ½å°†æ‰“å°ä¸ºå­—å…¸ï¼Œä¾‹å¦‚ï¼š
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Letâ€™s repeat one of the questions that we did before, but now using `ollama.generate()`
    from Ollama python library. This API will generate a response for the given prompt
    with the provided model. This is a streaming endpoint, so there will be a series
    of responses. The final response object will include statistics and additional
    data from the request.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é‡å¤ä¹‹å‰åšçš„ä¸€ä¸ªé—®é¢˜ï¼Œä½†ç°åœ¨ä½¿ç”¨Ollama Pythonåº“ä¸­çš„`ollama.generate()`ã€‚è¿™ä¸ªAPIå°†ä¸ºç»™å®šçš„æç¤ºç”Ÿæˆå“åº”ï¼Œå¹¶ä½¿ç”¨æä¾›çš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæµå¼ç«¯ç‚¹ï¼Œæ‰€ä»¥ä¼šæœ‰ä¸€ç³»åˆ—çš„å“åº”ã€‚æœ€ç»ˆçš„å“åº”å¯¹è±¡å°†åŒ…æ‹¬è¯·æ±‚çš„ç»Ÿè®¡ä¿¡æ¯å’Œé™„åŠ æ•°æ®ã€‚
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In case you are running the code as a Python script, you should save it, for
    example, test_ollama.py. You can use the IDE to run it or do it directly on the
    terminal. Also, remember that you should always call the model and define it when
    running a stand-alone script.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å°†ä»£ç ä½œä¸ºPythonè„šæœ¬è¿è¡Œï¼Œä½ åº”è¯¥ä¿å­˜å®ƒï¼Œä¾‹å¦‚ï¼Œtest_ollama.pyã€‚ä½ å¯ä»¥ä½¿ç”¨IDEè¿è¡Œå®ƒæˆ–åœ¨ç»ˆç«¯ä¸Šç›´æ¥è¿è¡Œã€‚æ­¤å¤–ï¼Œè¯·è®°ä½ï¼Œåœ¨è¿è¡Œç‹¬ç«‹è„šæœ¬æ—¶ï¼Œä½ åº”è¯¥å§‹ç»ˆè°ƒç”¨æ¨¡å‹å¹¶å®šä¹‰å®ƒã€‚
- en: '[PRE22]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As a result, we will have the model response in a JSON format:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†ä»¥JSONæ ¼å¼è·å¾—æ¨¡å‹å“åº”ï¼š
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see, several pieces of information are generated, such as:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç”Ÿæˆäº†å‡ æ¡ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
- en: '**response**: the main output text generated by the model in response to our
    prompt.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**response**ï¼šæ¨¡å‹é’ˆå¯¹æˆ‘ä»¬çš„æç¤ºç”Ÿæˆçš„æ–‡æœ¬è¾“å‡ºã€‚'
- en: '`The capital of France is **Paris**. ğŸ‡«ğŸ‡·`'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ³•å›½çš„é¦–éƒ½æ˜¯**å·´é»**ã€‚ğŸ‡«ğŸ‡·`'
- en: '**context**: the token IDs representing the input and context used by the model.
    Tokens are numerical representations of text used for processing by the language
    model.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**context**ï¼šè¡¨ç¤ºæ¨¡å‹ä½¿ç”¨çš„è¾“å…¥å’Œä¸Šä¸‹æ–‡çš„æ ‡è®°IDã€‚æ ‡è®°æ˜¯ç”¨äºè¯­è¨€æ¨¡å‹å¤„ç†çš„æ–‡æœ¬çš„æ•°å€¼è¡¨ç¤ºã€‚'
- en: '`[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516,
    108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248,
    108]`'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108, 106, 2516,
    108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428, 235248, 244304, 241035, 235248,
    108]`'
- en: 'The Performance Metrics:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½æŒ‡æ ‡ï¼š
- en: '**total_duration**: The total time taken for the operation in nanoseconds.
    In this case, approximately 24.26 seconds.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**total_duration**ï¼šæ“ä½œæ‰€éœ€çš„æ€»æ—¶é—´ï¼Œä»¥çº³ç§’ä¸ºå•ä½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¤§çº¦24.26ç§’ã€‚'
- en: '**load_duration**: The time taken to load the model or components in nanoseconds.
    About 19.83 seconds.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**load_duration**ï¼šåŠ è½½æ¨¡å‹æˆ–ç»„ä»¶æ‰€éœ€çš„æ—¶é—´ï¼Œä»¥çº³ç§’ä¸ºå•ä½ã€‚å¤§çº¦19.83ç§’ã€‚'
- en: '**prompt_eval_duration**: The time taken to evaluate the prompt in nanoseconds.
    Around 16 nanoseconds.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**prompt_eval_duration**ï¼šè¯„ä¼°æç¤ºæ‰€éœ€çš„æ—¶é—´ï¼Œä»¥çº³ç§’ä¸ºå•ä½ã€‚å¤§çº¦16çº³ç§’ã€‚'
- en: '**eval_count**: The number of tokens evaluated during the generation. Here,
    14 tokens.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eval_count**ï¼šç”Ÿæˆè¿‡ç¨‹ä¸­è¯„ä¼°çš„æ ‡è®°æ•°é‡ã€‚è¿™é‡Œï¼Œ14ä¸ªæ ‡è®°ã€‚'
- en: '**eval_duration**: The time taken for the model to generate the response in
    nanoseconds. Approximately 2.5 seconds.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eval_duration**ï¼šæ¨¡å‹ç”Ÿæˆå“åº”æ‰€éœ€çš„æ—¶é—´ï¼Œä»¥çº³ç§’ä¸ºå•ä½ã€‚å¤§çº¦2.5ç§’ã€‚'
- en: 'But, what we want is the plain â€˜responseâ€™ and, perhaps for analysis, the total
    duration of the inference, so letâ€™s change the code to extract it from the dictionary:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæˆ‘ä»¬æƒ³è¦çš„æ˜¯ç®€å•çš„â€˜å“åº”â€™ä»¥åŠï¼Œä¹Ÿè®¸ä¸ºäº†åˆ†æï¼Œæ¨ç†çš„æ€»æ—¶é•¿ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¿®æ”¹ä»£ç ä»å­—å…¸ä¸­æå–å®ƒï¼š
- en: '[PRE24]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we got:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ï¼š
- en: '[PRE25]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Using Ollama.chat()**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨Ollama.chat()**'
- en: 'Another way to get our response is to use `ollama.chat()`, which generates
    the next message in a chat with a provided model. This is a streaming endpoint,
    so a series of responses will occur. Streaming can be disabled using `"stream":
    false`. The final response object will also include statistics and additional
    data from the request.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 'è·å–æˆ‘ä»¬å“åº”çš„å¦ä¸€ç§æ–¹å¼æ˜¯ä½¿ç”¨`ollama.chat()`ï¼Œå®ƒç”Ÿæˆä¸æä¾›çš„æ¨¡å‹è¿›è¡Œçš„èŠå¤©ä¸­çš„ä¸‹ä¸€ä¸ªæ¶ˆæ¯ã€‚è¿™æ˜¯ä¸€ä¸ªæµå¼ç«¯ç‚¹ï¼Œå› æ­¤å°†å‘ç”Ÿä¸€ç³»åˆ—å“åº”ã€‚å¯ä»¥é€šè¿‡å°†`"stream":
    false`ç¦ç”¨æ¥å…³é—­æµã€‚æœ€ç»ˆå“åº”å¯¹è±¡è¿˜å°†åŒ…æ‹¬è¯·æ±‚çš„ç»Ÿè®¡ä¿¡æ¯å’Œé™„åŠ æ•°æ®ã€‚'
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The answer is the same as before.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ç­”æ¡ˆä¸ä¹‹å‰ç›¸åŒã€‚
- en: 'An important consideration is that by using `ollama.generate()`, the response
    is â€œclearâ€ from the modelâ€™s â€œmemoryâ€ after the end of inference (only used once),
    but If we want to keep a conversation, we must use `ollama.chat()`. Letâ€™s see
    it in action:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ æ˜¯ï¼Œä½¿ç”¨`ollama.generate()`åï¼Œå“åº”ä»æ¨¡å‹çš„â€œè®°å¿†â€ä¸­â€œæ¸…é™¤â€ï¼ˆä»…ä½¿ç”¨ä¸€æ¬¡ï¼‰ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³ä¿æŒå¯¹è¯ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨`ollama.chat()`ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the above code, we are running two queries, and the second prompt considers
    the result of the first one.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬è¿è¡Œäº†ä¸¤ä¸ªæŸ¥è¯¢ï¼Œç¬¬äºŒä¸ªæç¤ºè€ƒè™‘äº†ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœã€‚
- en: 'Here is how the model responded:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¨¡å‹çš„å“åº”ï¼š
- en: '[PRE28]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Getting an image description**:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**è·å–å›¾åƒæè¿°**ï¼š'
- en: 'In the same way that we have used the `LlaVa-PHI-3` model with the command
    line to analyze an image, the same can be done here with Python. Letâ€™s use the
    same image of Paris, but now with the `ollama.generate()`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬ä½¿ç”¨`LlaVa-PHI-3`æ¨¡å‹é€šè¿‡å‘½ä»¤è¡Œåˆ†æå›¾åƒä¸€æ ·ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ç”¨PythonåšåŒæ ·çš„äº‹æƒ…ã€‚è®©æˆ‘ä»¬ä½¿ç”¨åŒæ ·çš„å·´é»å›¾åƒï¼Œä½†ç°åœ¨ä½¿ç”¨`ollama.generate()`ï¼š
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the result:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç»“æœï¼š
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The model took about 4 minutes (256.45 s) to return with a detailed image description.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¤§çº¦ç”¨äº†4åˆ†é’Ÿï¼ˆ256.45ç§’ï¼‰è¿”å›è¯¦ç»†çš„å›¾åƒæè¿°ã€‚
- en: In the [10-Ollama_Python_Library](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb)
    notebook, it is possible to find the experiments with the Ollama Python library.
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨[10-Ollama_Python_Library](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb)ç¬”è®°æœ¬ä¸­ï¼Œå¯ä»¥æ‰¾åˆ°ä½¿ç”¨Ollama
    Pythonåº“çš„å®éªŒã€‚
- en: Function Calling
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‡½æ•°è°ƒç”¨
- en: So far, we can observe that by using the modelâ€™s response into a variable, we
    can effectively incorporate it into real-world projects. However, a major issue
    arises when the model provides varying responses to the same input. For instance,
    letâ€™s assume that we only need the name of a countryâ€™s capital and its coordinates
    as the modelâ€™s response in the previous examples, without any additional information,
    even when utilizing verbose models like Microsoft Phi. To ensure consistent responses,
    we can employ the â€˜Ollama function call,â€™ which is fully compatible with the OpenAI
    API.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡å°†æ¨¡å‹çš„å“åº”æ”¾å…¥ä¸€ä¸ªå˜é‡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°å°†å…¶èå…¥ç°å®ä¸–ç•Œçš„é¡¹ç›®ä¸­ã€‚ç„¶è€Œï¼Œå½“æ¨¡å‹å¯¹ç›¸åŒçš„è¾“å…¥æä¾›ä¸åŒçš„å“åº”æ—¶ï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªä¸»è¦é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åªéœ€è¦åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­æ¨¡å‹å“åº”çš„å›½å®¶é¦–éƒ½åç§°åŠå…¶åæ ‡ï¼Œæ²¡æœ‰ä»»ä½•é™„åŠ ä¿¡æ¯ï¼Œå³ä½¿ä½¿ç”¨åƒå¾®è½¯Phiè¿™æ ·çš„å†—é•¿æ¨¡å‹ã€‚ä¸ºäº†ç¡®ä¿å“åº”çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€œOllamaå‡½æ•°è°ƒç”¨â€ï¼Œå®ƒä¸OpenAI
    APIå®Œå…¨å…¼å®¹ã€‚
- en: But what exactly is â€œfunction callingâ€?
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½†â€œå‡½æ•°è°ƒç”¨â€åˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
- en: In modern artificial intelligence, function calling with Large Language Models
    (LLMs) allows these models to perform actions beyond generating text. By integrating
    with external functions or APIs, LLMs can access real-time data, automate tasks,
    and interact with various systems.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°ä»£äººå·¥æ™ºèƒ½ä¸­ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå‡½æ•°è°ƒç”¨å…è®¸è¿™äº›æ¨¡å‹æ‰§è¡Œç”Ÿæˆæ–‡æœ¬ä¹‹å¤–çš„æ“ä½œã€‚é€šè¿‡é›†æˆå¤–éƒ¨å‡½æ•°æˆ–APIï¼ŒLLMså¯ä»¥è®¿é—®å®æ—¶æ•°æ®ï¼Œè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œå¹¶ä¸å„ç§ç³»ç»Ÿäº¤äº’ã€‚
- en: For instance, instead of merely responding to a query about the weather, an
    LLM can call a weather API to fetch the current conditions and provide accurate,
    up-to-date information. This capability enhances the relevance and accuracy of
    the modelâ€™s responses and makes it a powerful tool for driving workflows and automating
    processes, transforming it into an active participant in real-world applications.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸å…¶ä»…ä»…å“åº”å…³äºå¤©æ°”çš„æŸ¥è¯¢ï¼Œä¸€ä¸ªLLMå¯ä»¥è°ƒç”¨å¤©æ°”APIæ¥è·å–å½“å‰æ¡ä»¶å¹¶æä¾›å‡†ç¡®ã€æœ€æ–°çš„ä¿¡æ¯ã€‚è¿™ç§èƒ½åŠ›å¢å¼ºäº†æ¨¡å‹å“åº”çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ï¼Œä½¿å…¶æˆä¸ºé©±åŠ¨å·¥ä½œæµç¨‹å’Œè‡ªåŠ¨åŒ–æµç¨‹çš„å¼ºå¤§å·¥å…·ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç§¯æå‚ä¸è€…ã€‚
- en: 'For more details about Function Calling, please see this video made by [Marvin
    Prison](https://www.youtube.com/@MervinPraison):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå‡½æ•°è°ƒç”¨çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç”±[Marvin Prison](https://www.youtube.com/@MervinPraison)åˆ¶ä½œçš„æ­¤è§†é¢‘ï¼š
- en: '[https://www.youtube.com/embed/eHfMCtlsb1o](https://www.youtube.com/embed/eHfMCtlsb1o)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/embed/eHfMCtlsb1o](https://www.youtube.com/embed/eHfMCtlsb1o)'
- en: Letâ€™s create a project.
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé¡¹ç›®ã€‚
- en: We want to create an *app* where the user enters a countryâ€™s name and gets,
    as an output, the distance in km from the capital city of such a country and the
    appâ€™s location (for simplicity, We will use Santiago, Chile, as the app location).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ª*åº”ç”¨ç¨‹åº*ï¼Œç”¨æˆ·è¾“å…¥ä¸€ä¸ªå›½å®¶çš„åç§°ï¼Œç„¶åå¾—åˆ°è¾“å‡ºï¼Œå³è¯¥å›½é¦–éƒ½ä¸è¯¥åº”ç”¨ç¨‹åºä½ç½®ï¼ˆä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ™ºåˆ©çš„åœ£åœ°äºšå“¥ä½œä¸ºåº”ç”¨ç¨‹åºä½ç½®ï¼‰ä¹‹é—´çš„è·ç¦»ï¼ˆå…¬é‡Œï¼‰ã€‚
- en: '![](../media/file931.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file931.png)'
- en: Once the user enters a country name, the model will return the name of its capital
    city (as a string) and the latitude and longitude of such city (in float). Using
    those coordinates, we can use a simple Python library ([haversine](https://pypi.org/project/haversine/))
    to calculate the distance between those 2 points.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç”¨æˆ·è¾“å…¥å›½å®¶åç§°ï¼Œæ¨¡å‹å°†è¿”å›è¯¥å›½å®¶çš„é¦–éƒ½åç§°ï¼ˆä½œä¸ºå­—ç¬¦ä¸²ï¼‰ä»¥åŠè¯¥åŸå¸‚çš„çº¬åº¦å’Œç»åº¦ï¼ˆä»¥æµ®ç‚¹æ•°è¡¨ç¤ºï¼‰ã€‚ä½¿ç”¨è¿™äº›åæ ‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç®€å•çš„Pythonåº“([haversine](https://pypi.org/project/haversine/))æ¥è®¡ç®—è¿™ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è·ç¦»ã€‚
- en: The idea of this project is to demonstrate a combination of language model interaction,
    structured data handling with Pydantic, and geospatial calculations using the
    Haversine formula (traditional computing).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¡¹ç›®çš„æƒ³æ³•æ˜¯å±•ç¤ºè¯­è¨€æ¨¡å‹äº¤äº’ã€ä½¿ç”¨Pydanticè¿›è¡Œç»“æ„åŒ–æ•°æ®å¤„ç†ä»¥åŠä½¿ç”¨Haversineå…¬å¼ï¼ˆä¼ ç»Ÿè®¡ç®—ï¼‰è¿›è¡Œåœ°ç†ç©ºé—´è®¡ç®—çš„ç»„åˆã€‚
- en: First, let us install some libraries. Besides *Haversine*, the main one is the
    [OpenAI Python library](https://github.com/openai/openai-python), which provides
    convenient access to the OpenAI REST API from any Python 3.7+ application. The
    other one is [Pydantic](https://docs.pydantic.dev/latest/) (and instructor), a
    robust data validation and settings management library engineered by Python to
    enhance the robustness and reliability of our codebase. In short, *Pydantic* will
    help ensure that our modelâ€™s response will always be consistent.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…ä¸€äº›åº“ã€‚é™¤äº†*Haversine*ï¼Œä¸»è¦çš„ä¸€ä¸ªæ˜¯[OpenAI Pythonåº“](https://github.com/openai/openai-python)ï¼Œå®ƒä¸ºä»»ä½•Python
    3.7+åº”ç”¨ç¨‹åºæä¾›äº†æ–¹ä¾¿çš„è®¿é—®OpenAI REST APIçš„æ–¹å¼ã€‚å¦ä¸€ä¸ªæ˜¯[Pydantic](https://docs.pydantic.dev/latest/)ï¼ˆå’Œinstructorï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±Pythonæ„å»ºçš„å¼ºå¤§æ•°æ®éªŒè¯å’Œè®¾ç½®ç®¡ç†åº“ï¼Œæ—¨åœ¨å¢å¼ºæˆ‘ä»¬ä»£ç åº“çš„å¥å£®æ€§å’Œå¯é æ€§ã€‚ç®€è€Œè¨€ä¹‹ï¼Œ*Pydantic*å°†å¸®åŠ©ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹å“åº”å§‹ç»ˆä¸€è‡´ã€‚
- en: '[PRE31]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we should create a Python script designed to interact with our model (LLM)
    to determine the coordinates of a countryâ€™s capital city and calculate the distance
    from Santiago de Chile to that capital.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬åº”è¯¥åˆ›å»ºä¸€ä¸ªPythonè„šæœ¬ï¼Œç”¨äºä¸æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’ï¼Œä»¥ç¡®å®šä¸€ä¸ªå›½å®¶é¦–éƒ½çš„åæ ‡ï¼Œå¹¶è®¡ç®—ä»åœ£åœ°äºšå“¥åˆ°è¯¥é¦–éƒ½çš„è·ç¦»ã€‚
- en: 'Letâ€™s go over the code:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç ï¼š
- en: 1\. Importing Libraries
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. å¯¼å…¥åº“
- en: '[PRE32]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**sys**: Provides access to system-specific parameters and functions. Itâ€™s
    used to get command-line arguments.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sys**ï¼šæä¾›å¯¹ç³»ç»Ÿç‰¹å®šå‚æ•°å’Œå‡½æ•°çš„è®¿é—®ã€‚å®ƒç”¨äºè·å–å‘½ä»¤è¡Œå‚æ•°ã€‚'
- en: '**haversine**: A function from the haversine library that calculates the distance
    between two geographic points using the Haversine formula.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**haversine**ï¼šä¸€ä¸ªæ¥è‡ªhaversineåº“çš„å‡½æ•°ï¼Œå®ƒä½¿ç”¨Haversineå…¬å¼è®¡ç®—ä¸¤ä¸ªåœ°ç†ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚'
- en: '**openAI**: A module for interacting with the OpenAI API (although itâ€™s used
    in conjunction with a local setup, Ollama). Everything is off-line here.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**openAI**ï¼šä¸€ä¸ªç”¨äºä¸OpenAI APIäº¤äº’çš„æ¨¡å—ï¼ˆå°½ç®¡å®ƒä¸æœ¬åœ°è®¾ç½®ï¼ˆOllamaï¼‰ä¸€èµ·ä½¿ç”¨ï¼Œä½†è¿™é‡Œä¸€åˆ‡éƒ½æ˜¯ç¦»çº¿çš„ï¼‰ã€‚'
- en: '**pydantic**: Provides data validation and settings management using Python-type
    annotations. Itâ€™s used to define the structure of expected response data.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pydantic**ï¼šä½¿ç”¨Pythonç±»å‹æ³¨è§£æä¾›æ•°æ®éªŒè¯å’Œè®¾ç½®ç®¡ç†ã€‚å®ƒç”¨äºå®šä¹‰é¢„æœŸå“åº”æ•°æ®çš„ç»“æ„ã€‚'
- en: '**instructor**: A module is used to patch the OpenAI client to work in a specific
    mode (likely related to structured data handling).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**instructor**ï¼šä¸€ä¸ªæ¨¡å—ï¼Œç”¨äºä¿®è¡¥OpenAIå®¢æˆ·ç«¯ä»¥åœ¨ç‰¹å®šæ¨¡å¼ä¸‹å·¥ä½œï¼ˆå¯èƒ½ä¸å…¶ç»“æ„åŒ–æ•°æ®å¤„ç†æœ‰å…³ï¼‰ã€‚'
- en: 2\. Defining Input and Model
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. å®šä¹‰è¾“å…¥å’Œæ¨¡å‹
- en: '[PRE33]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**country**: On a Python script, getting the country name from command-line
    arguments is possible. On a Jupyter notebook, we can enter its name, for example,'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**country**ï¼šåœ¨Pythonè„šæœ¬ä¸­ï¼Œå¯ä»¥ä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å–å›½å®¶åç§°ã€‚åœ¨Jupyterç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥å…¶åç§°ï¼Œä¾‹å¦‚ï¼Œ'
- en: '`country = "France"`'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`country = "France"`'
- en: '**MODEL**: Specifies the model being used, which is, in this example, the phi3.5.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MODEL**ï¼šæŒ‡å®šæ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯phi3.5ã€‚'
- en: '**mylat** **and** **mylon**: Coordinates of Santiago de Chile, used as the
    starting point for the distance calculation.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mylat** **å’Œ** **mylon**ï¼šåœ£åœ°äºšå“¥çš„åæ ‡ï¼Œç”¨ä½œè·ç¦»è®¡ç®—çš„èµ·ç‚¹ã€‚'
- en: 3\. Defining the Response Data Structure
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. å®šä¹‰å“åº”æ•°æ®ç»“æ„
- en: '[PRE34]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**CityCoord**: A Pydantic model that defines the expected structure of the
    response from the LLM. It expects three fields: city (name of the city), lat (latitude),
    and lon (longitude).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CityCoord**ï¼šä¸€ä¸ªPydanticæ¨¡å‹ï¼Œç”¨äºå®šä¹‰LLMå“åº”çš„é¢„æœŸç»“æ„ã€‚å®ƒæœŸæœ›æœ‰ä¸‰ä¸ªå­—æ®µï¼šcityï¼ˆåŸå¸‚åç§°ï¼‰ã€latï¼ˆçº¬åº¦ï¼‰å’Œlonï¼ˆç»åº¦ï¼‰ã€‚'
- en: 4\. Setting Up the OpenAI Client
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4. è®¾ç½®OpenAIå®¢æˆ·ç«¯
- en: '[PRE35]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**OpenAI**: This setup initializes an OpenAI client with a local base URL and
    an API key (ollama). It uses a local server.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**: æ­¤è®¾ç½®åˆå§‹åŒ–äº†ä¸€ä¸ªå¸¦æœ‰æœ¬åœ°åŸºæœ¬ URL å’Œ API å¯†é’¥ï¼ˆollamaï¼‰çš„ OpenAI å®¢æˆ·ç«¯ã€‚å®ƒä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨ã€‚'
- en: '**instructor.patch**: Patches the OpenAI client to work in JSON mode, enabling
    structured output that matches the Pydantic model.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**instructor.patch**: ä¿®è¡¥ OpenAI å®¢æˆ·ç«¯ä»¥åœ¨ JSON æ¨¡å¼ä¸‹å·¥ä½œï¼Œå¯ç”¨ä¸ Pydantic æ¨¡å‹åŒ¹é…çš„ç»“æ„åŒ–è¾“å‡ºã€‚'
- en: 5\. Generating the Response
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5. ç”Ÿæˆå“åº”
- en: '[PRE36]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**client.chat.completions.create**: Calls the LLM to generate a response.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**client.chat.completions.create**: è°ƒç”¨ LLM ç”Ÿæˆå“åº”ã€‚'
- en: '**model**: Specifies the model to use (llava-phi3).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**model**: æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆllava-phi3ï¼‰ã€‚'
- en: '**messages**: Contains the prompt for the LLM, asking for the latitude and
    longitude of the capital city of the specified country.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**messages**: åŒ…å«å¯¹ LLM çš„æç¤ºï¼Œè¦æ±‚æä¾›æŒ‡å®šå›½å®¶é¦–éƒ½çš„çº¬åº¦å’Œç»åº¦ã€‚'
- en: '**response_model**: Indicates that the response should conform to the CityCoord
    model.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**response_model**: æŒ‡ç¤ºå“åº”åº”ç¬¦åˆ CityCoord æ¨¡å‹ã€‚'
- en: '**max_retries**: The maximum number of retry attempts if the request fails.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_retries**: å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œæœ€å¤§é‡è¯•æ¬¡æ•°ã€‚'
- en: 6\. Calculating the Distance
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6. è®¡ç®—è·ç¦»
- en: '[PRE37]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '**haversine**: Calculates the distance between Santiago de Chile and the capital
    city returned by the LLM using their respective coordinates.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**haversine**: ä½¿ç”¨å„è‡ªçš„åæ ‡è®¡ç®— LLM è¿”å›çš„åœ£åœ°äºšå“¥Â·å¾·Â·æ™ºåˆ©çš„é¦–éƒ½ä¹‹é—´çš„è·ç¦»ã€‚'
- en: '**(mylat, mylon)**: Coordinates of Santiago de Chile.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(mylat, mylon)**: åœ£åœ°äºšå“¥Â·å¾·Â·æ™ºåˆ©çš„åæ ‡ã€‚'
- en: '**resp.city**: Name of the countryâ€™s capital'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**resp.city**: å›½å®¶çš„é¦–éƒ½åç§°'
- en: '**(resp.lat, resp.lon)**: Coordinates of the capital city are provided by the
    LLM response.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(resp.lat, resp.lon)**: é¦–éƒ½çš„åæ ‡ç”± LLM å“åº”æä¾›ã€‚'
- en: '**unit = â€˜kmâ€™**: Specifies that the distance should be calculated in kilometers.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**unit = â€˜kmâ€™**: æŒ‡å®šè·ç¦»åº”ä»¥å…¬é‡Œè®¡ç®—ã€‚'
- en: '**print**: Outputs the distance, rounded to the nearest 10 kilometers, with
    thousands of separators for readability.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**print**: è¾“å‡ºè·ç¦»ï¼Œå››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„ 10 å…¬é‡Œï¼Œå¹¶ä½¿ç”¨åƒä½åˆ†éš”ç¬¦ä»¥æé«˜å¯è¯»æ€§ã€‚'
- en: '**Running the code**'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿è¡Œä»£ç **'
- en: 'If we enter different countries, for example, France, Colombia, and the United
    States, We can note that we always receive the same structured information:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¾“å…¥ä¸åŒçš„å›½å®¶ï¼Œä¾‹å¦‚ï¼Œæ³•å›½ã€å“¥ä¼¦æ¯”äºšå’Œç¾å›½ï¼Œæˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°æˆ‘ä»¬æ€»æ˜¯æ”¶åˆ°ç›¸åŒç»“æ„åŒ–çš„ä¿¡æ¯ï¼š
- en: '[PRE38]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If you run the code as a script, the result will be printed on the terminal:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å°†ä»£ç ä½œä¸ºè„šæœ¬è¿è¡Œï¼Œç»“æœå°†åœ¨ç»ˆç«¯ä¸Šæ‰“å°ï¼š
- en: '![](../media/file932.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file932.png)'
- en: And the calculations are pretty good!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç»“æœç›¸å½“å‡†ç¡®ï¼
- en: '![](../media/file933.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file933.png)'
- en: In the [20-Ollama_Function_Calling](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb)
    notebook, it is possible to find experiments with all models installed.
  id: totrans-303
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨ [20-Ollama_Function_Calling](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb)
    ç¬”è®°æœ¬ä¸­ï¼Œå¯ä»¥æ‰¾åˆ°æ‰€æœ‰å·²å®‰è£…æ¨¡å‹çš„å®éªŒã€‚
- en: Adding images
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·»åŠ å›¾ç‰‡
- en: Now it is time to wrap up everything so far! Letâ€™s modify the script so that
    instead of entering the country name (as a text), the user enters an image, and
    the application (based on SLM) returns the city in the image and its geographic
    location. With those data, we can calculate the distance as before.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™æ€»ç»“åˆ°ç›®å‰ä¸ºæ­¢çš„å†…å®¹äº†ï¼è®©æˆ‘ä»¬ä¿®æ”¹è„šæœ¬ï¼Œä»¥ä¾¿ç”¨æˆ·è¾“å…¥çš„ä¸æ˜¯å›½å®¶åç§°ï¼ˆä½œä¸ºæ–‡æœ¬ï¼‰ï¼Œè€Œæ˜¯è¾“å…¥ä¸€ä¸ªå›¾åƒï¼Œå¹¶ä¸”åŸºäº SLM çš„åº”ç”¨ç¨‹åºè¿”å›å›¾åƒä¸­çš„åŸå¸‚åŠå…¶åœ°ç†ä½ç½®ã€‚æœ‰äº†è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·è®¡ç®—è·ç¦»ã€‚
- en: '![](../media/file934.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file934.png)'
- en: For simplicity, we will implement this new code in two steps. First, the LLM
    will analyze the image and create a description (text). This text will be passed
    on to another instance, where the model will extract the information needed to
    pass along.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åˆ†ä¸¤æ­¥å®ç°æ­¤æ–°ä»£ç ã€‚é¦–å…ˆï¼ŒLLM å°†åˆ†æå›¾åƒå¹¶åˆ›å»ºæè¿°ï¼ˆæ–‡æœ¬ï¼‰ã€‚æ­¤æ–‡æœ¬å°†è¢«ä¼ é€’åˆ°å¦ä¸€ä¸ªå®ä¾‹ï¼Œå…¶ä¸­æ¨¡å‹å°†æå–æ‰€éœ€ä¿¡æ¯ä»¥ä¼ é€’ã€‚
- en: We will start importing the libraries
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹å¯¼å…¥åº“
- en: '[PRE39]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can see the image if you run the code on the Jupyter Notebook. For that
    we need also import:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨ Jupyter Notebook ä¸Šè¿è¡Œä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™å¼ å›¾ç‰‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¼å…¥ï¼š
- en: '[PRE40]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Those libraries are unnecessary if we run the code as a script.
  id: totrans-312
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»¥è„šæœ¬æ–¹å¼è¿è¡Œä»£ç ï¼Œåˆ™ä¸éœ€è¦è¿™äº›åº“ã€‚
- en: 'Now, we define the model and the local coordinates:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰æ¨¡å‹å’Œæœ¬åœ°åæ ‡ï¼š
- en: '[PRE41]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can download a new image, for example, Machu Picchu from Wikipedia. On the
    Notebook we can see it:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä¸‹è½½ä¸€å¼ æ–°å›¾ç‰‡ï¼Œä¾‹å¦‚ï¼Œæ¥è‡ªç»´åŸºç™¾ç§‘çš„é©¬ä¸˜æ¯”ä¸˜ã€‚åœ¨ç¬”è®°æœ¬ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒï¼š
- en: '[PRE42]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](../media/file935.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file935.jpg)'
- en: Now, letâ€™s define a function that will receive the image and will `return the
    decimal latitude and decimal longitude of the city in the image, its name, and
    what country it is located`
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æ¥æ”¶å›¾åƒå¹¶å°†`è¿”å›å›¾åƒä¸­åŸå¸‚çš„åè¿›åˆ¶åº¦é‡å’Œåè¿›åˆ¶åº¦ï¼Œä»¥åŠè¯¥åŸå¸‚çš„åç§°å’Œæ‰€åœ¨å›½å®¶`
- en: '[PRE43]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can print the entire response for debug purposes.
  id: totrans-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ‰“å°æ•´ä¸ªå“åº”ä»¥ä¾›è°ƒè¯•ç›®çš„ã€‚
- en: The image description generated for the function will be passed as a prompt
    for the model again.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºè¯¥å‡½æ•°ç”Ÿæˆçš„å›¾åƒæè¿°å°†å†æ¬¡ä½œä¸ºæç¤ºä¼ é€’ç»™æ¨¡å‹ã€‚
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If we print the image description , we will get:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æ‰“å°å›¾åƒæè¿°ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š
- en: '[PRE45]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And the second response from the model (`resp`) will be:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„ç¬¬äºŒä¸ªå“åº”ï¼ˆ`resp`ï¼‰å°†æ˜¯ï¼š
- en: '[PRE46]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we can do a â€œPost-Processingâ€, calculating the distance and preparing
    the final answer:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œâ€œåå¤„ç†â€ï¼Œè®¡ç®—è·ç¦»å¹¶å‡†å¤‡æœ€ç»ˆç­”æ¡ˆï¼š
- en: '[PRE47]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'And we will get:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¾—åˆ°ï¼š
- en: '[PRE48]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the [30-Function_Calling_with_images](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb)
    notebook, it is possible to find the experiments with multiple images.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[30-Function_Calling_with_images](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb)ç¬”è®°æœ¬ä¸­ï¼Œå¯ä»¥æ‰¾åˆ°ä½¿ç”¨å¤šå¼ å›¾åƒçš„å®éªŒã€‚
- en: 'Letâ€™s now download the script `calc_distance_image.py` from the GitHub and
    run it on the terminal with the command:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»GitHubä¸‹è½½è„šæœ¬`calc_distance_image.py`å¹¶åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œå®ƒï¼š
- en: '[PRE49]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Enter with the Machu Picchu image full patch as an argument. We will get the
    same previous result.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é©¬ä¸˜æ¯”ä¸˜å›¾åƒå®Œæ•´è¡¥ä¸ä½œä¸ºå‚æ•°ã€‚æˆ‘ä»¬å°†å¾—åˆ°ç›¸åŒçš„å‰ä¸€ä¸ªç»“æœã€‚
- en: '![](../media/file936.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file936.png)'
- en: '*How* about Paris?'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå·´é»æ€ä¹ˆæ ·ï¼Ÿ
- en: '![](../media/file937.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file937.png)'
- en: Of course, there are many ways to optimize the code used here. Still, the idea
    is to explore the considerable potential of *function calling* with SLMs at the
    edge, allowing those models to integrate with external functions or APIs. Going
    beyond text generation, SLMs can access real-time data, automate tasks, and interact
    with various systems.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥ä¼˜åŒ–è¿™é‡Œä½¿ç”¨çš„ä»£ç ã€‚ä½†æƒ³æ³•æ˜¯æ¢ç´¢åœ¨è¾¹ç¼˜ä½¿ç”¨SLMsè¿›è¡Œ**å‡½æ•°è°ƒç”¨**çš„å·¨å¤§æ½œåŠ›ï¼Œä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿä¸å¤–éƒ¨å‡½æ•°æˆ–APIé›†æˆã€‚è¶…è¶Šæ–‡æœ¬ç”Ÿæˆï¼ŒSLMså¯ä»¥è®¿é—®å®æ—¶æ•°æ®ï¼Œè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œå¹¶ä¸å„ç§ç³»ç»Ÿäº¤äº’ã€‚
- en: 'SLMs: Optimization Techniques'
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SLMsï¼šä¼˜åŒ–æŠ€æœ¯
- en: Large Language Models (LLMs) have revolutionized natural language processing,
    but their deployment and optimization come with unique challenges. One significant
    issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding
    but factually incorrect information, a phenomenon known as **hallucination**.
    This occurs when models produce content that seems coherent but is not grounded
    in truth or real-world facts.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä½†å®ƒä»¬çš„éƒ¨ç½²å’Œä¼˜åŒ–å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¸€ä¸ªé‡å¤§é—®é¢˜æ˜¯LLMsï¼ˆä»¥åŠæ›´å¤šçš„SLMsï¼‰å€¾å‘äºç”Ÿæˆå¬èµ·æ¥åˆç†ä½†å®é™…ä¸Šé”™è¯¯çš„ä¿¡æ¯ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸º**å¹»è§‰**ã€‚è¿™å‘ç”Ÿåœ¨æ¨¡å‹äº§ç”Ÿçœ‹ä¼¼è¿è´¯ä½†ç¼ºä¹çœŸå®æˆ–ç°å®ä¸–ç•Œäº‹å®çš„å†…å®¹æ—¶ã€‚
- en: Other challenges include the immense computational resources required for training
    and running these models, the difficulty in maintaining up-to-date knowledge within
    the model, and the need for domain-specific adaptations. Privacy concerns also
    arise when handling sensitive data during training or inference. Additionally,
    ensuring consistent performance across diverse tasks and maintaining ethical use
    of these powerful tools present ongoing challenges. Addressing these issues is
    crucial for the effective and responsible deployment of LLMs in real-world applications.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æŒ‘æˆ˜åŒ…æ‹¬è®­ç»ƒå’Œè¿è¡Œè¿™äº›æ¨¡å‹æ‰€éœ€çš„å·¨å¤§è®¡ç®—èµ„æºã€åœ¨æ¨¡å‹ä¸­ç»´æŠ¤æœ€æ–°çŸ¥è¯†çš„éš¾åº¦ï¼Œä»¥åŠéœ€è¦ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ã€‚åœ¨è®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ä¸­å¤„ç†æ•æ„Ÿæ•°æ®æ—¶ï¼Œä¹Ÿä¼šå‡ºç°éšç§é—®é¢˜ã€‚æ­¤å¤–ï¼Œç¡®ä¿è·¨ä¸åŒä»»åŠ¡çš„ä¸€è‡´æ€§èƒ½ä»¥åŠä¿æŒè¿™äº›å¼ºå¤§å·¥å…·çš„é“å¾·ä½¿ç”¨ä¹Ÿå¸¦æ¥äº†æŒç»­æŒ‘æˆ˜ã€‚è§£å†³è¿™äº›é—®é¢˜å¯¹äºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æœ‰æ•ˆä¸”è´Ÿè´£ä»»åœ°éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡³å…³é‡è¦ã€‚
- en: The fundamental techniques for enhancing LLM (and SLM) performance and efficiency
    are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: æé«˜LLMï¼ˆå’ŒSLMï¼‰æ€§èƒ½å’Œæ•ˆç‡çš„åŸºæœ¬æŠ€æœ¯åŒ…æ‹¬å¾®è°ƒã€æç¤ºå·¥ç¨‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚
- en: '**Fine-tuning**, while more resource-intensive, offers a way to specialize
    LLMs for particular domains or tasks. This process involves further training the
    model on carefully curated datasets, allowing it to adapt its vast general knowledge
    to specific applications. Fine-tuning can lead to substantial improvements in
    performance, especially in specialized fields or for unique use cases.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒ**ï¼Œè™½ç„¶èµ„æºæ¶ˆè€—æ›´å¤§ï¼Œä½†ä¸ºå°†LLMsç‰¹å®šåŒ–åˆ°ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡æä¾›äº†ä¸€ç§æ–¹æ³•ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåœ¨ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå°†å¹¿æ³›çš„ä¸€èˆ¬çŸ¥è¯†é€‚åº”åˆ°ç‰¹å®šåº”ç”¨ä¸­ã€‚å¾®è°ƒå¯ä»¥å¸¦æ¥æ€§èƒ½çš„æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šé¢†åŸŸæˆ–ç‹¬ç‰¹ç”¨ä¾‹ä¸­ã€‚'
- en: '**Prompt engineering** is at the forefront of LLM optimization. By carefully
    crafting input prompts, we can guide models to produce more accurate and relevant
    outputs. This technique involves structuring queries that leverage the modelâ€™s
    pre-trained knowledge and capabilities, often incorporating examples or specific
    instructions to shape the desired response.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤ºå·¥ç¨‹**æ˜¯LLMä¼˜åŒ–çš„å‰æ²¿ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡è¾“å…¥æç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œç›¸å…³çš„è¾“å‡ºã€‚è¿™ç§æŠ€æœ¯æ¶‰åŠæ„å»ºåˆ©ç”¨æ¨¡å‹é¢„å…ˆè®­ç»ƒçš„çŸ¥è¯†å’Œèƒ½åŠ›çš„é—®é¢˜ç»“æ„ï¼Œé€šå¸¸åŒ…æ‹¬ç¤ºä¾‹æˆ–å…·ä½“æŒ‡ä»¤æ¥å¡‘é€ æœŸæœ›çš„å“åº”ã€‚'
- en: '**Retrieval-Augmented Generation (RAG)** represents another powerful approach
    to improving LLM performance. This method combines the vast knowledge embedded
    in pre-trained models with the ability to access and incorporate external, up-to-date
    information. By retrieving relevant data to supplement the modelâ€™s decision-making
    process, RAG can significantly enhance accuracy and reduce the likelihood of generating
    outdated or false information.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰**ä»£è¡¨äº†ä¸€ç§æé«˜LLMæ€§èƒ½çš„å¼ºå¤§æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒæ¨¡å‹ä¸­åµŒå…¥çš„å¹¿æ³›çŸ¥è¯†ä»¥åŠè®¿é—®å’Œæ•´åˆå¤–éƒ¨æœ€æ–°ä¿¡æ¯çš„èƒ½åŠ›ã€‚é€šè¿‡æ£€ç´¢ç›¸å…³æ•°æ®ä»¥è¡¥å……æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼ŒRAGå¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘ç”Ÿæˆè¿‡æ—¶æˆ–é”™è¯¯ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚'
- en: For edge applications, it is more beneficial to focus on techniques like RAG
    that can enhance model performance without needing on-device fine-tuning. Letâ€™s
    explore it.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾¹ç¼˜åº”ç”¨ï¼Œæ›´æœ‰ç›Šçš„æ˜¯å…³æ³¨åƒRAGè¿™æ ·çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œè€Œæ— éœ€åœ¨è®¾å¤‡ä¸Šè¿›è¡Œå¾®è°ƒã€‚è®©æˆ‘ä»¬æ¥æ¢è®¨å®ƒã€‚
- en: RAG Implementation
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAGå®ç°
- en: In a basic interaction between a user and a language model, the user asks a
    question, which is sent as a prompt to the model. The model generates a response
    based solely on its pre-trained knowledge. In a RAG process, thereâ€™s an additional
    step between the userâ€™s question and the modelâ€™s response. The userâ€™s question
    triggers a retrieval process from a knowledge base.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„åŸºæœ¬äº¤äº’ä¸­ï¼Œç”¨æˆ·æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜ä½œä¸ºæç¤ºå‘é€ç»™æ¨¡å‹ã€‚æ¨¡å‹æ ¹æ®å…¶é¢„å…ˆè®­ç»ƒçš„çŸ¥è¯†ç”Ÿæˆä¸€ä¸ªå“åº”ã€‚åœ¨RAGè¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·çš„é—®é¢˜å’Œæ¨¡å‹çš„å“åº”ä¹‹é—´æœ‰ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ã€‚ç”¨æˆ·çš„é—®é¢˜è§¦å‘ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢çš„è¿‡ç¨‹ã€‚
- en: '![](../media/file938.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file938.png)'
- en: A simple RAG project
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„RAGé¡¹ç›®
- en: 'Here are the steps to implement a basic Retrieval Augmented Generation (RAG):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å®ç°åŸºæœ¬æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ­¥éª¤ï¼š
- en: '**Determine the type of documents youâ€™ll be using**: The best types are documents
    from which we can get clean and unobscured text. PDFs can be problematic because
    they are designed for printing, not for extracting sensible text. To work with
    PDFs, we should get the source document or use tools to handle it.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¡®å®šä½ å°†ä½¿ç”¨çš„æ–‡æ¡£ç±»å‹**ï¼šæœ€ä½³ç±»å‹æ˜¯æˆ‘ä»¬å¯ä»¥ä»ä¸­è·å¾—å¹²å‡€ä¸”æœªè¢«é®æŒ¡çš„æ–‡æœ¬çš„æ–‡æ¡£ã€‚PDFå¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸ºæ‰“å°è€Œè®¾è®¡çš„ï¼Œè€Œä¸æ˜¯ä¸ºäº†æå–æœ‰æ„ä¹‰çš„æ–‡æœ¬ã€‚ä¸ºäº†å¤„ç†PDFï¼Œæˆ‘ä»¬åº”è¯¥è·å–æºæ–‡æ¡£æˆ–ä½¿ç”¨å·¥å…·æ¥å¤„ç†å®ƒã€‚'
- en: '**Chunk the text**: We canâ€™t store the text as one long stream because of context
    size limitations and the potential for confusion. Chunking involves splitting
    the text into smaller pieces. Chunk text has many ways, such as character count,
    tokens, words, paragraphs, or sections. It is also possible to overlap chunks.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†æ–‡æœ¬åˆ†å—**ï¼šç”±äºä¸Šä¸‹æ–‡å¤§å°é™åˆ¶å’Œæ··æ·†çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬ä¸èƒ½å°†æ–‡æœ¬å­˜å‚¨ä¸ºä¸€ä¸ªé•¿æµã€‚åˆ†å—æ¶‰åŠå°†æ–‡æœ¬åˆ†æˆæ›´å°çš„éƒ¨åˆ†ã€‚åˆ†å—æ–‡æœ¬æœ‰å¤šç§æ–¹å¼ï¼Œä¾‹å¦‚å­—ç¬¦è®¡æ•°ã€æ ‡è®°ã€å•è¯ã€æ®µè½æˆ–éƒ¨åˆ†ã€‚ä¹Ÿå¯ä»¥é‡å åˆ†å—ã€‚'
- en: '**Create embeddings**: Embeddings are numerical representations of text that
    capture semantic meaning. We create embeddings by passing each chunk of text through
    a particular embedding model. The model outputs a vector, the length of which
    depends on the embedding model used. We should pull one (or more) [embedding models](https://ollama.com/blog/embedding-models)
    from Ollama, to perform this task. Here are some examples of embedding models
    available at Ollama.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºåµŒå…¥**ï¼šåµŒå…¥æ˜¯æ•è·è¯­ä¹‰æ„ä¹‰çš„æ–‡æœ¬çš„æ•°å€¼è¡¨ç¤ºã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªæ–‡æœ¬å—é€šè¿‡ç‰¹å®šçš„åµŒå…¥æ¨¡å‹æ¥åˆ›å»ºåµŒå…¥ã€‚æ¨¡å‹è¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œå…¶é•¿åº¦å–å†³äºæ‰€ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ã€‚æˆ‘ä»¬åº”è¯¥ä»Ollamaæ‹‰å–ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰[åµŒå…¥æ¨¡å‹](https://ollama.com/blog/embedding-models)æ¥æ‰§è¡Œæ­¤ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯Ollamaå¯ç”¨çš„åµŒå…¥æ¨¡å‹çš„ä¸€äº›ç¤ºä¾‹ã€‚'
- en: '| Model | Parameter Size | Embedding Size |'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| æ¨¡å‹ | å‚æ•°å¤§å° | åµŒå…¥å¤§å° |'
- en: '| --- | --- | --- |'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| mxbai-embed-large | 334M | 1024 |'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| mxbai-embed-large | 334M | 1024 |'
- en: '| nomic-embed-text | 137M | 768 |'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| nomic-embed-text | 137M | 768 |'
- en: '| all-minilm | 23M | 384 |'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| all-minilm | 23M | 384 |'
- en: Generally, larger embedding sizes capture more nuanced information about the
    input. Still, they also require more computational resources to process, and a
    higher number of parameters should increase the latency (but also the quality
    of the response).
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæ›´å¤§çš„åµŒå…¥å¤§å°å¯ä»¥æ•è·å…³äºè¾“å…¥çš„æ›´ç»†å¾®çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºæ¥å¤„ç†ï¼Œå¹¶ä¸”å‚æ•°æ•°é‡çš„å¢åŠ åº”è¯¥ä¼šå¢åŠ å»¶è¿Ÿï¼ˆä½†ä¹Ÿä¼šæé«˜å“åº”çš„è´¨é‡ï¼‰ã€‚
- en: '**Store the chunks and embeddings in a vector database**: We will need a way
    to efficiently find the most relevant chunks of text for a given prompt, which
    is where a vector database comes in. We will use [Chromadb](https://www.trychroma.com/),
    an AI-native open-source vector database, which simplifies building RAGs by creating
    knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source
    text for each chunk are stored.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†ç‰‡æ®µå’ŒåµŒå…¥å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­**ï¼šæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥é«˜æ•ˆåœ°æ‰¾åˆ°ç»™å®šæç¤ºä¸­æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œè¿™å°±æ˜¯å‘é‡æ•°æ®åº“çš„ä½œç”¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[Chromadb](https://www.trychroma.com/)ï¼Œè¿™æ˜¯ä¸€ä¸ªAIåŸç”Ÿå¼€æºå‘é‡æ•°æ®åº“ï¼Œé€šè¿‡åˆ›å»ºçŸ¥è¯†ã€äº‹å®å’ŒæŠ€èƒ½æ’ä»¶ç®€åŒ–äº†RAGçš„æ„å»ºã€‚æ¯ä¸ªç‰‡æ®µçš„åµŒå…¥å’Œæºæ–‡æœ¬éƒ½å­˜å‚¨åœ¨å…¶ä¸­ã€‚'
- en: '**Build the prompt**: When we have a question, we create an embedding and query
    the vector database for the most similar chunks. Then, we select the top few results
    and include their text in the prompt.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ„å»ºæç¤º**ï¼šå½“æˆ‘ä»¬æœ‰ä¸€ä¸ªé—®é¢˜æ—¶ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåµŒå…¥å¹¶æŸ¥è¯¢å‘é‡æ•°æ®åº“ä»¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç‰‡æ®µã€‚ç„¶åï¼Œæˆ‘ä»¬é€‰æ‹©å‰å‡ ä¸ªç»“æœå¹¶å°†å®ƒä»¬çš„æ–‡æœ¬åŒ…å«åœ¨æç¤ºä¸­ã€‚'
- en: The goal of RAG is to provide the model with the most relevant information from
    our documents, allowing it to generate more accurate and informative responses.
    So, letâ€™s implement a simple example of an SLM incorporating a particular set
    of facts about bees (â€œBee Factsâ€).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: RAGçš„ç›®æ ‡æ˜¯ä¸ºæ¨¡å‹æä¾›æˆ‘ä»¬æ–‡æ¡£ä¸­æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®å’Œæ›´æœ‰ä¿¡æ¯é‡çš„å“åº”ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œå°†å…³äºèœœèœ‚çš„ç‰¹å®šäº‹å®é›†ï¼ˆâ€œèœœèœ‚äº‹å®â€ï¼‰çº³å…¥SLMä¸­ã€‚
- en: 'Inside the `ollama` env, enter the command in the terminal for Chromadb installation:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`ollama`ç¯å¢ƒä¸­ï¼Œåœ¨ç»ˆç«¯ä¸­è¾“å…¥Chromadbå®‰è£…å‘½ä»¤ï¼š
- en: '[PRE50]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Letâ€™s pull an intermediary embedding model, `nomic-embed-text`
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ‹‰å–ä¸€ä¸ªä¸­é—´åµŒå…¥æ¨¡å‹ï¼Œ`nomic-embed-text`
- en: '[PRE51]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'And create a working directory:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶åˆ›å»ºä¸€ä¸ªå·¥ä½œç›®å½•ï¼š
- en: '[PRE52]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Letâ€™s create a new Jupyter notebook, [40-RAG-simple-bee](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb)
    for some exploration:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„Jupyterç¬”è®°æœ¬ï¼Œ[40-RAG-simple-bee](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb)æ¥è¿›è¡Œä¸€äº›æ¢ç´¢ï¼š
- en: 'Import the needed libraries:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ‰€éœ€çš„åº“ï¼š
- en: '[PRE53]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'And define aor models:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰aoræ¨¡å‹ï¼š
- en: '[PRE54]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Initially, a knowledge base about bee facts should be created. This involves
    collecting relevant documents and converting them into vector embeddings. These
    embeddings are then stored in a vector database, allowing for efficient similarity
    searches later. Enter with the â€œdocument,â€ a base of â€œbee factsâ€ as a list:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåº”è¯¥åˆ›å»ºä¸€ä¸ªå…³äºèœœèœ‚äº‹å®çš„çŸ¥è¯†åº“ã€‚è¿™æ¶‰åŠåˆ°æ”¶é›†ç›¸å…³æ–‡æ¡£å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå‘é‡åµŒå…¥ã€‚ç„¶åï¼Œè¿™äº›åµŒå…¥å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œä»¥ä¾¿ä»¥åè¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ã€‚ä»¥â€œæ–‡æ¡£â€çš„å½¢å¼è¾“å…¥â€œèœœèœ‚äº‹å®â€çš„åŸºç¡€åˆ—è¡¨ï¼š
- en: '[PRE55]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We do not need to â€œchunkâ€ the document here because we will use each element
    of the list and a chunk.
  id: totrans-377
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œä¸éœ€è¦â€œåˆ†å—â€æ–‡æ¡£ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä¼šä½¿ç”¨åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ å’Œä¸€ä¸ªç‰‡æ®µã€‚
- en: 'Now, we will create our vector embedding database `bee_facts` and store the
    document in it:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºæˆ‘ä»¬çš„å‘é‡åµŒå…¥æ•°æ®åº“`bee_facts`å¹¶å°†æ–‡æ¡£å­˜å‚¨åœ¨å…¶ä¸­ï¼š
- en: '[PRE56]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now that we have our â€œKnowledge Baseâ€ created, we can start making queries,
    retrieving data from it:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åˆ›å»ºäº†â€œçŸ¥è¯†åº“â€ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è¿›è¡ŒæŸ¥è¯¢ï¼Œä»å…¶ä¸­æ£€ç´¢æ•°æ®ï¼š
- en: '![](../media/file939.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file939.png)'
- en: '**User Query**: The process begins when a user asks a question, such as â€œHow
    many bees are in a colony? Who lays eggs, and how much? How about common pests
    and diseases?â€'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç”¨æˆ·æŸ¥è¯¢**ï¼šè¿™ä¸ªè¿‡ç¨‹ä»ç”¨æˆ·æå‡ºé—®é¢˜å¼€å§‹ï¼Œä¾‹å¦‚ï¼šâ€œä¸€ä¸ªèœ‚ç¾¤ä¸­æœ‰å¤šå°‘èœœèœ‚ï¼Ÿè°äº§åµï¼Œæœ‰å¤šå°‘ï¼Ÿå¸¸è§çš„å®³è™«å’Œç–¾ç—…åˆæ˜¯æ€æ ·çš„ï¼Ÿâ€'
- en: '[PRE57]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '**Query Embedding**: The userâ€™s question is converted into a vector embedding
    using **the same embedding model** used for the knowledge base.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢åµŒå…¥**ï¼šç”¨æˆ·çš„æé—®è¢«è½¬æ¢ä¸ºå‘é‡åµŒå…¥ï¼Œä½¿ç”¨ä¸çŸ¥è¯†åº“ç›¸åŒçš„åµŒå…¥æ¨¡å‹ã€‚'
- en: '[PRE58]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '**Relevant Document Retrieval**: The system searches the knowledge base using
    the query embedding to find the most relevant documents (in this case, the 5 more
    probable). This is done using a similarity search, which compares the query embedding
    to the document embeddings in the database.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›¸å…³æ–‡æ¡£æ£€ç´¢**ï¼šç³»ç»Ÿä½¿ç”¨æŸ¥è¯¢åµŒå…¥åœ¨çŸ¥è¯†åº“ä¸­è¿›è¡Œæœç´¢ï¼Œä»¥æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ5ä¸ªæœ€å¯èƒ½çš„æ–‡æ¡£ï¼‰ã€‚è¿™æ˜¯é€šè¿‡ç›¸ä¼¼æ€§æœç´¢å®Œæˆçš„ï¼Œå®ƒå°†æŸ¥è¯¢åµŒå…¥ä¸æ•°æ®åº“ä¸­çš„æ–‡æ¡£åµŒå…¥è¿›è¡Œæ¯”è¾ƒã€‚'
- en: '[PRE59]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '**Prompt Augmentation**: The retrieved relevant information is combined with
    the original user query to create an augmented prompt. This prompt now contains
    the userâ€™s question and pertinent facts from the knowledge base.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤ºå¢å¼º**ï¼šæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ä¸åŸå§‹ç”¨æˆ·æŸ¥è¯¢ç›¸ç»“åˆï¼Œä»¥åˆ›å»ºå¢å¼ºæç¤ºã€‚ç°åœ¨ï¼Œè¿™ä¸ªæç¤ºåŒ…å«äº†ç”¨æˆ·çš„é—®é¢˜å’ŒçŸ¥è¯†åº“ä¸­çš„ç›¸å…³äº‹å®ã€‚'
- en: '[PRE60]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '**Answer Generation**: The augmented prompt is then fed into a language model,
    in this case, the `llama3.2:3b` model. The model uses this enriched context to
    generate a comprehensive answer. Parameters like temperature, top_k, and top_p
    are set to control the randomness and quality of the generated response.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆç”Ÿæˆ**ï¼šå¢å¼ºåçš„æç¤ºéšåè¢«è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯`llama3.2:3b`æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è¿™ä¸ªä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆå…¨é¢çš„ç­”æ¡ˆã€‚å‚æ•°å¦‚æ¸©åº¦ã€top_kå’Œtop_pè¢«è®¾ç½®ä¸ºæ§åˆ¶ç”Ÿæˆå“åº”çš„éšæœºæ€§å’Œè´¨é‡ã€‚'
- en: '[PRE61]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '**Response Delivery**: Finally, the system returns the generated answer to
    the user.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**å“åº”äº¤ä»˜**ï¼šæœ€åï¼Œç³»ç»Ÿå°†ç”Ÿæˆçš„ç­”æ¡ˆè¿”å›ç»™ç”¨æˆ·ã€‚'
- en: '[PRE62]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Letâ€™s create a function to help answer new questions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¸®åŠ©å›ç­”æ–°é—®é¢˜ï¼š
- en: '[PRE64]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We can now create queries and call the function:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ›å»ºæŸ¥è¯¢å¹¶è°ƒç”¨å‡½æ•°ï¼š
- en: '[PRE65]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'By the way, if the model used supports multiple languages, we can use it (for
    example, Portuguese), even if the dataset was created in English:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€å¥ï¼Œå¦‚æœä½¿ç”¨çš„æ¨¡å‹æ”¯æŒå¤šç§è¯­è¨€ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒï¼ˆä¾‹å¦‚ï¼Œè‘¡è„ç‰™è¯­ï¼‰ï¼Œå³ä½¿æ•°æ®é›†æ˜¯ç”¨è‹±è¯­åˆ›å»ºçš„ï¼š
- en: '[PRE67]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Going Further
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥æ¢ç´¢
- en: The small LLM models tested worked well at the edge, both in text and with images,
    but of course, they had high latency regarding the last one. A combination of
    specific and dedicated models can lead to better results; for example, in real
    cases, an Object Detection model (such as YOLO) can get a general description
    and count of objects on an image that, once passed to an LLM, can help extract
    essential insights and actions.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•çš„å°å‹LLMæ¨¡å‹åœ¨è¾¹ç¼˜ä¸Šè¿è¡Œè‰¯å¥½ï¼Œæ— è®ºæ˜¯æ–‡æœ¬è¿˜æ˜¯å›¾åƒï¼Œä½†å½“ç„¶ï¼Œåœ¨æœ€åä¸€ä¸ªæ–¹é¢æœ‰å¾ˆé«˜çš„å»¶è¿Ÿã€‚ç»“åˆç‰¹å®šå’Œä¸“ç”¨æ¨¡å‹å¯ä»¥å¸¦æ¥æ›´å¥½çš„ç»“æœï¼›ä¾‹å¦‚ï¼Œåœ¨å®é™…æ¡ˆä¾‹ä¸­ï¼Œä¸€ä¸ªç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆå¦‚YOLOï¼‰å¯ä»¥å¯¹å›¾åƒä¸­çš„å¯¹è±¡è¿›è¡Œä¸€èˆ¬æè¿°å’Œè®¡æ•°ï¼Œä¸€æ—¦ä¼ é€’ç»™LLMï¼Œå°±å¯ä»¥å¸®åŠ©æå–å…³é”®è§è§£å’Œè¡ŒåŠ¨ã€‚
- en: According to Avi Baum, CTO at Hailo,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ® Hailo çš„é¦–å¸­æŠ€æœ¯å®˜ Avi Baumï¼Œ
- en: In the vast landscape of artificial intelligence (AI), one of the most intriguing
    journeys has been the evolution of AI on the edge. This journey has taken us from
    classic machine vision to the realms of discriminative AI, enhancive AI, and now,
    the groundbreaking frontier of generative AI. Each step has brought us closer
    to a future where intelligent systems seamlessly integrate with our daily lives,
    offering an immersive experience of not just perception but also creation at the
    palm of our hand.
  id: totrans-406
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¹¿é˜”é¢†åŸŸä¸­ï¼Œæœ€å¼•äººå…¥èƒœçš„æ—…ç¨‹ä¹‹ä¸€å°±æ˜¯è¾¹ç¼˜AIçš„æ¼”å˜ã€‚è¿™ä¸€æ—…ç¨‹å¸¦æˆ‘ä»¬ä»ç»å…¸çš„æœºå™¨è§†è§‰é¢†åŸŸèµ°å‘äº†åˆ¤åˆ«æ€§AIã€å¢å¼ºæ€§AIï¼Œç°åœ¨ï¼Œæ˜¯çªç ´æ€§çš„ç”ŸæˆAIå‰æ²¿ã€‚æ¯ä¸€æ­¥éƒ½è®©æˆ‘ä»¬æ›´æ¥è¿‘ä¸€ä¸ªæœªæ¥ï¼Œåœ¨é‚£é‡Œæ™ºèƒ½ç³»ç»Ÿæ— ç¼åœ°èå…¥æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œæä¾›ä¸ä»…åœ¨æ„ŸçŸ¥ä¸Šè€Œä¸”åœ¨æ‰‹æŒä¸­ä¹Ÿèƒ½è¿›è¡Œåˆ›é€ çš„æ²‰æµ¸å¼ä½“éªŒã€‚
- en: '![](../media/file940.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file940.jpg)'
- en: Summary
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: This lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent
    AI hub capable of running large language models (LLMs) for real-time, on-site
    data analysis and insights using Ollama and Python. The Raspberry Piâ€™s versatility
    and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and
    LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å®éªŒå®¤å±•ç¤ºäº†å¦‚ä½•å°†Raspberry Pi 5è½¬å˜ä¸ºä¸€ä¸ªå¼ºå¤§çš„AIä¸­å¿ƒï¼Œèƒ½å¤Ÿè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåˆ©ç”¨Ollamaå’ŒPythonè¿›è¡Œå®æ—¶ã€ç°åœºæ•°æ®åˆ†æå’Œæ´å¯Ÿã€‚Raspberry
    Piçš„å¤šåŠŸèƒ½å’Œå¼ºå¤§æ€§èƒ½ï¼ŒåŠ ä¸Šè½»é‡çº§LLMså¦‚Llama 3.2å’ŒLLaVa-Phi-3-miniçš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¾¹ç¼˜è®¡ç®—åº”ç”¨çš„ä¼˜ç§€å¹³å°ã€‚
- en: 'The potential of running LLMs on the edge extends far beyond simple data processing,
    as in this labâ€™s examples. Here are some innovative suggestions for using this
    project:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾¹ç¼˜è¿è¡ŒLLMsçš„æ½œåŠ›è¿œè¿œè¶…å‡ºäº†ç®€å•çš„æ•°æ®å¤„ç†ï¼Œæ­£å¦‚æœ¬å®éªŒå®¤çš„ç¤ºä¾‹æ‰€ç¤ºã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åˆ›æ–°æ€§çš„ä½¿ç”¨è¿™ä¸ªé¡¹ç›®çš„å»ºè®®ï¼š
- en: '**1\. Smart Home Automation**:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. æ™ºèƒ½å®¶å±…è‡ªåŠ¨åŒ–**:'
- en: Integrate SLMs to interpret voice commands or analyze sensor data for intelligent
    home automation. This could include real-time monitoring and control of home devices,
    security systems, and energy management, all processed locally without relying
    on cloud services.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆSLMsä»¥è§£é‡Šè¯­éŸ³å‘½ä»¤æˆ–åˆ†æä¼ æ„Ÿå™¨æ•°æ®ä»¥å®ç°æ™ºèƒ½å®¶å±…è‡ªåŠ¨åŒ–ã€‚è¿™å¯èƒ½åŒ…æ‹¬å¯¹å®¶åº­è®¾å¤‡ã€å®‰å…¨ç³»ç»Ÿå’Œèƒ½æºç®¡ç†çš„å®æ—¶ç›‘æ§å’Œæ§åˆ¶ï¼Œæ‰€æœ‰è¿™äº›å¤„ç†éƒ½åœ¨æœ¬åœ°è¿›è¡Œï¼Œä¸ä¾èµ–äº‘æœåŠ¡ã€‚
- en: '**2\. Field Data Collection and Analysis**:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. åœºåœ°æ•°æ®æ”¶é›†å’Œåˆ†æ**ï¼š'
- en: Deploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection
    and analysis. This can be used in agriculture to monitor crop health, in environmental
    studies for wildlife tracking, or in disaster response for situational awareness
    and resource management.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿œç¨‹æˆ–ç§»åŠ¨è®¾ç½®ä¸­åœ¨Raspberry Piä¸Šéƒ¨ç½²SLMsè¿›è¡Œå®æ—¶æ•°æ®æ”¶é›†å’Œåˆ†æã€‚è¿™å¯ä»¥ç”¨äºå†œä¸šç›‘æµ‹ä½œç‰©å¥åº·ï¼Œåœ¨ç¯å¢ƒç ”ç©¶ä¸­è¿›è¡Œé‡ç”ŸåŠ¨ç‰©è¿½è¸ªï¼Œæˆ–åœ¨ç¾å®³å“åº”ä¸­è¿›è¡Œæ€åŠ¿æ„ŸçŸ¥å’Œèµ„æºç®¡ç†ã€‚
- en: '**3\. Educational Tools**:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. æ•™è‚²å·¥å…·**:'
- en: Create interactive educational tools that leverage SLMs to provide instant feedback,
    language translation, and tutoring. This can be particularly useful in developing
    regions with limited access to advanced technology and internet connectivity.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºåˆ©ç”¨SLMsæä¾›å³æ—¶åé¦ˆã€è¯­è¨€ç¿»è¯‘å’Œè¾…å¯¼çš„äº¤äº’å¼æ•™è‚²å·¥å…·ã€‚è¿™åœ¨æŠ€æœ¯å…ˆè¿›æ€§å’Œäº’è”ç½‘è¿æ¥æœ‰é™çš„å‘è¾¾åœ°åŒºå°¤å…¶æœ‰ç”¨ã€‚
- en: '**4\. Healthcare Applications**:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. åŒ»ç–—ä¿å¥åº”ç”¨**:'
- en: Use SLMs for medical diagnostics and patient monitoring. They can provide real-time
    analysis of symptoms and suggest potential treatments. This can be integrated
    into telemedicine platforms or portable health devices.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨SLMsè¿›è¡ŒåŒ»ç–—è¯Šæ–­å’Œæ‚£è€…ç›‘æµ‹ã€‚å®ƒä»¬å¯ä»¥æä¾›ç—‡çŠ¶çš„å®æ—¶åˆ†æå¹¶å»ºè®®æ½œåœ¨çš„æ²»ç–—æ–¹æ³•ã€‚è¿™å¯ä»¥é›†æˆåˆ°è¿œç¨‹åŒ»ç–—å¹³å°æˆ–ä¾¿æºå¼å¥åº·è®¾å¤‡ä¸­ã€‚
- en: '**5\. Local Business Intelligence**:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. æœ¬åœ°å•†ä¸šæ™ºèƒ½**:'
- en: Implement SLMs in retail or small business environments to analyze customer
    behavior, manage inventory, and optimize operations. The ability to process data
    locally ensures privacy and reduces dependency on external services.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é›¶å”®æˆ–å°å‹å•†ä¸šç¯å¢ƒä¸­å®æ–½SLMsä»¥åˆ†æå®¢æˆ·è¡Œä¸ºã€ç®¡ç†åº“å­˜å’Œä¼˜åŒ–è¿è¥ã€‚æœ¬åœ°å¤„ç†æ•°æ®çš„èƒ½åŠ›ç¡®ä¿äº†éšç§å¹¶å‡å°‘äº†å¯¹å¤–éƒ¨æœåŠ¡çš„ä¾èµ–ã€‚
- en: '**6\. Industrial IoT**:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**6. å·¥ä¸šç‰©è”ç½‘**:'
- en: Integrate SLMs into industrial IoT systems for predictive maintenance, quality
    control, and process optimization. The Raspberry Pi can serve as a localized data
    processing unit, reducing latency and improving the reliability of automated systems.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†SLMsé›†æˆåˆ°å·¥ä¸šç‰©è”ç½‘ç³»ç»Ÿä¸­ï¼Œç”¨äºé¢„æµ‹æ€§ç»´æŠ¤ã€è´¨é‡æ§åˆ¶å’Œå·¥ä½œæµç¨‹ä¼˜åŒ–ã€‚æ ‘è“æ´¾å¯ä»¥ä½œä¸ºæœ¬åœ°æ•°æ®å¤„ç†å•å…ƒï¼Œå‡å°‘å»¶è¿Ÿå¹¶æé«˜è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¯é æ€§ã€‚
- en: '**7\. Autonomous Vehicles**:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**7. è‡ªä¸»è½¦è¾†**:'
- en: Use SLMs to process sensory data from autonomous vehicles, enabling real-time
    decision-making and navigation. This can be applied to drones, robots, and self-driving
    cars for enhanced autonomy and safety.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨SLMså¤„ç†æ¥è‡ªè‡ªä¸»è½¦è¾†çš„æ„Ÿå®˜æ•°æ®ï¼Œå®ç°å®æ—¶å†³ç­–å’Œå¯¼èˆªã€‚è¿™å¯ä»¥åº”ç”¨äºæ— äººæœºã€æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä»¥å¢å¼ºè‡ªä¸»æ€§å’Œå®‰å…¨æ€§ã€‚
- en: '**8\. Cultural Heritage and Tourism**:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**8. æ–‡åŒ–é—äº§å’Œæ—…æ¸¸**:'
- en: Implement SLMs to provide interactive and informative cultural heritage sites
    and museum guides. Visitors can use these systems to get real-time information
    and insights, enhancing their experience without internet connectivity.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®æ–½SLMsä»¥æä¾›äº’åŠ¨å’Œå¯Œæœ‰ä¿¡æ¯é‡çš„æ–‡åŒ–é—äº§é—å€å’Œåšç‰©é¦†å¯¼æ¸¸ã€‚æ¸¸å®¢å¯ä»¥ä½¿ç”¨è¿™äº›ç³»ç»Ÿè·å–å®æ—¶ä¿¡æ¯å’Œè§è§£ï¼Œå¢å¼ºä»–ä»¬çš„ä½“éªŒï¼Œæ— éœ€äº’è”ç½‘è¿æ¥ã€‚
- en: '**9\. Artistic and Creative Projects**:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**9. è‰ºæœ¯å’Œåˆ›æ„é¡¹ç›®**:'
- en: Use SLMs to analyze and generate creative content, such as music, art, and literature.
    This can foster innovative projects in the creative industries and allow for unique
    interactive experiences in exhibitions and performances.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨SLMsåˆ†æå’Œç”Ÿæˆåˆ›æ„å†…å®¹ï¼Œå¦‚éŸ³ä¹ã€è‰ºæœ¯å’Œæ–‡å­¦ã€‚è¿™å¯ä»¥ä¿ƒè¿›åˆ›æ„äº§ä¸šä¸­çš„åˆ›æ–°é¡¹ç›®ï¼Œå¹¶åœ¨å±•è§ˆå’Œè¡¨æ¼”ä¸­æä¾›ç‹¬ç‰¹çš„äº’åŠ¨ä½“éªŒã€‚
- en: '**10\. Customized Assistive Technologies**:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**10. å®šåˆ¶è¾…åŠ©æŠ€æœ¯**:'
- en: Develop assistive technologies for individuals with disabilities, providing
    personalized and adaptive support through real-time text-to-speech, language translation,
    and other accessible tools.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºæ®‹ç–¾äººå£«å¼€å‘è¾…åŠ©æŠ€æœ¯ï¼Œé€šè¿‡å®æ—¶æ–‡æœ¬åˆ°è¯­éŸ³ã€è¯­è¨€ç¿»è¯‘å’Œå…¶ä»–å¯è®¿é—®å·¥å…·æä¾›ä¸ªæ€§åŒ–è‡ªé€‚åº”æ”¯æŒã€‚
- en: Resources
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: '[10-Ollama_Python_Library notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10-Ollama_Python_Libraryç¬”è®°æœ¬](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb)'
- en: '[20-Ollama_Function_Calling notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20-Ollama_Function_Callingç¬”è®°æœ¬](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb)'
- en: '[30-Function_Calling_with_images notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30-Function_Calling_with_imagesç¬”è®°æœ¬](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb)'
- en: '[40-RAG-simple-bee notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40-RAG-simple-beeç¬”è®°æœ¬](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb)'
- en: '[calc_distance_image python script](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/calc_distance_image.py)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[calc_distance_image python è„šæœ¬](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/calc_distance_image.py)'
