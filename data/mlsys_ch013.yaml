- en: AI Frameworks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI框架
- en: '*DALL·E 3 Prompt: Illustration in a rectangular format, designed for a professional
    textbook, where the content spans the entire width. The vibrant chart represents
    training and inference frameworks for ML. Icons for TensorFlow, Keras, PyTorch,
    ONNX, and TensorRT are spread out, filling the entire horizontal space, and aligned
    vertically. Each icon is accompanied by brief annotations detailing their features.
    The lively colors like blues, greens, and oranges highlight the icons and sections
    against a soft gradient background. The distinction between training and inference
    frameworks is accentuated through color-coded sections, with clean lines and modern
    typography maintaining clarity and focus.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：以矩形格式绘制插图，专为专业教科书设计，内容覆盖整个宽度。充满活力的图表展示了机器学习的训练和推理框架。TensorFlow、Keras、PyTorch、ONNX
    和 TensorRT 的图标分布在整个水平空间中，并垂直排列。每个图标旁边都有简要的注释，详细说明其功能。明亮的蓝色、绿色和橙色突出显示图标和部分，背景为柔和的渐变。通过颜色编码的部分强调了训练和推理框架之间的区别，清晰的线条和现代的字体保持了清晰度和焦点。*'
- en: '![](../media/file89.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file89.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why do machine learning frameworks represent the critical abstraction layer
    that determines system scalability, development velocity, and architectural flexibility
    in production AI systems?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么机器学习框架代表了决定生产AI系统中系统可扩展性、开发速度和架构灵活性的关键抽象层？*'
- en: 'Machine learning frameworks serve as the critical abstraction layer that bridges
    theoretical concepts and practical implementation, transforming abstract mathematical
    concepts into efficient, executable code while providing standardized interfaces
    for hardware acceleration, distributed computing, and model deployment. Without
    frameworks, every ML project would require reimplementing core operations like
    automatic differentiation and parallel computation, making large-scale development
    economically infeasible. This abstraction layer enables two crucial capabilities:
    development acceleration through pre-optimized implementations and hardware portability
    across CPUs, GPUs, and specialized accelerators. Framework selection becomes one
    of the most consequential engineering decisions, determining system architecture
    constraints, performance characteristics, and deployment flexibility throughout
    the development lifecycle.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架作为连接理论概念和实际实现的临界抽象层，将抽象的数学概念转化为高效、可执行的代码，同时提供标准化接口以实现硬件加速、分布式计算和模型部署。没有框架，每个ML项目都需要重新实现如自动微分和并行计算等核心操作，使得大规模开发在经济上不可行。这一抽象层使两个关键能力成为可能：通过预优化的实现加速开发和跨CPU、GPU和专用加速器的硬件可移植性。框架选择成为最具影响力的工程决策之一，决定了整个开发生命周期中的系统架构约束、性能特征和部署灵活性。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Trace the evolutionary progression of ML frameworks from numerical computing
    libraries through deep learning platforms to specialized deployment variants
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪机器学习框架从数值计算库到深度学习平台再到专用部署变体的进化过程
- en: Explain the architecture and implementation of computational graphs, automatic
    differentiation, and tensor operations in modern frameworks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释现代框架中计算图、自动微分和张量操作的结构和实现
- en: Compare static and dynamic execution models by analyzing their trade-offs in
    development flexibility, debugging capabilities, and production optimization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分析其在开发灵活性、调试能力和生产优化方面的权衡，比较静态和动态执行模型
- en: Analyze the design philosophies underlying major frameworks (research-first,
    production-first, functional programming) and their impact on system architecture
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析主要框架（研究优先、生产优先、函数式编程）背后的设计理念及其对系统架构的影响
- en: Evaluate framework selection criteria by systematically assessing model requirements,
    hardware constraints, and deployment contexts
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过系统地评估模型需求、硬件约束和部署环境来评估框架选择标准
- en: Design framework selection strategies for specific deployment scenarios including
    cloud, edge, mobile, and microcontroller environments
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为特定部署场景选择设计框架策略，包括云、边缘、移动和微控制器环境
- en: Critique common framework selection fallacies and assess their impact on system
    performance and maintainability
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批判常见的框架选择谬误并评估其对系统性能和可维护性的影响
- en: Framework Abstraction and Necessity
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架抽象和必要性
- en: The transformation of raw computational primitives into machine learning systems
    represents one of the most significant engineering challenges in modern computer
    science. Building upon the data pipelines established in the previous chapter,
    this chapter examines the software infrastructure that enables the efficient implementation
    of machine learning algorithms across diverse computational architectures. While
    the mathematical foundations of machine learning (linear algebra operations, optimization
    algorithms, and gradient computations) are well-established, their efficient realization
    in production systems demands software abstractions that bridge theoretical formulations
    with practical implementation constraints.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始的计算原语转化为机器学习系统代表了现代计算机科学中最具挑战性的工程任务之一。在本章中，基于上一章建立的数据管道，我们考察了支持在多样化的计算架构上高效实现机器学习算法的软件基础设施。尽管机器学习的数学基础（线性代数运算、优化算法和梯度计算）已经确立，但在生产系统中高效实现这些基础需要软件抽象，以将理论公式与实际实施约束相连接。
- en: The computational complexity of modern machine learning algorithms illustrates
    the necessity of these abstractions. Training a contemporary language model involves
    orchestrating billions of floating-point operations across distributed hardware
    configurations, requiring precise coordination of memory hierarchies, communication
    protocols, and numerical precision management. Each algorithmic component, from
    forward propagation through backpropagation, must be decomposed into elementary
    operations that can be mapped to heterogeneous processing units while maintaining
    numerical stability and computational reproducibility. The engineering complexity
    of implementing these systems from basic computational primitives would render
    large-scale machine learning development economically prohibitive for most organizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习算法的计算复杂性说明了这些抽象的必要性。训练一个当代语言模型需要协调分布式硬件配置中的数十亿次浮点运算，这要求精确协调内存层次、通信协议和数值精度管理。从正向传播到反向传播的每个算法组件都必须分解为可以映射到异构处理单元的基本操作，同时保持数值稳定性和计算可重复性。从基本计算原语实现这些系统的工程复杂性将使大多数组织的规模化机器学习开发在经济上变得不可行。
- en: 'This complexity becomes immediately apparent when considering specific implementation
    challenges. Implementing backpropagation for a simple 3-layer multilayer perceptron
    manually requires hundreds of lines of careful calculus and matrix manipulation
    code. A modern framework accomplishes this in a single line: `loss.backward()`.
    Frameworks don’t just make machine learning easier; they make modern deep learning
    *possible* by managing the complexity of gradient computation, hardware optimization,
    and distributed execution across millions of parameters.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑具体的实现挑战时，这种复杂性立即显现出来。手动实现一个简单的三层感知器的反向传播需要数百行细致的微积分和矩阵操作代码。现代框架只需一行代码就能完成：`loss.backward()`。框架不仅使机器学习变得更容易，而且通过管理梯度计算、硬件优化以及数百万参数的分布式执行复杂性，使得现代深度学习*成为可能*。
- en: 'Machine learning frameworks constitute the essential software infrastructure
    that mediates between high-level algorithmic specifications and low-level computational
    implementations. These platforms address the core abstraction problem in computational
    machine learning: enabling algorithmic expressiveness while maintaining computational
    efficiency across diverse hardware architectures. By providing standardized computational
    graphs, automatic differentiation engines, and optimized operator libraries, frameworks
    enable researchers and practitioners to focus on algorithmic innovation rather
    than implementation details. This abstraction layer has proven instrumental in
    accelerating both research discovery and industrial deployment of machine learning
    systems.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架构成了介于高级算法规范和低级计算实现之间的基本软件基础设施。这些平台解决了计算机器学习中的核心抽象问题：在多样化的硬件架构上保持计算效率的同时，实现算法的表达能力。通过提供标准化的计算图、自动微分引擎和优化的算子库，框架使研究人员和实践者能够专注于算法创新，而不是实现细节。这一抽象层已被证明在加速机器学习系统的科研发现和工业部署方面发挥了关键作用。
- en: '***Machine Learning Frameworks*** are software platforms that provide *abstractions*
    and *tools* for the complete ML lifecycle, bridging *application code* with *computational
    infrastructure* through standardized interfaces for model development, training,
    and deployment.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '***机器学习框架***是提供*抽象*和*工具*的软件平台，用于完整的机器学习生命周期，通过标准化的接口将*应用代码*与*计算基础设施*连接起来，以实现模型开发、训练和部署。'
- en: The evolutionary trajectory of machine learning frameworks reflects the broader
    maturation of the field from experimental research to industrial-scale deployment.
    Early computational frameworks addressed primarily the efficient expression of
    mathematical operations, focusing on optimizing linear algebra primitives and
    gradient computations. Contemporary platforms have expanded their scope to encompass
    the complete machine learning development lifecycle, integrating data preprocessing
    pipelines, distributed training orchestration, model versioning systems, and production
    deployment infrastructure. This architectural evolution demonstrates the field’s
    recognition that sustainable machine learning systems require engineering solutions
    that address not merely algorithmic performance, but operational concerns including
    scalability, reliability, maintainability, and reproducibility.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架的进化轨迹反映了该领域从实验研究到工业规模部署的更广泛成熟。早期的计算框架主要解决数学运算的高效表达，专注于优化线性代数原语和梯度计算。当代平台已扩展其范围，涵盖完整的机器学习开发生命周期，包括数据预处理管道、分布式训练编排、模型版本控制系统和产品部署基础设施。这种架构演变展示了该领域认识到可持续的机器学习系统需要解决不仅仅是算法性能，还包括可扩展性、可靠性、可维护性和可重复性等运营问题的工程解决方案。
- en: The architectural design decisions embedded within these frameworks exert profound
    influence on the characteristics and capabilities of machine learning systems
    built upon them. Design choices regarding computational graph representation,
    memory management strategies, parallelization schemes, and hardware abstraction
    layers directly determine system performance, scalability limits, and deployment
    flexibility. These architectural constraints propagate through every development
    phase, from initial research prototyping through production optimization, establishing
    the boundaries within which algorithmic innovations can be practically realized.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架内嵌的架构设计决策对基于它们的机器学习系统的特性和能力产生深远影响。关于计算图表示、内存管理策略、并行化方案和硬件抽象层的设计选择，直接决定了系统性能、可扩展性限制和部署灵活性。这些架构约束贯穿于每个开发阶段，从最初的研究原型到生产优化，确立了算法创新可以实际实现的边界。
- en: This chapter examines machine learning frameworks as both software engineering
    artifacts and enablers of contemporary artificial intelligence systems. We analyze
    the architectural principles governing these platforms, investigate the trade-offs
    that shape their design, and examine their role within the broader ecosystem of
    machine learning infrastructure. Through systematic study of framework evolution,
    architectural patterns, and implementation strategies, students will develop the
    technical understanding necessary to make informed framework selection decisions
    and effectively leverage these abstractions in the design and implementation of
    production machine learning systems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将机器学习框架视为软件工程工件和当代人工智能系统的促进者。我们分析了这些平台的架构原则，调查了塑造其设计的权衡，并检查了它们在更广泛的机器学习基础设施生态系统中的作用。通过系统地研究框架进化、架构模式和实现策略，学生将发展出必要的专业技术理解，以便做出明智的框架选择决策，并在设计和实现生产机器学习系统中有效地利用这些抽象。
- en: Historical Development Trajectory
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史发展轨迹
- en: 'To appreciate how modern frameworks achieved these capabilities, we can trace
    how they evolved from simple mathematical libraries into today’s platforms. The
    evolution of machine learning frameworks mirrors the broader development of artificial
    intelligence and computational capabilities, driven by three key factors: growing
    model complexity, increasing dataset sizes, and diversifying hardware architectures.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解现代框架如何实现这些功能，我们可以追溯它们如何从简单的数学库演变为今天的平台。机器学习框架的进化反映了人工智能和计算能力的更广泛发展，由三个关键因素驱动：模型复杂性的增长、数据集规模的增加和硬件架构的多样化。
- en: These driving forces shaped distinct evolutionary phases that reflect both technological
    advances and changing requirements of the AI community. This section explores
    how frameworks progressed from early numerical computing libraries to modern deep
    learning frameworks. This evolution builds upon the historical context of AI development
    introduced in [Chapter 1](ch007.xhtml#sec-introduction) and demonstrates how software
    infrastructure has enabled the practical realization of the theoretical advances
    in machine learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些驱动力塑造了不同的进化阶段，既反映了技术进步，也反映了人工智能社区需求的变化。本节探讨了框架如何从早期的数值计算库发展到现代深度学习框架。这一演变建立在第一章中介绍的AI发展历史背景之上，并展示了软件基础设施如何使机器学习理论进步的实用化成为可能。
- en: Chronological Framework Development
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 框架发展时间线
- en: The development of machine learning frameworks has been built upon decades of
    foundational work in computational libraries. From the early building blocks of
    BLAS and LAPACK to modern frameworks like TensorFlow, PyTorch, and JAX, this journey
    represents a steady progression toward higher-level abstractions that make machine
    learning more accessible and powerful.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架的发展建立在计算库几十年的基础工作之上。从BLAS和LAPACK的早期构建块到现代框架如TensorFlow、PyTorch和JAX，这一旅程代表了向更高层次抽象的稳步进步，使得机器学习更加易于访问和强大。
- en: The development trajectory becomes clear when examining the relationships between
    these foundational technologies. Looking at [Figure 7.1](ch013.xhtml#fig-mlfm-timeline),
    we can trace how these numerical computing libraries laid the groundwork for modern
    ML development. The mathematical foundations established by BLAS and LAPACK enabled
    the creation of more user-friendly tools like NumPy and SciPy, which in turn set
    the stage for today’s deep learning frameworks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当考察这些基础技术之间的关系时，发展轨迹变得清晰。查看[图7.1](ch013.xhtml#fig-mlfm-timeline)，我们可以追溯这些数值计算库如何为现代机器学习发展奠定基础。BLAS和LAPACK建立的数学基础使得NumPy和SciPy等更用户友好的工具得以创建，这些工具反过来又为今天的深度学习框架奠定了基础。
- en: '![](../media/file90.svg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file90.svg)'
- en: 'Figure 7.1: **Computational Library Evolution**: Modern machine learning frameworks
    build upon decades of numerical computing advancements, transitioning from low-level
    routines like BLAS and LAPACK to high-level abstractions in numpy, scipy, and
    finally to deep learning frameworks such as TensorFlow and PyTorch. This progression
    reflects a shift toward increased developer productivity and accessibility in
    machine learning system development.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：**计算库演变**：现代机器学习框架建立在数十年的数值计算进步之上，从BLAS和LAPACK等低级例程过渡到numpy、scipy等高级抽象，最终到TensorFlow和PyTorch等深度学习框架。这一进步反映了向机器学习系统开发中开发生产力和可访问性增加的转变。
- en: This progression demonstrates how frameworks achieve their capabilities through
    incremental innovation, building computational accessibility upon foundations
    established by their predecessors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种进步展示了框架如何通过渐进式创新实现其功能，在先辈们建立的基础之上构建计算的可访问性。
- en: Foundational Mathematical Computing Infrastructure
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础数学计算基础设施
- en: 'The foundation for modern ML frameworks begins at the core level of computation:
    matrix operations. Machine learning computations are primarily matrix-matrix and
    matrix-vector multiplications because neural networks process data through linear
    transformations[1](#fn1) applied to multidimensional arrays. The Basic Linear
    Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[2](#fn2), developed
    in 1979, provided these essential matrix operations that would become the computational
    backbone of machine learning ([H. T. Kung and Leiserson 1979](ch058.xhtml#ref-kung1979systolic)).
    These low-level operations, when combined and executed, enable the complex calculations
    required for training neural networks and other ML models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习框架的基础始于计算的核心层面：矩阵运算。机器学习计算主要是矩阵-矩阵和矩阵-向量乘法，因为神经网络通过作用于多维数组的线性变换[1](#fn1)来处理数据。1979年开发的[基本线性代数子程序](https://www.netlib.org/blas/)[2](#fn2)（BLAS）提供了这些基本的矩阵运算，这些运算将成为机器学习的计算骨干([H.
    T. Kung and Leiserson 1979](ch058.xhtml#ref-kung1979systolic))。这些低级操作在组合和执行时，能够实现训练神经网络和其他机器学习模型所需的复杂计算。
- en: Building upon BLAS, the Linear Algebra Package ([LAPACK](https://www.netlib.org/lapack/))[3](#fn3)
    emerged in 1992, extending these capabilities with advanced linear algebra operations
    such as matrix decompositions, eigenvalue problems, and linear system solutions.
    This layered approach of building increasingly complex operations from basic matrix
    computations became a defining characteristic of ML frameworks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BLAS，线性代数包([LAPACK](https://www.netlib.org/lapack/))[3](#fn3)于1992年出现，通过高级线性代数运算扩展了这些功能，如矩阵分解、特征值问题和线性系统求解。这种从基本矩阵计算构建越来越复杂运算的分层方法，成为机器学习框架的一个定义性特征。
- en: This foundation of optimized linear algebra operations set the stage for higher-level
    abstractions that would make numerical computing more accessible. The development
    of [NumPy](https://numpy.org/) in 2006 marked an important milestone in this evolution,
    building upon its predecessors Numeric and Numarray to become the primary package
    for numerical computation in Python. NumPy introduced n-dimensional array objects
    and essential mathematical functions, providing an efficient interface to these
    underlying BLAS and LAPACK operations. This abstraction allowed developers to
    work with high-level array operations while maintaining the performance of optimized
    low-level matrix computations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 优化线性代数运算的基础为更高层次的抽象奠定了舞台，使得数值计算更加易于访问。2006年[NumPy](https://numpy.org/)的发展标志着这一进化的一个重要里程碑，它建立在Numeric和Numarray的基础上，成为Python中数值计算的主要包。NumPy引入了多维数组对象和基本数学函数，为这些底层的BLAS和LAPACK操作提供了高效的接口。这种抽象允许开发者使用高级数组运算，同时保持优化低级矩阵计算的性能。
- en: The trend continued with [SciPy](https://scipy.org/), which built upon NumPy’s
    foundations to provide specialized functions for optimization, linear algebra,
    and signal processing, with its first stable release in 2008\. This layered architecture,
    progressing from basic matrix operations to numerical computations, established
    the blueprint for future ML frameworks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种趋势随着[SciPy](https://scipy.org/)的推出而持续，SciPy建立在NumPy的基础上，提供了用于优化、线性代数和信号处理的专用函数，其首个稳定版本发布于2008年。这种分层架构，从基本的矩阵运算到数值计算，为未来的机器学习框架奠定了蓝图。
- en: Early Machine Learning Platform Development
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早期机器学习平台开发
- en: The next evolutionary phase represented a conceptual leap from general numerical
    computing to domain-specific machine learning tools. The transition from numerical
    libraries to dedicated machine learning frameworks marked an important evolution
    in abstraction. While the underlying computations remained rooted in matrix operations,
    frameworks began to encapsulate these operations into higher-level machine learning
    primitives. The University of Waikato introduced Weka in 1993 ([Witten and Frank
    2002](ch058.xhtml#ref-witten2002data)), one of the earliest ML frameworks, which
    abstracted matrix operations into data mining tasks, though it was limited by
    its Java implementation and focus on smaller-scale computations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个进化阶段代表了从通用数值计算到特定领域机器学习工具的概念飞跃。从数值库到专用机器学习框架的转变标志着抽象的重要进化。虽然底层计算仍然根植于矩阵运算，但框架开始将这些运算封装到更高层次的机器学习原语中。怀卡托大学于1993年推出了Weka([Witten
    and Frank 2002](ch058.xhtml#ref-witten2002data))，这是最早的机器学习框架之一，它将矩阵运算抽象为数据挖掘任务，尽管其Java实现和关注小规模计算的限制限制了其发展。
- en: This paradigm shift became evident with [Scikit-learn](https://scikit-learn.org/stable/),
    emerging in 2007 as a significant advancement in machine learning abstraction.
    Building upon the NumPy and SciPy foundation, it transformed basic matrix operations
    into intuitive ML algorithms. For example, what amounts to a series of matrix
    multiplications and gradient computations became a simple `fit()` method call
    in a logistic regression model. This abstraction pattern, hiding complex matrix
    operations behind clean APIs, would become a defining characteristic of modern
    ML frameworks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式转变在[Scikit-learn](https://scikit-learn.org/stable/)的推出中变得明显，Scikit-learn于2007年出现，是机器学习抽象的一个重要进步。它建立在NumPy和SciPy的基础上，将基本的矩阵运算转化为直观的机器学习算法。例如，在逻辑回归模型中，一系列的矩阵乘法和梯度计算变成了简单的`fit()`方法调用。这种抽象模式，在干净的API背后隐藏复杂的矩阵运算，将成为现代机器学习框架的一个定义性特征。
- en: '[Theano](https://github.com/Theano/Theano)[4](#fn4), developed at the Montreal
    Institute for Learning Algorithms (MILA) and appearing in 2007, was a major advancement
    that introduced two revolutionary concepts: computational graphs[5](#fn5) and
    GPU acceleration ([T. T. D. Team et al. 2016](ch058.xhtml#ref-al2016theano)).
    Computational graphs represented mathematical operations as directed graphs, with
    matrix operations as nodes and data flowing between them. This graph-based approach
    allowed for automatic differentiation and optimization of the underlying matrix
    operations. More importantly, it enabled the framework to automatically route
    these operations to GPU hardware, dramatically accelerating matrix computations.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[Theano](https://github.com/Theano/Theano)[4](#fn4)，在蒙特利尔学习算法研究所（MILA）开发并于2007年出现，是一项重大进步，引入了两个革命性的概念：计算图[5](#fn5)和GPU加速（[T.
    T. D. Team等人2016](ch058.xhtml#ref-al2016theano)）。计算图将数学运算表示为有向图，其中矩阵运算作为节点，数据在它们之间流动。这种基于图的方法允许对底层矩阵运算进行自动微分和优化。更重要的是，它使框架能够自动将这些运算路由到GPU硬件，极大地加速了矩阵计算。'
- en: A parallel development track emerged with [Torch7](http://torch.ch/) (the Lua-based
    predecessor to PyTorch), created at NYU in 2002, which took a different approach
    to handling matrix operations. It emphasized immediate execution of operations
    (eager execution[6](#fn6)) and provided an adaptable interface for neural network
    implementations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着2002年在纽约大学创建的基于Lua的先祖[Torch7](http://torch.ch/)（PyTorch的前身），出现了一条并行开发路径，它采取了不同的矩阵运算处理方法。它强调操作的即时执行（即时执行[6](#fn6)）并为神经网络实现提供了可适应的接口。
- en: Torch’s design philosophy of prioritizing developer experience while maintaining
    high performance established design patterns that would later influence frameworks
    like PyTorch. Its architecture demonstrated how to balance high-level abstractions
    with efficient low-level matrix operations, introducing concepts that would prove
    crucial as deep learning complexity increased.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Torch的设计理念是在保持高性能的同时优先考虑开发者体验，这确立了后来影响PyTorch等框架的设计模式。其架构展示了如何平衡高级抽象与高效的低级矩阵运算，引入了随着深度学习复杂性增加而证明至关重要的概念。
- en: Deep Learning Computational Platform Innovation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习计算平台创新
- en: 'The emergence of deep learning created unprecedented computational demands
    that exposed the limitations of existing frameworks. The deep learning revolution
    required a major shift in how frameworks handled matrix operations, primarily
    due to three factors: the massive scale of computations, the complexity of gradient
    calculations through deep networks, and the need for distributed processing. Traditional
    frameworks, designed for classical machine learning algorithms, could not handle
    the billions of matrix operations required for training deep neural networks.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的出现产生了前所未有的计算需求，暴露了现有框架的局限性。深度学习革命要求框架在处理矩阵运算方面进行重大转变，这主要归因于三个因素：计算规模的巨大、通过深度网络的梯度计算的复杂性以及分布式处理的需求。为经典机器学习算法设计的传统框架无法处理训练深度神经网络所需的数十亿次矩阵运算。
- en: This computational challenge sparked innovation in academic research environments
    that would reshape framework development. The foundations for modern deep learning
    frameworks emerged from academic research. The University of Montreal’s [Theano](https://github.com/Theano/Theano),
    released in 2007, established the concepts that would shape future frameworks
    ([Bergstra et al. 2010](ch058.xhtml#ref-bergstra2010theano)). It introduced key
    concepts such as computational graphs for automatic differentiation and GPU acceleration,
    demonstrating how to organize and optimize complex neural network computations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这一计算挑战激发了学术研究环境中的创新，这将重塑框架开发。现代深度学习框架的基础源于学术研究。蒙特利尔大学在2007年发布的[Theano](https://github.com/Theano/Theano)确立了未来框架将采用的概念（[Bergstra等人2010](ch058.xhtml#ref-bergstra2010theano)）。它引入了诸如用于自动微分和GPU加速的计算图等关键概念，展示了如何组织和优化复杂的神经网络计算。
- en: '[Caffe](https://caffe.berkeleyvision.org/), released by UC Berkeley in 2013,
    advanced this evolution by introducing specialized implementations of convolutional
    operations ([Y. Jia et al. 2014](ch058.xhtml#ref-jia2014caffe)). While convolutions
    are mathematically equivalent to specific patterns of matrix multiplication, Caffe
    optimized these patterns specifically for computer vision tasks, demonstrating
    how specialized matrix operation implementations could dramatically improve performance
    for specific network architectures.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Caffe](https://caffe.berkeleyvision.org/)，由加州大学伯克利分校于2013年发布，通过引入卷积操作的专用实现（[Y.
    Jia 等人 2014](ch058.xhtml#ref-jia2014caffe)）推动了这一进化。虽然卷积在数学上等同于特定的矩阵乘法模式，但Caffe针对计算机视觉任务对这些模式进行了优化，展示了专用矩阵操作实现如何能显著提高特定网络架构的性能。'
- en: The next breakthrough came from industry, where computational scale demands
    required new architectural approaches. Google’s [TensorFlow](https://www.tensorflow.org/)[7](#fn7),
    introduced in 2015, revolutionized the field by treating matrix operations as
    part of a distributed computing problem ([Jeffrey Dean and Ghemawat 2008](ch058.xhtml#ref-dean2012large)).
    It represented all computations, from individual matrix multiplications to entire
    neural networks, as a static computational graph[8](#fn8) that could be split
    across multiple devices. This approach enabled training of unprecedented model
    sizes by distributing matrix operations across clusters of computers and specialized
    hardware. TensorFlow’s static graph approach, while initially constraining, allowed
    for aggressive optimization of matrix operations through techniques like kernel
    fusion[9](#fn9) (combining multiple operations into a single kernel for efficiency)
    and memory planning[10](#fn10) (pre-allocating memory for operations).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个突破来自工业界，计算规模的需求要求新的架构方法。谷歌的 [TensorFlow](https://www.tensorflow.org/)[7](#fn7)
    于2015年推出，通过将矩阵操作视为分布式计算问题的一部分，彻底改变了该领域（[Jeffrey Dean 和 Ghemawat 2008](ch058.xhtml#ref-dean2012large)）。它将所有计算，从单个矩阵乘法到整个神经网络，表示为一个静态计算图[8](#fn8)，可以跨多个设备分割。这种方法通过在计算机集群和专用硬件上分配矩阵操作，实现了前所未有的模型大小的训练。TensorFlow的静态图方法虽然最初限制了灵活性，但通过内核融合[9](#fn9)（将多个操作组合成一个内核以提高效率）和内存规划[10](#fn10)（为操作预分配内存）等技术，允许对矩阵操作进行积极的优化。
- en: The deep learning framework ecosystem continued to diversify as distinct organizations
    addressed specific computational challenges. Microsoft’s [CNTK](https://learn.microsoft.com/en-us/cognitive-toolkit/)
    entered the field in 2016, bringing implementations for speech recognition and
    natural language processing tasks ([Seide and Agarwal 2016](ch058.xhtml#ref-seide2016cntk)).
    Its architecture emphasized scalability across distributed systems while maintaining
    efficient computation for sequence-based models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着不同组织解决特定的计算挑战，深度学习框架生态系统继续多样化。微软的 [CNTK](https://learn.microsoft.com/en-us/cognitive-toolkit/)
    于2016年进入该领域，为语音识别和自然语言处理任务提供了实现（[Seide 和 Agarwal 2016](ch058.xhtml#ref-seide2016cntk)）。其架构强调在分布式系统中的可扩展性，同时保持基于序列模型的计算效率。
- en: Simultaneously, Facebook’s [PyTorch](https://pytorch.org/)[11](#fn11), also
    launched in 2016, took a radically different approach to handling matrix computations.
    Instead of static graphs, PyTorch introduced dynamic computational graphs that
    could be modified on the fly ([Paszke et al. 2019](ch058.xhtml#ref-paszke2019pytorch)).
    This dynamic approach, while potentially sacrificing optimization opportunities,
    simplified debugging and analysis of matrix operation flow in their models for
    researchers. PyTorch’s success demonstrated that the ability to introspect and
    modify computations dynamically was equally important as raw performance for research
    applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，Facebook的 [PyTorch](https://pytorch.org/)[11](#fn11)，也于2016年推出，在处理矩阵计算方面采取了截然不同的方法。PyTorch不是使用静态图，而是引入了可以即时修改的动态计算图（[Paszke
    等人 2019](ch058.xhtml#ref-paszke2019pytorch)）。这种动态方法虽然可能牺牲了优化机会，但简化了研究人员对其模型中矩阵操作流程的调试和分析。PyTorch的成功表明，对于研究应用来说，能够动态内省和修改计算的能力与原始性能同样重要。
- en: Framework development continued to expand with Amazon’s [MXNet](https://mxnet.apache.org/),
    which approached the challenge of large-scale matrix operations by focusing on
    memory efficiency and scalability across different hardware configurations. It
    introduced a hybrid approach that combined aspects of both static and dynamic
    graphs, enabling adaptable model development while maintaining aggressive optimization
    of the underlying matrix operations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的发展随着Amazon的[MXNet](https://mxnet.apache.org/)的推出而继续扩展，MXNet通过关注内存效率和跨不同硬件配置的可扩展性来应对大规模矩阵运算的挑战。它引入了一种混合方法，结合了静态和动态图的特点，使得模型开发具有适应性同时保持对底层矩阵运算的激进优化。
- en: These diverse approaches revealed that no single solution could address all
    deep learning requirements, leading to the development of specialized tools. As
    deep learning applications grew more diverse, the need for specialized and higher-level
    abstractions became apparent. [Keras](https://keras.io/) emerged in 2015 to address
    this need, providing a unified interface that could run on top of multiple lower-level
    frameworks ([Chollet et al. 2015](ch058.xhtml#ref-chollet2015keras)). This higher-level
    abstraction approach demonstrated how frameworks could focus on user experience
    while leveraging the computational power of existing systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的方法揭示了没有单一解决方案能够满足所有深度学习需求，这导致了专用工具的发展。随着深度学习应用变得更加多样化，对专用和高级抽象的需求变得明显。[Keras](https://keras.io/)在2015年出现，以解决这一需求，提供了一个可以在多个底层框架之上运行的统一接口([Chollet等人2015](ch058.xhtml#ref-chollet2015keras))。这种高级抽象方法展示了框架如何专注于用户体验同时利用现有系统的计算能力。
- en: Meanwhile, Google’s [JAX](https://github.com/google/jax)[12](#fn12), introduced
    in 2018, brought functional programming principles to deep learning computations,
    enabling new patterns of model development ([Bradbury et al. 2018](ch058.xhtml#ref-jax2018github)).
    [FastAI](https://www.fast.ai/) built upon PyTorch to package common deep learning
    patterns into reusable components, making advanced techniques more accessible
    to practitioners ([J. Howard and Gugger 2020](ch058.xhtml#ref-howard2020fastai)).
    These higher-level frameworks demonstrated how abstraction could simplify development
    while maintaining the performance benefits of their underlying implementations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，2018年引入的Google的[JAX](https://github.com/google/jax)[12](#fn12)，将函数式编程原则引入了深度学习计算，使得新的模型开发模式成为可能([Bradbury等人2018](ch058.xhtml#ref-jax2018github))。[FastAI](https://www.fast.ai/)基于PyTorch构建，将常见的深度学习模式打包成可重用组件，使得高级技术对实践者更加易于获取([J.
    Howard和Gugger 2020](ch058.xhtml#ref-howard2020fastai))。这些高级框架展示了抽象如何简化开发同时保持其底层实现性能的优势。
- en: Hardware-Driven Framework Architecture Evolution
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件驱动的框架架构演变
- en: The evolution of frameworks has been inextricably linked to advances in computational
    hardware, creating a dynamic relationship between software capabilities and hardware
    innovations. Hardware developments have significantly reshaped how frameworks
    implement and optimize matrix operations. The introduction of [NVIDIA’s CUDA platform](https://developer.nvidia.com/cuda-toolkit)[13](#fn13)
    in 2007 marked a critical moment in framework design by enabling general-purpose
    computing on GPUs ([Nickolls et al. 2008](ch058.xhtml#ref-nickolls2008scalable)).
    This was transformative because GPUs excel at parallel matrix operations, offering
    orders of magnitude speedup for the computations in deep learning. While a CPU
    might process matrix elements sequentially, a GPU can process thousands of elements
    simultaneously, significantly changing how frameworks approach computation scheduling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的演变与计算硬件的进步密不可分，形成了软件能力和硬件创新之间的动态关系。硬件的发展显著改变了框架实现和优化矩阵运算的方式。2007年[NVIDIA的CUDA平台](https://developer.nvidia.com/cuda-toolkit)[13](#fn13)的引入，通过在GPU上实现通用计算，标志着框架设计的一个关键时刻([Nickolls等人2008](ch058.xhtml#ref-nickolls2008scalable))。这是具有变革性的，因为GPU擅长并行矩阵运算，为深度学习中的计算提供了数量级的速度提升。虽然CPU可能按顺序处理矩阵元素，但GPU可以同时处理数千个元素，这显著改变了框架处理计算调度的方式。
- en: Modern GPU architectures demonstrate quantifiable efficiency advantages for
    ML workloads. NVIDIA A100 GPUs provide 312 TFLOPS of tensor operations at FP16
    precision with 1.6 TB/s memory bandwidth, compared to typical CPU configurations
    delivering 1-2 TFLOPS with 50-100 GB/s memory bandwidth. These hardware characteristics
    significantly change framework optimization strategies. Frameworks must design
    computational graphs that maximize GPU utilization by ensuring sufficient computational
    intensity (measured in FLOPS per byte transferred) to saturate the available memory
    bandwidth.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU架构在ML工作负载上展示了可量化的效率优势。NVIDIA A100 GPU在FP16精度下提供312 TFLOPS的张量操作，内存带宽为1.6
    TB/s，而典型的CPU配置只能提供1-2 TFLOPS，内存带宽为50-100 GB/s。这些硬件特性显著改变了框架优化策略。框架必须设计计算图，通过确保足够的计算强度（以每字节传输的FLOPS衡量）来饱和可用的内存带宽，从而最大化GPU利用率。
- en: Memory bandwidth optimization becomes critical when frameworks target GPU acceleration.
    The memory bandwidth-to-compute ratio (bytes per FLOP) determines whether operations
    are compute-bound or memory-bound. Matrix multiplication operations with large
    dimensions (typically N×N where N > 1024) achieve high computational intensity
    and become compute-bound, enabling near-peak GPU utilization. However, element-wise
    operations like activation functions frequently become memory-bound, achieving
    only 10-20% of peak performance. Frameworks address this through operator fusion
    techniques, combining memory-bound operations into single kernels that reduce
    memory transfers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当框架针对GPU加速时，内存带宽优化变得至关重要。内存带宽与计算比（每FLOP的字节数）决定了操作是计算受限还是内存受限。具有大维度（通常为N×N，其中N
    > 1024）的矩阵乘法操作具有高计算强度，成为计算受限，从而实现接近峰值GPU利用率。然而，像激活函数这样的逐元素操作通常成为内存受限，只能达到峰值性能的10-20%。框架通过算子融合技术来解决这个问题，将内存受限操作组合成单个内核，以减少内存传输。
- en: Beyond general GPU acceleration, the development of hardware-specific accelerators
    further revolutionized framework design. [Google’s Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[14](#fn14),
    first deployed in 2016, were purpose-built for tensor operations, the essential
    building blocks of deep learning computations. TPUs introduced systolic array[15](#fn15)
    architectures, which are particularly efficient for matrix multiplication and
    convolution operations. This hardware architecture prompted frameworks like TensorFlow
    to develop specialized compilation strategies that could map high-level operations
    directly to TPU instructions, bypassing traditional CPU-oriented optimizations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用的GPU加速之外，针对特定硬件的加速器的发展进一步革命化了框架设计。[谷歌的Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[14](#fn14)，首次于2016年部署，专为张量运算而设计，这是深度学习计算的基本构建块。TPUs引入了脉动阵列[15](#fn15)架构，这些架构特别适用于矩阵乘法和卷积操作。这种硬件架构促使像TensorFlow这样的框架开发出专门的编译策略，可以直接将高级操作映射到TPU指令，绕过传统的以CPU为中心的优化。
- en: TPU architecture demonstrates specialized efficiency gains through quantitative
    metrics. TPU v4 chips achieve 275 TFLOPS of BF16 compute with 1.2 TB/s memory
    bandwidth while consuming 200W power, delivering 1.375 TFLOPS/W power efficiency.
    This represents a 3-5x energy efficiency improvement over contemporary GPUs for
    large matrix operations. However, TPUs optimize specifically for dense matrix
    operations and show reduced efficiency for sparse computations or operations requiring
    complex control flow. Frameworks targeting TPUs must design computational graphs
    that maximize dense matrix operation usage while minimizing data movement between
    on-chip high-bandwidth memory (32 GB at 1.2 TB/s) and off-chip memory.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TPU架构通过定量指标展示了专门的效率提升。TPU v4芯片在1.2 TB/s的内存带宽下实现了275 TFLOPS的BF16计算，同时消耗200W功率，提供1.375
    TFLOPS/W的功率效率。这代表了相对于当代GPU在大矩阵操作上的3-5倍能效提升。然而，TPUs专门优化密集矩阵操作，对于稀疏计算或需要复杂控制流的操作效率降低。针对TPU的框架必须设计计算图，以最大化密集矩阵操作的使用，同时最小化芯片上高带宽内存（32
    GB，1.2 TB/s）和芯片外内存之间的数据移动。
- en: Mobile hardware accelerators, such as [Apple’s Neural Engine (2017)](https://machinelearning.apple.com/research/neural-engine)
    and Qualcomm’s Neural Processing Units, brought new constraints and opportunities
    to framework design. These devices emphasized power efficiency over raw computational
    speed, requiring frameworks to develop new strategies for quantization and operator
    fusion. Mobile frameworks like TensorFlow Lite (more recently rebranded to [LiteRT](https://ai.google.dev/edge/litert))
    and [PyTorch Mobile](https://pytorch.org/mobile/home/) needed to balance model
    accuracy with energy consumption, leading to innovations in how matrix operations
    are scheduled and executed.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 移动硬件加速器，如[苹果的神经引擎（2017）](https://machinelearning.apple.com/research/neural-engine)和高通的神经处理单元，为框架设计带来了新的约束和机遇。这些设备强调的是能效而非原始的计算速度，要求框架开发新的量化策略和算子融合策略。移动框架如TensorFlow
    Lite（最近更名为[LiteRT](https://ai.google.dev/edge/litert)）和[PyTorch Mobile](https://pytorch.org/mobile/home/)需要在模型准确性和能耗之间取得平衡，这导致了矩阵操作调度和执行方式的创新。
- en: Mobile accelerators demonstrate the critical importance of mixed-precision computation
    for energy efficiency. Apple’s Neural Engine in the A17 Pro chip provides 35 TOPS
    (trillion operations per second) of INT8 performance while consuming approximately
    5W, achieving 7.2 TOPS/W efficiency. This represents a 10-15x energy efficiency
    improvement over FP32 computation on the same chip. Frameworks targeting mobile
    hardware must provide automatic mixed-precision policies that determine optimal
    precision for each operation, balancing energy consumption against accuracy degradation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 移动加速器展示了混合精度计算在能效中的关键重要性。苹果A17 Pro芯片中的神经引擎提供了35 TOPS（每秒万亿操作）的INT8性能，同时消耗大约5W的功率，实现了7.2
    TOPS/W的效率。这比同一芯片上FP32计算的能效提高了10-15倍。针对移动硬件的框架必须提供自动混合精度策略，以确定每个操作的优化精度，在能耗和精度下降之间取得平衡。
- en: Sparse computation frameworks address the memory bandwidth limitations of mobile
    hardware. Sparse neural networks can reduce memory traffic by 50-90% for networks
    with structured sparsity patterns, directly improving energy efficiency since
    memory access consumes 10-100x more energy than arithmetic operations on mobile
    processors. Frameworks like Neural Magic’s SparseML automatically generate sparse
    models that maintain accuracy while conforming to hardware sparsity support. Qualcomm’s
    Neural Processing SDK provides specialized kernels for 2:4 structured sparse operations,
    where 2 out of every 4 consecutive weights are zero, enabling 1.5-2x speedup with
    minimal accuracy loss.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏计算框架解决了移动硬件的内存带宽限制问题。稀疏神经网络可以通过减少50-90%的内存流量来优化具有结构化稀疏模式的网络，这直接提高了能效，因为内存访问比移动处理器上的算术操作消耗10-100倍的能量。像Neural
    Magic的SparseML这样的框架可以自动生成保持准确性的稀疏模型，同时符合硬件稀疏支持。高通的Neural Processing SDK提供了针对2:4结构化稀疏操作的专用内核，其中每4个连续权重中有2个为零，从而在最小化精度损失的情况下实现1.5-2倍的速度提升。
- en: The emergence of custom ASIC[16](#fn16) (Application-Specific Integrated Circuit)
    solutions has further diversified the hardware landscape. Companies like [Graphcore](https://www.graphcore.ai/),
    [Cerebras](https://www.cerebras.net/), and [SambaNova](https://sambanova.ai/)
    have developed unique architectures for matrix computation, each with different
    strengths and optimization opportunities. This growth in specialized hardware
    has driven frameworks to adopt more adaptable intermediate representations[17](#fn17)
    of matrix operations, enabling target-specific optimization while maintaining
    a common high-level interface.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定制ASIC[16](#fn16)（专用集成电路）解决方案的出现进一步丰富了硬件格局。像[Graphcore](https://www.graphcore.ai/)、[Cerebras](https://www.cerebras.net/)和[SambaNova](https://sambanova.ai/)这样的公司为矩阵计算开发了独特的架构，每个架构都有不同的优势和优化机会。这种专用硬件的增长推动了框架采用更适应性的矩阵操作中间表示[17](#fn17)，从而在保持通用高级接口的同时实现针对特定目标的优化。
- en: The emergence of reconfigurable hardware added another layer of complexity and
    opportunity. Field Programmable Gate Arrays (FPGAs) introduced yet another dimension
    to framework optimization. Unlike fixed-function ASICs, FPGAs allow for reconfigurable
    circuits that can be optimized for specific matrix operation patterns. Frameworks
    responding to this capability developed just-in-time compilation strategies that
    could generate optimized hardware configurations based on the specific needs of
    a model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可重构硬件的出现增加了另一层复杂性和机会。现场可编程门阵列（FPGAs）为框架优化引入了另一个维度。与固定功能的ASIC不同，FPGAs允许配置可重构的电路，这些电路可以针对特定的矩阵运算模式进行优化。响应这一能力的框架开发了即时编译策略，可以根据模型的具体需求生成优化的硬件配置。
- en: This hardware-driven evolution demonstrates how framework design must constantly
    adapt to leverage new computational capabilities. Having traced how frameworks
    evolved from simple numerical libraries to platforms driven by hardware innovations,
    we now turn to understanding the core concepts that enable modern frameworks to
    manage this computational complexity. These key concepts (computational graphs,
    execution models, and system architectures) form the foundation upon which all
    framework capabilities are built.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种由硬件驱动的演变展示了框架设计如何必须不断适应以利用新的计算能力。在追踪了框架如何从简单的数值库发展到由硬件创新驱动的平台之后，我们现在转向理解使现代框架能够管理这种计算复杂性的核心概念。这些关键概念（计算图、执行模型和系统架构）构成了所有框架能力建立的基础。
- en: Fundamental Concepts
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本概念
- en: 'Modern machine learning frameworks operate through the integration of four
    key layers: Fundamentals, Data Handling, Developer Interface, and Execution and
    Abstraction. These layers function together to provide a structured and efficient
    foundation for model development and deployment, as illustrated in [Figure 7.2](ch013.xhtml#fig-fm_blocks).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习框架通过集成四个关键层来运行：基础、数据处理、开发者接口和执行与抽象。这些层共同作用，为模型开发和部署提供了一个结构化和高效的基石，如图[图7.2](ch013.xhtml#fig-fm_blocks)所示。
- en: '![](../media/file91.svg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file91.svg)'
- en: 'Figure 7.2: **Framework Layer Interaction**: Modern machine learning frameworks
    organize functionality into distinct layers (fundamentals, data handling, developer
    interface, and execution & abstraction) that collaborate to streamline model building
    and deployment. This layered architecture enables modularity and allows developers
    to focus on specific aspects of the machine learning workflow without needing
    to manage low-level infrastructure.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：**框架层交互**：现代机器学习框架将功能组织成不同的层（基础、数据处理、开发者接口和执行与抽象），这些层协同工作以简化模型构建和部署。这种分层架构实现了模块化，并允许开发者专注于机器学习工作流程的特定方面，而无需管理底层基础设施。
- en: The Fundamentals layer establishes the structural basis of these frameworks
    through computational graphs. These graphs use the directed acyclic graph (DAG)
    representation, enabling automatic differentiation and optimization. By organizing
    operations and data dependencies, computational graphs provide the framework with
    the ability to distribute workloads and execute computations across a variety
    of hardware platforms.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基础层通过计算图建立了这些框架的结构基础。这些图使用有向无环图（DAG）表示，实现了自动微分和优化。通过组织操作和数据依赖关系，计算图为框架提供了在多种硬件平台上分配工作负载和执行计算的能力。
- en: Building upon this structural foundation, the Data Handling layer manages numerical
    data and parameters essential for machine learning workflows. Central to this
    layer are specialized data structures, such as tensors, which handle high-dimensional
    arrays while optimizing memory usage and device placement. Memory management and
    data movement strategies ensure that computational workloads are executed effectively,
    particularly in environments with diverse or limited hardware resources.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结构基础之上，数据处理层管理着机器学习工作流程中必不可少的数值数据和参数。该层的关键是专门的数据结构，例如张量，它们在处理高维数组的同时优化内存使用和设备放置。内存管理和数据移动策略确保计算工作负载能够有效执行，尤其是在硬件资源多样或有限的环境中。
- en: The Developer Interface layer provides the tools and abstractions through which
    users interact with the framework. Programming models allow developers to define
    machine learning algorithms in a manner suited to their specific needs. These
    are categorized as either imperative or symbolic. Imperative models offer flexibility
    and ease of debugging, while symbolic models prioritize performance and deployment
    efficiency. Execution models further shape this interaction by defining whether
    computations are carried out eagerly (immediately) or as pre-optimized static
    graphs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者接口层提供了用户与框架交互的工具和抽象。编程模型允许开发者以适合其特定需求的方式定义机器学习算法。这些模型分为命令式或符号式。命令式模型提供灵活性和易于调试，而符号模型则优先考虑性能和部署效率。执行模型通过定义计算是立即执行（急切执行）还是作为预优化的静态图来进一步塑造这种交互。
- en: At the bottom of this architectural stack, the Execution and Abstraction layer
    transforms these high-level representations into efficient hardware-executable
    operations. Core operations, encompassing everything from basic linear algebra
    to complex neural network layers, are optimized for diverse hardware platforms.
    This layer also includes mechanisms for allocating resources and managing memory
    dynamically, ensuring scalable performance in both training and inference settings.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构堆栈的底部，执行和抽象层将这些高级表示转换为高效的硬件可执行操作。核心操作包括从基本的线性代数到复杂的神经网络层的一切，针对不同的硬件平台进行了优化。这一层还包括分配资源和动态管理内存的机制，确保在训练和推理设置中都具有可扩展的性能。
- en: These four layers work together through carefully designed interfaces and dependencies,
    creating a cohesive system that balances usability with performance. Understanding
    these interconnected layers is essential for leveraging machine learning frameworks
    effectively. Each layer plays a distinct yet interdependent role in facilitating
    experimentation, optimization, and deployment. By mastering these concepts, practitioners
    can make informed decisions about resource utilization, scaling strategies, and
    the suitability of specific frameworks for various tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个层通过精心设计的接口和依赖关系协同工作，创建了一个平衡可用性和性能的统一系统。理解这些相互关联的层对于有效地利用机器学习框架至关重要。每一层在促进实验、优化和部署中扮演着独特而又相互依赖的角色。通过掌握这些概念，从业者可以就资源利用、扩展策略以及特定框架对各种任务的适用性做出明智的决策。
- en: Our exploration begins with computational graphs because they form the structural
    foundation that enables all other framework capabilities. This core abstraction
    provides the mathematical representation underlying automatic differentiation,
    optimization, and hardware acceleration capabilities that distinguish modern frameworks
    from simple numerical libraries.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索从计算图开始，因为它们构成了所有其他框架功能的结构基础。这个核心抽象提供了自动微分、优化和硬件加速能力的数学表示，这些能力使现代框架与简单的数值库区分开来。
- en: Computational Graphs
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算图
- en: The computational graph is the central abstraction that enables frameworks to
    transform intuitive model descriptions into efficient hardware execution. This
    representation organizes mathematical operations and their dependencies to enable
    automatic optimization, parallelization, and hardware specialization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是框架将直观的模型描述转换为高效硬件执行的中央抽象。这种表示组织数学运算及其依赖关系，以实现自动优化、并行化和硬件专门化。
- en: Computational Graph Fundamentals
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算图基础
- en: Computational graphs emerged as a key abstraction in machine learning frameworks
    to address the growing complexity of deep learning models. As models grew larger
    and more complex, efficient execution across diverse hardware platforms became
    necessary. The computational graph transforms high-level model descriptions into
    efficient low-level hardware execution ([Baydin et al. 2017](ch058.xhtml#ref-baydin2018)),
    representing a machine learning model as a directed acyclic graph[18](#fn18) (DAG)
    where nodes represent operations and edges represent data flow. This DAG abstraction
    enables automatic differentiation and efficient optimization across diverse hardware
    platforms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图作为机器学习框架中的一个关键抽象，用于解决深度学习模型日益增长的复杂性。随着模型变得更大和更复杂，跨不同硬件平台的高效执行变得必要。计算图将高级模型描述转换为高效的低级硬件执行（[Baydin
    等人 2017](ch058.xhtml#ref-baydin2018)），将机器学习模型表示为一个有向无环图[18](#fn18)（DAG），其中节点代表操作，边代表数据流。这种
    DAG 抽象使得在多样化的硬件平台上实现自动微分和高效优化成为可能。
- en: For example, a node might represent a matrix multiplication operation, taking
    two input matrices (or tensors) and producing an output matrix (or tensor). To
    visualize this, consider the simple example in [Figure 7.3](ch013.xhtml#fig-comp-graph).
    The directed acyclic graph computes <semantics><mrow><mi>z</mi><mo>=</mo><mi>x</mi><mo>×</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">z = x \times y</annotation></semantics>, where each
    variable is just numbers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个节点可能代表矩阵乘法操作，接受两个输入矩阵（或张量）并生成一个输出矩阵（或张量）。为了可视化这一点，可以考虑[图 7.3](ch013.xhtml#fig-comp-graph)中的简单示例。这个有向无环图计算
    <semantics><mrow><mi>z</mi><mo>=</mo><mi>x</mi><mo>×</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">z = x \times y</annotation></semantics>，其中每个变量只是数字。
- en: '![](../media/file92.svg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file92.svg)'
- en: 'Figure 7.3: **Computational Graph**: Directed acyclic graphs represent machine
    learning models as a series of interconnected operations, enabling efficient computation
    and automatic differentiation. This example presents a simple computation, <semantics><mrow><mi>z</mi><mo>=</mo><mi>x</mi><mo>×</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">z = x \times y</annotation></semantics>, where nodes
    define operations and edges specify the flow of data between them.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：**计算图**：有向无环图将机器学习模型表示为一系列相互连接的操作，从而实现高效的计算和自动微分。本例展示了一个简单的计算，<semantics><mrow><mi>z</mi><mo>=</mo><mi>x</mi><mo>×</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">z = x \times y</annotation></semantics>，其中节点定义操作，边指定它们之间的数据流。
- en: This simple example illustrates the fundamental principle, but real machine
    learning models require much more complex graph structures. As shown in [Figure 7.4](ch013.xhtml#fig-mlfm-comp-graph),
    the structure of the computation graph involves defining interconnected layers,
    such as convolution, activation, pooling, and normalization, which are optimized
    before execution. The figure also demonstrates key system-level interactions,
    including memory management and device placement, showing how the static graph
    approach enables complete pre-execution analysis and resource allocation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单示例说明了基本原理，但真实的机器学习模型需要更复杂的图结构。如图 7.4[图 7.4](ch013.xhtml#fig-mlfm-comp-graph)所示，计算图的结构涉及定义相互连接的层，如卷积、激活、池化和归一化，这些层在执行前进行优化。该图还展示了关键的系统级交互，包括内存管理和设备放置，展示了静态图方法如何实现完整的预执行分析和资源分配。
- en: '![](../media/file93.svg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file93.svg)'
- en: 'Figure 7.4: **Computation Graph**: This diagram represents a computation as
    a directed acyclic graph, where nodes denote variables and edges represent operations.
    By expressing computations in this form, systems can efficiently perform automatic
    differentiation, which is essential for training machine learning models through
    gradient-based optimization, and optimize resource allocation before execution.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：**计算图**：此图表示计算为一个有向无环图，其中节点表示变量，边表示操作。通过以这种形式表达计算，系统可以高效地执行自动微分，这对于通过基于梯度的优化训练机器学习模型至关重要，并在执行前优化资源分配。
- en: Layers and Tensors
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 层和张量
- en: 'Modern machine learning frameworks implement neural network computations through
    two key abstractions: layers and tensors. Layers represent computational units
    that perform operations like convolution, pooling, or dense transformations. Each
    layer maintains internal states, including weights and biases, that evolve during
    model training. When data flows through these layers, it takes the form of tensors,
    immutable mathematical objects that hold and transmit numerical values.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习框架通过两个关键抽象实现神经网络计算：层和张量。层代表执行卷积、池化或密集变换等操作的计算单元。每个层在模型训练过程中保持内部状态，包括权重和偏差。当数据通过这些层流动时，它以张量的形式存在，张量是不可变的数学对象，用于存储和传输数值。
- en: 'The relationship between layers and tensors mirrors the distinction between
    operations and data in traditional programming. A layer defines how to transform
    input tensors into output tensors, much like a function defines how to transform
    its inputs into outputs. However, layers add an extra dimension: they maintain
    and update internal parameters during training. For example, a convolutional layer
    not only specifies how to perform convolution operations but also learns and stores
    the optimal convolution filters for a given task.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 层与张量之间的关系反映了传统编程中操作与数据之间的区别。一个层定义了如何将输入张量转换为输出张量，就像一个函数定义了如何将其输入转换为输出一样。然而，层增加了一个额外的维度：它们在训练过程中维护和更新内部参数。例如，卷积层不仅指定了如何执行卷积操作，而且还学习并存储了给定任务的优化卷积滤波器。
- en: This abstraction becomes particularly powerful when frameworks automate the
    graph construction process. When a developer writes `tf.keras.layers.Conv2D`,
    the framework constructs the necessary graph nodes for convolution operations,
    parameter management, and data flow, shielding developers from implementation
    complexities.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当框架自动化图构建过程时，这种抽象变得特别强大。当开发者编写`tf.keras.layers.Conv2D`时，框架构建必要的图节点以进行卷积操作、参数管理和数据流，从而屏蔽开发者的实现复杂性。
- en: Neural Network Construction
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 神经网络构建
- en: The power of computational graphs extends beyond basic layer operations. Activation
    functions, essential for introducing non-linearity in neural networks, become
    nodes in the graph. Functions like ReLU, sigmoid, and tanh transform the output
    tensors of layers, enabling networks to approximate complex mathematical functions.
    Frameworks provide optimized implementations of these activation functions, allowing
    developers to experiment with different non-linearities without worrying about
    implementation details.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图的力量不仅限于基本的层操作。激活函数，对于在神经网络中引入非线性至关重要，成为图中的节点。像ReLU、sigmoid和tanh这样的函数将层的输出张量进行转换，使网络能够近似复杂的数学函数。框架提供了这些激活函数的优化实现，使得开发者可以尝试不同的非线性，而无需担心实现细节。
- en: Modern frameworks extend this modular approach by providing complete model architectures
    as pre-configured computational graphs. Models like ResNet and MobileNet come
    ready to use, allowing developers to customize specific layers and leverage transfer
    learning from pre-trained weights.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架通过提供完整的模型架构作为预配置的计算图来扩展这种模块化方法。像ResNet和MobileNet这样的模型可以直接使用，允许开发者自定义特定层并利用预训练权重的迁移学习。
- en: System-Level Consequences
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统级影响
- en: Using the computational graph abstraction established earlier, frameworks can
    analyze and optimize entire computations before execution begins. The explicit
    representation of data dependencies enables automatic differentiation for gradient-based
    optimization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前建立的计算图抽象，框架可以在执行开始之前分析和优化整个计算。数据的显式依赖表示使得基于梯度的优化可以进行自动微分。
- en: Beyond optimization capabilities, this graph structure also provides flexibility
    in execution. The same model definition can run efficiently across different hardware
    platforms, from CPUs to GPUs to specialized accelerators. The framework handles
    the complexity of mapping operations to specific hardware capabilities, optimizing
    memory usage, and coordinating parallel execution. The graph structure also enables
    model serialization, allowing trained models to be saved, shared, and deployed
    across different environments.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了优化能力之外，这种图结构还提供了执行上的灵活性。相同的模型定义可以在不同的硬件平台上高效运行，从CPU到GPU再到专门的加速器。框架处理将操作映射到特定硬件能力的复杂性，优化内存使用，并协调并行执行。图结构还使模型序列化成为可能，允许训练好的模型在不同的环境中保存、共享和部署。
- en: These system benefits distinguish computational graphs from simpler visualization
    tools. While neural network diagrams help visualize model architecture, computational
    graphs serve a deeper purpose. They provide the precise mathematical representation
    needed to transform intuitive model design into efficient execution. Understanding
    this representation reveals how frameworks transform high-level model descriptions
    into optimized, hardware-specific implementations, making modern deep learning
    practical at scale.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统优势将计算图与更简单的可视化工具区分开来。虽然神经网络图有助于可视化模型架构，但计算图有更深层次的目的。它们提供了将直观模型设计转化为高效执行所需的精确数学表示。理解这种表示揭示了框架如何将高级模型描述转化为优化、针对不同硬件实现的实现，使得现代深度学习在规模上变得可行。
- en: It is important to differentiate computational graphs from neural network diagrams,
    such as those for multilayer perceptrons (MLPs), which depict nodes and layers.
    Neural network diagrams visualize the architecture and flow of data through nodes
    and layers, providing an intuitive understanding of the model’s structure. In
    contrast, computational graphs provide a low-level representation of the underlying
    mathematical operations and data dependencies required to implement and train
    these networks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 区分计算图和神经网络图（如多层感知器（MLPs）的图）是很重要的，这些图描绘了节点和层。神经网络图可视化了通过节点和层的架构和数据流，提供了对模型结构的直观理解。相比之下，计算图提供了实现和训练这些网络所需的底层数学运算和数据依赖的低级表示。
- en: These representational capabilities have far-reaching implications for framework
    design and performance. From a systems perspective, computational graphs provide
    several key capabilities that influence the entire machine learning pipeline.
    They enable automatic differentiation, which we will examine next, provide clear
    structure for analyzing data dependencies and potential parallelism, and serve
    as an intermediate representation that can be optimized and transformed for different
    hardware targets. However, the power of computational graphs depends critically
    on how and when they are executed, which brings us to the fundamental distinction
    between static and dynamic graph execution models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示能力对框架设计和性能有深远的影响。从系统角度来看，计算图提供了几个关键能力，这些能力影响整个机器学习流程。它们实现了自动微分，我们将在下一部分进行探讨，为分析数据依赖和潜在的并行性提供了清晰的架构，并作为可以针对不同硬件目标进行优化和转换的中间表示。然而，计算图的力量取决于它们如何以及何时执行，这使我们来到了静态和动态图执行模型之间的基本区别。
- en: Pre-Defined Computational Structure
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预定义的计算结构
- en: Static computation graphs, pioneered by early versions of TensorFlow, implement
    a “define-then-run” execution model. In this approach, developers must specify
    the entire computation graph before execution begins. This architectural choice
    has significant implications for both system performance and development workflow,
    as we will examine later.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 静态计算图，由TensorFlow早期版本首创，实现了“定义后运行”的执行模型。在这种方法中，开发者在执行开始之前必须指定整个计算图。这种架构选择对系统性能和开发工作流程都有重大影响，我们将在后面进行探讨。
- en: A static computation graph implements a clear separation between the definition
    of operations and their execution. During the definition phase, each mathematical
    operation, variable, and data flow connection is explicitly declared and added
    to the graph structure. This graph is a complete specification of the computation
    but does not perform any actual calculations. Instead, the framework constructs
    an internal representation of all operations and their dependencies, which will
    be executed in a subsequent phase.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 静态计算图在操作的定义和它们的执行之间实现了明确的分离。在定义阶段，每个数学运算、变量和数据流连接都被明确声明并添加到图结构中。这个图是对计算的完整规范，但并不执行任何实际计算。相反，框架构建了所有操作及其依赖关系的内部表示，这些将在后续阶段执行。
- en: 'This upfront definition enables powerful system-level optimizations. The framework
    can analyze the complete structure to identify opportunities for operation fusion,
    eliminating unnecessary intermediate results and reducing memory traffic by 3-10x
    through kernel fusion. Memory requirements can be precisely calculated and optimized
    in advance, leading to efficient allocation strategies. Static graphs enable compilation
    frameworks like XLA[19](#fn19) (Accelerated Linear Algebra) to perform aggressive
    optimizations. Graph rewriting can eliminate substantial numbers of redundant
    operations while hardware-specific kernel generation can provide significant speedups
    over generic implementations. This abstraction, while elegant, imposes fundamental
    constraints on expressible computations: static graphs achieve these performance
    gains by sacrificing flexibility in control flow and dynamic computation patterns.
    Once validated, the same computation can be run repeatedly with high confidence
    in its behavior and performance characteristics.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预先定义允许强大的系统级优化。框架可以分析完整的结构，以识别操作融合的机会，通过内核融合消除不必要的中间结果，并通过内核融合将内存流量减少3-10倍。内存需求可以精确计算并在事先优化，从而实现高效的分配策略。静态图使编译框架如
    XLA[19](#fn19)（加速线性代数）能够执行激进的优化。图重写可以消除大量冗余操作，而针对特定硬件的内核生成可以在通用实现之上提供显著的加速。这种抽象虽然优雅，但给可表达的计算施加了基本约束：静态图通过牺牲控制流和动态计算模式的灵活性来实现这些性能提升。一旦验证，相同的计算可以以高信心重复运行，对其行为和性能特征有很高的信心。
- en: '[Figure 7.5](ch013.xhtml#fig-mlfm-static-graph) illustrates this fundamental
    two-phase approach: first, the complete computational graph is constructed and
    optimized; then, during the execution phase, actual data flows through the graph
    to produce results. This separation enables the framework to perform thorough
    analysis and optimization of the entire computation before any execution begins.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.5](ch013.xhtml#fig-mlfm-static-graph) 展示了这种基本的两阶段方法：首先，构建并优化完整的计算图；然后，在执行阶段，实际数据流经图以产生结果。这种分离使得框架能够在任何执行开始之前对整个计算进行彻底的分析和优化。'
- en: '![](../media/file94.svg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file94.svg)'
- en: 'Figure 7.5: **Static Computation Graph**: Machine learning frameworks first
    define computations as a graph of operations, enabling global optimizations like
    operation fusion and efficient resource allocation before any data flows through
    the system. This two-phase approach separates graph construction and optimization
    from execution, improving performance and predictability.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：**静态计算图**：机器学习框架首先将计算定义为操作图，在数据流通过系统之前，实现操作融合和高效资源分配的全局优化。这种两阶段方法将图构建和优化与执行分离，提高了性能和可预测性。
- en: Runtime-Adaptive Computational Structure
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行时自适应计算结构
- en: Dynamic computation graphs, popularized by PyTorch, implement a “define-by-run”
    execution model. This approach constructs the graph during execution, offering
    greater flexibility in model definition and debugging. Unlike static graphs, which
    rely on predefined memory allocation, dynamic graphs allocate memory as operations
    execute, making them susceptible to memory fragmentation in long-running tasks.
    While dynamic graphs trade efficiency for flexibility in expressing control flow,
    they significantly limit compiler optimization opportunities. The inability to
    analyze the complete computation before execution prevents aggressive kernel fusion
    and graph rewriting optimizations that static graphs enable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 动态计算图，由 PyTorch 推广，实现了一种“运行时定义”的执行模型。这种方法在执行过程中构建图，为模型定义和调试提供了更大的灵活性。与依赖于预定义内存分配的静态图不同，动态图在操作执行时分配内存，这使得它们在长时间运行的任务中容易受到内存碎片化的影响。虽然动态图在表达控制流方面以效率换取灵活性，但它们显著限制了编译器的优化机会。在执行前无法分析完整的计算，阻止了静态图所允许的激进内核融合和图重写优化。
- en: As shown in [Figure 7.6](ch013.xhtml#fig-mlfm-dynamic-graph-flow), each operation
    is defined, executed, and completed before moving on to define the next operation.
    This contrasts sharply with static graphs, where all operations must be defined
    upfront. When an operation is defined, it is immediately executed, and its results
    become available for subsequent operations or for inspection during debugging.
    This cycle continues until all operations are complete.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7.6](ch013.xhtml#fig-mlfm-dynamic-graph-flow)所示，每个操作都是在定义、执行和完成之后，才继续定义下一个操作。这与静态图形成鲜明对比，在静态图中，所有操作都必须预先定义。当一个操作被定义时，它立即执行，其结果可供后续操作或调试期间检查使用。这个周期一直持续到所有操作都完成。
- en: '![](../media/file95.svg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file95.svg)'
- en: 'Figure 7.6: **Dynamic Graph Execution**: Machine learning frameworks define
    and execute operations sequentially at runtime, enabling flexible model construction
    and immediate evaluation of intermediate results. This contrasts with static graphs
    which require complete upfront definition, and supports debugging and adaptive
    computation during model training and inference.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：**动态图执行**：机器学习框架在运行时按顺序定义和执行操作，从而实现灵活的模型构建和中间结果的即时评估。这与需要完全预先定义的静态图形成对比，并支持在模型训练和推理期间的调试和自适应计算。
- en: Dynamic graphs excel in scenarios that require conditional execution or dynamic
    control flow, such as when processing variable-length sequences or implementing
    complex branching logic. They provide immediate feedback during development, making
    it easier to identify and fix issues in the computational pipeline. This flexibility
    aligns naturally with imperative programming patterns familiar to most developers,
    allowing them to inspect and modify computations at runtime. These characteristics
    make dynamic graphs particularly valuable during the research and development
    phase of ML projects.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 动态图在需要条件执行或动态控制流的场景中表现出色，例如在处理可变长度序列或实现复杂分支逻辑时。它们在开发过程中提供即时反馈，使得识别和修复计算管道中的问题更加容易。这种灵活性自然地与大多数开发者熟悉的命令式编程模式相吻合，允许他们在运行时检查和修改计算。这些特性使得动态图在机器学习项目的研发阶段特别有价值。
- en: Framework Architecture Trade-offs
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 框架架构权衡
- en: The architectural differences between static and dynamic computational graphs
    have multiple implications for how machine learning systems are designed and executed.
    These implications touch on various aspects of memory usage, device utilization,
    execution optimization, and debugging, all of which play important roles in determining
    the efficiency and scalability of a system. We focus on memory management and
    device placement as foundational concepts, with optimization techniques covered
    in detail in [Chapter 8](ch014.xhtml#sec-ai-training). This allows us to build
    a clear understanding before exploring more complex topics like optimization and
    fault tolerance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 静态和动态计算图之间的架构差异对机器学习系统的设计和执行方式有多个影响。这些影响涉及内存使用、设备利用、执行优化和调试的各个方面，所有这些都对系统的效率和可扩展性起着重要作用。我们重点关注内存管理和设备放置作为基础概念，优化技术将在[第8章](ch014.xhtml#sec-ai-training)中详细讨论。这使我们能够在探索更复杂的话题，如优化和容错性之前，建立清晰的理解。
- en: Memory Management
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存管理
- en: Memory management occurs when executing computational graphs. Static graphs
    benefit from their predefined structure, allowing for precise memory planning
    before execution. Frameworks can calculate memory requirements in advance, optimize
    allocation, and minimize overhead through techniques like memory reuse. This structured
    approach helps ensure consistent performance, particularly in resource-constrained
    environments, such as Mobile and Tiny ML systems. For large models, frameworks
    must efficiently handle memory bandwidth requirements that can range from 100GB/s
    for smaller models to over 1TB/s for large language models with billions of parameters,
    making memory planning critical for achieving optimal throughput.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行计算图时发生内存管理。静态图受益于其预定义的结构，允许在执行前进行精确的内存规划。框架可以预先计算内存需求，通过内存重用等技术优化分配，并最小化开销。这种结构化方法有助于确保一致的性能，尤其是在资源受限的环境中，如移动和微型ML系统。对于大型模型，框架必须高效地处理从100GB/s（较小模型）到超过1TB/s（具有数十亿参数的大型语言模型）的内存带宽需求，这使得内存规划对于实现最佳吞吐量至关重要。
- en: Dynamic graphs, by contrast, allocate memory dynamically as operations are executed.
    While this flexibility is invaluable for handling dynamic control flows or variable
    input sizes, it can result in higher memory overhead and fragmentation. These
    trade-offs are often most apparent during development, where dynamic graphs enable
    rapid iteration and debugging but may require additional optimization for production
    deployment. The dynamic allocation overhead becomes particularly significant when
    memory bandwidth utilization drops below 50% of available capacity due to fragmentation
    and suboptimal access patterns.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，动态图在执行操作时动态分配内存。虽然这种灵活性对于处理动态控制流或可变输入大小非常有价值，但它可能导致更高的内存开销和碎片化。这些权衡在开发期间最为明显，动态图使快速迭代和调试成为可能，但可能需要额外的优化以进行生产部署。当由于碎片化和不理想的访问模式导致内存带宽利用率低于可用容量的50%时，动态分配开销变得尤为重要。
- en: Device Placement
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设备放置
- en: Device placement, the process of assigning operations to hardware resources
    such as CPUs, GPUs, or specialized ASICS like TPUs, is another system-level consideration.
    Static graphs allow for detailed pre-execution analysis, enabling the framework
    to map computationally intensive operations to devices while minimizing communication
    overhead. This capability makes static graphs well-suited for optimizing execution
    on specialized hardware, where performance gains can be significant.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 设备放置，即将操作分配给硬件资源（如CPU、GPU或专门的ASIC如TPU）的过程，是另一个系统级考虑因素。静态图允许进行详细的预执行分析，使框架能够将计算密集型操作映射到设备上，同时最小化通信开销。这种能力使静态图非常适合优化在专用硬件上的执行，其中性能提升可能非常显著。
- en: Dynamic graphs, in contrast, handle device placement at runtime. This allows
    them to adapt to changing conditions, such as hardware availability or workload
    demands. However, the lack of a complete graph structure before execution can
    make it challenging to optimize device utilization fully, potentially leading
    to inefficiencies in large-scale or distributed setups.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，动态图在运行时处理设备放置。这使得它们能够适应不断变化的情况，例如硬件可用性或工作负载需求。然而，在执行之前缺乏完整的图结构可能会使完全优化设备利用率变得具有挑战性，可能导致大规模或分布式设置中的效率低下。
- en: Broader Perspective
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更广阔的视角
- en: The trade-offs between static and dynamic graphs extend well beyond memory and
    device considerations. As shown in [Table 7.1](ch013.xhtml#tbl-mlfm-graphs), these
    architectures influence optimization potential, debugging capabilities, scalability,
    and deployment complexity. These broader implications are explored in detail in
    [Chapter 8](ch014.xhtml#sec-ai-training) for training workflows and [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    for system-level optimizations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 静态图与动态图之间的权衡远不止内存和设备考虑。如[表7.1](ch013.xhtml#tbl-mlfm-graphs)所示，这些架构影响优化潜力、调试能力、可扩展性和部署复杂性。这些更广泛的影响在[第8章](ch014.xhtml#sec-ai-training)中详细探讨，该章节讨论了训练工作流程，以及在[第11章](ch017.xhtml#sec-ai-acceleration)中讨论了系统级优化。
- en: These hybrid solutions aim to provide the flexibility of dynamic graphs during
    development while enabling the performance optimizations of static graphs in production
    environments. The choice between static and dynamic graphs often depends on specific
    project requirements, balancing factors like development speed, production performance,
    and system complexity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些混合解决方案旨在在开发期间提供动态图的灵活性，同时在生产环境中实现静态图的性能优化。静态图与动态图的选择通常取决于具体的项目需求，平衡开发速度、生产性能和系统复杂性等因素。
- en: 'Table 7.1: **Graph Computation Modes**: Static graphs define the entire computation
    upfront, enabling optimization, while dynamic graphs construct the computation
    on-the-fly, offering flexibility for variable-length inputs and control flow.
    This distinction impacts both the efficiency of execution and the ease of model
    development and debugging.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：**图计算模式**：静态图在开始计算前定义整个计算过程，从而实现优化，而动态图在运行时动态构建计算，为可变长度输入和控制流提供灵活性。这种区别影响执行效率和模型开发及调试的易用性。
- en: '| **Aspect** | **Static Graphs** | **Dynamic Graphs** |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **静态图** | **动态图** |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Memory Management** | Precise allocation planning, optimized memory usage
    | Flexible but likely less efficient allocation |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **内存管理** | 精确的分配规划，优化的内存使用 | 灵活但可能效率较低的分配合适'
- en: '| **Optimization Potential** | Comprehensive graph-level optimizations possible
    | Limited to local optimizations due to runtime |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **优化潜力** | 可能进行全面的图级优化 | 由于运行时限制，仅限于局部优化 |'
- en: '| **Hardware Utilization** | Can generate highly optimized hardware-specific
    code | May sacrifice hardware-specific optimizations |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **硬件利用率** | 可以生成高度优化的特定于硬件的代码 | 可能会牺牲特定于硬件的优化 |'
- en: '| **Development Experience** | Requires more upfront planning, harder to debug
    | Better debugging, faster iteration cycles |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **开发体验** | 需要更多的前期规划，更难调试 | 更好的调试，更快的迭代周期 |'
- en: '| **Debugging Workflow** | Framework-specific tools, disconnected stack traces
    | Standard Python debugging (pdb, print, inspect) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **调试工作流程** | 框架特定的工具，堆栈跟踪脱节 | 标准Python调试（pdb、print、inspect） |'
- en: '| **Error Reporting** | Execution-time errors disconnected from definition
    | Intuitive stack traces pointing to exact lines |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **错误报告** | 执行时错误与定义脱节 | 直观的堆栈跟踪指向确切行 |'
- en: '| **Research Velocity** | Slower iteration due to define-then-run requirement
    | Faster prototyping and model experimentation |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **研究速度** | 由于定义后运行的要求而迭代较慢 | 更快的原型设计和模型实验 |'
- en: '| **Runtime Flexibility** | Fixed computation structure | Can adapt to runtime
    conditions |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **运行时灵活性** | 固定的计算结构 | 可以适应运行时条件 |'
- en: '| **Production Performance** | Generally better performance at scale | May
    have overhead from graph construction |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **生产性能** | 在规模上通常有更好的性能 | 可能因图构建而产生开销 |'
- en: '| **Integration with Legacy Code** | More separation between definition and
    execution | Natural integration with imperative code |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **与旧代码集成** | 定义和执行之间有更多的分离 | 与命令式代码自然集成 |'
- en: '| **Memory Overhead** | Lower memory overhead due to planned allocations |
    Higher overhead due to dynamic allocations |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **内存开销** | 由于计划分配而具有较低的内存开销 | 由于动态分配而具有较高的开销 |'
- en: '| **Deployment Complexity** | Simpler deployment due to fixed structure | May
    require additional runtime support |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **部署复杂性** | 由于固定结构而部署更简单 | 可能需要额外的运行时支持 |'
- en: Graph-Based Gradient Computation Implementation
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于图的梯度计算实现
- en: The computational graph serves as more than just an execution plan; it is the
    core data structure that makes reverse-mode automatic differentiation feasible
    and efficient. Understanding this connection reveals how frameworks compute gradients
    through arbitrarily complex neural networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图不仅作为执行计划，而且是使反向模式自动微分可行和高效的核心数据结构。理解这种联系揭示了框架如何通过任意复杂的神经网络计算梯度。
- en: During the forward pass, the framework constructs a computational graph where
    each node represents an operation and stores both the result and the information
    needed to compute gradients. This graph is not just a visualization tool but an
    actual data structure maintained in memory. When `loss.backward()` is called,
    the framework performs a reverse traversal of this graph in reverse topological
    order, systematically applying the chain rule at each node.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传递期间，框架构建一个计算图，其中每个节点代表一个操作，并存储结果以及计算梯度所需的信息。这个图不仅是一个可视化工具，而且是一个实际的数据结构，它在内存中维护。当调用`loss.backward()`时，框架以反向拓扑顺序遍历此图，并在每个节点上系统地应用链式法则。
- en: The key insight is that the graph structure encodes all the dependency relationships
    needed for the chain rule. Each edge in the graph represents a partial derivative,
    and reverse traversal automatically composes these partial derivatives according
    to the chain rule. The forward pass builds the computation history, and the backward
    pass is simply a graph traversal algorithm that accumulates gradients by following
    the recorded dependencies.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的洞察是图结构编码了链式法则所需的所有依赖关系。图中的每条边代表一个偏导数，反向遍历会根据链式法则自动组合这些偏导数。正向传递构建计算历史，反向传递则是一个简单的图遍历算法，通过跟随记录的依赖关系来累积梯度。
- en: This design enables automatic differentiation to scale to networks with millions
    of parameters because the complexity is linear in the number of operations, not
    exponential in the number of variables. The graph structure ensures that each
    gradient computation is performed exactly once and that shared subcomputations
    are properly handled through the dependency tracking built into the graph representation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此设计使自动微分能够扩展到具有数百万个参数的网络，因为其复杂性是操作数的线性关系，而不是变量数的指数关系。图结构确保每个梯度计算恰好执行一次，并且通过图表示中内置的依赖关系跟踪正确处理共享子计算。
- en: Automatic Differentiation
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动微分
- en: 'Machine learning frameworks must solve a core computational challenge: calculating
    derivatives through complex chains of mathematical operations accurately and efficiently.
    This capability enables the training of neural networks by computing how millions
    of parameters require adjustment to improve the model’s performance ([Baydin et
    al. 2017](ch058.xhtml#ref-baydin2018)).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架必须解决一个核心计算挑战：通过复杂的数学运算链准确且高效地计算导数。这种能力使得通过计算数百万个参数需要调整以改进模型性能来训练神经网络成为可能（[Baydin
    等人 2017](ch058.xhtml#ref-baydin2018)）。
- en: '[Listing 7.1](ch013.xhtml#lst-auto_diff_intro) shows a simple computation that
    illustrates this challenge.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.1](ch013.xhtml#lst-auto_diff_intro) 展示了一个简单的计算，说明了这一挑战。'
- en: 'Listing 7.1: **Automatic Differentiation**: Enables efficient computation of
    gradients for complex functions, crucial for optimizing neural network parameters.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1：**自动微分**：使复杂函数的梯度计算高效，这对于优化神经网络参数至关重要。
- en: '[PRE0]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Even in this basic example, computing derivatives manually would require careful
    application of calculus rules - the product rule, the chain rule, and derivatives
    of trigonometric functions. Now imagine scaling this to a neural network with
    millions of operations. This is where automatic differentiation (AD)[20](#fn20)
    becomes essential.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这个基本例子中，手动计算导数也需要仔细应用微积分规则——乘积规则、链式法则和三角函数的导数。现在想象一下将其扩展到具有数百万个操作的神经网络。这就是自动微分（AD）[20](#fn20)
    成为关键的地方。
- en: 'Automatic differentiation calculates derivatives of functions implemented as
    computer programs by decomposing them into elementary operations. In our example,
    AD breaks down `f(x)` into three basic steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分通过将函数分解为基本操作来计算作为计算机程序实现的函数的导数。在我们的例子中，AD 将 `f(x)` 分解为三个基本步骤：
- en: Computing `a = x * x` (squaring)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `a = x * x`（平方）
- en: Computing `b = sin(x)` (sine function)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `b = sin(x)`（正弦函数）
- en: Computing the final product `a * b`
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算最终乘积 `a * b`
- en: 'For each step, AD knows the basic derivative rules:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一步，AD 都知道基本的导数规则：
- en: 'For squaring: `d(x²)/dx = 2x`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于平方：`d(x²)/dx = 2x`
- en: 'For sine: `d(sin(x))/dx = cos(x)`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正弦：`d(sin(x))/dx = cos(x)`
- en: 'For products: `d(uv)/dx = u(dv/dx) + v(du/dx)`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于乘积：`d(uv)/dx = u(dv/dx) + v(du/dx)`
- en: By tracking how these operations combine and systematically applying the chain
    rule, AD computes exact derivatives through the entire computation. When implemented
    in frameworks like PyTorch or TensorFlow, this enables automatic computation of
    gradients through arbitrary neural network architectures, which becomes essential
    for the training algorithms and optimization techniques detailed in [Chapter 8](ch014.xhtml#sec-ai-training).
    This fundamental understanding of how AD decomposes and tracks computations sets
    the foundation for examining its implementation in machine learning frameworks.
    We will explore its mathematical principles, system architecture implications,
    and performance considerations that make modern machine learning possible.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过跟踪这些操作如何组合并系统地应用链式法则，AD 通过整个计算过程计算精确的导数。当在 PyTorch 或 TensorFlow 等框架中实现时，这可以自动计算任意神经网络架构的梯度，这对于第
    8 章中详细介绍的训练算法和优化技术至关重要。这种对 AD 如何分解和跟踪计算的基本理解，为检查其在机器学习框架中的实现奠定了基础。我们将探讨其数学原理、系统架构影响以及使现代机器学习成为可能的性能考虑。
- en: Forward and Reverse Mode Differentiation
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前向和反向模式微分
- en: Automatic differentiation can be implemented using two primary computational
    approaches, each with distinct characteristics in terms of efficiency, memory
    usage, and applicability to different problem types. This section examines forward
    mode and reverse mode automatic differentiation, analyzing their mathematical
    foundations, implementation structures, performance characteristics, and integration
    patterns within machine learning frameworks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分可以使用两种主要的计算方法实现，每种方法在效率、内存使用以及对不同问题类型的适用性方面都有独特的特点。本节将探讨前向模式和反向模式自动微分，分析它们的数学基础、实现结构、性能特征以及在机器学习框架中的集成模式。
- en: Forward Mode
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前向模式
- en: Forward mode automatic differentiation computes derivatives alongside the original
    computation, tracking how changes propagate from input to output. Building on
    the basic AD concepts introduced in [Section 7.3.2](ch013.xhtml#sec-ai-frameworks-automatic-differentiation-e286),
    forward mode mirrors manual derivative computation, making it intuitive to understand
    and implement.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式自动微分在原始计算的同时计算导数，跟踪变化如何从输入传播到输出。在[第7.3.2节](ch013.xhtml#sec-ai-frameworks-automatic-differentiation-e286)中介绍的基本AD概念的基础上，前向模式模仿了手动导数计算，使其易于理解和实现。
- en: Consider our previous example with a slight modification to show how forward
    mode works (see [Listing 7.2](ch013.xhtml#lst-forward_mode_ad)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们之前的例子，进行轻微修改以展示前向模式的工作方式（参见[列表7.2](ch013.xhtml#lst-forward_mode_ad)）。
- en: 'Listing 7.2: **Forward Mode Automatic Differentiation**: Computes derivatives
    alongside function evaluations using the product rule, illustrating how changes
    in inputs propagate to outputs.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2：**前向模式自动微分**：使用乘积法则在函数评估的同时计算导数，说明了输入变化如何传播到输出。
- en: '[PRE1]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Forward mode achieves this systematic derivative computation by augmenting
    each number with its derivative value, creating what mathematicians call a “dual
    number.” The example in [Listing 7.3](ch013.xhtml#lst-forward_mode_dual) shows
    how this works numerically when x = 2.0, the computation tracks both values and
    derivatives:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式通过为每个数字增加其导数值，创建数学家所说的“双数”，从而实现这种系统的导数计算。在[列表7.3](ch013.xhtml#lst-forward_mode_dual)中的例子展示了当x
    = 2.0时，这种计算是如何在数值上工作的：计算跟踪了两个值及其导数：
- en: 'Listing 7.3: **Forward Mode**: The example computes derivatives alongside function
    values using dual numbers, showcasing how to track changes in both the result
    and its rate of change.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3：**前向模式**：该例子使用双数在函数值的同时计算导数，展示了如何跟踪结果及其变化率的变化。
- en: '[PRE2]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Implementation Structure
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 实现结构
- en: Forward mode AD structures computations to track both values and derivatives
    simultaneously through programs. The structure of such computations can be seen
    again in [Listing 7.4](ch013.xhtml#lst-forward_structure), where each intermediate
    operation is made explicit.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式AD通过程序同时跟踪值和导数来结构化计算。这种计算的构成可以在[列表7.4](ch013.xhtml#lst-forward_structure)中再次看到，其中每个中间操作都被明确表示。
- en: 'Listing 7.4: **Forward Mode AD Structure**: Each operation tracks values and
    derivatives simultaneously, highlighting how computations are structured in forward
    mode automatic differentiation.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4：**前向模式AD结构**：每个操作同时跟踪值和导数，突出了前向模式自动微分中计算的构成。
- en: '[PRE3]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When a framework executes this function in forward mode, it augments each computation
    to carry two pieces of information: the value itself and how that value changes
    with respect to the input. This paired movement of value and derivative mirrors
    how we think about rates of change as shown in [Listing 7.5](ch013.xhtml#lst-dual_tracking).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当框架以前向模式执行此函数时，它增强每个计算以携带两份数据：值本身以及该值相对于输入的变化。这种值和导数的配对运动反映了我们如何考虑变化率，如[列表7.5](ch013.xhtml#lst-dual_tracking)所示。
- en: 'Listing 7.5: **Dual Tracking**: Each computation tracks both its value and
    derivative, illustrating how forward mode automatic differentiation works in practice.
    This example helps understand how values and their rates of change are simultaneously
    computed during function evaluation.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5：**双数跟踪**：每个计算同时跟踪其值和导数，说明了前向模式自动微分在实际中的工作方式。这个例子有助于理解在函数评估期间如何同时计算值及其变化率。
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This forward propagation of derivative information happens automatically within
    the framework’s computational machinery. The framework: 1\. Enriches each value
    with derivative information 2\. Transforms each basic operation to handle both
    value and derivative 3\. Propagates this information forward through the computation'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种导数信息的正向传播在框架的计算机制中自动发生。框架：1. 为每个值增加导数信息 2. 将每个基本操作转换为处理值和导数 3. 通过计算将此信息向前传播
- en: The beauty of this approach is that it follows the natural flow of computation
    - as values move forward through the program, their derivatives move with them.
    This makes forward mode particularly well-suited for functions with single inputs
    and multiple outputs, as the derivative information follows the same path as the
    regular computation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它遵循计算的自然流程——随着值通过程序向前移动，它们的导数也随之移动。这使得前向模式特别适合于具有单个输入和多个输出的函数，因为导数信息遵循与常规计算相同的路径。
- en: Performance Characteristics
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 性能特征
- en: Forward mode AD exhibits distinct performance patterns that influence when and
    how frameworks employ it. Understanding these characteristics helps explain why
    frameworks choose different AD approaches for different scenarios.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式AD表现出独特的性能模式，这影响了框架何时以及如何使用它。理解这些特征有助于解释为什么框架为不同的场景选择不同的AD方法。
- en: Forward mode performs one derivative computation alongside each original operation.
    For a function with one input variable, this means roughly doubling the computational
    work - once for the value, once for the derivative. The cost scales linearly with
    the number of operations in the program, making it predictable and manageable
    for simple computations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式在每个原始操作旁边执行一次导数计算。对于一个只有一个输入变量的函数，这意味着计算工作量大约翻倍——一次用于值，一次用于导数。成本与程序中的操作数量成线性关系，这使得它对于简单计算来说是可预测和可管理的。
- en: 'However, consider a neural network layer computing derivatives for matrix multiplication
    between weights and inputs. To compute derivatives with respect to all weights,
    forward mode would require performing the computation once for each weight parameter,
    potentially thousands of times. This reveals an important characteristic: forward
    mode’s efficiency depends on the number of input variables we need derivatives
    for.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑一个神经网络层，它计算权重和输入之间的矩阵乘法的导数。为了计算所有权重的导数，前向模式需要为每个权重参数执行一次计算，可能多达数千次。这揭示了一个重要的特征：前向模式的效率取决于我们需要对多少输入变量求导。
- en: Forward mode’s memory requirements are relatively modest. It needs to store
    the original value, a single derivative value, and temporary results during computation.
    The memory usage stays constant regardless of how complex the computation becomes.
    This predictable memory pattern makes forward mode particularly suitable for embedded
    systems with limited memory, real-time applications requiring consistent memory
    use, and systems where memory bandwidth is a bottleneck.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模式的内存需求相对较小。它需要存储原始值、单个导数值以及在计算过程中的临时结果。无论计算变得多么复杂，内存使用量都保持不变。这种可预测的内存模式使得前向模式特别适合于内存有限的嵌入式系统、需要一致内存使用的实时应用，以及内存带宽成为瓶颈的系统。
- en: This combination of computational scaling with input variables but constant
    memory usage creates specific trade-offs that influence framework design decisions.
    Forward mode shines in scenarios with few inputs but many outputs, where its straightforward
    implementation and predictable resource usage outweigh the computational cost
    of multiple passes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算随输入变量缩放但内存使用量恒定的组合产生了特定的权衡，影响了框架设计决策。前向模式在输入少但输出多的场景中表现出色，其简单实现和可预测的资源使用超过了多次遍历的计算成本。
- en: Use Cases
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 用例
- en: While forward mode automatic differentiation isn’t the primary choice for training
    full neural networks, it plays several important roles in modern machine learning
    frameworks. Its strength lies in scenarios where we need to understand how small
    changes in inputs affect a network’s behavior. Consider a data scientist seeking
    to understand why their model makes certain predictions. They may require analysis
    of how changing a single pixel in an image or a specific feature in their data
    affects the model’s output, as illustrated in [Listing 7.6](ch013.xhtml#lst-image_sensitivity).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向模式自动微分不是训练完整神经网络的首选，但在现代机器学习框架中它扮演着几个重要的角色。它的优势在于我们需要理解输入的微小变化如何影响网络行为的情况下。考虑一个数据科学家试图理解他们的模型为何做出某些预测。他们可能需要分析改变图像中的一个像素或数据中的特定特征如何影响模型输出，如[列表7.6](ch013.xhtml#lst-image_sensitivity)所示。
- en: 'Listing 7.6: **Sensitivity Analysis**: Small changes in input images affect
    a neural network’s predictions through forward mode automatic differentiation
    via This code. Understanding these effects helps in debugging models and improving
    their robustness.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6：**敏感性分析**：输入图像中的微小变化通过前向模式自动微分影响神经网络预测。通过此代码理解这些影响有助于调试模型并提高其鲁棒性。
- en: '[PRE5]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As the computation moves through each layer, forward mode carries both values
    and derivatives, making it straightforward to see how input perturbations ripple
    through to the final prediction. For each operation, we can track exactly how
    small changes propagate forward.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算通过每一层进行时，前向模式携带值和导数，这使得我们可以直观地看到输入扰动如何传播到最终的预测。对于每个操作，我们可以精确地追踪微小变化是如何向前传播的。
- en: Neural network interpretation presents another compelling application. When
    researchers generate saliency maps or attribution scores, they typically compute
    how each input element influences the output as shown in [Listing 7.7](ch013.xhtml#lst-feature_importance).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络解释展示了另一个引人入胜的应用。当研究人员生成显著性图或归因分数时，他们通常会计算每个输入元素如何影响输出，如[列表7.7](ch013.xhtml#lst-feature_importance)所示。
- en: 'Listing 7.7: **Forward Mode AD**: Efficiently computes feature importance by
    tracking input perturbations through network operations.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7：**前向模式AD**：通过跟踪网络操作中的输入扰动来有效地计算特征重要性。
- en: '[PRE6]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In specialized training scenarios, particularly those involving online learning
    where models update on individual examples, forward mode offers advantages. The
    framework can track derivatives for a single example through the network, though
    this approach becomes less practical when dealing with batch training or updating
    multiple model parameters simultaneously.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在专门的培训场景中，尤其是在涉及在线学习且模型在单个示例上更新的情况下，前向模式具有优势。该框架可以跟踪单个示例通过网络的反导数，尽管当处理批量训练或同时更新多个模型参数时，这种方法变得不太实用。
- en: Understanding these use cases helps explain why machine learning frameworks
    maintain forward mode capabilities alongside other differentiation strategies.
    While reverse mode handles the heavy lifting of full model training, forward mode
    provides an elegant solution for specific analytical tasks where its computational
    pattern matches the problem structure.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些用例有助于解释为什么机器学习框架在保持其他微分策略的同时，还维持前向模式的能力。虽然反向模式处理完整模型训练的重活，但前向模式为特定分析任务提供了一种优雅的解决方案，在这些任务中，其计算模式与问题结构相匹配。
- en: Reverse Mode
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向模式
- en: Reverse mode automatic differentiation forms the computational backbone of modern
    neural network training. This isn’t by accident - reverse mode’s structure perfectly
    matches what we need for training neural networks. During training, we have one
    scalar output (the loss function) and need derivatives with respect to millions
    of parameters (the network weights). Reverse mode is exceptionally efficient at
    computing exactly this pattern of derivatives.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式自动微分构成了现代神经网络训练的计算核心。这不是偶然的 - 反向模式的结构完美地符合我们训练神经网络的需求。在训练过程中，我们有一个标量输出（损失函数）和需要数百万个参数（网络权重）的导数。反向模式在计算这种导数模式方面特别高效。
- en: A closer look at [Listing 7.8](ch013.xhtml#lst-reverse_simple) reveals how reverse
    mode differentiation is structured.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看[列表7.8](ch013.xhtml#lst-reverse_simple)可以揭示反向模式微分是如何构建的。
- en: 'Listing 7.8: Basic example of reverse mode automatic differentiation'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8：反向模式自动微分的简单示例
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this function shown in [Listing 7.8](ch013.xhtml#lst-reverse_simple), we
    have three operations that create a computational chain. Notice how ‘x’ influences
    the final result ‘c’ through two different paths: once through squaring (a = x²)
    and once through sine (b = sin(x)). Both paths must be accounted for when computing
    derivatives.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.8](ch013.xhtml#lst-reverse_simple)中展示的此函数中，我们有三个操作创建了一个计算链。注意“x”如何通过两种不同的路径影响最终结果“c”：一次是通过平方（a
    = x²），一次是通过正弦（b = sin(x)）。在计算导数时，必须考虑这两条路径。
- en: First, the forward pass computes and stores values, as illustrated in [Listing 7.9](ch013.xhtml#lst-reverse_forward).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，前向传递计算并存储值，如[列表7.9](ch013.xhtml#lst-reverse_forward)所示。
- en: 'Listing 7.9: **Forward Pass**: Computes intermediate values that contribute
    to the final output through distinct paths.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9：**前向传递**：计算通过不同路径对最终输出有贡献的中间值。
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then comes the backward pass. This is where reverse mode shows its elegance.
    This process is demonstrated in [Listing 7.10](ch013.xhtml#lst-reverse_backward),
    where we compute the gradient starting from the output.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是反向传递。这正是反向模式展现其优雅之处的地方。这个过程在[列表7.10](ch013.xhtml#lst-reverse_backward)中得到了演示，其中我们从输出开始计算梯度。
- en: 'Listing 7.10: **Backward Pass**: Computes gradients through multiple paths
    to update model parameters. This caption directly informs students about the purpose
    of the backward pass in computing gradients for parameter updates, emphasizing
    its role in training machine learning models.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10：**反向传递**：通过多条路径计算梯度以更新模型参数。这个标题直接告知学生反向传递在计算参数更新梯度中的目的，强调其在训练机器学习模型中的作用。
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The power of reverse mode becomes clear when we consider what would happen if
    we added more operations that depend on x. Forward mode would require tracking
    derivatives through each new path, but reverse mode handles all paths in a single
    backward pass. This is exactly the scenario in neural networks, where each weight
    can affect the final loss through multiple paths in the network.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑如果添加更多依赖于x的操作会发生什么时，反向模式的力量变得明显。正向模式需要通过每个新路径跟踪导数，但反向模式可以在单个反向传播中处理所有路径。这正是神经网络的情况，其中每个权重可以通过网络中的多个路径影响最终损失。
- en: Implementation Structure
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 实现结构
- en: The implementation of reverse mode in machine learning frameworks requires careful
    orchestration of computation and memory. While forward mode simply augments each
    computation, reverse mode needs to maintain a record of the forward computation
    to enable the backward pass. Modern frameworks accomplish this through computational
    graphs and automatic gradient accumulation[21](#fn21).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习框架中实现反向模式需要仔细协调计算和内存。正向模式只是增强每个计算，而反向模式需要维护正向计算的记录以实现反向传播。现代框架通过计算图和自动梯度累积[21](#fn21)来完成此任务。
- en: We extend our previous example to a small neural network computation. See [Listing 7.11](ch013.xhtml#lst-reverse_simple_nn)
    for the code structure.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将之前的例子扩展到一个小型神经网络计算。请参阅[代码列表7.11](ch013.xhtml#lst-reverse_simple_nn)以了解代码结构。
- en: 'Listing 7.11: **Reverse Mode**: Neural networks compute gradients through backward
    passes on layered computations.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表7.11：**反向模式**：神经网络通过在分层计算上进行反向传播来计算梯度。
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: During the forward pass, the framework doesn’t just compute values. It builds
    a graph of operations while tracking intermediate results, as illustrated in [Listing 7.12](ch013.xhtml#lst-reverse_nn_forward).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传播期间，框架不仅计算值。它同时构建一个操作图并跟踪中间结果，如[代码列表7.12](ch013.xhtml#lst-reverse_nn_forward)所示。
- en: 'Listing 7.12: **Forward Pass**: Computes intermediate states using linear and
    non-linear transformations to produce the final output. Training Pipeline: Partitions
    datasets into distinct sets for training, validation, and testing to ensure model
    robustness and unbiased evaluation.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表7.12：**正向传播**：使用线性和非线性变换计算中间状态，以产生最终输出。训练流程：将数据集划分为不同的训练、验证和测试集，以确保模型鲁棒性和无偏评估。
- en: '[PRE11]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Refer to [Listing 7.13](ch013.xhtml#lst-reverse_nn_backward) for a step-by-step
    breakdown of gradient computation during the backward pass.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[代码列表7.13](ch013.xhtml#lst-reverse_nn_backward)以了解反向传播期间梯度计算的逐步分解。
- en: 'Listing 7.13: **Backward Pass**: This code calculates gradients for weights
    in a neural network, highlighting how changes propagate backward through layers
    to update parameters.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表7.13：**反向传播**：此代码计算神经网络中的权重梯度，突出了如何通过层反向传播以更新参数。
- en: '[PRE12]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This example illustrates several key implementation considerations: 1\. The
    framework must track dependencies between operations 2\. Intermediate values must
    be stored for the backward pass 3\. Gradient computations follow the reverse topological
    order of the forward computation 4\. Each operation needs both forward and backward
    implementations'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例说明了几个关键的实施考虑因素：1. 框架必须跟踪操作之间的依赖关系 2. 中间值必须存储以供反向传播使用 3. 梯度计算遵循正向计算的逆拓扑顺序
    4. 每个操作都需要正向和反向实现
- en: Memory Management Strategies
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内存管理策略
- en: Memory management represents one of the key challenges in implementing reverse
    mode differentiation in machine learning frameworks. Unlike forward mode where
    we can discard intermediate values as we go, reverse mode requires storing results
    from the forward pass to compute gradients during the backward pass.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理代表了在机器学习框架中实现反向模式微分的一个关键挑战。与正向模式不同，我们可以在进行过程中丢弃中间值，反向模式需要存储正向传播的结果，以便在反向传播期间计算梯度。
- en: This requirement is illustrated in [Listing 7.14](ch013.xhtml#lst-reverse_memory),
    which extends our neural network example to highlight how intermediate activations
    must be preserved for use during gradient computation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此要求在[代码列表7.14](ch013.xhtml#lst-reverse_memory)中得到了说明，它扩展了我们的神经网络示例，以突出中间激活必须保留以供梯度计算使用。
- en: 'Listing 7.14: **Reverse Mode Memory Management**: Stores intermediate values
    for gradient computation during backpropagation.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 代码列表7.14：**反向模式内存管理**：在反向传播期间存储中间值以进行梯度计算。
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each intermediate value needed for gradient computation must be kept in memory
    until its backward pass completes. As networks grow deeper, this memory requirement
    grows linearly with network depth. For a typical deep neural network processing
    a batch of images, this can mean gigabytes of stored activations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用于梯度计算的中间值必须保存在内存中，直到其反向传递完成。随着网络的加深，这种内存需求会线性增长。对于一个典型的处理图像批次的深度神经网络，这可能意味着需要存储数十亿字节的活动。
- en: Frameworks employ several strategies to manage this memory burden. One such
    approach is illustrated in [Listing 7.15](ch013.xhtml#lst-memory_strategies).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 框架采用几种策略来管理这种内存负担。其中一种方法在[列表 7.15](ch013.xhtml#lst-memory_strategies) 中展示。
- en: 'Listing 7.15: **Memory Management Strategies**: Training involves layered transformations
    where memory is managed to optimize performance. Checkpointing allows intermediate
    values to be freed during training, reducing memory usage while maintaining computational
    integrity via Explanation: The code. This emphasizes the trade-offs between memory
    management and model complexity in deep learning systems.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 7.15: **内存管理策略**: 训练涉及层叠的变换，其中内存被管理以优化性能。检查点允许在训练期间释放中间值，减少内存使用，同时通过解释：代码来保持计算完整性。这强调了在深度学习系统中内存管理和模型复杂度之间的权衡。'
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Modern frameworks automatically balance memory usage and computation speed.
    They might recompute some intermediate values during the backward pass rather
    than storing everything, particularly for memory-intensive operations. This trade-off
    between memory and computation becomes especially important in large-scale training
    scenarios.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架自动平衡内存使用和计算速度。它们可能在反向传递期间重新计算某些中间值，而不是存储所有内容，尤其是在内存密集型操作中。这种内存和计算之间的权衡在大型规模训练场景中变得尤为重要。
- en: Optimization Techniques
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 优化技术
- en: Reverse mode automatic differentiation in machine learning frameworks employs
    several key optimization techniques to enhance training efficiency. These optimizations
    become crucial when training large neural networks where computational and memory
    resources are pushed to their limits.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架中的反向模式自动微分采用了几种关键优化技术来提高训练效率。当训练大型神经网络，计算和内存资源被推到极限时，这些优化变得至关重要。
- en: Modern frameworks implement gradient checkpointing[22](#fn22), a technique that
    strategically balances computation and memory. A simplified forward pass of such
    a network is shown in [Listing 7.16](ch013.xhtml#lst-deep_forward).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架实现了梯度检查点[22](#fn22)，这是一种战略性地平衡计算和内存的技术。这种网络的简化正向传递在[列表 7.16](ch013.xhtml#lst-deep_forward)
    中展示。
- en: 'Listing 7.16: **Forward Pass**: Neural networks process input through sequential
    layers of transformations to produce an output, highlighting the hierarchical
    nature of deep learning architectures.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 7.16: **正向传递**: 神经网络通过一系列变换的层来处理输入，以产生输出，突出了深度学习架构的层次性质。'
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Instead of storing all intermediate activations, frameworks can strategically
    recompute certain values during the backward pass. [Listing 7.17](ch013.xhtml#lst-checkpoint_scheme)
    demonstrates how frameworks achieve this memory saving. The framework might save
    activations only every few layers.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 框架可以战略性地在反向传递过程中重新计算某些值，而不是存储所有中间激活。[列表 7.17](ch013.xhtml#lst-checkpoint_scheme)
    展示了框架如何实现这种内存节省。框架可能只在每几层中保存激活。
- en: 'Listing 7.17: **Checkpointing**: Reduces memory usage by selectively storing
    intermediate activations during forward passes. Frameworks balance storage needs
    with computational efficiency to optimize model training.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 7.17: **检查点**: 通过在正向传递过程中选择性地存储中间激活来减少内存使用。框架在存储需求和计算效率之间进行平衡，以优化模型训练。'
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Another crucial optimization involves operation fusion[23](#fn23). Rather than
    treating each mathematical operation separately, frameworks combine operations
    that commonly occur together. Matrix multiplication followed by bias addition,
    for instance, can be fused into a single operation, reducing memory transfers
    and improving hardware utilization.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键的优化涉及操作融合[23](#fn23)。框架不是单独处理每个数学运算，而是将常见一起发生的操作组合起来。例如，矩阵乘法后跟偏置加法可以融合成一个单一的操作，减少内存传输并提高硬件利用率。
- en: The backward pass itself can be optimized by reordering computations to maximize
    hardware efficiency. Consider the gradient computation for a convolution layer
    - rather than directly translating the mathematical definition into code, frameworks
    implement specialized backward operations that take advantage of modern hardware
    capabilities.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排序计算以最大化硬件效率来优化反向传播本身。考虑卷积层的梯度计算——而不是直接将数学定义转换为代码，框架实现了专门的反向操作，这些操作利用了现代硬件的能力。
- en: These optimizations work together to make the training of large neural networks
    practical. Without them, many modern architectures would be prohibitively expensive
    to train, both in terms of memory usage and computation time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化共同工作，使得大型神经网络的训练变得可行。没有它们，许多现代架构在内存使用和计算时间上都会变得过于昂贵。
- en: Framework Implementation of Automatic Differentiation
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动微分框架实现
- en: The integration of automatic differentiation into machine learning frameworks
    requires careful system design to balance flexibility, performance, and usability.
    Modern frameworks like PyTorch and TensorFlow expose AD capabilities through high-level
    APIs while maintaining the sophisticated underlying machinery.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 将自动微分集成到机器学习框架中需要仔细的系统设计，以平衡灵活性、性能和可用性。现代框架如PyTorch和TensorFlow通过高级API暴露AD功能，同时保持复杂的底层机制。
- en: Frameworks present AD to users through various interfaces. A typical example
    from PyTorch is shown in [Listing 7.18](ch013.xhtml#lst-ad_interface).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 框架通过各种接口向用户呈现AD。PyTorch的一个典型例子在[列表7.18](ch013.xhtml#lst-ad_interface)中展示。
- en: 'Listing 7.18: **Automatic Differentiation Interface**: PyTorch transparently
    tracks operations during neural network execution to enable efficient backpropagation.
    Training requires careful management of gradients and model parameters, highlighting
    the importance of automatic differentiation in achieving optimal performance.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18：**自动微分接口**：PyTorch在神经网络执行过程中透明地跟踪操作，以实现高效的反向传播。训练需要仔细管理梯度和模型参数，突出了自动微分在实现最佳性能中的重要性。
- en: '[PRE17]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'While this code appears straightforward, it masks considerable complexity.
    The framework must:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这段代码看起来很简单，但它掩盖了相当大的复杂性。框架必须：
- en: Track all operations during the forward pass
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在正向传播过程中跟踪所有操作
- en: Build and maintain the computational graph
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和维护计算图
- en: Manage memory for intermediate values
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理中间值的内存
- en: Schedule gradient computations efficiently
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高效安排梯度计算
- en: Interface with hardware accelerators
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与硬件加速器接口
- en: This integration extends beyond basic training. Frameworks must handle complex
    scenarios like higher-order gradients, where we compute derivatives of derivatives,
    and mixed-precision training. The ability to compute second-order derivatives
    is demonstrated in [Listing 7.19](ch013.xhtml#lst-higher_order).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成不仅限于基本训练。框架必须处理复杂场景，如高阶梯度，其中我们计算导数的导数，以及混合精度训练。在[列表7.19](ch013.xhtml#lst-higher_order)中展示了计算二阶导数的能力。
- en: 'Listing 7.19: **Higher-Order Gradients**: Second-order gradients reveal how
    changes in model parameters affect first-order gradients, essential for advanced
    optimization techniques.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.19：**高阶梯度**：二阶梯度揭示了模型参数的变化如何影响一阶梯度，这对于高级优化技术至关重要。
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The Systems Engineering Breakthrough
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统工程突破
- en: While the mathematical foundations of automatic differentiation were established
    decades ago, the practical implementation in machine learning frameworks represents
    a significant systems engineering achievement. Understanding this perspective
    illuminates why automatic differentiation systems enabled the deep learning revolution.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动微分的数学基础几十年前就已经确立，但在机器学习框架中的实际实现代表了重大的系统工程成就。理解这一视角可以阐明为什么自动微分系统推动了深度学习革命。
- en: Before automated systems, implementing gradient computation required manually
    deriving and coding gradients for every operation in a neural network. For a simple
    fully connected layer, this meant writing separate forward and backward functions,
    carefully tracking intermediate values, and ensuring mathematical correctness
    across dozens of operations. As architectures became more complex with convolutional
    layers, attention mechanisms, or custom operations, this manual process became
    error-prone and prohibitively time-consuming.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化系统出现之前，实现梯度计算需要手动推导和编写神经网络中每个操作的梯度。对于一个简单的全连接层，这意味着需要编写单独的前向和后向函数，仔细跟踪中间值，并确保数十个操作中的数学正确性。随着卷积层、注意力机制或自定义操作等架构变得更加复杂，这种手动过程变得容易出错且耗时。
- en: 'Addressing these challenges, the breakthrough in automatic differentiation
    lies not in mathematical innovation but in software engineering. Modern frameworks
    must handle memory management, operation scheduling, numerical stability, and
    optimization across diverse hardware while maintaining mathematical correctness.
    Consider the complexity: a single matrix multiplication requires different gradient
    computations depending on which inputs require gradients, tensor shapes, hardware
    capabilities, and memory constraints. Automatic differentiation systems handle
    these variations transparently, enabling researchers to focus on model architecture
    rather than gradient implementation details.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，自动微分领域的突破不在于数学创新，而在于软件工程。现代框架必须处理内存管理、操作调度、数值稳定性和跨不同硬件的优化，同时保持数学正确性。考虑其复杂性：单个矩阵乘法需要根据哪些输入需要梯度、张量形状、硬件能力和内存限制进行不同的梯度计算。自动微分系统透明地处理这些变化，使研究人员能够专注于模型架构而不是梯度实现的细节。
- en: Beyond simplifying existing workflows, autograd systems enabled architectural
    innovations that would be impossible with manual gradient implementation. Modern
    architectures like Transformers involve hundreds of operations with complex dependencies.
    Computing gradients manually for complex architectural components, layer normalization,
    and residual connections would require months of careful derivation and debugging.
    Automatic differentiation systems compute these gradients correctly and efficiently,
    enabling rapid experimentation with novel architectures.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简化现有工作流程外，autograd 系统还使架构创新成为可能，这在手动梯度实现中是不可能的。现代架构如 Transformer 涉及数百个操作，具有复杂的依赖关系。手动计算复杂架构组件、层归一化和残差连接的梯度需要数月的仔细推导和调试。自动微分系统可以正确且高效地计算这些梯度，从而能够快速实验新型架构。
- en: 'This systems perspective explains why deep learning accelerated dramatically
    after frameworks matured: not because the mathematics changed, but because software
    engineering finally made the mathematics practical to apply at scale. The computational
    graphs discussed earlier provide the infrastructure, but the automatic differentiation
    systems provide the intelligence to traverse these graphs correctly and efficiently.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统视角解释了为什么深度学习在框架成熟后加速发展：不是因为数学发生了变化，而是因为软件工程最终使数学能够在大规模上实际应用。前面讨论的计算图提供了基础设施，但自动微分系统提供了正确且高效遍历这些图的智能。
- en: Memory Management in Gradient Computation
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度计算中的内存管理
- en: 'The memory demands of automatic differentiation stem from a fundamental requirement:
    to compute gradients during the backward pass, we must remember what happened
    during the forward pass. This seemingly simple requirement creates interesting
    challenges for machine learning frameworks. Unlike traditional programs that can
    discard intermediate results as soon as they’re used, AD systems must carefully
    preserve computational history.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分的内存需求源于一个基本要求：在反向传播期间计算梯度时，我们必须记住正向传播期间发生的事情。这个看似简单的需求为机器学习框架带来了有趣的挑战。与可以一使用完就丢弃中间结果的传统程序不同，AD
    系统必须仔细保存计算历史。
- en: This necessity is illustrated in [Listing 7.20](ch013.xhtml#lst-forward_trace),
    which shows what happens during a neural network’s forward pass.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种必要性在[列表 7.20](ch013.xhtml#lst-forward_trace)中得到了说明，该列表展示了神经网络正向传播期间发生的情况。
- en: 'Listing 7.20: **Forward Pass**: Neural networks compute values sequentially,
    storing intermediate results for backpropagation to calculate gradients accurately.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20：**正向传播**：神经网络按顺序计算值，存储中间结果以便反向传播准确计算梯度。
- en: '[PRE19]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When this network processes data, each operation creates not just its output,
    but also a memory obligation. The multiplication in layer1 needs to remember its
    inputs because computing its gradient later will require them. Even the seemingly
    simple relu function must track which inputs were negative to correctly propagate
    gradients. As networks grow deeper, these memory requirements accumulate, as seen
    in [Listing 7.21](ch013.xhtml#lst-deep_memory).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个网络处理数据时，每个操作不仅创建其输出，还产生内存需求。层1中的乘法操作需要记住其输入，因为计算其梯度时将需要它们。即使是看似简单的relu函数也必须跟踪哪些输入是负的，以正确传播梯度。随着网络变深，这些内存需求会累积，如[列表
    7.21](ch013.xhtml#lst-deep_memory)所示。
- en: This memory challenge becomes particularly interesting with deep neural networks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，这个内存挑战变得特别有趣。
- en: 'Listing 7.21: **Memory Accumulation**: Each layer in a deep neural network
    retains information needed for backpropagation, highlighting the growing memory
    demands as networks deepen.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.21：**内存累积**：深度神经网络中的每一层都保留着反向传播所需的信息，突显了随着网络加深而增长的内存需求。
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each layer’s computation adds to our memory burden. The framework must keep
    hidden1 in memory until gradients are computed through hidden2, after which it
    can be safely discarded. This creates a wave of memory usage that peaks when we
    start the backward pass and gradually recedes as we compute gradients.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 每层的计算都增加了我们的内存负担。框架必须将隐藏1保留在内存中，直到通过隐藏2计算梯度后，才能安全地丢弃它。这产生了一波内存使用，当开始反向传播时达到峰值，随着我们计算梯度而逐渐消退。
- en: Modern frameworks handle this memory choreography automatically. They track
    the lifetime of each intermediate value - how long it must remain in memory for
    gradient computation. When training large models, this careful memory management
    becomes as crucial as the numerical computations themselves. The framework frees
    memory as soon as it’s no longer needed for gradient computation, ensuring that
    our memory usage, while necessarily large, remains as efficient as possible.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架自动处理这种内存编排。它们跟踪每个中间值的生命周期——它必须保留在内存中的时间，以便进行梯度计算。在训练大型模型时，这种谨慎的内存管理与数值计算本身一样重要。框架一旦不再需要用于梯度计算，就释放内存，确保我们的内存使用，尽管必然很大，但尽可能高效。
- en: Production System Integration Challenges
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生产系统集成挑战
- en: Automatic differentiation’s integration into machine learning frameworks raises
    important system-level considerations that affect both framework design and training
    performance. These considerations become particularly apparent when training large
    neural networks where efficiency at every level matters.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分集成到机器学习框架中提出了重要的系统级考虑因素，这些因素影响框架设计和训练性能。当训练大型神经网络时，这些考虑因素尤其明显，因为每一层的效率都很重要。
- en: As illustrated in [Listing 7.22](ch013.xhtml#lst-train_loop), a typical training
    loop handles both computation and system-level interaction.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表 7.22](ch013.xhtml#lst-train_loop)所示，典型的训练循环处理计算和系统级交互。
- en: 'Listing 7.22: **Training Pipeline**: Machine learning workflows partition datasets
    into training, validation, and test sets to ensure robust model development and
    unbiased evaluation.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.22：**训练流程**：机器学习工作流程将数据集划分为训练集、验证集和测试集，以确保稳健的模型开发和无偏评估。
- en: '[PRE21]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This simple loop masks complex system interactions. The AD system must coordinate
    with multiple framework components: the memory allocator, the device manager,
    the operation scheduler, and the optimizer. Each gradient computation potentially
    triggers data movement between devices, memory allocation, and kernel launches
    on accelerators.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的循环掩盖了复杂的系统交互。自动微分系统必须与多个框架组件协调：内存分配器、设备管理器、操作调度器和优化器。每次梯度计算都可能触发设备之间、内存分配以及在加速器上的内核启动之间的数据移动。
- en: The scheduling of AD operations on modern hardware accelerators is illustrated
    in [Listing 7.23](ch013.xhtml#lst-parallel_ad).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.23](ch013.xhtml#lst-parallel_ad) 展示了在现代硬件加速器上调度自动微分操作的示例。'
- en: 'Listing 7.23: **Parallel Computation**: Operations can run concurrently in
    a neural network, illustrating the need for synchronization to combine results
    effectively. Via The code'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.23：**并行计算**：操作可以在神经网络中并发运行，说明了有效组合结果需要同步。通过以下代码
- en: '[PRE22]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The AD system must track dependencies not just for correct gradient computation,
    but also for efficient hardware utilization. It needs to determine which gradient
    computations can run in parallel and which must wait for others to complete. This
    dependency tracking extends across both forward and backward passes, creating
    a complex scheduling problem.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: AD系统必须跟踪依赖关系，不仅是为了正确的梯度计算，还要为了高效的硬件利用。它需要确定哪些梯度计算可以并行运行，哪些必须等待其他计算完成。这种依赖关系跟踪跨越了正向和反向传播，从而产生了一个复杂的调度问题。
- en: Modern frameworks handle these system-level concerns while maintaining a simple
    interface for users. Behind the scenes, they make sophisticated decisions about
    operation scheduling, memory allocation, and data movement, all while ensuring
    correct gradient computation through the computational graph.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架在保持用户简单接口的同时处理这些系统级关注点。在幕后，它们在操作调度、内存分配和数据移动方面做出复杂的决策，同时确保通过计算图进行正确的梯度计算。
- en: These system-level concerns demonstrate the sophisticated engineering that modern
    frameworks handle automatically, enabling developers to focus on model design
    rather than low-level implementation details.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统级关注点展示了现代框架所处理的复杂工程，使开发者能够专注于模型设计，而不是底层实现细节。
- en: Framework-Specific Differentiation Strategies
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 框架特定的微分策略
- en: While automatic differentiation principles remain consistent across frameworks,
    implementation approaches vary significantly and directly impact research workflows
    and development experience. Understanding these differences helps developers choose
    appropriate frameworks and explains performance characteristics they observe in
    practice.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动微分原理在各个框架中保持一致，但实现方法差异很大，这直接影响研究工作流程和开发体验。理解这些差异有助于开发者选择合适的框架，并解释他们在实践中观察到的性能特征。
- en: PyTorch’s Dynamic Autograd System
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch的动态自动微分系统
- en: PyTorch implements automatic differentiation through a dynamic tape-based system
    that constructs the computational graph during execution. This approach directly
    supports the research workflows and debugging capabilities discussed earlier in
    the dynamic graphs section.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch通过基于动态带的系统实现自动微分，在执行过程中构建计算图。这种方法直接支持前面动态图部分讨论的研究工作流程和调试能力。
- en: '[Listing 7.24](ch013.xhtml#lst-pytorch_autograd) demonstrates PyTorch’s approach
    to gradient tracking, which occurs transparently during forward execution.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.24](ch013.xhtml#lst-pytorch_autograd)展示了PyTorch在正向执行过程中透明地跟踪梯度的方法。'
- en: 'Listing 7.24: **PyTorch Autograd Implementation**: Dynamic tape construction
    during forward pass enables transparent gradient computation with immediate debugging
    capabilities.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.24：**PyTorch Autograd实现**：正向传播期间的动态带构建使得梯度计算透明，并具有即时的调试能力。
- en: '[PRE23]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: PyTorch’s dynamic approach provides several advantages for research workflows.
    Operations are tracked automatically without requiring upfront graph definition,
    enabling natural Python control flow like conditionals and loops. Gradients become
    available immediately after backward pass completion, supporting interactive debugging
    and experimentation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的动态方法为研究工作流程提供了几个优势。操作自动跟踪，无需预先定义图，使得Python控制流（如条件语句和循环）自然实现。反向传播完成后，梯度立即可用，支持交互式调试和实验。
- en: The dynamic tape system also handles variable-length computations naturally.
    [Listing 7.25](ch013.xhtml#lst-pytorch_dynamic_length) shows how PyTorch adapts
    to runtime-determined computation graphs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 动态带系统也自然地处理可变长度的计算。[列表7.25](ch013.xhtml#lst-pytorch_dynamic_length)展示了PyTorch如何适应运行时确定的计算图。
- en: 'Listing 7.25: **Dynamic Length Computation**: PyTorch’s autograd handles variable
    computation patterns naturally, enabling flexible model architectures that adapt
    to input characteristics.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.25：**动态长度计算**：PyTorch的autograd自然地处理变量计算模式，使得模型架构能够灵活地适应输入特征。
- en: '[PRE24]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This flexibility comes with memory and computational overhead. PyTorch must
    maintain the entire computational graph in memory until backward pass completion,
    and gradient computation cannot benefit from global graph optimizations that require
    complete graph analysis.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性伴随着内存和计算开销。PyTorch必须在反向传播完成之前将整个计算图保持在内存中，并且梯度计算不能从需要完整图分析的全球图优化中受益。
- en: TensorFlow’s Static Graph Optimization
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorFlow的静态图优化
- en: TensorFlow’s traditional approach to automatic differentiation leverages static
    graph analysis to enable aggressive optimizations. While TensorFlow 2.x defaults
    to eager execution, understanding the static graph approach illuminates the trade-offs
    between flexibility and optimization.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow对自动微分的传统方法利用静态图分析来实现激进的优化。虽然TensorFlow 2.x默认使用即时执行，但理解静态图方法可以阐明灵活性和优化之间的权衡。
- en: '**Historical Context: TensorFlow 1.x Code**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**历史背景：TensorFlow 1.x 代码**'
- en: The following examples use TensorFlow 1.x style code with `placeholder`, `Session`,
    and `feed_dict` patterns. These APIs are deprecated in TensorFlow 2.x, which uses
    eager execution by default. We include these examples because (1) they clearly
    illustrate the conceptual difference between graph and eager execution, (2) you
    may encounter legacy codebases using these patterns, and (3) understanding graph
    execution helps explain why modern frameworks like `tf.function` exist.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用TensorFlow 1.x风格的代码，包括`placeholder`、`Session`和`feed_dict`模式。这些API在TensorFlow
    2.x中已弃用，TensorFlow 2.x默认使用即时执行。我们包括这些示例是因为（1）它们清楚地说明了图和即时执行之间的概念差异，（2）你可能会遇到使用这些模式的遗留代码库，以及（3）理解图执行有助于解释为什么现代框架如`tf.function`存在。
- en: '[Listing 7.26](ch013.xhtml#lst-tensorflow_static_ad) demonstrates TensorFlow’s
    static graph differentiation, which separates graph construction from execution.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.26](ch013.xhtml#lst-tensorflow_static_ad)展示了TensorFlow的静态图微分，它将图构建与执行分离。'
- en: 'Listing 7.26: **TensorFlow 1.x Static Graph AD**: Symbolic differentiation
    during graph construction enables global optimizations and efficient repeated
    execution.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.26：**TensorFlow 1.x 静态图 AD**：在图构建期间进行符号微分，可以启用全局优化和高效的重复执行。
- en: '[PRE25]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The static graph approach enables powerful optimizations unavailable to dynamic
    systems. TensorFlow can analyze the complete gradient computation graph and apply
    operation fusion, memory layout optimization, and parallel execution scheduling.
    These optimizations can provide 2-3x performance improvements for large models.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 静态图方法使动态系统无法实现的强大优化成为可能。TensorFlow可以分析完整的梯度计算图，并应用操作融合、内存布局优化和并行执行调度。这些优化可以为大型模型提供2-3倍的性能提升。
- en: Static graphs also enable efficient repeated execution. Once compiled, the same
    graph can process multiple batches with minimal overhead, making static graphs
    particularly effective for production serving where the same model structure processes
    many requests.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 静态图还使高效的重复执行成为可能。一旦编译，相同的图可以以最小的开销处理多个批次，这使得静态图在处理许多请求的相同模型结构的生产服务中特别有效。
- en: However, this approach historically required more complex debugging workflows
    and limited flexibility for dynamic computation patterns. Modern TensorFlow addresses
    these limitations through eager execution while maintaining static graph capabilities
    through `tf.function` compilation.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法在历史上需要更复杂的调试工作流程，并且对动态计算模式缺乏灵活性。现代TensorFlow通过即时执行来解决这些限制，同时通过`tf.function`编译保持静态图功能。
- en: JAX’s Functional Differentiation
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: JAX的函数微分
- en: JAX takes a fundamentally different approach to automatic differentiation based
    on functional programming principles and program transformation. This approach
    aligns with JAX’s functional programming philosophy, discussed further in the
    framework comparison section.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: JAX基于函数编程原则和程序转换的方法对自动微分采取了根本不同的方法。这种方法与JAX的函数编程哲学相一致，这在框架比较部分将进一步讨论。
- en: '[Listing 7.27](ch013.xhtml#lst-jax_functional_ad) demonstrates JAX’s transformation-based
    approach to differentiation.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.27](ch013.xhtml#lst-jax_functional_ad)展示了JAX基于转换的微分方法。'
- en: 'Listing 7.27: **JAX Functional Differentiation**: Program transformation approach
    enables both forward and reverse mode differentiation with mathematical transparency
    and composability.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.27：**JAX 函数微分**：程序转换方法使前向和反向模式微分具有数学透明性和可组合性。
- en: '[PRE26]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: JAX’s functional approach provides several unique advantages. The same function
    can be transformed for different differentiation modes, execution patterns, and
    optimization strategies. Forward and reverse mode differentiation are equally
    accessible, enabling optimal choice based on problem characteristics.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的函数方法提供了几个独特的优势。同一个函数可以根据不同的微分模式、执行模式和优化策略进行转换。前向和反向模式微分同样容易访问，可以根据问题特性进行最佳选择。
- en: The transformation approach also enables powerful composition patterns. [Listing 7.28](ch013.xhtml#lst-jax_composition)
    shows how different transformations combine naturally.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 转换方法还使强大的组合模式成为可能。[列表7.28](ch013.xhtml#lst-jax_composition)展示了不同的转换如何自然地结合。
- en: 'Listing 7.28: **JAX Transformation Composition**: Multiple program transformations
    compose naturally, enabling complex optimizations through simple function composition.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.28：**JAX转换组合**：多个程序转换可以自然组合，通过简单的函数组合实现复杂的优化。
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This functional approach requires immutable data structures and pure functions
    but enables mathematical reasoning about program transformations that would be
    impossible with stateful systems.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这种函数式方法需要不可变的数据结构和纯函数，但能够对程序转换进行数学推理，这在有状态系统中是不可能的。
- en: Research Productivity and Innovation Acceleration
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 研究生产力和创新加速
- en: These implementation differences have direct implications for research productivity
    and development workflows. PyTorch’s dynamic approach accelerates experimentation
    and debugging but may require optimization for production deployment. TensorFlow’s
    static graph capabilities provide production-ready performance but historically
    required more structured development approaches. JAX’s functional transformations
    enable powerful mathematical abstractions but require functional programming discipline.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实现差异对研究生产力和开发工作流程有直接影响。PyTorch的动态方法加速了实验和调试，但可能需要针对生产部署进行优化。TensorFlow的静态图功能提供了生产就绪的性能，但历史上需要更多结构化的开发方法。JAX的功能转换能够实现强大的数学抽象，但需要函数式编程的纪律。
- en: 'Understanding these trade-offs helps researchers choose appropriate frameworks
    for their specific use cases and explains the performance characteristics they
    observe during development and deployment. The choice between dynamic flexibility,
    static optimization, and functional transformation often depends on project priorities:
    rapid experimentation, production performance, or mathematical elegance.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些权衡有助于研究人员选择适合他们特定用例的框架，并解释他们在开发和部署期间观察到的性能特征。动态灵活性、静态优化和函数式转换之间的选择通常取决于项目优先级：快速实验、生产性能或数学优雅。
- en: Automatic Differentiation System Design Principles
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动微分系统设计原则
- en: Automatic differentiation systems transform the mathematical concept of derivatives
    into efficient implementations. By examining forward and reverse modes, we see
    how frameworks balance mathematical precision with computational efficiency for
    modern neural network training.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分系统将数学概念中的导数转换为高效的实现。通过检查正向和反向模式，我们可以看到框架如何在现代神经网络训练中平衡数学精度和计算效率。
- en: The implementation of AD systems reveals key design patterns in machine learning
    frameworks. One such pattern is shown in [Listing 7.29](ch013.xhtml#lst-ad_mechanics).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分系统的实现揭示了机器学习框架中的关键设计模式。其中一种模式在[列表7.29](ch013.xhtml#lst-ad_mechanics)中展示。
- en: 'Listing 7.29: **AD Mechanism**: Frameworks track operations for efficient backward
    passes during training through The code. This example emphasizes the importance
    of tracking intermediate computations to enable effective gradient calculations,
    a core aspect of automatic differentiation in machine learning systems.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.29：**AD机制**：框架通过代码跟踪操作，在训练期间进行高效的反向传递。此示例强调了跟踪中间计算以实现有效的梯度计算的重要性，这是机器学习系统中自动微分的核心方面。
- en: '[PRE28]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This simple computation embodies several fundamental concepts:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的计算体现了几个基本概念：
- en: Operation tracking for derivative computation
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导数计算的跟踪操作
- en: Memory management for intermediate values
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中间值的内存管理
- en: System coordination for efficient execution
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统协调以实现高效执行
- en: As shown in [Listing 7.30](ch013.xhtml#lst-ad_abstraction), modern frameworks
    abstract these complexities behind clean interfaces while maintaining high performance.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表7.30](ch013.xhtml#lst-ad_abstraction)所示，现代框架在保持高性能的同时，在干净的接口后面抽象了这些复杂性。
- en: 'Listing 7.30: **Minimal API**: Simplifies automatic differentiation by tracking
    forward computations and efficiently computing gradients, enabling effective model
    optimization.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.30：**最小API**：通过跟踪正向计算和高效计算梯度，简化了自动微分，从而实现有效的模型优化。
- en: '[PRE29]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The effectiveness of automatic differentiation systems stems from their careful
    balance of competing demands. They must maintain sufficient computational history
    for accurate gradients while managing memory constraints, schedule operations
    efficiently while preserving correctness, and provide flexibility while optimizing
    performance.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分系统的有效性源于其对竞争需求的精心平衡。它们必须在保持足够的计算历史以获得准确梯度的同时管理内存限制，高效调度操作同时保持正确性，并在优化性能的同时提供灵活性。
- en: Understanding these systems proves essential for both framework developers and
    practitioners. Framework developers must implement efficient AD to enable modern
    deep learning, while practitioners benefit from understanding AD’s capabilities
    and constraints when designing and training models.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些系统对于框架开发者和实践者来说至关重要。框架开发者必须实现高效的AD以支持现代深度学习，而实践者在设计和训练模型时从理解AD的能力和限制中受益。
- en: 'While automatic differentiation provides the computational foundation for gradient-based
    learning, its practical implementation depends heavily on how frameworks organize
    and manipulate data. This brings us to our next topic: the data structures that
    enable efficient computation and memory management in machine learning frameworks.
    These structures must not only support AD operations but also provide efficient
    access patterns for the diverse hardware platforms that power modern machine learning.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分提供了基于梯度的学习的计算基础，但其实际应用高度依赖于框架如何组织和操作数据。这引出了我们接下来的主题：在机器学习框架中实现高效计算和内存管理的数据结构。这些结构不仅必须支持AD操作，还必须为现代机器学习所依赖的多样化硬件平台提供高效的访问模式。
- en: Future Framework Architecture Directions
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来框架架构方向
- en: 'The automatic differentiation systems we’ve explored provide the computational
    foundation for neural network training, but they don’t operate in isolation. These
    systems require efficient ways to represent and manipulate the data flowing through
    them. This brings us to our next topic: the data structures that machine learning
    frameworks use to organize and process information.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所探讨的自动微分系统为神经网络训练提供了计算基础，但它们并非独立运作。这些系统需要高效的方式来表示和操作通过它们的数据流。这引出了我们接下来的主题：机器学习框架用来组织和处理信息的那些数据结构。
- en: Consider how our earlier examples handled numerical values ([Listing 7.31](ch013.xhtml#lst-numeric_interpretation)).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们之前示例中如何处理数值（[列表7.31](ch013.xhtml#lst-numeric_interpretation)）。
- en: 'Listing 7.31: **Layered Transformations**: Neural networks compute outputs
    through sequential operations on input data, illustrating how weights and activation
    functions influence final predictions. Numerical values are processed in neural
    network computations, highlighting the role of weight multiplications and activation
    functions. Via Data Flow: The code'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.31：**层叠变换**：神经网络通过在输入数据上执行顺序操作来计算输出，说明了权重和激活函数如何影响最终预测。数值在神经网络计算中被处理，突出了权重乘法和激活函数的作用。通过数据流：以下代码
- en: '[PRE30]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: These operations appear straightforward, but they raise important questions.
    How do frameworks represent these values? How do they organize data to enable
    efficient computation and automatic differentiation? How do they structure data
    to take advantage of modern hardware?
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作看似简单，但它们提出了重要的问题。框架如何表示这些值？它们如何组织数据以实现高效计算和自动微分？它们如何构建数据结构以利用现代硬件？
- en: The next section examines how frameworks answer these questions through specialized
    data structures, particularly tensors, that form the basic building blocks of
    machine learning computations.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨框架如何通过专门的数据结构，特别是张量，来回答这些问题，张量是机器学习计算的基本构建块。
- en: Data Structures
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据结构
- en: 'Machine learning frameworks extend computational graphs with specialized data
    structures, bridging high-level computations with practical implementations. These
    data structures have two essential purposes: they provide containers for the numerical
    data that powers machine learning models, and they manage how this data is stored
    and moved across different memory spaces and devices.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架通过扩展计算图与专门的数据结构相结合，将高级计算与实际实现相连接。这些数据结构有两个基本目的：它们为驱动机器学习模型的数值数据提供容器，并管理这些数据在不同内存空间和设备之间的存储和移动。
- en: While computational graphs specify the logical flow of operations, data structures
    determine how these operations actually access and manipulate data in memory.
    This dual role of organizing numerical data for model computations while handling
    the complexities of memory management and device placement shapes how frameworks
    translate mathematical operations into efficient executions across diverse computing
    platforms.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算图指定了操作的逻辑流程，但数据结构决定了这些操作如何在内存中实际访问和操作数据。这种在组织数值数据以供模型计算的同时处理内存管理和设备放置复杂性的双重角色，决定了框架如何将数学运算转换为在多样化的计算平台上的高效执行。
- en: The effectiveness of machine learning frameworks depends heavily on their underlying
    data organization. While machine learning theory can be expressed through mathematical
    equations, turning these equations into practical implementations demands thoughtful
    consideration of data organization, storage, and manipulation. Modern machine
    learning models must process enormous amounts of data during training and inference,
    making efficient data access and memory usage critical across diverse hardware
    platforms.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架的有效性在很大程度上取决于其底层的数据组织。虽然机器学习理论可以通过数学方程式表达，但将这些方程式转化为实际实现需要仔细考虑数据组织、存储和处理。现代机器学习模型在训练和推理过程中必须处理大量数据，这使得在多样化的硬件平台上实现高效的数据访问和内存使用变得至关重要。
- en: A framework’s data structures must excel in three key areas. First, they must
    deliver high performance, supporting rapid data access and efficient memory use
    across different hardware. This includes optimizing memory layouts for cache efficiency
    and enabling smooth data transfer between memory hierarchies and devices. Second,
    they must offer flexibility, accommodating various model architectures and training
    approaches while supporting different data types and precision requirements. Third,
    they should provide clear and intuitive interfaces to developers while handling
    complex memory management and device placement behind the scenes.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的数据结构必须在三个关键领域表现出色。首先，它们必须提供高性能，支持在不同硬件上快速的数据访问和高效的内存使用。这包括优化内存布局以提高缓存效率，并允许在不同内存层次结构和设备之间实现平滑的数据传输。其次，它们必须提供灵活性，以适应各种模型架构和训练方法，同时支持不同的数据类型和精度要求。第三，它们应该为开发者提供清晰直观的接口，同时在幕后处理复杂的内存管理和设备放置。
- en: These data structures bridge mathematical concepts and practical computing systems.
    The operations in machine learning, such as matrix multiplication, convolution,
    and activation functions, set basic requirements for how data must be organized.
    These structures must maintain numerical precision and stability while enabling
    efficient implementation of common operations and automatic gradient computation.
    However, they must also work within real-world computing constraints, dealing
    with limited memory bandwidth, varying hardware capabilities, and the needs of
    distributed computing.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据结构架起了数学概念和实际计算系统之间的桥梁。机器学习中的操作，如矩阵乘法、卷积和激活函数，为数据组织设定了基本要求。这些结构必须在保持数值精度和稳定性的同时，实现常见操作的高效实现和自动梯度计算。然而，它们也必须在现实世界的计算约束下工作，处理有限的内存带宽、变化的硬件能力和分布式计算的需求。
- en: The design choices made in implementing these data structures significantly
    influence what machine learning frameworks can achieve. Poor decisions in data
    structure design can result in excessive memory use, limiting model size and batch
    capabilities. They might create performance bottlenecks that slow down training
    and inference, or produce interfaces that make programming error-prone. On the
    other hand, thoughtful design enables automatic optimization of memory usage and
    computation, efficient scaling across hardware configurations, and intuitive programming
    interfaces that support rapid implementation of new techniques.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现这些数据结构时所做的设计选择，在很大程度上影响了机器学习框架能够实现什么。在数据结构设计上的不良决策可能导致过度使用内存，限制模型大小和批量处理能力。它们可能会创建性能瓶颈，减慢训练和推理速度，或者产生易于编程错误的接口。另一方面，深思熟虑的设计能够实现内存使用和计算的自动优化，跨硬件配置的高效扩展，以及支持快速实现新技术直观的编程接口。
- en: By exploring specific data structures, we’ll examine how frameworks address
    these challenges through careful design decisions and optimization approaches.
    This understanding proves essential for practitioners working with machine learning
    systems, whether developing new models, optimizing existing ones, or creating
    new framework capabilities. The analysis begins with tensor abstractions, the
    fundamental building blocks of modern machine learning frameworks, before exploring
    more specialized structures for parameter management, dataset handling, and execution
    control.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索特定的数据结构，我们将研究框架如何通过精心设计决策和优化方法来应对这些挑战。这种理解对于使用机器学习系统的从业者至关重要，无论是开发新模型、优化现有模型还是创建新的框架功能。分析从张量抽象开始，这是现代机器学习框架的基本构建块，然后探索更专业的结构，用于参数管理、数据集处理和执行控制。
- en: Tensors
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量
- en: '***Tensors*** are multidimensional arrays that serve as the fundamental data
    structure in machine learning systems, providing *unified representation* for
    scalars, vectors, matrices, and higher-dimensional data with *hardware-optimized
    operations*.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '***张量***是多维数组，在机器学习系统中作为基本的数据结构，提供了对标量、向量、矩阵以及更高维数据的*统一表示*，并具有*硬件优化的操作*。'
- en: Machine learning frameworks process and store numerical data as tensors. Every
    computation in a neural network, from processing input data to updating model
    weights, operates on tensors. Training batches of images, activation maps in convolutional
    networks, and parameter gradients during backpropagation all take the form of
    tensors. This unified representation allows frameworks to implement consistent
    interfaces for data manipulation and optimize operations across different hardware
    architectures.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架将数值数据作为张量进行处理和存储。神经网络中的每一次计算，从处理输入数据到更新模型权重，都是在张量上进行的。训练图像批次、卷积网络中的激活图以及在反向传播过程中的参数梯度都采取了张量的形式。这种统一的表示方式使得框架能够实现数据操作的一致接口，并优化不同硬件架构上的操作。
- en: Tensor Structure and Dimensions
  id: totrans-349
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 张量结构和维度
- en: 'A tensor is a mathematical object that generalizes scalars, vectors, and matrices
    to higher dimensions. The dimensionality forms a natural hierarchy: a scalar is
    a zero-dimensional tensor containing a single value, a vector is a one-dimensional
    tensor containing a sequence of values, and a matrix is a two-dimensional tensor
    containing values arranged in rows and columns. Higher-dimensional tensors extend
    this pattern through nested structures; for instance, as illustrated in [Figure 7.7](ch013.xhtml#fig-tensor-data-structure-a),
    a three-dimensional tensor can be visualized as a stack of matrices. Therefore,
    vectors and matrices can be considered special cases of tensors with 1D and 2D
    dimensions, respectively.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是一个数学对象，它将标量、向量和矩阵推广到更高维度。维度形成一个自然的层次结构：标量是一个包含单个值的零维张量，向量是一个包含一系列值的单维张量，矩阵是一个包含按行和列排列的值的二维张量。更高维的张量通过嵌套结构扩展这一模式；例如，如图[图7.7](ch013.xhtml#fig-tensor-data-structure-a)所示，一个三维张量可以表示为矩阵的堆叠。因此，向量和矩阵可以被认为是具有1D和2D维度的张量的特殊情况。
- en: '![](../media/file96.svg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file96.svg)'
- en: 'Figure 7.7: **Three-Dimensional Tensor**: Higher-rank tensors extend the concepts
    of scalars, vectors, and matrices by arranging data in nested structures; this
    figure represents a three-dimensional tensor as a stack of matrices, enabling
    representation of complex, multi-dimensional data relationships. Tensors with
    rank greater than two are fundamental to representing data in areas like image
    processing and natural language processing, where data possesses inherent multi-dimensional
    structure.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：**三维张量**：高阶张量通过在嵌套结构中排列数据来扩展标量、向量和矩阵的概念；此图将三维张量表示为矩阵的堆叠，从而能够表示复杂的多维数据关系。阶数大于二的高阶张量对于在图像处理和自然语言处理等领域表示数据至关重要，这些领域的数据具有固有的多维结构。
- en: In practical applications, tensors naturally arise when dealing with complex
    data structures. As illustrated in [Figure 7.8](ch013.xhtml#fig-tensor-data-structure-b),
    image data exemplifies this concept particularly well. Color images comprise three
    channels, where each channel represents the intensity values of red, green, or
    blue as a distinct matrix. These channels combine to create the full colored image,
    forming a natural 3D tensor structure. When processing multiple images simultaneously,
    such as in batch operations, a fourth dimension can be added to create a 4D tensor,
    where each slice represents a complete three-channel image. This hierarchical
    organization demonstrates how tensors efficiently handle multidimensional data
    while maintaining clear structural relationships.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，处理复杂数据结构时自然会涌现出张量。如图[图7.8](ch013.xhtml#fig-tensor-data-structure-b)所示，图像数据特别有效地说明了这一概念。彩色图像由三个通道组成，每个通道代表红色、绿色或蓝色的强度值，作为一个独立的矩阵。这些通道组合在一起形成完整的彩色图像，形成一个自然的3D张量结构。在同时处理多个图像的情况下，例如在批量操作中，可以添加一个第四维来创建4D张量，其中每个切片代表一个完整的三个通道图像。这种层次结构展示了张量如何有效地处理多维数据，同时保持清晰的结构的关联性。
- en: '![](../media/file97.svg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file97.svg)'
- en: 'Figure 7.8: **Multidimensional Data Representation**: Images naturally map
    to tensors with dimensions representing image height, width, and color channels,
    forming a three-dimensional array; stacking multiple images creates a fourth dimension
    for batch processing and efficient computation. *credit: niklas lang [https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff](https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff)*.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：**多维数据表示**：图像自然映射到张量，其维度代表图像高度、宽度和颜色通道，形成一个三维数组；堆叠多个图像创建一个第四维以进行批量处理和高效计算。*来源：niklas
    lang [https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff](https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff)*。
- en: In machine learning frameworks, tensors take on additional properties beyond
    their mathematical definition to meet the demands of modern ML systems. While
    mathematical tensors provide a foundation as multi-dimensional arrays with transformation
    properties, machine learning introduces requirements for practical computation.
    These requirements shape how frameworks balance mathematical precision with computational
    performance.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习框架中，张量除了其数学定义之外还具备额外的属性，以满足现代机器学习系统的需求。虽然数学张量提供了作为具有变换属性的多维数组的基石，但机器学习引入了实际计算的要求。这些要求决定了框架如何在数学精度和计算性能之间取得平衡。
- en: Framework tensors combine numerical data arrays with computational metadata.
    The dimensional structure, or shape, ranges from simple vectors and matrices to
    higher-dimensional arrays that represent complex data like image batches or sequence
    models. This dimensional information plays a critical role in operation validation
    and optimization. Matrix multiplication operations, for example, depend on shape
    metadata to verify dimensional compatibility and determine optimal computation
    paths.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 框架张量将数值数据数组和计算元数据相结合。其维度结构，或形状，范围从简单的向量和矩阵到表示复杂数据（如图像批次或序列模型）的高维数组。这种维度信息在操作验证和优化中起着关键作用。例如，矩阵乘法操作依赖于形状元数据来验证维度兼容性并确定最佳计算路径。
- en: Memory layout implementation introduces distinct challenges in tensor design.
    While tensors provide an abstraction of multi-dimensional data, physical computer
    memory remains linear. Stride patterns address this disparity by creating mappings
    between multi-dimensional tensor indices and linear memory addresses. These patterns
    significantly impact computational performance by determining memory access patterns
    during tensor operations. [Figure 7.9](ch013.xhtml#fig-tensor-memory-layout) demonstrates
    this concept using a 2×3 tensor, showing both row-major and column-major memory
    layouts with their corresponding stride calculations.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 内存布局实现为张量设计引入了独特的挑战。虽然张量提供了多维数据的抽象，但物理计算机内存仍然是线性的。步长模式通过在多维张量索引和线性内存地址之间创建映射来解决这种差异。这些模式通过确定张量操作期间的内存访问模式，对计算性能产生重大影响。[图7.9](ch013.xhtml#fig-tensor-memory-layout)使用2×3张量演示了这一概念，显示了行主序和列主序内存布局及其相应的步长计算。
- en: '![](../media/file98.svg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file98.svg)'
- en: 'Figure 7.9: **Tensor Memory Layout**: A 2×3 tensor can be stored in linear
    memory using either row-major (C-style) or column-major (Fortran-style) ordering.
    Strides define the number of elements to skip in each dimension when moving through
    memory, enabling frameworks to calculate memory addresses for tensor[i,j] as base_address
    + i×stride[0] + j×stride[1]. The choice of memory layout significantly impacts
    cache performance and computational efficiency.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：**张量内存布局**：2×3的张量可以使用行主序（C风格）或列主序（Fortran风格）的顺序存储在线性内存中。步长定义了在通过内存移动时每个维度需要跳过的元素数量，使得框架能够计算张量[i,j]的内存地址为基本地址
    + i×步长[0] + j×步长[1]。内存布局的选择对缓存性能和计算效率有重大影响。
- en: 'Understanding these memory layout patterns is crucial for framework performance
    optimization. Row-major layout (used by NumPy, PyTorch) stores elements row by
    row, making row-wise operations more cache-friendly. Column-major layout (used
    by some BLAS libraries) stores elements column by column, optimizing column-wise
    access patterns. The stride values encode this layout information: in row-major
    layout for a 2×3 tensor, moving to the next row requires skipping 3 elements (stride[0]=3),
    while moving to the next column requires skipping 1 element (stride[1]=1).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些内存布局模式对于框架性能优化至关重要。行主序布局（由NumPy、PyTorch使用）按行存储元素，使得行操作更友好地使用缓存。列主序布局（由一些BLAS库使用）按列存储元素，优化列访问模式。步长值编码了这种布局信息：对于2×3的张量，行主序布局中移动到下一行需要跳过3个元素（步长[0]=3），而移动到下一列需要跳过1个元素（步长[1]=1）。
- en: Careful alignment of stride patterns with hardware memory hierarchies maximizes
    cache efficiency and memory throughput, with optimal layouts achieving 80-90%
    of theoretical memory bandwidth (typically 100-500GB/s on modern GPUs) compared
    to suboptimal patterns that may achieve only 20-30% utilization.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 将步长模式与硬件内存层次结构进行仔细对齐，可以最大化缓存效率和内存吞吐量，最优布局可以达到理论内存带宽的80-90%（在现代GPU上通常是100-500GB/s），而次优模式可能只能达到20-30%的利用率。
- en: Type Systems and Precision
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类型系统和精度
- en: Tensor implementations use type systems to control numerical precision and memory
    consumption. The standard choice in machine learning has been 32-bit floating-point
    numbers (`float32`), offering a balance of precision and efficiency. Modern frameworks
    extend this with multiple numeric types for different needs. Integer types support
    indexing and embedding operations. Reduced-precision types like 16-bit floating-point
    numbers enable efficient mobile deployment. 8-bit integers allow fast inference
    on specialized hardware.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 张量实现使用类型系统来控制数值精度和内存消耗。在机器学习中，标准的选项一直是32位浮点数（`float32`），在精度和效率之间提供了平衡。现代框架通过为不同需求提供多种数值类型来扩展这一点。整数类型支持索引和嵌入操作。低精度类型，如16位浮点数，可以实现高效的移动部署。8位整数允许在专用硬件上进行快速推理。
- en: The choice of numeric type affects both model behavior and computational efficiency.
    Neural network training typically requires float32 precision to maintain stable
    gradient computations. Inference tasks can often use lower precision (`int8` or
    even `int4`), reducing memory usage and increasing processing speed. Mixed-precision
    training approaches combine these benefits by using float32 for critical accumulations
    while performing most computations at lower precision.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 数值类型的选择会影响模型行为和计算效率。神经网络训练通常需要float32精度以保持稳定的梯度计算。推理任务通常可以使用较低精度（`int8`甚至`int4`），减少内存使用并提高处理速度。混合精度训练方法通过使用float32进行关键累积，同时在较低精度下执行大多数计算来结合这些优点。
- en: Type conversions between different numeric representations require careful management.
    Operating on tensors with different types demands explicit conversion rules to
    preserve numerical correctness. These conversions introduce computational costs
    and risk precision loss. Frameworks provide type casting capabilities but rely
    on developers to maintain numerical precision across operations.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 之间不同数值表示的类型转换需要谨慎管理。对具有不同类型的张量进行操作需要显式的转换规则以保持数值正确性。这些转换引入了计算成本并可能导致精度损失。框架提供类型转换功能，但依赖于开发者保持操作过程中的数值精度。
- en: Device and Memory Management
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设备和内存管理
- en: 'The rise of heterogeneous computing has transformed how machine learning frameworks
    manage tensor operations. Modern frameworks must seamlessly operate across CPUs,
    GPUs, TPUs, and various other accelerators, each offering different computational
    advantages and memory characteristics. This diversity creates a fundamental challenge:
    tensors must move efficiently between devices while maintaining computational
    coherency throughout the execution of machine learning workloads.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 异构计算的兴起改变了机器学习框架管理张量操作的方式。现代框架必须无缝地在CPU、GPU、TPU和各种其他加速器之间运行，每个都提供不同的计算优势和内存特性。这种多样性创造了一个基本挑战：张量必须在执行机器学习工作负载的过程中高效地在设备之间移动，同时保持计算一致性。
- en: Device placement decisions significantly influence both computational performance
    and memory utilization. Moving tensors between devices introduces latency costs
    and consumes precious bandwidth on system interconnects. Keeping multiple copies
    of tensors across different devices can accelerate computation by reducing data
    movement, but this strategy increases overall memory consumption and requires
    careful management of consistency between copies. Frameworks must therefore implement
    sophisticated memory management systems that track tensor locations and orchestrate
    data movement while considering these tradeoffs.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 设备放置决策显著影响计算性能和内存利用率。在设备之间移动张量会引入延迟成本，并消耗系统互连上的宝贵带宽。在多个设备上保持张量的多个副本可以通过减少数据移动来加速计算，但这种策略会增加整体内存消耗，并需要仔细管理副本之间的一致性。因此，框架必须实施复杂的内存管理系统，以跟踪张量位置并协调数据移动，同时考虑这些权衡。
- en: These memory management systems maintain a dynamic view of available device
    memory and implement strategies for efficient data transfer. When operations require
    tensors that reside on different devices, the framework must either move data
    or redistribute computation. This decision process integrates deeply with the
    framework’s computational graph execution and operation scheduling. Memory pressure
    on individual devices, data transfer costs, and computational load all factor
    into placement decisions. Modern systems must optimize for data transfer rates
    that range from PCIe Gen4’s 32GB/s for CPU-GPU communication to NVLink’s 600GB/s
    for GPU-to-GPU transfers, with network interconnects typically providing 10-100Gbps
    for cross-node communication.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内存管理系统维护可用设备内存的动态视图，并实施有效的数据传输策略。当操作需要位于不同设备上的张量时，框架必须移动数据或重新分配计算。这个决策过程与框架的计算图执行和操作调度深度集成。单个设备上的内存压力、数据传输成本和计算负载都会影响放置决策。现代系统必须优化数据传输速率，从CPU-GPU通信的PCIe
    Gen4的32GB/s到GPU-to-GPU传输的NVLink的600GB/s，网络互连通常为跨节点通信提供10-100Gbps。
- en: The interplay between device placement and memory management extends beyond
    simple data movement. Frameworks must anticipate future computational needs to
    prefetch data efficiently, manage memory fragmentation across devices, and handle
    cases where memory demands exceed device capabilities. This requires close coordination
    between the memory management system and the operation scheduler, especially in
    scenarios involving parallel computation across multiple devices or distributed
    training across machine boundaries. Efficient prefetching strategies can hide
    latency costs by overlapping data movement with computation, maintaining sustained
    throughput even when individual transfers operate at only 10-20% of peak bandwidth.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 设备放置与内存管理之间的相互作用不仅限于简单的数据移动。框架必须预测未来的计算需求，以有效地预取数据，管理跨设备内存碎片化，并处理内存需求超过设备能力的情况。这需要在内存管理系统和操作调度器之间进行紧密协调，尤其是在涉及跨多个设备的并行计算或跨机器边界分布式训练的场景中。有效的预取策略可以通过将数据移动与计算重叠来隐藏延迟成本，即使在单个传输仅以峰值带宽的10-20%运行时，也能保持持续的吞吐量。
- en: Domain-Specific Data Organizations
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 领域特定数据组织
- en: While tensors are the building blocks of machine learning frameworks, they are
    not the only structures required for effective system operation. Frameworks rely
    on a suite of specialized data structures tailored to address the distinct needs
    of data processing, model parameter management, and execution coordination. These
    structures ensure that the entire workflow, ranging from raw data ingestion to
    optimized execution on hardware, proceeds seamlessly and efficiently.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然张量是机器学习框架的构建块，但它们并不是有效系统操作所需的所有结构。框架依赖于一系列专门的数据结构，这些结构针对数据处理、模型参数管理和执行协调的独特需求进行了定制。这些结构确保整个工作流程，从原始数据摄取到硬件上的优化执行，都能无缝且高效地进行。
- en: Dataset Structures
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集结构
- en: Dataset structures handle the critical task of transforming raw input data into
    a format suitable for machine learning computations. These structures seamlessly
    connect diverse data sources with the tensor abstractions required by models,
    automating the process of reading, parsing, and preprocessing data.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集结构处理将原始输入数据转换为适合机器学习计算格式的关键任务。这些结构无缝地将各种数据源与模型所需的张量抽象连接起来，自动化读取、解析和预处理数据的过程。
- en: Dataset structures must support efficient memory usage while dealing with input
    data far larger than what can fit into memory at once. For example, when training
    on large image datasets, these structures load images from disk, decode them into
    tensor-compatible formats, and apply transformations like normalization or augmentation
    in real time. Frameworks implement mechanisms such as data streaming, caching,
    and shuffling to ensure a steady supply of preprocessed batches without bottlenecks.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集结构必须支持高效的内存使用，同时处理一次无法全部装入内存的大量输入数据。例如，在训练大型图像数据集时，这些结构从磁盘加载图像，将它们解码成与张量兼容的格式，并在实时应用归一化或增强等转换。框架实现数据流、缓存和洗牌等机制，以确保预处理批次供应稳定，无瓶颈。
- en: The design of dataset structures directly impacts training performance. Poorly
    designed structures can create significant overhead, limiting data throughput
    to GPUs or other accelerators. In contrast, well-optimized dataset handling can
    leverage parallelism across CPU cores, disk I/O, and memory transfers to feed
    accelerators at full capacity. Modern training pipelines must sustain data loading
    rates of 1-10GB/s to match GPU computational throughput, requiring careful optimization
    of storage I/O patterns and preprocessing pipelines. Frameworks achieve this through
    techniques like parallel data loading, batch prefetching, and efficient data format
    selection (e.g., optimized formats can reduce loading overhead from 80% to under
    10% of training time).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集结构的设计直接影响训练性能。设计不良的结构可能会产生显著的开销，限制数据吞吐量到GPU或其他加速器。相比之下，优化的数据集处理可以利用CPU核心、磁盘I/O和内存传输的并行性，以全容量向加速器提供数据。现代训练管道必须维持1-10GB/s的数据加载速率，以匹配GPU的计算吞吐量，这需要仔细优化存储I/O模式和预处理管道。框架通过并行数据加载、批预取和高效的数据格式选择（例如，优化的格式可以将加载开销从训练时间的80%降低到10%以下）等技术来实现这一点。
- en: In large, multi-system distributed training scenarios, dataset structures also
    handle coordination between nodes, ensuring that each worker processes a distinct
    subset of data while maintaining consistency in operations like shuffling. This
    coordination prevents redundant computation and supports scalability across multiple
    devices and machines.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型、多系统分布式训练场景中，数据集结构还处理节点间的协调，确保每个工作器处理数据的不同子集，同时在洗牌等操作中保持一致性。这种协调防止了重复计算，并支持跨多个设备和机器的可扩展性。
- en: Parameter Structures
  id: totrans-379
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数结构
- en: Parameter structures store the numerical values that define a machine learning
    model. These include the weights and biases of neural network layers, along with
    auxiliary data such as batch normalization statistics and optimizer state. Unlike
    datasets, which are transient, parameters persist throughout the lifecycle of
    model training and inference.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 参数结构存储定义机器学习模型的数值。这包括神经网络层的权重和偏差，以及辅助数据，如批归一化统计信息和优化器状态。与数据集不同，参数在模型训练和推理的生命周期中持续存在。
- en: The design of parameter structures must balance efficient storage with rapid
    access during computation. For example, convolutional neural networks require
    parameters for filters, fully connected layers, and normalization layers, each
    with unique shapes and memory alignment requirements. Frameworks organize these
    parameters into compact representations that minimize memory consumption while
    enabling fast read and write operations.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 参数结构的设计必须在计算期间的快速访问与高效存储之间取得平衡。例如，卷积神经网络需要滤波器、全连接层和归一化层的参数，每个都有独特的形状和内存对齐要求。框架将这些参数组织成紧凑的表示，以最小化内存消耗并实现快速读写操作。
- en: A key challenge for parameter structures is managing memory efficiently across
    multiple devices ([M. Li et al. 2014](ch058.xhtml#ref-li2014communication)). During
    distributed training, frameworks may replicate parameters across GPUs for parallel
    computation while keeping a synchronized master copy on the CPU. This strategy
    ensures consistency while reducing the latency of gradient updates. Parameter
    structures often leverage memory sharing techniques to minimize duplication, such
    as storing gradients and optimizer states in place to conserve memory. The communication
    costs for parameter synchronization can be substantial. Synchronizing a 7B parameter
    model across 8 GPUs requires transferring approximately 28GB of gradients (assuming
    FP32 precision), which at 25Gbps network speeds takes over 9 seconds without optimization,
    highlighting why frameworks implement gradient compression and efficient communication
    patterns like ring all-reduce.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 参数结构的一个关键挑战是在多个设备之间高效地管理内存（[M. Li等人，2014](ch058.xhtml#ref-li2014communication)）。在分布式训练期间，框架可能会在GPU之间复制参数以实现并行计算，同时在CPU上保持同步的主副本。这种策略确保了一致性，同时减少了梯度更新的延迟。参数结构通常利用内存共享技术来最小化重复，例如将梯度优化器状态存储在原地以节省内存。参数同步的通信成本可能很大。在8个GPU上同步一个7B参数模型需要传输大约28GB的梯度（假设FP32精度），在25Gbps的网络速度下，不进行优化就需要超过9秒，这突显了为什么框架实现梯度压缩和高效的通信模式，如环形全归约。
- en: Parameter structures must also adapt to various precision requirements. While
    training typically uses 32-bit floating-point precision for stability, reduced
    precision such as 16-bit floating-point or even 8-bit integers is increasingly
    used for inference and large-scale training. Frameworks implement type casting
    and mixed-precision management to enable these optimizations without compromising
    numerical accuracy.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 参数结构也必须适应各种精度要求。虽然训练通常使用32位浮点精度以确保稳定性，但为了推理和大规模训练，越来越使用16位浮点精度甚至8位整数精度。框架通过实现类型转换和混合精度管理来启用这些优化，同时不牺牲数值精度。
- en: Execution Structures
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行结构
- en: Execution structures coordinate how computations are performed on hardware,
    ensuring that operations execute efficiently while respecting device constraints.
    These structures work closely with computational graphs, determining how data
    flows through the system and how memory is allocated for intermediate results.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 执行结构协调如何在硬件上执行计算，确保操作在尊重设备约束的同时高效执行。这些结构与计算图紧密合作，确定数据如何通过系统流动以及如何为中间结果分配内存。
- en: One of the primary roles of execution structures is memory management. During
    training or inference, intermediate computations such as activation maps or gradients
    can consume significant memory. Execution structures dynamically allocate and
    deallocate memory buffers to avoid fragmentation and maximize hardware utilization.
    For example, a deep neural network might reuse memory allocated for activation
    maps across layers, reducing the overall memory footprint.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 执行结构的主要角色之一是内存管理。在训练或推理期间，如激活图或梯度等中间计算可能会消耗大量内存。执行结构动态分配和释放内存缓冲区，以避免碎片化并最大化硬件利用率。例如，深度神经网络可能会跨层重用为激活图分配的内存，从而减少整体内存占用。
- en: These structures also handle operation scheduling, ensuring that computations
    are performed in the correct order and with optimal hardware utilization. On GPUs,
    for instance, execution structures can overlap computation and data transfer operations,
    hiding latency and improving throughput. When running on multiple devices, they
    synchronize dependent computations to maintain consistency without unnecessary
    delays.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构还处理操作调度，确保计算以正确的顺序和最优的硬件利用率执行。例如，在 GPU 上，执行结构可以重叠计算和数据传输操作，隐藏延迟并提高吞吐量。在多个设备上运行时，它们同步依赖的计算以避免不必要的延迟，同时保持一致性。
- en: Distributed training introduces additional complexity, as execution structures
    must manage data and computation across multiple nodes. This includes partitioning
    computational graphs, synchronizing gradients, and redistributing data as needed.
    Efficient execution structures minimize communication overhead, allowing distributed
    systems to scale linearly with additional hardware ([McMahan et al. 2017b](ch058.xhtml#ref-mcmahan2023communicationefficient)).
    [Figure 7.10](ch013.xhtml#fig-3d-parallelism) shows how distributed training can
    be defined over a grid of accelerators to parallelize over multiple dimensions
    for faster throughput.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练引入了额外的复杂性，因为执行结构必须在多个节点上管理数据和计算。这包括划分计算图、同步梯度和根据需要重新分配数据。高效的执行结构最小化通信开销，使得分布式系统可以随着额外硬件的增加而线性扩展（[McMahan
    等人 2017b](ch058.xhtml#ref-mcmahan2023communicationefficient)）。[图 7.10](ch013.xhtml#fig-3d-parallelism)
    展示了分布式训练如何在一个加速器网格上定义，以在多个维度上并行化以提高吞吐量。
- en: '![](../media/file99.svg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file99.svg)'
- en: 'Figure 7.10: **3D Parallelism**: Distributed training scales throughput by
    partitioning computation across multiple dimensions: data, pipeline stages, and
    model layers. This enables concurrent execution on a grid of accelerators. This
    approach minimizes communication overhead and maximizes hardware utilization by
    overlapping computation and communication across devices.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：**3D 并行性**：通过在多个维度（数据、管道阶段和模型层）上划分计算，分布式训练通过扩展吞吐量。这允许在加速器网格上并发执行。这种方法通过在设备之间重叠计算和通信来最小化通信开销并最大化硬件利用率。
- en: Programming and Execution Models
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编程和执行模型
- en: The way developers *write* code (the programming model) is closely tied to how
    frameworks *execute* it (the execution model). Understanding this relationship
    reveals why different frameworks make different design trade-offs and how these
    decisions impact both development experience and system performance. This unified
    perspective shows how programming paradigms directly map to execution strategies,
    creating distinct framework characteristics that influence everything from debugging
    workflows to production optimization.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者*编写*代码（编程模型）的方式与框架*执行*它的方式（执行模型）紧密相关。理解这种关系揭示了为什么不同的框架会做出不同的设计权衡，以及这些决策如何影响开发体验和系统性能。这种统一的视角展示了编程范式如何直接映射到执行策略，从而形成具有不同框架特性的独特框架，这些特性影响着从调试工作流程到生产优化的各个方面。
- en: 'In machine learning frameworks, we can identify three primary paradigms that
    combine programming style with execution strategy: imperative programming with
    eager execution, symbolic programming with graph execution, and hybrid approaches
    with just-in-time (JIT) compilation. Each represents a different balance between
    developer flexibility and system optimization capabilities.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习框架中，我们可以识别出三种主要范式，它们将编程风格与执行策略相结合：具有即时执行的命令式编程、具有图执行的符号编程和具有即时编译（JIT）的混合方法。每种方法都代表了开发者灵活性系统优化能力之间不同的平衡。
- en: Declarative Model Definition and Optimized Execution
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 声明式模型定义和优化执行
- en: Symbolic programming involves constructing abstract representations of computations
    first and executing them later. This programming paradigm maps directly to graph
    execution, where the framework builds a complete computational graph before execution
    begins. The tight coupling between symbolic programming and graph execution enables
    powerful optimization opportunities while requiring developers to think in terms
    of complete computational workflows.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 符号编程涉及首先构建计算的抽象表示，然后执行它们。这种编程范式直接映射到图执行，其中框架在执行开始之前构建一个完整的计算图。符号编程与图执行的紧密耦合在需要开发者从完整的计算工作流程的角度思考的同时，提供了强大的优化机会。
- en: For instance, in symbolic programming, variables and operations are represented
    as symbols. These symbolic expressions are not evaluated until explicitly executed,
    allowing the framework to analyze and optimize the computation graph before running
    it.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在符号编程中，变量和操作表示为符号。这些符号表达式在明确执行之前不会进行评估，允许框架在运行之前分析和优化计算图。
- en: Consider the symbolic programming example in [Listing 7.32](ch013.xhtml#lst-symbolic_example).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[列表 7.32](ch013.xhtml#lst-symbolic_example)中的符号编程示例。
- en: 'Listing 7.32: **Symbolic Computation (TensorFlow 1.x)**: Symbolic expressions
    are constructed without immediate evaluation, allowing for optimization before
    execution in machine learning workflows.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.32：**符号计算（TensorFlow 1.x）**：符号表达式在执行前不进行即时评估，允许在机器学习工作流程中执行前进行优化。
- en: '[PRE31]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This approach enables frameworks to apply global optimizations across the entire
    computation, making it efficient for deployment scenarios. Static graphs can be
    serialized and executed across different environments, enhancing portability.
    Predefined graphs also facilitate efficient parallel execution strategies. However,
    debugging can be challenging because errors often surface during execution rather
    than graph construction, and modifying a static graph dynamically is cumbersome.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使框架能够在整个计算上应用全局优化，使其在部署场景中效率更高。静态图可以序列化并在不同的环境中执行，增强了可移植性。预定义的图也促进了高效的并行执行策略。然而，调试可能具有挑战性，因为错误通常在执行过程中而不是在图构建时出现，并且动态修改静态图比较繁琐。
- en: Interactive Development with Immediate Execution
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带即时执行的交互式开发
- en: Imperative programming takes a more traditional approach, executing operations
    immediately as they are encountered. This programming paradigm maps directly to
    eager execution, where operations are computed as soon as they are called. The
    connection between imperative programming and eager execution creates dynamic
    computational graphs that evolve during execution, providing flexibility at the
    cost of optimization opportunities.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 命令式编程采用更传统的做法，在遇到操作时立即执行。这种编程范式直接映射到即时执行，其中操作在调用时立即计算。命令式编程与即时执行之间的联系创建了在执行过程中演变的动态计算图，以牺牲优化机会为代价提供了灵活性。
- en: In this programming paradigm, computations are performed directly as the code
    executes, closely resembling the procedural style of most general-purpose programming
    languages. This is demonstrated in [Listing 7.33](ch013.xhtml#lst-imperative_example),
    where each operation is evaluated immediately.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编程范式下，计算在代码执行时直接进行，与大多数通用编程语言的程序化风格非常相似。这可以在[列表 7.33](ch013.xhtml#lst-imperative_example)中看到，其中每个操作都是立即评估的。
- en: 'Listing 7.33: **Imperative Execution**: Each operation is evaluated immediately
    as the code runs, highlighting how computations proceed step-by-step in dynamic
    computational graphs.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.33：**命令式执行**：每个操作在代码运行时立即评估，突出了计算如何在动态计算图中逐步进行。
- en: '[PRE32]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The immediate execution model is intuitive and aligns with common programming
    practices, making it easier to use. Errors can be detected and resolved immediately
    during execution, simplifying debugging. Dynamic graphs allow for adjustments
    on-the-fly, making them ideal for tasks requiring variable graph structures, such
    as reinforcement learning or sequence modeling. However, the creation of dynamic
    graphs at runtime can introduce computational overhead, and the framework’s ability
    to optimize the entire computation graph is limited due to the step-by-step execution
    process.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 立即执行模型直观且符合常见的编程实践，使其更容易使用。错误可以在执行过程中立即检测和解决，简化了调试。动态图允许即时调整，对于需要可变图结构的任务（如强化学习或序列建模）非常理想。然而，在运行时创建动态图可能会引入计算开销，并且由于逐步执行过程，框架优化整个计算图的能力受到限制。
- en: Performance versus Development Productivity Balance
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能与开发生产力的平衡
- en: The choice between symbolic and imperative programming models significantly
    influences how ML frameworks manage system-level features such as memory management
    and optimization strategies.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在符号编程和命令式编程模型之间的选择显著影响机器学习框架如何管理系统级特性，如内存管理和优化策略。
- en: Performance Considerations
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 性能考虑
- en: In symbolic programming, frameworks can analyze the entire computation graph
    upfront. This allows for efficient memory allocation strategies. For example,
    memory can be reused for intermediate results that are no longer needed during
    later stages of computation. This global view also enables advanced optimization
    techniques such as operation fusion, automatic differentiation, and hardware-specific
    kernel selection. These optimizations make symbolic programming highly effective
    for production environments where performance is critical.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在符号编程中，框架可以在一开始就分析整个计算图。这允许采用高效的内存分配策略。例如，内存可以用于在计算后期阶段不再需要的中间结果。这种全局视角还使得高级优化技术，如操作融合、自动微分和针对特定硬件的内核选择成为可能。这些优化使得符号编程在性能至关重要的生产环境中非常有效。
- en: In contrast, imperative programming makes memory management and optimization
    more challenging since decisions must be made at runtime. Each operation executes
    immediately, which prevents the framework from globally analyzing the computation.
    This trade-off, however, provides developers with greater flexibility and immediate
    feedback during development. Beyond system-level features, the choice of programming
    model also impacts the developer experience, particularly during model development
    and debugging.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，命令式编程使得内存管理和优化更具挑战性，因为决策必须在运行时做出。每个操作立即执行，这阻止了框架对计算进行全局分析。然而，这种权衡为开发者提供了更大的灵活性，并在开发过程中提供了即时反馈。除了系统级特性之外，编程模型的选择还会影响开发者体验，尤其是在模型开发和调试期间。
- en: Development and Debugging
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 开发和调试
- en: Symbolic programming requires developers to conceptualize their models as complete
    computational graphs. This often involves extra steps to inspect intermediate
    values, as symbolic execution defers computation until explicitly invoked. For
    example, in TensorFlow 1.x, developers must use sessions and feed dictionaries
    to debug intermediate results, which can slow down the development process.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 符号编程要求开发者将他们的模型概念化为完整的计算图。这通常需要额外的步骤来检查中间值，因为符号执行将计算推迟到显式调用。例如，在TensorFlow 1.x中，开发者必须使用会话和feed字典来调试中间结果，这可能会减慢开发过程。
- en: Imperative programming offers a more straightforward debugging experience. Operations
    execute immediately, allowing developers to inspect tensor values and shapes as
    the code runs. This immediate feedback simplifies experimentation and makes it
    easier to identify and fix issues in the model. As a result, imperative programming
    is well-suited for rapid prototyping and iterative model development.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 命令式编程提供了一个更直接的调试体验。操作立即执行，允许开发者随着代码的运行检查张量值和形状。这种即时反馈简化了实验，并使得识别和修复模型中的问题变得更加容易。因此，命令式编程非常适合快速原型设计和迭代模型开发。
- en: Managing Trade-offs
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 管理权衡
- en: The choice between symbolic and imperative programming models often depends
    on the specific needs of a project. Symbolic programming excels in scenarios where
    performance and optimization are critical, such as production deployments. In
    contrast, imperative programming provides the flexibility and ease of use necessary
    for research and development.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 符号编程和命令式编程模型之间的选择通常取决于项目的具体需求。符号编程在性能和优化至关重要的场景中表现出色，例如生产部署。相比之下，命令式编程提供了研究和开发所需的灵活性和易用性。
- en: Adaptive Optimization Through Runtime Compilation
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过运行时编译进行自适应优化
- en: Modern frameworks have recognized that the choice between programming paradigms
    doesn’t need to be binary. Hybrid approaches combine the strengths of both paradigms
    through just-in-time (JIT) compilation, allowing developers to write code in an
    imperative style while achieving the performance benefits of graph execution.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架已经认识到，在编程范式之间的选择不一定是二元的。混合方法通过即时编译（JIT）结合了两种范式的优点，允许开发者以命令式风格编写代码，同时实现图执行的性能优势。
- en: 'JIT compilation represents the modern synthesis of programming and execution
    models. Developers write natural, imperative code that executes eagerly during
    development and debugging, but the framework can automatically convert frequently
    executed code paths into optimized static graphs for production deployment. This
    approach provides the best of both worlds: intuitive development experience with
    optimized execution performance.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: JIT编译代表了编程和执行模型的现代综合。开发者编写自然、命令式的代码，在开发和调试期间急切执行，但框架可以自动将频繁执行的代码路径转换为优化后的静态图，以用于生产部署。这种方法提供了两者的最佳结合：直观的开发体验和优化的执行性能。
- en: Examples of this hybrid approach include TensorFlow’s `tf.function` decorator,
    which converts imperative Python functions into optimized graph execution, and
    PyTorch’s `torch.jit.script`, which compiles dynamic PyTorch models into static
    graphs. JAX takes this further with its `jit` transformation that provides automatic
    graph compilation and optimization.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合方法的例子包括TensorFlow的`tf.function`装饰器，它将命令式Python函数转换为优化的图执行，以及PyTorch的`torch.jit.script`，它将动态PyTorch模型编译成静态图。JAX通过其`jit`转换进一步发展，提供了自动图编译和优化。
- en: These hybrid approaches demonstrate how modern frameworks have evolved beyond
    the traditional symbolic vs. imperative divide, recognizing that programming model
    and execution model can be decoupled to provide both developer productivity and
    system performance.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这些混合方法展示了现代框架如何超越了传统的符号与命令式之间的分歧，认识到编程模型和执行模型可以解耦，以提供开发者和系统性能。
- en: Execution Model Technical Implementation
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**执行模型技术实现**'
- en: Having established the three primary programming-execution paradigms, we can
    examine their implementation characteristics and performance implications. Each
    paradigm involves specific trade-offs in memory management, optimization capabilities,
    and development workflows that directly impact system performance and developer
    productivity.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了三种主要的编程-执行范式之后，我们可以检查它们的实现特性和性能影响。每个范式都涉及特定的权衡，包括内存管理、优化能力和开发工作流程，这些都会直接影响系统性能和开发者生产力。
- en: Eager Execution
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**急切执行**'
- en: Eager execution is the most straightforward and intuitive execution paradigm.
    In this model, operations are executed immediately as they are called in the code.
    This approach closely mirrors the way traditional imperative programming languages
    work, making it familiar to many developers.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '**急切执行**是最直接且直观的执行范式。在这个模型中，操作会立即在代码中被调用时执行。这种方法与传统命令式编程语言的工作方式非常相似，因此对许多开发者来说都很熟悉。'
- en: '[Listing 7.34](ch013.xhtml#lst-eager_tf2) demonstrates eager execution, where
    operations are evaluated immediately.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.34](ch013.xhtml#lst-eager_tf2)展示了急切执行，其中操作是立即评估的。'
- en: 'Listing 7.34: **Eager Execution**: Operations are evaluated immediately as
    they are called in the code, providing a more intuitive and flexible development
    experience.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.34：**急切执行**：操作在代码中被调用时立即评估，提供更直观和灵活的开发体验。
- en: '[PRE33]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In this code snippet, each line is executed sequentially. When we create the
    tensors `x` and `y`, they are immediately instantiated in memory. The matrix multiplication
    `tf.matmul(x, y)` is computed right away, and the result is stored in `z`. When
    we print `z`, we see the output of the computation immediately.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，每行都是顺序执行的。当我们创建张量`x`和`y`时，它们会立即在内存中实例化。矩阵乘法`tf.matmul(x, y)`立即计算，并将结果存储在`z`中。当我们打印`z`时，我们立即看到计算的结果。
- en: Eager execution offers several advantages. It provides immediate feedback, allowing
    developers to inspect intermediate values easily. This makes debugging more straightforward
    and intuitive. It also allows for more dynamic and flexible code structures, as
    the computation graph can change with each execution.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '**急切执行**提供了几个优点。它提供了即时反馈，允许开发者轻松检查中间值。这使得调试更加直接和直观。它还允许代码结构更加动态和灵活，因为计算图可以随着每次执行而改变。'
- en: However, eager execution has its trade-offs. Since operations are executed immediately,
    the framework has less opportunity to optimize the overall computation graph.
    This can lead to lower performance compared to more optimized execution paradigms,
    especially for complex models or when dealing with large datasets.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，急切执行也有其权衡。由于操作是立即执行的，框架优化整体计算图的机会较少。这可能导致与更优化的执行范式相比性能较低，尤其是在处理复杂模型或大数据集时。
- en: Eager execution is particularly well-suited for research, interactive development,
    and rapid prototyping. It allows data scientists and researchers to quickly iterate
    on their ideas and see results immediately. Many modern ML frameworks, including
    TensorFlow 2.x and PyTorch, use eager execution as their default mode due to its
    developer-friendly nature.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪执行特别适合研究、交互式开发和快速原型设计。它允许数据科学家和研究人员快速迭代他们的想法并立即看到结果。许多现代 ML 框架，包括 TensorFlow
    2.x 和 PyTorch，由于其开发者友好的特性，使用贪婪执行作为默认模式。
- en: Graph Execution
  id: totrans-433
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图执行
- en: Graph execution, also known as static graph execution, takes a different approach
    to computing operations in ML frameworks. In this paradigm, developers first define
    the entire computational graph, and then execute it as a separate step.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行，也称为静态图执行，在 ML 框架中对计算操作采取了不同的方法。在这个范式下，开发者首先定义整个计算图，然后作为一个单独的步骤执行它。
- en: '[Listing 7.35](ch013.xhtml#lst-tf1_graph_exec) illustrates an example in TensorFlow
    1.x style, which employs graph execution.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.35](ch013.xhtml#lst-tf1_graph_exec) 通过 TensorFlow 1.x 风格的示例说明了图执行。'
- en: 'Listing 7.35: **Graph Execution (TensorFlow 1.x)**: Defines a computational
    graph and provides session-based evaluation to execute it, highlighting the separation
    between graph definition and execution.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.35：**图执行（TensorFlow 1.x）**：定义了一个计算图，并提供基于会话的评估来执行它，突出了图定义与执行之间的分离。
- en: '[PRE34]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In this code snippet, we first define the structure of our computation. The
    `placeholder` operations create nodes in the graph for input data, while `tf.matmul`
    creates a node representing matrix multiplication. No actual computation occurs
    during this definition phase.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们首先定义我们计算的结构。`placeholder` 操作在图中创建用于输入数据的节点，而 `tf.matmul` 创建表示矩阵乘法的节点。在定义阶段不会发生任何实际计算。
- en: The execution of the graph happens when we create a session and call `sess.run()`.
    At this point, we provide the actual input data through the `feed_dict` parameter.
    The framework then has the complete graph and can perform optimizations before
    running the computation.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建会话并调用 `sess.run()` 时，图执行发生。此时，我们通过 `feed_dict` 参数提供实际输入数据。然后框架拥有完整的图，可以在运行计算之前进行优化。
- en: Graph execution offers several advantages. It allows the framework to see the
    entire computation ahead of time, enabling global optimizations that can improve
    performance, especially for complex models. Once defined, the graph can be easily
    saved and deployed across different environments, enhancing portability. It’s
    particularly efficient for scenarios where the same computation is repeated many
    times with different data inputs.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行提供了几个优点。它允许框架提前看到整个计算过程，从而实现全局优化，这可以提高性能，特别是对于复杂模型。一旦定义，图可以轻松保存并部署到不同的环境中，增强可移植性。它特别适用于相同的计算多次重复使用不同数据输入的场景。
- en: However, graph execution also has its trade-offs. It requires developers to
    think in terms of building a graph rather than writing sequential operations,
    which can be less intuitive. Debugging can be more challenging because errors
    often don’t appear until the graph is executed. Implementing dynamic computations
    can be more difficult with a static graph.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图执行也有其权衡之处。它要求开发者从构建图的角度思考，而不是编写顺序操作，这可能不太直观。调试可能更具挑战性，因为错误通常只有在图执行时才会出现。使用静态图实现动态计算可能更困难。
- en: Graph execution is well-suited for production environments where performance
    and deployment consistency are crucial. It is commonly used in scenarios involving
    large-scale distributed training and when deploying models for predictions in
    high-throughput applications.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行非常适合生产环境，在这些环境中性能和部署一致性至关重要。它通常用于涉及大规模分布式训练以及在高吞吐量应用中部署模型进行预测的场景。
- en: Dynamic Code Generation and Optimization
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态代码生成和优化
- en: Just-In-Time compilation[24](#fn24) is a middle ground between eager execution
    and graph execution. This paradigm aims to combine the flexibility of eager execution
    with the performance benefits of graph optimization.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 智能即时编译[24](#fn24) 是贪婪执行和图执行之间的折中方案。这个范式旨在结合贪婪执行的灵活性和图优化的性能优势。
- en: '[Listing 7.36](ch013.xhtml#lst-jit_pytorch) shows how scripted functions are
    compiled and reused in PyTorch.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.36](ch013.xhtml#lst-jit_pytorch) 展示了在 PyTorch 中脚本函数是如何编译和重用的。'
- en: 'Listing 7.36: **PyTorch JIT Compilation**: Compiles scripted functions for
    efficient reuse, illustrating how just-in-time compilation balances flexibility
    and performance in machine learning workflows.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.36：**PyTorch JIT编译**：编译脚本函数以实现高效重用，展示了即时编译如何在机器学习工作流程中平衡灵活性和性能。
- en: '[PRE35]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In this code snippet, we define a function `compute` and decorate it with `@torch.jit.script`.
    This decorator tells PyTorch to compile the function using its JIT compiler. The
    first time `compute` is called, PyTorch analyzes the function, optimizes it, and
    generates efficient machine code. This compilation process occurs just before
    the function is executed, hence the term “Just-In-Time”.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们定义了一个名为`compute`的函数，并用`@torch.jit.script`装饰器装饰它。这个装饰器告诉PyTorch使用其JIT编译器编译函数。第一次调用`compute`时，PyTorch分析函数，优化它，并生成高效的机器代码。这种编译过程发生在函数执行之前，因此得名“即时”。
- en: Subsequent calls to `compute` use the optimized version, potentially offering
    significant performance improvements, especially for complex operations or when
    called repeatedly.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 后续对`compute`的调用将使用优化版本，可能提供显著的性能提升，尤其是在复杂操作或重复调用时。
- en: JIT compilation provides a balance between development flexibility and runtime
    performance. It allows developers to write code in a natural, eager-style manner
    while still benefiting from many of the optimizations typically associated with
    graph execution.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: JIT编译在开发灵活性运行时性能之间提供了平衡。它允许开发者以自然、急切风格的方式编写代码，同时仍然受益于与图执行相关联的许多优化。
- en: This approach offers several advantages. It maintains the immediate feedback
    and intuitive debugging of eager execution, as most of the code still executes
    eagerly. At the same time, it can deliver performance improvements for critical
    parts of the computation. JIT compilation can also adapt to the specific data
    types and shapes being used, potentially resulting in more efficient code than
    static graph compilation.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了几个优点。它保持了急切执行立即反馈和直观调试的特点，因为大部分代码仍然以急切方式执行。同时，它可以为计算的关键部分带来性能提升。JIT编译还可以适应特定的数据类型和形状，可能产生比静态图编译更高效的代码。
- en: However, JIT compilation also has some considerations. The first execution of
    a compiled function may be slower due to the overhead of the compilation process.
    Some complex Python constructs may not be easily JIT-compiled, requiring developers
    to be aware of what can be optimized effectively.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，JIT编译也有一些考虑因素。编译函数的第一次执行可能由于编译过程的开销而较慢。一些复杂的Python结构可能不易于JIT编译，需要开发者了解什么可以被有效优化。
- en: JIT compilation is particularly useful in scenarios where you need both the
    flexibility of eager execution for development and prototyping, and the performance
    benefits of compilation for production or large-scale training. It’s commonly
    used in research settings where rapid iteration is necessary but performance is
    still a concern.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: JIT编译在需要开发原型和生产的灵活性以及编译性能优势的场景中特别有用。它通常用于需要快速迭代但性能仍然是一个关注点的科研环境中。
- en: Many modern ML frameworks incorporate JIT compilation to provide developers
    with a balance of ease-of-use and performance optimization, as shown in [Table 7.2](ch013.xhtml#tbl-mlfm-execmodes).
    This balance manifests across multiple dimensions, from the learning curve that
    gradually introduces optimization concepts to the runtime behavior that combines
    immediate feedback with performance enhancements. The table highlights how JIT
    compilation bridges the gap between eager execution’s programming simplicity and
    graph execution’s performance benefits, particularly in areas like memory usage
    and optimization scope.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代机器学习框架集成了JIT编译，为开发者提供易用性和性能优化的平衡，如[表7.2](ch013.xhtml#tbl-mlfm-execmodes)所示。这种平衡体现在多个维度上，从逐渐引入优化概念的曲线到结合即时反馈和性能提升的运行时行为。表格突出了JIT编译如何弥合急切执行编程简单性和图执行性能优势之间的差距，尤其是在内存使用和优化范围等方面。
- en: 'Table 7.2: **Execution Model Trade-Offs**: Machine learning frameworks offer
    varying execution strategies (eager, graph, and JIT compilation) that balance
    programming flexibility with runtime performance. The table details how each approach
    differs in aspects like debugging ease, memory consumption, and the scope of optimization
    techniques applied during model training and inference.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2：**执行模型权衡**：机器学习框架提供不同的执行策略（急切执行、图执行和即时编译），在编程灵活性和运行时性能之间取得平衡。该表详细说明了每种方法在调试便捷性、内存消耗以及模型训练和推理期间应用优化技术范围方面的差异。
- en: '| **Aspect** | **Eager Execution** | **Graph Execution** | **JIT Compilation**
    |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **急切执行** | **图执行** | **即时编译** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Approach** | Computes each operation immediately when encountered | Builds
    entire computation plan first, then executes | Analyzes code at runtime, creates
    optimized version |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | 遇到每个操作时立即计算 | 首先构建整个计算计划，然后执行 | 在运行时分析代码，创建优化版本 |'
- en: '| **Memory Usage** | Holds intermediate results throughout computation | Optimizes
    memory by planning complete data flow | Adapts memory usage based on actual execution
    patterns |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| **内存使用** | 在整个计算过程中保持中间结果 | 通过规划完整的数据流来优化内存 | 根据实际执行模式调整内存使用 |'
- en: '| **Optimization Scope** | Limited to local operation patterns | Global optimization
    across entire computation chain | Combines runtime analysis with targeted optimizations
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| **优化范围** | 限于局部操作模式 | 在整个计算链上全局优化 | 将运行时分析与有针对性的优化相结合 |'
- en: '| **Debugging Approach** | Examine values at any point during computation |
    Must set up specific monitoring points in graph | Initial runs show original behavior,
    then optimizes |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| **调试方法** | 在计算过程中的任何一点检查值 | 必须在图中设置特定的监控点 | 初始运行显示原始行为，然后进行优化 |'
- en: '| **Speed vs Flexibility** | Prioritizes flexibility over speed | Prioritizes
    performance over flexibility | Balances flexibility and performance |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| **速度与灵活性** | 优先考虑灵活性而非速度 | 优先考虑性能而非灵活性 | 平衡灵活性和性能 |'
- en: Distributed Execution
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布式执行
- en: As machine learning models continue to grow in size and complexity, training
    them on a single device is often no longer feasible. Large models require significant
    computational power and memory, while massive datasets demand efficient processing
    across multiple machines. To address these challenges, modern AI frameworks provide
    built-in support for distributed execution, allowing computations to be split
    across multiple GPUs, TPUs, or distributed clusters. By abstracting the complexities
    of parallel execution, these frameworks enable practitioners to scale machine
    learning workloads efficiently while maintaining ease of use.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型在规模和复杂性上不断增长，在单个设备上训练它们通常不再可行。大型模型需要大量的计算能力和内存，而大规模数据集需要跨多台机器的高效处理。为了解决这些挑战，现代AI框架提供了内置的分布式执行支持，允许计算在多个GPU、TPU或分布式集群之间分割。通过抽象并行执行的复杂性，这些框架使从业者能够高效地扩展机器学习工作负载，同时保持易用性。
- en: 'At the essence of distributed execution are two primary strategies: data parallelism[25](#fn25)
    and model parallelism[26](#fn26). Data parallelism allows multiple devices to
    train the same model on different subsets of data, ensuring faster convergence
    without increasing memory requirements. Model parallelism, on the other hand,
    partitions the model itself across multiple devices, allowing the training of
    architectures too large to fit into a single device’s memory. While model parallelism
    comes in several variations explored in detail in [Chapter 8](ch014.xhtml#sec-ai-training),
    both techniques are essential for training modern machine learning models efficiently.
    These distributed execution strategies become increasingly important as models
    scale to the sizes discussed in [Chapter 9](ch015.xhtml#sec-efficient-ai), and
    their implementation requires the hardware acceleration techniques covered in
    [Chapter 11](ch017.xhtml#sec-ai-acceleration).'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式执行的核心是两种主要策略：数据并行[25](#fn25) 和模型并行[26](#fn26)。数据并行允许多个设备在不同的数据子集上训练相同的模型，确保在没有增加内存需求的情况下更快地收敛。另一方面，模型并行将模型本身分割到多个设备上，允许训练那些无法适应单个设备内存的架构。虽然模型并行有几种变体，这些变体在[第8章](ch014.xhtml#sec-ai-training)中进行了详细探讨，但两种技术对于高效训练现代机器学习模型都是必不可少的。随着模型规模增长到[第9章](ch015.xhtml#sec-efficient-ai)中讨论的大小，这些分布式执行策略变得越来越重要，它们的实现需要[第11章](ch017.xhtml#sec-ai-acceleration)中涵盖的硬件加速技术。
- en: Data Parallelism
  id: totrans-466
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据并行
- en: Data parallelism is the most widely used approach for distributed training,
    enabling machine learning models to scale across multiple devices while maintaining
    efficiency. In this method, each computing device holds an identical copy of the
    model but processes a unique subset of the training data, as illustrated in [Figure 7.11](ch013.xhtml#fig-data-fm-parallelism).
    Once the computations are complete, the gradients computed on each device are
    synchronized before updating the model parameters, ensuring consistency across
    all copies. This approach allows models to learn from larger datasets in parallel
    without increasing memory requirements per device.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行是分布式训练中最广泛使用的方法，它使得机器学习模型能够在多个设备上扩展，同时保持效率。在此方法中，每个计算设备持有模型的一个相同副本，但处理的是训练数据的独特子集，如图[图7.11](ch013.xhtml#fig-data-fm-parallelism)所示。一旦计算完成，每个设备上计算出的梯度在更新模型参数之前会进行同步，确保所有副本的一致性。这种方法允许模型并行地从更大的数据集中学习，而无需增加每个设备的内存需求。
- en: '![](../media/file100.svg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file100.svg)'
- en: Figure 7.11
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11
- en: Data parallelism distributes training data across multiple devices while maintaining
    identical model copies on each device, enabling significant speedup for large
    datasets. AI frameworks provide built-in mechanisms to manage the key challenges
    of data parallel execution, including data distribution, gradient synchronization,
    and performance optimization. In PyTorch, the `DistributedDataParallel (DDP)`
    module automates these tasks, ensuring efficient training across multiple GPUs
    or nodes. TensorFlow offers `tf.distribute.MirroredStrategy`, which enables seamless
    gradient synchronization for multi-GPU training. Similarly, JAX’s `pmap()` function
    facilitates parallel execution across multiple accelerators, optimizing inter-device
    communication to reduce overhead. These frameworks abstract the complexity of
    gradient aggregation, which can require 10-100Gbps network bandwidth for large
    models. For instance, synchronizing gradients for a 175B parameter model across
    1024 GPUs requires communicating approximately 700GB of data per training step
    (FP32 precision), necessitating sophisticated algorithms to achieve near-linear
    scaling efficiency.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行将训练数据分布在多个设备上，同时在每个设备上保持相同的模型副本，这为大型数据集提供了显著的加速。AI框架提供了内置机制来管理数据并行执行的关键挑战，包括数据分布、梯度同步和性能优化。在PyTorch中，`DistributedDataParallel
    (DDP)`模块自动化这些任务，确保跨多个GPU或节点的有效训练。TensorFlow提供`tf.distribute.MirroredStrategy`，它使得多GPU训练的梯度同步无缝。类似地，JAX的`pmap()`函数促进了跨多个加速器的并行执行，优化了设备间的通信以减少开销。这些框架抽象了梯度聚合的复杂性，对于大型模型可能需要10-100Gbps的网络带宽。例如，同步一个175B参数模型在1024个GPU上的梯度，每个训练步骤（FP32精度）需要大约700GB的数据通信，这需要复杂的算法来实现接近线性的扩展效率。
- en: 'By handling synchronization and communication automatically, these frameworks
    make distributed training accessible to a wide range of users, from researchers
    exploring novel architectures to engineers deploying large-scale AI systems. The
    implementation details vary, but the fundamental goal remains the same: enabling
    efficient multi-device training without requiring users to manually manage low-level
    parallelization.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动处理同步和通信，这些框架使得分布式训练对广泛的用户变得可行，从探索新型架构的研究人员到部署大规模AI系统的工程师。虽然实现细节各不相同，但基本目标保持一致：在不要求用户手动管理底层并行化的情况下，实现高效的跨设备训练。
- en: Model Parallelism
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型并行
- en: While data parallelism is effective for many machine learning workloads, some
    models are too large to fit within the memory of a single device. Model parallelism
    addresses this limitation by partitioning the model itself across multiple devices,
    allowing each to process a different portion of the computation. Unlike data parallelism,
    where the entire model is replicated on each device, model parallelism divides
    layers, tensors, or specific operations among available hardware resources, as
    shown in [Figure 7.12](ch013.xhtml#fig-fm-model-parallelism). This approach enables
    training of large-scale models that would otherwise be constrained by single-device
    memory limits.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据并行对于许多机器学习工作负载来说非常有效，但有些模型太大，无法适应单个设备的内存。模型并行通过将模型本身分割到多个设备上来解决这一限制，使得每个设备可以处理计算的不同部分。与数据并行不同，在数据并行中整个模型在每个设备上都会被复制，模型并行将层、张量或特定操作分配到可用的硬件资源中，如图[图7.12](ch013.xhtml#fig-fm-model-parallelism)所示。这种方法使得训练大规模模型成为可能，否则这些模型会受到单设备内存限制的约束。
- en: '![](../media/file101.svg)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file101.svg)'
- en: Figure 7.12
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.12
- en: Model parallelism addresses memory constraints by distributing different parts
    of the model across multiple devices, enabling training of models too large for
    a single device. AI frameworks provide structured APIs to simplify model parallel
    execution, abstracting away much of the complexity associated with workload distribution
    and communication. PyTorch supports pipeline parallelism through `torch.distributed.pipeline.sync`,
    enabling different GPUs to process sequential layers of a model while maintaining
    efficient execution flow. TensorFlow’s `TPUStrategy` allows for automatic partitioning
    of large models across TPU cores, optimizing execution for high-speed interconnects.
    Frameworks like DeepSpeed and Megatron-LM extend PyTorch by implementing advanced
    model sharding techniques, including tensor parallelism, which splits model weights
    across multiple devices to reduce memory overhead. These techniques must manage
    substantial communication overhead. Tensor parallelism typically requires 100-400GB/s
    inter-device bandwidth to maintain efficiency, while pipeline parallelism can
    operate effectively with lower bandwidth (10-50Gbps) due to less frequent but
    larger activation transfers between pipeline stages.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行通过将模型的各个部分分布到多个设备上，解决了内存限制问题，使得单个设备无法训练的大型模型得以训练。AI框架提供了结构化的API来简化模型并行执行，抽象出与工作负载分配和通信相关的许多复杂性。PyTorch通过`torch.distributed.pipeline.sync`支持管道并行，允许不同的GPU处理模型的连续层，同时保持高效的执行流程。TensorFlow的`TPUStrategy`允许在TPU核心之间自动划分大型模型，优化高速互连的执行。DeepSpeed和Megatron-LM等框架通过实现高级模型分片技术扩展PyTorch，包括张量并行，这种技术将模型权重分布在多个设备上以减少内存开销。这些技术必须管理大量的通信开销。张量并行通常需要100-400GB/s的设备间带宽以保持效率，而管道并行由于管道阶段之间较少但较大的激活传输，可以在较低的带宽（10-50Gbps）下有效运行。
- en: There are multiple variations of model parallelism, each suited to different
    architectures and hardware configurations. Multiple parallelism strategies exist
    for different architectures and hardware configurations. The specific trade-offs
    and applications of these techniques are explored in [Chapter 8](ch014.xhtml#sec-ai-training)
    for distributed training strategies, and [Figure 7.13](ch013.xhtml#fig-tensor-vs-pipeline-parallelism)
    shows some initial intuition in comparing parallelism strategies. Regardless of
    the exact approach, AI frameworks play an important role in managing workload
    partitioning, scheduling computations efficiently, and minimizing communication
    overhead, ensuring that even the largest models can be trained at scale.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行有多种变体，每种都适合不同的架构和硬件配置。对于不同的架构和硬件配置，存在多种并行策略。这些技术的具体权衡和用途在[第8章](ch014.xhtml#sec-ai-training)中进行了探讨，该章节讨论了分布式训练策略，[图7.13](ch013.xhtml#fig-tensor-vs-pipeline-parallelism)展示了比较并行策略的一些初步直觉。无论具体方法如何，AI框架在管理工作负载分区、高效调度计算和最小化通信开销方面发挥着重要作用，确保即使是最大的模型也能进行大规模训练。
- en: '![](../media/file102.svg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file102.svg)'
- en: 'Figure 7.13: **Parallelism Strategies**: Tensor parallelism shards individual
    layers across multiple devices, reducing per-device memory requirements, while
    pipeline parallelism distributes consecutive layers to different devices, increasing
    throughput by overlapping computation and communication. This figure contrasts
    these approaches, highlighting how tensor parallelism replicates layer parameters
    across devices and pipeline parallelism partitions the model’s computational graph.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 7.13: **并行策略**：张量并行将单个层分散到多个设备上，减少了每个设备的内存需求，而管道并行将连续层分布到不同的设备上，通过重叠计算和通信来增加吞吐量。此图对比了这些方法，突出了张量并行如何在设备间复制层参数，以及管道并行如何划分模型的计算图。'
- en: Core Operations
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心操作
- en: Machine learning frameworks employ a three-layer operational hierarchy that
    transforms high-level model descriptions into efficient hardware computations.
    [Figure 7.14](ch013.xhtml#fig-mlfm-core-ops) illustrates how hardware abstraction
    operations manage computing platform complexity, basic numerical operations implement
    mathematical computations, and system-level operations coordinate resources and
    execution.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架采用三层操作层次结构，将高级模型描述转换为高效的硬件计算。[图7.14](ch013.xhtml#fig-mlfm-core-ops)说明了硬件抽象操作如何管理计算平台复杂性，基本数值操作实现数学计算，以及系统级操作如何协调资源和执行。
- en: '![](../media/file103.svg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file103.svg)'
- en: 'Figure 7.14: **Framework Operational Hierarchy**: Machine learning frameworks
    abstract hardware complexities through layered operations (scheduling, memory
    management, and resource optimization), enabling efficient execution of mathematical
    models on diverse computing platforms. This hierarchical structure transforms
    high-level model descriptions into practical implementations by coordinating resources
    and managing computations.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：**框架操作层次结构**：机器学习框架通过分层操作（调度、内存管理和资源优化）抽象硬件复杂性，使数学模型能够在不同的计算平台上高效执行。这种层次结构通过协调资源和管理工作，将高级模型描述转换为实际实现。
- en: Hardware Abstraction Operations
  id: totrans-484
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件抽象操作
- en: Hardware abstraction operations form the foundation layer, isolating higher
    levels from platform-specific details while maintaining computational efficiency.
    This layer handles compute kernel management, memory system abstraction, and execution
    control across diverse computing platforms.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件抽象操作构成了基础层，隔离了高级别与平台特定细节，同时保持计算效率。这一层处理计算内核管理、内存系统抽象以及跨不同计算平台的执行控制。
- en: Compute Kernel Management
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算内核管理
- en: Compute kernel management involves selecting and dispatching optimal implementations
    of mathematical operations for different hardware architectures. This requires
    maintaining multiple implementations of core operations and sophisticated dispatch
    logic. For example, a matrix multiplication operation might be implemented using
    AVX-512 vector instructions on modern CPUs, [cuBLAS](https://developer.nvidia.com/cublas)
    on NVIDIA GPUs, or specialized tensor processing instructions on AI accelerators.
    The kernel manager must consider input sizes, data layout, and hardware capabilities
    when selecting implementations. It must also handle fallback paths for when specialized
    implementations are unavailable or unsuitable.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 计算内核管理涉及为不同的硬件架构选择和调度数学运算的最佳实现。这需要维护多个核心操作的实现和复杂的调度逻辑。例如，矩阵乘法运算可能使用现代CPU上的AVX-512向量指令、NVIDIA
    GPU上的[cuBLAS](https://developer.nvidia.com/cublas)或AI加速器上的专用张量处理指令来实现。内核管理器在选择实现时必须考虑输入大小、数据布局和硬件能力。它还必须处理当专用实现不可用或不合适时的回退路径。
- en: Memory System Abstraction
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存系统抽象
- en: Memory system abstractions manage data movement through complex memory hierarchies.
    These abstractions must handle various memory types (registered, pinned, unified)
    and their specific access patterns. Data layouts often require transformation
    between hardware-preferred formats - for instance, between row-major and column-major
    matrix layouts, or between interleaved and planar image formats. The memory system
    must also manage alignment requirements, which can vary from 4-byte alignment
    on CPUs to 128-byte alignment on some accelerators. Additionally, it handles cache
    coherency issues when multiple execution units access the same data.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 内存系统抽象管理数据在复杂内存层次结构中的移动。这些抽象必须处理各种内存类型（注册的、固定的、统一的）及其特定的访问模式。数据布局通常需要在硬件首选格式之间进行转换——例如，在行主序和列主序矩阵布局之间，或在交错和平面图像格式之间。内存系统还必须管理对齐要求，这些要求可以从CPU上的4字节对齐变化到某些加速器上的128字节对齐。此外，它还处理多个执行单元访问相同数据时的缓存一致性问题时。
- en: Execution Control
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行控制
- en: Execution control operations coordinate computation across multiple execution
    units and memory spaces. This includes managing execution queues, handling event
    dependencies, and controlling asynchronous operations. Modern hardware often supports
    multiple execution streams that can operate concurrently. For example, independent
    GPU streams or CPU thread pools. The execution controller must manage these streams,
    handle synchronization points, and ensure correct ordering of dependent operations.
    It must also provide error handling and recovery mechanisms for hardware-specific
    failures.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 执行控制操作协调多个执行单元和内存空间之间的计算。这包括管理执行队列、处理事件依赖关系和控制异步操作。现代硬件通常支持可以并发操作的多条执行流。例如，独立的GPU流或CPU线程池。执行控制器必须管理这些流，处理同步点，并确保依赖操作的正确顺序。它还必须提供针对特定硬件故障的错误处理和恢复机制。
- en: Basic Numerical Operations
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本数值操作
- en: Building upon the hardware abstraction layer established above, frameworks implement
    fundamental numerical operations balancing mathematical precision with computational
    efficiency. General Matrix Multiply (GEMM) operations dominate ML computational
    costs, following the pattern C = <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>AB
    + <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>C,
    where A, B, and C are matrices, and <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    and <semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    are scaling factors.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述建立的硬件抽象层基础上，框架实现基本的数值运算，在数学精度和计算效率之间取得平衡。通用矩阵乘法（GEMM）操作主导了机器学习的计算成本，遵循以下模式
    C = αAB + βC，其中 A、B 和 C 是矩阵，α 和 β 是缩放因子。
- en: The implementation of GEMM operations requires sophisticated optimization techniques.
    These include blocking for cache efficiency, where matrices are divided into smaller
    tiles that fit in cache memory; loop unrolling to increase instruction-level parallelism;
    and specialized implementations for different matrix shapes and sparsity patterns.
    For example, fully-connected neural network layers typically use regular dense
    GEMM operations, while convolutional layers often employ specialized GEMM variants
    that exploit input locality patterns.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM 操作的实现需要复杂的优化技术。这包括用于缓存效率的阻塞，其中矩阵被分成适合缓存内存的小块；循环展开以增加指令级并行性；以及针对不同矩阵形状和稀疏模式的专用实现。例如，全连接神经网络层通常使用常规密集的
    GEMM 操作，而卷积层通常采用利用输入局部性模式的专用 GEMM 变体。
- en: Beyond GEMM, frameworks must efficiently implement BLAS operations such as vector
    addition (AXPY), matrix-vector multiplication (GEMV), and various reduction operations.
    These operations require different optimization strategies. AXPY operations are
    typically memory-bandwidth limited, while GEMV operations must balance memory
    access patterns with computational efficiency.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 GEMM 之外，框架还必须高效地实现 BLAS 操作，如向量加法（AXPY）、矩阵-向量乘法（GEMV）和不同的归约操作。这些操作需要不同的优化策略。AXPY
    操作通常受内存带宽限制，而 GEMV 操作必须平衡内存访问模式与计算效率。
- en: Element-wise operations form another critical category, including both basic
    arithmetic operations (addition, multiplication) and transcendental functions
    (exponential, logarithm, trigonometric functions). While conceptually simpler
    than GEMM, these operations present significant optimization opportunities through
    vectorization and operation fusion. For example, multiple element-wise operations
    can often be fused into a single kernel to reduce memory bandwidth requirements.
    The efficiency of these operations becomes particularly important in neural network
    activation functions and normalization layers, where they process large volumes
    of data.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级操作形成另一个关键类别，包括基本的算术运算（加法、乘法）和超越函数（指数、对数、三角函数）。虽然从概念上比GEMM简单，但这些操作通过向量化和操作融合提供了显著的优化机会。例如，多个元素级操作通常可以融合成一个单独的内核，以减少内存带宽需求。这些操作的效率在神经网络激活函数和归一化层中尤为重要，在这些层中，它们处理大量数据。
- en: Modern frameworks must also handle operations with varying numerical precision
    requirements. For example, training often requires 32-bit floating-point precision
    for numerical stability, while inference can often use reduced precision formats
    like 16-bit floating-point or even 8-bit integers. Frameworks must therefore provide
    efficient implementations across multiple numerical formats while maintaining
    acceptable accuracy.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架还必须处理具有不同数值精度要求的操作。例如，训练通常需要 32 位浮点精度以保证数值稳定性，而推理通常可以使用降低精度的格式，如 16 位浮点或甚至
    8 位整数。因此，框架必须在保持可接受的精度的同时，提供跨多个数值格式的有效实现。
- en: System-Level Operations
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 系统级操作
- en: System-level operations build upon the computational graph foundation and hardware
    abstractions to manage overall computation flow and resource utilization through
    operation scheduling, memory management, and resource optimization.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级操作建立在计算图基础和硬件抽象之上，通过操作调度、内存管理和资源优化来管理整体计算流程和资源利用。
- en: Operation scheduling leverages the computational graph structure discussed earlier
    to determine execution ordering. Using the static or dynamic graph representation,
    the scheduler must identify parallelization opportunities while respecting dependencies.
    The implementation challenges differ between static graphs, where the entire dependency
    structure is known in advance, and dynamic graphs, where dependencies emerge during
    execution. The scheduler must also handle advanced execution patterns like conditional
    operations and loops that create dynamic control flow within the graph structure.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 操作调度利用前面讨论的计算图结构来确定执行顺序。使用静态或动态图表示，调度器必须在尊重依赖关系的同时识别并行化机会。静态图和动态图的实现挑战不同，静态图中整个依赖结构事先已知，而动态图中依赖关系在执行过程中出现。调度器还必须处理高级执行模式，如条件操作和循环，这些操作在图结构内创建动态控制流。
- en: Memory management implements sophisticated strategies for allocating and deallocating
    memory resources across the computational graph. Different data types require
    different management strategies. Model parameters typically persist throughout
    execution and may require specific memory types for efficient access. Intermediate
    results have bounded lifetimes defined by the operation graph. For example, activation
    values are needed only during the backward pass. The memory manager employs techniques
    like reference counting for automatic cleanup, memory pooling to reduce allocation
    overhead, and workspace management for temporary buffers. It must also handle
    memory fragmentation, particularly in long-running training sessions where allocation
    patterns can change over time.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理在计算图中实现复杂的策略来分配和释放内存资源。不同的数据类型需要不同的管理策略。模型参数通常在整个执行过程中持续存在，可能需要特定的内存类型以实现高效访问。中间结果具有由操作图定义的有限生命周期。例如，激活值仅在反向传播期间需要。内存管理器采用诸如引用计数自动清理、内存池以减少分配开销和工作空间管理以处理临时缓冲区等技术。它还必须处理内存碎片化，特别是在长时间运行的训练会话中，其中分配模式可能会随时间变化。
- en: Resource optimization integrates scheduling and memory decisions to maximize
    performance within system constraints. A key optimization is gradient checkpointing,
    where some intermediate results are discarded and recomputed rather than stored,
    trading computation time for memory savings. The optimizer must also manage concurrent
    execution streams, balancing load across available compute units while respecting
    dependencies. For operations with multiple possible implementations, it selects
    between alternatives based on runtime conditions - for instance, choosing between
    matrix multiplication algorithms based on matrix shapes and system load.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 资源优化将调度和内存决策整合在一起，以在系统约束条件下最大化性能。一个关键的优化是梯度检查点，其中一些中间结果被丢弃并重新计算，而不是存储，以节省内存换取计算时间。优化器还必须管理并发执行流，平衡可用计算单元之间的负载，同时尊重依赖关系。对于具有多种可能实现的操作，它根据运行时条件选择替代方案
    - 例如，根据矩阵形状和系统负载选择矩阵乘法算法。
- en: Together, these operational layers build upon the computational graph foundation
    established in [Section 7.3.1](ch013.xhtml#sec-ai-frameworks-computational-graphs-f0ff)
    to execute machine learning workloads efficiently while abstracting implementation
    complexity from model developers. The interaction between these layers determines
    overall system performance and sets the foundation for advanced optimization techniques
    discussed in [Chapter 10](ch016.xhtml#sec-model-optimizations) and [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作层共同建立在[第7.3.1节](ch013.xhtml#sec-ai-frameworks-computational-graphs-f0ff)中建立的计算图基础上，以高效执行机器学习工作负载，同时从模型开发者那里抽象出实现复杂性。这些层之间的交互决定了整体系统性能，并为[第10章](ch016.xhtml#sec-model-optimizations)和[第11章](ch017.xhtml#sec-ai-acceleration)中讨论的高级优化技术奠定了基础。
- en: Having explored the fundamental concepts enabling framework functionality, we
    now examine how these concepts are packaged into practical development interfaces.
    Framework architecture defines how the underlying computational machinery is exposed
    to developers through APIs and abstractions that balance usability with performance.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了使框架功能化的基本概念之后，我们现在考察这些概念是如何打包成实际开发接口的。框架架构定义了底层计算机制如何通过API和抽象暴露给开发者，这些API和抽象在可用性和性能之间取得平衡。
- en: Framework Architecture
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架架构
- en: While the fundamental concepts provide the computational foundation, practical
    framework usage depends on well-designed architectural interfaces that make this
    power accessible to developers. Framework architecture organizes the capabilities
    we have discussed (computational graphs, execution models, and optimized operations)
    into structured layers that serve different aspects of the development workflow.
    Understanding these architectural choices helps developers leverage frameworks
    effectively and select appropriate tools for their specific requirements.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基本概念提供了计算基础，但实际框架的使用依赖于精心设计的架构接口，这些接口使得这种能力对开发者来说易于访问。框架架构将我们讨论过的能力（计算图、执行模型和优化操作）组织成结构化的层，服务于开发工作流程的不同方面。理解这些架构选择有助于开发者有效地利用框架，并为他们的特定需求选择合适的工具。
- en: APIs and Abstractions
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API和抽象
- en: 'The API layer of machine learning frameworks provides the primary interface
    through which developers interact with the framework’s capabilities. This layer
    must balance multiple competing demands: it must be intuitive enough for rapid
    development, flexible enough to support diverse use cases, and efficient enough
    to enable high-performance implementations.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架的API层提供了开发者与框架功能交互的主要接口。这一层必须平衡多个相互竞争的需求：它必须足够直观以支持快速开发，足够灵活以支持多样化的用例，并且足够高效以实现高性能的实现。
- en: Modern framework APIs implement multiple abstraction levels to address competing
    requirements. Low-level APIs provide direct access to tensor operations and computational
    graph construction, exposing the fundamental operations discussed previously for
    fine-grained control over computation, as illustrated in [Listing 7.37](ch013.xhtml#lst-low_level_api).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架API实现了多个抽象级别，以解决相互竞争的需求。低级API提供了对张量操作和计算图构建的直接访问，暴露了之前讨论的基本操作，以实现对计算的精细控制，如[列表7.37](ch013.xhtml#lst-low_level_api)所示。
- en: 'Listing 7.37: **Manual Tensor Operations**: To perform custom computations
    using pytorch’s low-level API, highlighting the flexibility for defining complex
    transformations.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.37：**手动张量操作**：使用pytorch的低级API执行自定义计算，突出定义复杂转换的灵活性。
- en: '[PRE36]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Building on this low-level foundation, frameworks provide higher-level APIs
    that package common patterns into reusable components. Neural network layers exemplify
    this approach, where pre-built layer abstractions handle implementation details
    rather than requiring manual tensor operations, as shown in [Listing 7.38](ch013.xhtml#lst-mid_level_api).
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个低级基础上，框架提供了更高层次的API，将常见模式打包成可重用的组件。神经网络层就是这种方法的例证，其中预构建的层抽象处理实现细节，而不是需要手动进行张量操作，如[列表7.38](ch013.xhtml#lst-mid_level_api)所示。
- en: 'Listing 7.38: **Mid-Level Abstraction**: Neural networks are constructed using
    layers like convolutions and fully connected layers, showcasing how high-level
    models build upon basic tensor operations for efficient implementation.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.38：**中级抽象**：使用卷积层和全连接层等层构建神经网络，展示了如何通过基本张量操作构建高级模型以实现高效实现。
- en: '[PRE37]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This layered approach culminates in comprehensive workflow automation. At the
    highest level ([Listing 7.39](ch013.xhtml#lst-high_level_api)), frameworks often
    provide model-level abstractions that automate common workflows. For example,
    the Keras API provides a highly abstract interface that hides most implementation
    details:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层方法最终导致全面的工作流程自动化。在最高级别([列表7.39](ch013.xhtml#lst-high_level_api))，框架通常提供模型级别的抽象，以自动化常见的工作流程。例如，Keras
    API提供了一个高度抽象的接口，隐藏了大多数实现细节：
- en: 'Listing 7.39: **High-level model definition**: Defines a convolutional neural
    network architecture using Keras, showcasing layer stacking for feature extraction
    and classification. Training workflow: Automates the training process by compiling
    the model with an optimizer and loss function, then fitting it to data over multiple
    epochs.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.39：**高级模型定义**：使用Keras定义卷积神经网络架构，展示了层堆叠用于特征提取和分类。训练工作流程：通过编译模型并使用优化器和损失函数，然后将其拟合到多个epoch的数据中来自动化训练过程。
- en: '[PRE38]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The organization of these API layers reflects fundamental trade-offs in framework
    design. Lower-level APIs provide maximum flexibility but require more expertise
    to use effectively. Higher-level APIs improve developer productivity but may constrain
    implementation choices. Framework APIs must therefore provide clear paths between
    abstraction levels, allowing developers to mix different levels of abstraction
    as needed for their specific use cases.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这些API层的组织反映了框架设计中的基本权衡。低级API提供了最大的灵活性，但需要更多的专业知识才能有效使用。高级API提高了开发者的生产力，但可能会限制实现选择。因此，框架API必须提供清晰的抽象层之间的路径，允许开发者根据其特定用例的需要混合不同级别的抽象。
- en: These carefully designed API layers provide the interface between developers
    and framework capabilities, but they represent only one component of the complete
    development experience. While APIs define how developers interact with frameworks,
    the complete development experience depends on the broader ecosystem of tools,
    libraries, and resources that surround the core framework. This ecosystem extends
    framework capabilities beyond basic model implementation to encompass the entire
    machine learning lifecycle.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 这些精心设计的API层为开发者和框架功能之间提供了接口，但它们只是完整开发体验的一个组成部分。虽然API定义了开发者如何与框架交互，但完整的开发体验取决于围绕核心框架的更广泛生态系统中的工具、库和资源。这个生态系统将框架能力扩展到基本模型实现之外，涵盖了整个机器学习生命周期。
- en: Framework Ecosystem
  id: totrans-520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框架生态系统
- en: Machine learning frameworks organize their fundamental capabilities into distinct
    components that work together to provide a complete development and deployment
    environment. These components create layers of abstraction that make frameworks
    both usable for high-level model development and efficient for low-level execution.
    Understanding how these components interact helps developers choose and use frameworks
    effectively, particularly as they support the complete ML lifecycle from data
    preprocessing [Chapter 6](ch012.xhtml#sec-data-engineering) through training [Chapter 8](ch014.xhtml#sec-ai-training)
    to deployment [Chapter 13](ch019.xhtml#sec-ml-operations). This ecosystem approach
    bridges the theoretical foundations presented in [Chapter 3](ch009.xhtml#sec-dl-primer)
    with the practical requirements of production ML systems described in [Chapter 2](ch008.xhtml#sec-ml-systems).
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架将它们的基本能力组织成不同的组件，这些组件协同工作，提供完整的发展和部署环境。这些组件创建了抽象层，使得框架既适用于高级模型开发，又适用于低级执行。了解这些组件之间的交互有助于开发者有效地选择和使用框架，尤其是在它们支持完整的机器学习生命周期时，从数据预处理[第6章](ch012.xhtml#sec-data-engineering)到训练[第8章](ch014.xhtml#sec-ai-training)，再到部署[第13章](ch019.xhtml#sec-ml-operations)。这种生态系统方法将[第3章](ch009.xhtml#sec-dl-primer)中提出的理论基础与[第2章](ch008.xhtml#sec-ml-systems)中描述的生产机器学习系统的实际需求联系起来。
- en: Core Libraries
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心库
- en: At the heart of every machine learning framework lies a set of core libraries,
    forming the foundation upon which all other components are built. These libraries
    provide the essential building blocks for machine learning operations, implementing
    fundamental tensor operations that serve as the backbone of numerical computations.
    Heavily optimized for performance, these operations often leverage low-level programming
    languages and hardware-specific optimizations to ensure efficient execution of
    tasks like matrix multiplication, a cornerstone of neural network computations.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习框架的核心都包含一系列核心库，这些库构成了所有其他组件构建的基础。这些库为机器学习操作提供了必要的构建块，实现了作为数值计算骨干的基本张量操作。这些操作经过高度优化以提高性能，通常利用底层编程语言和针对特定硬件的优化，以确保矩阵乘法等任务的执行效率，而矩阵乘法是神经网络计算的基础。
- en: These computational primitives support more sophisticated capabilities. Alongside
    these basic operations, core libraries implement automatic differentiation capabilities,
    enabling the efficient computation of gradients for complex functions. This feature
    is crucial for the gradient-based training that powers most neural network optimization.
    The implementation often involves intricate graph manipulation and symbolic computation
    techniques, abstracting away the complexities of gradient calculation from the
    end-user.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算原语支持更高级的功能。除了这些基本操作外，核心库还实现了自动微分功能，能够高效地计算复杂函数的梯度。这一特性对于大多数神经网络优化的基于梯度的训练至关重要。实现这一功能通常涉及复杂的图操作和符号计算技术，从而将梯度计算的复杂性抽象化，让最终用户无需关注。
- en: These foundational capabilities enable higher-level abstractions that accelerate
    development. Building upon these fundamental operations, core libraries typically
    provide pre-implemented neural network layers such as various neural network layer
    types. These ready-to-use components save developers from reinventing the wheel
    for common model architectures, allowing them to focus on higher-level model design
    rather than low-level implementation details. Similarly, optimization algorithms
    are provided out-of-the-box, further streamlining the model development process.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础能力使得高级抽象得以加速开发。在这些基本操作的基础上，核心库通常提供预实现的神经网络层，如各种神经网络层类型。这些现成的组件让开发者无需为常见的模型架构重新发明轮子，从而能够专注于高级模型设计，而不是低级实现细节。同样，优化算法也提供现成，进一步简化了模型开发过程。
- en: The integration of these components creates a cohesive development environment.
    A simplified example of how these components might be used in practice is shown
    in [Listing 7.40](ch013.xhtml#lst-integrated_example).
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件的集成创造了一个统一的开发环境。以下是如何在实际中可能使用这些组件的简化示例：[列表 7.40](ch013.xhtml#lst-integrated_example)。
- en: 'Listing 7.40: **Training Pipeline**: Machine learning workflows partition datasets
    into training, validation, and test sets to ensure robust model development and
    unbiased evaluation.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.40：**训练流程**：机器学习工作流程将数据集划分为训练集、验证集和测试集，以确保稳健的模型开发和无偏的评估。
- en: '[PRE39]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This example demonstrates how core libraries provide high-level abstractions
    for model creation, loss computation, and optimization, while handling low-level
    details internally. The seamless integration of these components exemplifies how
    core libraries create the foundation for the broader framework ecosystem.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 本例演示了核心库如何提供模型创建、损失计算和优化的高级抽象，同时内部处理低级细节。这些组件的无缝集成展示了核心库如何为更广泛的框架生态系统奠定基础。
- en: Extensions and Plugins
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展和插件
- en: While core libraries offer essential functionality, the true power of modern
    machine learning frameworks often lies in their extensibility. Extensions and
    plugins expand the capabilities of frameworks, allowing them to address specialized
    needs and leverage recent research advances. Domain-specific libraries, for instance,
    cater to particular areas like computer vision or natural language processing,
    providing pre-trained models, specialized data augmentation techniques, and task-specific
    layers.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然核心库提供了基本功能，但现代机器学习框架的真正力量往往在于其可扩展性。扩展和插件扩展了框架的功能，使其能够满足特定需求并利用最新的研究进展。例如，特定领域的库针对特定领域，如计算机视觉或自然语言处理，提供预训练模型、专门的数据增强技术和特定任务的层。
- en: Beyond domain specialization, performance optimization drives another crucial
    category of extensions. Hardware acceleration plugins play an important role in
    performance optimization as it enables frameworks to take advantage of specialized
    hardware like GPUs or TPUs. These plugins dramatically speed up computations and
    allow seamless switching between different hardware backends, a key feature for
    scalability and flexibility in modern machine learning workflows.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 除了领域专业化之外，性能优化驱动着另一个关键的扩展类别。硬件加速插件在性能优化中扮演着重要角色，因为它使得框架能够利用专门的硬件，如GPU或TPU。这些插件显著加快了计算速度，并允许在不同硬件后端之间无缝切换，这是现代机器学习工作流程可扩展性和灵活性的关键特征。
- en: The increasing scale of modern machine learning creates additional extension
    needs. As models and datasets grow in size and complexity, distributed computing
    extensions also become important. These tools enable training across multiple
    devices or machines, handling complex tasks like data parallelism, model parallelism,
    and synchronization between compute nodes. This capability is essential for researchers
    and companies tackling large-scale machine learning problems.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习的规模不断扩大，产生了额外的扩展需求。随着模型和数据集在规模和复杂性上的增长，分布式计算的扩展也变得至关重要。这些工具使得可以在多个设备或机器上训练，处理复杂任务，如数据并行、模型并行和计算节点之间的同步。这种能力对于解决大规模机器学习问题的研究人员和企业至关重要。
- en: To support the research and development process, complementing these computational
    tools are visualization and experiment tracking extensions. Visualization tools
    provide invaluable insights into the training process and model behavior, displaying
    real-time metrics and even offering interactive debugging capabilities. Experiment
    tracking extensions help manage the complexity of machine learning research, allowing
    systematic logging and comparison of different model configurations and hyperparameters.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持研究和开发过程，这些计算工具之外，还需要可视化工具和实验跟踪扩展。可视化工具提供了对训练过程和模型行为的宝贵见解，显示实时指标，甚至提供交互式调试功能。实验跟踪扩展有助于管理机器学习研究的复杂性，允许系统地记录和比较不同的模型配置和超参数。
- en: Integrated Development and Debugging Environment
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成开发和调试环境
- en: Beyond the core framework and its extensions, the ecosystem of development tools
    surrounding a machine learning framework further enhances its effectiveness and
    adoption. Interactive development environments, such as Jupyter notebooks, have
    become nearly ubiquitous in machine learning workflows, allowing for rapid prototyping
    and seamless integration of code, documentation, and outputs. Many frameworks
    provide custom extensions for these environments to enhance the development experience.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心框架及其扩展之外，围绕机器学习框架的开发工具生态系统进一步增强了其有效性和普及度。交互式开发环境，如Jupyter笔记本，在机器学习工作流程中几乎无处不在，允许快速原型设计和代码、文档和输出的无缝集成。许多框架为这些环境提供定制扩展，以增强开发体验。
- en: The complexity of machine learning systems requires specialized development
    support. Debugging and profiling tools address the unique challenges presented
    by machine learning models. Specialized debuggers allow developers to inspect
    the internal state of models during training and inference, while profiling tools
    identify bottlenecks in model execution, guiding optimization efforts. These tools
    are essential for developing efficient and reliable machine learning systems.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的复杂性需要专业的开发支持。调试和性能分析工具解决了机器学习模型带来的独特挑战。专业的调试器允许开发者在训练和推理过程中检查模型的内部状态，而性能分析工具则识别模型执行中的瓶颈，指导优化工作。这些工具对于开发高效和可靠的机器学习系统至关重要。
- en: As projects grow in complexity, version control integration becomes increasingly
    important. Tools that allow versioning of not just code, but also model weights,
    hyperparameters, and training data, help manage the iterative nature of model
    development. This comprehensive versioning approach ensures reproducibility and
    facilitates collaboration in large-scale machine learning projects.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 随着项目复杂性的增加，版本控制集成变得越来越重要。允许对代码、模型权重、超参数和训练数据进行版本控制的工具，有助于管理模型开发的迭代性质。这种全面的版本控制方法确保了可重复性，并促进了大规模机器学习项目中的协作。
- en: Finally, deployment utilities streamline the transition between development
    and production environments. These tools handle tasks like model compression,
    conversion to deployment-friendly formats, and integration with serving infrastructure,
    streamlining the process of moving models from experimental settings to real-world
    applications.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署工具简化了开发和生产环境之间的过渡。这些工具处理诸如模型压缩、转换为部署友好格式以及与服务基础设施集成等任务，从而简化了将模型从实验设置转移到实际应用的过程。
- en: System Integration
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统集成
- en: Moving from development environments to production deployment requires careful
    consideration of system integration challenges. System integration is about implementing
    machine learning frameworks in real-world environments. This section explores
    how ML frameworks integrate with broader software and hardware ecosystems, addressing
    the challenges and considerations at each level of the integration process.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发环境转移到生产部署需要仔细考虑系统集成挑战。系统集成是在现实世界环境中实施机器学习框架。本节探讨了ML框架如何与更广泛的软件和硬件生态系统集成，并解决集成过程中的每个层次的挑战和考虑因素。
- en: Hardware Integration
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件集成
- en: Effective hardware integration is crucial for optimizing the performance of
    machine learning models. Modern ML frameworks must adapt to a diverse range of
    computing environments, from high-performance GPU clusters to resource-constrained
    edge devices.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的硬件集成对于优化机器学习模型性能至关重要。现代ML框架必须适应各种计算环境，从高性能GPU集群到资源受限的边缘设备。
- en: This adaptation begins with accelerated computing platforms. For GPU acceleration,
    frameworks like TensorFlow and PyTorch provide robust support, allowing seamless
    utilization of NVIDIA’s CUDA platform. This integration enables significant speedups
    in both training and inference tasks. Similarly, support for Google’s TPUs in
    TensorFlow allows for even further acceleration of specific workloads.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这种适应从加速计算平台开始。对于GPU加速，TensorFlow和PyTorch等框架提供了强大的支持，允许无缝利用NVIDIA的CUDA平台。这种集成使得训练和推理任务都显著加速。同样，TensorFlow中对Google的TPU的支持允许进一步加速特定的工作负载。
- en: 'In distributed computing scenarios, frameworks must efficiently manage multi-device
    and multi-node setups through sophisticated coordination abstractions. Data parallelism
    replicates the same model across devices and requires all-reduce communication
    patterns. Frameworks implement ring all-reduce algorithms that achieve O(N) communication
    complexity with optimal bandwidth utilization for large gradients, typically achieving
    85-95% of theoretical network bandwidth on high-speed interconnects like InfiniBand
    (100-400Gbps). Model parallelism distributes different model partitions across
    hardware units, necessitating point-to-point communication between partitions
    and careful synchronization of forward and backward passes, with communication
    overhead often consuming 20-40% of total training time when network bandwidth
    falls below 25Gbps per node. At scale, failure becomes inevitable: Google reports
    TPU pod training jobs experience failures every few hours due to memory errors,
    hardware failures, and network partitions. Modern frameworks address this through
    elastic training capabilities that adapt to changing cluster sizes dynamically
    and checkpointing strategies that save model state every N iterations. Frameworks
    like Horovod[27](#fn27) and specialized systems like DeepSpeed have emerged to
    abstract these distributed training complexities across different backend frameworks,
    optimizing communication patterns to sustain training throughput even when aggregate
    network bandwidth utilization exceeds 80% of available capacity.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算场景中，框架必须通过复杂的协调抽象有效地管理多设备和多节点设置。数据并行性在设备间复制相同的模型，并需要全量减少通信模式。框架实现了环形全量减少算法，以最优带宽利用率实现O(N)通信复杂度，对于大型梯度通常在高速互连（如InfiniBand，100-400Gbps）上达到理论网络带宽的85-95%。模型并行性将不同的模型分区分布到硬件单元中，需要分区间的点对点通信以及正向和反向传递的仔细同步，当每个节点的网络带宽低于25Gbps时，通信开销通常消耗总训练时间的20-40%。在规模扩大时，故障变得不可避免：谷歌报告称TPU
    pod训练作业每隔几小时就会因内存错误、硬件故障和网络分区而失败。现代框架通过弹性训练能力来应对这一问题，这些能力可以动态地适应集群大小的变化，并通过每N次迭代保存模型状态的检查点策略。像Horovod[27](#fn27)这样的框架和像DeepSpeed这样的专用系统已经出现，以抽象不同后端框架中的分布式训练复杂性，优化通信模式以维持训练吞吐量，即使当总网络带宽利用率超过可用容量的80%。
- en: For edge deployment, frameworks are increasingly offering lightweight versions
    optimized for mobile and IoT devices. TensorFlow Lite and PyTorch Mobile, for
    instance, provide tools for model compression and optimization, ensuring efficient
    execution on devices with limited computational resources and power constraints.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边缘部署，框架越来越多地提供针对移动和物联网设备优化的轻量级版本。例如，TensorFlow Lite和PyTorch Mobile提供模型压缩和优化的工具，确保在计算资源有限和功率受限的设备上高效执行。
- en: Framework Infrastructure Dependencies
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 框架基础设施依赖
- en: Integrating ML frameworks into existing software stacks presents unique challenges
    and opportunities. A key consideration is how the ML system interfaces with data
    processing pipelines. Frameworks often provide connectors to popular big data
    tools like Apache Spark or Apache Beam, allowing seamless data flow between data
    processing systems and ML training environments.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习框架集成到现有的软件堆栈中带来了独特的挑战和机遇。一个关键考虑因素是机器学习系统如何与数据处理管道接口。框架通常提供连接器，连接到流行的大数据工具，如Apache
    Spark或Apache Beam，允许数据处理系统和机器学习训练环境之间的无缝数据流。
- en: Containerization technologies like Docker have become essential in ML workflows,
    ensuring consistency between development and production environments. Kubernetes
    has emerged as a popular choice for orchestrating containerized ML workloads,
    providing scalability and manageability for complex deployments.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 容器化技术如Docker已成为机器学习工作流程中的关键，确保开发环境和生产环境之间的一致性。Kubernetes已成为编排容器化机器学习工作负载的流行选择，为复杂的部署提供可扩展性和可管理性。
- en: ML frameworks must also interface with other enterprise systems such as databases,
    message queues, and web services. For instance, TensorFlow Serving provides a
    flexible, high-performance serving system for machine learning models, which can
    be easily integrated into existing microservices architectures.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架还必须与其他企业系统（如数据库、消息队列和Web服务）进行交互。例如，TensorFlow Serving提供了一个灵活、高性能的机器学习模型服务系统，可以轻松集成到现有的微服务架构中。
- en: Production Environment Integration Requirements
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产环境集成要求
- en: Deploying ML models to production environments involves several critical considerations.
    Model serving strategies must balance performance, scalability, and resource efficiency.
    Approaches range from batch prediction for large-scale offline processing to real-time
    serving for interactive applications.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习模型部署到生产环境涉及几个关键考虑因素。模型服务策略必须在性能、可扩展性和资源效率之间取得平衡。方法范围从用于大规模离线处理的批量预测到用于交互式应用的实时服务。
- en: Scaling ML systems to meet production demands often involves techniques like
    horizontal scaling of inference servers, caching of frequent predictions, and
    load balancing across multiple model versions. Frameworks like TensorFlow Serving
    and TorchServe provide built-in solutions for many of these scaling challenges.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习系统扩展以满足生产需求通常涉及像推理服务器水平扩展、频繁预测的缓存以及跨多个模型版本之间的负载均衡等技术。TensorFlow Serving和TorchServe等框架为许多这些扩展挑战提供了内置解决方案。
- en: Monitoring and logging are crucial for maintaining ML systems in production.
    This includes tracking model performance metrics, detecting concept drift, and
    logging prediction inputs and outputs for auditing purposes. Tools like Prometheus
    and Grafana are often integrated with ML serving systems to provide comprehensive
    monitoring solutions.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和日志记录对于维护生产中的机器学习系统至关重要。这包括跟踪模型性能指标，检测概念漂移，以及记录预测输入和输出以供审计。像Prometheus和Grafana这样的工具通常与机器学习服务系统集成，以提供全面的监控解决方案。
- en: End-to-End Machine Learning Pipeline Management
  id: totrans-555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端机器学习管道管理
- en: Managing end-to-end ML pipelines requires orchestrating multiple stages, from
    data preparation and model training to deployment and monitoring. MLOps practices
    have emerged to address these challenges, bringing DevOps principles to machine
    learning workflows.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 管理端到端机器学习管道需要协调多个阶段，从数据准备和模型训练到部署和监控。MLOps实践应运而生，以解决这些挑战，将DevOps原则引入机器学习工作流程。
- en: Continuous Integration and Continuous Deployment (CI/CD) practices are being
    adapted for ML workflows. This involves automating model testing, validation,
    and deployment processes. Tools like Jenkins or GitLab CI can be extended with
    ML-specific stages to create robust CI/CD pipelines for machine learning projects.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成和持续部署（CI/CD）实践正在适应机器学习工作流程。这包括自动化模型测试、验证和部署过程。Jenkins或GitLab CI等工具可以通过添加机器学习特定的阶段来扩展，以创建用于机器学习项目的强大CI/CD管道。
- en: Automated model retraining and updating is another critical aspect of ML workflow
    orchestration. This involves setting up systems to automatically retrain models
    on new data, evaluate their performance, and seamlessly update production models
    when certain criteria are met. Frameworks like Kubeflow provide end-to-end ML
    pipelines that can automate many of these processes. [Figure 7.15](ch013.xhtml#fig-workflow-orchestration)
    shows an example orchestration flow, where a user submits DAGs, or directed acyclic
    graphs of workloads to process and train to be executed.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化模型重新训练和更新是机器学习工作流程编排的另一个关键方面。这包括设置系统以自动在新数据上重新训练模型，评估其性能，并在满足某些标准时无缝更新生产模型。像Kubeflow这样的框架提供了端到端的机器学习管道，可以自动化许多这些流程。[图7.15](ch013.xhtml#fig-workflow-orchestration)展示了示例编排流程，其中用户提交DAGs，即工作负载的有向无环图，以进行处理和训练。
- en: Version control for ML assets, including data, model architectures, and hyperparameters,
    is essential for reproducibility and collaboration. Tools like DVC (Data Version
    Control) and MLflow have emerged to address these ML-specific version control
    needs.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习资产（包括数据、模型架构和超参数）进行版本控制对于可重复性和协作至关重要。DVC（数据版本控制）和MLflow等工具的出现是为了解决这些特定的机器学习版本控制需求。
- en: '![](../media/file104.svg)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file104.svg)'
- en: 'Figure 7.15: **Workflow Orchestration**: Data engineering and machine learning
    pipelines benefit from orchestration tools like Airflow, which automate task scheduling,
    distributed execution, and result monitoring for repeatable and scalable model
    training and deployment. Directed acyclic graphs (DAGs) define these workflows,
    enabling complex sequences of operations to be managed efficiently as part of
    a CI/CD system.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：**工作流程编排**：数据工程和机器学习管道从像Airflow这样的编排工具中受益，这些工具自动化任务调度、分布式执行和结果监控，以实现可重复和可扩展的模型训练和部署。有向无环图（DAGs）定义了这些工作流程，使得复杂的操作序列可以有效地作为CI/CD系统的一部分进行管理。
- en: Major Framework Platform Analysis
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主要框架平台分析
- en: Having explored the fundamental concepts, architecture, and ecosystem components
    that define modern frameworks, we now examine how these principles manifest in
    real-world implementations. Machine learning frameworks exhibit considerable architectural
    complexity. Over the years, several machine learning frameworks have emerged,
    each with its unique strengths and ecosystem, but few have remained as industry
    standards. This section examines the established and dominant frameworks in the
    field, analyzing how their design philosophies translate the discussed concepts
    into practical development tools.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了定义现代框架的基本概念、架构和生态系统组件之后，我们现在考察这些原则如何在现实世界的实现中体现。机器学习框架表现出相当大的架构复杂性。多年来，已经出现了几个机器学习框架，每个都有其独特的优势和生态系统，但很少有框架成为行业标准。本节考察了该领域的确立和主导框架，分析其设计哲学如何将讨论的概念转化为实际的开发工具。
- en: TensorFlow Ecosystem
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow生态系统
- en: TensorFlow was developed by the Google Brain team and was released as an open-source
    software library on November 9, 2015\. It was designed for numerical computation
    using data flow graphs and has since become popular for a wide range of machine
    learning applications.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由Google Brain团队开发的，并于2015年11月9日作为开源软件库发布。它旨在使用数据流图进行数值计算，并因此成为广泛机器学习应用的流行选择。
- en: This comprehensive design approach reflects TensorFlow’s production-oriented
    philosophy. TensorFlow is a training and inference framework that provides built-in
    functionality to handle everything from model creation and training to deployment,
    as shown in [Figure 7.16](ch013.xhtml#fig-tensorflow-architecture). Since its
    initial development, the TensorFlow ecosystem has grown to include many different
    “varieties” of TensorFlow, each intended to allow users to support ML on different
    platforms.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的设计方法反映了TensorFlow的生产导向哲学。TensorFlow是一个训练和推理框架，它提供内置功能来处理从模型创建和训练到部署的各个方面，如图[图7.16](ch013.xhtml#fig-tensorflow-architecture)所示。自其最初开发以来，TensorFlow生态系统已经发展到包括许多不同的“品种”的TensorFlow，每种都旨在让用户能够在不同的平台上支持机器学习。
- en: '[TensorFlow Core](https://www.tensorflow.org/tutorials): primary package that
    most developers engage with. It provides a complete, flexible platform for defining,
    training, and deploying machine learning models. It includes [tf.keras](https://www.tensorflow.org/guide/keras)
    as its high-level API.'
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Core](https://www.tensorflow.org/tutorials)：大多数开发者与之交互的主要包。它提供了一个完整、灵活的平台，用于定义、训练和部署机器学习模型。它包括[tf.keras](https://www.tensorflow.org/guide/keras)作为其高级API。'
- en: '[TensorFlow Lite](https://www.tensorflow.org/lite): designed for deploying
    lightweight models on mobile, embedded, and edge devices. It offers tools to convert
    TensorFlow models to a more compact format suitable for limited-resource devices
    and provides optimized pre-trained models for mobile.'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Lite](https://www.tensorflow.org/lite)：专为在移动、嵌入式和边缘设备上部署轻量级模型而设计。它提供工具将TensorFlow模型转换为更适合资源有限设备的更紧凑格式，并为移动设备提供优化的预训练模型。'
- en: '[TensorFlow Lite Micro](https://www.tensorflow.org/lite/microcontrollers):
    designed for running machine learning models on microcontrollers with minimal
    resources. It operates without the need for operating system support, standard
    C or C++ libraries, or dynamic memory allocation, using only a few kilobytes of
    memory.'
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Lite Micro](https://www.tensorflow.org/lite/microcontrollers)：专为在资源有限的微控制器上运行机器学习模型而设计。它无需操作系统支持、标准C或C++库或动态内存分配，仅使用几KB的内存即可运行。'
- en: '[TensorFlow.js](https://www.tensorflow.org/js): JavaScript library that allows
    training and deployment of machine learning models directly in the browser or
    on Node.js. It also provides tools for porting pre-trained TensorFlow models to
    the browser-friendly format.'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow.js](https://www.tensorflow.org/js)：允许在浏览器或Node.js中直接训练和部署机器学习模型的JavaScript库。它还提供工具，用于将预训练的TensorFlow模型转换为浏览器友好的格式。'
- en: '[TensorFlow on Edge Devices (Coral)](https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html):
    platform of hardware components and software tools from Google that allows the
    execution of TensorFlow models on edge devices, leveraging Edge TPUs for acceleration.'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow在边缘设备（Coral）上](https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html)：由Google提供的硬件组件和软件工具平台，允许在边缘设备上执行TensorFlow模型，利用Edge
    TPUs进行加速。'
- en: '[TensorFlow Federated (TFF)](https://www.tensorflow.org/federated): framework
    for machine learning and other computations on decentralized data. TFF facilitates
    federated learning, allowing model training across many devices without centralizing
    the data.'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Federated (TFF)](https://www.tensorflow.org/federated)：用于在去中心化数据上执行机器学习和其他计算的框架。TFF促进了联邦学习，允许在许多设备上跨设备进行模型训练，而不需要集中数据。'
- en: '[TensorFlow Graphics](https://www.tensorflow.org/graphics): library for using
    TensorFlow to carry out graphics-related tasks, including 3D shapes and point
    clouds processing, using deep learning.'
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Graphics](https://www.tensorflow.org/graphics)：使用TensorFlow执行图形相关任务的库，包括3D形状和点云处理，使用深度学习。'
- en: '[TensorFlow Hub](https://www.tensorflow.org/hub): repository of reusable machine
    learning model components to allow developers to reuse pre-trained model components,
    facilitating transfer learning and model composition.'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Hub](https://www.tensorflow.org/hub)：可重用机器学习模型组件的存储库，允许开发者重用预训练模型组件，促进迁移学习和模型组合。'
- en: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving): framework
    designed for serving and deploying machine learning models for inference in production
    environments. It provides tools for versioning and dynamically updating deployed
    models without service interruption.'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)：为生产环境中的推理部署和部署机器学习模型设计的框架。它提供工具，用于版本控制和动态更新已部署模型，而不会中断服务。'
- en: '[TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx): end-to-end platform
    designed to deploy and manage machine learning pipelines in production settings.
    TFX encompasses data validation, preprocessing, model training, validation, and
    serving components.'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx)：为生产环境设计的端到端平台，用于部署和管理机器学习管道。TFX包括数据验证、预处理、模型训练、验证和服务组件。'
- en: '![](../media/file105.svg)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file105.svg)'
- en: 'Figure 7.16: **TensorFlow 2.0 Architecture**: This diagram outlines TensorFlow’s
    modular design, separating eager execution from graph construction for increased
    flexibility and ease of debugging. TensorFlow core provides foundational apis,
    while Keras serves as its high-level interface for simplified model building and
    training, supporting deployment across various platforms and hardware accelerators.
    Source: [TensorFlow.](https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html).'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：**TensorFlow 2.0架构**：此图概述了TensorFlow的模块化设计，将急切执行与图构建分离，以增加灵活性和便于调试。TensorFlow核心提供基础API，而Keras作为其高级接口，简化了模型构建和训练，支持跨各种平台和硬件加速器的部署。来源：[TensorFlow.](https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html).
- en: Production-Scale Deployment
  id: totrans-579
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生产规模部署
- en: 'Real-world production systems demonstrate how framework selection directly
    impacts system performance under operational constraints. Framework optimization
    often achieves dramatic improvements: production systems commonly see 4-10x latency
    reductions and 2-5x cost savings through systematic optimization including quantization,
    operator fusion, and hardware-specific acceleration.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的工业生产系统展示了框架选择如何直接影响在操作约束下的系统性能。框架优化通常可以实现显著的改进：通过包括量化、操作融合和特定硬件加速的系统优化，生产系统通常可以看到4-10倍的延迟减少和2-5倍的成本节约。
- en: However, these optimizations require significant engineering investment, typically
    4-12 weeks of specialized effort for custom operator implementation, validation
    testing, and performance tuning. Framework selection emerges as a systems engineering
    decision that extends far beyond API preferences to encompass the entire optimization
    and deployment pipeline.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些优化需要大量的工程投入，通常需要4-12周的专业努力来实现自定义操作实现、验证测试和性能调整。框架选择成为一个系统工程决策，它远远超出了API偏好的范畴，涵盖了整个优化和部署管道。
- en: The detailed production deployment examples, optimization techniques, and quantitative
    trade-off analysis are covered comprehensively in [Chapter 13](ch019.xhtml#sec-ml-operations),
    where operational constraints and deployment strategies are systematically addressed.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的生产部署示例、优化技术和定量权衡分析在[第13章](ch019.xhtml#sec-ml-operations)中得到了全面覆盖，其中系统地解决了操作约束和部署策略。
- en: PyTorch
  id: totrans-583
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: In contrast to TensorFlow’s production-first approach, PyTorch, developed by
    Facebook’s AI Research lab, has gained significant traction in the machine learning
    community, particularly among researchers and academics. Its design philosophy
    emphasizes ease of use, flexibility, and dynamic computation, which aligns well
    with the iterative nature of research and experimentation.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorFlow的生产优先方法相比，由Facebook的人工智能研究实验室开发的PyTorch在机器学习社区中获得了显著的吸引力，尤其是在研究人员和学者中。其设计理念强调易用性、灵活性和动态计算，这与研究和实验的迭代性质非常吻合。
- en: PyTorch’s research-oriented philosophy manifests in its dynamic computational
    graph system. Unlike TensorFlow’s traditional static graphs, PyTorch builds computational
    graphs on-the-fly during execution through its “define-by-run” approach. This
    enables intuitive model design, easier debugging, and standard Python control
    flow within models. The dynamic approach supports variable-length inputs and complex
    architectures while providing immediate execution and inspection capabilities.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的研究导向哲学体现在其动态计算图系统中。与TensorFlow的传统静态图不同，PyTorch通过“运行时定义”方法在执行过程中动态构建计算图。这使模型设计直观，调试更容易，并在模型内部提供标准的Python控制流。动态方法支持可变长度的输入和复杂架构，同时提供即时执行和检查能力。
- en: PyTorch shares fundamental abstractions with other frameworks, including tensors
    as the core data structure and seamless CUDA integration for GPU acceleration.
    The autograd system automatically tracks operations for gradient-based optimization.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他框架共享基本抽象的PyTorch，包括作为核心数据结构的张量以及无缝的CUDA集成以实现GPU加速。autograd系统自动跟踪操作以进行基于梯度的优化。
- en: JAX
  id: totrans-587
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JAX
- en: JAX represents a third distinct approach, developed by Google Research for high-performance
    numerical computing and advanced machine learning research. Unlike TensorFlow’s
    static graphs or PyTorch’s dynamic execution, JAX centers on functional programming
    principles and composition of transformations.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: JAX代表了一种第三种独特的方法，由Google Research为高性能数值计算和高级机器学习研究开发。与TensorFlow的静态图或PyTorch的动态执行不同，JAX侧重于函数式编程原则和转换的组合。
- en: Built as a NumPy-compatible library with automatic differentiation and just-in-time
    compilation, JAX feels familiar to scientific Python developers while providing
    powerful optimization tools. JAX can differentiate native Python and NumPy functions,
    including those with loops, branches, and recursion, extending beyond simple transformations
    to enable vectorization and JIT compilation.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: JAX作为一个与NumPy兼容的库，具有自动微分和即时编译功能，对科学Python开发者来说感觉熟悉，同时提供了强大的优化工具。JAX可以对原生Python和NumPy函数进行微分，包括具有循环、分支和递归的函数，不仅限于简单的转换，还能实现向量化和即时编译。
- en: JAX’s compilation strategy leverages XLA more centrally than TensorFlow, optimizing
    Python code for various hardware accelerators. The functional programming approach
    uses pure functions and immutable data, creating predictable, easily optimized
    code. JAX’s composable transformations include automatic differentiation (grad),
    vectorization (vmap), and parallel execution (pmap), enabling powerful operations
    that distinguish it from imperative frameworks.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的编译策略比TensorFlow更集中地利用XLA，为各种硬件加速器优化Python代码。函数式编程方法使用纯函数和不可变数据，创建可预测、易于优化的代码。JAX的可组合转换包括自动微分（grad）、向量化（vmap）和并行执行（pmap），这些功能使它区别于命令式框架。
- en: Quantitative Platform Performance Analysis
  id: totrans-591
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量平台性能分析
- en: '[Table 7.3](ch013.xhtml#tbl-mlfm-comparison) provides a concise comparison
    of three major machine learning frameworks: TensorFlow, PyTorch, and JAX. These
    frameworks, while serving similar purposes, exhibit fundamental differences in
    their design philosophies and technical implementations.'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.3](ch013.xhtml#tbl-mlfm-comparison)提供了三个主要机器学习框架：TensorFlow、PyTorch和JAX的简洁比较。虽然这些框架服务于类似的目的，但它们在设计理念和技术实现方面存在根本性的差异。'
- en: 'Table 7.3: **Framework Characteristics**: TensorFlow, PyTorch, and JAX differ
    in their graph construction (static, dynamic, or functional), which influences
    programming style and execution speed. Core distinctions include data mutability
    (arrays in JAX are immutable) and automatic differentiation capabilities, with
    JAX supporting both forward and reverse modes. Performance characteristics shown
    are representative benchmarks that can vary significantly based on workload, hardware
    configuration, and optimization settings. JAX typically achieves higher GPU utilization
    and distributed scaling efficiency, while PyTorch offers the most intuitive debugging
    experience through dynamic graphs.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3：**框架特性**：TensorFlow、PyTorch和JAX在图构建（静态、动态或函数式）方面存在差异，这影响了编程风格和执行速度。核心区别包括数据可变性（JAX中的数组是不可变的）和自动微分能力，JAX支持前向和反向模式。所示的性能特性是代表性的基准，可以根据工作负载、硬件配置和优化设置有显著差异。JAX通常实现更高的GPU利用率和分布式扩展效率，而PyTorch通过动态图提供最直观的调试体验。
- en: '| **Aspect** | **TensorFlow** | **PyTorch** | **JAX** |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **TensorFlow** | **PyTorch** | **JAX** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Graph Type** | Static (1.x), Dynamic (2.x) | Dynamic | Functional transformations
    |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| **图类型** | 静态（1.x），动态（2.x） | 动态 | 函数式转换 |'
- en: '| **Programming Model** | Imperative (2.x), Symbolic (1.x) | Imperative | Functional
    |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| **编程模型** | 命令式（2.x），符号式（1.x） | 命令式 | 函数式 |'
- en: '| **Core Data Structure** | Tensor (mutable) | Tensor (mutable) | Array (immutable)
    |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| **核心数据结构** | 张量（可变） | 张量（可变） | 数组（不可变） |'
- en: '| **Execution Mode** | Eager (2.x default), Graph | Eager | Just-in-time compilation
    |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| **执行模式** | 想法式（2.x默认），图 | 想法式 | 即时编译 |'
- en: '| **Automatic Differentiation** | Reverse mode | Reverse mode | Forward and
    Reverse mode |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| **自动微分** | 反向模式 | 反向模式 | 前向和反向模式 |'
- en: '| **Hardware Acceleration** | CPU, GPU, TPU | CPU, GPU | CPU, GPU, TPU |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| **硬件加速** | CPU，GPU，TPU | CPU，GPU | CPU，GPU，TPU |'
- en: '| **Compilation Optimization** | XLA: 3-10x speedup | TorchScript: 2x | XLA:
    3-10x speedup |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| **编译优化** | XLA：3-10倍加速 | TorchScript：2倍 | XLA：3-10倍加速 |'
- en: '| **Memory Efficiency** | 85% GPU utilization | 82% GPU util. | 91% GPU utilization
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| **内存效率** | 85% GPU利用率 | 82% GPU利用率 | 91% GPU利用率 |'
- en: '| **Distributed Scalability** | 92% efficiency (1024 GPUs) | 88% efficiency
    | 95% efficiency (1024 GPUs) |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| **分布式可扩展性** | 92%效率（1024个GPU） | 88%效率 | 95%效率（1024个GPU） |'
- en: These architectural differences manifest in distinct programming paradigms and
    API design choices. The following example illustrates how the same simple neural
    network (a single linear layer mapping 10 inputs to 1 output) varies dramatically
    across these major frameworks, revealing their fundamental design philosophies.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构差异体现在不同的编程范式和API设计选择上。以下示例说明了相同的简单神经网络（将10个输入映射到1个输出的单个线性层）在这些主要框架中的巨大差异，揭示了它们的基本设计理念。
- en: 'Here’s how the same simple neural network looks across major frameworks to
    illustrate syntax differences:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何在主要框架中查看相同的简单神经网络，以说明语法差异：
- en: '[PRE40]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The PyTorch implementation exemplifies object-oriented design with explicit
    class inheritance from `nn.Module`. Developers define model architecture in `__init__()`
    and computation flow in `forward()`, providing clear separation between structure
    and execution. This imperative style allows dynamic graph construction where the
    computational graph is built during execution, enabling flexible control flow
    and debugging.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的实现展示了面向对象设计，通过从`nn.Module`显式继承类来定义。开发者可以在`__init__()`中定义模型架构，在`forward()`中定义计算流程，从而在结构和执行之间提供清晰的分离。这种命令式风格允许动态图构建，在执行期间构建计算图，从而实现灵活的控制流和调试。
- en: In contrast, TensorFlow/Keras demonstrates declarative programming through sequential
    layer composition. The `Sequential` API abstracts away implementation details,
    automatically handling layer connections, weight initialization, and forward pass
    orchestration behind the scenes. When instantiated, Sequential creates a container
    that manages the computational graph, automatically connecting each layer’s output
    to the next layer’s input. This approach reflects TensorFlow’s evolution toward
    eager execution while maintaining compatibility with graph-based optimization
    for production deployment.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，TensorFlow/Keras通过序列层组合展示了声明式编程。`Sequential` API抽象了实现细节，在幕后自动处理层连接、权重初始化和前向传递编排。实例化时，Sequential创建一个管理计算图的容器，自动将每一层的输出连接到下一层的输入。这种方法反映了TensorFlow向即时执行的演变，同时保持了与基于图的优化在生产部署中的兼容性。
- en: JAX takes a fundamentally different approach, embracing functional programming
    principles with immutable data structures[28](#fn28) and explicit parameter management.
    The `simple_net` function implements the linear transformation manually using
    `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication
    and bias addition that PyTorch and TensorFlow handle automatically. Parameters
    are stored in a dictionary structure (`params`) containing weights `'w'` and bias
    `'b'`, initialized separately using JAX’s random number generation with explicit
    seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[29](#fn29);
    it contains no parameters internally and depends entirely on external parameter
    passing. This design enables powerful program transformations like automatic vectorization[30](#fn30)
    (`vmap`), just-in-time compilation[31](#fn31) (`jit`), and automatic differentiation
    (`grad`) because the function remains mathematically pure[32](#fn32) without hidden
    state or side effects.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: JAX采取了一种根本不同的方法，通过不可变数据结构和显式参数管理来拥抱函数式编程原则[28](#fn28)。`simple_net`函数手动实现线性变换，使用`jnp.dot(x,
    params['w']) + params['b']`，显式执行PyTorch和TensorFlow自动处理的矩阵乘法和偏置添加。参数存储在一个包含权重`'w'`和偏置`'b'`的字典结构（`params`）中，使用JAX的随机数生成和显式播种（`random.PRNGKey(0)`）分别初始化。这种分离意味着模型函数是无状态的[29](#fn29)；它内部不包含任何参数，完全依赖于外部参数传递。这种设计使得像自动向量化[30](#fn30)
    (`vmap`)、即时编译[31](#fn31) (`jit`)和自动微分[32](#fn32) (`grad`)这样的强大程序转换成为可能，因为这些函数在数学上保持纯净，没有隐藏状态或副作用。
- en: Framework Design Philosophy
  id: totrans-611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 框架设计哲学
- en: Beyond technical specifications, machine learning frameworks embody distinct
    design philosophies that reflect their creators’ priorities and intended use cases.
    Understanding these philosophical approaches helps developers choose frameworks
    that align with their project requirements and working styles. The design philosophy
    of a framework influences everything from API design to performance characteristics,
    ultimately affecting both developer productivity and system performance.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架不仅体现了其创造者的优先考虑和预期用例，还体现了独特的设计哲学。理解这些哲学方法有助于开发者选择与项目需求和工作风格相一致的框架。框架的设计哲学影响着从API设计到性能特征的一切，最终影响开发者的生产力和系统性能。
- en: 'Research-First Philosophy: PyTorch'
  id: totrans-613
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 研究为先的哲学：PyTorch
- en: PyTorch exemplifies a research-first philosophy, prioritizing developer experience
    and experimental flexibility over performance optimization. Key design decisions
    include eager execution for immediate inspection capabilities, embracing Python’s
    native control structures rather than domain-specific languages, and exposing
    computational details for precise researcher control. This approach enables rapid
    prototyping and debugging, driving adoption in academic settings where exploration
    and experimentation are paramount.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch体现了以研究为先的哲学，优先考虑开发者体验和实验灵活性，而非性能优化。关键设计决策包括支持即时检查能力的即时执行，采用Python的本地控制结构而非特定领域的语言，以及暴露计算细节以供研究人员精确控制。这种方法使得快速原型设计和调试成为可能，推动了在探索和实验至关重要的学术环境中的采用。
- en: Scalability and Deployment-Optimized Design
  id: totrans-615
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性和部署优化设计
- en: TensorFlow prioritizes production deployment and scalability, reflecting Google’s
    experience with massive-scale machine learning systems. This production-first
    approach emphasizes static graph optimization through XLA compilation, providing
    3-10x performance improvements via operation fusion and hardware-specific code
    generation. The framework includes comprehensive production tools like TensorFlow
    Serving and TFX, designed for distributed deployment and serving at scale. Higher-level
    abstractions like Keras prioritize reliability over flexibility, while API evolution
    emphasizes backward compatibility and gradual migration paths for production stability.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow优先考虑生产部署和可扩展性，反映了谷歌在处理大规模机器学习系统方面的经验。这种以生产为先的方法强调通过XLA编译进行静态图优化，通过操作融合和硬件特定的代码生成提供3-10倍的性能提升。该框架包括全面的生产工具，如TensorFlow
    Serving和TFX，旨在进行分布式部署和大规模服务。高级抽象如Keras优先考虑可靠性而非灵活性，而API的演变强调向后兼容性和生产稳定性的逐步迁移路径。
- en: Mathematical Transformation and Composability Focus
  id: totrans-617
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学转换和可组合性重点
- en: JAX represents a functional programming approach emphasizing mathematical purity
    and program transformation capabilities. Immutable arrays and pure functions enable
    automatic vectorization (`vmap`), parallelization (`pmap`), and differentiation
    (`grad`) without hidden state concerns. Rather than ML-specific abstractions,
    JAX provides general program transformations that compose to create complex behaviors,
    separating computation from execution strategy. While maintaining NumPy compatibility,
    the functional constraints enable powerful optimization capabilities that make
    research code mirror mathematical algorithm descriptions.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: JAX代表了一种强调数学纯净性和程序转换能力的函数式编程方法。不可变数组和纯函数使自动向量化(`vmap`)、并行化(`pmap`)和微分(`grad`)成为可能，而不必担心隐藏状态。JAX不提供特定于机器学习的抽象，而是提供通用的程序转换，这些转换可以组合以创建复杂的行为，将计算与执行策略分离。在保持NumPy兼容性的同时，函数式约束使强大的优化能力成为可能，使研究代码类似于数学算法描述。
- en: Framework Philosophy Alignment with Project Requirements
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与项目需求相匹配的框架哲学
- en: These philosophical differences have practical implications for framework selection.
    Teams engaged in exploratory research often benefit from PyTorch’s research-first
    philosophy. Organizations focused on deploying models at scale may prefer TensorFlow’s
    production-first approach. Research groups working on fundamental algorithmic
    development might choose JAX’s functional approach for program transformation
    and mathematical reasoning.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 这些哲学上的差异对框架选择有实际的影响。从事探索性研究的团队通常从PyTorch以研究为先的哲学中受益。专注于大规模部署模型的组织可能更喜欢TensorFlow以生产为先的方法。致力于基本算法开发的科研团队可能会选择JAX的功能方法来进行程序转换和数学推理。
- en: Understanding these philosophies helps teams anticipate both current capabilities
    and future evolution. PyTorch’s research focus suggests continued investment in
    developer experience. TensorFlow’s production orientation implies ongoing deployment
    and scaling tool development. JAX’s functional philosophy points toward continued
    program transformation exploration.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些哲学有助于团队预测当前的能力和未来的演变。PyTorch的研究重点表明持续投资于开发者体验。TensorFlow的生产导向意味着持续的部署和工具开发。JAX的功能哲学指向持续的程序转换探索。
- en: The choice of framework philosophy often has lasting implications for a project’s
    development trajectory, influencing everything from code organization to debugging
    workflows to deployment strategies. Teams that align their framework choice with
    their fundamental priorities and working styles typically achieve better long-term
    outcomes than those who focus solely on technical specifications.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 框架哲学的选择往往对项目的发展轨迹有持久的影响，影响着从代码组织到调试工作流程再到部署策略的各个方面。将框架选择与他们的基本优先级和工作风格相一致的组织通常比那些只关注技术规格的组织实现更好的长期成果。
- en: Deployment Environment-Specific Frameworks
  id: totrans-623
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对特定部署环境的框架
- en: Beyond the core framework philosophies explored above, machine learning frameworks
    have evolved significantly to meet the diverse needs of different computational
    environments. As ML applications expand beyond traditional data centers to encompass
    edge devices, mobile platforms, and even tiny microcontrollers, the need for specialized
    frameworks has become increasingly apparent.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述探索的核心框架哲学之外，机器学习框架已经显著发展，以满足不同计算环境的不同需求。随着ML应用从传统数据中心扩展到边缘设备、移动平台，甚至微型微控制器，对专用框架的需求变得越来越明显。
- en: This diversification reflects the fundamental challenge of deployment heterogeneity.
    Framework specialization refers to the process of tailoring ML frameworks to optimize
    performance, efficiency, and functionality for specific deployment environments.
    This specialization is crucial because the computational resources, power constraints,
    and use cases vary dramatically across different platforms.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样化反映了部署异质性的基本挑战。框架专业化指的是根据特定部署环境对机器学习框架进行定制，以优化性能、效率和功能。这种专业化至关重要，因为计算资源、功率限制和用例在不同平台之间差异很大。
- en: The proliferation of specialized frameworks creates potential fragmentation
    challenges that the ML community has addressed through standardization efforts.
    Machine learning frameworks have addressed interoperability challenges through
    standardized model formats, with the Open Neural Network Exchange (ONNX)[33](#fn33)
    emerging as a widely adopted solution. ONNX defines a common representation for
    neural network models that enables seamless translation between different frameworks
    and deployment environments.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 专用框架的激增创造了潜在的碎片化挑战，机器学习社区通过标准化努力解决了这些问题。机器学习框架通过标准化模型格式解决了互操作性挑战，其中开放的神经网络交换（ONNX）[33](#fn33)已成为广泛采用解决方案。ONNX定义了神经网络模型的通用表示，使得在不同框架和部署环境之间实现无缝转换成为可能。
- en: This standardization addresses practical workflow needs. The ONNX format serves
    two primary purposes. First, it provides a framework-neutral specification for
    describing model architecture and parameters. Second, it includes runtime implementations
    that can execute these models across diverse hardware platforms. This standardization
    eliminates the need to manually convert or reimplement models when moving between
    frameworks.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标准化解决了实际工作流程需求。ONNX格式有两个主要目的。首先，它提供了一个框架中立的规范，用于描述模型架构和参数。其次，它包括可以在不同硬件平台上执行这些模型的运行时实现。这种标准化消除了在框架之间移动时手动转换或重新实现模型的需要。
- en: In practice, ONNX facilitates important workflow patterns in production machine
    learning systems. For example, a research team may develop and train a model using
    PyTorch’s dynamic computation graphs, then export it to ONNX for deployment using
    TensorFlow’s production-optimized serving infrastructure. Similarly, models can
    be converted to ONNX format for execution on edge devices using specialized runtimes
    like ONNX Runtime. This interoperability, illustrated in [Figure 7.17](ch013.xhtml#fig-onnx),
    has become increasingly important as the machine learning ecosystem has expanded.
    Organizations frequently require leveraging different frameworks’ strengths at
    various stages of the machine learning lifecycle, from research and development.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，ONNX简化了生产级机器学习系统中的重要工作流程模式。例如，一个研究团队可能会使用PyTorch的动态计算图开发和训练一个模型，然后将其导出为ONNX格式，以便使用TensorFlow的生产优化服务基础设施进行部署。同样，模型可以通过使用专门的运行时（如ONNX
    Runtime）转换为ONNX格式，在边缘设备上执行。这种互操作性，如图7.17所示，随着机器学习生态系统的扩展而变得越来越重要。组织通常需要在机器学习生命周期的各个阶段利用不同框架的优势，从研发阶段开始。
- en: '![](../media/file106.jpg)'
  id: totrans-629
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file106.jpg)'
- en: 'Figure 7.17: **Framework Interoperability**: The open neural network exchange
    (ONNX) format enables model portability across machine learning frameworks, allowing
    researchers to train models in one framework (e.g., PyTorch) and deploy them using
    another (e.g., TensorFlow) without code rewriting. This standardization streamlines
    machine learning workflows and facilitates leveraging specialized runtimes like
    ONNX runtime for diverse hardware platforms.'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：**框架互操作性**：开放的神经网络交换（ONNX）格式允许模型在不同机器学习框架之间进行移植，使得研究人员可以在一个框架（例如PyTorch）中训练模型，并使用另一个框架（例如TensorFlow）进行部署，而无需重写代码。这种标准化简化了机器学习工作流程，并促进了在多样化的硬件平台上利用专门的运行时（如ONNX运行时）。
- en: The diversity of deployment targets necessitates distinct specialization strategies
    for different environments. Machine learning deployment environments shape how
    frameworks specialize and evolve. Cloud ML environments leverage high-performance
    servers that offer abundant computational resources for complex operations. Edge
    ML operates on devices with moderate computing power, where real-time processing
    often takes priority. Mobile ML adapts to the varying capabilities and energy
    constraints of smartphones and tablets. Tiny ML functions within the strict limitations
    of microcontrollers and other highly constrained devices that possess minimal
    resources.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 部署目标的多样性需要为不同环境制定不同的专业化策略。机器学习部署环境塑造了框架专业化和演化的方式。云机器学习环境利用高性能服务器，为复杂操作提供丰富的计算资源。边缘机器学习在具有适度计算能力的设备上运行，其中实时处理通常优先。移动机器学习适应智能手机和平板电脑的变异性能和能源限制。TinyML在微控制器和其他资源极少的严格限制设备上运行。
- en: These environmental constraints drive specific architectural decisions. Each
    of these environments presents unique challenges that influence framework design.
    Cloud frameworks prioritize scalability and distributed computing. Edge frameworks
    focus on low-latency inference and adaptability to diverse hardware. Mobile frameworks
    emphasize energy efficiency and integration with device-specific features. TinyML
    frameworks specialize in extreme resource optimization for severely constrained
    environments.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环境限制驱动了特定的架构决策。每个环境都提出了独特的挑战，这些挑战影响着框架的设计。云框架优先考虑可扩展性和分布式计算。边缘框架专注于低延迟推理和对不同硬件的适应性。移动框架强调能源效率和与特定设备功能的集成。TinyML框架专注于在资源严重受限的环境中实现极端资源优化。
- en: We will explore how ML frameworks adapt to each of these environments. We will
    examine the specific techniques and design choices that enable frameworks to address
    the unique challenges of each domain, highlighting the trade-offs and optimizations
    that characterize framework specialization.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨机器学习框架如何适应这些环境中的每一个。我们将检查特定的技术和设计选择，这些技术和设计选择使框架能够解决每个领域的独特挑战，突出框架专业化的权衡和优化。
- en: Distributed Computing Platform Optimization
  id: totrans-634
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式计算平台优化
- en: 'Cloud environments offer the most abundant computational resources, enabling
    frameworks to prioritize scalability and sophisticated optimizations over resource
    constraints. Cloud ML frameworks are sophisticated software infrastructures designed
    to leverage the vast computational resources available in cloud environments.
    These frameworks specialize in three primary areas: distributed computing architectures,
    management of large-scale data and models, and integration with cloud-native services.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 云环境提供了最丰富的计算资源，使框架能够优先考虑可扩展性和复杂的优化，而不是资源限制。云机器学习框架是复杂的软件基础设施，旨在利用云环境中可用的庞大计算资源。这些框架在三个主要领域专业化：分布式计算架构、大规模数据和模型的管理以及与云原生服务的集成。
- en: The first specialization area reflects the scale advantages available in cloud
    deployments. Distributed computing is a fundamental specialization of cloud ML
    frameworks. These frameworks implement advanced strategies for partitioning and
    coordinating computational tasks across multiple machines or graphics processing
    units (GPUs). This capability is essential for training large-scale models on
    massive datasets. Both TensorFlow and PyTorch, two leading cloud ML frameworks,
    offer robust support for distributed computing. TensorFlow’s graph-based approach
    (in its 1.x version) was particularly well-suited for distributed execution, while
    PyTorch’s dynamic computational graph allows for more flexible distributed training
    strategies.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个专业化领域反映了云部署中可用的规模优势。分布式计算是云机器学习框架的基本专业化。这些框架在多台机器或图形处理单元（GPU）之间实施高级策略，以分区和协调计算任务。这种能力对于在大型数据集上训练大规模模型至关重要。TensorFlow和PyTorch这两个领先的云机器学习框架都提供了强大的分布式计算支持。TensorFlow的基于图的（在其1.x版本中）方法特别适合分布式执行，而PyTorch的动态计算图允许更灵活的分布式训练策略。
- en: The ability to handle large-scale data and models is another key specialization.
    Cloud ML frameworks are optimized to work with datasets and models that far exceed
    the capacity of single machines. This specialization is reflected in the data
    structures of these frameworks. For instance, both TensorFlow and PyTorch use
    mutable Tensor objects as their primary data structure, allowing for efficient
    in-place operations on large datasets. JAX, a more recent framework, uses immutable
    arrays, which can provide benefits in terms of functional programming paradigms
    and optimization opportunities in distributed settings.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大规模数据和模型的能力是另一个关键专业化领域。云机器学习框架经过优化，可以处理远超单机容量的数据集和模型。这种专业化体现在这些框架的数据结构中。例如，TensorFlow
    和 PyTorch 都使用可变的 Tensor 对象作为它们的主要数据结构，允许对大型数据集进行高效的就地操作。JAX，作为一个较新的框架，使用不可变数组，这可以在函数式编程范式和分布式环境中的优化机会方面提供好处。
- en: Integration with cloud-native services is the third major specialization area.
    This integration enables automated resource scaling, seamless access to cloud
    storage, and incorporation of cloud-based monitoring and logging systems. The
    execution modes of different frameworks play a role here. TensorFlow 2.x and PyTorch
    both default to eager execution, which allows for easier integration with cloud
    services and debugging. JAX’s just-in-time compilation offers potential performance
    benefits in cloud environments by optimizing computations for specific hardware.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 与云原生服务的集成是第三个主要的专业化领域。这种集成使得资源自动扩展、无缝访问云存储以及整合基于云的监控和日志系统成为可能。不同框架的执行模式在这里发挥着作用。TensorFlow
    2.x 和 PyTorch 默认采用即时执行，这有助于更容易地与云服务集成和调试。JAX 的即时编译通过针对特定硬件优化计算，在云环境中提供了潜在的性能优势。
- en: Hardware acceleration is an important aspect of cloud ML frameworks. All major
    frameworks support CPU and GPU execution, with TensorFlow and JAX also offering
    native support for Google’s TPU. [NVIDIA’s TensorRT](https://developer.nvidia.com/tensorrt)[34](#fn34)
    is an optimization tool dedicated for GPU-based inference, providing sophisticated
    optimizations like layer fusion, precision calibration, and kernel auto-tuning
    to maximize throughput on NVIDIA GPUs. These hardware acceleration options allow
    cloud ML frameworks to efficiently utilize the diverse computational resources
    available in cloud environments.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加速是云机器学习框架的一个重要方面。所有主要框架都支持 CPU 和 GPU 执行，TensorFlow 和 JAX 还提供了对谷歌 TPU 的原生支持。[NVIDIA
    的 TensorRT](https://developer.nvidia.com/tensorrt)[34](#fn34) 是一个针对基于 GPU 推理的优化工具，提供了诸如层融合、精度校准和内核自动调优等复杂优化，以最大化在
    NVIDIA GPU 上的吞吐量。这些硬件加速选项允许云机器学习框架有效地利用云环境中可用的各种计算资源。
- en: The automatic differentiation capabilities of these frameworks are particularly
    important in cloud settings where complex models with millions of parameters are
    common. While TensorFlow and PyTorch primarily use reverse-mode differentiation,
    JAX’s support for both forward and reverse-mode differentiation can offer advantages
    in certain large-scale optimization scenarios.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架的自动微分能力在云环境中尤为重要，因为这里常见的是具有数百万参数的复杂模型。虽然 TensorFlow 和 PyTorch 主要使用反向模式微分，但
    JAX 对正向和反向模式微分的支持在某些大规模优化场景中可以提供优势。
- en: These specializations enable cloud ML frameworks to fully utilize the scalability
    and computational power of cloud infrastructure. However, this capability comes
    with increased complexity in deployment and management, often requiring specialized
    knowledge to fully leverage these frameworks. The focus on scalability and integration
    makes cloud ML frameworks particularly suitable for large-scale research projects,
    enterprise-level ML applications, and scenarios requiring massive computational
    resources.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 这些专业化使云机器学习框架能够充分利用云基础设施的可扩展性和计算能力。然而，这种能力在部署和管理方面带来了更高的复杂性，通常需要专业知识才能充分利用这些框架。对可扩展性和集成的关注使云机器学习框架特别适合于大规模研究项目、企业级机器学习应用以及需要大量计算资源的场景。
- en: Local Processing and Low-Latency Optimization
  id: totrans-642
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地处理和低延迟优化
- en: 'Moving from the resource-abundant cloud environment to edge deployments introduces
    significant new constraints that reshape framework priorities. Edge ML frameworks
    are specialized software tools designed to facilitate machine learning operations
    in edge computing environments, characterized by proximity to data sources, stringent
    latency requirements, and limited computational resources. Examples of popular
    edge ML frameworks include [TensorFlow Lite](https://www.tensorflow.org/lite)
    and [Edge Impulse](https://www.edgeimpulse.com). The specialization of these frameworks
    addresses three primary challenges: real-time inference optimization, adaptation
    to heterogeneous hardware, and resource-constrained operation. These challenges
    directly relate to the efficiency techniques discussed in [Chapter 9](ch015.xhtml#sec-efficient-ai)
    and require the hardware acceleration strategies covered in [Chapter 11](ch017.xhtml#sec-ai-acceleration).'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 从资源丰富的云环境转移到边缘部署引入了新的重大约束，这些约束重塑了框架的优先级。边缘机器学习框架是专门设计的软件工具，旨在促进边缘计算环境中的机器学习操作，这些环境以数据源附近、严格的延迟要求和有限的计算资源为特征。流行的边缘机器学习框架的例子包括[TensorFlow
    Lite](https://www.tensorflow.org/lite)和[Edge Impulse](https://www.edgeimpulse.com)。这些框架的专业化解决了三个主要挑战：实时推理优化、适应异构硬件和资源受限的运行。这些挑战直接关联到[第9章](ch015.xhtml#sec-efficient-ai)中讨论的效率技术，并需要[第11章](ch017.xhtml#sec-ai-acceleration)中涵盖的硬件加速策略。
- en: Real-time inference optimization is a critical feature of edge ML frameworks.
    This often involves leveraging different execution modes and graph types. For
    instance, while TensorFlow Lite (the edge-focused version of TensorFlow) uses
    a static graph approach to optimize inference, frameworks like [PyTorch Mobile](https://pytorch.org/mobile/home/)
    maintain a dynamic graph capability, allowing for more flexible model structures
    at the cost of some performance. The choice between static and dynamic graphs
    in edge frameworks often is a trade-off between optimization potential and model
    flexibility.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 实时推理优化是边缘机器学习框架的关键特性。这通常涉及利用不同的执行模式和图类型。例如，虽然TensorFlow Lite（TensorFlow的边缘版本）使用静态图方法来优化推理，但像[PyTorch
    Mobile](https://pytorch.org/mobile/home/)这样的框架则保持动态图功能，以牺牲一些性能为代价，允许更灵活的模型结构。在边缘框架中，静态和动态图之间的选择通常是在优化潜力和模型灵活性之间的一种权衡。
- en: Adaptation to heterogeneous hardware is crucial for edge deployments. Edge ML
    frameworks extend the hardware acceleration capabilities of their cloud counterparts
    but with a focus on edge-specific hardware. For instance, TensorFlow Lite supports
    acceleration on mobile GPUs and edge TPUs, while frameworks like [ARM’s Compute
    Library](https://developer.arm.com/solutions/machine-learning-on-arm/developer-material/how-to-guides)
    optimize for ARM-based processors. This specialization often involves custom operator
    implementations and low-level optimizations specific to edge hardware.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 适应异构硬件对于边缘部署至关重要。边缘机器学习框架扩展了它们云版本硬件加速的能力，但专注于边缘特定硬件。例如，TensorFlow Lite支持在移动GPU和边缘TPU上的加速，而像[ARM的Compute
    Library](https://developer.arm.com/solutions/machine-learning-on-arm/developer-material/how-to-guides)这样的框架针对ARM处理器进行优化。这种专业化通常涉及自定义算子实现和针对边缘硬件的低级优化。
- en: Operating within resource constraints is another aspect of edge ML framework
    specialization. This is reflected in the data structures and execution models
    of these frameworks. For instance, many edge frameworks use quantized tensors
    as their primary data structure, representing values with reduced precision (e.g.,
    8-bit integers instead of 32-bit floats) to decrease memory usage and computational
    demands. These quantization techniques, along with other optimization methods
    like pruning and knowledge distillation, are explored in detail in [Chapter 10](ch016.xhtml#sec-model-optimizations).
    The automatic differentiation capabilities, while crucial for training in cloud
    environments, are often stripped down or removed entirely in edge frameworks to
    reduce model size and improve inference speed.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源受限的环境中运行是边缘机器学习框架专业化的另一个方面。这体现在这些框架的数据结构和执行模型中。例如，许多边缘框架使用量化张量作为它们的主要数据结构，用降低精度（例如，8位整数而不是32位浮点数）来表示值，以减少内存使用和计算需求。这些量化技术，以及其他优化方法如剪枝和知识蒸馏，在[第10章](ch016.xhtml#sec-model-optimizations)中进行了详细探讨。虽然自动微分能力对于云环境中的训练至关重要，但在边缘框架中通常会被削减或完全移除，以减小模型大小并提高推理速度。
- en: Edge ML frameworks also often include features for model versioning and updates,
    allowing for the deployment of new models with minimal system downtime. Some frameworks
    support limited on-device learning, enabling models to adapt to local data without
    compromising data privacy. These on-device learning capabilities are explored
    in depth in [Chapter 14](ch020.xhtml#sec-ondevice-learning), while the privacy
    implications are thoroughly covered in [Chapter 15](ch021.xhtml#sec-security-privacy).
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘机器学习框架通常还包括模型版本控制和更新的功能，允许以最小的系统停机时间部署新模型。一些框架支持有限的设备上学习，使模型能够适应本地数据，同时不损害数据隐私。这些设备上学习的能力在[第14章](ch020.xhtml#sec-ondevice-learning)中进行了深入探讨，而隐私影响则在[第15章](ch021.xhtml#sec-security-privacy)中得到了全面覆盖。
- en: The specializations of edge ML frameworks collectively enable high-per­for­mance
    inference in resource-constrained environments. This capability expands the potential
    applications of AI in areas with limited cloud connectivity or where real-time
    processing is crucial. However, effective utilization of these frameworks requires
    careful consideration of target hardware specifications and application-specific
    requirements, necessitating a balance between model accuracy and resource utilization.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘机器学习框架的专业化共同使得在资源受限的环境中实现高性能推理成为可能。这种能力扩大了AI在云连接有限或实时处理至关重要的领域的应用潜力。然而，有效利用这些框架需要仔细考虑目标硬件规格和应用特定要求，需要在模型准确性和资源利用之间取得平衡。
- en: Resource-Constrained Device Optimization
  id: totrans-649
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源受限设备优化
- en: Mobile environments introduce additional constraints beyond those found in general
    edge computing, particularly regarding energy efficiency and user experience requirements.
    Mobile ML frameworks are specialized software tools designed for deploying and
    executing machine learning models on smartphones and tablets. Examples include
    TensorFlow Lite and [Apple’s Core ML](https://developer.apple.com/documentation/coreml/).
    These frameworks address the unique challenges of mobile environments, including
    limited computational resources, constrained power consumption, and diverse hardware
    configurations. The specialization of mobile ML frameworks primarily focuses on
    on-device inference optimization, energy efficiency, and integration with mobile-specific
    hardware and sensors.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 移动环境引入了比通用边缘计算中发现的约束更多的限制，尤其是在能效和用户体验要求方面。移动机器学习框架是为在智能手机和平板电脑上部署和执行机器学习模型而设计的专业软件工具。例如包括TensorFlow
    Lite和[苹果的Core ML](https://developer.apple.com/documentation/coreml/)。这些框架解决了移动环境中的独特挑战，包括有限的计算资源、受限的功耗和多样化的硬件配置。移动机器学习框架的专业化主要关注设备上的推理优化、能效以及与移动特定硬件和传感器的集成。
- en: On-device inference optimization in mobile ML frameworks often involves a careful
    balance between graph types and execution modes. For instance, TensorFlow Lite,
    also a popular mobile ML framework, uses a static graph approach to optimize inference
    performance. This contrasts with the dynamic graph capability of PyTorch Mobile,
    which offers more flexibility at the cost of some performance. The choice between
    static and dynamic graphs in mobile frameworks is a trade-off between optimization
    potential and model adaptability, crucial in the diverse and changing mobile environment.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动机器学习框架中对设备上的推理优化通常涉及在图类型和执行模式之间进行仔细的平衡。例如，TensorFlow Lite，也是一个流行的移动机器学习框架，采用静态图方法来优化推理性能。这与PyTorch
    Mobile的动态图能力形成对比，后者在牺牲一些性能的代价下提供了更多的灵活性。在移动框架中选择静态图和动态图是在优化潜力和模型适应性之间的一种权衡，这在多样化的移动环境中至关重要。
- en: The data structures in mobile ML frameworks are optimized for efficient memory
    usage and computation. While cloud-based frameworks like TensorFlow and PyTorch
    use mutable tensors, mobile frameworks often employ more specialized data structures.
    For example, many mobile frameworks use quantized tensors, representing values
    with reduced precision (e.g., 8-bit integers instead of 32-bit floats) to decrease
    memory footprint and computational demands. This specialization is critical given
    the limited RAM and processing power of mobile devices.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习框架中的数据结构针对高效内存使用和计算进行了优化。虽然基于云的框架如TensorFlow和PyTorch使用可变张量，但移动框架通常采用更专业的数据结构。例如，许多移动框架使用量化张量，用降低精度的值（例如，8位整数而不是32位浮点数）来减少内存占用和计算需求。鉴于移动设备的有限RAM和计算能力，这种专业化至关重要。
- en: Energy efficiency, a key concern in mobile environments, influences the design
    of execution modes in mobile ML frameworks. Unlike cloud frameworks that may use
    eager execution for ease of development, mobile frameworks often prioritize graph-based
    execution for its potential energy savings. For instance, Apple’s Core ML uses
    a compiled model approach, converting ML models into a form that can be efficiently
    executed by iOS devices, optimizing for both performance and energy consumption.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 能效，在移动环境中的一个关键关注点，影响了移动机器学习框架中执行模式的设计。与可能为了开发便利而使用急切执行的云框架不同，移动框架通常优先考虑基于图的执行，以实现潜在的节能。例如，苹果的Core
    ML采用编译模型方法，将机器学习模型转换为iOS设备可以高效执行的形式，优化性能和能耗。
- en: Integration with mobile-specific hardware and sensors is another key specialization
    area. Mobile ML frameworks extend the hardware acceleration capabilities of their
    cloud counterparts but with a focus on mobile-specific processors. For example,
    TensorFlow Lite can leverage mobile GPUs and neural processing units (NPUs) found
    in many modern smartphones. Qualcomm’s Neural Processing SDK is designed to efficiently
    utilize the AI accelerators present in Snapdragon SoCs. This hardware-specific
    optimization often involves custom operator implementations and low-level optimizations
    tailored for mobile processors.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 与移动特定硬件和传感器的集成是另一个关键专门化领域。移动机器学习框架扩展了其云对应方的硬件加速能力，但重点在于移动特定处理器。例如，TensorFlow
    Lite可以利用许多现代智能手机中发现的移动GPU和神经处理单元（NPUs）。高通的神经处理SDK旨在高效利用Snapdragon SoC中存在的AI加速器。这种针对特定硬件的优化通常涉及自定义算子实现和针对移动处理器的低级优化。
- en: Automatic differentiation, while crucial for training in cloud environments,
    is often minimized or removed entirely in mobile frameworks to reduce model size
    and improve inference speed. Instead, mobile ML frameworks focus on efficient
    inference, with model updates typically performed off-device and then deployed
    to the mobile application.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分对于云环境中的训练至关重要，但在移动框架中通常被最小化或完全移除，以减少模型大小并提高推理速度。相反，移动机器学习框架专注于高效的推理，模型更新通常在设备外执行，然后部署到移动应用程序中。
- en: Mobile ML frameworks also often include features for model updating and versioning,
    allowing for the deployment of improved models without requiring full app updates.
    Some frameworks support limited on-device learning, enabling models to adapt to
    user behavior or environmental changes without compromising data privacy. The
    technical approaches and implementation strategies for on-device learning are
    detailed in [Chapter 14](ch020.xhtml#sec-ondevice-learning), while privacy preservation
    techniques are covered in [Chapter 15](ch021.xhtml#sec-security-privacy).
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习框架通常包括模型更新和版本控制功能，允许在不需要完整应用更新的情况下部署改进的模型。一些框架支持有限的设备上学习，使模型能够适应用户行为或环境变化，同时不损害数据隐私。设备上学习的具体技术和实现策略在[第14章](ch020.xhtml#sec-ondevice-learning)中详细说明，而隐私保护技术在[第15章](ch021.xhtml#sec-security-privacy)中介绍。
- en: The specializations of mobile ML frameworks collectively enable the deployment
    of sophisticated ML models on resource-constrained mobile devices. This expands
    the potential applications of AI in mobile environments, ranging from real-time
    image and speech recognition to personalized user experiences. However, effectively
    utilizing these frameworks requires careful consideration of the target device
    capabilities, user experience requirements, and privacy implications, necessitating
    a balance between model performance and resource utilization.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习框架的专门化共同使得在资源受限的移动设备上部署复杂的机器学习模型成为可能。这扩大了人工智能在移动环境中的潜在应用，从实时图像和语音识别到个性化用户体验。然而，有效地利用这些框架需要仔细考虑目标设备的性能、用户体验要求以及隐私影响，需要在模型性能和资源利用之间取得平衡。
- en: Microcontroller and Embedded System Implementation
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微控制器和嵌入式系统实现
- en: At the extreme end of the resource constraint spectrum, TinyML frameworks operate
    under conditions that push the boundaries of what is computationally feasible.
    TinyML frameworks are specialized software infrastructures designed for deploying
    machine learning models on extremely resource-constrained devices, typically microcontrollers
    and low-power embedded systems. These frameworks address the severe limitations
    in processing power, memory, and energy consumption characteristic of tiny devices.
    The specialization of TinyML frameworks primarily focuses on extreme model compression,
    optimizations for severely constrained environments, and integration with microcontroller-specific
    architectures.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源限制的极端端，TinyML框架在推动计算可行性的边界条件下运行。TinyML框架是专为在极端资源受限的设备上部署机器学习模型而设计的专用软件基础设施，通常用于微控制器和低功耗嵌入式系统。这些框架解决了小型设备特有的处理能力、内存和能耗的严重限制。TinyML框架的专门化主要关注极端模型压缩、严重受限环境的优化以及与微控制器特定架构的集成。
- en: Extreme model compression in TinyML frameworks takes the quantization techniques
    mentioned in mobile and edge frameworks to their logical conclusion. While mobile
    frameworks might use 8-bit quantization, TinyML often employs even more aggressive
    techniques, such as 4-bit, 2-bit, or even 1-bit (binary) representations of model
    parameters. Frameworks like TensorFlow Lite Micro exemplify this approach ([David
    et al. 2021](ch058.xhtml#ref-david2021tensorflow)), pushing the boundaries of
    model compression to fit within the kilobytes of memory available on microcontrollers.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 在TinyML框架中实现极端模型压缩是将移动和边缘框架中提到的量化技术推向了逻辑上的极限。虽然移动框架可能使用8位量化，但TinyML通常采用更加激进的技巧，例如模型参数的4位、2位甚至1位（二进制）表示。例如，TensorFlow
    Lite Micro框架就体现了这种做法([David等人，2021](ch058.xhtml#ref-david2021tensorflow))，将模型压缩的边界推向了适合微控制器上千字节内存的极限。
- en: The execution model in TinyML frameworks is highly specialized. Unlike the dynamic
    graph capabilities seen in some cloud and mobile frameworks, TinyML frameworks
    almost exclusively use static, highly optimized graphs. The just-in-time compilation
    approach seen in frameworks like JAX is typically not feasible in TinyML due to
    memory constraints. Instead, these frameworks often employ ahead-of-time compilation
    techniques to generate highly optimized, device-specific code.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML框架中的执行模型高度专门化。与一些云和移动框架中看到的动态图功能不同，TinyML框架几乎只使用静态、高度优化的图。在JAX等框架中看到的即时编译方法在TinyML中通常不可行，因为内存限制。相反，这些框架通常采用提前编译技术来生成高度优化的、特定于设备的代码。
- en: Memory management in TinyML frameworks is far more constrained than in other
    environments. While edge and mobile frameworks might use dynamic memory allocation,
    TinyML frameworks like [uTensor](https://github.com/uTensor/uTensor) often rely
    on static memory allocation to avoid runtime overhead and fragmentation. This
    approach requires careful planning of the memory layout at compile time, a stark
    contrast to the more flexible memory management in cloud-based frameworks.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML框架中的内存管理比其他环境要受限制得多。虽然边缘和移动框架可能使用动态内存分配，但TinyML框架如[uTensor](https://github.com/uTensor/uTensor)通常依赖于静态内存分配以避免运行时开销和碎片化。这种方法需要在编译时仔细规划内存布局，与基于云的框架中更为灵活的内存管理形成鲜明对比。
- en: Hardware integration in TinyML frameworks is highly specific to microcontroller
    architectures. Unlike the general GPU support seen in cloud frameworks or the
    mobile GPU/NPU support in mobile frameworks, TinyML frameworks often provide optimizations
    for specific microcontroller instruction sets. For example, ARM’s CMSIS-NN ([Lai,
    Suda, and Chandra 2018](ch058.xhtml#ref-lai2018cmsis)) provides optimized neural
    network kernels for Cortex-M series microcontrollers, which are often integrated
    into TinyML frameworks.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML框架中的硬件集成高度特定于微控制器架构。与云框架中普遍看到的通用GPU支持或移动框架中移动GPU/NPU支持不同，TinyML框架通常为特定的微控制器指令集提供优化。例如，ARM的CMSIS-NN([Lai,
    Suda, 和 Chandra，2018](ch058.xhtml#ref-lai2018cmsis))为Cortex-M系列微控制器提供了优化的神经网络内核，这些内核通常集成到TinyML框架中。
- en: The concept of automatic differentiation, central to cloud-based frameworks
    and present to some degree in edge and mobile frameworks, is typically absent
    in TinyML frameworks. The focus is almost entirely on inference, with any learning
    or model updates usually performed off-device due to the severe computational
    constraints.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分的概念，在基于云的框架中至关重要，并在边缘和移动框架中存在一定程度的体现，但在TinyML框架中通常不存在。重点几乎完全集中在推理上，由于严重的计算限制，任何学习或模型更新通常都在设备外进行。
- en: TinyML frameworks also specialize in power management to a degree not seen in
    other ML environments. Features like duty cycling and ultra-low-power wake-up
    capabilities are often integrated directly into the ML pipeline, enabling always-on
    sensing applications that can run for years on small batteries.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML框架在某种程度上也专注于电源管理，这在其他机器学习环境中是看不到的。像轮询和超低功耗唤醒功能这样的特性通常直接集成到机器学习管道中，使得始终在线的传感应用能够在小型电池上运行数年。
- en: The extreme specialization of TinyML frameworks enables ML deployments in previously
    infeasible environments, from smart dust sensors to implantable medical devices.
    However, this specialization comes with significant trade-offs in model complexity
    and accuracy, requiring careful consideration of the balance between ML capabilities
    and the severe resource constraints of target devices.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML框架的极端专业化使得机器学习部署在之前不可行的环境中成为可能，从智能尘埃传感器到植入式医疗设备。然而，这种专业化在模型复杂性和准确性方面带来了显著的权衡，需要仔细考虑机器学习能力与目标设备严重资源限制之间的平衡。
- en: Performance and Resource Optimization Platforms
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能和资源优化平台
- en: Beyond deployment-specific specializations, modern machine learning frameworks
    increasingly incorporate efficiency as a first-class design principle. Efficiency-oriented
    frameworks are specialized tools that treat computational efficiency, memory optimization,
    and energy consumption as primary design constraints rather than secondary considerations.
    These frameworks address the growing demand for practical AI deployment where
    resource constraints fundamentally shape algorithmic choices.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特定于部署的专业化之外，现代机器学习框架越来越多地将效率作为一级设计原则。以效率为导向的框架是专门工具，将计算效率、内存优化和能耗视为主要设计约束，而不是次要考虑因素。这些框架满足了日益增长的实用人工智能部署需求，其中资源限制从根本上塑造了算法选择。
- en: Traditional frameworks often treat efficiency optimizations as optional add-ons,
    applied after model development. In contrast, efficiency-oriented frameworks integrate
    optimization techniques directly into the development workflow, enabling developers
    to train and deploy models with quantization, pruning, and compression constraints
    from the beginning. This efficiency-first approach enables deployment scenarios
    where traditional frameworks would be computationally infeasible.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 传统框架通常将效率优化视为可选的附加功能，在模型开发之后应用。相比之下，以效率为导向的框架将优化技术直接集成到开发工作流程中，使开发者能够从开始就使用量化、剪枝和压缩约束来训练和部署模型。这种以效率为先的方法使得在传统框架计算上不可行的部署场景成为可能。
- en: The significance of efficiency-oriented frameworks has grown with the expansion
    of AI applications into resource-constrained environments. Modern production systems
    require models that balance accuracy with strict constraints on inference latency
    (often sub-10ms requirements), memory usage (fitting within GPU memory limits),
    energy consumption (extending battery life), and computational cost (reducing
    cloud infrastructure expenses). These constraints create substantially different
    framework requirements compared to research environments with abundant computational
    resources.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能应用扩展到资源受限的环境，以效率为导向的框架的重要性也随之增长。现代生产系统需要能够在推理延迟（通常要求低于10毫秒）、内存使用（适应GPU内存限制）、能耗（延长电池寿命）和计算成本（减少云基础设施费用）等方面进行平衡的模型。这些限制与拥有丰富计算资源的科研环境相比，产生了显著不同的框架需求。
- en: Model Size and Computational Reduction Techniques
  id: totrans-671
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型大小和计算缩减技术
- en: Efficiency-oriented frameworks distinguish themselves through compression-aware
    computational graph design. Unlike traditional frameworks that optimize mathematical
    operations independently, these frameworks optimize for compressed representations
    throughout the computation pipeline. This integration affects every layer of the
    framework stack, from data structures to execution engines.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 以效率为导向的框架通过压缩感知的计算图设计来区分自己。与独立优化数学运算的传统框架不同，这些框架在整个计算管道中优化压缩表示。这种集成影响了框架堆栈的每一层，从数据结构到执行引擎。
- en: Neural network compression techniques require framework support for specialized
    data types and operations. Quantization-aware training demands frameworks that
    can simulate reduced precision arithmetic during training while maintaining full-precision
    gradients for stable optimization. Intel Neural Compressor exemplifies this approach,
    providing APIs that seamlessly integrate INT8 quantization into existing PyTorch
    and TensorFlow workflows. The framework automatically inserts fake quantization
    operations during training, allowing models to adapt to quantization constraints
    while preserving accuracy.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络压缩技术需要框架支持专门的数据类型和操作。量化感知训练需要能够在训练期间模拟降低精度算术的框架，同时保持全精度梯度以稳定优化。Intel Neural
    Compressor是这种方法的例证，它提供了无缝集成INT8量化的API，这些API可以集成到现有的PyTorch和TensorFlow工作流程中。该框架在训练期间自动插入模拟量化操作，使模型能够适应量化约束同时保持精度。
- en: Structured pruning techniques require frameworks that can handle sparse tensor
    operations efficiently. This involves specialized storage formats (such as compressed
    sparse row representations), optimized sparse matrix operations, and runtime systems
    that can take advantage of structural zeros. Apache TVM demonstrates advanced
    sparse tensor compilation, automatically generating efficient code for sparse
    operations across different hardware backends.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝技术需要能够高效处理稀疏张量操作的框架。这涉及到专门的存储格式（如压缩稀疏行表示）、优化的稀疏矩阵操作以及可以利用结构零的运行时系统。Apache
    TVM展示了高级稀疏张量编译，自动为不同硬件后端生成稀疏操作的效率代码。
- en: Knowledge distillation workflows represent another efficiency-oriented framework
    capability. These frameworks must orchestrate teacher-student training pipelines,
    managing the computational overhead of running multiple models simultaneously
    while providing APIs for custom distillation losses. Hugging Face Optimum provides
    comprehensive distillation workflows that automatically configure teacher-student
    training for various model architectures, reducing the engineering complexity
    of implementing efficiency optimizations.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏工作流程代表了另一种以效率为导向的框架能力。这些框架必须协调教师-学生训练管道，管理运行多个模型同时产生的计算开销，同时提供用于自定义蒸馏损失的API。Hugging
    Face Optimum提供了全面的蒸馏工作流程，自动配置各种模型架构的教师-学生训练，减少了实现效率优化的工程复杂性。
- en: Integrated Hardware-Framework Performance Tuning
  id: totrans-676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成硬件-框架性能调优
- en: Efficiency-oriented frameworks excel at hardware-software co-design, where framework
    architecture and hardware capabilities are optimized together. This approach moves
    beyond generic hardware acceleration to target-specific optimization strategies
    that consider hardware constraints during algorithmic design.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 以效率为导向的框架在硬件-软件协同设计中表现出色，这种设计将框架架构和硬件能力一起优化。这种方法超越了通用的硬件加速，转向针对特定优化策略，这些策略在算法设计期间考虑硬件约束。
- en: Mixed-precision training frameworks demonstrate this co-design philosophy. NVIDIA’s
    Automatic Mixed Precision (AMP) in PyTorch automatically identifies operations
    that can use FP16 arithmetic while maintaining FP32 precision for numerical stability.
    The framework analyzes computational graphs to determine optimal precision policies,
    balancing training speed improvements (up to 1.5-2x speedup on modern GPUs) against
    numerical accuracy requirements. This analysis requires deep integration between
    framework scheduling and hardware capabilities.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练框架展示了这种协同设计理念。NVIDIA的PyTorch自动混合精度（AMP）自动识别可以使用FP16算术操作的操作，同时保持FP32精度以保持数值稳定性。该框架分析计算图以确定最佳精度策略，在训练速度提升（在现代GPU上可达1.5-2倍）与数值精度要求之间取得平衡。这种分析需要在框架调度和硬件能力之间进行深度集成。
- en: Sparse computation frameworks extend this co-design approach to leverage hardware
    sparsity support. Modern hardware like NVIDIA A100 GPUs includes specialized sparse
    matrix multiplication units that can achieve 2:4 structured sparsity (50% zeros
    in specific patterns) with minimal performance degradation. Frameworks like Neural
    Magic’s SparseML provide automated tools for training models that conform to these
    hardware-specific sparsity patterns, achieving significant speedups without accuracy
    loss.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏计算框架将这种协同设计方法扩展到利用硬件稀疏性支持。现代硬件，如 NVIDIA A100 GPU，包括专门的稀疏矩阵乘法单元，可以实现 2:4 结构化稀疏度（特定模式中
    50% 的零），同时性能下降最小。像 Neural Magic 的 SparseML 这样的框架提供了用于训练符合这些特定硬件稀疏模式的模型的自动化工具，在不损失精度的同时实现显著的加速。
- en: Compilation frameworks represent the most sophisticated form of hardware-software
    co-design. Apache TVM and MLIR provide domain-specific languages for expressing
    hardware-specific optimizations. These frameworks analyze computational graphs
    to automatically generate optimized kernels for specific hardware targets, including
    custom ASICs and specialized accelerators. The compilation process considers hardware
    memory hierarchies, instruction sets, and parallelization capabilities to generate
    code that often outperforms hand-optimized implementations.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 编译框架代表了硬件-软件协同设计的最复杂形式。Apache TVM 和 MLIR 提供了用于表达特定硬件优化的领域特定语言。这些框架分析计算图以自动为特定硬件目标生成优化的内核，包括定制
    ASIC 和专用加速器。编译过程考虑硬件内存层次结构、指令集和并行化能力，以生成通常优于手动优化的代码。
- en: Real-World Deployment Performance Requirements
  id: totrans-681
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 真实世界部署性能要求
- en: 'Efficiency-oriented frameworks address production deployment challenges through
    systematic approaches to resource management and performance optimization. Production
    environments impose strict constraints that differ substantially from research
    settings: inference latency must meet real-time requirements, memory usage must
    fit within allocated resources, and energy consumption must stay within power
    budgets.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 以效率为导向的框架通过系统性的资源管理和性能优化方法解决生产部署挑战。生产环境施加的约束与研究环境大不相同：推理延迟必须满足实时要求，内存使用必须适合分配的资源，能耗必须保持在预算内。
- en: Inference optimization frameworks like NVIDIA TensorRT and ONNX Runtime provide
    comprehensive toolchains for production deployment. TensorRT applies aggressive
    optimization techniques including layer fusion (combining multiple operations
    into single kernels), precision calibration (automatically determining optimal
    quantization levels), and memory optimization (reducing memory transfers between
    operations). These optimizations can achieve 3-7x inference speedup compared to
    unoptimized frameworks while maintaining accuracy within acceptable bounds.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 推理优化框架，如 NVIDIA TensorRT 和 ONNX Runtime，为生产部署提供了全面的工具链。TensorRT 应用激进优化技术，包括层融合（将多个操作组合成单个内核）、精度校准（自动确定最佳量化级别）和内存优化（减少操作之间的内存传输）。这些优化与未优化的框架相比，可以实现
    3-7 倍的推理速度提升，同时保持精度在可接受的范围内。
- en: Memory optimization represents a critical production constraint. DeepSpeed and
    FairScale demonstrate advanced memory management techniques that enable training
    and inference of models that exceed GPU memory capacity. DeepSpeed’s ZeRO optimizer
    partitions optimizer states, gradients, and parameters across multiple devices,
    reducing memory usage by 4-8x compared to traditional data parallelism. These
    techniques enable training of models with hundreds of billions of parameters on
    standard hardware configurations.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 内存优化代表了生产中的一个关键约束。DeepSpeed 和 FairScale 展示了高级内存管理技术，这些技术使得训练和推理超出 GPU 内存容量的模型成为可能。DeepSpeed
    的 ZeRO 优化器将优化器状态、梯度和参数分区到多个设备上，与传统数据并行相比，内存使用量减少 4-8 倍。这些技术使得在标准硬件配置上训练具有数百亿参数的模型成为可能。
- en: Energy-aware frameworks address the growing importance of computational sustainability.
    Power consumption directly impacts deployment costs in cloud environments and
    battery life in mobile applications. Frameworks like NVIDIA’s Triton Inference
    Server provide power-aware scheduling that can dynamically adjust inference batching
    and frequency scaling to meet energy budgets while maintaining throughput requirements.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 能量感知框架应对计算可持续性日益增长的重要性。功耗直接影响到云环境中的部署成本和移动应用中的电池寿命。例如，NVIDIA 的 Triton 推理服务器提供了功率感知调度，可以动态调整推理批处理和频率缩放，以满足能源预算同时保持吞吐量要求。
- en: Systematic Performance Assessment Methodologies
  id: totrans-686
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 系统性性能评估方法
- en: Evaluating efficiency-oriented frameworks requires comprehensive metrics that
    capture the multi-dimensional trade-offs between accuracy, performance, and resource
    consumption. Traditional ML evaluation focuses primarily on accuracy metrics,
    but efficiency evaluation must consider computational efficiency (FLOPS reduction,
    inference speedup), memory efficiency (peak memory usage, memory bandwidth utilization),
    energy efficiency (power consumption, energy per inference), and deployment efficiency
    (model size reduction, deployment complexity).
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 评估效率导向型框架需要综合指标，这些指标捕捉了精度、性能和资源消耗之间的多维权衡。传统的机器学习评估主要关注精度指标，但效率评估必须考虑计算效率（FLOPS
    减少、推理加速）、内存效率（峰值内存使用、内存带宽利用率）、能源效率（功耗、每推理能耗）和部署效率（模型尺寸减少、部署复杂性）。
- en: Quantitative framework comparison requires standardized benchmarks that measure
    these efficiency dimensions across representative workloads. MLPerf Inference
    provides standardized benchmarks for measuring inference performance across different
    frameworks and hardware configurations. These benchmarks measure latency, throughput,
    and energy consumption for common model architectures, enabling direct comparison
    of framework efficiency characteristics.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 定量框架比较需要标准化的基准，这些基准测量代表性工作负载中的效率维度。MLPerf 推理提供了测量不同框架和硬件配置推理性能的标准基准。这些基准测量了常见模型架构的延迟、吞吐量和能耗，使得框架效率特性的直接比较成为可能。
- en: Performance profiling frameworks enable developers to understand efficiency
    bottlenecks in their specific applications. NVIDIA Nsight Systems and Intel VTune
    provide detailed analysis of framework execution, identifying memory bandwidth
    limitations, computational bottlenecks, and opportunities for optimization. These
    tools integrate with efficiency-oriented frameworks to provide actionable insights
    for improving application performance.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析框架使开发者能够了解其特定应用中的效率瓶颈。NVIDIA Nsight Systems 和 Intel VTune 提供了框架执行的详细分析，识别内存带宽限制、计算瓶颈和优化机会。这些工具与效率导向型框架集成，为提高应用性能提供可操作的见解。
- en: The evolution of efficiency-oriented frameworks represents a fundamental shift
    in ML systems design, where computational constraints shape algorithmic choices
    from the beginning of development. This approach enables practical AI deployment
    across resource-constrained environments while maintaining the flexibility and
    expressiveness that makes modern ML frameworks powerful development tools.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 效率导向型框架的演变代表了机器学习系统设计中的一次根本性转变，其中计算约束从开发初期就塑造了算法选择。这种方法使得在资源受限的环境中实现实用的人工智能部署成为可能，同时保持了现代机器学习框架作为强大开发工具的灵活性和表现力。
- en: Systematic Framework Selection Methodology
  id: totrans-691
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统性框架选择方法
- en: 'Choosing the right machine learning framework requires a systematic evaluation
    that balances technical requirements with operational constraints. This decision-making
    process extends beyond simple feature comparisons to encompass the entire system
    lifecycle, from development through deployment and maintenance. Engineers must
    evaluate multiple interdependent factors: technical capabilities (supported operations,
    execution models, hardware targets), operational requirements (deployment constraints,
    performance needs, scalability demands), and organizational factors (team expertise,
    development timeline, maintenance resources).'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的机器学习框架需要系统性的评估，平衡技术需求与运营约束。这一决策过程不仅超越了简单的功能比较，还涵盖了整个系统生命周期，从开发到部署和维护。工程师必须评估多个相互关联的因素：技术能力（支持的运算、执行模型、硬件目标）、运营需求（部署约束、性能需求、可扩展性需求）和组织因素（团队专业知识、开发时间表、维护资源）。
- en: 'The framework selection process follows a structured approach that considers
    three primary dimensions: model requirements determine which operations and architectures
    the framework must support, software dependencies define operating system and
    runtime requirements, and hardware constraints establish memory and processing
    limitations. These technical considerations must be balanced with practical factors
    like team expertise, learning curve, community support, and long-term maintenance
    commitments.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 框架选择过程遵循一种结构化方法，考虑三个主要维度：模型需求决定了框架必须支持哪些操作和架构，软件依赖定义了操作系统和运行时要求，硬件约束确立了内存和处理限制。这些技术考虑必须与团队专业知识、学习曲线、社区支持和长期维护承诺等实际因素相平衡。
- en: 'This decision-making process must also consider the broader system architecture
    principles outlined in [Chapter 2](ch008.xhtml#sec-ml-systems) and align with
    the deployment patterns detailed in [Chapter 13](ch019.xhtml#sec-ml-operations).
    Different deployment scenarios often favor different framework architectures:
    cloud training requires high throughput and distributed capabilities, edge inference
    prioritizes low latency and minimal resource usage, mobile deployment balances
    performance with battery constraints, and embedded systems optimize for minimal
    memory footprint and real-time execution.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策过程还必须考虑第2章中概述的更广泛的系统架构原则，并与第13章中详细说明的部署模式相一致。不同的部署场景通常青睐不同的框架架构：云训练需要高吞吐量和分布式能力，边缘推理优先考虑低延迟和最小资源使用，移动部署在性能和电池限制之间取得平衡，嵌入式系统优化最小内存占用和实时执行。
- en: 'To illustrate how these factors interact in practice, we examine the TensorFlow
    ecosystem, which demonstrates the spectrum of trade-offs through its variants:
    TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro. While TensorFlow serves
    as our detailed case study, the same selection methodology applies broadly across
    the framework landscape, including PyTorch for research-oriented workflows, ONNX
    for cross-platform deployment, JAX for functional programming approaches, and
    specialized frameworks for specific domains.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些因素在实际中的相互作用，我们考察了TensorFlow生态系统，它通过其变体：TensorFlow、TensorFlow Lite和TensorFlow
    Lite Micro，展示了权衡的范围。虽然TensorFlow是我们的详细案例研究，但同样的选择方法广泛适用于框架景观，包括PyTorch用于研究型工作流程、ONNX用于跨平台部署、JAX用于函数式编程方法，以及针对特定领域的专用框架。
- en: '[Table 7.4](ch013.xhtml#tbl-tf-comparison) illustrates key differences between
    TensorFlow variants. Each variant represents specific trade-offs between computational
    capability and resource requirements. These trade-offs manifest in supported operations,
    binary size, and integration requirements.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.4](ch013.xhtml#tbl-tf-comparison)展示了TensorFlow变体之间的关键差异。每个变体代表了计算能力和资源需求之间的特定权衡。这些权衡在支持的运算、二进制大小和集成需求中体现出来。'
- en: 'Table 7.4: **TensorFlow Variant Trade-Offs**: TensorFlow, TensorFlow lite,
    and TensorFlow lite micro represent a spectrum of design choices balancing model
    expressiveness, binary size, and resource constraints for diverse deployment scenarios.
    Supported operations decrease from approximately 1400 in full TensorFlow to 50
    in TensorFlow lite micro, reflecting a shift from training capability to efficient
    inference on edge devices; native quantization tooling enables further optimization
    for resource-constrained environments.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.4：**TensorFlow变体权衡**：TensorFlow、TensorFlow lite和TensorFlow lite micro代表了一系列设计选择，这些选择在模型表达能力、二进制大小和资源限制之间进行平衡，以适应不同的部署场景。支持的运算从完整的TensorFlow中的大约1400个减少到TensorFlow
    lite micro中的50个，反映了从训练能力到边缘设备上高效推理的转移；本机量化工具能够进一步优化资源受限环境。
- en: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Training** | Yes | No | No |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| **训练** | 是 | 否 | 否 |'
- en: '| **Inference** | Yes (*but inefficient on edge*) | Yes (*and efficient*) |
    Yes (*and even more efficient*) |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| **推理** | 是（但边缘效率低） | 是（且效率高） | 是（且更高效） |'
- en: '| **How Many Ops** | ~1400 | ~130 | ~50 |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| **操作数量** | ~1400 | ~130 | ~50 |'
- en: '| **Native Quantization Tooling** | No | Yes | Yes |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| **本机量化工具** | 否 | 是 | 是 |'
- en: 'Engineers analyze three primary aspects when selecting a framework:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师在选择框架时分析三个主要方面：
- en: Model requirements determine which operations and architectures the framework
    must support
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型需求决定了框架必须支持哪些操作和架构
- en: Software dependencies define operating system and runtime requirements
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 软件依赖定义了操作系统和运行时要求
- en: Hardware constraints establish memory and processing limitations
  id: totrans-707
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件限制确立了内存和处理限制
- en: This systematic analysis enables engineers to select frameworks that align with
    their specific deployment requirements and organizational context. As we examine
    the TensorFlow variants in detail, we will explore how each selection dimension
    influences framework choice and shapes system capabilities, providing a methodology
    that can be applied to evaluate any framework ecosystem.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统分析使工程师能够选择与其特定的部署要求和组织环境相一致的框架。当我们详细检查TensorFlow变体时，我们将探讨每个选择维度如何影响框架选择并塑造系统能力，提供一种可以应用于评估任何框架生态系统的方法。
- en: Model Requirements
  id: totrans-709
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型要求
- en: 'Model architecture capabilities vary significantly across TensorFlow variants,
    with clear trade-offs between functionality and efficiency. [Table 7.4](ch013.xhtml#tbl-tf-comparison)
    quantifies these differences across four key dimensions: training capability,
    inference efficiency, operation support, and quantization features.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow变体之间的模型架构能力差异很大，在功能与效率之间有明显的权衡。[表7.4](ch013.xhtml#tbl-tf-comparison)量化了这四个关键维度之间的差异：训练能力、推理效率、操作支持和量化功能。
- en: '**Dynamic vs Static Computational Graphs**'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态与静态计算图**'
- en: A key architectural distinction between frameworks is their computational graph
    construction approach. Static graphs (TensorFlow 1.x) require defining the entire
    computation before execution, similar to compiling a program before running it.
    Dynamic graphs (PyTorch, TensorFlow 2.x eager mode) build the graph during execution,
    akin to interpreted languages. This affects debugging ease (dynamic graphs allow
    standard Python debugging), optimization opportunities (static graphs enable more
    aggressive optimization), and deployment complexity (static graphs simplify deployment
    but require more upfront design).
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 框架之间一个关键的建筑区分在于它们的计算图构建方法。静态图（TensorFlow 1.x）需要在执行前定义整个计算，类似于在运行程序之前编译程序。动态图（PyTorch、TensorFlow
    2.x eager模式）在执行期间构建图，类似于解释型语言。这影响了调试的容易程度（动态图允许标准的Python调试）、优化机会（静态图允许更激进的优化）和部署复杂性（静态图简化了部署但需要更多的前期设计）。
- en: TensorFlow supports approximately 1,400 operations and enables both training
    and inference. However, as [Table 7.4](ch013.xhtml#tbl-tf-comparison) indicates,
    its inference capabilities are inefficient for edge deployment. TensorFlow Lite
    reduces the operation count to roughly 130 operations while improving inference
    efficiency. It eliminates training support but adds native quantization tooling.
    TensorFlow Lite Micro further constrains the operation set to approximately 50
    operations, achieving even higher inference efficiency through these constraints.
    Like TensorFlow Lite, it includes native quantization support but removes training
    capabilities.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持大约1,400个操作，并支持训练和推理。然而，正如[表7.4](ch013.xhtml#tbl-tf-comparison)所示，其推理能力对于边缘部署来说效率不高。TensorFlow
    Lite将操作数量减少到大约130个操作，同时提高了推理效率。它消除了训练支持但添加了原生量化工具。TensorFlow Lite Micro进一步限制了操作集到大约50个操作，通过这些限制实现了更高的推理效率。与TensorFlow
    Lite一样，它包括原生量化支持但移除了训练能力。
- en: This progressive reduction in operations enables deployment on increasingly
    constrained devices. The addition of native quantization in both TensorFlow Lite
    and TensorFlow Lite Micro provides essential optimization capabilities absent
    in the full TensorFlow framework. Quantization transforms models to use lower
    precision operations, reducing computational and memory requirements for resource-constrained
    deployments. These optimization techniques, detailed further in [Chapter 10](ch016.xhtml#sec-model-optimizations),
    must be considered alongside data pipeline requirements discussed in [Chapter 6](ch012.xhtml#sec-data-engineering)
    when selecting appropriate frameworks for specific deployment scenarios.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作的逐步减少使得在越来越受限的设备上部署成为可能。在TensorFlow Lite和TensorFlow Lite Micro中添加原生量化提供了在完整TensorFlow框架中缺失的必要优化能力。量化将模型转换为使用更低精度的操作，从而减少了资源受限部署的计算和内存需求。这些优化技术，在[第10章](ch016.xhtml#sec-model-optimizations)中进一步详细说明，在选择适用于特定部署场景的框架时，必须与[第6章](ch012.xhtml#sec-data-engineering)中讨论的数据管道需求一起考虑。
- en: Software Dependencies
  id: totrans-715
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件依赖
- en: '[Table 7.5](ch013.xhtml#tbl-tf-sw-comparison) reveals three key software considerations
    that differentiate TensorFlow variants: operating system requirements, memory
    management capabilities, and accelerator support. These differences reflect each
    variant’s optimization for specific deployment'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7.5](ch013.xhtml#tbl-tf-sw-comparison) 展示了区分 TensorFlow 变体的三个关键软件考虑因素：操作系统要求、内存管理能力和加速器支持。这些差异反映了每个变体针对特定部署的优化。'
- en: 'Table 7.5: **TensorFlow Variant Trade-Offs**: TensorFlow, TensorFlow lite,
    and TensorFlow lite micro offer different capabilities regarding operating system
    dependence, memory management, and hardware acceleration, reflecting design choices
    for diverse deployment scenarios. These distinctions enable developers to select
    the variant best suited for resource-constrained devices or full-scale server
    deployments, balancing functionality with efficiency.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.5：**TensorFlow 变体权衡**：TensorFlow、TensorFlow Lite 和 TensorFlow Lite Micro
    在操作系统依赖性、内存管理和硬件加速方面提供不同的功能，反映了针对不同部署场景的设计选择。这些区别使开发者能够选择最适合资源受限设备或大规模服务器部署的变体，在功能与效率之间取得平衡。
- en: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Needs an OS** | Yes | Yes | No |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| **需要操作系统** | 是 | 是 | 否 |'
- en: '| **Memory Mapping of Models** | No | Yes | Yes |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| **模型内存映射** | 否 | 是 | 是 |'
- en: '| **Delegation to accelerators** | Yes | Yes | No |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| **委托给加速器** | 是 | 是 | 否 |'
- en: Operating system dependencies mark a fundamental distinction between variants.
    TensorFlow and TensorFlow Lite require an operating system, while TensorFlow Lite
    Micro operates without OS support. This enables TensorFlow Lite Micro to reduce
    memory overhead and startup time, though it can still integrate with real-time
    operating systems like FreeRTOS, Zephyr, and Mbed OS when needed.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统依赖性在变体之间划定了根本的区别。TensorFlow 和 TensorFlow Lite 需要操作系统支持，而 TensorFlow Lite
    Micro 则无需操作系统支持。这使得 TensorFlow Lite Micro 能够减少内存开销和启动时间，尽管在需要时它仍然可以与实时操作系统如 FreeRTOS、Zephyr
    和 Mbed OS 集成。
- en: Memory management capabilities also distinguish the variants. TensorFlow Lite
    and TensorFlow Lite Micro support model memory mapping, enabling direct model
    access from flash storage rather than loading into RAM. TensorFlow lacks this
    capability, reflecting its design for environments with abundant memory resources.
    Memory mapping becomes increasingly important as deployment moves toward resource-constrained
    devices.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理能力也区分了这些变体。TensorFlow Lite 和 TensorFlow Lite Micro 支持模型内存映射，允许直接从闪存存储访问模型，而不是将其加载到
    RAM 中。TensorFlow 缺乏这种功能，反映了其针对内存资源丰富的环境的设计。随着部署转向资源受限的设备，内存映射变得越来越重要。
- en: Accelerator delegation capabilities further differentiate the variants. Both
    TensorFlow and TensorFlow Lite support delegation to accelerators, enabling efficient
    computation distribution. TensorFlow Lite Micro omits this feature, acknowledging
    the limited availability of specialized accelerators in embedded systems. This
    design choice maintains the framework’s minimal footprint while matching typical
    embedded hardware configurations.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器委托能力进一步区分了这些变体。TensorFlow 和 TensorFlow Lite 都支持将计算委托给加速器，从而实现高效的计算分配。TensorFlow
    Lite Micro 缺少了这一功能，承认在嵌入式系统中专用加速器的可用性有限。这种设计选择保持了框架的最小占用空间，同时匹配典型的嵌入式硬件配置。
- en: Hardware Constraints
  id: totrans-726
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件限制
- en: '[Table 7.6](ch013.xhtml#tbl-tf-hw-comparison) quantifies the hardware requirements
    across TensorFlow variants through three metrics: base binary size, memory footprint,
    and processor architecture support. These metrics demonstrate the progressive
    optimization for constrained computing environments.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7.6](ch013.xhtml#tbl-tf-hw-comparison) 通过三个指标：基础二进制大小、内存占用和处理器架构支持，量化了 TensorFlow
    变体之间的硬件要求。这些指标展示了针对受限计算环境的渐进式优化。'
- en: 'Table 7.6: **TensorFlow Hardware Optimization**: TensorFlow variants exhibit
    decreasing resource requirements (binary size and memory footprint) as they target
    increasingly constrained hardware architectures, enabling deployment on devices
    ranging from servers to microcontrollers. Optimized architectures reflect this
    trend, shifting from general-purpose cpus and gpus to arm cortex-m processors
    and digital signal processors for resource-limited environments.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.6：**TensorFlow硬件优化**：随着TensorFlow变体针对越来越受限的硬件架构进行优化，它们展现出的资源需求（二进制大小和内存占用）逐渐减少，从而使得在从服务器到微控制器等不同设备上部署成为可能。优化的架构反映了这一趋势，从通用CPU和GPU转向适用于资源受限环境的Arm
    Cortex-M处理器和数字信号处理器。
- en: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '|  | **TensorFlow** | **TensorFlow Lite** | **TensorFlow Lite for Microcontrollers**
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Base Binary Size** | ~3-5 MB (varies by platform and build configuration)
    | 100 KB | ~10 KB |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| **基础二进制大小** | ~3-5 MB（根据平台和构建配置而变化） | 100 KB | ~10 KB |'
- en: '| **Base Memory Footprint** | ~5+ MB (minimum runtime overhead) | 300 KB |
    20 KB |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| **基础内存占用** | ~5+ MB（最小运行开销） | 300 KB | 20 KB |'
- en: '| **Optimized Architectures** | X86, TPUs, GPUs | Arm Cortex A, x86 | Arm Cortex
    M, DSPs, MCUs |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '| **优化架构** | X86, TPUs, GPUs | Arm Cortex A, x86 | Arm Cortex M, DSPs, MCUs
    |'
- en: 'As established in [Table 7.4](ch013.xhtml#tbl-tf-comparison), binary size decreases
    dramatically across variants: from 3+ MB (TensorFlow) to 100 KB (TensorFlow Lite)
    to 10 KB (TensorFlow Lite Micro), reflecting progressive feature reduction and
    optimization.'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 如第7.4表[第7.4表](ch013.xhtml#tbl-tf-comparison)所示，不同变体之间的二进制文件大小显著减小：从TensorFlow的3+
    MB到TensorFlow Lite的100 KB，再到TensorFlow Lite Micro的10 KB，反映了逐步减少功能和优化。
- en: Memory footprint follows a similar pattern of reduction. TensorFlow requires
    approximately 5 MB of base memory, while TensorFlow Lite operates within 300 KB.
    TensorFlow Lite Micro further reduces memory requirements to 20 KB, enabling deployment
    on highly constrained devices.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 内存占用遵循类似的减少模式。TensorFlow大约需要5 MB的基础内存，而TensorFlow Lite在300 KB内运行。TensorFlow
    Lite Micro进一步降低内存需求至20 KB，使得在高度受限的设备上部署成为可能。
- en: Processor architecture support aligns with each variant’s intended deployment
    environment. TensorFlow supports x86 processors and accelerators including TPUs
    and GPUs, enabling high-performance computing in data centers as detailed in [Chapter 11](ch017.xhtml#sec-ai-acceleration).
    TensorFlow Lite targets mobile and edge processors, supporting Arm Cortex-A and
    x86 architectures. TensorFlow Lite Micro specializes in microcontroller deployment,
    supporting Arm Cortex-M cores, digital signal processors (DSPs), and various microcontroller
    units (MCUs) including STM32, NXP Kinetis, and Microchip AVR. The hardware acceleration
    strategies and architectures discussed in [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    provide essential context for understanding these processor optimization choices.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器架构支持与每个变体的预期部署环境相匹配。TensorFlow支持x86处理器和包括TPU和GPU在内的加速器，如第11章[第11节](ch017.xhtml#sec-ai-acceleration)中详细所述，从而在数据中心实现高性能计算。TensorFlow
    Lite针对移动和边缘处理器，支持Arm Cortex-A和x86架构。TensorFlow Lite Micro专注于微控制器部署，支持Arm Cortex-M核心、数字信号处理器(DSPs)以及包括STM32、NXP
    Kinetis和Microchip AVR在内的各种微控制器单元(MCUs)。第11章[第11节](ch017.xhtml#sec-ai-acceleration)中讨论的硬件加速策略和架构为理解这些处理器优化选择提供了基本背景。
- en: Production-Ready Evaluation Factors
  id: totrans-737
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产就绪评估因素
- en: Framework selection for embedded systems extends beyond technical specifications
    of model architecture, hardware requirements, and software dependencies. Additional
    factors affect development efficiency, maintenance requirements, and deployment
    success. Framework migration presents significant operational challenges including
    backward compatibility breaks, custom operator migration between versions, and
    production downtime risks. These migration concerns are addressed comprehensively
    in [Chapter 13](ch019.xhtml#sec-ml-operations), which covers migration planning,
    testing procedures, and rollback strategies. These factors require systematic
    evaluation to ensure optimal framework selection.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式系统的框架选择不仅超越了模型架构的技术规范、硬件要求和软件依赖，还包括其他因素，这些因素会影响开发效率、维护需求和部署成功。框架迁移带来了重大的操作挑战，包括向后兼容性中断、版本间的自定义算子迁移以及生产停机风险。这些迁移问题在第13章[第13节](ch019.xhtml#sec-ml-operations)中得到全面解决，该章节涵盖了迁移规划、测试程序和回滚策略。这些因素需要系统评估，以确保框架选择的最佳性。
- en: Performance Optimization
  id: totrans-739
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能优化
- en: 'Performance in embedded systems encompasses multiple metrics beyond computational
    speed. Framework evaluation must consider quantitative trade-offs across efficiency
    dimensions:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式系统中的性能涵盖了多个指标，而不仅仅是计算速度。框架评估必须考虑效率维度的定量权衡：
- en: Inference latency determines system responsiveness and real-time processing
    capabilities. For mobile applications, typical targets are 10-50ms for image classification
    and 1-5ms for keyword spotting. Edge deployments often require sub-millisecond
    response times for industrial control applications. TensorFlow Lite achieves 2-5x
    latency reduction compared to TensorFlow on mobile CPUs for typical inference
    workloads, while specialized frameworks like TensorRT can achieve 10-20x speedup
    on NVIDIA hardware through kernel fusion and precision optimization.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 推理延迟决定了系统的响应性和实时处理能力。对于移动应用，典型的目标是图像分类的10-50ms和关键词检测的1-5ms。边缘部署通常需要亚毫秒级响应时间以用于工业控制应用。与移动CPU上的TensorFlow相比，TensorFlow
    Lite在典型的推理工作负载上实现了2-5倍的延迟降低，而像TensorRT这样的专用框架可以通过内核融合和精度优化在NVIDIA硬件上实现10-20倍的速度提升。
- en: 'Memory utilization affects both static storage requirements and runtime efficiency.
    Framework memory overhead varies dramatically: TensorFlow requires 5+ MB baseline
    memory, TensorFlow Lite operates within 300KB, while TensorFlow Lite Micro runs
    in 20KB. Model memory scaling follows similar patterns: a MobileNetV2 model consumes
    approximately 14MB in TensorFlow but only 3.4MB when quantized in TensorFlow Lite,
    representing a 4x reduction while maintaining 95%+ accuracy.'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 内存利用率影响静态存储需求和运行时效率。框架内存开销差异很大：TensorFlow需要5+ MB的基线内存，TensorFlow Lite在300KB内运行，而TensorFlow
    Lite Micro在20KB内运行。模型内存缩放遵循类似的模式：MobileNetV2模型在TensorFlow中消耗大约14MB，但在TensorFlow
    Lite中量化后仅消耗3.4MB，这代表了一个4倍减少，同时保持了95%以上的准确性。
- en: Power consumption impacts battery life and thermal management requirements.
    Quantized INT8 inference consumes 4-8x less energy than FP32 operations on typical
    mobile processors. Apple’s Neural Engine achieves 7.2 TOPS/W efficiency for INT8
    operations compared to 0.1-0.5 TOPS/W for CPU-based FP32 computation. Sparse computation
    can provide additional 2-3x energy savings when frameworks support structured
    sparsity patterns optimized for specific hardware.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 功耗影响电池寿命和热管理需求。在典型的移动处理器上，量化INT8推理比FP32操作节省4-8倍的能量。与基于CPU的FP32计算的0.1-0.5 TOPS/W相比，苹果的神经网络引擎在INT8操作上实现了7.2
    TOPS/W的效率。当框架支持针对特定硬件优化的结构化稀疏模式时，稀疏计算可以提供额外的2-3倍能量节省。
- en: Computational efficiency measured in FLOPS provides standardized performance
    comparison. Modern mobile frameworks achieve 10-50 GFLOPS on high-end smartphone
    processors, while specialized accelerators like Google’s Edge TPU deliver 4 TOPS
    (INT8) in 2W power budget. Framework optimization techniques including operator
    fusion can improve FLOPS utilization from 10-20% to 60-80% of theoretical peak
    performance on typical workloads.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 以FLOPS衡量的计算效率提供了标准化的性能比较。现代移动框架在高性能智能手机处理器上实现10-50 GFLOPS，而像谷歌的Edge TPU这样的专用加速器在2W的功耗预算下提供4
    TOPS（INT8）。包括操作融合在内的框架优化技术可以将FLOPS利用率从典型工作负载的10-20%提高到理论峰值性能的60-80%。
- en: Deployment Scalability
  id: totrans-745
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署可扩展性
- en: 'Scalability requirements span both technical capabilities and operational considerations.
    Framework support must extend across deployment scales and scenarios:'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性需求涵盖了技术能力和运营考虑因素。框架支持必须扩展到部署规模和场景：
- en: Device scaling enables consistent deployment from microcontrollers to more powerful
    embedded processors. Operational scaling supports the transition from development
    prototypes to production deployments. Version management facilitates model updates
    and maintenance across deployed devices. The framework must maintain consistent
    performance characteristics throughout these scaling dimensions.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 设备缩放使得从微控制器到更强大的嵌入式处理器的一致部署成为可能。运营缩放支持从开发原型到生产部署的过渡。版本管理促进了部署设备上的模型更新和维护。框架必须在这些缩放维度上保持一致的性能特征。
- en: The TensorFlow ecosystem demonstrates how framework design must balance competing
    requirements across diverse deployment scenarios. The systematic evaluation methodology
    illustrated through this case study (analyzing model requirements, software dependencies,
    and hardware constraints alongside operational factors) provides a template for
    evaluating any framework ecosystem. Whether comparing PyTorch’s dynamic execution
    model for research workflows, ONNX’s cross-platform standardization for deployment
    flexibility, JAX’s functional programming approach for performance optimization,
    or specialized frameworks for domain-specific applications, the same analytical
    framework guides informed decision-making that aligns technical capabilities with
    project requirements and organizational constraints.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow生态系统展示了框架设计如何在不同的部署场景中平衡相互竞争的需求。通过这个案例研究（分析模型需求、软件依赖、硬件约束以及运营因素）所展示的系统评估方法为评估任何框架生态系统提供了一个模板。无论是比较PyTorch用于研究工作流程的动态执行模型、ONNX用于部署灵活性的跨平台标准化、JAX用于性能优化的函数式编程方法，还是针对特定应用领域的专业框架，相同的分析框架指导着与项目需求和组织约束相一致的有信息决策。
- en: Development Support and Long-term Viability Assessment
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发支持与长期可行性评估
- en: Framework selection extends beyond technical capabilities to encompass the broader
    ecosystem that determines long-term viability and development velocity. The community
    and ecosystem surrounding a framework significantly influence its evolution, support
    quality, and integration possibilities. Understanding these ecosystem dynamics
    helps predict framework sustainability and development productivity over project
    lifecycles.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 框架选择不仅超越了技术能力，还包括决定长期可行性和开发速度的更广泛生态系统。围绕框架的社区和生态系统对其演变、支持质量和集成可能性有重大影响。理解这些生态系统动态有助于预测框架在项目生命周期中的可持续性和开发生产力。
- en: Developer Resources and Knowledge Sharing Networks
  id: totrans-751
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开发者资源和知识共享网络
- en: The vitality of a framework’s community affects multiple practical aspects of
    development and deployment. Active communities drive faster bug fixes, more comprehensive
    documentation, and broader hardware support. Community size and engagement metrics
    (such as GitHub activity, Stack Overflow question volume, and conference presence)
    provide indicators of framework momentum and longevity.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 框架社区的活力影响着开发和部署的多个实际方面。活跃的社区推动着更快的错误修复、更全面的文档和更广泛的硬件支持。社区规模和参与度指标（如GitHub活动、Stack
    Overflow问题数量和会议出席情况）提供了框架势头和持久性的指标。
- en: PyTorch’s academic community has driven rapid innovation in research-oriented
    features, contributing to extensive support for novel architectures and experimental
    techniques. This community focus has resulted in excellent educational resources,
    research reproducibility tools, and advanced feature development. However, production
    tooling has historically lagged behind research capabilities, though initiatives
    like PyTorch Lightning and TorchServe have addressed many operational gaps.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的学术社区推动了研究导向功能的快速创新，为新颖架构和实验技术提供了广泛的支持。这种社区关注导致了优秀的教育资源、研究可重复性工具和高级功能开发。然而，生产工具在历史上一直落后于研究能力，尽管像PyTorch
    Lightning和TorchServe这样的倡议解决了许多运营差距。
- en: TensorFlow’s enterprise community has emphasized production-ready tools and
    scalable deployment solutions. This focus has produced robust serving infrastructure,
    comprehensive monitoring tools, and enterprise integration capabilities. The broader
    TensorFlow ecosystem includes specialized tools like TensorFlow Extended (TFX)
    for production ML pipelines, TensorBoard for visualization, and TensorFlow Model
    Analysis for model evaluation and validation.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的企业社区强调了现成工具和可扩展部署解决方案。这种关注产生了健壮的服务基础设施、全面的监控工具和企业集成能力。更广泛的TensorFlow生态系统包括专门工具，如用于生产机器学习管道的TensorFlow
    Extended (TFX)、用于可视化的TensorBoard以及用于模型评估和验证的TensorFlow Model Analysis。
- en: JAX’s functional programming community has concentrated on mathematical rigor
    and program transformation capabilities. This specialized focus has led to powerful
    research tools and elegant mathematical abstractions, but with a steeper learning
    curve for developers not familiar with functional programming concepts.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的函数式编程社区专注于数学严谨性和程序转换能力。这种专业化的关注导致了强大的研究工具和优雅的数学抽象，但对于不熟悉函数式编程概念的开发者来说，学习曲线较为陡峭。
- en: Supporting Infrastructure and Third-Party Compatibility
  id: totrans-756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持基础设施和第三方兼容性
- en: The practical utility of a framework often depends more on its ecosystem tools
    than its core capabilities. These tools determine development velocity, debugging
    effectiveness, and deployment flexibility.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的实际效用往往更多地取决于其生态系统工具，而不是其核心能力。这些工具决定了开发速度、调试有效性和部署灵活性。
- en: Hugging Face has become a de facto standard for natural language processing
    model libraries, providing consistent APIs across PyTorch, TensorFlow, and JAX
    backends. The availability of high-quality pretrained models and fine-tuning tools
    can dramatically accelerate project development. TensorFlow Hub and PyTorch Hub
    provide official model repositories, though third-party collections often offer
    broader selection and more recent architectures.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 已成为自然语言处理模型库的事实标准，为 PyTorch、TensorFlow 和 JAX 后端提供一致的 API。高质量预训练模型和微调工具的可用性可以显著加速项目开发。TensorFlow
    Hub 和 PyTorch Hub 提供官方模型存储库，尽管第三方集合通常提供更广泛的选择和更新的架构。
- en: PyTorch Lightning has abstracted much of PyTorch’s training boilerplate while
    maintaining research flexibility, addressing one of PyTorch’s historical weaknesses
    in structured training workflows. Weights & Biases and MLflow provide experiment
    tracking across multiple frameworks, enabling consistent workflow management regardless
    of underlying framework choice. TensorBoard has evolved into a cross-framework
    visualization tool, though its integration remains tightest with TensorFlow.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 在保持研究灵活性的同时，抽象了 PyTorch 大部分训练模板代码，解决了 PyTorch 在结构化训练工作流程中的历史弱点。Weights
    & Biases 和 MLflow 提供了跨多个框架的实验跟踪，使得无论底层框架选择如何，都能实现一致的工作流程管理。TensorBoard 已发展成为跨框架的可视化工具，尽管其集成与
    TensorFlow 的结合最为紧密。
- en: TensorFlow Serving and TorchServe provide production-ready serving solutions,
    though their feature sets and operational characteristics differ significantly.
    ONNX Runtime has emerged as a framework-agnostic serving solution, enabling deployment
    flexibility at the cost of some framework-specific optimizations. Cloud provider
    ML services (AWS SageMaker, Google AI Platform, Azure ML) often provide native
    integration for specific frameworks while supporting others through containerized
    deployments.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 和 TorchServe 提供了生产就绪的服务解决方案，尽管它们的特性和操作特性差异很大。ONNX Runtime
    已成为框架无关的服务解决方案，以牺牲一些框架特定优化为代价，实现了部署灵活性。云提供商的 ML 服务（如 AWS SageMaker、Google AI Platform、Azure
    ML）通常为特定框架提供原生集成，同时通过容器化部署支持其他框架。
- en: Framework-specific optimization tools can provide significant performance advantages
    but create vendor lock-in. TensorFlow’s XLA compiler and PyTorch’s TorchScript
    offer framework-native optimization paths, while tools like Apache TVM provide
    cross-framework optimization capabilities. The choice between framework-specific
    and cross-framework optimization tools affects both performance and deployment
    flexibility.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 框架特定的优化工具可以提供显著的性能优势，但会创造供应商锁定。TensorFlow 的 XLA 编译器和 PyTorch 的 TorchScript 提供了框架本地的优化路径，而像
    Apache TVM 这样的工具则提供了跨框架的优化能力。框架特定与跨框架优化工具的选择会影响性能和部署灵活性。
- en: Long-term Technology Investment Considerations
  id: totrans-762
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长期技术投资考虑因素
- en: 'Long-term framework decisions must consider ecosystem evolution and sustainability.
    Framework popularity can shift rapidly in response to technical innovations, community
    momentum, or corporate strategy changes. Organizations should evaluate ecosystem
    health through multiple indicators: contributor diversity (avoiding single-company
    dependence), funding stability, roadmap transparency, and backward compatibility
    commitments.'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 长期框架决策必须考虑生态系统的发展和可持续性。框架的流行度可能会因技术创新、社区动力或企业战略变化而迅速变化。组织应通过多个指标评估生态系统健康：贡献者多样性（避免单一公司依赖）、资金稳定性、路线图透明度和向后兼容性承诺。
- en: The ecosystem perspective also influences hiring and team development strategies.
    Framework choice affects the available talent pool, training requirements, and
    knowledge transfer capabilities. Teams must consider whether their framework choice
    aligns with local expertise, educational institution curricula, and industry hiring
    trends.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 生态系统视角也影响着招聘和团队发展策略。框架选择会影响可用的人才库、培训需求和知识转移能力。团队必须考虑他们的框架选择是否与当地的专业知识、教育机构课程和行业招聘趋势相一致。
- en: Integration with existing organizational tools and processes represents another
    critical ecosystem consideration. Framework compatibility with continuous integration
    systems, deployment pipelines, monitoring infrastructure, and security tooling
    can significantly affect operational overhead. Some frameworks integrate more
    naturally with specific cloud providers or enterprise software stacks, creating
    operational advantages or vendor dependencies.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有组织工具和流程的集成代表另一个关键的生态系统考虑因素。框架与持续集成系统、部署管道、监控基础设施和安全工具的兼容性可以显著影响运营成本。一些框架与特定的云提供商或企业软件堆栈集成得更自然，从而创造运营优势或供应商依赖。
- en: While deep ecosystem integration can provide development velocity advantages,
    teams should maintain awareness of migration paths and cross-framework compatibility.
    Using standardized model formats like ONNX, maintaining framework-agnostic data
    pipelines, and documenting framework-specific customizations can preserve flexibility
    for future framework transitions.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度生态系统集成可以提供开发速度优势，但团队应保持对迁移路径和跨框架兼容性的意识。使用标准化的模型格式如ONNX，维护框架无关的数据管道，以及记录框架特定的定制化，可以保留未来框架转换的灵活性。
- en: The ecosystem perspective reminds us that framework selection involves choosing
    not just a software library, but joining a community and committing to an evolving
    technological ecosystem. Understanding these broader implications helps teams
    make framework decisions that remain viable and advantageous throughout project
    lifecycles.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 生态系统视角提醒我们，框架选择不仅涉及选择一个软件库，而且是加入一个社区并承诺一个不断发展的技术生态系统。理解这些更广泛的影响有助于团队做出在整个项目生命周期中保持可行和有利的框架决策。
- en: Systematic Framework Performance Assessment
  id: totrans-768
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统框架性能评估
- en: Systematic evaluation of framework efficiency requires comprehensive metrics
    that capture the multi-dimensional trade-offs between accuracy, performance, and
    resource consumption. Traditional machine learning evaluation focuses primarily
    on accuracy metrics, but production deployment demands systematic assessment of
    computational efficiency, memory utilization, energy consumption, and operational
    constraints.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 对框架效率的系统评估需要全面的指标，这些指标能够捕捉准确度、性能和资源消耗之间的多维度权衡。传统的机器学习评估主要关注准确度指标，但生产部署需要系统地评估计算效率、内存利用率、能耗和操作限制。
- en: Framework efficiency evaluation encompasses four primary dimensions that reflect
    real-world deployment requirements. Computational efficiency measures the framework’s
    ability to utilize available hardware resources effectively, typically quantified
    through FLOPS utilization, kernel efficiency, and parallelization effectiveness.
    Memory efficiency evaluates both peak memory usage and memory bandwidth utilization,
    critical factors for deployment on resource-constrained devices. Energy efficiency
    quantifies power consumption characteristics, essential for mobile applications
    and sustainable computing. Deployment efficiency assesses the operational characteristics
    including model size, initialization time, and integration complexity.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 框架效率评估包括四个主要维度，这些维度反映了现实世界的部署需求。计算效率衡量框架有效利用可用硬件资源的能力，通常通过FLOPS利用率、内核效率和并行化有效性来量化。内存效率评估峰值内存使用和内存带宽利用率，这对于在资源受限设备上的部署是关键因素。能源效率量化功耗特性，对于移动应用和可持续计算至关重要。部署效率评估操作特性，包括模型大小、初始化时间和集成复杂性。
- en: Quantitative Multi-Dimensional Performance Analysis
  id: totrans-771
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定量多维度性能分析
- en: Standardized comparison requires quantitative metrics across representative
    workloads and hardware configurations. [Table 7.7](ch013.xhtml#tbl-framework-efficiency-matrix)
    provides systematic comparison of major frameworks across efficiency dimensions
    using benchmark workloads representative of production deployment scenarios.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化比较需要跨代表性工作负载和硬件配置的定量指标。[表7.7](ch013.xhtml#tbl-framework-efficiency-matrix)提供了使用代表生产部署场景的基准工作负载，对主要框架在效率维度上的系统比较。
- en: 'Table 7.7: **Framework Efficiency Comparison**: Quantitative comparison of
    major machine learning frameworks across efficiency dimensions using ResNet-50
    inference on representative hardware (NVIDIA A100 GPU for server frameworks, ARM
    Cortex-A78 for mobile frameworks). Metrics reflect production-representative workloads
    with accuracy maintained within 1% of baseline. Hardware utilization represents
    percentage of theoretical peak performance achieved on typical operations.'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.7：**框架效率比较**：使用代表性硬件（服务器框架的NVIDIA A100 GPU，移动框架的ARM Cortex-A78）上的ResNet-50推理对主要机器学习框架在效率维度上的定量比较。指标反映了具有准确度保持在基线1%以内的生产代表性工作负载。硬件利用率表示在典型操作中达到理论峰值性能的百分比。
- en: '| **Framework** | **Inference** **Latency (ms)** | **Memory** **Usage (MB)**
    | **Energy** **(mJ/inference)** | **Model Size** **Reduction** | **Hardware**
    **Utilization (%)** |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| **框架** | **推理** **延迟 (ms)** | **内存** **使用 (MB)** | **能耗** **(mJ/inference)**
    | **模型大小** **缩减** | **硬件** **利用率 (%)** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **TensorFlow** | 45 | 2,100 | 850 | None | 35 |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| **TensorFlow** | 45 | 2,100 | 850 | 无 | 35 |'
- en: '| **TensorFlow Lite** | 12 | 180 | 120 | 4x (quantized) | 65 |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| **TensorFlow Lite** | 12 | 180 | 120 | 4x (量化) | 65 |'
- en: '| **TensorFlow Lite Micro** | 8 | 32 | 45 | 8x (pruned+quant) | 75 |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| **TensorFlow Lite Micro** | 8 | 32 | 45 | 8x（剪枝+量化） | 75 |'
- en: '| **PyTorch** | 52 | 1,800 | 920 | None | 32 |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| **PyTorch** | 52 | 1,800 | 920 | 无 | 32 |'
- en: '| **PyTorch Mobile** | 18 | 220 | 180 | 3x (quantized) | 58 |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| **PyTorch Mobile** | 18 | 220 | 180 | 3x (量化) | 58 |'
- en: '| **ONNX Runtime** | 15 | 340 | 210 | 2x (optimized) | 72 |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| **ONNX Runtime** | 15 | 340 | 210 | 2x (优化) | 72 |'
- en: '| **TensorRT** | 3 | 450 | 65 | 2x (precision opt) | 88 |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '| **TensorRT** | 3 | 450 | 65 | 2x (精度优化) | 88 |'
- en: '| **Apache TVM** | 6 | 280 | 95 | 3x (compiled) | 82 |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| **Apache TVM** | 6 | 280 | 95 | 3x (编译) | 82 |'
- en: Standardized Benchmarking Protocols
  id: totrans-784
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化基准测试协议
- en: Systematic framework evaluation requires standardized benchmarking approaches
    that capture efficiency characteristics across diverse deployment scenarios. The
    evaluation methodology employs representative model architectures (ResNet-50 for
    vision, BERT-Base for language processing, MobileNetV2 for mobile deployment),
    standardized datasets (ImageNet for vision, GLUE for language), and consistent
    hardware configurations (NVIDIA A100 for server evaluation, ARM Cortex-A78 for
    mobile assessment).
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性框架评估需要标准化的基准测试方法，以捕捉不同部署场景下的效率特征。评估方法采用代表性的模型架构（ResNet-50用于视觉，BERT-Base用于语言处理，MobileNetV2用于移动部署），标准化数据集（ImageNet用于视觉，GLUE用于语言），以及一致的硬件配置（NVIDIA
    A100用于服务器评估，ARM Cortex-A78用于移动评估）。
- en: Performance profiling uses instrumentation to measure framework overhead, kernel
    efficiency, and resource utilization patterns. Memory analysis includes peak allocation
    measurement, memory bandwidth utilization assessment, and garbage collection overhead
    quantification. Energy measurement employs hardware-level power monitoring (NVIDIA-SMI
    for GPU power, specialized mobile power measurement tools) to capture actual energy
    consumption during inference and training operations.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析使用仪器测量框架开销、内核效率和资源利用模式。内存分析包括峰值分配测量、内存带宽利用率评估和垃圾收集开销量化。能耗测量采用硬件级功率监控（NVIDIA-SMI用于GPU功率，专用移动功率测量工具）以捕捉推理和训练操作的实际能耗。
- en: Accuracy preservation validation ensures that efficiency optimizations maintain
    model quality within acceptable bounds. Quantization-aware training validates
    that INT8 models achieve <1% accuracy degradation. Pruning techniques verify that
    sparse models maintain target accuracy while achieving specified compression ratios.
    Knowledge distillation confirms that compressed models preserve teacher model
    capability.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度保持验证确保效率优化在可接受的范围内保持模型质量。量化感知训练验证INT8模型实现<1%的准确度下降。剪枝技术验证稀疏模型在达到指定压缩比的同时保持目标准确度。知识蒸馏确认压缩模型保留了教师模型的能力。
- en: Real-World Operational Performance Considerations
  id: totrans-788
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际操作性能考虑因素
- en: Framework efficiency evaluation must consider operational constraints that affect
    real-world deployment success. Latency analysis includes cold-start performance
    (framework initialization time), warm-up characteristics (performance stabilization
    requirements), and steady-state inference speed. Memory analysis encompasses both
    static requirements (framework binary size, model storage) and dynamic usage patterns
    (peak allocation, memory fragmentation, cleanup efficiency).
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 框架效率评估必须考虑影响实际部署成功的运营约束。延迟分析包括冷启动性能（框架初始化时间）、预热特性（性能稳定要求）和稳态推理速度。内存分析包括静态要求（框架二进制大小、模型存储）和动态使用模式（峰值分配、内存碎片、清理效率）。
- en: Scalability assessment evaluates framework behavior under production load conditions
    including concurrent request handling, batching efficiency, and resource sharing
    across multiple model instances. Integration testing validates framework compatibility
    with production infrastructure including container deployment, service mesh integration,
    monitoring system compatibility, and observability tool support.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性评估评估框架在以下生产负载条件下的行为：并发请求处理、批处理效率以及多个模型实例之间的资源共享。集成测试验证框架与生产基础设施的兼容性，包括容器部署、服务网格集成、监控系统兼容性和可观察性工具支持。
- en: Reliability evaluation assesses framework stability under extended operation,
    error handling capabilities, and recovery mechanisms. Performance consistency
    measurement identifies variance in execution time, memory usage stability, and
    thermal behavior under sustained load conditions.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性评估评估框架在长时间运行下的稳定性、错误处理能力和恢复机制。性能一致性测量识别在持续负载条件下的执行时间、内存使用稳定性和热行为的变化。
- en: Structured Framework Selection Process
  id: totrans-792
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化框架选择流程
- en: Systematic framework selection requires structured evaluation that balances
    efficiency metrics against operational requirements and organizational constraints.
    The decision framework evaluates technical capabilities (supported operations,
    hardware acceleration, optimization features), operational requirements (deployment
    flexibility, monitoring integration, maintenance overhead), and organizational
    factors (team expertise, development velocity, ecosystem compatibility).
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 系统化的框架选择需要结构化的评估，平衡效率指标与运营要求和组织约束。决策框架评估技术能力（支持的操作、硬件加速、优化功能），运营要求（部署灵活性、监控集成、维护开销），以及组织因素（团队专业知识、开发速度、生态系统兼容性）。
- en: Efficiency requirements specification defines acceptable trade-offs between
    accuracy and performance, establishes resource constraints (memory limits, power
    budgets, latency requirements), and identifies critical optimization features
    (quantization support, pruning capabilities, hardware-specific acceleration).
    These requirements guide framework evaluation priorities and eliminate options
    that cannot meet fundamental constraints.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 效率要求规范定义了精度和性能之间的可接受权衡，建立了资源限制（内存限制、电力预算、延迟要求），并确定了关键优化功能（量化支持、剪枝能力、硬件特定加速）。这些要求指导框架评估优先级，并消除无法满足基本限制的选项。
- en: Risk assessment considers framework maturity, ecosystem stability, and migration
    complexity. Vendor dependency evaluation assesses framework governance, licensing
    terms, and long-term support commitments. Migration cost analysis estimates effort
    required for framework adoption, team training requirements, and infrastructure
    modifications.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 风险评估考虑框架成熟度、生态系统稳定性和迁移复杂性。供应商依赖性评估评估框架治理、许可条款和长期支持承诺。迁移成本分析估计采用框架所需的工作量、团队培训需求和基础设施修改。
- en: The systematic approach to framework efficiency evaluation provides quantitative
    foundation for deployment decisions while considering the broader operational
    context that determines production success. This methodology enables teams to
    select frameworks that optimize for their specific efficiency requirements while
    maintaining the flexibility needed for evolving deployment scenarios.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 对框架效率的系统评估方法为部署决策提供了定量基础，同时考虑了决定生产成功的更广泛的运营环境。这种方法使团队能够选择针对其特定效率要求的框架，同时保持适应不断变化的部署场景所需的灵活性。
- en: Common Framework Selection Misconceptions
  id: totrans-797
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见框架选择误区
- en: Machine learning frameworks represent complex software ecosystems that abstract
    significant computational complexity while making critical architectural decisions
    on behalf of developers. The diversity of available frameworks (each with distinct
    design philosophies and optimization strategies) often leads to misconceptions
    about their interchangeability and appropriate selection criteria. Understanding
    these common fallacies and pitfalls helps practitioners make more informed framework
    choices.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架代表复杂的软件生态系统，它们在代表开发者做出关键架构决策的同时，抽象了大量的计算复杂性。可用的框架多样性（每个框架都有独特的设计哲学和优化策略）往往导致对它们可互换性和适当选择标准的误解。了解这些常见的谬误和陷阱有助于从业者做出更明智的框架选择。
- en: '**Fallacy:** *All frameworks provide equivalent performance for the same model.*'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *所有框架为同一模型提供等效的性能。*'
- en: This misconception leads teams to select frameworks based solely on API convenience
    or familiarity without considering performance implications. Different frameworks
    implement operations using varying optimization strategies, memory management
    approaches, and hardware utilization patterns. A model that performs efficiently
    in PyTorch might execute poorly in TensorFlow due to different graph optimization
    strategies. Similarly, framework overhead, automatic differentiation implementation,
    and tensor operation scheduling can create significant performance differences
    even for identical model architectures. Framework selection requires benchmarking
    actual workloads rather than assuming performance equivalence.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解导致团队在选择框架时仅基于API的便利性或熟悉度，而没有考虑性能影响。不同的框架使用不同的优化策略、内存管理方法和硬件利用模式来实现操作。一个在PyTorch中表现高效的模型可能在TensorFlow中执行不佳，这是由于不同的图优化策略。同样，框架开销、自动微分实现和张量操作调度甚至对于相同的模型架构也能造成显著的性能差异。框架选择需要基准测试实际的工作负载，而不是假设性能等效。
- en: '**Pitfall:** *Choosing frameworks based on popularity rather than project requirements.*'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *根据流行度而非项目需求选择框架。*'
- en: Many practitioners select frameworks based on community size, tutorial availability,
    or industry adoption without analyzing their specific technical requirements.
    Popular frameworks often target general-use cases rather than specialized deployment
    scenarios. A framework optimized for large-scale cloud training might be inappropriate
    for mobile deployment, while research-focused frameworks might lack production
    deployment capabilities. Effective framework selection requires matching technical
    capabilities to specific requirements rather than following popularity trends.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者根据社区规模、教程可用性或行业采用率来选择框架，而没有分析他们的具体技术需求。流行的框架通常针对通用用例，而不是专门的部署场景。针对大规模云训练优化的框架可能不适合移动部署，而专注于研究的框架可能缺乏生产部署能力。有效的框架选择需要将技术能力与特定需求相匹配，而不是跟随流行趋势。
- en: '**Fallacy:** *Framework abstractions hide all system-level complexity from
    developers.*'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *框架抽象隐藏了所有系统级复杂性。*'
- en: This belief assumes that frameworks automatically handle all performance optimization
    and hardware utilization without developer understanding. While frameworks provide
    convenient abstractions, achieving optimal performance requires understanding
    their underlying computational models, memory management strategies, and hardware
    mapping approaches. Developers who treat frameworks as black boxes often encounter
    unexpected performance bottlenecks, memory issues, or deployment failures. Effective
    framework usage requires understanding both the abstractions provided and their
    underlying implementation implications.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设框架会自动处理所有性能优化和硬件利用，而不需要开发者理解。虽然框架提供了便利的抽象，但要实现最佳性能需要理解它们背后的计算模型、内存管理策略和硬件映射方法。将框架视为黑盒的开发者经常会遇到意外的性能瓶颈、内存问题或部署失败。有效的框架使用需要理解提供的抽象及其背后的实现影响。
- en: '**Pitfall:** *Vendor lock-in through framework-specific model formats and APIs.*'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *通过框架特定的模型格式和API实现供应商锁定。*'
- en: Teams often build entire development workflows around single frameworks without
    considering interoperability requirements. Framework-specific model formats, custom
    operators, and proprietary optimization techniques create dependencies that complicate
    migration, deployment, or collaboration across different tools. This lock-in becomes
    problematic when deployment requirements change, performance needs evolve, or
    framework development directions diverge from project goals. Maintaining model
    portability requires attention to standards-based formats and avoiding framework-specific
    features that cannot be translated across platforms. These considerations become
    particularly important when implementing responsible AI practices [Chapter 17](ch023.xhtml#sec-responsible-ai)
    that may require model auditing, fairness testing, or bias mitigation across different
    deployment environments.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 团队通常围绕单个框架构建整个开发工作流程，而不考虑互操作性需求。框架特定的模型格式、自定义操作和专有优化技术创建了复杂的依赖关系，这些依赖关系会使得迁移、部署或跨不同工具的协作变得复杂。当部署需求变化、性能需求发展或框架开发方向与项目目标不一致时，这种锁定变得问题重重。维护模型的可移植性需要关注基于标准的格式，并避免无法跨平台转换的框架特定功能。在实施可能需要在不同部署环境中进行模型审计、公平性测试或偏差缓解的负责任AI实践[第17章](ch023.xhtml#sec-responsible-ai)时，这些考虑变得尤为重要。
- en: '**Pitfall:** *Overlooking production infrastructure requirements when selecting
    development frameworks.*'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 在选择开发框架时**忽视**生产基础设施需求。'
- en: Many teams choose frameworks based on ease of development without considering
    how they integrate with production infrastructure for model serving, monitoring,
    and lifecycle management. A framework excellent for research and prototyping may
    lack robust model serving capabilities, fail to integrate with existing monitoring
    systems, or provide inadequate support for A/B testing and gradual rollouts. Production
    deployment often requires additional components for load balancing, caching, model
    versioning, and rollback mechanisms that may not align well with the chosen development
    framework. Some frameworks excel at training but require separate serving systems,
    while others provide integrated pipelines that may not meet enterprise security
    or scalability requirements. Effective framework selection must consider the entire
    production ecosystem including container orchestration, API gateway integration,
    observability tools, and operational procedures rather than focusing solely on
    model development convenience.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队在选择框架时基于开发便利性，而没有考虑它们如何与生产基础设施集成以进行模型服务、监控和生命周期管理。一个在研究和原型设计方面出色的框架可能缺乏强大的模型服务能力，无法与现有的监控系统集成，或提供不足的A/B测试和逐步部署支持。生产部署通常需要额外的组件，如负载均衡、缓存、模型版本控制和回滚机制，这些可能与选定的开发框架不太匹配。一些框架在训练方面表现出色，但需要单独的服务系统，而其他框架提供集成管道，可能不符合企业安全或可扩展性要求。有效的框架选择必须考虑整个生产生态系统，包括容器编排、API网关集成、可观察性工具和操作程序，而不仅仅是关注模型开发的便利性。
- en: Summary
  id: totrans-809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning frameworks represent software abstractions that transform mathematical
    concepts into practical computational tools for building and deploying AI systems.
    These frameworks encapsulate complex operations like automatic differentiation,
    distributed training, and hardware acceleration behind programmer-friendly interfaces
    that enable efficient development across diverse application domains. The evolution
    from basic numerical libraries to modern frameworks demonstrates how software
    infrastructure shapes the accessibility and capability of machine learning development.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架代表了将数学概念转化为构建和部署AI系统的实用计算工具的软件抽象。这些框架封装了复杂的操作，如自动微分、分布式训练和硬件加速，通过程序员友好的接口实现，从而在多样化的应用领域内实现高效的开发。从基本的数值库到现代框架的演变展示了软件基础设施如何塑造机器学习开发的可访问性和能力。
- en: This evolution has produced a diverse ecosystem with distinct optimization strategies.
    Contemporary frameworks embody different design philosophies that reflect varying
    priorities in machine learning development. Research-focused frameworks prioritize
    flexibility and rapid experimentation, enabling quick iteration on novel architectures
    and algorithms. Production-oriented frameworks emphasize scalability, reliability,
    and deployment efficiency for large-scale systems. Specialized frameworks target
    specific deployment contexts, from cloud-scale distributed systems to resource-constrained
    edge devices, each optimizing for distinct performance and efficiency requirements.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 这种进化产生了一个具有不同优化策略的多样化生态系统。当代框架体现了不同的设计理念，反映了机器学习开发中的不同优先级。以研究为重点的框架优先考虑灵活性和快速实验，使开发者能够快速迭代新的架构和算法。面向生产的框架强调可扩展性、可靠性和大规模系统的部署效率。专门化的框架针对特定的部署环境，从云规模的分布式系统到资源受限的边缘设备，每个框架都针对不同的性能和效率要求进行优化。
- en: '**Key Takeaways**'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Frameworks abstract complex computational operations like automatic differentiation
    and distributed training behind developer-friendly interfaces
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架通过开发者友好的接口抽象了复杂的计算操作，如自动微分和分布式训练
- en: 'Different frameworks embody distinct design philosophies: research flexibility
    vs production scalability vs deployment efficiency'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的框架体现了不同的设计理念：研究灵活性 vs 生产可扩展性 vs 部署效率
- en: Specialization across computing environments requires framework variants optimized
    for cloud, edge, mobile, and microcontroller deployments
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同的计算环境中进行专业化需要针对云、边缘、移动和微控制器部署进行优化的框架变体
- en: Framework architecture understanding enables informed tool selection, performance
    optimization, and effective debugging across diverse deployment contexts
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对框架架构的理解能够使开发者在不同部署环境中进行明智的工具选择、性能优化和有效的调试
- en: Framework development continues evolving toward greater developer productivity,
    broader hardware support, and more flexible deployment options. Cross-platform
    compilation, dynamic optimization, and unified programming models aim to reduce
    the complexity of developing and deploying machine learning systems across diverse
    computing environments. Understanding framework capabilities and limitations enables
    developers to make informed architectural decisions for the model optimization
    techniques in [Chapter 10](ch016.xhtml#sec-model-optimizations), hardware acceleration
    strategies in [Chapter 11](ch017.xhtml#sec-ai-acceleration), and deployment patterns
    in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 框架开发持续向提高开发者生产力、更广泛的硬件支持和更灵活的部署选项进化。跨平台编译、动态优化和统一的编程模型旨在降低在多样化的计算环境中开发和部署机器学习系统的复杂性。了解框架的能力和限制使开发者能够为[第10章](ch016.xhtml#sec-model-optimizations)中的模型优化技术、[第11章](ch017.xhtml#sec-ai-acceleration)中的硬件加速策略和[第13章](ch019.xhtml#sec-ml-operations)中的部署模式做出明智的架构决策。
- en: '* * *'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
