- en: Benchmarking AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试AI
- en: '*DALL·E 3 Prompt: Photo of a podium set against a tech-themed backdrop. On
    each tier of the podium, there are AI chips with intricate designs. The top chip
    has a gold medal hanging from it, the second one has a silver medal, and the third
    has a bronze medal. Banners with ‘AI Olympics’ are displayed prominently in the
    background.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3提示：一张背景为科技主题的讲台照片。讲台的每一层都放置着设计复杂的AI芯片。最顶层的芯片上挂着金牌，第二层芯片上挂着银牌，第三层芯片上挂着铜牌。背景中显著地展示着“AI奥运会”的横幅。*'
- en: '![](../media/file192.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file192.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why does systematic measurement form the foundation of engineering progress
    in machine learning systems, and how does standardized benchmarking enable scientific
    advancement in this emerging field?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么系统测量是机器学习系统中工程进步的基础，以及标准化基准测试如何促进这一新兴领域的科学进步？*'
- en: Engineering disciplines advance through measurement and comparison, establishing
    benchmarking as essential to machine learning systems development. Without systematic
    evaluation frameworks, optimization claims lack scientific rigor, hardware investments
    proceed without evidence, and system improvements cannot be verified or reproduced.
    Benchmarking transforms subjective impressions into objective data, enabling engineers
    to distinguish genuine advances from implementation artifacts. This measurement
    discipline is essential because ML systems involve complex interactions between
    algorithms, hardware, and data that defy intuitive performance prediction. Standardized
    benchmarks establish shared baselines allowing meaningful comparison across research
    groups, enable cumulative progress through reproducible results, and provide empirical
    foundations necessary for engineering decision-making. Understanding benchmarking
    principles enables systematic evaluation driving continuous improvement and establishes
    machine learning systems engineering as rigorous scientific discipline.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 工程学科通过测量和比较来进步，将基准测试确立为机器学习系统开发的基本要素。没有系统性的评估框架，优化主张缺乏科学严谨性，硬件投资缺乏证据，系统改进无法得到验证或复制。基准测试将主观印象转化为客观数据，使工程师能够区分真正的进步和实现上的瑕疵。这种测量学科是必不可少的，因为机器学习系统涉及算法、硬件和数据之间的复杂交互，这些交互难以进行直观的性能预测。标准化的基准测试建立了共享的基线，允许跨研究小组进行有意义的比较，通过可重复的结果实现累积进步，并为工程决策提供必要的经验基础。理解基准测试原则能够推动系统评估，促进持续改进，并将机器学习系统工程确立为严格的科学学科。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Analyze the evolution of ML benchmarking and explain how benchmark gaming lessons
    inform current design
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析机器学习基准测试的演变，并解释基准测试游戏教训如何影响当前设计
- en: Distinguish between the three dimensions of ML benchmarking (algorithmic, systems,
    and data) and evaluate how each dimension contributes to comprehensive system
    assessment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分机器学习基准测试的三个维度（算法、系统和数据），并评估每个维度对全面系统评估的贡献
- en: Compare training and inference benchmarking methodologies, identifying specific
    metrics and evaluation protocols appropriate for each phase of the ML lifecycle
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较训练和推理基准测试方法，确定适用于机器学习生命周期每个阶段的特定指标和评估协议
- en: Apply MLPerf benchmarking standards to evaluate solutions and guide optimization
    decisions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用MLPerf基准测试标准来评估解决方案并指导优化决策
- en: Design statistically rigorous experimental protocols that account for ML system
    variability, including appropriate sample sizes and confidence interval reporting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计统计上严格的实验方案，考虑到机器学习系统的可变性，包括适当的样本量和置信区间报告
- en: Critique existing benchmark results for common fallacies and pitfalls, distinguishing
    between benchmark performance and real-world deployment effectiveness
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批判现有的基准测试结果，以识别常见的谬误和陷阱，区分基准性能和现实世界部署的有效性
- en: Implement production monitoring strategies that extend benchmarking principles
    to operational environments, including A/B testing and continuous model validation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施生产监控策略，将基准测试原则扩展到操作环境，包括A/B测试和持续模型验证
- en: Evaluate performance trade-offs across accuracy, latency, energy, and fairness
    for deployment optimization
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署优化中评估性能权衡，包括准确性、延迟、能量和公平性
- en: Machine Learning Benchmarking Framework
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习基准测试框架
- en: The systematic evaluation of machine learning systems presents a critical methodological
    challenge within the broader discipline of performance engineering. While previous
    chapters have established comprehensive optimization frameworks, particularly
    hardware acceleration strategies ([Chapter 11](ch017.xhtml#sec-ai-acceleration)),
    the validation of these approaches requires rigorous measurement methodologies
    that extend beyond traditional computational benchmarking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习系统进行系统评估是性能工程更广泛学科中的一个关键方法论挑战。虽然前几章已经建立了全面的优化框架，特别是硬件加速策略（[第11章](ch017.xhtml#sec-ai-acceleration)），但这些方法的验证需要严格的测量方法，这些方法超出了传统的计算基准测试。
- en: Consider the challenge facing engineers evaluating competing AI hardware solutions.
    A vendor might demonstrate impressive performance gains on carefully selected
    benchmarks, yet fail to deliver similar improvements in production workloads.
    Without comprehensive evaluation frameworks, distinguishing genuine advances from
    implementation artifacts becomes nearly impossible. This challenge illustrates
    why systematic measurement forms the foundation of engineering progress in machine
    learning systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑工程师在评估竞争性AI硬件解决方案时所面临的挑战。供应商可能在精心选择的基准测试中展示出令人印象深刻的性能提升，但在生产工作负载中却无法实现类似的改进。没有全面的评估框架，区分真正的进步和实现上的伪象几乎是不可能的。这个挑战说明了为什么系统测量是机器学习系统中工程进步的基础。
- en: This chapter examines benchmarking as an essential empirical discipline that
    enables quantitative assessment of machine learning system performance across
    diverse operational contexts. Benchmarking establishes the methodological foundation
    for evidence-based engineering decisions, providing systematic evaluation frameworks
    that allow practitioners to compare competing approaches, validate optimization
    strategies, and ensure reproducible performance claims in both research and production
    environments.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了基准测试作为一种基本的实证学科，它能够对机器学习系统在多种操作环境下的性能进行量化评估。基准测试为基于证据的工程决策提供了方法论基础，提供了系统性的评估框架，使从业者能够比较竞争方法、验证优化策略，并在研究和生产环境中确保可重复的性能声明。
- en: Machine learning benchmarking presents unique challenges that distinguish it
    from conventional systems evaluation. The probabilistic nature of machine learning
    algorithms introduces inherent performance variability that traditional deterministic
    benchmarks cannot adequately characterize. ML system performance exhibits complex
    dependencies on data characteristics, model architectures, and computational resources,
    creating multidimensional evaluation spaces that require specialized measurement
    approaches.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习基准测试提出了独特的挑战，使其与传统系统评估区分开来。机器学习算法的概率性质引入了固有的性能可变性，传统的确定性基准测试无法充分表征。机器学习系统的性能表现出对数据特征、模型架构和计算资源的复杂依赖，从而形成了需要专门测量方法的多维评估空间。
- en: Contemporary machine learning systems demand evaluation frameworks that accommodate
    multiple, often competing, performance objectives. Beyond computational efficiency,
    these systems must be assessed across dimensions including predictive accuracy,
    convergence properties, energy consumption, fairness, and robustness. This multi-objective
    evaluation paradigm necessitates sophisticated benchmarking methodologies that
    can characterize trade-offs and guide system design decisions within specific
    operational constraints.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当代机器学习系统需要能够适应多个、通常是相互竞争的性能目标的评估框架。除了计算效率之外，这些系统必须在预测准确性、收敛性、能耗、公平性和鲁棒性等多个维度上进行评估。这种多目标评估范式需要复杂的基准测试方法，这些方法能够描述权衡并指导在特定操作约束下的系统设计决策。
- en: 'The field has evolved to address these challenges through comprehensive evaluation
    approaches that operate across three core dimensions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域已经通过在三个核心维度上运行的全面评估方法来应对这些挑战：
- en: '***Machine Learning Benchmarking*** is the systematic evaluation of ML systems
    across three dimensions: *computational performance*, *algorithmic accuracy*,
    and *data quality*, enabling objective comparison and reproducible assessment
    of system capabilities.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***机器学习基准测试*** 是对机器学习系统在三个维度上进行系统评估：*计算性能*、*算法准确性*和*数据质量*，从而实现系统能力的客观比较和可重复评估。'
- en: This chapter provides a systematic examination of machine learning benchmarking
    methodologies, beginning with the historical evolution of computational evaluation
    frameworks and their adaptation to address the unique requirements of probabilistic
    systems. We analyze standardized evaluation frameworks such as MLPerf that establish
    comparative baselines across diverse hardware architectures and implementation
    strategies. The discussion subsequently examines the essential distinctions between
    training and inference evaluation, exploring the specialized metrics and methodologies
    required to characterize their distinct computational profiles and operational
    requirements.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章系统地考察了机器学习基准测试方法，从计算评估框架的历史演变及其适应概率系统独特需求开始。我们分析了标准化评估框架，如MLPerf，它在不同的硬件架构和实现策略之间建立了比较基准。随后的讨论考察了训练评估和推理评估之间的基本区别，探讨了表征它们独特的计算配置文件和操作需求所需的专用指标和方法。
- en: The analysis extends to specialized evaluation contexts, including resource-constrained
    mobile and edge deployment scenarios that present unique measurement challenges.
    We conclude by investigating production monitoring methodologies that extend benchmarking
    principles beyond controlled experimental environments into dynamic operational
    contexts. This comprehensive treatment demonstrates how rigorous measurement validates
    the performance improvements achieved through the optimization techniques and
    hardware acceleration strategies examined in preceding chapters, while establishing
    the empirical foundation essential for the deployment strategies explored in Part
    IV.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分析扩展到专门的评估环境，包括资源受限的移动和边缘部署场景，这些场景提出了独特的测量挑战。我们通过调查生产监控方法来结束讨论，这些方法将基准测试原则扩展到受控实验环境之外的动态操作环境。这种全面的处理展示了严格的测量如何验证通过前几章中检查的优化技术和硬件加速策略实现的性能改进，同时为第IV部分中探讨的部署策略建立了经验基础。
- en: Historical Context
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史背景
- en: The evolution from simple performance metrics to comprehensive ML benchmarking
    reveals three critical methodological shifts, each addressing failures of previous
    evaluation paradigms that directly inform our current approach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从简单的性能指标到全面的机器学习基准测试的演变，揭示了三个关键的方法论转变，每个转变都针对了先前评估范例的失败，这些失败直接影响了我们当前的方法。
- en: Performance Benchmarks
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能基准
- en: The evolution from synthetic operations to representative workloads emerged
    when early benchmark gaming undermined evaluation validity. Mainframe benchmarks
    like Whetstone (1964) and LINPACK (1979) measured isolated operations, enabling
    vendors to optimize for narrow tests rather than practical performance. SPEC CPU
    (1989) pioneered using real application workloads to ensure evaluation reflects
    actual deployment scenarios. This lesson directly shapes ML benchmarking, as optimization
    claims from [Chapter 10](ch016.xhtml#sec-model-optimizations) require validation
    on representative tasks. MLPerf’s inclusion of real models like ResNet-50 and
    BERT ensures benchmarks capture deployment complexity rather than idealized test
    cases.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从合成操作到代表性工作负载的演变出现在早期基准测试游戏破坏评估有效性的情况下。像Whetstone（1964）和LINPACK（1979）这样的主机基准测试测量了孤立的操作，使供应商能够针对狭窄的测试进行优化，而不是实际性能。SPEC
    CPU（1989）开创了使用真实应用程序工作负载的先河，以确保评估反映了实际的部署场景。这一经验直接塑造了机器学习基准测试，因为[第10章](ch016.xhtml#sec-model-optimizations)中的优化声明需要在代表性任务上进行验证。MLPerf包括ResNet-50和Bert等真实模型确保基准测试能够捕捉部署复杂性，而不是理想化的测试案例。
- en: As deployment contexts diversified, benchmarks evolved from single-dimension
    to multi-objective evaluation. Graphics benchmarks measured quality alongside
    speed; mobile benchmarks evaluated battery life with performance. The multi-objective
    challenges from [Chapter 9](ch015.xhtml#sec-efficient-ai), balancing accuracy,
    latency, and energy, manifest directly in modern ML evaluation where no single
    metric captures deployment viability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着部署环境多样化，基准测试从单维度演变为多目标评估。图形基准测试在速度的同时衡量质量；移动基准测试在性能的同时评估电池寿命。[第9章](ch015.xhtml#sec-efficient-ai)中的多目标挑战，平衡精度、延迟和能耗，直接体现在现代机器学习评估中，其中没有单一指标能够捕捉部署的可行性。
- en: The shift from isolated components to integrated systems occurred when distributed
    computing revealed that component optimization fails to predict system performance.
    ML training depends not just on accelerator compute ([Chapter 11](ch017.xhtml#sec-ai-acceleration))
    but on data pipelines, gradient synchronization, and storage throughput. MLPerf
    evaluates complete workflows, recognizing that performance emerges from component
    interactions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当分布式计算揭示组件优化无法预测系统性能时，从独立组件到集成系统的转变发生了。机器学习训练不仅依赖于加速器计算([第11章](ch017.xhtml#sec-ai-acceleration))，还依赖于数据管道、梯度同步和存储吞吐量。MLPerf评估完整的工作流程，认识到性能来自于组件的交互。
- en: These lessons culminate in MLPerf (2018), which synthesizes representative workloads,
    multi-objective evaluation, and integrated measurement while addressing ML-specific
    challenges ([Ranganathan and Hölzle 2024](ch058.xhtml#ref-ranganathan2024twenty)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经验教训在MLPerf (2018)中达到高潮，它综合了代表性工作负载、多目标评估和集成测量，同时解决机器学习特有的挑战([Ranganathan
    and Hölzle 2024](ch058.xhtml#ref-ranganathan2024twenty))。
- en: Energy Benchmarks
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 能源基准
- en: The multi-objective evaluation paradigm naturally extended to energy efficiency
    as computing diversified beyond mainframes with unlimited power budgets. Mobile
    devices demanded battery life optimization, while warehouse-scale systems faced
    energy costs rivaling hardware expenses. This shift established energy as a first-class
    metric alongside performance, spawning benchmarks like SPEC Power[1](#fn1) for
    servers, Green500[2](#fn2) for supercomputers, and ENERGY STAR[3](#fn3) for consumer
    systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算超越了具有无限电力预算的主机，多目标评估范例自然扩展到能源效率。移动设备需要电池寿命优化，而仓库规模系统面临与硬件费用相媲美的能源成本。这种转变将能源确立为与性能并列的一级指标，催生了为服务器设计的SPEC
    Power[1](#fn1)、为超级计算机设计的Green500[2](#fn2)以及为消费系统设计的ENERGY STAR[3](#fn3)基准。
- en: Despite these advances, power benchmarking faces ongoing challenges in accounting
    for diverse workload patterns and system configurations across computing environments.
    Recent advancements, such as the [MLPerf Power](https://mlcommons.org/) benchmark,
    have introduced specialized methodologies for measuring the energy impact of machine
    learning workloads, directly addressing the growing importance of energy efficiency
    in AI-driven computing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进步，但在计算环境中考虑多样化的工作负载模式和系统配置时，电力基准测试仍然面临持续的挑战。最近的发展，例如[MLPerf Power](https://mlcommons.org/)基准，引入了专门的方法来衡量机器学习工作负载的能源影响，直接应对了在人工智能驱动计算中能源效率日益增长的重要性。
- en: 'Energy benchmarking extends beyond hardware energy measurement alone. Algorithmic
    energy optimization represents an equally critical dimension of modern AI benchmarking,
    where energy-efficient algorithms achieve performance improvements through computational
    reduction rather than purely hardware enhancement. Neural network pruning reduces
    energy consumption by eliminating unnecessary computations: pruned BERT models
    can achieve 90% of original task accuracy with 10x fewer parameters, delivering
    4-8x inference speedup and 8-12x energy reduction depending on pruning method
    and hardware ([Han, Mao, and Dally 2015b](ch058.xhtml#ref-han2016deep)). Quantization
    techniques achieve similar gains by reducing precision requirements: INT8 quantization
    typically provides 4x inference speedup with 4x energy reduction while maintaining
    99%+ accuracy preservation ([Jacob et al. 2018b](ch058.xhtml#ref-jacob2018quantization)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 能源基准测试不仅超越了硬件能源测量。算法能源优化代表了现代人工智能基准测试的一个同样关键的维度，其中能源高效的算法通过计算减少而不是纯粹硬件增强来实现性能提升。神经网络剪枝通过消除不必要的计算来减少能源消耗：剪枝后的BERT模型可以在10倍更少的参数下达到原始任务90%的准确率，提供4-8倍的推理速度提升和8-12倍的能源减少（取决于剪枝方法和硬件[Han,
    Mao, and Dally 2015b](ch058.xhtml#ref-han2016deep)）。量化技术通过减少精度要求实现类似收益：INT8量化通常提供4倍的推理速度提升和4倍的能源减少，同时保持99%以上的准确率保留([Jacob
    et al. 2018b](ch058.xhtml#ref-jacob2018quantization))。
- en: Knowledge distillation offers another algorithmic energy optimization pathway,
    where smaller “student” models learn from larger “teacher” models. MobileNet architectures
    demonstrate this principle, achieving 10x energy reduction versus ResNet while
    maintaining similar accuracy through depthwise separable convolutions and width
    multipliers ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets)).
    Model compression techniques collectively enable deployment of sophisticated AI
    capabilities within severe energy constraints, making techniques essential for
    mobile and edge computing scenarios.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏提供了另一种算法能量优化途径，其中较小的“学生”模型从较大的“教师”模型中学习。MobileNet架构展示了这一原则，在保持类似精度的同时，通过深度可分离卷积和宽度乘数实现了比ResNet高10倍的能量降低（[A.
    G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets)）。模型压缩技术共同使得在严格的能量约束下部署复杂的AI能力成为可能，这对于移动和边缘计算场景是必不可少的。
- en: Energy-aware benchmarking must evaluate not just hardware power consumption,
    but also algorithmic efficiency metrics including FLOP reduction through sparsity,
    memory access reduction through compression, and computational energy benefits
    from quantization. These algorithmic optimizations often achieve greater energy
    savings than hardware improvements alone, providing a critical dimension for energy
    benchmarking frameworks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 能源感知基准必须评估的不仅是硬件功耗，还包括算法效率指标，包括通过稀疏性减少FLOP、通过压缩减少内存访问以及量化带来的计算能量效益。这些算法优化通常比单纯的硬件改进实现更大的节能效果，为能源基准框架提供了关键维度。
- en: As artificial intelligence and edge computing evolve, power benchmarking will
    drive energy-efficient hardware and software innovations. This connects directly
    to sustainable AI practices discussed in [Chapter 18](ch024.xhtml#sec-sustainable-ai),
    where energy-aware design principles guide environmentally responsible AI development.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能和边缘计算的演进，功耗基准将推动节能硬件和软件的创新。这直接关联到第18章（ch024.xhtml#sec-sustainable-ai）中讨论的可持续人工智能实践，其中能源感知设计原则指导了负责任的人工智能开发。
- en: Domain-Specific Benchmarks
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域特定基准
- en: 'Computing diversification necessitated specialized benchmarks tailored to domain-specific
    requirements that generic metrics cannot capture. Domain-specific benchmarks address
    three categories of specialization:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 计算多样化需要针对特定领域需求定制基准，这些基准能够捕捉到通用指标无法涵盖的特定要求。领域特定基准解决以下三个类别的专业化问题：
- en: Deployment constraints shape core metric priorities. Datacenter workloads optimize
    for throughput with kilowatt-scale power budgets, while mobile AI operates within
    2-5W thermal envelopes, and IoT devices require milliwatt-scale operation. These
    constraints, rooted in efficiency principles from [Chapter 9](ch015.xhtml#sec-efficient-ai),
    determine whether benchmarks prioritize total throughput or energy per operation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 部署约束塑造核心指标优先级。数据中心工作负载在千瓦级功率预算下优化吞吐量，而移动AI在2-5W的热量范围内运行，物联网设备需要毫瓦级的操作。这些约束源于第9章（ch015.xhtml#sec-efficient-ai）中的效率原则，决定了基准是优先考虑总吞吐量还是每操作的能量。
- en: 'Application requirements impose functional and regulatory constraints beyond
    performance. Healthcare AI demands interpretability metrics alongside accuracy;
    financial systems require microsecond latency with audit compliance; autonomous
    vehicles need safety-critical reliability (ASIL-D: <10^-8 failure/hour). These
    requirements, connecting to responsible AI principles in [Chapter 17](ch023.xhtml#sec-responsible-ai),
    extend evaluation beyond traditional performance metrics.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 应用需求在性能之外还强加了功能和监管约束。医疗AI需要可解释性指标与准确性；金融系统需要微秒级延迟并符合审计合规性；自动驾驶汽车需要关键安全性的可靠性（ASIL-D：<10^-8故障/小时）。这些要求与第17章（ch023.xhtml#sec-responsible-ai）中讨论的责任人工智能原则相联系，将评估扩展到传统的性能指标之外。
- en: Operational conditions determine real-world viability. Autonomous vehicles face
    -40°C to +85°C temperatures and degraded sensor inputs; datacenters handle millions
    of concurrent requests with network partitions; industrial IoT endures years-long
    deployment without maintenance. Hardware capabilities from [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    only deliver value when validated under these conditions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 运行条件决定了现实世界的可行性。自动驾驶汽车面临-40°C至+85°C的温度和退化的传感器输入；数据中心处理数百万并发请求并经历网络分区；工业物联网设备在没有维护的情况下持续部署数年。第11章（ch017.xhtml#sec-ai-acceleration）中提到的硬件能力只有在这些条件下得到验证时才能提供价值。
- en: 'Machine learning presents a prominent example of this transition toward domain-specific
    evaluation. Traditional CPU and GPU benchmarks prove insufficient for assessing
    ML workloads, which involve complex interactions between computation, memory bandwidth,
    and data movement patterns. MLPerf has standardized performance measurement for
    machine learning models across these three categories: MLPerf Training addresses
    datacenter deployment constraints with multi-node scaling benchmarks, MLPerf Inference
    evaluates latency-critical application requirements across server to edge deployments,
    and MLPerf Tiny assesses ultra-constrained operational conditions for microcontroller
    deployments. This tiered structure reflects the systematic application of our
    three-category framework to ML-specific evaluation needs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是向特定领域评估转变的一个突出例子。传统的CPU和GPU基准测试不足以评估涉及计算、内存带宽和数据移动模式之间复杂交互的机器学习工作负载。MLPerf为机器学习模型在这三个类别中实现了性能测量的标准化：MLPerf
    Training通过多节点扩展基准测试解决数据中心部署限制，MLPerf Inference评估服务器到边缘部署中的延迟关键应用需求，而MLPerf Tiny评估微控制器部署的超约束操作条件。这种分层结构反映了我们的三类框架在机器学习特定评估需求上的系统应用。
- en: The strength of domain-specific benchmarks lies in their ability to capture
    these specialized requirements that general benchmarks overlook. By systematically
    addressing deployment constraints, application requirements, and operational conditions,
    these benchmarks provide insights that drive targeted optimizations in both hardware
    and software while ensuring that improvements translate to real-world deployment
    success rather than merely optimizing for narrow laboratory conditions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域基准测试的强大之处在于它们能够捕捉到一般基准测试忽视的这些专业需求。通过系统地解决部署限制、应用需求和操作条件，这些基准测试提供了推动硬件和软件针对性优化的见解，同时确保改进能够转化为实际部署的成功，而不仅仅是优化狭窄的实验室条件。
- en: This historical progression from general computing benchmarks through energy-aware
    measurement to domain-specific evaluation frameworks provides the foundation for
    understanding contemporary ML benchmarking challenges. The lessons learned (representative
    workloads over synthetic tests, multi-objective over single metrics, and integrated
    systems over isolated components) directly shape how we approach AI system evaluation
    today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从通用计算基准测试到能源感知测量再到特定领域评估框架的历史进步，为理解当代机器学习基准测试挑战提供了基础。学到的经验（代表性工作负载超过合成测试、多目标超过单一指标，以及集成系统超过独立组件）直接塑造了我们今天如何接近人工智能系统评估。
- en: Machine Learning Benchmarks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习基准
- en: The historical evolution culminates in machine learning benchmarking, where
    the complexity exceeds all previous computing domains. Unlike traditional workloads
    with deterministic behavior, ML systems introduce inherent uncertainty through
    their probabilistic nature. A CPU benchmark produces identical results given the
    same inputs; an ML model’s performance varies with training data, initialization,
    and even the order of operations. This inherent variability, combined with the
    lessons from decades of benchmark evolution, necessitates our three-dimensional
    evaluation framework.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 历史演变的最终结果是机器学习基准测试，其复杂性超过了所有之前的计算领域。与具有确定性行为的传统工作负载不同，ML系统通过其概率性质引入了固有的不确定性。CPU基准测试在相同的输入下产生相同的结果；ML模型的性能会随着训练数据、初始化甚至操作顺序而变化。这种固有的可变性，加上几十年来基准测试的经验教训，需要我们三维评估框架。
- en: Building on the framework and optimization techniques from previous chapters,
    ML benchmarks must evaluate not just computational efficiency but the intricate
    interplay between algorithms, hardware, and data. The evolution of benchmarks
    reaches its current apex in machine learning, where our established three-dimensional
    framework reflects decades of computing measurement evolution. Early machine learning
    benchmarks focused primarily on algorithmic performance, measuring how well models
    could perform specific tasks ([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient)).
    However, as machine learning applications scaled dramatically and computational
    demands grew exponentially, the focus naturally expanded to include system performance
    and hardware efficiency ([Norman P. Jouppi et al. 2017d](ch058.xhtml#ref-jouppi2017datacenter)).
    Recently, the role of data quality has emerged as the third dimension of evaluation
    ([Gebru et al. 2021b](ch058.xhtml#ref-gebru2021datasheets)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在前面章节的框架和优化技术之上，机器学习基准测试必须评估的不仅仅是计算效率，还包括算法、硬件和数据之间复杂的相互作用。基准测试的演变在机器学习领域达到了目前的顶峰，我们建立的三维框架反映了数十年的计算测量进化。早期的机器学习基准测试主要关注算法性能，衡量模型执行特定任务的能力（[Lecun
    等人 1998](ch058.xhtml#ref-lecun1998gradient)）。然而，随着机器学习应用的规模急剧扩大和计算需求呈指数增长，关注的焦点自然扩展到包括系统性能和硬件效率（[Norman
    P. Jouppi 等人 2017d](ch058.xhtml#ref-jouppi2017datacenter)）。最近，数据质量已成为评估的第三维度（[Gebru
    等人 2021b](ch058.xhtml#ref-gebru2021datasheets)）。
- en: 'AI benchmarks differ from traditional performance metrics through their inherent
    variability, which introduces accuracy as a new evaluation dimension alongside
    deterministic characteristics like computational speed or energy consumption.
    The probabilistic nature of machine learning models means the same system can
    produce different results depending on the data it encounters, making accuracy
    a defining factor in performance assessment. This distinction adds complexity:
    benchmarking AI systems requires measuring not only raw computational efficiency
    but also understanding trade-offs between accuracy, generalization, and resource
    constraints.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能基准测试与传统性能指标的不同之处在于其固有的可变性，这引入了准确性作为新的评估维度，与计算速度或能耗等确定性特征并列。机器学习模型的概率性质意味着相同的系统可能会根据遇到的数据产生不同的结果，这使得准确性成为性能评估的决定性因素。这种区别增加了复杂性：对人工智能系统进行基准测试不仅需要测量原始的计算效率，还需要理解准确性、泛化能力和资源限制之间的权衡。
- en: 'Energy efficiency emerges as a cross-cutting concern that influences all three
    dimensions of our framework: algorithmic choices affect computational complexity
    and power requirements, hardware capabilities determine energy-performance trade-offs,
    and dataset characteristics influence training energy costs. This multifaceted
    evaluation approach represents a departure from earlier benchmarks that focused
    on isolated aspects like computational speed or energy efficiency ([Hernandez
    and Brown 2020](ch058.xhtml#ref-hernandez2020measuring)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 能源效率成为一个跨领域的关注点，影响着我们框架的三个维度：算法选择影响计算复杂度和功耗，硬件能力决定能效权衡，数据集特征影响训练能耗。这种多方面的评估方法代表了与早期基准测试的背离，后者主要关注孤立方面，如计算速度或能源效率（[Hernandez
    和 Brown 2020](ch058.xhtml#ref-hernandez2020measuring)）。
- en: This evolution in benchmark complexity directly mirrors the field’s evolving
    understanding of what truly drives machine learning system success. While algorithmic
    innovations initially dominated progress metrics throughout the research phase,
    the practical challenges of deploying models at scale revealed the critical importance
    of hardware efficiency ([Norman P. Jouppi et al. 2021b](ch058.xhtml#ref-jouppi2021ten)).
    Subsequently, high-profile failures of machine learning systems in real-world
    deployments highlighted how data quality and representation directly determine
    system reliability and fairness ([Bender et al. 2021](ch058.xhtml#ref-bender2021stochastic)).
    Understanding how these dimensions interact has become necessary for accurately
    assessing machine learning system performance, informing development decisions,
    and measuring technological progress in the field.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基准复杂性的这种演变直接反映了该领域对真正推动机器学习系统成功因素的理解的演变。虽然算法创新最初主导了研究阶段的进展指标，但在大规模部署模型时遇到的实际挑战揭示了硬件效率的重要性([Norman
    P. Jouppi等人 2021b](ch058.xhtml#ref-jouppi2021ten))。随后，机器学习系统在实际部署中的高调失败凸显了数据质量和表示如何直接决定系统可靠性和公平性([Bender等人
    2021](ch058.xhtml#ref-bender2021stochastic))。理解这些维度如何相互作用已成为准确评估机器学习系统性能、指导开发决策以及衡量该领域技术进步的必要条件。
- en: ML Measurement Challenges
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习测量挑战
- en: 'The unique characteristics of ML systems create measurement challenges that
    traditional benchmarks never faced. Unlike deterministic algorithms that produce
    identical outputs given the same inputs, ML systems exhibit inherent variability
    from multiple sources: algorithmic randomness from weight initialization and data
    shuffling, hardware thermal states affecting clock speeds, system load variations
    from concurrent processes, and environmental factors including network conditions
    and power management. This variability requires rigorous statistical methodology
    to distinguish genuine performance improvements from measurement noise.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的独特特性创造了传统基准测试从未面临的测量挑战。与给定相同输入产生相同输出的确定性算法不同，机器学习系统表现出来自多个来源的固有可变性：来自权重初始化和数据洗牌的算法随机性，影响时钟速度的硬件热状态，来自并发进程的系统负载变化，以及包括网络条件和电源管理在内的环境因素。这种可变性需要严格的统计方法来区分真正的性能改进和测量噪声。
- en: To address this variability, effective benchmark protocols require multiple
    experimental runs with different random seeds. Running each benchmark 5-10 times
    and reporting statistical measures beyond simple means (including standard deviations
    or 95% confidence intervals) quantifies result stability and allows practitioners
    to distinguish genuine performance improvements from measurement noise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这种可变性，有效的基准测试协议需要使用不同的随机种子进行多次实验运行。每次运行基准测试5-10次，并报告除简单平均值之外的统计指标（包括标准偏差或95%置信区间），可以量化结果稳定性，并允许从业者区分真正的性能改进和测量噪声。
- en: Recent studies have highlighted how inadequate statistical rigor can lead to
    misleading conclusions. Many reinforcement learning papers report improvements
    that fall within statistical noise ([Henderson et al. 2018](ch058.xhtml#ref-henderson2018deep)),
    while GAN comparisons often lack proper experimental protocols, leading to inconsistent
    rankings across different random seeds ([Lucic et al. 2018](ch058.xhtml#ref-lucic2018gans)).
    These findings underscore the importance of establishing comprehensive measurement
    protocols that account for ML’s probabilistic nature.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 近期研究突出了统计严谨性不足可能导致误导性结论。许多强化学习论文报告的改进结果落在统计噪声范围内([Henderson等人 2018](ch058.xhtml#ref-henderson2018deep))，而生成对抗网络（GAN）的比较通常缺乏适当的实验协议，导致在不同随机种子下排名不一致([Lucic等人
    2018](ch058.xhtml#ref-lucic2018gans))。这些发现强调了建立全面测量协议的重要性，该协议考虑了机器学习的概率性质。
- en: 'Representative workload selection critically determines benchmark validity.
    Synthetic microbenchmarks often fail to capture the complexity of real ML workloads
    where data movement, memory allocation, and dynamic batching create performance
    patterns not visible in simplified tests. Comprehensive benchmarking requires
    workloads that reflect actual deployment patterns: variable sequence lengths in
    language models, mixed precision training regimes, and realistic data loading
    patterns that include preprocessing overhead. The distinction between statistical
    significance and practical significance requires careful interpretation. A small
    performance improvement might achieve statistical significance across hundreds
    of trials but prove operationally irrelevant if it falls within measurement noise
    or costs exceed benefits.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性工作负载选择对于基准测试的有效性至关重要。合成微基准测试通常无法捕捉到真实机器学习工作负载的复杂性，其中数据移动、内存分配和动态批处理创造了在简化测试中不可见的性能模式。全面的基准测试需要反映实际部署模式的工作负载：语言模型中的可变序列长度、混合精度训练方案以及包括预处理开销在内的真实数据加载模式。统计意义和实际意义的区别需要仔细解释。小的性能提升可能在数百次试验中达到统计意义，但如果它落在测量噪声中或成本超过收益，则可能在实际操作上无关紧要。
- en: Addressing this requires careful benchmark design that prioritizes representative
    workloads over synthetic tests. Effective system evaluation relies on end-to-end
    application benchmarks like MLPerf that incorporate data preprocessing and reflect
    realistic deployment patterns. When developing custom evaluation frameworks, profiling
    production workloads helps identify the representative data distributions, batch
    sizes, and computational patterns essential for meaningful assessment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题需要仔细的基准设计，优先考虑代表性工作负载而不是合成测试。有效的系统评估依赖于端到端应用程序基准，如MLPerf，它包含数据预处理并反映现实部署模式。在开发自定义评估框架时，分析生产工作负载有助于确定对有意义评估至关重要的代表性数据分布、批处理大小和计算模式。
- en: Current benchmarking paradigms often fall short by measuring narrow task performance
    while missing characteristics that determine real-world system effectiveness.
    Most existing benchmarks evaluate supervised learning performance on static datasets,
    primarily testing pattern recognition capabilities rather than the adaptability
    and resilience required for production deployment. This limitation becomes apparent
    when models achieve excellent benchmark performance yet fail when deployed in
    slightly different conditions or domains. To address these shortcomings, comprehensive
    system evaluation must measure learning efficiency, continual learning capability,
    and out-of-distribution generalization alongside traditional metrics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的基准测试范式通常在衡量狭窄的任务性能时存在不足，同时忽略了决定实际系统有效性的特征。大多数现有的基准测试主要评估静态数据集上的监督学习性能，主要测试模式识别能力，而不是生产部署所需的适应性和弹性。当模型在略微不同的条件或领域部署时，即使基准测试性能出色但失败，这种局限性变得明显。为了解决这些不足，全面的系统评估必须测量学习效率、持续学习能力以及分布外泛化能力，同时衡量传统指标。
- en: Algorithmic Benchmarks
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法基准测试
- en: 'Algorithmic benchmarks focus specifically on the first dimension of our framework:
    measuring model performance, accuracy, and efficiency. While hardware systems
    and training data quality certainly influence results, algorithmic benchmarks
    deliberately isolate model capabilities to enable clear understanding of the trade-offs
    between accuracy, computational complexity, and generalization.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法基准测试专注于我们框架的第一个维度：衡量模型性能、准确性和效率。虽然硬件系统和训练数据质量当然会影响结果，但算法基准测试故意隔离模型能力，以便清楚地理解准确度、计算复杂性和泛化之间的权衡。
- en: AI algorithms face the complex challenge of balancing multiple performance objectives
    simultaneously, including accuracy, speed, resource efficiency, and generalization
    capability. As machine learning applications continue to span diverse domains,
    including computer vision, natural language processing, speech recognition, and
    reinforcement learning, evaluating these competing objectives requires carefully
    standardized methodologies tailored to each domain’s unique challenges. Algorithmic
    benchmarks, such as ImageNet[4](#fn4) ([J. Deng et al. 2009](ch058.xhtml#ref-deng2009imagenet)),
    establish these evaluation frameworks, providing a consistent basis for comparing
    different machine learning approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能算法面临着平衡多个性能目标的复杂挑战，包括准确性、速度、资源效率和泛化能力。随着机器学习应用继续跨越不同的领域，包括计算机视觉、自然语言处理、语音识别和强化学习，评估这些竞争目标需要针对每个领域独特挑战精心标准化的方法。算法基准，如ImageNet[4](#fn4)
    ([J. Deng等人2009](ch058.xhtml#ref-deng2009imagenet))，建立了这些评估框架，为比较不同的机器学习方法提供了一个一致的基础。
- en: '***ML Algorithmic Benchmarks*** are standardized evaluations of machine learning
    *model performance* on *predefined tasks* and *datasets*, enabling objective comparison
    of *accuracy*, *efficiency*, and *generalization* across different approaches.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习算法基准**是对机器学习模型在**预定义任务**和**数据集**上的**性能**进行标准化的评估，使得不同方法之间的**准确性**、**效率**和**泛化能力**的客观比较成为可能。'
- en: Algorithmic benchmarks advance AI through several functions. They establish
    clear performance baselines, enabling objective comparisons between competing
    approaches. By systematically evaluating trade-offs between model complexity,
    computational requirements, and task performance, they help researchers and practitioners
    identify optimal design choices. They track technological progress by documenting
    improvements over time, guiding the development of new techniques while exposing
    limitations in existing methodologies.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 算法基准通过多个功能推动人工智能的发展。它们建立了清晰的性能基准线，使得不同方法之间的客观比较成为可能。通过系统地评估模型复杂度、计算需求和任务性能之间的权衡，它们帮助研究人员和从业者确定最佳的设计选择。它们通过记录随时间的技术进步，指导新技术的开发，同时揭示现有方法中的局限性。
- en: The graph in [Figure 12.1](ch018.xhtml#fig-imagenet-challenge) illustrates the
    reduction in error rates on the [ImageNet Large Scale Visual Recognition Challenge
    (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) classification task over
    the years. Starting from the baseline models in 2010 and 2011, the introduction
    of AlexNet[5](#fn5) in 2012 marked an improvement, reducing the error rate from
    25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[6](#fn6)
    continued this trend, with ResNet achieving an error rate of 3.57% by 2015 ([Russakovsky
    et al. 2015](ch058.xhtml#ref-russakovsky2015imagenet)). This progression highlights
    how algorithmic benchmarks measure current capabilities and drive advancements
    in AI performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.1](ch018.xhtml#fig-imagenet-challenge)中的图表展示了在[ImageNet大规模视觉识别挑战（ILSVRC）](https://www.image-net.org/challenges/LSVRC/)分类任务上，错误率随年份的降低。从2010年和2011年的基线模型开始，2012年引入的AlexNet[5](#fn5)标志着进步，将错误率从25.8%降低到16.4%。随后的模型如ZFNet、VGGNet、GoogleNet和ResNet[6](#fn6)继续这一趋势，到2015年ResNet实现了3.57%的错误率（[Russakovsky等人2015](ch058.xhtml#ref-russakovsky2015imagenet)）。这一进展突出了算法基准如何衡量当前的能力并推动人工智能性能的进步。'
- en: '![](../media/file193.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file193.png)'
- en: 'Figure 12.1: **ImageNet Challenge Progression**: Neural networks have reduced
    error rates from 25.8% in 2010 to 3.57% by 2015, highlighting the impact of architectural
    advancements on classification accuracy.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：**ImageNet挑战进展**：神经网络将错误率从2010年的25.8%降低到2015年的3.57%，突出了架构进步对分类准确性的影响。
- en: System Benchmarks
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统基准
- en: 'Moving to the second dimension of our framework, we address hardware performance:
    how efficiently different computational systems execute machine learning workloads.
    System benchmarks measure the computational foundation that enables algorithmic
    capabilities, systematically examining how hardware architectures, memory systems,
    and interconnects affect overall performance. Understanding these hardware limitations
    and capabilities proves necessary for optimizing the algorithm-system interaction.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 转到我们框架的第二维度，我们解决硬件性能问题：不同的计算系统如何有效地执行机器学习工作负载。系统基准测量使算法能力得以实现的计算基础，系统地检查硬件架构、内存系统和互连如何影响整体性能。理解这些硬件的局限性和能力对于优化算法-系统交互是必要的。
- en: AI computations place significant demands on computational resources, far exceeding
    traditional computing workloads. The underlying hardware infrastructure, encompassing
    general-purpose CPUs, graphics processing units (GPUs), tensor processing units
    (TPUs)[7](#fn7), and application-specific integrated circuits (ASICs)[8](#fn8),
    determines the speed, efficiency, and scalability of AI solutions. System benchmarks
    establish standardized methodologies for evaluating hardware performance across
    AI workloads, measuring metrics including computational throughput, memory bandwidth,
    power efficiency, and scaling characteristics ([Reddi et al. 2019b](ch058.xhtml#ref-reddi2020mlperf);
    [Mattson et al. 2020](ch058.xhtml#ref-mattson2020mlperf)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能计算对计算资源的需求极大，远超传统计算工作负载。其底层硬件基础设施，包括通用CPU、图形处理单元（GPU）、张量处理单元（TPUs）[7](#fn7)
    和专用集成电路（ASICs）[8](#fn8]，决定了人工智能解决方案的速度、效率和可扩展性。系统基准测试建立了评估人工智能工作负载下硬件性能的标准方法，包括计算吞吐量、内存带宽、能效和可扩展性等指标
    ([Reddi等人 2019b](ch058.xhtml#ref-reddi2020mlperf)；[Mattson等人 2020](ch058.xhtml#ref-mattson2020mlperf))。
- en: These system benchmarks perform two critical functions in the AI ecosystem.
    First, they enable developers and organizations to make informed decisions when
    selecting hardware platforms for their AI applications by providing comparative
    performance data across system configurations. Evaluation factors include training
    speed, inference latency, energy efficiency, and cost-effectiveness. Second, hardware
    manufacturers rely on these benchmarks to quantify generational improvements and
    guide the development of specialized AI accelerators, driving advancement in computational
    capabilities.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统基准在人工智能生态系统中执行两个关键功能。首先，它们通过提供系统配置之间的比较性能数据，使开发者和组织在为他们的AI应用选择硬件平台时能够做出明智的决定。评估因素包括训练速度、推理延迟、能效和成本效益。其次，硬件制造商依赖这些基准来量化代际改进并指导专用AI加速器的开发，推动计算能力的进步。
- en: '***ML System Benchmarks*** are standardized evaluations of *computational infrastructure*
    for ML workloads, measuring *performance*, *energy efficiency*, and *scalability*
    to enable objective comparison across hardware and software configurations.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**ML系统基准**是对机器学习工作负载的计算基础设施的标准化评估，衡量**性能**、**能效**和**可扩展性**，以实现硬件和软件配置之间的客观比较。'
- en: However, effective benchmark interpretation requires deep understanding of the
    performance characteristics inherent to target hardware. Critically, understanding
    whether specific AI workloads are compute-bound or memory-bound provides essential
    insight for optimization decisions. Computational intensity, measured as FLOPS[9](#fn9)
    per byte of data movement, determines performance limits. Consider an NVIDIA A100
    GPU with 312 TFLOPS of tensor performance and 1.6 TB/s memory bandwidth, yielding
    an arithmetic intensity threshold of 195 FLOPS/byte. The architectural foundations
    for understanding these hardware characteristics are established in [Chapter 11](ch017.xhtml#sec-ai-acceleration),
    which provides context for interpreting system benchmark results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有效解释基准测试结果需要深入理解目标硬件的性能特性。关键的是，了解特定人工智能工作负载是计算密集型还是内存密集型，对于优化决策提供了重要见解。计算强度，以每字节数据移动的FLOPS[9](#fn9)来衡量，决定了性能限制。以NVIDIA
    A100 GPU为例，其张量性能为312 TFLOPS，内存带宽为1.6 TB/s，从而得出算术强度阈值为195 FLOPS/byte。理解这些硬件特性的架构基础在[第11章](ch017.xhtml#sec-ai-acceleration)中确立，它为解释系统基准测试结果提供了背景。
- en: High-intensity operations like dense matrix multiplication in certain AI model
    operations (typically >200 FLOPS/byte) achieve near-peak computational throughput
    on the A100\. For example, a ResNet-50 forward pass on large batch sizes (256+)
    achieves arithmetic intensity of ~300 FLOPS/byte, enabling 85-90% of peak tensor
    performance (approximately 280 TFLOPS achieved vs 312 TFLOPS theoretical) ([Choquette
    et al. 2021](ch058.xhtml#ref-nvidia2020a100)). Conversely, low-intensity operations
    like activation functions and certain lightweight operations (<10 FLOPS/byte)
    become memory bandwidth limited, utilizing only a fraction of the GPU’s computational
    capacity. A BERT inference with batch size 1 achieves only 8 FLOPS/byte arithmetic
    intensity, limiting performance to 12.8 TFLOPS (1.6 TB/s × 8 FLOPS/byte), representing
    just 4% of peak computational capability.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些AI模型操作（通常>200 FLOPS/byte）中的高强度操作，如密集矩阵乘法，在A100上实现了接近峰值计算吞吐量。例如，在大型批量大小（256+）上进行的ResNet-50正向传递达到约300
    FLOPS/byte的算术强度，使得85-90%的峰值张量性能（实际达到280 TFLOPS，理论值为312 TFLOPS）([Choquette等人2021](ch058.xhtml#ref-nvidia2020a100))。相反，低强度操作，如激活函数和某些轻量级操作（<10
    FLOPS/byte），成为内存带宽限制，只利用了GPU计算能力的一小部分。批量大小为1的BERT推理仅达到8 FLOPS/byte的算术强度，性能限制在12.8
    TFLOPS（1.6 TB/s × 8 FLOPS/byte），仅代表峰值计算能力的4%。
- en: This quantitative analysis, formalized in roofline models[10](#fn10), provides
    a systematic framework that guides both algorithm design and hardware selection
    by clearly identifying the dominant performance constraints for specific workloads.
    Understanding these quantitative relationships allows engineers to predict performance
    bottlenecks accurately and optimize both model architectures and deployment strategies
    accordingly. For instance, increasing batch size from 1 to 32 for transformer
    inference can shift operations from memory-bound (8 FLOPS/byte) to compute-bound
    (150 FLOPS/byte), improving GPU utilization from 4% to 65% ([Pope et al. 2022](ch058.xhtml#ref-pope2022efficiently)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种定量分析，在roofline模型[10](#fn10)中形式化，提供了一个系统框架，通过明确识别特定工作负载的主要性能约束，指导算法设计和硬件选择。理解这些定量关系使得工程师能够准确预测性能瓶颈，并相应地优化模型架构和部署策略。例如，将transformer推理的批量大小从1增加到32可以将操作从内存受限（8
    FLOPS/byte）转变为计算受限（150 FLOPS/byte），将GPU利用率从4%提高到65%([Pope等人2022](ch058.xhtml#ref-pope2022efficiently))。
- en: System benchmarks evaluate performance across scales, ranging from single-chip
    configurations to large distributed systems, and AI workloads including both training
    and inference tasks. This evaluation approach ensures that benchmarks accurately
    reflect real-world deployment scenarios and deliver insights that inform both
    hardware selection decisions and system architecture design. [Figure 12.2](ch018.xhtml#fig-imagenet-gpus)
    illustrates the correlation between ImageNet classification error rates and GPU
    adoption from 2010 to 2014\. These results highlight how improved hardware capabilities,
    combined with algorithmic advances, drove progress in computer vision performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 系统基准测试评估了从单芯片配置到大型分布式系统以及包括训练和推理任务在内的AI工作负载在内的性能。这种评估方法确保基准测试能够准确反映现实世界的部署场景，并提供有关硬件选择决策和系统架构设计的见解。[图12.2](ch018.xhtml#fig-imagenet-gpus)展示了2010年至2014年间ImageNet分类错误率与GPU采用率之间的相关性。这些结果突出了硬件能力的提升以及算法进步如何推动计算机视觉性能的进步。
- en: '![](../media/file194.svg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file194.svg)'
- en: 'Figure 12.2: **ImageNet Benchmark**: Advancements in GPU technology have driven
    improvements in ImageNet classification accuracy since 2012, showcasing the interplay
    between hardware and algorithmic progress.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：**ImageNet基准测试**：自2012年以来，GPU技术的进步推动了ImageNet分类精度的提高，展示了硬件和算法进步之间的相互作用。
- en: The ImageNet example above demonstrates how hardware advances enable algorithmic
    breakthroughs, but effective system benchmarking requires understanding the nuanced
    relationship between workload characteristics and hardware utilization. Modern
    AI systems rarely achieve theoretical peak performance due to complex interactions
    between computational patterns, memory hierarchies, and system architectures.
    This reality gap between theoretical and achieved performance shapes how we design
    meaningful system benchmarks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的ImageNet示例展示了硬件进步如何促进算法突破，但有效的系统基准测试需要理解工作负载特征与硬件利用率之间微妙的关系。现代AI系统很少能达到理论峰值性能，因为计算模式、内存层次结构和系统架构之间的复杂交互。这种理论与实际性能之间的差距塑造了我们设计有意义的系统基准测试的方式。
- en: Understanding realistic hardware utilization patterns becomes essential for
    actionable benchmark design. Different AI workloads interact with hardware architectures
    in distinctly different ways, creating utilization patterns that vary dramatically
    based on model architecture, batch size, and precision choices. GPU utilization
    varies from 85% for well-optimized ResNet-50 training with batch size 64 to only
    15% with batch size 1 ([Y. You et al. 2019](ch058.xhtml#ref-you2019scaling)) due
    to insufficient parallelism. Memory bandwidth utilization ranges from 20% for
    parameter-heavy transformer models to 90% for activation-heavy convolutional networks,
    directly impacting achievable performance across different precision levels.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 理解实际的硬件利用率模式对于可操作的基准设计变得至关重要。不同的AI工作负载以截然不同的方式与硬件架构交互，根据模型架构、批量大小和精度选择，创建出差异巨大的利用率模式。GPU利用率从批量大小为64的优化良好的ResNet-50训练的85%到批量大小为1的仅15%
    ([Y. You et al. 2019](ch058.xhtml#ref-you2019scaling))，这是由于并行性不足。内存带宽利用率从参数密集型的transformer模型的20%到激活密集型的卷积网络的90%，直接影响到不同精度级别上可达到的性能。
- en: 'Energy efficiency considerations add another critical dimension to system benchmarking.
    Performance per watt varies by three orders of magnitude across computing platforms,
    making energy efficiency a critical benchmark dimension for production deployments.
    Utilization significantly impacts efficiency: underutilized GPUs consume disproportionate
    power while delivering minimal performance, creating substantial efficiency penalties
    that affect operational costs and environmental impact.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 能效考虑因素为系统基准测试增加了另一个关键维度。性能每瓦特在计算平台之间差异三个数量级，使得能效成为生产部署的关键基准维度。利用率对效率有显著影响：利用率低的GPU消耗了不成比例的电力，同时提供最小的性能，造成巨大的效率损失，影响运营成本和环境影响。
- en: Distributed system performance introduces additional complexity that system
    benchmarks must capture. Traditional roofline models extend to multi-GPU and multi-node
    scenarios, but distributed training introduces communication bottlenecks that
    often dominate performance. Inter-node bandwidth limitations, NUMA topology effects,
    and network congestion create performance variations that single-node benchmarks
    cannot reveal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统性能引入了系统基准测试必须捕捉的额外复杂性。传统的屋顶线模型扩展到多GPU和多节点场景，但分布式训练引入了通信瓶颈，这通常占主导地位。节点间带宽限制、NUMA拓扑效应和网络拥塞创造了单节点基准测试无法揭示的性能变化。
- en: Production distributed systems face challenges that require specialized benchmarking
    methodologies addressing real-world deployment scenarios. Network partitions during
    multi-node training affect gradient synchronization and model consistency, requiring
    fault tolerance evaluation under partial connectivity conditions. Clock synchronization
    becomes critical for accurate distributed performance measurement across geographically
    distributed nodes, where timestamp drift can invalidate benchmark results.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级分布式系统面临需要专门基准测试方法解决的实际部署场景的挑战。多节点训练期间的网络分区影响梯度同步和模型一致性，需要在部分连接条件下进行容错评估。时钟同步对于准确测量地理上分布的节点间的分布式性能变得至关重要，因为时间戳漂移会无效化基准测试结果。
- en: 'Scaling efficiency measurement reveals critical distributed systems bottlenecks
    in production ML workloads. Linear scaling efficiency degrades significantly beyond
    64-128 nodes for most models due to communication overhead: ResNet-50 training
    achieves 90% scaling efficiency up to 32 nodes but only 60% efficiency at 128
    nodes. Gradient aggregation latency increases quadratically with cluster size
    in traditional parameter server architectures, while all-reduce communication
    patterns achieve better scaling but require high-bandwidth interconnects.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 规模化效率测量揭示了生产级机器学习工作负载中关键分布式系统瓶颈。对于大多数模型，由于通信开销，线性扩展效率在64-128节点之后显著下降：ResNet-50训练在32节点时达到90%的扩展效率，但在128节点时只有60%的效率。在传统的参数服务器架构中，梯度聚合延迟随着集群规模的增加而呈二次方增长，而all-reduce通信模式虽然实现了更好的扩展，但需要高带宽的互连。
- en: Consensus mechanisms for benchmark completion across distributed nodes introduce
    coordination challenges absent from single-node evaluation. Determining benchmark
    completion requires distributed agreement on convergence criteria, handling node
    failures during benchmark execution, and ensuring consistent state across all
    participating nodes. Byzantine fault tolerance becomes necessary for benchmarks
    spanning multiple administrative domains or cloud providers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式节点间完成基准的共识机制引入了单节点评估中不存在的协调挑战。确定基准完成需要分布式节点对收敛标准的共识，处理基准执行期间的节点故障，并确保所有参与节点的状态一致性。拜占庭容错对于跨越多个管理域或云提供商的基准成为必要。
- en: Network topology effects significantly impact distributed training performance
    in production environments. InfiniBand interconnects achieve 200 Gbps per link
    with microsecond latency, enabling near-linear scaling for communication-intensive
    workloads. Ethernet-based clusters with 100 Gbps links experience 10-100x higher
    latency, limiting scaling efficiency for gradient-heavy models. NUMA topology
    within nodes creates memory bandwidth contention that affects local gradient computation
    before network communication.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑效应在生产环境中显著影响分布式训练性能。InfiniBand互连每个链路达到200 Gbps，具有微秒级延迟，为通信密集型工作负载提供近似线性扩展。基于以太网的集群具有100
    Gbps链路，但延迟高达10-100倍，限制了梯度密集型模型的扩展效率。节点内的NUMA拓扑创建内存带宽竞争，影响网络通信之前的本地梯度计算。
- en: Dynamic resource allocation in production distributed systems requires benchmarking
    frameworks that account for resource heterogeneity and temporal variations. Cloud
    instances with different memory capacities, CPU speeds, and network bandwidth
    create load imbalance that degrades overall training performance. Spot instance
    availability fluctuations require fault-tolerant benchmarking that measures recovery
    time from node failures and resource scaling responsiveness.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产级分布式系统中动态资源分配需要考虑资源异构性和时间变化的基准框架。不同内存容量、CPU速度和网络带宽的云实例会创建负载不平衡，降低整体训练性能。Spot实例可用性的波动需要容错基准，以衡量节点故障后的恢复时间和资源扩展响应性。
- en: These distributed systems considerations highlight the gap between idealized
    single-node benchmarks and production deployment realities. Effective distributed
    ML benchmarking must therefore evaluate communication patterns, fault tolerance,
    resource heterogeneity, and coordination overhead to guide real-world system design
    decisions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分布式系统考虑因素突显了理想化单节点基准与生产部署现实之间的差距。因此，有效的分布式机器学习基准必须评估通信模式、容错性、资源异构性和协调开销，以指导现实世界的系统设计决策。
- en: These hardware utilization insights directly inform benchmark design principles.
    Effective system benchmarks must evaluate performance across realistic utilization
    scenarios rather than focusing solely on peak theoretical capabilities. This approach
    ensures that benchmark results translate to practical deployment guidance, enabling
    engineers to make informed decisions about hardware selection, system configuration,
    and optimization strategies.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些硬件利用率洞察直接指导基准设计原则。有效的系统基准必须评估在现实利用率场景下的性能，而不是仅仅关注峰值理论能力。这种方法确保基准结果转化为实际部署指导，使工程师能够就硬件选择、系统配置和优化策略做出明智的决定。
- en: 'This transition from computational infrastructure evaluation naturally leads
    us to the third and equally critical dimension of comprehensive ML system benchmarking:
    data quality assessment.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从计算基础设施评估的过渡自然地引导我们进入全面机器学习系统基准的第三个同样关键的维度：数据质量评估。
- en: Data Benchmarks
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据基准
- en: The third dimension of our framework systematically examines data quality, representativeness,
    and bias in machine learning evaluation. Data benchmarks assess how dataset characteristics
    affect model performance and reveal critical limitations that may not be apparent
    from algorithmic or system metrics alone. This dimension is particularly critical
    because data quality constraints often determine real-world deployment success
    regardless of algorithmic sophistication or hardware capability.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们框架的第三个维度系统地检查数据质量、代表性以及机器学习评估中的偏差。数据基准评估数据集特征如何影响模型性能，并揭示仅从算法或系统指标中可能不明显的关键限制。这一维度尤其关键，因为数据质量约束通常决定了实际部署的成功，无论算法的复杂性或硬件能力如何。
- en: Data quality, scale, and diversity shape machine learning system performance,
    directly influencing how effectively algorithms learn and generalize to new situations.
    To address this dependency, data benchmarks establish standardized datasets and
    evaluation methodologies that enable consistent comparison of different approaches.
    These frameworks assess critical aspects of data quality, including domain coverage,
    potential biases, and resilience to real-world variations in input data ([Gebru
    et al. 2021b](ch058.xhtml#ref-gebru2021datasheets)). The data engineering practices
    necessary for creating reliable benchmarks are detailed in [Chapter 6](ch012.xhtml#sec-data-engineering),
    while fairness considerations in benchmark design connect to broader responsible
    AI principles covered in [Chapter 17](ch023.xhtml#sec-responsible-ai).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量、规模和多样性塑造了机器学习系统的性能，直接影响算法学习新情况的有效性和泛化能力。为了解决这种依赖性，数据基准建立了标准化的数据集和评估方法，使不同方法的比较保持一致。这些框架评估数据质量的关键方面，包括领域覆盖范围、潜在偏差以及对输入数据现实世界变化的抗性（[Gebru等人2021b](ch058.xhtml#ref-gebru2021datasheets)）。创建可靠基准所需的数据工程实践在[第6章](ch012.xhtml#sec-data-engineering)中详细说明，而基准设计中的公平性考虑与[第17章](ch023.xhtml#sec-responsible-ai)中涵盖的更广泛的负责任AI原则相联系。
- en: '***ML Data Benchmarks*** are standardized evaluations of *dataset quality*,
    assessing *coverage*, *bias*, *representativeness*, and *robustness* to enable
    objective comparison of data’s impact on model performance.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**ML数据基准**是对**数据集质量**的标准化评估，评估**覆盖范围**、**偏差**、**代表性**和**鲁棒性**，以实现数据对模型性能影响的客观比较。'
- en: Data benchmarks serve an essential function in understanding AI system behavior
    under diverse data conditions. Through systematic evaluation, they help identify
    common failure modes, expose critical gaps in data coverage, and reveal underlying
    biases that could significantly impact model behavior in deployment. By providing
    common frameworks for data evaluation, these benchmarks enable the AI community
    to systematically improve data quality and address potential issues before deploying
    systems in production environments. This proactive approach to data quality assessment
    has become increasingly critical as AI systems take on more complex and consequential
    tasks across different domains.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据基准在理解AI系统在不同数据条件下的行为中发挥着至关重要的作用。通过系统性的评估，它们有助于识别常见的故障模式，揭示数据覆盖的临界差距，并揭示可能显著影响模型部署行为的潜在偏差。通过提供数据评估的通用框架，这些基准使AI社区能够系统地提高数据质量，并在将系统部署到生产环境中之前解决潜在问题。随着AI系统承担更多复杂和重要的任务，这种主动的数据质量评估方法变得越来越关键。
- en: Community-Driven Standardization
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社区驱动的标准化
- en: 'Building on our three-dimensional framework, we face a critical challenge created
    by the proliferation of benchmarks spanning performance, energy efficiency, and
    domain-specific applications: establishing industry-wide standards. While early
    computing benchmarks primarily measured simple metrics like processor speed and
    memory bandwidth, modern benchmarks must evaluate sophisticated aspects of system
    performance, from complex power consumption profiles to highly specialized application-specific
    capabilities. This evolution in scope and complexity necessitates comprehensive
    validation and consensus from the computing community, particularly in rapidly
    evolving fields like machine learning where performance must be evaluated across
    multiple interdependent dimensions.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的三维框架基础上，我们面临着由基准的激增带来的关键挑战，这些基准涵盖了性能、能源效率和特定领域的应用：建立行业标准。虽然早期的计算基准主要测量简单的指标，如处理器速度和内存带宽，但现代基准必须评估系统性能的复杂方面，从复杂的功耗配置文件到高度专业化的特定应用能力。这种范围和复杂性的演变需要计算社区的全面验证和共识，尤其是在机器学习等快速发展的领域，必须从多个相互依赖的维度评估性能。
- en: The lasting impact of any benchmark depends critically on its acceptance by
    the broader research community, where technical excellence alone is insufficient
    for adoption. Benchmarks developed without broad community input often fail to
    gain meaningful traction, frequently missing critical metrics that leading research
    groups consider essential. Successful benchmarks emerge through collaborative
    development involving academic institutions, industry partners, and domain experts.
    This inclusive approach ensures benchmarks evaluate capabilities most crucial
    for advancing the field, while balancing theoretical and practical considerations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 任何基准的持久影响都严重依赖于更广泛研究社区的接受程度，仅仅技术上的卓越不足以保证其被采用。没有广泛社区参与的基准开发往往难以获得有意义的推广，常常遗漏了领先研究组认为至关重要的关键指标。成功的基准是通过涉及学术机构、行业合作伙伴和领域专家的协作开发而产生的。这种包容性方法确保基准评估了推动该领域发展的最关键能力，同时平衡了理论和实践考虑。
- en: In contrast, benchmarks developed through extensive collaboration among respected
    institutions carry the authority necessary to drive widespread adoption, while
    those perceived as advancing particular corporate interests face skepticism and
    limited acceptance. The remarkable success of ImageNet demonstrates how sustained
    community engagement through workshops and challenges establishes long-term viability
    and lasting impact. This community-driven development creates a foundation for
    formal standardization, where organizations like IEEE and ISO transform these
    benchmarks into official standards.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，通过在受尊敬的机构之间广泛合作开发的基准，具有推动广泛采用所需的权威性，而那些被认为是在推进特定企业利益方面的基准则面临怀疑和有限的接受。ImageNet
    的显著成功展示了通过研讨会和挑战持续社区参与如何建立长期可行性和持久影响。这种社区驱动的发展为正式标准化奠定了基础，其中像 IEEE 和 ISO 这样的组织将这些基准转化为官方标准。
- en: The standardization process provides crucial infrastructure for benchmark formalization
    and adoption. [IEEE working groups](https://standards.ieee.org/develop/wg/) transform
    community-developed benchmarking methodologies into formal industry standards,
    establishing precise specifications for measurement and reporting. The [IEEE 2416-2019](https://standards.ieee.org/ieee/2416/7065/)
    standard for system power modeling exemplifies this process, codifying best practices
    developed through community consensus. Similarly, [ISO/IEC technical committees](https://www.iso.org/committee/45020.html)
    develop international standards for benchmark validation and certification, ensuring
    consistent evaluation across global research and industry communities. These organizations
    bridge the gap between community-driven innovation and formal standardization,
    providing frameworks that enable reliable comparison of results across different
    institutions and geographic regions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化过程为基准的正式化和采用提供了关键基础设施。[IEEE 工作组](https://standards.ieee.org/develop/wg/)将社区开发的基准方法转化为正式的行业标准，为测量和报告建立了精确的规范。[IEEE
    2416-2019](https://standards.ieee.org/ieee/2416/7065/) 系统功率建模标准就是这一过程的例证，它将社区共识中发展出的最佳实践法典化。同样，[ISO/IEC
    技术委员会](https://www.iso.org/committee/45020.html)制定基准验证和认证的国际标准，确保全球研究和行业社区的评价一致性。这些组织在社区驱动创新和正式标准化之间架起了桥梁，提供了框架，使得不同机构和地理区域之间可靠地比较结果成为可能。
- en: Successful community benchmarks establish clear governance structures for managing
    their evolution. Through rigorous version control systems and detailed change
    documentation, benchmarks maintain backward compatibility while incorporating
    new advances. This governance includes formal processes for proposing, reviewing,
    and implementing changes, ensuring that benchmarks remain relevant while maintaining
    stability. Modern benchmarks increasingly emphasize reproducibility requirements,
    incorporating automated verification systems and standardized evaluation environments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的社区基准建立了明确的治理结构来管理其演变。通过严格的版本控制系统和详细的变更文档，基准在吸收新进展的同时保持向后兼容。这种治理包括正式的提案、审查和实施变更的程序，确保基准在保持稳定性的同时保持相关性。现代基准越来越强调可重复性要求，纳入了自动验证系统和标准化的评估环境。
- en: Open access accelerates benchmark adoption and ensures consistent implementation.
    Projects that provide open-source reference implementations, comprehensive documentation,
    validation suites, and containerized evaluation environments reduce barriers to
    entry. This standardization enables research groups to evaluate solutions using
    uniform methods and metrics. Without such coordinated implementation frameworks,
    organizations might interpret benchmarks inconsistently, compromising result reproducibility
    and meaningful comparison across studies.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 开放访问加速了基准测试的采用并确保了一致的实施。提供开源参考实现、全面文档、验证套件和容器化评估环境的项目降低了进入门槛。这种标准化使得研究团队能够使用统一的方法和指标评估解决方案。如果没有这样的协调实施框架，组织可能会不一致地解释基准测试，从而损害结果的可重复性和跨研究的有意义比较。
- en: The most successful benchmarks strike a careful balance between academic rigor
    and industry practicality. Academic involvement ensures theoretical soundness
    and comprehensive evaluation methodology, while industry participation grounds
    benchmarks in practical constraints and real-world applications. This balance
    proves particularly crucial in machine learning benchmarks, where theoretical
    advances must translate to practical improvements in deployed systems ([D. Patterson
    et al. 2021a](ch058.xhtml#ref-patterson2021carbon)). These evaluation methodology
    principles guide both training and inference benchmark design throughout this
    chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最成功的基准测试在学术严谨性和行业实用性之间取得了谨慎的平衡。学术参与确保了理论上的严谨性和全面的评估方法，而行业参与则将基准测试建立在实际约束和现实世界应用的基础上。这种平衡在机器学习基准测试中尤其重要，因为理论进步必须转化为部署系统中实际改进
    ([D. Patterson et al. 2021a](ch058.xhtml#ref-patterson2021carbon))。这些评估方法原则指导了本章中训练和推理基准设计。
- en: Community consensus establishes enduring benchmark relevance, while fragmentation
    impedes scientific progress. Through collaborative development and transparent
    operation, benchmarks evolve into authoritative standards for measuring advancement.
    The most successful benchmarks in energy efficiency and domain-specific applications
    share this foundation of community development and governance, demonstrating how
    collective expertise and shared purpose create lasting impact in rapidly advancing
    fields.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 社区共识确立了基准测试的持久相关性，而碎片化则阻碍了科学进步。通过协作开发和透明操作，基准测试演变为衡量进步的权威标准。在能效和特定领域应用中最成功的基准测试共享这一社区发展和治理的基础，展示了集体专业知识和共同目标如何在快速发展的领域中产生持久影响。
- en: Benchmarking Granularity
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试粒度
- en: The three-dimensional framework and measurement foundations established above
    provide the conceptual structure for benchmarking. However, implementing these
    principles requires choosing the appropriate level of detail for evaluation, from
    individual tensor operations to complete ML applications. Just as the optimization
    techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations) operate at different
    granularities, benchmarks must adapt their evaluation scope to match specific
    optimization goals. This hierarchical perspective allows practitioners to isolate
    performance bottlenecks at the micro level or assess system-wide behavior at the
    macro level.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 上述建立的三维框架和测量基础为基准测试提供了概念结构。然而，实施这些原则需要选择适当的细节级别进行评估，从单个张量操作到完整的机器学习应用。正如第10章中提到的优化技术在不同粒度上运行一样，基准测试必须调整其评估范围以匹配特定的优化目标。这种分层视角允许实践者从微观层面隔离性能瓶颈，或在宏观层面评估整个系统的行为。
- en: System level benchmarking provides a structured and systematic approach to assessing
    a ML system’s performance across various dimensions. Given the complexity of ML
    systems, we can dissect their performance through different levels of granularity
    and obtain a comprehensive view of the system’s efficiency, identify potential
    bottlenecks, and pinpoint areas for improvement. To this end, various types of
    benchmarks have evolved over the years and continue to persist.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级基准测试提供了一种结构化和系统化的方法来评估机器学习系统在各个维度的性能。鉴于机器学习系统的复杂性，我们可以通过不同级别的粒度来剖析其性能，从而获得系统效率的全面视图，识别潜在的瓶颈，并确定改进的区域。为此，各种类型的基准测试在多年中不断发展并持续存在。
- en: '[Figure 12.3](ch018.xhtml#fig-granularity) shows the different layers of granularity
    of an ML system. At the application level, end-to-end benchmarks assess the overall
    system performance, considering factors like data preprocessing, model training,
    and inference. While at the model layer, benchmarks focus on assessing the efficiency
    and accuracy of specific models. This includes evaluating how well models generalize
    to new data and their computational efficiency during training and inference.
    Benchmarking can extend to hardware and software infrastructure, examining the
    performance of individual components like GPUs or TPUs.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.3](ch018.xhtml#fig-granularity) 展示了机器学习系统的不同粒度层次。在应用层面，端到端基准评估整体系统性能，考虑因素包括数据预处理、模型训练和推理。而在模型层面，基准则专注于评估特定模型的效率和准确性。这包括评估模型对新数据的泛化能力以及训练和推理过程中的计算效率。基准测试可以扩展到硬件和软件基础设施，检查单个组件（如GPU或TPU）的性能。'
- en: '![](../media/file195.svg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file195.svg)'
- en: 'Figure 12.3: **Benchmarking Granularity**: ML system performance assessment
    occurs at multiple levels, from end-to-end application metrics to individual model
    and hardware component efficiency, enabling targeted optimization and bottleneck
    identification. This hierarchical approach allows practitioners to systematically
    analyze system performance and prioritize improvements based on specific component
    limitations.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：**基准粒度**：机器学习系统性能评估发生在多个层面，从端到端应用指标到单个模型和硬件组件效率，从而实现有针对性的优化和瓶颈识别。这种分层方法允许从业者系统地分析系统性能，并根据特定组件的限制优先考虑改进。
- en: Micro Benchmarks
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微基准测试
- en: Micro-benchmarks are specialized evaluation tools that assess distinct components
    or specific operations within a broader machine learning process. These benchmarks
    isolate individual tasks to provide detailed insights into the computational demands
    of particular system elements, from neural network layers to optimization techniques
    to activation functions. For example, micro-benchmarks might measure the time
    required to execute a convolutional layer in a deep learning model or evaluate
    the speed of data preprocessing operations that prepare training data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试是专门的评价工具，用于评估更广泛机器学习过程中的不同组件或特定操作。这些基准将单个任务隔离出来，以提供对特定系统元素计算需求的详细见解，从神经网络层到优化技术到激活函数。例如，微基准测试可能会测量执行深度学习模型中卷积层所需的时间，或评估准备训练数据的数据预处理操作的速度。
- en: A key area of micro-benchmarking focuses on tensor operations[11](#fn11), which
    are the computational core of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[12](#fn12)
    by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions
    and matrix multiplications across different hardware configurations. These measurements
    help developers understand how their hardware handles the core mathematical operations
    that dominate ML workloads.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试的关键领域集中在张量操作[11](#fn11)上，这是深度学习的计算核心。NVIDIA的库[cuDNN](https://developer.nvidia.com/cudnn)[12](#fn12)提供了在不同硬件配置下测量基本计算（如卷积和矩阵乘法）的基准。这些测量帮助开发者了解他们的硬件如何处理主导机器学习工作负载的核心数学运算。
- en: Micro-benchmarks also examine activation functions and neural network layers
    in isolation. This includes measuring the performance of various activation functions
    like ReLU, Sigmoid[13](#fn13), and Tanh[14](#fn14) under controlled conditions,
    as well as evaluating the computational efficiency of distinct neural network
    components such as LSTM[15](#fn15) cells or Transformer blocks when processing
    standardized inputs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试还单独检查激活函数和神经网络层。这包括在受控条件下测量各种激活函数（如ReLU、Sigmoid[13](#fn13) 和 Tanh[14](#fn14)）的性能，以及评估不同神经网络组件（如LSTM[15](#fn15)
    单元或Transformer块）在处理标准化输入时的计算效率。
- en: '[DeepBench](https://github.com/baidu-research/DeepBench), developed by Baidu,
    was one of the first to demonstrate the value of comprehensive micro-benchmarking.
    It evaluates these fundamental operations across different hardware platforms,
    providing detailed performance data that helps developers optimize their deep
    learning implementations. By isolating and measuring individual operations, DeepBench
    enables precise comparison of hardware platforms and identification of potential
    performance bottlenecks.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 百度开发的[DeepBench](https://github.com/baidu-research/DeepBench)是第一个展示全面微观基准价值的项目之一。它评估了不同硬件平台上的这些基本操作，提供了有助于开发者优化其深度学习实现的详细性能数据。通过隔离和测量单个操作，DeepBench能够精确比较硬件平台并识别潜在的性能瓶颈。
- en: Macro Benchmarks
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 宏观基准
- en: While micro-benchmarks examine individual operations like tensor computations
    and layer performance, macro benchmarks evaluate complete machine learning models.
    This shift from component-level to model-level assessment provides insights into
    how architectural choices and component interactions affect overall model behavior.
    For instance, while micro-benchmarks might show optimal performance for individual
    convolutional layers, macro-benchmarks reveal how these layers work together within
    a complete convolutional neural network.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当微观基准检查单个操作，如张量计算和层性能时，宏观基准评估的是完整的机器学习模型。这种从组件级到模型级评估的转变，有助于了解架构选择和组件交互如何影响整体模型行为。例如，虽然微观基准可能显示单个卷积层的最佳性能，但宏观基准揭示了这些层如何在完整的卷积神经网络中协同工作。
- en: Macro-benchmarks measure multiple performance dimensions that emerge only at
    the model level. These include prediction accuracy, which shows how well the model
    generalizes to new data; memory consumption patterns across different batch sizes
    and sequence lengths; throughput under varying computational loads; and latency
    across different hardware configurations. Understanding these metrics helps developers
    make informed decisions about model architecture, optimization strategies, and
    deployment configurations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观基准衡量的是仅在模型级别出现的多个性能维度。这包括预测准确性，它显示了模型对新数据的泛化能力；不同批量大小和序列长度下的内存消耗模式；不同计算负载下的吞吐量；以及不同硬件配置下的延迟。理解这些指标有助于开发者就模型架构、优化策略和部署配置做出明智的决定。
- en: The assessment of complete models occurs under standardized conditions using
    established datasets and tasks. For example, computer vision models might be evaluated
    on [ImageNet](https://www.image-net.org/), measuring both computational efficiency
    and prediction accuracy. Natural language processing models might be assessed
    on translation tasks, examining how they balance quality and speed across different
    language pairs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 完整模型的评估是在标准化的条件下，使用既定的数据集和任务进行的。例如，计算机视觉模型可能会在[ImageNet](https://www.image-net.org/)上进行评估，衡量其计算效率和预测准确性。自然语言处理模型可能会在翻译任务上进行评估，检查它们在不同语言对之间如何平衡质量和速度。
- en: Several industry-standard benchmarks enable consistent model evaluation across
    platforms. [MLPerf Inference](https://github.com/mlcommons/inference) provides
    comprehensive testing suites adapted for different computational environments
    ([Reddi et al. 2019b](ch058.xhtml#ref-reddi2020mlperf)). [MLPerf Mobile](https://github.com/mlcommons/mobile_app_open)
    focuses on mobile device constraints ([Janapa Reddi et al. 2022](ch058.xhtml#ref-janapa2022mlperf)),
    while [MLPerf Tiny](https://github.com/mlcommons/tiny) addresses microcontroller
    deployments ([C. Banbury et al. 2021](ch058.xhtml#ref-banbury2021mlperf)). For
    embedded systems, [EEMBC’s MLMark](https://github.com/eembc/mlmark) emphasizes
    both performance and power efficiency. The [AI-Benchmark](https://ai-benchmark.com/)
    suite specializes in mobile platforms, evaluating models across diverse tasks
    from image recognition to face parsing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 几个行业标准基准使模型在不同平台上的评估保持一致。[MLPerf Inference](https://github.com/mlcommons/inference)提供了适用于不同计算环境的全面测试套件（[Reddi
    等人 2019b](ch058.xhtml#ref-reddi2020mlperf)）。[MLPerf Mobile](https://github.com/mlcommons/mobile_app_open)专注于移动设备限制（[Janapa
    Reddi 等人 2022](ch058.xhtml#ref-janapa2022mlperf)），而[MLPerf Tiny](https://github.com/mlcommons/tiny)针对微控制器部署（[C.
    Banbury 等人 2021](ch058.xhtml#ref-banbury2021mlperf)）。对于嵌入式系统，[EEMBC的MLMark](https://github.com/eembc/mlmark)强调性能和功耗效率。[AI-Benchmark](https://ai-benchmark.com/)套件专注于移动平台，评估从图像识别到人脸解析的多种任务上的模型。
- en: End-to-End Benchmarks
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端基准
- en: End-to-end benchmarks provide an all-inclusive evaluation that extends beyond
    the boundaries of the ML model itself. Rather than focusing solely on a machine
    learning model’s computational efficiency or accuracy, these benchmarks encompass
    the entire pipeline of an AI system. This includes initial ETL (Extract-Transform-Load)
    or ELT (Extract-Load-Transform) data processing, the core model’s performance,
    post-processing of results, and critical infrastructure components like storage
    and network systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端基准测试提供了一个全面评估，它超越了机器学习模型本身的边界。这些基准不仅关注机器学习模型的计算效率或准确性，还涵盖了人工智能系统的整个流程。这包括初始的ETL（提取-转换-加载）或ELT（提取-加载-转换）数据处理、核心模型性能、结果的后处理以及存储和网络系统等关键基础设施组件。
- en: Data processing is the foundation of all AI systems, transforming raw data into
    a format suitable for model training or inference. In ETL pipelines, data undergoes
    extraction from source systems, transformation through cleaning and feature engineering,
    and loading into model-ready formats. These preprocessing steps’ efficiency, scalability,
    and accuracy significantly impact overall system performance. End-to-end benchmarks
    must assess standardized datasets through these pipelines to ensure data preparation
    doesn’t become a bottleneck.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理是所有人工智能系统的基石，它将原始数据转换成适合模型训练或推理的格式。在ETL管道中，数据从源系统提取，经过清洗和特征工程等转换，然后加载到模型准备好的格式中。这些预处理步骤的效率、可扩展性和准确性对整体系统性能有重大影响。端到端基准测试必须通过这些管道评估标准化数据集，以确保数据准备不会成为瓶颈。
- en: The post-processing phase plays an equally important role. This involves interpreting
    the model’s raw outputs, converting scores into meaningful categories, filtering
    results based on predefined tasks, or integrating with other systems. For instance,
    a computer vision system might need to post-process detection boundaries, apply
    confidence thresholds, and format results for downstream applications. In real-world
    deployments, this phase proves crucial for delivering actionable insights.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理阶段同样重要。这涉及到解释模型的原始输出，将分数转换为有意义的类别，根据预定义的任务过滤结果，或与其他系统集成。例如，计算机视觉系统可能需要后处理检测边界，应用置信度阈值，并将结果格式化为下游应用。在实际部署中，这一阶段对于提供可操作的见解至关重要。
- en: Beyond core AI operations, infrastructure components heavily influence overall
    performance and user experience. Storage solutions, whether cloud-based, on-premises,
    or hybrid, can significantly impact data retrieval and storage times, especially
    with vast AI datasets. Network interactions, vital for distributed systems, can
    become performance bottlenecks if not optimized. End-to-end benchmarks must evaluate
    these components under specified environmental conditions to ensure reproducible
    measurements of the entire system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心人工智能操作之外，基础设施组件对整体性能和用户体验有重大影响。存储解决方案，无论是基于云、本地还是混合型，都可能显著影响数据检索和存储时间，尤其是在庞大的AI数据集情况下。对于分布式系统至关重要的网络交互，如果没有优化，可能会成为性能瓶颈。端到端基准测试必须在指定的环境条件下评估这些组件，以确保整个系统的可重复测量。
- en: To date, there are no public, end-to-end benchmarks that fully account for data
    storage, network, and compute performance. While MLPerf Training and Inference
    approach end-to-end evaluation, they primarily focus on model performance rather
    than real-world deployment scenarios. Nonetheless, they provide valuable baseline
    metrics for assessing AI system capabilities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还没有公开的、端到端的基准测试能够完全考虑到数据存储、网络和计算性能。虽然MLPerf Training和Inference方法接近端到端评估，但它们主要关注模型性能，而不是实际部署场景。尽管如此，它们为评估人工智能系统能力提供了有价值的基线指标。
- en: Given the inherent specificity of end-to-end benchmarking, organizations typically
    perform these evaluations internally by instrumenting production deployments.
    This allows engineers to develop result interpretation guidelines based on realistic
    workloads, but given the sensitivity and specificity of the information, these
    benchmarks rarely appear in public settings.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于端到端基准测试的固有特定性，组织通常通过在生产部署中实施这些评估来内部执行这些评估。这允许工程师根据实际工作负载制定结果解释指南，但鉴于信息的敏感性和特定性，这些基准很少出现在公共场合。
- en: Granularity Trade-offs and Selection Criteria
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 粒度权衡和选择标准
- en: 'As shown in [Table 12.1](ch018.xhtml#tbl-benchmark-comparison), different challenges
    emerge at different stages of an AI system’s lifecycle. Each benchmarking approach
    provides unique insights: micro-benchmarks help engineers optimize specific components
    like GPU kernel implementations or data loading operations, macro-benchmarks guide
    model architecture decisions and algorithm selection, while end-to-end benchmarks
    reveal system-level bottlenecks in production environments.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表12.1](ch018.xhtml#tbl-benchmark-comparison)所示，人工智能系统生命周期不同阶段会涌现出不同的挑战。每种基准测试方法都提供独特的见解：微基准测试帮助工程师优化特定组件，如GPU内核实现或数据加载操作，宏基准测试指导模型架构决策和算法选择，而端到端基准测试则揭示了生产环境中的系统级瓶颈。
- en: 'Table 12.1: **Benchmarking Granularity Levels**: Different benchmark scopes
    (micro, macro, and end-to-end) target distinct stages of ML system development
    and reveal unique performance bottlenecks. Micro-benchmarks isolate individual
    operations for low-level optimization, macro-benchmarks evaluate complete models
    to guide architectural choices, and end-to-end benchmarks assess full system performance
    in production environments.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1：**基准粒度级别**：不同的基准范围（微、宏和端到端）针对机器学习系统开发的特定阶段，并揭示独特的性能瓶颈。微基准测试隔离单个操作以进行低级优化，宏基准测试评估完整模型以指导架构选择，而端到端基准测试评估生产环境中的完整系统性能。
- en: '| **Component** | **Micro Benchmarks** | **Macro Benchmarks** | **End-to-End
    Benchmarks** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **组件** | **微基准测试** | **宏基准测试** | **端到端基准测试** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Focus** | Individual operations | Complete models | Full system pipeline
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **关注点** | 单个操作 | 完整模型 | 全系统管道 |'
- en: '| **Scope** | Tensor ops, layers, activations | Model architecture, training,
    inference | ETL, model, infrastructure |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **范围** | 张量操作、层、激活 | 模型架构、训练、推理 | ETL、模型、基础设施 |'
- en: '| **Example** | Conv layer performance on cuDNN | ResNet-50 on ImageNet | Production
    recommendation system |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **示例** | cuDNN上的卷积层性能 | ImageNet上的ResNet-50 | 生产推荐系统 |'
- en: '| **Advantages** | Precise bottleneck identification, Component optimization
    | Model architecture comparison, Standardized evaluation | Realistic performance
    assessment, System-wide insights |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | 精确瓶颈识别，组件优化 | 模型架构比较，标准化评估 | 现实性能评估，系统级洞察 |'
- en: '| **Challenges** | May miss interaction effects | Limited infrastructure insights
    | Complex to standardize, Often proprietary |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | 可能错过交互效应 | 基础设施洞察有限 | 复杂标准化，通常专有 |'
- en: '| **Typical Use** | Hardware selection, Operation optimization | Model selection,
    Research comparison | Production system evaluation |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **典型用途** | 硬件选择，操作优化 | 模型选择，研究比较 | 生产系统评估 |'
- en: '[Figure 12.4](ch018.xhtml#fig-benchmark-tradeoffs) visualizes the core trade-off
    between diagnostic power and real-world representativeness across benchmark granularity
    levels. This relationship illustrates why comprehensive ML system evaluation requires
    multiple benchmark types: micro-benchmarks provide precise optimization guidance
    for isolated components, while end-to-end benchmarks capture the complex interactions
    that emerge in production systems. The optimal benchmarking strategy combines
    insights from all three levels to balance detailed component analysis with realistic
    system-wide assessment.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.4](ch018.xhtml#fig-benchmark-tradeoffs)可视化了基准粒度级别之间诊断能力和现实世界代表性之间的核心权衡。这种关系说明了为什么全面的机器学习系统评估需要多种基准类型：微基准测试为孤立组件提供精确的优化指导，而端到端基准测试则捕捉生产系统中出现的复杂交互。最佳的基准测试策略结合了所有三个级别的见解，以平衡详细组件分析与现实系统评估。'
- en: '![](../media/file196.svg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file196.svg)'
- en: 'Figure 12.4: **Benchmark Granularity Trade-offs**: The core trade-off in benchmarking
    granularity between isolation/diagnostic power and real-world representativeness.
    Micro-benchmarks provide high diagnostic precision but limited real-world relevance,
    while end-to-end benchmarks capture realistic system behavior but offer less precise
    component-level insights. Effective ML system evaluation requires strategic combination
    of all three levels.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：**基准粒度权衡**：基准粒度在隔离/诊断能力和现实世界代表性之间的核心权衡。微基准测试提供高诊断精度但现实世界相关性有限，而端到端基准测试捕捉现实系统行为但提供较少的组件级洞察。有效的机器学习系统评估需要战略性地结合所有三个级别的信息。
- en: Component interaction often produces unexpected behaviors. For example, while
    micro-benchmarks might show excellent performance for individual convolutional
    layers, and macro-benchmarks might demonstrate strong accuracy for the complete
    model, end-to-end evaluation could reveal that data preprocessing creates unexpected
    bottlenecks during high-traffic periods. These system-level insights often remain
    hidden when components undergo isolated testing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 组件之间的交互往往会产生意外的行为。例如，虽然微基准测试可能显示单个卷积层表现出色，宏基准测试可能显示完整模型具有强大的精度，但端到端评估可能会揭示数据预处理在高流量期间创造了意外的瓶颈。当组件进行隔离测试时，这些系统级见解往往隐藏起来。
- en: With benchmarking granularity established, understanding which level of evaluation
    serves specific optimization goals, we now examine the concrete components that
    constitute benchmark implementations at any granularity level.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了基准粒度后，了解哪个评估级别服务于特定的优化目标，我们现在来审视构成任何粒度级别基准实现的具体组件。
- en: Benchmark Components
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准组件
- en: Using our established framework, we now examine the practical components that
    constitute any benchmark implementation. These components provide the concrete
    structure for measuring performance across all three dimensions simultaneously.
    Whether evaluating model accuracy (algorithmic dimension), measuring inference
    latency (system dimension), or assessing dataset quality (data dimension), benchmarks
    share common structural elements that ensure systematic and reproducible evaluation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们建立的框架，我们现在来审视构成任何基准实现的实际组件。这些组件为同时测量性能的三个维度提供了具体结构。无论是评估模型精度（算法维度）、测量推理延迟（系统维度）还是评估数据集质量（数据维度），基准测试都共享一些共同的结构性元素，确保系统性和可重复的评估。
- en: 'The granularity level established in the previous section directly shapes how
    these components are instantiated. Micro-benchmarks measuring tensor operations
    require synthetic inputs that isolate specific computational patterns, enabling
    precise performance characterization of individual kernels as discussed in [Chapter 11](ch017.xhtml#sec-ai-acceleration).
    Macro-benchmarks evaluating complete models demand representative datasets like
    ImageNet that capture realistic task complexity while enabling standardized comparison
    across architectures. End-to-end benchmarks assessing production systems must
    incorporate real-world data characteristics including distribution shift, noise,
    and edge cases absent from curated evaluation sets. Similarly, evaluation metrics
    shift focus across granularity levels: micro-benchmarks emphasize FLOPS and memory
    bandwidth utilization, macro-benchmarks balance accuracy and inference speed,
    while end-to-end benchmarks prioritize system reliability and operational efficiency
    under load. Understanding this systematic variation ensures that component choices
    align with evaluation objectives rather than applying uniform approaches across
    different benchmarking scales.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中建立的粒度级别直接决定了这些组件是如何实例化的。测量张量操作的微基准测试需要合成输入，以隔离特定的计算模式，从而能够精确地描述单个内核的性能，如第11章[第11章](ch017.xhtml#sec-ai-acceleration)中所述。评估完整模型的宏基准测试需要像ImageNet这样的代表性数据集，它能够捕捉现实任务复杂性，同时允许在架构之间进行标准化的比较。评估生产系统的端到端基准测试必须包含真实世界的数据特征，包括分布偏移、噪声和边缘情况，这些在精心挑选的评估集中是不存在的。同样，评估指标在粒度级别之间转移焦点：微基准测试强调FLOPS和内存带宽利用率，宏基准测试在精度和推理速度之间取得平衡，而端到端基准测试在负载下的系统可靠性和操作效率方面优先考虑。理解这种系统性的变化确保组件选择与评估目标相一致，而不是在不同基准测试规模上应用统一的方法。
- en: Having established how benchmark granularity shapes evaluation scope (from micro-benchmarks
    isolating tensor operations to end-to-end assessments of complete systems), we
    now examine how these conceptual levels translate into concrete benchmark implementations.
    The components discussed abstractly above must be instantiated through specific
    choices about tasks, datasets, models, and metrics. This implementation process
    follows a systematic workflow that ensures reproducible and meaningful evaluation
    regardless of the chosen granularity level.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了基准粒度如何塑造评估范围（从隔离张量操作的微基准测试到完整系统的端到端评估）之后，我们现在来探讨这些概念级别如何转化为具体的基准实现。上面讨论的抽象组件必须通过关于任务、数据集、模型和指标的具体选择来实例化。这个实现过程遵循一个系统性的工作流程，确保无论选择的粒度级别如何，都能进行可重复且有意义的评估。
- en: An AI benchmark provides this structured framework for systematically evaluating
    artificial intelligence systems. While individual benchmarks vary significantly
    in their specific focus and granularity, they share common implementation components
    that enable consistent evaluation and comparison across different approaches.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: AI基准提供了一个结构化框架，用于系统地评估人工智能系统。虽然各个基准在其具体焦点和粒度上差异很大，但它们共享共同的实现组件，这些组件使得在不同方法之间的一致评估和比较成为可能。
- en: '[Figure 12.5](ch018.xhtml#fig-benchmark-components) illustrates this structured
    workflow, showcasing how the essential components (task definition, dataset selection,
    model selection, and evaluation metrics) interconnect to form a complete evaluation
    pipeline. Each component builds upon the previous one, creating a systematic progression
    from problem specification through deployment assessment.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.5](ch018.xhtml#fig-benchmark-components)展示了这个结构化工作流程，展示了基本组件（任务定义、数据集选择、模型选择和评估指标）如何相互连接形成一个完整的评估管道。每个组件都建立在之前的基础上，从问题规范到部署评估创建一个系统的进展。'
- en: '![](../media/file197.svg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5](../media/file197.svg)'
- en: 'Figure 12.5: **Benchmark Workflow**: AI benchmarks standardize evaluation through
    a structured pipeline, enabling reproducible performance comparisons across different
    models and systems. This workflow systematically assesses AI capabilities by defining
    tasks, selecting datasets, training models, and rigorously evaluating results.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：**基准工作流程**：AI基准通过结构化管道标准化评估，使不同模型和系统之间的性能比较具有可重复性。此工作流程通过定义任务、选择数据集、训练模型和严格评估结果来系统地评估AI能力。
- en: Effective benchmark design must account for the optimization techniques established
    in preceding chapters. Quantization and pruning affect model accuracy-efficiency
    trade-offs, requiring benchmarks that measure both speedup and accuracy preservation
    simultaneously. Hardware acceleration techniques influence arithmetic intensity
    and memory bandwidth utilization, necessitating roofline model analysis to interpret
    results correctly. Understanding these optimization foundations enables benchmark
    selection that validates claimed improvements rather than measuring artificial
    scenarios.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的基准设计必须考虑前几章中确立的优化技术。量化剪枝影响模型精度-效率权衡，需要同时衡量加速和精度保持的基准。硬件加速技术影响算术强度和内存带宽利用率，需要通过屋顶线模型分析来正确解释结果。理解这些优化基础能够使基准选择验证所声称的改进，而不是衡量人工场景。
- en: Problem Definition
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题定义
- en: As illustrated in [Figure 12.5](ch018.xhtml#fig-benchmark-components), a benchmark
    implementation begins with a formal specification of the machine learning task
    and its evaluation criteria. In machine learning, tasks represent well-defined
    problems that AI systems must solve. Consider an anomaly detection system that
    processes audio signals to identify deviations from normal operation patterns,
    as shown in [Figure 12.5](ch018.xhtml#fig-benchmark-components). This industrial
    monitoring application exemplifies how formal task specifications translate into
    practical implementations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图12.5](ch018.xhtml#fig-benchmark-components)所示，基准实现从对机器学习任务及其评估标准的正式规范开始。在机器学习中，任务代表AI系统必须解决的明确问题。考虑一个异常检测系统，它处理音频信号以识别偏离正常操作模式的变化，如[图12.5](ch018.xhtml#fig-benchmark-components)所示。这个工业监控应用示例说明了正式任务规范如何转化为实际实现。
- en: The formal definition of any benchmark task encompasses both the computational
    problem and its evaluation framework. While the specific tasks vary significantly
    by domain, well-established categories have emerged across major fields of AI
    research. Natural language processing tasks, for example, include machine translation,
    question answering ([Hirschberg and Manning 2015](ch058.xhtml#ref-hirschberg2015advances)),
    and text classification. Computer vision similarly employs standardized tasks
    such as object detection, image segmentation, and facial recognition ([Everingham
    et al. 2009](ch058.xhtml#ref-everingham2010pascal)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 任何基准任务的正式定义都包括计算问题和其评估框架。虽然具体任务因领域而异，但已在大规模AI研究领域中出现了成熟的类别。例如，自然语言处理任务包括机器翻译、问答([Hirschberg
    and Manning 2015](ch058.xhtml#ref-hirschberg2015advances))和文本分类。计算机视觉同样采用标准化的任务，如目标检测、图像分割和面部识别([Everingham
    et al. 2009](ch058.xhtml#ref-everingham2010pascal))。
- en: Every benchmark task specification must define three essential elements. The
    input specification determines what data the system processes. In [Figure 12.5](ch018.xhtml#fig-benchmark-components),
    this consists of audio waveform data. The output specification describes the required
    system response, such as the binary classification of normal versus anomalous
    patterns. The performance specification establishes quantitative requirements
    for accuracy, processing speed, and resource utilization.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 每个基准测试任务规范必须定义三个基本要素。输入规范确定系统处理的数据。在图12.5([Figure 12.5](ch018.xhtml#fig-benchmark-components))中，这包括音频波形数据。输出规范描述所需的系统响应，例如正常与异常模式的二进制分类。性能规范建立了准确性、处理速度和资源利用的定量要求。
- en: 'Task design directly impacts the benchmark’s ability to evaluate AI systems
    effectively. The audio anomaly detection example clearly illustrates this relationship
    through its specific requirements: processing continuous signal data, adapting
    to varying noise conditions, and operating within strict time constraints. These
    practical constraints create a detailed framework for assessing model performance,
    ensuring evaluations reflect real-world operational demands.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 任务设计直接影响基准测试评估AI系统的有效性。音频异常检测示例通过其具体要求清楚地说明了这种关系：处理连续信号数据、适应变化的噪声条件以及在严格的时间约束内运行。这些实际约束为评估模型性能提供了一个详细的框架，确保评估反映了现实世界的操作需求。
- en: The implementation of a benchmark proceeds systematically from this foundational
    task definition. Each subsequent phase, from dataset selection through deployment,
    builds directly upon these initial specifications, ensuring that evaluations maintain
    consistency while addressing the defined requirements across different approaches
    and implementations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的实施从这一基础任务定义系统地展开。从数据集选择到部署的每个后续阶段，都直接建立在这些初始规范之上，确保评估保持一致性，同时满足不同方法和实现中定义的要求。
- en: Standardized Datasets
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化数据集
- en: Building directly upon the problem definition established in the previous phase,
    standardized datasets provide the essential foundation for training and evaluating
    models. These carefully curated collections ensure all models undergo testing
    under identical conditions, enabling direct comparisons across different approaches
    and architectures. [Figure 12.5](ch018.xhtml#fig-benchmark-components) demonstrates
    this through an audio anomaly detection example, where waveform data serves as
    the standardized input for evaluating detection performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个阶段建立的问题定义的基础上，标准化数据集为训练和评估模型提供了基本的基础。这些精心挑选的集合确保所有模型在相同条件下进行测试，从而能够直接比较不同方法和架构。图12.5([Figure 12.5](ch018.xhtml#fig-benchmark-components))通过音频异常检测示例展示了这一点，其中波形数据作为评估检测性能的标准化输入。
- en: In computer vision, datasets such as [ImageNet](http://www.image-net.org/) ([J.
    Deng et al. 2009](ch058.xhtml#ref-deng2009imagenet)), [COCO](https://cocodataset.org/)
    ([T.-Y. Lin et al. 2014](ch058.xhtml#ref-lin2014microsoft)), and [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)[16](#fn16)
    ([Krizhevsky, Hinton, et al. 2009](ch058.xhtml#ref-krizhevsky2009learning)) serve
    as reference standards. For natural language processing, collections such as [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)[17](#fn17)
    ([Rajpurkar et al. 2016](ch058.xhtml#ref-rajpurkar2016squad)), [GLUE](https://gluebenchmark.com/)[18](#fn18)
    ([A. Wang et al. 2018](ch058.xhtml#ref-wang2018glue)), and [WikiText](https://www.salesforce.com/blog/the-wikitext-long-term-dependency-language-modeling-dataset/)
    ([Merity et al. 2016](ch058.xhtml#ref-merity2016pointer)) fulfill similar functions.
    These datasets encompass a range of complexities and edge cases to thoroughly
    evaluate machine learning systems.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，如[ImageNet](http://www.image-net.org/) ([J. Deng et al. 2009](ch058.xhtml#ref-deng2009imagenet))、[COCO](https://cocodataset.org/)
    ([T.-Y. Lin et al. 2014](ch058.xhtml#ref-lin2014microsoft))和[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)[16](#fn16)
    ([Krizhevsky, Hinton, et al. 2009](ch058.xhtml#ref-krizhevsky2009learning))等数据集作为参考标准。对于自然语言处理，如[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)[17](#fn17)
    ([Rajpurkar et al. 2016](ch058.xhtml#ref-rajpurkar2016squad))、[GLUE](https://gluebenchmark.com/)[18](#fn18)
    ([A. Wang et al. 2018](ch058.xhtml#ref-wang2018glue))和[WikiText](https://www.salesforce.com/blog/the-wikitext-long-term-dependency-language-modeling-dataset/)
    ([Merity et al. 2016](ch058.xhtml#ref-merity2016pointer))等集合也发挥着类似的作用。这些数据集涵盖了各种复杂性和边缘情况，以全面评估机器学习系统。
- en: The strategic selection of datasets, shown early in the workflow of [Figure 12.5](ch018.xhtml#fig-benchmark-components),
    shapes all subsequent implementation steps and ultimately determines the benchmark’s
    effectiveness. In the audio anomaly detection example, the dataset must include
    representative waveform samples of normal operation alongside comprehensive examples
    of various anomalous conditions. Notable examples include datasets like ToyADMOS
    for industrial manufacturing anomalies and Google Speech Commands for general
    sound recognition. Regardless of the specific dataset chosen, the data volume
    must suffice for both model training and validation, while incorporating real-world
    signal characteristics and noise patterns that reflect deployment conditions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图12.5](ch018.xhtml#fig-benchmark-components)的工作流程早期，数据集的战略选择塑造了所有后续的实施步骤，并最终决定了基准的有效性。在音频异常检测的例子中，数据集必须包括正常操作的代表性波形样本，以及各种异常条件的综合示例。值得注意的例子包括用于工业制造异常的ToyADMOS数据集和用于通用声音识别的Google
    Speech Commands。无论选择的具体数据集是什么，数据量必须足够用于模型训练和验证，同时包含反映部署条件的现实世界信号特征和噪声模式。
- en: 'The selection of benchmark datasets directly shapes experimental outcomes and
    model evaluation. Effective datasets must balance two key requirements: accurately
    representing real-world challenges while maintaining sufficient complexity to
    differentiate model performance meaningfully. While research often utilizes simplified
    datasets like ToyADMOS[19](#fn19) ([Koizumi et al. 2019](ch058.xhtml#ref-koizumi2019toyadmos)),
    these controlled environments, though valuable for methodological development,
    may not fully capture real-world deployment complexities.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集的选择直接决定了实验结果和模型评估。有效的数据集必须平衡两个关键要求：准确反映现实世界的挑战，同时保持足够的复杂性，以便有意义地区分模型性能。虽然研究通常使用简化的数据集，如ToyADMOS[19](#fn19)
    ([Koizumi et al. 2019](ch058.xhtml#ref-koizumi2019toyadmos))，但这些受控环境虽然对方法论发展有价值，可能无法完全捕捉现实世界的部署复杂性。
- en: Model Selection
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择
- en: Following dataset specification, the benchmark process advances systematically
    to model architecture selection and implementation. This critical phase establishes
    performance baselines and determines the optimal modeling approach for the specific
    task at hand. The selection process directly builds upon the architectural foundations
    established in [Chapter 4](ch010.xhtml#sec-dnn-architectures) and must account
    for the framework-specific considerations discussed in [Chapter 7](ch013.xhtml#sec-ai-frameworks).
    [Figure 12.5](ch018.xhtml#fig-benchmark-components) illustrates this progression
    through the model selection stage and subsequent training code development.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定数据集之后，基准过程系统地推进到模型架构选择和实现。这一关键阶段建立了性能基准，并确定了特定任务的优化建模方法。选择过程直接建立在[第4章](ch010.xhtml#sec-dnn-architectures)中建立的架构基础上，并必须考虑到[第7章](ch013.xhtml#sec-ai-frameworks)中讨论的框架特定考虑因素。[图12.5](ch018.xhtml#fig-benchmark-components)说明了通过模型选择阶段和随后的训练代码开发这一进展。
- en: Baseline models serve as the reference points for evaluating novel approaches.
    These span from basic implementations, including linear regression for continuous
    predictions and logistic regression for classification tasks, to advanced architectures
    with proven success in comparable domains. The choice of baseline depends critically
    on the deployment framework—a PyTorch implementation may exhibit different performance
    characteristics than its TensorFlow equivalent due to framework-specific optimizations
    and operator implementations. In natural language processing applications, advanced
    language models like BERT[20](#fn20) have emerged as standard benchmarks for comparative
    analysis. The architectural details of transformers and their performance characteristics
    are thoroughly covered in [Chapter 4](ch010.xhtml#sec-dnn-architectures).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型作为评估新方法的标准参考点。这些模型从基本的实现开始，包括用于连续预测的线性回归和用于分类任务的逻辑回归，到在可比领域证明成功的先进架构。基准模型的选择关键取决于部署框架——一个PyTorch实现可能由于其特定的优化和操作实现，与它的TensorFlow等效版本表现出不同的性能特征。在自然语言处理应用中，先进的语言模型如BERT[20](#fn20)已成为比较分析的基准标准。变换器及其性能特征在[第4章](ch010.xhtml#sec-dnn-architectures)中得到了详尽的介绍。
- en: Selecting the right baseline model requires careful evaluation of architectures
    against benchmark requirements. This selection process directly informs the development
    of training code, which is the cornerstone of benchmark reproducibility. The training
    implementation must thoroughly document all aspects of the model pipeline, from
    data preprocessing through training procedures, enabling precise replication of
    model behavior across research teams.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的基线模型需要对架构与基准要求进行仔细评估。这一选择过程直接指导训练代码的开发，这是基准可重复性的基石。训练实现必须详细记录模型管道的所有方面，从数据预处理到训练过程，以便在研究团队间精确复制模型行为。
- en: 'With model architecture selected, model development follows two primary optimization
    paths: training and inference. During training optimization, efforts concentrate
    on achieving target accuracy metrics while operating within computational constraints.
    The training implementation must demonstrate consistent achievement of performance
    thresholds under specified conditions.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构选定后，模型开发遵循两个主要优化路径：训练和推理。在训练优化过程中，努力集中在达到目标精度指标的同时，在计算约束条件下进行。训练实现必须在指定条件下展示对性能阈值的持续达成。
- en: In parallel, the inference optimization path addresses deployment considerations,
    particularly the critical transition from development to production environments.
    A key example involves precision reduction through numerical optimization techniques,
    progressing from high-precision to lower-precision representations to enhance
    deployment efficiency. This process demands careful calibration to maintain model
    accuracy while reducing resource requirements. The benchmark must detail both
    the quantization methodology and verification procedures that confirm preserved
    performance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，推理优化路径关注部署考虑因素，特别是从开发环境到生产环境的临界过渡。一个关键例子是通过数值优化技术降低精度，从高精度表示过渡到低精度表示以提升部署效率。这个过程需要仔细校准，以在降低资源需求的同时保持模型精度。基准测试必须详细说明量化方法和验证程序，以确认性能的保留。
- en: The intersection of these two optimization paths with real-world constraints
    shapes overall deployment strategy. Comprehensive benchmarks must therefore specify
    requirements for both training and inference scenarios, ensuring models maintain
    consistent performance from development through deployment. This crucial connection
    between development and production metrics naturally leads to the establishment
    of evaluation criteria.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这两条优化路径与实际约束条件的交集决定了整体部署策略。因此，全面的基准测试必须明确训练和推理场景的要求，确保模型在开发到部署过程中保持一致的性能。这种开发与生产指标之间的关键联系自然导致评估标准的建立。
- en: 'The optimization process must balance four key objectives: model accuracy,
    computational speed, memory utilization, and energy efficiency. Following our
    three-dimensional benchmarking framework, this complex optimization landscape
    necessitates robust evaluation metrics that can effectively quantify performance
    across algorithmic, system, and data dimensions. As models transition from development
    to deployment, these metrics serve as critical tools for guiding optimization
    decisions and validating performance enhancements.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程必须平衡四个关键目标：模型精度、计算速度、内存利用率和能源效率。遵循我们的三维基准测试框架，这个复杂的优化领域需要强大的评估指标，能够有效量化算法、系统和数据维度的性能。随着模型从开发过渡到部署，这些指标作为指导优化决策和验证性能提升的关键工具。
- en: Evaluation Metrics
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: Building upon the optimization framework established through model selection,
    evaluation metrics provide the quantitative measures needed to assess machine
    learning model performance. These metrics establish objective standards for comparing
    different approaches, allowing researchers and practitioners to gauge solution
    effectiveness. The selection of appropriate metrics represents a critical aspect
    of benchmark design, as they must align with task objectives while providing meaningful
    insights into model behavior across both training and deployment scenarios. Importantly,
    metric computation can vary between frameworks—the training methodologies from
    [Chapter 8](ch014.xhtml#sec-ai-training) demonstrate how different frameworks
    handle loss computation and gradient accumulation differently, affecting reported
    metrics.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择中建立的优化框架基础上，评估指标提供了评估机器学习模型性能所需的定量度量。这些指标为比较不同方法建立了客观标准，使研究人员和实践者能够衡量解决方案的有效性。选择合适的指标是基准设计的关键方面，因为它们必须与任务目标一致，同时为训练和部署场景中的模型行为提供有意义的见解。重要的是，指标的计算可能在框架之间有所不同——[第8章](ch014.xhtml#sec-ai-training)中的训练方法展示了不同的框架如何以不同的方式处理损失计算和梯度累积，这影响了报告的指标。
- en: Task-specific metrics quantify a model’s performance on its intended function.
    For example, classification tasks employ metrics including accuracy (overall correct
    predictions), precision (positive prediction accuracy), recall (positive case
    detection rate), and F1 score (precision-recall harmonic mean) ([Sokolova and
    Lapalme 2009](ch058.xhtml#ref-sokolova2009systematic)). Regression problems utilize
    error measurements like Mean Squared Error (MSE) and Mean Absolute Error (MAE)
    to assess prediction accuracy. Domain-specific applications often require specialized
    metrics - for example, machine translation uses the BLEU score[21](#fn21) to evaluate
    the semantic and syntactic similarity between machine-generated and human reference
    translations ([Papineni et al. 2001](ch058.xhtml#ref-papineni2002bleu)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 任务特定指标量化了模型在其预期功能上的性能。例如，分类任务采用包括准确率（总体正确预测）、精确率（正面预测准确率）、召回率（正面案例检测率）和F1分数（精确率-召回率调和平均）[Sokolova和Lapalme
    2009](ch058.xhtml#ref-sokolova2009systematic)在内的指标。回归问题使用如均方误差（MSE）和平均绝对误差（MAE）等误差测量方法来评估预测准确性。特定领域的应用通常需要专门的指标——例如，机器翻译使用BLEU分数[21](#fn21)来评估机器生成翻译与人工参考翻译之间的语义和句法相似度([Papineni等
    2001](ch058.xhtml#ref-papineni2002bleu))。
- en: However, as models transition from research to production deployment, implementation
    metrics become equally important. Model size, measured in parameters or memory
    footprint, directly affects deployment feasibility across different hardware platforms.
    Processing latency, typically measured in milliseconds per inference, determines
    whether the model meets real-time requirements. Energy consumption, measured in
    watts or joules per inference, indicates operational efficiency. These practical
    considerations reflect the growing need for solutions that balance accuracy with
    computational efficiency. The operational challenges of maintaining these metrics
    in production environments are explored in deployment strategies ([Chapter 13](ch019.xhtml#sec-ml-operations)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着模型从研究阶段过渡到生产部署，实施指标同样重要。模型大小，以参数或内存占用量来衡量，直接影响到不同硬件平台上的部署可行性。处理延迟，通常以每次推理的毫秒数来衡量，决定了模型是否满足实时性要求。能耗，以每次推理的瓦特或焦耳来衡量，表明了操作效率。这些实际考虑反映了在准确性与计算效率之间寻求平衡的解决方案日益增长的需求。在生产环境中维持这些指标的操作挑战在部署策略中得到了探讨（[第13章](ch019.xhtml#sec-ml-operations)）。
- en: Consequently, the selection of appropriate metrics requires careful consideration
    of both task requirements and deployment constraints. A single metric rarely captures
    all relevant aspects of performance in real-world scenarios. For instance, in
    anomaly detection systems, high accuracy alone may not indicate good performance
    if the model generates frequent false alarms. Similarly, a fast model with poor
    accuracy fails to provide practical value.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择合适的指标需要仔细考虑任务需求和部署限制。在现实场景中，单一指标很少能捕捉到性能的所有相关方面。例如，在异常检测系统中，如果模型频繁产生误报，仅高准确率可能并不表明良好的性能。同样，一个速度快但准确率低的模型无法提供实际价值。
- en: '[Figure 12.5](ch018.xhtml#fig-benchmark-components) demonstrates this multi-metric
    evaluation approach. The anomaly detection system reports performance across multiple
    dimensions: model size (270 Kparameters), processing speed (10.4 ms/inference),
    and detection accuracy (0.86 AUC[22](#fn22)). This combination of metrics ensures
    the model meets both technical and operational requirements in real-world deployment
    scenarios.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12.5](ch018.xhtml#fig-benchmark-components)展示了这种多指标评估方法。异常检测系统在多个维度上报告性能：模型大小（270
    K参数）、处理速度（10.4 ms/推理）和检测精度（0.86 AUC[22](#fn22)）。这些指标的结合确保模型在现实世界的部署场景中满足技术和操作要求。'
- en: Benchmark Harness
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准工具链
- en: While evaluation metrics provide the measurement framework, a benchmark harness
    implements the systematic infrastructure for evaluating model performance under
    controlled conditions. This critical component ensures reproducible testing by
    managing how inputs are delivered to the system under test and how measurements
    are collected, effectively transforming theoretical metrics into quantifiable
    measurements.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然评估指标提供了测量框架，但基准工具链实现了在受控条件下评估模型性能的系统基础设施。这个关键组件通过管理输入如何传递给被测系统以及如何收集测量数据，有效地将理论指标转化为可量化的测量。
- en: The harness design should align with the intended deployment scenario and usage
    patterns. For server deployments, the harness implements request patterns that
    simulate real-world traffic, typically generating inputs using a Poisson distribution[23](#fn23)
    to model random but statistically consistent server workloads. The harness manages
    concurrent requests and varying load intensities to evaluate system behavior under
    different operational conditions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 工具链的设计应与预期的部署场景和用法模式相一致。对于服务器部署，工具链实现模拟真实世界流量的请求模式，通常使用泊松分布[23](#fn23)来模拟随机但统计上一致的服务器工作负载。工具链管理并发请求和不同的负载强度，以评估系统在不同操作条件下的行为。
- en: For embedded and mobile applications, the harness generates input patterns that
    reflect actual deployment conditions. This might involve sequential image injection
    for mobile vision applications or synchronized multi-sensor streams for autonomous
    systems. Such precise input generation and timing control ensures the system experiences
    realistic operational patterns, revealing performance characteristics that would
    emerge in actual device deployment.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入式和移动应用，工具链生成反映实际部署条件的输入模式。这可能涉及为移动视觉应用进行顺序图像注入，或为自主系统提供同步的多传感器流。这种精确的输入生成和时序控制确保系统经历真实的操作模式，揭示在实际设备部署中出现的性能特征。
- en: The harness must also accommodate different throughput models. Batch processing
    scenarios require the ability to evaluate system performance on large volumes
    of parallel inputs, while real-time applications need precise timing control for
    sequential processing. [Figure 12.5](ch018.xhtml#fig-benchmark-components) illustrates
    this in the embedded implementation phase, where the harness must support precise
    measurement of inference time and energy consumption per operation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 工具链还必须适应不同的吞吐量模型。批量处理场景需要评估系统在大量并行输入上的性能，而实时应用则需要精确的时间控制以进行顺序处理。[图12.5](ch018.xhtml#fig-benchmark-components)在嵌入式实现阶段展示了这一点，其中工具链必须支持对每个操作推理时间和能耗的精确测量。
- en: Reproducibility demands that the harness maintain consistent testing conditions
    across different evaluation runs. This includes controlling environmental factors
    such as background processes, thermal conditions, and power states that might
    affect performance measurements. The harness must also provide mechanisms for
    collecting and logging performance metrics without significantly impacting the
    system under test.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复性要求在各个评估运行中，工具链保持一致的测试条件。这包括控制可能影响性能测量的环境因素，如后台进程、热条件和电源状态。工具链还必须提供机制，以便在不显著影响被测系统的情况下收集和记录性能指标。
- en: System Specifications
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统规格
- en: Complementing the benchmark harness that controls test execution, system specifications
    are fundamental components of machine learning benchmarks that directly impact
    model performance, training time, and experimental reproducibility. These specifications
    encompass the complete computational environment, ensuring that benchmarking results
    can be properly contextualized, compared, and reproduced by other researchers.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 补充控制测试执行的基准工具，系统规格是机器学习基准的基本组成部分，直接影响模型性能、训练时间和实验的可重复性。这些规格涵盖了完整的计算环境，确保基准测试结果可以被其他研究人员正确地情境化、比较和复制。
- en: 'Hardware specifications typically include:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件规格通常包括：
- en: Processor type and speed (e.g., CPU model, clock rate)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理器类型和速度（例如，CPU型号，时钟速率）
- en: GPUs, or TPUs, including model, memory capacity, and quantity if used for distributed
    training
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU或TPU，包括如果用于分布式训练则包括模型、内存容量和数量
- en: Memory capacity and type (e.g., RAM size, DDR4)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存容量和类型（例如，RAM大小，DDR4）
- en: Storage type and capacity (e.g., SSD, HDD)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储类型和容量（例如，SSD，HDD）
- en: Network configuration, if relevant for distributed computing
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果与分布式计算相关，则包括网络配置
- en: 'Software specifications generally include:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 软件规格通常包括：
- en: Operating system and version
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 操作系统和版本
- en: Programming language and version
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编程语言和版本
- en: Machine learning frameworks and libraries (e.g., TensorFlow, PyTorch) with version
    numbers
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习框架和库（例如，TensorFlow，PyTorch）及其版本号
- en: Compiler information and optimization flags
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译器信息和优化标志
- en: Custom software or scripts used in the benchmark process
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基准过程中使用的自定义软件或脚本
- en: Environment management tools and configuration (e.g., Docker containers[24](#fn24),
    virtual environments)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境管理工具和配置（例如，Docker容器[24](#fn24)，虚拟环境）
- en: The precise documentation of these specifications is essential for experimental
    validity and reproducibility. This documentation enables other researchers to
    replicate the benchmark environment with high fidelity, provides critical context
    for interpreting performance metrics, and facilitates understanding of resource
    requirements and scaling characteristics across different models and tasks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规格的精确文档对于实验的有效性和可重复性至关重要。此文档使其他研究人员能够以高保真度复制基准环境，为解释性能指标提供关键背景，并促进对不同模型和任务资源需求和扩展特性的理解。
- en: In many cases, benchmarks may include results from multiple hardware configurations
    to provide a more comprehensive view of model performance across different computational
    environments. This approach is particularly valuable as it highlights the trade-offs
    between model complexity, computational resources, and performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，基准测试可能包括来自多个硬件配置的结果，以提供更全面的模型性能视图，涵盖不同的计算环境。这种方法特别有价值，因为它突出了模型复杂性、计算资源和性能之间的权衡。
- en: 'As the field evolves, hardware and software specifications increasingly incorporate
    detailed energy consumption metrics and computational efficiency measures, such
    as FLOPS/watt and total power usage over training time. This expansion reflects
    growing concerns about the environmental impact of large-scale machine learning
    models and supports the development of more sustainable AI practices. Comprehensive
    specification documentation thus serves multiple purposes: enabling reproducibility,
    supporting fair comparisons, and advancing both the technical and environmental
    aspects of machine learning research.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随着该领域的发展，硬件和软件规格越来越多地纳入详细的能耗指标和计算效率措施，如FLOPS/watt和训练时间内的总功耗。这种扩展反映了人们对大规模机器学习模型环境影响的日益关注，并支持更可持续的AI实践的发展。因此，全面的规格文档具有多重作用：实现可重复性、支持公平比较，并推进机器学习研究的技术和环境方面。
- en: Run Rules
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行规则
- en: Beyond the technical infrastructure, run rules establish the procedural framework
    that ensures benchmark results can be reliably replicated by researchers and practitioners,
    complementing the technical environment defined by system specifications. These
    guidelines are essential for validating research claims, building upon existing
    work, and advancing machine learning. Central to reproducibility in AI benchmarks
    is the management of controlled randomness, the systematic handling of stochastic
    processes such as weight initialization and data shuffling that ensures consistent,
    verifiable results.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术基础设施之外，运行规则建立了确保研究人员和实践者可以可靠地复制基准结果的程序框架，补充了由系统规范定义的技术环境。这些指南对于验证研究主张、建立在现有工作之上以及推进机器学习至关重要。在人工智能基准中的可重复性核心是控制随机性的管理，系统地处理权重初始化和数据洗牌等随机过程，以确保一致、可验证的结果。
- en: Comprehensive documentation of hyperparameters forms a critical component of
    reproducibility. Hyperparameters are configuration settings that control how models
    learn, such as learning rates and batch sizes, which must be documented for reproducibility.
    Given that minor hyperparameter adjustments can significantly impact model performance,
    their precise documentation is essential. Benchmarks mandate the preservation
    and sharing of training and evaluation datasets. When direct data sharing is restricted
    by privacy or licensing constraints, benchmarks must provide detailed specifications
    for data preprocessing and selection criteria, enabling researchers to construct
    comparable datasets or understand the characteristics of the original experimental
    data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对超参数的全面文档是可重复性的关键组成部分。超参数是控制模型如何学习的配置设置，例如学习率和批量大小，这些必须为了可重复性而记录。鉴于微小的超参数调整可能会显著影响模型性能，其精确的文档至关重要。基准要求保存和共享训练和评估数据集。当直接数据共享受到隐私或许可约束的限制时，基准必须提供详细的数据预处理和选择标准的规范，使研究人员能够构建可比较的数据集或了解原始实验数据的特点。
- en: Code provenance and availability constitute another vital aspect of reproducibility
    guidelines. Contemporary benchmarks typically require researchers to publish implementation
    code in version-controlled repositories, encompassing not only the model implementation
    but also comprehensive scripts for data preprocessing, training, and evaluation.
    Advanced benchmarks often provide containerized environments that encapsulate
    all dependencies and configurations. Detailed experimental logging is mandatory,
    including systematic recording of training metrics, model checkpoints, and documentation
    of any experimental adjustments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 代码来源和可用性是可重复性指南的另一个重要方面。当代基准通常要求研究人员在版本控制的存储库中发布实现代码，这不仅包括模型实现，还包括数据预处理、训练和评估的全面脚本。高级基准通常提供封装了所有依赖项和配置的容器化环境。详细的实验日志是强制性的，包括系统地记录训练指标、模型检查点和任何实验调整的文档。
- en: 'These reproducibility guidelines serve multiple crucial functions: they enhance
    transparency, enable rigorous peer review, and accelerate scientific progress
    in AI research. By following these protocols, the research community can effectively
    verify results, iterate on successful approaches, and identify methodological
    limitations. In the rapidly evolving landscape of machine learning, these robust
    reproducibility practices form the foundation for reliable and progressive research.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可重复性指南发挥着多个关键作用：它们增强了透明度，使严格的同行评审成为可能，并加速了人工智能研究中的科学进步。通过遵循这些协议，研究界可以有效地验证结果，迭代成功的方案，并确定方法论限制。在快速发展的机器学习领域，这些稳健的可重复性实践构成了可靠和进步研究的基础。
- en: Result Interpretation
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果解释
- en: Building on the foundation established by run rules, result interpretation guidelines
    provide the essential framework for understanding and contextualizing benchmark
    outcomes. These guidelines help researchers and practitioners draw meaningful
    conclusions from benchmark results, ensuring fair and informative comparisons
    between different models or approaches. A critical aspect is understanding the
    statistical significance of performance differences. Benchmarks typically specify
    protocols for conducting statistical tests and reporting confidence intervals,
    enabling practitioners to distinguish between meaningful improvements and variations
    attributable to random factors.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行规则的基础上建立起来的结果解释指南为理解和情境化基准结果提供了必要的框架。这些指南帮助研究人员和从业者从基准结果中得出有意义的结论，确保在不同模型或方法之间进行公平和有信息量的比较。一个关键方面是理解性能差异的统计显著性。基准通常指定进行统计测试和报告置信区间的协议，使从业者能够区分有意义的改进和归因于随机因素的变异。
- en: However, result interpretation requires careful consideration of real-world
    applications and context. While a 1% improvement in accuracy might be crucial
    for medical diagnostics or financial systems, other applications might prioritize
    inference speed or model efficiency over marginal accuracy gains. Understanding
    these context-specific requirements is essential for meaningful interpretation
    of benchmark results. Users must also recognize inherent benchmark limitations,
    as no single evaluation framework can encompass all possible use cases. Common
    limitations include dataset biases, task-specific characteristics, and constraints
    of evaluation metrics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结果解释需要仔细考虑现实世界的应用和背景。虽然准确性提高1%对于医疗诊断或金融系统可能至关重要，但其他应用可能更重视推理速度或模型效率，而不是微小的准确性提升。理解这些特定需求的背景对于基准结果的有意义解释至关重要。用户还必须认识到基准固有的局限性，因为没有任何单一的评估框架可以涵盖所有可能的使用案例。常见的局限性包括数据集偏差、任务特定特征和评估指标的约束。
- en: Modern benchmarks often necessitate multi-dimensional analysis across various
    performance metrics. For instance, when a model demonstrates superior accuracy
    but requires substantially more computational resources, interpretation guidelines
    help practitioners evaluate these trade-offs based on their specific constraints
    and requirements. The guidelines also address the critical issue of benchmark
    overfitting, where models might be excessively optimized for specific benchmark
    tasks at the expense of real-world generalization. To mitigate this risk, guidelines
    often recommend evaluating model performance on related but distinct tasks and
    considering practical deployment scenarios.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现代基准通常需要在多个性能指标上进行多维分析。例如，当一个模型表现出优越的准确性但需要大量计算资源时，解释指南帮助从业者根据他们特定的约束和要求评估这些权衡。指南还解决了基准过度拟合的关键问题，即模型可能为了特定基准任务而过度优化，从而牺牲了现实世界的泛化能力。为了减轻这种风险，指南通常建议在相关但不同的任务上评估模型性能，并考虑实际部署场景。
- en: 'These comprehensive interpretation frameworks ensure that benchmarks serve
    their intended purpose: providing standardized performance measurements while
    enabling nuanced understanding of model capabilities. This balanced approach supports
    evidence-based decision-making in both research contexts and practical machine
    learning applications.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些综合的解释框架确保基准能够实现其预期目的：提供标准化的性能测量，同时允许对模型能力进行细微的理解。这种平衡的方法支持在研究环境和实际机器学习应用中的基于证据的决策。
- en: Example Benchmark
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例基准
- en: To illustrate how these components work together in practice, a complete benchmark
    run evaluates system performance by synthesizing multiple components under controlled
    conditions to produce reproducible measurements. [Figure 12.5](ch018.xhtml#fig-benchmark-components)
    illustrates this integration through an audio anomaly detection system. It shows
    how performance metrics are systematically measured and reported within a framework
    that encompasses problem definition, datasets, model selection, evaluation criteria,
    and standardized run rules.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些组件在实际中是如何协同工作的，一个完整的基准运行通过在受控条件下综合多个组件来生成可重复的测量值，从而评估系统性能。[图12.5](ch018.xhtml#fig-benchmark-components)通过一个音频异常检测系统展示了这种集成。它展示了在涵盖问题定义、数据集、模型选择、评估标准和标准化运行规则框架内，性能指标是如何系统性地测量和报告的。
- en: The benchmark measures several key performance dimensions. For computational
    resources, the system reports a model size of 270 Kparameters and requires 10.4
    milliseconds per inference. For task effectiveness, it achieves a detection accuracy
    of 0.86 AUC (Area Under Curve) in distinguishing normal from anomalous audio patterns.
    For operational efficiency, it consumes 516 µJ of energy per inference.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准测量了几个关键性能维度。对于计算资源，系统报告模型大小为270 K个参数，每次推理需要10.4毫秒。对于任务有效性，它实现了0.86 AUC（曲线下面积）的检测精度，以区分正常和异常的音频模式。对于操作效率，每次推理消耗516
    µJ的能量。
- en: The relative importance of these metrics varies by deployment context. Energy
    consumption per inference is critical for battery-powered devices but less consequential
    for systems with constant power supply. Model size constraints differ significantly
    between cloud deployments with abundant resources and embedded devices with limited
    memory. Processing speed requirements depend on whether the system must operate
    in real-time or can process data in batches.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标相对于部署环境的重要性各不相同。对于电池供电的设备，每推理一次的能量消耗至关重要，但对于恒定电源的系统来说则不那么重要。模型大小限制在资源丰富的云部署和内存有限的嵌入式设备之间差异很大。处理速度要求取决于系统是否必须在实时操作或在批量处理数据。
- en: The benchmark reveals inherent trade-offs between performance metrics in machine
    learning systems. For instance, reducing the model size from 270 Kparameters might
    improve processing speed and energy efficiency but could decrease the 0.86 AUC
    detection accuracy. [Figure 12.5](ch018.xhtml#fig-benchmark-components) illustrates
    how these interconnected metrics contribute to overall system performance in the
    deployment phase.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准揭示了机器学习系统中性能指标之间的固有权衡。例如，将模型大小从270 K个参数减少可能会提高处理速度和能源效率，但可能会降低0.86 AUC检测精度。[图12.5](ch018.xhtml#fig-benchmark-components)说明了这些相互关联的指标如何有助于部署阶段的整体系统性能。
- en: Ultimately, whether these measurements constitute a “passing” benchmark depends
    on the specific requirements of the intended application. The benchmark framework
    provides the structure and methodology for consistent evaluation, while the acceptance
    criteria must align with deployment constraints and performance requirements.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这些测量是否构成一个“通过”的基准取决于预期应用的特定要求。基准测试框架提供了持续评估的结构和方法，而验收标准必须与部署限制和性能要求相一致。
- en: Compression Benchmarks
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 压缩基准
- en: Extending beyond general benchmarking principles, as machine learning models
    continue to grow in size and complexity, neural network compression has emerged
    as a critical optimization technique for deployment across resource-constrained
    environments. Compression benchmarking methodologies evaluate the effectiveness
    of techniques including pruning, quantization, knowledge distillation, and architecture
    optimization. These specialized benchmarks measure the core trade-offs between
    model size reduction, accuracy preservation, and computational efficiency improvements.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 超越了一般的基准测试原则，随着机器学习模型在规模和复杂性上的增长，神经网络压缩已成为在资源受限环境中部署的关键优化技术。压缩基准测试方法评估包括剪枝、量化、知识蒸馏和架构优化在内的技术效果。这些专业基准测量了模型大小减少、精度保持和计算效率改进之间的核心权衡。
- en: 'Model compression benchmarks assess multiple dimensions simultaneously. The
    primary dimension involves size reduction metrics that evaluate parameters (counting),
    memory footprint (bytes), and storage requirements (compressed file size). Effective
    compression achieves significant reduction while maintaining accuracy: MobileNetV2
    achieves approximately 72% ImageNet top-1 accuracy with 3.4 million parameters
    versus ResNet-50’s 76% accuracy with 25.6 million parameters, representing a 7.5x
    efficiency improvement in the parameter-to-accuracy ratio.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩基准同时评估多个维度。主要维度涉及尺寸减少指标，评估参数（计数）、内存占用（字节）和存储需求（压缩文件大小）。有效的压缩在保持精度的同时实现了显著减少：MobileNetV2在3.4百万参数下实现了约72%的ImageNet
    top-1精度，而ResNet-50在2560万个参数下实现了76%的精度，参数到精度比率的效率提高了7.5倍。
- en: Beyond basic size metrics, sparsity evaluation frameworks distinguish between
    structured and unstructured pruning efficiency. Structured pruning removes entire
    neurons or filters, achieving consistent speedups but typically lower compression
    ratios (2-4x). Unstructured pruning eliminates individual weights, achieving higher
    compression ratios (10-100x) but requiring specialized sparse computation support
    for speedup realization. Benchmark protocols must specify hardware platform and
    software implementation to ensure meaningful sparse acceleration measurements.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的大小指标外，稀疏评估框架区分了结构化和非结构化剪枝效率。结构化剪枝移除整个神经元或滤波器，实现一致的加速但通常压缩比较低（2-4倍）。非结构化剪枝消除单个权重，实现更高的压缩比（10-100倍），但需要专门的稀疏计算支持以实现加速。基准测试协议必须指定硬件平台和软件实现，以确保有意义的稀疏加速测量。
- en: 'Complementing sparsity techniques, quantization benchmarking protocols evaluate
    precision reduction techniques across multiple data types. INT8 quantization typically
    provides 4x memory reduction and 2-4x inference speedup while maintaining 99%+
    accuracy preservation for most computer vision models. Mixed-precision approaches
    achieve optimal efficiency by applying different precision levels to different
    layers: critical layers retain FP16 precision while computation-heavy layers utilize
    INT8 or INT4, enabling fine-grained efficiency optimization.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 补充稀疏技术，量化基准测试协议评估了跨多种数据类型的精度降低技术。INT8量化通常提供4倍内存减少和2-4倍推理速度提升，同时保持99%以上的精度保留，适用于大多数计算机视觉模型。混合精度方法通过将不同精度级别应用于不同层来实现最佳效率：关键层保留FP16精度，而计算密集层使用INT8或INT4，从而实现细粒度效率优化。
- en: Another critical dimension involves knowledge transfer effectiveness metrics
    that measure performance relationships between different model sizes. Successful
    knowledge transfer achieves 90-95% of larger model accuracy while reducing model
    size by 5-10x. Compact models can demonstrate this approach, achieving high performance
    with significantly fewer parameters and faster inference, illustrating the potential
    for efficiency without significant capability loss.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键维度涉及知识迁移有效性指标，这些指标衡量不同模型大小之间的性能关系。成功知识迁移实现了较大模型90-95%的精度，同时将模型大小减少5-10倍。紧凑模型可以展示这种方法，以显著较少的参数和更快的推理实现高性能，展示了在显著能力损失不大的情况下提高效率的潜力。
- en: 'Finally, acceleration factor measurements for optimized models reveal the practical
    benefits across different hardware platforms. Optimized models achieve varying
    speedup factors: sparse models deliver 2-5x speedup on CPUs, reduced-precision
    models achieve 2-8x speedup on mobile processors, and efficient architectures
    provide 5-20x speedup on specialized edge accelerators. These hardware-specific
    measurements ensure efficiency benchmarks reflect real deployment scenarios.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，优化模型的加速因子测量揭示了在不同硬件平台上的实际益处。优化模型实现了不同的加速因子：稀疏模型在CPU上提供2-5倍的加速，降低精度模型在移动处理器上实现2-8倍的加速，而高效架构在专用边缘加速器上提供5-20倍的加速。这些针对特定硬件的测量确保效率基准反映了实际的部署场景。
- en: Efficiency-aware benchmarking addresses critical gaps in traditional evaluation
    frameworks. Current benchmark suites like MLPerf focus primarily on dense, unoptimized
    models that do not represent production deployments, where optimized models are
    ubiquitous. Future benchmarking frameworks should include efficiency model divisions
    specifically evaluating optimized architectures, reduced-precision inference,
    and compact models to accurately reflect real deployment practices and guide efficiency
    research toward practical impact.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 效率感知基准测试解决了传统评估框架中的关键差距。当前的基准测试套件，如MLPerf，主要关注密集型、未优化的模型，这些模型并不代表生产部署，而在生产部署中优化模型无处不在。未来的基准测试框架应包括专门评估优化架构、降低精度推理和紧凑模型的效率模型分类，以准确反映实际部署实践并指导效率研究向实际影响方向发展。
- en: Mobile and Edge Benchmarks
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移动和边缘基准测试
- en: 'Mobile SoCs integrate heterogeneous processors (CPU, GPU, DSP, NPU) requiring
    specialized benchmarking that captures workload distribution complexity while
    accounting for thermal and battery constraints. Effective processor coordination
    achieves 3-5x performance improvements, but sustained workloads trigger thermal
    throttling. Snapdragon 8 Gen 3 drops from 35 TOPS peak to 20 TOPS sustained. Battery
    impact varies dramatically: computational photography consumes 2-5W while background
    AI requires 5-50mW for acceptable endurance.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 移动系统级芯片（SoC）集成了异构处理器（CPU、GPU、DSP、NPU），需要专门的基准测试来捕捉工作负载分布复杂性，同时考虑到热和电池限制。有效的处理器协调可以实现3-5倍的性能提升，但持续的工作负载会触发热限制。Snapdragon
    8 Gen 3的峰值TOPS从35下降到20，而持续性能。电池影响差异很大：计算摄影消耗2-5W，而后台AI需要5-50mW才能达到可接受的续航能力。
- en: Mobile benchmarking must also evaluate 5G/WiFi edge-cloud coordination, with
    URLLC[25](#fn25) demanding <1ms latency for critical applications. Automotive
    deployments add ASIL validation, multi-sensor fusion, and -40°C to +85°C environmental
    testing. These unique requirements necessitate comprehensive frameworks evaluating
    sustained performance under thermal constraints, battery efficiency across usage
    patterns, and connectivity-dependent behavior, extending beyond isolated peak
    measurements.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 移动基准测试还必须评估5G/WiFi边缘云的协调，URLLC[25](#fn25)对关键应用要求<1ms的延迟。汽车部署增加了ASIL验证、多传感器融合以及-40°C至+85°C的环境测试。这些独特的要求需要综合框架来评估在热约束下的持续性能、使用模式下的电池效率以及依赖连接性的行为，这些评估超越了孤立峰值测量的范畴。
- en: Training vs. Inference Evaluation
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与推理评估
- en: 'The benchmark components and granularity levels apply differently to ML systems’
    two primary operational phases: training and inference. While both phases process
    data through neural networks, their contrasting objectives create distinct benchmarking
    requirements. The training methodologies from [Chapter 8](ch014.xhtml#sec-ai-training)
    focus on iterative optimization over large datasets, while deployment strategies
    from [Chapter 13](ch019.xhtml#sec-ml-operations) prioritize consistent, low-latency
    serving. These differences cascade through metric selection, resource allocation,
    and scaling behavior.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试组件和粒度级别在机器学习系统的两个主要操作阶段：训练和推理中应用不同。虽然这两个阶段都通过神经网络处理数据，但它们不同的目标创造了不同的基准测试要求。[第8章](ch014.xhtml#sec-ai-training)中的训练方法侧重于在大数据集上的迭代优化，而[第13章](ch019.xhtml#sec-ml-operations)中的部署策略则优先考虑一致、低延迟的服务。这些差异贯穿于指标选择、资源分配和扩展行为。
- en: Training involves iterative optimization with bidirectional computation (forward
    and backward passes), while inference performs single forward passes with fixed
    model parameters. ResNet-50 training requires 8GB GPU memory for gradients and
    optimizer states compared to 0.5GB for inference-only forward passes. Training
    GPT-3 utilized 1024 A100 GPUs for months, while inference deploys single models
    across thousands of concurrent requests with millisecond response requirements.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练涉及迭代优化和双向计算（正向和反向传递），而推理则执行单次正向传递并使用固定的模型参数。ResNet-50的训练需要8GB GPU内存来存储梯度优化器状态，相比之下，仅推理的正向传递只需要0.5GB。训练GPT-3使用了1024个A100
    GPU数月，而推理则部署单个模型，在数千个并发请求中运行，并满足毫秒级响应要求。
- en: Training prioritizes throughput and convergence speed, measured in samples processed
    per unit time and training completion time. BERT-Large training achieves optimal
    performance at batch size 512 with 32-hour convergence time, while BERT inference
    optimizes for <10ms latency per query with batch size 1-4\. Training can sacrifice
    latency for throughput (processing 10,000 samples/second), while inference sacrifices
    throughput for latency consistency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 训练优先考虑吞吐量和收敛速度，以每单位时间内处理的样本数和训练完成时间来衡量。BERT-Large在批处理大小为512时达到最佳性能，收敛时间为32小时，而BERT推理通过批处理大小为1-4来优化每个查询的<10ms延迟。训练可以牺牲延迟以换取吞吐量（每秒处理10,000个样本），而推理则牺牲吞吐量以保持延迟一致性。
- en: Training can leverage extensive computational resources with batch processing,
    accepting longer completion times for better resource efficiency. Multi-node training
    scales efficiently with batch sizes 4096-32,768, achieving 90% compute utilization.
    Inference must respond to individual requests with minimal latency, constraining
    batch sizes to 1-16 for real-time applications, resulting in 15-40% GPU utilization
    but meeting strict latency requirements.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可以利用大量的计算资源进行批量处理，接受更长的完成时间以实现更好的资源效率。多节点训练在批量大小为4096-32,768时效率高，实现了90%的计算利用率。推理必须以最小的延迟响应单个请求，这限制了批量大小为1-16，适用于实时应用，导致GPU利用率在15-40%，但满足了严格的延迟要求。
- en: Training requires simultaneous access to parameters, gradients, optimizer states,
    and activations, creating 3-4x memory overhead compared to inference. Mixed-precision
    training (FP16/FP32) reduces memory usage by 50% while maintaining convergence,
    whereas inference can utilize INT8 quantization for 4x memory reduction with minimal
    accuracy loss.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要同时访问参数、梯度、优化器状态和激活，与推理相比，内存开销增加了3-4倍。混合精度训练（FP16/FP32）通过保持收敛的同时将内存使用量减少50%，而推理可以利用INT8量化实现4倍的内存减少，同时精度损失最小。
- en: Training employs gradient compression, mixed-precision training, and progressive
    pruning during optimization, achieving 1.8x speedup with 0.1% accuracy loss. Inference
    optimization utilizes post-training quantization (4x speedup), knowledge distillation
    (5-10x model size reduction), and neural architecture search, delivering 4x inference
    speedup with 0.5% accuracy degradation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中采用梯度压缩、混合精度训练和渐进式剪枝进行优化，在0.1%的精度损失下实现了1.8倍的速度提升。推理优化利用了训练后量化（4倍速度提升）、知识蒸馏（5-10倍模型尺寸缩减）和神经架构搜索，在0.5%的精度下降下实现了4倍的推理速度提升。
- en: 'Training energy costs are amortized across model lifetime and measured in total
    energy per trained model. GPT-3 training consumed approximately 1,287 MWh over
    several months. Inference energy costs accumulate per query and directly impact
    operational efficiency: transformer inference consumes 0.01-0.1 Wh per query,
    making energy optimization critical for billion-query services.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 训练能耗在模型生命周期内分摊，并以每训练一个模型的总能耗来衡量。GPT-3的训练在数月内消耗了大约1,287 MWh。推理能耗按查询累积，直接影响运营效率：变压器推理每个查询消耗0.01-0.1
    Wh，这使得能源优化对于亿级查询服务至关重要。
- en: This comparative framework guides benchmark design by highlighting which metrics
    matter most for each phase and how evaluation methodologies should differ to capture
    phase-specific performance characteristics. Training benchmarks emphasize convergence
    time and scaling efficiency, while inference benchmarks prioritize latency consistency
    and resource efficiency across diverse deployment scenarios.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此比较框架通过突出每个阶段最重要的指标以及评估方法应该如何不同来指导基准设计，以捕捉特定阶段的性能特征。训练基准强调收敛时间和扩展效率，而推理基准则优先考虑不同部署场景下的延迟一致性和资源效率。
- en: Training Benchmarks
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练基准
- en: Building on our three-dimensional benchmarking framework, training benchmarks
    focus on evaluating the efficiency, scalability, and resource demands during model
    training. They allow practitioners to assess how different design choices, including
    model architectures, data loading mechanisms, hardware configurations, and distributed
    training strategies, impact performance across the system dimension of our framework.
    These benchmarks are particularly vital as machine learning systems grow in scale,
    requiring billions of parameters, terabytes of data, and distributed computing
    environments.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的三维基准测试框架的基础上，训练基准专注于评估模型训练过程中的效率、可扩展性和资源需求。它们允许从业者评估不同的设计选择，包括模型架构、数据加载机制、硬件配置和分布式训练策略，如何影响我们框架的系统维度性能。随着机器学习系统规模的扩大，需要数十亿个参数、数TB的数据和分布式计算环境，这些基准尤其重要。
- en: For instance, large-scale models like [OpenAI’s GPT-3](https://arxiv.org/abs/2005.14165)[26](#fn26)
    ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)), which consists of
    175 billion parameters trained on 45 terabytes of data, highlight the immense
    computational demands of modern training. Training benchmarks provide systematic
    evaluation of the underlying systems to ensure that hardware and software configurations
    can meet these unprecedented demands efficiently.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像[OpenAI的GPT-3](https://arxiv.org/abs/2005.14165)[26](#fn26) ([T. Brown et
    al. 2020](ch058.xhtml#ref-brown2020language))这样的大型模型，它由1750亿个参数组成，在4500TB的数据上进行训练，突显了现代训练的巨大计算需求。训练基准测试对底层系统进行系统评估，以确保硬件和软件配置能够高效地满足这些前所未有的需求。
- en: '***ML Training Benchmarks*** are standardized evaluations of the *training
    phase*, measuring *time-to-accuracy*, *scaling efficiency*, and *resource utilization*
    to assess training infrastructure and distributed training performance.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**ML Training Benchmarks**是对**训练阶段**的标准化评估，衡量**时间到准确度**、**扩展效率**和**资源利用率**，以评估训练基础设施和分布式训练性能。'
- en: Beyond computational demands, efficient data storage and delivery during training
    also play a major role in the training process. For instance, in a machine learning
    model that predicts bounding boxes around objects in an image, thousands of images
    may be required. However, loading an entire image dataset into memory is typically
    infeasible, so practitioners rely on data loaders from ML frameworks. Successful
    model training depends on timely and efficient data delivery, making it essential
    to benchmark tools like data pipelines, preprocessing speed, and storage retrieval
    times to understand their impact on training performance.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算需求之外，在训练过程中高效的数据存储和交付也起着至关重要的作用。例如，在一个预测图像中物体周围边界框的机器学习模型中，可能需要数千张图像。然而，将整个图像数据集加载到内存中通常是不可行的，因此从业者依赖于来自机器学习框架的数据加载器。成功的模型训练依赖于及时高效的数据交付，这使得对数据管道、预处理速度和存储检索时间等基准工具进行基准测试，以了解它们对训练性能的影响至关重要。
- en: 'In addition to data pipeline efficiency, hardware selection represents another
    key factor in training machine learning systems, as it can significantly impact
    training time. Training benchmarks evaluate CPU, GPU, memory, and network utilization
    during the training phase to guide system optimizations. Understanding how resources
    are used is essential: Are GPUs being fully leveraged? Is there unnecessary memory
    overhead? Benchmarks can uncover bottlenecks or inefficiencies in resource utilization,
    leading to cost savings and performance improvements.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据管道效率之外，硬件选择也是训练机器学习系统中的另一个关键因素，因为它可以显著影响训练时间。训练基准测试评估训练阶段中的CPU、GPU、内存和网络利用率，以指导系统优化。了解资源的使用情况至关重要：GPU是否被充分利用？是否存在不必要的内存开销？基准测试可以发现资源利用中的瓶颈或不效率，从而实现成本节约和性能提升。
- en: In many cases, using a single hardware accelerator, such as a single GPU, is
    insufficient to meet the computational demands of large-scale model training.
    Machine learning models are often trained in data centers with multiple GPUs or
    TPUs, where distributed computing enables parallel processing across nodes. Training
    benchmarks assess how efficiently the system scales across multiple nodes, manages
    data sharding, and handles challenges like node failures or drop-offs during training.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，使用单个硬件加速器，例如单个GPU，不足以满足大规模模型训练的计算需求。机器学习模型通常在拥有多个GPU或TPUs的数据中心进行训练，分布式计算使得节点间能够进行并行处理。训练基准测试评估系统在多个节点上扩展的效率、数据分片的管理以及处理训练过程中节点故障或掉线等挑战的能力。
- en: To illustrate these benchmarking principles, we will reference [MLPerf Training](https://mlcommons.org/benchmarks/training/)
    throughout this section. MLPerf, introduced earlier in [Section 12.2](ch018.xhtml#sec-benchmarking-ai-historical-context-1c54),
    provides the standardized framework we reference throughout this analysis of training
    benchmarks.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些基准测试原则，我们将在本节中引用[MLPerf Training](https://mlcommons.org/benchmarks/training/)。MLPerf，如[第12.2节](ch018.xhtml#sec-benchmarking-ai-historical-context-1c54)中较早介绍的那样，为我们提供了在分析训练基准测试过程中的标准化框架。
- en: Training Benchmark Motivation
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练基准测试的动机
- en: From a systems perspective, training machine learning models represents a computationally
    intensive process that requires careful optimization of resources. Training benchmarks
    serve as essential tools for evaluating system efficiency, identifying bottlenecks,
    and ensuring that machine learning systems can scale effectively. They provide
    a standardized approach to measuring how various system components, including
    hardware accelerators, memory, storage, and network infrastructure, affect training
    performance.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，训练机器学习模型代表了一个计算密集型过程，需要仔细优化资源。训练基准是评估系统效率、识别瓶颈和确保机器学习系统可以有效地扩展的基本工具。它们提供了一种标准化的方法来衡量各种系统组件，包括硬件加速器、内存、存储和网络基础设施，如何影响训练性能。
- en: Consequently, training benchmarks allow researchers and engineers to push the
    state-of-the-art, optimize configurations, improve scalability, and reduce overall
    resource consumption by systematically evaluating these factors. As shown in [Figure 12.6](ch018.xhtml#fig-mlperf-training-improve),
    the performance improvements in progressive versions of MLPerf Training benchmarks
    have consistently outpaced Moore’s Law, which demonstrates that what gets measured
    gets improved. Using standardized benchmarking trends allows us to rigorously
    showcase the rapid evolution of ML computing.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练基准允许研究人员和工程师通过系统地评估这些因素来推动最先进的技术，优化配置，提高可扩展性，并减少整体资源消耗。如图[图12.6](ch018.xhtml#fig-mlperf-training-improve)所示，MLPerf
    Training基准的渐进版本中的性能改进始终超过了摩尔定律，这表明被测量的东西会得到改进。使用标准化的基准趋势趋势使我们能够严格展示ML计算的快速演变。
- en: '![](../media/file198.svg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file198.svg)'
- en: 'Figure 12.6: **MLPerf Training Progress**: Standardized benchmarks reveal that
    machine learning training performance consistently surpasses moore’s law, indicating
    substantial gains from systems-level optimizations. These trends emphasize how
    focused measurement and iterative improvement drive rapid advancements in ML training
    efficiency and scalability. Source: ([Tschand et al. 2024](ch058.xhtml#ref-tschand2024mlperf)).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：**MLPerf Training进展**：标准化基准显示，机器学习训练性能始终超过摩尔定律，表明系统级优化带来了实质性收益。这些趋势强调了专注的测量和迭代改进如何推动ML训练效率和可扩展性的快速进步。来源：([Tschand等人2024](ch058.xhtml#ref-tschand2024mlperf))。
- en: Importance of Training Benchmarks
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练基准的重要性
- en: As machine learning models grow in complexity, training becomes increasingly
    demanding in terms of compute power, memory, and data storage. The ability to
    measure and compare training efficiency is critical to ensuring that systems can
    effectively handle large-scale workloads. Training benchmarks provide a structured
    methodology for assessing performance across different hardware platforms, software
    frameworks, and optimization techniques.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型复杂性的增加，训练在计算能力、内存和数据存储方面变得越来越有挑战性。能够衡量和比较训练效率对于确保系统可以有效地处理大规模工作负载至关重要。训练基准提供了一种结构化的方法，用于评估不同硬件平台、软件框架和优化技术之间的性能。
- en: One of the primary challenges in training machine learning models is the efficient
    allocation of computational resources. Training a large-scale language model such
    as GPT-3, which consists of 175 billion parameters and requires processing terabytes
    of data, places an enormous burden on modern computing infrastructure. Without
    standardized benchmarks, it becomes difficult to determine whether a system is
    fully utilizing its resources or whether inefficiencies, including slow data loading,
    underutilized accelerators, and excessive memory overhead, are limiting performance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型的主要挑战之一是高效分配计算资源。训练一个像GPT-3这样的大型语言模型，它包含1750亿个参数并需要处理数TB的数据，对现代计算基础设施造成了巨大的负担。没有标准化的基准，很难确定系统是否充分利用了其资源，或者是否存在包括缓慢的数据加载、未充分利用的加速器和过度的内存开销等低效，这些因素是否限制了性能。
- en: Training benchmarks help uncover such inefficiencies by measuring key performance
    indicators, including system throughput, time-to-accuracy, and hardware utilization.
    Recall from [Chapter 11](ch017.xhtml#sec-ai-acceleration) that GPUs achieve approximately
    15,700 GFLOPS for mixed-precision operations while TPUs deliver 275,000 INT8 operations
    per second for specialized tensor workloads. Training benchmarks allow us to measure
    whether these theoretical hardware capabilities translate to actual training speedups
    under realistic conditions. These benchmarks allow practitioners to analyze whether
    accelerators are being leveraged effectively or whether specific bottlenecks,
    such as memory bandwidth constraints from hardware limitations ([Chapter 11](ch017.xhtml#sec-ai-acceleration)),
    are reducing overall system performance. For example, a system using TF32 precision1
    may achieve higher throughput than one using FP32, but if TF32 introduces numerical
    instability that increases the number of iterations required to reach the target
    accuracy, the overall training time may be longer. By providing insights into
    these factors, benchmarks support the design of more efficient training workflows
    that maximize hardware potential while minimizing unnecessary computation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基准通过测量关键性能指标，如系统吞吐量、达到准确性的时间和硬件利用率，帮助揭示此类低效。回想一下[第 11 章](ch017.xhtml#sec-ai-acceleration)，GPU
    在混合精度操作中可以达到大约 15,700 GFLOPS，而 TPU 每秒可以执行 275,000 个 INT8 操作，专门用于张量工作负载。训练基准使我们能够测量这些理论硬件能力在现实条件下的实际训练加速效果。这些基准测试允许从业者分析加速器是否被有效利用，或者是否存在特定的瓶颈，例如来自硬件限制的内存带宽限制([第
    11 章](ch017.xhtml#sec-ai-acceleration))，这些瓶颈正在降低整体系统性能。例如，使用 TF32 精度1的系统可能比使用
    FP32 的系统具有更高的吞吐量，但如果 TF32 引入了增加达到目标精度所需迭代次数的数值不稳定性，整体训练时间可能会更长。通过提供对这些因素的了解，基准测试支持设计更高效的训练工作流程，在最大化硬件潜力的同时，最小化不必要的计算。
- en: Hardware & Software Optimization
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件与软件优化
- en: The performance of machine learning training is heavily influenced by the choice
    of hardware and software. Training benchmarks guide system designers in selecting
    optimal configurations by measuring how different architectures, including GPUs,
    TPUs, and emerging AI accelerators, handle computational workloads. These benchmarks
    also evaluate how well deep learning frameworks, such as TensorFlow and PyTorch,
    optimize performance across different hardware setups.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习训练的性能受到硬件和软件选择的影响很大。训练基准通过测量不同架构（包括 GPU、TPU 和新兴的 AI 加速器）如何处理计算工作负载，指导系统设计者选择最佳配置。这些基准测试还评估深度学习框架（如
    TensorFlow 和 PyTorch）在不同硬件设置下优化性能的效果。
- en: For example, the MLPerf Training benchmark suite is widely used to compare the
    performance of different accelerator architectures on tasks such as image classification,
    natural language processing, and recommendation systems. By running standardized
    benchmarks across multiple hardware configurations, engineers can determine whether
    certain accelerators are better suited for specific training workloads. This information
    is particularly valuable in large-scale data centers and cloud computing environments,
    where selecting the right combination of hardware and software can lead to significant
    performance gains and cost savings.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MLPerf 训练基准套件被广泛用于比较不同加速器架构在图像分类、自然语言处理和推荐系统等任务上的性能。通过在多个硬件配置上运行标准化的基准测试，工程师可以确定某些加速器是否更适合特定的训练工作负载。在大型数据中心和云计算环境中，选择合适的硬件和软件组合可以带来显著的性能提升和成本节约，这些信息尤其宝贵。
- en: Beyond hardware selection, training benchmarks also inform software optimizations.
    Machine learning frameworks implement various low-level optimizations, including
    mixed-precision training[27](#fn27), memory-efficient data loading, and distributed
    training strategies, that can significantly impact system performance. Benchmarks
    help quantify the impact of these optimizations, ensuring that training systems
    are configured for maximum efficiency.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件选择之外，训练基准还可以指导软件优化。机器学习框架实现了各种低级优化，包括混合精度训练[27](#fn27)、内存高效的数据加载和分布式训练策略，这些优化可以显著影响系统性能。基准测试有助于量化这些优化对系统性能的影响，确保训练系统配置达到最大效率。
- en: Scalability & Efficiency
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性与效率
- en: As machine learning workloads continue to grow, efficient scaling across distributed
    computing environments has become a key concern. Many modern deep learning models
    are trained across multiple GPUs or TPUs, requiring efficient parallelization
    strategies to ensure that additional computing resources lead to meaningful performance
    improvements. Training benchmarks measure how well a system scales by evaluating
    system throughput, memory efficiency, and overall training time as additional
    computational resources are introduced.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习工作负载的不断增长，在分布式计算环境中的高效扩展已成为一个关键问题。许多现代深度学习模型是在多个GPU或TPU上训练的，需要高效的并行化策略来确保额外的计算资源能够带来有意义的性能提升。训练基准通过评估系统吞吐量、内存效率和整体训练时间，随着引入额外的计算资源来衡量系统的扩展效果。
- en: Effective scaling is not always guaranteed. While adding more GPUs or TPUs should,
    in theory, reduce training time, issues such as communication overhead, data synchronization
    latency, and memory bottlenecks can limit scaling efficiency. Training benchmarks
    help identify these challenges by quantifying how performance scales with increasing
    hardware resources. A well-designed system should exhibit near-linear scaling,
    where doubling the number of GPUs results in a near-halving of training time.
    However, real-world inefficiencies often prevent perfect scaling, and benchmarks
    provide the necessary insights to optimize system design accordingly.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的扩展并不总是有保证的。理论上，增加更多的GPU或TPU应该会减少训练时间，但诸如通信开销、数据同步延迟和内存瓶颈等问题可能会限制扩展效率。训练基准通过量化性能如何随着硬件资源的增加而扩展来帮助识别这些挑战。一个设计良好的系统应该表现出接近线性的扩展，即GPU数量加倍会导致训练时间几乎减半。然而，现实世界中的低效率往往阻止了完美的扩展，而基准提供了必要的见解，以便相应地优化系统设计。
- en: Another crucial factor in training efficiency is time-to-accuracy, which measures
    how quickly a model reaches a target accuracy level. This metric bridges the algorithmic
    and system dimensions of our framework, connecting model convergence characteristics
    with computational efficiency. By leveraging training benchmarks, system designers
    can assess whether their infrastructure is capable of handling large-scale workloads
    efficiently while maintaining training stability and accuracy.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 训练效率的另一个关键因素是达到准确度的时间，它衡量模型达到目标准确度水平有多快。这个指标连接了我们框架的算法和系统维度，将模型收敛特性与计算效率联系起来。通过利用训练基准，系统设计者可以评估他们的基础设施是否能够高效地处理大规模工作负载，同时保持训练的稳定性和准确性。
- en: Cost & Energy Factors
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本与能源因素
- en: The computational cost of training large-scale models has risen sharply in recent
    years, making cost-efficiency a critical consideration. Training a model such
    as GPT-3 can require millions of dollars in cloud computing resources, making
    it imperative to evaluate cost-effectiveness across different hardware and software
    configurations. Training benchmarks provide a means to quantify the cost per training
    run by analyzing computational expenses, cloud pricing models, and energy consumption.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，训练大规模模型的计算成本急剧上升，使得成本效益成为一个关键考虑因素。训练一个如GPT-3这样的模型可能需要数百万美元的云计算资源，这使得评估不同硬件和软件配置的成本效益变得至关重要。训练基准通过分析计算成本、云定价模型和能源消耗来提供量化每次训练运行成本的手段。
- en: Beyond financial cost, energy efficiency has become an increasingly important
    metric. Large-scale training runs consume vast amounts of electricity, contributing
    to significant carbon emissions. Benchmarks help evaluate energy efficiency by
    measuring power consumption per unit of training progress, allowing organizations
    to identify sustainable approaches to AI development.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 除了财务成本之外，能源效率已成为一个越来越重要的指标。大规模的训练运行消耗了大量的电力，导致显著的碳排放。基准通过测量每单位训练进展的电力消耗来帮助评估能源效率，使组织能够确定可持续的AI开发方法。
- en: For example, MLPerf includes an energy benchmarking component that tracks the
    power consumption of various hardware accelerators during training. This allows
    researchers to compare different computing platforms not only in terms of raw
    performance but also in terms of their environmental impact. By integrating energy
    efficiency metrics into benchmarking studies, organizations can design AI systems
    that balance computational power with sustainability goals.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MLPerf包括一个能量基准测试组件，它跟踪训练过程中各种硬件加速器的功耗。这允许研究人员不仅从原始性能的角度，而且从其对环境的影响角度比较不同的计算平台。通过将能源效率指标整合到基准测试研究中，组织可以设计出在计算能力和可持续性目标之间取得平衡的人工智能系统。
- en: Fair ML Systems Comparison
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 公平的机器学习系统比较
- en: One of the primary functions of training benchmarks is to establish a standardized
    framework for comparing ML systems. Given the wide variety of hardware architectures,
    deep learning frameworks, and optimization techniques available today, ensuring
    fair and reproducible comparisons is essential.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基准的一个主要功能是建立一个标准化的框架，用于比较机器学习系统。鉴于今天可用的广泛硬件架构、深度学习框架和优化技术，确保公平和可重复的比较是至关重要的。
- en: Standardized benchmarks provide a common evaluation methodology, allowing researchers
    and practitioners to assess how different training systems perform under identical
    conditions. MLPerf Training benchmarks enable vendor-neutral comparisons by defining
    strict evaluation criteria for deep learning tasks such as image classification,
    language modeling, and recommendation systems. This ensures that performance results
    are meaningful and not skewed by differences in dataset preprocessing, hyperparameter
    tuning, or implementation details.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化基准提供了一种通用的评估方法，使得研究人员和实践者能够评估不同训练系统在相同条件下的表现。MLPerf Training基准通过定义严格的评估标准，如图像分类、语言建模和推荐系统等深度学习任务，实现了供应商中立的比较。这确保了性能结果具有意义，并且不会因数据集预处理、超参数调整或实现细节的差异而失真。
- en: This standardized approach addresses reproducibility concerns in machine learning
    research by providing clearly defined evaluation methodologies. Results can be
    consistently reproduced across different computing environments, enabling researchers
    to make informed decisions when selecting hardware, software, and training methodologies
    while driving systematic progress in AI systems development.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这种标准化方法通过提供明确定义的评估方法来解决机器学习研究中的可重复性问题。结果可以在不同的计算环境中一致地重现，使研究人员在选择硬件、软件和训练方法时能够做出明智的决定，并推动人工智能系统开发的系统进步。
- en: Training Metrics
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练指标
- en: Evaluating the performance of machine learning training requires a set of well-defined
    metrics that go beyond conventional algorithmic measures. From a systems perspective,
    training benchmarks assess how efficiently and effectively a machine learning
    model can be trained to a predefined accuracy threshold. Metrics such as throughput,
    scalability, and energy efficiency are only meaningful in relation to whether
    the model successfully reaches its target accuracy. Without this constraint, optimizing
    for raw speed or resource utilization may lead to misleading conclusions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器学习训练的性能需要一套定义良好的指标，这些指标超越了传统的算法度量。从系统角度来看，训练基准评估机器学习模型被训练到预定义准确度阈值的有效性和效率。吞吐量、可扩展性和能源效率等指标只有在模型成功达到其目标准确度时才有意义。没有这个限制，仅仅优化原始速度或资源利用率可能会导致误导性的结论。
- en: Training benchmarks, such as MLPerf Training, define specific accuracy targets
    for different machine learning tasks, ensuring that performance measurements are
    made in a fair and reproducible manner. A system that trains a model quickly but
    fails to reach the required accuracy is not considered a valid benchmark result.
    Conversely, a system that achieves the best possible accuracy but takes an excessive
    amount of time or resources may not be practically useful. Effective benchmarking
    requires balancing speed, efficiency, and accuracy convergence.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基准，例如MLPerf Training，为不同的机器学习任务定义了特定的准确度目标，确保性能测量是以公平和可重复的方式进行。一个训练模型速度快但未能达到所需准确度的系统不被视为有效的基准结果。相反，一个达到最佳可能准确度但花费过多时间或资源的系统可能在实际应用中并不实用。有效的基准测试需要平衡速度、效率和准确度收敛。
- en: Time and Throughput
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间和吞吐量
- en: 'One of the primary metrics for evaluating training efficiency is the time required
    to reach a predefined accuracy threshold. Training time (<semantics><msub><mi>T</mi><mtext
    mathvariant="normal">train</mtext></msub><annotation encoding="application/x-tex">T_{\text{train}}</annotation></semantics>)
    measures how long a model takes to converge to an acceptable performance level,
    reflecting the overall computational efficiency of the system. It is formally
    defined as: <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">train</mtext></msub><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mi>t</mi></munder><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">{</mo><mtext mathvariant="normal">accuracy</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mtext
    mathvariant="normal">target accuracy</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">}</mo></mrow> <annotation encoding="application/x-tex">T_{\text{train}}
    = \arg\min_{t} \big\{ \text{accuracy}(t) \geq \text{target accuracy} \big\}</annotation></semantics>'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 评估训练效率的主要指标之一是达到预定义准确度阈值所需的时间。训练时间 (<semantics><msub><mi>T</mi><mtext mathvariant="normal">train</mtext></msub><annotation
    encoding="application/x-tex">T_{\text{train}}</annotation></semantics>) 衡量模型达到可接受性能水平所需的时间，反映了系统的整体计算效率。它正式定义为：<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">train</mtext></msub><mo>=</mo><mo>arg</mo><munder><mo>min</mo><mi>t</mi></munder><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">{</mo><mtext mathvariant="normal">accuracy</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mtext
    mathvariant="normal">target accuracy</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">}</mo></mrow> <annotation encoding="application/x-tex">T_{\text{train}}
    = \arg\min_{t} \big\{ \text{accuracy}(t) \geq \text{target accuracy} \big\}</annotation></semantics>
- en: This metric ensures that benchmarking focuses on how quickly and effectively
    a system can achieve meaningful results.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标确保基准测试关注系统如何快速有效地实现有意义的结果。
- en: 'Throughput, often expressed as the number of training samples processed per
    second, provides an additional measure of system performance: <semantics><mrow><mtext
    mathvariant="normal">Throughput</mtext><mo>=</mo><mfrac><msub><mi>N</mi><mtext
    mathvariant="normal">samples</mtext></msub><msub><mi>T</mi><mtext mathvariant="normal">train</mtext></msub></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Throughput} = \frac{N_{\text{samples}}}{T_{\text{train}}}</annotation></semantics>
    where <semantics><msub><mi>N</mi><mtext mathvariant="normal">samples</mtext></msub><annotation
    encoding="application/x-tex">N_{\text{samples}}</annotation></semantics> is the
    total number of training samples processed. However, throughput alone does not
    guarantee meaningful results, as a model may process a large number of samples
    quickly without necessarily reaching the desired accuracy.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量，通常以每秒处理的训练样本数量来表示，提供了系统性能的额外衡量标准：<semantics><mrow><mtext mathvariant="normal">吞吐量</mtext><mo>=</mo><mfrac><msub><mi>N</mi><mtext
    mathvariant="normal">samples</mtext></msub><msub><mi>T</mi><mtext mathvariant="normal">train</mtext></msub></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Throughput} = \frac{N_{\text{samples}}}{T_{\text{train}}}</annotation></semantics>
    其中 <semantics><msub><mi>N</mi><mtext mathvariant="normal">samples</mtext></msub><annotation
    encoding="application/x-tex">N_{\text{samples}}</annotation></semantics> 是处理的训练样本总数。然而，仅吞吐量并不能保证有意义的结果，因为一个模型可能快速处理大量样本，但并不一定能达到所需的准确度。
- en: For example, in MLPerf Training, the benchmark for ResNet-50 may require reaching
    an accuracy target like 75.9% top-1 on the ImageNet dataset. A system that processes
    10,000 images per second but fails to achieve this accuracy is not considered
    a valid benchmark result, while a system that processes fewer images per second
    but converges efficiently is preferable. This highlights why throughput must always
    be evaluated in relation to time-to-accuracy rather than as an independent performance
    measure.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在MLPerf Training中，ResNet-50的基准可能需要达到ImageNet数据集上的75.9% top-1的准确度目标。一个每秒处理10,000张图像但未能达到这一准确度的系统不被视为有效的基准结果，而一个每秒处理图像较少但收敛效率高的系统则更可取。这突出了为什么吞吐量必须始终与时间到准确度相关联来评估，而不是作为一个独立的性能指标。
- en: Scalability & Parallelism
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性与并行性
- en: As machine learning models increase in size, training workloads often require
    distributed computing across multiple processors or accelerators. Scalability
    measures how effectively training performance improves as more computational resources
    are added. An ideal system should exhibit near-linear scaling, where doubling
    the number of GPUs or TPUs leads to a proportional reduction in training time.
    However, real-world performance is often constrained by factors such as communication
    overhead, memory bandwidth limitations, and inefficiencies in parallelization
    strategies.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型规模的增加，训练工作负载通常需要在多个处理器或加速器之间进行分布式计算。可扩展性衡量了随着更多计算资源的增加，训练性能如何有效提高。理想的系统应表现出接近线性的扩展，即GPU或TPU数量的加倍会导致训练时间的成比例减少。然而，现实世界的性能通常受到通信开销、内存带宽限制和并行化策略中的低效等因素的限制。
- en: When training large-scale models such as GPT-3, OpenAI employed approximately
    10,000 NVIDIA V100 GPUs in a distributed training setup. Google’s systems have
    demonstrated similar scaling challenges with their 4,096-node TPU v4 clusters,
    where adding computational resources provides more raw power but performance improvements
    are constrained by network communication overhead between nodes. Benchmarks such
    as MLPerf quantify how well a system scales across multiple GPUs, providing insights
    into where inefficiencies arise in distributed training.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练像GPT-3这样的大型模型时，OpenAI在分布式训练设置中使用了大约10,000个NVIDIA V100 GPU。谷歌的系统在他们的4,096节点TPU
    v4集群中也展示了类似的扩展挑战，增加计算资源提供了更多的原始功率，但性能提升受到节点间网络通信开销的限制。例如，MLPerf这样的基准测试量化了系统在多个GPU上的扩展能力，为分布式训练中效率低下的地方提供了见解。
- en: Parallelism in training is categorized into data parallelism[28](#fn28), model
    parallelism[29](#fn29), and pipeline parallelism, each presenting distinct challenges.
    Data parallelism, the most commonly used strategy, involves splitting the training
    dataset across multiple compute nodes. The efficiency of this approach depends
    on synchronization mechanisms and gradient communication overhead. In contrast,
    model parallelism partitions the neural network itself, requiring efficient coordination
    between processors. Benchmarks evaluate how well a system manages these parallelism
    strategies without degrading accuracy convergence.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中的并行性分为数据并行[28](#fn28)、模型并行[29](#fn29)和流水线并行，每种都面临着不同的挑战。数据并行是最常用的策略，它涉及将训练数据集分割到多个计算节点上。这种方法的有效性取决于同步机制和梯度通信开销。相比之下，模型并行将神经网络本身进行分区，需要处理器之间的高效协调。基准测试评估系统在不降低精度收敛的情况下管理这些并行策略的能力。
- en: Resource Utilization
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源利用率
- en: The efficiency of machine learning training depends not only on speed and scalability
    but also on how well available hardware resources are utilized. Compute utilization
    measures the extent to which processing units, such as GPUs or TPUs, are actively
    engaged during training. Low utilization may indicate bottlenecks in data movement,
    memory access, or inefficient workload scheduling.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习训练的效率不仅取决于速度和可扩展性，还取决于可用硬件资源的使用情况。计算利用率衡量了在训练过程中处理单元（如GPU或TPU）的活跃程度。低利用率可能表明数据移动、内存访问或工作负载调度效率低下存在瓶颈。
- en: For instance, when training BERT on a TPU cluster, researchers observed that
    input pipeline inefficiencies were limiting overall throughput. Although the TPUs
    had high raw compute power, the system was not keeping them fully utilized due
    to slow data retrieval from storage. By profiling the resource utilization, engineers
    identified the bottleneck and optimized the input pipeline using TFRecord and
    data prefetching, leading to improved performance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当在TPU集群上训练BERT时，研究人员观察到输入管道的低效限制了整体吞吐量。尽管TPU具有很高的原始计算能力，但由于从存储中检索数据的速度慢，系统没有充分利用它们。通过分析资源利用率，工程师确定了瓶颈，并使用TFRecord和数据预取优化了输入管道，从而提高了性能。
- en: Memory bandwidth is another critical factor, as deep learning models require
    frequent access to large volumes of data during training. If memory bandwidth
    becomes a limiting factor, increasing compute power alone will not improve training
    speed. Benchmarks assess how well models leverage available memory, ensuring that
    data transfer rates between storage, main memory, and processing units do not
    become performance bottlenecks.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽是另一个关键因素，因为深度学习模型在训练期间需要频繁访问大量数据。如果内存带宽成为限制因素，仅仅增加计算能力并不能提高训练速度。基准测试评估模型如何有效地利用可用内存，确保存储、主内存和处理单元之间的数据传输速率不会成为性能瓶颈。
- en: I/O performance also plays a significant role in training efficiency, particularly
    when working with large datasets that cannot fit entirely in memory. Benchmarks
    evaluate the efficiency of data loading pipelines, including preprocessing operations,
    caching mechanisms, and storage retrieval speeds. Systems that fail to optimize
    data loading can experience significant slowdowns, regardless of computational
    power.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: I/O性能在训练效率中也起着重要作用，尤其是在处理无法完全装入内存的大型数据集时。基准测试评估数据加载管道的效率，包括预处理操作、缓存机制和存储检索速度。未能优化数据加载的系统可能会经历显著的减速，无论计算能力如何。
- en: Energy Efficiency & Cost
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 能效与成本
- en: Training large-scale machine learning models requires substantial computational
    resources, leading to significant energy consumption and financial costs. Energy
    efficiency metrics quantify the power usage of training workloads, helping identify
    systems that optimize computational efficiency while minimizing energy waste.
    The increasing focus on sustainability has led to the inclusion of energy-based
    benchmarks, such as those in MLPerf Training, which measure power consumption
    per training run.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大规模机器学习模型需要大量的计算资源，导致显著的能源消耗和财务成本。能效指标量化了训练工作负载的功耗，有助于识别那些优化计算效率同时最小化能源浪费的系统。对可持续性的日益关注导致了基于能源的基准测试的纳入，例如MLPerf
    Training中的基准测试，这些测试衡量每次训练运行的功耗。
- en: Training GPT-3 was estimated to consume 1,287 MWh of electricity ([D. Patterson
    et al. 2021a](ch058.xhtml#ref-patterson2021carbon)), which is comparable to the
    yearly energy usage of 100 US households. If a system can achieve the same accuracy
    with fewer training iterations, it directly reduces energy consumption. Energy-aware
    benchmarks help guide the development of hardware and training strategies that
    optimize power efficiency while maintaining accuracy targets.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GPT-3估计消耗了1,287 MWh的电力([D. Patterson et al. 2021a](ch058.xhtml#ref-patterson2021carbon))，这相当于100个美国家庭一年的能源使用量。如果一个系统可以用更少的训练迭代达到相同的精度，那么它将直接减少能源消耗。能源感知基准测试有助于指导硬件和训练策略的开发，以优化功率效率同时保持精度目标。
- en: Cost considerations extend beyond electricity usage to include hardware expenses,
    cloud computing costs, and infrastructure maintenance. Training benchmarks provide
    insights into the cost-effectiveness of different hardware and software configurations
    by measuring training time in relation to resource expenditure. Organizations
    can use these benchmarks to balance performance and budget constraints when selecting
    training infrastructure.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 成本考虑不仅限于电力使用，还包括硬件费用、云计算成本和基础设施维护。训练基准测试通过测量与资源支出相关的训练时间，为不同硬件和软件配置的成本效益提供见解。组织可以使用这些基准测试在选择训练基础设施时平衡性能和预算限制。
- en: Fault Tolerance & Robustness
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 容错与鲁棒性
- en: Training workloads often run for extended periods, sometimes spanning days or
    weeks, making fault tolerance an essential consideration. A robust system must
    be capable of handling unexpected failures, including hardware malfunctions, network
    disruptions, and memory errors, without compromising accuracy convergence.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作负载通常需要运行很长时间，有时跨越数天或数周，这使得容错成为一项基本考虑因素。一个健壮的系统必须能够处理意外的故障，包括硬件故障、网络中断和内存错误，而不会影响精度收敛。
- en: In large-scale cloud-based training, node failures are common due to hardware
    instability. If a GPU node in a distributed cluster fails, training must continue
    without corrupting the model. MLPerf Training includes evaluations of fault-tolerant
    training strategies, such as checkpointing, where models periodically save their
    progress. This ensures that failures do not require restarting the entire training
    process.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模基于云的训练中，由于硬件不稳定，节点故障是常见的。如果一个分布式集群中的 GPU 节点失败，训练必须继续进行，而不会损坏模型。MLPerf Training
    包括对容错训练策略的评估，例如检查点，其中模型定期保存其进度。这确保了故障不需要重新启动整个训练过程。
- en: Reproducibility & Standardization
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可重复性与标准化
- en: For benchmarks to be meaningful, results must be reproducible across different
    runs, hardware platforms, and software frameworks. Variability in training results
    can arise due to stochastic processes, hardware differences, and software optimizations.
    Ensuring reproducibility requires standardizing evaluation protocols, controlling
    for randomness in model initialization, and enforcing consistency in dataset processing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使基准测试具有意义，结果必须在不同的运行、硬件平台和软件框架之间可重复。训练结果的不确定性可能源于随机过程、硬件差异和软件优化。确保可重复性需要标准化评估协议，控制模型初始化中的随机性，并确保数据集处理的一致性。
- en: MLPerf Training enforces strict reproducibility requirements, ensuring that
    accuracy results remain stable across multiple training runs. When NVIDIA submitted
    benchmark results for MLPerf, they had to demonstrate that their ResNet-50 ImageNet
    training time remained consistent across different GPUs. This ensures that benchmarks
    measure true system performance rather than noise from randomness.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf Training 强制执行严格的可重复性要求，确保精度结果在多次训练运行中保持稳定。当 NVIDIA 为 MLPerf 提交基准测试结果时，他们必须证明他们的
    ResNet-50 ImageNet 训练时间在不同 GPU 上保持一致。这确保了基准测试测量的是真正的系统性能，而不是随机性的噪声。
- en: Training Performance Evaluation
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练性能评估
- en: Evaluating the performance of machine learning training systems involves more
    than just measuring how fast a model can be trained. A comprehensive benchmarking
    approach considers multiple dimensions, each capturing a different aspect of system
    behavior. The specific metrics used depend on the goals of the evaluation, whether
    those are optimizing speed, improving resource efficiency, reducing energy consumption,
    or ensuring robustness and reproducibility.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 评估机器学习训练系统的性能不仅涉及测量模型训练的速度。全面的基准测试方法考虑多个维度，每个维度都捕捉到系统行为的不同方面。所使用的具体指标取决于评估的目标，无论是优化速度、提高资源效率、减少能耗，还是确保鲁棒性和可重复性。
- en: '[Table 12.2](ch018.xhtml#tbl-training-metrics) provides an overview of the
    core categories and associated metrics commonly used to benchmark system-level
    training performance. These categories serve as a framework for understanding
    how training systems behave under different workloads and configurations.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12.2](ch018.xhtml#tbl-training-metrics) 提供了核心类别及其相关指标的概述，这些指标通常用于基准测试系统级训练性能。这些类别作为理解训练系统在不同工作负载和配置下如何表现的一个框架。'
- en: 'Table 12.2: **Training Benchmark Dimensions**: Key categories and metrics for
    comprehensively evaluating machine learning training systems, moving beyond simple
    speed to assess resource efficiency, reproducibility, and overall performance
    tradeoffs. understanding these dimensions enables systematic comparison of different
    training approaches and infrastructure configurations.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.2：**训练基准维度**：全面评估机器学习训练系统的关键类别和指标，超越简单的速度，评估资源效率、可重复性和整体性能权衡。理解这些维度能够系统地比较不同的训练方法和基础设施配置。
- en: '| **Category** | **Key Metrics** | **Example Benchmark Use** |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **关键指标** | **示例基准用途** |'
- en: '| --- | --- | --- |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Training Time and Throughput** | Time-to-accuracy (seconds, minutes, hours);
    Throughput (samples/sec) | Comparing training speed across different GPU architectures
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **训练时间和吞吐量** | 达到精度时间（秒、分钟、小时）；吞吐量（样本/秒） | 比较不同 GPU 架构的训练速度 |'
- en: '| **Scalability and Parallelism** | Scaling efficiency (% of ideal speedup);
    Communication overhead (latency, bandwidth) | Analyzing distributed training performance
    for large models |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性和并行性** | 扩展效率（理想加速的百分比）；通信开销（延迟，带宽） | 分析大型模型的分布式训练性能 |'
- en: '| **Resource Utilization** | Compute utilization (% GPU/TPU usage); Memory
    bandwidth (GB/s); I/O efficiency (data loading speed) | Optimizing data pipelines
    to improve GPU utilization |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| **资源利用率** | 计算利用率（% GPU/TPU 使用率）；内存带宽（GB/s）；I/O 效率（数据加载速度） | 优化数据管道以提高 GPU
    利用率 |'
- en: '| **Energy Efficiency and Cost** | Energy consumption per run (MWh, kWh); Performance
    per watt (TOPS/W) | Evaluating energy-efficient training strategies |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| **能源效率和成本** | 每次运行的能耗（MWh、kWh）；每瓦性能（TOPS/W） | 评估节能训练策略 |'
- en: '| **Fault Tolerance and Robustness** | Checkpoint overhead (time per save);
    Recovery success rate (%) | Assessing failure recovery in cloud-based training
    systems |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **容错性和鲁棒性** | 检查点开销（每次保存的时间）；恢复成功率（%） | 评估基于云的训练系统中的故障恢复 |'
- en: '| **Reproducibility and Standardization** | Variance across runs (% difference
    in accuracy, training time); Framework consistency (TensorFlow vs. PyTorch vs. JAX)
    | Ensuring consistency in benchmark results across hardware |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **可重复性和标准化** | 运行间的差异（准确率、训练时间的百分比差异）；框架一致性（TensorFlow 与 PyTorch 与 JAX） |
    确保基准测试结果在不同硬件上的一致性 |'
- en: Training time and throughput are often the first metrics considered when evaluating
    system performance. Time-to-accuracy, the duration required for a model to achieve
    a specified accuracy level, is a practical and widely used benchmark. Throughput,
    typically measured in samples per second, provides insight into how efficiently
    data is processed during training. For example, when comparing a ResNet-50 model
    trained on NVIDIA A100 versus V100 GPUs, the A100 generally offers higher throughput
    and faster convergence. However, it is important to ensure that increased throughput
    does not come at the expense of convergence quality, especially when reduced numerical
    precision (e.g., TF32) is used to speed up computation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间和吞吐量通常是评估系统性能时首先考虑的指标。达到指定准确率水平所需的时间（时间到准确率）是一个实用且广泛使用的基准。吞吐量，通常以每秒样本数来衡量，提供了关于训练期间数据处理效率的见解。例如，当比较在
    NVIDIA A100 和 V100 GPU 上训练的 ResNet-50 模型时，A100 通常提供更高的吞吐量和更快的收敛速度。然而，确保增加的吞吐量不会以收敛质量为代价是很重要的，尤其是在使用降低数值精度（例如，TF32）来加速计算时。
- en: As model sizes continue to grow, scalability becomes a critical performance
    dimension. Efficient use of multiple GPUs or TPUs is essential for training large
    models such as GPT-3 or T5\. In this context, scaling efficiency and communication
    overhead are key metrics. A system might scale linearly up to 64 GPUs, but beyond
    that, performance gains may taper off due to increased synchronization and communication
    costs. Benchmarking tools that monitor interconnect bandwidth and gradient aggregation
    latency can reveal how well a system handles distributed training.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型尺寸的持续增长，可扩展性成为关键的性能维度。高效地使用多个 GPU 或 TPU 对于训练大型模型（如 GPT-3 或 T5）至关重要。在这种情况下，扩展效率和通信开销是关键指标。一个系统可能线性扩展到
    64 个 GPU，但超过这个范围，由于同步和通信成本的提高，性能提升可能会减少。监控互连带宽和梯度聚合延迟的基准测试工具可以揭示系统处理分布式训练的能力。
- en: Resource utilization complements these measures by examining how effectively
    a system leverages its compute and memory resources. Metrics such as GPU utilization,
    memory bandwidth, and data loading efficiency help identify performance bottlenecks.
    For instance, a BERT pretraining task that exhibits only moderate GPU utilization
    may be constrained by an underperforming data pipeline. Optimizations like sharding
    input files or prefetching data into device memory can often resolve these inefficiencies.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 资源利用率通过检查系统如何有效地利用其计算和内存资源来补充这些措施。例如，GPU 利用率、内存带宽和数据加载效率等指标有助于识别性能瓶颈。例如，一个仅表现出适度
    GPU 利用率的 BERT 预训练任务可能受到性能不佳的数据管道的限制。像分割输入文件或预取数据到设备内存这样的优化通常可以解决这些低效问题。
- en: In addition to raw performance, energy efficiency and cost have become increasingly
    important considerations. Training large models at scale can consume significant
    power, raising environmental and financial concerns. Metrics such as energy consumed
    per training run and performance per watt (e.g., TOPS/W) help evaluate the sustainability
    of different hardware and system configurations. For example, while two systems
    may reach the same accuracy in the same amount of time, the one that uses significantly
    less energy may be preferred for long-term deployment.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始性能之外，能效和成本已成为越来越重要的考虑因素。大规模训练大型模型可能会消耗大量电力，引发环境和财务担忧。例如，每训练运行消耗的能源和每瓦性能（例如，TOPS/W）等指标有助于评估不同硬件和系统配置的可持续性。例如，虽然两个系统可能在相同的时间内达到相同的精度，但使用显著更少能源的系统可能更适合长期部署。
- en: Fault tolerance and robustness address how well a system performs under non-ideal
    conditions, which are common in real-world deployments. Training jobs frequently
    encounter hardware failures, preemptions, or network instability. Metrics like
    checkpoint overhead and recovery success rate provide insight into the resilience
    of a training system. In practice, checkpointing can introduce non-trivial overhead.
    For example, pausing training every 30 minutes to write a full checkpoint may
    reduce overall throughput by 5-10%. Systems must strike a balance between failure
    recovery and performance impact.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 容错性和鲁棒性解决了系统在非理想条件下的性能表现，这在现实世界的部署中很常见。训练作业经常遇到硬件故障、中断或网络不稳定。检查点开销和恢复成功率等指标可以提供对训练系统弹性的洞察。在实践中，检查点可能会引入非微不足道的开销。例如，每30分钟暂停训练以写入完整的检查点可能会降低整体吞吐量5-10%。系统必须在故障恢复和性能影响之间取得平衡。
- en: Finally, reproducibility and standardization ensure that benchmark results are
    consistent, interpretable, and transferable. Even minor differences in software
    libraries, initialization seeds, or floating-point behavior can affect training
    outcomes. Comparing the same model across frameworks, such as comparing PyTorch
    with Automatic Mixed Precision to TensorFlow with XLA, can reveal variation in
    convergence rates or final accuracy. Reliable benchmarking requires careful control
    of these variables, along with repeated runs to assess statistical variance.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可重复性和标准化确保基准测试结果是连贯的、可解释的和可转移的。即使是软件库、初始化种子或浮点行为中的微小差异也可能影响训练结果。跨框架比较相同模型，例如将PyTorch与自动混合精度与TensorFlow与XLA进行比较，可以揭示收敛速度或最终精度的差异。可靠的基准测试需要仔细控制这些变量，并通过重复运行来评估统计方差。
- en: Together, these dimensions provide a holistic view of training performance.
    They help researchers, engineers, and system designers move beyond simplistic
    comparisons and toward a more nuanced understanding of how machine learning systems
    behave under realistic conditions. As established in our statistical rigor framework
    earlier, measuring these dimensions accurately requires systematic methodology
    that distinguishes between true performance differences and statistical noise,
    accounting for factors like GPU boost clock[30](#fn30) behavior and thermal throttling[31](#fn31)
    that can significantly impact measurements.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这些维度共同提供了对训练性能的整体视角。它们帮助研究人员、工程师和系统设计师超越简单的比较，转向对机器学习系统在现实条件下的行为有更细微的理解。正如我们在先前的统计严谨框架中确立的，准确测量这些维度需要系统的方法，该方法区分了真实性能差异和统计噪声，并考虑了如GPU加速时钟[30](#fn30)行为和热限制[31](#fn31)等因素，这些因素可能会对测量产生重大影响。
- en: Training Benchmark Pitfalls
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练基准陷阱
- en: Despite the availability of well-defined benchmarking methodologies, certain
    misconceptions and flawed evaluation practices often lead to misleading conclusions.
    Understanding these pitfalls is important for interpreting benchmark results correctly.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在定义良好的基准测试方法，但某些误解和有缺陷的评估实践往往导致误导性的结论。理解这些陷阱对于正确解释基准测试结果非常重要。
- en: Overemphasis on Raw Throughput
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 过度强调原始吞吐量
- en: A common mistake in training benchmarks is assuming that higher throughput always
    translates to better training performance. It is possible to artificially increase
    throughput by using lower numerical precision, reducing synchronization, or even
    bypassing certain computations. However, these optimizations do not necessarily
    lead to faster convergence.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基准测试中常见的错误是假设更高的吞吐量总是意味着更好的训练性能。可以通过使用较低的数值精度、减少同步或甚至绕过某些计算来人为地提高吞吐量。然而，这些优化并不一定导致更快收敛。
- en: For example, a system using TF32 precision may achieve higher throughput than
    one using FP32, but if TF32 introduces numerical instability that increases the
    number of iterations required to reach the target accuracy, the overall training
    time may be longer. The correct way to evaluate throughput is in relation to time-to-accuracy,
    ensuring that speed optimizations do not come at the expense of convergence efficiency.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 TF32 精度的系统可能比使用 FP32 的系统具有更高的吞吐量，但如果 TF32 引入了导致达到目标精度所需的迭代次数增加的数值不稳定性，整体训练时间可能会更长。正确评估吞吐量的方法是与时间到精度相关，确保速度优化不会以收敛效率为代价。
- en: Isolated Single-Node Performance
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隔离的单节点性能
- en: Benchmarking training performance on a single node without considering distributed
    scaling can lead to misleading conclusions. A GPU may demonstrate excellent throughput
    when used independently, but when deployed in large clusters like Google’s 4,096-node
    TPU v4 configurations, communication overhead and synchronization constraints
    significantly diminish these efficiency gains.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个节点上评估训练性能而不考虑分布式扩展可能导致误导性的结论。当独立使用时，GPU 可能会显示出卓越的吞吐量，但当部署在像谷歌的 4,096 节点 TPU
    v4 配置的大型集群中时，通信开销和同步限制会显著减少这些效率提升。
- en: For instance, a system optimized for single-node performance may employ memory
    optimizations that do not generalize to multi-node environments. Large-scale models
    such as GPT-3 require efficient gradient synchronization across thousands of nodes,
    making comprehensive scalability assessment essential. Google’s experience with
    4,096-node TPU clusters demonstrates that gradient synchronization challenges
    become dominant performance factors at this scale.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，针对单节点性能优化的系统可能采用不适用于多节点环境的内存优化。像 GPT-3 这样的大型模型需要在数千个节点之间进行高效的梯度同步，这使得全面的可扩展性评估变得至关重要。谷歌在
    4,096 节点 TPU 集群中的经验表明，梯度同步挑战成为此规模的主要性能因素。
- en: Ignoring Failures & Interference
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略故障与干扰
- en: Many benchmarks assume an idealized training environment where hardware failures,
    memory corruption, network instability, or interference from other processes do
    not occur. However, real-world training jobs often experience unexpected failures
    and workload interference that require checkpointing, recovery mechanisms, and
    resource management.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基准测试假设了一个理想化的训练环境，其中硬件故障、内存损坏、网络不稳定或来自其他进程的干扰不会发生。然而，现实世界的训练作业经常遇到意外的故障和工作负载干扰，这需要检查点、恢复机制和资源管理。
- en: A system optimized for ideal-case performance but lacking fault tolerance and
    interference handling may achieve impressive benchmark results under controlled
    conditions, but frequent failures, inefficient recovery, and resource contention
    could make it impractical for large-scale deployment. Effective benchmarking should
    consider checkpointing overhead, failure recovery efficiency, and the impact of
    interference from other processes rather than assuming perfect execution conditions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 优化于理想情况性能但缺乏容错性和干扰处理的系统可能在受控条件下实现令人印象深刻的基准测试结果，但频繁的故障、低效的恢复和资源竞争可能使其在大规模部署中不切实际。有效的基准测试应考虑检查点开销、故障恢复效率和来自其他进程干扰的影响，而不是假设完美的执行条件。
- en: Linear Scaling Assumption
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性扩展假设
- en: When evaluating distributed training, it is often assumed that increasing the
    number of GPUs or TPUs will result in proportional speedups. In practice, communication
    bottlenecks, memory contention, and synchronization overheads lead to diminishing
    returns as more compute nodes are added.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估分布式训练时，通常假设增加 GPU 或 TPUs 的数量将导致成比例的速度提升。在实践中，随着计算节点的增加，通信瓶颈、内存竞争和同步开销会导致回报递减。
- en: For example, training a model across 1,000 GPUs does not necessarily provide
    100 times the speed of training on 10 GPUs. At a certain scale, gradient communication
    costs become a limiting factor, offsetting the benefits of additional parallelism.
    Proper benchmarking should assess scalability efficiency rather than assuming
    idealized linear improvements.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在1,000个GPU上训练模型并不一定比在10个GPU上训练快100倍。在某个规模上，梯度通信成本成为限制因素，抵消了额外并行性的好处。适当的基准测试应该评估可扩展性效率，而不是假设理想化的线性改进。
- en: Ignoring Reproducibility
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略可重复性
- en: Benchmark results are often reported without verifying their reproducibility
    across different hardware and software frameworks. Even minor variations in floating-point
    arithmetic, memory layouts, or optimization strategies can introduce statistical
    differences in training time and accuracy.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 基准结果通常在没有验证它们在不同硬件和软件框架中的可重复性时就被报告。即使是浮点运算、内存布局或优化策略的微小变化，也可能在训练时间和精度上引入统计差异。
- en: For example, a benchmark run on TensorFlow with XLA optimizations may exhibit
    different convergence characteristics compared to the same model trained using
    PyTorch with Automatic Mixed Precision (AMP). Proper benchmarking requires evaluating
    results across multiple frameworks to ensure that software-specific optimizations
    do not distort performance comparisons.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在TensorFlow上使用XLA优化进行的基准测试可能表现出与使用PyTorch和自动混合精度（AMP）训练的相同模型不同的收敛特性。适当的基准测试需要评估多个框架的结果，以确保软件特定的优化不会扭曲性能比较。
- en: Training Benchmark Synthesis
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练基准合成
- en: Training benchmarks provide valuable insights into machine learning system performance,
    but their interpretation requires careful consideration of real-world constraints.
    High throughput does not necessarily mean faster training if it compromises accuracy
    convergence. Similarly, scaling efficiency must be evaluated holistically, taking
    into account both computational efficiency and communication overhead.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 训练基准为机器学习系统性能提供了宝贵的见解，但对其解释需要仔细考虑现实世界的限制。高吞吐量并不一定意味着训练速度更快，如果它牺牲了精度收敛。同样，扩展效率必须全面评估，考虑到计算效率和通信开销。
- en: Avoiding common benchmarking pitfalls and employing structured evaluation methodologies
    allows machine learning practitioners to gain a deeper understanding of how to
    optimize training workflows, design efficient AI systems, and develop scalable
    machine learning infrastructure. As models continue to increase in complexity,
    benchmarking methodologies must evolve to reflect real-world challenges, ensuring
    that benchmarks remain meaningful and actionable in guiding AI system development.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 避免常见的基准测试陷阱并采用结构化评估方法，使机器学习从业者能够更深入地了解如何优化训练工作流程、设计高效的AI系统以及开发可扩展的机器学习基础设施。随着模型复杂性的持续增加，基准测试方法必须演变以反映现实世界的挑战，确保基准在指导AI系统开发中保持有意义和可操作。
- en: Inference Benchmarks
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理基准
- en: Complementing training benchmarks within our framework, inference benchmarks
    focus on evaluating the efficiency, latency, and resource demands during model
    deployment and serving. Unlike training, where the focus is on optimizing large-scale
    computations over extensive datasets, inference involves deploying trained models
    to make real-time or batch predictions efficiently. These benchmarks help assess
    how various factors, including model architectures, hardware configurations, precision
    optimization techniques, and runtime optimizations, impact inference performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的框架中补充训练基准，推理基准专注于评估模型部署和服务的效率、延迟和资源需求。与训练不同，训练的重点是优化大规模计算，而推理涉及将训练好的模型部署到实时或批量预测中。这些基准有助于评估包括模型架构、硬件配置、精度优化技术和运行时优化在内的各种因素如何影响推理性能。
- en: As deep learning models grow exponentially in complexity and size, efficient
    inference becomes an increasingly critical challenge, particularly for applications
    requiring real-time decision-making, such as autonomous driving, healthcare diagnostics,
    and conversational AI. For example, serving large-scale language models involves
    handling billions of parameters while maintaining acceptably low latency. Inference
    benchmarks provide systematic evaluation of the underlying hardware and software
    stacks to ensure that models can be deployed efficiently across different environments,
    from cloud data centers to edge devices.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习模型在复杂性和规模上的指数级增长，高效的推理成为一个越来越关键的挑战，尤其是对于需要实时决策的应用，如自动驾驶、医疗诊断和对话式AI。例如，服务大规模语言模型需要处理数十亿个参数，同时保持可接受的低延迟。推理基准测试对底层硬件和软件堆栈进行系统评估，以确保模型可以在不同的环境中高效部署，从云数据中心到边缘设备。
- en: '***ML Inference Benchmarks*** are standardized evaluations of the *inference
    phase*, measuring *latency*, *throughput*, *energy consumption*, and *memory footprint*
    to assess deployment performance across hardware and software configurations.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '**ML推理基准**是对**推理阶段**的标准化评估，衡量**延迟**、**吞吐量**、**能耗**和**内存占用**，以评估不同硬件和软件配置下的部署性能。'
- en: Unlike training, which is typically conducted in large-scale data centers with
    ample computational resources, inference must be optimized for dramatically diverse
    deployment scenarios, including mobile devices, IoT systems, and embedded processors.
    Efficient inference depends on multiple interconnected factors, such as optimized
    data pipelines, model optimization techniques, and hardware acceleration. Benchmarks
    help evaluate how well these optimizations improve real-world deployment performance.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与通常在大规模数据中心进行且拥有充足计算资源的训练不同，推理必须针对极其多样化的部署场景进行优化，包括移动设备、物联网系统和嵌入式处理器。高效的推理依赖于多个相互关联的因素，如优化的数据管道、模型优化技术和硬件加速。基准测试有助于评估这些优化如何改善实际部署性能。
- en: Building on these optimization requirements, hardware selection plays an increasingly
    important role in inference efficiency. While GPUs and TPUs are widely used for
    training, inference workloads often require specialized accelerators like NPUs
    (Neural Processing Units)[32](#fn32), FPGAs[33](#fn33), and dedicated inference
    chips such as Google’s Edge TPU[34](#fn34). Inference benchmarks evaluate the
    utilization and performance of these hardware components, helping practitioners
    choose the right configurations for their deployment needs.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些优化要求的基础上，硬件选择在推理效率中扮演着越来越重要的角色。虽然GPU和TPU在训练中广泛使用，但推理工作负载通常需要专门的加速器，如NPUs（神经网络处理器）[32](#fn32)、FPGAs[33](#fn33)以及如Google的Edge
    TPU[34](#fn34)等专用推理芯片。推理基准测试评估这些硬件组件的利用率和性能，帮助实践者根据他们的部署需求选择合适的配置。
- en: Scaling inference workloads across cloud servers, edge platforms, mobile devices,
    and tinyML systems introduces additional complexity. As illustrated in [Figure 12.7](ch018.xhtml#fig-power-differentials),
    there is a significant differential in power consumption among these systems,
    ranging from microwatts to megawatts. Inference benchmarks evaluate the trade-offs
    between latency, cost, and energy efficiency, thereby assisting organizations
    in making informed deployment decisions.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在云服务器、边缘平台、移动设备和tinyML系统之间扩展推理工作负载引入了额外的复杂性。如图12.7[图](ch018.xhtml#fig-power-differentials)所示，这些系统之间的功耗差异很大，从微瓦到兆瓦不等。推理基准测试评估了延迟、成本和能效之间的权衡，从而帮助组织做出明智的部署决策。
- en: '![](../media/file199.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file199.png)'
- en: 'Figure 12.7: **Energy Consumption**: The figure emphasizes the significant
    differences in power usage across various system types, from microwatts to megawatts,
    emphasizing the trade-offs between latency, cost, and energy efficiency in inference
    benchmarks.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：**能耗**：该图强调了各种系统类型之间功率使用的显著差异，从微瓦到兆瓦，强调了推理基准测试中延迟、成本和能效之间的权衡。
- en: As with training, we will reference MLPerf Inference throughout this section
    to illustrate benchmarking principles. MLPerf’s inference benchmarks, building
    on the foundation established in [Section 12.2](ch018.xhtml#sec-benchmarking-ai-historical-context-1c54),
    provide standardized evaluation across deployment scenarios from cloud to edge
    devices.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练类似，在本节中我们将参考 MLPerf 推理来阐述基准测试原则。MLPerf 的推理基准，建立在第 12.2 节[Section 12.2](ch018.xhtml#sec-benchmarking-ai-historical-context-1c54)中确立的基础之上，为从云端到边缘设备的部署场景提供了标准化的评估。
- en: Inference Benchmark Motivation
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理基准动机
- en: Deploying machine learning models for inference introduces a unique set of challenges
    distinct from training. While training optimizes large-scale computation over
    extensive datasets, inference must deliver predictions efficiently and at scale
    in real-world environments. Inference benchmarks evaluate deployment-specific
    performance challenges, identifying bottlenecks that emerge when models transition
    from development to production serving.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习模型部署用于推理引入了一组独特的挑战，这些挑战与训练不同。虽然训练优化了大规模计算和大量数据集，但推理必须在现实世界环境中高效且大规模地提供预测。推理基准评估了特定部署的性能挑战，识别了模型从开发过渡到生产服务时出现的瓶颈。
- en: Unlike training, which typically runs on dedicated high-performance hardware,
    inference must adapt to varying constraints. A model deployed in a cloud server
    might prioritize high-throughput batch processing, while the same model running
    on a mobile device must operate under strict latency and power constraints. On
    edge devices with limited compute and memory, model optimization techniques become
    critical. Benchmarks help assess these trade-offs, ensuring that inference systems
    maintain the right balance between accuracy, speed, and efficiency across different
    platforms.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 与通常在专用高性能硬件上运行的训练不同，推理必须适应不同的约束条件。部署在云服务器的模型可能优先考虑高吞吐量的批量处理，而运行在移动设备上的相同模型必须在严格的延迟和功耗约束下运行。在计算和内存有限的边缘设备上，模型优化技术变得至关重要。基准有助于评估这些权衡，确保推理系统在不同平台上保持准确率、速度和效率之间的正确平衡。
- en: Inference benchmarks help answer essential questions about model deployment.
    How quickly can a model generate predictions in real-world conditions? What are
    the trade-offs between inference speed and accuracy? Can an inference system handle
    increasing demand while maintaining low latency? By evaluating these factors,
    benchmarks guide optimizations in both hardware and software to improve overall
    efficiency ([Reddi et al. 2019b](ch058.xhtml#ref-reddi2020mlperf)).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 推理基准有助于回答关于模型部署的基本问题。模型在现实世界条件下生成预测的速度有多快？推理速度与准确率之间的权衡是什么？推理系统能否在保持低延迟的同时处理不断增长的需求？通过评估这些因素，基准指导硬件和软件的优化，以提升整体效率
    ([Reddi 等人 2019b](ch058.xhtml#ref-reddi2020mlperf))。
- en: Importance of Inference Benchmarks
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理基准的重要性
- en: Inference plays a critical role in AI applications, where performance directly
    affects usability and cost. Unlike training, which is often performed offline,
    inference typically operates in real-time or near real-time, making latency a
    primary concern. A self-driving car processing camera feeds must react within
    milliseconds, while a voice assistant generating responses should feel instantaneous
    to users.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 推理在人工智能应用中扮演着至关重要的角色，其性能直接影响可用性和成本。与通常离线进行的训练不同，推理通常在实时或接近实时的情况下进行，使得延迟成为主要关注点。自动驾驶汽车处理摄像头数据必须在毫秒内做出反应，而语音助手生成响应应该对用户来说感觉瞬间完成。
- en: Different applications impose varying constraints on inference. Some workloads
    require single-instance inference, where predictions must be made as quickly as
    possible for each individual input. This is crucial in real-time systems such
    as robotics, augmented reality, and conversational AI, where even small delays
    can impact responsiveness. Other workloads, such as large-scale recommendation
    systems or search engines, process massive batches of queries simultaneously,
    prioritizing throughput over per-query latency. Benchmarks allow engineers to
    evaluate both scenarios and ensure models are optimized for their intended use
    case.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用对推理施加了不同的约束。一些工作负载需要单实例推理，即必须尽可能快地为每个单独的输入做出预测。这在实时系统中至关重要，如机器人、增强现实和对话式人工智能，在这些系统中，即使是微小的延迟也可能影响响应性。其他工作负载，如大规模推荐系统或搜索引擎，同时处理大量查询的批次，优先考虑吞吐量而非每个查询的延迟。基准允许工程师评估这两种场景，并确保模型针对其预期用途进行了优化。
- en: A key difference between training and inference is that inference workloads
    often run continuously in production, meaning that small inefficiencies can compound
    over time. Unlike a training job that runs once and completes, an inference system
    deployed in the cloud may serve millions of queries daily, and a model running
    on a smartphone must manage battery consumption over extended use. Benchmarks
    provide a structured way to measure inference efficiency under these real-world
    constraints, helping developers make informed choices about model optimization,
    hardware selection, and deployment strategies.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与推理之间的一个关键区别是，推理工作负载通常在生产环境中持续运行，这意味着小的低效可能会随着时间的推移而累积。与一次运行并完成的训练作业不同，部署在云中的推理系统可能每天处理数百万个查询，而运行在智能手机上的模型必须在长时间使用中管理电池消耗。基准测试提供了一种结构化的方式来衡量在这些现实世界约束下的推理效率，帮助开发者就模型优化、硬件选择和部署策略做出明智的选择。
- en: Hardware & Software Optimization
  id: totrans-355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件与软件优化
- en: Efficient inference depends on both hardware acceleration and software optimizations.
    While GPUs and TPUs dominate training, inference is more diverse in its hardware
    needs. A cloud-based AI service might leverage powerful accelerators for large-scale
    workloads, whereas mobile devices rely on specialized inference chips like NPUs
    or optimized CPU execution. On embedded systems, where resources are constrained,
    achieving high performance requires careful memory and compute efficiency. Benchmarks
    help evaluate how well different hardware platforms handle inference workloads,
    guiding deployment decisions.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 高效推理依赖于硬件加速和软件优化。虽然GPU和TPU在训练中占据主导地位，但在推理方面，对硬件的需求更加多样化。基于云的AI服务可能会利用强大的加速器来处理大规模工作负载，而移动设备则依赖于专门的推理芯片，如NPUs或优化的CPU执行。在资源受限的嵌入式系统中，实现高性能需要仔细考虑内存和计算效率。基准测试有助于评估不同硬件平台处理推理工作负载的能力，从而指导部署决策。
- en: Software optimizations are just as important. Frameworks like TensorRT[35](#fn35),
    ONNX Runtime[36](#fn36), and TVM[37](#fn37) apply optimizations such as operator
    fusion[38](#fn38), numerical precision adjustments, and kernel tuning to improve
    inference speed and reduce computational overhead. These optimizations can make
    a significant difference, especially in environments with limited resources. Benchmarks
    allow developers to measure the impact of such techniques on latency, throughput,
    and power efficiency, ensuring that optimizations translate into real-world improvements
    without degrading model accuracy.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 软件优化同样重要。例如，TensorRT[35](#fn35)、ONNX Runtime[36](#fn36)和TVM[37](#fn37)等框架通过操作融合[38](#fn38)、数值精度调整和内核调优等优化来提高推理速度并减少计算开销。这些优化可以产生显著差异，尤其是在资源有限的环境中。基准测试允许开发者衡量这些技术对延迟、吞吐量和功耗效率的影响，确保优化能够转化为现实世界的改进，而不会降低模型精度。
- en: Scalability & Efficiency
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展性与效率
- en: Inference workloads vary significantly in their scaling requirements. A cloud-based
    AI system handling millions of queries per second must ensure that increasing
    demand does not cause delays, while a mobile application running a model locally
    must execute quickly even under power constraints. Unlike training, which is typically
    performed on a fixed set of high-performance machines, inference must scale dynamically
    based on usage patterns and available computational resources.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 推理工作负载在扩展需求上差异很大。处理每秒数百万个查询的基于云的AI系统必须确保增加的需求不会导致延迟，而运行本地模型的移动应用程序即使在电源限制下也必须快速执行。与通常在固定的高性能机器上进行的训练不同，推理必须根据使用模式和可用计算资源动态扩展。
- en: Benchmarks evaluate how inference systems scale under different conditions.
    They measure how well performance holds up under increasing query loads, whether
    additional compute resources improve inference speed, and how efficiently models
    run across different deployment environments. Large-scale inference deployments
    often involve distributed inference servers, where multiple copies of a model
    process incoming requests in parallel. Benchmarks assess how efficiently this
    scaling occurs and whether additional resources lead to meaningful improvements
    in latency and throughput.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试评估推理系统在不同条件下的扩展性。它们衡量在查询负载增加的情况下性能如何保持，额外的计算资源是否提高了推理速度，以及模型在不同部署环境中的运行效率。大规模推理部署通常涉及分布式推理服务器，其中多个模型副本并行处理传入的请求。基准测试评估这种扩展的效率，以及额外资源是否导致延迟和吞吐量的实质性改进。
- en: Another key factor in inference efficiency is cold-start performance, the time
    it takes for a model to load and begin processing queries. This is especially
    relevant for applications that do not run inference continuously but instead load
    models on demand. Benchmarks help determine whether a system can quickly transition
    from idle to active execution without significant overhead.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 推理效率的另一个关键因素是冷启动性能，即模型加载并开始处理查询所需的时间。这对于不持续运行推理而是按需加载模型的应用尤其相关。基准测试有助于确定系统是否可以快速从空闲状态过渡到活跃执行状态，而不会产生显著的开销。
- en: Cost & Energy Factors
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本与能源因素
- en: Because inference workloads run continuously, operational cost and energy efficiency
    are critical factors. Unlike training, where compute costs are incurred once,
    inference costs accumulate over time as models are deployed in production. Running
    an inefficient model at scale can significantly increase cloud compute expenses,
    while an inefficient mobile inference system can drain battery life quickly. Benchmarks
    provide insights into cost per inference request, helping organizations optimize
    for both performance and affordability.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推理工作负载持续运行，运营成本和能源效率是关键因素。与训练不同，训练中的计算成本是一次性发生的，而推理成本会随着模型在生产中的部署而累积。大规模运行低效模型会显著增加云计算费用，而低效的移动推理系统会迅速耗尽电池电量。基准测试提供了每条推理请求的成本洞察，帮助组织在性能和成本效益之间进行优化。
- en: Energy efficiency is also a growing concern, particularly for mobile and edge
    AI applications. Many inference workloads run on battery-powered devices, where
    excessive computation can impact usability. A model running on a smartphone, for
    example, must be optimized to minimize power consumption while maintaining responsiveness.
    Benchmarks help evaluate inference efficiency per watt, ensuring that models can
    operate sustainably across different platforms.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 能源效率也是一个日益关注的问题，尤其是对于移动和边缘人工智能应用。许多推理工作负载在电池供电的设备上运行，过度的计算会影响可用性。例如，在智能手机上运行的模型必须优化以最小化功耗，同时保持响应性。基准测试有助于评估每瓦推理效率，确保模型可以在不同平台上可持续运行。
- en: Fair ML Systems Comparison
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 公平机器学习系统比较
- en: Applying the standardized evaluation principles established for training benchmarks,
    inference evaluation requires the same rigorous comparison methodologies. MLPerf
    Inference extends these principles to deployment scenarios, defining evaluation
    criteria for tasks such as image classification, object detection, and speech
    recognition across different hardware platforms and optimization techniques. This
    ensures that inference performance comparisons remain meaningful and reproducible
    while accounting for deployment-specific constraints like latency requirements
    and energy efficiency.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 应用为训练基准建立的标准化评估原则，推理评估需要相同的严格比较方法。MLPerf 推理将这些原则扩展到部署场景，为图像分类、目标检测和语音识别等任务定义了不同硬件平台和优化技术下的评估标准。这确保了推理性能比较在考虑部署特定约束（如延迟要求和能源效率）的同时，保持其意义和可重复性。
- en: Inference Metrics
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理度量
- en: Evaluating the performance of inference systems requires a distinct set of metrics
    from those used for training. While training benchmarks emphasize throughput,
    scalability, and time-to-accuracy, inference benchmarks must focus on latency,
    efficiency, and resource utilization in practical deployment settings. These metrics
    ensure that machine learning models perform well across different environments,
    from cloud data centers handling millions of requests to mobile and edge devices
    operating under strict power and memory constraints.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 评估推理系统的性能需要与训练中使用的指标截然不同。虽然训练基准强调吞吐量、可扩展性和达到准确性的时间，但推理基准必须关注实际部署设置中的延迟、效率和资源利用率。这些指标确保机器学习模型在不同环境中表现良好，从处理数百万请求的云数据中心到在严格的电源和内存约束下运行的移动和边缘设备。
- en: Unlike training benchmarks that emphasize throughput and time-to-accuracy as
    established earlier, inference benchmarks evaluate how efficiently a trained model
    can process inputs and generate predictions at scale. The following sections describe
    the most important inference benchmarking metrics, explaining their relevance
    and how they are used to compare different systems.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前强调吞吐量和达到准确性的时间训练基准不同，推理基准评估训练好的模型在规模上处理输入和生成预测的效率。以下章节描述了最重要的推理基准测试指标，解释了它们的相关性和如何用于比较不同的系统。
- en: Latency & Tail Latency
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 延迟与尾部延迟
- en: Latency is one of the most critical performance metrics for inference, particularly
    in real-time applications where delays can negatively impact user experience or
    system safety. Latency refers to the time taken for an inference system to process
    an input and produce a prediction. While the average latency of a system is useful,
    it does not capture performance in high-demand scenarios where occasional delays
    can degrade reliability.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是推理的最关键性能指标之一，尤其是在实时应用中，延迟可能会对用户体验或系统安全产生负面影响。延迟是指推理系统处理输入并产生预测所需的时间。虽然系统的平均延迟是有用的，但它无法捕捉到高需求场景中的性能，在这些场景中偶尔的延迟可能会降低可靠性。
- en: To account for this, benchmarks often measure tail latency[39](#fn39), which
    reflects the worst-case delays in a system. These are typically reported as the
    95th percentile (p95) or 99th percentile (p99) latency, meaning that 95% or 99%
    of inferences are completed within a given time. For applications such as autonomous
    driving or real-time trading, maintaining low tail latency is essential to avoid
    unpredictable delays that could lead to catastrophic outcomes.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑这一点，基准测试通常测量尾部延迟[39](#fn39)，这反映了系统中的最坏情况延迟。这些通常报告为95百分位数（p95）或99百分位数（p99）延迟，意味着95%或99%的推理在给定时间内完成。对于自动驾驶或实时交易等应用，保持低尾部延迟对于避免可能导致灾难性后果的不可预测延迟至关重要。
- en: 'Tail latency’s connection to user experience at scale becomes critical in production
    systems serving millions of users. Even small P99 latency degradations create
    compounding effects across large user bases: if 1% of requests experience 10x
    latency (e.g., 1000ms instead of 100ms), this affects 10,000 users per million
    requests, potentially leading to timeout errors, poor user experience, and customer
    churn. Search engines and recommendation systems demonstrate this sensitivity:
    Google found that 500ms additional latency reduces search traffic by 20%, while
    Amazon discovered that 100ms latency increase decreases sales by 1%.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 尾部延迟与大规模生产系统中用户体验的联系变得至关重要，这些系统服务于数百万用户。即使是小的P99延迟下降也会在大用户群体中产生累积效应：如果1%的请求经历10倍延迟（例如，1000ms而不是100ms），这将影响每百万请求中的10,000个用户，可能导致超时错误、用户体验差和客户流失。搜索引擎和推荐系统展示了这种敏感性：谷歌发现额外的500ms延迟会减少20%的搜索流量，而亚马逊发现延迟增加100ms会减少1%的销售额。
- en: Service level objectives (SLOs) in production systems therefore focus on tail
    latency rather than mean latency to ensure consistent user experience. Typical
    production SLOs specify P95 < 100ms and P99 < 500ms for interactive services,
    recognizing that occasional slow responses have disproportionate impact on user
    satisfaction. Large-scale systems like Netflix and Uber optimize for P99.9 latency
    to handle traffic spikes and infrastructure variations that affect service reliability.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 生产系统中的服务级别目标（SLOs）因此关注尾部延迟而不是平均延迟，以确保一致的用户体验。典型的生产SLOs指定交互式服务的P95 < 100ms和P99
    < 500ms，认识到偶尔的缓慢响应会对用户满意度产生不成比例的影响。像Netflix和Uber这样的大规模系统优化P99.9延迟以处理流量峰值和影响服务可靠性的基础设施变化。
- en: Throughput & Batch Efficiency
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吞吐量与批量效率
- en: While latency measures the speed of individual inference requests, throughput
    measures how many inference requests a system can process per second. It is typically
    expressed in queries per second (QPS) or frames per second (FPS) for vision tasks.
    Some inference systems operate on a single-instance basis, where each input is
    processed independently as soon as it arrives. Other systems process multiple
    inputs in parallel using batch inference, which can significantly improve efficiency
    by leveraging hardware optimizations.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然延迟衡量单个推理请求的速度，但吞吐量衡量系统每秒可以处理多少个推理请求。对于视觉任务，通常以每秒查询数（QPS）或每秒帧数（FPS）表示。一些推理系统基于单实例操作，其中每个输入在到达后立即独立处理。其他系统通过批量推理并行处理多个输入，这可以通过利用硬件优化显著提高效率。
- en: For example, cloud-based services handling millions of queries per second benefit
    from batch inference, where large groups of inputs are processed together to maximize
    computational efficiency. In contrast, applications like robotics, interactive
    AI, and augmented reality require low-latency single-instance inference, where
    the system must respond immediately to each new input.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，每秒处理数百万查询的云服务从批量推理中受益，其中大量输入一起处理以最大化计算效率。相比之下，机器人、交互式AI和增强现实等应用需要低延迟的单实例推理，系统必须立即对新输入做出响应。
- en: Benchmarks must consider both single-instance and batch throughput to provide
    a comprehensive understanding of inference performance across different deployment
    scenarios.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试必须考虑单实例和批量吞吐量，以全面了解不同部署场景下的推理性能。
- en: Precision & Accuracy Trade-offs
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度与准确度权衡
- en: Optimizing inference performance often involves reducing numerical precision,
    which can significantly accelerate computation while reducing memory and energy
    consumption. However, lower-precision calculations can introduce accuracy degradation,
    making it essential to benchmark the trade-offs between speed and predictive quality.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 优化推理性能通常涉及降低数值精度，这可以显著加速计算同时减少内存和能耗。然而，低精度计算可能会引入精度下降，因此评估速度和预测质量之间的权衡变得至关重要。
- en: Inference benchmarks evaluate how well models perform under different numerical
    settings, such as FP32[40](#fn40), FP16[41](#fn41), and INT8[42](#fn42). Many
    modern AI accelerators support mixed-precision inference, allowing systems to
    dynamically adjust numerical representation based on workload requirements. Model
    compression techniques[43](#fn43) further improve efficiency, but their impact
    on model accuracy varies depending on the task and dataset. Benchmarks help determine
    whether these optimizations are viable for deployment, ensuring that improvements
    in efficiency do not come at the cost of unacceptable accuracy loss.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 推理基准测试评估模型在不同数值设置下的表现，例如FP32[40](#fn40)、FP16[41](#fn41)和INT8[42](#fn42)。许多现代AI加速器支持混合精度推理，允许系统根据工作负载需求动态调整数值表示。模型压缩技术[43](#fn43)进一步提高了效率，但它们对模型精度的影响取决于任务和数据集。基准测试有助于确定这些优化是否适用于部署，确保效率的提升不会以不可接受的精度损失为代价。
- en: Memory Footprint & Model Size
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存占用与模型大小
- en: Beyond computational optimizations, memory footprint is another critical consideration
    for inference systems, particularly for devices with limited resources. Efficient
    inference depends not only on speed but also on memory usage. Unlike training,
    where large models can be distributed across powerful GPUs or TPUs, inference
    often requires models to run within strict memory budgets. The total model size
    determines how much storage is required for deployment, while RAM usage reflects
    the working memory needed during execution. Some models require large memory bandwidth
    to efficiently transfer data between processing units, which can become a bottleneck
    if the hardware lacks sufficient capacity.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算优化之外，内存占用对于推理系统来说也是另一个关键考虑因素，尤其是对于资源有限的设备。高效的推理不仅取决于速度，还取决于内存使用。与训练不同，大型模型可以在强大的GPU或TPU上分布，推理通常要求模型在严格的内存预算内运行。总模型大小决定了部署所需的存储量，而RAM使用反映了执行期间所需的运行内存。一些模型需要较大的内存带宽来高效地在处理单元之间传输数据，如果硬件缺乏足够的容量，这可能会成为瓶颈。
- en: Inference benchmarks evaluate these factors to ensure that models can be deployed
    effectively across a range of devices. A model that achieves high accuracy but
    exceeds memory constraints may be impractical for real-world use. To address this,
    various compression techniques are often applied to reduce model size while maintaining
    accuracy. Benchmarks help assess whether these optimizations strike the right
    balance between memory efficiency and predictive performance.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 推理基准测试评估这些因素，以确保模型可以在各种设备上有效部署。一个精度高但超出内存限制的模型可能在实际应用中不切实际。为了解决这个问题，通常会应用各种压缩技术来减小模型大小同时保持精度。基准测试有助于评估这些优化是否在内存效率和预测性能之间取得了正确的平衡。
- en: Cold-Start & Model Load Time
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 冷启动与模型加载时间
- en: Once memory requirements are optimized, cold-start performance becomes critical
    for ensuring inference systems are ready to respond quickly upon deployment. In
    many deployment scenarios, models are not always kept in memory but instead loaded
    on demand when needed. This can introduce significant delays, particularly in
    serverless AI environments[44](#fn44), where resources are allocated dynamically
    based on incoming requests. Cold-start performance measures how quickly a system
    can transition from idle to active execution, ensuring that inference is available
    without excessive wait times.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦内存需求得到优化，冷启动性能对于确保推理系统在部署后能够快速响应变得至关重要。在许多部署场景中，模型并不总是保留在内存中，而是在需要时按需加载。这可能会引入显著的延迟，尤其是在无服务器AI环境中[44](#fn44)，其中资源根据传入请求动态分配。冷启动性能衡量系统从空闲状态快速切换到活动执行状态的速度，确保推理可用而无需过长的等待时间。
- en: Model load time refers to the duration required to load a trained model into
    memory before it can process inputs. In some cases, particularly on resource-limited
    devices, models must be reloaded frequently to free up memory for other applications.
    The time taken for the first inference request is also an important consideration,
    as it reflects the total delay users experience when interacting with an AI-powered
    service. Benchmarks help quantify these delays, ensuring that inference systems
    can meet real-world responsiveness requirements.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 模型加载时间指的是将训练好的模型加载到内存中以便处理输入所需的时间。在某些情况下，尤其是在资源受限的设备上，模型必须频繁重新加载以释放内存供其他应用程序使用。第一次推理请求所需的时间也是一个重要考虑因素，因为它反映了用户与AI服务交互时体验到的总延迟。基准测试有助于量化这些延迟，确保推理系统可以满足现实世界的响应速度要求。
- en: Dynamic Workload Scaling
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态工作负载扩展
- en: While cold-start latency addresses initial responsiveness, scalability ensures
    that inference systems can handle fluctuating workloads and concurrent demands
    over time Inference workloads must scale effectively across different usage patterns.
    In cloud-based AI services, this means efficiently handling millions of concurrent
    users, while on mobile or embedded devices, it involves managing multiple AI models
    running simultaneously without overloading the system.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然冷启动延迟解决了初始响应速度问题，但可扩展性确保推理系统可以随着时间的推移处理波动的工作负载和并发需求。推理工作负载必须在不同使用模式中有效地进行扩展。在基于云的AI服务中，这意味着高效地处理数百万并发用户，而在移动或嵌入式设备上，则涉及管理多个同时运行的AI模型，而不会过载系统。
- en: Scalability measures how well inference performance improves when additional
    computational resources are allocated. In some cases, adding more GPUs or TPUs
    increases throughput significantly, but in other scenarios, bottlenecks such as
    memory bandwidth limitations or network latency may limit scaling efficiency.
    Benchmarks also assess how well a system balances multiple concurrent models in
    real-world deployment, where different AI-powered features may need to run at
    the same time without interference.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性衡量在分配额外的计算资源时推理性能的改善程度。在某些情况下，添加更多的GPU或TPU可以显著提高吞吐量，但在其他场景中，如内存带宽限制或网络延迟等瓶颈可能会限制扩展效率。基准测试还评估系统在实际部署中平衡多个并发模型的能力，其中不同的AI功能可能需要同时运行而不会相互干扰。
- en: For cloud-based AI, benchmarks evaluate how efficiently a system handles fluctuating
    demand, ensuring that inference servers can dynamically allocate resources without
    compromising latency. In mobile and embedded AI, efficient multi-model execution
    is essential for running multiple AI-powered features simultaneously without degrading
    system performance.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于云的AI，基准测试评估系统处理波动需求的有效性，确保推理服务器可以动态分配资源而不影响延迟。在移动和嵌入式AI中，高效的多模型执行对于同时运行多个AI功能至关重要，而不会降低系统性能。
- en: Energy Consumption & Efficiency
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 能耗与效率
- en: Since inference workloads run continuously in production, power consumption
    and energy efficiency are critical considerations. This is particularly important
    for mobile and edge devices, where battery life and thermal constraints limit
    available computational resources. Even in large-scale cloud environments, power
    efficiency directly impacts operational costs and sustainability goals.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推理工作负载在生产环境中持续运行，功耗和能效成为关键考虑因素。这对于移动和边缘设备尤为重要，因为电池寿命和热限制限制了可用的计算资源。即使在大型云环境中，电力效率也直接影响运营成本和可持续性目标。
- en: The energy required for a single inference is often measured in joules per inference,
    reflecting how efficiently a system processes inputs while minimizing power draw.
    In cloud-based inference, efficiency is commonly expressed as queries per second
    per watt (QPS/W) to quantify how well a system balances performance and energy
    consumption. For mobile AI applications, optimizing inference power consumption
    extends battery life and allows models to run efficiently on resource-constrained
    devices. Reducing energy use also plays a key role in making large-scale AI systems
    more environmentally sustainable, ensuring that computational advancements align
    with energy-conscious deployment strategies. By balancing power consumption with
    performance, energy-efficient inference systems enable AI to scale sustainably
    across diverse applications, from data centers to edge devices.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 单次推理所需的能量通常以每推理焦耳（J/inference）来衡量，反映了系统在最小化功耗的同时处理输入的效率。在基于云的推理中，效率通常以每秒每瓦特查询数（QPS/W）来表示，以量化系统在性能和能耗之间的平衡程度。对于移动AI应用，优化推理功耗可以延长电池寿命，并允许模型在资源受限的设备上高效运行。降低能耗在使大规模AI系统更加环保可持续方面也发挥着关键作用，确保计算进步与节能部署策略相一致。通过平衡功耗与性能，节能推理系统使AI能够在各种应用中可持续扩展，从数据中心到边缘设备。
- en: Inference Performance Evaluation
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理性能评估
- en: Evaluating inference performance is a critical step in understanding how well
    machine learning systems meet the demands of real-world applications. Unlike training,
    which is typically conducted offline, inference systems must process inputs and
    generate predictions efficiently across a wide range of deployment scenarios.
    Metrics such as latency, throughput, memory usage, and energy efficiency provide
    a structured way to measure system performance and identify areas for improvement.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 评估推理性能是了解机器学习系统如何满足现实世界应用需求的关键步骤。与通常在离线进行的训练不同，推理系统必须在广泛的部署场景中高效地处理输入并生成预测。诸如延迟、吞吐量、内存使用和能源效率等指标提供了一种结构化的方式来衡量系统性能并确定改进领域。
- en: '[Table 12.3](ch018.xhtml#tbl-inference-metrics) below summarizes the key metrics
    used to evaluate inference systems, highlighting their relevance to different
    contexts. While each metric offers unique insights, it is important to approach
    inference benchmarking holistically. Trade-offs between metrics, including speed
    versus accuracy and throughput versus power consumption, are common, and understanding
    these trade-offs is essential for effective system design.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12.3](ch018.xhtml#tbl-inference-metrics) 以下总结了用于评估推理系统的关键指标，突出了它们在不同环境中的相关性。虽然每个指标都提供了独特的见解，但全面地对待推理基准测试非常重要。指标之间的权衡，包括速度与准确性的权衡以及吞吐量与功耗的权衡，是常见的，理解这些权衡对于有效的系统设计至关重要。'
- en: 'Table 12.3: **Inference Performance Metrics**: Evaluating latency, throughput,
    and resource usage provides a quantitative basis for optimizing deployed machine
    learning systems and selecting appropriate hardware configurations. Understanding
    these metrics and the trade-offs between them is crucial for balancing speed,
    cost, and accuracy in real-world applications.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.3：**推理性能指标**：评估延迟、吞吐量和资源使用为优化部署的机器学习系统和选择适当的硬件配置提供了定量基础。理解这些指标及其之间的权衡对于在现实世界应用中平衡速度、成本和准确性至关重要。
- en: '| **Category** | **Key Metrics** | **Example Benchmark Use** |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **关键指标** | **示例基准用途** |'
- en: '| --- | --- | --- |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Latency and Tail Latency** | Mean latency (ms/request); Tail latency (p95,
    p99, p99.9) | Evaluating real-time performance for safety-critical AI |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| **延迟和尾部延迟** | 平均延迟（ms/请求）；尾部延迟（p95，p99，p99.9） | 评估对安全至关重要的AI的实时性能 |'
- en: '| **Throughput and Efficiency** | Queries per second (QPS); Frames per second
    (FPS); Batch throughput | Comparing large-scale cloud inference systems |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| **吞吐量和效率** | 每秒查询数（QPS）；每秒帧数（FPS）；批量吞吐量 | 比较大规模云推理系统 |'
- en: '| **Numerical Precision Impact** | Accuracy degradation (FP32 vs. INT8); Speedup
    from reduced precision | Balancing accuracy vs. efficiency in optimized inference
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| **数值精度影响** | 准确度下降（FP32与INT8）；降低精度带来的加速 | 在优化推理中平衡准确性与效率 |'
- en: '| **Memory Footprint** | Model size (MB/GB); RAM usage (MB); Memory bandwidth
    utilization | Assessing feasibility for edge and mobile deployments |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **内存占用** | 模型大小（MB/GB）；RAM使用（MB）；内存带宽利用率 | 评估边缘和移动部署的可行性 |'
- en: '| **Cold-Start and Load Time** | Model load time (s); First inference latency
    (s) | Evaluating responsiveness in serverless AI |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| **冷启动和加载时间** | 模型加载时间（秒）；首次推理延迟（秒） | 评估无服务器AI的响应性 |'
- en: '| **Scalability** | Efficiency under load; Multi-model serving performance
    | Measuring robustness for dynamic, high-demand systems |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 负载下的效率；多模型服务性能 | 测量动态、高需求系统的鲁棒性 |'
- en: '| **Power and Energy Efficiency** | Power consumption (Watts); Performance
    per Watt (QPS/W) | Optimizing energy use for mobile and sustainable AI |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| **功率和能源效率** | 功耗（瓦特）；每瓦性能（QPS/W） | 优化移动和可持续AI的能源使用 |'
- en: Inference Systems Considerations
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理系统考虑因素
- en: Inference systems face unique challenges depending on where and how they are
    deployed. Real-time applications, such as self-driving cars or voice assistants,
    require low latency to ensure timely responses, while large-scale cloud deployments
    focus on maximizing throughput to handle millions of queries. Edge devices, on
    the other hand, are constrained by memory and power, making efficiency critical.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 推理系统根据其部署的位置和方式面临独特的挑战。实时应用，如自动驾驶汽车或语音助手，需要低延迟以确保及时响应，而大规模云部署则侧重于最大化吞吐量以处理数百万个查询。另一方面，边缘设备受限于内存和电力，使得效率变得至关重要。
- en: One of the most important aspects of evaluating inference performance is understanding
    the trade-offs between metrics. For example, optimizing for high throughput might
    increase latency, making a system unsuitable for real-time applications. Similarly,
    reducing numerical precision improves power efficiency and speed but may lead
    to minor accuracy degradation. A thoughtful evaluation must balance these trade-offs
    to align with the intended application.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 评估推理性能最重要的方面之一是理解不同指标之间的权衡。例如，为了提高吞吐量而进行的优化可能会增加延迟，使系统不适合实时应用。同样，降低数值精度可以提高功率效率和速度，但可能会导致轻微的精度下降。深思熟虑的评估必须平衡这些权衡，以符合预期的应用。
- en: The deployment environment also plays a significant role in determining evaluation
    priorities. Cloud-based systems often prioritize scalability and adaptability
    to dynamic workloads, while mobile and edge systems require careful attention
    to memory usage and energy efficiency. These differing priorities mean that benchmarks
    must be tailored to the context of the system’s use, rather than relying on one-size-fits-all
    evaluations.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 部署环境也在确定评估优先级方面发挥着重要作用。基于云的系统通常优先考虑可扩展性和对动态工作负载的适应性，而移动和边缘系统则需要仔细关注内存使用和能源效率。这些不同的优先级意味着基准必须根据系统使用的上下文量身定制，而不是依赖于一刀切的评价。
- en: Ultimately, evaluating inference performance requires a holistic approach. Focusing
    on a single metric, such as latency or energy efficiency, provides an incomplete
    picture. Instead, all relevant dimensions must be considered together to ensure
    that the system meets its functional, resource, and performance goals in a balanced
    way.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，评估推理性能需要一种全面的方法。专注于单一指标，如延迟或能源效率，只能提供一个不完整的画面。相反，必须考虑所有相关维度，以确保系统以平衡的方式满足其功能、资源和性能目标。
- en: Context-Dependent Metrics
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文相关指标
- en: Different deployment scenarios require distinctly different metric priorities,
    as the operational constraints and success criteria vary dramatically across contexts.
    Understanding these priorities allows engineers to focus benchmarking efforts
    effectively and interpret results within appropriate decision frameworks. [Table 12.4](ch018.xhtml#tbl-metric-priorities)
    illustrates how performance priorities shift across five major deployment contexts,
    revealing the systematic relationship between operational constraints and optimization
    targets.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的部署场景需要明显不同的指标优先级，因为操作约束和成功标准在各个环境中差异很大。理解这些优先级允许工程师有效地集中基准测试工作，并在适当的决策框架内解释结果。[表12.4](ch018.xhtml#tbl-metric-priorities)说明了性能优先级如何在五个主要部署环境中转变，揭示了操作约束和优化目标之间的系统性关系。
- en: 'Table 12.4: **Performance Metric Priorities by Deployment Context**: Different
    operational environments demand distinct optimization focuses, reflecting varying
    constraints and success criteria. Understanding these priorities guides both benchmark
    selection and result interpretation within appropriate decision frameworks.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.4：**按部署环境优先级排序的性能指标**：不同的操作环境需要不同的优化重点，反映了不同的约束和成功标准。理解这些优先级有助于指导基准选择和结果解释，在适当的决策框架内进行。
- en: '| **Deployment Context** | **Primary Priority** | **Secondary Priority** |
    **Tertiary Priority** | **Key Design Constraint** |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| **部署环境** | **主要优先级** | **次要优先级** | **三级优先级** | **关键设计约束** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Real-Time Applications** | Latency (p95 < 50ms) | Reliability (99.9%) |
    Memory Footprint | User experience demands immediate response |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| **实时应用** | 延迟（p95 < 50ms） | 可靠性（99.9%） | 内存占用 | 用户经验要求即时响应 |'
- en: '| **Cloud-Scale Services** | Throughput (QPS) | Cost Efficiency | Average Latency
    | Business viability requires massive scale |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| **云规模服务** | 吞吐量（QPS） | 成本效率 | 平均延迟 | 业务可行性需要大规模 |'
- en: '| **Edge/Mobile Devices** | Power Consumption | Memory Footprint | Latency
    | Battery life and resource limits dominate |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| **边缘/移动设备** | 功耗 | 内存占用 | 延迟 | 电池寿命和资源限制主导 |'
- en: '| **Training Workloads** | Training Time | GPU Utilization | Memory Efficiency
    | Research velocity enables faster experimentation |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| **训练工作负载** | 训练时间 | GPU 利用率 | 内存效率 | 研究速度使实验更快 |'
- en: '| **Scientific/Medical** | Accuracy | Reliability | Explainability | Correctness
    cannot be compromised for performance |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| **科学/医疗** | 准确性 | 可靠性 | 可解释性 | 为了性能不能妥协正确性 |'
- en: The hierarchy shown in [Table 12.4](ch018.xhtml#tbl-metric-priorities) reflects
    how operational constraints drive performance optimization strategies. Real-time
    applications exemplify latency-critical deployments where user experience depends
    on immediate system response. Autonomous vehicle perception systems must process
    sensor data within strict timing deadlines, making p95 latency more important
    than peak throughput. The table shows reliability as the secondary priority because
    system failures in autonomous vehicles carry safety implications that transcend
    performance concerns.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[表12.4](ch018.xhtml#tbl-metric-priorities)中所示的结构层次反映了操作约束如何驱动性能优化策略。实时应用是延迟关键部署的例子，其中用户体验依赖于系统的即时响应。自动驾驶车辆感知系统必须在严格的时序截止日期内处理传感器数据，这使得p95延迟比峰值吞吐量更重要。表格显示可靠性作为次要优先级，因为自动驾驶车辆中的系统故障具有超越性能问题的安全影响。'
- en: 'Conversely, cloud-scale services prioritize aggregate throughput to handle
    millions of concurrent users, accepting higher average latency in exchange for
    improved cost efficiency per query. The progression from throughput to cost efficiency
    to latency reflects economic realities: cloud providers must optimize for revenue
    per server while maintaining acceptable user experience. Notice how the same metric
    (latency) ranks as primary for real-time applications but tertiary for cloud services,
    demonstrating the context-dependent nature of performance evaluation.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，云规模服务优先考虑总吞吐量以处理数百万并发用户，通过提高查询的成本效率来接受更高的平均延迟。从吞吐量到成本效率再到延迟的演变反映了经济现实：云服务提供商必须在优化每台服务器的收入的同时，保持可接受的用户体验。注意，相同的指标（延迟）对于实时应用是主要优先级，但对于云服务是三级优先级，这证明了性能评估的上下文依赖性。
- en: Edge and mobile deployments face distinctly different constraints, where battery
    life and thermal limitations dominate design decisions. A smartphone AI assistant
    that improves throughput by 50% but increases power consumption by 30% represents
    a net regression, as reduced battery life directly impacts user satisfaction.
    Training workloads present another distinct optimization landscape, where research
    productivity depends on experiment turnaround time, making GPU utilization efficiency
    and memory bandwidth critical for enabling larger model exploration.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘和移动部署面临明显不同的约束，其中电池寿命和热限制主导设计决策。一个通过提高50%的吞吐量但增加30%功耗的智能手机AI助手代表了净退步，因为电池寿命的减少直接影响用户满意度。训练工作负载呈现另一个独特的优化领域，其中研究生产力取决于实验周转时间，这使得GPU利用效率和内存带宽对于实现更大模型探索至关重要。
- en: Scientific and medical applications establish accuracy and reliability as non-negotiable
    requirements, with performance optimization serving these primary objectives rather
    than substituting for them. A medical diagnostic system achieving 99.2% accuracy
    at 10ms latency provides superior value compared to 98.8% accuracy at 5ms latency,
    demonstrating how context-specific priorities guide meaningful performance evaluation.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 科学和医疗应用将准确性和可靠性确立为不可协商的要求，性能优化服务于这些主要目标而不是替代它们。一个在10ms延迟下实现99.2%准确性的医疗诊断系统，与在5ms延迟下实现98.8%准确性的系统相比，提供了更高的价值，这证明了特定上下文优先级如何引导有意义的性能评估。
- en: This prioritization framework fundamentally shapes benchmark interpretation
    and optimization strategies. Achieving 2x throughput improvement represents significant
    value for cloud deployments but provides minimal benefit for battery-powered edge
    devices where 20% power reduction delivers superior operational impact.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这个优先级框架从根本上塑造了基准测试的解释和优化策略。实现2倍吞吐量提升对云部署具有显著价值，但对于电池供电的边缘设备来说，20%的电力减少就能带来更好的操作影响。
- en: Inference Benchmark Pitfalls
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理基准测试陷阱
- en: Even with well-defined metrics, benchmarking inference systems can be challenging.
    Missteps during the evaluation process often lead to misleading conclusions. Below
    are common pitfalls that students and practitioners should be aware of when analyzing
    inference performance.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有明确定义的指标，基准测试推理系统也可能具有挑战性。评估过程中的失误往往会导致误导性的结论。以下是在分析推理性能时学生和从业者应了解的常见陷阱。
- en: Overemphasis on Average Latency
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 过度强调平均延迟
- en: While average latency provides a baseline measure of response time, it fails
    to capture how a system performs under peak load. In real-world scenarios, worst-case
    latency, which is captured through metrics such as p95[45](#fn45) or p99[46](#fn46)
    tail latency, can significantly impact system reliability. For instance, a conversational
    AI system may fail to provide timely responses if occasional latency spikes exceed
    acceptable thresholds.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然平均延迟提供了响应时间的基准度量，但它无法捕捉系统在峰值负载下的性能。在现实场景中，最坏情况下的延迟，如通过p95[45](#fn45)或p99[46](#fn46)尾部延迟等指标捕捉到的，可能会对系统可靠性产生重大影响。例如，如果偶尔的延迟峰值超过可接受阈值，会话式人工智能系统可能无法及时响应。
- en: Ignoring Memory & Energy Constraints
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略内存和能源限制
- en: A model with excellent throughput or latency may be unsuitable for mobile or
    edge deployments if it requires excessive memory or power. For example, an inference
    system designed for cloud environments might fail to operate efficiently on a
    battery-powered device. Proper benchmarks must consider memory footprint and energy
    consumption to ensure practicality across deployment contexts.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有出色吞吐量或延迟的模型，如果需要过多的内存或电力，可能不适合移动或边缘部署。例如，为云环境设计的推理系统可能在电池供电设备上无法高效运行。适当的基准测试必须考虑内存占用和能耗，以确保在不同部署环境中的实用性。
- en: Ignoring Cold-Start Performance
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略冷启动性能
- en: In serverless environments, where models are loaded on demand, cold-start latency[47](#fn47)
    is a critical factor. Ignoring the time it takes to initialize a model and process
    the first request can result in unrealistic expectations for responsiveness. Evaluating
    both model load time and first-inference latency ensures that systems are designed
    to meet real-world responsiveness requirements.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在按需加载模型的函数式环境中，冷启动延迟[47](#fn47)是一个关键因素。忽略初始化模型和处理第一个请求所需的时间可能导致对响应速度的不切实际的期望。评估模型加载时间和首次推理延迟确保系统设计能够满足现实世界的响应速度要求。
- en: Isolated Metrics Evaluation
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 独立指标评估
- en: Benchmarking inference systems often involves balancing competing metrics. For
    example, maximizing batch throughput might degrade latency, while aggressive precision
    reduction could reduce accuracy. Focusing on a single metric without considering
    its impact on others can lead to incomplete or misleading evaluations.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试推理系统通常涉及平衡相互竞争的指标。例如，最大化批量吞吐量可能会降低延迟，而激进地降低精度可能会降低准确性。只关注单一指标而忽略其对其他指标的影响可能导致评估不完整或误导。
- en: Numerical precision optimization exemplifies this challenge particularly well.
    Individual accelerator benchmarks show INT8 operations achieving 4x higher TOPS[48](#fn48)
    (Tera Operations Per Second) compared to FP32, creating compelling performance
    narratives.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 数值精度优化很好地体现了这一挑战。单个加速器基准测试显示INT8操作相比FP32实现了4倍更高的TOPS[48](#fn48)（每秒万亿操作），创造了引人注目的性能叙事。
- en: Linear Scaling Assumption
  id: totrans-439
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线性缩放假设
- en: Inference performance does not always scale proportionally with additional resources.
    Bottlenecks such as memory bandwidth, thermal limits, or communication overhead
    can limit the benefits of adding more GPUs or TPUs. As discussed in [Chapter 11](ch017.xhtml#sec-ai-acceleration),
    these scaling limitations arise from fundamental hardware constraints and interconnect
    architectures. Benchmarks that assume linear scaling behavior may overestimate
    system performance, particularly in distributed deployments.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 推理性能并不总是与额外资源的增加成比例。内存带宽、热限制或通信开销等瓶颈可能会限制添加更多GPU或TPU带来的好处。如第11章[第11章](ch017.xhtml#sec-ai-acceleration)所述，这些扩展限制源于基本的硬件约束和互连架构。假设线性扩展行为的基准测试可能会高估系统性能，尤其是在分布式部署中。
- en: Ignoring Application Requirements
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 忽略应用需求
- en: Generic benchmarking results may fail to account for the specific needs of an
    application. For instance, a benchmark optimized for cloud inference might be
    irrelevant for edge devices, where energy and memory constraints dominate. Tailoring
    benchmarks to the deployment context ensures that results are meaningful and actionable.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 通用基准测试结果可能无法满足特定应用的需求。例如，针对云推理优化的基准测试可能对边缘设备不相关，在边缘设备中，能源和内存限制占主导地位。根据部署环境定制基准测试确保结果具有意义且可操作。
- en: Statistical Significance & Noise
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统计显著性及噪声
- en: Distinguishing meaningful performance improvements from measurement noise requires
    proper statistical analysis. Following the evaluation methodology principles established
    earlier, MLPerf addresses measurement variability by requiring multiple benchmark
    runs and reporting percentile-based metrics rather than single measurements ([Reddi
    et al. 2019b](ch058.xhtml#ref-reddi2020mlperf)). For instance, MLPerf Inference
    reports 99th percentile latency alongside mean performance, capturing both typical
    behavior and worst-case scenarios that single-run measurements might miss. This
    approach recognizes that system performance naturally varies due to factors like
    thermal throttling, memory allocation patterns, and background processes.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 区分有意义的性能改进与测量噪声需要适当的统计分析。遵循之前建立的评估方法原则，MLPerf通过要求多次基准测试和报告基于百分比的指标来解决测量变异性([Reddi等人2019b](ch058.xhtml#ref-reddi2020mlperf))。例如，MLPerf推理报告了99百分位延迟和平均性能，同时捕捉到单次测量可能错过的典型行为和最坏情况。这种方法认识到系统性能因热限制、内存分配模式和后台进程等因素而自然变化。
- en: Inference Benchmark Synthesis
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理基准测试合成
- en: Inference benchmarks are essential tools for understanding system performance,
    but their utility depends on careful and holistic evaluation. Metrics like latency,
    throughput, memory usage, and energy efficiency provide valuable insights, but
    their importance varies depending on the application and deployment context. Students
    should approach benchmarking as a process of balancing multiple priorities, rather
    than optimizing for a single metric.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 推理基准测试是理解系统性能的关键工具，但其效用取决于仔细和全面的评估。如延迟、吞吐量、内存使用和能效等指标提供了有价值的见解，但它们的重要性取决于应用和部署环境。学生应将基准测试视为平衡多个优先级的流程，而不是优化单一指标。
- en: Avoiding common pitfalls and considering the trade-offs between different metrics
    allows practitioners to design inference systems that are reliable, efficient,
    and suitable for real-world deployment. The ultimate goal of benchmarking is to
    guide system improvements that align with the demands of the intended application.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 避免常见陷阱并考虑不同指标之间的权衡，使从业者能够设计出可靠、高效且适合实际部署的推理系统。基准测试的最终目标是引导与预期应用需求相一致的系统改进。
- en: MLPerf Inference Benchmarks
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPerf推理基准测试
- en: The MLPerf Inference benchmark, developed by MLCommons[49](#fn49), provides
    a standardized framework for evaluating machine learning inference performance
    across a range of deployment environments. Initially, MLPerf started with a single
    inference benchmark, but as machine learning systems expanded into diverse applications,
    it became clear that a one-size-fits-all benchmark was insufficient. Different
    inference scenarios, including cloud-based AI services and resource-constrained
    embedded devices, demanded tailored evaluations. This realization led to the development
    of a family of MLPerf inference benchmarks, each designed to assess performance
    within a specific deployment setting.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 由 MLCommons[49](#fn49) 开发的 MLPerf Inference 基准测试，提供了一个标准化的框架，用于评估不同部署环境下的机器学习推理性能。最初，MLPerf
    仅从一个推理基准测试开始，但随着机器学习系统扩展到各种应用，很明显，一个通用的基准测试是不够的。不同的推理场景，包括基于云的 AI 服务和资源受限的嵌入式设备，需要定制的评估。这一认识导致了
    MLPerf 推理基准测试系列的发展，每个基准测试都是为了评估特定部署环境中的性能。
- en: MLPerf Inference
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLPerf Inference
- en: '[MLPerf Inference](https://mlcommons.org/en/inference-datacenter/) serves as
    the baseline benchmark, originally designed to evaluate large-scale inference
    systems. It primarily focuses on data center and cloud-based inference workloads,
    where high throughput, low latency, and efficient resource utilization are essential.
    The benchmark assesses performance across a range of deep learning models, including
    image classification, object detection, natural language processing, and recommendation
    systems. This version of MLPerf remains the gold standard for comparing AI accelerators,
    GPUs, TPUs, and CPUs in high-performance computing environments.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLPerf Inference](https://mlcommons.org/en/inference-datacenter/) 作为基准测试的基准，最初是为了评估大规模推理系统而设计的。它主要关注数据中心和基于云的推理工作负载，在这些场景中，高吞吐量、低延迟和高效资源利用至关重要。该基准测试评估了包括图像分类、目标检测、自然语言处理和推荐系统在内的多种深度学习模型的性能。这个版本的
    MLPerf 仍然是比较高性能计算环境中 AI 加速器、GPU、TPU 和 CPU 的黄金标准。'
- en: Major technology companies regularly reference MLPerf results for hardware procurement
    decisions. When evaluating hardware for recommendation systems infrastructure,
    MLPerf benchmark scores on DLRM[50](#fn50) (Deep Learning Recommendation Model)
    workloads directly inform choices between different accelerator generations. Benchmarks
    consistently show that newer GPU architectures deliver 2-3x higher throughput
    on recommendation inference compared to previous generations, often justifying
    premium costs for production deployment at scale. This demonstrates how standardized
    benchmarks translate directly into multi-million dollar infrastructure decisions
    across the industry.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 主要技术公司通常会参考 MLPerf 结果来做出硬件采购决策。在评估推荐系统基础设施的硬件时，MLPerf 基准测试在 DLRM[50](#fn50)（深度学习推荐模型）工作负载上的分数会直接影响到不同加速器代际之间的选择。基准测试持续显示，与上一代相比，新的
    GPU 架构在推荐推理上的吞吐量提高了 2-3 倍，这往往足以证明大规模生产部署时高额成本的合理性。这展示了标准化基准测试如何直接转化为整个行业数百万美元的基础设施决策。
- en: '**The Cost of Comprehensive Benchmarking**'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**全面基准测试的成本**'
- en: While benchmarking is essential for ML system development, it comes with substantial
    costs that limit participation to well-resourced organizations. Submitting to
    MLPerf can require significant engineering effort and hundreds of thousands of
    dollars in hardware and cloud compute time. A comprehensive MLPerf Training submission
    involves months of engineering time for optimization, tuning, and validation across
    multiple hardware configurations. The computational costs alone can exceed $100,000
    for a full submission covering multiple workloads and system scales.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基准测试对于机器学习系统开发至关重要，但它伴随着巨大的成本，限制了参与者的范围，仅限于资源充足的机构。提交 MLPerf 可能需要大量的工程努力，以及数十万美元的硬件和云计算时间。一个全面的
    MLPerf 训练提交需要数月的工程时间进行优化、调整和验证，涵盖多个硬件配置。仅计算成本就可能超过 10 万美元，以涵盖多个工作负载和系统规模的全套提交。
- en: This cost barrier explains why MLPerf submissions are dominated by major technology
    companies and hardware vendors, while smaller organizations rely on published
    results rather than conducting their own comprehensive evaluations. The high barrier
    to entry motivates the need for more lightweight, internal benchmarking practices
    that organizations can use to make informed decisions without the expense of full-scale
    standardized benchmarking.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这种成本障碍解释了为什么 MLPerf 提交主要由主要技术公司和硬件供应商主导，而较小的组织则依赖于已发布的结果而不是进行他们自己的全面评估。进入门槛的高要求促使需要更多轻量级的内部基准测试实践，组织可以使用这些实践在不花费全面标准化基准测试费用的情况下做出明智的决策。
- en: MLPerf Mobile
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLPerf Mobile
- en: '[MLPerf Mobile](https://mlcommons.org/en/mlperf-mobile/) extends MLPerf’s evaluation
    framework to smartphones and other mobile devices. Unlike cloud-based inference,
    mobile inference operates under strict power and memory constraints, requiring
    models to be optimized for efficiency without sacrificing responsiveness. The
    benchmark measures latency and responsiveness for real-time AI tasks, such as
    camera-based scene detection, speech recognition, and augmented reality applications.
    MLPerf Mobile has become an industry standard for assessing AI performance on
    flagship smartphones and mobile AI chips, helping developers optimize models for
    on-device AI workloads.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLPerf Mobile](https://mlcommons.org/en/mlperf-mobile/) 将 MLPerf 的评估框架扩展到智能手机和其他移动设备。与基于云的推理不同，移动推理在严格的电源和内存限制下运行，要求模型在保持响应性的同时进行优化以提高效率。该基准测试测量实时人工智能任务的延迟和响应性，例如基于摄像头的场景检测、语音识别和增强现实应用。MLPerf
    Mobile 已成为评估旗舰智能手机和移动人工智能芯片上人工智能性能的行业标准，帮助开发者优化适用于设备上人工智能工作负载的模型。'
- en: MLPerf Client
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLPerf 客户端
- en: '[MLPerf Client](https://mlcommons.org/en/inference-edge/) focuses on inference
    performance on consumer computing devices, such as laptops, desktops, and workstations.
    This benchmark addresses local AI workloads that run directly on personal devices,
    eliminating reliance on cloud inference. Tasks such as real-time video editing,
    speech-to-text transcription, and AI-enhanced productivity applications fall under
    this category. Unlike cloud-based benchmarks, MLPerf Client evaluates how AI workloads
    interact with general-purpose hardware, such as CPUs, discrete GPUs, and integrated
    Neural Processing Units (NPUs), making it relevant for consumer and enterprise
    AI applications.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLPerf 客户端](https://mlcommons.org/en/inference-edge/) 关注于消费计算设备上的推理性能，例如笔记本电脑、台式机和工作站。这个基准测试针对直接在个人设备上运行的本地人工智能工作负载，消除了对云推理的依赖。实时视频编辑、语音转文字转录和人工智能增强生产力应用等任务属于此类。与基于云的基准测试不同，MLPerf
    客户端评估人工智能工作负载与通用硬件（如 CPU、离散 GPU 和集成神经网络处理器（NPUs））的交互，使其对消费和企业人工智能应用相关。'
- en: MLPerf Tiny
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLPerf Tiny
- en: '[MLPerf Tiny](https://mlcommons.org/en/inference-tiny/) was created to benchmark
    embedded and ultra-low-power AI systems, such as IoT devices, wearables, and microcontrollers.
    Unlike other MLPerf benchmarks, which assess performance on powerful accelerators,
    MLPerf Tiny evaluates inference on devices with limited compute, memory, and power
    resources. This benchmark is particularly relevant for applications such as smart
    sensors, AI-driven automation, and real-time industrial monitoring, where models
    must run efficiently on hardware with minimal processing capabilities. MLPerf
    Tiny plays a crucial role in the advancement of AI at the edge, helping developers
    optimize models for constrained environments.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLPerf Tiny](https://mlcommons.org/en/inference-tiny/) 的创建是为了对嵌入式和超低功耗人工智能系统进行基准测试，例如物联网设备、可穿戴设备和微控制器。与其他评估在强大加速器上性能的
    MLPerf 基准测试不同，MLPerf Tiny 评估的是在计算、内存和电源资源有限的设备上的推理性能。这个基准测试对于智能传感器、AI 驱动的自动化和实时工业监控等应用尤其相关，在这些应用中，模型必须在处理能力最少的硬件上高效运行。MLPerf
    Tiny 在边缘人工智能的进步中发挥着关键作用，帮助开发者优化适用于受限环境的模型。'
- en: Evolution and Future Directions
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 进化与未来方向
- en: The evolution of MLPerf Inference from a single benchmark to a spectrum of benchmarks
    reflects the diversity of AI deployment scenarios. Different environments, including
    cloud, mobile, desktop, and embedded environments, have unique constraints and
    requirements, and MLPerf provides a structured way to evaluate AI models accordingly.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf 推理从单一基准测试到一系列基准测试的演变反映了人工智能部署场景的多样性。不同的环境，包括云、移动、桌面和嵌入式环境，都有独特的约束和要求，MLPerf
    提供了一种结构化的方式来相应地评估人工智能模型。
- en: 'MLPerf is an essential tool for:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf 是一个必不可少的工具：
- en: Understanding how inference performance varies across deployment settings.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解推理性能在不同部署设置中的变化。
- en: Learning which performance metrics are most relevant for different AI applications.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习哪些性能指标对不同人工智能应用最为相关。
- en: Optimizing models and hardware choices based on real-world usage constraints.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据实际使用约束优化模型和硬件选择。
- en: Recognizing the necessity of tailored inference benchmarks deepens our understanding
    of AI deployment challenges and highlights the importance of benchmarking in developing
    efficient, scalable, and practical machine learning systems.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到定制推理基准的必要性加深了我们对于人工智能部署挑战的理解，并突出了基准测试在开发高效、可扩展和实用机器学习系统中的重要性。
- en: Energy efficiency considerations are integrated throughout Training (Section
    8.2) and Inference (Section 8.3) benchmark methodologies, recognizing that power
    consumption affects both phases differently. Training energy costs are amortized
    across model lifetime, while inference energy costs accumulate per query and directly
    impact operational efficiency. The following analysis of power measurement techniques
    supports the energy metrics covered within each benchmarking phase.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 能源效率考虑因素贯穿于训练（第8.2节）和推理（第8.3节）基准方法中，认识到功耗对这两个阶段的影响不同。训练能源成本分摊到模型寿命中，而推理能源成本按查询累积，直接影响操作效率。以下关于功率测量技术的分析支持了每个基准测试阶段中涵盖的能源指标。
- en: Power Measurement Techniques
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功率测量技术
- en: Energy efficiency benchmarking requires specialized measurement techniques that
    account for the diverse power scales across ML deployment environments. Building
    upon energy considerations established in training and inference sections, these
    techniques enable systematic validation of optimization claims from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    and hardware efficiency improvements from [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 能源效率基准测试需要专门的测量技术，这些技术考虑了机器学习部署环境中不同的功率范围。在训练和推理部分建立能源考虑的基础上，这些技术使系统性地验证第10章中提到的优化声明和第11章中提到的硬件效率改进成为可能。
- en: While performance benchmarks help optimize speed and accuracy, they do not always
    account for energy efficiency, which has become an increasingly critical factor
    in real-world deployment. The energy efficiency principles from [Chapter 9](ch015.xhtml#sec-efficient-ai),
    balancing computational complexity, memory access patterns, and hardware utilization,
    require quantitative validation through standardized energy benchmarks. These
    benchmarks enable us to verify whether architectural optimizations from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    and hardware-aware designs from [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    actually deliver promised energy savings in practice.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然性能基准有助于优化速度和精度，但它们并不总是考虑能源效率，这在现实世界的部署中已成为一个越来越关键的因素。第9章中关于能源效率的原则，平衡计算复杂性、内存访问模式和硬件利用率，需要通过标准化的能源基准进行定量验证。这些基准使我们能够验证第10章中提到的架构优化和第11章中提到的硬件感知设计是否在实践上真正实现了承诺的能源节约。
- en: However, measuring power consumption in machine learning systems presents fundamentally
    unique challenges. The energy demands of ML models vary dramatically across deployment
    environments, spanning multiple orders of magnitude as shown in [Table 12.5](ch018.xhtml#tbl-power).
    This wide spectrum, spanning from TinyML devices consuming mere microwatts to
    data center racks requiring kilowatts, illustrates the fundamental challenge in
    creating standardized benchmarking methodologies ([Henderson et al. 2020a](ch058.xhtml#ref-henderson2020towards)).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习系统中测量功耗面临着根本性的独特挑战。机器学习模型的能源需求在部署环境中差异很大，如[表12.5](ch018.xhtml#tbl-power)所示，跨越多个数量级。从仅消耗微瓦特的TinyML设备到需要千瓦时的数据中心机架，这一广泛的范围说明了创建标准化基准测试方法的基本挑战（[Henderson等人2020a](ch058.xhtml#ref-henderson2020towards)）。
- en: 'Table 12.5: **Power Consumption Spectrum**: Machine learning deployments exhibit
    a wide range of power demands, from microwatt-scale TinyML devices to milliwatt-scale
    microcontrollers; this variability challenges the development of standardized
    energy efficiency benchmarks. Understanding these differences is crucial for optimizing
    model deployment across resource-constrained and high-performance computing environments.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.5：**功耗谱**：机器学习部署展现出广泛的功耗需求，从微瓦级的TinyML设备到毫瓦级的微控制器；这种可变性对标准化能效基准的开发提出了挑战。理解这些差异对于优化在资源受限和高性能计算环境中的模型部署至关重要。
- en: '| **Category** | **Device Type** | **Power Consumption** |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **设备类型** | **功耗** |'
- en: '| --- | --- | --- |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Tiny** | Neural Decision Processor (NDP) | 150 µW |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| **微型** | 神经决策处理器（NDP） | 150 µW |'
- en: '| **Tiny** | M7 Microcontroller | 25 mW |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| **微型** | M7 微控制器 | 25 mW |'
- en: '| **Mobile** | Raspberry Pi 4 | 3.5 W |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| **移动** | Raspberry Pi 4 | 3.5 W |'
- en: '| **Mobile** | Smartphone | 4 W |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| **移动** | 智能手机 | 4 W |'
- en: '| **Edge** | Smart Camera | 10-15 W |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| **边缘** | 智能摄像头 | 10-15 W |'
- en: '| **Edge** | Edge Server | 65-95 W |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| **边缘** | 边缘服务器 | 65-95 W |'
- en: '| **Cloud** | ML Server Node | 300-500 W |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| **云** | 机器学习服务器节点 | 300-500 W |'
- en: '| **Cloud** | ML Server Rack | 4-10 kW |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| **云** | 机器学习服务器机架 | 4-10 kW |'
- en: This dramatic range in power requirements, which spans over four orders of magnitude,
    presents significant challenges for measurement and benchmarking. Consequently,
    creating a unified methodology requires careful consideration of each scale’s
    unique characteristics. For example, accurately measuring microwatt-level consumption
    in TinyML devices demands different instrumentation and techniques than monitoring
    kilowatt-scale server racks. Any comprehensive benchmarking framework must accommodate
    these vastly different scales while ensuring measurements remain consistent, fair,
    and reproducible across diverse hardware configurations.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这种功耗需求的巨大范围，跨越了四个数量级，对测量和基准测试提出了重大挑战。因此，创建一个统一的方法需要仔细考虑每个规模独特的特征。例如，在TinyML设备中准确测量微瓦级功耗需要不同于监控千瓦级服务器机架的仪器和技术。任何全面的基准测试框架都必须适应这些截然不同的规模，同时确保测量在不同硬件配置中保持一致、公平和可重复。
- en: Power Measurement Boundaries
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功率测量边界
- en: To address these measurement challenges, [Figure 12.8](ch018.xhtml#fig-power-diagram)
    illustrates how power consumption is measured at different system scales, from
    TinyML devices to full-scale data center inference nodes. Each scenario highlights
    distinct measurement boundaries, shown in green, which indicate the components
    included in energy accounting. Components outside these boundaries, shown with
    red dashed outlines, are excluded from power measurements.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些测量挑战，[图12.8](ch018.xhtml#fig-power-diagram)展示了在不同系统规模下如何测量功耗，从TinyML设备到全规模数据中心推理节点。每个场景都突出了不同的测量边界，以绿色显示，表示包含在能源核算中的组件。这些边界之外的组件，以红色虚线表示，不包括在功率测量中。
- en: '![](../media/file200.svg)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file200.svg)'
- en: 'Figure 12.8: **Power Measurement Boundaries**: MLPerf defines system boundaries
    for power measurement, ranging from single-chip devices to full data center nodes,
    to enable fair comparisons of energy efficiency across diverse hardware platforms.
    these boundaries delineate which components’ power consumption is included in
    reported metrics, impacting the interpretation of performance results. Source:
    ([Tschand et al. 2024](ch058.xhtml#ref-tschand2024mlperf)).'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：**功率测量边界**：MLPerf定义了功率测量的系统边界，范围从单芯片设备到完整的数据中心节点，以实现不同硬件平台间能效的公平比较。这些边界界定了哪些组件的功耗包含在报告的指标中，影响了对性能结果的解释。来源：([Tschand等人2024](ch058.xhtml#ref-tschand2024mlperf))。
- en: The diagram is organized into three categories, Tiny, Inference, and Training
    examples, each reflecting different measurement scopes based on system architecture
    and deployment environment. In TinyML systems, the entire low-power SoC, including
    compute, memory, and basic interconnects, typically falls within the measurement
    boundary. Inference nodes introduce more complexity, incorporating multiple SoCs,
    local storage, accelerators, and memory, while often excluding remote storage
    and off-chip components. Training deployments span multiple racks, where only
    selected elements, including compute nodes and network switches, are measured,
    while storage systems, cooling infrastructure, and parts of the interconnect fabric
    are often excluded.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 该图分为三个类别，分别是微型、推理和训练示例，每个类别都反映了基于系统架构和部署环境的不同测量范围。在TinyML系统中，整个低功耗SoC，包括计算、内存和基本互连，通常都位于测量边界内。推理节点引入了更多的复杂性，包括多个SoC、本地存储、加速器和内存，而通常不包括远程存储和片外组件。训练部署跨越多个机架，其中仅测量选定的元素，包括计算节点和网络交换机，而存储系统、冷却基础设施和互连布线的部分通常被排除在外。
- en: System-level power measurement offers a more holistic view than measuring individual
    components in isolation. While component-level metrics (e.g., accelerator or processor
    power) are valuable for performance tuning, real-world ML workloads involve intricate
    interactions between compute units, memory systems, and supporting infrastructure.
    For instance, analysis of Google’s TensorFlow Mobile workloads shows that data
    movement accounts for 57.3% of total inference energy consumption ([Boroumand
    et al. 2018](ch058.xhtml#ref-BoroumandASPLOS2018)), highlighting how memory-bound
    operations can dominate system power usage.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级功耗测量比单独测量单个组件提供了更全面的视角。虽然组件级指标（例如，加速器或处理器功耗）对于性能调整很有价值，但现实世界的机器学习工作负载涉及计算单元、内存系统和支持基础设施之间复杂的交互。例如，对谷歌的TensorFlow
    Mobile工作负载的分析表明，数据移动占用了总推理能耗的57.3%（[Boroumand 等人 2018](ch058.xhtml#ref-BoroumandASPLOS2018)），突显了内存受限操作如何主导系统功耗。
- en: Shared infrastructure presents additional challenges. In data centers, resources
    such as cooling systems and power delivery are shared across workloads, complicating
    attribution of energy use to specific ML tasks. Cooling alone can account for
    20-30% of total facility power consumption, making it a major factor in energy
    efficiency assessments ([Barroso, Clidaras, and Hölzle 2013](ch058.xhtml#ref-barroso2022datacenter)).
    Even at the edge, components like memory and I/O interfaces may serve both ML
    and non-ML functions, further blurring measurement boundaries.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 共享基础设施带来了额外的挑战。在数据中心，冷却系统和电力供应等资源需要在不同的工作负载之间共享，这使得将能源使用归因于特定的机器学习任务变得复杂。仅冷却一项就可能占整个设施电力消耗的20-30%，使其成为能源效率评估中的主要因素（[Barroso,
    Clidaras, 和 Hölzle 2013](ch058.xhtml#ref-barroso2022datacenter)）。即使在边缘，内存和I/O接口等组件可能同时服务于机器学习和非机器学习功能，进一步模糊了测量边界。
- en: Shared infrastructure complexity is further compounded by dynamic power management
    techniques that modern systems employ to optimize energy efficiency. Dynamic voltage
    and frequency scaling (DVFS) adjusts processor voltage and clock frequency based
    on workload demands, enabling significant power reductions during periods of lower
    computational intensity. Advanced DVFS implementations using on-chip switching
    regulators can achieve substantial energy savings ([W. Kim et al. 2008](ch058.xhtml#ref-kim2008system)),
    causing power consumption to vary by 30-50% for the same ML model depending on
    system load and concurrent activity. This variability affects not only the compute
    components but also the supporting infrastructure, as reduced processor activity
    can lower cooling requirements and overall facility power draw.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 共享基础设施的复杂性因现代系统采用的动态电源管理技术而进一步加剧，这些技术旨在优化能源效率。动态电压和频率缩放（DVFS）根据工作负载需求调整处理器电压和时钟频率，在计算强度较低期间实现显著的功耗降低。使用片上开关调节器的先进DVFS实现可以显著节省能源（[W.
    Kim 等人 2008](ch058.xhtml#ref-kim2008system)），导致相同机器学习模型在不同系统负载和并发活动下的功耗变化在30-50%之间。这种可变性不仅影响计算组件，也影响支持基础设施，因为降低的处理器活动可以降低冷却需求和整体设施电力消耗。
- en: Support infrastructure, particularly cooling systems, is a major component of
    total energy consumption in large-scale deployments. Data centers must maintain
    operational temperatures, typically between 20-25°C, to ensure system reliability.
    Cooling overhead is captured in the Power Usage Effectiveness (PUE) metric, which
    ranges from 1.1 in highly efficient facilities to over 2.0 in less optimized ones
    ([Barroso, Hölzle, and Ranganathan 2019](ch058.xhtml#ref-barroso2019datacenter)).
    The interaction between compute workloads and cooling infrastructure creates complex
    dependencies; for example, power management techniques like DVFS not only reduce
    direct processor power consumption but also decrease heat generation, creating
    cascading effects on cooling requirements. Even edge devices require basic thermal
    management.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 支持基础设施，尤其是冷却系统，是大规模部署中总能耗的主要组成部分。数据中心必须维持操作温度，通常在20-25°C之间，以确保系统可靠性。冷却开销体现在电力使用效率（PUE）指标中，该指标在高度高效的设施中为1.1，而在优化较差的设施中超过2.0([Barroso,
    Hölzle, and Ranganathan 2019](ch058.xhtml#ref-barroso2019datacenter))。计算工作负载与冷却基础设施之间的相互作用产生了复杂的依赖关系；例如，像动态电压频率调整（DVFS）这样的电源管理技术不仅减少了直接处理器功耗，还降低了热量产生，对冷却需求产生了级联效应。即使是边缘设备也需要基本的热管理。
- en: Computational Efficiency vs. Power Consumption
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算效率与功耗
- en: The relationship between computational performance and energy efficiency is
    one of the most important tradeoffs in modern ML system design. As systems push
    for higher performance, they often encounter diminishing returns in energy efficiency
    due to fundamental physical limitations in semiconductor scaling and power delivery
    ([Koomey et al. 2011](ch058.xhtml#ref-koomey2011web)). This relationship is particularly
    evident in processor frequency scaling, where increasing clock frequency by 20%
    typically yields only modest performance improvements (around 5%) while dramatically
    increasing power consumption by up to 50%, reflecting the cubic relationship between
    voltage, frequency, and power consumption ([Le Sueur and Heiser 2010](ch058.xhtml#ref-le2010dynamic)).
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 计算性能与能源效率之间的关系是现代机器学习系统设计中最重要的权衡之一。随着系统追求更高的性能，它们往往因为半导体缩放和电力传输的基本物理限制而在能源效率上遇到递减的回报([Koomey等人
    2011](ch058.xhtml#ref-koomey2011web))。这种关系在处理器频率缩放中尤为明显，提高时钟频率20%通常只能带来适度的性能提升（大约5%），而功耗却可能增加高达50%，反映了电压、频率和功耗之间的立方关系([Le
    Sueur和Heiser 2010](ch058.xhtml#ref-le2010dynamic))。
- en: In deployment scenarios with strict energy constraints, particularly battery-powered
    edge devices and mobile applications, optimizing this performance-energy tradeoff
    becomes essential for practical viability. Model optimization techniques offer
    promising approaches to achieve better efficiency without significant accuracy
    degradation. Numerical precision optimization techniques, which reduce computational
    requirements while maintaining model quality, demonstrate this tradeoff effectively.
    Research shows that reduced-precision computation can maintain model accuracy
    within 1-2% of the original while delivering 3-4x improvements in both inference
    speed and energy efficiency.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有严格能源约束的部署场景中，尤其是电池供电的边缘设备和移动应用，优化这种性能-能源权衡对于实际可行性至关重要。模型优化技术提供了实现更好效率而不显著降低准确性的有希望的方法。数值精度优化技术，在保持模型质量的同时减少计算需求，有效地展示了这种权衡。研究表明，降低精度计算可以在保持模型精度在原始精度1-2%的范围内，同时将推理速度和能源效率提高3-4倍。
- en: 'These optimization strategies span three interconnected dimensions: accuracy,
    computational performance, and energy efficiency. Advanced optimization methods
    enable fine-tuned control over this tradeoff space. Similarly, model optimization
    and compression techniques require careful balancing of accuracy losses against
    efficiency gains. The optimal operating point among these factors depends heavily
    on deployment requirements and constraints; mobile applications typically prioritize
    energy efficiency to extend battery life, while cloud-based services might optimize
    for accuracy even at higher power consumption costs, leveraging economies of scale
    and dedicated cooling infrastructure.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化策略涵盖了三个相互关联的维度：准确性、计算性能和能源效率。高级优化方法能够对这种权衡空间进行精细控制。同样，模型优化和压缩技术需要在准确性损失和效率提升之间进行仔细平衡。这些因素中的最佳工作点在很大程度上取决于部署需求和限制；移动应用程序通常优先考虑能源效率以延长电池寿命，而基于云的服务可能会在更高的功耗成本下优化准确性，利用规模经济和专用冷却基础设施。
- en: As benchmarking methodologies continue to evolve, energy efficiency metrics
    are becoming increasingly central to AI system evaluation and optimization. The
    integration of power measurement standards, such as those established in MLPerf
    Power ([Tschand et al. 2024](ch058.xhtml#ref-tschand2024mlperf)), provides standardized
    frameworks for comparing energy efficiency across diverse hardware platforms and
    deployment scenarios. Future advancements in sustainable AI benchmarking will
    help researchers and engineers design systems that systematically balance performance,
    power consumption, and environmental impact, ensuring that ML systems operate
    efficiently while minimizing unnecessary energy waste and supporting broader sustainability
    goals.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基准测试方法的不断发展，能源效率指标在人工智能系统评估和优化中变得越来越重要。将功率测量标准（如MLPerf Power中建立的）的整合为比较不同硬件平台和部署场景的能源效率提供了标准化框架。未来在可持续人工智能基准测试方面的进步将帮助研究人员和工程师设计系统，这些系统能够系统地平衡性能、功耗和环境影响，确保机器学习系统高效运行，同时最大限度地减少不必要的能源浪费并支持更广泛的可持续性目标。
- en: Standardized Power Measurement
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化功率测量
- en: While power measurement techniques, such as [SPEC Power](https://www.spec.org/power/),
    have long existed for general computing systems ([Lange 2009](ch058.xhtml#ref-lange2009identifying)),
    machine learning workloads present unique challenges that require specialized
    measurement approaches. Machine learning systems exhibit distinct power consumption
    patterns characterized by phases of intense computation interspersed with data
    movement and preprocessing operations. These patterns vary significantly across
    different types of models and tasks. A large language model’s power profile looks
    very different from that of a computer vision inference task.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像[SPEC Power](https://www.spec.org/power/)这样的功率测量技术长期以来一直存在于通用计算系统中([Lange
    2009](ch058.xhtml#ref-lange2009identifying))，但机器学习工作负载提出了独特的挑战，需要专门的测量方法。机器学习系统表现出独特的功率消耗模式，特点是强烈的计算阶段与数据移动和预处理操作交替出现。这些模式在不同类型的模型和任务之间差异很大。大型语言模型的功率特征与计算机视觉推理任务的功率特征大相径庭。
- en: Direct power measurement requires careful consideration of sampling rates and
    measurement windows. For example, certain neural network architectures create
    short, intense power spikes during complex computations, requiring high-frequency
    sampling (> 1 KHz) to capture accurately. In contrast, CNN inference tends to
    show more consistent power draw patterns that can be captured with lower sampling
    rates. The measurement duration must also account for ML-specific behaviors like
    warm-up periods, where initial inferences may consume more power due to cache
    population and pipeline initialization.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 直接功率测量需要仔细考虑采样率和测量窗口。例如，某些神经网络架构在复杂计算期间会产生短暂的、强烈的功率峰值，需要高频采样（> 1 KHz）才能准确捕捉。相比之下，CNN推理通常显示出更一致的功率消耗模式，可以用较低的采样率捕捉。测量持续时间还必须考虑到机器学习特有的行为，如预热期，初始推理可能由于缓存填充和流水线初始化而消耗更多功率。
- en: Memory access patterns in ML workloads significantly impact power consumption
    measurements. While traditional compute benchmarks might focus primarily on processor
    power, ML systems often spend substantial energy moving data between memory hierarchies.
    For example, recommendation models like DLRM can spend more energy on memory access
    than computation. This requires measurement approaches that can capture both compute
    and memory subsystem power consumption.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载中的内存访问模式对功耗测量有显著影响。虽然传统的计算基准测试可能主要关注处理器功耗，但机器学习系统通常在内存层次结构之间移动数据时消耗大量能量。例如，推荐模型如DLRM在内存访问上可能比计算消耗更多的能量。这需要能够捕获计算和内存子系统功耗的测量方法。
- en: Accelerator-specific considerations further complicate power measurement. Many
    ML systems employ specialized hardware like GPUs, TPUs, or NPUs. These accelerators
    often have their own power management schemes and can operate independently of
    the main system processor. Accurate measurement requires capturing power consumption
    across all relevant compute units while maintaining proper time synchronization.
    This is particularly challenging in heterogeneous systems that may dynamically
    switch between different compute resources based on workload characteristics or
    power constraints.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器特定的考虑因素进一步复杂化了功耗测量。许多机器学习系统采用专门的硬件，如GPU、TPU或NPU。这些加速器通常有自己的电源管理方案，并且可以独立于主系统处理器运行。准确测量需要捕获所有相关计算单元的功耗，同时保持适当的时间同步。这在可能根据工作负载特征或功耗限制动态切换不同计算资源的异构系统中尤其具有挑战性。
- en: The scale and distribution of ML workloads also influences measurement methodology.
    In distributed training scenarios, power measurement must account for both local
    compute power and the energy cost of gradient synchronization across nodes. Similarly,
    edge ML deployments must consider both active inference power and the energy cost
    of model updates or data preprocessing.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载的规模和分布也影响着测量方法。在分布式训练场景中，功耗测量必须考虑到本地计算功率以及节点间梯度同步的能量成本。同样，边缘机器学习部署必须考虑主动推理的功耗以及模型更新或数据预处理的能量成本。
- en: Batch size and throughput considerations add another layer of complexity. Unlike
    traditional computing workloads, ML systems often process inputs in batches to
    improve computational efficiency. However, the relationship between batch size
    and power consumption is non-linear. While larger batches generally improve compute
    efficiency, they also increase memory pressure and peak power requirements. Measurement
    methodologies must therefore capture power consumption across different batch
    sizes to provide a complete efficiency profile.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小和吞吐量考虑因素又增加了一层复杂性。与传统的计算工作负载不同，机器学习系统通常以批处理方式处理输入以提高计算效率。然而，批次大小与功耗之间的关系是非线性的。虽然较大的批次通常可以提高计算效率，但它们也会增加内存压力和峰值功耗要求。因此，测量方法必须捕获不同批次大小的功耗，以提供完整的效率概览。
- en: System idle states require special attention in ML workloads, particularly in
    edge scenarios where systems operate intermittently, actively processing when
    new data arrives, then entering low-power states between inferences. A wake-word
    detection Tiny ML system, for instance, might only actively process audio for
    a small fraction of its operating time, making idle power consumption a critical
    factor in overall efficiency.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作中，系统空闲状态需要特别注意，尤其是在边缘场景中，系统间歇性地运行，当新数据到达时积极处理，然后在推理之间进入低功耗状态。例如，一个唤醒词检测的微型机器学习系统可能只有很小一部分运行时间处于积极处理音频状态，这使得空闲功耗成为整体效率的关键因素。
- en: Temperature effects play a crucial role in ML system power measurement. Sustained
    ML workloads can cause significant temperature increases, triggering thermal throttling
    and changing power consumption patterns. This is especially relevant in edge devices
    where thermal constraints may limit sustained performance. Measurement methodologies
    must account for these thermal effects and their impact on power consumption,
    particularly during extended benchmarking runs.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 温度效应对机器学习系统功耗测量起着至关重要的作用。持续的机器学习工作负载可能导致显著的温度升高，触发热限制并改变功耗模式。这在边缘设备中尤为重要，因为热限制可能会限制持续性能。测量方法必须考虑到这些热效应及其对功耗的影响，尤其是在长时间基准测试运行期间。
- en: MLPerf Power Case Study
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPerf 功耗案例研究
- en: MLPerf Power ([Tschand et al. 2024](ch058.xhtml#ref-tschand2024mlperf)) is a
    standard methodology for measuring energy efficiency in machine learning systems.
    This comprehensive benchmarking framework provides accurate assessment of power
    consumption across diverse ML deployments. At the datacenter level, it measures
    power usage in large-scale AI workloads, where energy consumption optimization
    directly impacts operational costs. For edge computing, it evaluates power efficiency
    in consumer devices like smartphones and laptops, where battery life constraints
    are critical. In tiny inference scenarios, it assesses energy consumption for
    ultra-low-power AI systems, particularly IoT sensors and microcontrollers operating
    with strict power budgets.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf Power ([Tschand等人2024](ch058.xhtml#ref-tschand2024mlperf)) 是衡量机器学习系统能源效率的标准方法。这个全面的基准测试框架提供了对各种机器学习部署中功耗的准确评估。在数据中心层面，它测量大规模人工智能工作负载中的功耗，能源消耗优化直接影响到运营成本。对于边缘计算，它评估智能手机和笔记本电脑等消费设备的功耗效率，其中电池寿命限制至关重要。在微型推理场景中，它评估超低功耗人工智能系统的能源消耗，特别是严格预算下的物联网传感器和微控制器。
- en: The MLPerf Power methodology applies the standardized evaluation principles
    discussed earlier, adapting to various hardware architectures from general-purpose
    CPUs to specialized AI accelerators. This approach ensures meaningful cross-platform
    comparisons while maintaining measurement integrity across different computing
    scales.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf Power方法应用了之前讨论的标准评估原则，适应从通用CPU到专用人工智能加速器的各种硬件架构。这种方法确保了跨平台的比较有意义，同时在不同计算规模上保持测量的一致性。
- en: The benchmark has accumulated thousands of reproducible measurements submitted
    by industry organizations, which demonstrates their latest hardware capabilities
    and the sector-wide focus on energy-efficient AI technology. [Figure 12.9](ch018.xhtml#fig-power-trends)
    illustrates the evolution of energy efficiency across system scales through successive
    MLPerf versions.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准积累了由行业组织提交的数千个可重复测量结果，这展示了他们的最新硬件能力以及整个行业对节能人工智能技术的关注。[图12.9](ch018.xhtml#fig-power-trends)通过连续的MLPerf版本展示了系统规模上能源效率的演变。
- en: '![](../media/file201.svg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file201.svg)'
- en: 'Figure 12.9: **Energy Efficiency Gains**: Successive MLPerf inference benchmark
    versions consistently improve energy efficiency (samples/watt) across diverse
    system scales (datacenter, edge, and tiny), reflecting ongoing advancements in
    both hardware and software optimization for AI workloads. Standardized measurement
    protocols enable meaningful comparisons of energy efficiency improvements across
    different AI systems and deployment scenarios, driving sector-wide progress toward
    sustainable AI technologies. Source: ([Tschand et al. 2024](ch058.xhtml#ref-tschand2024mlperf)).'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：**能源效率提升**：MLPerf推理基准的连续版本在多样化的系统规模（数据中心、边缘和微型）上持续提高能源效率（样本/瓦特），反映了硬件和软件优化在人工智能工作负载方面的持续进步。标准化的测量协议使得不同人工智能系统和部署场景之间能源效率改进的比较变得有意义，推动整个行业向可持续的人工智能技术发展。来源：([Tschand等人2024](ch058.xhtml#ref-tschand2024mlperf))。
- en: The MLPerf Power methodology adapts to different hardware architectures, ranging
    from general-purpose CPUs to specialized AI accelerators, while maintaining a
    uniform measurement standard. This ensures that comparisons across platforms are
    meaningful and unbiased.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf Power方法适应不同的硬件架构，从通用CPU到专用人工智能加速器，同时保持统一的测量标准。这确保了平台间的比较既有意义又无偏见。
- en: Across the versions and ML deployment scales of the MLPerf benchmark suite,
    industry organizations have submitted reproducible measurements on their most
    recent hardware to observe and quantify the industry-wide emphasis on optimizing
    AI technology for energy efficiency. [Figure 12.9](ch018.xhtml#fig-power-trends)
    shows the trends in energy efficiency from tiny to datacenter scale systems across
    MLPerf versions.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLPerf基准套件的各个版本和机器学习部署规模中，行业组织提交了他们最新硬件的可重复测量结果，以观察和量化行业对优化人工智能技术以提高能源效率的重视。![图12.9](ch018.xhtml#fig-power-trends)显示了MLPerf版本中从微型到数据中心规模系统能源效率的趋势。
- en: 'Analysis of these trends reveals two significant patterns: first, a plateauing
    of energy efficiency improvements across all three scales for traditional ML workloads,
    and second, a dramatic increase in energy efficiency specifically for generative
    AI applications. This dichotomy suggests both the maturation of optimization techniques
    for conventional ML tasks and the rapid innovation occurring in the generative
    AI space. These trends underscore the dual challenges facing the field: developing
    novel approaches to break through efficiency plateaus while ensuring sustainable
    scaling practices for increasingly powerful generative AI models.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 这些趋势的分析揭示了两个显著的模式：首先，在传统机器学习工作负载的所有三个尺度上，能源效率的改进趋于平缓；其次，对于生成式人工智能应用，能源效率显著提高。这种二分法表明，传统机器学习任务的优化技术已经成熟，生成式人工智能领域的快速创新正在发生。这些趋势强调了该领域面临的双重挑战：开发新的方法来突破效率平台，同时确保越来越强大的生成式人工智能模型可持续扩展的实践。
- en: Benchmarking Limitations and Best Practices
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试的局限性和最佳实践
- en: Effective benchmarking requires understanding its inherent limitations and implementing
    practices that mitigate these constraints. Rather than avoiding benchmarks due
    to their limitations, successful practitioners recognize these challenges and
    adapt their methodology accordingly. The following analysis examines four interconnected
    categories of benchmarking challenges while providing actionable guidance for
    addressing each limitation through improved design and interpretation practices.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的基准测试需要理解其固有的局限性并实施缓解这些限制的实践。而不是因为其局限性而避免基准测试，成功的从业者认识到这些挑战并相应地调整他们的方法。以下分析检查了四个相互关联的基准测试挑战类别，同时提供了通过改进设计和解释实践来解决每个限制的可操作指导。
- en: Statistical & Methodological Issues
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统计与方法问题
- en: The foundation of reliable benchmarking rests on sound statistical methodology.
    Three fundamental issues undermine this foundation if left unaddressed.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠基准测试的基础在于稳健的统计方法。如果未解决，以下三个基本问题会破坏这一基础。
- en: Incomplete problem coverage represents one of the most fundamental limitations.
    Many benchmarks, while useful for controlled comparisons, fail to capture the
    full diversity of real-world applications. For instance, common image classification
    datasets, such as [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), contain
    a limited variety of images. As a result, models that perform well on these datasets
    may struggle when applied to more complex, real-world scenarios with greater variability
    in lighting, perspective, and object composition. This gap between benchmark tasks
    and real-world complexity means strong benchmark performance provides limited
    guarantees about practical deployment success.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 不完整的问题覆盖是最基本的局限性之一。许多基准虽然对受控比较有用，但未能捕捉到现实世界应用的全部多样性。例如，常见的图像分类数据集，如[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)，包含有限的图像种类。因此，在这些数据集上表现良好的模型在应用于更复杂、现实世界场景时可能会遇到困难，这些场景具有更大的光照、视角和物体组成的变化。基准任务与现实世界复杂性之间的差距意味着强大的基准性能为实际部署的成功提供了有限的保证。
- en: Statistical insignificance arises when benchmark evaluations are conducted on
    too few data samples or trials. For example, testing an optical character recognition
    (OCR) system on a small dataset may not accurately reflect its performance on
    large-scale, noisy text documents. Without sufficient trials and diverse input
    distributions, benchmarking results may be misleading or fail to capture true
    system reliability. The statistical confidence intervals around benchmark scores
    often go unreported, obscuring whether measured differences represent genuine
    improvements or measurement noise.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 统计不显著出现在基准评估在太少的数据样本或试验上进行时。例如，在一个小数据集上测试光学字符识别（OCR）系统可能无法准确反映其在大规模、嘈杂文本文档上的性能。如果没有足够的试验和多样化的输入分布，基准测试结果可能会误导或未能捕捉到真正的系统可靠性。围绕基准分数的统计置信区间通常未报告，掩盖了测量的差异是否代表真正的改进或测量噪声。
- en: Reproducibility represents a major ongoing challenge. Benchmark results can
    vary significantly depending on factors such as hardware configurations, software
    versions, and system dependencies. Small differences in compilers, numerical precision,
    or library updates can lead to inconsistent performance measurements across different
    environments. To mitigate this issue, MLPerf addresses reproducibility by providing
    reference implementations, standardized test environments, and strict submission
    guidelines. Even with these efforts, achieving true consistency across diverse
    hardware platforms remains an ongoing challenge. The proliferation of optimization
    libraries, framework versions, and compiler flags creates a vast configuration
    space where slight variations produce different results.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性代表了一个持续存在的重大挑战。基准测试结果可能会因硬件配置、软件版本和系统依赖等因素而显著不同。编译器、数值精度或库更新的微小差异可能导致不同环境中的性能测量不一致。为了减轻这一问题，MLPerf通过提供参考实现、标准化的测试环境和严格的提交指南来解决可重现性问题。即使有这些努力，在多样化的硬件平台上实现真正的统一性仍然是一个持续存在的挑战。优化库、框架版本和编译器标志的激增创造了一个巨大的配置空间，其中微小的变化会产生不同的结果。
- en: Laboratory-to-Deployment Performance Gaps
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验室到部署的性能差距
- en: Beyond statistical rigor, benchmarks must align with practical deployment objectives.
    Misalignment with Real-World Goals occurs when benchmarks emphasize metrics such
    as speed, accuracy, and throughput, but practical AI deployments often require
    balancing multiple objectives, including power efficiency, cost, and robustness.
    A model that achieves state-of-the-art accuracy on a benchmark may be impractical
    for deployment if it consumes excessive energy or requires expensive hardware.
    Similarly, optimizing for average-case performance on benchmark datasets may neglect
    tail-latency requirements that determine user experience in production systems.
    The multi-objective nature of real deployment, encompassing resource constraints,
    operational costs, maintenance complexity, and business requirements, extends
    far beyond the single-metric optimization that most benchmarks reward.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统计严谨性之外，基准测试必须与实际部署目标相一致。当基准测试强调速度、准确性和吞吐量等指标时，与实际目标不匹配的情况就会发生。在实际的人工智能部署中，通常需要平衡多个目标，包括能效、成本和鲁棒性。一个在基准测试上实现最先进准确性的模型，如果消耗过多能量或需要昂贵的硬件，可能在实际部署中并不实用。同样，针对基准数据集的平均性能进行优化可能会忽视生产系统中决定用户体验的尾部延迟需求。实际部署的多目标性质，包括资源限制、运营成本、维护复杂性和商业需求，远远超出了大多数基准测试所奖励的单指标优化。
- en: System Design Challenges
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统设计挑战
- en: Physical and architectural factors introduce additional variability that benchmarks
    must address using our established comparison methodologies across diverse deployment
    contexts.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 物理和架构因素引入了额外的可变性，基准测试必须通过我们建立的跨不同部署环境下的比较方法来应对。
- en: Environmental Conditions
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境条件
- en: Environmental conditions in AI benchmarking refer to the physical and operational
    circumstances under which experiments are conducted. These conditions, while often
    overlooked in benchmark design, can significantly influence benchmark results
    and impact the reproducibility of experiments. Physical environmental factors
    include ambient temperature, humidity, air quality, and altitude. These elements
    can affect hardware performance in subtle but measurable ways. For instance, elevated
    temperatures may lead to thermal throttling in processors, potentially reducing
    computational speed and affecting benchmark outcomes. Similarly, variations in
    altitude can impact cooling system efficiency and hard drive performance due to
    changes in air pressure.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能基准测试的环境条件指的是实验进行的物理和操作环境。这些条件虽然在基准设计时往往被忽视，但可能会显著影响基准测试结果并影响实验的可重现性。物理环境因素包括环境温度、湿度、空气质量和海拔。这些因素可以通过微妙但可测量的方式影响硬件性能。例如，较高的温度可能导致处理器热限制，从而降低计算速度并影响基准测试结果。同样，海拔的变化可能会由于空气压力的变化而影响冷却系统效率和硬盘性能。
- en: Beyond physical factors, operational environmental factors encompass the broader
    system context in which benchmarks are executed. This includes background processes
    running on the system, network conditions, and power supply stability. The presence
    of other active programs or services can compete for computational resources,
    potentially altering the performance characteristics of the model under evaluation.
    To ensure the validity and reproducibility of benchmark results, it is essential
    to document and control these environmental conditions to the extent possible.
    This may involve conducting experiments in temperature-controlled environments,
    monitoring and reporting ambient conditions, standardizing the operational state
    of benchmark systems, and documenting any background processes or system loads.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 除了物理因素之外，操作环境因素涵盖了基准测试执行的更广泛系统背景。这包括系统上运行的背景进程、网络条件和电源稳定性。其他活跃程序或服务可能会竞争计算资源，从而改变正在评估的模型的表现特性。为确保基准测试结果的准确性和可重现性，尽可能详细地记录和控制这些环境条件是至关重要的。这可能涉及在温度控制环境中进行实验、监测和报告环境条件、标准化基准测试系统的操作状态，以及记录任何背景进程或系统负载。
- en: In scenarios where controlling all environmental variables is impractical, such
    as in distributed or cloud-based benchmarking, it becomes essential to report
    these conditions in detail. This information allows other researchers to account
    for potential variations when interpreting or attempting to reproduce results.
    As machine learning models are increasingly deployed in diverse real-world environments,
    understanding the impact of environmental conditions on model performance becomes
    even more critical. This knowledge not only ensures more accurate benchmarking
    but also informs the development of robust models capable of consistent performance
    across varying operational conditions.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有环境变量难以控制的情况下，例如在分布式或基于云的基准测试中，详细报告这些条件变得至关重要。这些信息使其他研究人员在解释或尝试重现结果时能够考虑到潜在的变异。随着机器学习模型在多样化的现实世界环境中越来越多地部署，了解环境条件对模型性能的影响变得尤为重要。这种知识不仅确保了更准确的基准测试，而且也指导了开发能够适应不同操作条件并保持一致性能的鲁棒模型。
- en: Hardware Lottery
  id: totrans-533
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件彩票
- en: A critical and often underappreciated issue in benchmarking is what has been
    described as the hardware lottery[51](#fn51), a concept introduced by ([Hooker
    2021](ch058.xhtml#ref-hooker2021hardware)). The success of a machine learning
    model is often dictated not only by its architecture and training data but also
    by how well it aligns with the underlying hardware used for inference. Some models
    perform exceptionally well, not because they are inherently better, but because
    they are optimized for the parallel processing capabilities of GPUs or TPUs. Meanwhile,
    other promising architectures may be overlooked because they do not map efficiently
    to dominant hardware platforms.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试中的一个关键且常被忽视的问题是所谓的硬件彩票[51](#fn51)，这是一个由([Hooker 2021](ch058.xhtml#ref-hooker2021hardware))提出的概念。机器学习模型的成功不仅取决于其架构和训练数据，还取决于其与用于推理的底层硬件的匹配程度。一些模型表现异常出色，并不是因为它们本质上更好，而是因为它们针对GPU或TPU的并行处理能力进行了优化。与此同时，一些有潜力的架构可能被忽视，因为它们无法有效地映射到主导的硬件平台。
- en: This dependence on hardware compatibility introduces subtle but significant
    biases into benchmarking results. A model that is highly efficient on a specific
    GPU may perform poorly on a CPU or a custom AI accelerator. For instance, [Figure 12.10](ch018.xhtml#fig-hw-lottery)
    compares the performance of models across different hardware platforms. The multi-hardware
    models show comparable results to “MobileNetV3 Large min” on both the CPU `uint8`
    and GPU configurations. However, these multi-hardware models demonstrate significant
    performance improvements over the MobileNetV3 Large baseline when run on the EdgeTPU
    and DSP hardware. This emphasizes the variable efficiency of multi-hardware models
    in specialized computing environments.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对硬件兼容性的依赖在基准测试结果中引入了微妙但重要的偏差。一个在特定GPU上效率很高的模型可能在CPU或定制AI加速器上表现不佳。例如，[图12.10](ch018.xhtml#fig-hw-lottery)比较了不同硬件平台上的模型性能。多硬件模型在CPU
    `uint8`和GPU配置上与“MobileNetV3 Large min”具有可比的结果。然而，当在EdgeTPU和DSP硬件上运行时，这些多硬件模型在MobileNetV3
    Large基准上显示出显著的性能提升。这强调了多硬件模型在专用计算环境中效率的变动性。
- en: '![](../media/file202.svg)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file202.svg)'
- en: 'Figure 12.10: **Hardware-Dependent Accuracy**: Model performance varies significantly
    across hardware platforms, indicating that architectural efficiency is not solely
    determined by design but also by hardware compatibility. Multi-hardware models
    exhibit comparable accuracy to mobilenetv3 large on CPU and GPU configurations,
    yet achieve substantial gains on EdgeTPU and DSP, emphasizing the importance of
    hardware-aware model optimization for specialized computing environments. Source:
    ([Chu et al. 2021](ch058.xhtml#ref-chu2021discovering)).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：**硬件依赖的准确性**：模型性能在不同硬件平台上差异显著，表明架构效率不仅由设计决定，还由硬件兼容性决定。多硬件模型在CPU和GPU配置上与mobilenetv3
    large具有可比的准确性，但在EdgeTPU和DSP上实现了实质性提升，强调了在专用计算环境中进行硬件感知模型优化的重要性。来源：([Chu等人2021](ch058.xhtml#ref-chu2021discovering))。
- en: Without careful benchmarking across diverse hardware configurations, the field
    risks favoring architectures that “win” the hardware lottery rather than selecting
    models based on their intrinsic strengths. This bias can shape research directions,
    influence funding allocation, and impact the design of next-generation AI systems.
    In extreme cases, it may even stifle innovation by discouraging exploration of
    alternative architectures that do not align with current hardware trends.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有仔细跨不同硬件配置进行基准测试的情况下，该领域可能会偏向于“赢得”硬件随机性的架构，而不是根据其内在优势选择模型。这种偏见可能会塑造研究方向，影响资金分配，并影响下一代人工智能系统的设计。在极端情况下，它甚至可能通过阻碍探索与当前硬件趋势不一致的替代架构来抑制创新。
- en: Organizational & Strategic Issues
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组织与战略问题
- en: Competitive pressures and research incentives create systematic biases in how
    benchmarks are used and interpreted. These organizational dynamics require governance
    mechanisms and community standards to maintain benchmark integrity.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争压力和研究激励导致了对基准的使用和解释存在系统性偏见。这些组织动态需要治理机制和社区标准来维护基准的完整性。
- en: Benchmark Engineering
  id: totrans-541
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准工程
- en: While the hardware lottery is an unintended consequence of hardware trends,
    benchmark engineering is an intentional practice where models or systems are explicitly
    optimized to excel on specific benchmark tests. This practice can lead to misleading
    performance claims and results that do not generalize beyond the benchmarking
    environment.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然硬件随机性是硬件趋势的意外后果，但基准工程是一种有意为之的实践，其中模型或系统被明确优化以在特定的基准测试中表现出色。这种做法可能导致误导性的性能声明和无法推广到基准测试环境之外的结果。
- en: Benchmark engineering occurs when AI developers fine-tune hyperparameters, preprocessing
    techniques, or model architectures specifically to maximize benchmark scores rather
    than improve real-world performance. For example, an object detection model might
    be carefully optimized to achieve record-low latency on a benchmark but fail when
    deployed in dynamic, real-world environments with varying lighting, motion blur,
    and occlusions. Similarly, a language model might be tuned to excel on benchmark
    datasets but struggle when processing conversational speech with informal phrasing
    and code-switching.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 当人工智能开发者针对特定基准分数微调超参数、预处理技术或模型架构时，就会发生基准工程，而不是为了提高实际性能。例如，一个目标检测模型可能会被精心优化以在基准测试中实现创纪录的低延迟，但在具有变化光照、运动模糊和遮挡的动态、现实世界环境中部署时可能会失败。同样，一个语言模型可能会被调整以在基准数据集上表现出色，但在处理非正式用语和代码切换的对话语音时可能会遇到困难。
- en: The pressure to achieve high benchmark scores is often driven by competition,
    marketing, and research recognition. Benchmarks are frequently used to rank AI
    models and systems, creating an incentive to optimize specifically for them. While
    this can drive technical advancements, it also risks prioritizing benchmark-specific
    optimizations at the expense of broader generalization. This phenomenon exemplifies
    Goodhart’s Law[52](#fn52).
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 实现高基准分数的压力通常由竞争、营销和研究认可驱动。基准经常被用来对人工智能模型和系统进行排名，从而产生针对它们进行优化的激励。虽然这可以推动技术进步，但它也冒着以牺牲更广泛的泛化能力为代价来优先考虑基准特定优化的风险。这一现象体现了Goodhart定律[52](#fn52)。
- en: Bias and Over-Optimization
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏见和过度优化
- en: To ensure that benchmarks remain useful and fair, several strategies can be
    employed. Transparency is one of the most important factors in maintaining benchmarking
    integrity. Benchmark submissions should include detailed documentation on any
    optimizations applied, ensuring that improvements are clearly distinguished from
    benchmark-specific tuning. Researchers and developers should report both benchmark
    performance and real-world deployment results to provide a complete picture of
    a system’s capabilities.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保基准测试保持有用和公平，可以采用几种策略。透明度是维护基准测试完整性的最重要的因素之一。基准测试提交应包括任何应用的优化详细文档，确保改进与基准特定的调整清晰区分。研究人员和开发者应报告基准性能和现实世界的部署结果，以提供系统能力的完整图景。
- en: Another approach is to diversify and evolve benchmarking methodologies. Instead
    of relying on a single static benchmark, AI systems should be evaluated across
    multiple, continuously updated benchmarks that reflect real-world complexity.
    This reduces the risk of models being overfitted to a single test set and encourages
    general-purpose improvements rather than narrow optimizations.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是多样化和进化基准测试方法。而不是依赖于单一静态的基准，人工智能系统应该被评估在多个、持续更新的基准上，这些基准反映了现实世界的复杂性。这降低了模型过度拟合单一测试集的风险，并鼓励通用改进而不是狭窄优化。
- en: Standardization and third-party verification can also help mitigate bias. By
    establishing industry-wide benchmarking standards and requiring independent third-party
    audits of results, the AI community can improve the reliability and credibility
    of benchmarking outcomes. Third-party verification ensures that reported results
    are reproducible across different settings and helps prevent unintentional benchmark
    gaming.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化和第三方验证也可以帮助减轻偏差。通过建立行业范围内的基准测试标准并要求对结果进行独立的第三方审计，人工智能社区可以提高基准测试结果的可信度和可靠性。第三方验证确保报告的结果可以在不同的环境中重复，并有助于防止无意的基准测试操纵。
- en: Another important strategy is application-specific testing. While benchmarks
    provide controlled evaluations, real-world deployment testing remains essential.
    AI models should be assessed not only on benchmark datasets but also in practical
    deployment environments. For instance, an autonomous driving model should be tested
    in a variety of weather conditions and urban settings rather than being judged
    solely on controlled benchmark datasets.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的策略是应用特定测试。虽然基准测试提供了受控评估，但现实世界的部署测试仍然至关重要。人工智能模型不仅应在基准数据集上评估，还应在实际部署环境中进行评估。例如，自动驾驶模型应在各种天气条件和城市环境中进行测试，而不仅仅是根据受控的基准测试数据集进行评判。
- en: Finally, fairness across hardware platforms must be considered. Benchmarks should
    test AI models on multiple hardware configurations to ensure that performance
    is not being driven solely by compatibility with a specific platform. This helps
    reduce the risk of the hardware lottery and provides a more balanced evaluation
    of AI system efficiency.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，必须考虑硬件平台之间的公平性。基准测试应该在多种硬件配置上测试人工智能模型，以确保性能不是仅仅由与特定平台的兼容性驱动的。这有助于降低硬件彩票的风险，并为人工智能系统效率提供更平衡的评估。
- en: Benchmark Evolution
  id: totrans-551
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准测试进化
- en: One of the greatest challenges in benchmarking is that benchmarks are never
    static. As AI systems evolve, so must the benchmarks that evaluate them. What
    defines “good performance” today may be irrelevant tomorrow as models, hardware,
    and application requirements change. While benchmarks are essential for tracking
    progress, they can also quickly become outdated, leading to over-optimization
    for old metrics rather than real-world performance improvements.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准测试中面临的一个最大挑战是，基准测试永远不会静止不变。随着人工智能系统的不断发展，评估它们的基准也必须随之进化。今天定义的“良好性能”可能在明天就变得无关紧要，因为模型、硬件和应用需求都在变化。虽然基准测试对于跟踪进展至关重要，但它们也可能迅速过时，导致过度优化旧指标而不是真正的性能提升。
- en: This evolution is evident in the history of AI benchmarks. Early model benchmarks,
    for instance, focused heavily on image classification and object detection, as
    these were some of the first widely studied deep learning tasks. However, as AI
    expanded into natural language processing, recommendation systems, and generative
    AI, it became clear that these early benchmarks no longer reflected the most important
    challenges in the field. In response, new benchmarks emerged to measure language
    understanding ([A. Wang et al. 2018](ch058.xhtml#ref-wang2018glue), [2019](ch058.xhtml#ref-wang2019superglue))
    and generative AI ([Liang et al. 2022](ch058.xhtml#ref-liang2022helm)).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 这种演变在AI基准测试的历史中表现得非常明显。例如，早期的模型基准测试主要关注图像分类和目标检测，因为这些是首批广泛研究的深度学习任务之一。然而，随着AI扩展到自然语言处理、推荐系统和生成式AI，很明显，这些早期的基准测试不再反映该领域最重要的挑战。为此，新的基准测试出现，以衡量语言理解([Wang等人2018](ch058.xhtml#ref-wang2018glue),
    [2019](ch058.xhtml#ref-wang2019superglue))和生成式AI([Liang等人2022](ch058.xhtml#ref-liang2022helm))。
- en: 'Benchmark evolution extends beyond the addition of new tasks to encompass new
    dimensions of performance measurement. While traditional AI benchmarks emphasized
    accuracy and throughput, modern applications demand evaluation across multiple
    criteria: fairness, robustness, scalability, and energy efficiency. [Figure 12.11](ch018.xhtml#fig-sciml-graph)
    illustrates this complexity through scientific applications, which span orders
    of magnitude in their performance requirements. For instance, Large Hadron Collider
    sensors must process data at rates approaching 10<semantics><msup><mn>14</mn></msup><annotation
    encoding="application/x-tex">^{14}</annotation></semantics> bytes per second (equivalent
    to about 100 terabytes per second) with nanosecond-scale computation times, while
    mobile applications operate at 10<semantics><msup><mn>4</mn></msup><annotation
    encoding="application/x-tex">^{4}</annotation></semantics> bytes per second with
    longer computational windows. This range of requirements necessitates specialized
    benchmarks. For example, edge AI applications require benchmarks like MLPerf that
    specifically evaluate performance under resource constraints and scientific application
    domains need their own “Fast ML for Science” benchmarks ([Duarte et al. 2022a](ch058.xhtml#ref-duarte2022fastml)).'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试的演变不仅限于新任务的添加，还包括性能测量新维度的涵盖。虽然传统的AI基准测试强调准确性和吞吐量，但现代应用需要跨多个标准进行评估：公平性、鲁棒性、可扩展性和能效。[图12.11](ch018.xhtml#fig-sciml-graph)通过科学应用展示了这种复杂性，这些应用在性能要求上跨越了多个数量级。例如，大型强子对撞机传感器必须以接近10<semantics><msup><mn>14</mn></msup><annotation
    encoding="application/x-tex">^{14}</annotation></semantics>字节每秒的速率处理数据（相当于大约每秒100太字节）并具有纳秒级的计算时间，而移动应用则以每秒10<semantics><msup><mn>4</mn></msup><annotation
    encoding="application/x-tex">^{4}</annotation></semantics>字节的速率运行，具有更长的计算窗口。这种需求范围需要专门的基准测试。例如，边缘AI应用需要像MLPerf这样的基准测试，它专门评估在资源限制下的性能，而科学应用领域需要它们自己的“快速科学机器学习”基准测试([Duarte等人2022a](ch058.xhtml#ref-duarte2022fastml)))。
- en: '![](../media/file203.svg)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file203.svg)'
- en: 'Figure 12.11: **Performance Spectrum**: Scientific applications and edge devices
    demand vastly different computational resources, spanning multiple orders of magnitude
    in data rates and latency requirements. Consequently, traditional benchmarks focused
    solely on accuracy are insufficient; specialized evaluation metrics and benchmarks
    like MLPerf become essential for optimizing AI systems across diverse deployment
    scenarios. Source: ([Duarte et al. 2022b](ch058.xhtml#ref-duarte2022fastmlsciencebenchmarksaccelerating)).'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：**性能谱**：科学应用和边缘设备对计算资源的需求差异极大，数据速率和延迟要求跨越多个数量级。因此，仅关注准确性的传统基准测试是不够的；针对特定评估指标和基准测试，如MLPerf，对于优化跨不同部署场景的AI系统变得至关重要。来源：([Duarte等人2022b](ch058.xhtml#ref-duarte2022fastmlsciencebenchmarksaccelerating))。
- en: 'The need for evolving benchmarks also presents a challenge: stability versus
    adaptability. On the one hand, benchmarks must remain stable for long enough to
    allow meaningful comparisons over time. If benchmarks change too frequently, it
    becomes difficult to track long-term progress and compare new results with historical
    performance. On the other hand, failing to update benchmarks leads to stagnation,
    where models are optimized for outdated tasks rather than advancing the field.
    Striking the right balance between benchmark longevity and adaptation is an ongoing
    challenge for the AI community.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 需要不断演进的基准测试也带来了一项挑战：稳定性与适应性之间的平衡。一方面，基准测试必须保持足够稳定，以便在一段时间内进行有意义的比较。如果基准测试变化过于频繁，就难以追踪长期进展并将新结果与历史性能进行比较。另一方面，未能更新基准测试会导致停滞，模型将针对过时的任务进行优化，而不是推动领域的发展。在基准测试的长期性和适应性之间取得正确的平衡是人工智能社区持续面临的挑战。
- en: Despite these difficulties, evolving benchmarks is essential for ensuring that
    AI progress remains meaningful. Without updates, benchmarks risk becoming detached
    from real-world needs, leading researchers and engineers to focus on optimizing
    models for artificial test cases rather than solving practical challenges. As
    AI continues to expand into new domains, benchmarking must keep pace, ensuring
    that performance evaluations remain relevant, fair, and aligned with real-world
    deployment scenarios.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些困难，但不断演进基准测试对于确保人工智能进展具有意义至关重要。如果没有更新，基准测试可能会脱离现实世界的需求，导致研究人员和工程师专注于优化模型以适应人工测试案例，而不是解决实际挑战。随着人工智能继续扩展到新的领域，基准测试必须保持同步，确保性能评估保持相关、公平并与现实世界部署场景保持一致。
- en: MLPerf as Industry Standard
  id: totrans-559
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPerf作为行业标准
- en: MLPerf has played a crucial role in improving benchmarking by reducing bias,
    increasing generalizability, and ensuring benchmarks evolve alongside AI advancements.
    One of its key contributions is the standardization of benchmarking environments.
    By providing reference implementations, clearly defined rules, and reproducible
    test environments, MLPerf ensures that performance results are consistent across
    different hardware and software platforms, reducing variability in benchmarking
    outcomes.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: MLPerf在通过减少偏差、增加泛化性和确保基准测试与人工智能进步同步发展方面发挥了关键作用。其关键贡献之一是基准测试环境的标准化。通过提供参考实现、明确定义的规则和可重复的测试环境，MLPerf确保了性能结果在不同硬件和软件平台之间的一致性，从而减少了基准测试结果的可变性。
- en: Recognizing that AI is deployed in a variety of real-world settings, MLPerf
    has also introduced different categories of inference benchmarks that align with
    our three-dimensional framework. The inclusion of MLPerf Inference, MLPerf Mobile,
    MLPerf Client, and MLPerf Tiny reflects an effort to evaluate models across different
    deployment constraints while maintaining the systematic evaluation principles
    established throughout this chapter.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到人工智能被部署在各种现实世界环境中，MLPerf还引入了与我们的三维框架相一致的不同类别的推理基准测试。MLPerf推理、MLPerf移动、MLPerf客户端和MLPerf微型的纳入反映了评估模型在不同部署约束下的努力，同时保持本章中确立的系统评估原则。
- en: Beyond providing a structured benchmarking framework, MLPerf is continuously
    evolving to keep pace with the rapid progress in AI. New tasks are incorporated
    into benchmarks to reflect emerging challenges, such as generative AI models and
    energy-efficient computing, ensuring that evaluations remain relevant and forward-looking.
    By regularly updating its benchmarking methodologies, MLPerf helps prevent benchmarks
    from becoming outdated or encouraging overfitting to legacy performance metrics.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供结构化的基准测试框架之外，MLPerf还在不断演进，以跟上人工智能快速发展的步伐。新的任务被纳入基准测试，以反映新兴的挑战，例如生成式人工智能模型和节能计算，确保评估保持相关性和前瞻性。通过定期更新其基准测试方法，MLPerf有助于防止基准测试过时或鼓励对旧性能指标的过度拟合。
- en: By prioritizing fairness, transparency, and adaptability, MLPerf ensures that
    benchmarking remains a meaningful tool for guiding AI research and deployment.
    Instead of simply measuring raw speed or accuracy, MLPerf’s evolving benchmarks
    aim to capture the complexities of real-world AI performance, ultimately fostering
    more reliable, efficient, and impactful AI systems.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优先考虑公平性、透明度和适应性，MLPerf确保基准测试始终是指导人工智能研究和部署的有意义工具。MLPerf的演进基准测试旨在捕捉现实世界人工智能性能的复杂性，最终促进更可靠、高效和有影响力的人工智能系统。
- en: Model and Data Benchmarking
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型和数据基准测试
- en: Our three-dimensional benchmarking framework encompasses systems (covered extensively
    above), models, and data. While system benchmarking has been our primary focus,
    comprehensive AI evaluation requires understanding how algorithmic and data quality
    factors complement system performance measurement. AI performance is not determined
    by system efficiency alone. Machine learning models and datasets play an equally
    crucial role in shaping AI capabilities. Model benchmarking evaluates algorithmic
    performance, while data benchmarking ensures that training datasets are high-quality,
    unbiased, and representative of real-world distributions. Understanding these
    aspects is vital because AI systems are not just computational pipelines but are
    deeply dependent on the models they execute and the data they are trained on.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的三维基准测试框架包括系统（如上所述已广泛涵盖）、模型和数据。虽然系统基准测试一直是我们的主要关注点，但全面的AI评估需要理解算法和数据质量因素如何补充系统性能测量。AI性能不仅仅取决于系统效率。机器学习模型和数据集在塑造AI能力方面同样起着至关重要的作用。模型基准测试评估算法性能，而数据基准测试确保训练数据集高质量、无偏见，并且能够代表现实世界的分布。理解这些方面至关重要，因为AI系统不仅仅是计算管道，它们深深依赖于它们所执行的模型和所训练的数据。
- en: Model Benchmarking
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型基准测试
- en: Model benchmarks measure how well different machine learning algorithms perform
    on specific tasks. Historically, benchmarks focused almost exclusively on accuracy,
    but as models have grown more complex, additional factors, including fairness,
    robustness, efficiency, and generalizability, have become equally important.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 模型基准测试衡量不同机器学习算法在特定任务上的表现如何。历史上，基准测试几乎完全集中在准确性上，但随着模型变得更加复杂，包括公平性、鲁棒性、效率和泛化能力在内的其他因素也变得同样重要。
- en: The evolution of machine learning has been largely driven by benchmark datasets.
    The MNIST dataset ([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient)) was
    one of the earliest catalysts, advancing handwritten digit recognition, while
    the ImageNet dataset ([J. Deng et al. 2009](ch058.xhtml#ref-deng2009imagenet))
    sparked the deep learning revolution in image classification. More recently, datasets
    like COCO ([T.-Y. Lin et al. 2014](ch058.xhtml#ref-lin2014microsoft)) for object
    detection and GPT-3’s training corpus ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language))
    have pushed the boundaries of model capabilities even further.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的演变在很大程度上是由基准数据集驱动的。MNIST数据集（[Lecun等人，1998](ch058.xhtml#ref-lecun1998gradient)）是早期催化剂之一，推动了手写数字识别的进步，而ImageNet数据集（[J.
    Deng等人，2009](ch058.xhtml#ref-deng2009imagenet)）引发了图像分类的深度学习革命。最近，像COCO（[T.-Y.
    Lin等人，2014](ch058.xhtml#ref-lin2014microsoft)）用于目标检测和GPT-3的训练语料库（[T. Brown等人，2020](ch058.xhtml#ref-brown2020language)）等数据集进一步推动了模型能力的发展边界。
- en: 'However, model benchmarks face significant limitations, particularly in the
    era of Large Language Models (LLMs). Beyond the traditional challenge of models
    failing in real-world conditions, commonly referred to as the Sim2Real gap, a
    new form of benchmark optimization has emerged, analogous to but distinct from
    classical benchmark engineering in computer systems. In traditional systems evaluation,
    developers would explicitly optimize their code implementations to perform well
    on benchmark suites like SPEC or TPC, which we discussed earlier under “Benchmark
    Engineering”. In the case of LLMs, this phenomenon manifests through data rather
    than code: benchmark datasets may inadvertently appear in training data when models
    are trained on large web corpora, leading to artificially inflated performance
    scores that reflect memorization rather than genuine capability. For example,
    if a benchmark test is widely discussed online, it might be included in the web
    data used to train an LLM, making the model perform well on that test not due
    to genuine understanding but due to having seen similar examples during training
    ([R. Xu et al. 2024](ch058.xhtml#ref-xu2024benchmarking)). This creates fundamental
    challenges for model evaluation, as high performance on benchmark tasks may reflect
    memorization rather than genuine capability. The key distinction lies in the mechanism:
    while systems benchmark engineering occurred through explicit code optimization,
    LLM benchmark adaptation can occur implicitly through data exposure during pre-training,
    raising new questions about the validity of current evaluation methodologies.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型基准测试面临着重大的局限性，尤其是在大型语言模型（LLMs）的时代。除了模型在现实世界条件下失败的传统挑战，通常被称为Sim2Real差距之外，一种新的基准优化形式已经出现，类似于但与计算机系统中的经典基准工程不同。在传统的系统评估中，开发者会明确优化他们的代码实现，以便在像SPEC或TPC这样的基准测试套件上表现良好，我们之前在“基准工程”部分讨论过。在LLMs的情况下，这种现象通过数据而不是代码表现出来：当模型在大规模网络语料库上训练时，基准数据集可能会无意中出现在训练数据中，导致人为地夸大了性能分数，这些分数反映了记忆而不是真正的能力。例如，如果一个基准测试在网络上广泛讨论，它可能会被包含在用于训练LLM的网页数据中，使得模型在该测试中表现良好不是由于真正的理解，而是由于在训练期间看到了类似的例子（[R.
    Xu 等人 2024](ch058.xhtml#ref-xu2024benchmarking)）。这为模型评估创造了根本性的挑战，因为基准任务上的高性能可能反映了记忆而不是真正的能力。关键的区别在于机制：虽然系统基准工程是通过显式的代码优化发生的，但LLM基准适应可以通过预训练期间的数据暴露隐式发生，这提出了关于当前评估方法有效性的新问题。
- en: These challenges extend beyond just LLMs. Traditional machine learning systems
    continue to struggle with problems of overfitting and bias. The Gender Shades
    project ([Buolamwini and Gebru 2018](ch058.xhtml#ref-buolamwini2018gender)), for
    instance, revealed that commercial facial recognition models performed significantly
    worse on darker-skinned individuals, highlighting the critical importance of fairness
    in model evaluation. Such findings underscore the limitations of focusing solely
    on aggregate accuracy metrics.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战不仅限于大型语言模型（LLMs）。传统的机器学习系统仍然在过拟合和偏差问题上挣扎。例如，性别阴影项目（[Buolamwini 和 Gebru 2018](ch058.xhtml#ref-buolamwini2018gender)）揭示了商业面部识别模型在深色皮肤个体上的表现显著更差，突显了在模型评估中公平性的关键重要性。这样的发现强调了仅仅关注整体准确率指标的局限性。
- en: Moving forward, we must fundamentally rethink its approach to benchmarking.
    This evolution requires developing evaluation frameworks that go beyond traditional
    metrics to assess multiple dimensions of model behavior, from generalization and
    robustness to fairness and efficiency. Key challenges include creating benchmarks
    that remain relevant as models advance, developing methodologies that can differentiate
    between genuine capabilities and artificial performance gains, and establishing
    standards for benchmark documentation and transparency. Success in these areas
    will help ensure that benchmark results provide meaningful insights about model
    capabilities rather than reflecting artifacts of training procedures or evaluation
    design.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 面向未来，我们必须从根本上重新思考其基准测试的方法。这种演变需要开发评估框架，超越传统的指标，以评估模型行为的多个维度，从泛化性和鲁棒性到公平性和效率。关键挑战包括创建随着模型进步而保持相关性的基准，开发能够区分真正能力和人工性能提升的方法，以及建立基准文档和透明度的标准。在这些领域的成功将有助于确保基准结果提供了关于模型能力的有意义的见解，而不是反映了训练程序或评估设计的产物。
- en: Data Benchmarking
  id: totrans-572
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据基准测试
- en: The evolution of artificial intelligence has traditionally focused on model-centric
    approaches, emphasizing architectural improvements and optimization techniques.
    However, contemporary AI development reveals that data quality, rather than model
    design alone, often determines performance boundaries. This recognition has elevated
    data benchmarking to a critical field that ensures AI models learn from datasets
    that are high-quality, diverse, and free from bias.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的传统发展一直侧重于以模型为中心的方法，强调架构改进和优化技术。然而，当代人工智能的发展表明，数据质量，而不仅仅是模型设计，通常决定了性能界限。这种认识已经将数据基准测试提升到一个关键领域，确保人工智能模型从高质量、多样化和无偏见的数据集中学习。
- en: This evolution represents a fundamental shift from model-centric to data-centric
    AI approaches, as illustrated in [Figure 12.12](ch018.xhtml#fig-model-vs-data).
    The traditional model-centric paradigm focuses on enhancing model architectures,
    refining algorithms, and improving computational efficiency while treating datasets
    as fixed components. In contrast, the emerging data-centric approach systematically
    improves dataset quality through better annotations, increased diversity, and
    bias reduction, while maintaining consistent model architectures and system configurations.
    Research increasingly demonstrates that methodical dataset enhancement can yield
    superior performance gains compared to model refinements alone, challenging the
    conventional emphasis on architectural innovation.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 这种演变代表了从以模型为中心到以数据为中心的人工智能方法的根本转变，如图12.12([Figure 12.12](ch018.xhtml#fig-model-vs-data))所示。传统的以模型为中心的范式侧重于增强模型架构、细化算法和改进计算效率，同时将数据集视为固定组件。相比之下，新兴的数据为中心的方法通过更好的标注、增加多样性和减少偏见系统地提高数据集质量，同时保持一致的模型架构和系统配置。研究越来越多地表明，有系统的数据集增强可以带来比单独的模型改进更优越的性能提升，挑战了传统上对架构创新的重视。
- en: '![](../media/file204.svg)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file204.svg)'
- en: 'Figure 12.12: **Development Paradigms**: Model-centric AI prioritizes architectural
    innovation with fixed datasets, while data-centric AI systematically improves
    dataset quality (annotations, diversity, and bias) with consistent model architectures
    to achieve performance gains. Modern research indicates that strategic data enhancement
    often yields greater improvements than solely refining model complexity.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：**发展范式**：以模型为中心的人工智能优先考虑固定数据集的架构创新，而以数据为中心的人工智能通过一致的模型架构系统地提高数据集质量（标注、多样性和偏见）以实现性能提升。现代研究表明，战略性的数据增强往往比仅仅细化模型复杂性带来更大的改进。
- en: 'Data quality’s primacy in AI development reflects a fundamental shift in understanding:
    superior datasets, not just sophisticated models, produce more reliable and robust
    AI systems. Initiatives like DataPerf and DataComp have emerged to systematically
    evaluate how dataset improvements affect model performance. For instance, DataComp
    ([Nishigaki 2024](ch058.xhtml#ref-gadre2024datacomp)) demonstrated that models
    trained on a carefully curated 30% subset of data achieved better results than
    those trained on the complete dataset, challenging the assumption that more data
    automatically leads to better performance ([Northcutt, Athalye, and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive)).'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能发展中，数据质量的首要性反映了理解上的根本转变：优越的数据集，而不仅仅是复杂的模型，能产生更可靠和稳健的人工智能系统。像DataPerf和DataComp这样的倡议已经出现，以系统地评估数据集改进如何影响模型性能。例如，DataComp
    ([Nishigaki 2024](ch058.xhtml#ref-gadre2024datacomp)) 表明，在精心挑选的30%数据子集上训练的模型比在完整数据集上训练的模型取得了更好的结果，挑战了更多数据自动导致更好性能的假设
    ([Northcutt, Athalye, and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive))。
- en: A significant challenge in data benchmarking emerges from dataset saturation.
    When models achieve near-perfect accuracy on benchmarks like ImageNet, it becomes
    crucial to distinguish whether performance gains represent genuine advances in
    AI capability or merely optimization to existing test sets. [Figure 12.13](ch018.xhtml#fig-dataset-saturation)
    illustrates this trend, showing AI systems surpassing human performance across
    various applications over the past decade.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 数据基准测试中的一个重大挑战来自数据集饱和。当模型在ImageNet等基准测试上达到几乎完美的准确性时，区分性能提升是否代表人工智能能力的真正进步或仅仅是现有测试集的优化变得至关重要。[图12.13](ch018.xhtml#fig-dataset-saturation)展示了这一趋势，显示了在过去十年中人工智能系统在各种应用中超越人类性能。
- en: '![](../media/file205.svg)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file205.svg)'
- en: 'Figure 12.13: **Dataset Saturation**: AI systems surpass human performance
    on benchmark datasets, indicating that continued gains may not reflect genuine
    improvements in intelligence but rather optimization to fixed evaluation sets.
    This trend underscores the need for dynamic, challenging datasets that accurately
    assess AI capabilities and drive meaningful progress beyond simple pattern recognition.
    Source: ([Kiela et al. 2021](ch058.xhtml#ref-kiela2021dynabench)).'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：**数据集饱和**：人工智能系统在基准数据集上超越人类表现，表明持续的进步可能并不反映真正的智能提升，而是对固定评估集的优化。这一趋势强调了需要动态、具有挑战性的数据集，以准确评估人工智能能力并推动超越简单模式识别的实质性进步。来源：([Kiela等人2021年](ch058.xhtml#ref-kiela2021dynabench))。
- en: 'This saturation phenomenon raises fundamental methodological questions ([Kiela
    et al. 2021](ch058.xhtml#ref-kiela2021dynabench)). The MNIST dataset provides
    an illustrative example: certain test images, though nearly illegible to humans,
    were assigned specific labels during the dataset’s creation in 1994\. When models
    correctly predict these labels, their apparent superhuman performance may actually
    reflect memorization of dataset artifacts rather than true digit recognition capabilities.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 这种饱和现象引发了根本性的方法论问题([Kiela等人2021年](ch058.xhtml#ref-kiela2021dynabench))。MNIST数据集提供了一个说明性的例子：某些测试图像，尽管对人类几乎无法辨认，但在1994年数据集创建时被赋予了特定的标签。当模型正确预测这些标签时，它们看似超凡的表现实际上可能只是对数据集缺陷的记忆，而不是真正的数字识别能力。
- en: These challenges extend beyond individual domains. The provocative question
    “Are we done with ImageNet?” ([Beyer et al. 2020](ch058.xhtml#ref-beyer2020we))
    highlights broader concerns about the limitations of static benchmarks. Models
    optimized for fixed datasets often struggle with distribution shifts, real-world
    changes that occur after training data collection. This limitation has driven
    the development of dynamic benchmarking approaches, such as Dynabench ([Kiela
    et al. 2021](ch058.xhtml#ref-kiela2021dynabench)), which continuously evolves
    test data based on model performance to maintain benchmark relevance.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战不仅限于单个领域。挑衅性的问题“我们是否已经完成了ImageNet？”([Beyer等人2020年](ch058.xhtml#ref-beyer2020we))突显了对静态基准测试局限性的更广泛担忧。针对固定数据集优化的模型往往难以应对分布变化，即训练数据收集之后的现实世界变化。这种局限性推动了动态基准测试方法的发展，如Dynabench([Kiela等人2021年](ch058.xhtml#ref-kiela2021dynabench))，该方法根据模型性能不断演变测试数据，以保持基准的相关性。
- en: Current data benchmarking efforts encompass several critical dimensions. Label
    quality assessment remains a central focus, as explored in DataPerf’s debugging
    challenge. Initiatives like MSWC ([Mazumder et al. 2021](ch058.xhtml#ref-mazumder2021multilingual))
    for speech recognition address bias and representation in datasets. Out-of-distribution
    generalization receives particular attention through benchmarks like RxRx and
    WILDS ([Koh et al. 2021](ch058.xhtml#ref-koh2021wilds)). These diverse efforts
    reflect a growing recognition that advancing AI capabilities requires not just
    better models and systems, but fundamentally better approaches to data quality
    assessment and benchmark design.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的数据基准测试努力涵盖了几个关键维度。标签质量评估仍然是核心关注点，如DataPerf的调试挑战所探讨的那样。像MSWC([Mazumder等人2021年](ch058.xhtml#ref-mazumder2021multilingual))这样的语音识别倡议解决了数据集中的偏差和代表性问题。通过RxRx和WILDS([Koh等人2021年](ch058.xhtml#ref-koh2021wilds))等基准测试，特别关注了分布外泛化。这些多样化的努力反映了人们越来越认识到，提升人工智能能力不仅需要更好的模型和系统，还需要在数据质量评估和基准设计方面采取根本性的改进方法。
- en: Holistic System-Model-Data Evaluation
  id: totrans-584
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全面的系统-模型-数据评估
- en: AI benchmarking has traditionally evaluated systems, models, and data as separate
    entities. However, real-world AI performance emerges from the interplay between
    these three components. A fast system cannot compensate for a poorly trained model,
    and even the most powerful model is constrained by the quality of the data it
    learns from. This interdependence necessitates a holistic benchmarking approach
    that considers all three dimensions together.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的AI基准测试将系统、模型和数据视为独立的实体。然而，现实世界的AI性能源于这三个组件之间的相互作用。一个快速的系统无法弥补一个训练不佳的模型，即使是功能最强大的模型也受限于其学习数据的质量。这种相互依赖性需要一种全面的基准测试方法，该方法将这三个维度综合考虑。
- en: As illustrated in [Figure 12.14](ch018.xhtml#fig-benchmarking-trifecta), the
    future of benchmarking lies in an integrated framework that jointly evaluates
    system efficiency, model performance, and data quality. This approach enables
    researchers to identify optimization opportunities that remain invisible when
    these components are analyzed in isolation. For example, co-designing efficient
    AI models with hardware-aware optimizations and carefully curated datasets can
    lead to superior performance while reducing computational costs.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图12.14](ch018.xhtml#fig-benchmarking-trifecta)所示，基准测试的未来在于一个综合框架，该框架共同评估系统效率、模型性能和数据质量。这种方法使研究人员能够识别出当这些组件单独分析时不可见的优化机会。例如，与硬件感知优化协同设计高效的AI模型和精心挑选的数据集可以带来卓越的性能，同时降低计算成本。
- en: '![](../media/file206.svg)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file206.svg)'
- en: 'Figure 12.14: **AI System Interdependence**: Highlights the critical interplay
    between infrastructure, models, and data in determining overall AI system performance,
    emphasizing that optimization requires a holistic approach rather than isolated
    improvements. this figure illustrates that gains in one component cannot fully
    compensate for limitations in others, necessitating co-design strategies for efficient
    and effective AI.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：**AI系统互依性**：突出基础设施、模型和数据在决定整体AI系统性能中的关键相互作用，强调优化需要整体方法而不是孤立的改进。此图说明一个组件的收益不能完全弥补其他组件的局限性，需要协同设计策略以实现高效和有效的AI。
- en: As AI continues to evolve, benchmarking methodologies must advance in tandem.
    Evaluating AI performance through the lens of systems, models, and data ensures
    that benchmarks drive improvements not just in accuracy, but also in efficiency,
    fairness, and robustness. This holistic perspective will be critical for developing
    AI that is not only powerful but also practical, scalable, and ethical.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI的持续发展，基准测试方法必须同步进步。通过系统、模型和数据的视角评估AI性能确保基准测试不仅推动准确性的改进，还推动效率、公平性和鲁棒性的改进。这种整体视角对于开发不仅强大而且实用、可扩展和符合伦理的AI至关重要。
- en: Production Environment Evaluation
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境评估
- en: The benchmarking methodologies discussed thus far (from micro to end-to-end
    granularity, from training to inference evaluation) primarily address system performance
    under controlled conditions. However, the deployment strategies introduced in
    [Chapter 13](ch019.xhtml#sec-ml-operations) reveal that production environments
    introduce distinctly different challenges requiring specialized evaluation approaches.
    Production machine learning systems must handle dynamic workloads, varying data
    quality, infrastructure failures, and concurrent user demands while maintaining
    consistent performance and reliability. This necessitates extending our benchmarking
    framework beyond single-point performance measurement to evaluate system behavior
    over time, under stress, and during failure scenarios.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的基准测试方法（从微观到端到端粒度，从训练到推理评估）主要针对受控条件下的系统性能。然而，在[第13章](ch019.xhtml#sec-ml-operations)中介绍的实施策略揭示了生产环境引入了明显不同的挑战，需要专门的评估方法。生产机器学习系统必须在保持一致性能和可靠性的同时处理动态工作负载、变化的数据质量、基础设施故障和并发用户需求。这需要将我们的基准测试框架扩展到单点性能测量之外，以评估系统在时间、压力和故障场景下的行为。
- en: Silent failure detection represents a critical production benchmarking dimension
    absent from research evaluation frameworks. Machine learning models can degrade
    silently without obvious error signals, producing plausible but incorrect outputs
    that escape traditional monitoring. Production benchmarking must establish baseline
    performance distributions and detect subtle accuracy degradation through statistical
    process control methods. A/B testing frameworks compare new model versions against
    stable baselines under identical traffic conditions, measuring not just average
    performance but performance variance and tail behavior.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 静默故障检测代表了生产基准测试中缺失的研究评估框架的一个关键维度。机器学习模型可以在没有明显错误信号的情况下无声退化，产生看似合理但实际上错误的输出，从而逃避传统的监控。生产基准测试必须建立基线性能分布，并通过统计过程控制方法检测细微的准确性退化。A/B测试框架在相同的流量条件下将新模型版本与稳定的基线进行比较，不仅衡量平均性能，还衡量性能变异性以及尾部行为。
- en: Continuous data quality monitoring addresses the dynamic nature of production
    data streams that can introduce distribution shift, adversarial examples, or corrupted
    inputs. Production benchmarks must evaluate model robustness under realistic data
    quality variations including missing features, out-of-range values, and input
    format changes. Monitoring systems track feature distribution drift over time,
    measuring statistical distances between training and production data to predict
    when retraining becomes necessary. Data validation pipelines benchmark preprocessing
    robustness, ensuring models gracefully handle data quality issues without silent
    failures.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 持续数据质量监控解决生产数据流动态性带来的问题，这些问题可能导致分布偏移、对抗性示例或损坏的输入。生产级基准测试必须评估模型在现实数据质量变化下的鲁棒性，包括缺失特征、超出范围的值和输入格式变化。监控系统跟踪特征分布漂移，测量训练数据和生产数据之间的统计距离，以预测何时需要重新训练。数据验证管道基准测试预处理鲁棒性，确保模型能够优雅地处理数据质量问题，而不会出现无声故障。
- en: Load testing and capacity planning evaluate system performance under varying
    traffic patterns that reflect real user behavior. Production ML systems must handle
    request spikes, concurrent user sessions, and sustained high-throughput operation
    while maintaining latency requirements. Benchmarking protocols simulate realistic
    load patterns including diurnal traffic variations, flash traffic events, and
    organic growth scenarios. Capacity planning benchmarks measure how performance
    degrades as system utilization approaches limits, enabling proactive scaling decisions.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 压力测试和容量规划评估系统在反映真实用户行为的各种流量模式下的性能。生产级机器学习系统必须处理请求峰值、并发用户会话以及持续的高吞吐量操作，同时保持延迟要求。基准测试协议模拟现实负载模式，包括日间流量变化、突发流量事件和有机增长场景。容量规划基准测试衡量系统利用率接近极限时性能的下降情况，从而实现主动的扩展决策。
- en: 'Operational resilience benchmarking evaluates system behavior during infrastructure
    failures, network partitions, and resource constraints. Production systems must
    maintain service availability during partial failures, gracefully degrade when
    resources become unavailable, and recover quickly from outages. Chaos engineering
    approaches systematically introduce failures to measure system resilience: killing
    inference servers, inducing network latency, and limiting computational resources
    to observe degradation patterns and recovery characteristics.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 运营弹性基准测试评估系统在基础设施故障、网络分区和资源限制情况下的行为。生产系统必须在部分故障期间保持服务可用性，当资源不可用时优雅降级，并快速从故障中恢复。混沌工程方法系统地引入故障以测量系统弹性：关闭推理服务器、引入网络延迟和限制计算资源以观察降级模式和恢复特性。
- en: Multi-objective optimization in production requires benchmarking frameworks
    that balance accuracy, latency, cost, and resource utilization simultaneously.
    Production systems optimize for user experience metrics like conversion rates
    and engagement alongside traditional ML metrics. Cost efficiency benchmarks evaluate
    compute cost per prediction, storage costs for model artifacts, and operational
    overhead for system maintenance. Service level objectives (SLOs) define acceptable
    performance ranges across multiple dimensions, enabling systematic evaluation
    of production system health.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 生产中的多目标优化需要平衡精度、延迟、成本和资源利用率的基准测试框架。生产系统在优化用户体验指标（如转化率和参与度）的同时，也优化传统的机器学习指标。成本效率基准测试评估每预测的计算成本、模型工件存储成本和系统维护的运营开销。服务级别目标（SLOs）定义了多个维度的可接受性能范围，从而实现生产系统健康状况的系统评估。
- en: Continuous model validation implements automated benchmarking pipelines that
    evaluate model performance on held-out datasets and synthetic test cases over
    time. Shadow deployment techniques run new models alongside production systems,
    comparing outputs without affecting user experience. Champion-challenger frameworks
    systematically evaluate model improvements through controlled rollouts, measuring
    both performance improvements and potential negative impacts on downstream systems.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 持续模型验证实现自动基准测试管道，这些管道随着时间的推移评估模型在保留数据集和合成测试案例上的性能。影子部署技术在新模型与生产系统并行运行时，比较输出结果而不影响用户体验。冠军挑战者框架通过受控发布系统地评估模型改进，衡量性能改进和可能对下游系统产生的负面影响。
- en: Production benchmarking therefore requires end-to-end evaluation frameworks
    that extend far beyond model accuracy to encompass system reliability, operational
    efficiency, and user experience optimization. This comprehensive approach ensures
    that ML systems deliver consistent value in dynamic production environments while
    maintaining the robustness necessary for mission-critical applications.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，生产基准测试需要端到端评估框架，这些框架不仅超越了模型准确度，还涵盖了系统可靠性、运营效率和用户体验优化。这种全面的方法确保了机器学习系统在动态生产环境中提供一致的价值，同时保持对任务关键应用的必要鲁棒性。
- en: Fallacies and Pitfalls
  id: totrans-599
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误与陷阱
- en: The benchmarking methodologies and frameworks established throughout this chapter
    (from our three-dimensional evaluation framework to the specific metrics for training
    and inference) provide powerful tools for systematic evaluation. However, their
    effectiveness depends critically on avoiding common misconceptions and methodological
    errors that can undermine benchmark validity. The standardized nature of benchmarks,
    while enabling fair comparison, often creates false confidence about their universal
    applicability.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中建立的基准测试方法和框架（从我们的三维评估框架到训练和推理的具体指标）为系统评估提供了强大的工具。然而，它们的有效性关键在于避免常见的误解和方法论错误，这些错误可能会损害基准的有效性。基准的标准化性质，虽然促进了公平比较，但往往会导致对其普遍适用性的错误自信。
- en: '**Fallacy:** *Benchmark performance directly translates to real-world application
    performance.*'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *基准性能直接转化为实际应用性能。*'
- en: This misconception leads teams to select models and systems based solely on
    benchmark rankings without considering deployment context differences. Benchmarks
    typically use curated datasets, standardized evaluation protocols, and optimal
    configurations that rarely match real-world conditions. Production systems face
    data quality issues, distribution shifts, latency constraints, and resource limitations
    not captured in benchmark scenarios. A model that achieves state-of-the-art benchmark
    performance might fail catastrophically when deployed due to these environmental
    differences. Effective system selection requires augmenting benchmark results
    with deployment-specific evaluation rather than relying solely on standardized
    metrics.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解导致团队仅根据基准排名选择模型和系统，而不考虑部署环境的不同。基准通常使用精心挑选的数据集、标准化的评估协议和最佳配置，这些很少与实际条件相匹配。生产系统面临基准场景中未捕获的数据质量问题、分布变化、延迟限制和资源限制。一个在基准测试中达到最先进性能的模型，由于这些环境差异，在部署时可能会失败得非常严重。有效的系统选择需要通过部署特定的评估来补充基准结果，而不是仅仅依赖于标准化指标。
- en: '**Pitfall:** *Optimizing exclusively for benchmark metrics without considering
    broader system requirements.*'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *仅针对基准指标进行优化，不考虑更广泛的系统需求。*'
- en: Many practitioners focus intensively on improving benchmark scores without understanding
    how these optimizations affect overall system behavior. Techniques that boost
    specific metrics might degrade other important characteristics like robustness,
    calibration, fairness, or energy efficiency. Overfitting to benchmark evaluation
    protocols can create models that perform well on specific test conditions but
    fail to generalize to varied real-world scenarios. This narrow optimization approach,
    a manifestation of Goodhart’s Law[53](#fn53) discussed in [Section 12.10.4.1](ch018.xhtml#sec-benchmarking-ai-benchmark-engineering-99d3),
    often produces systems that excel in controlled environments but struggle with
    the complexity and unpredictability of practical deployments.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者专注于提高基准分数，而不理解这些优化如何影响整体系统行为。提升特定指标的技术可能会降低其他重要特性，如鲁棒性、校准、公平性或能源效率。过度拟合到基准评估协议可能会创建在特定测试条件下表现良好但无法推广到各种现实世界场景的模型。这种狭隘的优化方法，是第12.10.4.1节中讨论的Goodhart定律[53](#fn53)的表现，通常会产生在受控环境中表现优异但在实际部署的复杂性和不可预测性面前挣扎的系统。
- en: '**Fallacy:** *Single-metric evaluation provides sufficient insight into system
    performance.*'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *单一指标评估足以深入了解系统性能。*'
- en: This belief assumes that one primary metric captures all relevant aspects of
    system performance. Modern AI systems require evaluation across multiple dimensions
    including accuracy, latency, throughput, energy consumption, fairness, and robustness.
    Optimizing for accuracy alone might create systems with unacceptable inference
    delays, while focusing on throughput might compromise result quality. Different
    stakeholders prioritize different metrics, and deployment contexts create varying
    constraints that single metrics cannot capture. Comprehensive evaluation requires
    multidimensional assessment frameworks that reveal trade-offs across all relevant
    performance aspects.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设一个主要指标可以捕捉系统性能的所有相关方面。现代人工智能系统需要从多个维度进行评估，包括准确性、延迟、吞吐量、能耗、公平性和鲁棒性。仅优化准确性可能会产生具有不可接受的推理延迟的系统，而专注于吞吐量可能会损害结果质量。不同的利益相关者会优先考虑不同的指标，部署环境也会产生单指标无法捕捉的不同约束。全面的评估需要多维评估框架，以揭示所有相关性能方面的权衡。
- en: '**Pitfall:** *Using outdated benchmarks that no longer reflect current challenges
    and requirements.*'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *使用过时的基准，这些基准不再反映当前的挑战和需求。*'
- en: Teams often continue using established benchmarks long after they cease to represent
    meaningful challenges or current deployment realities. As model capabilities advance,
    benchmarks can become saturated, providing little discriminatory power between
    approaches. Similarly, changing application requirements, new deployment contexts,
    and evolving fairness standards can make existing benchmarks irrelevant or misleading.
    Benchmark datasets may also develop hidden biases or quality issues over time
    as they age. Effective benchmarking requires regular assessment of whether evaluation
    frameworks still provide meaningful insights for current challenges and deployment
    scenarios.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 团队往往在基准不再代表有意义的挑战或当前部署现实之后继续使用既定的基准。随着模型能力的提升，基准可能会饱和，在方法之间提供很少的区分力。同样，不断变化的应用需求、新的部署环境和不断发展的公平性标准可能会使现有基准变得无关或误导。随着时间的推移，基准数据集也可能发展出隐藏的偏差或质量问题。有效的基准测试需要定期评估评估框架是否仍然为当前的挑战和部署场景提供有意义的见解。
- en: '**Pitfall:** *Applying research-oriented benchmarks to evaluate production
    system performance without accounting for operational constraints.*'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *在评估生产系统性能时，没有考虑到操作约束而应用研究型基准。*'
- en: Many teams use academic benchmarks designed for research comparisons to evaluate
    production systems, overlooking fundamental differences between research and operational
    environments. Research benchmarks typically assume unlimited computational resources,
    optimal data quality, and idealized deployment conditions that rarely exist in
    production settings. Production systems must handle concurrent user loads, varying
    input quality, network latency, memory constraints, and system failures that significantly
    impact performance compared to controlled benchmark conditions. Additionally,
    production systems require optimization for multiple objectives simultaneously
    including cost efficiency, availability, and user experience that single-metric
    research benchmarks cannot capture. Effective production evaluation requires augmenting
    research benchmarks with operational metrics like sustained throughput under load,
    recovery time from failures, resource utilization efficiency, and end-to-end latency
    including data preprocessing and postprocessing overhead.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队使用为研究比较设计的学术基准来评估生产系统，而忽略了研究和操作环境之间的基本差异。研究基准通常假设有无限的计算资源、最佳数据质量和理想化的部署条件，这些条件在现实生产环境中很少存在。生产系统必须处理并发用户负载、变化的输入质量、网络延迟、内存限制和系统故障，这些因素与受控基准条件相比会显著影响性能。此外，生产系统需要同时优化多个目标，包括成本效率、可用性和用户体验，而单指标研究基准无法捕捉到这些。有效的生产评估需要通过操作指标（如负载下的持续吞吐量、从故障中恢复的时间、资源利用效率以及包括数据预处理和后处理开销在内的端到端延迟）来补充研究基准。
- en: Summary
  id: totrans-611
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter established benchmarking as the critical measurement discipline
    that validates the performance claims and optimization strategies introduced throughout
    Parts II and III. By developing a comprehensive three-dimensional framework evaluating
    algorithms, systems, and data simultaneously, we demonstrated how systematic measurement
    transforms the theoretical advances in efficient AI design ([Chapter 9](ch015.xhtml#sec-efficient-ai)),
    model optimization ([Chapter 10](ch016.xhtml#sec-model-optimizations)), and hardware
    acceleration ([Chapter 11](ch017.xhtml#sec-ai-acceleration)) into quantifiable
    engineering improvements. The progression from historical computing benchmarks
    through specialized ML evaluation methodologies revealed why modern AI systems
    require multifaceted assessment approaches that capture the complexity of real-world
    deployment.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将基准测试确立为关键的测量学科，验证了第二部分和第三部分中引入的性能主张和优化策略。通过开发一个全面的三维框架，同时评估算法、系统和数据，我们展示了系统测量如何将高效AI设计（[第9章](ch015.xhtml#sec-efficient-ai)）、模型优化（[第10章](ch016.xhtml#sec-model-optimizations)）和硬件加速（[第11章](ch017.xhtml#sec-ai-acceleration)）的理论进步转化为可量化的工程改进。从历史计算基准到专门的ML评估方法的发展，揭示了为什么现代AI系统需要多方面的评估方法，以捕捉现实世界部署的复杂性。
- en: The technical sophistication of modern benchmarking frameworks reveals how measurement
    methodology directly influences innovation direction and resource allocation decisions
    across the entire AI ecosystem. System benchmarks like MLPerf drive hardware optimization
    and infrastructure development by establishing standardized workloads and metrics
    that enable fair comparison across diverse architectures. Model benchmarks push
    algorithmic innovation by defining challenging tasks and evaluation protocols
    that reveal limitations and guide research priorities. Data benchmarks expose
    critical issues around representation, bias, and quality that directly impact
    model fairness and generalization capabilities. The integration of these benchmarking
    dimensions creates a comprehensive evaluation framework that captures the complexity
    of real-world AI deployment challenges.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 现代基准测试框架的技术复杂性揭示了测量方法如何直接影响整个AI生态系统中创新方向和资源配置决策。系统基准如MLPerf通过建立标准化的工作负载和指标，使得不同架构之间可以进行公平的比较，从而推动硬件优化和基础设施发展。模型基准通过定义具有挑战性的任务和评估协议，揭示局限性并指导研究优先级，从而推动算法创新。数据基准揭示了关于表示、偏差和质量的关键问题，这些问题直接影响模型的公平性和泛化能力。这些基准测试维度的整合创建了一个全面的评估框架，捕捉了现实世界AI部署挑战的复杂性。
- en: '**Key Takeaways**'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Effective benchmarking requires multidimensional evaluation across systems,
    models, and data to capture real-world deployment challenges
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的基准测试需要对系统、模型和数据的多维度评估，以捕捉现实世界的部署挑战。
- en: Standardized benchmarks like MLPerf drive hardware innovation and enable fair
    comparison across diverse architectures and implementations
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化基准如MLPerf推动硬件创新，并使不同架构和实现之间的公平比较成为可能。
- en: Benchmark design choices fundamentally shape research priorities and resource
    allocation across the entire AI ecosystem
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准设计选择从根本上塑造了整个AI生态系统中研究优先级和资源配置。
- en: Future benchmarking must evolve to address emerging challenges around AI safety,
    fairness, and environmental impact
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的基准测试必须发展以应对围绕AI安全、公平性和环境影响的新兴挑战。
- en: 'The benchmarking foundations established here provide the measurement infrastructure
    necessary for the operational deployment strategies explored in Part IV: Robust
    Deployment. The transition from performance measurement to production deployment
    requires extending benchmark validation beyond laboratory conditions. While this
    chapter focused on systematic evaluation under controlled conditions, Part IV
    addresses the additional complexities of dynamic workloads, evolving data distributions,
    and operational constraints that characterize real-world ML system deployment.
    In [Chapter 13](ch019.xhtml#sec-ml-operations), we extend these benchmarking principles
    to production environments, where continuous monitoring detects silent failures,
    tracks model performance degradation, and validates system behavior under dynamic
    workloads that offline benchmarks cannot capture. The A/B testing frameworks and
    champion-challenger methodologies introduced in production monitoring build directly
    upon the comparative evaluation principles established through training and inference
    benchmarking.'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 这里建立的基准测试基础为第IV部分：鲁棒部署中探索的操作部署策略提供了必要的测量基础设施。从性能测量到生产部署的转变需要将基准验证扩展到实验室条件之外。虽然本章专注于受控条件下的系统评估，但第IV部分解决了动态工作负载、数据分布演变和现实世界ML系统部署中具有代表性的操作约束带来的额外复杂性。在第13章（[ch019.xhtml#sec-ml-operations](ch019.xhtml#sec-ml-operations)）中，我们将这些基准测试原则扩展到生产环境，其中连续监控可以检测到静默故障，跟踪模型性能下降，并在离线基准测试无法捕捉的动态工作负载下验证系统行为。在生产监控中引入的A/B测试框架和冠军挑战者方法直接建立在通过训练和推理基准测试建立的比较评估原则之上。
- en: The privacy and security challenges in [Chapter 15](ch021.xhtml#sec-security-privacy)
    similarly require specialized benchmarking methodologies that evaluate dimensions
    beyond pure performance. Adversarial robustness benchmarks measure model resilience
    against intentional attacks, while privacy-preserving computation frameworks require
    benchmarking trade-offs between utility and privacy guarantees. The robustness
    requirements in [Chapter 16](ch022.xhtml#sec-robust-ai) demand evaluation protocols
    that assess model behavior under distribution shift, data corruption, and edge
    cases that traditional benchmarks overlook.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 第15章（[ch021.xhtml#sec-security-privacy](ch021.xhtml#sec-security-privacy)）中隐私和安全挑战同样需要专门的基准测试方法，这些方法评估的维度超出了纯粹的性能。对抗鲁棒性基准测试衡量模型对有意攻击的抵抗能力，而隐私保护计算框架则需要权衡效用和隐私保证之间的基准测试。第16章（[ch022.xhtml#sec-robust-ai](ch022.xhtml#sec-robust-ai)）中的鲁棒性要求需要评估协议，以评估模型在分布变化、数据损坏和传统基准测试忽视的边缘情况下的行为。
- en: As AI systems become increasingly influential in critical applications, the
    benchmarking frameworks developed today determine whether we can effectively measure
    and optimize for societal impacts extending far beyond traditional performance
    metrics. The responsible AI principles in [Chapter 17](ch023.xhtml#sec-responsible-ai)
    and sustainability considerations in [Chapter 18](ch024.xhtml#sec-sustainable-ai)
    establish new evaluation dimensions that must be integrated alongside efficiency
    and accuracy in comprehensive system assessment.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI系统在关键应用中的影响力日益增强，今天开发的基准测试框架决定了我们是否能够有效地衡量和优化超出传统性能指标的社会影响。第17章（[ch023.xhtml#sec-responsible-ai](ch023.xhtml#sec-responsible-ai)）中的负责任AI原则和第18章（[ch024.xhtml#sec-sustainable-ai](ch024.xhtml#sec-sustainable-ai)）中的可持续性考虑因素在全面系统评估中建立了必须与效率和精度相结合的新评估维度。
- en: '* * *'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
