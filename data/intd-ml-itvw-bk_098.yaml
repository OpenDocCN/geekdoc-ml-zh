- en: 8.2.4 Other
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2.4 其他
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html](https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html](https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html)
- en: '[M] An autoencoder is a neural network that learns to copy its input to its
    output. When would this be useful?'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 自动编码器是一种学习将输入复制到输出的神经网络。在什么情况下这会有用？'
- en: Self-attention.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力。
- en: '[E] What’s the motivation for self-attention?'
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 自注意力的动机是什么？'
- en: '[E] Why would you choose a self-attention architecture over RNNs or CNNs?'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么你会选择自注意力架构而不是RNN或CNN？'
- en: '[M] Why would you need multi-headed attention instead of just one head for
    attention?'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么你需要多头注意力而不是只有一个头部的注意力？'
- en: '[M] How would changing the number of heads in multi-headed attention affect
    the model’s performance?'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 改变多头注意力中的头数会如何影响模型的表现？'
- en: Transfer learning
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: '[E] You want to build a classifier to predict sentiment in tweets but you have
    very little labeled data (say 1000). What do you do?'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你想构建一个分类器来预测推文的情感，但你只有非常少的标记数据（比如说1000）。你该怎么办？'
- en: '[M] What’s gradual unfreezing? How might it help with transfer learning?'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 渐进解冻是什么？它如何帮助迁移学习？'
- en: Bayesian methods.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯方法。
- en: '[M] How do Bayesian methods differ from the mainstream deep learning approach?'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 贝叶斯方法与主流深度学习方法有何不同？'
- en: '[M] How are the pros and cons of Bayesian neural networks compared to the mainstream
    neural networks?'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 与主流神经网络相比，贝叶斯神经网络的优缺点是什么？'
- en: '[M] Why do we say that Bayesian neural networks are natural ensembles?'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 我们为什么说贝叶斯神经网络是自然的集成？'
- en: GANs.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GANs。
- en: '[E] What do GANs converge to?'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] GANs会收敛到什么？'
- en: '[M] Why are GANs so hard to train?'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么GANs如此难以训练？'
