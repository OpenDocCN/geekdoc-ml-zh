- en: 7.3 Objective functions, metrics, and evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.3 目标函数、指标和评估
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html](https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html](https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html)
- en: Convergence.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛。
- en: '[E] When we say an algorithm converges, what does convergence mean?'
  id: totrans-3
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 当我们说一个算法收敛时，收敛意味着什么？'
- en: '[E] How do we know when a model has converged?'
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 我们如何知道模型何时收敛？'
- en: '[E] Draw the loss curves for overfitting and underfitting.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 绘制过拟合和欠拟合的损失曲线。'
- en: Bias-variance trade-off
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: '[E] What’s the bias-variance trade-off?'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 偏差-方差权衡是什么？'
- en: '[M] How’s this tradeoff related to overfitting and underfitting?'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 这个权衡与过拟合和欠拟合有什么关系？'
- en: '[M] How do you know that your model is high variance, low bias? What would
    you do in this case?'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你如何知道你的模型是高方差、低偏差？在这种情况下你会怎么做？'
- en: '[M] How do you know that your model is low variance, high bias? What would
    you do in this case?'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你如何知道你的模型是低方差、高偏差？在这种情况下你会怎么做？'
- en: Cross-validation.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证。
- en: '[E] Explain different methods for cross-validation.'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 解释交叉验证的不同方法。'
- en: '[M] Why don’t we see more cross-validation in deep learning?'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么我们在深度学习中看不到更多的交叉验证？'
- en: Train, valid, test splits.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练、验证、测试分割。
- en: '[E] What’s wrong with training and testing a model on the same data?'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 在同一数据上训练和测试模型有什么问题？'
- en: '[E] Why do we need a validation set on top of a train set and a test set?'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么我们除了训练集和测试集之外还需要验证集？'
- en: '[M] Your model’s loss curves on the train, valid, and test sets look like this.
    What might have been the cause of this? What would you do?![Problematic loss curves](../Images/d5852d1412ffaf6b2f4afe31c459b3f2.png
    "image_tooltip")'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你的模型在训练、验证和测试集上的损失曲线如下。这可能是什么原因？你会怎么做？![问题性的损失曲线](../Images/d5852d1412ffaf6b2f4afe31c459b3f2.png
    "image_tooltip")'
- en: '[E] Your team is building a system to aid doctors in predicting whether a patient
    has cancer or not from their X-ray scan. Your colleague announces that the problem
    is solved now that they’ve built a system that can predict with 99.99% accuracy.
    How would you respond to that claim?'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你的团队正在构建一个系统，帮助医生从X光扫描中预测患者是否患有癌症。你的同事宣布，现在他们已经构建了一个可以以99.99%的准确率预测的系统，问题已经解决了。你会如何回应这个说法？'
- en: F1 score.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F1分数。
- en: '[E] What’s the benefit of F1 over the accuracy?'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] F1相对于准确率有什么好处？'
- en: '[M] Can we still use F1 for a problem with more than two classes. How?'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 对于有超过两个类别的问题，我们是否还可以使用F1？如何？'
- en: Given a binary classifier that outputs the following confusion matrix.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个输出以下混淆矩阵的二分类器。
- en: '|  | Predicted True | Predicted False |'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | 预测真 | 预测假 |'
- en: '| Actual True | 30 | 20 |'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 实际真 | 30 | 20 |'
- en: '| Actual False | 5 | 40 |'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 实际假 | 5 | 40 |'
- en: '[E] Calculate the model’s precision, recall, and F1.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 计算模型的精确率、召回率和F1。'
- en: '[M] What can we do to improve the model’s performance?'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 我们可以采取什么措施来提高模型的表现？'
- en: Consider a classification where 99% of data belongs to class A and 1% of data
    belongs to class B.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个分类，其中99%的数据属于类别A，1%的数据属于类别B。
- en: '[M] If your model predicts A 100% of the time, what would the F1 score be?
    **Hint**: The F1 score when A is mapped to 0 and B to 1 is different from the
    F1 score when A is mapped to 1 and B to 0.'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 如果你的模型100%预测A，F1分数会是多少？**提示**：当A映射到0而B映射到1时的F1分数与A映射到1而B映射到0时的F1分数不同。'
- en: '[M] If we have a model that predicts A and B at a random (uniformly), what
    would the expected F1 be?'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 如果我们有一个模型随机（均匀）地预测A和B，预期的F1分数是多少？'
- en: '[M] For logistic regression, why is log loss recommended over MSE (mean squared
    error)?'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 对于逻辑回归，为什么推荐使用对数损失而不是均方误差（MSE）？'
- en: '[M] When should we use RMSE (Root Mean Squared Error) over MAE (Mean Absolute
    Error) and vice versa?'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 我们应该在什么时候使用均方根误差（RMSE）而不是平均绝对误差（MAE）以及相反的情况？'
- en: '[M] Show that the negative log-likelihood and cross-entropy are the same for
    binary classification tasks.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 证明对于二分类任务，负对数似然和交叉熵是相同的。'
- en: '[M] For classification tasks with more than two labels (e.g. MNIST with 10
    labels), why is cross-entropy a better loss function than MSE?'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 对于有超过两个标签的分类任务（例如，MNIST有10个标签），为什么交叉熵比均方误差（MSE）是一个更好的损失函数？'
- en: '[E] Consider a language with an alphabet of 27 characters. What would be the
    maximal entropy of this language?'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 考虑一个有27个字符的字母表的语言。这个语言的熵最大值是多少？'
- en: '[E] A lot of machine learning models aim to approximate probability distributions.
    Let’s say P is the distribution of the data and Q is the distribution learned
    by our model. How do measure how close Q is to P?'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 许多机器学习模型旨在近似概率分布。假设P是数据的分布，Q是我们模型学习到的分布。我们如何衡量Q与P的接近程度？'
- en: MPE (Most Probable Explanation) vs. MAP (Maximum A Posteriori)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MPE（最可能解释）与MAP（最大后验概率）
- en: '[E] How do MPE and MAP differ?'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] MPE和MAP有什么不同？'
- en: '[H] Give an example of when they would produce different results.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 给出一个它们会产生不同结果的例子。'
- en: '[E] Suppose you want to build a model to predict the price of a stock in the
    next 8 hours and that the predicted price should never be off more than 10% from
    the actual price. Which metric would you use?'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 假设你想构建一个模型来预测未来8小时内股票的价格，并且预测的价格不应比实际价格高出10%。你会使用哪个指标？'
- en: '**Hint**: check out MAPE.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**提示**：查看MAPE。'
- en: '* * *'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In case you need a refresh on information entropy, here's an explanation without
    any math.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你需要对信息熵进行复习，这里有一个没有数学解释的解释。
- en: ''
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your parents are finally letting you adopt a pet! They spend the entire weekend
    taking you to various pet shelters to find a pet.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的父母终于让你领养宠物了！他们整个周末都带你去了各种宠物收容所，想找到一只宠物。
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first shelter has only dogs. Your mom covers your eyes when your dad picks
    out an animal for you. You don't need to open your eyes to know that this animal
    is a dog. It isn't hard to guess.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一个庇护所里只有狗。当你的爸爸为你挑选动物时，你的妈妈遮住了你的眼睛。你不需要睁开眼睛就能知道这只动物是狗。猜测起来不难。
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The second shelter has both dogs and cats. Again your mom covers your eyes and
    your dad picks out an email. This time, you have to think harder to guess which
    animal is that. You make a guess that it's a dog, and your dad says no. So you
    guess it's a cat and you're right. It takes you two guesses to know for sure what
    animal it is.
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第二个庇护所里既有狗也有猫。再次，你的妈妈遮住了你的眼睛，你的爸爸挑选了一只动物。这次，你必须更努力地思考才能猜出是哪种动物。你猜测它是一只狗，你爸爸说不是。所以你猜测它是一只猫，你猜对了。你用了两次猜测才确定是什么动物。
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The next shelter is the biggest one of them all. They have so many different
    kinds of animals: dogs, cats, hamsters, fish, parrots, cute little pigs, bunnies,
    ferrets, hedgehogs, chickens, even the exotic bearded dragons! There must be close
    to a hundred different types of pets. Now it''s really hard for you to guess which
    one your dad brings you. It takes you a dozen guesses to guess the right animal.'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 下一个庇护所是其中最大的一个。他们有各种各样的动物：狗、猫、仓鼠、鱼、鹦鹉、可爱的小猪、兔子、雪貂、刺猬、鸡，甚至有异国风情的鬃龙！可能有近一百种不同的宠物。现在你很难猜出你爸爸给你带来的是哪种动物。你用了十几次猜测才猜对正确的动物。
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Entropy is a measure of the "spread out" in diversity. The more spread out the
    diversity, the header it is to guess an item correctly. The first shelter has
    very low entropy. The second shelter has a little bit higher entropy. The third
    shelter has the highest entropy.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 熵是衡量多样性“分散”程度的指标。多样性越分散，猜测一个项目正确的难度就越大。第一个庇护所的熵非常低。第二个庇护所的熵略高。第三个庇护所的熵最高。
