- en: 8.2.1 Natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.2.1 自然语言处理
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html](https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html](https://huyenchip.com/ml-interviews-book/contents/8.2.1-natural-language-processing.html)
- en: RNNs
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNNs
- en: '[E] What’s the motivation for RNN?'
  id: totrans-3
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] RNN的动机是什么？'
- en: '[E] What’s the motivation for LSTM?'
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] LSTM的动机是什么？'
- en: '[M] How would you do dropouts in an RNN?'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你会如何在RNN中进行dropout？'
- en: '[E] What’s density estimation? Why do we say a language model is a density
    estimator?'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么是密度估计？为什么我们说语言模型是一个密度估计器？'
- en: '[M] Language models are often referred to as unsupervised learning, but some
    say its mechanism isn’t that different from supervised learning. What are your
    thoughts?'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 语言模型通常被称为无监督学习，但有人说其机制与监督学习并没有太大的区别。你的看法是什么？'
- en: Word embeddings.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词嵌入。
- en: '[M] Why do we need word embeddings?'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么我们需要词嵌入？'
- en: '[M] What’s the difference between count-based and prediction-based word embeddings?'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 基于计数和预测的词嵌入有什么区别？'
- en: '[H] Most word embedding algorithms are based on the assumption that words that
    appear in similar contexts have similar meanings. What are some of the problems
    with context-based word embeddings?'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 大多数词嵌入算法基于这样的假设：在相似上下文中出现的词具有相似的意义。基于上下文的词嵌入有哪些问题？'
- en: 'Given 5 documents:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定5个文档：
- en: '[PRE0]'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[M] Given a query Q: “The early bird gets the worm”, find the two top-ranked
    documents according to the TF/IDF rank using the cosine similarity measure and
    the term set {bird, duck, worm, early, get, love}. Are the top-ranked documents
    relevant to the query?'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 给定一个查询Q：“The early bird gets the worm”，使用余弦相似度度量以及术语集{bird, duck, worm,
    early, get, love}，根据TF/IDF排名找到两个排名最高的文档。这些排名最高的文档与查询相关吗？'
- en: '[M] Assume that document D5 goes on to tell more about the duck and the bird
    and mentions “bird” three times, instead of just once. What happens to the rank
    of D5? Is this change in the ranking of D5 a desirable property of TF/IDF? Why?'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 假设文档D5继续讲述关于鸭子和鸟的内容，并且提到“bird”三次，而不是一次。D5的排名会发生什么变化？这种对D5排名的变化是TF/IDF的理想属性吗？为什么？'
- en: '[E] Your client wants you to train a language model on their dataset but their
    dataset is very small with only about 10,000 tokens. Would you use an n-gram or
    a neural language model?'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你的客户希望你在他们的数据集上训练一个语言模型，但他们的数据集非常小，只有大约10,000个标记。你会使用n-gram模型还是神经语言模型？'
- en: '[E] For n-gram language models, does increasing the context length (n) improve
    the model’s performance? Why or why not?'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 对于n-gram语言模型，增加上下文长度（n）是否会提高模型的表现？为什么或为什么不？'
- en: '[M] What problems might we encounter when using softmax as the last layer for
    word-level language models? How do we fix it?'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 当我们将softmax作为词级语言模型的最后一层时，可能会遇到哪些问题？我们如何解决这个问题？'
- en: '[E] What''s the Levenshtein distance of the two words “doctor” and “bottle”?'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 两个单词“doctor”和“bottle”的Levenshtein距离是多少？'
- en: '[M] BLEU is a popular metric for machine translation. What are the pros and
    cons of BLEU?'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] BLEU是机器翻译中常用的一个指标。BLEU的优点和缺点是什么？'
- en: '[H] On the same test set, LM model A has a character-level entropy of 2 while
    LM model A has a word-level entropy of 6\. Which model would you choose to deploy?'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 在相同的测试集上，LM模型A具有2的字符级熵，而LM模型A具有6的词级熵。你会选择哪个模型进行部署？'
- en: '[M] Imagine you have to train a NER model on the text corpus A. Would you make
    A case-sensitive or case-insensitive?'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 假设你需要在文本语料库A上训练一个NER模型。你会使A区分大小写还是不区分大小写？'
- en: '[M] Why does removing stop words sometimes hurt a sentiment analysis model?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么去除停用词有时会损害情感分析模型？'
- en: '[M] Many models use relative position embedding instead of absolute position
    embedding. Why is that?'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 许多模型使用相对位置嵌入而不是绝对位置嵌入。为什么？'
- en: '[H] Some NLP models use the same weights for both the embedding layer and the
    layer just before softmax. What’s the purpose of this?'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 一些NLP模型在嵌入层和softmax层之前使用相同的权重。这种做法的目的是什么？'
