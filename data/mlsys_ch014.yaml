- en: AI Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI è®­ç»ƒ
- en: '*DALLÂ·E 3 Prompt: An illustration for AI training, depicting a neural network
    with neurons that are being repaired and firing. The scene includes a vast network
    of neurons, each glowing and firing to represent activity and learning. Among
    these neurons, small figures resembling engineers and scientists are actively
    working, repairing and tweaking the neurons. These miniature workers symbolize
    the process of training the network, adjusting weights and biases to achieve convergence.
    The entire scene is a visual metaphor for the intricate and collaborative effort
    involved in AI training, with the workers representing the continuous optimization
    and learning within a neural network. The background is a complex array of interconnected
    neurons, creating a sense of depth and complexity.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALLÂ·E 3 æç¤ºï¼šä¸€å¹…ç”¨äºAIè®­ç»ƒçš„æ’å›¾ï¼Œæç»˜äº†ä¸€ä¸ªç¥ç»ç½‘ç»œçš„ç¥ç»å…ƒæ­£åœ¨è¢«ä¿®å¤å’Œæ¿€æ´»ã€‚åœºæ™¯åŒ…æ‹¬ä¸€ä¸ªåºå¤§çš„ç¥ç»å…ƒç½‘ç»œï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½åœ¨å‘å…‰å’Œæ¿€æ´»ï¼Œä»¥è¡¨ç¤ºæ´»åŠ¨å’Œå­¦ä¹ ã€‚åœ¨è¿™äº›ç¥ç»å…ƒä¸­ï¼Œç±»ä¼¼å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶çš„å¾®å°äººç‰©æ­£åœ¨ç§¯æå·¥ä½œï¼Œä¿®å¤å’Œè°ƒæ•´ç¥ç»å…ƒã€‚è¿™äº›å¾®å‹å·¥äººè±¡å¾ç€è®­ç»ƒç½‘ç»œçš„è¿‡ç¨‹ï¼Œè°ƒæ•´æƒé‡å’Œåå·®ä»¥å®ç°æ”¶æ•›ã€‚æ•´ä¸ªåœºæ™¯æ˜¯AIè®­ç»ƒä¸­å¤æ‚ä¸”åä½œåŠªåŠ›çš„è§†è§‰éšå–»ï¼Œå·¥äººä»£è¡¨ç€ç¥ç»ç½‘ç»œå†…éƒ¨çš„æŒç»­ä¼˜åŒ–å’Œå­¦ä¹ ã€‚èƒŒæ™¯æ˜¯ä¸€ä¸ªå¤æ‚çš„ç›¸äº’è¿æ¥çš„ç¥ç»å…ƒé˜µåˆ—ï¼Œè¥é€ å‡ºæ·±åº¦å’Œå¤æ‚æ€§çš„æ„Ÿè§‰ã€‚*'
- en: '![](../media/file107.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file107.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®çš„
- en: '*Why do modern machine learning problems require new approaches to distributed
    computing and system architecture?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆç°ä»£æœºå™¨å­¦ä¹ é—®é¢˜éœ€è¦æ–°çš„åˆ†å¸ƒå¼è®¡ç®—å’Œç³»ç»Ÿæ¶æ„æ–¹æ³•ï¼Ÿ*'
- en: 'Machine learning training creates computational demands that exceed single
    machine capabilities, requiring distributed systems that coordinate computation
    across multiple devices and data centers. Training workloads have unique characteristics:
    massive datasets that cannot fit in memory, models with billions of parameters
    requiring coordinated updates, and iterative algorithms requiring continuous synchronization
    across distributed resources. These scale requirements create systems challenges
    in memory management, communication efficiency, fault tolerance, and resource
    scheduling that traditional systems were not designed to handle. As model complexity
    grows exponentially, understanding distributed training systems becomes necessary
    for any machine learning application of practical significance. The systems engineering
    principles developed for training at scale directly influence deployment architectures,
    cost structures, and feasibility of solutions across industries.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ è®­ç»ƒäº§ç”Ÿçš„è®¡ç®—éœ€æ±‚è¶…å‡ºäº†å•æœºèƒ½åŠ›ï¼Œéœ€è¦åè°ƒå¤šä¸ªè®¾å¤‡å’Œæ•°æ®ä¸­å¿ƒè®¡ç®—çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚è®­ç»ƒå·¥ä½œè´Ÿè½½å…·æœ‰ç‹¬ç‰¹çš„ç‰¹å¾ï¼šåºå¤§çš„æ•°æ®é›†æ— æ³•å…¨éƒ¨è£…å…¥å†…å­˜ï¼Œæ‹¥æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹éœ€è¦åè°ƒæ›´æ–°ï¼Œä»¥åŠéœ€è¦è·¨åˆ†å¸ƒå¼èµ„æºæŒç»­åŒæ­¥çš„è¿­ä»£ç®—æ³•ã€‚è¿™äº›æ‰©å±•éœ€æ±‚åœ¨å†…å­˜ç®¡ç†ã€é€šä¿¡æ•ˆç‡ã€å®¹é”™æ€§å’Œèµ„æºè°ƒåº¦æ–¹é¢ç»™ç³»ç»Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè€Œä¼ ç»Ÿç³»ç»Ÿå¹¶æœªè®¾è®¡æ¥å¤„ç†è¿™äº›æŒ‘æˆ˜ã€‚éšç€æ¨¡å‹å¤æ‚æ€§çš„æŒ‡æ•°å¢é•¿ï¼Œç†è§£åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå¯¹äºä»»ä½•å…·æœ‰å®é™…æ„ä¹‰çš„æœºå™¨å­¦ä¹ åº”ç”¨éƒ½å˜å¾—å¿…è¦ã€‚ä¸ºå¤§è§„æ¨¡è®­ç»ƒå¼€å‘çš„ç³»ç»Ÿå·¥ç¨‹åŸåˆ™ç›´æ¥å½±å“ç€éƒ¨ç½²æ¶æ„ã€æˆæœ¬ç»“æ„å’Œè·¨è¡Œä¸šçš„è§£å†³æ–¹æ¡ˆå¯è¡Œæ€§ã€‚
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç›®æ ‡**'
- en: Explain how mathematical operations in neural networks (matrix multiplications,
    activation functions, backpropagation) translate to computational and memory system
    requirements
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£é‡Šç¥ç»ç½‘ç»œä¸­çš„æ•°å­¦è¿ç®—ï¼ˆçŸ©é˜µä¹˜æ³•ã€æ¿€æ´»å‡½æ•°ã€åå‘ä¼ æ’­ï¼‰å¦‚ä½•è½¬åŒ–ä¸ºè®¡ç®—å’Œå†…å­˜ç³»ç»Ÿéœ€æ±‚
- en: Analyze performance bottlenecks in training pipelines including data loading,
    memory bandwidth limitations, and compute utilization patterns
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†æè®­ç»ƒç®¡é“ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€å†…å­˜å¸¦å®½é™åˆ¶å’Œè®¡ç®—åˆ©ç”¨ç‡æ¨¡å¼
- en: Design training pipeline architectures integrating data preprocessing, computation
    passes, and parameter updates efficiently
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜æ•ˆåœ°è®¾è®¡é›†æˆæ•°æ®é¢„å¤„ç†ã€è®¡ç®—éå†å’Œå‚æ•°æ›´æ–°çš„è®­ç»ƒç®¡é“æ¶æ„
- en: Apply single-machine optimization techniques including mixed-precision training,
    gradient accumulation, and activation checkpointing to maximize resource utilization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨å•æœºä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œä»¥æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡
- en: Compare distributed training strategies (data parallelism, model parallelism,
    pipeline parallelism) and select appropriate approaches based on model characteristics
    and hardware constraints
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒåˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼ˆæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œï¼‰ï¼Œå¹¶æ ¹æ®æ¨¡å‹ç‰¹æ€§å’Œç¡¬ä»¶çº¦æŸé€‰æ‹©é€‚å½“çš„æ–¹æ³•
- en: Evaluate specialized hardware platforms (GPUs, TPUs, FPGAs, ASICs) for training
    workloads and optimize code for specific architectural features
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç”¨äºè®­ç»ƒå·¥ä½œè´Ÿè½½çš„ä¸“ç”¨ç¡¬ä»¶å¹³å°ï¼ˆGPUã€TPUã€FPGAã€ASICï¼‰ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šæ¶æ„ç‰¹æ€§ä¼˜åŒ–ä»£ç 
- en: Implement optimization algorithms (SGD, Adam, AdamW) within training frameworks
    while understanding their memory and computational implications
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¡†æ¶ä¸­å®ç°ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Adamã€AdamWï¼‰çš„åŒæ—¶ï¼Œç†è§£å®ƒä»¬çš„å†…å­˜å’Œè®¡ç®—å½±å“
- en: Critique common training system design decisions to avoid performance pitfalls
    and scaling bottlenecks
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹åˆ¤å¸¸è§çš„è®­ç»ƒç³»ç»Ÿè®¾è®¡å†³ç­–ï¼Œä»¥é¿å…æ€§èƒ½é™·é˜±å’Œæ‰©å±•ç“¶é¢ˆ
- en: Training Systems Evolution and Architecture
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿæ¼”å˜ä¸æ¶æ„
- en: Training represents the most demanding phase in machine learning systems, where
    theoretical constructs become practical reality through computational optimization.
    Building upon the system design methodologies established in [ChapterÂ 2](ch008.xhtml#sec-ml-systems),
    data pipeline architectures explored in [ChapterÂ 6](ch012.xhtml#sec-data-engineering),
    and computational frameworks examined in [ChapterÂ 7](ch013.xhtml#sec-ai-frameworks),
    this chapter examines how algorithmic theory, data processing, and hardware architecture
    converge in the iterative refinement of intelligent systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„é˜¶æ®µï¼Œåœ¨è¿™ä¸€é˜¶æ®µï¼Œç†è®ºç»“æ„é€šè¿‡è®¡ç®—ä¼˜åŒ–å˜ä¸ºå®é™…ç°å®ã€‚åœ¨ç¬¬äºŒç« [ChapterÂ 2](ch008.xhtml#sec-ml-systems)ä¸­å»ºç«‹çš„ç³»ç»Ÿè®¾è®¡æ–¹æ³•ã€ç¬¬å…­ç« [ChapterÂ 6](ch012.xhtml#sec-data-engineering)ä¸­æ¢ç´¢çš„æ•°æ®ç®¡é“æ¶æ„ä»¥åŠç¬¬ä¸ƒç« [ChapterÂ 7](ch013.xhtml#sec-ai-frameworks)ä¸­è€ƒå¯Ÿçš„è®¡ç®—æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œæœ¬ç« æ¢è®¨äº†ç®—æ³•ç†è®ºã€æ•°æ®å¤„ç†å’Œç¡¬ä»¶æ¶æ„å¦‚ä½•åœ¨æ™ºèƒ½ç³»ç»Ÿçš„è¿­ä»£ä¼˜åŒ–ä¸­ç›¸äº’èåˆã€‚
- en: 'Training constitutes the most computationally demanding phase in the machine
    learning systems lifecycle, requiring careful orchestration of mathematical optimization
    processes with distributed systems engineering principles. Contemporary training
    workloads impose computational requirements that exceed conventional computing
    paradigms: models with billions of parameters demand terabytes of memory capacity,
    training corpora span petabyte-scale storage systems, and gradient-based optimization
    algorithms require synchronized computation across thousands of processing units.
    These computational scales create systems engineering challenges in memory hierarchy
    management, inter-node communication efficiency, and resource allocation strategies
    that distinguish training infrastructure from general-purpose computing architectures.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ„æˆäº†æœºå™¨å­¦ä¹ ç³»ç»Ÿç”Ÿå‘½å‘¨æœŸä¸­æœ€è®¡ç®—å¯†é›†çš„é˜¶æ®µï¼Œéœ€è¦è°¨æ…åœ°è¿ç”¨åˆ†å¸ƒå¼ç³»ç»Ÿå·¥ç¨‹åŸåˆ™æ¥ç¼–æ’æ•°å­¦ä¼˜åŒ–è¿‡ç¨‹ã€‚å½“ä»£çš„è®­ç»ƒå·¥ä½œè´Ÿè½½å¯¹è®¡ç®—èƒ½åŠ›æå‡ºäº†è¶…å‡ºä¼ ç»Ÿè®¡ç®—èŒƒå¼çš„éœ€æ±‚ï¼šæ‹¥æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹éœ€è¦TBçº§çš„å†…å­˜å®¹é‡ï¼Œè®­ç»ƒè¯­æ–™åº“è·¨è¶ŠPBçº§å­˜å‚¨ç³»ç»Ÿï¼ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•éœ€è¦åœ¨æ•°åƒä¸ªå¤„ç†å•å…ƒä¹‹é—´è¿›è¡ŒåŒæ­¥è®¡ç®—ã€‚è¿™äº›è®¡ç®—è§„æ¨¡åœ¨å†…å­˜å±‚æ¬¡ç®¡ç†ã€èŠ‚ç‚¹é—´é€šä¿¡æ•ˆç‡å’Œèµ„æºåˆ†é…ç­–ç•¥ä¸Šä¸ºç³»ç»Ÿå·¥ç¨‹å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜å°†è®­ç»ƒåŸºç¡€è®¾æ–½ä¸é€šç”¨è®¡ç®—æ¶æ„åŒºåˆ†å¼€æ¥ã€‚
- en: The design methodologies established in preceding chapters serve as architectural
    foundations during the training phase. The modular system architectures from [ChapterÂ 2](ch008.xhtml#sec-ml-systems)
    enable distributed training orchestration, the engineered data pipelines from
    [ChapterÂ 6](ch012.xhtml#sec-data-engineering) provide continuous training sample
    streams, and the computational frameworks from [ChapterÂ 7](ch013.xhtml#sec-ai-frameworks)
    supply necessary algorithmic abstractions. Training systems integration represents
    where theoretical design principles meet performance engineering constraints,
    establishing the computational foundation for the optimization techniques investigated
    in Part III.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‡ ç« ä¸­å»ºç«‹çš„è®¾è®¡æ–¹æ³•åœ¨è®­ç»ƒé˜¶æ®µå……å½“äº†æ¶æ„åŸºç¡€ã€‚ç¬¬äºŒç« ä¸­æåˆ°çš„æ¨¡å—åŒ–ç³»ç»Ÿæ¶æ„[ChapterÂ 2](ch008.xhtml#sec-ml-systems)ä½¿å¾—åˆ†å¸ƒå¼è®­ç»ƒç¼–æ’æˆä¸ºå¯èƒ½ï¼Œç¬¬å…­ç« ä¸­æåˆ°çš„å·¥ç¨‹åŒ–æ•°æ®å¤„ç†ç®¡é“[ChapterÂ 6](ch012.xhtml#sec-data-engineering)æä¾›äº†æŒç»­çš„è®­ç»ƒæ ·æœ¬æµï¼Œç¬¬ä¸ƒç« ä¸­æåˆ°çš„è®¡ç®—æ¡†æ¶[ChapterÂ 7](ch013.xhtml#sec-ai-frameworks)æä¾›äº†å¿…è¦çš„ç®—æ³•æŠ½è±¡ã€‚è®­ç»ƒç³»ç»Ÿé›†æˆä»£è¡¨äº†ç†è®ºè®¾è®¡åŸåˆ™ä¸æ€§èƒ½å·¥ç¨‹çº¦æŸç›¸é‡çš„åœ°æ–¹ï¼Œä¸ºç¬¬ä¸‰éƒ¨åˆ†ä¸­æ¢è®¨çš„ä¼˜åŒ–æŠ€æœ¯å»ºç«‹äº†è®¡ç®—åŸºç¡€ã€‚
- en: This chapter develops systems engineering foundations for scalable training
    infrastructure. We examine the translation of mathematical operations in parametric
    models into concrete computational requirements, analyze performance bottlenecks
    within training pipelines including memory bandwidth limitations and computational
    throughput constraints, and architect systems that achieve high efficiency while
    maintaining fault tolerance guarantees. Through exploration of single-node optimization
    strategies, distributed training methodologies, and specialized hardware utilization
    patterns, this chapter develops the systems engineering perspective needed for
    constructing training infrastructure capable of scaling from experimental prototypes
    to production-grade deployments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å¼€å‘äº†å¯æ‰©å±•è®­ç»ƒåŸºç¡€è®¾æ–½çš„ç³»ç»Ÿå·¥ç¨‹åŸºç¡€ã€‚æˆ‘ä»¬æ¢è®¨äº†åœ¨å‚æ•°æ¨¡å‹ä¸­å°†æ•°å­¦è¿ç®—è½¬æ¢ä¸ºå…·ä½“è®¡ç®—éœ€æ±‚çš„è¿‡ç¨‹ï¼Œåˆ†æäº†è®­ç»ƒç®¡é“ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼ŒåŒ…æ‹¬å†…å­˜å¸¦å®½é™åˆ¶å’Œè®¡ç®—ååé‡çº¦æŸï¼Œå¹¶è®¾è®¡äº†åœ¨ä¿æŒå®¹é”™ä¿è¯çš„åŒæ—¶å®ç°é«˜æ•ˆç‡çš„ç³»ç»Ÿã€‚é€šè¿‡æ¢ç´¢å•èŠ‚ç‚¹ä¼˜åŒ–ç­–ç•¥ã€åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•å’Œä¸“ç”¨ç¡¬ä»¶åˆ©ç”¨æ¨¡å¼ï¼Œæœ¬ç« å‘å±•äº†æ„å»ºèƒ½å¤Ÿä»å®éªŒåŸå‹æ‰©å±•åˆ°ç”Ÿäº§çº§éƒ¨ç½²çš„è®­ç»ƒåŸºç¡€è®¾æ–½æ‰€éœ€çš„ç³»ç»Ÿå·¥ç¨‹è§†è§’ã€‚
- en: '**Lighthouse Example: Training GPT-2**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¯å¡”ç¤ºä¾‹ï¼šè®­ç»ƒGPT-2**'
- en: 'This chapter uses **training GPT-2 (1.5 billion parameters)** as a consistent
    reference point to ground abstract concepts in concrete reality. GPT-2 represents
    an ideal teaching example because it:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä½¿ç”¨**è®­ç»ƒGPT-2ï¼ˆ15äº¿å‚æ•°ï¼‰**ä½œä¸ºä¸€ä¸ªä¸€è‡´çš„å‚è€ƒç‚¹ï¼Œå°†æŠ½è±¡æ¦‚å¿µå…·ä½“åŒ–ã€‚GPT-2æ˜¯ä¸€ä¸ªç†æƒ³çš„æ•™å­¦ç¤ºä¾‹ï¼Œå› ä¸ºå®ƒï¼š
- en: '**Spans the scale spectrum**: Large enough to require serious optimization,
    small enough to train without massive infrastructure'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è·¨è¶Šè§„æ¨¡èŒƒå›´**ï¼šè¶³å¤Ÿå¤§ï¼Œéœ€è¦ä¸¥è‚ƒçš„ä¼˜åŒ–ï¼Œè¶³å¤Ÿå°ï¼Œæ— éœ€å¤§è§„æ¨¡åŸºç¡€è®¾æ–½è¿›è¡Œè®­ç»ƒ'
- en: '**Has well-documented architecture**: 48 transformer layers, 1280 hidden dimensions,
    20 attention heads'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…·æœ‰å®Œå–„çš„æ¶æ„æ–‡æ¡£**ï¼š48ä¸ªtransformerå±‚ï¼Œ1280ä¸ªéšè—ç»´åº¦ï¼Œ20ä¸ªæ³¨æ„åŠ›å¤´'
- en: '**Exhibits all key training challenges**: Memory pressure, computational intensity,
    data pipeline complexity'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¡¨ç°å‡ºæ‰€æœ‰å…³é”®è®­ç»ƒæŒ‘æˆ˜**ï¼šå†…å­˜å‹åŠ›ã€è®¡ç®—å¼ºåº¦ã€æ•°æ®ç®¡é“å¤æ‚æ€§'
- en: '**Represents modern ML systems**: Transformer-based models dominate contemporary
    machine learning'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»£è¡¨ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿ**ï¼šåŸºäºTransformerçš„æ¨¡å‹ä¸»å¯¼ç€å½“ä»£æœºå™¨å­¦ä¹ '
- en: '**Transformer Architecture Primer:**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformeræ¶æ„å…¥é—¨**ï¼š'
- en: 'GPT-2 uses a transformer architecture (detailed in [ChapterÂ 4](ch010.xhtml#sec-dnn-architectures))
    that processes text through self-attention mechanisms. Understanding these key
    computational patterns provides essential context for the training examples throughout
    this chapter:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2ä½¿ç”¨Transformeræ¶æ„ï¼ˆåœ¨ç¬¬4ç« ä¸­è¯¦ç»†è¯´æ˜ï¼‰ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤„ç†æ–‡æœ¬ã€‚ç†è§£è¿™äº›å…³é”®è®¡ç®—æ¨¡å¼ä¸ºæœ¬ç« ä¸­çš„è®­ç»ƒç¤ºä¾‹æä¾›äº†å¿…è¦çš„èƒŒæ™¯ï¼š
- en: '**Self-attention**: Computes relationships between all words in a sequence
    through matrix operations (Query Ã— Key^T), producing attention scores that weight
    how much each word should influence others'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªæ³¨æ„åŠ›**ï¼šé€šè¿‡çŸ©é˜µè¿ç®—ï¼ˆæŸ¥è¯¢Ã—é”®çš„è½¬ç½®ï¼‰è®¡ç®—åºåˆ—ä¸­æ‰€æœ‰å•è¯ä¹‹é—´çš„å…³ç³»ï¼Œäº§ç”Ÿæ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°è¡¡é‡æ¯ä¸ªå•è¯å¯¹å…¶ä»–å•è¯çš„å½±å“ç¨‹åº¦'
- en: '**Multi-head attention**: Parallelizes attention across multiple â€œheadsâ€ (GPT-2
    uses 20), each learning different relationship patterns'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šå¤´æ³¨æ„åŠ›**ï¼šåœ¨å¤šä¸ªâ€œå¤´â€ä¹‹é—´å¹¶è¡ŒåŒ–æ³¨æ„åŠ›ï¼ˆGPT-2ä½¿ç”¨20ä¸ªï¼‰ï¼Œæ¯ä¸ªâ€œå¤´â€å­¦ä¹ ä¸åŒçš„å…³ç³»æ¨¡å¼'
- en: '**Transformer layers**: Stack attention with feed-forward networks (GPT-2 has
    48 layers), enabling hierarchical feature learning'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformerå±‚**ï¼šå †å æ³¨æ„åŠ›ä¸å‰é¦ˆç½‘ç»œï¼ˆGPT-2æœ‰48å±‚ï¼‰ï¼Œå®ç°å±‚æ¬¡åŒ–ç‰¹å¾å­¦ä¹ '
- en: '**Key computational pattern**: Dominated by large matrix multiplications (attention
    score calculation, feed-forward networks) that benefit from GPU parallelization'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…³é”®è®¡ç®—æ¨¡å¼**ï¼šä¸»è¦ç”±å¤§å‹çŸ©é˜µä¹˜æ³•ï¼ˆæ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ã€å‰é¦ˆç½‘ç»œï¼‰ä¸»å¯¼ï¼Œè¿™äº›è®¡ç®—æ¨¡å¼å—ç›ŠäºGPUå¹¶è¡ŒåŒ–'
- en: 'This architectureâ€™s heavy reliance on matrix multiplication and sequential
    dependencies creates the specific training system challenges we explore: massive
    activation memory requirements, communication bottlenecks in distributed training,
    and opportunities for mixed-precision optimization.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¶æ„å¯¹çŸ©é˜µä¹˜æ³•å’Œé¡ºåºä¾èµ–çš„ä¸¥é‡ä¾èµ–ï¼Œä¸ºæˆ‘ä»¬æ¢ç´¢çš„å…·ä½“è®­ç»ƒç³»ç»ŸæŒ‘æˆ˜åˆ›é€ äº†æ¡ä»¶ï¼šå·¨å¤§çš„æ¿€æ´»å†…å­˜éœ€æ±‚ã€åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„é€šä¿¡ç“¶é¢ˆä»¥åŠæ··åˆç²¾åº¦ä¼˜åŒ–çš„æœºä¼šã€‚
- en: '**Key GPT-2 Specifications:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®GPT-2è§„æ ¼**ï¼š'
- en: '**Parameters**: 1.542B (1,558,214,656 exact count)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚æ•°**ï¼š1.542Bï¼ˆç¡®åˆ‡è®¡æ•°ä¸º1,558,214,656ï¼‰'
- en: '**Training Data**: OpenWebText (~40GB text, ~9B tokens)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ•°æ®**ï¼šOpenWebTextï¼ˆçº¦40GBæ–‡æœ¬ï¼Œçº¦90äº¿ä¸ªæ ‡è®°ï¼‰'
- en: '**Batch Configuration**: Typically 512 effective batch size across 8-32 GPUs'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹å¤„ç†é…ç½®**ï¼šé€šå¸¸åœ¨8-32ä¸ªGPUä¸Šå®ç°512ä¸ªæœ‰æ•ˆæ‰¹å¤„ç†å¤§å°'
- en: '**Memory Footprint**: ~3GB parameters (FP16: 16-bit floating point, using 2
    bytes per value vs 4 bytes for FP32), ~18GB activations (batch_size=32)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å†…å­˜å ç”¨**ï¼šçº¦3GBå‚æ•°ï¼ˆFP16ï¼š16ä½æµ®ç‚¹æ•°ï¼Œæ¯ä¸ªå€¼ä½¿ç”¨2å­—èŠ‚ï¼Œè€ŒFP32ä½¿ç”¨4å­—èŠ‚ï¼‰ï¼Œçº¦18GBæ¿€æ´»ï¼ˆbatch_size=32ï¼‰'
- en: '**Training Time**: ~2 weeks on 32 V100 GPUs'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ—¶é—´**ï¼šåœ¨32ä¸ªV100 GPUä¸Šçº¦ä¸º2å‘¨'
- en: '**Note on precision formats**: Throughout this chapter, we reference **FP32**
    (32-bit) and **FP16** (16-bit) floating-point formats. FP16 halves memory requirements
    and enables faster computation on modern GPUs with Tensor Cores. **Mixed-precision
    training** (detailed in [SectionÂ 8.5.4](ch014.xhtml#sec-ai-training-mixedprecision-training-77ad))
    strategically combines FP16 for most operations with FP32 for numerical stability,
    achieving 2Ã— memory savings and 2-3Ã— speedups while maintaining accuracy.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³äºç²¾åº¦æ ¼å¼çš„è¯´æ˜**ï¼šåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å‚è€ƒäº†**FP32**ï¼ˆ32ä½ï¼‰å’Œ**FP16**ï¼ˆ16ä½ï¼‰æµ®ç‚¹æ•°æ ¼å¼ã€‚FP16å°†å†…å­˜éœ€æ±‚å‡åŠï¼Œå¹¶å…è®¸åœ¨ç°ä»£å…·æœ‰Tensor
    Coreçš„GPUä¸Šå®ç°æ›´å¿«çš„è®¡ç®—ã€‚**æ··åˆç²¾åº¦è®­ç»ƒ**ï¼ˆè¯¦è§[ç¬¬8.5.4èŠ‚](ch014.xhtml#sec-ai-training-mixedprecision-training-77ad)ï¼‰ç­–ç•¥æ€§åœ°å°†FP16ç”¨äºå¤§å¤šæ•°æ“ä½œä¸FP32ç”¨äºæ•°å€¼ç¨³å®šæ€§ç›¸ç»“åˆï¼Œå®ç°äº†2å€çš„å†…å­˜èŠ‚çœå’Œ2-3å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒç²¾åº¦ã€‚'
- en: '**ğŸ”„ GPT-2 Example Markers** appear at strategic points where this specific
    model illuminates the concept under discussion. Each example provides quantitative
    specifications, performance tradeoffs, and concrete implementation decisions encountered
    in training this model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ”„ GPT-2ç¤ºä¾‹æ ‡è®°**å‡ºç°åœ¨è¯¥ç‰¹å®šæ¨¡å‹é˜æ˜è®¨è®ºä¸­çš„æ¦‚å¿µçš„å…³é”®ç‚¹ã€‚æ¯ä¸ªç¤ºä¾‹éƒ½æä¾›äº†å®šé‡è§„æ ¼ã€æ€§èƒ½æƒè¡¡ä»¥åŠåœ¨è®­ç»ƒæ­¤æ¨¡å‹æ—¶é‡åˆ°çš„å…·ä½“å®ç°å†³ç­–ã€‚'
- en: Training Systems
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿ
- en: The development of modern machine learning models relies on specialized computational
    frameworks that manage the complex process of iterative optimization. These systems
    differ from traditional computing infrastructures, requiring careful orchestration
    of data processing, gradient computation, parameter updates, and distributed coordination
    across potentially thousands of devices. Understanding what constitutes a training
    system and how it differs from general-purpose computing provides the foundation
    for the architectural decisions and optimization strategies that follow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ä¾èµ–äºä¸“é—¨çš„è®¡ç®—æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶ç®¡ç†ç€è¿­ä»£ä¼˜åŒ–çš„å¤æ‚è¿‡ç¨‹ã€‚è¿™äº›ç³»ç»Ÿä¸ä¼ ç»Ÿè®¡ç®—åŸºç¡€è®¾æ–½ä¸åŒï¼Œéœ€è¦ä»”ç»†åè°ƒæ•°æ®å¤„ç†ã€æ¢¯åº¦è®¡ç®—ã€å‚æ•°æ›´æ–°ä»¥åŠè·¨å¯èƒ½æˆåƒä¸Šä¸‡çš„è®¾å¤‡è¿›è¡Œåˆ†å¸ƒå¼åè°ƒã€‚ç†è§£æ„æˆè®­ç»ƒç³»ç»ŸåŠå…¶ä¸é€šç”¨è®¡ç®—çš„ä¸åŒï¼Œä¸ºåç»­çš„æ¶æ„å†³ç­–å’Œä¼˜åŒ–ç­–ç•¥æä¾›äº†åŸºç¡€ã€‚
- en: '***Machine Learning Training Systems*** are computational frameworks that execute
    the *iterative optimization* of model parameters through coordinated *data processing*,
    *gradient computation*, and *distributed computation* across hardware and software
    infrastructure.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿ***æ˜¯æ‰§è¡Œæ¨¡å‹å‚æ•°çš„*è¿­ä»£ä¼˜åŒ–*çš„è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡åè°ƒ*æ•°æ®å¤„ç†*ã€*æ¢¯åº¦è®¡ç®—*å’Œè·¨ç¡¬ä»¶å’Œè½¯ä»¶åŸºç¡€è®¾æ–½çš„*åˆ†å¸ƒå¼è®¡ç®—*ã€‚'
- en: Designing effective training architectures requires recognizing that machine
    learning training systems represent a distinct class of computational workload
    with unique demands on hardware and software infrastructure. When you execute
    training commands in frameworks like PyTorch or TensorFlow, these systems must
    efficiently orchestrate repeated computations over large datasets while managing
    memory requirements and data movement patterns that exceed the capabilities of
    general-purpose computing architectures.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾è®¡æœ‰æ•ˆçš„è®­ç»ƒæ¶æ„éœ€è¦è®¤è¯†åˆ°æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿä»£è¡¨ä¸€ç±»ç‹¬ç‰¹çš„è®¡ç®—å·¥ä½œè´Ÿè½½ï¼Œå¯¹ç¡¬ä»¶å’Œè½¯ä»¶åŸºç¡€è®¾æ–½æœ‰ç‹¬ç‰¹çš„éœ€æ±‚ã€‚å½“ä½ åœ¨PyTorchæˆ–TensorFlowç­‰æ¡†æ¶ä¸­æ‰§è¡Œè®­ç»ƒå‘½ä»¤æ—¶ï¼Œè¿™äº›ç³»ç»Ÿå¿…é¡»é«˜æ•ˆåœ°åœ¨å¤§æ•°æ®é›†ä¸Šç®¡ç†é‡å¤è®¡ç®—ï¼ŒåŒæ—¶å¤„ç†è¶…å‡ºé€šç”¨è®¡ç®—æ¶æ„èƒ½åŠ›çš„å†…å­˜éœ€æ±‚å’Œæ•°æ®ç§»åŠ¨æ¨¡å¼ã€‚
- en: 'Training workloads exhibit three characteristics that distinguish them from
    traditional computing: extreme computational intensity from iterative gradient
    computations across massive models, substantial memory pressure from storing parameters,
    activations, and optimizer states simultaneously, and complex data dependencies
    requiring synchronized parameter updates across distributed resources. A single
    training run for large language models requires approximately <semantics><msup><mn>10</mn><mn>23</mn></msup><annotation
    encoding="application/x-tex">10^{23}</annotation></semantics> floating-point operations
    ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)), memory footprints
    reaching terabytes when including activation storage, and coordination across
    thousands of devicesâ€”demands that general-purpose systems were never designed
    to handle.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå·¥ä½œè´Ÿè½½è¡¨ç°å‡ºä¸‰ä¸ªç‰¹å¾ï¼Œä½¿å…¶ä¸ä¼ ç»Ÿè®¡ç®—åŒºåˆ†å¼€æ¥ï¼šä»å¤§è§„æ¨¡æ¨¡å‹ä¸­è¿›è¡Œçš„è¿­ä»£æ¢¯åº¦è®¡ç®—å¸¦æ¥çš„æç«¯è®¡ç®—å¼ºåº¦ï¼Œå­˜å‚¨å‚æ•°ã€æ¿€æ´»å’Œä¼˜åŒ–å™¨çŠ¶æ€æ—¶äº§ç”Ÿçš„å·¨å¤§å†…å­˜å‹åŠ›ï¼Œä»¥åŠéœ€è¦è·¨åˆ†å¸ƒå¼èµ„æºåŒæ­¥å‚æ•°æ›´æ–°çš„å¤æ‚æ•°æ®ä¾èµ–ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€æ¬¡è®­ç»ƒè¿è¡Œå¤§çº¦éœ€è¦çº¦<semantics><msup><mn>10</mn><mn>23</mn></msup><annotation
    encoding="application/x-tex">10^{23}</annotation></semantics>æ¬¡æµ®ç‚¹è¿ç®—ï¼ˆ[T. Brownç­‰äºº
    2020](ch058.xhtml#ref-brown2020language)ï¼‰ï¼ŒåŒ…æ‹¬æ¿€æ´»å­˜å‚¨åœ¨å†…çš„å†…å­˜å ç”¨è¾¾åˆ°æ•°å¤ªå­—èŠ‚ï¼Œä»¥åŠæ•°åƒå°è®¾å¤‡çš„åè°ƒâ€”â€”è¿™äº›éœ€æ±‚æ˜¯é€šç”¨ç³»ç»Ÿä»æœªè®¾è®¡æ¥å¤„ç†çš„ã€‚
- en: Understanding why contemporary training systems evolved their current architectures
    requires examining how computing systems progressively adapted to increasingly
    demanding workloads. While training focuses on iterative optimization for learning,
    inference systems (detailed throughout this book) optimize for low-latency prediction
    serving. These represent two complementary but distinct computational paradigms.
    The architectural progression from general-purpose computing to specialized training
    systems reveals systems principles that inform modern training infrastructure
    design. Unlike traditional high-performance computing workloads, training systems
    exhibit specific characteristics that influence their design and implementation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£ä¸ºä»€ä¹ˆå½“ä»£è®­ç»ƒç³»ç»Ÿæ¼”å˜æˆå½“å‰çš„æ¶æ„éœ€è¦è€ƒå¯Ÿè®¡ç®—ç³»ç»Ÿå¦‚ä½•é€æ­¥é€‚åº”æ—¥ç›Šå¢é•¿çš„å·¥ä½œè´Ÿè½½ã€‚è™½ç„¶è®­ç»ƒä¾§é‡äºè¿­ä»£çš„ä¼˜åŒ–ä»¥å®ç°å­¦ä¹ ï¼Œä½†æ¨ç†ç³»ç»Ÿï¼ˆæœ¬ä¹¦ä¸­è¯¦ç»†é˜è¿°ï¼‰ä¼˜åŒ–çš„æ˜¯ä½å»¶è¿Ÿçš„é¢„æµ‹æœåŠ¡ã€‚è¿™äº›ä»£è¡¨äº†ä¸¤ç§äº’è¡¥ä½†ä¸åŒçš„è®¡ç®—èŒƒå¼ã€‚ä»é€šç”¨è®¡ç®—åˆ°ä¸“ç”¨è®­ç»ƒç³»ç»Ÿçš„æ¶æ„æ¼”è¿›æ­ç¤ºäº†ç°ä»£è®­ç»ƒåŸºç¡€è®¾æ–½è®¾è®¡æ‰€ä¾æ®çš„ç³»ç»ŸåŸåˆ™ã€‚ä¸ä¼ ç»Ÿçš„è¶…é«˜æ€§èƒ½è®¡ç®—å·¥ä½œè´Ÿè½½ä¸åŒï¼Œè®­ç»ƒç³»ç»Ÿè¡¨ç°å‡ºç‰¹å®šçš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å½±å“äº†å®ƒä»¬çš„è®¾è®¡å’Œå®ç°ã€‚
- en: Computing Architecture Evolution for ML Training
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ è®­ç»ƒçš„è®¡ç®—æ¶æ„æ¼”å˜
- en: Computing system architectures have evolved through distinct generations, with
    each new era building upon previous advances while introducing specialized optimizations
    for emerging application requirements ([FigureÂ 8.1](ch014.xhtml#fig-evolution-systems)).
    This evolution parallels the development of ML frameworks and software stacks
    detailed in [ChapterÂ 7](ch013.xhtml#sec-ai-frameworks), which have co-evolved
    with hardware to enable efficient utilization of these computational resources.
    This progression demonstrates how hardware adaptation to application needs shapes
    modern machine learning systems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç³»ç»Ÿæ¶æ„ç»å†äº†ä¸åŒçš„ä¸–ä»£æ¼”å˜ï¼Œæ¯ä¸ªæ–°æ—¶ä»£éƒ½æ˜¯åœ¨å‰ä¸€ä»£è¿›æ­¥çš„åŸºç¡€ä¸Šå»ºç«‹çš„ï¼ŒåŒæ—¶å¼•å…¥äº†é’ˆå¯¹æ–°å…´åº”ç”¨éœ€æ±‚çš„ä¸“ç”¨ä¼˜åŒ–ï¼ˆ[å›¾8.1](ch014.xhtml#fig-evolution-systems)ï¼‰ã€‚è¿™ç§æ¼”å˜ä¸ç¬¬7ç« ä¸­è¯¦ç»†ä»‹ç»çš„æœºå™¨å­¦ä¹ æ¡†æ¶å’Œè½¯ä»¶å †æ ˆçš„å‘å±•å¹¶è¡Œï¼Œå®ƒä»¬ä¸ç¡¬ä»¶å…±åŒè¿›åŒ–ï¼Œä»¥å®ç°è¿™äº›è®¡ç®—èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚è¿™ä¸€è¿›å±•å±•ç¤ºäº†ç¡¬ä»¶å¦‚ä½•é€‚åº”åº”ç”¨éœ€æ±‚æ¥å¡‘é€ ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿã€‚
- en: '![](../media/file108.svg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file108.svg)'
- en: 'FigureÂ 8.1: **Computing System Evolution**: Hardware advancements continuously
    adapt to the increasing demands of machine learning workloads, transitioning from
    centralized mainframes to specialized architectures like gpus and AI hypercomputing
    systems optimized for parallel processing and massive datasets. This progression
    reflects a shift toward accelerating model training and inference through increased
    computational power and memory bandwidth.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.1ï¼š**è®¡ç®—ç³»ç»Ÿæ¼”å˜**ï¼šç¡¬ä»¶è¿›æ­¥æŒç»­é€‚åº”æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ä¸æ–­å¢é•¿çš„éœ€æ±‚ï¼Œä»é›†ä¸­å¼ä¸»æœºè¿‡æ¸¡åˆ°å¦‚gpuå’Œé’ˆå¯¹å¹¶è¡Œå¤„ç†å’Œå¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–çš„AIè¶…è®¡ç®—ç³»ç»Ÿç­‰ä¸“ç”¨æ¶æ„ã€‚è¿™ä¸€è¿›å±•åæ˜ äº†é€šè¿‡å¢åŠ è®¡ç®—èƒ½åŠ›å’Œå†…å­˜å¸¦å®½æ¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„è¶‹åŠ¿ã€‚
- en: Electronic computation began with the mainframe era. ENIAC[1](#fn1) (1945) established
    the viability of electronic computation at scale, while the IBM System/360[2](#fn2)
    (1964) introduced architectural principles of standardized instruction sets and
    memory hierarchies. These basic concepts provided the foundation for all subsequent
    computing systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­è®¡ç®—å§‹äºä¸»æœºæ—¶ä»£ã€‚ENIAC[1](#fn1)ï¼ˆ1945å¹´ï¼‰è¯æ˜äº†å¤§è§„æ¨¡ç”µå­è®¡ç®—çš„å¯è¡Œæ€§ï¼Œè€ŒIBM System/360[2](#fn2)ï¼ˆ1964å¹´ï¼‰å¼•å…¥äº†æ ‡å‡†åŒ–æŒ‡ä»¤é›†å’Œå†…å­˜å±‚æ¬¡ç»“æ„çš„æ¶æ„åŸåˆ™ã€‚è¿™äº›åŸºæœ¬æ¦‚å¿µä¸ºæ‰€æœ‰åç»­è®¡ç®—ç³»ç»Ÿæä¾›äº†åŸºç¡€ã€‚
- en: Building upon these foundational computing principles, high-performance computing
    (HPC) systems ([Thornton 1965](ch058.xhtml#ref-thornton1965cdc)) specialized for
    scientific computation. The CDC 6600[3](#fn3) and later systems like the CM-5[4](#fn4)
    ([T. M. Corporation 1992](ch058.xhtml#ref-thinking_machines_cm5)) optimized for
    dense matrix operations and floating-point calculations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›åŸºç¡€è®¡ç®—åŸåˆ™çš„åŸºç¡€ä¸Šï¼Œé«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ç³»ç»Ÿï¼ˆ[Thornton 1965](ch058.xhtml#ref-thornton1965cdc)ï¼‰ä¸“é—¨ç”¨äºç§‘å­¦è®¡ç®—ã€‚CDC
    6600[3](#fn3)å’Œåæ¥çš„ç³»ç»Ÿå¦‚CM-5[4](#fn4) ([T. M. Corporation 1992](ch058.xhtml#ref-thinking_machines_cm5)ï¼‰é’ˆå¯¹å¯†é›†çŸ©é˜µè¿ç®—å’Œæµ®ç‚¹è®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: 'HPC systems implemented specific architectural features for scientific workloads:
    high-bandwidth memory systems for array operations, vector processing units for
    mathematical computations, and specialized interconnects for collective communication
    patterns. Scientific computing demanded emphasis on numerical precision and stability,
    with processors and memory systems designed for regular, predictable access patterns.
    The interconnects supported tightly synchronized parallel execution, enabling
    efficient collective operations across computing nodes.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: HPCç³»ç»Ÿä¸ºç§‘å­¦å·¥ä½œè´Ÿè½½å®ç°äº†ç‰¹å®šçš„æ¶æ„ç‰¹æ€§ï¼šç”¨äºæ•°ç»„æ“ä½œçš„é«˜å¸¦å®½å†…å­˜ç³»ç»Ÿã€ç”¨äºæ•°å­¦è®¡ç®—çš„å‘é‡å¤„ç†å•å…ƒä»¥åŠç”¨äºé›†ä½“é€šä¿¡æ¨¡å¼çš„ä¸“ç”¨äº’è¿ã€‚ç§‘å­¦è®¡ç®—éœ€è¦å¼ºè°ƒæ•°å€¼ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œå¤„ç†å™¨å’Œå†…å­˜ç³»ç»Ÿè®¾è®¡ç”¨äºå¸¸è§„ã€å¯é¢„æµ‹çš„è®¿é—®æ¨¡å¼ã€‚äº’è¿æ”¯æŒç´§å¯†åŒæ­¥çš„å¹¶è¡Œæ‰§è¡Œï¼Œä½¿è®¡ç®—èŠ‚ç‚¹ä¹‹é—´çš„é›†ä½“æ“ä½œæ•ˆç‡æ›´é«˜ã€‚
- en: As the demand for internet-scale processing grew, warehouse-scale computing
    marked the next evolutionary step. Googleâ€™s data center implementations[5](#fn5)
    ([Barroso and HÃ¶lzle 2007](ch058.xhtml#ref-barroso2003web)) introduced new optimizations
    for internet-scale data processing. Unlike HPC systems focused on tightly coupled
    scientific calculations, warehouse computing handled loosely coupled tasks with
    irregular data access patterns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€å¯¹äº’è”ç½‘è§„æ¨¡å¤„ç†éœ€æ±‚çš„å¢é•¿ï¼Œä»“åº“è§„æ¨¡è®¡ç®—æ ‡å¿—ç€ä¸‹ä¸€ä¸ªè¿›åŒ–æ­¥éª¤ã€‚è°·æ­Œæ•°æ®ä¸­å¿ƒå®ç°[5](#fn5) ([Barroso and HÃ¶lzle 2007](ch058.xhtml#ref-barroso2003web)ï¼‰ä¸ºäº’è”ç½‘è§„æ¨¡æ•°æ®å¤„ç†å¼•å…¥äº†æ–°çš„ä¼˜åŒ–ã€‚ä¸ä¸“æ³¨äºç´§å¯†è€¦åˆç§‘å­¦è®¡ç®—çš„é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿä¸åŒï¼Œä»“åº“è®¡ç®—å¤„ç†æ¾æ•£è€¦åˆçš„ä»»åŠ¡ï¼Œå…·æœ‰ä¸è§„åˆ™çš„æ•°æ®è®¿é—®æ¨¡å¼ã€‚
- en: WSC systems introduced architectural changes to support high throughput for
    independent tasks, with robust fault tolerance and recovery mechanisms. The storage
    and memory systems adapted to handle sparse data structures efficiently, moving
    away from the dense array optimizations of HPC. Resource management systems evolved
    to support multiple applications sharing the computing infrastructure, contrasting
    with HPCâ€™s dedicated application execution model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: WSCç³»ç»Ÿå¼•å…¥äº†æ¶æ„å˜åŒ–ï¼Œä»¥æ”¯æŒç‹¬ç«‹ä»»åŠ¡çš„é«˜ååé‡ï¼Œå…·æœ‰å¼ºå¤§çš„å®¹é”™å’Œæ¢å¤æœºåˆ¶ã€‚å­˜å‚¨å’Œå†…å­˜ç³»ç»Ÿé€‚åº”äº†é«˜æ•ˆå¤„ç†ç¨€ç–æ•°æ®ç»“æ„ï¼Œè¿œç¦»äº†HPCçš„å¯†é›†æ•°ç»„ä¼˜åŒ–ã€‚èµ„æºç®¡ç†ç³»ç»Ÿæ¼”å˜ä¸ºæ”¯æŒå¤šä¸ªåº”ç”¨ç¨‹åºå…±äº«è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œä¸HPCçš„ä¸“ç”¨åº”ç”¨ç¨‹åºæ‰§è¡Œæ¨¡å‹å½¢æˆå¯¹æ¯”ã€‚
- en: 'Neither HPC nor warehouse-scale systems fully addressed the unique demands
    of machine learning training. Each computing era optimized for distinct workload
    characteristics that only partially matched AI training requirements:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ€§èƒ½è®¡ç®—å’Œä»“åº“è§„æ¨¡ç³»ç»Ÿéƒ½æ²¡æœ‰å®Œå…¨æ»¡è¶³æœºå™¨å­¦ä¹ è®­ç»ƒçš„ç‹¬ç‰¹éœ€æ±‚ã€‚æ¯ä¸ªè®¡ç®—æ—¶ä»£éƒ½é’ˆå¯¹ä¸åŒçš„å·¥ä½œè´Ÿè½½ç‰¹å¾è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿™äº›ç‰¹å¾ä»…éƒ¨åˆ†ç¬¦åˆAIè®­ç»ƒéœ€æ±‚ï¼š
- en: '**High-Performance Computing**: Optimized for dense, floating-point heavy,
    tightly-coupled simulations. HPC established the foundation for high-bandwidth
    interconnects and parallel numerical computation essential for AI training, but
    focused on regular, predictable access patterns unsuited to the dynamic memory
    requirements of neural network training.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜æ€§èƒ½è®¡ç®—**ï¼šé’ˆå¯¹å¯†é›†ã€æµ®ç‚¹è¿ç®—é‡å¤§ã€ç´§å¯†è€¦åˆçš„æ¨¡æ‹Ÿè¿›è¡Œäº†ä¼˜åŒ–ã€‚é«˜æ€§èƒ½è®¡ç®—ä¸ºAIè®­ç»ƒæ‰€éœ€çš„é«˜å¸¦å®½äº’è¿å’Œå¹¶è¡Œæ•°å€¼è®¡ç®—å¥ å®šäº†åŸºç¡€ï¼Œä½†ä¸“æ³¨äºå¸¸è§„ã€å¯é¢„æµ‹çš„è®¿é—®æ¨¡å¼ï¼Œä¸é€‚åˆç¥ç»ç½‘ç»œè®­ç»ƒçš„åŠ¨æ€å†…å­˜éœ€æ±‚ã€‚'
- en: '**Warehouse-Scale Computing**: Optimized for sparse, integer-heavy, loosely-coupled
    data processing. WSC demonstrated fault tolerance and massive scale essential
    for production AI systems, but emphasized independent parallel tasks that contrasted
    with the synchronized gradient updates required in distributed training.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»“åº“è§„æ¨¡è®¡ç®—**ï¼šé’ˆå¯¹ç¨€ç–ã€æ•´æ•°å¯†é›†å‹ã€æ¾æ•£è€¦åˆçš„æ•°æ®å¤„ç†è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä»“åº“è§„æ¨¡è®¡ç®—å±•ç¤ºäº†åœ¨ç”Ÿäº§äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å¿…è¦çš„å®¹é”™æ€§å’Œå¤§è§„æ¨¡ï¼Œä½†å¼ºè°ƒäº†ç‹¬ç«‹å¹¶è¡Œä»»åŠ¡ï¼Œè¿™ä¸åˆ†å¸ƒå¼è®­ç»ƒä¸­æ‰€éœ€çš„åŒæ­¥æ¢¯åº¦æ›´æ–°å½¢æˆå¯¹æ¯”ã€‚'
- en: '**AI Training**: Presents the unique challenge of requiring **both** dense
    FP16/FP32 computation (like HPC) **and** massive data scale (like WSC), while
    adding the complexity of iterative, synchronized gradient updates. This unique
    combination of requirementsâ€”intensive parameter updates, complex memory access
    patterns, and coordinated distributed computationâ€”drove the development of todayâ€™s
    specialized AI hypercomputing systems.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI è®­ç»ƒ**ï¼šæå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œéœ€è¦**åŒæ—¶**è¿›è¡Œå¯†é›†çš„ FP16/FP32 è®¡ç®—ï¼ˆå¦‚é«˜æ€§èƒ½è®¡ç®—ï¼‰å’Œå¤§è§„æ¨¡æ•°æ®é‡ï¼ˆå¦‚ä»“åº“è§„æ¨¡è®¡ç®—ï¼‰ï¼ŒåŒæ—¶è¿˜è¦å¤„ç†è¿­ä»£ã€åŒæ­¥çš„æ¢¯åº¦æ›´æ–°å¸¦æ¥çš„å¤æ‚æ€§ã€‚è¿™ç§ç‹¬ç‰¹çš„éœ€æ±‚ç»„åˆâ€”â€”å¯†é›†çš„å‚æ•°æ›´æ–°ã€å¤æ‚çš„å†…å­˜è®¿é—®æ¨¡å¼ä»¥åŠåè°ƒçš„åˆ†å¸ƒå¼è®¡ç®—â€”â€”æ¨åŠ¨äº†ä»Šå¤©ä¸“é—¨åŒ–äººå·¥æ™ºèƒ½è¶…è®¡ç®—ç³»ç»Ÿçš„å‘å±•ã€‚'
- en: AlexNetâ€™s[6](#fn6) ([Krizhevsky, Sutskever, and Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet))
    success in 2012 demonstrated that existing systems could not efficiently handle
    this convergence of requirements. Neural network training demanded new approaches
    to memory management and inter-device communication that neither HPCâ€™s tightly-coupled
    scientific focus nor warehouse computingâ€™s loosely-coupled data processing had
    addressed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2012 å¹´ AlexNet[6](#fn6) ([Krizhevsky, Sutskever, and Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet))
    çš„æˆåŠŸè¯æ˜äº†ç°æœ‰ç³»ç»Ÿæ— æ³•æœ‰æ•ˆåœ°å¤„ç†è¿™ç§éœ€æ±‚çš„èåˆã€‚ç¥ç»ç½‘ç»œè®­ç»ƒéœ€è¦æ–°çš„å†…å­˜ç®¡ç†å’Œè®¾å¤‡é—´é€šä¿¡æ–¹æ³•ï¼Œè€Œé«˜æ€§èƒ½è®¡ç®—çš„ç§‘å­¦é‡ç‚¹ç´§å¯†è€¦åˆå’Œä»“åº“è®¡ç®—çš„æ•°æ®å¤„ç†æ¾æ•£è€¦åˆéƒ½æ²¡æœ‰è§£å†³è¿™äº›é—®é¢˜ã€‚
- en: This need for specialization ushered in the AI hypercomputing era, beginning
    in 2015, which represents the latest step in this evolutionary chain. NVIDIA GPUs[7](#fn7)
    and Google TPUs[8](#fn8) introduced hardware designs specifically optimized for
    neural network computations, moving beyond adaptations of existing architectures.
    These systems implemented new approaches to parallel processing, memory access,
    and device communication to handle the distinct patterns of model training. The
    resulting architectures balanced the numerical precision needs of scientific computing
    with the scale requirements of warehouse systems, while adding specialized support
    for the iterative nature of neural network optimization. The comprehensive design
    principles, architectural details, and optimization strategies for these specialized
    training accelerators are explored in detail in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration),
    while this chapter focuses on training system orchestration and pipeline optimization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¸“ä¸šåŒ–çš„éœ€æ±‚å¼•é¢†äº†äººå·¥æ™ºèƒ½è¶…è®¡ç®—æ—¶ä»£çš„åˆ°æ¥ï¼Œå§‹äº 2015 å¹´ï¼Œè¿™æ˜¯è¿™ä¸€è¿›åŒ–é“¾çš„æœ€æ–°ä¸€æ­¥ã€‚NVIDIA GPU[7](#fn7) å’Œ Google
    TPU[8](#fn8) å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹ç¥ç»ç½‘ç»œè®¡ç®—è¿›è¡Œä¼˜åŒ–çš„ç¡¬ä»¶è®¾è®¡ï¼Œè¶…è¶Šäº†ç°æœ‰æ¶æ„çš„é€‚åº”æ€§ã€‚è¿™äº›ç³»ç»Ÿå®ç°äº†æ–°çš„å¹¶è¡Œå¤„ç†ã€å†…å­˜è®¿é—®å’Œè®¾å¤‡é€šä¿¡æ–¹æ³•ï¼Œä»¥å¤„ç†æ¨¡å‹è®­ç»ƒçš„ç‹¬ç‰¹æ¨¡å¼ã€‚è¿™äº›æ¶æ„åœ¨æ»¡è¶³ç§‘å­¦è®¡ç®—çš„æ•°å€¼ç²¾åº¦éœ€æ±‚çš„åŒæ—¶ï¼Œä¹Ÿæ»¡è¶³äº†ä»“åº“ç³»ç»Ÿçš„è§„æ¨¡è¦æ±‚ï¼Œå¹¶å¢åŠ äº†å¯¹ç¥ç»ç½‘ç»œä¼˜åŒ–è¿­ä»£ç‰¹æ€§çš„ä¸“é—¨æ”¯æŒã€‚è¿™äº›ä¸“ç”¨è®­ç»ƒåŠ é€Ÿå™¨çš„å…¨é¢è®¾è®¡åŸåˆ™ã€æ¶æ„ç»†èŠ‚å’Œä¼˜åŒ–ç­–ç•¥åœ¨[ç¬¬
    11 ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­è¿›è¡Œäº†è¯¦ç»†æ¢è®¨ï¼Œè€Œæœ¬ç« åˆ™ä¸“æ³¨äºè®­ç»ƒç³»ç»Ÿç¼–æ’å’Œç®¡é“ä¼˜åŒ–ã€‚
- en: This architectural progression illuminates why traditional computing systems
    proved insufficient for neural network training. As shown in [TableÂ 8.1](ch014.xhtml#tbl-computing-eras),
    while HPC systems provided the foundation for parallel numerical computation and
    warehouse-scale systems demonstrated distributed processing at scale, neither
    fully addressed the computational patterns of model training. Modern neural networks
    combine intensive parameter updates, complex memory access patterns, and coordinated
    distributed computation in ways that demanded new architectural approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¶æ„çš„æ¼”è¿›è¯´æ˜äº†ä¸ºä»€ä¹ˆä¼ ç»Ÿçš„è®¡ç®—ç³»ç»Ÿåœ¨ç¥ç»ç½‘ç»œè®­ç»ƒæ–¹é¢è¯æ˜æ˜¯ä¸å¤Ÿçš„ã€‚å¦‚[è¡¨ 8.1](ch014.xhtml#tbl-computing-eras)æ‰€ç¤ºï¼Œè™½ç„¶é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿä¸ºå¹¶è¡Œæ•°å€¼è®¡ç®—æä¾›äº†åŸºç¡€ï¼Œè€Œä»“åº“è§„æ¨¡ç³»ç»Ÿå±•ç¤ºäº†å¤§è§„æ¨¡çš„åˆ†å¸ƒå¼å¤„ç†ï¼Œä½†å®ƒä»¬éƒ½æ²¡æœ‰å®Œå…¨è§£å†³æ¨¡å‹è®­ç»ƒçš„è®¡ç®—æ¨¡å¼ã€‚ç°ä»£ç¥ç»ç½‘ç»œä»¥å¯†é›†çš„å‚æ•°æ›´æ–°ã€å¤æ‚çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œåè°ƒçš„åˆ†å¸ƒå¼è®¡ç®—ç›¸ç»“åˆçš„æ–¹å¼ï¼Œè¦æ±‚æ–°çš„æ¶æ„æ–¹æ³•ã€‚
- en: Understanding these distinct characteristics and their evolution from previous
    computing eras explains why modern AI training systems require dedicated hardware
    features and optimized system designs. This historical context provides the foundation
    for examining machine learning training system architectures in detail.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£è¿™äº›ç‹¬ç‰¹çš„ç‰¹å¾åŠå…¶ä»å…ˆå‰è®¡ç®—æ—¶ä»£çš„æ¼”å˜ï¼Œå¯ä»¥è§£é‡Šä¸ºä»€ä¹ˆç°ä»£AIè®­ç»ƒç³»ç»Ÿéœ€è¦ä¸“é—¨çš„ç¡¬ä»¶ç‰¹æ€§å’Œä¼˜åŒ–çš„ç³»ç»Ÿè®¾è®¡ã€‚è¿™ä¸€å†å²èƒŒæ™¯ä¸ºè¯¦ç»†æ£€æŸ¥æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿæ¶æ„æä¾›äº†åŸºç¡€ã€‚
- en: 'TableÂ 8.1: **Computing Era Evolution**: System architectures progressively
    adapted to meet the demands of evolving workloads, transitioning from general-purpose
    computation to specialized designs optimized for neural network training. High-performance
    computing (HPC) established parallel processing foundations, while warehouse-scale
    systems enabled distributed computation; however, modern neural networks require
    architectures that balance intensive parameter updates, complex memory access,
    and coordinated distributed computation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.1ï¼š**è®¡ç®—æ—¶ä»£æ¼”å˜**ï¼šç³»ç»Ÿæ¶æ„é€æ¸é€‚åº”ä¸æ–­å˜åŒ–çš„å·¥ä½œè´Ÿè½½éœ€æ±‚ï¼Œä»é€šç”¨è®¡ç®—è¿‡æ¸¡åˆ°é’ˆå¯¹ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ–çš„ä¸“ç”¨è®¾è®¡ã€‚é«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰ç¡®ç«‹äº†å¹¶è¡Œå¤„ç†åŸºç¡€ï¼Œè€Œä»“åº“è§„æ¨¡ç³»ç»Ÿå®ç°äº†åˆ†å¸ƒå¼è®¡ç®—ï¼›ç„¶è€Œï¼Œç°ä»£ç¥ç»ç½‘ç»œéœ€è¦å¹³è¡¡å¯†é›†å‚æ•°æ›´æ–°ã€å¤æ‚å†…å­˜è®¿é—®å’Œåè°ƒåˆ†å¸ƒå¼è®¡ç®—çš„æ¶æ„ã€‚
- en: '| **Era** | **Primary Workload** | **Memory Patterns** | **Processing Model**
    | **System Focus** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **æ—¶ä»£** | **ä¸»è¦å·¥ä½œè´Ÿè½½** | **å†…å­˜æ¨¡å¼** | **å¤„ç†æ¨¡å‹** | **ç³»ç»Ÿé‡ç‚¹** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Mainframe** | Sequential batch processing | Simple memory hierarchy | Single
    instruction stream | General-purpose computation |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸»æœº** | é¡ºåºæ‰¹å¤„ç† | ç®€å•çš„å†…å­˜å±‚æ¬¡ç»“æ„ | å•æŒ‡ä»¤æµ | é€šç”¨è®¡ç®— |'
- en: '| **HPC** | Scientific simulation | Regular array access | Synchronized parallel
    | Numerical precision, collective operations |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **HPC** | ç§‘å­¦æ¨¡æ‹Ÿ | è§„åˆ™æ•°ç»„è®¿é—® | åŒæ­¥å¹¶è¡Œ | æ•°å€¼ç²¾åº¦ï¼Œé›†ä½“æ“ä½œ |'
- en: '| **Warehouse-scale** | Internet services | Sparse, irregular access | Independent
    parallel tasks | Throughput, fault tolerance |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **ä»“åº“è§„æ¨¡** | äº’è”ç½‘æœåŠ¡ | ç¨€ç–ï¼Œä¸è§„åˆ™è®¿é—® | ç‹¬ç«‹å¹¶è¡Œä»»åŠ¡ | ååé‡ï¼Œå®¹é”™æ€§ |'
- en: '| **AI Hypercomputing** | Neural network training | Parameter-heavy, mixed
    access | Hybrid parallel, distributed | Training optimization, model scale |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **AIè¶…è®¡ç®—** | ç¥ç»ç½‘ç»œè®­ç»ƒ | å‚æ•°å¯†é›†ï¼Œæ··åˆè®¿é—® | æ··åˆå¹¶è¡Œï¼Œåˆ†å¸ƒå¼ | è®­ç»ƒä¼˜åŒ–ï¼Œæ¨¡å‹è§„æ¨¡ |'
- en: Training Systems in the ML Development Lifecycle
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„è®­ç»ƒç³»ç»Ÿ
- en: Training systems function through specialized computational frameworks. The
    development of modern machine learning models relies on specialized systems for
    training and optimization. These systems combine hardware and software components
    that must efficiently handle massive datasets while maintaining numerical precision
    and computational stability. Training systems share common characteristics and
    requirements that distinguish them from traditional computing infrastructures,
    despite their rapid evolution and diverse implementations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿé€šè¿‡ä¸“é—¨çš„è®¡ç®—æ¡†æ¶è¿è¡Œã€‚ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ä¾èµ–äºç”¨äºè®­ç»ƒå’Œä¼˜åŒ–çš„ä¸“ç”¨ç³»ç»Ÿã€‚è¿™äº›ç³»ç»Ÿç»“åˆäº†ç¡¬ä»¶å’Œè½¯ä»¶ç»„ä»¶ï¼Œå¿…é¡»é«˜æ•ˆåœ°å¤„ç†å¤§é‡æ•°æ®é›†ï¼ŒåŒæ—¶ä¿æŒæ•°å€¼ç²¾åº¦å’Œè®¡ç®—ç¨³å®šæ€§ã€‚å°½ç®¡è¿™äº›ç³»ç»Ÿå¿«é€Ÿå‘å±•å’Œå®æ–½å¤šæ ·åŒ–ï¼Œä½†è®­ç»ƒç³»ç»Ÿå…·æœ‰å…±åŒçš„ç‰¹å¾å’Œéœ€æ±‚ï¼Œä½¿å…¶ä¸ä¼ ç»Ÿè®¡ç®—åŸºç¡€è®¾æ–½åŒºåˆ†å¼€æ¥ã€‚
- en: These training systems provide the core infrastructure required for developing
    predictive models. They execute the mathematical optimization of model parameters,
    converting input data into computational representations for tasks such as pattern
    recognition, language understanding, and decision automation. The training process
    involves systematic iteration over datasets to minimize error functions and achieve
    optimal model performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®­ç»ƒç³»ç»Ÿæä¾›äº†å¼€å‘é¢„æµ‹æ¨¡å‹æ‰€éœ€çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚å®ƒä»¬æ‰§è¡Œæ¨¡å‹å‚æ•°çš„æ•°å­¦ä¼˜åŒ–ï¼Œå°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºç”¨äºæ¨¡å¼è¯†åˆ«ã€è¯­è¨€ç†è§£å’Œå†³ç­–è‡ªåŠ¨åŒ–ç­‰ä»»åŠ¡çš„è®¡ç®—è¡¨ç¤ºã€‚è®­ç»ƒè¿‡ç¨‹æ¶‰åŠå¯¹æ•°æ®é›†çš„ç³»ç»Ÿè¿­ä»£ï¼Œä»¥æœ€å°åŒ–è¯¯å·®å‡½æ•°å¹¶å®ç°æœ€ä½³æ¨¡å‹æ€§èƒ½ã€‚
- en: Training systems function as integral components within the broader machine
    learning pipeline, building upon the foundational concepts introduced in [ChapterÂ 1](ch007.xhtml#sec-introduction).
    They interface with preprocessing frameworks that standardize and transform raw
    data, while connecting to deployment architectures that enable model serving.
    The computational efficiency and reliability of training systems directly influence
    the development cycle, from initial experimentation through model validation to
    production deployment. This end-to-end perspective connects training optimization
    with the broader AI system lifecycle considerations explored in [ChapterÂ 13](ch019.xhtml#sec-ml-operations).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿä½œä¸ºæ›´å¹¿æ³›æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå»ºç«‹åœ¨[ç¬¬1ç« ](ch007.xhtml#sec-introduction)ä¸­å¼•å…¥çš„åŸºç¡€æ¦‚å¿µä¹‹ä¸Šã€‚å®ƒä»¬ä¸é¢„å¤„ç†æ¡†æ¶æ¥å£ï¼Œè¿™äº›æ¡†æ¶æ ‡å‡†åŒ–å¹¶è½¬æ¢åŸå§‹æ•°æ®ï¼ŒåŒæ—¶è¿æ¥åˆ°éƒ¨ç½²æ¶æ„ï¼Œä»¥å®ç°æ¨¡å‹æœåŠ¡ã€‚è®­ç»ƒç³»ç»Ÿçš„è®¡ç®—æ•ˆç‡å’Œå¯é æ€§ç›´æ¥å½±å“ä»åˆå§‹å®éªŒåˆ°æ¨¡å‹éªŒè¯å†åˆ°ç”Ÿäº§éƒ¨ç½²çš„å¼€å‘å‘¨æœŸã€‚è¿™ç§ç«¯åˆ°ç«¯è§†è§’å°†è®­ç»ƒä¼˜åŒ–ä¸[ç¬¬13ç« ](ch019.xhtml#sec-ml-operations)ä¸­æ¢è®¨çš„æ›´å¹¿æ³›äººå·¥æ™ºèƒ½ç³»ç»Ÿç”Ÿå‘½å‘¨æœŸè€ƒè™‘å› ç´ è”ç³»èµ·æ¥ã€‚
- en: This operational scope has expanded with recent architectural advances. The
    emergence of transformer architectures[9](#fn9) and large-scale models has introduced
    new requirements for training systems. Current implementations must efficiently
    process petabyte-scale datasets, orchestrate distributed training across multiple
    accelerators, and optimize memory utilization for models containing billions of
    parameters. The management of data parallelism[10](#fn10), model parallelism[11](#fn11),
    and inter-device communication presents technical challenges in modern training
    architectures. These distributed system complexities motivate the specialized
    AI workflow management tools ([ChapterÂ 5](ch011.xhtml#sec-ai-workflow)) that automate
    many aspects of large-scale training orchestration.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æœ€è¿‘æ¶æ„çš„è¿›æ­¥ï¼Œè¿™ç§æ“ä½œèŒƒå›´å·²ç»æ‰©å¤§ã€‚Transformeræ¶æ„[9](#fn9)å’Œå¤§è§„æ¨¡æ¨¡å‹çš„å‡ºç°ä¸ºè®­ç»ƒç³»ç»Ÿå¼•å…¥äº†æ–°çš„è¦æ±‚ã€‚å½“å‰çš„å®ç°å¿…é¡»é«˜æ•ˆåœ°å¤„ç†PBçº§æ•°æ®é›†ï¼Œåè°ƒè·¨å¤šä¸ªåŠ é€Ÿå™¨çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ä¼˜åŒ–åŒ…å«æ•°åäº¿å‚æ•°çš„æ¨¡å‹çš„å†…å­˜åˆ©ç”¨ç‡ã€‚æ•°æ®å¹¶è¡Œ[10](#fn10)ã€æ¨¡å‹å¹¶è¡Œ[11](#fn11)å’Œè·¨è®¾å¤‡é€šä¿¡çš„ç®¡ç†åœ¨ç°ä»£è®­ç»ƒæ¶æ„ä¸­æå‡ºäº†æŠ€æœ¯æŒ‘æˆ˜ã€‚è¿™äº›åˆ†å¸ƒå¼ç³»ç»Ÿå¤æ‚æ€§ä¿ƒä½¿ä¸“é—¨çš„AIå·¥ä½œæµç¨‹ç®¡ç†å·¥å…·([ç¬¬5ç« ](ch011.xhtml#sec-ai-workflow))è‡ªåŠ¨åŒ–å¤§è§„æ¨¡è®­ç»ƒç¼–æ’çš„è®¸å¤šæ–¹é¢ã€‚
- en: 'Training systems also impact the operational considerations of machine learning
    development. System design must address multiple technical constraints: computational
    throughput, energy consumption, hardware compatibility, and scalability with increasing
    model complexity. While this chapter focuses on the computational and architectural
    aspects of training systems, energy efficiency and sustainability considerations
    are explored in [ChapterÂ 18](ch024.xhtml#sec-sustainable-ai). These factors determine
    the technical feasibility and operational viability of machine learning implementations
    across different scales and applications.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿè¿˜å¯¹æœºå™¨å­¦ä¹ å¼€å‘çš„æ“ä½œè€ƒè™‘äº§ç”Ÿå½±å“ã€‚ç³»ç»Ÿè®¾è®¡å¿…é¡»è§£å†³å¤šä¸ªæŠ€æœ¯çº¦æŸï¼šè®¡ç®—ååé‡ã€èƒ½è€—ã€ç¡¬ä»¶å…¼å®¹æ€§å’Œéšç€æ¨¡å‹å¤æ‚æ€§çš„å¢åŠ çš„å¯æ‰©å±•æ€§ã€‚è™½ç„¶æœ¬ç« é‡ç‚¹ä»‹ç»è®­ç»ƒç³»ç»Ÿçš„è®¡ç®—å’Œæ¶æ„æ–¹é¢ï¼Œä½†èƒ½æºæ•ˆç‡å’Œå¯æŒç»­æ€§è€ƒè™‘åœ¨[ç¬¬18ç« ](ch024.xhtml#sec-sustainable-ai)ä¸­è¿›è¡Œäº†æ¢è®¨ã€‚è¿™äº›å› ç´ å†³å®šäº†æœºå™¨å­¦ä¹ å®ç°åœ¨ä¸åŒè§„æ¨¡å’Œåº”ç”¨ä¸­çš„æŠ€æœ¯å¯è¡Œæ€§å’Œæ“ä½œå¯è¡Œæ€§ã€‚
- en: System Design Principles for Training Infrastructure
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®­ç»ƒåŸºç¡€è®¾æ–½çš„ç³»ç»Ÿè®¾è®¡åŸåˆ™
- en: Training implementation requires a systems perspective. The practical execution
    of training models is deeply tied to system design. Training is not merely a mathematical
    optimization problem; it is a system-driven process that requires careful orchestration
    of computing hardware, memory, and data movement.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®ç°éœ€è¦ç³»ç»Ÿè§†è§’ã€‚è®­ç»ƒæ¨¡å‹çš„å®é™…æ‰§è¡Œä¸ç³»ç»Ÿè®¾è®¡ç´§å¯†ç›¸è¿ã€‚è®­ç»ƒä¸ä»…ä»…æ˜¯æ•°å­¦ä¼˜åŒ–é—®é¢˜ï¼›å®ƒæ˜¯ä¸€ä¸ªç³»ç»Ÿé©±åŠ¨çš„æµç¨‹ï¼Œéœ€è¦ä»”ç»†ç¼–æ’è®¡ç®—ç¡¬ä»¶ã€å†…å­˜å’Œæ•°æ®ç§»åŠ¨ã€‚
- en: 'Training workflows consist of interdependent stages: data preprocessing, forward
    and backward passes, and parameter updates, extending the basic neural network
    concepts from [ChapterÂ 3](ch009.xhtml#sec-dl-primer). Each stage imposes specific
    demands on system resources. The data preprocessing stage, for instance, relies
    on storage and I/O subsystems to provide computing hardware with continuous input.
    The quality and reliability of this input data are criticalâ€”data validation, corruption
    detection, feature engineering, schema enforcement, and pipeline reliability strategies
    are covered in [ChapterÂ 6](ch012.xhtml#sec-data-engineering). While [ChapterÂ 6](ch012.xhtml#sec-data-engineering)
    focuses on ensuring data quality and consistency, this chapter examines the systems-level
    efficiency of data movement, transformation throughput, and delivery to computational
    resources during training.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå·¥ä½œæµç¨‹ç”±ç›¸äº’ä¾èµ–çš„é˜¶æ®µç»„æˆï¼šæ•°æ®é¢„å¤„ç†ã€æ­£å‘å’Œåå‘ä¼ é€’ä»¥åŠå‚æ•°æ›´æ–°ï¼Œè¿™äº›é˜¶æ®µæ‰©å±•äº†æ¥è‡ª[ç¬¬3ç« ](ch009.xhtml#sec-dl-primer)çš„åŸºæœ¬ç¥ç»ç½‘ç»œæ¦‚å¿µã€‚æ¯ä¸ªé˜¶æ®µéƒ½å¯¹ç³»ç»Ÿèµ„æºæå‡ºç‰¹å®šçš„è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œæ•°æ®é¢„å¤„ç†é˜¶æ®µä¾èµ–äºå­˜å‚¨å’ŒI/Oå­ç³»ç»Ÿä¸ºè®¡ç®—ç¡¬ä»¶æä¾›è¿ç»­è¾“å…¥ã€‚è¾“å…¥æ•°æ®çš„è´¨é‡å’Œå¯é æ€§è‡³å…³é‡è¦â€”â€”æ•°æ®éªŒè¯ã€æŸåæ£€æµ‹ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å¼å¼ºåˆ¶å’Œç®¡é“å¯é æ€§ç­–ç•¥åœ¨[ç¬¬6ç« ](ch012.xhtml#sec-data-engineering)ä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚è™½ç„¶[ç¬¬6ç« ](ch012.xhtml#sec-data-engineering)ä¾§é‡äºç¡®ä¿æ•°æ®è´¨é‡å’Œä¸€è‡´æ€§ï¼Œä½†æœ¬ç« æ¢è®¨äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•°æ®ç§»åŠ¨ã€è½¬æ¢ååé‡å’Œäº¤ä»˜ç»™è®¡ç®—èµ„æºçš„ç³»ç»Ÿçº§æ•ˆç‡ã€‚
- en: While traditional processors like CPUs handle many training tasks effectively,
    increasingly complex models have driven the adoption of hardware accelerators.
    Graphics Processing Units (GPUs) and specialized machine learning processors can
    process mathematical operations in parallel, offering substantial speedups for
    matrix-heavy computations. These accelerators, alongside CPUs, handle operations
    like gradient computation and parameter updates, enabling the training of hierarchical
    representations whose theoretical foundations are explored in [ChapterÂ 4](ch010.xhtml#sec-dnn-architectures).
    The performance of these stages depends on how well the system manages bottlenecks
    such as memory bandwidth and communication latency.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¼ ç»Ÿçš„å¤„ç†å™¨å¦‚CPUèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è®¸å¤šè®­ç»ƒä»»åŠ¡ï¼Œä½†æ—¥ç›Šå¤æ‚çš„æ¨¡å‹æ¨åŠ¨äº†ç¡¬ä»¶åŠ é€Ÿå™¨çš„é‡‡ç”¨ã€‚å›¾å½¢å¤„ç†å•å…ƒ(GPUs)å’Œä¸“é—¨çš„æœºå™¨å­¦ä¹ å¤„ç†å™¨å¯ä»¥å¹¶è¡Œå¤„ç†æ•°å­¦è¿ç®—ï¼Œä¸ºçŸ©é˜µå¯†é›†å‹è®¡ç®—æä¾›æ˜¾è‘—åŠ é€Ÿã€‚è¿™äº›åŠ é€Ÿå™¨ä¸CPUä¸€èµ·å¤„ç†æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°ç­‰æ“ä½œï¼Œä½¿å¾—èƒ½å¤Ÿè®­ç»ƒå…·æœ‰ç†è®ºåŸºç¡€çš„åˆ†å±‚è¡¨ç¤ºï¼Œè¿™äº›ç†è®ºåŸºç¡€åœ¨[ç¬¬4ç« ](ch010.xhtml#sec-dnn-architectures)ä¸­è¿›è¡Œäº†æ¢è®¨ã€‚è¿™äº›é˜¶æ®µçš„æ€§èƒ½å–å†³äºç³»ç»Ÿå¦‚ä½•ç®¡ç†å†…å­˜å¸¦å®½å’Œé€šä¿¡å»¶è¿Ÿç­‰ç“¶é¢ˆã€‚
- en: These interconnected workflow stages reveal how system architecture directly
    impacts training efficiency. System constraints often dictate the performance
    limits of training workloads. Modern accelerators are frequently bottlenecked
    by memory bandwidth, as data movement between memory hierarchies can be slower
    and more energy-intensive than the computations themselves ([David A. Patterson
    and Hennessy 2021a](ch058.xhtml#ref-patterson2021hardware)). In distributed setups,
    synchronization across devices introduces additional latency, with the performance
    of interconnects (e.g., NVLink, InfiniBand) playing an important role.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç›¸äº’å…³è”çš„å·¥ä½œæµç¨‹é˜¶æ®µæ­ç¤ºäº†ç³»ç»Ÿæ¶æ„å¦‚ä½•ç›´æ¥å½±å“è®­ç»ƒæ•ˆç‡ã€‚ç³»ç»Ÿé™åˆ¶é€šå¸¸å†³å®šäº†è®­ç»ƒå·¥ä½œè´Ÿè½½çš„æ€§èƒ½é™åˆ¶ã€‚ç°ä»£åŠ é€Ÿå™¨é€šå¸¸å—å†…å­˜å¸¦å®½çš„é™åˆ¶ï¼Œå› ä¸ºæ•°æ®åœ¨å†…å­˜å±‚æ¬¡ç»“æ„ä¹‹é—´çš„ç§»åŠ¨å¯èƒ½æ¯”è®¡ç®—æœ¬èº«æ…¢ä¸”èƒ½è€—æ›´é«˜([David
    A. Patterson and Hennessy 2021a](ch058.xhtml#ref-patterson2021hardware))ã€‚åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œè®¾å¤‡é—´çš„åŒæ­¥å¼•å…¥äº†é¢å¤–çš„å»¶è¿Ÿï¼Œè€Œäº’è¿çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒNVLinkã€InfiniBandï¼‰åœ¨å…¶ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚
- en: Optimizing training workflows overcomes these limitations through systematic
    approaches detailed in [SectionÂ 8.5.1](ch014.xhtml#sec-ai-training-systematic-optimization-framework-9f23).
    Techniques like overlapping computation with data loading, mixed-precision training
    ([Micikevicius et al. 2017](ch058.xhtml#ref-micikevicius2017mixed)), and efficient
    memory allocation address the three primary bottlenecks that constrain training
    performance. These low-level optimizations complement the higher-level model compression
    strategies covered in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations), creating
    an integrated approach to training efficiency.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡[ç¬¬8.5.1èŠ‚](ch014.xhtml#sec-ai-training-systematic-optimization-framework-9f23)ä¸­è¯¦ç»†è¯´æ˜çš„ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œä¼˜åŒ–è®­ç»ƒå·¥ä½œæµç¨‹å…‹æœäº†è¿™äº›é™åˆ¶ã€‚åƒé‡å è®¡ç®—ä¸æ•°æ®åŠ è½½ã€æ··åˆç²¾åº¦è®­ç»ƒ([Micikevicius
    et al. 2017](ch058.xhtml#ref-micikevicius2017mixed))å’Œé«˜æ•ˆå†…å­˜åˆ†é…ç­‰æŠ€æœ¯è§£å†³äº†é™åˆ¶è®­ç»ƒæ€§èƒ½çš„ä¸‰ä¸ªä¸»è¦ç“¶é¢ˆã€‚è¿™äº›ä½çº§ä¼˜åŒ–è¡¥å……äº†[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)ä¸­æ¶µç›–çš„é«˜çº§æ¨¡å‹å‹ç¼©ç­–ç•¥ï¼Œå½¢æˆäº†ä¸€ç§ç»¼åˆçš„è®­ç»ƒæ•ˆç‡æ–¹æ³•ã€‚
- en: Systems thinking extends beyond infrastructure optimization to design decisions.
    System-level constraints often guide the development of new model architectures
    and training approaches. The hardware-software co-design principles discussed
    in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration) demonstrate how understanding
    system capabilities can inspire entirely new architectural innovations. For example,
    memory limitations have motivated research into more efficient neural network
    architectures ([Vaswani et al. 2017](ch058.xhtml#ref-vaswani2017attention)), while
    communication overhead in distributed systems has influenced the design of optimization
    algorithms. These adaptations demonstrate how practical system considerations
    shape the evolution of machine learning approaches within given computational
    bounds.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿæ€ç»´ä¸ä»…è¶…è¶Šäº†åŸºç¡€è®¾æ–½ä¼˜åŒ–ï¼Œè¿˜æ‰©å±•åˆ°äº†è®¾è®¡å†³ç­–ã€‚ç³»ç»Ÿçº§çº¦æŸé€šå¸¸æŒ‡å¯¼æ–°çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•çš„å¼€å‘ã€‚åœ¨ç¬¬11ç« ä¸­è®¨è®ºçš„ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡åŸåˆ™[ChapterÂ 11](ch017.xhtml#sec-ai-acceleration)å±•ç¤ºäº†ç†è§£ç³»ç»Ÿèƒ½åŠ›å¦‚ä½•æ¿€å‘å…¨æ–°çš„æ¶æ„åˆ›æ–°ã€‚ä¾‹å¦‚ï¼Œå†…å­˜é™åˆ¶æ¨åŠ¨äº†æ›´é«˜æ•ˆç¥ç»ç½‘ç»œæ¶æ„çš„ç ”ç©¶([Vaswani
    et al. 2017](ch058.xhtml#ref-vaswani2017attention))ï¼Œè€Œåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„é€šä¿¡å¼€é”€å½±å“äº†ä¼˜åŒ–ç®—æ³•çš„è®¾è®¡ã€‚è¿™äº›è°ƒæ•´å±•ç¤ºäº†å¦‚ä½•åœ¨ç»™å®šçš„è®¡ç®—èŒƒå›´å†…ï¼Œå®é™…ç³»ç»Ÿè€ƒè™‘å¦‚ä½•å¡‘é€ æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ¼”å˜ã€‚
- en: For example, training large Transformer models[12](#fn12) requires partitioning
    data and model parameters across multiple devices. This introduces synchronization
    challenges, particularly during gradient updates. Communication libraries such
    as [NVIDIAâ€™s Collective Communications Library (NCCL)](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html)
    enable efficient gradient sharing, providing the foundation for distributed training
    optimization techniques. The benchmarking methodologies in [ChapterÂ 12](ch018.xhtml#sec-benchmarking-ai)
    provide systematic approaches for evaluating these distributed training performance
    characteristics. These examples illustrate how system-level considerations influence
    the feasibility and efficiency of modern training workflows.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè®­ç»ƒå¤§å‹Transformeræ¨¡å‹[12](#fn12)éœ€è¦åœ¨å¤šä¸ªè®¾å¤‡ä¸Šåˆ’åˆ†æ•°æ®å’Œæ¨¡å‹å‚æ•°ã€‚è¿™å¼•å…¥äº†åŒæ­¥æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¢¯åº¦æ›´æ–°æœŸé—´ã€‚å¦‚[NVIDIAçš„é›†ä½“é€šä¿¡åº“ï¼ˆNCCLï¼‰](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html)ä¹‹ç±»çš„é€šä¿¡åº“èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„æ¢¯åº¦å…±äº«ï¼Œä¸ºåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–æŠ€æœ¯æä¾›äº†åŸºç¡€ã€‚ç¬¬12ç« ä¸­è®¨è®ºçš„åŸºå‡†æµ‹è¯•æ–¹æ³•[ChapterÂ 12](ch018.xhtml#sec-benchmarking-ai)æä¾›äº†è¯„ä¼°è¿™äº›åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½ç‰¹æ€§çš„ç³»ç»Ÿæ–¹æ³•ã€‚è¿™äº›ä¾‹å­è¯´æ˜äº†ç³»ç»Ÿçº§è€ƒè™‘å¦‚ä½•å½±å“ç°ä»£è®­ç»ƒå·¥ä½œæµç¨‹çš„å¯è¡Œæ€§å’Œæ•ˆç‡ã€‚
- en: Mathematical Foundations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°å­¦åŸºç¡€
- en: The systems perspective established above reveals why understanding the mathematical
    operations at the heart of training is essential. These operations are not abstract
    concepts but concrete computations that dictate every aspect of training system
    design. The computational characteristics of neural network mathematics directly
    determine hardware requirements, memory architectures, and parallelization constraints.
    When system architects choose GPUs over CPUs, design memory hierarchies, or select
    distributed training strategies, they are responding to the specific demands of
    these mathematical operations.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å»ºç«‹çš„ç³»ç»Ÿè§†è§’æ­ç¤ºäº†ä¸ºä»€ä¹ˆç†è§£è®­ç»ƒæ ¸å¿ƒçš„æ•°å­¦æ“ä½œè‡³å…³é‡è¦ã€‚è¿™äº›æ“ä½œä¸æ˜¯æŠ½è±¡çš„æ¦‚å¿µï¼Œè€Œæ˜¯å…·ä½“çš„è®¡ç®—ï¼Œå®ƒä»¬å†³å®šäº†è®­ç»ƒç³»ç»Ÿè®¾è®¡çš„å„ä¸ªæ–¹é¢ã€‚ç¥ç»ç½‘ç»œæ•°å­¦çš„è®¡ç®—ç‰¹æ€§ç›´æ¥å†³å®šäº†ç¡¬ä»¶éœ€æ±‚ã€å†…å­˜æ¶æ„å’Œå¹¶è¡ŒåŒ–çº¦æŸã€‚å½“ç³»ç»Ÿæ¶æ„å¸ˆé€‰æ‹©GPUè€Œä¸æ˜¯CPUã€è®¾è®¡å†…å­˜å±‚æ¬¡ç»“æ„æˆ–é€‰æ‹©åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥æ—¶ï¼Œä»–ä»¬æ˜¯åœ¨å›åº”è¿™äº›æ•°å­¦æ“ä½œçš„å…·ä½“éœ€æ±‚ã€‚
- en: 'The specialized training systems discussed above are designed specifically
    to execute these operations efficiently. Understanding these mathematical foundations
    is essential because they directly determine system requirements: the type of
    operations dictates hardware specialization needs (why matrix multiplication units
    dominate modern accelerators), the memory access patterns influence cache design
    (why activation storage becomes a bottleneck), and the computational dependencies
    shape parallelization strategies (why some operations cannot be trivially distributed).
    When we discussed how AI hypercomputing differs from HPC systems earlier, the
    distinction emerges from differences in the mathematical operations each must
    perform.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è®¨è®ºçš„ä¸“ä¸šåŒ–è®­ç»ƒç³»ç»Ÿæ˜¯ä¸“é—¨è®¾è®¡æ¥é«˜æ•ˆæ‰§è¡Œè¿™äº›æ“ä½œçš„ã€‚ç†è§£è¿™äº›æ•°å­¦åŸºç¡€æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºå®ƒä»¬ç›´æ¥å†³å®šäº†ç³»ç»Ÿéœ€æ±‚ï¼šæ“ä½œç±»å‹å†³å®šäº†ç¡¬ä»¶ä¸“ä¸šåŒ–çš„éœ€æ±‚ï¼ˆä¸ºä»€ä¹ˆçŸ©é˜µä¹˜æ³•å•å…ƒåœ¨ç°ä»£åŠ é€Ÿå™¨ä¸­å ä¸»å¯¼åœ°ä½ï¼‰ï¼Œå†…å­˜è®¿é—®æ¨¡å¼å½±å“äº†ç¼“å­˜è®¾è®¡ï¼ˆä¸ºä»€ä¹ˆæ¿€æ´»å­˜å‚¨æˆä¸ºç“¶é¢ˆï¼‰ï¼Œè®¡ç®—ä¾èµ–æ€§å¡‘é€ äº†å¹¶è¡ŒåŒ–ç­–ç•¥ï¼ˆä¸ºä»€ä¹ˆæŸäº›æ“ä½œä¸èƒ½ç®€å•åœ°åˆ†å¸ƒå¼ï¼‰ã€‚å½“æˆ‘ä»¬ä¹‹å‰è®¨è®ºäººå·¥æ™ºèƒ½è¶…è®¡ç®—ä¸é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿä¸åŒæ—¶ï¼Œè¿™ç§åŒºåˆ«æºäºæ¯ä¸ªç³»ç»Ÿå¿…é¡»æ‰§è¡Œçš„æ•°å­¦æ“ä½œçš„ä¸åŒã€‚
- en: Training systems must execute three categories of operations repeatedly. First,
    forward propagation computes predictions through matrix multiplications and activation
    functions. Second, gradient computation via backpropagation calculates parameter
    updates using stored activations and the chain rule. Third, parameter updates
    apply gradients using optimization algorithms that maintain momentum and adaptive
    learning rate state. Each category exhibits distinct computational patterns and
    system requirements that training architectures must accommodate.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿå¿…é¡»åå¤æ‰§è¡Œä¸‰ç±»æ“ä½œã€‚é¦–å…ˆï¼Œå‰å‘ä¼ æ’­é€šè¿‡çŸ©é˜µä¹˜æ³•å’Œæ¿€æ´»å‡½æ•°è®¡ç®—é¢„æµ‹ã€‚å…¶æ¬¡ï¼Œé€šè¿‡åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨å­˜å‚¨çš„æ¿€æ´»å’Œé“¾å¼æ³•åˆ™æ¥è®¡ç®—å‚æ•°æ›´æ–°ã€‚ç¬¬ä¸‰ï¼Œå‚æ•°æ›´æ–°ä½¿ç”¨ä¿æŒåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡çŠ¶æ€çš„ä¼˜åŒ–ç®—æ³•åº”ç”¨æ¢¯åº¦ã€‚æ¯ä¸€ç±»éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„è®¡ç®—æ¨¡å¼å’Œç³»ç»Ÿè¦æ±‚ï¼Œè®­ç»ƒæ¶æ„å¿…é¡»é€‚åº”ã€‚
- en: The computational characteristics of these operations directly inform the system
    design decisions discussed previously. Matrix multiplications dominate forward
    and backward passes, accounting for 60-90% of training time ([K. He et al. 2016](ch058.xhtml#ref-he2016residual)),
    which explains why specialized matrix units (GPU tensor cores, TPU systolic arrays)
    became central to training hardware. This computational dominance shapes modern
    training architectures, from hardware design choices to software optimization
    strategies. Activation storage for gradient computation creates memory pressure
    proportional to batch size and network depth, motivating the memory hierarchies
    and optimization techniques like gradient checkpointing we will explore. The iterative
    dependencies between forward passes, gradient computations, and parameter updates
    prevent arbitrary parallelization, constraining the distributed training strategies
    available for scaling. Understanding these mathematical operations and their system-level
    implications provides the foundation for understanding how modern training systems
    achieve efficiency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ“ä½œçš„è®¡ç®—ç‰¹æ€§ç›´æ¥å½±å“äº†ä¹‹å‰è®¨è®ºçš„ç³»ç»Ÿè®¾è®¡å†³ç­–ã€‚çŸ©é˜µä¹˜æ³•ä¸»å¯¼äº†å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå ç”¨äº†60-90%çš„è®­ç»ƒæ—¶é—´([K. He et al. 2016](ch058.xhtml#ref-he2016residual))ï¼Œè¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆä¸“é—¨çš„çŸ©é˜µå•å…ƒï¼ˆGPUå¼ é‡æ ¸å¿ƒã€TPUæ”¶ç¼©é˜µåˆ—ï¼‰æˆä¸ºäº†è®­ç»ƒç¡¬ä»¶çš„æ ¸å¿ƒã€‚è¿™ç§è®¡ç®—ä¸»å¯¼æ€§å¡‘é€ äº†ç°ä»£è®­ç»ƒæ¶æ„ï¼Œä»ç¡¬ä»¶è®¾è®¡é€‰æ‹©åˆ°è½¯ä»¶ä¼˜åŒ–ç­–ç•¥ã€‚ç”¨äºæ¢¯åº¦è®¡ç®—çš„æ¿€æ´»å­˜å‚¨äº§ç”Ÿäº†ä¸æ‰¹é‡å¤§å°å’Œç½‘ç»œæ·±åº¦æˆæ¯”ä¾‹çš„å†…å­˜å‹åŠ›ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬æ¢ç´¢å†…å­˜å±‚æ¬¡ç»“æ„å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚å‰å‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°ä¹‹é—´çš„è¿­ä»£ä¾èµ–å…³ç³»é˜»æ­¢äº†ä»»æ„å¹¶è¡ŒåŒ–ï¼Œé™åˆ¶äº†å¯æ‰©å±•çš„åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ã€‚ç†è§£è¿™äº›æ•°å­¦æ“ä½œåŠå…¶ç³»ç»Ÿçº§å½±å“æ˜¯ç†è§£ç°ä»£è®­ç»ƒç³»ç»Ÿå¦‚ä½•å®ç°æ•ˆç‡çš„åŸºç¡€ã€‚
- en: Neural Network Computation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œè®¡ç®—
- en: Neural network training consists of repeated matrix operations and nonlinear
    transformations. These operations, while conceptually simple, create the system-level
    challenges that dominate modern training infrastructure. Foundational works by
    Rumelhart, Hinton, and Williams ([1986](ch058.xhtml#ref-rumelhart1986learning))
    through the introduction of backpropagation and the development of efficient matrix
    computation libraries, e.g., BLAS ([Dongarra et al. 1988](ch058.xhtml#ref-dongarra1988extended)),
    laid the groundwork for modern training architectures.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œè®­ç»ƒåŒ…æ‹¬é‡å¤çš„çŸ©é˜µæ“ä½œå’Œéçº¿æ€§å˜æ¢ã€‚è™½ç„¶è¿™äº›æ“ä½œåœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†å®ƒä»¬åˆ›é€ äº†ç³»ç»Ÿçº§æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜ä¸»å¯¼äº†ç°ä»£è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚é€šè¿‡å¼•å…¥åå‘ä¼ æ’­å’Œå¼€å‘é«˜æ•ˆçš„çŸ©é˜µè®¡ç®—åº“ï¼Œä¾‹å¦‚BLAS([Dongarra
    et al. 1988](ch058.xhtml#ref-dongarra1988extended))ï¼ŒRumelhartã€Hintonå’ŒWilliams([1986](ch058.xhtml#ref-rumelhart1986learning))çš„åŸºç¡€æ€§å·¥ä½œä¸ºç°ä»£è®­ç»ƒæ¶æ„å¥ å®šäº†åŸºç¡€ã€‚
- en: Mathematical Operations in Neural Networks
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„æ•°å­¦æ“ä½œ
- en: 'At the heart of a neural network is the process of forward propagation, which
    in its simplest case involves two primary operations: matrix multiplication and
    the application of an activation function. Matrix multiplication forms the basis
    of the linear transformation in each layer of the network. This equation represents
    how information flows through each layer of a neural network:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯å‰å‘ä¼ æ’­çš„è¿‡ç¨‹ï¼Œåœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œå®ƒæ¶‰åŠä¸¤ä¸ªä¸»è¦æ“ä½œï¼šçŸ©é˜µä¹˜æ³•å’Œæ¿€æ´»å‡½æ•°çš„åº”ç”¨ã€‚çŸ©é˜µä¹˜æ³•æ„æˆäº†ç½‘ç»œæ¯ä¸€å±‚çš„çº¿æ€§å˜æ¢çš„åŸºç¡€ã€‚è¿™ä¸ªæ–¹ç¨‹ä»£è¡¨äº†ä¿¡æ¯å¦‚ä½•é€šè¿‡ç¥ç»ç½‘ç»œæ¯ä¸€å±‚æµåŠ¨ï¼š
- en: 'At layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    the computation can be described as: <semantics><mrow><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>W</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A^{(l)}
    = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)</annotation></semantics> Where:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚ï¼Œè®¡ç®—å¯ä»¥æè¿°ä¸ºï¼š<semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>W</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A^{(l)}
    = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)</annotation></semantics> å…¶ä¸­ï¼š
- en: <semantics><msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(l-1)}</annotation></semantics>
    represents the activations from the previous layer (or the input layer for the
    first layer),
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(l-1)}</annotation></semantics>
    è¡¨ç¤ºå‰ä¸€å±‚ï¼ˆæˆ–ç¬¬ä¸€å±‚çš„è¾“å…¥å±‚ï¼‰çš„æ¿€æ´»å€¼ï¼Œ
- en: <semantics><msup><mi>W</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">W^{(l)}</annotation></semantics>
    is the weight matrix at layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    which contains the parameters learned by the network,
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msup><mi>W</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">W^{(l)}</annotation></semantics>
    æ˜¯ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„æƒé‡çŸ©é˜µï¼Œå…¶ä¸­åŒ…å«ç½‘ç»œå­¦ä¹ åˆ°çš„å‚æ•°ï¼Œ
- en: <semantics><msup><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">b^{(l)}</annotation></semantics>
    is the bias vector for layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msup><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">b^{(l)}</annotation></semantics>
    æ˜¯ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„åç½®å‘é‡ï¼Œ
- en: <semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics>
    is the activation function applied element-wise (e.g., ReLU, sigmoid) to introduce
    non-linearity.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics>
    æ˜¯é€å…ƒç´ åº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚ ReLUã€sigmoidï¼‰ä»¥å¼•å…¥éçº¿æ€§ã€‚
- en: Matrix Operations
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: çŸ©é˜µè¿ç®—
- en: Understanding how these mathematical operations translate to system requirements
    requires examining the computational patterns in neural networks, which revolve
    around various types of matrix operations. Understanding these operations and
    their evolution reveals the reasons why specific system designs and optimizations
    emerged in machine learning training systems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£è¿™äº›æ•°å­¦è¿ç®—å¦‚ä½•è½¬åŒ–ä¸ºç³»ç»Ÿéœ€æ±‚éœ€è¦æ£€æŸ¥ç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å›´ç»•å„ç§ç±»å‹çš„çŸ©é˜µè¿ç®—å±•å¼€ã€‚ç†è§£è¿™äº›è¿ç®—åŠå…¶æ¼”å˜æ­ç¤ºäº†ä¸ºä»€ä¹ˆåœ¨æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿä¸­å‡ºç°äº†ç‰¹å®šçš„ç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–ã€‚
- en: Dense Matrix-Matrix Multiplication
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç¨ å¯†çŸ©é˜µ-çŸ©é˜µä¹˜æ³•
- en: Building on the matrix multiplication dominance established above, the evolution
    of these computational patterns has driven both algorithmic and hardware innovations.
    Early neural network implementations relied on standard CPU-based linear algebra
    libraries, but the scale of modern training demanded specialized optimizations.
    From Strassenâ€™s algorithm[13](#fn13), which reduced the naive <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics>
    complexity to approximately <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msup><mi>n</mi><mn>2.81</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(n^{2.81})</annotation></semantics> ([Strassen 1969](ch058.xhtml#ref-strassen1969gauss)),
    to contemporary hardware-accelerated libraries like [cuBLAS](https://developer.nvidia.com/cublas),
    these innovations have continually pushed the limits of computational efficiency.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¸Šè¿°å»ºç«‹çš„çŸ©é˜µä¹˜æ³•ä¸»å¯¼åœ°ä½ï¼Œè¿™äº›è®¡ç®—æ¨¡å¼çš„æ¼”å˜æ¨åŠ¨äº†ç®—æ³•å’Œç¡¬ä»¶çš„åˆ›æ–°ã€‚æ—©æœŸçš„ç¥ç»ç½‘ç»œå®ç°ä¾èµ–äºåŸºäºæ ‡å‡†CPUçš„çº¿æ€§ä»£æ•°åº“ï¼Œä½†ç°ä»£è®­ç»ƒçš„è§„æ¨¡éœ€è¦ä¸“é—¨çš„ä¼˜åŒ–ã€‚ä»æ–¯ç‰¹æ‹‰æ–¯æ©ç®—æ³•[13](#fn13)ï¼Œå®ƒå°†åŸå§‹çš„<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics>å¤æ‚åº¦é™ä½åˆ°å¤§çº¦<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>2.81</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^{2.81})</annotation></semantics>
    ([Strassen 1969](ch058.xhtml#ref-strassen1969gauss))ï¼Œåˆ°å½“ä»£çš„ç¡¬ä»¶åŠ é€Ÿåº“å¦‚[cuBLAS](https://developer.nvidia.com/cublas)ï¼Œè¿™äº›åˆ›æ–°ä¸æ–­æ¨åŠ¨è®¡ç®—æ•ˆç‡çš„æé™ã€‚
- en: This computational dominance has driven system-level optimizations. Modern systems
    implement blocked matrix computations for parallel processing across multiple
    units. As neural architectures grew in scale, these multiplications began to demand
    significant memory resources, since weight matrices and activation matrices must
    both remain accessible for the backward pass during training. Hardware designs
    adapted to optimize for these dense multiplication patterns while managing growing
    memory requirements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¡ç®—ä¸»å¯¼åœ°ä½æ¨åŠ¨äº†ç³»ç»Ÿçº§ä¼˜åŒ–ã€‚ç°ä»£ç³»ç»Ÿé€šè¿‡å¤šä¸ªå•å…ƒçš„å¹¶è¡Œå¤„ç†å®ç°åˆ†å—çŸ©é˜µè®¡ç®—ã€‚éšç€ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œè¿™äº›ä¹˜æ³•å¼€å§‹éœ€è¦å¤§é‡çš„å†…å­˜èµ„æºï¼Œå› ä¸ºæƒé‡çŸ©é˜µå’Œæ¿€æ´»çŸ©é˜µéƒ½å¿…é¡»åœ¨è®­ç»ƒæœŸé—´çš„é€†ä¼ æ’­è¿‡ç¨‹ä¸­ä¿æŒå¯è®¿é—®ã€‚ç¡¬ä»¶è®¾è®¡é€‚åº”äº†ä¼˜åŒ–è¿™äº›å¯†é›†ä¹˜æ³•æ¨¡å¼ï¼ŒåŒæ—¶ç®¡ç†ä¸æ–­å¢é•¿çš„å†…å­˜éœ€æ±‚ã€‚
- en: '**GPT-2 Attention Layer Computation**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 æ³¨æ„åŠ›å±‚è®¡ç®—**'
- en: 'Each GPT-2 layer performs attention computations that exemplify dense matrix
    multiplication demands. For a single attention head with batch_size=32, sequence_length=1024,
    hidden_dim=1280:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPT-2å±‚æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™äº›è®¡ç®—ä½“ç°äº†å¯†é›†çŸ©é˜µä¹˜æ³•çš„éœ€æ±‚ã€‚å¯¹äºä¸€ä¸ªbatch_size=32ï¼Œsequence_length=1024ï¼Œhidden_dim=1280çš„å•ä¸ªæ³¨æ„åŠ›å¤´ï¼š
- en: '**Query, Key, Value Projections** (3 separate matrix multiplications): <semantics><mrow><mtext
    mathvariant="normal">FLOPS</mtext><mo>=</mo><mn>3</mn><mo>Ã—</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mtext mathvariant="normal">batch</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext mathvariant="normal">hidden</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">hidden</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\text{FLOPS} = 3 \times (\text{batch}
    \times \text{seq} \times \text{hidden} \times \text{hidden})</annotation></semantics>
    <semantics><mrow><mo>=</mo><mn>3</mn><mo>Ã—</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>32</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>1280</mn><mo>Ã—</mo><mn>1280</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>161</mn> <mrow><mtext
    mathvariant="normal">billion FLOPS</mtext></mrow></mrow> <annotation encoding="application/x-tex">=
    3 \times (32 \times 1024 \times 1280 \times 1280) = 161 \text{ billion FLOPS}</annotation></semantics>'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±**ï¼ˆ3æ¬¡ç‹¬ç«‹çš„çŸ©é˜µä¹˜æ³•ï¼‰ï¼š<semantics><mrow><mtext mathvariant="normal">FLOPS</mtext><mo>=</mo><mn>3</mn><mo>Ã—</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">batch</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext mathvariant="normal">hidden</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">hidden</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\text{FLOPS} = 3 \times (\text{batch}
    \times \text{seq} \times \text{hidden} \times \text{hidden})</annotation></semantics>
    <semantics><mrow><mo>=</mo><mn>3</mn><mo>Ã—</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>32</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>1280</mn><mo>Ã—</mo><mn>1280</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>161</mn> <mrow><mtext
    mathvariant="normal">billion FLOPS</mtext></mrow></mrow> <annotation encoding="application/x-tex">=
    3 \times (32 \times 1024 \times 1280 \times 1280) = 161 \text{ billion FLOPS}</annotation></semantics>'
- en: '**Attention Score Computation** (Q Ã— K^T): <semantics><mrow><mtext mathvariant="normal">FLOPS</mtext><mo>=</mo><mtext
    mathvariant="normal">batch</mtext><mo>Ã—</mo><mtext mathvariant="normal">heads</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">hidden/heads</mtext></mrow> <annotation encoding="application/x-tex">\text{FLOPS}
    = \text{batch} \times \text{heads} \times \text{seq} \times \text{seq} \times
    \text{hidden/heads}</annotation></semantics> <semantics><mrow><mo>=</mo><mn>32</mn><mo>Ã—</mo><mn>20</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>64</mn><mo>=</mo><mn>42.9</mn>
    <mrow><mtext mathvariant="normal">billion FLOPS</mtext></mrow></mrow> <annotation
    encoding="application/x-tex">= 32 \times 20 \times 1024 \times 1024 \times 64
    = 42.9 \text{ billion FLOPS}</annotation></semantics>'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—**ï¼ˆQ Ã— K^Tï¼‰ï¼š<semantics><mrow><mtext mathvariant="normal">FLOPS</mtext><mo>=</mo><mtext
    mathvariant="normal">batch</mtext><mo>Ã—</mo><mtext mathvariant="normal">heads</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext mathvariant="normal">seq</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">hidden/heads</mtext></mrow> <annotation encoding="application/x-tex">\text{FLOPS}
    = \text{batch} \times \text{heads} \times \text{seq} \times \text{seq} \times
    \text{hidden/heads}</annotation></semantics> <semantics><mrow><mo>=</mo><mn>32</mn><mo>Ã—</mo><mn>20</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>1024</mn><mo>Ã—</mo><mn>64</mn><mo>=</mo><mn>42.9</mn>
    <mrow><mtext mathvariant="normal">billion FLOPS</mtext></mrow></mrow> <annotation
    encoding="application/x-tex">= 32 \times 20 \times 1024 \times 1024 \times 64
    = 42.9 \text{ billion FLOPS}</annotation></semantics>'
- en: '**Computation Scale**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¡ç®—è§„æ¨¡**'
- en: 'Total for one attention layer: ~204B FLOPS forward pass'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä¸ªæ³¨æ„åŠ›å±‚çš„æ€»é‡ï¼š~204B FLOPSæ­£å‘ä¼ é€’
- en: 'With 48 layers in GPT-2: ~9.8 trillion FLOPS per training step'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨GPT-2çš„48å±‚ä¸­ï¼šæ¯è®­ç»ƒæ­¥éª¤~9.8ä¸‡äº¿æ¬¡FLOPS
- en: 'At 50K training steps: ~490 petaFLOPS total training computation'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨50Kä¸ªè®­ç»ƒæ­¥éª¤åï¼š~490 petaFLOPSæ€»è®­ç»ƒè®¡ç®—
- en: '**System Implication:** A V100 GPU (125 TFLOPS peak FP16 with Tensor Cores,
    28 TFLOPS without) would require 79 seconds just for the attention computations
    per step at 100% utilization. Actual training steps take 180 to 220ms, requiring
    8 to 32 GPUs to achieve this throughput.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿå½±å“ï¼šV100 GPUçš„å½±å“**ï¼šä¸€ä¸ªV100 GPUï¼ˆå³°å€¼FP16ä¸º125 TFLOPSï¼Œå¸¦æœ‰Tensor Coresæ—¶ä¸º28 TFLOPSï¼Œä¸å¸¦æ—¶ä¸º28
    TFLOPSï¼‰åœ¨100%åˆ©ç”¨ç‡ä¸‹ï¼Œä»…æ³¨æ„åŠ›è®¡ç®—æ¯æ­¥å°±éœ€è¦79ç§’ã€‚å®é™…çš„è®­ç»ƒæ­¥éª¤éœ€è¦180åˆ°220æ¯«ç§’ï¼Œéœ€è¦8åˆ°32ä¸ªGPUæ‰èƒ½è¾¾åˆ°è¿™ç§ååé‡ã€‚'
- en: Matrix-Vector Operations
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: çŸ©é˜µ-å‘é‡æ“ä½œ
- en: Beyond matrix-matrix operations, matrix-vector multiplication became essential
    with the introduction of normalization techniques in neural architectures. Although
    computationally simpler than matrix-matrix multiplication, these operations present
    system challenges. They exhibit lower hardware utilization due to their limited
    parallelization potential. This characteristic influences hardware design and
    model architecture decisions, particularly in networks processing sequential inputs
    or computing layer statistics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çŸ©é˜µ-çŸ©é˜µæ“ä½œä¹‹å¤–ï¼Œéšç€ç¥ç»ç½‘ç»œæ¶æ„ä¸­å½’ä¸€åŒ–æŠ€æœ¯çš„å¼•å…¥ï¼ŒçŸ©é˜µ-å‘é‡ä¹˜æ³•å˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡åœ¨è®¡ç®—ä¸Šæ¯”çŸ©é˜µ-çŸ©é˜µä¹˜æ³•ç®€å•ï¼Œä½†è¿™äº›æ“ä½œåœ¨ç³»ç»Ÿä¸Šæå‡ºäº†æŒ‘æˆ˜ã€‚ç”±äºå®ƒä»¬çš„å¹¶è¡ŒåŒ–æ½œåŠ›æœ‰é™ï¼Œå®ƒä»¬è¡¨ç°å‡ºè¾ƒä½çš„ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚è¿™ä¸€ç‰¹æ€§å½±å“äº†ç¡¬ä»¶è®¾è®¡å’Œæ¨¡å‹æ¶æ„å†³ç­–ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é¡ºåºè¾“å…¥æˆ–è®¡ç®—å±‚ç»Ÿè®¡çš„ç½‘ç»œä¸­ã€‚
- en: Batched Operations
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ‰¹é‡æ“ä½œ
- en: Recognizing the limitations of matrix-vector operations, the introduction of
    batching[14](#fn14) transformed matrix computation in neural networks. By processing
    multiple inputs simultaneously, training systems convert matrix-vector operations
    into more efficient matrix-matrix operations. This approach improves hardware
    utilization but increases memory demands for storing intermediate results. Modern
    implementations must balance batch sizes against available memory, leading to
    specific optimizations in memory management and computation scheduling.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è®¤è¯†åˆ°çŸ©é˜µ-å‘é‡æ“ä½œçš„å±€é™æ€§ï¼Œæ‰¹æ¬¡çš„å¼•å…¥[14](#fn14)æ”¹å˜äº†ç¥ç»ç½‘ç»œä¸­çš„çŸ©é˜µè®¡ç®—ã€‚é€šè¿‡åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥ï¼Œè®­ç»ƒç³»ç»Ÿå°†çŸ©é˜µ-å‘é‡æ“ä½œè½¬æ¢ä¸ºæ›´é«˜æ•ˆçš„çŸ©é˜µ-çŸ©é˜µæ“ä½œã€‚è¿™ç§æ–¹æ³•æé«˜äº†ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œä½†å¢åŠ äº†å­˜å‚¨ä¸­é—´ç»“æœçš„å†…å­˜éœ€æ±‚ã€‚ç°ä»£å®ç°å¿…é¡»åœ¨æ‰¹å¤„ç†å¤§å°å’Œå¯ç”¨å†…å­˜ä¹‹é—´è¿›è¡Œå¹³è¡¡ï¼Œå¯¼è‡´å†…å­˜ç®¡ç†å’Œè®¡ç®—è°ƒåº¦æ–¹é¢çš„ç‰¹å®šä¼˜åŒ–ã€‚
- en: Hardware accelerators like Googleâ€™s TPU ([Norman P. Jouppi et al. 2017b](ch058.xhtml#ref-jouppi2017tpu))
    reflect this evolution, incorporating specialized matrix units and memory hierarchies
    for these diverse multiplication patterns. These hardware adaptations enable training
    of large-scale models like GPT-3 ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language))
    through efficient handling of varied matrix operations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºè°·æ­ŒTPUï¼ˆ[Norman P. Jouppiç­‰äºº 2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰çš„ç¡¬ä»¶åŠ é€Ÿå™¨åæ˜ äº†è¿™ç§æ¼”å˜ï¼Œå®ƒä»¬ç»“åˆäº†ä¸“é—¨åŒ–çš„çŸ©é˜µå•å…ƒå’Œç”¨äºè¿™äº›å¤šæ ·åŒ–ä¹˜æ³•æ¨¡å¼çš„å†…å­˜å±‚æ¬¡ç»“æ„ã€‚è¿™äº›ç¡¬ä»¶é€‚åº”æ€§ä½¿å¾—é€šè¿‡é«˜æ•ˆå¤„ç†å„ç§çŸ©é˜µæ“ä½œæ¥è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹å¦‚GPT-3ï¼ˆ[T.
    Brownç­‰äºº 2020](ch058.xhtml#ref-brown2020language)ï¼‰æˆä¸ºå¯èƒ½ã€‚
- en: '**Systems Implication: Why GPUs Dominate Training**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿå½±å“ï¼šä¸ºä»€ä¹ˆGPUåœ¨è®­ç»ƒä¸­å ä¸»å¯¼åœ°ä½**'
- en: 'The matrix operations described above directly explain modern training hardware
    architecture. GPUs dominate training because:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šæ–‡æ‰€è¿°çš„çŸ©é˜µè¿ç®—ç›´æ¥è§£é‡Šäº†ç°ä»£è®­ç»ƒç¡¬ä»¶æ¶æ„ã€‚GPUä¸»å¯¼è®­ç»ƒçš„åŸå› æ˜¯ï¼š
- en: '**Massive parallelism**: Matrix multiplicationâ€™s independent element calculations
    map perfectly to GPUâ€™s thousands of cores (NVIDIA A100: 6,912 CUDA cores)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤§è§„æ¨¡å¹¶è¡Œæ€§**ï¼šçŸ©é˜µä¹˜æ³•çš„ç‹¬ç«‹å…ƒç´ è®¡ç®—ä¸GPUçš„æ•°åƒä¸ªæ ¸å¿ƒï¼ˆNVIDIA A100ï¼š6,912 CUDAæ ¸å¿ƒï¼‰å®Œç¾æ˜ å°„'
- en: '**Specialized hardware units**: Tensor Cores accelerate matrix operations by
    10-20Ã— through dedicated hardware for the dominant workload'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“ç”¨ç¡¬ä»¶å•å…ƒ**ï¼šTensoræ ¸å¿ƒé€šè¿‡ä¸ºå ä¸»å¯¼åœ°ä½çš„å·¥ä½œè´Ÿè½½æä¾›ä¸“ç”¨ç¡¬ä»¶ï¼Œå°†çŸ©é˜µè¿ç®—åŠ é€Ÿ10-20å€'
- en: '**Memory bandwidth optimization**: Blocked matrix computation patterns enable
    efficient use of GPU memory hierarchy (L1/L2 cache â†’ shared memory â†’ global memory)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å†…å­˜å¸¦å®½ä¼˜åŒ–**ï¼šåˆ†å—çŸ©é˜µè®¡ç®—æ¨¡å¼èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨GPUå†…å­˜å±‚æ¬¡ç»“æ„ï¼ˆL1/L2ç¼“å­˜â†’å…±äº«å†…å­˜â†’å…¨å±€å†…å­˜ï¼‰'
- en: When GPT-2 examples later show why V100 GPUs achieve 2.4Ã— speedup with mixed
    precision (line 2018), this acceleration comes from Tensor Cores executing the
    matrix multiplications we just analyzed. Understanding matrix operation characteristics
    is prerequisite for appreciating why pipeline optimizations like mixed-precision
    training provide such substantial benefits.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å½“GPT-2çš„ä¾‹å­åæ¥æ˜¾ç¤ºä¸ºä»€ä¹ˆV100 GPUé€šè¿‡æ··åˆç²¾åº¦ï¼ˆç¬¬2018è¡Œï¼‰å®ç°2.4å€çš„é€Ÿåº¦æå‡æ—¶ï¼Œè¿™ç§åŠ é€Ÿæ¥è‡ªäºæ‰§è¡Œæˆ‘ä»¬åˆšåˆšåˆ†æçš„çŸ©é˜µä¹˜æ³•çš„Tensoræ ¸å¿ƒã€‚ç†è§£çŸ©é˜µæ“ä½œç‰¹æ€§æ˜¯æ¬£èµä¸ºä»€ä¹ˆæµæ°´çº¿ä¼˜åŒ–å¦‚æ··åˆç²¾åº¦è®­ç»ƒæä¾›å¦‚æ­¤å·¨å¤§å¥½å¤„çš„å‰æã€‚
- en: Activation Functions
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: 'In [ChapterÂ 3](ch009.xhtml#sec-dl-primer), we established that activation functionsâ€”sigmoid,
    tanh, ReLU, and softmaxâ€”provide the nonlinearity essential for neural networks
    to learn complex patterns. We examined their mathematical properties: sigmoidâ€™s
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>
    bounded output, tanhâ€™s zero-centered <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>
    range, ReLUâ€™s gradient flow advantages, and softmaxâ€™s probability distributions.
    Recall from [FigureÂ 3.11](ch009.xhtml#fig-activation-functions) how each function
    transforms inputs differently, with distinct implications for gradient behavior
    and learning dynamics.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ç¬¬3ç« ](ch009.xhtml#sec-dl-primer)ä¸­ï¼Œæˆ‘ä»¬ç¡®ç«‹äº†æ¿€æ´»å‡½æ•°â€”â€”sigmoidã€tanhã€ReLUå’Œsoftmaxâ€”â€”ä¸ºç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚æ¨¡å¼æä¾›å¿…è¦çš„éçº¿æ€§ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å®ƒä»¬çš„æ•°å­¦ç‰¹æ€§ï¼šsigmoidçš„<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>æœ‰ç•Œè¾“å‡ºï¼Œtanhçš„é›¶ä¸­å¿ƒ<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>èŒƒå›´ï¼ŒReLUçš„æ¢¯åº¦æµä¼˜åŠ¿ï¼Œä»¥åŠsoftmaxçš„æ¦‚ç‡åˆ†å¸ƒã€‚å›æƒ³ä¸€ä¸‹[å›¾3.11](ch009.xhtml#fig-activation-functions)ä¸­æ¯ä¸ªå‡½æ•°å¦‚ä½•ä»¥ä¸åŒçš„æ–¹å¼è½¬æ¢è¾“å…¥ï¼Œå¯¹æ¢¯åº¦è¡Œä¸ºå’Œå­¦ä¹ åŠ¨æ€æœ‰ç‹¬ç‰¹çš„å«ä¹‰ã€‚
- en: While activation functions are applied element-wise and contribute only 5-10%
    of total computation time compared to matrix operations, their implementation
    characteristics significantly impact training system performance. The question
    facing ML systems engineers is not *what* activation functions do mathematicallyâ€”that
    foundation is establishedâ€”but rather *how* to implement them efficiently at scale.
    Why does ReLU train 3Ã— faster than sigmoid on CPUs but show different relative
    performance on GPUs? How do hardware accelerators optimize these operations? What
    memory access patterns do different activation functions create during backpropagation?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¿€æ´»å‡½æ•°æ˜¯é€å…ƒç´ åº”ç”¨çš„ï¼Œä¸çŸ©é˜µè¿ç®—ç›¸æ¯”ï¼Œå®ƒä»¬åªè´¡çŒ®äº†æ€»è®¡ç®—æ—¶é—´çš„5-10%ï¼Œä½†å®ƒä»¬çš„å®ç°ç‰¹æ€§å¯¹è®­ç»ƒç³»ç»Ÿæ€§èƒ½çš„å½±å“å¾ˆå¤§ã€‚é¢å¯¹æœºå™¨å­¦ä¹ ç³»ç»Ÿå·¥ç¨‹å¸ˆçš„é—®é¢˜ä¸æ˜¯æ•°å­¦ä¸Š*ä»€ä¹ˆ*æ¿€æ´»å‡½æ•°â€”â€”è¿™ä¸ªåŸºç¡€å·²ç»ç¡®ç«‹â€”â€”è€Œæ˜¯*å¦‚ä½•*åœ¨è§„æ¨¡ä¸Šé«˜æ•ˆåœ°å®ç°å®ƒä»¬ã€‚ä¸ºä»€ä¹ˆReLUåœ¨CPUä¸Šæ¯”sigmoidå¿«3å€ï¼Œä½†åœ¨GPUä¸Šå´è¡¨ç°å‡ºä¸åŒçš„ç›¸å¯¹æ€§èƒ½ï¼Ÿç¡¬ä»¶åŠ é€Ÿå™¨å¦‚ä½•ä¼˜åŒ–è¿™äº›æ“ä½œï¼Ÿä¸åŒçš„æ¿€æ´»å‡½æ•°åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¼šåˆ›å»ºä»€ä¹ˆæ ·çš„å†…å­˜è®¿é—®æ¨¡å¼ï¼Ÿ
- en: This section examines activation functions from a systems perspective, analyzing
    computational costs, hardware implementation strategies, and performance trade-offs
    that determine real-world training efficiency. Understanding these practical constraints
    enables informed architectural decisions when designing training systems for specific
    hardware environments.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»ç³»ç»Ÿè§’åº¦è€ƒå¯Ÿæ¿€æ´»å‡½æ•°ï¼Œåˆ†æè®¡ç®—æˆæœ¬ã€ç¡¬ä»¶å®ç°ç­–ç•¥å’Œæ€§èƒ½æƒè¡¡ï¼Œè¿™äº›æƒè¡¡å†³å®šäº†å®é™…è®­ç»ƒæ•ˆç‡ã€‚äº†è§£è¿™äº›å®é™…é™åˆ¶ï¼Œæœ‰åŠ©äºåœ¨è®¾è®¡é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ç¯å¢ƒçš„è®­ç»ƒç³»ç»Ÿæ—¶åšå‡ºæ˜æ™ºçš„æ¶æ„å†³ç­–ã€‚
- en: Benchmarking Activation Functions
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•æ¿€æ´»å‡½æ•°
- en: 'Activation functions in neural networks significantly impact both mathematical
    properties and system-level performance. The selection of an activation function
    directly influences training time, model scalability, and hardware efficiency
    through three primary factors: computational cost, gradient behavior, and memory
    usage.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°å¯¹æ•°å­¦ç‰¹æ€§å’Œç³»ç»Ÿçº§æ€§èƒ½éƒ½æœ‰æ˜¾è‘—å½±å“ã€‚æ¿€æ´»å‡½æ•°çš„é€‰æ‹©ç›´æ¥å½±å“äº†è®­ç»ƒæ—¶é—´ã€æ¨¡å‹å¯æ‰©å±•æ€§å’Œç¡¬ä»¶æ•ˆç‡ï¼Œè¿™ä¸»è¦é€šè¿‡ä¸‰ä¸ªä¸»è¦å› ç´ æ¥å®ç°ï¼šè®¡ç®—æˆæœ¬ã€æ¢¯åº¦è¡Œä¸ºå’Œå†…å­˜ä½¿ç”¨ã€‚
- en: Benchmarking common activation functions on an Apple M2 single-threaded CPU
    reveals meaningful performance differences, as illustrated in [FigureÂ 8.2](ch014.xhtml#fig-activation-perf).
    The data demonstrates that Tanh and ReLU execute more efficiently than Sigmoid
    on CPU architectures, making them particularly suitable for real-time applications
    and large-scale systems.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Apple M2å•çº¿ç¨‹CPUä¸Šå¯¹å¸¸è§æ¿€æ´»å‡½æ•°è¿›è¡ŒåŸºå‡†æµ‹è¯•æ­ç¤ºäº†æœ‰æ„ä¹‰çš„æ€§èƒ½å·®å¼‚ï¼Œå¦‚å›¾8.2[å›¾8.2](ch014.xhtml#fig-activation-perf)æ‰€ç¤ºã€‚æ•°æ®æ˜¾ç¤ºï¼ŒTanhå’ŒReLUåœ¨CPUæ¶æ„ä¸Šæ¯”Sigmoidæ‰§è¡Œæ•ˆç‡æ›´é«˜ï¼Œè¿™ä½¿å¾—å®ƒä»¬ç‰¹åˆ«é€‚åˆå®æ—¶åº”ç”¨å’Œå¤§è§„æ¨¡ç³»ç»Ÿã€‚
- en: '![](../media/file109.svg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file109.svg)'
- en: 'FigureÂ 8.2: **Activation Function Performance**: CPU execution time varies
    significantly across common activation functions, with tanh and relu offering
    substantial speed advantages over sigmoid on this architecture. These differences
    impact system-level considerations such as training time and real-time inference
    capabilities, guiding activation function selection for performance-critical applications.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.2ï¼š**æ¿€æ´»å‡½æ•°æ€§èƒ½**ï¼šåœ¨CPUæ‰§è¡Œæ—¶é—´ä¸Šï¼Œå¸¸è§æ¿€æ´»å‡½æ•°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œtanhå’Œreluåœ¨å½“å‰æ¶æ„ä¸Šç›¸å¯¹äºsigmoidæä¾›äº†æ˜¾è‘—çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚è¿™äº›å·®å¼‚å½±å“äº†ç³»ç»Ÿçº§è€ƒè™‘å› ç´ ï¼Œå¦‚è®­ç»ƒæ—¶é—´å’Œå®æ—¶æ¨ç†èƒ½åŠ›ï¼ŒæŒ‡å¯¼äº†æ€§èƒ½å…³é”®åº”ç”¨çš„æ¿€æ´»å‡½æ•°é€‰æ‹©ã€‚
- en: While these benchmark results provide valuable insights, they represent CPU-only
    performance without hardware acceleration. In production environments, modern
    hardware accelerators like GPUs can substantially alter the relative performance
    characteristics of activation functions. System architects must therefore consider
    their specific hardware environment and deployment context when evaluating computational
    efficiency.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›åŸºå‡†æµ‹è¯•ç»“æœæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä½†å®ƒä»¬ä»…ä»£è¡¨äº†æ²¡æœ‰ç¡¬ä»¶åŠ é€Ÿçš„CPUæ€§èƒ½ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç°ä»£ç¡¬ä»¶åŠ é€Ÿå™¨å¦‚GPUå¯ä»¥æ˜¾è‘—æ”¹å˜æ¿€æ´»å‡½æ•°çš„ç›¸å¯¹æ€§èƒ½ç‰¹å¾ã€‚å› æ­¤ï¼Œç³»ç»Ÿæ¶æ„å¸ˆåœ¨è¯„ä¼°è®¡ç®—æ•ˆç‡æ—¶å¿…é¡»è€ƒè™‘å…¶ç‰¹å®šçš„ç¡¬ä»¶ç¯å¢ƒå’Œéƒ¨ç½²ä¸Šä¸‹æ–‡ã€‚
- en: 'Recall from [ChapterÂ 3](ch009.xhtml#sec-dl-primer) that each activation function
    exhibits different gradient behavior, sparsity characteristics, and computational
    complexity. The question now is: how do these mathematical properties translate
    into hardware constraints and system performance? The following subsections examine
    each functionâ€™s implementation characteristics, focusing on software versus hardware
    trade-offs that determine real-world training efficiency:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›é¡¾[ç¬¬3ç« ](ch009.xhtml#sec-dl-primer)ï¼Œæˆ‘ä»¬çŸ¥é“æ¯ä¸ªæ¿€æ´»å‡½æ•°éƒ½è¡¨ç°å‡ºä¸åŒçš„æ¢¯åº¦è¡Œä¸ºã€ç¨€ç–ç‰¹æ€§å’Œè®¡ç®—å¤æ‚åº¦ã€‚ç°åœ¨çš„é—®é¢˜æ˜¯ï¼šè¿™äº›æ•°å­¦å±æ€§å¦‚ä½•è½¬åŒ–ä¸ºç¡¬ä»¶çº¦æŸå’Œç³»ç»Ÿæ€§èƒ½ï¼Ÿä»¥ä¸‹å°èŠ‚å°†æ£€æŸ¥æ¯ä¸ªå‡½æ•°çš„å®ç°ç‰¹æ€§ï¼Œé‡ç‚¹å…³æ³¨è½¯ä»¶ä¸ç¡¬ä»¶ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™äº›æƒè¡¡å†³å®šäº†ç°å®ä¸–ç•Œçš„è®­ç»ƒæ•ˆç‡ï¼š
- en: Sigmoid
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Sigmoidâ€™s smooth <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>
    bounded output makes it useful for probabilistic interpretation, but its vanishing
    gradient problem and non-zero-centered outputs present optimization challenges.
    From a systems perspective, the exponential function computation becomes the critical
    bottleneck. In software, this computation is expensive and inefficient[15](#fn15),
    particularly for deep networks or large datasets where millions of sigmoid evaluations
    occur per forward pass.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoidçš„å¹³æ»‘<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>æœ‰ç•Œè¾“å‡ºä½¿å…¶åœ¨æ¦‚ç‡è§£é‡Šæ–¹é¢å¾ˆæœ‰ç”¨ï¼Œä½†å…¶æ¶ˆå¤±æ¢¯åº¦é—®é¢˜å’Œéé›¶ä¸­å¿ƒè¾“å‡ºå¸¦æ¥äº†ä¼˜åŒ–æŒ‘æˆ˜ã€‚ä»ç³»ç»Ÿè§’åº¦æ¥çœ‹ï¼ŒæŒ‡æ•°å‡½æ•°çš„è®¡ç®—æˆä¸ºå…³é”®ç“¶é¢ˆã€‚åœ¨è½¯ä»¶ä¸­ï¼Œè¿™ç§è®¡ç®—æ—¢æ˜‚è´µåˆä½æ•ˆ[15](#fn15)ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦ç½‘ç»œæˆ–å¤§å‹æ•°æ®é›†ä¸­ï¼Œæ¯æ¬¡å‰å‘ä¼ é€’éƒ½ä¼šå‘ç”Ÿæ•°ç™¾ä¸‡æ¬¡sigmoidè¯„ä¼°ã€‚
- en: These computational challenges are addressed differently in hardware. Modern
    accelerators like GPUs and TPUs typically avoid direct computation of the exponential
    function, instead using lookup tables (LUTs) or piece-wise linear approximations
    to balance accuracy with speed. While these hardware optimizations help, the multiple
    memory lookups and interpolation calculations still make sigmoid more resource-intensive
    than simpler functions like ReLU, even on highly parallel architectures.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¡¬ä»¶ä¸­ï¼Œè¿™äº›è®¡ç®—æŒ‘æˆ˜çš„å¤„ç†æ–¹å¼ä¸åŒã€‚ç°ä»£åŠ é€Ÿå™¨å¦‚ GPU å’Œ TPU é€šå¸¸é¿å…ç›´æ¥è®¡ç®—æŒ‡æ•°å‡½æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨æŸ¥æ‰¾è¡¨ï¼ˆLUTsï¼‰æˆ–åˆ†æ®µçº¿æ€§è¿‘ä¼¼æ¥å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ã€‚å°½ç®¡è¿™äº›ç¡¬ä»¶ä¼˜åŒ–æœ‰æ‰€å¸®åŠ©ï¼Œä½†å¤šæ¬¡å†…å­˜æŸ¥æ‰¾å’Œæ’å€¼è®¡ç®—ä»ç„¶ä½¿å¾—
    sigmoid æ¯”èµ· ReLU ç­‰ç®€å•å‡½æ•°æ›´å ç”¨èµ„æºï¼Œå³ä½¿åœ¨é«˜åº¦å¹¶è¡Œçš„æ¶æ„ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Tanh
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tanh
- en: While tanh improves upon sigmoid with its <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>
    zero-centered outputs, it shares sigmoidâ€™s computational burden. The exponential
    computations required for tanh create similar performance bottlenecks in both
    software and hardware implementations. In software, this computational overhead
    can slow training, particularly when working with large datasets or deep models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ tanh åœ¨å…¶ <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>
    é›¶ä¸­å¿ƒè¾“å‡ºæ–¹é¢ä¼˜äº sigmoidï¼Œä½†å®ƒä¸ sigmoid ä¸€æ ·ï¼Œå­˜åœ¨è®¡ç®—è´Ÿæ‹…ã€‚tanh æ‰€éœ€çš„æŒ‡æ•°è®¡ç®—åœ¨è½¯ä»¶å’Œç¡¬ä»¶å®ç°ä¸­éƒ½é€ æˆäº†ç±»ä¼¼çš„æ€§èƒ½ç“¶é¢ˆã€‚åœ¨è½¯ä»¶ä¸­ï¼Œè¿™ç§è®¡ç®—å¼€é”€å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æˆ–æ·±åº¦æ¨¡å‹æ—¶ã€‚
- en: 'In hardware, tanh uses its mathematical relationship with sigmoid (a scaled
    and shifted version) to optimize implementation. Modern hardware often implements
    tanh using a hybrid approach: lookup tables for common input ranges combined with
    piece-wise approximations for edge cases. This approach helps balance accuracy
    with computational efficiency, though tanh remains more resource-intensive than
    simpler functions. Despite these challenges, tanh remains common in RNNs and LSTMs[16](#fn16)
    where balanced gradients are necessary.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¡¬ä»¶ä¸­ï¼Œtanh åˆ©ç”¨å…¶ä¸ sigmoidï¼ˆç¼©æ”¾å’Œç§»ä½ç‰ˆæœ¬ï¼‰çš„æ•°å­¦å…³ç³»æ¥ä¼˜åŒ–å®ç°ã€‚ç°ä»£ç¡¬ä»¶é€šå¸¸é‡‡ç”¨æ··åˆæ–¹æ³•å®ç° tanhï¼šå¯¹äºå¸¸è§è¾“å…¥èŒƒå›´ä½¿ç”¨æŸ¥æ‰¾è¡¨ï¼Œå¯¹äºè¾¹ç¼˜æƒ…å†µä½¿ç”¨åˆ†æ®µè¿‘ä¼¼ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºå¹³è¡¡ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œå°½ç®¡
    tanh ä»ç„¶æ¯”ç®€å•å‡½æ•°æ›´å ç”¨èµ„æºã€‚å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œtanh ä»ç„¶åœ¨éœ€è¦å¹³è¡¡æ¢¯åº¦çš„ RNN å’Œ LSTM[16](#fn16) ä¸­å¾ˆå¸¸è§ã€‚
- en: ReLU
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ReLU
- en: ReLU represents a shift in activation function design. Its mathematical simplicityâ€”<semantics><mrow><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>â€”avoids
    vanishing gradients and introduces beneficial sparsity, though it can suffer from
    dying neurons. This straightforward form has profound implications for system
    performance. In software, ReLUâ€™s simple thresholding operation results in dramatically
    faster computation compared to sigmoid or tanh, requiring only a single comparison
    rather than exponential calculations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU ä»£è¡¨äº†æ¿€æ´»å‡½æ•°è®¾è®¡çš„è½¬å˜ã€‚å…¶æ•°å­¦ç®€å•æ€§â€”<semantics><mrow><mo>max</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\max(0,x)</annotation></semantics>â€”é¿å…äº†æ¢¯åº¦æ¶ˆå¤±å¹¶å¼•å…¥äº†æœ‰ç›Šçš„ç¨€ç–æ€§ï¼Œå°½ç®¡å®ƒå¯èƒ½é­å—ç¥ç»å…ƒæ­»äº¡çš„é—®é¢˜ã€‚è¿™ç§ç®€å•å½¢å¼å¯¹ç³»ç»Ÿæ€§èƒ½æœ‰æ·±è¿œçš„å½±å“ã€‚åœ¨è½¯ä»¶ä¸­ï¼ŒReLU
    çš„ç®€å•é˜ˆå€¼æ“ä½œä¸ sigmoid æˆ– tanh ç›¸æ¯”ï¼Œè®¡ç®—é€Ÿåº¦å¤§å¤§åŠ å¿«ï¼Œåªéœ€è¦è¿›è¡Œä¸€æ¬¡æ¯”è¾ƒè€Œä¸æ˜¯æŒ‡æ•°è®¡ç®—ã€‚
- en: The hardware implementation of ReLU showcases why it has become the dominant
    activation function in modern neural networks. Its simple <semantics><mrow><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>
    operation requires just a single comparison and conditional set, translating to
    minimal circuit complexity[17](#fn17). Modern GPUs and TPUs can implement ReLU
    using a simple multiplexer that checks the inputâ€™s sign bit, allowing for extremely
    efficient parallel processing. This hardware efficiency, combined with the sparsity
    it introduces, results in both reduced computation time and lower memory bandwidth
    requirements.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUçš„ç¡¬ä»¶å®ç°å±•ç¤ºäº†å®ƒä¸ºä½•æˆä¸ºç°ä»£ç¥ç»ç½‘ç»œä¸­å ä¸»å¯¼åœ°ä½çš„æ¿€æ´»å‡½æ•°ã€‚å…¶ç®€å•çš„<semantics><mrow><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>æ“ä½œåªéœ€è¦ä¸€æ¬¡æ¯”è¾ƒå’Œæ¡ä»¶è®¾ç½®ï¼Œè½¬åŒ–ä¸ºæœ€å°çš„ç”µè·¯å¤æ‚æ€§[17](#fn17)ã€‚ç°ä»£GPUå’ŒTPUå¯ä»¥é€šè¿‡ç®€å•çš„å¤šè·¯å¤ç”¨å™¨å®ç°ReLUï¼Œè¯¥å¤ç”¨å™¨æ£€æŸ¥è¾“å…¥çš„ç¬¦å·ä½ï¼Œä»è€Œå®ç°æå…¶é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†ã€‚è¿™ç§ç¡¬ä»¶æ•ˆç‡ï¼ŒåŠ ä¸Šå®ƒå¼•å…¥çš„ç¨€ç–æ€§ï¼Œå¯¼è‡´è®¡ç®—æ—¶é—´å‡å°‘å’Œå†…å­˜å¸¦å®½éœ€æ±‚é™ä½ã€‚
- en: Softmax
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Softmax
- en: Softmax differs from the element-wise functions above. Rather than processing
    inputs independently, softmax converts logits into probability distributions through
    global normalization, creating unique computational challenges. Its computation
    involves exponentiating each input value and normalizing by their sum, a process
    that becomes increasingly complex with larger output spaces. In software, this
    creates significant computational overhead for tasks like natural language processing,
    where vocabulary sizes can reach hundreds of thousands of terms. The function
    also requires keeping all values in memory during computation, as each output
    probability depends on the entire input vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Softmaxä¸ä¸Šè¿°é€å…ƒç´ å‡½æ•°ä¸åŒã€‚å®ƒä¸æ˜¯ç‹¬ç«‹å¤„ç†è¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡å…¨å±€å½’ä¸€åŒ–å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œåˆ›å»ºç‹¬ç‰¹çš„è®¡ç®—æŒ‘æˆ˜ã€‚å…¶è®¡ç®—æ¶‰åŠå¯¹æ¯ä¸ªè¾“å…¥å€¼è¿›è¡ŒæŒ‡æ•°è¿ç®—å¹¶æŒ‰å®ƒä»¬çš„å’Œè¿›è¡Œå½’ä¸€åŒ–ï¼Œéšç€è¾“å‡ºç©ºé—´å˜å¤§ï¼Œè¿™ä¸ªè¿‡ç¨‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚åœ¨è½¯ä»¶ä¸­ï¼Œè¿™ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡å¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œå…¶ä¸­è¯æ±‡é‡å¯èƒ½è¾¾åˆ°æ•°åä¸‡ä¸ªæœ¯è¯­ã€‚è¯¥å‡½æ•°è¿˜è¦æ±‚åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¿æŒæ‰€æœ‰å€¼åœ¨å†…å­˜ä¸­ï¼Œå› ä¸ºæ¯ä¸ªè¾“å‡ºæ¦‚ç‡éƒ½ä¾èµ–äºæ•´ä¸ªè¾“å…¥å‘é‡ã€‚
- en: At the hardware level, softmax faces unique challenges because it canâ€™t process
    each value independently like other activation functions. Unlike ReLUâ€™s simple
    threshold or even sigmoidâ€™s per-value computation, softmax needs access to all
    values to perform normalization. This becomes particularly demanding in modern
    transformer architectures[18](#fn18), where softmax computations in attention
    mechanisms process thousands of values simultaneously. To manage these demands,
    hardware implementations often use approximation techniques or simplified versions
    of softmax, especially when dealing with large vocabularies or attention mechanisms.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¡¬ä»¶å±‚é¢ï¼Œsoftmaxé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä¸èƒ½åƒå…¶ä»–æ¿€æ´»å‡½æ•°é‚£æ ·ç‹¬ç«‹å¤„ç†æ¯ä¸ªå€¼ã€‚ä¸ReLUçš„ç®€å•é˜ˆå€¼æˆ–ç”šè‡³sigmoidçš„é€å€¼è®¡ç®—ä¸åŒï¼Œsoftmaxéœ€è¦è®¿é—®æ‰€æœ‰å€¼ä»¥æ‰§è¡Œå½’ä¸€åŒ–ã€‚è¿™åœ¨ç°ä»£å˜å‹å™¨æ¶æ„[18](#fn18)ä¸­å°¤å…¶è¦æ±‚é«˜ï¼Œå…¶ä¸­æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„softmaxè®¡ç®—åŒæ—¶å¤„ç†æ•°åƒä¸ªå€¼ã€‚ä¸ºäº†ç®¡ç†è¿™äº›éœ€æ±‚ï¼Œç¡¬ä»¶å®ç°é€šå¸¸ä½¿ç”¨è¿‘ä¼¼æŠ€æœ¯æˆ–softmaxçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹è¯æ±‡è¡¨æˆ–æ³¨æ„åŠ›æœºåˆ¶æ—¶ã€‚
- en: '[TableÂ 8.2](ch014.xhtml#tbl-compare-activations) summarizes the trade-offs
    of these commonly used activation functions and highlights how these choices affect
    system performance.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨8.2](ch014.xhtml#tbl-compare-activations)æ€»ç»“äº†è¿™äº›å¸¸ç”¨æ¿€æ´»å‡½æ•°çš„æƒè¡¡ï¼Œå¹¶å¼ºè°ƒäº†è¿™äº›é€‰æ‹©å¦‚ä½•å½±å“ç³»ç»Ÿæ€§èƒ½ã€‚'
- en: 'TableÂ 8.2: **Activation Function Trade-Offs**: Comparing activation functions
    exposes inherent advantages and disadvantages impacting system performance; for
    example, softmaxâ€™s normalization requirement poses hardware challenges in large-scale
    transformer models, while relu offers computational efficiency but can suffer
    from dying neurons. This table clarifies how activation function choices influence
    both model behavior and the practical constraints of machine learning system design.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.2ï¼š**æ¿€æ´»å‡½æ•°æƒè¡¡**ï¼šæ¯”è¾ƒæ¿€æ´»å‡½æ•°æ­ç¤ºäº†å½±å“ç³»ç»Ÿæ€§èƒ½çš„å›ºæœ‰ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼›ä¾‹å¦‚ï¼Œsoftmaxçš„å½’ä¸€åŒ–è¦æ±‚åœ¨å¤§è§„æ¨¡å˜å‹å™¨æ¨¡å‹ä¸­ç»™ç¡¬ä»¶å¸¦æ¥æŒ‘æˆ˜ï¼Œè€Œreluæä¾›äº†è®¡ç®—æ•ˆç‡ï¼Œä½†å¯èƒ½é­å—ç¥ç»å…ƒæ­»äº¡çš„é—®é¢˜ã€‚æ­¤è¡¨é˜æ˜äº†æ¿€æ´»å‡½æ•°çš„é€‰æ‹©å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºå’Œæœºå™¨å­¦ä¹ ç³»ç»Ÿè®¾è®¡çš„å®é™…çº¦æŸã€‚
- en: '| **Function** | **Key Advantages** | **Key Disadvantages** | **System Implications**
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **å‡½æ•°** | **ä¸»è¦ä¼˜åŠ¿** | **ä¸»è¦åŠ£åŠ¿** | **ç³»ç»Ÿå½±å“** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Sigmoid** | Smooth gradients; bounded output in <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(0, 1)</annotation></semantics>. | Vanishing gradients;
    non-zero-centered output. | Exponential computation adds overhead; limited scalability
    for deep networks on modern accelerators. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **Sigmoid** | å¹³æ»‘æ¢¯åº¦ï¼›è¾“å‡ºåœ¨<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,
    1)</annotation></semantics>ä¹‹é—´ã€‚ | æ¢¯åº¦æ¶ˆå¤±ï¼›éé›¶ä¸­å¿ƒè¾“å‡ºã€‚ | æŒ‡æ•°è®¡ç®—å¢åŠ å¼€é”€ï¼›åœ¨ç°ä»£åŠ é€Ÿå™¨ä¸Šæ·±åº¦ç½‘ç»œçš„æ‰©å±•æ€§æœ‰é™ã€‚ |'
- en: '| **Tanh** | Zero-centered output in <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,
    1)</annotation></semantics>; stabilizes gradients. | Vanishing gradients for large
    inputs. | More expensive than ReLU; still commonly used in RNNs/LSTMs but less
    common in CNNs and Transformers. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **Tanh** | é›¶ä¸­å¿ƒè¾“å‡ºåœ¨<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,
    1)</annotation></semantics>ä¹‹é—´ï¼›ç¨³å®šæ¢¯åº¦ã€‚ | å¤§è¾“å…¥æ—¶æ¢¯åº¦æ¶ˆå¤±ã€‚ | æ¯”ReLUæ›´æ˜‚è´µï¼›åœ¨RNNs/LSTMsä¸­ä»å¸¸ç”¨ï¼Œä½†åœ¨CNNså’ŒTransformersä¸­è¾ƒå°‘ä½¿ç”¨ã€‚
    |'
- en: '| **ReLU** | Computationally efficient; avoids vanishing gradients; introduces
    sparsity. | Dying neurons; unbounded output. | Simple operations optimize well
    on GPUs/TPUs; sparse activations reduce memory and computation needs. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **ReLU** | è®¡ç®—æ•ˆç‡é«˜ï¼›é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼›å¼•å…¥ç¨€ç–æ€§ã€‚ | æ­»äº¡ç¥ç»å…ƒï¼›æ— ç•Œè¾“å‡ºã€‚ | ç®€å•æ“ä½œåœ¨GPU/TPUä¸Šä¼˜åŒ–è‰¯å¥½ï¼›ç¨€ç–æ¿€æ´»å‡å°‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚
    |'
- en: '| **Softmax** | Converts logits into probabilities; sums to <semantics><mn>1</mn><annotation
    encoding="application/x-tex">1</annotation></semantics>. | Computationally expensive
    for large outputs. | High cost for large vocabularies; hierarchical or sampled
    softmax needed for scalability in NLP tasks. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **Softmax** | å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡ï¼›æ€»å’Œä¸º<semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics>ã€‚
    | å¤§è¾“å‡ºæ—¶è®¡ç®—æˆæœ¬é«˜ã€‚ | å¤§è¯æ±‡è¡¨çš„æˆæœ¬é«˜ï¼›åœ¨NLPä»»åŠ¡ä¸­éœ€è¦å±‚æ¬¡åŒ–æˆ–é‡‡æ ·softmaxä»¥å®ç°å¯æ‰©å±•æ€§ã€‚ |'
- en: The choice of activation function should balance computational considerations
    with their mathematical properties, such as handling vanishing gradients or introducing
    sparsity in neural activations. This data emphasizes the importance of evaluating
    both theoretical and practical performance when designing neural networks. For
    large-scale networks or real-time applications, ReLU is often the best choice
    due to its efficiency and scalability. However, for tasks requiring probabilistic
    outputs, such as classification, softmax remains indispensable despite its computational
    cost. Ultimately, the ideal activation function depends on the specific task,
    network architecture, and hardware environment.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°çš„é€‰æ‹©åº”åœ¨è®¡ç®—è€ƒè™‘ä¸å®ƒä»¬çš„æ•°å­¦å±æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä¾‹å¦‚å¤„ç†æ¢¯åº¦æ¶ˆå¤±æˆ–åœ¨ç¥ç»æ¿€æ´»ä¸­å¼•å…¥ç¨€ç–æ€§ã€‚æ­¤æ•°æ®å¼ºè°ƒäº†åœ¨è®¾è®¡ç¥ç»ç½‘ç»œæ—¶è¯„ä¼°ç†è®ºå’Œå®é™…æ€§èƒ½çš„é‡è¦æ€§ã€‚å¯¹äºå¤§è§„æ¨¡ç½‘ç»œæˆ–å®æ—¶åº”ç”¨ï¼ŒReLUç”±äºå…¶æ•ˆç‡å’Œå¯æ‰©å±•æ€§é€šå¸¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦æ¦‚ç‡è¾“å‡ºï¼Œå¦‚åˆ†ç±»çš„ä»»åŠ¡ï¼Œå°½ç®¡è®¡ç®—æˆæœ¬é«˜ï¼Œsoftmaxä»ç„¶ä¸å¯æˆ–ç¼ºã€‚æœ€ç»ˆï¼Œç†æƒ³çš„æ¿€æ´»å‡½æ•°å–å†³äºå…·ä½“ä»»åŠ¡ã€ç½‘ç»œæ¶æ„å’Œç¡¬ä»¶ç¯å¢ƒã€‚
- en: '**GPT-2 GELU Activation Function**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 GELUæ¿€æ´»å‡½æ•°**'
- en: 'While the table above covers classical activation functions, GPT-2 uses the
    Gaussian Error Linear Unit (GELU), defined as: <semantics><mrow><mtext mathvariant="normal">GELU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><mo>â‹…</mo><mi>Î¦</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><mo>â‹…</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo
    stretchy="true" form="prefix">[</mo><mn>1</mn><mo>+</mo><mtext mathvariant="normal">erf</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><msqrt><mn>2</mn></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\text{GELU}(x) = x \cdot \Phi(x) = x
    \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]</annotation></semantics>'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ä¸Šè¡¨æ¶µç›–äº†ç»å…¸çš„æ¿€æ´»å‡½æ•°ï¼Œä½†GPT-2ä½¿ç”¨é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆGELUï¼‰ï¼Œå®šä¹‰ä¸ºï¼š<semantics><mrow><mtext mathvariant="normal">GELU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><mo>â‹…</mo><mi>Î¦</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><mo>â‹…</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo
    stretchy="true" form="prefix">[</mo><mn>1</mn><mo>+</mo><mtext mathvariant="normal">erf</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mi>x</mi><msqrt><mn>2</mn></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\text{GELU}(x) = x \cdot \Phi(x) = x
    \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]</annotation></semantics>
- en: where <semantics><mrow><mi>Î¦</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(x)</annotation></semantics>
    is the cumulative distribution function of the standard normal distribution.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <semantics><mrow><mi>Î¦</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi(x)</annotation></semantics>
    æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚
- en: '**Why GELU for GPT-2?**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆGPT-2é€‰æ‹©GELUï¼Ÿ**'
- en: Smoother gradients than ReLU, reducing the dying neuron problem
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯”ReLUæ›´å¹³æ»‘çš„æ¢¯åº¦ï¼Œå‡å°‘äº†ç¥ç»å…ƒæ­»äº¡é—®é¢˜
- en: 'Stochastic regularization effect: acts like dropout by probabilistically dropping
    inputs'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæœºæ­£åˆ™åŒ–æ•ˆåº”ï¼šé€šè¿‡æ¦‚ç‡æ€§åœ°ä¸¢å¼ƒè¾“å…¥ï¼Œç±»ä¼¼äºdropout
- en: Better empirical performance on language modeling tasks
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šå…·æœ‰æ›´å¥½çš„ç»éªŒæ€§èƒ½
- en: '**System Performance Tradeoff**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿæ€§èƒ½æƒè¡¡**'
- en: 'Computational cost: ~3 to 4x more expensive than ReLU (requires erf function
    evaluation)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—æˆæœ¬ï¼šæ¯”ReLUè´µ3åˆ°4å€ï¼ˆéœ€è¦è¯„ä¼°erfå‡½æ•°ï¼‰
- en: 'Memory: Same as ReLU (element-wise operation)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜ï¼šä¸ReLUç›¸åŒï¼ˆé€å…ƒç´ æ“ä½œï¼‰
- en: 'Training time impact: For GPT-2â€™s 48 layers, GELU adds ~5 to 8% to total forward
    pass time'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ—¶é—´å½±å“ï¼šå¯¹äºGPT-2çš„48å±‚ï¼ŒGELUå°†æ€»å‰å‘ä¼ é€’æ—¶é—´å¢åŠ äº†çº¦5%åˆ°8%
- en: 'Worth it: The improved model quality (lower perplexity) offsets the computational
    overhead'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å€¼å¾—ï¼šæ”¹è¿›çš„æ¨¡å‹è´¨é‡ï¼ˆæ›´ä½çš„å›°æƒ‘åº¦ï¼‰æŠµæ¶ˆäº†è®¡ç®—å¼€é”€
- en: '**Fast Approximation:** Modern frameworks (PyTorch, TensorFlow) implement GELU
    with optimized approximations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¿«é€Ÿè¿‘ä¼¼**ï¼šç°ä»£æ¡†æ¶ï¼ˆPyTorchã€TensorFlowï¼‰é€šè¿‡ä¼˜åŒ–çš„è¿‘ä¼¼å®ç°GELUï¼š'
- en: '[PRE0]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This approximation reduces computational cost to ~1.5x ReLU while maintaining
    GELUâ€™s benefits, demonstrating how production systems balance mathematical properties
    with implementation efficiency.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¿‘ä¼¼å°†è®¡ç®—æˆæœ¬é™ä½åˆ°çº¦1.5å€ReLUï¼ŒåŒæ—¶ä¿æŒGELUçš„ä¼˜ç‚¹ï¼Œå±•ç¤ºäº†ç”Ÿäº§ç³»ç»Ÿå¦‚ä½•å¹³è¡¡æ•°å­¦å±æ€§ä¸å®ç°æ•ˆç‡ã€‚
- en: '**Systems Implication: Memory Bandwidth Bottlenecks**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿå½±å“ï¼šå†…å­˜å¸¦å®½ç“¶é¢ˆ**'
- en: 'Activation functions reveal a critical systems principle: not all operations
    are compute-bound. While matrix multiplications saturate GPU compute units, activation
    functions often become **memory-bandwidth-bound**:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°æ­ç¤ºäº†å…³é”®çš„ç³»ç»ŸåŸç†ï¼šå¹¶éæ‰€æœ‰æ“ä½œéƒ½æ˜¯è®¡ç®—å¯†é›†å‹çš„ã€‚è™½ç„¶çŸ©é˜µä¹˜æ³•ä¼šé¥±å’ŒGPUçš„è®¡ç®—å•å…ƒï¼Œä½†æ¿€æ´»å‡½æ•°é€šå¸¸ä¼šæˆä¸º**å†…å­˜å¸¦å®½é™åˆ¶**ï¼š
- en: '**Low arithmetic intensity**: Element-wise operations perform few calculations
    per memory access (ReLU: 1 operation per load)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½ç®—æœ¯å¼ºåº¦**ï¼šé€å…ƒç´ æ“ä½œæ¯è®¿é—®ä¸€æ¬¡å†…å­˜è¿›è¡Œçš„è®¡ç®—å¾ˆå°‘ï¼ˆReLUï¼šæ¯æ¬¡åŠ è½½è¿›è¡Œ1æ¬¡æ“ä½œï¼‰'
- en: '**Limited parallelism benefit**: Simple operations complete faster than memory
    transfer time'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœ‰é™çš„å¹¶è¡ŒåŒ–ä¼˜åŠ¿**ï¼šç®€å•æ“ä½œå®Œæˆé€Ÿåº¦æ¯”å†…å­˜ä¼ è¾“æ—¶é—´å¿«'
- en: '**Bandwidth constraints**: Modern GPUs have 10-100Ã— more compute throughput
    than memory bandwidth'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¸¦å®½é™åˆ¶**ï¼šç°ä»£GPUçš„è®¡ç®—ååé‡æ¯”å†…å­˜å¸¦å®½é«˜10-100å€'
- en: This explains why activation function choice matters less than expectedâ€”ReLU
    vs sigmoid shows only 2-3Ã— difference despite vastly different computational complexity,
    because both are bottlenecked by memory access. The forward pass must carefully
    manage activation storage to prevent memory bandwidth from limiting overall training
    throughput.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ¿€æ´»å‡½æ•°çš„é€‰æ‹©ä¸å¦‚é¢„æœŸé‚£ä¹ˆé‡è¦â€”â€”ReLUä¸sigmoidä¹‹é—´çš„å·®å¼‚ä»…ä¸º2-3å€ï¼Œå°½ç®¡å®ƒä»¬çš„è®¡ç®—å¤æ‚åº¦å·®å¼‚å¾ˆå¤§ï¼Œå› ä¸ºä¸¤è€…éƒ½å—é™äºå†…å­˜è®¿é—®ã€‚å‰å‘ä¼ æ’­å¿…é¡»ä»”ç»†ç®¡ç†æ¿€æ´»å­˜å‚¨ï¼Œä»¥é˜²æ­¢å†…å­˜å¸¦å®½é™åˆ¶æ•´ä½“è®­ç»ƒååé‡ã€‚
- en: Optimization Algorithms
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ç®—æ³•
- en: 'Optimization algorithms play an important role in neural network training by
    guiding the adjustment of model parameters to minimize a loss function. This process
    enables neural networks to learn from data, and it involves finding the optimal
    set of parameters that yield the best model performance on a given task. Broadly,
    these algorithms can be divided into two categories: classical methods, which
    provide the theoretical foundation, and advanced methods, which introduce enhancements
    for improved performance and efficiency.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ç®—æ³•åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œé€šè¿‡æŒ‡å¯¼æ¨¡å‹å‚æ•°çš„è°ƒæ•´ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚è¿™ä¸ªè¿‡ç¨‹ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶æ¶‰åŠæ‰¾åˆ°æœ€ä½³å‚æ•°é›†ï¼Œä»¥åœ¨ç»™å®šä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ¨¡å‹æ€§èƒ½ã€‚å¹¿ä¹‰ä¸Šï¼Œè¿™äº›ç®—æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šç»å…¸æ–¹æ³•ï¼Œå®ƒæä¾›äº†ç†è®ºåŸºç¡€ï¼›ä»¥åŠå…ˆè¿›æ–¹æ³•ï¼Œå®ƒå¼•å…¥äº†æ”¹è¿›æ€§èƒ½å’Œæ•ˆç‡çš„å¢å¼ºã€‚
- en: These algorithms explore the complex, high-dimensional loss function surface,
    identifying regions where the function achieves its lowest values. This task is
    challenging because the loss function surface is rarely smooth or simple, often
    characterized by local minima, saddle points, and sharp gradients. Effective optimization
    algorithms are designed to overcome these challenges, ensuring convergence to
    a solution that generalizes well to unseen data. While this section covers optimization
    algorithms used during training, advanced optimization techniques including quantization,
    pruning, and knowledge distillation are detailed in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç®—æ³•æ¢ç´¢å¤æ‚çš„é«˜ç»´æŸå¤±å‡½æ•°è¡¨é¢ï¼Œè¯†åˆ«å‡ºå‡½æ•°è¾¾åˆ°æœ€ä½å€¼çš„ä½ç½®ã€‚è¿™é¡¹ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæŸå¤±å‡½æ•°è¡¨é¢å¾ˆå°‘æ˜¯å¹³æ»‘æˆ–ç®€å•çš„ï¼Œé€šå¸¸å…·æœ‰å±€éƒ¨æœ€å°å€¼ã€éç‚¹å’Œå°–é”çš„æ¢¯åº¦ã€‚æœ‰æ•ˆçš„ä¼˜åŒ–ç®—æ³•æ—¨åœ¨å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œç¡®ä¿æ”¶æ•›åˆ°ä¸€ä¸ªå¯¹æœªè§æ•°æ®å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶æœ¬èŠ‚æ¶µç›–äº†è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œä½†åŒ…æ‹¬é‡åŒ–ã€å‰ªæå’ŒçŸ¥è¯†è’¸é¦åœ¨å†…çš„å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯å°†åœ¨ç¬¬10ç« ï¼ˆ[Chapter
    10](ch016.xhtml#sec-model-optimizations)ï¼‰ä¸­è¯¦ç»†è¯´æ˜ã€‚
- en: The selection and design of optimization algorithms have significant system-level
    implications, such as computation efficiency, memory requirements, and scalability
    to large datasets or models. Systematic approaches to hyperparameter optimization,
    including grid search, Bayesian optimization, and automated machine learning workflows,
    are covered in [ChapterÂ 5](ch011.xhtml#sec-ai-workflow). A deeper understanding
    of these algorithms is essential for addressing the trade-offs between accuracy,
    speed, and resource usage.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©å’Œè®¾è®¡å¯¹ç³»ç»Ÿçº§å½±å“é‡å¤§ï¼Œä¾‹å¦‚è®¡ç®—æ•ˆç‡ã€å†…å­˜éœ€æ±‚ä»¥åŠæ‰©å±•åˆ°å¤§å‹æ•°æ®é›†æˆ–æ¨¡å‹çš„èƒ½åŠ›ã€‚åŒ…æ‹¬ç½‘æ ¼æœç´¢ã€è´å¶æ–¯ä¼˜åŒ–å’Œè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹åœ¨å†…çš„è¶…å‚æ•°ä¼˜åŒ–ç³»ç»Ÿæ–¹æ³•å°†åœ¨ç¬¬5ç« ï¼ˆ[Chapter
    5](ch011.xhtml#sec-ai-workflow)ï¼‰ä¸­ä»‹ç»ã€‚å¯¹è¿™äº›ç®—æ³•çš„æ·±å…¥äº†è§£å¯¹äºè§£å†³ç²¾åº¦ã€é€Ÿåº¦å’Œèµ„æºä½¿ç”¨ä¹‹é—´çš„æƒè¡¡è‡³å…³é‡è¦ã€‚
- en: Gradient-Based Optimization Methods
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•
- en: Modern neural network training relies on variations of gradient descent for
    parameter optimization. These approaches differ in how they process training data,
    leading to distinct system-level implications.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç¥ç»ç½‘ç»œè®­ç»ƒä¾èµ–äºæ¢¯åº¦ä¸‹é™çš„å˜ä½“æ¥è¿›è¡Œå‚æ•°ä¼˜åŒ–ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†è®­ç»ƒæ•°æ®çš„æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒï¼Œå¯¼è‡´ç³»ç»Ÿçº§å½±å“å„ä¸ç›¸åŒã€‚
- en: Gradient Descent
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™
- en: 'Gradient descent is the mathematical foundation of neural network training,
    iteratively adjusting parameters to minimize a loss function. The basic gradient
    descent algorithm computes the gradient of the loss with respect to each parameter,
    then updates parameters in the opposite direction of the gradient: <semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \nabla L(\theta_t)</annotation></semantics>'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ•°å­¦åŸºç¡€ï¼Œé€šè¿‡è¿­ä»£è°ƒæ•´å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚åŸºæœ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è®¡ç®—æ¯ä¸ªå‚æ•°ç›¸å¯¹äºæŸå¤±çš„æ¢¯åº¦ï¼Œç„¶åæ›´æ–°å‚æ•°ä»¥æ¢¯åº¦ç›¸åçš„æ–¹å‘ï¼š<semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \nabla L(\theta_t)</annotation></semantics>
- en: The effectiveness of gradient descent in training systems reveals deep questions
    in optimization theory. Unlike convex optimization where gradient descent guarantees
    finding the global minimum, neural network loss surfaces contain exponentially
    many local minima. Yet gradient descent consistently finds solutions that generalize
    well, suggesting the optimization process has implicit biases toward solutions
    with desirable properties. Modern overparameterized networks, with more parameters
    than training examples, paradoxically achieve better generalization than smaller
    models, challenging traditional optimization intuitions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™åœ¨è®­ç»ƒç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§æ­ç¤ºäº†ä¼˜åŒ–ç†è®ºä¸­çš„æ·±å±‚æ¬¡é—®é¢˜ã€‚ä¸æ¢¯åº¦ä¸‹é™ä¿è¯æ‰¾åˆ°å…¨å±€æœ€å°å€¼çš„å‡¸ä¼˜åŒ–ä¸åŒï¼Œç¥ç»ç½‘ç»œæŸå¤±è¡¨é¢åŒ…å«æŒ‡æ•°æ•°é‡çš„å±€éƒ¨æœ€å°å€¼ã€‚ç„¶è€Œï¼Œæ¢¯åº¦ä¸‹é™å§‹ç»ˆæ‰¾åˆ°æ³›åŒ–è‰¯å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™è¡¨æ˜ä¼˜åŒ–è¿‡ç¨‹å…·æœ‰å¯¹å…·æœ‰ç†æƒ³ç‰¹æ€§çš„è§£å†³æ–¹æ¡ˆçš„éšå«åå·®ã€‚ç°ä»£è¿‡å‚æ•°åŒ–ç½‘ç»œï¼Œå‚æ•°æ•°é‡å¤šäºè®­ç»ƒç¤ºä¾‹ï¼Œåå¸¸åœ°å®ç°äº†æ¯”å°æ¨¡å‹æ›´å¥½çš„æ³›åŒ–ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„ä¼˜åŒ–ç›´è§‰ã€‚
- en: 'In training systems, this mathematical operation translates into specific computational
    patterns. For each iteration, the system must:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒç³»ç»Ÿä¸­ï¼Œè¿™ç§æ•°å­¦è¿ç®—è½¬åŒ–ä¸ºç‰¹å®šçš„è®¡ç®—æ¨¡å¼ã€‚å¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œç³»ç»Ÿå¿…é¡»ï¼š
- en: Compute forward pass activations
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—å‰å‘ä¼ é€’çš„æ¿€æ´»å€¼
- en: Calculate loss value
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸå¤±å€¼
- en: Compute gradients through backpropagation
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
- en: Update parameters using the gradient values
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¢¯åº¦å€¼æ›´æ–°å‚æ•°
- en: The computational demands of gradient descent scale with both model size and
    dataset size. Consider a neural network with <semantics><mi>M</mi><annotation
    encoding="application/x-tex">M</annotation></semantics> parameters training on
    <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    examples. Computing gradients requires storing intermediate activations during
    the forward pass for use in backpropagation. These activations consume memory
    proportional to the depth of the network and the number of examples being processed.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„è®¡ç®—éœ€æ±‚éšç€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°çš„å¢åŠ è€Œå¢åŠ ã€‚è€ƒè™‘ä¸€ä¸ªå…·æœ‰<semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics>ä¸ªå‚æ•°çš„ç¥ç»ç½‘ç»œåœ¨<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚è®¡ç®—æ¢¯åº¦éœ€è¦åœ¨å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­ä¸­ä½¿ç”¨ã€‚è¿™äº›æ¿€æ´»å€¼æ¶ˆè€—çš„å†…å­˜ä¸ç½‘ç»œçš„æ·±åº¦å’Œæ­£åœ¨å¤„ç†çš„ç¤ºä¾‹æ•°é‡æˆæ¯”ä¾‹ã€‚
- en: 'Traditional gradient descent processes the entire dataset in each iteration.
    For a training set with 1 million examples, computing gradients requires evaluating
    and storing results for each example before performing a parameter update. This
    approach poses significant system challenges: <semantics><mrow><mtext mathvariant="normal">Memory
    Required</mtext><mo>=</mo><mi>N</mi><mo>Ã—</mo><mtext mathvariant="normal">(Activation
    Memory + Gradient Memory)</mtext></mrow> <annotation encoding="application/x-tex">\text{Memory
    Required} = N \times \text{(Activation Memory + Gradient Memory)}</annotation></semantics>'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™è¿‡ç¨‹åœ¨æ¯ä¸ªè¿­ä»£ä¸­å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚å¯¹äºä¸€ä¸ªåŒ…å«ä¸€ç™¾ä¸‡ä¸ªç¤ºä¾‹çš„è®­ç»ƒé›†ï¼Œè®¡ç®—æ¢¯åº¦éœ€è¦è¯„ä¼°å’Œå­˜å‚¨æ¯ä¸ªç¤ºä¾‹çš„ç»“æœï¼Œç„¶åå†è¿›è¡Œå‚æ•°æ›´æ–°ã€‚è¿™ç§æ–¹æ³•å¸¦æ¥äº†é‡å¤§çš„ç³»ç»ŸæŒ‘æˆ˜ï¼š<semantics><mrow><mtext
    mathvariant="normal">æ‰€éœ€å†…å­˜</mtext><mo>=</mo><mi>N</mi><mo>Ã—</mo><mtext mathvariant="normal">(æ¿€æ´»å†…å­˜
    + æ¢¯åº¦å†…å­˜)</mtext></mrow> <annotation encoding="application/x-tex">\text{æ‰€éœ€å†…å­˜} =
    N \times \text{(æ¿€æ´»å†…å­˜ + æ¢¯åº¦å†…å­˜)}</annotation></semantics>
- en: The memory requirements often exceed available hardware resources on modern
    hardware. A ResNet-50 model processing ImageNet-scale datasets would require hundreds
    of gigabytes of memory using this approach. Processing the full dataset before
    each update creates long iteration times, reducing the rate at which the model
    can learn from the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç°ä»£ç¡¬ä»¶ä¸Šï¼Œå†…å­˜éœ€æ±‚å¾€å¾€è¶…è¿‡å¯ç”¨çš„ç¡¬ä»¶èµ„æºã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•å¤„ç† ImageNet è§„æ¨¡çš„æ•°æ®é›†éœ€è¦æ•°ç™¾ GB çš„å†…å­˜ã€‚åœ¨æ¯ä¸ªæ›´æ–°ä¹‹å‰å¤„ç†æ•´ä¸ªæ•°æ®é›†ä¼šåˆ›å»ºé•¿çš„è¿­ä»£æ—¶é—´ï¼Œä»è€Œé™ä½äº†æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ çš„é€Ÿç‡ã€‚
- en: Stochastic Gradient Descent
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: éšæœºæ¢¯åº¦ä¸‹é™
- en: These system constraints led to the development of variants that better align
    with hardware capabilities. The key insight was that exact gradient computation,
    while mathematically appealing, is not necessary for effective learning. This
    realization opened the door to methods that trade gradient accuracy for improved
    system efficiency.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç³»ç»Ÿçº¦æŸå¯¼è‡´äº†å¼€å‘å‡ºä¸ç¡¬ä»¶èƒ½åŠ›æ›´å¥½åœ°å¯¹é½çš„å˜ä½“ã€‚å…³é”®æ´å¯Ÿæ˜¯ï¼Œè™½ç„¶ç²¾ç¡®çš„æ¢¯åº¦è®¡ç®—åœ¨æ•°å­¦ä¸Šå¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å¯¹äºæœ‰æ•ˆçš„å­¦ä¹ æ¥è¯´å¹¶éå¿…è¦ã€‚è¿™ä¸€è®¤è¯†ä¸ºä»¥æ”¹è¿›çš„ç³»ç»Ÿæ•ˆç‡ä¸ºä»£ä»·æ¢å–æ¢¯åº¦ç²¾åº¦çš„æ–¹æ³•æ‰“å¼€äº†å¤§é—¨ã€‚
- en: 'These system limitations motivated the development of more efficient optimization
    approaches. SGD[19](#fn19) is a big shift in the optimization strategy. Rather
    than computing gradients over the entire dataset, SGD estimates gradients using
    individual training examples: <semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \nabla L(\theta_t; x_i, y_i)</annotation></semantics> where
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x_i,
    y_i)</annotation></semantics> represents a single training example. This approach
    drastically reduces memory requirements since only one exampleâ€™s activations and
    gradients need storage at any time.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç³»ç»Ÿé™åˆ¶ä¿ƒä½¿å¼€å‘äº†æ›´é«˜æ•ˆçš„ä¼˜åŒ–æ–¹æ³•ã€‚SGD[19](#fn19) æ˜¯ä¼˜åŒ–ç­–ç•¥çš„ä¸€æ¬¡é‡å¤§è½¬å˜ã€‚ä¸åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®¡ç®—æ¢¯åº¦ä¸åŒï¼ŒSGD ä½¿ç”¨å•ä¸ªè®­ç»ƒç¤ºä¾‹æ¥ä¼°è®¡æ¢¯åº¦ï¼š<semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \nabla L(\theta_t; x_i, y_i)</annotation></semantics> å…¶ä¸­ <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x_i,
    y_i)</annotation></semantics> ä»£è¡¨ä¸€ä¸ªå•ç‹¬çš„è®­ç»ƒç¤ºä¾‹ã€‚è¿™ç§æ–¹æ³•å¤§å¤§å‡å°‘äº†å†…å­˜éœ€æ±‚ï¼Œå› ä¸ºä»»ä½•æ—¶åˆ»åªéœ€è¦å­˜å‚¨ä¸€ä¸ªç¤ºä¾‹çš„æ¿€æ´»å’Œæ¢¯åº¦ã€‚
- en: However, processing single examples creates new system challenges. Modern accelerators
    achieve peak performance through parallel computation, processing multiple data
    elements simultaneously. Single-example updates leave most computing resources
    idle, resulting in poor hardware utilization. The frequent parameter updates also
    increase memory bandwidth requirements, as weights must be read and written for
    each example rather than amortizing these operations across multiple examples.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤„ç†å•ä¸ªç¤ºä¾‹ä¼šå¸¦æ¥æ–°çš„ç³»ç»ŸæŒ‘æˆ˜ã€‚ç°ä»£åŠ é€Ÿå™¨é€šè¿‡å¹¶è¡Œè®¡ç®—è¾¾åˆ°å³°å€¼æ€§èƒ½ï¼ŒåŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®å…ƒç´ ã€‚å•ä¸ªç¤ºä¾‹æ›´æ–°å¯¼è‡´å¤§å¤šæ•°è®¡ç®—èµ„æºé—²ç½®ï¼Œå¯¼è‡´ç¡¬ä»¶åˆ©ç”¨ç‡ä½ä¸‹ã€‚é¢‘ç¹çš„å‚æ•°æ›´æ–°ä¹Ÿå¢åŠ äº†å†…å­˜å¸¦å®½éœ€æ±‚ï¼Œå› ä¸ºå¿…é¡»ä¸ºæ¯ä¸ªç¤ºä¾‹è¯»å–å’Œå†™å…¥æƒé‡ï¼Œè€Œä¸æ˜¯å°†è¿™äº›æ“ä½œåˆ†æ‘Šåˆ°å¤šä¸ªç¤ºä¾‹ä¸Šã€‚
- en: Mini-batch Processing
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å°æ‰¹é‡å¤„ç†
- en: '***Batch Processing*** is the technique of computing gradients over *groups
    of training examples* simultaneously, enabling efficient *parallel computation*
    and improved *hardware utilization* during model training.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ‰¹é‡å¤„ç†*** æ˜¯åŒæ—¶è®¡ç®— *è®­ç»ƒç¤ºä¾‹ç»„* çš„æ¢¯åº¦çš„æŠ€æœ¯ï¼Œåœ¨æ¨¡å‹è®­ç»ƒæœŸé—´å®ç°é«˜æ•ˆçš„ *å¹¶è¡Œè®¡ç®—* å’Œæ”¹è¿›çš„ *ç¡¬ä»¶åˆ©ç”¨ç‡*ã€‚'
- en: Mini-batch gradient descent emerges as a practical compromise between full-batch
    and stochastic methods. It computes gradients over small batches of examples,
    enabling parallel computations that align well with modern GPU architectures ([Jeffrey
    Dean and Ghemawat 2008](ch058.xhtml#ref-dean2012large)). <semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \frac{1}{B} \sum_{i=1}^B \nabla L(\theta_t; x_i, y_i)</annotation></semantics>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æˆä¸ºå…¨æ‰¹é‡ä¸éšæœºæ–¹æ³•ä¹‹é—´çš„ä¸€ç§å®ç”¨æŠ˜è¡·æ–¹æ¡ˆã€‚å®ƒå¯¹ç¤ºä¾‹çš„å°æ‰¹é‡è®¡ç®—æ¢¯åº¦ï¼Œä½¿å¾—ä¸ç°ä»£GPUæ¶æ„ç›¸åŒ¹é…çš„å¹¶è¡Œè®¡ç®—æˆä¸ºå¯èƒ½ï¼ˆ[Jeffrey Deanå’ŒGhemawat
    2008](ch058.xhtml#ref-dean2012large)ï¼‰ã€‚<semantics><mrow><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>;</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \alpha \frac{1}{B} \sum_{i=1}^B \nabla L(\theta_t; x_i, y_i)</annotation></semantics>
- en: Mini-batch processing aligns well with modern hardware capabilities. Consider
    a training system using GPU hardware. These devices contain thousands of cores
    designed for parallel computation. Mini-batch processing allows these cores to
    simultaneously compute gradients for multiple examples, improving hardware utilization.
    The batch size B becomes a key system parameter, influencing both computational
    efficiency and memory requirements.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: å°æ‰¹é‡å¤„ç†ä¸ç°ä»£ç¡¬ä»¶èƒ½åŠ›ç›¸åŒ¹é…ã€‚è€ƒè™‘ä½¿ç”¨GPUç¡¬ä»¶çš„è®­ç»ƒç³»ç»Ÿã€‚è¿™äº›è®¾å¤‡åŒ…å«æˆåƒä¸Šä¸‡çš„ç”¨äºå¹¶è¡Œè®¡ç®—çš„å†…æ ¸ã€‚å°æ‰¹é‡å¤„ç†å…è®¸è¿™äº›å†…æ ¸åŒæ—¶è®¡ç®—å¤šä¸ªç¤ºä¾‹çš„æ¢¯åº¦ï¼Œä»è€Œæé«˜ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚æ‰¹æ¬¡å¤§å°Bæˆä¸ºå…³é”®ç³»ç»Ÿå‚æ•°ï¼Œå½±å“è®¡ç®—æ•ˆç‡å’Œå†…å­˜éœ€æ±‚ã€‚
- en: 'The relationship between batch size and system performance follows clear patterns
    that reveal hardware-software trade-offs. Memory requirements scale linearly with
    batch size, but the specific costs vary dramatically by model architecture: <semantics><mtable><mtr><mtd
    columnalign="right" style="text-align: right"><mtext mathvariant="normal">Memory
    Required</mtext><mo>=</mo><mi>B</mi><mo>Ã—</mo><mo stretchy="false" form="prefix">(</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mtext mathvariant="normal">Activation
    Memory</mtext></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>+</mi><mtext
    mathvariant="normal">Gradient Memory</mtext></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><mi>+</mi><mtext mathvariant="normal">Parameter Memory</mtext><mo
    stretchy="false" form="postfix">)</mo></mtd></mtr></mtable> <annotation encoding="application/x-tex">\begin{aligned}
    \text{Memory Required} = B \times (&\text{Activation Memory} \\ &+ \text{Gradient
    Memory} \\ &+ \text{Parameter Memory}) \end{aligned}</annotation></semantics>'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ‰¹æ¬¡å¤§å°ä¸ç³»ç»Ÿæ€§èƒ½ä¹‹é—´çš„å…³ç³»éµå¾ªæ¸…æ™°çš„æ¨¡å¼ï¼Œæ­ç¤ºäº†ç¡¬ä»¶ä¸è½¯ä»¶ä¹‹é—´çš„æƒè¡¡ã€‚å†…å­˜éœ€æ±‚ä¸æ‰¹æ¬¡å¤§å°æˆçº¿æ€§å…³ç³»ï¼Œä½†å…·ä½“æˆæœ¬å› æ¨¡å‹æ¶æ„è€Œå¼‚ï¼š<semantics><mtable><mtr><mtd
    columnalign="right" style="text-align: right"><mtext mathvariant="normal">å†…å­˜éœ€æ±‚</mtext><mo>=</mo><mi>B</mi><mo>Ã—</mo><mo
    stretchy="false" form="prefix">(</mo></mtd><mtd columnalign="left" style="text-align:
    left"><mtext mathvariant="normal">æ¿€æ´»å†…å­˜</mtext></mtd></mtr><mtr><mtd columnalign="left"
    style="text-align: left"><mi>+</mi><mtext mathvariant="normal">æ¢¯åº¦å†…å­˜</mtext></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mi>+</mi><mtext mathvariant="normal">å‚æ•°å†…å­˜</mtext><mo
    stretchy="false" form="postfix">)</mo></mtd></mtr></mtable> <annotation encoding="application/x-tex">\begin{aligned}
    \text{å†…å­˜éœ€æ±‚} = B \times (&\text{æ¿€æ´»å†…å­˜} \\ &+ \text{æ¢¯åº¦å†…å­˜} \\ &+ \text{å‚æ•°å†…å­˜}) \end{aligned}</annotation></semantics>'
- en: For concrete understanding, consider ResNet-50 training with different batch
    sizes. At batch size 32, the model requires approximately 8GB of activation memory,
    4GB for gradients, and 200MB for parameters per GPU. Doubling to batch size 64
    doubles these memory requirements to 16GB activations and 8GB gradients. This
    linear scaling quickly exhausts GPU memory, with high-end training GPUs typically
    providing 40-80GB of HBM.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…·ä½“ç†è§£ï¼Œè€ƒè™‘ä½¿ç”¨ä¸åŒæ‰¹æ¬¡å¤§å°çš„ResNet-50è®­ç»ƒã€‚åœ¨æ‰¹æ¬¡å¤§å°ä¸º32æ—¶ï¼Œæ¨¡å‹æ¯ä¸ªGPUéœ€è¦å¤§çº¦8GBçš„æ¿€æ´»å†…å­˜ï¼Œ4GBç”¨äºæ¢¯åº¦ï¼Œä»¥åŠ200MBç”¨äºå‚æ•°ã€‚å°†æ‰¹æ¬¡å¤§å°åŠ å€åˆ°64ï¼Œè¿™äº›å†…å­˜éœ€æ±‚ä¹ŸåŠ å€åˆ°16GBçš„æ¿€æ´»å†…å­˜å’Œ8GBçš„æ¢¯åº¦ã€‚è¿™ç§çº¿æ€§æ‰©å±•å¾ˆå¿«å°±ä¼šè€—å°½GPUå†…å­˜ï¼Œé«˜ç«¯è®­ç»ƒGPUé€šå¸¸æä¾›40-80GBçš„HBMã€‚
- en: 'Larger batches enable more efficient computation through improved parallelism
    and better memory access patterns. GPU utilization efficiency demonstrates this
    trade-off: batch sizes of 256 or higher typically achieve over 90% hardware utilization
    on modern training accelerators, while smaller batches of 16-32 may only achieve
    60-70% utilization due to insufficient parallelism to saturate the hardware.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒå¤§çš„æ‰¹é‡é€šè¿‡æé«˜å¹¶è¡Œæ€§å’Œæ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼æ¥å®ç°æ›´æœ‰æ•ˆçš„è®¡ç®—ã€‚GPU åˆ©ç”¨æ•ˆç‡å±•ç¤ºäº†è¿™ç§æƒè¡¡ï¼š256 æˆ–æ›´é«˜çš„æ‰¹é‡å¤§å°é€šå¸¸åœ¨ç°ä»£è®­ç»ƒåŠ é€Ÿå™¨ä¸Šå®ç°è¶…è¿‡
    90% çš„ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œè€Œè¾ƒå°çš„ 16-32 æ‰¹é‡å¯èƒ½åªèƒ½è¾¾åˆ° 60-70% çš„åˆ©ç”¨ç‡ï¼Œå› ä¸ºå¹¶è¡Œæ€§ä¸è¶³ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶ã€‚
- en: 'This establishes a central theme in training systems: the hardware-software
    trade-off between memory constraints and computational efficiency. Training systems
    must select batch sizes that maximize hardware utilization while fitting within
    available memory. The optimal choice often requires gradient accumulation when
    memory constraints prevent using efficiently large batches, trading increased
    computation for the same effective batch size.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨è®­ç»ƒç³»ç»Ÿä¸­ç¡®ç«‹äº†ä¸€ä¸ªä¸­å¿ƒä¸»é¢˜ï¼šåœ¨å†…å­˜çº¦æŸå’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„ç¡¬ä»¶-è½¯ä»¶æƒè¡¡ã€‚è®­ç»ƒç³»ç»Ÿå¿…é¡»é€‰æ‹©æ‰¹é‡å¤§å°ï¼Œä»¥æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡åŒæ—¶é€‚åº”å¯ç”¨çš„å†…å­˜ã€‚æœ€ä½³é€‰æ‹©é€šå¸¸éœ€è¦åœ¨å†…å­˜çº¦æŸé˜»æ­¢ä½¿ç”¨é«˜æ•ˆçš„å¤§æ‰¹é‡æ—¶è¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œä»¥å¢åŠ çš„è®¡ç®—é‡æ¢å–ç›¸åŒçš„æœ‰æ•ˆæ‰¹é‡å¤§å°ã€‚
- en: Adaptive and Momentum-Based Optimizers
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”å’ŒåŸºäºåŠ¨é‡çš„ä¼˜åŒ–å™¨
- en: Advanced optimization algorithms introduce mechanisms like momentum and adaptive
    learning rates to improve convergence. These methods have been instrumental in
    addressing the inefficiencies of classical approaches ([Kingma and Ba 2014](ch058.xhtml#ref-kingma2014adam)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜çº§ä¼˜åŒ–ç®—æ³•å¼•å…¥äº†å¦‚åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ç­‰æœºåˆ¶æ¥æé«˜æ”¶æ•›æ€§ã€‚è¿™äº›æ–¹æ³•åœ¨è§£å†³ç»å…¸æ–¹æ³•çš„ä¸æ•ˆç‡æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼ˆ[Kingma å’Œ Ba 2014](ch058.xhtml#ref-kingma2014adam)ï¼‰ã€‚
- en: Momentum-Based Methods
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åŸºäºåŠ¨é‡çš„æ–¹æ³•
- en: 'Momentum methods enhance gradient descent by accumulating a velocity vector
    across iterations. The momentum update equations introduce an additional term
    to track the history of parameter updates: <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>Î²</mi><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} v_{t+1} = \beta v_t + \nabla L(\theta_t)
    \\ \theta_{t+1} = \theta_t - \alpha v_{t+1} \end{gather*}</annotation></semantics>
    where <semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    is the momentum coefficient, typically set between 0.9 and 0.99\. From a systems
    perspective, momentum introduces additional memory requirements. The training
    system must maintain a velocity vector with the same dimensionality as the parameter
    vector, effectively doubling the memory needed for optimization state.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŠ¨é‡æ–¹æ³•é€šè¿‡åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ç´¯ç§¯é€Ÿåº¦å‘é‡æ¥å¢å¼ºæ¢¯åº¦ä¸‹é™ã€‚åŠ¨é‡æ›´æ–°æ–¹ç¨‹å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„é¡¹æ¥è·Ÿè¸ªå‚æ•°æ›´æ–°çš„å†å²ï¼š<semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>Î²</mi><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} v_{t+1} = \beta v_t + \nabla L(\theta_t)
    \\ \theta_{t+1} = \theta_t - \alpha v_{t+1} \end{gather*}</annotation></semantics>
    å…¶ä¸­ <semantics><mi>Î²</mi><annotation encoding="application/x-tex">\beta</annotation></semantics>
    æ˜¯åŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸è®¾ç½®åœ¨ 0.9 å’Œ 0.99 ä¹‹é—´ã€‚ä»ç³»ç»Ÿè§’åº¦æ¥çœ‹ï¼ŒåŠ¨é‡å¼•å…¥äº†é¢å¤–çš„å†…å­˜éœ€æ±‚ã€‚è®­ç»ƒç³»ç»Ÿå¿…é¡»ç»´æŠ¤ä¸€ä¸ªä¸å‚æ•°å‘é‡ç›¸åŒç»´åº¦çš„é€Ÿåº¦å‘é‡ï¼Œå®é™…ä¸Šå°†ä¼˜åŒ–çŠ¶æ€çš„å†…å­˜éœ€æ±‚ç¿»å€ã€‚'
- en: Adaptive Learning Rate Methods
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•
- en: 'RMSprop modifies the basic gradient descent update by maintaining a moving
    average of squared gradients for each parameter: <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mi>Î³</mi><msub><mi>s</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î³</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><msup><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">)</mo><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><mrow><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><msqrt><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>+</mo><mi>Ïµ</mi></mrow></msqrt></mfrac></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} s_t = \gamma s_{t-1} + (1-\gamma)\big(\nabla
    L(\theta_t)\big)^2 \\ \theta_{t+1} = \theta_t - \alpha \frac{\nabla L(\theta_t)}{\sqrt{s_t
    + \epsilon}} \end{gather*}</annotation></semantics>'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'RMSprop é€šè¿‡ç»´æŠ¤æ¯ä¸ªå‚æ•°çš„å¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡æ¥ä¿®æ”¹åŸºæœ¬çš„æ¢¯åº¦ä¸‹é™æ›´æ–°ï¼š<semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mi>Î³</mi><msub><mi>s</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><mi>Î³</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><msup><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msup><mo minsize="1.2" maxsize="1.2"
    stretchy="false" form="prefix">(</mo><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><mrow><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><msqrt><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>+</mo><mi>Ïµ</mi></mrow></msqrt></mfrac></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} s_t = \gamma s_{t-1} + (1-\gamma)\big(\nabla
    L(\theta_t)\big)^2 \\ \theta_{t+1} = \theta_t - \alpha \frac{\nabla L(\theta_t)}{\sqrt{s_t
    + \epsilon}} \end{gather*}</annotation></semantics>'
- en: This per-parameter adaptation requires storing the moving average <semantics><msub><mi>s</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">s_t</annotation></semantics>, creating memory overhead
    similar to momentum methods. The element-wise operations in RMSprop also introduce
    additional computational steps compared to basic gradient descent.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¯å‚æ•°çš„é€‚åº”éœ€è¦å­˜å‚¨ç§»åŠ¨å¹³å‡ <semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_t</annotation></semantics>ï¼Œè¿™ä¼šåˆ›å»ºä¸åŠ¨é‡æ–¹æ³•ç±»ä¼¼çš„å†…å­˜å¼€é”€ã€‚ä¸åŸºæœ¬æ¢¯åº¦ä¸‹é™ç›¸æ¯”ï¼ŒRMSprop
    ä¸­çš„é€å…ƒç´ æ“ä½œä¹Ÿå¼•å…¥äº†é¢å¤–çš„è®¡ç®—æ­¥éª¤ã€‚
- en: Adam Optimization
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Adam ä¼˜åŒ–
- en: 'Adam combines concepts from both momentum and RMSprop, maintaining two moving
    averages for each parameter: <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>m</mi><mi>t</mi></msub><mo>=</mo><msub><mi>Î²</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>âˆ‡</mi><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><msub><mi>Î²</mi><mn>2</mn></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>2</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msup><mo minsize="1.2" maxsize="1.2"
    stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><msub><mi>m</mi><mi>t</mi></msub><msqrt><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi>Ïµ</mi></mrow></msqrt></mfrac></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla
    L(\theta_t) \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)\big(\nabla L(\theta_t)\big)^2
    \\ \theta_{t+1} = \theta_t - \alpha \frac{m_t}{\sqrt{v_t + \epsilon}} \end{gather*}</annotation></semantics>'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'Adamç®—æ³•ç»“åˆäº†åŠ¨é‡å’ŒRMSpropçš„æ¦‚å¿µï¼Œä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸¤ä¸ªç§»åŠ¨å¹³å‡å€¼ï¼š<semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>m</mi><mi>t</mi></msub><mo>=</mo><msub><mi>Î²</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>âˆ‡</mi><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><msub><mi>Î²</mi><mn>2</mn></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>2</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mi>âˆ‡</mi><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msup><mo minsize="1.2" maxsize="1.2"
    stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mi>Î¸</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>Î¸</mi><mi>t</mi></msub><mo>âˆ’</mo><mi>Î±</mi><mfrac><msub><mi>m</mi><mi>t</mi></msub><msqrt><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi>Ïµ</mi></mrow></msqrt></mfrac></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla
    L(\theta_t) \\ v_t = \beta_2 v_{t-1} + (1-\beta_2)\big(\nabla L(\theta_t)\big)^2
    \\ \theta_{t+1} = \theta_t - \alpha \frac{m_t}{\sqrt{v_t + \epsilon}} \end{gather*}</annotation></semantics>'
- en: The system implications of Adam are more substantial than previous methods.
    The optimizer must store two additional vectors (<semantics><msub><mi>m</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">m_t</annotation></semantics> and <semantics><msub><mi>v</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">v_t</annotation></semantics>) for each parameter,
    tripling the memory required for optimization state. For a model with 100 million
    parameters using 32-bit floating-point numbers, the additional memory requirement
    is approximately 800 MB.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Adamç®—æ³•çš„ç³»ç»Ÿå½±å“æ¯”ä¹‹å‰çš„æ–¹æ³•æ›´ä¸ºæ˜¾è‘—ã€‚ä¼˜åŒ–å™¨å¿…é¡»ä¸ºæ¯ä¸ªå‚æ•°å­˜å‚¨ä¸¤ä¸ªé¢å¤–çš„å‘é‡ï¼ˆ<semantics><msub><mi>m</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">m_t</annotation></semantics> å’Œ <semantics><msub><mi>v</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">v_t</annotation></semantics>ï¼‰ï¼Œè¿™å°†ä¼˜åŒ–çŠ¶æ€æ‰€éœ€çš„å†…å­˜å¢åŠ ä¸‰å€ã€‚å¯¹äºä¸€ä¸ªæ‹¥æœ‰1äº¿ä¸ªå‚æ•°ä¸”ä½¿ç”¨32ä½æµ®ç‚¹æ•°çš„æ¨¡å‹ï¼Œé¢å¤–çš„å†…å­˜éœ€æ±‚å¤§çº¦ä¸º800
    MBã€‚
- en: Optimization Algorithm System Implications
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ç®—æ³•ç³»ç»Ÿå½±å“
- en: The practical implementation of both classical and advanced optimization methods
    requires careful consideration of system resources and hardware capabilities.
    Understanding these implications helps inform algorithm selection and system design
    choices.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ç®—æ³•çš„å®é™…å®ç°ï¼Œæ— è®ºæ˜¯ç»å…¸è¿˜æ˜¯é«˜çº§æ–¹æ³•ï¼Œéƒ½éœ€è¦ä»”ç»†è€ƒè™‘ç³»ç»Ÿèµ„æºå’Œç¡¬ä»¶èƒ½åŠ›ã€‚ç†è§£è¿™äº›å½±å“æœ‰åŠ©äºæŒ‡å¯¼ç®—æ³•é€‰æ‹©å’Œç³»ç»Ÿè®¾è®¡å†³ç­–ã€‚
- en: Optimization Trade-offs
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–æƒè¡¡
- en: 'The choice of optimization algorithm creates specific patterns of computation
    and memory access that influence training efficiency. Memory requirements increase
    progressively from basic gradient descent to more sophisticated methods: <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">SGD</mtext></msub><mo>=</mo><msub><mtext mathvariant="normal">Size</mtext><mtext
    mathvariant="normal">params</mtext></msub></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">Momentum</mtext></msub><mo>=</mo><mn>2</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">Adam</mtext></msub><mo>=</mo><mn>3</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} \text{Memory}_{\text{SGD}} = \text{Size}_{\text{params}}
    \\ \text{Memory}_{\text{Momentum}} = 2 \times \text{Size}_{\text{params}} \\ \text{Memory}_{\text{Adam}}
    = 3 \times \text{Size}_{\text{params}} \end{gather*}</annotation></semantics>'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ä¼šåˆ›å»ºç‰¹å®šçš„è®¡ç®—å’Œå†…å­˜è®¿é—®æ¨¡å¼ï¼Œä»è€Œå½±å“è®­ç»ƒæ•ˆç‡ã€‚å†…å­˜éœ€æ±‚ä»åŸºæœ¬çš„æ¢¯åº¦ä¸‹é™åˆ°æ›´å¤æ‚çš„æ–¹æ³•é€æ¸å¢åŠ ï¼š<semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">SGD</mtext></msub><mo>=</mo><msub><mtext mathvariant="normal">Size</mtext><mtext
    mathvariant="normal">params</mtext></msub></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">Momentum</mtext></msub><mo>=</mo><mn>2</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">Adam</mtext></msub><mo>=</mo><mn>3</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} \text{Memory}_{\text{SGD}} = \text{Size}_{\text{params}}
    \\ \text{Memory}_{\text{Momentum}} = 2 \times \text{Size}_{\text{params}} \\ \text{Memory}_{\text{Adam}}
    = 3 \times \text{Size}_{\text{params}} \end{gather*}</annotation></semantics>'
- en: These memory costs must be balanced against convergence benefits. While Adam
    often requires fewer iterations to reach convergence, its per-iteration memory
    and computation overhead may impact training speed on memory-constrained systems.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å­˜æˆæœ¬å¿…é¡»ä¸æ”¶æ•›æ”¶ç›Šç›¸å¹³è¡¡ã€‚è™½ç„¶Adamé€šå¸¸éœ€è¦æ›´å°‘çš„è¿­ä»£æ¬¡æ•°æ‰èƒ½è¾¾åˆ°æ”¶æ•›ï¼Œä½†å…¶æ¯è¿­ä»£å†…å­˜å’Œè®¡ç®—å¼€é”€å¯èƒ½ä¼šå½±å“å†…å­˜å—é™ç³»ç»Ÿä¸Šçš„è®­ç»ƒé€Ÿåº¦ã€‚
- en: '**GPT-2 Adam Optimizer Memory Requirements**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 Adamä¼˜åŒ–å™¨å†…å­˜éœ€æ±‚**'
- en: 'GPT-2 training uses the Adam optimizer with these hyperparameters:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2è®­ç»ƒä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå…¶è¶…å‚æ•°å¦‚ä¸‹ï¼š
- en: Î²â‚ = 0.9 (momentum decay)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î²â‚ = 0.9ï¼ˆåŠ¨é‡è¡°å‡ï¼‰
- en: Î²â‚‚ = 0.999 (second moment decay)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Î²â‚‚ = 0.999ï¼ˆç¬¬äºŒåŠ¨é‡è¡°å‡ï¼‰
- en: 'Learning rate: Warmed up from 0 to 2.5e-4 over first 500 steps, then cosine
    decay'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ï¼šåœ¨å‰500æ­¥ä»0é¢„çƒ­åˆ°2.5e-4ï¼Œç„¶åè¿›è¡Œä½™å¼¦è¡°å‡
- en: 'Weight decay: 0.01'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒé‡è¡°å‡ï¼š0.01
- en: 'Gradient clipping: Global norm clipping at 1.0'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è£å‰ªï¼šå…¨å±€èŒƒæ•°è£å‰ªè‡³1.0
- en: '**Memory Overhead Calculation**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†…å­˜å¼€é”€è®¡ç®—**'
- en: 'For GPT-2â€™s 1.5B parameters in FP32 (4 bytes each):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºGPT-2çš„1.5Bå‚æ•°åœ¨FP32ï¼ˆæ¯ä¸ª4å­—èŠ‚ï¼‰ä¸­ï¼š
- en: 'Parameters: 1.5B Ã— 4 bytes = 6.0 GB'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚æ•°ï¼š1.5B Ã— 4å­—èŠ‚ = 6.0 GB
- en: 'Gradients: 1.5B Ã— 4 bytes = 6.0 GB'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ï¼š1.5B Ã— 4å­—èŠ‚ = 6.0 GB
- en: 'Adam first moment (m): 1.5B Ã— 4 bytes = 6.0 GB'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adamç¬¬ä¸€åŠ¨é‡ï¼ˆmï¼‰ï¼š1.5B Ã— 4å­—èŠ‚ = 6.0 GB
- en: 'Adam second moment (v): 1.5B Ã— 4 bytes = 6.0 GB'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adamç¬¬äºŒåŠ¨é‡ï¼ˆvï¼‰ï¼š1.5B Ã— 4å­—èŠ‚ = 6.0 GB
- en: 'Total optimizer state: 24 GB'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»ä¼˜åŒ–å™¨çŠ¶æ€ï¼š24 GB
- en: This explains why GPT-2 training requires 32GB+ V100 GPUs even before considering
    activation memory.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆåœ¨è€ƒè™‘æ¿€æ´»å†…å­˜ä¹‹å‰ï¼ŒGPT-2è®­ç»ƒéœ€è¦32GB+ V100 GPUã€‚
- en: '**System Decisions Driven by Optimizer**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç”±ä¼˜åŒ–å™¨é©±åŠ¨çš„ç³»ç»Ÿå†³ç­–**'
- en: Mixed precision training (FP16 params, FP32 optimizer state) cuts this to ~15GB
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16å‚æ•°ï¼ŒFP32ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰å°†æ­¤é™è‡³çº¦15GB
- en: Gradient accumulation (splitting effective batches into smaller micro-batches,
    accumulating gradients across multiple forward/backward passes before updating,
    detailed in [SectionÂ 8.5.5](ch014.xhtml#sec-ai-training-gradient-accumulation-checkpointing-26ab))
    allows effective batch_size=512 despite memory limits
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯ï¼ˆå°†æœ‰æ•ˆæ‰¹æ¬¡æ‹†åˆ†ä¸ºæ›´å°çš„å¾®æ‰¹æ¬¡ï¼Œåœ¨æ›´æ–°å‰è·¨å¤šä¸ªæ­£å‘/åå‘ä¼ é€’ç´¯ç§¯æ¢¯åº¦ï¼Œè¯¦æƒ…è§[ç¬¬8.5.5èŠ‚](ch014.xhtml#sec-ai-training-gradient-accumulation-checkpointing-26ab)ï¼‰å…è®¸åœ¨å†…å­˜é™åˆ¶ä¸‹æœ‰æ•ˆä½¿ç”¨batch_size=512
- en: Optimizer state sharding (ZeRO-2) distributes Adam state across GPUs in distributed
    training
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼ˆZeRO-2ï¼‰å°†AdamçŠ¶æ€åˆ†å¸ƒåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„GPUä¸Š
- en: '**Convergence Tradeoff:** Adamâ€™s memory overhead is worth it. GPT-2 converges
    in ~50K steps vs.Â ~150K+ steps with SGD+Momentum, saving weeks of training time
    despite higher per-step cost.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¶æ•›æƒè¡¡ï¼š** Adamçš„å†…å­˜å¼€é”€æ˜¯å€¼å¾—çš„ã€‚GPT-2åœ¨å¤§çº¦50Kæ­¥å†…æ”¶æ•›ï¼Œè€Œä½¿ç”¨SGD+åŠ¨é‡åˆ™éœ€è¦å¤§çº¦150K+æ­¥ï¼Œå°½ç®¡æ¯æ­¥çš„æˆæœ¬æ›´é«˜ï¼Œä½†èŠ‚çœäº†æ•°å‘¨çš„è®­ç»ƒæ—¶é—´ã€‚'
- en: Implementation Considerations
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å®ç°è€ƒè™‘å› ç´ 
- en: The efficient implementation of optimization algorithms in training frameworks
    hinges on strategic system-level considerations that directly influence performance.
    Key factors include memory bandwidth management, operation fusion techniques,
    and numerical precision optimization. These elements collectively determine the
    computational efficiency, memory utilization, and scalability of optimizers across
    diverse hardware architectures.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¡†æ¶ä¸­ä¼˜åŒ–ç®—æ³•çš„é«˜æ•ˆå®ç°å–å†³äºç›´æ¥å½±å“æ€§èƒ½çš„æˆ˜ç•¥æ€§ç³»ç»Ÿçº§è€ƒè™‘ã€‚å…³é”®å› ç´ åŒ…æ‹¬å†…å­˜å¸¦å®½ç®¡ç†ã€æ“ä½œèåˆæŠ€æœ¯å’Œæ•°å€¼ç²¾åº¦ä¼˜åŒ–ã€‚è¿™äº›å…ƒç´ å…±åŒå†³å®šäº†ä¼˜åŒ–å™¨åœ¨å¤šç§ç¡¬ä»¶æ¶æ„ä¸Šçš„è®¡ç®—æ•ˆç‡ã€å†…å­˜åˆ©ç”¨ç‡å’Œå¯æ‰©å±•æ€§ã€‚
- en: 'Memory bandwidth presents the primary bottleneck in optimizer implementation.
    Modern frameworks address this through operation fusion, which reduces memory
    access overhead by combining multiple operations into a single kernel. For example,
    the Adam optimizerâ€™s memory access requirements can grow linearly with parameter
    size when operations are performed separately: <semantics><mrow><msub><mtext mathvariant="normal">Bandwidth</mtext><mtext
    mathvariant="normal">separate</mtext></msub><mo>=</mo><mn>5</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{Bandwidth}_{\text{separate}} =
    5 \times \text{Size}_{\text{params}}</annotation></semantics>'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜å¸¦å®½æ˜¯ä¼˜åŒ–å™¨å®ç°ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚ç°ä»£æ¡†æ¶é€šè¿‡æ“ä½œèåˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡å°†å¤šä¸ªæ“ä½œç»„åˆæˆä¸€ä¸ªå•ç‹¬çš„å†…æ ¸æ¥å‡å°‘å†…å­˜è®¿é—®å¼€é”€ã€‚ä¾‹å¦‚ï¼Œå½“å•ç‹¬æ‰§è¡Œæ“ä½œæ—¶ï¼ŒAdamä¼˜åŒ–å™¨çš„å†…å­˜è®¿é—®éœ€æ±‚å¯ä»¥éšç€å‚æ•°å¤§å°çº¿æ€§å¢é•¿ï¼š<semantics><mrow><msub><mtext
    mathvariant="normal">å¸¦å®½</mtext><mtext mathvariant="normal">å•ç‹¬</mtext></msub><mo>=</mo><mn>5</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">å¤§å°</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{å¸¦å®½}_{\text{å•ç‹¬}} = 5 \times \text{å¤§å°}_{\text{params}}</annotation></semantics>
- en: 'However, fusing these operations into a single computational kernel significantly
    reduces the bandwidth requirement: <semantics><mrow><msub><mtext mathvariant="normal">Bandwidth</mtext><mtext
    mathvariant="normal">fused</mtext></msub><mo>=</mo><mn>2</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{Bandwidth}_{\text{fused}} = 2 \times
    \text{Size}_{\text{params}}</annotation></semantics>'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå°†è¿™äº›æ“ä½œèåˆæˆä¸€ä¸ªå•ä¸€çš„è®¡ç®—å†…æ ¸æ˜¾è‘—é™ä½äº†å¸¦å®½éœ€æ±‚ï¼š<semantics><mrow><msub><mtext mathvariant="normal">å¸¦å®½</mtext><mtext
    mathvariant="normal">èåˆ</mtext></msub><mo>=</mo><mn>2</mn><mo>Ã—</mo><msub><mtext
    mathvariant="normal">å¤§å°</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{å¸¦å®½}_{\text{èåˆ}} = 2 \times \text{å¤§å°}_{\text{params}}</annotation></semantics>
- en: These techniques have been effectively demonstrated in systems like cuDNN and
    other GPU-accelerated frameworks that optimize memory bandwidth usage and operation
    fusion ([Chetlur et al. 2014](ch058.xhtml#ref-chetlur2014cudnn); [Norman P. Jouppi
    et al. 2017b](ch058.xhtml#ref-jouppi2017tpu)).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŠ€æœ¯åœ¨cuDNNå’Œå…¶ä»–ä¼˜åŒ–å†…å­˜å¸¦å®½ä½¿ç”¨å’Œæ“ä½œèåˆçš„GPUåŠ é€Ÿæ¡†æ¶ï¼ˆ[Chetlurç­‰äºº2014](ch058.xhtml#ref-chetlur2014cudnn)ï¼›[Norman
    P. Jouppiç­‰äºº2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰ä¸­å¾—åˆ°äº†æœ‰æ•ˆå±•ç¤ºã€‚
- en: Memory access patterns also play an important role in determining the efficiency
    of cache utilization. Sequential access to parameter and optimizer state vectors
    maximizes cache hit rates and effective memory bandwidth. This principle is evident
    in hardware such as GPUs and tensor processing units (TPUs), where optimized memory
    layouts significantly improve performance ([Norman P. Jouppi et al. 2017b](ch058.xhtml#ref-jouppi2017tpu)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜è®¿é—®æ¨¡å¼ä¹Ÿåœ¨ç¡®å®šç¼“å­˜åˆ©ç”¨ç‡æ•ˆç‡æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å¯¹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å‘é‡çš„é¡ºåºè®¿é—®æœ€å¤§åŒ–äº†ç¼“å­˜å‘½ä¸­ç‡ä»¥åŠæœ‰æ•ˆå†…å­˜å¸¦å®½ã€‚è¿™ä¸€åŸåˆ™åœ¨GPUå’Œå¼ é‡å¤„ç†å•å…ƒï¼ˆTPUï¼‰ç­‰ç¡¬ä»¶ä¸­è¡¨ç°å¾—å°¤ä¸ºæ˜æ˜¾ï¼Œä¼˜åŒ–çš„å†…å­˜å¸ƒå±€æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ˆ[Norman
    P. Jouppiç­‰äºº2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰ã€‚
- en: 'Numerical precision represents another important tradeoff in implementation.
    Empirical studies have shown that optimizer states remain stable even when reduced
    precision formats, such as 16-bit floating-point (FP16), are used. Transitioning
    from 32-bit to 16-bit formats reduces memory requirements, as illustrated for
    the Adam optimizer: <semantics><mrow><msub><mtext mathvariant="normal">Memory</mtext><mtext
    mathvariant="normal">Adam-FP16</mtext></msub><mo>=</mo><mfrac><mn>3</mn><mn>2</mn></mfrac><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{Memory}_{\text{Adam-FP16}} = \frac{3}{2}
    \times \text{Size}_{\text{params}}</annotation></semantics>'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å€¼ç²¾åº¦æ˜¯å®ç°ä¸­çš„å¦ä¸€ä¸ªé‡è¦æƒè¡¡ã€‚ç»éªŒç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨é™ä½ç²¾åº¦æ ¼å¼ï¼ˆå¦‚16ä½æµ®ç‚¹æ•°ï¼ˆFP16ï¼‰ï¼‰ï¼Œä¼˜åŒ–å™¨çŠ¶æ€ä»ç„¶ä¿æŒç¨³å®šã€‚ä»32ä½åˆ°16ä½æ ¼å¼çš„è½¬æ¢å¯ä»¥å‡å°‘å†…å­˜éœ€æ±‚ï¼Œå¦‚Adamä¼˜åŒ–å™¨æ‰€ç¤ºï¼š<semantics><mrow><msub><mtext
    mathvariant="normal">Memory</mtext><mtext mathvariant="normal">Adam-FP16</mtext></msub><mo>=</mo><mfrac><mn>3</mn><mn>2</mn></mfrac><mo>Ã—</mo><msub><mtext
    mathvariant="normal">Size</mtext><mtext mathvariant="normal">params</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{Memory}_{\text{Adam-FP16}} = \frac{3}{2}
    \times \text{Size}_{\text{params}}</annotation></semantics>
- en: Mixed-precision training[20](#fn20) has been shown to achieve comparable accuracy
    while significantly reducing memory consumption and computational overhead ([Micikevicius
    et al. 2017](ch058.xhtml#ref-micikevicius2017mixed); [Krishnamoorthi 2018](ch058.xhtml#ref-krishnamoorthi2018quantizing)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒ[20](#fn20)å·²è¢«è¯æ˜å¯ä»¥è¾¾åˆ°ç›¸å½“çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¼€é”€ ([Micikevicius et al. 2017](ch058.xhtml#ref-micikevicius2017mixed);
    [Krishnamoorthi 2018](ch058.xhtml#ref-krishnamoorthi2018quantizing))ã€‚
- en: The above implementation factors determine the practical performance of optimization
    algorithms in deep learning systems, emphasizing the importance of tailoring memory,
    computational, and numerical strategies to the underlying hardware architecture
    ([T. Chen et al. 2015](ch058.xhtml#ref-chen2015mxnet)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å®ç°å› ç´ å†³å®šäº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­ä¼˜åŒ–ç®—æ³•çš„å®é™…æ€§èƒ½ï¼Œå¼ºè°ƒäº†æ ¹æ®åº•å±‚ç¡¬ä»¶æ¶æ„è°ƒæ•´å†…å­˜ã€è®¡ç®—å’Œæ•°å€¼ç­–ç•¥çš„é‡è¦æ€§ ([T. Chen et al. 2015](ch058.xhtml#ref-chen2015mxnet))ã€‚
- en: Optimizer Trade-offs
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨æƒè¡¡
- en: The evolution of optimization algorithms in neural network training reveals
    an intersection between algorithmic efficiency and system performance. While optimizers
    were primarily developed to improve model convergence, their implementation significantly
    impacts memory usage, computational requirements, and hardware utilization.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä¼˜åŒ–ç®—æ³•çš„å‘å±•æ­ç¤ºäº†ç®—æ³•æ•ˆç‡ä¸ç³»ç»Ÿæ€§èƒ½ä¹‹é—´çš„äº¤é›†ã€‚è™½ç„¶ä¼˜åŒ–å™¨æœ€åˆæ˜¯ä¸ºäº†æé«˜æ¨¡å‹æ”¶æ•›è€Œå¼€å‘çš„ï¼Œä½†å®ƒä»¬çš„å®ç°æ–¹å¼å¯¹å†…å­˜ä½¿ç”¨ã€è®¡ç®—éœ€æ±‚å’Œç¡¬ä»¶åˆ©ç”¨ç‡äº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚
- en: A deeper examination of popular optimization algorithms reveals their varying
    impacts on system resources. As shown in [TableÂ 8.3](ch014.xhtml#tbl-optimizer-properties),
    each optimizer presents distinct trade-offs between memory usage, computational
    patterns, and convergence behavior. SGD maintains minimal memory overhead, requiring
    storage only for model parameters and current gradients. This lightweight memory
    footprint comes at the cost of slower convergence and potentially poor hardware
    utilization due to its sequential update nature.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æµè¡Œçš„ä¼˜åŒ–ç®—æ³•è¿›è¡Œæ›´æ·±å…¥çš„è€ƒå¯Ÿï¼Œå¯ä»¥å‘ç°å®ƒä»¬å¯¹ç³»ç»Ÿèµ„æºçš„å½±å“å„ä¸ç›¸åŒã€‚å¦‚è¡¨8.3æ‰€ç¤ºï¼Œæ¯ä¸ªä¼˜åŒ–å™¨åœ¨å†…å­˜ä½¿ç”¨ã€è®¡ç®—æ¨¡å¼å’Œæ”¶æ•›è¡Œä¸ºä¹‹é—´éƒ½æœ‰ç‹¬ç‰¹çš„æƒè¡¡ã€‚SGDä¿æŒæœ€å°çš„å†…å­˜å¼€é”€ï¼Œåªéœ€å­˜å‚¨æ¨¡å‹å‚æ•°å’Œå½“å‰æ¢¯åº¦ã€‚è¿™ç§è½»é‡çº§çš„å†…å­˜å ç”¨æ˜¯ä»¥è¾ƒæ…¢çš„æ”¶æ•›é€Ÿåº¦å’Œç”±äºå…¶é¡ºåºæ›´æ–°ç‰¹æ€§è€Œå¯èƒ½å¯¼è‡´çš„ç¡¬ä»¶åˆ©ç”¨ç‡ä¸ä½³ä¸ºä»£ä»·çš„ã€‚
- en: 'TableÂ 8.3: **Optimizer Memory Footprint**: Different optimization algorithms
    impose varying memory costs due to the storage of intermediate values like gradients,
    velocities, and squared gradients; understanding these trade-offs is important
    for resource-constrained deployments and large-scale model training. Selecting
    an optimizer involves balancing convergence speed with available memory and computational
    resources.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.3ï¼š**ä¼˜åŒ–å™¨å†…å­˜å ç”¨**ï¼šä¸åŒçš„ä¼˜åŒ–ç®—æ³•ç”±äºå­˜å‚¨ä¸­é—´å€¼ï¼ˆå¦‚æ¢¯åº¦ã€é€Ÿåº¦å’Œå¹³æ–¹æ¢¯åº¦ï¼‰è€Œäº§ç”Ÿäº†ä¸åŒçš„å†…å­˜æˆæœ¬ï¼›ç†è§£è¿™äº›æƒè¡¡å¯¹äºèµ„æºå—é™çš„éƒ¨ç½²å’Œå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒéå¸¸é‡è¦ã€‚é€‰æ‹©ä¼˜åŒ–å™¨éœ€è¦å¹³è¡¡æ”¶æ•›é€Ÿåº¦ä¸å¯ç”¨å†…å­˜å’Œè®¡ç®—èµ„æºã€‚
- en: '| **Property** | **SGD** | **Momentum** | **RMSprop** | **Adam** |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **å±æ€§** | **SGD** | **åŠ¨é‡** | **RMSprop** | **Adam** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Memory Overhead** | None | Velocity terms | Squared gradients | Both velocity
    and squared gradients |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜å¼€é”€** | æ—  | é€Ÿåº¦é¡¹ | å¹³æ–¹æ¢¯åº¦ | é€Ÿåº¦å’Œå¹³æ–¹æ¢¯åº¦ |'
- en: '| **Memory Cost** | <semantics><mrow><mn>1</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">1\times</annotation></semantics> | <semantics><mrow><mn>2</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">2\times</annotation></semantics> | <semantics><mrow><mn>2</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">2\times</annotation></semantics> | <semantics><mrow><mn>3</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">3\times</annotation></semantics> |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜æˆæœ¬** | <semantics><mrow><mn>1</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">1\times</annotation></semantics>
    | <semantics><mrow><mn>2</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics>
    | <semantics><mrow><mn>2</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics>
    | <semantics><mrow><mn>3</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">3\times</annotation></semantics>
    |'
- en: '| **Access Pattern** | Sequential | Sequential | Random | Random |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| **è®¿é—®æ¨¡å¼** | é¡ºåº | é¡ºåº | éšæœº | éšæœº |'
- en: '| **Operations/Parameter** | 2 | 3 | 4 | 5 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| **æ“ä½œ/å‚æ•°** | 2 | 3 | 4 | 5 |'
- en: '| **Hardware Efficiency** | Low | Medium | High | Highest |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| **ç¡¬ä»¶æ•ˆç‡** | ä½ | ä¸­ç­‰ | é«˜ | æœ€é«˜ |'
- en: '| **Convergence Speed** | Slowest | Medium | Fast | Fastest |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **æ”¶æ•›é€Ÿåº¦** | æœ€æ…¢ | ä¸­ç­‰ | å¿« | æœ€å¿« |'
- en: Momentum methods introduce additional memory requirements by storing velocity
    terms for each parameter, doubling the memory footprint compared to SGD. This
    increased memory cost brings improved convergence through better gradient estimation,
    while maintaining relatively efficient memory access patterns. The sequential
    nature of momentum updates allows for effective hardware prefetching and cache
    utilization.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡æ–¹æ³•é€šè¿‡ä¸ºæ¯ä¸ªå‚æ•°å­˜å‚¨é€Ÿåº¦é¡¹ï¼Œå¼•å…¥é¢å¤–çš„å†…å­˜éœ€æ±‚ï¼Œä¸ SGD ç›¸æ¯”ï¼Œå†…å­˜å ç”¨ç¿»å€ã€‚è¿™ç§å¢åŠ çš„å†…å­˜æˆæœ¬é€šè¿‡æ›´å¥½çš„æ¢¯åº¦ä¼°è®¡å¸¦æ¥æ”¹è¿›çš„æ”¶æ•›ï¼ŒåŒæ—¶ä¿æŒç›¸å¯¹é«˜æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚åŠ¨é‡æ›´æ–°çš„é¡ºåºæ€§è´¨å…è®¸æœ‰æ•ˆçš„ç¡¬ä»¶é¢„å–å’Œç¼“å­˜åˆ©ç”¨ã€‚
- en: RMSprop adapts learning rates per parameter by tracking squared gradient statistics.
    Its memory overhead matches momentum methods, but its computation patterns become
    more irregular. The algorithm requires additional arithmetic operations for maintaining
    running averages and computing adaptive learning rates, increasing computational
    intensity from 3 to 4 operations per parameter.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop é€šè¿‡è·Ÿè¸ªå¹³æ–¹æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯æ¥ä¸ºæ¯ä¸ªå‚æ•°è°ƒæ•´å­¦ä¹ ç‡ã€‚å®ƒçš„å†…å­˜å¼€é”€ä¸åŠ¨é‡æ–¹æ³•ç›¸å½“ï¼Œä½†å…¶è®¡ç®—æ¨¡å¼å˜å¾—æ›´åŠ ä¸è§„åˆ™ã€‚è¯¥ç®—æ³•éœ€è¦é¢å¤–çš„ç®—æœ¯æ“ä½œæ¥ç»´æŠ¤è¿è¡Œå¹³å‡å€¼å’Œè®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå°†æ¯å‚æ•°çš„è®¡ç®—å¼ºåº¦ä»
    3 å¢åŠ åˆ° 4 æ¬¡æ“ä½œã€‚
- en: Adam combines the benefits of momentum and adaptive learning rates, but at the
    highest system resource cost. [TableÂ 8.3](ch014.xhtml#tbl-optimizer-properties)
    reveals that it maintains both velocity terms and squared gradient statistics,
    tripling the memory requirements compared to SGD. The algorithmâ€™s computational
    patterns involve 5 operations per parameter update, though these operations often
    utilize hardware more effectively due to their regular structure and potential
    for parallelization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Adam ç®—æ³•ç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜ç‚¹ï¼Œä½†ç³»ç»Ÿèµ„æºæˆæœ¬æœ€é«˜ã€‚[è¡¨ 8.3](ch014.xhtml#tbl-optimizer-properties)
    æ˜¾ç¤ºï¼Œå®ƒåŒæ—¶ç»´æŠ¤é€Ÿåº¦é¡¹å’Œå¹³æ–¹æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ SGD ç›¸æ¯”ï¼Œå†…å­˜éœ€æ±‚å¢åŠ äº†ä¸‰å€ã€‚è¯¥ç®—æ³•çš„è®¡ç®—æ¨¡å¼æ¶‰åŠæ¯ä¸ªå‚æ•°æ›´æ–°æ—¶çš„ 5 æ¬¡æ“ä½œï¼Œä½†ç”±äºè¿™äº›æ“ä½œçš„è§„åˆ™ç»“æ„å’Œå¹¶è¡ŒåŒ–æ½œåŠ›ï¼Œè¿™äº›æ“ä½œé€šå¸¸èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶ã€‚
- en: Training system designers must balance these trade-offs when selecting optimization
    strategies. Modern hardware architectures influence these decisions. GPUs excel
    at the parallel computations required by adaptive methods, while memory-constrained
    systems might favor simpler optimizers. The choice of optimizer affects not only
    training dynamics but also maximum feasible model size, achievable batch size,
    hardware utilization efficiency, and overall training time to convergence. Beyond
    optimizer selection, learning rate scheduling strategies, including cosine annealing,
    linear warmup, and cyclical schedules, further influence convergence behavior
    and final model performance, with large-batch training requiring careful scaling
    adjustments as detailed in distributed training discussions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©ä¼˜åŒ–ç­–ç•¥æ—¶ï¼Œè®­ç»ƒç³»ç»Ÿè®¾è®¡è€…å¿…é¡»å¹³è¡¡è¿™äº›æƒè¡¡ã€‚ç°ä»£ç¡¬ä»¶æ¶æ„å½±å“äº†è¿™äº›å†³ç­–ã€‚GPU åœ¨è‡ªé€‚åº”æ–¹æ³•æ‰€éœ€çš„å¹¶è¡Œè®¡ç®—æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œå†…å­˜å—é™çš„ç³»ç»Ÿå¯èƒ½æ›´å€¾å‘äºç®€å•çš„ä¼˜åŒ–å™¨ã€‚ä¼˜åŒ–å™¨çš„é€‰æ‹©ä¸ä»…å½±å“è®­ç»ƒåŠ¨æ€ï¼Œè¿˜å½±å“æœ€å¤§å¯è¡Œæ¨¡å‹å¤§å°ã€å¯å®ç°çš„æ‰¹é‡å¤§å°ã€ç¡¬ä»¶åˆ©ç”¨æ•ˆç‡ä»¥åŠæ•´ä½“è®­ç»ƒæ—¶é—´åˆ°æ”¶æ•›ã€‚
- en: Modern training frameworks continue to evolve, developing techniques like optimizer
    state sharding, mixed-precision storage, and fused operations to better balance
    these competing demands. Understanding these system implications helps practitioners
    make informed decisions about optimization strategies based on their specific
    hardware constraints and training requirements.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£è®­ç»ƒæ¡†æ¶æŒç»­å‘å±•ï¼Œå¼€å‘å‡ºè¯¸å¦‚ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ã€æ··åˆç²¾åº¦å­˜å‚¨å’Œèåˆæ“ä½œç­‰æŠ€æœ¯ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡è¿™äº›ç«äº‰éœ€æ±‚ã€‚ç†è§£è¿™äº›ç³»ç»Ÿå½±å“æœ‰åŠ©äºä»ä¸šè€…æ ¹æ®å…¶ç‰¹å®šçš„ç¡¬ä»¶çº¦æŸå’Œè®­ç»ƒéœ€æ±‚ï¼Œåšå‡ºå…³äºä¼˜åŒ–ç­–ç•¥çš„æ˜æ™ºå†³ç­–ã€‚
- en: Framework Optimizer Interface
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¡†æ¶ä¼˜åŒ–å™¨æ¥å£
- en: While the mathematical formulations of SGD, momentum, and Adam establish the
    theoretical foundations for parameter optimization, frameworks provide standardized
    interfaces that abstract these algorithms into practical training loops. Understanding
    how frameworks like PyTorch implement optimizer APIs demonstrates how complex
    mathematical operations become accessible through clean abstractions.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶SGDã€åŠ¨é‡å’ŒAdamçš„æ•°å­¦å…¬å¼ä¸ºå‚æ•°ä¼˜åŒ–å»ºç«‹äº†ç†è®ºåŸºç¡€ï¼Œä½†æ¡†æ¶æä¾›äº†æ ‡å‡†åŒ–çš„æ¥å£ï¼Œå°†è¿™äº›ç®—æ³•æŠ½è±¡ä¸ºå®ç”¨çš„è®­ç»ƒå¾ªç¯ã€‚ç†è§£åƒPyTorchè¿™æ ·çš„æ¡†æ¶å¦‚ä½•å®ç°ä¼˜åŒ–å™¨APIï¼Œå±•ç¤ºäº†å¤æ‚æ•°å­¦è¿ç®—å¦‚ä½•é€šè¿‡å¹²å‡€çš„æŠ½è±¡å˜å¾—å¯è®¿é—®ã€‚
- en: The framework optimizer interface follows a consistent pattern that separates
    gradient computation from parameter updates. This separation enables the mathematical
    algorithms to be applied systematically across diverse model architectures and
    training scenarios.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶ä¼˜åŒ–å™¨æ¥å£éµå¾ªä¸€ç§ä¸€è‡´çš„æ¨¡å¼ï¼Œå°†æ¢¯åº¦è®¡ç®—ä¸å‚æ•°æ›´æ–°åˆ†ç¦»ã€‚è¿™ç§åˆ†ç¦»ä½¿å¾—æ•°å­¦ç®—æ³•å¯ä»¥ç³»ç»Ÿæ€§åœ°åº”ç”¨äºä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒåœºæ™¯ã€‚
- en: 'Framework optimizers implement a four-step training cycle that encapsulates
    the mathematical operations within a clean API. The following example demonstrates
    how Adam optimization integrates into a standard training loop:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶ä¼˜åŒ–å™¨å®ç°äº†ä¸€ä¸ªå››æ­¥è®­ç»ƒå‘¨æœŸï¼Œå°†æ•°å­¦è¿ç®—å°è£…åœ¨ä¸€ä¸ªå¹²å‡€çš„APIä¸­ã€‚ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºäº†Adamä¼˜åŒ–å¦‚ä½•é›†æˆåˆ°æ ‡å‡†è®­ç»ƒå¾ªç¯ä¸­ï¼š
- en: '[PRE1]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `optimizer.zero_grad()` call addresses a critical framework implementation
    detail: gradients accumulate across calls to `backward()`, requiring explicit
    clearing between batches. This behavior enables gradient accumulation patterns
    for large effective batch sizes but requires careful management in standard training
    loops.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimizer.zero_grad()` è°ƒç”¨è§£å†³äº†æ¡†æ¶å®ç°çš„ä¸€ä¸ªå…³é”®ç»†èŠ‚ï¼šæ¢¯åº¦åœ¨å¤šæ¬¡è°ƒç”¨ `backward()` ä¹‹é—´ç´¯ç§¯ï¼Œéœ€è¦åœ¨æ‰¹æ¬¡ä¹‹é—´æ˜¾å¼æ¸…é™¤ã€‚è¿™ç§è¡Œä¸ºä½¿å¾—å¯ä»¥é’ˆå¯¹å¤§å‹æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å®ç°æ¢¯åº¦ç´¯ç§¯æ¨¡å¼ï¼Œä½†åœ¨æ ‡å‡†è®­ç»ƒå¾ªç¯ä¸­éœ€è¦ä»”ç»†ç®¡ç†ã€‚'
- en: 'The `optimizer.step()` method encapsulates the mathematical update equations.
    For Adam optimization, this single call implements the momentum estimation, squared
    gradient tracking, bias correction, and parameter update computation automatically.
    The following code illustrates the mathematical operations that occur within the
    optimizer:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimizer.step()` æ–¹æ³•å°è£…äº†æ•°å­¦æ›´æ–°æ–¹ç¨‹ã€‚å¯¹äºAdamä¼˜åŒ–ï¼Œè¿™ä¸ªå•ä¸€è°ƒç”¨è‡ªåŠ¨å®ç°äº†åŠ¨é‡ä¼°è®¡ã€å¹³æ–¹æ¢¯åº¦è·Ÿè¸ªã€åå·®æ ¡æ­£å’Œå‚æ•°æ›´æ–°è®¡ç®—ã€‚ä»¥ä¸‹ä»£ç è¯´æ˜äº†ä¼˜åŒ–å™¨å†…éƒ¨å‘ç”Ÿçš„æ•°å­¦è¿ç®—ï¼š'
- en: '[PRE2]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Framework implementations also handle the memory management challenges in optimizer
    trade-offs. The optimizer automatically allocates storage for momentum terms and
    squared gradient statistics, managing the 2-3x memory overhead transparently while
    providing efficient memory access patterns optimized for the underlying hardware.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶å®ç°è¿˜å¤„ç†ä¼˜åŒ–å™¨æƒè¡¡ä¸­çš„å†…å­˜ç®¡ç†æŒ‘æˆ˜ã€‚ä¼˜åŒ–å™¨è‡ªåŠ¨ä¸ºåŠ¨é‡é¡¹å’Œå¹³æ–¹æ¢¯åº¦ç»Ÿè®¡åˆ†é…å­˜å‚¨ç©ºé—´ï¼Œé€æ˜åœ°ç®¡ç†2-3å€çš„å†…å­˜å¼€é”€ï¼ŒåŒæ—¶æä¾›é’ˆå¯¹åº•å±‚ç¡¬ä»¶ä¼˜åŒ–çš„é«˜æ•ˆå†…å­˜è®¿é—®æ¨¡å¼ã€‚
- en: Learning Rate Scheduling Integration
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡è°ƒåº¦å™¨é›†æˆ
- en: Frameworks integrate learning rate scheduling directly into the optimizer interface,
    enabling dynamic adjustment of the learning rate Î± during training. This integration
    demonstrates how frameworks compose multiple optimization techniques through modular
    design patterns.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶å°†å­¦ä¹ ç‡è°ƒåº¦ç›´æ¥é›†æˆåˆ°ä¼˜åŒ–å™¨æ¥å£ä¸­ï¼Œä½¿è®­ç»ƒæœŸé—´åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ Î± æˆä¸ºå¯èƒ½ã€‚è¿™ç§é›†æˆå±•ç¤ºäº†æ¡†æ¶å¦‚ä½•é€šè¿‡æ¨¡å—åŒ–è®¾è®¡æ¨¡å¼ç»„åˆå¤šä¸ªä¼˜åŒ–æŠ€æœ¯ã€‚
- en: 'Learning rate schedulers modify the optimizerâ€™s learning rate according to
    predefined schedules, such as cosine annealing, exponential decay, or step-wise
    reductions. The following example demonstrates how to integrate cosine annealing
    with Adam optimization:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡è°ƒåº¦å™¨æ ¹æ®é¢„å®šä¹‰çš„è®¡åˆ’ä¿®æ”¹ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ï¼Œä¾‹å¦‚ä½™å¼¦é€€ç«ã€æŒ‡æ•°è¡°å‡æˆ–é˜¶æ¢¯å¼å‡å°‘ã€‚ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•å°†ä½™å¼¦é€€ç«ä¸Adamä¼˜åŒ–é›†æˆï¼š
- en: '[PRE3]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This composition pattern allows practitioners to combine base optimization algorithms
    (SGD, Adam) with scheduling strategies (cosine annealing, linear warmup) without
    modifying the core mathematical implementations. The framework handles the coordination
    between components while maintaining the mathematical properties of each algorithm.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç»„åˆæ¨¡å¼å…è®¸å®è·µè€…å°†åŸºç¡€ä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Adamï¼‰ä¸è°ƒåº¦ç­–ç•¥ï¼ˆä½™å¼¦é€€ç«ã€çº¿æ€§é¢„çƒ­ï¼‰ç»“åˆèµ·æ¥ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒæ•°å­¦å®ç°ã€‚æ¡†æ¶å¤„ç†ç»„ä»¶ä¹‹é—´çš„åè°ƒï¼ŒåŒæ—¶ä¿æŒæ¯ä¸ªç®—æ³•çš„æ•°å­¦å±æ€§ã€‚
- en: The optimizer interface exemplifies how frameworks balance mathematical rigor
    with practical usability. The underlying algorithms implement the precise mathematical
    formulations we studied, while the API design enables practitioners to focus on
    model architecture and training dynamics rather than optimization implementation
    details.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨æ¥å£å±•ç¤ºäº†æ¡†æ¶å¦‚ä½•åœ¨æ•°å­¦ä¸¥è°¨æ€§å’Œå®ç”¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åº•å±‚ç®—æ³•å®ç°äº†æˆ‘ä»¬ç ”ç©¶çš„ç²¾ç¡®æ•°å­¦å…¬å¼ï¼Œè€ŒAPIè®¾è®¡ä½¿å¾—å®è·µè€…å¯ä»¥ä¸“æ³¨äºæ¨¡å‹æ¶æ„å’Œè®­ç»ƒåŠ¨æ€ï¼Œè€Œä¸æ˜¯ä¼˜åŒ–å®ç°çš„ç»†èŠ‚ã€‚
- en: Backpropagation Mechanics
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸç†
- en: The backpropagation algorithm[21](#fn21) computes gradients by systematically
    moving backward through a neural networkâ€™s computational graph. While earlier
    discussions introduced backpropagationâ€™s mathematical principles, implementing
    this algorithm in training systems requires careful management of memory, computation,
    and data flow.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­ç®—æ³•[21](#fn21)é€šè¿‡ç³»ç»Ÿåœ°éå†ç¥ç»ç½‘ç»œçš„è®¡ç®—å›¾æ¥è®¡ç®—æ¢¯åº¦ã€‚è™½ç„¶ä¹‹å‰çš„è®¨è®ºä»‹ç»äº†åå‘ä¼ æ’­çš„æ•°å­¦åŸç†ï¼Œä½†åœ¨è®­ç»ƒç³»ç»Ÿä¸­å®ç°æ­¤ç®—æ³•éœ€è¦ä»”ç»†ç®¡ç†å†…å­˜ã€è®¡ç®—å’Œæ•°æ®æµã€‚
- en: Backpropagation Algorithm Mechanics
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­ç®—æ³•åŸç†
- en: Neural networks learn by adjusting their parameters to reduce errors through
    the backpropagation algorithm, which computes how much each parameter contributed
    to the error by systematically moving backward through the networkâ€™s computational
    graph.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œé€šè¿‡è°ƒæ•´å…¶å‚æ•°æ¥å‡å°‘è¯¯å·®ï¼Œè¿™æ˜¯é€šè¿‡åå‘ä¼ æ’­ç®—æ³•å®ç°çš„ï¼Œè¯¥ç®—æ³•é€šè¿‡ç³»ç»Ÿåœ°éå†ç½‘ç»œçš„è®¡ç®—å›¾æ¥è®¡ç®—æ¯ä¸ªå‚æ•°å¯¹è¯¯å·®çš„è´¡çŒ®ã€‚
- en: 'During the forward pass, each layer performs computations and produces activations
    that must be stored for the backward pass: <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\ a^{(l)} = f(z^{(l)}) \end{gather*}</annotation></semantics>
    where <semantics><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">z^{(l)}</annotation></semantics>
    represents the pre-activation values and <semantics><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">a^{(l)}</annotation></semantics> represents the activations
    at layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>.
    The storage of these intermediate values creates specific memory requirements
    that scale with network depth and batch size.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€å±‚æ‰§è¡Œè®¡ç®—å¹¶äº§ç”Ÿæ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼å¿…é¡»è¢«å­˜å‚¨ä»¥ä¾›åå‘ä¼ æ’­ä½¿ç”¨ï¼š<semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\ a^{(l)} = f(z^{(l)}) \end{gather*}</annotation></semantics>
    å…¶ä¸­ <semantics><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">z^{(l)}</annotation></semantics>
    è¡¨ç¤ºå‰æ¿€æ´»å€¼ï¼Œè€Œ <semantics><msup><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">a^{(l)}</annotation></semantics>
    è¡¨ç¤ºåœ¨ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„æ¿€æ´»å€¼ã€‚è¿™äº›ä¸­é—´å€¼çš„å­˜å‚¨åˆ›å»ºäº†ç‰¹å®šçš„å†…å­˜éœ€æ±‚ï¼Œè¿™äº›éœ€æ±‚éšç€ç½‘ç»œæ·±åº¦å’Œæ‰¹é‡å¤§å°è€Œæ‰©å±•ã€‚'
- en: 'The backward pass computes gradients by applying the chain rule, starting from
    the networkâ€™s output and moving toward the input: <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>z</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>âŠ™</mo><mi>f</mi><mi>â€²</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>z</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><msup><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">)</mo><mi>T</mi></msup></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    \frac{\partial L}{\partial z^{(l)}}=\frac{\partial L}{\partial a^{(l)}} \odot
    f''(z^{(l)}) \\ \frac{\partial L}{\partial W^{(l)}}=\frac{\partial L}{\partial
    z^{(l)}}\big(a^{(l-1)}\big)^T \end{gather*}</annotation></semantics>'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'For a network with parameters <semantics><msub><mi>W</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">W_i</annotation></semantics> at each layer, computing
    <semantics><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msub><mi>W</mi><mi>i</mi></msub></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{\partial L}{\partial W_i}</annotation></semantics>
    determines how much the loss L changes when adjusting each parameter. The chain
    rule provides a systematic way to organize these computations: <semantics><mrow><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>f</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>i</mi></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>i</mi></msub></mrow></mfrac><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>i</mi></msub></mrow></mfrac><mi>.</mi><mi>.</mi><mi>.</mi><mfrac><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>n</mi></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>n</mi></msub></mrow></mfrac><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>f</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>n</mi></msub></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L_{full}}{\partial L_{i}}
    = \frac{\partial A_{i}}{\partial L_{i}} \frac{\partial L_{i+1}}{\partial A_{i}}
    ... \frac{\partial A_{n}}{\partial L_{n}} \frac{\partial L_{full}}{\partial A_{n}}</annotation></semantics>'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸€å±‚å…·æœ‰å‚æ•° <semantics><msub><mi>W</mi><mi>i</mi></msub><annotation encoding="application/x-tex">W_i</annotation></semantics>
    çš„ç½‘ç»œï¼Œè®¡ç®— <semantics><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msub><mi>W</mi><mi>i</mi></msub></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{\partial L}{\partial W_i}</annotation></semantics>
    ç¡®å®šäº†è°ƒæ•´æ¯ä¸ªå‚æ•°æ—¶æŸå¤± L çš„å˜åŒ–é‡ã€‚é“¾å¼æ³•åˆ™æä¾›äº†ä¸€ç§ç»„ç»‡è¿™äº›è®¡ç®—çš„ç³»ç»Ÿæ–¹æ³•ï¼š<semantics><mrow><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>f</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>i</mi></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>i</mi></msub></mrow></mfrac><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>i</mi></msub></mrow></mfrac><mi>.</mi><mi>.</mi><mi>.</mi><mfrac><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>n</mi></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mi>n</mi></msub></mrow></mfrac><mfrac><mrow><mi>âˆ‚</mi><msub><mi>L</mi><mrow><mi>f</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub></mrow><mrow><mi>âˆ‚</mi><msub><mi>A</mi><mi>n</mi></msub></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L_{full}}{\partial L_{i}}
    = \frac{\partial A_{i}}{\partial L_{i}} \frac{\partial L_{i+1}}{\partial A_{i}}
    ... \frac{\partial A_{n}}{\partial L_{n}} \frac{\partial L_{full}}{\partial A_{n}}</annotation></semantics>
- en: This equation reveals key requirements for training systems. Computing gradients
    for early layers requires information from all later layers, creating specific
    patterns in data storage and access. Each gradient computation requires access
    to stored activations from the forward pass, creating a specific pattern of memory
    access and computation that training systems must manage efficiently. These patterns
    directly influence the efficiency of optimization algorithms like SGD or Adam
    discussed earlier. Modern training systems use autodifferentiation[22](#fn22)
    to handle these computations automatically, but the underlying system requirements
    remain the same.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹æ­ç¤ºäº†è®­ç»ƒç³»ç»Ÿçš„å…³é”®è¦æ±‚ã€‚è®¡ç®—æ—©æœŸå±‚çš„æ¢¯åº¦éœ€è¦æ‰€æœ‰åç»­å±‚çš„ä¿¡æ¯ï¼Œä»è€Œåœ¨æ•°æ®å­˜å‚¨å’Œè®¿é—®ä¸­åˆ›å»ºç‰¹å®šçš„æ¨¡å¼ã€‚æ¯æ¬¡æ¢¯åº¦è®¡ç®—éƒ½éœ€è¦è®¿é—®æ­£å‘ä¼ æ’­ä¸­å­˜å‚¨çš„æ¿€æ´»å€¼ï¼Œä»è€Œåœ¨è®­ç»ƒç³»ç»Ÿä¸­åˆ›å»ºç‰¹å®šçš„å†…å­˜è®¿é—®å’Œè®¡ç®—æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¿…é¡»è¢«é«˜æ•ˆåœ°ç®¡ç†ã€‚è¿™äº›æ¨¡å¼ç›´æ¥å½±å“å‰é¢è®¨è®ºçš„
    SGD æˆ– Adam ç­‰ä¼˜åŒ–ç®—æ³•çš„æ•ˆç‡ã€‚ç°ä»£è®­ç»ƒç³»ç»Ÿä½¿ç”¨è‡ªåŠ¨å¾®åˆ†[22](#fn22)æ¥è‡ªåŠ¨å¤„ç†è¿™äº›è®¡ç®—ï¼Œä½†åº•å±‚ç³»ç»Ÿè¦æ±‚ä¿æŒä¸å˜ã€‚
- en: Activation Memory Requirements
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¿€æ´»å†…å­˜éœ€æ±‚
- en: 'Training systems must maintain intermediate values (activations) from the forward
    pass to compute gradients during the backward pass. This requirement compounds
    the memory demands of optimization algorithms. For each layer l, the system must
    store:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿå¿…é¡»åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ç»´æŠ¤ä¸­é—´å€¼ï¼ˆæ¿€æ´»å€¼ï¼‰ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—æ¢¯åº¦ã€‚è¿™ä¸€è¦æ±‚å¢åŠ äº†ä¼˜åŒ–ç®—æ³•çš„å†…å­˜éœ€æ±‚ã€‚å¯¹äºæ¯ä¸€å±‚ lï¼Œç³»ç»Ÿå¿…é¡»å­˜å‚¨ï¼š
- en: Input activations from the forward pass
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£å‘ä¼ æ’­çš„è¾“å…¥æ¿€æ´»
- en: Output activations after applying layer operations
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åº”ç”¨å±‚æ“ä½œåçš„è¾“å‡ºæ¿€æ´»
- en: Layer parameters being optimized
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£åœ¨ä¼˜åŒ–çš„å±‚å‚æ•°
- en: Computed gradients for parameter updates
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºå‚æ•°æ›´æ–°çš„è®¡ç®—æ¢¯åº¦
- en: 'Consider a batch of training examples passing through a network. The forward
    pass computes and stores: <semantics><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\ a^{(l)} = f(z^{(l)}) \end{gather*}</annotation></semantics>'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 'è€ƒè™‘ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹æ‰¹æ¬¡é€šè¿‡ç½‘ç»œã€‚æ­£å‘ä¼ æ’­è®¡ç®—å¹¶å­˜å‚¨ï¼š<semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>a</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>z</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\ a^{(l)} = f(z^{(l)}) \end{gather*}</annotation></semantics>'
- en: 'Both <semantics><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">z^{(l)}</annotation></semantics>
    and <semantics><msup><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">a^{(l)}</annotation></semantics>
    must be cached for the backward pass. This creates a multiplicative effect on
    memory usage: each layerâ€™s memory requirement is multiplied by the batch size,
    and the optimizerâ€™s memory overhead (discussed in the previous section) applies
    to each parameter.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ª <semantics><msup><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">z^{(l)}</annotation></semantics>
    å’Œ <semantics><msup><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">a^{(l)}</annotation></semantics>
    éƒ½å¿…é¡»åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è¿›è¡Œç¼“å­˜ã€‚è¿™ä¼šåœ¨å†…å­˜ä½¿ç”¨ä¸Šäº§ç”Ÿä¹˜æ³•æ•ˆåº”ï¼šæ¯ä¸€å±‚çš„å†…å­˜éœ€æ±‚éƒ½ä¼šä¹˜ä»¥æ‰¹é‡å¤§å°ï¼Œå¹¶ä¸”ä¼˜åŒ–å™¨çš„å†…å­˜å¼€é”€ï¼ˆåœ¨ä¸Šä¸€èŠ‚ä¸­è®¨è®ºï¼‰é€‚ç”¨äºæ¯ä¸ªå‚æ•°ã€‚
- en: 'The total memory needed scales with:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å†…å­˜éœ€æ±‚ä¸ä»¥ä¸‹å› ç´ æˆæ¯”ä¾‹ï¼š
- en: Network depth (number of layers)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ·±åº¦ï¼ˆå±‚æ•°ï¼‰
- en: Layer widths (number of parameters per layer)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±‚å®½åº¦ï¼ˆæ¯å±‚çš„å‚æ•°æ•°é‡ï¼‰
- en: Batch size (number of examples processed together)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°ï¼ˆä¸€èµ·å¤„ç†çš„ç¤ºä¾‹æ•°é‡ï¼‰
- en: Optimizer state (additional memory for algorithms like Adam)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚Adamç­‰ç®—æ³•çš„é¢å¤–å†…å­˜ï¼‰
- en: This creates a complex set of trade-offs. Larger batch sizes enable more efficient
    computation and better gradient estimates for optimization, but require proportionally
    more memory for storing activations. More sophisticated optimizers like Adam can
    achieve faster convergence but require additional memory per parameter.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº§ç”Ÿäº†ä¸€ç³»åˆ—å¤æ‚çš„æƒè¡¡ã€‚è¾ƒå¤§çš„æ‰¹é‡å¤§å°å¯ä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œè®¡ç®—å¹¶è·å¾—æ›´å¥½çš„æ¢¯åº¦ä¼°è®¡ï¼Œä½†éœ€è¦æŒ‰æ¯”ä¾‹æ›´å¤šçš„å†…å­˜æ¥å­˜å‚¨æ¿€æ´»ã€‚æ›´å¤æ‚çš„ä¼˜åŒ–å™¨ï¼Œå¦‚Adamï¼Œå¯ä»¥å®ç°æ›´å¿«çš„æ”¶æ•›ï¼Œä½†æ¯ä¸ªå‚æ•°éƒ½éœ€è¦é¢å¤–çš„å†…å­˜ã€‚
- en: '**GPT-2 Activation Memory Breakdown**'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 æ¿€æ´»å†…å­˜åˆ†è§£**'
- en: 'For GPT-2 with batch_size=32, seq_len=1024, hidden_dim=1280, 48 layers:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰¹é‡å¤§å°ä¸º32ï¼Œåºåˆ—é•¿åº¦ä¸º1024ï¼Œéšè—ç»´åº¦ä¸º1280ï¼Œ48å±‚çš„GPT-2ï¼š
- en: '**Per-Layer Activation Memory**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¯å±‚æ¿€æ´»å†…å­˜**'
- en: 'Attention activations: `batch Ã— seq Ã— hidden Ã— 4` (Q, K, V, output) = 32 Ã—
    1024 Ã— 1280 Ã— 4 Ã— 2 bytes (FP16) = 335 MB'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ¿€æ´»ï¼š`batch Ã— seq Ã— hidden Ã— 4`ï¼ˆQ, K, V, è¾“å‡ºï¼‰= 32 Ã— 1024 Ã— 1280 Ã— 4 Ã— 2å­—èŠ‚ï¼ˆFP16ï¼‰=
    335 MB
- en: 'FFN activations: `batch Ã— seq Ã— (hidden Ã— 4)` (intermediate expansion) = 32
    Ã— 1024 Ã— 5120 Ã— 2 bytes = 335 MB'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFNæ¿€æ´»ï¼š`batch Ã— seq Ã— (hidden Ã— 4)`ï¼ˆä¸­é—´æ‰©å±•ï¼‰= 32 Ã— 1024 Ã— 5120 Ã— 2å­—èŠ‚ = 335 MB
- en: 'Layer norm states: Minimal (~10 MB per layer)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å±‚å½’ä¸€åŒ–çŠ¶æ€ï¼šæœ€å°ï¼ˆæ¯å±‚çº¦10 MBï¼‰
- en: 'Total per layer: ~680 MB'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯å±‚çš„æ€»é‡ï¼š~680 MB
- en: '**Full Model Activation Memory**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…¨æ¨¡å‹æ¿€æ´»å†…å­˜**'
- en: 48 layers Ã— 680 MB = **32.6 GB** just for activations
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 48å±‚ Ã— 680 MB = **32.6 GB** ä»…ç”¨äºæ¿€æ´»
- en: 'Parameters (FP16): 3 GB'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚æ•°ï¼ˆFP16ï¼‰ï¼š3 GB
- en: 'Gradients: 3 GB'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ï¼š3 GB
- en: 'Optimizer state (Adam, FP32): 12 GB'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdam, FP32ï¼‰ï¼š12 GB
- en: 'Peak memory during training: **~51 GB**'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæœŸé—´çš„å³°å€¼å†…å­˜ï¼š**~51 GB**
- en: This exceeds a single V100â€™s 32GB capacity.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¶…è¿‡äº†å•ä¸ªV100çš„32GBå®¹é‡ã€‚
- en: '**System Solutions Applied**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç³»ç»Ÿè§£å†³æ–¹æ¡ˆåº”ç”¨**'
- en: 'Gradient checkpointing: Recompute activations during backward pass, reducing
    activation memory by 75% (to ~8 GB) at cost of 33% more compute'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šåœ¨åå‘ä¼ æ’­æœŸé—´é‡æ–°è®¡ç®—æ¿€æ´»ï¼Œé€šè¿‡å¢åŠ 33%çš„è®¡ç®—æˆæœ¬ï¼Œå°†æ¿€æ´»å†…å­˜å‡å°‘75%ï¼ˆè‡³çº¦8 GBï¼‰
- en: 'Activation CPU offloading: Store some activations in CPU RAM, transfer during
    backward pass'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¿€æ´»CPUå¸è½½ï¼šå°†ä¸€äº›æ¿€æ´»å­˜å‚¨åœ¨CPU RAMä¸­ï¼Œåœ¨åå‘ä¼ æ’­æœŸé—´è¿›è¡Œä¼ è¾“
- en: 'Mixed precision: FP16 activations (already applied above) vs FP32 (would be
    65 GB)'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦ï¼šFP16æ¿€æ´»ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰ä¸FP32ï¼ˆå°†æ˜¯65 GBï¼‰
- en: 'Reduced batch size: Use batch_size=16 per GPU + gradient accumulation over
    2 steps = effective batch_size=32'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡å°‘æ‰¹é‡å¤§å°ï¼šä½¿ç”¨æ¯ä¸ªGPUçš„batch_size=16 + 2æ­¥çš„æ¢¯åº¦ç´¯ç§¯ = æœ‰æ•ˆæ‰¹é‡å¤§å°=32
- en: '**Training Configuration:** Most GPT-2 implementations use gradient checkpointing
    + batch_size=16 per GPU, fitting comfortably in 32GB V100s while maintaining training
    efficiency.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒé…ç½®**ï¼šå¤§å¤šæ•°GPT-2å®ç°ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ + æ¯ä¸ªGPUçš„batch_size=16ï¼Œèˆ’é€‚åœ°é€‚åº”32GB V100sï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ•ˆç‡ã€‚'
- en: Memory-Computation Trade-offs
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜-è®¡ç®—æƒè¡¡
- en: 'Training systems must balance memory usage against computational efficiency.
    Each forward pass through the network generates a set of activations that must
    be stored for the backward pass. For a neural network with <semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics> layers, processing a batch
    of <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    examples requires storing: <semantics><mrow><mtext mathvariant="normal">Memory
    per batch</mtext><mo>=</mo><mi>B</mi><mo>Ã—</mo><munderover><mo>âˆ‘</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>s</mi><mi>l</mi></msub><mo>+</mo><msub><mi>a</mi><mi>l</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{Memory
    per batch} = B \times \sum_{l=1}^L (s_l + a_l)</annotation></semantics> where
    <semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding="application/x-tex">s_l</annotation></semantics>
    represents the size of intermediate computations (like <semantics><msup><mi>z</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">z^{(l)}</annotation></semantics>) and <semantics><msub><mi>a</mi><mi>l</mi></msub><annotation
    encoding="application/x-tex">a_l</annotation></semantics> represents the activation
    outputs at layer l.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç³»ç»Ÿå¿…é¡»åœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚ç½‘ç»œä¸­çš„æ¯ä¸€å‰å‘ä¼ é€’éƒ½ä¼šç”Ÿæˆä¸€ç»„æ¿€æ´»ï¼Œè¿™äº›æ¿€æ´»å¿…é¡»å­˜å‚¨ä»¥ä¾›åå‘ä¼ é€’ä½¿ç”¨ã€‚å¯¹äºä¸€ä¸ªå…·æœ‰<semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics>å±‚çš„ç¥ç»ç½‘ç»œï¼Œå¤„ç†ä¸€ä¸ªåŒ…å«<semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics>ä¸ªç¤ºä¾‹çš„æ‰¹é‡éœ€è¦å­˜å‚¨ï¼š<semantics><mrow><mtext
    mathvariant="normal">æ¯æ‰¹æ¬¡çš„å†…å­˜</mtext><mo>=</mo><mi>B</mi><mo>Ã—</mo><munderover><mo>âˆ‘</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>s</mi><mi>l</mi></msub><mo>+</mo><msub><mi>a</mi><mi>l</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{Memory
    per batch} = B \times \sum_{l=1}^L (s_l + a_l)</annotation></semantics> å…¶ä¸­ <semantics><msub><mi>s</mi><mi>l</mi></msub><annotation
    encoding="application/x-tex">s_l</annotation></semantics> è¡¨ç¤ºä¸­é—´è®¡ç®—çš„å¤§å°ï¼ˆå¦‚ <semantics><msup><mi>z</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">z^{(l)}</annotation></semantics>ï¼‰å’Œ <semantics><msub><mi>a</mi><mi>l</mi></msub><annotation
    encoding="application/x-tex">a_l</annotation></semantics> è¡¨ç¤ºç¬¬lå±‚çš„æ¿€æ´»è¾“å‡ºã€‚
- en: 'This memory requirement compounds with the optimizerâ€™s memory needs discussed
    in the previous section. The total memory consumption of a training system includes
    both the stored activations and the optimizer state: <semantics><mrow><mtext mathvariant="normal">Total
    Memory</mtext><mo>=</mo><mtext mathvariant="normal">Memory per batch</mtext><mo>+</mo><msub><mtext
    mathvariant="normal">Memory</mtext><mtext mathvariant="normal">optimizer</mtext></msub></mrow>
    <annotation encoding="application/x-tex">\text{Total Memory} = \text{Memory per
    batch} + \text{Memory}_{\text{optimizer}}</annotation></semantics>'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å†…å­˜éœ€æ±‚ä¸ä¸Šä¸€èŠ‚ä¸­è®¨è®ºçš„ä¼˜åŒ–å™¨çš„å†…å­˜éœ€æ±‚ç›¸å åŠ ã€‚è®­ç»ƒç³»ç»Ÿçš„æ€»å†…å­˜æ¶ˆè€—åŒ…æ‹¬å­˜å‚¨çš„æ¿€æ´»å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼š<semantics><mrow><mtext mathvariant="normal">æ€»å†…å­˜</mtext><mo>=</mo><mtext
    mathvariant="normal">æ¯æ‰¹æ¬¡çš„å†…å­˜</mtext><mo>+</mo><msub><mtext mathvariant="normal">å†…å­˜</mtext><mtext
    mathvariant="normal">ä¼˜åŒ–å™¨</mtext></msub></mrow> <annotation encoding="application/x-tex">\text{æ€»å†…å­˜}
    = \text{æ¯æ‰¹æ¬¡çš„å†…å­˜} + \text{ä¼˜åŒ–å™¨å†…å­˜}</annotation></semantics>
- en: To manage these substantial memory requirements, training systems use several
    sophisticated strategies. Gradient checkpointing is a basic approach, strategically
    recomputing some intermediate values during the backward pass rather than storing
    them. While this increases computational work, it can significantly reduce memory
    usage, enabling training of deeper networks or larger batch sizes on memory-constrained
    hardware ([T. Chen et al. 2016](ch058.xhtml#ref-chen2016training)).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®¡ç†è¿™äº›å·¨å¤§çš„å†…å­˜éœ€æ±‚ï¼Œè®­ç»ƒç³»ç»Ÿä½¿ç”¨äº†å‡ ç§å¤æ‚çš„ç­–ç•¥ã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹æ˜¯ä¸€ç§åŸºæœ¬æ–¹æ³•ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°é‡æ–°è®¡ç®—ä¸€äº›ä¸­é—´å€¼ï¼Œè€Œä¸æ˜¯å­˜å‚¨å®ƒä»¬ã€‚è™½ç„¶è¿™å¢åŠ äº†è®¡ç®—å·¥ä½œé‡ï¼Œä½†å®ƒå¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä»è€Œåœ¨å†…å­˜å—é™çš„ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´æ·±çš„ç½‘ç»œæˆ–æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼ˆ[T.
    Chen ç­‰äºº 2016](ch058.xhtml#ref-chen2016training)ï¼‰ã€‚
- en: The efficiency of these memory management strategies depends heavily on the
    underlying hardware architecture. GPU systems, with their high computational throughput
    but limited memory bandwidth, often encounter different bottlenecks than CPU systems.
    Memory bandwidth limitations on GPUs mean that even when sufficient storage exists,
    moving data between memory and compute units can become the primary performance
    constraint ([Norman P. Jouppi et al. 2017b](ch058.xhtml#ref-jouppi2017tpu)).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å­˜ç®¡ç†ç­–ç•¥çš„æ•ˆç‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåº•å±‚ç¡¬ä»¶æ¶æ„ã€‚GPUç³»ç»Ÿï¼Œå°½ç®¡å…·æœ‰é«˜è®¡ç®—ååé‡ä½†å†…å­˜å¸¦å®½æœ‰é™ï¼Œé€šå¸¸ä¼šé‡åˆ°ä¸CPUç³»ç»Ÿä¸åŒçš„ç“¶é¢ˆã€‚GPUä¸Šçš„å†…å­˜å¸¦å®½é™åˆ¶æ„å‘³ç€å³ä½¿å­˜åœ¨è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ï¼Œæ•°æ®åœ¨å†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´çš„ç§»åŠ¨ä¹Ÿå¯èƒ½æˆä¸ºä¸»è¦çš„æ€§èƒ½ç“¶é¢ˆï¼ˆ[Norman
    P. Jouppi ç­‰äºº 2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰ã€‚
- en: These hardware considerations naturally guide the implementation of backpropagation
    in modern training systems. Responding to these constraints, specialized memory-efficient
    algorithms for operations like convolutions compute gradients in tiles or chunks,
    adapting to available memory bandwidth. Dynamic memory management tracks the lifetime
    of intermediate values throughout the computation graph, deallocating memory as
    soon as tensors become unnecessary for subsequent computations ([Paszke et al.
    2019](ch058.xhtml#ref-paszke2019pytorch)).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç¡¬ä»¶è€ƒè™‘å› ç´ è‡ªç„¶å¼•å¯¼äº†ç°ä»£è®­ç»ƒç³»ç»Ÿä¸­åå‘ä¼ æ’­çš„å®ç°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é™åˆ¶ï¼Œä¸“é—¨é’ˆå¯¹å·ç§¯ç­‰æ“ä½œçš„å†…å­˜é«˜æ•ˆç®—æ³•åœ¨ç“¦ç‰‡æˆ–å—ä¸­è®¡ç®—æ¢¯åº¦ï¼Œä»¥é€‚åº”å¯ç”¨çš„å†…å­˜å¸¦å®½ã€‚åŠ¨æ€å†…å­˜ç®¡ç†åœ¨æ•´ä¸ªè®¡ç®—å›¾ä¸­è·Ÿè¸ªä¸­é—´å€¼çš„ç”Ÿå‘½å‘¨æœŸï¼Œä¸€æ—¦å¼ é‡å¯¹åç»­è®¡ç®—ä¸å†å¿…è¦ï¼Œå°±ç«‹å³é‡Šæ”¾å†…å­˜ï¼ˆ[Paszke
    ç­‰äºº 2019](ch058.xhtml#ref-paszke2019pytorch)ï¼‰ã€‚
- en: Mathematical Foundations System Implications
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°å­¦åŸºç¡€ç³»ç»Ÿå½±å“
- en: The mathematical operations we have examinedâ€”forward propagation, gradient computation,
    and parameter updatesâ€”define what training systems must compute. Understanding
    these operations in mathematical terms provides essential knowledge, but implementing
    them in practical training systems requires translating mathematical abstractions
    into orchestrated computational workflows. This translation introduces distinct
    challenges centered on resource coordination, timing, and data movement.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€è€ƒå¯Ÿçš„æ•°å­¦è¿ç®—â€”â€”å‰å‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°â€”â€”å®šä¹‰äº†è®­ç»ƒç³»ç»Ÿå¿…é¡»è®¡ç®—çš„å†…å®¹ã€‚ç”¨æ•°å­¦æœ¯è¯­ç†è§£è¿™äº›æ“ä½œæä¾›äº†åŸºæœ¬çŸ¥è¯†ï¼Œä½†åœ¨å®é™…è®­ç»ƒç³»ç»Ÿä¸­å®ç°å®ƒä»¬éœ€è¦å°†æ•°å­¦æŠ½è±¡è½¬åŒ–ä¸ºåè°ƒçš„è®¡ç®—å·¥ä½œæµç¨‹ã€‚è¿™ç§è½¬æ¢å¼•å…¥äº†ä»¥èµ„æºåè°ƒã€æ—¶åºå’Œæ•°æ®ç§»åŠ¨ä¸ºä¸­å¿ƒçš„ç‰¹å®šæŒ‘æˆ˜ã€‚
- en: Efficiently executing training requires coordinating these mathematical operations
    with data loading pipelines, preprocessing workflows, hardware accelerators, and
    monitoring systems. The matrix multiplications that dominate forward and backward
    passes must be scheduled to overlap with data transfer operations to prevent GPU
    idle time. Activation storage requirements from forward propagation influence
    batch size selection and memory allocation strategies. The sequential dependencies
    imposed by backpropagation constrain parallelization opportunities and shape distributed
    training architectures. These system-level considerations transform mathematical
    operations into concrete computational pipelines.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ•ˆæ‰§è¡Œè®­ç»ƒéœ€è¦åè°ƒè¿™äº›æ•°å­¦è¿ç®—ä¸æ•°æ®åŠ è½½ç®¡é“ã€é¢„å¤„ç†å·¥ä½œæµç¨‹ã€ç¡¬ä»¶åŠ é€Ÿå™¨å’Œç›‘æ§ç³»ç»Ÿã€‚ä¸»å¯¼æ­£å‘å’Œåå‘ä¼ æ’­çš„çŸ©é˜µä¹˜æ³•å¿…é¡»å®‰æ’ä¸æ•°æ®ä¼ è¾“æ“ä½œé‡å ï¼Œä»¥é˜²æ­¢GPUç©ºé—²æ—¶é—´ã€‚æ­£å‘ä¼ æ’­çš„æ¿€æ´»å­˜å‚¨éœ€æ±‚å½±å“æ‰¹æ¬¡å¤§å°é€‰æ‹©å’Œå†…å­˜åˆ†é…ç­–ç•¥ã€‚åå‘ä¼ æ’­å¼ºåŠ çš„é¡ºåºä¾èµ–æ€§é™åˆ¶äº†å¹¶è¡ŒåŒ–æœºä¼šå¹¶å¡‘é€ äº†åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„ã€‚è¿™äº›ç³»ç»Ÿçº§è€ƒè™‘å°†æ•°å­¦è¿ç®—è½¬åŒ–ä¸ºå…·ä½“çš„è®¡ç®—ç®¡é“ã€‚
- en: Pipeline Architecture
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®¡é“æ¶æ„
- en: The mathematical operations examined above define what training systems must
    compute. Pipeline architecture determines how to orchestrate these computations
    efficiently across real hardware with finite memory and bandwidth constraints.
    A training pipeline provides the organizational framework that coordinates mathematical
    operations with data movement, system resources, and operational monitoring. This
    architectural perspective enables optimization not just of individual operations,
    but their orchestration across the entire training process.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šæ–‡æ‰€è€ƒå¯Ÿçš„æ•°å­¦è¿ç®—å®šä¹‰äº†è®­ç»ƒç³»ç»Ÿå¿…é¡»è®¡ç®—çš„å†…å®¹ã€‚ç®¡é“æ¶æ„å†³å®šäº†å¦‚ä½•åœ¨å…·æœ‰æœ‰é™å†…å­˜å’Œå¸¦å®½çº¦æŸçš„çœŸå®ç¡¬ä»¶ä¸Šé«˜æ•ˆåœ°ç¼–æ’è¿™äº›è®¡ç®—ã€‚è®­ç»ƒç®¡é“æä¾›äº†ç»„ç»‡æ¡†æ¶ï¼Œåè°ƒæ•°å­¦è¿ç®—ä¸æ•°æ®ç§»åŠ¨ã€ç³»ç»Ÿèµ„æºå’Œæ“ä½œç›‘æ§ã€‚è¿™ç§æ¶æ„è§†è§’ä¸ä»…ä¼˜åŒ–äº†å•ä¸ªæ“ä½œï¼Œè¿˜ä¼˜åŒ–äº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ“ä½œç¼–æ’ã€‚
- en: 'As shown in [FigureÂ 8.3](ch014.xhtml#fig-training-pipeline), the training pipeline
    consists of three main components: the data pipeline for ingestion and preprocessing,
    the training loop that handles model updates, and the evaluation pipeline for
    assessing performance. These components work together in a coordinated manner,
    with processed batches flowing from the data pipeline to the training loop, and
    evaluation metrics providing feedback to guide the training process.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾8.3](ch014.xhtml#fig-training-pipeline)æ‰€ç¤ºï¼Œè®­ç»ƒç®¡é“ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šç”¨äºæ‘„å–å’Œé¢„å¤„ç†çš„ æ•°æ®ç®¡é“ã€å¤„ç†æ¨¡å‹æ›´æ–°çš„è®­ç»ƒå¾ªç¯ï¼Œä»¥åŠç”¨äºè¯„ä¼°æ€§èƒ½çš„è¯„ä¼°ç®¡é“ã€‚è¿™äº›ç»„ä»¶ä»¥åè°ƒä¸€è‡´çš„æ–¹å¼ååŒå·¥ä½œï¼Œå¤„ç†åçš„æ‰¹æ¬¡ä»æ•°æ®ç®¡é“æµå‘è®­ç»ƒå¾ªç¯ï¼Œè¯„ä¼°æŒ‡æ ‡æä¾›åé¦ˆä»¥æŒ‡å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚
- en: '![](../media/file110.svg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file110.svg)'
- en: 'FigureÂ 8.3: **Pipeline Architecture**: Machine learning systems organize training
    through interconnected data, training, and evaluation pipelines, enabling iterative
    model refinement and performance assessment. Data flows sequentially through these
    components, with evaluation metrics providing feedback to optimize the training
    process and ensure reproducible results.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.3ï¼š**ç®¡é“æ¶æ„**ï¼šæœºå™¨å­¦ä¹ ç³»ç»Ÿé€šè¿‡ç›¸äº’è¿æ¥çš„æ•°æ®ã€è®­ç»ƒå’Œè¯„ä¼°ç®¡é“ç»„ç»‡è®­ç»ƒï¼Œä»è€Œå®ç°è¿­ä»£æ¨¡å‹ä¼˜åŒ–å’Œæ€§èƒ½è¯„ä¼°ã€‚æ•°æ®æŒ‰é¡ºåºé€šè¿‡è¿™äº›ç»„ä»¶æµåŠ¨ï¼Œè¯„ä¼°æŒ‡æ ‡æä¾›åé¦ˆä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹å¹¶ç¡®ä¿å¯é‡å¤çš„ç»“æœã€‚
- en: Architectural Overview
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¶æ„æ¦‚è¿°
- en: 'To understand how these mathematical operations translate into practical systems,
    the architecture of a training pipeline is organized around three interconnected
    components: the data pipeline, the training loop, and the evaluation pipeline.
    These components collectively process raw data, train the model, and assess its
    performance, ensuring that the training process is efficient and effective.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£è¿™äº›æ•°å­¦è¿ç®—å¦‚ä½•è½¬åŒ–ä¸ºå®é™…ç³»ç»Ÿï¼Œè®­ç»ƒç®¡é“çš„æ¶æ„å›´ç»•ä¸‰ä¸ªç›¸äº’å…³è”çš„ç»„ä»¶ç»„ç»‡ï¼šæ•°æ®ç®¡é“ã€è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°ç®¡é“ã€‚è¿™äº›ç»„ä»¶å…±åŒå¤„ç†åŸå§‹æ•°æ®ã€è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆä¸”æœ‰æ•ˆã€‚
- en: This modular organization enables efficient resource utilization and clear separation
    of concerns. The data pipeline initiates the process by ingesting raw data and
    transforming it into a format suitable for the model. This data is passed to the
    training loop, where the model performs its core computations to learn from the
    inputs. Periodically, the evaluation pipeline assesses the modelâ€™s performance
    using a separate validation dataset. This modular structure ensures that each
    stage operates efficiently while contributing to the overall workflow.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¨¡å—åŒ–ç»„ç»‡ç»“æ„ä½¿å¾—èµ„æºåˆ©ç”¨æ•ˆç‡é«˜ï¼Œå…³æ³¨ç‚¹åˆ†ç¦»æ¸…æ™°ã€‚æ•°æ®ç®¡é“é€šè¿‡æ‘„å…¥åŸå§‹æ•°æ®å¹¶å°†å…¶è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹æ ¼å¼çš„æ•°æ®æ¥å¯åŠ¨è¿‡ç¨‹ã€‚è¿™äº›æ•°æ®ä¼ é€’ç»™è®­ç»ƒå¾ªç¯ï¼Œæ¨¡å‹åœ¨æ­¤å¤„æ‰§è¡Œå…¶æ ¸å¿ƒè®¡ç®—ä»¥ä»è¾“å…¥ä¸­å­¦ä¹ ã€‚å®šæœŸï¼Œè¯„ä¼°ç®¡é“ä½¿ç”¨å•ç‹¬çš„éªŒè¯æ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è¿™ç§æ¨¡å—åŒ–ç»“æ„ç¡®ä¿æ¯ä¸ªé˜¶æ®µéƒ½é«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¸ºæ•´ä½“å·¥ä½œæµç¨‹åšå‡ºè´¡çŒ®ã€‚
- en: Data Pipeline
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“
- en: Understanding each componentâ€™s role begins with the data pipeline, which manages
    the ingestion, preprocessing, and batching of data for training. Raw data is typically
    loaded from local storage and transformed dynamically during training to avoid
    redundancy and enhance diversity. For instance, image datasets may undergo preprocessing
    steps like normalization, resizing, and augmentation to improve the robustness
    of the model. These operations are performed in real time to minimize storage
    overhead and adapt to the specific requirements of the task ([Yann LeCun et al.
    1998](ch058.xhtml#ref-lecun1998efficient)). Once processed, the data is packaged
    into batches and handed off to the training loop.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ä»æ•°æ®ç®¡é“å¼€å§‹ï¼Œå®ƒè´Ÿè´£ç®¡ç†æ•°æ®çš„æ‘„å…¥ã€é¢„å¤„ç†å’Œæ‰¹å¤„ç†ï¼Œä»¥ä¾¿è¿›è¡Œè®­ç»ƒã€‚åŸå§‹æ•°æ®é€šå¸¸ä»æœ¬åœ°å­˜å‚¨åŠ è½½ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è½¬æ¢ï¼Œä»¥é¿å…å†—ä½™å¹¶å¢å¼ºå¤šæ ·æ€§ã€‚ä¾‹å¦‚ï¼Œå›¾åƒæ•°æ®é›†å¯èƒ½ç»è¿‡å½’ä¸€åŒ–ã€è°ƒæ•´å¤§å°å’Œå¢å¼ºç­‰é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚è¿™äº›æ“ä½œå®æ—¶æ‰§è¡Œï¼Œä»¥æœ€å°åŒ–å­˜å‚¨å¼€é”€å¹¶é€‚åº”ç‰¹å®šä»»åŠ¡çš„è¦æ±‚([Yann
    LeCunç­‰äººï¼Œ1998](ch058.xhtml#ref-lecun1998efficient))ã€‚ä¸€æ—¦å¤„ç†å®Œæ¯•ï¼Œæ•°æ®å°±è¢«æ‰“åŒ…æˆæ‰¹æ¬¡å¹¶ä¼ é€’ç»™è®­ç»ƒå¾ªç¯ã€‚
- en: Training Loop
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯
- en: 'The training loop is the computational core of the pipeline, where the model
    learns from the prepared data. [FigureÂ 8.4](ch014.xhtml#fig-training-loop) illustrates
    this process, highlighting the forward pass, loss computation, and parameter updates
    on a single GPU:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯æ˜¯ç®¡é“çš„è®¡ç®—æ ¸å¿ƒï¼Œæ¨¡å‹åœ¨æ­¤å¤„ä»å‡†å¤‡å¥½çš„æ•°æ®ä¸­å­¦ä¹ ã€‚[å›¾8.4](ch014.xhtml#fig-training-loop)å±•ç¤ºäº†è¿™ä¸€è¿‡ç¨‹ï¼Œçªå‡ºäº†åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œçš„æ­£å‘ä¼ é€’ã€æŸå¤±è®¡ç®—å’Œå‚æ•°æ›´æ–°ï¼š
- en: '![](../media/file111.svg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file111.svg)'
- en: 'FigureÂ 8.4: **GPU-Accelerated Training**: Modern deep learning relies on gpus
    to parallelize matrix operations, significantly accelerating the forward and backward
    passes required for parameter updates during training. This single-GPU workflow
    iteratively refines model parameters by computing gradients from loss functions
    and applying them to minimize prediction errors.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.4ï¼š**GPUåŠ é€Ÿè®­ç»ƒ**ï¼šç°ä»£æ·±åº¦å­¦ä¹ ä¾èµ–äºGPUæ¥å¹¶è¡ŒåŒ–çŸ©é˜µè¿ç®—ï¼Œæ˜¾è‘—åŠ é€Ÿäº†è®­ç»ƒæœŸé—´å‚æ•°æ›´æ–°æ‰€éœ€çš„å‰å‘å’Œåå‘ä¼ é€’ã€‚è¿™ç§å•GPUå·¥ä½œæµç¨‹é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦å¹¶å°†å…¶åº”ç”¨äºæœ€å°åŒ–é¢„æµ‹è¯¯å·®æ¥è¿­ä»£åœ°ç»†åŒ–æ¨¡å‹å‚æ•°ã€‚
- en: 'Each iteration of the training loop involves several key steps:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„æ¯æ¬¡è¿­ä»£æ¶‰åŠå‡ ä¸ªå…³é”®æ­¥éª¤ï¼š
- en: '**Step 1 â€“ Forward Pass**: A batch of data from the dataset is passed through
    the neural network on the GPU to generate predictions. The model applies matrix
    multiplications and activation functions to transform the input into meaningful
    outputs.'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1 â€“ æ­£å‘ä¼ é€’**ï¼šä»æ•°æ®é›†ä¼ é€’ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®åˆ°GPUä¸Šçš„ç¥ç»ç½‘ç»œï¼Œä»¥ç”Ÿæˆé¢„æµ‹ã€‚æ¨¡å‹åº”ç”¨çŸ©é˜µä¹˜æ³•å’Œæ¿€æ´»å‡½æ•°å°†è¾“å…¥è½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„è¾“å‡ºã€‚'
- en: '**Step 2 â€“ Compute Gradients**: The predicted values are compared with the
    ground truth labels to compute the error using a loss function. The loss function
    outputs a scalar value that quantifies the modelâ€™s performance. This error signal
    is then propagated backward through the network using backpropagation, which applies
    the chain rule of differentiation to compute gradients for each layerâ€™s parameters.
    These gradients indicate the necessary adjustments required to minimize the loss.'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤2 â€“ è®¡ç®—æ¢¯åº¦**ï¼šå°†é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—è¯¯å·®ã€‚æŸå¤±å‡½æ•°è¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶åï¼Œä½¿ç”¨åå‘ä¼ æ’­å°†è¿™ä¸ªè¯¯å·®ä¿¡å·åå‘ä¼ æ’­é€šè¿‡ç½‘ç»œï¼Œåå‘ä¼ æ’­åº”ç”¨å¾®åˆ†é“¾å¼æ³•åˆ™æ¥è®¡ç®—æ¯ä¸€å±‚å‚æ•°çš„æ¢¯åº¦ã€‚è¿™äº›æ¢¯åº¦æŒ‡ç¤ºäº†æ‰€éœ€çš„è°ƒæ•´ï¼Œä»¥æœ€å°åŒ–æŸå¤±ã€‚'
- en: '**Step 3 â€“ Update Parameters**: The computed gradients are passed to an optimizer,
    which updates the modelâ€™s parameters to minimize the loss. Different optimization
    algorithms, such as SGD or Adam, influence how the parameters are adjusted. The
    choice of optimizer impacts convergence speed and stability.'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤3 â€“ æ›´æ–°å‚æ•°**ï¼šè®¡ç®—å‡ºçš„æ¢¯åº¦ä¼ é€’ç»™ä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–å™¨é€šè¿‡æ›´æ–°æ¨¡å‹çš„å‚æ•°æ¥æœ€å°åŒ–æŸå¤±ã€‚ä¸åŒçš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚SGDæˆ–Adamï¼Œå½±å“å‚æ•°è°ƒæ•´çš„æ–¹å¼ã€‚ä¼˜åŒ–å™¨çš„é€‰æ‹©å½±å“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚'
- en: This process repeats iteratively across multiple batches and epochs, gradually
    refining the model to improve its predictive accuracy.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è¿‡ç¨‹åœ¨å¤šä¸ªæ‰¹æ¬¡å’Œepochä¸­è¿­ä»£é‡å¤ï¼Œé€æ¸ç»†åŒ–æ¨¡å‹ä»¥æé«˜å…¶é¢„æµ‹å‡†ç¡®æ€§ã€‚
- en: Evaluation Pipeline
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç®¡é“
- en: Completing the pipeline architecture, the evaluation pipeline provides periodic
    feedback on the modelâ€™s performance during training. Using a separate validation
    dataset, the modelâ€™s predictions are compared against known outcomes to compute
    metrics such as accuracy or loss. These metrics help to monitor progress and detect
    issues like overfitting or underfitting. Evaluation is typically performed at
    regular intervals, such as at the end of each epoch, ensuring that the training
    process aligns with the desired objectives.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆç®¡é“æ¶æ„åï¼Œè¯„ä¼°ç®¡é“åœ¨è®­ç»ƒæœŸé—´å®šæœŸæä¾›å…³äºæ¨¡å‹æ€§èƒ½çš„åé¦ˆã€‚ä½¿ç”¨å•ç‹¬çš„éªŒè¯æ•°æ®é›†ï¼Œå°†æ¨¡å‹çš„é¢„æµ‹ä¸å·²çŸ¥ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œä»¥è®¡ç®—å‡†ç¡®åº¦æˆ–æŸå¤±ç­‰æŒ‡æ ‡ã€‚è¿™äº›æŒ‡æ ‡æœ‰åŠ©äºç›‘æ§è¿›åº¦å¹¶æ£€æµ‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆç­‰é—®é¢˜ã€‚è¯„ä¼°é€šå¸¸åœ¨å›ºå®šé—´éš”è¿›è¡Œï¼Œä¾‹å¦‚åœ¨æ¯ä¸ªepochç»“æŸæ—¶ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸é¢„æœŸç›®æ ‡ä¸€è‡´ã€‚
- en: Component Integration
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç»„ä»¶é›†æˆ
- en: Having examined each component individually, we can now understand how they
    work together. The data pipeline, training loop, and evaluation pipeline are tightly
    integrated to ensure a smooth and efficient workflow. Data preparation often overlaps
    with computation, such as when preprocessing the next batch while the current
    batch is being processed in the training loop. Similarly, the evaluation pipeline
    operates in tandem with training, providing insights that inform adjustments to
    the model or training procedure. This integration minimizes idle time for the
    systemâ€™s resources and ensures that training proceeds without interruptions.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å•ç‹¬æ£€æŸ¥äº†æ¯ä¸ªç»„ä»¶ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç†è§£å®ƒä»¬æ˜¯å¦‚ä½•ååŒå·¥ä½œçš„ã€‚æ•°æ®ç®¡é“ã€è®­ç»ƒå¾ªç¯å’Œè¯„ä¼°ç®¡é“ç´§å¯†é›†æˆï¼Œä»¥ç¡®ä¿æµç•…å’Œé«˜æ•ˆçš„æµç¨‹ã€‚æ•°æ®å‡†å¤‡é€šå¸¸ä¸è®¡ç®—é‡å ï¼Œä¾‹å¦‚åœ¨è®­ç»ƒå¾ªç¯ä¸­å¤„ç†å½“å‰æ‰¹æ¬¡çš„åŒæ—¶é¢„å¤„ç†ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ã€‚åŒæ ·ï¼Œè¯„ä¼°ç®¡é“ä¸è®­ç»ƒåŒæ­¥è¿è¡Œï¼Œæä¾›æœ‰å…³è°ƒæ•´æ¨¡å‹æˆ–è®­ç»ƒè¿‡ç¨‹çš„ä¿¡æ¯ã€‚è¿™ç§é›†æˆæœ€å°åŒ–äº†ç³»ç»Ÿèµ„æºçš„ç©ºé—²æ—¶é—´ï¼Œå¹¶ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸é—´æ–­è¿›è¡Œã€‚
- en: Data Pipeline
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“
- en: We can now examine each component in detail, starting with the data pipeline.
    The data pipeline moves data from storage to computational devices during training.
    Like a highway system moving vehicles from neighborhoods to city centers, the
    data pipeline transports training data through multiple stages to reach computational
    resources.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥è¯¦ç»†æ£€æŸ¥æ¯ä¸ªç»„ä»¶ï¼Œä»æ•°æ®ç®¡é“å¼€å§‹ã€‚æ•°æ®ç®¡é“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ•°æ®ä»å­˜å‚¨ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡ã€‚å°±åƒé«˜é€Ÿå…¬è·¯ç³»ç»Ÿå°†è½¦è¾†ä»ç¤¾åŒºç§»åŠ¨åˆ°å¸‚ä¸­å¿ƒä¸€æ ·ï¼Œæ•°æ®ç®¡é“é€šè¿‡å¤šä¸ªé˜¶æ®µå°†è®­ç»ƒæ•°æ®ä¼ è¾“åˆ°è®¡ç®—èµ„æºã€‚
- en: While this section focuses on the systems aspects of data movement and preprocessing
    for training efficiency, the upstream data engineering practicesâ€”including data
    quality assurance, feature engineering, schema validation, and dataset versioningâ€”are
    covered in [ChapterÂ 6](ch012.xhtml#sec-data-engineering). Together, these practices
    ensure both high-quality training data and efficient data delivery to computational
    resources. This chapter examines how to optimize the throughput, memory usage,
    and coordination of data pipelines once data engineering has prepared validated,
    properly formatted datasets.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ¬èŠ‚é‡ç‚¹ä»‹ç»æ•°æ®ç§»åŠ¨å’Œé¢„å¤„ç†ä»¥æé«˜è®­ç»ƒæ•ˆç‡çš„ç³»ç»Ÿæ–¹é¢ï¼Œä½†ä¸Šæ¸¸æ•°æ®å·¥ç¨‹å®è·µï¼ŒåŒ…æ‹¬æ•°æ®è´¨é‡ä¿è¯ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å¼éªŒè¯å’Œæ•°æ®é›†ç‰ˆæœ¬æ§åˆ¶ï¼Œåœ¨[ç¬¬6ç« ](ch012.xhtml#sec-data-engineering)ä¸­æœ‰æ‰€æ¶µç›–ã€‚è¿™äº›å®è·µå…±åŒç¡®ä¿äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å’Œé«˜æ•ˆçš„æ•°æ®ä¼ è¾“åˆ°è®¡ç®—èµ„æºã€‚æœ¬ç« æ¢è®¨äº†åœ¨æ•°æ®å·¥ç¨‹å‡†å¤‡å¹¶éªŒè¯äº†æ ¼å¼æ­£ç¡®çš„æ•°æ®é›†åï¼Œå¦‚ä½•ä¼˜åŒ–æ•°æ®ç®¡é“çš„ååé‡ã€å†…å­˜ä½¿ç”¨å’Œåè°ƒã€‚
- en: '![](../media/file112.svg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file112.svg)'
- en: 'FigureÂ 8.5: **Data Pipeline Architecture**: Modern machine learning systems
    utilize pipelines to efficiently move data from storage to gpus for parallel processing,
    enabling faster model training and inference. This diagram presents a typical
    pipeline with stages for formatting, preprocessing, batching, and distributing
    data across multiple GPU workers.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.5ï¼š**æ•°æ®ç®¡é“æ¶æ„**ï¼šç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿåˆ©ç”¨ç®¡é“é«˜æ•ˆåœ°å°†æ•°æ®ä»å­˜å‚¨ç§»åŠ¨åˆ°GPUè¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚æ­¤å›¾å±•ç¤ºäº†å…¸å‹çš„ç®¡é“ï¼ŒåŒ…æ‹¬æ ¼å¼åŒ–ã€é¢„å¤„ç†ã€æ‰¹å¤„ç†å’Œå°†æ•°æ®åˆ†å‘åˆ°å¤šä¸ªGPUå·¥ä½œè€…çš„é˜¶æ®µã€‚
- en: 'The data pipeline running on the CPU serves as a bridge between raw data storage
    and GPU computation. As shown in [FigureÂ 8.5](ch014.xhtml#fig-data-pipeline),
    the pipeline consists of three main zones: storage, CPU preprocessing, and GPU
    training. Each zone plays a distinct role in preparing and delivering data for
    model training.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨CPUä¸Šè¿è¡Œçš„ç®¡é“ä½œä¸ºåŸå§‹æ•°æ®å­˜å‚¨å’ŒGPUè®¡ç®—ä¹‹é—´çš„æ¡¥æ¢ã€‚å¦‚å›¾[å›¾8.5](ch014.xhtml#fig-data-pipeline)æ‰€ç¤ºï¼Œè¯¥ç®¡é“ç”±ä¸‰ä¸ªä¸»è¦åŒºåŸŸç»„æˆï¼šå­˜å‚¨ã€CPUé¢„å¤„ç†å’ŒGPUè®­ç»ƒã€‚æ¯ä¸ªåŒºåŸŸåœ¨å‡†å¤‡å’Œäº¤ä»˜æ•°æ®ä»¥ä¾›æ¨¡å‹è®­ç»ƒæ–¹é¢éƒ½å‘æŒ¥ç€ç‹¬ç‰¹çš„ä½œç”¨ã€‚
- en: 'In the storage zone, raw data resides on disk, typically in formats like image
    files for computer vision tasks or text files for natural language processing.
    The CPU preprocessing zone handles the transformation of this raw data through
    multiple stages. For example, in an image recognition model, these stages include:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­˜å‚¨åŒºåŸŸï¼ŒåŸå§‹æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼Œé€šå¸¸ä»¥å›¾åƒæ–‡ä»¶ï¼ˆç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼‰æˆ–æ–‡æœ¬æ–‡ä»¶ï¼ˆç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ç­‰æ ¼å¼å­˜åœ¨ã€‚CPUé¢„å¤„ç†åŒºåŸŸé€šè¿‡å¤šä¸ªé˜¶æ®µå¤„ç†è¿™äº›åŸå§‹æ•°æ®ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå›¾åƒè¯†åˆ«æ¨¡å‹ä¸­ï¼Œè¿™äº›é˜¶æ®µåŒ…æ‹¬ï¼š
- en: 'Format conversion: Reading image files and converting them to standardized
    formats'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¼å¼è½¬æ¢ï¼šè¯»å–å›¾åƒæ–‡ä»¶å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
- en: 'Processing: Applying operations like resizing, normalization, and data augmentation'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤„ç†ï¼šåº”ç”¨è¯¸å¦‚è°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–å’Œæ•°æ®å¢å¼ºç­‰æ“ä½œ
- en: 'Batching: Organizing processed examples into batches for efficient GPU computation'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†ï¼šå°†å¤„ç†åçš„ç¤ºä¾‹ç»„ç»‡æˆæ‰¹é‡ä»¥è¿›è¡Œé«˜æ•ˆçš„GPUè®¡ç®—
- en: The final zone shows multiple GPUs receiving preprocessed batches for training.
    This organization ensures that each GPU maintains a steady supply of data, maximizing
    computational efficiency and minimizing idle time. The effectiveness of this pipeline
    directly impacts training performance, as any bottleneck in data preparation can
    leave expensive GPU resources underutilized.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªåŒºåŸŸæ˜¾ç¤ºäº†å¤šä¸ªGPUæ¥æ”¶é¢„å¤„ç†åçš„æ‰¹é‡æ•°æ®ä»¥è¿›è¡Œè®­ç»ƒã€‚è¿™ç§ç»„ç»‡ç¡®ä¿æ¯ä¸ªGPUéƒ½èƒ½ä¿æŒç¨³å®šçš„æ•°æ®ä¾›åº”ï¼Œæœ€å¤§åŒ–è®¡ç®—æ•ˆç‡å¹¶æœ€å°åŒ–ç©ºé—²æ—¶é—´ã€‚è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ç›´æ¥å½±å“è®­ç»ƒæ€§èƒ½ï¼Œå› ä¸ºæ•°æ®å‡†å¤‡ä¸­çš„ä»»ä½•ç“¶é¢ˆéƒ½å¯èƒ½ä½¿æ˜‚è´µçš„GPUèµ„æºå¾—ä¸åˆ°å……åˆ†åˆ©ç”¨ã€‚
- en: Core Components
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒç»„ä»¶
- en: The performance of machine learning systems is primarily constrained by storage
    access speed, which determines the rate at which training data can be retrieved.
    The data engineering practices described in [ChapterÂ 6](ch012.xhtml#sec-data-engineering)â€”including
    data format selection (Parquet, TFRecord, Arrow), data partitioning strategies,
    and data locality optimizationâ€”directly impact these storage performance characteristics.
    This section examines the systems-level implications of data access patterns and
    throughput constraints during training.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½ä¸»è¦å—é™äºå­˜å‚¨è®¿é—®é€Ÿåº¦ï¼Œè¿™å†³å®šäº†è®­ç»ƒæ•°æ®å¯ä»¥æ£€ç´¢çš„é€Ÿåº¦ã€‚åœ¨[ç¬¬6ç« ](ch012.xhtml#sec-data-engineering)ä¸­æè¿°çš„æ•°æ®å·¥ç¨‹å®è·µâ€”â€”åŒ…æ‹¬æ•°æ®æ ¼å¼é€‰æ‹©ï¼ˆParquetã€TFRecordã€Arrowï¼‰ã€æ•°æ®åˆ†åŒºç­–ç•¥å’Œæ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–â€”â€”ç›´æ¥å½±å“åˆ°è¿™äº›å­˜å‚¨æ€§èƒ½ç‰¹æ€§ã€‚æœ¬èŠ‚æ¢è®¨äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•°æ®è®¿é—®æ¨¡å¼å’Œååé‡é™åˆ¶çš„ç³»ç»Ÿçº§å½±å“ã€‚
- en: 'This access speed is governed by two primary hardware constraints: disk bandwidth
    and network bandwidth. The maximum theoretical throughput is determined by the
    following relationship: <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">storage</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">disk</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">network</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{storage}} =\min(B_{\text{disk}}, B_{\text{network}})</annotation></semantics>
    where <semantics><msub><mi>B</mi><mtext mathvariant="normal">disk</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{disk}}</annotation></semantics> is the physical
    disk bandwidth (the rate at which data can be read from storage devices) and <semantics><msub><mi>B</mi><mtext
    mathvariant="normal">network</mtext></msub><annotation encoding="application/x-tex">B_{\text{network}}</annotation></semantics>
    represents the network bandwidth (the rate of data transfer across distributed
    storage systems). Both quantities are measured in bytes per second.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¿é—®é€Ÿåº¦å—ä¸¤ä¸ªä¸»è¦ç¡¬ä»¶é™åˆ¶ï¼šç£ç›˜å¸¦å®½å’Œç½‘ç»œå¸¦å®½ã€‚æœ€å¤§ç†è®ºååé‡ç”±ä»¥ä¸‹å…³ç³»ç¡®å®šï¼š<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">storage</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">disk</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">network</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{storage}} =\min(B_{\text{disk}}, B_{\text{network}})</annotation></semantics>
    å…¶ä¸­ <semantics><msub><mi>B</mi><mtext mathvariant="normal">disk</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{disk}}</annotation></semantics> æ˜¯ç‰©ç†ç£ç›˜å¸¦å®½ï¼ˆä»å­˜å‚¨è®¾å¤‡è¯»å–æ•°æ®çš„é€Ÿç‡ï¼‰å’Œ
    <semantics><msub><mi>B</mi><mtext mathvariant="normal">network</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{network}}</annotation></semantics> ä»£è¡¨ç½‘ç»œå¸¦å®½ï¼ˆè·¨åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®ä¼ è¾“é€Ÿç‡ï¼‰ã€‚è¿™ä¸¤ä¸ªé‡éƒ½æ˜¯ä»¥æ¯ç§’å­—èŠ‚æ•°æ¥è¡¡é‡çš„ã€‚
- en: 'The actual throughput achieved during training operations falls below this
    theoretical maximum due to non-sequential data access patterns. The effective
    throughput can be expressed as: <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">effective</mtext></msub><mo>=</mo><msub><mi>T</mi><mtext
    mathvariant="normal">storage</mtext></msub><mo>Ã—</mo><msub><mi>F</mi><mtext mathvariant="normal">access</mtext></msub></mrow><annotation
    encoding="application/x-tex">T_{\text{effective}} = T_{\text{storage}} \times
    F_{\text{access}}</annotation></semantics> where <semantics><msub><mi>F</mi><mtext
    mathvariant="normal">access</mtext></msub><annotation encoding="application/x-tex">F_{\text{access}}</annotation></semantics>
    represents the access pattern factor. In typical training scenarios, <semantics><msub><mi>F</mi><mtext
    mathvariant="normal">access</mtext></msub><annotation encoding="application/x-tex">F_{\text{access}}</annotation></semantics>
    approximates 0.1, indicating that effective throughput achieves only 10% of the
    theoretical maximum. This significant reduction occurs because storage systems
    are optimized for sequential access patterns rather than the random access patterns
    common in training procedures.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ“ä½œæœŸé—´å®é™…è¾¾åˆ°çš„ååé‡ä½äºè¿™ä¸ªç†è®ºæœ€å¤§å€¼ï¼Œå› ä¸ºæ•°æ®è®¿é—®æ¨¡å¼æ˜¯éé¡ºåºçš„ã€‚æœ‰æ•ˆååé‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">effective</mtext></msub><mo>=</mo><msub><mi>T</mi><mtext
    mathvariant="normal">storage</mtext></msub><mo>Ã—</mo><msub><mi>F</mi><mtext mathvariant="normal">access</mtext></msub></mrow><annotation
    encoding="application/x-tex">T_{\text{effective}} = T_{\text{storage}} \times
    F_{\text{access}}</annotation></semantics> å…¶ä¸­ <semantics><msub><mi>F</mi><mtext
    mathvariant="normal">access</mtext></msub><annotation encoding="application/x-tex">F_{\text{access}}</annotation></semantics>
    ä»£è¡¨è®¿é—®æ¨¡å¼å› å­ã€‚åœ¨å…¸å‹çš„è®­ç»ƒåœºæ™¯ä¸­ï¼Œ<semantics><msub><mi>F</mi><mtext mathvariant="normal">access</mtext></msub><annotation
    encoding="application/x-tex">F_{\text{access}}</annotation></semantics> è¿‘ä¼¼ä¸º 0.1ï¼Œè¿™æ„å‘³ç€æœ‰æ•ˆååé‡ä»…è¾¾åˆ°ç†è®ºæœ€å¤§å€¼çš„
    10%ã€‚è¿™ç§æ˜¾è‘—é™ä½çš„åŸå› æ˜¯å­˜å‚¨ç³»ç»Ÿé’ˆå¯¹é¡ºåºè®¿é—®æ¨¡å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸è§çš„éšæœºè®¿é—®æ¨¡å¼ã€‚
- en: This relationship between theoretical and effective throughput has important
    implications for system design and training optimization. Understanding these
    constraints allows practitioners to make informed decisions about data pipeline
    architecture and training methodology.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºååé‡å’Œå®é™…ååé‡ä¹‹é—´çš„å…³ç³»å¯¹ç³»ç»Ÿè®¾è®¡å’Œè®­ç»ƒä¼˜åŒ–å…·æœ‰é‡è¦æ„ä¹‰ã€‚äº†è§£è¿™äº›é™åˆ¶æ¡ä»¶ä½¿ä»ä¸šè€…èƒ½å¤Ÿå°±æ•°æ®ç®¡é“æ¶æ„å’Œè®­ç»ƒæ–¹æ³•åšå‡ºæ˜æ™ºçš„å†³ç­–ã€‚
- en: Preprocessing
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'As the data becomes available, data preprocessing transforms raw input data
    into a format suitable for model training. This process, traditionally implemented
    through Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) pipelines[23](#fn23),
    is a critical determinant of training system performance. The throughput of preprocessing
    operations can be expressed mathematically as: <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">preprocessing</mtext></msub><mo>=</mo><mfrac><msub><mi>N</mi><mtext
    mathvariant="normal">workers</mtext></msub><msub><mi>t</mi><mtext mathvariant="normal">transform</mtext></msub></mfrac></mrow><annotation
    encoding="application/x-tex">T_{\text{preprocessing}} = \frac{N_{\text{workers}}}{t_{\text{transform}}}</annotation></semantics>'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ•°æ®çš„å¯ç”¨æ€§ï¼Œæ•°æ®é¢„å¤„ç†å°†åŸå§‹è¾“å…¥æ•°æ®è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹è®­ç»ƒçš„æ ¼å¼ã€‚è¿™ä¸ªè¿‡ç¨‹ï¼Œä¼ ç»Ÿä¸Šé€šè¿‡æå–-è½¬æ¢-åŠ è½½ï¼ˆETLï¼‰æˆ–æå–-åŠ è½½-è½¬æ¢ï¼ˆELTï¼‰ç®¡é“[23](#fn23)å®ç°ï¼Œæ˜¯è®­ç»ƒç³»ç»Ÿæ€§èƒ½çš„å…³é”®å†³å®šå› ç´ ã€‚é¢„å¤„ç†æ“ä½œçš„ååé‡å¯ä»¥ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼š`<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">preprocessing</mtext></msub><mo>=</mo><mfrac><msub><mi>N</mi><mtext
    mathvariant="normal">workers</mtext></msub><msub><mi>t</mi><mtext mathvariant="normal">transform</mtext></msub></mfrac></mrow><annotation
    encoding="application/x-tex">T_{\text{preprocessing}} = \frac{N_{\text{workers}}}{t_{\text{transform}}}</annotation></semantics>`
- en: 'This equation captures two key factors:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹ç¨‹æ•æ‰äº†ä¸¤ä¸ªå…³é”®å› ç´ ï¼š
- en: <semantics><msub><mi>N</mi><mtext mathvariant="normal">workers</mtext></msub><annotation
    encoding="application/x-tex">N_{\text{workers}}</annotation></semantics> represents
    the number of parallel processing threads
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<semantics><msub><mi>N</mi><mtext mathvariant="normal">workers</mtext></msub><annotation
    encoding="application/x-tex">N_{\text{workers}}</annotation></semantics>` è¡¨ç¤ºå¹¶è¡Œå¤„ç†çº¿ç¨‹çš„æ•°é‡'
- en: <semantics><msub><mi>t</mi><mtext mathvariant="normal">transform</mtext></msub><annotation
    encoding="application/x-tex">t_{\text{transform}}</annotation></semantics> represents
    the time required for each transformation operation
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<semantics><msub><mi>t</mi><mtext mathvariant="normal">transform</mtext></msub><annotation
    encoding="application/x-tex">t_{\text{transform}}</annotation></semantics>` è¡¨ç¤ºæ¯ä¸ªè½¬æ¢æ“ä½œæ‰€éœ€çš„æ—¶é—´'
- en: Modern training architectures employ multiple processing threads to ensure preprocessing
    keeps pace with the consumption rates. This parallel processing approach is essential
    for maintaining efficient high processor utilization.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£è®­ç»ƒæ¶æ„é‡‡ç”¨å¤šä¸ªå¤„ç†çº¿ç¨‹ä»¥ç¡®ä¿é¢„å¤„ç†ä¸æ¶ˆè€—ç‡ä¿æŒåŒæ­¥ã€‚è¿™ç§å¹¶è¡Œå¤„ç†æ–¹æ³•å¯¹äºä¿æŒé«˜æ•ˆçš„å¤„ç†å™¨åˆ©ç”¨ç‡è‡³å…³é‡è¦ã€‚
- en: 'The final stage of preprocessing involves transferring the processed data to
    computational devices (typically GPUs). The overall training throughput is constrained
    by three factors, expressed as: <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">training</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>T</mi><mtext mathvariant="normal">preprocessing</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">GPU_transfer</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">GPU_compute</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{training}} =\min(T_{\text{preprocessing}},
    B_{\text{GPU\_transfer}}, B_{\text{GPU\_compute}})</annotation></semantics> where:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†é˜¶æ®µçš„æœ€åä¸€æ­¥æ˜¯å°†å¤„ç†åçš„æ•°æ®ä¼ è¾“åˆ°è®¡ç®—è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯ GPUï¼‰ã€‚æ•´ä½“è®­ç»ƒååé‡å—ä¸‰ä¸ªå› ç´ çš„é™åˆ¶ï¼Œè¡¨ç¤ºä¸ºï¼š`<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">training</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>T</mi><mtext mathvariant="normal">preprocessing</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">GPU_transfer</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">GPU_compute</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{training}} =\min(T_{\text{preprocessing}},
    B_{\text{GPU\_transfer}}, B_{\text{GPU\_compute}})</annotation></semantics>` å…¶ä¸­ï¼š
- en: <semantics><msub><mi>B</mi><mtext mathvariant="normal">GPU_transfer</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{GPU\_transfer}}</annotation></semantics>
    represents GPU memory bandwidth
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<semantics><msub><mi>B</mi><mtext mathvariant="normal">GPU_transfer</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{GPU\_transfer}}</annotation></semantics>`
    è¡¨ç¤º GPU å†…å­˜å¸¦å®½'
- en: <semantics><msub><mi>B</mi><mtext mathvariant="normal">GPU_compute</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{GPU\_compute}}</annotation></semantics>
    represents GPU computational throughput
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<semantics><msub><mi>B</mi><mtext mathvariant="normal">GPU_compute</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{GPU\_compute}}</annotation></semantics>`
    è¡¨ç¤º GPU è®¡ç®—ååé‡'
- en: 'This relationship illustrates a key principle in training system design: the
    systemâ€™s overall performance is limited by its slowest component. Whether preprocessing
    speed, data transfer rates, or computational capacity, the bottleneck stage determines
    the effective training throughput of the entire system. Understanding these relationships
    enables system architects to design balanced training pipelines where preprocessing
    capacity aligns with computational resources, ensuring optimal resource utilization.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å…³ç³»è¯´æ˜äº†è®­ç»ƒç³»ç»Ÿè®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®åŸåˆ™ï¼šç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å—å…¶æœ€æ…¢ç»„ä»¶çš„é™åˆ¶ã€‚æ— è®ºæ˜¯é¢„å¤„ç†é€Ÿåº¦ã€æ•°æ®ä¼ è¾“é€Ÿç‡è¿˜æ˜¯è®¡ç®—èƒ½åŠ›ï¼Œç“¶é¢ˆé˜¶æ®µå†³å®šäº†æ•´ä¸ªç³»ç»Ÿçš„æœ‰æ•ˆè®­ç»ƒååé‡ã€‚ç†è§£è¿™äº›å…³ç³»ä½¿ç³»ç»Ÿæ¶æ„å¸ˆèƒ½å¤Ÿè®¾è®¡å¹³è¡¡çš„è®­ç»ƒç®¡é“ï¼Œå…¶ä¸­é¢„å¤„ç†èƒ½åŠ›ä¸è®¡ç®—èµ„æºç›¸åŒ¹é…ï¼Œç¡®ä¿æœ€ä½³èµ„æºåˆ©ç”¨ã€‚
- en: '**GPT-2 Language Model Data Pipeline**'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2è¯­è¨€æ¨¡å‹æ•°æ®ç®¡é“**'
- en: Training language models like GPT-2 requires a specialized data pipeline optimized
    for text processing.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¦‚GPT-2ä¹‹ç±»çš„è¯­è¨€æ¨¡å‹éœ€è¦ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬å¤„ç†ä¼˜åŒ–çš„ä¸“ç”¨æ•°æ®ç®¡é“ã€‚
- en: '**Pipeline Stages**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç®¡é“é˜¶æ®µ**'
- en: Raw Text Storage (Storage Zone)
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬å­˜å‚¨ï¼ˆå­˜å‚¨åŒºï¼‰
- en: 'OpenWebText dataset: ~40GB raw text files'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenWebTextæ•°æ®é›†ï¼šçº¦40GBåŸå§‹æ–‡æœ¬æ–‡ä»¶
- en: 'Stored on NVMe SSD: 3.5 GB/s sequential read bandwidth'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­˜å‚¨åœ¨NVMe SSDä¸Šï¼š3.5 GB/sé¡ºåºè¯»å–å¸¦å®½
- en: 'Random access to different documents: ~0.35 GB/s effective (F_access â‰ˆ 0.1)'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæœºè®¿é—®ä¸åŒæ–‡æ¡£ï¼šçº¦0.35 GB/sæœ‰æ•ˆï¼ˆF_access â‰ˆ 0.1ï¼‰
- en: Tokenization (CPU Preprocessing Zone)
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ†è¯ï¼ˆCPUé¢„å¤„ç†åŒºï¼‰
- en: BPE (Byte-Pair Encoding) tokenizer (50,257 vocabulary) converts text to token
    IDs
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPE (å­—èŠ‚å¯¹ç¼–ç ) åˆ†è¯å™¨ï¼ˆ50,257 è¯æ±‡é‡ï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°ID
- en: BPE segments text into subword units (e.g., â€œunbreakableâ€ â†’ [â€œunâ€, â€œbreakâ€,
    â€œableâ€])
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPEå°†æ–‡æœ¬åˆ†å‰²æˆå­è¯å•å…ƒï¼ˆä¾‹å¦‚ï¼Œâ€œunbreakableâ€ â†’ [â€œunâ€, â€œbreakâ€, â€œableâ€])
- en: 'Processing rate: ~500K tokens/second per CPU core'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤„ç†é€Ÿç‡ï¼šæ¯ä¸ªCPUæ ¸å¿ƒçº¦500Kæ ‡è®°/ç§’
- en: 'For batch_size=32, seq_len=1024: need 32K tokens/batch'
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºbatch_size=32ï¼Œseq_len=1024ï¼šéœ€è¦32Kæ ‡è®°/æ‰¹
- en: 'Single core: 32K tokens Ã· 500K tokens/s = 64ms per batch'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•æ ¸ï¼š32Kæ ‡è®° Ã· 500Kæ ‡è®°/ç§’ = æ¯æ‰¹64ms
- en: 'Bottleneck: GPU forward pass only takes 80ms'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç“¶é¢ˆï¼šGPUæ­£å‘ä¼ é€’ä»…éœ€è¦80ms
- en: Batching & Padding (CPU)
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†ä¸å¡«å……ï¼ˆCPUï¼‰
- en: Pad sequences to uniform length (1024 tokens)
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†åºåˆ—å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦ï¼ˆ1024ä¸ªæ ‡è®°ï¼‰
- en: 'Pack into tensors: [32, 1024] int64 = 256KB per batch'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰“åŒ…æˆå¼ é‡ï¼š[32, 1024] int64 = æ¯æ‰¹256KB
- en: 'Trivial time: <5ms'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®€å•æ—¶é—´ï¼š<5ms
- en: GPU Transfer (PCIe)
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPUä¼ è¾“ï¼ˆPCIeï¼‰
- en: 'PCIe Gen3 x16: 15.75 GB/s theoretical'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCIe Gen3 x16ï¼š15.75 GB/sç†è®ºå€¼
- en: 256KB per batch Ã· 15.75 GB/s = 0.016ms (negligible)
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ‰¹256KB Ã· 15.75 GB/s = 0.016msï¼ˆå¯å¿½ç•¥ä¸è®¡ï¼‰
- en: '**Bottleneck Analysis**'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç“¶é¢ˆåˆ†æ**'
- en: 'Tokenization: 64ms'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†è¯ï¼š64ms
- en: 'GPU compute: 80ms'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUè®¡ç®—ï¼š80ms
- en: 'Transfer: <1ms'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼ è¾“ï¼š<1ms
- en: 'System is balanced (tokenization â‰ˆ GPU compute), but tokenization becomes bottleneck
    with faster GPUs (A100: 45ms compute means tokenization limits throughput).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå¹³è¡¡ï¼ˆåˆ†è¯ â‰ˆ GPUè®¡ç®—ï¼‰ï¼Œä½†åˆ†è¯æˆä¸ºç“¶é¢ˆï¼Œéšç€GPUé€Ÿåº¦çš„æé«˜ï¼ˆA100ï¼š45msè®¡ç®—æ„å‘³ç€åˆ†è¯é™åˆ¶äº†ååé‡ï¼‰ã€‚
- en: '**Optimization Applied**'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**åº”ç”¨ä¼˜åŒ–**'
- en: 'Multi-worker dataloading: 8 CPU workers tokenize in parallel â†’ 64ms Ã· 8 = 8ms'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šå·¥ä½œè€…æ•°æ®åŠ è½½ï¼š8ä¸ªCPUå·¥ä½œè€…å¹¶è¡Œåˆ†è¯ â†’ 64ms Ã· 8 = 8ms
- en: 'Prefetching: Tokenize next batch while GPU processes current batch'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„å–ï¼šåœ¨GPUå¤„ç†å½“å‰æ‰¹æ¬¡çš„æ–‡æœ¬æ—¶ï¼Œåˆ†è¯ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
- en: 'Result: GPU utilization >95%, training throughput: 380 samples/second on 8Ã—V100'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœï¼šGPUåˆ©ç”¨ç‡>95%ï¼Œè®­ç»ƒååé‡ï¼š8Ã—V100ä¸Šçš„380ä¸ªæ ·æœ¬/ç§’
- en: '**Key Insight:** Text tokenization is CPU-bound (unlike image preprocessing
    which is I/O-bound). Language model training requires different pipeline optimizations
    than vision models.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®è§è§£**ï¼šæ–‡æœ¬åˆ†è¯æ˜¯CPUå¯†é›†å‹ï¼ˆä¸I/Oå¯†é›†å‹çš„å›¾åƒé¢„å¤„ç†ä¸åŒï¼‰ã€‚è¯­è¨€æ¨¡å‹è®­ç»ƒéœ€è¦ä¸è§†è§‰æ¨¡å‹ä¸åŒçš„ç®¡é“ä¼˜åŒ–ã€‚'
- en: Byte-Pair Encoding is a subword tokenization algorithm that segments text into
    frequent subword units rather than complete words, enabling efficient representation
    with fixed vocabulary size while handling rare words through composition. This
    preprocessing step transforms variable-length text into fixed-length integer sequences
    suitable for neural network processing.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: å­—èŠ‚å¯¹ç¼–ç æ˜¯ä¸€ç§å­è¯åˆ†è¯ç®—æ³•ï¼Œå®ƒå°†æ–‡æœ¬åˆ†å‰²æˆé¢‘ç¹çš„å­è¯å•å…ƒï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å•è¯ï¼Œä»è€Œåœ¨å›ºå®šè¯æ±‡é‡ä¸‹å®ç°é«˜æ•ˆçš„è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ç»„åˆå¤„ç†ç½•è§å•è¯ã€‚è¿™ä¸ªé¢„å¤„ç†æ­¥éª¤å°†å¯å˜é•¿åº¦çš„æ–‡æœ¬è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„æ•´æ•°åºåˆ—ï¼Œé€‚åˆç¥ç»ç½‘ç»œå¤„ç†ã€‚
- en: System Implications
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: 'The relationship between data pipeline architecture and computational resources
    directly determines the performance of machine learning training systems. This
    relationship can be simply expressed through a basic throughput equation: <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">system</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>T</mi><mtext mathvariant="normal">pipeline</mtext></msub><mo>,</mo><msub><mi>T</mi><mtext
    mathvariant="normal">compute</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{system}} =\min(T_{\text{pipeline}}, T_{\text{compute}})</annotation></semantics>
    where <semantics><msub><mi>T</mi><mtext mathvariant="normal">system</mtext></msub><annotation
    encoding="application/x-tex">T_{\text{system}}</annotation></semantics> represents
    the overall system throughput, constrained by both pipeline throughput (<semantics><msub><mi>T</mi><mtext
    mathvariant="normal">pipeline</mtext></msub><annotation encoding="application/x-tex">T_{\text{pipeline}}</annotation></semantics>)
    and computational speed (<semantics><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub><annotation
    encoding="application/x-tex">T_{\text{compute}}</annotation></semantics>).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“æ¶æ„ä¸è®¡ç®—èµ„æºä¹‹é—´çš„å…³ç³»ç›´æ¥å†³å®šäº†æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿçš„æ€§èƒ½ã€‚è¿™ç§å…³ç³»å¯ä»¥é€šè¿‡ä¸€ä¸ªåŸºæœ¬çš„ååé‡æ–¹ç¨‹ç®€å•åœ°è¡¨è¾¾ï¼š<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">system</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>T</mi><mtext mathvariant="normal">pipeline</mtext></msub><mo>,</mo><msub><mi>T</mi><mtext
    mathvariant="normal">compute</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">T_{\text{system}} =\min(T_{\text{pipeline}}, T_{\text{compute}})</annotation></semantics>
    å…¶ä¸­ <semantics><msub><mi>T</mi><mtext mathvariant="normal">system</mtext></msub><annotation
    encoding="application/x-tex">T_{\text{system}}</annotation></semantics> ä»£è¡¨æ•´ä½“ç³»ç»Ÿååé‡ï¼Œå—ç®¡é“ååé‡ï¼ˆ<semantics><msub><mi>T</mi><mtext
    mathvariant="normal">pipeline</mtext></msub><annotation encoding="application/x-tex">T_{\text{pipeline}}</annotation></semantics>ï¼‰å’Œè®¡ç®—é€Ÿåº¦ï¼ˆ<semantics><msub><mi>T</mi><mtext
    mathvariant="normal">compute</mtext></msub><annotation encoding="application/x-tex">T_{\text{compute}}</annotation></semantics>ï¼‰çš„åŒé‡é™åˆ¶ã€‚
- en: To illustrate these constraints, consider image classification systems. The
    performance dynamics can be analyzed through two critical metrics. The GPU Processing
    Rate (<semantics><msub><mi>R</mi><mtext mathvariant="normal">GPU</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{GPU}}</annotation></semantics>) represents
    the maximum number of images a GPU can process per second, determined by model
    architecture complexity and GPU hardware capabilities. The Pipeline Delivery Rate
    (<semantics><msub><mi>R</mi><mtext mathvariant="normal">pipeline</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{pipeline}}</annotation></semantics>) is
    the rate at which the data pipeline can deliver preprocessed images to the GPU.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™äº›é™åˆ¶ï¼Œå¯ä»¥è€ƒè™‘å›¾åƒåˆ†ç±»ç³»ç»Ÿã€‚å…¶æ€§èƒ½åŠ¨æ€å¯ä»¥é€šè¿‡ä¸¤ä¸ªå…³é”®æŒ‡æ ‡æ¥åˆ†æã€‚GPUå¤„ç†é€Ÿç‡ï¼ˆ<semantics><msub><mi>R</mi><mtext
    mathvariant="normal">GPU</mtext></msub><annotation encoding="application/x-tex">R_{\text{GPU}}</annotation></semantics>ï¼‰è¡¨ç¤ºGPUæ¯ç§’å¯ä»¥å¤„ç†çš„å›¾åƒæœ€å¤§æ•°é‡ï¼Œç”±æ¨¡å‹æ¶æ„å¤æ‚æ€§å’ŒGPUç¡¬ä»¶èƒ½åŠ›å†³å®šã€‚ç®¡é“äº¤ä»˜é€Ÿç‡ï¼ˆ<semantics><msub><mi>R</mi><mtext
    mathvariant="normal">pipeline</mtext></msub><annotation encoding="application/x-tex">R_{\text{pipeline}}</annotation></semantics>ï¼‰æ˜¯æ•°æ®ç®¡é“å°†é¢„å¤„ç†å›¾åƒäº¤ä»˜ç»™GPUçš„é€Ÿç‡ã€‚
- en: 'In this case, at a high level, the systemâ€™s effective training speed is governed
    by the lower of these two rates. When <semantics><msub><mi>R</mi><mtext mathvariant="normal">pipeline</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{pipeline}}</annotation></semantics> is less
    than <semantics><msub><mi>R</mi><mtext mathvariant="normal">GPU</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{GPU}}</annotation></semantics>, the system
    experiences underutilization of GPU resources. The degree of GPU utilization can
    be expressed as: <semantics><mrow><mtext mathvariant="normal">GPU Utilization</mtext><mo>=</mo><mfrac><msub><mi>R</mi><mtext
    mathvariant="normal">pipeline</mtext></msub><msub><mi>R</mi><mtext mathvariant="normal">GPU</mtext></msub></mfrac><mo>Ã—</mo><mn>100</mn><mi>%</mi></mrow><annotation
    encoding="application/x-tex">\text{GPU Utilization} = \frac{R_{\text{pipeline}}}{R_{\text{GPU}}}
    \times 100\%</annotation></semantics>'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œç³»ç»Ÿçš„æœ‰æ•ˆè®­ç»ƒé€Ÿåº¦ç”±è¿™ä¸¤ä¸ªé€Ÿç‡ä¸­è¾ƒä½çš„ä¸€ä¸ªå†³å®šã€‚å½“ <semantics><msub><mi>R</mi><mtext mathvariant="normal">pipeline</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{pipeline}}</annotation></semantics> å°äº <semantics><msub><mi>R</mi><mtext
    mathvariant="normal">GPU</mtext></msub><annotation encoding="application/x-tex">R_{\text{GPU}}</annotation></semantics>
    æ—¶ï¼Œç³»ç»Ÿä¼šç»å† GPU èµ„æºçš„ä½åˆ©ç”¨ç‡ã€‚GPU åˆ©ç”¨åº¦çš„ç¨‹åº¦å¯ä»¥è¡¨ç¤ºä¸ºï¼š<semantics><mrow><mtext mathvariant="normal">GPU
    Utilization</mtext><mo>=</mo><mfrac><msub><mi>R</mi><mtext mathvariant="normal">pipeline</mtext></msub><msub><mi>R</mi><mtext
    mathvariant="normal">GPU</mtext></msub></mfrac><mo>Ã—</mo><mn>100</mn><mi>%</mi></mrow><annotation
    encoding="application/x-tex">\text{GPU Utilization} = \frac{R_{\text{pipeline}}}{R_{\text{GPU}}}
    \times 100\%</annotation></semantics>
- en: Consider an example. A ResNet-50 model implemented on modern GPU hardware might
    achieve a processing rate of 1000 images per second. However, if the data pipeline
    can only deliver 200 images per second, the GPU utilization would be merely 20%,
    meaning the GPU remains idle 80% of the time. This results in significantly reduced
    training efficiency. This inefficiency persists even with more powerful GPU hardware,
    as the pipeline throughput becomes the limiting factor in system performance.
    This demonstrates why balanced system design, where pipeline and computational
    capabilities are well-matched, is necessary for optimal training performance.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªä¾‹å­ã€‚åœ¨ç°ä»£ GPU ç¡¬ä»¶ä¸Šå®ç°çš„ ResNet-50 æ¨¡å‹å¯èƒ½è¾¾åˆ°æ¯ç§’å¤„ç† 1000 å¼ å›¾ç‰‡çš„é€Ÿç‡ã€‚ç„¶è€Œï¼Œå¦‚æœæ•°æ®ç®¡é“åªèƒ½æ¯ç§’æä¾› 200 å¼ å›¾ç‰‡ï¼ŒGPU
    åˆ©ç”¨ç‡å°†ä»…ä¸º 20%ï¼Œè¿™æ„å‘³ç€ GPU æœ‰ 80% çš„æ—¶é—´å¤„äºç©ºé—²çŠ¶æ€ã€‚è¿™ä¼šå¯¼è‡´è®­ç»ƒæ•ˆç‡æ˜¾è‘—é™ä½ã€‚å³ä½¿æ‹¥æœ‰æ›´å¼ºå¤§çš„ GPU ç¡¬ä»¶ï¼Œè¿™ç§ä½æ•ˆæ€§ä»ç„¶å­˜åœ¨ï¼Œå› ä¸ºç®¡é“ååé‡æˆä¸ºç³»ç»Ÿæ€§èƒ½çš„é™åˆ¶å› ç´ ã€‚è¿™è¯æ˜äº†ä¸ºä»€ä¹ˆå¹³è¡¡çš„ç³»ç»Ÿè®¾è®¡ï¼Œå…¶ä¸­ç®¡é“å’Œè®¡ç®—èƒ½åŠ›ç›¸åŒ¹é…ï¼Œå¯¹äºæœ€ä½³è®­ç»ƒæ€§èƒ½æ˜¯å¿…è¦çš„ã€‚
- en: Data Flows
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®æµ
- en: 'Machine learning systems manage complex data flows through multiple memory
    tiers[24](#fn24) while coordinating pipeline operations. The interplay between
    memory bandwidth constraints and pipeline execution directly impacts training
    performance. The maximum data transfer rate through the memory hierarchy is bounded
    by: <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">memory</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">storage</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">system</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext mathvariant="normal">accelerator</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_{\text{memory}}
    =\min(B_{\text{storage}}, B_{\text{system}}, B_{\text{accelerator}})</annotation></semantics>
    Where bandwidth varies significantly across tiers:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ç³»ç»Ÿé€šè¿‡å¤šä¸ªå†…å­˜å±‚çº§[24](#fn24)ç®¡ç†å¤æ‚çš„æ•°æ®æµï¼ŒåŒæ—¶åè°ƒç®¡é“æ“ä½œã€‚å†…å­˜å¸¦å®½çº¦æŸä¸ç®¡é“æ‰§è¡Œä¹‹é—´çš„ç›¸äº’ä½œç”¨ç›´æ¥å½±å“è®­ç»ƒæ€§èƒ½ã€‚é€šè¿‡å†…å­˜å±‚æ¬¡ç»“æ„çš„æ•°æ®ä¼ è¾“æœ€å¤§é€Ÿç‡å—é™äºï¼š<semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">memory</mtext></msub><mo>=</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">storage</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext
    mathvariant="normal">system</mtext></msub><mo>,</mo><msub><mi>B</mi><mtext mathvariant="normal">accelerator</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_{\text{memory}}
    =\min(B_{\text{storage}}, B_{\text{system}}, B_{\text{accelerator}})</annotation></semantics>
    å…¶ä¸­ï¼Œä¸åŒå±‚çº§çš„å¸¦å®½å·®å¼‚å¾ˆå¤§ï¼š
- en: 'Storage (<semantics><msub><mi>B</mi><mtext mathvariant="normal">storage</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{storage}}</annotation></semantics>): NVMe
    storage devices provide 1-2 GB/s'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å­˜å‚¨ (<semantics><msub><mi>B</mi><mtext mathvariant="normal">storage</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{storage}}</annotation></semantics>): NVMe
    å­˜å‚¨è®¾å¤‡æä¾› 1-2 GB/s'
- en: 'System (<semantics><msub><mi>B</mi><mtext mathvariant="normal">system</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{system}}</annotation></semantics>): Main
    memory transfers data at 50-100 GB/s'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ç³»ç»Ÿ (<semantics><msub><mi>B</mi><mtext mathvariant="normal">system</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{system}}</annotation></semantics>): ä¸»å†…å­˜çš„æ•°æ®ä¼ è¾“é€Ÿç‡ä¸º
    50-100 GB/s'
- en: 'Accelerator (<semantics><msub><mi>B</mi><mtext mathvariant="normal">accelerator</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{accelerator}}</annotation></semantics>):
    GPU memory achieves 900 GB/s or higher'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿå™¨ï¼ˆ<semantics><msub><mi>B</mi><mtext mathvariant="normal">accelerator</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{accelerator}}</annotation></semantics>ï¼‰ï¼šGPUå†…å­˜è¾¾åˆ°900
    GB/sæˆ–æ›´é«˜
- en: 'These order-of-magnitude differences create distinct performance characteristics
    that must be carefully managed. The total time required for each training iteration
    comprises multiple pipelined operations: <semantics><mrow><msub><mi>t</mi><mtext
    mathvariant="normal">iteration</mtext></msub><mo>=</mo><mo>max</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>t</mi><mtext mathvariant="normal">fetch</mtext></msub><mo>,</mo><msub><mi>t</mi><mtext
    mathvariant="normal">process</mtext></msub><mo>,</mo><msub><mi>t</mi><mtext mathvariant="normal">transfer</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t_{\text{iteration}}
    =\max(t_{\text{fetch}}, t_{\text{process}}, t_{\text{transfer}})</annotation></semantics>'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ•°é‡çº§ä¸Šçš„å·®å¼‚åˆ›é€ äº†å¿…é¡»ä»”ç»†ç®¡ç†çš„ç‹¬ç‰¹æ€§èƒ½ç‰¹å¾ã€‚æ¯ä¸ªè®­ç»ƒè¿­ä»£æ‰€éœ€çš„æ€»æ—¶é—´åŒ…æ‹¬å¤šä¸ªæµæ°´çº¿æ“ä½œï¼š<semantics><mrow><msub><mi>t</mi><mtext
    mathvariant="normal">iteration</mtext></msub><mo>=</mo><mo>max</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>t</mi><mtext mathvariant="normal">fetch</mtext></msub><mo>,</mo><msub><mi>t</mi><mtext
    mathvariant="normal">process</mtext></msub><mo>,</mo><msub><mi>t</mi><mtext mathvariant="normal">transfer</mtext></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t_{\text{iteration}}
    =\max(t_{\text{fetch}}, t_{\text{process}}, t_{\text{transfer}})</annotation></semantics>
- en: 'This equation captures three components: storage read time (<semantics><msub><mi>t</mi><mtext
    mathvariant="normal">fetch?</mtext></msub><annotation encoding="application/x-tex">t_{\text{fetch?}}</annotation></semantics>),
    preprocessing time (<semantics><msub><mi>t</mi><mtext mathvariant="normal">process</mtext></msub><annotation
    encoding="application/x-tex">t_{\text{process}}</annotation></semantics>), and
    accelerator transfer time (<semantics><msub><mi>t</mi><mtext mathvariant="normal">transfer</mtext></msub><annotation
    encoding="application/x-tex">t_{\text{transfer}}</annotation></semantics>).'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹ç¨‹æ•æ‰äº†ä¸‰ä¸ªç»„ä»¶ï¼šå­˜å‚¨è¯»å–æ—¶é—´ï¼ˆ<semantics><msub><mi>t</mi><mtext mathvariant="normal">fetch?</mtext></msub><annotation
    encoding="application/x-tex">t_{\text{fetch?}}</annotation></semantics>ï¼‰ã€é¢„å¤„ç†æ—¶é—´ï¼ˆ<semantics><msub><mi>t</mi><mtext
    mathvariant="normal">process</mtext></msub><annotation encoding="application/x-tex">t_{\text{process}}</annotation></semantics>ï¼‰å’ŒåŠ é€Ÿå™¨ä¼ è¾“æ—¶é—´ï¼ˆ<semantics><msub><mi>t</mi><mtext
    mathvariant="normal">transfer</mtext></msub><annotation encoding="application/x-tex">t_{\text{transfer}}</annotation></semantics>ï¼‰ã€‚
- en: Modern training architectures optimize performance by overlapping these operations.
    When one batch undergoes preprocessing, the system simultaneously fetches the
    next batch from storage while transferring the previously processed batch to accelerator
    memory.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£è®­ç»ƒæ¶æ„é€šè¿‡é‡å è¿™äº›æ“ä½œæ¥ä¼˜åŒ–æ€§èƒ½ã€‚å½“ä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†æ—¶ï¼Œç³»ç»ŸåŒæ—¶ä»å­˜å‚¨ä¸­æ£€ç´¢ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼ŒåŒæ—¶å°†ä¹‹å‰å¤„ç†è¿‡çš„æ‰¹æ¬¡ä¼ è¾“åˆ°åŠ é€Ÿå™¨å†…å­˜ä¸­ã€‚
- en: This coordinated movement requires precise management of system resources, particularly
    memory buffers and processing units. The memory hierarchy must account for bandwidth
    disparities while maintaining continuous data flow. Effective pipelining minimizes
    idle time and maximizes resource utilization through careful buffer sizing and
    memory allocation strategies. The successful orchestration of these components
    enables efficient training across the memory hierarchy while managing the inherent
    bandwidth constraints of each tier.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åè°ƒè¿åŠ¨éœ€è¦ç²¾ç¡®ç®¡ç†ç³»ç»Ÿèµ„æºï¼Œå°¤å…¶æ˜¯å†…å­˜ç¼“å†²åŒºå’Œå¤„ç†å•å…ƒã€‚å†…å­˜å±‚æ¬¡ç»“æ„å¿…é¡»è€ƒè™‘åˆ°å¸¦å®½å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒè¿ç»­çš„æ•°æ®æµã€‚æœ‰æ•ˆçš„æµæ°´çº¿é€šè¿‡ä»”ç»†çš„ç¼“å†²åŒºå¤§å°å’Œå†…å­˜åˆ†é…ç­–ç•¥æœ€å°åŒ–ç©ºé—²æ—¶é—´ï¼Œæœ€å¤§åŒ–èµ„æºåˆ©ç”¨ã€‚è¿™äº›ç»„ä»¶çš„æˆåŠŸç¼–æ’ä½¿å¾—åœ¨å†…å­˜å±‚æ¬¡ç»“æ„ä¸­é«˜æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ç®¡ç†æ¯ä¸ªå±‚çš„å›ºæœ‰å¸¦å®½é™åˆ¶ã€‚
- en: Practical Architectures
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å®é™…æ¶æ„
- en: The ImageNet dataset serves as a canonical example for understanding data pipeline
    requirements in modern machine learning systems. This analysis examines system
    performance characteristics when training vision models on large-scale image datasets.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNetæ•°æ®é›†æ˜¯ç†è§£ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­æ•°æ®ç®¡é“éœ€æ±‚çš„ä¸€ä¸ªå…¸å‹ç¤ºä¾‹ã€‚è¯¥åˆ†æè€ƒå¯Ÿäº†åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒè§†è§‰æ¨¡å‹æ—¶çš„ç³»ç»Ÿæ€§èƒ½ç‰¹å¾ã€‚
- en: 'Storage performance in practical systems follows a defined relationship between
    theoretical and practical throughput: <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">practical</mtext></msub><mo>=</mo><mn>0.5</mn><mo>Ã—</mo><msub><mi>B</mi><mtext
    mathvariant="normal">theoretical</mtext></msub></mrow><annotation encoding="application/x-tex">T_{\text{practical}}
    = 0.5 \times B_{\text{theoretical}}</annotation></semantics>'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ç³»ç»Ÿä¸­çš„å­˜å‚¨æ€§èƒ½éµå¾ªç†è®ºååé‡å’Œå®é™…ååé‡ä¹‹é—´çš„å®šä¹‰å…³ç³»ï¼š<semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">practical</mtext></msub><mo>=</mo><mn>0.5</mn><mo>Ã—</mo><msub><mi>B</mi><mtext
    mathvariant="normal">theoretical</mtext></msub></mrow><annotation encoding="application/x-tex">T_{\text{practical}}
    = 0.5 \times B_{\text{theoretical}}</annotation></semantics>
- en: To illustrate this relationship, consider an NVMe storage device with 3GB/s
    theoretical bandwidth. Such a device achieves approximately 1.5GB/s sustained
    read performance. However, the random access patterns required for training data
    shuffling further reduce this effective bandwidth by 90%. System designers must
    account for this reduction through careful memory buffer design.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è¿™ç§å…³ç³»ï¼Œè€ƒè™‘ä¸€ä¸ªå…·æœ‰3GB/sç†è®ºå¸¦å®½çš„NVMeå­˜å‚¨è®¾å¤‡ã€‚è¿™æ ·çš„è®¾å¤‡å¤§çº¦èƒ½è¾¾åˆ°1.5GB/sçš„æŒç»­è¯»å–æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”¨äºè®­ç»ƒæ•°æ®æ´—ç‰Œæ‰€éœ€çš„éšæœºè®¿é—®æ¨¡å¼è¿›ä¸€æ­¥å°†æœ‰æ•ˆå¸¦å®½é™ä½äº†90%ã€‚ç³»ç»Ÿè®¾è®¡è€…å¿…é¡»é€šè¿‡ä»”ç»†çš„å†…å­˜ç¼“å†²åŒºè®¾è®¡æ¥è€ƒè™‘è¿™ç§é™ä½ã€‚
- en: 'The total memory requirements for the system scale with batch size according
    to the following relationship: <semantics><mrow><msub><mi>M</mi><mtext mathvariant="normal">required</mtext></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">prefetch</mtext></msub><mo>+</mo><msub><mi>B</mi><mtext
    mathvariant="normal">processing</mtext></msub><mo>+</mo><msub><mi>B</mi><mtext
    mathvariant="normal">transfer</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><msub><mi>S</mi><mtext
    mathvariant="normal">batch</mtext></msub></mrow><annotation encoding="application/x-tex">M_{\text{required}}
    = (B_{\text{prefetch}} + B_{\text{processing}} + B_{\text{transfer}}) \times S_{\text{batch}}</annotation></semantics>'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿçš„æ€»å†…å­˜éœ€æ±‚æ ¹æ®ä»¥ä¸‹å…³ç³»éšæ‰¹æ¬¡å¤§å°è€Œæ‰©å±•ï¼š<semantics><mrow><msub><mi>M</mi><mtext mathvariant="normal">required</mtext></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mtext mathvariant="normal">prefetch</mtext></msub><mo>+</mo><msub><mi>B</mi><mtext
    mathvariant="normal">processing</mtext></msub><mo>+</mo><msub><mi>B</mi><mtext
    mathvariant="normal">transfer</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><msub><mi>S</mi><mtext
    mathvariant="normal">batch</mtext></msub></mrow><annotation encoding="application/x-tex">M_{\text{required}}
    = (B_{\text{prefetch}} + B_{\text{processing}} + B_{\text{transfer}}) \times S_{\text{batch}}</annotation></semantics>
- en: In this equation, <semantics><msub><mi>B</mi><mtext mathvariant="normal">prefetch</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{prefetch}}</annotation></semantics> represents
    memory allocated for data prefetching, <semantics><msub><mi>B</mi><mtext mathvariant="normal">processing</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{processing}}</annotation></semantics> represents
    memory required for active preprocessing operations, <semantics><msub><mi>B</mi><mtext
    mathvariant="normal">transfer</mtext></msub><annotation encoding="application/x-tex">B_{\text{transfer}}</annotation></semantics>
    represents memory allocated for accelerator transfers, and <semantics><msub><mi>S</mi><mtext
    mathvariant="normal">batch</mtext></msub><annotation encoding="application/x-tex">S_{\text{batch}}</annotation></semantics>
    represents the training batch size.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œ<semantics><msub><mi>B</mi><mtext mathvariant="normal">prefetch</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{prefetch}}</annotation></semantics> è¡¨ç¤ºåˆ†é…ç»™æ•°æ®é¢„å–çš„å†…å­˜ï¼Œ<semantics><msub><mi>B</mi><mtext
    mathvariant="normal">processing</mtext></msub><annotation encoding="application/x-tex">B_{\text{processing}}</annotation></semantics>
    è¡¨ç¤ºç”¨äºä¸»åŠ¨é¢„å¤„ç†æ“ä½œçš„å†…å­˜éœ€æ±‚ï¼Œ<semantics><msub><mi>B</mi><mtext mathvariant="normal">transfer</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{transfer}}</annotation></semantics> è¡¨ç¤ºåˆ†é…ç»™åŠ é€Ÿå™¨ä¼ è¾“çš„å†…å­˜ï¼Œè€Œ
    <semantics><msub><mi>S</mi><mtext mathvariant="normal">batch</mtext></msub><annotation
    encoding="application/x-tex">S_{\text{batch}}</annotation></semantics> è¡¨ç¤ºè®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚
- en: 'Preprocessing operations introduce additional computational requirements. Common
    operations such as image resizing, augmentation, and normalization consume CPU
    resources. These preprocessing operations must satisfy a basic time constraint:
    <semantics><mrow><msub><mi>t</mi><mtext mathvariant="normal">preprocessing</mtext></msub><mo><</mo><msub><mi>t</mi><mtext
    mathvariant="normal">GPU_compute</mtext></msub></mrow><annotation encoding="application/x-tex">t_{\text{preprocessing}}
    < t_{\text{GPU\_compute}}</annotation></semantics>'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ“ä½œå¼•å…¥äº†é¢å¤–çš„è®¡ç®—éœ€æ±‚ã€‚å¸¸è§çš„æ“ä½œï¼Œå¦‚å›¾åƒç¼©æ”¾ã€å¢å¼ºå’Œå½’ä¸€åŒ–ï¼Œæ¶ˆè€—CPUèµ„æºã€‚è¿™äº›é¢„å¤„ç†æ“ä½œå¿…é¡»æ»¡è¶³ä¸€ä¸ªåŸºæœ¬çš„æ—¶é—´çº¦æŸï¼š<semantics><mrow><msub><mi>t</mi><mtext
    mathvariant="normal">preprocessing</mtext></msub><mo><</mo><msub><mi>t</mi><mtext
    mathvariant="normal">GPU_compute</mtext></msub></mrow><annotation encoding="application/x-tex">t_{\text{preprocessing}}
    < t_{\text{GPU\_compute}}</annotation></semantics>
- en: This inequality determines system efficiency. When preprocessing time exceeds
    GPU computation time, accelerator utilization decreases proportionally. The relationship
    between preprocessing and computation time thus establishes efficiency limits
    in training system design.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸ç­‰å¼å†³å®šäº†ç³»ç»Ÿæ•ˆç‡ã€‚å½“é¢„å¤„ç†æ—¶é—´è¶…è¿‡GPUè®¡ç®—æ—¶é—´æ—¶ï¼ŒåŠ é€Ÿå™¨çš„åˆ©ç”¨ç‡æˆæ¯”ä¾‹ä¸‹é™ã€‚å› æ­¤ï¼Œé¢„å¤„ç†å’Œè®¡ç®—æ—¶é—´ä¹‹é—´çš„å…³ç³»åœ¨è®­ç»ƒç³»ç»Ÿè®¾è®¡ä¸­å»ºç«‹äº†æ•ˆç‡é™åˆ¶ã€‚
- en: Forward Pass
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ­£å‘ä¼ æ’­
- en: With the data pipeline providing prepared batches, we can now examine how the
    training loop processes this data. The forward pass implements the mathematical
    operations described in [SectionÂ 8.3.1.1](ch014.xhtml#sec-ai-training-mathematical-operations-neural-networks-abbd),
    where input data propagates through the model to generate predictions. While the
    conceptual flow follows the layer-by-layer transformation <semantics><mrow><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>W</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A^{(l)}
    = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)</annotation></semantics> established
    earlier, the system-level implementation poses several challenges critical for
    efficient execution.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®ç®¡é“æä¾›å‡†å¤‡å¥½çš„æ‰¹æ¬¡åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯å¦‚ä½•å¤„ç†è¿™äº›æ•°æ®ã€‚æ­£å‘ä¼ æ’­å®ç°äº†åœ¨[ç¬¬8.3.1.1èŠ‚](ch014.xhtml#sec-ai-training-mathematical-operations-neural-networks-abbd)ä¸­æè¿°çš„æ•°å­¦è¿ç®—ï¼Œå…¶ä¸­è¾“å…¥æ•°æ®é€šè¿‡æ¨¡å‹ä¼ æ’­ä»¥ç”Ÿæˆé¢„æµ‹ã€‚è™½ç„¶æ¦‚å¿µæµç¨‹éµå¾ªä¹‹å‰å»ºç«‹çš„å±‚å å¼è½¬æ¢
    <semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>W</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A^{(l)}
    = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)</annotation></semantics>ï¼Œä½†ç³»ç»Ÿçº§å®ç°æå‡ºäº†å‡ ä¸ªå¯¹é«˜æ•ˆæ‰§è¡Œè‡³å…³é‡è¦çš„æŒ‘æˆ˜ã€‚
- en: Compute Operations
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ“ä½œ
- en: The forward pass orchestrates the computational patterns introduced in [SectionÂ 8.3.1.2](ch014.xhtml#sec-ai-training-matrix-operations-d7e9),
    optimizing them for specific neural network operations. Building on the matrix
    multiplication foundations, the system must efficiently execute the <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>M</mi><mo>Ã—</mo><mi>B</mi></mrow><annotation
    encoding="application/x-tex">N \times M \times B</annotation></semantics> floating-point
    operations required for each layer, where typical layers with dimensions of <semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>1024</mn></mrow><annotation
    encoding="application/x-tex">512\times1024</annotation></semantics> processing
    batches of 64 samples execute over 33 million operations.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å‘ä¼ æ’­åè°ƒäº†åœ¨[ç¬¬8.3.1.2èŠ‚](ch014.xhtml#sec-ai-training-matrix-operations-d7e9)ä¸­å¼•å…¥çš„è®¡ç®—æ¨¡å¼ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šçš„ç¥ç»ç½‘ç»œæ“ä½œè¿›è¡Œä¼˜åŒ–ã€‚åŸºäºçŸ©é˜µä¹˜æ³•çš„åŸºç¡€ï¼Œç³»ç»Ÿå¿…é¡»é«˜æ•ˆæ‰§è¡Œæ¯ä¸ªå±‚æ‰€éœ€çš„<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>M</mi><mo>Ã—</mo><mi>B</mi></mrow><annotation
    encoding="application/x-tex">N \times M \times B</annotation></semantics>æµ®ç‚¹è¿ç®—ï¼Œå…¶ä¸­å…¸å‹çš„å±‚ï¼Œå…¶ç»´åº¦ä¸º<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>1024</mn></mrow><annotation
    encoding="application/x-tex">512\times1024</annotation></semantics>å¤„ç†64ä¸ªæ ·æœ¬çš„æ‰¹æ¬¡ï¼Œæ‰§è¡Œè¶…è¿‡3300ä¸‡æ¬¡æ“ä½œã€‚
- en: Modern neural architectures extend beyond these basic matrix operations to include
    specialized computational patterns. Convolutional networks[25](#fn25), for instance,
    perform systematic kernel operations across input tensors. Consider a typical
    input tensor of dimensions <semantics><mrow><mn>64</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">64 \times 224 \times 224 \times 3</annotation></semantics>
    (batch size <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    height <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    width <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    channels) processed by <semantics><mrow><mn>7</mn><mo>Ã—</mo><mn>7</mn></mrow><annotation
    encoding="application/x-tex">7 \times 7</annotation></semantics> kernels. Each
    position requires 147 multiply-accumulate operations, and with 64 filters operating
    across <semantics><mrow><mn>218</mn><mo>Ã—</mo><mn>218</mn></mrow><annotation encoding="application/x-tex">218
    \times 218</annotation></semantics> spatial dimensions, the computational demands
    become substantial.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„ä¸ä»…æ‰©å±•åˆ°è¿™äº›åŸºæœ¬çš„çŸ©é˜µè¿ç®—ï¼Œè¿˜åŒ…æ‹¬ä¸“é—¨çš„è®¡ç®—æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œå·ç§¯ç½‘ç»œ[25](#fn25)åœ¨è¾“å…¥å¼ é‡ä¸Šæ‰§è¡Œç³»ç»Ÿçš„æ ¸è¿ç®—ã€‚è€ƒè™‘ä¸€ä¸ªå…¸å‹çš„è¾“å…¥å¼ é‡ï¼Œå…¶ç»´åº¦ä¸º<semantics><mrow><mn>64</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">64 \times 224 \times 224 \times 3</annotation></semantics>ï¼ˆæ‰¹é‡å¤§å°
    <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    é«˜åº¦ <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    å®½åº¦ <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    é€šé“ï¼‰ï¼Œç”±<semantics><mrow><mn>7</mn><mo>Ã—</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7
    \times 7</annotation></semantics>æ ¸å¤„ç†ã€‚æ¯ä¸ªä½ç½®éœ€è¦147æ¬¡ä¹˜åŠ è¿ç®—ï¼Œå¹¶ä¸”æœ‰64ä¸ªè¿‡æ»¤å™¨åœ¨<semantics><mrow><mn>218</mn><mo>Ã—</mo><mn>218</mn></mrow><annotation
    encoding="application/x-tex">218 \times 218</annotation></semantics>ç©ºé—´ç»´åº¦ä¸Šæ“ä½œï¼Œè®¡ç®—éœ€æ±‚å˜å¾—ç›¸å½“å¤§ã€‚
- en: Transformer architectures introduce attention mechanisms[26](#fn26), which compute
    similarity scores between sequences. These operations combine matrix multiplications
    with softmax normalization, requiring efficient broadcasting and reduction operations
    across varying sequence lengths. The computational pattern here differs significantly
    from convolutions, demanding flexible execution strategies from hardware accelerators.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Transformeræ¶æ„å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶[26](#fn26)ï¼Œè¿™äº›æœºåˆ¶è®¡ç®—åºåˆ—ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚è¿™äº›æ“ä½œç»“åˆäº†çŸ©é˜µä¹˜æ³•ä¸softmaxå½’ä¸€åŒ–ï¼Œéœ€è¦åœ¨ä¸åŒåºåˆ—é•¿åº¦ä¸Šæ‰§è¡Œé«˜æ•ˆçš„å¹¿æ’­å’Œå½’çº¦æ“ä½œã€‚è¿™é‡Œçš„è®¡ç®—æ¨¡å¼ä¸å·ç§¯æ˜¾è‘—ä¸åŒï¼Œéœ€è¦ç¡¬ä»¶åŠ é€Ÿå™¨çµæ´»çš„æ‰§è¡Œç­–ç•¥ã€‚
- en: Throughout these networks, element-wise operations play an important supporting
    role. Activation functions like ReLU and sigmoid transform values independently.
    While conceptually simple, these operations can become bottlenecked by memory
    bandwidth rather than computational capacity, as they perform relatively few calculations
    per memory access. Batch normalization presents similar challenges, computing
    statistics and normalizing values across batch dimensions while creating synchronization
    points in the computation pipeline.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›ç½‘ç»œä¸­ï¼Œé€å…ƒç´ æ“ä½œæ‰®æ¼”ç€é‡è¦çš„è¾…åŠ©è§’è‰²ã€‚æ¿€æ´»å‡½æ•°å¦‚ReLUå’Œsigmoidç‹¬ç«‹åœ°è½¬æ¢å€¼ã€‚è™½ç„¶æ¦‚å¿µä¸Šç®€å•ï¼Œä½†è¿™äº›æ“ä½œå¯èƒ½ä¼šå› ä¸ºå†…å­˜å¸¦å®½è€Œä¸æ˜¯è®¡ç®—èƒ½åŠ›è€Œæˆä¸ºç“¶é¢ˆï¼Œå› ä¸ºå®ƒä»¬åœ¨æ¯æ¬¡å†…å­˜è®¿é—®ä¸­è¿›è¡Œçš„è®¡ç®—ç›¸å¯¹è¾ƒå°‘ã€‚æ‰¹é‡å½’ä¸€åŒ–ä¹Ÿé¢ä¸´ç€ç±»ä¼¼çš„æŒ‘æˆ˜ï¼Œå®ƒåœ¨æ‰¹å¤„ç†ç»´åº¦ä¸Šè®¡ç®—ç»Ÿè®¡æ•°æ®å¹¶å½’ä¸€åŒ–å€¼ï¼ŒåŒæ—¶åœ¨è®¡ç®—ç®¡é“ä¸­åˆ›å»ºåŒæ­¥ç‚¹ã€‚
- en: Modern hardware accelerators, particularly GPUs, optimize these diverse computations
    through massive parallelization. Achieving peak performance requires careful attention
    to hardware architecture. GPUs process data in fixed-size blocks of threads called
    warps (in NVIDIA architectures) or wavefronts (in AMD architectures). Peak efficiency
    occurs when matrix dimensions align with these hardware-specific sizes. For instance,
    NVIDIA GPUs typically achieve optimal performance when processing matrices aligned
    to <semantics><mrow><mn>32</mn><mo>Ã—</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times32</annotation></semantics>
    dimensions.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œå°¤å…¶æ˜¯GPUï¼Œé€šè¿‡å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ä¼˜åŒ–è¿™äº›å¤šæ ·åŒ–çš„è®¡ç®—ã€‚å®ç°å³°å€¼æ€§èƒ½éœ€è¦ä»”ç»†å…³æ³¨ç¡¬ä»¶æ¶æ„ã€‚GPUåœ¨å›ºå®šå¤§å°çš„çº¿ç¨‹å—ä¸­å¤„ç†æ•°æ®ï¼Œè¿™äº›çº¿ç¨‹å—ç§°ä¸ºwarpï¼ˆåœ¨NVIDIAæ¶æ„ä¸­ï¼‰æˆ–wavefrontï¼ˆåœ¨AMDæ¶æ„ä¸­ï¼‰ã€‚å½“çŸ©é˜µç»´åº¦ä¸è¿™äº›ç¡¬ä»¶ç‰¹å®šçš„å°ºå¯¸å¯¹é½æ—¶ï¼Œå³°å€¼æ•ˆç‡å‘ç”Ÿã€‚ä¾‹å¦‚ï¼ŒNVIDIA
    GPUåœ¨å¤„ç†å¯¹é½åˆ°<semantics><mrow><mn>32</mn><mo>Ã—</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times32</annotation></semantics>ç»´åº¦çš„çŸ©é˜µæ—¶é€šå¸¸è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚
- en: Libraries like [cuDNN](https://developer.nvidia.com/cudnn) address these challenges
    by providing optimized implementations for each operation type. These systems
    dynamically select algorithms based on input dimensions, hardware capabilities,
    and memory constraints. The selection process balances computational efficiency
    with memory usage, often requiring empirical measurement to determine optimal
    configurations for specific hardware setups.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº [cuDNN](https://developer.nvidia.com/cudnn) è¿™æ ·çš„åº“é€šè¿‡ä¸ºæ¯ç§æ“ä½œç±»å‹æä¾›ä¼˜åŒ–çš„å®ç°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¿™äº›ç³»ç»Ÿæ ¹æ®è¾“å…¥ç»´åº¦ã€ç¡¬ä»¶èƒ½åŠ›å’Œå†…å­˜é™åˆ¶åŠ¨æ€é€‰æ‹©ç®—æ³•ã€‚é€‰æ‹©è¿‡ç¨‹åœ¨è®¡ç®—æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé€šå¸¸éœ€è¦é€šè¿‡ç»éªŒæµ‹é‡æ¥ç¡®å®šç‰¹å®šç¡¬ä»¶é…ç½®çš„æœ€ä½³é…ç½®ã€‚
- en: These hardware utilization patterns reinforce the efficiency principles established
    earlier. When batch size decreases from 32 to 16, GPU utilization often drops
    due to incomplete warp occupation. The tension between larger batch sizes (better
    utilization) and memory constraints (forcing smaller batches) exemplifies how
    the central hardware-software trade-offs permeate all levels of training system
    design.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç¡¬ä»¶åˆ©ç”¨æ¨¡å¼å¼ºåŒ–äº†ä¹‹å‰å»ºç«‹çš„æ•ˆç‡åŸåˆ™ã€‚å½“æ‰¹é‡å¤§å°ä» 32 å‡å°‘åˆ° 16 æ—¶ï¼Œç”±äºæœªå®Œå…¨å ç”¨ warpï¼ŒGPU åˆ©ç”¨ç‡é€šå¸¸ä¼šä¸‹é™ã€‚åœ¨æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼ˆæ›´å¥½çš„åˆ©ç”¨ç‡ï¼‰å’Œå†…å­˜é™åˆ¶ï¼ˆè¿«ä½¿æ‰¹é‡æ›´å°ï¼‰ä¹‹é—´çš„å¼ åŠ›ï¼Œä½“ç°äº†ä¸­å¤®ç¡¬ä»¶-è½¯ä»¶æƒè¡¡å¦‚ä½•æ¸—é€åˆ°è®­ç»ƒç³»ç»Ÿè®¾è®¡çš„æ‰€æœ‰å±‚é¢ã€‚
- en: Memory Management
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜ç®¡ç†
- en: Memory management is a critical challenge in general, but it is particularly
    important during the forward pass when intermediate activations must be stored
    for subsequent backward propagation. The total memory footprint grows with both
    network depth and batch size, following a basic relationship. <semantics><mrow><mtext
    mathvariant="normal">Total Memory</mtext><mo>âˆ¼</mo><mi>B</mi><mo>Ã—</mo><munderover><mo>âˆ‘</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>A</mi><mi>l</mi></msub></mrow>
    <annotation encoding="application/x-tex">\text{Total Memory} \sim B \times \sum_{l=1}^{L}
    A_l</annotation></semantics> where <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    represents the batch size, <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    is the number of layers, and <semantics><msub><mi>A</mi><mi>l</mi></msub><annotation
    encoding="application/x-tex">A_l</annotation></semantics> represents the activation
    size at layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>.
    This simple equation masks considerable complexity in practice.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜ç®¡ç†åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œä½†åœ¨æ­£å‘ä¼ æ’­æœŸé—´å°¤å…¶é‡è¦ï¼Œå› ä¸ºå¿…é¡»å­˜å‚¨ä¸­é—´æ¿€æ´»ä»¥ä¾›åç»­åå‘ä¼ æ’­ã€‚æ€»å†…å­˜å ç”¨éšç€ç½‘ç»œæ·±åº¦å’Œæ‰¹é‡å¤§å°çš„å¢åŠ è€Œå¢é•¿ï¼Œéµå¾ªä¸€ä¸ªåŸºæœ¬å…³ç³»ã€‚
    <semantics><mrow><mtext mathvariant="normal">æ€»å†…å­˜</mtext><mo>âˆ¼</mo><mi>B</mi><mo>Ã—</mo><munderover><mo>âˆ‘</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><msub><mi>A</mi><mi>l</mi></msub></mrow>
    <annotation encoding="application/x-tex">\text{æ€»å†…å­˜} \sim B \times \sum_{l=1}^{L}
    A_l</annotation></semantics> å…¶ä¸­ <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    ä»£è¡¨æ‰¹é‡å¤§å°ï¼Œ<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    æ˜¯å±‚æ•°ï¼Œ<semantics><msub><mi>A</mi><mi>l</mi></msub><annotation encoding="application/x-tex">A_l</annotation></semantics>
    ä»£è¡¨ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„æ¿€æ´»å¤§å°ã€‚è¿™ä¸ªç®€å•çš„æ–¹ç¨‹æ©ç›–äº†å®è·µä¸­ç›¸å½“å¤§çš„å¤æ‚æ€§ã€‚
- en: 'Consider a representative large model like ResNet-50 (a widely-used image classification
    architecture) processing images at <semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times224</annotation></semantics> resolution
    with a batch size of 32\. The initial convolutional layer produces activation
    maps of dimension <semantics><mrow><mn>112</mn><mo>Ã—</mo><mn>112</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">112\times112\times64</annotation></semantics>. Using
    single-precision floating-point format (4 bytes per value), this single layerâ€™s
    activation storage requires approximately 98 MB. As the network progresses through
    its 50 layers, the cumulative memory demands grow substantially: the complete
    forward pass activations total approximately 8GB, gradients require an additional
    4GB, and model parameters consume 200MB. This 12.2GB total represents over 30%
    of a high-end A100 GPUâ€™s 40GB memory capacity for a single batch.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªä»£è¡¨æ€§çš„å¤§å‹æ¨¡å‹ï¼Œå¦‚ResNet-50ï¼ˆä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å›¾åƒåˆ†ç±»æ¶æ„ï¼‰ï¼Œä»¥32ä¸ªæ‰¹æ¬¡çš„å°ºå¯¸å¤„ç†åˆ†è¾¨ç‡ä¸º<semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times224</annotation></semantics>çš„å›¾åƒã€‚åˆå§‹å·ç§¯å±‚äº§ç”Ÿç»´åº¦ä¸º<semantics><mrow><mn>112</mn><mo>Ã—</mo><mn>112</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">112\times112\times64</annotation></semantics>çš„æ¿€æ´»å›¾ã€‚ä½¿ç”¨å•ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼ˆæ¯ä¸ªå€¼4å­—èŠ‚ï¼‰ï¼Œè¿™ä¸€å±‚æ¿€æ´»çš„å­˜å‚¨éœ€è¦å¤§çº¦98
    MBã€‚éšç€ç½‘ç»œé€šè¿‡å…¶50å±‚ï¼Œç´¯ç§¯çš„å†…å­˜éœ€æ±‚æ˜¾è‘—å¢åŠ ï¼šå®Œæ•´çš„æ­£å‘ä¼ é€’æ¿€æ´»æ€»é‡å¤§çº¦ä¸º8GBï¼Œæ¢¯åº¦éœ€è¦é¢å¤–çš„4GBï¼Œæ¨¡å‹å‚æ•°æ¶ˆè€—200MBã€‚è¿™12.2GBçš„æ€»æ•°ä»£è¡¨äº†é«˜ç«¯A100
    GPU 40GBå†…å­˜å®¹é‡çš„30%ä»¥ä¸Šï¼Œå¯¹äºä¸€ä¸ªå•ä¸ªæ‰¹æ¬¡ã€‚
- en: The memory scaling patterns reveal critical hardware utilization trade-offs.
    Doubling the batch size to 64 increases activation memory to 16GB and gradient
    memory to 8GB, totaling 24.2GB and approaching memory limits. Training larger
    models at the scale of GPT-3 (175B parameters, representing current large language
    models) requires approximately 700GB just for parameters in FP32 (350GB in FP16),
    necessitating distributed memory strategies across multiple high-memory nodes.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜ç¼©æ”¾æ¨¡å¼æ­ç¤ºäº†å…³é”®çš„ç¡¬ä»¶åˆ©ç”¨æƒè¡¡ã€‚å°†æ‰¹æ¬¡å¤§å°åŠ å€åˆ°64ï¼Œå°†æ¿€æ´»å†…å­˜å¢åŠ åˆ°16GBï¼Œæ¢¯åº¦å†…å­˜å¢åŠ åˆ°8GBï¼Œæ€»è®¡24.2GBï¼Œæ¥è¿‘å†…å­˜é™åˆ¶ã€‚åœ¨GPT-3ï¼ˆ175Bå‚æ•°ï¼Œä»£è¡¨å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„è§„æ¨¡ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ï¼Œä»…å‚æ•°å°±éœ€è¦å¤§çº¦700GBçš„FP32ï¼ˆFP16ä¸º350GBï¼‰ï¼Œè¿™éœ€è¦è·¨å¤šä¸ªé«˜å†…å­˜èŠ‚ç‚¹åˆ†å¸ƒå¼å†…å­˜ç­–ç•¥ã€‚
- en: 'Modern GPUs typically provide between 40-80 GB of memory in high-end training
    configurations, which must accommodate not just these activations but also model
    parameters, gradients, and optimization states. This constraint has motivated
    several memory management strategies:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£GPUåœ¨é«˜æ€§èƒ½è®­ç»ƒé…ç½®ä¸­é€šå¸¸æä¾›40-80 GBçš„å†…å­˜ï¼Œè¿™å¿…é¡»å®¹çº³ä¸ä»…ä»…æ˜¯è¿™äº›æ¿€æ´»ï¼Œè¿˜åŒ…æ‹¬æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–çŠ¶æ€ã€‚è¿™ä¸€é™åˆ¶ä¿ƒä½¿å‡ºç°äº†å‡ ç§å†…å­˜ç®¡ç†ç­–ç•¥ï¼š
- en: Activation checkpointing trades computational cost for memory efficiency by
    strategically discarding and recomputing activations during the backward pass.
    Rather than storing all intermediate values, the system maintains checkpoints
    at selected layers. During backpropagation, it regenerates necessary activations
    from these checkpoints. While this approach can reduce memory usage by 50% or
    more, it typically increases computation time by 20-30%.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹é€šè¿‡åœ¨åå‘ä¼ é€’æœŸé—´æœ‰ç­–ç•¥åœ°ä¸¢å¼ƒå’Œé‡æ–°è®¡ç®—æ¿€æ´»ï¼Œä»¥è®¡ç®—æˆæœ¬æ¢å–å†…å­˜æ•ˆç‡ã€‚è€Œä¸æ˜¯å­˜å‚¨æ‰€æœ‰ä¸­é—´å€¼ï¼Œç³»ç»Ÿåœ¨é€‰å®šçš„å±‚ä¸Šç»´æŠ¤æ£€æŸ¥ç‚¹ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå®ƒä»è¿™äº›æ£€æŸ¥ç‚¹é‡æ–°ç”Ÿæˆå¿…è¦çš„æ¿€æ´»ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘50%ä»¥ä¸Šï¼Œä½†å®ƒé€šå¸¸ä¼šå¢åŠ è®¡ç®—æ—¶é—´20-30%ã€‚
- en: Mixed precision training offers another approach to memory efficiency. By storing
    activations in half-precision (FP16) format instead of single-precision (FP32),
    memory requirements are immediately halved. Modern hardware architectures provide
    specialized support for these reduced-precision operations, often maintaining
    computational throughput while saving memory.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒæä¾›äº†å¦ä¸€ç§æé«˜å†…å­˜æ•ˆç‡çš„æ–¹æ³•ã€‚é€šè¿‡å°†æ¿€æ´»å­˜å‚¨åœ¨åŠç²¾åº¦ï¼ˆFP16ï¼‰æ ¼å¼è€Œä¸æ˜¯å•ç²¾åº¦ï¼ˆFP32ï¼‰æ ¼å¼ï¼Œå†…å­˜éœ€æ±‚ç«‹å³å‡åŠã€‚ç°ä»£ç¡¬ä»¶æ¶æ„ä¸ºè¿™äº›ä½ç²¾åº¦æ“ä½œæä¾›äº†ä¸“é—¨çš„æ”¯æŒï¼Œé€šå¸¸åœ¨ä¿æŒè®¡ç®—ååé‡çš„åŒæ—¶èŠ‚çœå†…å­˜ã€‚
- en: The relationship between batch size and memory usage creates practical trade-offs
    in training regimes. While larger batch sizes can improve computational efficiency,
    they proportionally increase memory demands. A machine learning practitioner might
    start with large batch sizes during initial development on smaller networks, then
    adjust downward when scaling to deeper architectures or when working with memory-constrained
    hardware.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹æ¬¡å¤§å°ä¸å†…å­˜ä½¿ç”¨ä¹‹é—´çš„å…³ç³»åœ¨è®­ç»ƒåˆ¶åº¦ä¸­åˆ›é€ äº†å®é™…çš„æƒè¡¡ã€‚è™½ç„¶è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œä½†å®ƒä»¬æŒ‰æ¯”ä¾‹å¢åŠ å†…å­˜éœ€æ±‚ã€‚æœºå™¨å­¦ä¹ ä»ä¸šè€…å¯èƒ½ä¼šåœ¨è¾ƒå°ç½‘ç»œä¸Šçš„åˆå§‹å¼€å‘ä¸­ä½¿ç”¨è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ï¼Œç„¶ååœ¨æ‰©å±•åˆ°æ›´æ·±å±‚çš„æ¶æ„æˆ–ä¸å†…å­˜å—é™çš„ç¡¬ä»¶ä¸€èµ·å·¥ä½œæ—¶å°†å…¶è°ƒæ•´ä¸‹æ¥ã€‚
- en: This memory management challenge becomes particularly acute in state-of-the-art
    models. Recent transformer architectures can require tens of gigabytes just for
    activations, necessitating sophisticated memory management strategies or distributed
    training approaches. Understanding these memory constraints and management strategies
    proves essential for designing and deploying machine learning systems effectively.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€å…ˆè¿›çš„æ¨¡å‹ä¸­ï¼Œè¿™ç§å†…å­˜ç®¡ç†æŒ‘æˆ˜å˜å¾—å°¤ä¸ºå°–é”ã€‚æœ€è¿‘çš„è½¬æ¢å™¨æ¶æ„å¯èƒ½åªéœ€è¦æ•°åGBçš„æ¿€æ´»ï¼Œè¿™å°±éœ€è¦å¤æ‚çš„å†…å­˜ç®¡ç†ç­–ç•¥æˆ–åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•ã€‚ç†è§£è¿™äº›å†…å­˜çº¦æŸå’Œç®¡ç†ç­–ç•¥å¯¹äºæœ‰æ•ˆåœ°è®¾è®¡å’Œéƒ¨ç½²æœºå™¨å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚
- en: Backward Pass
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åå‘ä¼ é€’
- en: Following the forward passâ€™s computation of predictions and loss, the backward
    pass implements the backpropagation algorithm detailed in [SectionÂ 8.3.3.1](ch014.xhtml#sec-ai-training-backpropagation-algorithm-mechanics-d1a4).
    This computationally intensive phase propagates gradients through the network
    using the chain rule formulations established earlier. The system-level implementation
    involves complex interactions between computation and memory systems, requiring
    careful analysis of both computational demands and data movement patterns.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‘ä¼ é€’è®¡ç®—é¢„æµ‹å’ŒæŸå¤±ä¹‹åï¼Œåå‘ä¼ é€’å®ç°äº†[ç¬¬8.3.3.1èŠ‚](ch014.xhtml#sec-ai-training-backpropagation-algorithm-mechanics-d1a4)ä¸­è¯¦ç»†æè¿°çš„åå‘ä¼ æ’­ç®—æ³•ã€‚è¿™ä¸€è®¡ç®—å¯†é›†çš„é˜¶æ®µä½¿ç”¨ä¹‹å‰å»ºç«‹çš„é“¾å¼æ³•åˆ™å…¬å¼ï¼Œé€šè¿‡ç½‘ç»œä¼ æ’­æ¢¯åº¦ã€‚ç³»ç»Ÿçº§å®ç°æ¶‰åŠè®¡ç®—å’Œå†…å­˜ç³»ç»Ÿä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œéœ€è¦ä»”ç»†åˆ†æè®¡ç®—éœ€æ±‚å’Œæ•°æ®ç§»åŠ¨æ¨¡å¼ã€‚
- en: Compute Operations
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ“ä½œ
- en: The backward pass executes the gradient computations described in [SectionÂ 8.3.3.1](ch014.xhtml#sec-ai-training-backpropagation-algorithm-mechanics-d1a4),
    processing parameter gradients in reverse order through the networkâ€™s layers.
    As established in the algorithm mechanics section, computing gradients requires
    matrix operations that combine stored activations with gradient signals, demanding
    twice the memory compared to forward computation.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ é€’æ‰§è¡Œ[ç¬¬8.3.3.1èŠ‚](ch014.xhtml#sec-ai-training-backpropagation-algorithm-mechanics-d1a4)ä¸­æè¿°çš„æ¢¯åº¦è®¡ç®—ï¼Œé€šè¿‡ç½‘ç»œçš„å±‚åå‘å¤„ç†å‚æ•°æ¢¯åº¦ã€‚æ­£å¦‚ç®—æ³•æœºåˆ¶éƒ¨åˆ†æ‰€ç¡®ç«‹çš„ï¼Œè®¡ç®—æ¢¯åº¦éœ€è¦çŸ©é˜µè¿ç®—ï¼Œè¿™äº›è¿ç®—ç»“åˆå­˜å‚¨çš„æ¿€æ´»å’Œæ¢¯åº¦ä¿¡å·ï¼Œéœ€è¦æ¯”å‰å‘è®¡ç®—å¤šä¸€å€çš„å†…å­˜ã€‚
- en: The gradient computation <semantics><mrow><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><msup><mi>Î´</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>â‹…</mo><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)}
    \cdot \left(a^{(l-1)}\right)^T</annotation></semantics> forms the primary computational
    load, where gradient signals multiply with transposed activations as detailed
    in the mathematical framework. For layers with 1000 input features and 100 output
    features, this results in millions of floating-point operations as calculated
    in the algorithm mechanics analysis.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è®¡ç®—<semantics><mrow><mfrac><mrow><mi>âˆ‚</mi><mi>L</mi></mrow><mrow><mi>âˆ‚</mi><msup><mi>W</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><msup><mi>Î´</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>â‹…</mo><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>a</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation
    encoding="application/x-tex">\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)}
    \cdot \left(a^{(l-1)}\right)^T</annotation></semantics>æ„æˆäº†ä¸»è¦çš„è®¡ç®—è´Ÿè½½ï¼Œå…¶ä¸­æ¢¯åº¦ä¿¡å·ä¸è½¬ç½®æ¿€æ´»ç›¸ä¹˜ï¼Œæ­£å¦‚æ•°å­¦æ¡†æ¶ä¸­è¯¦ç»†æè¿°çš„é‚£æ ·ã€‚å¯¹äºå…·æœ‰1000ä¸ªè¾“å…¥ç‰¹å¾å’Œ100ä¸ªè¾“å‡ºç‰¹å¾çš„å±‚ï¼Œè¿™ä¼šå¯¼è‡´æ•°ç™¾ä¸‡æ¬¡æµ®ç‚¹è¿ç®—ï¼Œå¦‚ç®—æ³•æœºåˆ¶åˆ†æä¸­æ‰€è®¡ç®—çš„ã€‚
- en: Memory Operations
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜æ“ä½œ
- en: The backward pass moves large amounts of data between memory and compute units.
    Each time a layer computes gradients, it orchestrates a sequence of memory operations.
    The GPU first loads stored activations from memory, then reads incoming gradient
    signals, and finally writes the computed gradients back to memory.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åœ¨å†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´ç§»åŠ¨å¤§é‡æ•°æ®ã€‚æ¯æ¬¡å±‚è®¡ç®—æ¢¯åº¦æ—¶ï¼Œå®ƒéƒ½ä¼šåè°ƒä¸€ç³»åˆ—å†…å­˜æ“ä½œã€‚GPUé¦–å…ˆä»å†…å­˜ä¸­åŠ è½½å­˜å‚¨çš„æ¿€æ´»ï¼Œç„¶åè¯»å–ä¼ å…¥çš„æ¢¯åº¦ä¿¡å·ï¼Œæœ€åå°†è®¡ç®—å‡ºçš„æ¢¯åº¦å†™å›å†…å­˜ã€‚
- en: To understand the scale of these memory transfers, consider a convolutional
    layer processing a batch of 64 images. Each image measures <semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics> pixels with
    3 color channels. The activation maps alone occupy 0.38 GB of memory, storing
    64 copies of the input images. The gradient signals expand this memory usage significantly
    - they require 8.1 GB to hold gradients for each of the layerâ€™s 64 filters. Even
    the weight gradients, which only store updates for the convolutional kernels,
    need 0.037 GB.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£è¿™äº›è®°å¿†è½¬ç§»çš„è§„æ¨¡ï¼Œå¯ä»¥è€ƒè™‘ä¸€ä¸ªå¤„ç†64å¼ å›¾åƒæ‰¹æ¬¡çš„å·ç§¯å±‚ã€‚æ¯å¼ å›¾åƒæœ‰3ä¸ªé¢œè‰²é€šé“ï¼Œåƒç´ å°ºå¯¸ä¸º<semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics>ã€‚ä»…æ¿€æ´»å›¾å°±å ç”¨0.38
    GBçš„å†…å­˜ï¼Œå­˜å‚¨64å¼ è¾“å…¥å›¾åƒçš„å‰¯æœ¬ã€‚æ¢¯åº¦ä¿¡å·æ˜¾è‘—å¢åŠ äº†å†…å­˜ä½¿ç”¨é‡â€”â€”æ¯ä¸ªå±‚çš„64ä¸ªè¿‡æ»¤å™¨éœ€è¦8.1 GBæ¥å­˜å‚¨æ¢¯åº¦ã€‚å³ä½¿æ˜¯åªå­˜å‚¨å·ç§¯æ ¸æ›´æ–°ä¿¡æ¯çš„æƒé‡æ¢¯åº¦ï¼Œä¹Ÿéœ€è¦0.037
    GBã€‚
- en: The backward pass in neural networks requires coordinated data movement through
    a hierarchical memory system. During backpropagation, each computation requires
    specific activation values from the forward pass, creating a pattern of data movement
    between memory levels. This movement pattern shapes the performance characteristics
    of neural network training.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­éœ€è¦é€šè¿‡åˆ†å±‚å†…å­˜ç³»ç»Ÿåè°ƒæ•°æ®ç§»åŠ¨ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è®¡ç®—éƒ½éœ€è¦ä»æ­£å‘ä¼ æ’­ä¸­è·å–ç‰¹å®šçš„æ¿€æ´»å€¼ï¼Œä»è€Œåœ¨å†…å­˜çº§åˆ«ä¹‹é—´åˆ›å»ºæ•°æ®ç§»åŠ¨æ¨¡å¼ã€‚è¿™ç§ç§»åŠ¨æ¨¡å¼å†³å®šäº†ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ€§èƒ½ç‰¹å¾ã€‚
- en: These backward pass computations operate across a memory hierarchy that balances
    speed and capacity requirements. When computing gradients, the processor must
    retrieve activation values stored in HBM or system memory, transfer them to fast
    static RAM (SRAM) for computation, and write results back to larger storage. Each
    gradient calculation triggers this sequence of memory transfers, making memory
    access patterns a key factor in backward pass performance. The frequent transitions
    between memory levels introduce latency that accumulates across the backward pass
    computation chain.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åå‘ä¼ æ’­çš„è®¡ç®—æ“ä½œè·¨è¶Šäº†ä¸€ä¸ªå¹³è¡¡é€Ÿåº¦å’Œå®¹é‡è¦æ±‚çš„å†…å­˜å±‚æ¬¡ç»“æ„ã€‚åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œå¤„ç†å™¨å¿…é¡»æ£€ç´¢å­˜å‚¨åœ¨HBMæˆ–ç³»ç»Ÿå†…å­˜ä¸­çš„æ¿€æ´»å€¼ï¼Œå°†å®ƒä»¬ä¼ è¾“åˆ°å¿«é€Ÿçš„é™æ€RAMï¼ˆSRAMï¼‰è¿›è¡Œè®¡ç®—ï¼Œå¹¶å°†ç»“æœå†™å›æ›´å¤§çš„å­˜å‚¨ã€‚æ¯æ¬¡æ¢¯åº¦è®¡ç®—éƒ½ä¼šè§¦å‘è¿™ä¸€ç³»åˆ—çš„å†…å­˜ä¼ è¾“ï¼Œä½¿å†…å­˜è®¿é—®æ¨¡å¼æˆä¸ºåå‘ä¼ æ’­æ€§èƒ½çš„å…³é”®å› ç´ ã€‚é¢‘ç¹åœ°åœ¨å†…å­˜çº§åˆ«ä¹‹é—´çš„è½¬æ¢å¼•å…¥äº†å»¶è¿Ÿï¼Œè¿™äº›å»¶è¿Ÿåœ¨åå‘ä¼ æ’­è®¡ç®—é“¾ä¸­ç´¯ç§¯ã€‚
- en: Production Considerations
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç”Ÿäº§æ³¨æ„äº‹é¡¹
- en: 'Consider training a ResNet-50 model on the ImageNet dataset with a batch of
    64 images. The first convolutional layer applies 64 filters of size <semantics><mrow><mn>7</mn><mo>Ã—</mo><mn>7</mn></mrow><annotation
    encoding="application/x-tex">7 \times 7</annotation></semantics> to RGB images
    sized <semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224\times
    224</annotation></semantics>. During the backward pass, this single layerâ€™s computation
    requires: <semantics><mrow><mtext mathvariant="normal">Memory per image</mtext><mo>=</mo><mn>224</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>64</mn><mo>Ã—</mo><mn>4</mn>
    <mrow><mtext mathvariant="normal">bytes</mtext></mrow></mrow> <annotation encoding="application/x-tex">\text{Memory
    per image} = 224 \times 224 \times 64 \times 4 \text{ bytes}</annotation></semantics>'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åœ¨ImageNetæ•°æ®é›†ä¸Šä½¿ç”¨64å¼ å›¾åƒçš„æ‰¹æ¬¡è®­ç»ƒResNet-50æ¨¡å‹ã€‚ç¬¬ä¸€å±‚å·ç§¯åº”ç”¨äº†64ä¸ªå¤§å°ä¸º<semantics><mrow><mn>7</mn><mo>Ã—</mo><mn>7</mn></mrow><annotation
    encoding="application/x-tex">7 \times 7</annotation></semantics>çš„è¿‡æ»¤å™¨åˆ°å°ºå¯¸ä¸º<semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation
    encoding="application/x-tex">224\times 224</annotation></semantics>çš„RGBå›¾åƒã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸€å±‚çš„è®¡ç®—éœ€è¦ï¼š<semantics><mrow><mtext
    mathvariant="normal">æ¯å¼ å›¾åƒçš„å†…å­˜</mtext><mo>=</mo><mn>224</mn><mo>Ã—</mo><mn>224</mn><mo>Ã—</mo><mn>64</mn><mo>Ã—</mo><mn>4</mn>
    <mrow><mtext mathvariant="normal">å­—èŠ‚</mtext></mrow></mrow> <annotation encoding="application/x-tex">\text{Memory
    per image} = 224 \times 224 \times 64 \times 4 \text{ bytes}</annotation></semantics>
- en: The total memory requirement multiplies by the batch size of 64, reaching approximately
    3.2 GB just for storing gradients. When we add memory for activations, weight
    updates, and intermediate computations, a single layer approaches the memory limits
    of many GPUs.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»å†…å­˜éœ€æ±‚ä¹˜ä»¥64ä¸ªæ‰¹å¤„ç†å¤§å°ï¼Œä»…ç”¨äºå­˜å‚¨æ¢¯åº¦å°±è¾¾åˆ°å¤§çº¦3.2 GBã€‚å½“æˆ‘ä»¬åŠ ä¸Šæ¿€æ´»ã€æƒé‡æ›´æ–°å’Œä¸­é—´è®¡ç®—æ‰€éœ€çš„å†…å­˜æ—¶ï¼Œå•ä¸ªå±‚çš„å†…å­˜éœ€æ±‚æ¥è¿‘è®¸å¤šGPUçš„å†…å­˜é™åˆ¶ã€‚
- en: Deeper in the network, layers with more filters demand even greater resources.
    A mid-network convolutional layer might use 256 filters, quadrupling the memory
    and computation requirements. The backward pass must manage these resources while
    maintaining efficient computation. Each layerâ€™s computation can only begin after
    receiving gradient signals from the subsequent layer, creating a strict sequential
    dependency in memory usage and computation patterns.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç½‘ç»œçš„æ·±å±‚ï¼Œå…·æœ‰æ›´å¤šæ»¤æ³¢å™¨çš„å±‚éœ€è¦æ›´å¤šçš„èµ„æºã€‚ä¸€ä¸ªä¸­é—´å·ç§¯å±‚å¯èƒ½ä½¿ç”¨256ä¸ªæ»¤æ³¢å™¨ï¼Œè¿™ä¼šä½¿å†…å­˜å’Œè®¡ç®—éœ€æ±‚å¢åŠ å››å€ã€‚åå‘ä¼ æ’­å¿…é¡»åœ¨ä¿æŒé«˜æ•ˆè®¡ç®—çš„åŒæ—¶ç®¡ç†è¿™äº›èµ„æºã€‚æ¯ä¸€å±‚çš„è®¡ç®—åªèƒ½åœ¨æ¥æ”¶åˆ°åç»­å±‚çš„æ¢¯åº¦ä¿¡å·åå¼€å§‹ï¼Œä»è€Œåœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ¨¡å¼ä¸Šåˆ›å»ºä¸¥æ ¼çš„é¡ºåºä¾èµ–æ€§ã€‚
- en: This dependency means the GPU must maintain a large working set of memory throughout
    the backward pass. As gradients flow backward through the network, each layer
    temporarily requires peak memory usage during its computation phase. The system
    cannot release this memory until the layer completes its gradient calculations
    and passes the results to the previous layer.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¾èµ–æ€§æ„å‘³ç€GPUå¿…é¡»åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¿æŒä¸€ä¸ªå¤§çš„å·¥ä½œé›†å†…å­˜ã€‚éšç€æ¢¯åº¦åœ¨ç½‘ç»œä¸­åå‘æµåŠ¨ï¼Œæ¯ä¸ªå±‚åœ¨å…¶è®¡ç®—é˜¶æ®µæš‚æ—¶éœ€è¦å³°å€¼å†…å­˜ä½¿ç”¨ã€‚ç³»ç»Ÿä¸èƒ½é‡Šæ”¾è¿™äº›å†…å­˜ï¼Œç›´åˆ°å±‚å®Œæˆå…¶æ¢¯åº¦è®¡ç®—å¹¶å°†ç»“æœä¼ é€’ç»™å‰ä¸€å±‚ã€‚
- en: Parameter Updates and Optimizers
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°å’Œä¼˜åŒ–å™¨
- en: Completing the training loop cycle, the process of updating model parameters
    is a core operation in machine learning systems. During training, after gradients
    are computed in the backward pass, the system must allocate and manage memory
    for both the parameters and their gradients, then perform the update computations.
    The choice of optimizer determines not only the mathematical update rule, but
    also the system resources required for training.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆè®­ç»ƒå¾ªç¯å‘¨æœŸï¼Œæ›´æ–°æ¨¡å‹å‚æ•°çš„è¿‡ç¨‹æ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„æ ¸å¿ƒæ“ä½œã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåœ¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦åï¼Œç³»ç»Ÿå¿…é¡»ä¸ºå‚æ•°åŠå…¶æ¢¯åº¦åˆ†é…å’Œç®¡ç†å†…å­˜ï¼Œç„¶åæ‰§è¡Œæ›´æ–°è®¡ç®—ã€‚ä¼˜åŒ–å™¨çš„é€‰æ‹©ä¸ä»…å†³å®šäº†æ•°å­¦æ›´æ–°è§„åˆ™ï¼Œè¿˜å†³å®šäº†è®­ç»ƒæ‰€éœ€çš„ç³»ç»Ÿèµ„æºã€‚
- en: '[ListingÂ 8.1](ch014.xhtml#lst-param_update) shows the parameter update process
    in a machine learning framework.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[åˆ—è¡¨8.1](ch014.xhtml#lst-param_update)å±•ç¤ºäº†æœºå™¨å­¦ä¹ æ¡†æ¶ä¸­çš„å‚æ•°æ›´æ–°è¿‡ç¨‹ã€‚'
- en: 'ListingÂ 8.1: **Parameter Update**: Computes gradients and applies optimization
    to adjust model parameters based on loss function. Training requires computing
    gradients through backpropagation and then updating weights using an optimizer
    to minimize loss, ensuring model performance improves over epochs.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨8.1ï¼š**å‚æ•°æ›´æ–°**ï¼šè®¡ç®—æ¢¯åº¦å¹¶åº”ç”¨ä¼˜åŒ–ç®—æ³•æ¥æ ¹æ®æŸå¤±å‡½æ•°è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚è®­ç»ƒéœ€è¦é€šè¿‡åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œç„¶åä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æƒé‡ä»¥æœ€å°åŒ–æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹æ€§èƒ½åœ¨å¤šä¸ªè®­ç»ƒå‘¨æœŸä¸­æé«˜ã€‚
- en: '[PRE4]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These operations initiate a sequence of memory accesses and computations. The
    system must load parameters from memory, compute updates using the stored gradients,
    and write the modified parameters back to memory. Different optimizers vary in
    their memory requirements and computational patterns, directly affecting system
    performance and resource utilization.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ“ä½œå¯åŠ¨äº†ä¸€ç³»åˆ—å†…å­˜è®¿é—®å’Œè®¡ç®—ã€‚ç³»ç»Ÿå¿…é¡»ä»å†…å­˜ä¸­åŠ è½½å‚æ•°ï¼Œä½¿ç”¨å­˜å‚¨çš„æ¢¯åº¦è®¡ç®—æ›´æ–°ï¼Œå¹¶å°†ä¿®æ”¹åçš„å‚æ•°å†™å›å†…å­˜ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨åœ¨å†…å­˜éœ€æ±‚å’Œè®¡ç®—æ¨¡å¼ä¸Šæœ‰æ‰€ä¸åŒï¼Œè¿™ç›´æ¥å½±å“ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚
- en: Optimizer Memory Requirements
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨å†…å­˜éœ€æ±‚
- en: The choice of optimizer is not just an algorithmic decision; it is a primary
    driver of memory consumption and system resource allocation. While advanced optimizers
    like Adam can accelerate convergence, they do so at the cost of a 2-3x increase
    in memory usage compared to simpler methods like SGD, as they must store historical
    gradient information. This trade-off becomes critical in memory-constrained environments
    where optimizer state can exceed model parameter memory requirements.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çš„é€‰æ‹©ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®—æ³•å†³ç­–ï¼›å®ƒä¹Ÿæ˜¯å†…å­˜æ¶ˆè€—å’Œç³»ç»Ÿèµ„æºåˆ†é…çš„ä¸»è¦é©±åŠ¨å› ç´ ã€‚è™½ç„¶åƒAdamè¿™æ ·çš„é«˜çº§ä¼˜åŒ–å™¨å¯ä»¥åŠ é€Ÿæ”¶æ•›ï¼Œä½†å®ƒä»¬æ˜¯ä»¥2-3å€çš„å†…å­˜ä½¿ç”¨å¢åŠ ä¸ºä»£ä»·çš„ï¼Œä¸åƒSGDè¿™æ ·çš„ç®€å•æ–¹æ³•ç›¸æ¯”ï¼Œå› ä¸ºå®ƒä»¬å¿…é¡»å­˜å‚¨å†å²æ¢¯åº¦ä¿¡æ¯ã€‚è¿™ç§æƒè¡¡åœ¨å†…å­˜å—é™çš„ç¯å¢ƒä¸­å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºä¼˜åŒ–å™¨çŠ¶æ€å¯èƒ½è¶…è¿‡æ¨¡å‹å‚æ•°å†…å­˜éœ€æ±‚ã€‚
- en: 'Gradient descent, the most basic optimization algorithm that we discussed earlier,
    illustrates the core memory and computation patterns in parameter updates. From
    a systems perspective, each parameter update must:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„æœ€åŸºæœ¬çš„ä¼˜åŒ–ç®—æ³•ï¼Œè¯´æ˜äº†å‚æ•°æ›´æ–°ä¸­çš„æ ¸å¿ƒå†…å­˜å’Œè®¡ç®—æ¨¡å¼ã€‚ä»ç³»ç»Ÿè§’åº¦æ¥çœ‹ï¼Œæ¯æ¬¡å‚æ•°æ›´æ–°å¿…é¡»ï¼š
- en: Read the current parameter value from memory
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å†…å­˜ä¸­è¯»å–å½“å‰å‚æ•°å€¼
- en: Access the computed gradient from memory
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å†…å­˜ä¸­è®¿é—®è®¡ç®—å‡ºçš„æ¢¯åº¦
- en: Perform the multiplication and subtraction operations
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œä¹˜æ³•å’Œå‡æ³•æ“ä½œ
- en: Write the new parameter value back to memory
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ–°çš„å‚æ•°å€¼å†™å›å†…å­˜
- en: 'Because gradient descent only requires memory for storing parameters and gradients,
    it has relatively low memory overhead compared to more complex optimizers. However,
    more advanced optimizers introduce additional memory requirements and computational
    complexity that directly impact system design. For example, as we discussed previously,
    Adam maintains two extra vectors for each parameter: one for the first moment
    (the moving average of gradients) and one for the second moment (the moving average
    of squared gradients). This triples the memory usage but can lead to faster convergenceâ€”a
    classic systems trade-off between memory efficiency and training speed. Consider
    the situation where there are 100,000 parameters, and each gradient requires 4
    bytes (32 bits):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¢¯åº¦ä¸‹é™åªéœ€è¦å­˜å‚¨å‚æ•°å’Œæ¢¯åº¦çš„å†…å­˜ï¼Œä¸æ›´å¤æ‚çš„ä¼˜åŒ–å™¨ç›¸æ¯”ï¼Œå®ƒçš„å†…å­˜å¼€é”€ç›¸å¯¹è¾ƒä½ã€‚ç„¶è€Œï¼Œæ›´é«˜çº§çš„ä¼˜åŒ–å™¨å¼•å…¥äº†é¢å¤–çš„å†…å­˜éœ€æ±‚å’Œè®¡ç®—å¤æ‚æ€§ï¼Œè¿™ç›´æ¥å½±å“ç³»ç»Ÿè®¾è®¡ã€‚ä¾‹å¦‚ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„ï¼ŒAdamä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸¤ä¸ªé¢å¤–çš„å‘é‡ï¼šä¸€ä¸ªç”¨äºç¬¬ä¸€çŸ©ï¼ˆæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ï¼‰å’Œä¸€ä¸ªç”¨äºç¬¬äºŒçŸ©ï¼ˆå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ï¼‰ã€‚è¿™ä½¿å†…å­˜ä½¿ç”¨é‡å¢åŠ äº†ä¸‰å€ï¼Œä½†å¯ä»¥å¯¼è‡´æ›´å¿«åœ°æ”¶æ•›â€”â€”è¿™æ˜¯ç»å…¸ç³»ç»Ÿåœ¨å†…å­˜æ•ˆç‡å’Œè®­ç»ƒé€Ÿåº¦ä¹‹é—´çš„æƒè¡¡ã€‚è€ƒè™‘æœ‰100,000ä¸ªå‚æ•°çš„æƒ…å†µï¼Œæ¯ä¸ªæ¢¯åº¦éœ€è¦4å­—èŠ‚ï¼ˆ32ä½ï¼‰ï¼š
- en: 'Gradient Descent: 100,000 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    4 bytes = 400,000 bytes = 0.4 MB'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™ï¼š100,000 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    4 bytes = 400,000 bytes = 0.4 MB
- en: 'Adam: 3 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    100,000 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    4 bytes = 1,200,000 bytes = 1.2 MB'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adam: 3 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    100,000 <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    4 bytes = 1,200,000 bytes = 1.2 MB'
- en: This problem becomes especially apparent for billion parameter models, as model
    sizes (without counting optimizer states and gradients) alone can already take
    up significant portions of GPU memory. As one way of solving this problem, the
    authors of GaLoRE tackle this by compressing optimizer state and gradients and
    computing updates in this compressed space ([J. Zhao et al. 2024](ch058.xhtml#ref-zhao2024galorememoryefficientllmtraining)),
    greatly reducing memory footprint as shown below in [FigureÂ 8.6](ch014.xhtml#fig-galore-llm-memory-breakdown).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå…·æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¿™ä¸ªé—®é¢˜å°¤å…¶æ˜æ˜¾ï¼Œå› ä¸ºæ¨¡å‹å¤§å°ï¼ˆä¸è®¡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼‰æœ¬èº«å°±å·²ç»å æ®äº†GPUå†…å­˜çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ä½œä¸ºè§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼ŒGaLoREçš„ä½œè€…é€šè¿‡å‹ç¼©ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œå¹¶åœ¨å‹ç¼©ç©ºé—´ä¸­è®¡ç®—æ›´æ–°ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†å†…å­˜å ç”¨ï¼Œå¦‚å›¾8.6æ‰€ç¤ºã€‚
- en: '![](../media/file113.svg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file113.svg)'
- en: 'FigureÂ 8.6: **Memory Footprint Breakdown**: Large language models require substantial
    memory, with optimizer states and gradients often exceeding the size of model
    weights themselves. This figure quantifies the memory usage of the llama-7B model,
    revealing how techniques like compression can significantly reduce the overall
    footprint by minimizing the storage requirements for optimizer data.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.6ï¼š**å†…å­˜å ç”¨åˆ†è§£**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„å†…å­˜ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦é€šå¸¸è¶…è¿‡æ¨¡å‹æƒé‡çš„æœ¬èº«å¤§å°ã€‚æ­¤å›¾é‡åŒ–äº†llama-7Bæ¨¡å‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œæ­ç¤ºäº†å‹ç¼©ç­‰æŠ€æœ¯çš„å¦‚ä½•é€šè¿‡æœ€å°åŒ–ä¼˜åŒ–å™¨æ•°æ®çš„å­˜å‚¨éœ€æ±‚æ¥æ˜¾è‘—å‡å°‘æ•´ä½“å ç”¨ç©ºé—´ã€‚
- en: Computational Load
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—è´Ÿè½½
- en: The computational cost of parameter updates also depends on the optimizerâ€™s
    complexity. For gradient descent, each update involves simple gradient calculation
    and application. More sophisticated optimizers like Adam require additional calculations,
    such as computing running averages of gradients and their squares. This increases
    the computational load per parameter update.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°çš„è®¡ç®—æˆæœ¬ä¹Ÿå–å†³äºä¼˜åŒ–å™¨çš„å¤æ‚æ€§ã€‚å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ¯æ¬¡æ›´æ–°éƒ½æ¶‰åŠç®€å•çš„æ¢¯åº¦è®¡ç®—å’Œåº”ç”¨ã€‚æ›´å¤æ‚çš„ä¼˜åŒ–å™¨ï¼Œå¦‚Adamï¼Œéœ€è¦é¢å¤–çš„è®¡ç®—ï¼Œä¾‹å¦‚è®¡ç®—æ¢¯åº¦å’Œå…¶å¹³æ–¹çš„è¿è¡Œå¹³å‡å€¼ã€‚è¿™å¢åŠ äº†æ¯ä¸ªå‚æ•°æ›´æ–°çš„è®¡ç®—è´Ÿè½½ã€‚
- en: The efficiency of these computations on modern hardware like GPUs and TPUs depends
    on how well the optimizerâ€™s operations can be parallelized. While matrix operations
    in Adam may be efficiently handled by these accelerators, some operations in complex
    optimizers might not parallelize well, potentially leading to hardware underutilization.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®¡ç®—åœ¨ç°ä»£ç¡¬ä»¶ï¼ˆå¦‚GPUå’ŒTPUï¼‰ä¸Šçš„æ•ˆç‡å–å†³äºä¼˜åŒ–å™¨æ“ä½œå¹¶è¡ŒåŒ–çš„ç¨‹åº¦ã€‚è™½ç„¶Adamä¸­çš„çŸ©é˜µè¿ç®—å¯èƒ½è¢«è¿™äº›åŠ é€Ÿå™¨é«˜æ•ˆå¤„ç†ï¼Œä½†ä¸€äº›å¤æ‚ä¼˜åŒ–å™¨ä¸­çš„æ“ä½œå¯èƒ½æ— æ³•å¾ˆå¥½åœ°å¹¶è¡ŒåŒ–ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¡¬ä»¶åˆ©ç”¨ç‡ä¸è¶³ã€‚
- en: the choice of optimizer directly impacts both system memory requirements and
    computational load. More sophisticated optimizers often trade increased memory
    usage and computational complexity for potentially faster convergence, presenting
    important considerations for system design and resource allocation in ML systems.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çš„é€‰æ‹©ç›´æ¥å½±å“ç³»ç»Ÿå†…å­˜éœ€æ±‚å’Œè®¡ç®—è´Ÿè½½ã€‚æ›´å¤æ‚çš„ä¼˜åŒ–å™¨é€šå¸¸ä»¥å¢åŠ å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¤æ‚åº¦ä¸ºä»£ä»·ï¼Œä»¥æ¢å–å¯èƒ½æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¿™å¯¹ç³»ç»Ÿè®¾è®¡å’Œæœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„èµ„æºåˆ†é…æå‡ºäº†é‡è¦çš„è€ƒè™‘ã€‚
- en: Batch Size and Parameter Updates
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°ä¸å‚æ•°æ›´æ–°
- en: Batch size, a critical hyperparameter in machine learning systems, significantly
    influences the parameter update process, memory usage, and hardware efficiency.
    It determines the number of training examples processed in a single iteration
    before the model parameters are updated.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°æ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„ä¸€ä¸ªå…³é”®è¶…å‚æ•°ï¼Œå¯¹å‚æ•°æ›´æ–°è¿‡ç¨‹ã€å†…å­˜ä½¿ç”¨å’Œç¡¬ä»¶æ•ˆç‡æœ‰æ˜¾è‘—å½±å“ã€‚å®ƒå†³å®šäº†åœ¨æ¨¡å‹å‚æ•°æ›´æ–°ä¹‹å‰ï¼Œå•æ¬¡è¿­ä»£ä¸­å¤„ç†çš„è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚
- en: 'Larger batch sizes generally provide more accurate gradient estimates, potentially
    leading to faster convergence and more stable parameter updates. However, they
    also increase memory demands proportionally: <semantics><mrow><mtext mathvariant="normal">Memory
    for Batch</mtext><mo>=</mo><mtext mathvariant="normal">Batch Size</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">Size of One Training Example</mtext></mrow> <annotation encoding="application/x-tex">\text{Memory
    for Batch} = \text{Batch Size} \times \text{Size of One Training Example}</annotation></semantics>'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒå¤§çš„æ‰¹é‡å¤§å°é€šå¸¸æä¾›æ›´å‡†ç¡®çš„æ¢¯åº¦ä¼°è®¡ï¼Œå¯èƒ½å¸¦æ¥æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ç¨³å®šçš„å‚æ•°æ›´æ–°ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹ŸæŒ‰æ¯”ä¾‹å¢åŠ å†…å­˜éœ€æ±‚ï¼š<semantics><mrow><mtext
    mathvariant="normal">æ‰¹é‡çš„å†…å­˜</mtext><mo>=</mo><mtext mathvariant="normal">æ‰¹é‡å¤§å°</mtext><mo>Ã—</mo><mtext
    mathvariant="normal">å•ä¸ªè®­ç»ƒæ ·æœ¬çš„å¤§å°</mtext></mrow> <annotation encoding="application/x-tex">\text{æ‰¹é‡çš„å†…å­˜}
    = \text{æ‰¹é‡å¤§å°} \times \text{å•ä¸ªè®­ç»ƒæ ·æœ¬çš„å¤§å°}</annotation></semantics>
- en: This increase in memory usage directly affects the parameter update process,
    as it determines how much data is available for computing gradients in each iteration.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜ä½¿ç”¨é‡çš„å¢åŠ ç›´æ¥å½±å“åˆ°å‚æ•°æ›´æ–°è¿‡ç¨‹ï¼Œå› ä¸ºå®ƒå†³å®šäº†æ¯æ¬¡è¿­ä»£ä¸­å¯ç”¨äºè®¡ç®—æ¢¯åº¦çš„æ•°æ®é‡ã€‚
- en: Building on the efficiency patterns established in previous sections, larger
    batches improve hardware utilization, particularly on GPUs and TPUs optimized
    for parallel processing. This leads to more efficient parameter updates and faster
    training times, provided sufficient memory is available.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹åœ¨å‰é¢ç« èŠ‚ä¸­å»ºç«‹çš„æ•ˆç‡æ¨¡å¼ä¹‹ä¸Šï¼Œæ›´å¤§çš„æ‰¹é‡å¯ä»¥æå‡ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œå°¤å…¶æ˜¯åœ¨é’ˆå¯¹å¹¶è¡Œå¤„ç†ä¼˜åŒ–çš„GPUå’ŒTPUä¸Šã€‚è¿™å¯¼è‡´å‚æ•°æ›´æ–°æ›´åŠ é«˜æ•ˆï¼Œè®­ç»ƒæ—¶é—´æ›´å¿«ï¼Œå‰ææ˜¯æœ‰è¶³å¤Ÿçš„å†…å­˜å¯ç”¨ã€‚
- en: As discussed earlier, this computational efficiency comes with memory costs.
    Systems with limited memory must reduce batch size, creating the same fundamental
    trade-offs that shape training system architecture throughout.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¿™ç§è®¡ç®—æ•ˆç‡ä¼´éšç€å†…å­˜æˆæœ¬ã€‚å†…å­˜æœ‰é™çš„ç³»ç»Ÿå¿…é¡»å‡å°‘æ‰¹é‡å¤§å°ï¼Œä»è€Œäº§ç”Ÿä¸è®­ç»ƒç³»ç»Ÿæ¶æ„ç›¸å…³çš„æ ¹æœ¬æ€§æƒè¡¡ã€‚
- en: 'The choice of batch size interacts with various aspects of the optimization
    process. For instance, it affects the frequency of parameter updates: larger batches
    result in less frequent but potentially more impactful updates. Batch size influences
    the behavior of adaptive optimization algorithms, which may need to be tuned differently
    depending on the batch size. In distributed training scenarios, batch size often
    determines the degree of data parallelism, impacting how gradient computations
    and parameter updates are distributed across devices.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°çš„é€‰æ‹©ä¸ä¼˜åŒ–è¿‡ç¨‹çš„å„ä¸ªæ–¹é¢ç›¸äº’ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œå®ƒå½±å“å‚æ•°æ›´æ–°çš„é¢‘ç‡ï¼šè¾ƒå¤§çš„æ‰¹é‡å¯¼è‡´æ›´æ–°é¢‘ç‡è¾ƒä½ï¼Œä½†å¯èƒ½å½±å“æ›´å¤§ã€‚æ‰¹é‡å¤§å°å½±å“è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•çš„è¡Œä¸ºï¼Œå¯èƒ½éœ€è¦æ ¹æ®æ‰¹é‡å¤§å°è¿›è¡Œä¸åŒçš„è°ƒæ•´ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯ä¸­ï¼Œæ‰¹é‡å¤§å°é€šå¸¸å†³å®šäº†æ•°æ®å¹¶è¡Œçš„ç¨‹åº¦ï¼Œå½±å“æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°å¦‚ä½•åœ¨è®¾å¤‡ä¹‹é—´åˆ†å¸ƒã€‚
- en: Determining the optimal batch size involves balancing these factors within hardware
    constraints. It often requires experimentation to find the sweet spot that maximizes
    both learning efficiency and hardware utilization while ensuring effective parameter
    updates.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®šæœ€ä½³æ‰¹æ¬¡å¤§å°éœ€è¦åœ¨ç¡¬ä»¶çº¦æŸå†…å¹³è¡¡è¿™äº›å› ç´ ã€‚è¿™é€šå¸¸éœ€è¦å®éªŒæ¥æ‰¾åˆ°æœ€ä½³ç‚¹ï¼Œä»¥æœ€å¤§åŒ–å­¦ä¹ æ•ˆç‡å’Œç¡¬ä»¶åˆ©ç”¨ç‡ï¼ŒåŒæ—¶ç¡®ä¿æœ‰æ•ˆçš„å‚æ•°æ›´æ–°ã€‚
- en: Pipeline Optimizations
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®¡é“ä¼˜åŒ–
- en: 'Even well-designed pipeline architectures rarely achieve optimal performance
    without targeted optimization. The gap between theoretical hardware capability
    and realized training throughput often reaches 50-70%: GPUs advertised at 300
    TFLOPS may deliver only 90-150 TFLOPS for training workloads, and distributed
    systems with aggregate 1000 TFLOPS capacity frequently achieve under 500 TFLOPS
    effective throughput ([L. Wang et al. 2018](ch058.xhtml#ref-wang2019superneurons)).
    This efficiency gap stems from systematic bottlenecks that optimization techniques
    can address.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ˜¯è®¾è®¡è‰¯å¥½çš„ç®¡é“æ¶æ„ï¼Œå¦‚æœæ²¡æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ï¼Œä¹Ÿå¾ˆéš¾è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç†è®ºç¡¬ä»¶èƒ½åŠ›ä¸å®é™…è®­ç»ƒååé‡ä¹‹é—´çš„å·®è·é€šå¸¸è¾¾åˆ°50-70%ï¼šæ ‡ç§°300 TFLOPSçš„GPUåœ¨è®­ç»ƒå·¥ä½œè´Ÿè½½ä¸­å¯èƒ½åªèƒ½æä¾›90-150
    TFLOPSï¼Œè€Œæ€»å®¹é‡ä¸º1000 TFLOPSçš„åˆ†å¸ƒå¼ç³»ç»Ÿé€šå¸¸åªèƒ½å®ç°ä½äº500 TFLOPSçš„æœ‰æ•ˆååé‡ï¼ˆ[L. Wangç­‰äºº2018](ch058.xhtml#ref-wang2019superneurons)ï¼‰ã€‚è¿™ç§æ•ˆç‡å·®è·æºäºä¼˜åŒ–æŠ€æœ¯å¯ä»¥è§£å†³çš„ç³»ç»Ÿæ€§ç“¶é¢ˆã€‚
- en: 'The following table provides a roadmap for matching optimization techniques
    to the bottlenecks they solve, serving as a practical guide for systematic performance
    improvement:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è¡¨æä¾›äº†ä¸€ä¸ªè·¯çº¿å›¾ï¼Œç”¨äºå°†ä¼˜åŒ–æŠ€æœ¯ä¸å…¶è§£å†³çš„é—®é¢˜ç›¸åŒ¹é…ï¼Œä½œä¸ºç³»ç»Ÿæ€§èƒ½æ”¹è¿›çš„å®ç”¨æŒ‡å—ï¼š
- en: 'TableÂ 8.4: **Optimization Technique Roadmap**: Each primary bottleneck category
    has targeted solutions that address specific performance constraints. This mapping
    guides systematic optimization by matching techniques to profiling results.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.4ï¼š**ä¼˜åŒ–æŠ€æœ¯è·¯çº¿å›¾**ï¼šæ¯ä¸ªä¸»è¦ç“¶é¢ˆç±»åˆ«éƒ½æœ‰é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥è§£å†³ç‰¹å®šçš„æ€§èƒ½é™åˆ¶ã€‚è¿™ç§æ˜ å°„æŒ‡å¯¼ç³»ç»Ÿä¼˜åŒ–ï¼Œé€šè¿‡å°†æŠ€æœ¯ä¸å…¶åˆ†æç»“æœç›¸åŒ¹é…ã€‚
- en: '| **Bottleneck** | **Primary Solution(s)** |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| **ç“¶é¢ˆ** | **ä¸»è¦è§£å†³æ–¹æ¡ˆ** |'
- en: '| --- | --- |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Data Movement Latency** | Prefetching & Pipeline Overlapping |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| **æ•°æ®ç§»åŠ¨å»¶è¿Ÿ** | é¢„å–ä¸ç®¡é“é‡å  |'
- en: '| **Compute Throughput** | Mixed-Precision Training |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| **è®¡ç®—ååé‡** | æ··åˆç²¾åº¦è®­ç»ƒ |'
- en: '| **Memory Capacity** | Gradient Accumulation & Activation Checkpointing |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜å®¹é‡** | æ¢¯åº¦ç´¯ç§¯ä¸æ¿€æ´»æ£€æŸ¥ç‚¹ |'
- en: 'Training pipeline performance is constrained by three primary bottlenecks that
    determine overall system efficiency ([TableÂ 8.4](ch014.xhtml#tbl-optimization-roadmap)):
    data movement latency, computational throughput limitations, and memory capacity
    constraints. Data movement latency emerges when training batches cannot flow from
    storage through preprocessing to compute units fast enough to keep accelerators
    utilized. Computational throughput limitations occur when mathematical operations
    execute below hardware peak performance due to suboptimal parallelization, precision
    choices, or kernel inefficiencies. Memory capacity constraints restrict both the
    model sizes we can train and the batch sizes we can process, directly limiting
    both model complexity and training efficiency. These bottlenecks manifest differently
    across system scalesâ€”a 100GB model faces different constraints than a 1GB modelâ€”but
    their systematic identification and mitigation follows consistent principles.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç®¡é“æ€§èƒ½å—ä¸‰ä¸ªä¸»è¦ç“¶é¢ˆçš„é™åˆ¶ï¼Œè¿™äº›ç“¶é¢ˆå†³å®šäº†æ•´ä½“ç³»ç»Ÿæ•ˆç‡ï¼ˆ[è¡¨8.4](ch014.xhtml#tbl-optimization-roadmap)ï¼‰ï¼šæ•°æ®ç§»åŠ¨å»¶è¿Ÿã€è®¡ç®—ååé‡é™åˆ¶å’Œå†…å­˜å®¹é‡é™åˆ¶ã€‚å½“è®­ç»ƒæ‰¹æ¬¡æ— æ³•è¶³å¤Ÿå¿«åœ°ä»å­˜å‚¨é€šè¿‡é¢„å¤„ç†åˆ°è®¡ç®—å•å…ƒæµåŠ¨ï¼Œä»¥ä¿æŒåŠ é€Ÿå™¨çš„åˆ©ç”¨ç‡æ—¶ï¼Œå°±ä¼šå‡ºç°æ•°æ®ç§»åŠ¨å»¶è¿Ÿã€‚å½“æ•°å­¦è¿ç®—ç”±äºæ¬¡ä¼˜å¹¶è¡ŒåŒ–ã€ç²¾åº¦é€‰æ‹©æˆ–å†…æ ¸æ•ˆç‡ä½ä¸‹è€Œä½äºç¡¬ä»¶å³°å€¼æ€§èƒ½æ—¶ï¼Œå°±ä¼šå‡ºç°è®¡ç®—ååé‡é™åˆ¶ã€‚å†…å­˜å®¹é‡é™åˆ¶æ—¢é™åˆ¶äº†æˆ‘ä»¬å¯ä»¥è®­ç»ƒçš„æ¨¡å‹å¤§å°ï¼Œä¹Ÿé™åˆ¶äº†æˆ‘ä»¬å¯ä»¥å¤„ç†çš„æ‰¹æ¬¡å¤§å°ï¼Œç›´æ¥é™åˆ¶äº†æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿™äº›ç“¶é¢ˆåœ¨ä¸åŒç³»ç»Ÿè§„æ¨¡ä¸Šè¡¨ç°ä¸åŒâ€”â€”ä¸€ä¸ª100GBçš„æ¨¡å‹é¢ä¸´ä¸1GBæ¨¡å‹ä¸åŒçš„é™åˆ¶ï¼Œä½†å®ƒä»¬çš„ç³»ç»Ÿè¯†åˆ«å’Œç¼“è§£éµå¾ªä¸€è‡´çš„åŸåˆ™ã€‚
- en: These bottlenecks interact in complex ways. When data loading becomes a bottleneck,
    GPUs sit idle waiting for batches. When computation is suboptimal, memory bandwidth
    goes underutilized. When memory is constrained, we resort to smaller batches that
    reduce GPU efficiency. The optimization challenge involves identifying which bottleneck
    currently limits performance, then selecting techniques that address that specific
    constraint without introducing new bottlenecks elsewhere.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç“¶é¢ˆä»¥å¤æ‚çš„æ–¹å¼ç›¸äº’ä½œç”¨ã€‚å½“æ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆæ—¶ï¼ŒGPUä¼šç©ºé—²ç­‰å¾…æ‰¹æ¬¡ã€‚å½“è®¡ç®—æ¬¡ä¼˜æ—¶ï¼Œå†…å­˜å¸¦å®½å¾—ä¸åˆ°å……åˆ†åˆ©ç”¨ã€‚å½“å†…å­˜å—é™æ—¶ï¼Œæˆ‘ä»¬ä¸å¾—ä¸ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡ï¼Œè¿™é™ä½äº†GPUçš„æ•ˆç‡ã€‚ä¼˜åŒ–æŒ‘æˆ˜åœ¨äºè¯†åˆ«å½“å‰é™åˆ¶æ€§èƒ½çš„ç“¶é¢ˆï¼Œç„¶åé€‰æ‹©è§£å†³è¯¥ç‰¹å®šçº¦æŸçš„æŠ€æœ¯ï¼ŒåŒæ—¶ä¸ä¼šåœ¨åˆ«å¤„å¼•å…¥æ–°çš„ç“¶é¢ˆã€‚
- en: Systematic Optimization Framework
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿä¼˜åŒ–æ¡†æ¶
- en: 'The pipeline architecture established above creates opportunities for targeted
    optimizations. Effective optimization follows a systematic methodology that applies
    regardless of system scale or model architecture. This three-phase framework provides
    the foundation for all optimization work: profile to identify bottlenecks, select
    appropriate techniques for the identified constraints, and compose solutions that
    address multiple bottlenecks simultaneously without creating conflicts.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å»ºç«‹çš„ç®¡é“æ¶æ„ä¸ºæœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–åˆ›é€ äº†æœºä¼šã€‚æœ‰æ•ˆçš„ä¼˜åŒ–éµå¾ªä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•è®ºï¼Œè¿™ç§æ–¹æ³•è®ºé€‚ç”¨äºä»»ä½•ç³»ç»Ÿè§„æ¨¡æˆ–æ¨¡å‹æ¶æ„ã€‚è¿™ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ä¸ºæ‰€æœ‰ä¼˜åŒ–å·¥ä½œæä¾›äº†åŸºç¡€ï¼šåˆ†æä»¥è¯†åˆ«ç“¶é¢ˆï¼Œä¸ºè¯†åˆ«åˆ°çš„çº¦æŸé€‰æ‹©é€‚å½“çš„æŠ€å·§ï¼Œå¹¶ç»„åˆè§£å†³æ–¹æ¡ˆä»¥åŒæ—¶è§£å†³å¤šä¸ªç“¶é¢ˆï¼Œè€Œä¸äº§ç”Ÿå†²çªã€‚
- en: The profiling phase employs tools like PyTorch Profiler, TensorFlow Profiler,
    or NVIDIA Nsight Systems to reveal where time is spent during training iterations.
    These are the same profiling approaches introduced in the overviewâ€”now applied
    systematically to quantify which bottleneck dominates. A profile might show 40%
    of time in data loading, 35% in computation, and 25% in memory operationsâ€”clearly
    indicating data loading as the primary target for optimization.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†æé˜¶æ®µä½¿ç”¨PyTorch Profilerã€TensorFlow Profileræˆ–NVIDIA Nsight Systemsç­‰å·¥å…·æ¥æ­ç¤ºè®­ç»ƒè¿­ä»£è¿‡ç¨‹ä¸­æ—¶é—´èŠ±è´¹åœ¨å“ªé‡Œã€‚è¿™äº›æ˜¯åœ¨æ¦‚è¿°ä¸­ä»‹ç»è¿‡çš„ç›¸åŒåˆ†ææ–¹æ³•â€”â€”ç°åœ¨ç³»ç»Ÿåœ°åº”ç”¨æ¥é‡åŒ–å“ªä¸ªç“¶é¢ˆå ä¸»å¯¼åœ°ä½ã€‚ä¸€ä¸ªåˆ†æå¯èƒ½æ˜¾ç¤º40%çš„æ—¶é—´ç”¨äºæ•°æ®åŠ è½½ï¼Œ35%ç”¨äºè®¡ç®—ï¼Œ25%ç”¨äºå†…å­˜æ“ä½œâ€”â€”æ˜æ˜¾è¡¨æ˜æ•°æ®åŠ è½½æ˜¯ä¼˜åŒ–çš„ä¸»è¦ç›®æ ‡ã€‚
- en: 'The selection phase matches optimization techniques to identified bottlenecks.
    Each technique we examine targets specific constraints: prefetching addresses
    data movement latency, mixed-precision training tackles both computational throughput
    and memory constraints, and gradient accumulation manages memory limitations.
    Selection requires understanding not just which bottleneck exists, but the characteristics
    of the hardware, model architecture, and training configuration that influence
    technique effectiveness.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©é˜¶æ®µå°†ä¼˜åŒ–æŠ€å·§ä¸è¯†åˆ«åˆ°çš„ç“¶é¢ˆç›¸åŒ¹é…ã€‚æˆ‘ä»¬æ£€æŸ¥çš„æ¯ç§æŠ€å·§éƒ½é’ˆå¯¹ç‰¹å®šçš„çº¦æŸï¼šé¢„å–è§£å†³æ•°æ®ç§»åŠ¨å»¶è¿Ÿï¼Œæ··åˆç²¾åº¦è®­ç»ƒè§£å†³è®¡ç®—ååé‡å’Œå†…å­˜çº¦æŸï¼Œæ¢¯åº¦ç´¯ç§¯ç®¡ç†å†…å­˜é™åˆ¶ã€‚é€‰æ‹©ä¸ä»…éœ€è¦ç†è§£å“ªäº›ç“¶é¢ˆå­˜åœ¨ï¼Œè¿˜éœ€è¦ç†è§£å½±å“æŠ€å·§æœ‰æ•ˆæ€§çš„ç¡¬ä»¶ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒé…ç½®çš„ç‰¹ç‚¹ã€‚
- en: 'The composition phase combines multiple techniques to achieve cumulative benefits.
    Prefetching and mixed-precision training complement each otherâ€”one addresses data
    loading, the other computation and memoryâ€”allowing simultaneous application. However,
    some combinations create conflicts: aggressive prefetching increases memory pressure,
    potentially conflicting with memory-constrained configurations. Successful composition
    requires understanding technique interactions and dependencies.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„åˆé˜¶æ®µç»“åˆå¤šç§æŠ€æœ¯ä»¥å®ç°ç´¯ç§¯æ•ˆç›Šã€‚é¢„å–å’Œæ··åˆç²¾åº¦è®­ç»ƒç›¸äº’è¡¥å……â€”â€”ä¸€ä¸ªè§£å†³æ•°æ®åŠ è½½é—®é¢˜ï¼Œå¦ä¸€ä¸ªè§£å†³è®¡ç®—å’Œå†…å­˜é—®é¢˜â€”â€”å…è®¸åŒæ—¶åº”ç”¨ã€‚ç„¶è€Œï¼ŒæŸäº›ç»„åˆä¼šäº§ç”Ÿå†²çªï¼šæ¿€è¿›çš„é¢„å–ä¼šå¢åŠ å†…å­˜å‹åŠ›ï¼Œå¯èƒ½ä¸å…¶ä»–å†…å­˜å—é™é…ç½®å†²çªã€‚æˆåŠŸçš„ç»„åˆéœ€è¦ç†è§£æŠ€å·§ä¹‹é—´çš„ç›¸äº’ä½œç”¨å’Œä¾èµ–å…³ç³»ã€‚
- en: This systematic frameworkâ€”profile, select, composeâ€”applies three core optimization
    techniques to the primary bottleneck categories. Prefetching and overlapping targets
    data movement latency by coordinating data transfer with computation. Mixed-precision
    training addresses both computational throughput and memory constraints through
    reduced precision arithmetic. Gradient accumulation and checkpointing manages
    memory constraints by trading computation for memory usage. These techniques are
    not mutually exclusive; effective optimization often combines multiple approaches
    to achieve cumulative benefits.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç³»ç»Ÿæ¡†æ¶â€”â€”åˆ†æã€é€‰æ‹©ã€ç»„åˆâ€”â€”å°†ä¸‰ä¸ªæ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯åº”ç”¨äºä¸»è¦ç“¶é¢ˆç±»åˆ«ã€‚é¢„å–é€šè¿‡åè°ƒæ•°æ®ä¼ è¾“ä¸è®¡ç®—æ¥é‡å ç›®æ ‡æ•°æ®ç§»åŠ¨å»¶è¿Ÿã€‚æ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡é™ä½ç²¾åº¦ç®—æœ¯æ¥è§£å†³è®¡ç®—ååé‡å’Œå†…å­˜çº¦æŸã€‚æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹é€šè¿‡ä»¥è®¡ç®—æ¢å–å†…å­˜ä½¿ç”¨æ¥ç®¡ç†å†…å­˜çº¦æŸã€‚è¿™äº›æŠ€æœ¯ä¸æ˜¯äº’æ–¥çš„ï¼›æœ‰æ•ˆçš„ä¼˜åŒ–é€šå¸¸ç»“åˆå¤šç§æ–¹æ³•ä»¥å®ç°ç´¯ç§¯æ•ˆç›Šã€‚
- en: Production Optimization Decision Framework
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”Ÿäº§ä¼˜åŒ–å†³ç­–æ¡†æ¶
- en: While the systematic framework establishes methodology, production environments
    introduce additional operational constraints. The production decision framework
    extends the systematic approach with operational factors that influence technique
    selection in real deployment contexts.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç³»ç»Ÿæ¡†æ¶ç¡®ç«‹äº†æ–¹æ³•è®ºï¼Œä½†ç”Ÿäº§ç¯å¢ƒå¼•å…¥äº†é¢å¤–çš„æ“ä½œçº¦æŸã€‚ç”Ÿäº§å†³ç­–æ¡†æ¶é€šè¿‡åœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸­å½±å“æŠ€å·§é€‰æ‹©çš„æ“ä½œå› ç´ æ‰©å±•äº†ç³»ç»Ÿæ–¹æ³•ã€‚
- en: 'Production optimization decisions must balance performance improvements against
    implementation complexity, operational monitoring requirements, and system reliability.
    Four factors guide technique selection: performance impact potential quantifies
    expected speedup or memory savings, implementation complexity assesses development
    and debugging effort required, operational overhead evaluates ongoing monitoring
    and maintenance needs, and system reliability implications examines how techniques
    affect fault tolerance and reproducibility.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿäº§ä¼˜åŒ–å†³ç­–å¿…é¡»åœ¨æ€§èƒ½æ”¹è¿›ä¸å®ç°å¤æ‚æ€§ã€æ“ä½œç›‘æ§éœ€æ±‚å’Œç³»ç»Ÿå¯é æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å››ä¸ªå› ç´ æŒ‡å¯¼æŠ€æœ¯é€‰æ‹©ï¼šæ€§èƒ½å½±å“æ½œåŠ›é‡åŒ–é¢„æœŸçš„åŠ é€Ÿæˆ–å†…å­˜èŠ‚çœï¼Œå®ç°å¤æ‚æ€§è¯„ä¼°å¼€å‘å’Œè°ƒè¯•æ‰€éœ€çš„å·¥ä½œé‡ï¼Œæ“ä½œå¼€é”€è¯„ä¼°æŒç»­çš„ç›‘æ§å’Œç»´æŠ¤éœ€æ±‚ï¼Œä»¥åŠç³»ç»Ÿå¯é æ€§å½±å“æ£€æŸ¥æŠ€æœ¯å¦‚ä½•å½±å“å®¹é”™æ€§å’Œå¯é‡å¤æ€§ã€‚
- en: High-impact, low-complexity optimizations like data prefetching should be implemented
    first, providing immediate benefits with minimal risk. Complex optimizations such
    as gradient checkpointing require careful cost-benefit analysis including development
    time, debugging complexity, and ongoing maintenance requirements. We examine each
    optimization technique through this production lens, providing specific guidance
    on implementation priorities, monitoring requirements, and operational considerations
    that enable practitioners to make informed decisions for their specific deployment
    environments.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜å½±å“ã€ä½å¤æ‚åº¦çš„ä¼˜åŒ–ï¼Œå¦‚æ•°æ®é¢„å–ï¼Œåº”é¦–å…ˆå®æ–½ï¼Œä»¥æä¾›å³æ—¶çš„å¥½å¤„ï¼ŒåŒæ—¶é£é™©æœ€å°ã€‚å¦‚æ¢¯åº¦æ£€æŸ¥ç‚¹è¿™æ ·çš„å¤æ‚ä¼˜åŒ–éœ€è¦ä»”ç»†çš„æˆæœ¬æ•ˆç›Šåˆ†æï¼ŒåŒ…æ‹¬å¼€å‘æ—¶é—´ã€è°ƒè¯•å¤æ‚æ€§å’ŒæŒç»­ç»´æŠ¤éœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡ç”Ÿäº§è§†è§’æ¥å®¡è§†æ¯ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæä¾›å…³äºå®æ–½ä¼˜å…ˆçº§ã€ç›‘æ§è¦æ±‚å’Œæ“ä½œè€ƒè™‘çš„å…·ä½“æŒ‡å¯¼ï¼Œä»¥ä¾¿ä»ä¸šè€…èƒ½å¤Ÿä¸ºä»–ä»¬çš„ç‰¹å®šéƒ¨ç½²ç¯å¢ƒåšå‡ºæ˜æ™ºçš„å†³å®šã€‚
- en: Data Prefetching and Pipeline Overlapping
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®é¢„å–å’Œç®¡é“é‡å 
- en: To illustrate the systematic framework in action, we begin with prefetching
    and overlapping techniques that target data movement latency bottlenecks by coordinating
    data transfer with computation. This optimization proves most effective when profiling
    reveals that computational units remain idle while waiting for data transfers
    to complete.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜ç³»ç»Ÿæ¡†æ¶çš„å®é™…åº”ç”¨ï¼Œæˆ‘ä»¬é¦–å…ˆä»é¢„å–å’Œé‡å æŠ€æœ¯å¼€å§‹ï¼Œè¿™äº›æŠ€æœ¯é€šè¿‡åè°ƒæ•°æ®ä¼ è¾“ä¸è®¡ç®—æ¥é’ˆå¯¹æ•°æ®ç§»åŠ¨å»¶è¿Ÿç“¶é¢ˆã€‚å½“åˆ†æè¡¨æ˜è®¡ç®—å•å…ƒåœ¨ç­‰å¾…æ•°æ®ä¼ è¾“å®Œæˆæ—¶å¤„äºç©ºé—²çŠ¶æ€æ—¶ï¼Œè¿™ç§ä¼˜åŒ–è¯æ˜æœ€ä¸ºæœ‰æ•ˆã€‚
- en: 'Training machine learning models involves significant data movement between
    storage, memory, and computational units. The data pipeline consists of sequential
    transfers: from disk storage to CPU memory, CPU memory to GPU memory, and through
    the GPU processing units. In standard implementations, each transfer must complete
    before the next begins, as shown in [FigureÂ 8.7](ch014.xhtml#fig-fetching-naive),
    resulting in computational inefficiencies.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ¶‰åŠåœ¨å­˜å‚¨ã€å†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´çš„å¤§é‡æ•°æ®ç§»åŠ¨ã€‚æ•°æ®ç®¡é“ç”±ä¸€ç³»åˆ—è¿ç»­çš„ä¼ è¾“ç»„æˆï¼šä»ç£ç›˜å­˜å‚¨åˆ°CPUå†…å­˜ï¼ŒCPUå†…å­˜åˆ°GPUå†…å­˜ï¼Œä»¥åŠé€šè¿‡GPUå¤„ç†å•å…ƒã€‚åœ¨æ ‡å‡†å®ç°ä¸­ï¼Œæ¯ä¸ªä¼ è¾“å¿…é¡»åœ¨ä¸‹ä¸€ä¸ªå¼€å§‹ä¹‹å‰å®Œæˆï¼Œå¦‚å›¾8.7æ‰€ç¤ºï¼Œè¿™å¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚
- en: '![](../media/file114.svg)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file114.svg)'
- en: 'FigureÂ 8.7: **Sequential Data Transfer**: Standard data fetching pipelines
    execute transfers from disk to CPU, CPU to GPU, and through GPU processing one
    at a time, creating bottlenecks and limiting computational throughput during model
    training. This serial approach prevents overlapping computation and data movement,
    hindering efficient resource utilization.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.7ï¼š**é¡ºåºæ•°æ®ä¼ è¾“**ï¼šæ ‡å‡†æ•°æ®æå–ç®¡é“ä¸€æ¬¡æ‰§è¡Œä»ç£ç›˜åˆ°CPUã€CPUåˆ°GPUä»¥åŠé€šè¿‡GPUå¤„ç†å•å…ƒçš„ä¼ è¾“ï¼Œåˆ›å»ºç“¶é¢ˆå¹¶é™åˆ¶æ¨¡å‹è®­ç»ƒæœŸé—´çš„è®¡ç®—ååé‡ã€‚è¿™ç§ä¸²è¡Œæ–¹æ³•é˜»æ­¢äº†è®¡ç®—å’Œæ•°æ®ç§»åŠ¨çš„é‡å ï¼Œé˜»ç¢äº†èµ„æºçš„é«˜æ•ˆåˆ©ç”¨ã€‚
- en: Prefetching addresses these inefficiencies by loading data into memory before
    its scheduled computation time. During the processing of the current batch, the
    system loads and prepares subsequent batches, maintaining a consistent supply
    of ready data ([MartÃ­n Abadi et al. 2015](ch058.xhtml#ref-tensorflow_data_2015)).
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å–æŠ€æœ¯é€šè¿‡åœ¨é¢„å®šè®¡ç®—æ—¶é—´ä¹‹å‰å°†æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­æ¥è§£å†³è¿™äº›ä½æ•ˆé—®é¢˜ã€‚åœ¨å¤„ç†å½“å‰æ‰¹æ¬¡çš„è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿä¼šåŠ è½½å¹¶å‡†å¤‡åç»­æ‰¹æ¬¡ï¼Œä¿æŒæŒç»­çš„æ•°æ®ä¾›åº”([MartÃ­n
    Abadi ç­‰äºº 2015](ch058.xhtml#ref-tensorflow_data_2015))ã€‚
- en: Overlapping builds upon prefetching by coordinating multiple pipeline stages
    to execute concurrently. The system processes the current batch while simultaneously
    preparing future batches through data loading and preprocessing operations. This
    coordination establishes a continuous data flow through the training pipeline,
    as illustrated in [FigureÂ 8.8](ch014.xhtml#fig-fetching-optimized).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å é€šè¿‡åè°ƒå¤šä¸ªç®¡é“é˜¶æ®µä»¥å¹¶å‘æ‰§è¡Œæ¥å»ºç«‹åœ¨é¢„å–çš„åŸºç¡€ä¸Šã€‚ç³»ç»Ÿåœ¨å¤„ç†å½“å‰æ‰¹æ¬¡çš„åŒæ—¶ï¼Œé€šè¿‡æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ“ä½œå‡†å¤‡æœªæ¥çš„æ‰¹æ¬¡ã€‚è¿™ç§åè°ƒåœ¨è®­ç»ƒç®¡é“ä¸­å»ºç«‹äº†è¿ç»­çš„æ•°æ®æµï¼Œå¦‚å›¾8.8æ‰€ç¤ºã€‚
- en: '![](../media/file115.svg)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file115.svg)'
- en: 'FigureÂ 8.8: **Pipeline Parallelism**: Overlapping computation and data fetching
    reduces overall job completion time by concurrently processing data and preparing
    subsequent batches. This optimization achieves a 40% speedup, finishing in 00:40
    seconds compared to 01:30 seconds with naive sequential fetching.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.8ï¼š**ç®¡é“å¹¶è¡Œæ€§**ï¼šé€šè¿‡å¹¶å‘å¤„ç†æ•°æ®å’Œå‡†å¤‡åç»­æ‰¹æ¬¡ï¼Œé‡å è®¡ç®—å’Œæ•°æ®è·å–å‡å°‘äº†æ•´ä½“ä½œä¸šå®Œæˆæ—¶é—´ã€‚è¿™ç§ä¼˜åŒ–å®ç°äº†40%çš„é€Ÿåº¦æå‡ï¼Œå®Œæˆæ—¶é—´ä¸º00:40ç§’ï¼Œè€Œä½¿ç”¨ç®€å•çš„é¡ºåºè·å–åˆ™éœ€è¦01:30ç§’ã€‚
- en: These optimization techniques demonstrate particular value in scenarios involving
    large-scale datasets, preprocessing-intensive data, multi-GPU training configurations,
    or high-latency storage systems. The following section examines the specific mechanics
    of implementing these techniques in modern training systems.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¼˜åŒ–æŠ€æœ¯åœ¨æ¶‰åŠå¤§è§„æ¨¡æ•°æ®é›†ã€é¢„å¤„ç†å¯†é›†å‹æ•°æ®ã€å¤šGPUè®­ç»ƒé…ç½®æˆ–é«˜å»¶è¿Ÿå­˜å‚¨ç³»ç»Ÿçš„åœºæ™¯ä¸­æ˜¾ç¤ºå‡ºç‰¹åˆ«çš„ä»·å€¼ã€‚ä¸‹ä¸€èŠ‚å°†æ¢è®¨åœ¨ç°ä»£è®­ç»ƒç³»ç»Ÿä¸­å®ç°è¿™äº›æŠ€æœ¯çš„å…·ä½“æœºåˆ¶ã€‚
- en: Prefetching Mechanics
  id: totrans-532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é¢„å–æœºåˆ¶
- en: Prefetching and overlapping optimize the training pipeline by enabling different
    stages of data processing and computation to operate concurrently rather than
    sequentially. These techniques maximize resource utilization by addressing bottlenecks
    in data transfer and preprocessing.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å–å’Œé‡å ä¼˜åŒ–äº†è®­ç»ƒæµç¨‹ï¼Œé€šè¿‡ä½¿æ•°æ®å¤„ç†å’Œè®¡ç®—çš„å„ä¸ªé˜¶æ®µèƒ½å¤Ÿå¹¶å‘è€Œä¸æ˜¯é¡ºåºæ‰§è¡Œã€‚è¿™äº›æŠ€æœ¯é€šè¿‡è§£å†³æ•°æ®ä¼ è¾“å’Œé¢„å¤„ç†ä¸­çš„ç“¶é¢ˆæ¥æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚
- en: 'As you recall, training data undergoes three main stages: retrieval from storage,
    transformation into a suitable format, and utilization in model training. An unoptimized
    pipeline executes these stages sequentially. The GPU remains idle during data
    fetching and preprocessing, waiting for data preparation to complete. This sequential
    execution creates significant inefficiencies in the training process.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€å›å¿†çš„é‚£æ ·ï¼Œè®­ç»ƒæ•°æ®ç»å†ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šä»å­˜å‚¨ä¸­æ£€ç´¢ã€è½¬æ¢ä¸ºåˆé€‚çš„æ ¼å¼ä»¥åŠåœ¨æ¨¡å‹è®­ç»ƒä¸­çš„åˆ©ç”¨ã€‚æœªä¼˜åŒ–çš„ç®¡é“æŒ‰é¡ºåºæ‰§è¡Œè¿™äº›é˜¶æ®µã€‚GPUåœ¨æ•°æ®è·å–å’Œé¢„å¤„ç†æœŸé—´å¤„äºç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…æ•°æ®å‡†å¤‡å®Œæˆã€‚è¿™ç§é¡ºåºæ‰§è¡Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿäº†æ˜¾è‘—çš„ä½æ•ˆç‡ã€‚
- en: Prefetching eliminates waiting time by loading data asynchronously during model
    computation. Data loaders operate as separate threads or processes, preparing
    the next batch while the current batch trains. This ensures immediate data availability
    for the GPU when the current batch completes.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å–é€šè¿‡åœ¨æ¨¡å‹è®¡ç®—æœŸé—´å¼‚æ­¥åŠ è½½æ•°æ®æ¥æ¶ˆé™¤ç­‰å¾…æ—¶é—´ã€‚æ•°æ®åŠ è½½å™¨ä½œä¸ºç‹¬ç«‹çš„çº¿ç¨‹æˆ–è¿›ç¨‹è¿è¡Œï¼Œåœ¨å½“å‰æ‰¹æ¬¡è®­ç»ƒçš„åŒæ—¶å‡†å¤‡ä¸‹ä¸€æ‰¹æ¬¡ã€‚è¿™ç¡®ä¿äº†å½“å‰æ‰¹æ¬¡å®ŒæˆåGPUå¯ä»¥ç«‹å³è·å¾—æ•°æ®ã€‚
- en: Overlapping extends this efficiency by coordinating all three pipeline stages
    simultaneously. As the GPU processes one batch, preprocessing begins on the next
    batch, while data fetching starts for the subsequent batch. This coordination
    maintains constant activity across all pipeline stages.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å é€šè¿‡åŒæ—¶åè°ƒæ‰€æœ‰ä¸‰ä¸ªç®¡é“é˜¶æ®µæ¥æ‰©å±•è¿™ç§æ•ˆç‡ã€‚å½“GPUå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡æ—¶ï¼Œé¢„å¤„ç†å¼€å§‹äºä¸‹ä¸€ä¸ªæ‰¹æ¬¡ï¼Œè€Œæ•°æ®è·å–å¼€å§‹äºåç»­æ‰¹æ¬¡ã€‚è¿™ç§åè°ƒåœ¨æ‰€æœ‰ç®¡é“é˜¶æ®µä¿æŒæ’å®šçš„æ´»åŠ¨ã€‚
- en: Modern machine learning frameworks implement these techniques through built-in
    utilities. PyTorchâ€™s `DataLoader` class demonstrates this implementation. An example
    of this usage is shown in [ListingÂ 8.2](ch014.xhtml#lst-dataloader_usage).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶é€šè¿‡å†…ç½®å®ç”¨ç¨‹åºå®ç°è¿™äº›æŠ€æœ¯ã€‚PyTorchçš„`DataLoader`ç±»å±•ç¤ºäº†è¿™ç§å®ç°ã€‚è¿™ç§ç”¨æ³•çš„ç¤ºä¾‹åœ¨[åˆ—è¡¨8.2](ch014.xhtml#lst-dataloader_usage)ä¸­ç»™å‡ºã€‚
- en: 'ListingÂ 8.2: **Pipeline Optimization**: Machine learning workflows benefit
    from efficient data handling through batching and prefetching to maintain constant
    GPU utilization.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨8.2ï¼š**ç®¡é“ä¼˜åŒ–**ï¼šæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹é€šè¿‡æ‰¹å¤„ç†å’Œé¢„å–æ¥é«˜æ•ˆå¤„ç†æ•°æ®ï¼Œä»¥ä¿æŒGPUçš„æŒç»­åˆ©ç”¨ç‡ã€‚
- en: '[PRE5]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The parameters `num_workers` and `prefetch_factor` control parallel processing
    and data buffering. Multiple worker processes handle data loading and preprocessing
    concurrently, while prefetch_factor determines the number of batches prepared
    in advance.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°`num_workers`å’Œ`prefetch_factor`æ§åˆ¶å¹¶è¡Œå¤„ç†å’Œæ•°æ®ç¼“å†²ã€‚å¤šä¸ªå·¥ä½œè¿›ç¨‹åŒæ—¶å¤„ç†æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼Œè€Œ`prefetch_factor`ç¡®å®šé¢„å…ˆå‡†å¤‡æ‰¹æ¬¡çš„æ•°é‡ã€‚
- en: Buffer management plays a key role in pipeline efficiency. The prefetch buffer
    size requires careful tuning to balance resource utilization. A buffer that is
    too small causes the GPU to wait for data preparation, reintroducing the idle
    time these techniques aim to eliminate. Conversely, allocating an overly large
    buffer consumes memory that could otherwise store model parameters or larger batch
    sizes.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼“å†²åŒºç®¡ç†åœ¨æµæ°´çº¿æ•ˆç‡ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚é¢„å–ç¼“å†²åŒºçš„å¤§å°éœ€è¦ä»”ç»†è°ƒæ•´ä»¥å¹³è¡¡èµ„æºåˆ©ç”¨ç‡ã€‚ç¼“å†²åŒºè¿‡å°ä¼šå¯¼è‡´GPUç­‰å¾…æ•°æ®å‡†å¤‡ï¼Œé‡æ–°å¼•å…¥è¿™äº›æŠ€æœ¯æ—¨åœ¨æ¶ˆé™¤çš„ç©ºé—²æ—¶é—´ã€‚ç›¸åï¼Œåˆ†é…è¿‡å¤§çš„ç¼“å†²åŒºä¼šæ¶ˆè€—æœ¬å¯ä»¥å­˜å‚¨æ¨¡å‹å‚æ•°æˆ–æ›´å¤§æ‰¹æ¬¡çš„å†…å­˜ã€‚
- en: The implementation relies on effective CPU-GPU coordination. The CPU manages
    data preparation tasks while the GPU handles computation. This division of labor,
    combined with storage I/O operations, creates an efficient pipeline that minimizes
    idle time across hardware resources.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ä¾èµ–äºæœ‰æ•ˆçš„CPU-GPUåè°ƒã€‚CPUç®¡ç†æ•°æ®å‡†å¤‡ä»»åŠ¡ï¼Œè€ŒGPUå¤„ç†è®¡ç®—ã€‚è¿™ç§åŠ³åŠ¨åˆ†å·¥ï¼ŒåŠ ä¸Šå­˜å‚¨I/Oæ“ä½œï¼Œåˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æµæ°´çº¿ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†ç¡¬ä»¶èµ„æºä¸Šçš„ç©ºé—²æ—¶é—´ã€‚
- en: These optimization techniques yield particular benefits in scenarios involving
    slow storage access, complex data preprocessing, or large datasets. These techniques
    offer specific advantages in different training contexts depending on the computational
    and data characteristics.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¼˜åŒ–æŠ€æœ¯åœ¨æ¶‰åŠæ…¢é€Ÿå­˜å‚¨è®¿é—®ã€å¤æ‚æ•°æ®é¢„å¤„ç†æˆ–å¤§æ•°æ®é›†çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç›Šã€‚è¿™äº›æŠ€æœ¯åœ¨ä¸åŒçš„è®­ç»ƒç¯å¢ƒä¸­æä¾›ç‰¹å®šçš„ä¼˜åŠ¿ï¼Œå…·ä½“å–å†³äºè®¡ç®—å’Œæ•°æ®ç‰¹å¾ã€‚
- en: Prefetching Benefits
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é¢„å–ä¼˜åŠ¿
- en: 'Prefetching and overlapping are techniques that significantly enhance the efficiency
    of training pipelines by addressing key bottlenecks in data handling and computation.
    To illustrate the impact of these benefits, [TableÂ 8.5](ch014.xhtml#tbl-prefetching)
    presents the following comparison:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å–å’Œé‡å æ˜¯ä¸¤ç§æŠ€æœ¯ï¼Œé€šè¿‡è§£å†³æ•°æ®å¤„ç†å’Œè®¡ç®—ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒæµæ°´çº¿çš„æ•ˆç‡ã€‚ä¸ºäº†è¯´æ˜è¿™äº›ç›Šå¤„çš„å½±å“ï¼Œ[è¡¨8.5](ch014.xhtml#tbl-prefetching)å±•ç¤ºäº†ä»¥ä¸‹æ¯”è¾ƒï¼š
- en: 'TableÂ 8.5: **Pipeline Optimization**: Prefetching and overlapping maximize
    hardware utilization and reduce training time by enabling parallel data loading
    and computation, overcoming bottlenecks inherent in sequential pipelines. Increased
    resource usage and adaptability to varying bottlenecks demonstrate the scalability
    advantages of these techniques.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.5ï¼š**æµæ°´çº¿ä¼˜åŒ–**ï¼šé€šè¿‡å¯ç”¨å¹¶è¡Œæ•°æ®åŠ è½½å’Œè®¡ç®—ï¼Œé¢„å–å’Œé‡å æœ€å¤§åŒ–äº†ç¡¬ä»¶åˆ©ç”¨ç‡å¹¶å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œå…‹æœäº†é¡ºåºæµæ°´çº¿ä¸­å›ºæœ‰çš„ç“¶é¢ˆã€‚å¢åŠ çš„èµ„æºä½¿ç”¨å’Œé€‚åº”ä¸åŒç“¶é¢ˆçš„èƒ½åŠ›å±•ç¤ºäº†è¿™äº›æŠ€æœ¯çš„å¯æ‰©å±•æ€§ä¼˜åŠ¿ã€‚
- en: '| **Aspect** | **Traditional Pipeline** | **With Prefetching & Overlapping**
    |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| **æ–¹é¢** | **ä¼ ç»Ÿæµæ°´çº¿** | **å¸¦æœ‰é¢„å–å’Œé‡å ** |'
- en: '| --- | --- | --- |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **GPU Utilization** | Frequent idle periods | Near-constant utilization |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| **GPUåˆ©ç”¨ç‡** | ç»å¸¸ç©ºé—² | æ¥è¿‘æ’å®š |'
- en: '| **Training Time** | Longer due to sequential operations | Reduced through
    parallelism |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| **è®­ç»ƒæ—¶é—´** | ç”±äºé¡ºåºæ“ä½œè€Œæ›´é•¿ | é€šè¿‡å¹¶è¡ŒåŒ–è€Œå‡å°‘ |'
- en: '| **Resource Usage** | Often suboptimal | Maximized across available hardware
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| **èµ„æºä½¿ç”¨** | å¸¸å¸¸æ¬¡ä¼˜ | åœ¨å¯ç”¨ç¡¬ä»¶ä¸Šæœ€å¤§åŒ– |'
- en: '| **Scalability** | Limited by slowest component | Adaptable to various bottlenecks
    |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| **å¯æ‰©å±•æ€§** | å—é™äºæœ€æ…¢çš„ç»„ä»¶ | é€‚åº”å„ç§ç“¶é¢ˆ |'
- en: One of the most critical advantages of these methods is the improvement in GPU
    utilization. In traditional, unoptimized pipelines, the GPU often remains idle
    while waiting for data to be fetched and preprocessed. This idle time creates
    inefficiencies, especially in workflows where data augmentation or preprocessing
    involves complex transformations. By introducing asynchronous data loading and
    overlapping, these techniques ensure that the GPU consistently has data ready
    to process, eliminating unnecessary delays.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•æœ€å…³é”®çš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯æé«˜äº†GPUçš„åˆ©ç”¨ç‡ã€‚åœ¨ä¼ ç»Ÿçš„æœªä¼˜åŒ–æµæ°´çº¿ä¸­ï¼ŒGPUç»å¸¸åœ¨ç­‰å¾…æ•°æ®è¢«æ£€ç´¢å’Œé¢„å¤„ç†æ—¶å¤„äºç©ºé—²çŠ¶æ€ã€‚è¿™ç§ç©ºé—²æ—¶é—´äº§ç”Ÿäº†ä½æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å¢å¼ºæˆ–é¢„å¤„ç†æ¶‰åŠå¤æ‚è½¬æ¢çš„å·¥ä½œæµç¨‹ä¸­ã€‚é€šè¿‡å¼•å…¥å¼‚æ­¥æ•°æ®åŠ è½½å’Œé‡å ï¼Œè¿™äº›æŠ€æœ¯ç¡®ä¿GPUå§‹ç»ˆæœ‰æ•°æ®å¯ä¾›å¤„ç†ï¼Œæ¶ˆé™¤äº†ä¸å¿…è¦çš„å»¶è¿Ÿã€‚
- en: Another important benefit is the reduction in overall training time. Prefetching
    and overlapping allow the computational pipeline to operate continuously, with
    multiple stages working simultaneously rather than sequentially. For example,
    while the GPU processes the current batch, the data loader fetches and preprocesses
    the next batch, ensuring a steady flow of data through the system. This parallelism
    minimizes latency between training iterations, allowing for faster completion
    of training cycles, particularly in scenarios involving large-scale datasets.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé‡è¦çš„å¥½å¤„æ˜¯å‡å°‘äº†æ•´ä½“è®­ç»ƒæ—¶é—´ã€‚é¢„å–å’Œé‡å å¤„ç†å…è®¸è®¡ç®—ç®¡é“æŒç»­è¿è¡Œï¼Œå¤šä¸ªé˜¶æ®µåŒæ—¶å·¥ä½œè€Œä¸æ˜¯é¡ºåºè¿›è¡Œã€‚ä¾‹å¦‚ï¼Œå½“GPUå¤„ç†å½“å‰æ‰¹æ¬¡æ—¶ï¼Œæ•°æ®åŠ è½½å™¨å¯ä»¥æ£€ç´¢å¹¶é¢„å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡ï¼Œç¡®ä¿æ•°æ®é€šè¿‡ç³»ç»Ÿçš„ç¨³å®šæµåŠ¨ã€‚è¿™ç§å¹¶è¡Œæ€§æœ€å°åŒ–äº†è®­ç»ƒè¿­ä»£ä¹‹é—´çš„å»¶è¿Ÿï¼Œä½¿å¾—è®­ç»ƒå‘¨æœŸå¯ä»¥æ›´å¿«å®Œæˆï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤§è§„æ¨¡æ•°æ®é›†çš„åœºæ™¯ä¸­ã€‚
- en: These techniques are highly scalable and adaptable to various hardware configurations.
    Prefetching buffers and overlapping mechanisms can be tuned to match the specific
    requirements of a system, whether the bottleneck lies in slow storage, limited
    network bandwidth, or computational constraints. By aligning the data pipeline
    with the capabilities of the underlying hardware, prefetching and overlapping
    maximize resource utilization, making them invaluable for large-scale machine
    learning workflows.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŠ€æœ¯å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼Œå¯ä»¥é€‚åº”å„ç§ç¡¬ä»¶é…ç½®ã€‚é¢„å–ç¼“å†²åŒºå’Œé‡å æœºåˆ¶å¯ä»¥æ ¹æ®ç³»ç»Ÿçš„å…·ä½“è¦æ±‚è¿›è¡Œè°ƒæ•´ï¼Œæ— è®ºæ˜¯ç“¶é¢ˆåœ¨äºç¼“æ…¢çš„å­˜å‚¨ã€æœ‰é™çš„ç½‘ç»œå¸¦å®½è¿˜æ˜¯è®¡ç®—é™åˆ¶ã€‚é€šè¿‡å°†æ•°æ®ç®¡é“ä¸åº•å±‚ç¡¬ä»¶çš„èƒ½åŠ›ç›¸åŒ¹é…ï¼Œé¢„å–å’Œé‡å å¤„ç†æœ€å¤§åŒ–äº†èµ„æºåˆ©ç”¨ç‡ï¼Œå¯¹äºå¤§è§„æ¨¡æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹æ¥è¯´æ˜¯æ— ä»·çš„ã€‚
- en: Overall, prefetching and overlapping directly address some of the most common
    inefficiencies in training pipelines. By optimizing data flow and computation,
    these methods not only improve hardware efficiency but also enable the training
    of more complex models within shorter timeframes.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œé¢„å–å’Œé‡å å¤„ç†ç›´æ¥è§£å†³äº†è®­ç»ƒç®¡é“ä¸­æœ€å¸¸è§çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®æµå’Œè®¡ç®—ï¼Œè¿™äº›æ–¹æ³•ä¸ä»…æé«˜äº†ç¡¬ä»¶æ•ˆç‡ï¼Œè¿˜èƒ½å¤Ÿåœ¨æ›´çŸ­çš„æ—¶é—´å†…è®­ç»ƒæ›´å¤æ‚çš„æ¨¡å‹ã€‚
- en: Data Pipeline Optimization Applications
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ä¼˜åŒ–åº”ç”¨
- en: Prefetching and overlapping are highly versatile techniques that can be applied
    across various machine learning domains and tasks to enhance pipeline efficiency.
    Their benefits are most evident in scenarios where data handling and preprocessing
    are computationally expensive or where large-scale datasets create potential bottlenecks
    in data transfer and loading.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å–å’Œé‡å æ˜¯é«˜åº¦é€šç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æœºå™¨å­¦ä¹ é¢†åŸŸå’Œä»»åŠ¡ï¼Œä»¥æå‡ç®¡é“æ•ˆç‡ã€‚å®ƒä»¬çš„ç›Šå¤„åœ¨æ•°æ®å¤„ç†å’Œé¢„å¤„ç†è®¡ç®—æˆæœ¬é«˜æ˜‚æˆ–å¤§è§„æ¨¡æ•°æ®é›†å¯èƒ½é€ æˆæ•°æ®ä¼ è¾“å’ŒåŠ è½½ç“¶é¢ˆçš„åœºæ™¯ä¸­æœ€ä¸ºæ˜æ˜¾ã€‚
- en: One of the primary use cases is in computer vision, where datasets often consist
    of high-resolution images requiring extensive preprocessing. Tasks such as image
    classification, object detection, or semantic segmentation typically involve operations
    like resizing, normalization, and data augmentation, all of which can significantly
    increase preprocessing time. By employing prefetching and overlapping, these operations
    can be carried out concurrently with computation, ensuring that the GPU remains
    busy during the training process.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªä¸»è¦çš„åº”ç”¨åœºæ™¯æ˜¯è®¡ç®—æœºè§†è§‰ï¼Œæ•°æ®é›†é€šå¸¸åŒ…å«éœ€è¦å¤§é‡é¢„å¤„ç†çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹æˆ–è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡é€šå¸¸æ¶‰åŠè°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–å’Œæ•°æ®å¢å¼ºç­‰æ“ä½œï¼Œæ‰€æœ‰è¿™äº›æ“ä½œéƒ½å¯èƒ½æ˜¾è‘—å¢åŠ é¢„å¤„ç†æ—¶é—´ã€‚é€šè¿‡é‡‡ç”¨é¢„å–å’Œé‡å å¤„ç†ï¼Œè¿™äº›æ“ä½œå¯ä»¥ä¸è®¡ç®—åŒæ—¶è¿›è¡Œï¼Œç¡®ä¿GPUåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå¿™ç¢Œã€‚
- en: For example, a typical image classification pipeline might include random cropping
    (10 ms), color jittering (15 ms), and normalization (5 ms). Without prefetching,
    these 30 ms of preprocessing would delay each training step. Prefetching allows
    these operations to occur during the previous batchâ€™s computation.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸€ä¸ªå…¸å‹çš„å›¾åƒåˆ†ç±»ç®¡é“å¯èƒ½åŒ…æ‹¬éšæœºè£å‰ªï¼ˆ10 msï¼‰ã€é¢œè‰²æŠ–åŠ¨ï¼ˆ15 msï¼‰å’Œå½’ä¸€åŒ–ï¼ˆ5 msï¼‰ã€‚å¦‚æœæ²¡æœ‰é¢„å–ï¼Œè¿™30 msçš„é¢„å¤„ç†å°†å»¶è¿Ÿæ¯ä¸ªè®­ç»ƒæ­¥éª¤ã€‚é¢„å–å…è®¸è¿™äº›æ“ä½œåœ¨ä¸Šä¸€æ‰¹æ¬¡çš„è®¡ç®—æœŸé—´è¿›è¡Œã€‚
- en: NLP workflows also benefit from these techniques, particularly when working
    with large corpora of text data. For instance, preprocessing text data involves
    tokenization (converting words to numbers), padding sequences to equal length,
    and potentially subword tokenization. In a BERT model training pipeline, these
    steps might process thousands of sentences per batch. Prefetching allows this
    text processing to happen concurrently with model training. Prefetching ensures
    that these transformations occur in parallel with training, while overlapping
    optimizes data transfer and computation. This is especially useful in transformer-based
    models like BERT or GPT, which require consistent throughput to maintain efficiency
    given their high computational demand.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: NLPå·¥ä½œæµç¨‹ä¹Ÿå—ç›Šäºè¿™äº›æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®çš„å¤§å‹è¯­æ–™åº“æ—¶ã€‚ä¾‹å¦‚ï¼Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®åŒ…æ‹¬åˆ†è¯ï¼ˆå°†å•è¯è½¬æ¢ä¸ºæ•°å­—ï¼‰ã€å¡«å……åºåˆ—ä»¥è·å¾—ç›¸åŒé•¿åº¦ï¼Œä»¥åŠå¯èƒ½çš„å­è¯åˆ†è¯ã€‚åœ¨BERTæ¨¡å‹è®­ç»ƒæµç¨‹ä¸­ï¼Œè¿™äº›æ­¥éª¤å¯èƒ½æ¯æ‰¹å¤„ç†æ•°åƒä¸ªå¥å­ã€‚é¢„å–å…è®¸è¿™ç§æ–‡æœ¬å¤„ç†ä¸æ¨¡å‹è®­ç»ƒåŒæ—¶è¿›è¡Œã€‚é¢„å–ç¡®ä¿è¿™äº›è½¬æ¢ä¸è®­ç»ƒå¹¶è¡Œå‘ç”Ÿï¼Œè€Œé‡å ä¼˜åŒ–äº†æ•°æ®ä¼ è¾“å’Œè®¡ç®—ã€‚è¿™åœ¨åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼ˆå¦‚BERTæˆ–GPTï¼‰ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œè¿™äº›æ¨¡å‹ç”±äºè®¡ç®—éœ€æ±‚é«˜ï¼Œéœ€è¦ä¿æŒä¸€è‡´çš„ååé‡ä»¥ç»´æŒæ•ˆç‡ã€‚
- en: Distributed training systems involve multiple GPUs or nodes, present another
    critical application for prefetching and overlapping. In distributed setups, network
    latency and data transfer rates often become the primary bottleneck. Prefetching
    mitigates these issues by ensuring that data is ready and available before it
    is required by any specific GPU. Overlapping further optimizes distributed training
    pipelines by coordinating the data preprocessing on individual nodes while the
    central computation continues, thus reducing overall synchronization delays.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ¶‰åŠå¤šä¸ªGPUæˆ–èŠ‚ç‚¹ï¼Œæ˜¯é¢„å–å’Œé‡å çš„å¦ä¸€ä¸ªå…³é”®åº”ç”¨ã€‚åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œç½‘ç»œå»¶è¿Ÿå’Œæ•°æ®ä¼ è¾“é€Ÿç‡é€šå¸¸æˆä¸ºä¸»è¦çš„ç“¶é¢ˆã€‚é¢„å–é€šè¿‡ç¡®ä¿åœ¨ç‰¹å®šGPUéœ€è¦ä¹‹å‰æ•°æ®å·²å‡†å¤‡å¥½å¹¶å¯ç”¨æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚é‡å é€šè¿‡åœ¨ä¸­å¤®è®¡ç®—ç»§ç»­çš„åŒæ—¶åè°ƒå„ä¸ªèŠ‚ç‚¹ä¸Šçš„æ•°æ®é¢„å¤„ç†ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹ï¼Œä»è€Œå‡å°‘æ•´ä½“åŒæ­¥å»¶è¿Ÿã€‚
- en: Beyond these domains, prefetching and overlapping are particularly valuable
    in workflows involving large-scale datasets stored on remote or cloud-based systems.
    When training on cloud platforms, the data may need to be fetched over a network
    or from distributed storage, which introduces additional latency. Using prefetching
    and overlapping in such cases helps minimize the impact of these delays, ensuring
    that training proceeds smoothly despite slower data access speeds.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è¿™äº›é¢†åŸŸä¹‹å¤–ï¼Œé¢„å–å’Œé‡å åœ¨æ¶‰åŠå­˜å‚¨åœ¨è¿œç¨‹æˆ–åŸºäºäº‘ç³»ç»Ÿä¸Šçš„å¤§è§„æ¨¡æ•°æ®é›†çš„å·¥ä½œæµç¨‹ä¸­å°¤å…¶æœ‰ä»·å€¼ã€‚åœ¨äº‘å¹³å°ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ•°æ®å¯èƒ½éœ€è¦é€šè¿‡ç½‘ç»œæˆ–ä»åˆ†å¸ƒå¼å­˜å‚¨ä¸­è·å–ï¼Œè¿™å¼•å…¥äº†é¢å¤–çš„å»¶è¿Ÿã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨é¢„å–å’Œé‡å æœ‰åŠ©äºæœ€å°åŒ–è¿™äº›å»¶è¿Ÿçš„å½±å“ï¼Œç¡®ä¿å³ä½¿åœ¨è¾ƒæ…¢çš„æ•°æ®è®¿é—®é€Ÿåº¦ä¸‹ï¼Œè®­ç»ƒä¹Ÿèƒ½é¡ºåˆ©è¿›è¡Œã€‚
- en: These use cases illustrate how prefetching and overlapping address inefficiencies
    in various machine learning pipelines. By optimizing the flow of data and computation,
    these techniques enable faster, more reliable training workflows across a wide
    range of applications.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç”¨ä¾‹è¯´æ˜äº†é¢„å–å’Œé‡å å¦‚ä½•è§£å†³å„ç§æœºå™¨å­¦ä¹ ç®¡é“ä¸­çš„ä½æ•ˆé—®é¢˜ã€‚é€šè¿‡ä¼˜åŒ–æ•°æ®å’Œè®¡ç®—çš„æµåŠ¨ï¼Œè¿™äº›æŠ€æœ¯ä½¿å„ç§åº”ç”¨ä¸­çš„è®­ç»ƒå·¥ä½œæµç¨‹æ›´å¿«ã€æ›´å¯é ã€‚
- en: Pipeline Optimization Implementation Challenges
  id: totrans-565
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç®¡é“ä¼˜åŒ–å®ç°æŒ‘æˆ˜
- en: While prefetching and overlapping are useful techniques for optimizing training
    pipelines, their implementation comes with certain challenges and trade-offs.
    Understanding these limitations is important for effectively applying these methods
    in real-world machine learning workflows.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶é¢„å–å’Œé‡å æ˜¯ä¼˜åŒ–è®­ç»ƒæµç¨‹çš„æœ‰ç”¨æŠ€æœ¯ï¼Œä½†å®ƒä»¬çš„å®ç°ä¹Ÿä¼´éšç€æŸäº›æŒ‘æˆ˜å’Œæƒè¡¡ã€‚ç†è§£è¿™äº›é™åˆ¶å¯¹äºæœ‰æ•ˆåœ°å°†è¿™äº›æ–¹æ³•åº”ç”¨äºç°å®ä¸–ç•Œçš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­éå¸¸é‡è¦ã€‚
- en: One of the primary challenges is the increased memory usage that accompanies
    prefetching and overlapping. By design, these techniques rely on maintaining a
    buffer of prefetched data batches, which requires additional memory resources.
    For large datasets or high-resolution inputs, this memory demand can become significant,
    especially when training on GPUs with limited memory capacity. If the buffer size
    is not carefully tuned, it may lead to out-of-memory errors, forcing practitioners
    to reduce batch sizes or adjust other parameters, which can impact overall efficiency.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯é¢„å–å’Œé‡å ä¼´éšçš„å†…å­˜ä½¿ç”¨å¢åŠ ã€‚æŒ‰è®¾è®¡ï¼Œè¿™äº›æŠ€æœ¯ä¾èµ–äºç»´æŠ¤ä¸€ä¸ªé¢„å–æ•°æ®æ‰¹æ¬¡çš„ç¼“å†²åŒºï¼Œè¿™éœ€è¦é¢å¤–çš„å†…å­˜èµ„æºã€‚å¯¹äºå¤§å‹æ•°æ®é›†æˆ–é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼Œè¿™ç§å†…å­˜éœ€æ±‚å¯èƒ½å˜å¾—ç›¸å½“å¤§ï¼Œå°¤å…¶æ˜¯åœ¨å†…å­˜å®¹é‡æœ‰é™çš„GPUä¸Šè®­ç»ƒæ—¶ã€‚å¦‚æœç¼“å†²åŒºå¤§å°æ²¡æœ‰ä»”ç»†è°ƒæ•´ï¼Œå¯èƒ½ä¼šå¯¼è‡´å†…å­˜ä¸è¶³é”™è¯¯ï¼Œè¿«ä½¿ä»ä¸šè€…å‡å°‘æ‰¹å¤§å°æˆ–è°ƒæ•´å…¶ä»–å‚æ•°ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ•´ä½“æ•ˆç‡ã€‚
- en: For example, with a prefetch factor of 2 and batch size of 256 high-resolution
    images (<semantics><mrow><mn>1024</mn><mo>Ã—</mo><mn>1024</mn></mrow><annotation
    encoding="application/x-tex">1024\times1024</annotation></semantics> pixels),
    the buffer might require an additional 2 GB of GPU memory. This becomes particularly
    challenging when training vision models that already require significant memory
    for their parameters and activations.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨é¢„å–å› å­ä¸º2å’Œæ‰¹å¤„ç†å¤§å°ä¸º256çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆ<semantics><mrow><mn>1024</mn><mo>Ã—</mo><mn>1024</mn></mrow><annotation
    encoding="application/x-tex">1024\times1024</annotation></semantics>åƒç´ ï¼‰æ—¶ï¼Œç¼“å†²åŒºå¯èƒ½éœ€è¦é¢å¤–çš„2
    GB GPUå†…å­˜ã€‚å½“è®­ç»ƒéœ€è¦å¤§é‡å†…å­˜æ¥å­˜å‚¨å…¶å‚æ•°å’Œæ¿€æ´»çš„è§†è§‰æ¨¡å‹æ—¶ï¼Œè¿™å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: Another difficulty lies in tuning the parameters that control prefetching and
    overlapping. Settings such as `num_workers` and `prefetch_factor` in PyTorch,
    or buffer sizes in other frameworks, need to be optimized for the specific hardware
    and workload. For instance, increasing the number of worker threads can improve
    throughput up to a point, but beyond that, it may lead to contention for CPU resources
    or even degrade performance due to excessive context switching. Determining the
    optimal configuration often requires empirical testing, which can be time-consuming.
    A common starting point is to set `num_workers` to the number of CPU cores available.
    However, on a 16-core system processing large images, using all cores for data
    loading might leave insufficient CPU resources for other essential operations,
    potentially slowing down the entire pipeline.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå›°éš¾åœ¨äºè°ƒæ•´æ§åˆ¶é¢„å–å’Œé‡å çš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œåœ¨PyTorchä¸­çš„`num_workers`å’Œ`prefetch_factor`è®¾ç½®ï¼Œæˆ–å…¶ä»–æ¡†æ¶ä¸­çš„ç¼“å†²åŒºå¤§å°ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šçš„ç¡¬ä»¶å’Œå·¥ä½œè´Ÿè½½è¿›è¡Œä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼Œå¢åŠ å·¥ä½œçº¿ç¨‹çš„æ•°é‡å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜ååé‡ï¼Œä½†è¶…è¿‡è¿™ä¸ªç‚¹ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¯¹CPUèµ„æºçš„ç«äº‰ï¼Œç”šè‡³ç”±äºè¿‡å¤šçš„ä¸Šä¸‹æ–‡åˆ‡æ¢è€Œé™ä½æ€§èƒ½ã€‚ç¡®å®šæœ€ä½³é…ç½®é€šå¸¸éœ€è¦ç»éªŒæµ‹è¯•ï¼Œè¿™å¯èƒ½å¾ˆè€—æ—¶ã€‚ä¸€ä¸ªå¸¸è§çš„èµ·ç‚¹æ˜¯å°†`num_workers`è®¾ç½®ä¸ºå¯ç”¨çš„CPUæ ¸å¿ƒæ•°ã€‚ç„¶è€Œï¼Œåœ¨ä¸€ä¸ª16æ ¸å¿ƒçš„ç³»ç»Ÿä¸Šå¤„ç†å¤§å›¾åƒæ—¶ï¼Œä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒè¿›è¡Œæ•°æ®åŠ è½½å¯èƒ½ä¼šç•™ä¸‹ä¸è¶³çš„CPUèµ„æºç”¨äºå…¶ä»–åŸºæœ¬æ“ä½œï¼Œè¿™å¯èƒ½ä¼šå‡æ…¢æ•´ä¸ªç®¡é“çš„é€Ÿåº¦ã€‚
- en: Debugging also becomes more complex in pipelines that employ prefetching and
    overlapping. Asynchronous data loading and multithreading or multiprocessing introduce
    potential race conditions, deadlocks, or synchronization issues. Diagnosing errors
    in such systems can be challenging because the execution flow is no longer straightforward.
    Developers may need to invest additional effort into monitoring, logging, and
    debugging tools to ensure that the pipeline operates reliably.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‡‡ç”¨é¢„å–å’Œé‡å çš„ç®¡é“ä¸­ï¼Œè°ƒè¯•ä¹Ÿå˜å¾—æ›´åŠ å¤æ‚ã€‚å¼‚æ­¥æ•°æ®åŠ è½½å’Œå¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹å¼•å…¥äº†æ½œåœ¨çš„ç«äº‰æ¡ä»¶ã€æ­»é”æˆ–åŒæ­¥é—®é¢˜ã€‚åœ¨è¿™æ ·ç³»ç»Ÿä¸­è¯Šæ–­é”™è¯¯å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ‰§è¡Œæµç¨‹ä¸å†ç®€å•ã€‚å¼€å‘è€…å¯èƒ½éœ€è¦æŠ•å…¥é¢å¤–çš„åŠªåŠ›æ¥ç›‘æ§ã€è®°å½•å’Œè°ƒè¯•å·¥å…·ï¼Œä»¥ç¡®ä¿ç®¡é“å¯é è¿è¡Œã€‚
- en: There are scenarios where prefetching and overlapping may offer minimal benefits.
    For instance, in systems where storage access or network bandwidth is significantly
    faster than the computation itself, these techniques might not noticeably improve
    throughput. In such cases, the additional complexity and memory overhead introduced
    by prefetching may not justify its use.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé¢„å–å’Œé‡å å¯èƒ½åªæä¾›å¾®å°çš„å¥½å¤„ã€‚ä¾‹å¦‚ï¼Œåœ¨å­˜å‚¨è®¿é—®æˆ–ç½‘ç»œå¸¦å®½æ˜¾è‘—å¿«äºè®¡ç®—æœ¬èº«çš„ç³»ç»Ÿä¸­ï¼Œè¿™äº›æŠ€æœ¯å¯èƒ½ä¸ä¼šæ˜æ˜¾æé«˜ååé‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„å–å¼•å…¥çš„é¢å¤–å¤æ‚æ€§å’Œå†…å­˜å¼€é”€å¯èƒ½ä¸è¶³ä»¥è¯æ˜å…¶ä½¿ç”¨æ˜¯åˆç†çš„ã€‚
- en: Finally, prefetching and overlapping require careful coordination across different
    components of the training pipeline, such as storage, CPUs, and GPUs. Poorly designed
    pipelines can lead to imbalances where one stage becomes a bottleneck, negating
    the advantages of these techniques. For example, if the data loading process is
    too slow to keep up with the GPUâ€™s processing speed, the benefits of overlapping
    will be limited.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œé¢„å–å’Œé‡å éœ€è¦åœ¨è®­ç»ƒç®¡é“çš„ä¸åŒç»„ä»¶ä¹‹é—´è¿›è¡Œä»”ç»†çš„åè°ƒï¼Œä¾‹å¦‚å­˜å‚¨ã€CPUå’ŒGPUã€‚è®¾è®¡ä¸è‰¯çš„ç®¡é“å¯èƒ½å¯¼è‡´ä¸å¹³è¡¡ï¼Œå…¶ä¸­ä¸€ä¸ªé˜¶æ®µæˆä¸ºç“¶é¢ˆï¼ŒæŠµæ¶ˆäº†è¿™äº›æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®åŠ è½½è¿‡ç¨‹å¤ªæ…¢ï¼Œæ— æ³•è·Ÿä¸ŠGPUçš„å¤„ç†é€Ÿåº¦ï¼Œé‡å çš„å¥½å¤„å°†å—åˆ°é™åˆ¶ã€‚
- en: Despite these challenges, prefetching and overlapping remain essential tools
    for optimizing training pipelines when used appropriately. By understanding and
    addressing their trade-offs, practitioners can implement these techniques effectively,
    ensuring smoother and more efficient machine learning workflows.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œä½†é¢„å–å’Œé‡å ä»ç„¶æ˜¯ä¼˜åŒ–è®­ç»ƒç®¡é“æ—¶ä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œåªè¦ä½¿ç”¨å¾—å½“ã€‚é€šè¿‡ç†è§£å’Œè§£å†³å®ƒä»¬çš„æƒè¡¡ï¼Œä»ä¸šè€…å¯ä»¥æœ‰æ•ˆåœ°å®æ–½è¿™äº›æŠ€æœ¯ï¼Œç¡®ä¿æ›´å¹³æ»‘ã€æ›´é«˜æ•ˆçš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ã€‚
- en: Mixed-Precision Training
  id: totrans-574
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒ
- en: While prefetching optimizes data movement, mixed-precision training addresses
    both computational throughput limitations and memory capacity constraints by strategically
    using reduced precision arithmetic where possible while maintaining numerical
    stability. This technique proves most effective when profiling reveals that training
    is constrained by GPU memory capacity or when computational units are not fully
    utilized due to memory bandwidth limitations.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰çš„é¢„å–ä¼˜åŒ–æ•°æ®ç§»åŠ¨ï¼Œè€Œæ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡åœ¨å¯èƒ½çš„æƒ…å†µä¸‹æˆ˜ç•¥æ€§åœ°ä½¿ç”¨é™ä½ç²¾åº¦çš„ç®—æœ¯ï¼ŒåŒæ—¶ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼Œæ¥è§£å†³è®¡ç®—ååé‡é™åˆ¶å’Œå†…å­˜å®¹é‡çº¦æŸã€‚å½“åˆ†æè¡¨æ˜è®­ç»ƒå—é™äºGPUå†…å­˜å®¹é‡æˆ–ç”±äºå†…å­˜å¸¦å®½é™åˆ¶å¯¼è‡´è®¡ç®—å•å…ƒæœªå……åˆ†åˆ©ç”¨æ—¶ï¼Œè¿™ç§æŠ€æœ¯è¯æ˜æœ€ä¸ºæœ‰æ•ˆã€‚
- en: Mixed-precision training combines different numerical precisions during model
    training to optimize computational efficiency. This approach uses combinations
    of FP32, 16-bit floating-point (FP16), and brain floating-point (bfloat16) formats
    to reduce memory usage and speed up computation while preserving model accuracy
    ([Micikevicius et al. 2017](ch058.xhtml#ref-micikevicius2017mixed); [Y. Wang and
    Kanwar 2019](ch058.xhtml#ref-wang_bfloat16_2019)).
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆä¸åŒçš„æ•°å€¼ç²¾åº¦ä»¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨FP32ã€16ä½æµ®ç‚¹æ•°ï¼ˆFP16ï¼‰å’Œè„‘æµ®ç‚¹æ•°ï¼ˆbfloat16ï¼‰æ ¼å¼çš„ç»„åˆæ¥å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ï¼ˆ[Micikeviciusç­‰äºº2017](ch058.xhtml#ref-micikevicius2017mixed)ï¼›[ç‹å‹‡å’ŒKanwar
    2019](ch058.xhtml#ref-wang_bfloat16_2019)ï¼‰ã€‚
- en: A neural network trained in FP32 requires 4 bytes per parameter, while both
    FP16 and bfloat16 use 2 bytes. For a model with <semantics><msup><mn>10</mn><mn>9</mn></msup><annotation
    encoding="application/x-tex">10^9</annotation></semantics> parameters, this reduction
    cuts memory usage from 4 GB to 2 GB. This memory reduction enables larger batch
    sizes and deeper architectures on the same hardware.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨FP32ä¸­è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¯ä¸ªå‚æ•°éœ€è¦4ä¸ªå­—èŠ‚ï¼Œè€ŒFP16å’Œbfloat16éƒ½ä½¿ç”¨2ä¸ªå­—èŠ‚ã€‚å¯¹äºä¸€ä¸ªå…·æœ‰<semantics><msup><mn>10</mn><mn>9</mn></msup><annotation
    encoding="application/x-tex">10^9</annotation></semantics>ä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œè¿™ç§å‡å°‘å°†å†…å­˜ä½¿ç”¨ä»4 GBé™ä½åˆ°2
    GBã€‚è¿™ç§å†…å­˜å‡å°‘ä½¿å¾—åœ¨ç›¸åŒç¡¬ä»¶ä¸Šèƒ½å¤Ÿä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°å’Œæ›´æ·±çš„æ¶æ„ã€‚
- en: The numerical precision differences between these formats shape their use cases.
    FP32 represents numbers from approximately <semantics><mrow><mi>Â±</mi><mn>1.18</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>38</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm1.18 \times 10^{-38}</annotation></semantics>
    to <semantics><mrow><mi>Â±</mi><mn>3.4</mn><mo>Ã—</mo><msup><mn>10</mn><mn>38</mn></msup></mrow><annotation
    encoding="application/x-tex">\pm3.4 \times 10^{38}</annotation></semantics> with
    7 decimal digits of precision. FP16 ranges from <semantics><mrow><mi>Â±</mi><mn>6.10</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>5</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm6.10 \times 10^{-5}</annotation></semantics> to
    <semantics><mrow><mi>Â±</mi><mn>65</mn><mo>,</mo><mn>504</mn></mrow><annotation
    encoding="application/x-tex">\pm65,504</annotation></semantics> with 3-4 decimal
    digits of precision. Bfloat16, developed by Google Brain, maintains the same dynamic
    range as FP32 (<semantics><mrow><mi>Â±</mi><mn>1.18</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>38</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm1.18 \times 10^{-38}</annotation></semantics>
    to <semantics><mrow><mi>Â±</mi><mn>3.4</mn><mo>Ã—</mo><msup><mn>10</mn><mn>38</mn></msup></mrow><annotation
    encoding="application/x-tex">\pm3.4 \times 10^{38}</annotation></semantics>) but
    with reduced precision (3-4 decimal digits). This range preservation makes bfloat16
    particularly suited for deep learning training, as it handles large and small
    gradients more effectively than FP16.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ ¼å¼ä¹‹é—´çš„æ•°å€¼ç²¾åº¦å·®å¼‚å†³å®šäº†å®ƒä»¬çš„ä½¿ç”¨åœºæ™¯ã€‚FP32è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´å¤§çº¦ä»<semantics><mrow><mi>Â±</mi><mn>1.18</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>38</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm1.18 \times 10^{-38}</annotation></semantics>åˆ°<semantics><mrow><mi>Â±</mi><mn>3.4</mn><mo>Ã—</mo><msup><mn>10</mn><mn>38</mn></msup></mrow><annotation
    encoding="application/x-tex">\pm3.4 \times 10^{38}</annotation></semantics>ï¼Œå…·æœ‰7ä½å°æ•°çš„ç²¾åº¦ã€‚FP16çš„èŒƒå›´ä»<semantics><mrow><mi>Â±</mi><mn>6.10</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>5</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm6.10 \times 10^{-5}</annotation></semantics>åˆ°<semantics><mrow><mi>Â±</mi><mn>65</mn><mo>,</mo><mn>504</mn></mrow><annotation
    encoding="application/x-tex">\pm65,504</annotation></semantics>ï¼Œå…·æœ‰3-4ä½å°æ•°çš„ç²¾åº¦ã€‚ç”±Google
    Brainå¼€å‘çš„Bfloat16ä¿æŒäº†ä¸FP32ç›¸åŒçš„åŠ¨æ€èŒƒå›´ï¼ˆ<semantics><mrow><mi>Â±</mi><mn>1.18</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>38</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">\pm1.18 \times 10^{-38}</annotation></semantics>åˆ°<semantics><mrow><mi>Â±</mi><mn>3.4</mn><mo>Ã—</mo><msup><mn>10</mn><mn>38</mn></msup></mrow><annotation
    encoding="application/x-tex">\pm3.4 \times 10^{38}</annotation></semantics>ï¼‰ï¼Œä½†ç²¾åº¦æœ‰æ‰€é™ä½ï¼ˆ3-4ä½å°æ•°ï¼‰ã€‚è¿™ç§èŒƒå›´ä¿æŒä½¿å¾—bfloat16ç‰¹åˆ«é€‚åˆæ·±åº¦å­¦ä¹ è®­ç»ƒï¼Œå› ä¸ºå®ƒæ¯”FP16æ›´æœ‰æ•ˆåœ°å¤„ç†å¤§èŒƒå›´å’Œå°èŒƒå›´çš„æ¢¯åº¦ã€‚
- en: The hybrid approach proceeds in three main phases, as illustrated in [FigureÂ 8.9](ch014.xhtml#fig-mixed-precision).
    During the forward pass, input data converts to reduced precision (FP16 or bfloat16),
    and matrix multiplications execute in this format, including activation function
    computations. In the gradient computation phase, the backward pass calculates
    gradients in reduced precision, but results are stored in FP32 master weights.
    Finally, during weight updates, the optimizer updates the main weights in FP32,
    and these updated weights convert back to reduced precision for the next forward
    pass.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œå¦‚å›¾[å›¾8.9](ch014.xhtml#fig-mixed-precision)æ‰€ç¤ºã€‚åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®è½¬æ¢ä¸ºé™ä½ç²¾åº¦ï¼ˆFP16æˆ–bfloat16ï¼‰ï¼ŒçŸ©é˜µä¹˜æ³•ä»¥è¿™ç§æ ¼å¼æ‰§è¡Œï¼ŒåŒ…æ‹¬æ¿€æ´»å‡½æ•°çš„è®¡ç®—ã€‚åœ¨æ¢¯åº¦è®¡ç®—é˜¶æ®µï¼Œåå‘ä¼ æ’­ä»¥é™ä½ç²¾åº¦è®¡ç®—æ¢¯åº¦ï¼Œä½†ç»“æœå­˜å‚¨åœ¨FP32ä¸»æƒé‡ä¸­ã€‚æœ€åï¼Œåœ¨æƒé‡æ›´æ–°æœŸé—´ï¼Œä¼˜åŒ–å™¨ä»¥FP32æ›´æ–°ä¸»æƒé‡ï¼Œå¹¶ä¸”è¿™äº›æ›´æ–°çš„æƒé‡åœ¨ä¸‹ä¸€ä¸ªæ­£å‘ä¼ æ’­ä¸­è½¬æ¢å›é™ä½ç²¾åº¦ã€‚
- en: '![](../media/file116.svg)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file116.svg)'
- en: 'FigureÂ 8.9: **Mixed Precision Training**: Reduced precision formats (FP16,
    bfloat16) accelerate deep learning by decreasing memory bandwidth and computational
    requirements during both forward and backward passes. Master weights stored in
    FP32 precision accumulate updates from reduced precision gradients, preserving
    accuracy while leveraging performance gains from lower precision arithmetic.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.9ï¼š**æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šé™ä½ç²¾åº¦æ ¼å¼ï¼ˆFP16ï¼Œbfloat16ï¼‰é€šè¿‡åœ¨æ­£å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­å‡å°‘å†…å­˜å¸¦å®½å’Œè®¡ç®—éœ€æ±‚æ¥åŠ é€Ÿæ·±åº¦å­¦ä¹ ã€‚å­˜å‚¨åœ¨FP32ç²¾åº¦çš„ä¸»æƒé‡ä»é™ä½ç²¾åº¦çš„æ¢¯åº¦ä¸­ç´¯ç§¯æ›´æ–°ï¼Œåœ¨åˆ©ç”¨ä½ç²¾åº¦ç®—æœ¯çš„æ€§èƒ½æå‡çš„åŒæ—¶ï¼Œä¿æŒç²¾åº¦ã€‚
- en: Modern hardware architectures are specifically designed to accelerate reduced
    precision computations. GPUs from NVIDIA include Tensor Cores optimized for FP16
    and bfloat16 operations ([Xianyan Jia et al. 2018](ch058.xhtml#ref-nvidia_tensors_fp16_2017)).
    Googleâ€™s TPUs natively support bfloat16, as this format was specifically designed
    for machine learning workloads. These architectural optimizations typically enable
    an order of magnitude higher computational throughput for reduced precision operations
    compared to FP32, making mixed-precision training particularly efficient on modern
    hardware.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç¡¬ä»¶æ¶æ„ä¸“é—¨è®¾è®¡ç”¨äºåŠ é€Ÿé™ä½ç²¾åº¦è®¡ç®—ã€‚NVIDIAçš„GPUåŒ…æ‹¬é’ˆå¯¹FP16å’Œbfloat16æ“ä½œä¼˜åŒ–çš„Tensor Coresï¼ˆ[Xianyan Jiaç­‰äºº2018](ch058.xhtml#ref-nvidia_tensors_fp16_2017)ï¼‰ã€‚Googleçš„TPUåŸç”Ÿæ”¯æŒbfloat16ï¼Œå› ä¸ºè¿™ç§æ ¼å¼ä¸“é—¨ä¸ºæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½è€Œè®¾è®¡ã€‚è¿™äº›æ¶æ„ä¼˜åŒ–é€šå¸¸ä½¿é™ä½ç²¾åº¦æ“ä½œçš„è®¡ç®—ååé‡æ¯”FP32é«˜ä¸€ä¸ªæ•°é‡çº§ï¼Œä½¿å¾—æ··åˆç²¾åº¦è®­ç»ƒåœ¨ç°ä»£ç¡¬ä»¶ä¸Šç‰¹åˆ«é«˜æ•ˆã€‚
- en: FP16 Computation
  id: totrans-583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FP16è®¡ç®—
- en: The majority of operations in mixed-precision training, such as matrix multiplications
    and activation functions, are performed in FP16\. The reduced precision allows
    these calculations to be executed faster and with less memory consumption compared
    to FP32\. FP16 operations are particularly effective on modern GPUs equipped with
    Tensor Cores, which are designed to accelerate computations involving half-precision
    values. These cores perform FP16 operations natively, resulting in significant
    speedups.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œå¤§å¤šæ•°æ“ä½œï¼Œå¦‚çŸ©é˜µä¹˜æ³•å’Œæ¿€æ´»å‡½æ•°ï¼Œéƒ½æ˜¯åœ¨FP16ç²¾åº¦ä¸‹æ‰§è¡Œçš„ã€‚é™ä½ç²¾åº¦ä½¿å¾—è¿™äº›è®¡ç®—å¯ä»¥æ›´å¿«åœ°æ‰§è¡Œï¼Œå¹¶ä¸”ä¸FP32ç›¸æ¯”ï¼Œå†…å­˜æ¶ˆè€—æ›´å°‘ã€‚FP16æ“ä½œåœ¨é…å¤‡Tensor
    Coresçš„ç°ä»£GPUä¸Šå°¤å…¶æœ‰æ•ˆï¼Œè¿™äº›æ ¸å¿ƒä¸“ä¸ºåŠ é€Ÿæ¶‰åŠåŠç²¾åº¦å€¼çš„è®¡ç®—è€Œè®¾è®¡ã€‚è¿™äº›æ ¸å¿ƒèƒ½å¤ŸåŸç”Ÿåœ°æ‰§è¡ŒFP16æ“ä½œï¼Œä»è€Œå®ç°æ˜¾è‘—çš„åŠ é€Ÿã€‚
- en: FP32 Accumulation
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FP32ç´¯ç§¯
- en: While FP16 is efficient, its limited precision can lead to numerical instability,
    especially in critical operations like gradient updates. To mitigate this, mixed-precision
    training retains FP32 precision for certain steps, such as weight updates and
    gradient accumulation. By maintaining higher precision for these calculations,
    the system avoids the risk of gradient underflow or overflow, ensuring the model
    converges correctly during training.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶FP16æ•ˆç‡é«˜ï¼Œä½†å…¶æœ‰é™çš„ç²¾åº¦å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¢¯åº¦æ›´æ–°ç­‰å…³é”®æ“ä½œä¸­ã€‚ä¸ºäº†å‡è½»è¿™ä¸€ç‚¹ï¼Œæ··åˆç²¾åº¦è®­ç»ƒåœ¨æƒé‡æ›´æ–°å’Œæ¢¯åº¦ç´¯ç§¯ç­‰æŸäº›æ­¥éª¤ä¸­ä¿ç•™äº†FP32ç²¾åº¦ã€‚é€šè¿‡ä¿æŒè¿™äº›è®¡ç®—çš„é«˜ç²¾åº¦ï¼Œç³»ç»Ÿé¿å…äº†æ¢¯åº¦ä¸‹æº¢æˆ–ä¸Šæº¢çš„é£é™©ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ­£ç¡®æ”¶æ•›ã€‚
- en: Loss Scaling
  id: totrans-587
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æŸå¤±ç¼©æ”¾
- en: One of the key challenges with FP16 is its reduced dynamic range[27](#fn27),
    which increases the likelihood of gradient values becoming too small to be represented
    accurately. Loss scaling addresses this issue by temporarily amplifying gradient
    values during backpropagation. Specifically, the loss value is scaled by a large
    factor (e.g., <semantics><msup><mn>2</mn><mn>10</mn></msup><annotation encoding="application/x-tex">2^{10}</annotation></semantics>)
    before gradients are computed, ensuring they remain within the representable range
    of FP16.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: FP16çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å…¶é™ä½çš„åŠ¨æ€èŒƒå›´[27](#fn27)ï¼Œè¿™å¢åŠ äº†æ¢¯åº¦å€¼å˜å¾—å¤ªå°è€Œæ— æ³•å‡†ç¡®è¡¨ç¤ºçš„å¯èƒ½æ€§ã€‚é€šè¿‡åœ¨åå‘ä¼ æ’­æœŸé—´ä¸´æ—¶æ”¾å¤§æ¢¯åº¦å€¼ï¼ŒæŸå¤±ç¼©æ”¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®¡ç®—æ¢¯åº¦ä¹‹å‰ï¼ŒæŸå¤±å€¼é€šè¿‡ä¸€ä¸ªå¤§çš„å› å­ï¼ˆä¾‹å¦‚ï¼Œ<semantics><msup><mn>2</mn><mn>10</mn></msup><annotation
    encoding="application/x-tex">2^{10}</annotation></semantics>ï¼‰è¿›è¡Œç¼©æ”¾ï¼Œç¡®ä¿å®ƒä»¬ä¿æŒåœ¨FP16çš„å¯è¡¨ç¤ºèŒƒå›´å†…ã€‚
- en: Modern machine learning frameworks, such as PyTorch and TensorFlow, provide
    built-in support for mixed-precision training. These frameworks abstract the complexities
    of managing different precisions, enabling practitioners to implement mixed-precision
    workflows with minimal effort. For instance, PyTorchâ€™s `torch.cuda.amp` (Automatic
    Mixed Precision) library automates the process of selecting which operations to
    perform in FP16 or FP32, as well as applying loss scaling when necessary.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå¦‚PyTorchå’ŒTensorFlowï¼Œæä¾›äº†å†…ç½®çš„æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒã€‚è¿™äº›æ¡†æ¶æŠ½è±¡äº†ç®¡ç†ä¸åŒç²¾åº¦çš„å¤æ‚æ€§ï¼Œä½¿å¾—ä»ä¸šè€…å¯ä»¥ä»¥æœ€å°çš„åŠªåŠ›å®ç°æ··åˆç²¾åº¦å·¥ä½œæµç¨‹ã€‚ä¾‹å¦‚ï¼ŒPyTorchçš„`torch.cuda.amp`ï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰åº“è‡ªåŠ¨å¤„ç†é€‰æ‹©åœ¨FP16æˆ–FP32ä¸­æ‰§è¡Œå“ªäº›æ“ä½œçš„è¿‡ç¨‹ï¼Œå¹¶åœ¨å¿…è¦æ—¶åº”ç”¨æŸå¤±ç¼©æ”¾ã€‚
- en: Combining FP16 computation, FP32 accumulation, and loss scaling allows us to
    achieve mixed-precision training, resulting in a significant reduction in memory
    usage and computational overhead without compromising the accuracy or stability
    of the training process. The following sections will explore the practical advantages
    of this approach and its impact on modern machine learning workflows.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆFP16è®¡ç®—ã€FP32ç´¯åŠ å’ŒæŸå¤±ç¼©æ”¾ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°æ··åˆç²¾åº¦è®­ç»ƒï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²è®­ç»ƒè¿‡ç¨‹çš„ç²¾åº¦æˆ–ç¨³å®šæ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¼€é”€ã€‚ä»¥ä¸‹ç« èŠ‚å°†æ¢è®¨è¿™ç§æ–¹æ³•çš„å®é™…ä¼˜åŠ¿åŠå…¶å¯¹ç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹çš„å½±å“ã€‚
- en: Mixed-Precision Benefits
  id: totrans-591
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦ä¼˜åŠ¿
- en: Mixed-precision training offers advantages that make it an optimization technique
    for modern machine learning workflows. By reducing memory usage and computational
    load, it enables practitioners to train larger models, process bigger batches,
    and achieve faster results, all while maintaining model accuracy and convergence.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒæä¾›äº†ä½¿å…¶æˆä¸ºç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¼˜åŒ–æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚é€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—è´Ÿè½½ï¼Œå®ƒä½¿å¾—ä»ä¸šè€…èƒ½å¤Ÿè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ï¼Œå¤„ç†æ›´å¤§çš„æ‰¹é‡ï¼Œå¹¶å®ç°æ›´å¿«çš„æˆæœï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦å’Œæ”¶æ•›æ€§ã€‚
- en: Mixed-precision training reduces memory consumption. FP16 computations require
    only half the memory of FP32 computations, which directly reduces the storage
    required for activations, weights, and gradients during training. For instance,
    a transformer model with 1 billion parameters requires 4 GB of memory for weights
    in FP32, but only 2 GB in FP16\. This memory efficiency allows for larger batch
    sizes, which can lead to more stable gradient estimates and faster convergence.
    With less memory consumed per operation, practitioners can train deeper and more
    complex models on the same hardware, unlocking capabilities that were previously
    limited by memory constraints.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒå‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚FP16è®¡ç®—åªéœ€è¦FP32è®¡ç®—ä¸€åŠçš„å†…å­˜ï¼Œè¿™ç›´æ¥å‡å°‘äº†è®­ç»ƒæœŸé—´æ¿€æ´»ã€æƒé‡å’Œæ¢¯åº¦çš„å­˜å‚¨éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…·æœ‰10äº¿ä¸ªå‚æ•°çš„transformeræ¨¡å‹åœ¨FP32ä¸­éœ€è¦4GBçš„å†…å­˜æ¥å­˜å‚¨æƒé‡ï¼Œä½†åœ¨FP16ä¸­åªéœ€è¦2GBã€‚è¿™ç§å†…å­˜æ•ˆç‡å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡å’Œæ›´å¿«çš„æ”¶æ•›ã€‚ç”±äºæ¯ä¸ªæ“ä½œæ¶ˆè€—çš„å†…å­˜æ›´å°‘ï¼Œä»ä¸šè€…å¯ä»¥åœ¨ç›¸åŒçš„ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´æ·±ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œè§£é”ä¹‹å‰å—å†…å­˜é™åˆ¶çš„èƒ½åŠ›ã€‚
- en: Mixed-precision training also accelerates computations. Modern GPUs, such as
    those equipped with Tensor Cores, are specifically optimized for FP16 operations.
    These cores enable hardware to process more operations per cycle compared to FP32,
    resulting in faster training times. Leveraging the matrix multiplication patterns
    detailed earlier, FP16 can achieve 2-3<semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    speedup compared to FP32 for these dominant operations. This computational speedup
    becomes noticeable in large-scale models, such as transformers and convolutional
    neural networks, where these patterns concentrate the computational workload.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒä¹ŸåŠ é€Ÿäº†è®¡ç®—ã€‚ç°ä»£ GPUï¼Œå¦‚é…å¤‡ Tensor æ ¸çš„ GPUï¼Œä¸“é—¨é’ˆå¯¹ FP16 æ“ä½œè¿›è¡Œäº†ä¼˜åŒ–ã€‚è¿™äº›æ ¸å¿ƒä½¿ç¡¬ä»¶åœ¨æ¯ä¸ªå‘¨æœŸå†…å¯ä»¥å¤„ç†æ¯” FP32
    æ›´å¤šçš„æ“ä½œï¼Œä»è€Œå®ç°æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚åˆ©ç”¨å‰é¢è¯¦ç»†è¯´æ˜çš„çŸ©é˜µä¹˜æ³•æ¨¡å¼ï¼ŒFP16 å¯ä»¥åœ¨è¿™äº›ä¸»å¯¼æ“ä½œä¸Šæ¯” FP32 å®ç°é«˜è¾¾ 2-3 å€çš„é€Ÿåº¦æå‡ã€‚è¿™ç§è®¡ç®—é€Ÿåº¦æå‡åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œä¾‹å¦‚å˜å‹å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œè¿™äº›æ¨¡å¼é›†ä¸­äº†è®¡ç®—å·¥ä½œè´Ÿè½½ã€‚
- en: Mixed-precision training also improves hardware utilization by better matching
    the capabilities of modern accelerators. In traditional FP32 workflows, the computational
    throughput of GPUs is often underutilized due to their design for parallel processing.
    FP16 operations, being less demanding, allow more computations to be performed
    simultaneously, ensuring that the hardware operates closer to its full capacity.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒè¿˜é€šè¿‡æ›´å¥½åœ°åŒ¹é…ç°ä»£åŠ é€Ÿå™¨çš„åŠŸèƒ½æ¥æé«˜ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚åœ¨ä¼ ç»Ÿçš„ FP32 å·¥ä½œæµç¨‹ä¸­ï¼ŒGPU çš„è®¡ç®—ååé‡å¾€å¾€ç”±äºå¹¶è¡Œå¤„ç†çš„è®¾è®¡è€Œæœªè¢«å……åˆ†åˆ©ç”¨ã€‚FP16
    æ“ä½œè¦æ±‚è¾ƒä½ï¼Œå…è®¸åŒæ—¶æ‰§è¡Œæ›´å¤šè®¡ç®—ï¼Œç¡®ä¿ç¡¬ä»¶æ¥è¿‘å…¶æ»¡è´Ÿè·è¿è¡Œã€‚
- en: Finally, mixed-precision training aligns well with the requirements of distributed
    and cloud-based systems. In distributed training, where large-scale models are
    trained across multiple GPUs or nodes, memory and bandwidth become critical constraints.
    By reducing the size of tensors exchanged between devices, mixed precision not
    only speeds up inter-device communication but also decreases overall resource
    demands. This makes it particularly effective in environments where scalability
    and cost-efficiency are priorities.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ··åˆç²¾åº¦è®­ç»ƒä¸åˆ†å¸ƒå¼å’ŒåŸºäºäº‘çš„ç³»ç»Ÿéœ€æ±‚ç›¸å»åˆã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œå¤§è§„æ¨¡æ¨¡å‹åœ¨å¤šä¸ª GPU æˆ–èŠ‚ç‚¹ä¸Šè®­ç»ƒï¼Œå†…å­˜å’Œå¸¦å®½æˆä¸ºå…³é”®çº¦æŸã€‚é€šè¿‡å‡å°‘è®¾å¤‡é—´äº¤æ¢çš„å¼ é‡å¤§å°ï¼Œæ··åˆç²¾åº¦ä¸ä»…åŠ å¿«äº†è®¾å¤‡é—´é€šä¿¡ï¼Œè¿˜é™ä½äº†æ•´ä½“èµ„æºéœ€æ±‚ã€‚è¿™ä½¿å¾—å®ƒåœ¨å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šä¼˜å…ˆçš„ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚
- en: Overall, the benefits of mixed-precision training extend beyond performance
    improvements. By optimizing memory usage and computation, this technique enables
    machine learning practitioners to train advanced models more efficiently, making
    it a cornerstone of modern machine learning.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæ··åˆç²¾åº¦è®­ç»ƒçš„å¥½å¤„ä¸ä»…é™äºæ€§èƒ½æå‡ã€‚é€šè¿‡ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—ï¼Œè¿™é¡¹æŠ€æœ¯ä½¿æœºå™¨å­¦ä¹ ä»ä¸šè€…èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è®­ç»ƒé«˜çº§æ¨¡å‹ï¼Œæˆä¸ºç°ä»£æœºå™¨å­¦ä¹ çš„åŸºçŸ³ã€‚
- en: '**GPT-2 Mixed Precision Training Impact**'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 æ··åˆç²¾åº¦è®­ç»ƒå½±å“**'
- en: GPT-2 training heavily relies on mixed-precision (FP16) to fit within GPU memory
    constraints.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 è®­ç»ƒä¸¥é‡ä¾èµ–æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰ä»¥é€‚åº” GPU å†…å­˜é™åˆ¶ã€‚
- en: '**Memory Savings**'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†…å­˜èŠ‚çœ**'
- en: 'FP32 Baseline:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: FP32 åŸºå‡†ï¼š
- en: 'Parameters: 1.5B Ã— 4 bytes = 6.0 GB'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚æ•°ï¼š1.5B Ã— 4 å­—èŠ‚ = 6.0 GB
- en: 'Activations (batch=32): ~65 GB'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»ï¼ˆæ‰¹å¤„ç†=32ï¼‰ï¼šçº¦ ~65 GB
- en: 'Gradients: 6.0 GB'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ï¼š6.0 GB
- en: 'Total: ~77 GB (exceeds any single GPU)'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»è®¡ï¼šçº¦ ~77 GBï¼ˆè¶…è¿‡ä»»ä½•å•ä¸ª GPUï¼‰
- en: 'FP16 Mixed Precision:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: FP16 æ··åˆç²¾åº¦ï¼š
- en: 'Parameters (FP16): 1.5B Ã— 2 bytes = 3.0 GB'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚æ•°ï¼ˆFP16ï¼‰ï¼š1.5B Ã— 2 å­—èŠ‚ = 3.0 GB
- en: 'Activations (FP16): ~32.6 GB'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»ï¼ˆFP16ï¼‰ï¼šçº¦ ~32.6 GB
- en: 'Gradients (FP16): 3.0 GB'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ï¼ˆFP16ï¼‰ï¼š3.0 GB
- en: 'Optimizer state (FP32 master weights): 12.0 GB (Adam m, v)'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆFP32 ä¸»æƒé‡ï¼‰ï¼š12.0 GBï¼ˆAdam m, vï¼‰
- en: 'Total: ~51 GB (still tight, but manageable with optimizations)'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»è®¡ï¼šçº¦ ~51 GBï¼ˆä»ç„¶å¾ˆç´§å‡‘ï¼Œä½†é€šè¿‡ä¼˜åŒ–å¯ä»¥ç®¡ç†ï¼‰
- en: 'With Mixed Precision + Gradient Checkpointing:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ··åˆç²¾åº¦ + æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼š
- en: Activations reduced to ~8 GB (recompute during backward)
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡å°‘åˆ° ~8 GBï¼ˆåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼‰
- en: 'Total: ~26 GB â†’ fits comfortably in 32GB V100'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»è®¡ï¼šçº¦ ~26 GB â†’ èˆ’é€‚åœ°é€‚åº” 32GB V100
- en: '**Computational Speedup**'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¡ç®—åŠ é€Ÿ**'
- en: 'On NVIDIA V100 (Tensor Cores enabled):'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ NVIDIA V100ï¼ˆå¯ç”¨ Tensor æ ¸ï¼‰ä¸Šï¼š
- en: 'FP32 throughput: ~90 samples/sec'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP32 ååé‡ï¼šçº¦ ~90 ä¸ªæ ·æœ¬/ç§’
- en: 'FP16 throughput: ~220 samples/sec'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16 ååé‡ï¼šçº¦ ~220 ä¸ªæ ·æœ¬/ç§’
- en: 'Speedup: 2.4Ã— faster training'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿï¼š2.4 å€æ›´å¿«çš„è®­ç»ƒ
- en: '**Critical Implementation Details**'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®å®ç°ç»†èŠ‚**'
- en: 'Loss Scaling: Start with scale=2^15, dynamically reduce if overflow detected.
    Gradients in attention layers can range from 10^-6 to 10^3, so loss scaling prevents
    underflow.'
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŸå¤±ç¼©æ”¾ï¼šèµ·å§‹ç¼©æ”¾=2^15ï¼Œå¦‚æœæ£€æµ‹åˆ°æº¢å‡ºåˆ™åŠ¨æ€å‡å°‘ã€‚æ³¨æ„å±‚ä¸­çš„æ¢¯åº¦å¯ä»¥ä» 10^-6 åˆ° 10^3ï¼Œå› æ­¤æŸå¤±ç¼©æ”¾å¯ä»¥é˜²æ­¢ä¸‹æº¢ã€‚
- en: 'FP32 Master Weights: Optimizer updates in FP32 prevent weight stagnation. Small
    learning rate (2.5e-4) Ã— FP16 gradient might round to zero; FP32 accumulation
    preserves these tiny updates.'
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FP32ä¸»æƒé‡ï¼šåœ¨FP32ä¸­è¿›è¡Œçš„ä¼˜åŒ–å™¨æ›´æ–°å¯ä»¥é˜²æ­¢æƒé‡åœæ»ã€‚å°çš„å­¦ä¹ ç‡ï¼ˆ2.5e-4ï¼‰ä¹˜ä»¥FP16æ¢¯åº¦å¯èƒ½ä¼šå››èˆäº”å…¥ä¸ºé›¶ï¼›FP32ç´¯ç§¯ä¿ç•™äº†è¿™äº›å¾®å°çš„æ›´æ–°ã€‚
- en: 'Selective FP32 Operations:'
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ€§FP32æ“ä½œï¼š
- en: 'LayerNorm: Computed in FP32 (requires high precision for variance calculation)'
  id: totrans-624
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LayerNormï¼šåœ¨FP32ä¸­è®¡ç®—ï¼ˆæ–¹å·®è®¡ç®—éœ€è¦é«˜ç²¾åº¦ï¼‰
- en: 'Softmax: Computed in FP32 (exponentials need full range)'
  id: totrans-625
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmaxï¼šåœ¨FP32ä¸­è®¡ç®—ï¼ˆæŒ‡æ•°éœ€è¦å…¨èŒƒå›´ï¼‰
- en: 'All else: FP16'
  id: totrans-626
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å…¶ä»–æ“ä½œï¼šFP16
- en: '**Training Cost Impact**'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæˆæœ¬å½±å“**'
- en: 'FP32: ~$50,000 for 2 weeks on 32 V100s'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP32ï¼šåœ¨32ä¸ªV100ä¸Šè¿è¡Œ2å‘¨å¤§çº¦éœ€è¦$50,000
- en: 'FP16: ~$28,000 for 1.2 weeks on 32 V100s'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP16ï¼šåœ¨32ä¸ªV100ä¸Šè¿è¡Œ1.2å‘¨å¤§çº¦éœ€è¦$28,000
- en: 'Savings: $22,000 + 6 days faster iteration'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚çœï¼š$22,000 + 6å¤©æ›´å¿«çš„è¿­ä»£
- en: '**Quality Impact:** Minimal. GPT-2 perplexity within 0.5% of FP32 baseline,
    well within noise margin.'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´¨é‡å½±å“ï¼š** æœ€å°ã€‚GPT-2çš„å›°æƒ‘åº¦åœ¨FP32åŸºçº¿0.5%ä»¥å†…ï¼Œè¿œåœ¨å™ªå£°èŒƒå›´å†…ã€‚'
- en: Mixed-Precision Training Applications
  id: totrans-632
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒåº”ç”¨
- en: Mixed-precision training has become essential in machine learning workflows,
    particularly in domains and scenarios where computational efficiency and memory
    optimization are critical. Its ability to enable faster training and larger model
    capacities makes it highly applicable across a variety of machine learning tasks
    and architectures.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒå·²æˆä¸ºæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­çš„å…³é”®è¦ç´ ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æ•ˆç‡å’Œå†…å­˜ä¼˜åŒ–è‡³å…³é‡è¦çš„é¢†åŸŸå’Œåœºæ™¯ä¸­ã€‚å®ƒèƒ½å¤Ÿå®ç°æ›´å¿«çš„è®­ç»ƒå’Œæ›´å¤§çš„æ¨¡å‹å®¹é‡ï¼Œä½¿å…¶åœ¨å„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡å’Œæ¶æ„ä¸­é«˜åº¦é€‚ç”¨ã€‚
- en: One of the most prominent use cases is in training large-scale machine learning
    models. In natural language processing, models such as BERT (345M parameters),
    GPT-3 (175B parameters), and Transformer-based architectures exemplify the computational
    patterns discussed throughout this chapter. Mixed-precision training allows these
    models to operate with larger batch sizes or deeper configurations, facilitating
    faster convergence and improved accuracy on massive datasets.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€çªå‡ºçš„ç”¨ä¾‹ä¹‹ä¸€æ˜¯åœ¨è®­ç»ƒå¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼ŒBERTï¼ˆ345Må‚æ•°ï¼‰ã€GPT-3ï¼ˆ175Bå‚æ•°ï¼‰ä»¥åŠåŸºäºTransformerçš„æ¶æ„æ˜¯æœ¬ç« è®¨è®ºçš„è®¡ç®—æ¨¡å¼çš„å…¸å‹ä¾‹å­ã€‚æ··åˆç²¾åº¦è®­ç»ƒå…è®¸è¿™äº›æ¨¡å‹ä»¥æ›´å¤§çš„æ‰¹é‡å¤§å°æˆ–æ›´æ·±çš„é…ç½®è¿è¡Œï¼Œä»è€ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦å¹¶æé«˜åœ¨å¤§é‡æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚
- en: In computer vision, tasks such as image classification, object detection, and
    segmentation often require handling high-resolution images and applying computationally
    intensive convolutional operations. By leveraging mixed-precision training, these
    workloads can be executed more efficiently, enabling the training of advanced
    architectures like ResNet, EfficientNet, and vision transformers within practical
    resource limits.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç­‰ä»»åŠ¡é€šå¸¸éœ€è¦å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå¹¶åº”ç”¨è®¡ç®—å¯†é›†å‹çš„å·ç§¯æ“ä½œã€‚é€šè¿‡åˆ©ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¿™äº›å·¥ä½œè´Ÿè½½å¯ä»¥æ›´æœ‰æ•ˆåœ°æ‰§è¡Œï¼Œä½¿å¾—åœ¨å®ç”¨çš„èµ„æºé™åˆ¶å†…è®­ç»ƒResNetã€EfficientNetå’Œè§†è§‰Transformerç­‰é«˜çº§æ¶æ„æˆä¸ºå¯èƒ½ã€‚
- en: Mixed-precision training is also particularly valuable in reinforcement learning
    (RL), where models interact with environments to optimize decision-making policies.
    RL often involves high-dimensional state spaces and requires substantial computational
    resources for both model training and simulation. Mixed precision reduces the
    overhead of these processes, allowing researchers to focus on larger environments
    and more complex policy networks.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ä¹Ÿç‰¹åˆ«æœ‰ä»·å€¼ï¼Œå…¶ä¸­æ¨¡å‹ä¸ç¯å¢ƒäº¤äº’ä»¥ä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ é€šå¸¸æ¶‰åŠé«˜ç»´çŠ¶æ€ç©ºé—´ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒæ¨¡å‹å’Œè¿›è¡Œæ¨¡æ‹Ÿã€‚æ··åˆç²¾åº¦å‡å°‘äº†è¿™äº›è¿‡ç¨‹çš„å¼€é”€ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿä¸“æ³¨äºæ›´å¤§çš„ç¯å¢ƒå’Œæ›´å¤æ‚çš„ç­–ç•¥ç½‘ç»œã€‚
- en: Another critical application is in distributed training systems. When training
    models across multiple GPUs or nodes, memory and bandwidth become limiting factors
    for scalability. Mixed precision addresses these issues by reducing the size of
    activations, weights, and gradients exchanged between devices. For example, in
    a distributed training setup with 8 GPUs, reducing tensor sizes from FP32 to FP16
    can halve the communication bandwidth requirements from 320 GB/s to 160 GB/s.
    This optimization is beneficial in cloud-based environments, where resource allocation
    and cost efficiency are critical.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå…³é”®åº”ç”¨æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ã€‚å½“åœ¨å¤šä¸ªGPUæˆ–èŠ‚ç‚¹ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ï¼Œå†…å­˜å’Œå¸¦å®½æˆä¸ºå¯æ‰©å±•æ€§çš„é™åˆ¶å› ç´ ã€‚æ··åˆç²¾åº¦é€šè¿‡å‡å°‘è®¾å¤‡ä¹‹é—´äº¤æ¢çš„æ¿€æ´»ã€æƒé‡å’Œæ¢¯åº¦çš„å°ºå¯¸æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªåŒ…å«8ä¸ªGPUçš„åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®ä¸­ï¼Œå°†å¼ é‡å¤§å°ä»FP32å‡å°‘åˆ°FP16å¯ä»¥å°†é€šä¿¡å¸¦å®½éœ€æ±‚ä»320
    GB/så‡åŠåˆ°160 GB/sã€‚è¿™ç§ä¼˜åŒ–åœ¨åŸºäºäº‘çš„ç¯å¢ƒä¸­éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºåœ¨äº‘ç¯å¢ƒä¸­èµ„æºåˆ†é…å’Œæˆæœ¬æ•ˆç‡è‡³å…³é‡è¦ã€‚
- en: Mixed-precision training is increasingly used in areas such as speech processing,
    generative modeling, and scientific simulations. Models in these fields often
    have large data and parameter requirements that can push the limits of traditional
    FP32 workflows. By optimizing memory usage and leveraging the speedups provided
    by Tensor Cores, practitioners can train advanced models faster and more cost-effectively.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒåœ¨è¯­éŸ³å¤„ç†ã€ç”Ÿæˆå»ºæ¨¡å’Œç§‘å­¦æ¨¡æ‹Ÿç­‰é¢†åŸŸå¾—åˆ°è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ã€‚è¿™äº›é¢†åŸŸçš„æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡æ•°æ®å’Œå‚æ•°ï¼Œè¿™å¯èƒ½ä¼šè¶…å‡ºä¼ ç»ŸFP32å·¥ä½œæµç¨‹çš„æé™ã€‚é€šè¿‡ä¼˜åŒ–å†…å­˜ä½¿ç”¨å¹¶åˆ©ç”¨Tensoræ ¸å¿ƒæä¾›çš„åŠ é€Ÿï¼Œä»ä¸šè€…å¯ä»¥æ›´å¿«ã€æ›´ç»æµåœ°è®­ç»ƒé«˜çº§æ¨¡å‹ã€‚
- en: The adaptability of mixed-precision training to diverse tasks and domains underscores
    its importance in modern machine learning. Whether applied to large-scale natural
    language models, computationally intensive vision architectures, or distributed
    training environments, this technique empowers researchers and engineers to push
    the boundaries of what is computationally feasible.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒå¯¹ä¸åŒä»»åŠ¡å’Œé¢†åŸŸçš„é€‚åº”æ€§å‡¸æ˜¾äº†å…¶åœ¨ç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚æ— è®ºåº”ç”¨äºå¤§è§„æ¨¡è‡ªç„¶è¯­è¨€æ¨¡å‹ã€è®¡ç®—å¯†é›†å‹è§†è§‰æ¶æ„è¿˜æ˜¯åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œè¿™é¡¹æŠ€æœ¯éƒ½èµ‹äºˆç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆæ¨åŠ¨è®¡ç®—å¯è¡Œæ€§çš„è¾¹ç•Œã€‚
- en: Mixed-Precision Training Limitations
  id: totrans-640
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒå±€é™æ€§
- en: While mixed-precision training offers significant advantages in terms of memory
    efficiency and computational speed, it also introduces several challenges and
    trade-offs that must be carefully managed to ensure successful implementation.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ··åˆç²¾åº¦è®­ç»ƒåœ¨å†…å­˜æ•ˆç‡å’Œè®¡ç®—é€Ÿåº¦æ–¹é¢æä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†å‡ ä¸ªæŒ‘æˆ˜å’Œæƒè¡¡ï¼Œå¿…é¡»è°¨æ…ç®¡ç†ä»¥ç¡®ä¿æˆåŠŸå®æ–½ã€‚
- en: One of the primary challenges lies in the reduced precision of FP16\. While
    FP16 computations are faster and require less memory, their limited dynamic range
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>Â±</mi><mn>65</mn><mo>,</mo><mn>504</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\pm65,504)</annotation></semantics>
    can lead to numerical instability, particularly during gradient computations.
    Small gradient values below <semantics><mrow><mn>6</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>5</mn></mrow></msup></mrow><annotation
    encoding="application/x-tex">6 \times 10^{-5}</annotation></semantics> become
    too small to be represented accurately in FP16, resulting in underflow. While
    loss scaling addresses this by multiplying gradients by factors like <semantics><msup><mn>2</mn><mn>8</mn></msup><annotation
    encoding="application/x-tex">2^{8}</annotation></semantics> to <semantics><msup><mn>2</mn><mn>14</mn></msup><annotation
    encoding="application/x-tex">2^{14}</annotation></semantics>, implementing and
    tuning this scaling factor adds complexity to the training process.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜åœ¨äºFP16çš„ç²¾åº¦é™ä½ã€‚è™½ç„¶FP16è®¡ç®—æ›´å¿«ä¸”å†…å­˜éœ€æ±‚æ›´ä½ï¼Œä½†å®ƒä»¬çš„æœ‰é™åŠ¨æ€èŒƒå›´ï¼ˆÂ±65,504ï¼‰å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ¢¯åº¦è®¡ç®—æœŸé—´ã€‚å°äº6Ã—10^-5çš„å°æ¢¯åº¦å€¼åœ¨FP16ä¸­å˜å¾—å¤ªå°ï¼Œæ— æ³•å‡†ç¡®è¡¨ç¤ºï¼Œä»è€Œå¯¼è‡´ä¸‹æº¢ã€‚è™½ç„¶æŸå¤±ç¼©æ”¾é€šè¿‡å°†æ¢¯åº¦ä¹˜ä»¥2^8åˆ°2^14è¿™æ ·çš„å› å­æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ç°å’Œè°ƒæ•´æ­¤ç¼©æ”¾å› å­ä¼šå¢åŠ è®­ç»ƒè¿‡ç¨‹çš„å¤æ‚æ€§ã€‚
- en: Another trade-off involves the increased risk of convergence issues. While many
    modern machine learning tasks perform well with mixed-precision training, certain
    models or datasets may require higher precision to achieve stable and reliable
    results. For example, recurrent neural networks with long sequences often accumulate
    numerical errors in FP16, requiring careful gradient clipping and precision management.
    In such cases, practitioners may need to experiment with selectively enabling
    or disabling FP16 computations for specific operations, which can complicate the
    training workflow.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæƒè¡¡æ¶‰åŠæ”¶æ•›é—®é¢˜çš„é£é™©å¢åŠ ã€‚è™½ç„¶è®¸å¤šç°ä»£æœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†æŸäº›æ¨¡å‹æˆ–æ•°æ®é›†å¯èƒ½éœ€è¦æ›´é«˜çš„ç²¾åº¦æ‰èƒ½å®ç°ç¨³å®šå¯é çš„ç»“æœã€‚ä¾‹å¦‚ï¼Œå…·æœ‰é•¿åºåˆ—çš„å¾ªç¯ç¥ç»ç½‘ç»œåœ¨FP16ä¸­å¾€å¾€ä¼šç§¯ç´¯æ•°å€¼è¯¯å·®ï¼Œéœ€è¦ä»”ç»†çš„æ¢¯åº¦è£å‰ªå’Œç²¾åº¦ç®¡ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ä¸šè€…å¯èƒ½éœ€è¦å°è¯•é€‰æ‹©æ€§åœ°å¯ç”¨æˆ–ç¦ç”¨ç‰¹å®šæ“ä½œçš„FP16è®¡ç®—ï¼Œè¿™å¯èƒ½ä¼šä½¿è®­ç»ƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ã€‚
- en: Debugging and monitoring mixed-precision training also require additional attention.
    Numerical issues such as NaN (Not a Number) values in gradients or activations
    are more common in FP16 workflows and may be difficult to trace without proper
    tools and logging. For instance, gradient explosions in deep networks might manifest
    differently in mixed precision, appearing as infinities in FP16 before they would
    in FP32\. Frameworks like PyTorch and TensorFlow provide utilities for debugging
    mixed-precision training, but these tools may not catch every edge case, especially
    in custom implementations.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒè¯•å’Œç›‘æ§æ··åˆç²¾åº¦è®­ç»ƒä¹Ÿéœ€è¦é¢å¤–çš„å…³æ³¨ã€‚åœ¨æ¢¯åº¦æˆ–æ¿€æ´»ä¸­å‡ºç°çš„ NaNï¼ˆéæ•°å­—ï¼‰å€¼ç­‰æ•°å€¼é—®é¢˜åœ¨ FP16 å·¥ä½œæµç¨‹ä¸­æ›´ä¸ºå¸¸è§ï¼Œå¦‚æœæ²¡æœ‰é€‚å½“çš„å·¥å…·å’Œæ—¥å¿—è®°å½•ï¼Œå¯èƒ½éš¾ä»¥è¿½è¸ªã€‚ä¾‹å¦‚ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­ï¼Œæ¢¯åº¦çˆ†ç‚¸å¯èƒ½åœ¨æ··åˆç²¾åº¦ä¸­è¡¨ç°å‡ºä¸åŒçš„ç°è±¡ï¼Œåœ¨
    FP16 ä¸­è¡¨ç°ä¸ºæ— ç©·å¤§ï¼Œè€Œåœ¨ FP32 ä¸­åˆ™ä¸ä¼šã€‚
- en: Another challenge is the dependency on specialized hardware. Mixed-precision
    training relies heavily on GPU architectures optimized for FP16 operations, such
    as Tensor Cores in NVIDIAâ€™s GPUs. While these GPUs are becoming increasingly common,
    not all hardware supports mixed-precision operations, limiting the applicability
    of this technique in some environments.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å¯¹ä¸“ç”¨ç¡¬ä»¶çš„ä¾èµ–ã€‚æ··åˆç²¾åº¦è®­ç»ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé’ˆå¯¹ FP16 æ“ä½œä¼˜åŒ–çš„ GPU æ¶æ„ï¼Œä¾‹å¦‚ NVIDIA GPU ä¸­çš„ Tensor Coresã€‚è™½ç„¶è¿™äº›
    GPU æ­£åœ¨å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œä½†å¹¶éæ‰€æœ‰ç¡¬ä»¶éƒ½æ”¯æŒæ··åˆç²¾åº¦æ“ä½œï¼Œè¿™é™åˆ¶äº†è¯¥æŠ€æœ¯åœ¨æŸäº›ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚
- en: Finally, there are scenarios where mixed-precision training may not provide
    significant benefits. Models with relatively low computational demand (less than
    10M parameters) or small parameter sizes may not fully utilize the speedups offered
    by FP16 operations. In such cases, the additional complexity of mixed-precision
    workflows may outweigh their potential advantages.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå­˜åœ¨ä¸€äº›åœºæ™¯ï¼Œæ··åˆç²¾åº¦è®­ç»ƒå¯èƒ½ä¸ä¼šå¸¦æ¥æ˜¾è‘—çš„å¥½å¤„ã€‚è®¡ç®—éœ€æ±‚ç›¸å¯¹è¾ƒä½ï¼ˆå‚æ•°å°‘äº 10Mï¼‰æˆ–å‚æ•°è§„æ¨¡è¾ƒå°çš„æ¨¡å‹å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨ FP16 æ“ä½œæä¾›çš„åŠ é€Ÿã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ··åˆç²¾åº¦å·¥ä½œæµç¨‹çš„é¢å¤–å¤æ‚æ€§å¯èƒ½è¶…è¿‡äº†å®ƒä»¬çš„æ½œåœ¨ä¼˜åŠ¿ã€‚
- en: Despite these challenges, mixed-precision training remains a highly effective
    optimization technique for most large-scale machine learning tasks. By understanding
    and addressing its trade-offs, practitioners can use its benefits while minimizing
    potential drawbacks, ensuring efficient and reliable training workflows.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ··åˆç²¾åº¦è®­ç»ƒä»ç„¶æ˜¯å¤§å¤šæ•°å¤§è§„æ¨¡æœºå™¨å­¦ä¹ ä»»åŠ¡çš„é«˜æ•ˆä¼˜åŒ–æŠ€æœ¯ã€‚é€šè¿‡ç†è§£å’Œè§£å†³å…¶æƒè¡¡ï¼Œä»ä¸šè€…å¯ä»¥åˆ©ç”¨å…¶ä¼˜åŠ¿ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘æ½œåœ¨ç¼ºç‚¹ï¼Œç¡®ä¿é«˜æ•ˆä¸”å¯é çš„è®­ç»ƒå·¥ä½œæµç¨‹ã€‚
- en: Gradient Accumulation and Checkpointing
  id: totrans-648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹
- en: Complementing mixed-precisionâ€™s approach to memory optimization, gradient accumulation
    and checkpointing techniques address memory capacity constraints by trading computational
    time for reduced memory usage. These techniques prove most effective when profiling
    reveals that training is limited by available memory rather than computational
    throughput, enabling larger models or batch sizes on memory-constrained hardware.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¥å……æ··åˆç²¾åº¦å¯¹å†…å­˜ä¼˜åŒ–çš„æ–¹æ³•ï¼Œæ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹æŠ€æœ¯é€šè¿‡ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ä¸ºä»£ä»·æ¥æ¢å–è®¡ç®—æ—¶é—´ï¼Œè§£å†³äº†å†…å­˜å®¹é‡é™åˆ¶ã€‚å½“åˆ†æè¡¨æ˜è®­ç»ƒå—é™äºå¯ç”¨å†…å­˜è€Œä¸æ˜¯è®¡ç®—ååé‡æ—¶ï¼Œè¿™äº›æŠ€æœ¯è¯æ˜æœ€ä¸ºæœ‰æ•ˆï¼Œä»è€Œåœ¨å†…å­˜å—é™çš„ç¡¬ä»¶ä¸Šå®ç°æ›´å¤§çš„æ¨¡å‹æˆ–æ‰¹å¤§å°ã€‚
- en: 'Training large machine learning models often requires significant memory resources,
    particularly for storing three key components: activations (intermediate layer
    outputs), gradients (parameter updates), and model parameters (weights and biases)
    during forward and backward passes. However, memory constraints on GPUs can limit
    the batch size or the complexity of models that can be trained on a given device.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤§å‹æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„å†…å­˜èµ„æºï¼Œå°¤å…¶æ˜¯å­˜å‚¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ¿€æ´»ï¼ˆä¸­é—´å±‚è¾“å‡ºï¼‰ã€æ¢¯åº¦ï¼ˆå‚æ•°æ›´æ–°ï¼‰å’Œæ¨¡å‹å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰åœ¨æ­£å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ã€‚ç„¶è€Œï¼ŒGPU
    ä¸Šçš„å†…å­˜é™åˆ¶å¯èƒ½ä¼šé™åˆ¶å¯ä»¥è®­ç»ƒçš„æ‰¹å¤§å°æˆ–æ¨¡å‹çš„å¤æ‚æ€§ã€‚
- en: Gradient accumulation and activation checkpointing are two techniques designed
    to address these limitations by optimizing how memory is utilized during training.
    Both techniques enable researchers and practitioners to train larger and more
    complex models, making them indispensable tools for modern deep learning workflows.
    Understanding when to apply these techniques requires careful analysis of memory
    usage patterns and performance bottlenecks in specific training scenarios.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æ˜¯ä¸¤ç§æ—¨åœ¨é€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨æ¥å…‹æœè¿™äº›é™åˆ¶çš„æŠ€æœ¯ã€‚è¿™ä¸¤ç§æŠ€æœ¯éƒ½ä½¿ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…èƒ½å¤Ÿè®­ç»ƒæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œæˆä¸ºç°ä»£æ·±åº¦å­¦ä¹ å·¥ä½œæµç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚äº†è§£ä½•æ—¶åº”ç”¨è¿™äº›æŠ€æœ¯éœ€è¦å¯¹ç‰¹å®šè®­ç»ƒåœºæ™¯ä¸­çš„å†…å­˜ä½¿ç”¨æ¨¡å¼å’Œæ€§èƒ½ç“¶é¢ˆè¿›è¡Œä»”ç»†åˆ†æã€‚
- en: Gradient Accumulation and Checkpointing Mechanics
  id: totrans-652
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯ä¸æ£€æŸ¥ç‚¹æœºåˆ¶
- en: Gradient accumulation and activation checkpointing operate on distinct principles,
    but both aim to optimize memory usage during training by modifying how forward
    and backward computations are handled.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æ“ä½œåŸºäºä¸åŒçš„åŸç†ï¼Œä½†ä¸¤è€…éƒ½æ—¨åœ¨é€šè¿‡ä¿®æ”¹å‰å‘å’Œåå‘è®¡ç®—çš„å¤„ç†æ–¹å¼æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ã€‚
- en: Gradient Accumulation
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯
- en: Gradient accumulation simulates larger batch sizes by splitting a single effective
    batch into smaller â€œmicro-batches.â€ As illustrated in [FigureÂ 8.10](ch014.xhtml#fig-grad-accumulation),
    during each forward and backward pass, the gradients for a micro-batch are computed
    and added to an accumulated gradient buffer. Instead of immediately applying the
    gradients to update the model parameters, this process repeats for several micro-batches.
    Once the gradients from all micro-batches in the effective batch are accumulated,
    the parameters are updated using the combined gradients.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯é€šè¿‡å°†å•ä¸ªæœ‰æ•ˆæ‰¹æ¬¡æ‹†åˆ†ä¸ºæ›´å°çš„â€œå¾®æ‰¹æ¬¡â€æ¥æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ã€‚å¦‚å›¾ 8.10 æ‰€ç¤ºï¼Œåœ¨æ¯æ¬¡å‰å‘å’Œåå‘ä¼ é€’è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—å¾®æ‰¹æ¬¡çš„æ¢¯åº¦å¹¶å°†å…¶æ·»åŠ åˆ°ç´¯ç§¯æ¢¯åº¦ç¼“å†²åŒºä¸­ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æ˜¯ç«‹å³å°†æ¢¯åº¦åº”ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯å¯¹å¤šä¸ªå¾®æ‰¹æ¬¡é‡å¤æ­¤è¿‡ç¨‹ã€‚ä¸€æ—¦ç´¯ç§¯äº†æœ‰æ•ˆæ‰¹æ¬¡ä¸­æ‰€æœ‰å¾®æ‰¹æ¬¡çš„æ¢¯åº¦ï¼Œå°±ä½¿ç”¨ç»„åˆæ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚
- en: '![](../media/file117.svg)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file117.svg)'
- en: 'FigureÂ 8.10: **Gradient Accumulation**: Effective batch size increases without
    increasing per-step memory requirements by accumulating gradients from multiple
    micro-batches before updating model parameters, simulating training with a larger
    batch. This technique enables training with large models or datasets when memory
    is limited, improving training stability and potentially generalization performance.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.10ï¼š**æ¢¯åº¦ç´¯ç§¯**ï¼šé€šè¿‡åœ¨æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹å‰ç´¯ç§¯å¤šä¸ªå¾®æ‰¹æ¬¡çš„æ¢¯åº¦ï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å°å¢åŠ ï¼Œè€Œæ¯æ­¥å†…å­˜éœ€æ±‚ä¸å¢åŠ ï¼Œä»è€Œæ¨¡æ‹Ÿä½¿ç”¨æ›´å¤§æ‰¹æ¬¡çš„è®­ç»ƒã€‚è¿™é¡¹æŠ€æœ¯ä½¿å¾—åœ¨å†…å­˜æœ‰é™çš„æƒ…å†µä¸‹èƒ½å¤Ÿä½¿ç”¨å¤§å‹æ¨¡å‹æˆ–æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ½œåœ¨çš„æ³›åŒ–æ€§èƒ½ã€‚
- en: This process allows models to achieve the benefits of training with larger batch
    sizes, such as improved gradient estimates and convergence stability, without
    requiring the memory to store an entire batch at once. For instance, in PyTorch,
    this can be implemented by adjusting the learning rate proportionally to the number
    of accumulated micro-batches and calling `optimizer.step()` only after processing
    the entire effective batch.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤è¿‡ç¨‹å…è®¸æ¨¡å‹åœ¨ä¸éœ€è¦ä¸€æ¬¡æ€§å­˜å‚¨æ•´ä¸ªæ‰¹æ¬¡çš„æƒ…å†µä¸‹ï¼Œè·å¾—ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡è®­ç»ƒçš„å¥½å¤„ï¼Œä¾‹å¦‚æ”¹è¿›çš„æ¢¯åº¦ä¼°è®¡å’Œæ”¶æ•›ç¨³å®šæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­ï¼Œå¯ä»¥é€šè¿‡æŒ‰æ¯”ä¾‹è°ƒæ•´å­¦ä¹ ç‡ä»¥åŒ¹é…ç´¯ç§¯çš„å¾®æ‰¹æ¬¡æ•°é‡ï¼Œå¹¶åœ¨å¤„ç†å®Œæ•´ä¸ªæœ‰æ•ˆæ‰¹æ¬¡åè°ƒç”¨
    `optimizer.step()` æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: 'The key steps in gradient accumulation are:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯çš„å…³é”®æ­¥éª¤æ˜¯ï¼š
- en: Perform the forward pass for a micro-batch.
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹å¾®æ‰¹æ¬¡æ‰§è¡Œå‰å‘ä¼ é€’ã€‚
- en: Compute the gradients during the backward pass.
  id: totrans-661
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨åå‘ä¼ é€’æœŸé—´è®¡ç®—æ¢¯åº¦ã€‚
- en: Accumulate the gradients into a buffer without updating the model parameters.
  id: totrans-662
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ä¸æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå°†æ¢¯åº¦ç´¯ç§¯åˆ°ä¸€ä¸ªç¼“å†²åŒºä¸­ã€‚
- en: Repeat steps 1-3 for all micro-batches in the effective batch.
  id: totrans-663
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æœ‰æ•ˆæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰å¾®æ‰¹æ¬¡é‡å¤æ­¥éª¤ 1-3ã€‚
- en: Update the model parameters using the accumulated gradients after all micro-batches
    are processed.
  id: totrans-664
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†å®Œæ‰€æœ‰å¾®æ‰¹æ¬¡åï¼Œä½¿ç”¨ç´¯ç§¯çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
- en: Activation Checkpointing
  id: totrans-665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹
- en: Activation checkpointing reduces memory usage during the backward pass by discarding
    and selectively recomputing activations. In standard training, activations from
    the forward pass are stored in memory for use in gradient computations during
    backpropagation. However, these activations can consume significant memory, particularly
    in deep networks.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹é€šè¿‡ä¸¢å¼ƒå¹¶é€‰æ‹©æ€§åœ°é‡æ–°è®¡ç®—æ¿€æ´»æ¥å‡å°‘åå‘ä¼ é€’æœŸé—´çš„å†…å­˜ä½¿ç”¨ã€‚åœ¨æ ‡å‡†è®­ç»ƒä¸­ï¼Œå‰å‘ä¼ é€’çš„æ¿€æ´»å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œç”¨äºåå‘ä¼ æ’­æœŸé—´çš„æ¢¯åº¦è®¡ç®—ã€‚ç„¶è€Œï¼Œè¿™äº›æ¿€æ´»å¯èƒ½ä¼šæ¶ˆè€—å¤§é‡çš„å†…å­˜ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦ç½‘ç»œä¸­ã€‚
- en: With checkpointing, only a subset of the activations is retained during the
    forward pass. When gradients need to be computed during the backward pass, the
    discarded activations are recomputed on demand by re-executing parts of the forward
    pass, as illustrated in [FigureÂ 8.11](ch014.xhtml#fig-activation-checkpointing).
    This approach trades computational efficiency for memory savings, as the recomputation
    increases training time but allows deeper models to be trained within limited
    memory constraints. The figure shows how memory is saved by avoiding storage of
    unnecessarily large intermediate tensors from the forward pass, and simply recomputing
    them on demand in the backwards pass.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»…ä¿ç•™æ¿€æ´»å­é›†ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦è®¡ç®—æ¢¯åº¦æ—¶ï¼Œè¢«ä¸¢å¼ƒçš„æ¿€æ´»å°†æ ¹æ®éœ€è¦é‡æ–°è®¡ç®—ï¼Œå¦‚[å›¾8.11](ch014.xhtml#fig-activation-checkpointing)æ‰€ç¤ºã€‚è¿™ç§æ–¹æ³•ä»¥è®¡ç®—æ•ˆç‡ä¸ºä»£ä»·æ¢å–å†…å­˜èŠ‚çœï¼Œå› ä¸ºé‡æ–°è®¡ç®—å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œä½†å…è®¸åœ¨æœ‰é™çš„å†…å­˜çº¦æŸä¸‹è®­ç»ƒæ›´æ·±çš„æ¨¡å‹ã€‚è¯¥å›¾æ˜¾ç¤ºäº†é€šè¿‡é¿å…å­˜å‚¨å‰å‘ä¼ æ’­ä¸­ä¸å¿…è¦çš„è¾ƒå¤§ä¸­é—´å¼ é‡æ¥èŠ‚çœå†…å­˜ï¼Œå¹¶åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æŒ‰éœ€é‡æ–°è®¡ç®—è¿™äº›å¼ é‡ã€‚
- en: 'The implementation involves:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°æ¶‰åŠï¼š
- en: Splitting the model into segments.
  id: totrans-669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åˆ†æˆæ®µã€‚
- en: Retaining activations only at the boundaries of these segments during the forward
    pass.
  id: totrans-670
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»…ä¿ç•™è¿™äº›æ®µè½çš„è¾¹ç•Œæ¿€æ´»ã€‚
- en: Recomputing activations for intermediate layers during the backward pass when
    needed.
  id: totrans-671
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨éœ€è¦æ—¶ï¼Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é‡æ–°è®¡ç®—ä¸­é—´å±‚çš„æ¿€æ´»ã€‚
- en: '![](../media/file118.svg)'
  id: totrans-672
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file118.svg)'
- en: 'FigureÂ 8.11: **Activation Checkpointing**: Trading memory usage for recomputation
    during backpropagation enables training deeper neural networks. By storing only
    a subset of activations from the forward pass and recomputing others on demand,
    this technique reduces peak memory requirements at the cost of increased training
    time.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.11ï¼š**æ¿€æ´»æ£€æŸ¥ç‚¹**ï¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­é€šè¿‡äº¤æ¢å†…å­˜ä½¿ç”¨å’Œé‡æ–°è®¡ç®—æ¥è®­ç»ƒæ›´æ·±çš„ç¥ç»ç½‘ç»œã€‚é€šè¿‡ä»…å­˜å‚¨å‰å‘ä¼ æ’­ä¸­çš„ä¸€éƒ¨åˆ†æ¿€æ´»å¹¶åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å…¶ä»–éƒ¨åˆ†ï¼Œè¯¥æŠ€æœ¯ä»¥å¢åŠ è®­ç»ƒæ—¶é—´ä¸ºä»£ä»·ï¼Œå‡å°‘äº†å³°å€¼å†…å­˜éœ€æ±‚ã€‚
- en: Frameworks like PyTorch provide tools such as `torch.utils.checkpoint` to simplify
    this process. Checkpointing is particularly effective for very deep architectures,
    such as transformers or large convolutional networks, where the memory required
    for storing activations can exceed the GPUâ€™s capacity.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºPyTorchçš„æ¡†æ¶æä¾›äº†å¦‚`torch.utils.checkpoint`ä¹‹ç±»çš„å·¥å…·æ¥ç®€åŒ–æ­¤è¿‡ç¨‹ã€‚æ£€æŸ¥ç‚¹æŠ€æœ¯åœ¨éå¸¸æ·±çš„æ¶æ„ï¼ˆå¦‚å˜æ¢å™¨æˆ–å¤§å‹å·ç§¯ç½‘ç»œï¼‰ä¸­ç‰¹åˆ«æœ‰æ•ˆï¼Œå…¶ä¸­å­˜å‚¨æ¿€æ´»æ‰€éœ€çš„å†…å­˜å¯èƒ½è¶…è¿‡GPUçš„å®¹é‡ã€‚
- en: The synergy between gradient accumulation and checkpointing enables training
    of larger, more complex models. Gradient accumulation manages memory constraints
    related to batch size, while checkpointing optimizes memory usage for intermediate
    activations. Together, these techniques expand the range of models that can be
    trained on available hardware.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹ä¹‹é—´çš„ååŒä½œç”¨ä½¿å¾—è®­ç»ƒæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æ¢¯åº¦ç´¯ç§¯ç®¡ç†ä¸æ‰¹é‡å¤§å°ç›¸å…³çš„å†…å­˜çº¦æŸï¼Œè€Œæ£€æŸ¥ç‚¹ä¼˜åŒ–äº†ä¸­é—´æ¿€æ´»çš„å†…å­˜ä½¿ç”¨ã€‚è¿™äº›æŠ€æœ¯å…±åŒæ‰©å±•äº†å¯ä»¥åœ¨ç°æœ‰ç¡¬ä»¶ä¸Šè®­ç»ƒçš„æ¨¡å‹èŒƒå›´ã€‚
- en: Memory and Computational Benefits
  id: totrans-676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜å’Œè®¡ç®—æ•ˆç›Š
- en: Gradient accumulation[28](#fn28) and activation checkpointing[29](#fn29) provide
    solutions to the memory limitations often encountered in training large-scale
    machine learning models. By optimizing how memory is used during training, these
    techniques enable the development and deployment of complex architectures, even
    on hardware with constrained resources.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯[28](#fn28)å’Œæ¿€æ´»æ£€æŸ¥ç‚¹[29](#fn29)ä¸ºåœ¨è®­ç»ƒå¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶é‡åˆ°çš„å†…å­˜é™åˆ¶æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜çš„ä½¿ç”¨æ–¹å¼ï¼Œè¿™äº›æŠ€æœ¯ä½¿å¾—å³ä½¿åœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šä¹Ÿèƒ½å¼€å‘å’Œéƒ¨ç½²å¤æ‚çš„æ¶æ„ã€‚
- en: One of the primary benefits of gradient accumulation is its ability to simulate
    larger batch sizes without increasing the memory requirements for storing the
    full batch. Larger batch sizes are known to improve gradient estimates, leading
    to more stable convergence and faster training. With gradient accumulation, practitioners
    can achieve these benefits while working with smaller micro-batches that fit within
    the GPUâ€™s memory. This flexibility is useful when training models on high-resolution
    data, such as large images or 3D volumetric data, where even a single batch may
    exceed available memory.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯çš„ä¸»è¦ä¼˜ç‚¹ä¹‹ä¸€æ˜¯å®ƒèƒ½å¤Ÿæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œè€Œæ— éœ€å¢åŠ å­˜å‚¨å®Œæ•´æ‰¹é‡çš„å†…å­˜éœ€æ±‚ã€‚æ›´å¤§çš„æ‰¹é‡å¤§å°å·²çŸ¥å¯ä»¥æ”¹å–„æ¢¯åº¦ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„æ”¶æ•›å’Œæ›´å¿«çš„è®­ç»ƒã€‚ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå®è·µè€…å¯ä»¥åœ¨ä½¿ç”¨é€‚åˆGPUå†…å­˜çš„å°å¾®æ‰¹é‡æ—¶å®ç°è¿™äº›å¥½å¤„ã€‚è¿™ç§çµæ´»æ€§åœ¨è®­ç»ƒé«˜åˆ†è¾¨ç‡æ•°æ®ï¼ˆå¦‚å¤§å›¾åƒæˆ–3Dä½“æ•°æ®ï¼‰çš„æ¨¡å‹æ—¶éå¸¸æœ‰ç”¨ï¼Œå³ä½¿å•ä¸ªæ‰¹æ¬¡ä¹Ÿå¯èƒ½è¶…è¿‡å¯ç”¨å†…å­˜ã€‚
- en: Activation checkpointing, on the other hand, significantly reduces the memory
    footprint of intermediate activations during the forward pass. This allows for
    the training of deeper models, which would otherwise be infeasible due to memory
    constraints. By discarding and recomputing activations as needed, checkpointing
    frees up memory that can be used for larger models, additional layers, or higher
    resolution data. This is especially important in advanced architectures, such
    as transformers or dense convolutional networks, which require substantial memory
    to store intermediate computations.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹ï¼Œå¦ä¸€æ–¹é¢ï¼Œåœ¨æ­£å‘ä¼ é€’æœŸé—´æ˜¾è‘—å‡å°‘äº†ä¸­é—´æ¿€æ´»çš„å†…å­˜å ç”¨ã€‚è¿™å…è®¸è®­ç»ƒæ›´æ·±çš„æ¨¡å‹ï¼Œå¦åˆ™ç”±äºå†…å­˜é™åˆ¶è€Œä¸å¯è¡Œã€‚é€šè¿‡æŒ‰éœ€ä¸¢å¼ƒå’Œé‡æ–°è®¡ç®—æ¿€æ´»ï¼Œæ£€æŸ¥ç‚¹é‡Šæ”¾å‡ºå¯ç”¨äºæ›´å¤§æ¨¡å‹ã€é¢å¤–å±‚æˆ–æ›´é«˜åˆ†è¾¨ç‡æ•°æ®çš„å†…å­˜ã€‚è¿™åœ¨é«˜çº§æ¶æ„ä¸­å°¤ä¸ºé‡è¦ï¼Œä¾‹å¦‚transformeræˆ–å¯†é›†å·ç§¯ç½‘ç»œï¼Œè¿™äº›æ¶æ„éœ€è¦å¤§é‡å†…å­˜æ¥å­˜å‚¨ä¸­é—´è®¡ç®—ã€‚
- en: Both techniques enhance the scalability of machine learning workflows. In resource-constrained
    environments, such as cloud-based platforms or edge devices, these methods provide
    a means to train models efficiently without requiring expensive hardware upgrades.
    They enable researchers to experiment with larger and more complex architectures,
    pushing the boundaries of what is computationally feasible.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§æŠ€æœ¯éƒ½å¢å¼ºäº†æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹çš„å¯æ‰©å±•æ€§ã€‚åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚åŸºäºäº‘çš„å¹³å°æˆ–è¾¹ç¼˜è®¾å¤‡ï¼Œè¿™äº›æ–¹æ³•æä¾›äº†ä¸€ç§åœ¨æ— éœ€æ˜‚è´µç¡¬ä»¶å‡çº§çš„æƒ…å†µä¸‹é«˜æ•ˆè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒä»¬ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿå°è¯•æ›´å¤§å’Œæ›´å¤æ‚çš„æ¶æ„ï¼Œæ¨åŠ¨è®¡ç®—å¯è¡Œæ€§çš„è¾¹ç•Œã€‚
- en: Beyond memory optimization, these techniques also contribute to cost efficiency.
    By reducing the hardware requirements for training, gradient accumulation and
    checkpointing lower the overall cost of development, making them valuable for
    organizations working within tight budgets. This is particularly relevant for
    startups, academic institutions, or projects running on shared computing resources.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å†…å­˜ä¼˜åŒ–ä¹‹å¤–ï¼Œè¿™äº›æŠ€æœ¯è¿˜ä¿ƒè¿›äº†æˆæœ¬æ•ˆç‡ã€‚é€šè¿‡é™ä½è®­ç»ƒçš„ç¡¬ä»¶è¦æ±‚ï¼Œæ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹é™ä½äº†æ•´ä½“å¼€å‘æˆæœ¬ï¼Œå¯¹é¢„ç®—ç´§å¼ çš„ç»„ç»‡æ¥è¯´éå¸¸æœ‰ä»·å€¼ã€‚è¿™å¯¹äºåˆåˆ›å…¬å¸ã€å­¦æœ¯æœºæ„æˆ–è¿è¡Œåœ¨å…±äº«è®¡ç®—èµ„æºä¸Šçš„é¡¹ç›®å°¤å…¶ç›¸å…³ã€‚
- en: Gradient accumulation and activation checkpointing provide both technical and
    practical advantages. These techniques create a more flexible, scalable, and cost-effective
    approach to training large-scale models, empowering practitioners to tackle increasingly
    complex machine learning challenges.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æä¾›äº†æŠ€æœ¯å’Œå®è·µä¸Šçš„ä¼˜åŠ¿ã€‚è¿™äº›æŠ€æœ¯åˆ›é€ äº†ä¸€ç§æ›´çµæ´»ã€å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿåº”å¯¹æ—¥ç›Šå¤æ‚çš„æœºå™¨å­¦ä¹ æŒ‘æˆ˜ã€‚
- en: '**GPT-2 Gradient Accumulation Strategy**'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2æ¢¯åº¦ç´¯ç§¯ç­–ç•¥**'
- en: GPT-2â€™s training configuration demonstrates the essential role of gradient accumulation.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2çš„è®­ç»ƒé…ç½®å±•ç¤ºäº†æ¢¯åº¦ç´¯ç§¯çš„å¿…è¦ä½œç”¨ã€‚
- en: '**Memory Constraints**'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†…å­˜é™åˆ¶**'
- en: 'V100 32GB GPU with gradient checkpointing: Can fit batch_size=16 (as shown
    in activation memory example)'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V100 32GB GPUå¸¦æœ‰æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šå¯ä»¥é€‚åº”batch_size=16ï¼ˆå¦‚æ¿€æ´»å†…å­˜ç¤ºä¾‹æ‰€ç¤ºï¼‰
- en: 'Desired effective batch_size: 512 (optimal for transformer convergence)'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€éœ€çš„æœ‰æ•ˆæ‰¹é‡å¤§å°ï¼š512ï¼ˆtransformeræ”¶æ•›çš„æœ€ä¼˜å€¼ï¼‰
- en: 'Problem: 512 Ã· 16 = 32 GPUs needed just for batch size'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é—®é¢˜ï¼š512 Ã· 16 = ä»…ä¸ºäº†æ‰¹é‡å¤§å°å°±éœ€è¦32ä¸ªGPU
- en: '**Gradient Accumulation Solution**'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦ç´¯ç§¯è§£å†³æ–¹æ¡ˆ**'
- en: 'Instead of 32 GPUs, use 8 GPUs with gradient accumulation:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ˜¯ä½¿ç”¨32ä¸ªGPUï¼Œè€Œæ˜¯ä½¿ç”¨8ä¸ªGPUè¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼š
- en: 'Configuration:'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®ï¼š
- en: 'Per-GPU micro-batch: 16'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPUçš„å¾®æ‰¹é‡ï¼š16
- en: 'Accumulation steps: 4'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç´¯ç§¯æ­¥éª¤ï¼š4
- en: 'Effective batch per GPU: 16 Ã— 4 = 64'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPUçš„æœ‰æ•ˆæ‰¹é‡ï¼š16 Ã— 4 = 64
- en: 'Global effective batch: 8 GPUs Ã— 64 = **512** âœ“'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¨å±€æœ‰æ•ˆæ‰¹é‡ï¼š8 GPUs Ã— 64 = **512** âœ“
- en: 'Training Loop:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯ï¼š
- en: '[PRE6]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Performance Impact**'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€§èƒ½å½±å“**'
- en: 'Without Accumulation (naive approach):'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ç´¯ç§¯ï¼ˆå¤©çœŸæ–¹æ³•ï¼‰ï¼š
- en: 32 GPUs Ã— batch_size=16 = 512 effective batch
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 32 GPUs Ã— batch_size=16 = 512ä¸ªæœ‰æ•ˆæ‰¹é‡
- en: 'Gradient sync: 32 GPUs â†’ high communication overhead'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åŒæ­¥ï¼š32ä¸ªGPU â†’ é«˜é€šä¿¡å¼€é”€
- en: 'Cost: $16/hour Ã— 32 GPUs = $512/hour'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆæœ¬ï¼š$16/hour Ã— 32 GPUs = $512/hour
- en: 'With Accumulation (actual GPT-2 approach):'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ç´¯ç§¯ï¼ˆå®é™…çš„GPT-2æ–¹æ³•ï¼‰ï¼š
- en: 8 GPUs Ã— (16 Ã— 4 accumulation) = 512 effective batch
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8 GPUs Ã— (16 Ã— 4ç´¯ç§¯) = 512ä¸ªæœ‰æ•ˆæ‰¹é‡
- en: 'Gradient sync: Only every 4 steps, only 8 GPUs'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åŒæ­¥ï¼šä»…æ¯4æ­¥è¿›è¡Œä¸€æ¬¡ï¼Œåªæœ‰8ä¸ªGPU
- en: 'Cost: $16/hour Ã— 8 GPUs = $128/hour'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆæœ¬ï¼š$16/hour Ã— 8 GPUs = $128/hour
- en: 'Savings: $384/hour = 75% cost reduction'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚çœï¼š$384/hour = 75%æˆæœ¬é™ä½
- en: '**Tradeoff Analysis**'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒè¡¡åˆ†æ**'
- en: 'Compute overhead: 4Ã— forward passes per update = ~8% slower (pipeline overlaps
    some cost)'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—å¼€é”€ï¼šæ¯æ¬¡æ›´æ–°4æ¬¡æ­£å‘ä¼ é€’ = ~8%æ›´æ…¢ï¼ˆç®¡é“é‡å ä¸€äº›æˆæœ¬ï¼‰
- en: 'Memory overhead: Gradient accumulation buffer = negligible (gradients already
    needed)'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜å¼€é”€ï¼šæ¢¯åº¦ç´¯ç§¯ç¼“å†²åŒº = å¯å¿½ç•¥ï¼ˆæ¢¯åº¦å·²ç»éœ€è¦ï¼‰
- en: 'Communication benefit: Sync frequency reduced by 4Ã— â†’ communication time drops
    by 75%'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šä¿¡æ•ˆç›Šï¼šåŒæ­¥é¢‘ç‡é™ä½4å€ â†’ é€šä¿¡æ—¶é—´é™ä½75%
- en: 'Cost benefit: Training 2 weeks on 8 GPUs = $21.5K vs.Â 32 GPUs = $86K'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•ˆç›Šï¼šåœ¨8ä¸ªGPUä¸Šè®­ç»ƒ2å‘¨ = $21.5Kï¼Œè€Œåœ¨32ä¸ªGPUä¸Š = $86K
- en: '**Convergence Quality**'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¶æ•›è´¨é‡**'
- en: 'Effective batch 512 with accumulation: Perplexity 18.3'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆæ‰¹é‡512ï¼Œç´¯ç§¯ï¼šå›°æƒ‘åº¦18.3
- en: 'True batch 512 without accumulation: Perplexity 18.2'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çœŸå®æ‰¹é‡512ï¼Œä¸ç´¯ç§¯ï¼šå›°æƒ‘åº¦18.2
- en: 'Difference: 0.5% (within noise margin)'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·®å¼‚ï¼š0.5%ï¼ˆåœ¨å™ªå£°èŒƒå›´å†…ï¼‰
- en: '**Why This Works:** Gradient accumulation is mathematically equivalent to larger
    batches because gradients are additive: <semantics><mrow><mi>âˆ‡</mi><msub><mi>L</mi><mtext
    mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover><mrow><mo
    stretchy="true" form="prefix">[</mo><mfrac><mn>1</mn><mn>16</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>16</mn></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla L_{\text{batch}} = \frac{1}{N}\sum_{i=1}^N
    \nabla L(x_i) = \frac{1}{4}\sum_{j=1}^4 \left[\frac{1}{16}\sum_{k=1}^{16} \nabla
    L(x_{jk})\right]</annotation></semantics>'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆè¿™ç§æ–¹æ³•æœ‰æ•ˆï¼š** æ¢¯åº¦ç´¯ç§¯åœ¨æ•°å­¦ä¸Šç­‰åŒäºæ›´å¤§çš„æ‰¹é‡ï¼Œå› ä¸ºæ¢¯åº¦æ˜¯å¯åŠ çš„ï¼š<semantics><mrow><mi>âˆ‡</mi><msub><mi>L</mi><mtext
    mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover><mrow><mo
    stretchy="true" form="prefix">[</mo><mfrac><mn>1</mn><mn>16</mn></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>16</mn></munderover><mi>âˆ‡</mi><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\nabla L_{\text{batch}} = \frac{1}{N}\sum_{i=1}^N
    \nabla L(x_i) = \frac{1}{4}\sum_{j=1}^4 \left[\frac{1}{16}\sum_{k=1}^{16} \nabla
    L(x_{jk})\right]</annotation></semantics>'
- en: '**Key Insight:** For memory-bound models like GPT-2, gradient accumulation
    + moderate GPU count is more cost-effective than scaling to many GPUs with small
    batches.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®è§è§£ï¼š** å¯¹äºåƒGPT-2è¿™æ ·çš„å†…å­˜å—é™æ¨¡å‹ï¼Œæ¢¯åº¦ç´¯ç§¯åŠ ä¸Šé€‚ä¸­çš„GPUæ•°é‡æ¯”æ‰©å±•åˆ°å¤šä¸ªGPUçš„å°æ‰¹é‡æ›´å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚'
- en: Gradient Accumulation and Checkpointing Applications
  id: totrans-719
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹åº”ç”¨
- en: Gradient accumulation and activation checkpointing are particularly valuable
    in scenarios where hardware memory limitations present significant challenges
    during training. These techniques are widely used in training large-scale models,
    working with high-resolution data, and optimizing workflows in resource-constrained
    environments.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¡¬ä»¶å†…å­˜é™åˆ¶å¸¦æ¥é‡å¤§æŒ‘æˆ˜çš„åœºæ™¯ä¸­å°¤å…¶æœ‰ä»·å€¼ã€‚è¿™äº›æŠ€æœ¯åœ¨è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ã€å¤„ç†é«˜åˆ†è¾¨ç‡æ•°æ®ä»¥åŠä¼˜åŒ–èµ„æºå—é™ç¯å¢ƒä¸­çš„å·¥ä½œæµç¨‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚
- en: A common use case for gradient accumulation is in training models that require
    large batch sizes to achieve stable convergence. For example, models like GPT,
    BERT, and other transformer architectures[30](#fn30) often benefit from larger
    batch sizes due to their improved gradient estimates. However, these batch sizes
    can quickly exceed the memory capacity of GPUs, especially when working with high-dimensional
    inputs or multiple GPUs. By accumulating gradients over multiple smaller micro-batches,
    gradient accumulation enables the use of effective large batch sizes without exceeding
    memory limits. This is particularly beneficial for tasks like language modeling,
    sequence-to-sequence learning, and image classification, where batch size significantly
    impacts training dynamics.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯çš„ä¸€ä¸ªå¸¸è§ç”¨ä¾‹æ˜¯åœ¨è®­ç»ƒéœ€è¦å¤§æ‰¹é‡ä»¥å®ç°ç¨³å®šæ”¶æ•›çš„æ¨¡å‹æ—¶ã€‚ä¾‹å¦‚ï¼ŒåƒGPTã€BERTå’Œå…¶ä»–Transformeræ¶æ„çš„æ¨¡å‹ç”±äºå®ƒä»¬çš„æ¢¯åº¦ä¼°è®¡æ”¹è¿›ï¼Œé€šå¸¸ä»æ›´å¤§çš„æ‰¹é‡ä¸­å—ç›Šã€‚ç„¶è€Œï¼Œè¿™äº›æ‰¹é‡å¤§å°å¯èƒ½ä¼šè¿…é€Ÿè¶…è¿‡GPUçš„å†…å­˜å®¹é‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é«˜ç»´è¾“å…¥æˆ–å¤šä¸ªGPUæ—¶ã€‚é€šè¿‡åœ¨å¤šä¸ªè¾ƒå°çš„å¾®æ‰¹é‡ä¸Šç´¯ç§¯æ¢¯åº¦ï¼Œæ¢¯åº¦ç´¯ç§¯ä½¿å¾—åœ¨ä¸è¶…å‡ºå†…å­˜é™åˆ¶çš„æƒ…å†µä¸‹ä½¿ç”¨æœ‰æ•ˆçš„å¤§æ‰¹é‡æˆä¸ºå¯èƒ½ã€‚è¿™å¯¹äºè¯­è¨€å»ºæ¨¡ã€åºåˆ—åˆ°åºåˆ—å­¦ä¹ å’Œå›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ç‰¹åˆ«æœ‰ç›Šï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæ‰¹é‡å¤§å°å¯¹è®­ç»ƒåŠ¨æ€æœ‰æ˜¾è‘—å½±å“ã€‚
- en: Activation checkpointing enables training of deep neural networks with numerous
    layers or complex computations. In computer vision, architectures like ResNet-152,
    EfficientNet, and DenseNet require substantial memory to store intermediate activations
    during training. Checkpointing reduces this memory requirement through strategic
    recomputation of activations, making it possible to train these deeper architectures
    within GPU memory constraints.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯ä½¿å¾—è®­ç»ƒå…·æœ‰ä¼—å¤šå±‚æˆ–å¤æ‚è®¡ç®—çš„æ·±åº¦ç¥ç»ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå¦‚ResNet-152ã€EfficientNetå’ŒDenseNetç­‰æ¶æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡å†…å­˜æ¥å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼Œæ£€æŸ¥ç‚¹æŠ€æœ¯å‡å°‘äº†å†…å­˜éœ€æ±‚ï¼Œä½¿å¾—åœ¨GPUå†…å­˜é™åˆ¶å†…è®­ç»ƒè¿™äº›æ›´æ·±å±‚çš„æ¶æ„æˆä¸ºå¯èƒ½ã€‚
- en: In the domain of natural language processing, models like GPT-3 or T5, with
    hundreds of layers and billions of parameters, rely heavily on checkpointing to
    manage memory usage. These models often exceed the memory capacity of a single
    GPU, making checkpointing a necessity for efficient training. Similarly, in generative
    adversarial networks (GANs), which involve both generator and discriminator models,
    checkpointing helps manage the combined memory requirements of both networks during
    training.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒåƒGPT-3æˆ–T5è¿™æ ·çš„æ¨¡å‹ï¼Œå…·æœ‰æ•°ç™¾å±‚å’Œæ•°åäº¿å‚æ•°ï¼Œä¸¥é‡ä¾èµ–æ£€æŸ¥ç‚¹æ¥ç®¡ç†å†…å­˜ä½¿ç”¨ã€‚è¿™äº›æ¨¡å‹é€šå¸¸è¶…å‡ºäº†å•ä¸ªGPUçš„å†…å­˜å®¹é‡ï¼Œä½¿å¾—æ£€æŸ¥ç‚¹å¯¹äºé«˜æ•ˆè®­ç»ƒæˆä¸ºå¿…è¦ã€‚åŒæ ·ï¼Œåœ¨æ¶‰åŠç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å‹çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­ï¼Œæ£€æŸ¥ç‚¹æœ‰åŠ©äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç®¡ç†ä¸¤ä¸ªç½‘ç»œçš„è”åˆå†…å­˜éœ€æ±‚ã€‚
- en: Another critical application is in resource-constrained environments, such as
    edge devices or cloud-based platforms. In these scenarios, memory is often a limiting
    factor, and upgrading hardware may not always be a viable option. Gradient accumulation
    and checkpointing provide a cost-effective solution for training models on existing
    hardware, enabling efficient workflows without requiring additional investment
    in resources.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå…³é”®åº”ç”¨æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚è¾¹ç¼˜è®¾å¤‡æˆ–åŸºäºäº‘çš„å¹³å°ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå†…å­˜é€šå¸¸æ˜¯é™åˆ¶å› ç´ ï¼Œè€Œå‡çº§ç¡¬ä»¶å¯èƒ½å¹¶ä¸æ€»æ˜¯å¯è¡Œçš„é€‰æ‹©ã€‚æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹æŠ€æœ¯ä¸ºåœ¨ç°æœ‰ç¡¬ä»¶ä¸Šè®­ç»ƒæ¨¡å‹æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿å¾—èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„æµç¨‹ï¼Œè€Œæ— éœ€é¢å¤–æŠ•èµ„èµ„æºã€‚
- en: These techniques are also indispensable in research and experimentation. They
    allow practitioners to prototype and test larger and more complex models, exploring
    novel architectures that would otherwise be infeasible due to memory constraints.
    This is particularly valuable for academic researchers and startups operating
    within limited budgets.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŠ€æœ¯åœ¨ç ”ç©¶å’Œå®éªŒä¸­ä¹Ÿæ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚å®ƒä»¬å…è®¸ä»ä¸šè€…åŸå‹åŒ–å’Œæµ‹è¯•æ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ï¼Œæ¢ç´¢ç”±äºå†…å­˜é™åˆ¶è€Œæ— æ³•å®ç°çš„åˆ›æ–°æ¶æ„ã€‚è¿™å¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œé¢„ç®—æœ‰é™çš„åˆåˆ›å…¬å¸å°¤å…¶æœ‰ä»·å€¼ã€‚
- en: Gradient accumulation and activation checkpointing solve core challenges in
    training large-scale models within memory-constrained environments. These techniques
    have become essential tools for practitioners in natural language processing,
    computer vision, generative modeling, and edge computing, enabling broader adoption
    of advanced machine learning architectures.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯è§£å†³äº†åœ¨å†…å­˜å—é™ç¯å¢ƒä¸­è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¿™äº›æŠ€æœ¯å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€ç”Ÿæˆå»ºæ¨¡å’Œè¾¹ç¼˜è®¡ç®—ä»ä¸šè€…çš„é‡è¦å·¥å…·ï¼Œä¿ƒè¿›äº†é«˜çº§æœºå™¨å­¦ä¹ æ¶æ„çš„æ›´å¹¿æ³›é‡‡ç”¨ã€‚
- en: Memory-Computation Trade-off Challenges
  id: totrans-727
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜-è®¡ç®—æƒè¡¡æŒ‘æˆ˜
- en: While gradient accumulation and activation checkpointing are useful tools for
    optimizing memory usage during training, their implementation introduces several
    challenges and trade-offs that must be carefully managed to ensure efficient and
    reliable workflows.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯åœ¨ä¼˜åŒ–è®­ç»ƒæœŸé—´çš„å†…å­˜ä½¿ç”¨æ–¹é¢æ˜¯æœ‰ç”¨çš„å·¥å…·ï¼Œä½†å®ƒä»¬çš„å®ç°å¼•å…¥äº†å‡ ä¸ªæŒ‘æˆ˜å’Œæƒè¡¡ï¼Œå¿…é¡»è°¨æ…ç®¡ç†ä»¥ç¡®ä¿é«˜æ•ˆå’Œå¯é çš„æµç¨‹ã€‚
- en: One of the primary trade-offs of activation checkpointing is the additional
    computational overhead it introduces. By design, checkpointing saves memory by
    discarding and recomputing intermediate activations during the backward pass.
    This recomputation increases the training time, as portions of the forward pass
    must be executed multiple times. For example, in a transformer model with 12 layers,
    if checkpoints are placed every 4 layers, each intermediate activation would need
    to be recomputed up to three times during the backward pass. The extent of this
    overhead depends on how the model is segmented for checkpointing and the computational
    cost of each segment. Practitioners must strike a balance between memory savings
    and the additional time spent on recomputation, which may affect overall training
    efficiency.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: æ¿€æ´»æ£€æŸ¥ç‚¹çš„ä¸€ä¸ªä¸»è¦æƒè¡¡æ˜¯å…¶å¼•å…¥çš„é¢å¤–è®¡ç®—å¼€é”€ã€‚æŒ‰è®¾è®¡ï¼Œæ£€æŸ¥ç‚¹é€šè¿‡åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸¢å¼ƒå’Œé‡æ–°è®¡ç®—ä¸­é—´æ¿€æ´»æ¥èŠ‚çœå†…å­˜ã€‚è¿™ç§é‡æ–°è®¡ç®—å¢åŠ äº†è®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºå¿…é¡»å¤šæ¬¡æ‰§è¡Œå‰å‘ä¼ æ’­çš„éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªæœ‰12å±‚çš„transformeræ¨¡å‹ä¸­ï¼Œå¦‚æœæ¯4å±‚æ”¾ç½®ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ¯ä¸ªä¸­é—´æ¿€æ´»åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦é‡æ–°è®¡ç®—å¤šè¾¾ä¸‰æ¬¡ã€‚è¿™ç§å¼€é”€çš„ç¨‹åº¦å–å†³äºæ¨¡å‹å¦‚ä½•åˆ†å‰²ä»¥è¿›è¡Œæ£€æŸ¥ç‚¹ï¼Œä»¥åŠæ¯ä¸ªæ®µè½çš„è®¡ç®—æˆæœ¬ã€‚ä»ä¸šè€…å¿…é¡»åœ¨å†…å­˜èŠ‚çœå’Œé¢å¤–è®¡ç®—æ—¶é—´ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ•´ä½“è®­ç»ƒæ•ˆç‡ã€‚
- en: Gradient accumulation, while effective at simulating larger batch sizes, can
    lead to slower parameter updates. Since gradients are accumulated over multiple
    micro-batches, the model parameters are updated less frequently compared to training
    with full batches. This delay in updates can impact the speed of convergence,
    particularly in models sensitive to batch size dynamics. Gradient accumulation
    requires careful tuning of the learning rate. For instance, if accumulating gradients
    over 4 micro-batches to simulate a batch size of 128, the learning rate typically
    needs to be scaled up by a factor of 4 to maintain the same effective learning
    rate as training with full batches. The effective batch size increases with accumulation,
    necessitating proportional adjustments to the learning rate to maintain stable
    training.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯è™½ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´å‚æ•°æ›´æ–°é€Ÿåº¦å˜æ…¢ã€‚ç”±äºæ¢¯åº¦æ˜¯åœ¨å¤šä¸ªå¾®æ‰¹æ¬¡ä¸Šç´¯ç§¯çš„ï¼Œå› æ­¤ä¸ä½¿ç”¨å®Œæ•´æ‰¹æ¬¡è®­ç»ƒç›¸æ¯”ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°çš„é¢‘ç‡è¾ƒä½ã€‚è¿™ç§æ›´æ–°å»¶è¿Ÿå¯èƒ½ä¼šå½±å“æ”¶æ•›é€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æ‰¹é‡å¤§å°åŠ¨æ€æ•æ„Ÿçš„æ¨¡å‹ä¸­ã€‚æ¢¯åº¦ç´¯ç§¯éœ€è¦ä»”ç»†è°ƒæ•´å­¦ä¹ ç‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç´¯ç§¯4ä¸ªå¾®æ‰¹æ¬¡çš„æ¢¯åº¦æ¥æ¨¡æ‹Ÿ128ä¸ªæ‰¹é‡å¤§å°æ—¶ï¼Œé€šå¸¸éœ€è¦å°†å­¦ä¹ ç‡ä¹˜ä»¥4ï¼Œä»¥ä¿æŒä¸ä½¿ç”¨å®Œæ•´æ‰¹æ¬¡è®­ç»ƒç›¸åŒçš„æœ‰æ•ˆå­¦ä¹ ç‡ã€‚æœ‰æ•ˆæ‰¹é‡å¤§å°éšç€ç´¯ç§¯è€Œå¢åŠ ï¼Œéœ€è¦ç›¸åº”åœ°è°ƒæ•´å­¦ä¹ ç‡ä»¥ä¿æŒç¨³å®šçš„è®­ç»ƒã€‚
- en: Debugging and monitoring are also more complex when using these techniques.
    In activation checkpointing, errors may arise during recomputation, making it
    more difficult to trace issues back to their source. Similarly, gradient accumulation
    requires ensuring that gradients are correctly accumulated and reset after each
    effective batch, which can introduce bugs if not handled properly.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›æŠ€æœ¯æ—¶ï¼Œè°ƒè¯•å’Œç›‘æ§ä¹Ÿå˜å¾—æ›´åŠ å¤æ‚ã€‚åœ¨æ¿€æ´»æ£€æŸ¥ç‚¹ä¸­ï¼Œå¯èƒ½åœ¨é‡æ–°è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¿™ä½¿å¾—å°†é—®é¢˜è¿½æº¯åˆ°å…¶æºå¤´å˜å¾—æ›´åŠ å›°éš¾ã€‚åŒæ ·ï¼Œæ¢¯åº¦ç´¯ç§¯éœ€è¦ç¡®ä¿æ¢¯åº¦åœ¨æ¯ä¸ªæœ‰æ•ˆçš„æ‰¹æ¬¡ä¹‹åè¢«æ­£ç¡®ç´¯ç§¯å’Œé‡ç½®ï¼Œå¦‚æœä¸å¦¥å–„å¤„ç†ï¼Œå¯èƒ½ä¼šå¼•å…¥é”™è¯¯ã€‚
- en: Another challenge is the increased complexity in implementation. While modern
    frameworks like PyTorch provide utilities to simplify gradient accumulation and
    checkpointing, effective use still requires understanding the underlying principles.
    For instance, activation checkpointing demands segmenting the model appropriately
    to minimize recomputation overhead while achieving meaningful memory savings.
    Improper segmentation can lead to suboptimal performance or excessive computational
    cost.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å®ç°è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§å¢åŠ ã€‚è™½ç„¶ç°ä»£æ¡†æ¶å¦‚PyTorchæä¾›äº†ç®€åŒ–æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹çš„å·¥å…·ï¼Œä½†æœ‰æ•ˆä½¿ç”¨è¿™äº›å·¥å…·ä»ç„¶éœ€è¦ç†è§£å…¶èƒŒåçš„åŸç†ã€‚ä¾‹å¦‚ï¼Œæ¿€æ´»æ£€æŸ¥ç‚¹éœ€è¦é€‚å½“åœ°åˆ†å‰²æ¨¡å‹ä»¥æœ€å°åŒ–é‡æ–°è®¡ç®—çš„å¼€é”€ï¼ŒåŒæ—¶å®ç°æœ‰æ„ä¹‰çš„å†…å­˜èŠ‚çœã€‚ä¸æ°å½“çš„åˆ†å‰²å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³æˆ–è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚
- en: These techniques may also have limited benefits in certain scenarios. For example,
    if the computational cost of recomputation in activation checkpointing is too
    high relative to the memory savings, it may negate the advantages of the technique.
    Similarly, for models or datasets that do not require large batch sizes, the complexity
    introduced by gradient accumulation may not justify its use.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æŠ€æœ¯åœ¨æŸäº›åœºæ™¯ä¸­å¯èƒ½ä¹Ÿå…·æœ‰æœ‰é™çš„ç›Šå¤„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ¿€æ´»æ£€æŸ¥ç‚¹ä¸­é‡æ–°è®¡ç®—çš„è®¡ç®—æˆæœ¬ç›¸å¯¹äºå†…å­˜èŠ‚çœè¿‡é«˜ï¼Œå¯èƒ½ä¼šæŠµæ¶ˆè¯¥æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚åŒæ ·ï¼Œå¯¹äºä¸éœ€è¦å¤§æ‰¹é‡çš„æ¨¡å‹æˆ–æ•°æ®é›†ï¼Œæ¢¯åº¦ç´¯ç§¯å¼•å…¥çš„å¤æ‚æ€§å¯èƒ½ä¸è¶³ä»¥è¯æ˜å…¶ä½¿ç”¨ã€‚
- en: Despite these challenges, gradient accumulation and activation checkpointing
    remain indispensable for training large-scale models under memory constraints.
    By carefully managing their trade-offs and tailoring their application to specific
    workloads, practitioners can maximize the efficiency and effectiveness of these
    techniques.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ¢¯åº¦ç´¯ç§¯å’Œæ¿€æ´»æ£€æŸ¥ç‚¹å¯¹äºåœ¨å†…å­˜å—é™æ¡ä»¶ä¸‹è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ä»ç„¶æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚é€šè¿‡ä»”ç»†ç®¡ç†å®ƒä»¬çš„æƒè¡¡å¹¶é’ˆå¯¹ç‰¹å®šå·¥ä½œè´Ÿè½½è¿›è¡Œå®šåˆ¶ï¼Œä»ä¸šè€…å¯ä»¥æœ€å¤§åŒ–è¿™äº›æŠ€æœ¯çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚
- en: Optimization Technique Comparison
  id: totrans-735
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–æŠ€æœ¯æ¯”è¾ƒ
- en: As summarized in [TableÂ 8.6](ch014.xhtml#tbl-optimization), these techniques
    vary in their implementation complexity, hardware requirements, and impact on
    computation speed and memory usage. The selection of an appropriate optimization
    strategy depends on factors such as the specific use case, available hardware
    resources, and the nature of performance bottlenecks in the training process.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[è¡¨8.6](ch014.xhtml#tbl-optimization)æ€»ç»“æ‰€ç¤ºï¼Œè¿™äº›æŠ€æœ¯åœ¨å®ç°å¤æ‚æ€§ã€ç¡¬ä»¶è¦æ±‚ã€å¯¹è®¡ç®—é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨çš„å½±å“æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥å–å†³äºè¯¸å¦‚ç‰¹å®šç”¨ä¾‹ã€å¯ç”¨çš„ç¡¬ä»¶èµ„æºä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­æ€§èƒ½ç“¶é¢ˆçš„æ€§è´¨ç­‰å› ç´ ã€‚
- en: 'TableÂ 8.6: **Optimization Strategies**: Prefetching, mixed-precision training,
    and gradient accumulation address distinct bottlenecks in AI training pipelinesâ€”data
    transfer, memory consumption, and backpropagationâ€”to improve computational efficiency
    and enable larger models. Selecting an appropriate strategy balances implementation
    complexity against gains in speed and resource utilization, depending on hardware
    and workload characteristics.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.6ï¼š**ä¼˜åŒ–ç­–ç•¥**ï¼šé¢„å–ã€æ··åˆç²¾åº¦è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯é’ˆå¯¹AIè®­ç»ƒç®¡é“ä¸­çš„ä¸åŒç“¶é¢ˆâ€”â€”æ•°æ®ä¼ è¾“ã€å†…å­˜æ¶ˆè€—å’Œåå‘ä¼ æ’­â€”â€”ä»¥æé«˜è®¡ç®—æ•ˆç‡å¹¶æ”¯æŒæ›´å¤§æ¨¡å‹ã€‚é€‰æ‹©åˆé€‚çš„ç­–ç•¥éœ€è¦åœ¨å®ç°å¤æ‚æ€§ã€é€Ÿåº¦æå‡å’Œèµ„æºåˆ©ç”¨ç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè¿™å–å†³äºç¡¬ä»¶å’Œå·¥ä½œè´Ÿè½½çš„ç‰¹ç‚¹ã€‚
- en: '| **Aspect** | **Prefetching and Overlapping** | **Mixed-Precision Training**
    | **Gradient Accumulation and Checkpointing** |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| **æ–¹é¢** | **é¢„å–å’Œé‡å ** | **æ··åˆç²¾åº¦è®­ç»ƒ** | **æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Primary Goal** | Minimize data transfer delays and maximize GPU utilization
    | Reduce memory consumption and computational overhead | Overcome memory limitations
    during backpropagation and parameter updates |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸»è¦ç›®æ ‡** | æœ€å°åŒ–æ•°æ®ä¼ è¾“å»¶è¿Ÿå¹¶æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡ | å‡å°‘å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¼€é”€ | å…‹æœåå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°æœŸé—´çš„å†…å­˜é™åˆ¶ |'
- en: '| **Key Mechanism** | Asynchronous data loading and parallel processing | Combining
    FP16 and FP32 computations | Simulating larger batch sizes and selective activation
    storage |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| **å…³é”®æœºåˆ¶** | å¼‚æ­¥æ•°æ®åŠ è½½å’Œå¹¶è¡Œå¤„ç† | ç»“åˆFP16å’ŒFP32è®¡ç®— | æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡å¤§å°å’Œé€‰æ‹©æ€§æ¿€æ´»å­˜å‚¨ |'
- en: '| **Memory Impact** | Increases memory usage for prefetch buffer | Reduces
    memory usage by using FP16 | Reduces memory usage for activations and gradients
    |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜å½±å“** | å¢åŠ é¢„å–ç¼“å†²åŒºçš„å†…å­˜ä½¿ç”¨ | é€šè¿‡ä½¿ç”¨FP16å‡å°‘å†…å­˜ä½¿ç”¨ | å‡å°‘æ¿€æ´»å’Œæ¢¯åº¦çš„å†…å­˜ä½¿ç”¨ |'
- en: '| **Computation Speed** | Improves by reducing idle time | Accelerates computations
    using FP16 | May slow down due to recomputations in checkpointing |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| **è®¡ç®—é€Ÿåº¦** | é€šè¿‡å‡å°‘ç©ºé—²æ—¶é—´æ¥æé«˜ | ä½¿ç”¨FP16åŠ é€Ÿè®¡ç®— | ç”±äºæ£€æŸ¥ç‚¹ä¸­çš„é‡æ–°è®¡ç®—å¯èƒ½ä¼šå‡æ…¢ |'
- en: '| **Scalability** | Highly scalable, especially for large datasets | Enables
    training of larger models | Allows training deeper models on limited hardware
    |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| **å¯æ‰©å±•æ€§** | é«˜åº¦å¯æ‰©å±•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹æ•°æ®é›† | æ”¯æŒè®­ç»ƒæ›´å¤§æ¨¡å‹ | å…è®¸åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´æ·±çš„æ¨¡å‹ |'
- en: '| **Hardware Requirements** | Benefits from fast storage and multi-core CPUs
    | Requires GPUs with FP16 support (e.g., Tensor Cores) | Works on standard hardware
    |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| **ç¡¬ä»¶è¦æ±‚** | å—ç›Šäºå¿«é€Ÿå­˜å‚¨å’Œå¤šæ ¸CPU | éœ€è¦æ”¯æŒFP16çš„GPUï¼ˆä¾‹å¦‚ï¼ŒTensoræ ¸å¿ƒï¼‰ | åœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šè¿è¡Œ |'
- en: '| **Implementation Complexity** | Moderate (requires tuning of prefetch parameters)
    | Low to moderate (with framework support) | Moderate (requires careful segmentation
    and accumulation) |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| **å®ç°å¤æ‚æ€§** | ä¸­ç­‰ï¼ˆéœ€è¦è°ƒæ•´é¢„å–å‚æ•°ï¼‰ | ä½åˆ°ä¸­ç­‰ï¼ˆæœ‰æ¡†æ¶æ”¯æŒï¼‰ | ä¸­ç­‰ï¼ˆéœ€è¦ä»”ç»†åˆ†å‰²å’Œç´¯ç§¯ï¼‰ |'
- en: '| **Main Benefits** | Reduces training time, improves hardware utilization
    | Faster training, larger models, reduced memory usage | Enables larger batch
    sizes and deeper models |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸»è¦ä¼˜åŠ¿** | å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œæé«˜ç¡¬ä»¶åˆ©ç”¨ç‡ | åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæ”¯æŒæ›´å¤§æ¨¡å‹ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ | æ”¯æŒæ›´å¤§çš„æ‰¹é‡å¤§å°å’Œæ›´æ·±çš„æ¨¡å‹ |'
- en: '| **Primary Challenges** | Tuning buffer sizes, increased memory usage | Potential
    numerical instability, loss scaling needed | Increased computational overhead,
    slower parameter updates |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| **ä¸»è¦æŒ‘æˆ˜** | è°ƒæ•´ç¼“å†²åŒºå¤§å°ï¼Œå¢åŠ å†…å­˜ä½¿ç”¨ | å¯èƒ½å‡ºç°æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œéœ€è¦æŸå¤±ç¼©æ”¾ | å¢åŠ è®¡ç®—å¼€é”€ï¼Œå‚æ•°æ›´æ–°é€Ÿåº¦å‡æ…¢ |'
- en: '| **Ideal Use Cases** | Large datasets, complex preprocessing | Large-scale
    models, especially in NLP and computer vision | Very deep networks, memory-constrained
    environments |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| **ç†æƒ³ç”¨ä¾‹** | å¤§æ•°æ®é›†ï¼Œå¤æ‚çš„é¢„å¤„ç† | å¤§è§„æ¨¡æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨NLPå’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸ | éå¸¸æ·±çš„ç½‘ç»œï¼Œå†…å­˜å—é™çš„ç¯å¢ƒ |'
- en: 'While these three techniques represent core optimization strategies in machine
    learning, they are part of broader optimization approaches that extend beyond
    single-machine boundaries. At some point, even perfectly optimized single-machine
    training reaches limits: memory capacity constraints prevent larger models, computational
    throughput bounds limit training speed, and dataset sizes exceed single-machine
    storage capabilities.'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸‰ç§æŠ€æœ¯ä»£è¡¨äº†æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ï¼Œä½†å®ƒä»¬æ˜¯æ›´å¹¿æ³›ä¼˜åŒ–æ–¹æ³•çš„ä¸€éƒ¨åˆ†ï¼Œè¿™äº›æ–¹æ³•è¶…è¶Šäº†å•æœºè¾¹ç•Œã€‚åœ¨æŸä¸ªç‚¹ä¸Šï¼Œå³ä½¿æ˜¯å®Œç¾ä¼˜åŒ–çš„å•æœºè®­ç»ƒä¹Ÿä¼šè¾¾åˆ°æé™ï¼šå†…å­˜å®¹é‡é™åˆ¶é˜»ç¢äº†æ›´å¤§æ¨¡å‹çš„ä½¿ç”¨ï¼Œè®¡ç®—ååé‡é™åˆ¶è®­ç»ƒé€Ÿåº¦ï¼Œæ•°æ®é›†å¤§å°è¶…è¿‡äº†å•æœºçš„å­˜å‚¨èƒ½åŠ›ã€‚
- en: The systematic profiling methodology established for single-machine optimization
    extends to determining when distributed approaches become necessary. When profiling
    reveals that bottlenecks cannot be resolved through single-machine techniques,
    scaling to multiple machines becomes the logical next step.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºå•æœºä¼˜åŒ–å»ºç«‹çš„ç³»ç»Ÿçº§åˆ†ææ–¹æ³•è®ºä¹Ÿé€‚ç”¨äºç¡®å®šä½•æ—¶åˆ†å¸ƒå¼æ–¹æ³•å˜å¾—å¿…è¦ã€‚å½“åˆ†æè¡¨æ˜ç“¶é¢ˆæ— æ³•é€šè¿‡å•æœºæŠ€æœ¯è§£å†³æ—¶ï¼Œæ‰©å±•åˆ°å¤šå°æœºå™¨å°±æˆä¸ºé€»è¾‘ä¸Šçš„ä¸‹ä¸€æ­¥ã€‚
- en: Multi-Machine Scaling Fundamentals
  id: totrans-752
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤šæœºç¼©æ”¾åŸºç¡€
- en: The transition from single-machine to distributed training represents a shift
    in optimization strategy and system complexity. While single-machine optimization
    focuses on efficiently utilizing available resources through techniques we have
    exploredâ€”prefetching, mixed precision, gradient accumulationâ€”distributed training
    introduces qualitatively different challenges that require new conceptual frameworks
    and engineering approaches.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å•æœºåˆ°åˆ†å¸ƒå¼è®­ç»ƒçš„è½¬å˜ä»£è¡¨äº†ä¼˜åŒ–ç­–ç•¥å’Œç³»ç»Ÿå¤æ‚æ€§çš„è½¬å˜ã€‚è™½ç„¶å•æœºä¼˜åŒ–ä¾§é‡äºé€šè¿‡æˆ‘ä»¬å·²ç»æ¢ç´¢çš„æŠ€æœ¯â€”â€”é¢„å–ã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯â€”â€”æ¥é«˜æ•ˆåˆ©ç”¨å¯ç”¨èµ„æºï¼Œä½†åˆ†å¸ƒå¼è®­ç»ƒå¼•å…¥äº†è´¨çš„ä¸åŒæŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜éœ€è¦æ–°çš„æ¦‚å¿µæ¡†æ¶å’Œå·¥ç¨‹æ–¹æ³•ã€‚
- en: Multi-Machine Training Requirements
  id: totrans-754
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤šæœºè®­ç»ƒéœ€æ±‚
- en: Three concrete signals indicate that distributed training has become necessary
    rather than merely beneficial. First, memory exhaustion occurs when model parameters,
    optimizer states, and activation storage exceed single-device capacity even after
    applying gradient checkpointing and mixed precision. For transformer models, this
    threshold typically occurs around 10-20 billion parameters on current generation
    GPUs with 40-80GB memory ([Rajbhandari et al. 2020b](ch058.xhtml#ref-rajbhandari2020zero)).
    Second, unacceptable training duration emerges when single-device training would
    require weeks or months to converge, making iteration impossible. Training GPT-3
    on a single V100 GPU would require approximately 355 years ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)),
    making distributed approaches not optional but essential. Third, dataset scale
    exceeds single-machine storage when training data reaches multiple terabytes,
    as occurs in large-scale vision or language modeling tasks.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªå…·ä½“ä¿¡å·è¡¨æ˜åˆ†å¸ƒå¼è®­ç»ƒå˜å¾—å¿…è¦è€Œéä»…ä»…æœ‰ç›Šã€‚é¦–å…ˆï¼Œå½“æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¿€æ´»å­˜å‚¨è¶…è¿‡å•è®¾å¤‡å®¹é‡ï¼Œå³ä½¿åº”ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦åï¼Œä¹Ÿä¼šå‡ºç°å†…å­˜è€—å°½ã€‚å¯¹äºTransformeræ¨¡å‹ï¼Œè¿™ä¸ªé˜ˆå€¼é€šå¸¸å‡ºç°åœ¨å½“å‰ä¸€ä»£å…·æœ‰40-80GBå†…å­˜çš„GPUä¸Šï¼Œå¤§çº¦æœ‰10-20äº¿ä¸ªå‚æ•°ï¼ˆ[Rajbhandariç­‰äºº2020b](ch058.xhtml#ref-rajbhandari2020zero)ï¼‰ã€‚å…¶æ¬¡ï¼Œå½“å•è®¾å¤‡è®­ç»ƒéœ€è¦æ•°å‘¨æˆ–æ•°æœˆæ‰èƒ½æ”¶æ•›æ—¶ï¼Œä¼šå‡ºç°ä¸å¯æ¥å—çš„è®­ç»ƒæ—¶é•¿ï¼Œä½¿å¾—è¿­ä»£å˜å¾—ä¸å¯èƒ½ã€‚åœ¨å•ä¸ªV100
    GPUä¸Šè®­ç»ƒGPT-3éœ€è¦å¤§çº¦355å¹´ï¼ˆ[T. Brownç­‰äºº2020](ch058.xhtml#ref-brown2020language)ï¼‰ï¼Œè¿™ä½¿å¾—åˆ†å¸ƒå¼æ–¹æ³•ä¸å†æ˜¯å¯é€‰çš„ï¼Œè€Œæ˜¯å¿…éœ€çš„ã€‚ç¬¬ä¸‰ï¼Œå½“è®­ç»ƒæ•°æ®è¾¾åˆ°æ•°ä¸ªTBæ—¶ï¼Œæ•°æ®é›†è§„æ¨¡è¶…è¿‡äº†å•æœºçš„å­˜å‚¨èƒ½åŠ›ï¼Œè¿™åœ¨å¤§è§„æ¨¡è§†è§‰æˆ–è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­å¾ˆå¸¸è§ã€‚
- en: Distributed Training Complexity Trade-offs
  id: totrans-756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒå¤æ‚æ€§æƒè¡¡
- en: 'Distributed training introduces three primary complexity dimensions absent
    from single-machine scenarios. Communication overhead emerges from gradient synchronization,
    where each training step must aggregate gradients across all devices. For a model
    with <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    parameters distributed across <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>
    devices, all-reduce operations must transfer approximately <semantics><mrow><mn>2</mn><mi>N</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>D</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>/</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">2N(D-1)/D</annotation></semantics>
    bytes per step. On commodity network infrastructure (10-100 Gbps), this communication
    can dominate computation time for models under 1 billion parameters ([Sergeev
    and Balso 2018](ch058.xhtml#ref-sergeev2018horovod)). Fault tolerance requirements
    increase exponentially with cluster size: a 100-node cluster with 99.9% per-node
    reliability experiences failures every few hours on average, necessitating checkpoint
    and recovery mechanisms. Algorithmic considerations change because distributed
    training alters optimization dynamicsâ€”large batch sizes from data parallelism
    affect convergence behavior, requiring learning rate scaling and warmup strategies
    that single-machine training does not require ([Goyal et al. 2017](ch058.xhtml#ref-goyal2017accurate)).'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒå¼•å…¥äº†å•æœºåœºæ™¯ä¸­ä¸å­˜åœ¨çš„ä¸‰ä¸ªä¸»è¦å¤æ‚æ€§ç»´åº¦ã€‚é€šä¿¡å¼€é”€æ¥è‡ªæ¢¯åº¦åŒæ­¥ï¼Œæ¯ä¸ªè®­ç»ƒæ­¥éª¤å¿…é¡»æ±‡æ€»æ‰€æœ‰è®¾å¤‡ä¸Šçš„æ¢¯åº¦ã€‚å¯¹äºä¸€ä¸ªå‚æ•°åˆ†å¸ƒåœ¨<semantics><mi>D</mi><annotation
    encoding="application/x-tex">D</annotation></semantics>ä¸ªè®¾å¤‡ä¸Šçš„æ¨¡å‹ï¼Œæ‰€æœ‰-reduceæ“ä½œå¿…é¡»æ¯æ­¥ä¼ è¾“å¤§çº¦<semantics><mrow><mn>2</mn><mi>N</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>D</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>/</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">2N(D-1)/D</annotation></semantics>å­—èŠ‚ã€‚åœ¨å•†å“ç½‘ç»œåŸºç¡€è®¾æ–½ï¼ˆ10-100
    Gbpsï¼‰ä¸Šï¼Œè¿™ç§é€šä¿¡å¯ä»¥ä¸»å¯¼å°äº10äº¿å‚æ•°çš„æ¨¡å‹è®¡ç®—æ—¶é—´ï¼ˆ[Sergeevå’ŒBalso 2018](ch058.xhtml#ref-sergeev2018horovod)ï¼‰ã€‚éšç€é›†ç¾¤è§„æ¨¡çš„å¢åŠ ï¼Œå®¹é”™è¦æ±‚å‘ˆæŒ‡æ•°å¢é•¿ï¼šä¸€ä¸ªæ‹¥æœ‰100ä¸ªèŠ‚ç‚¹ä¸”æ¯ä¸ªèŠ‚ç‚¹å¯é æ€§ä¸º99.9%çš„é›†ç¾¤å¹³å‡æ¯å‡ ä¸ªå°æ—¶å°±ä¼šå‘ç”Ÿæ•…éšœï¼Œè¿™éœ€è¦æ£€æŸ¥ç‚¹å’Œæ¢å¤æœºåˆ¶ã€‚ç®—æ³•è€ƒè™‘å› ç´ å‘ç”Ÿå˜åŒ–ï¼Œå› ä¸ºåˆ†å¸ƒå¼è®­ç»ƒæ”¹å˜äº†ä¼˜åŒ–åŠ¨æ€â€”â€”æ¥è‡ªæ•°æ®å¹¶è¡Œçš„è¾ƒå¤§æ‰¹é‡å¤§å°ä¼šå½±å“æ”¶æ•›è¡Œä¸ºï¼Œéœ€è¦å­¦ä¹ ç‡ç¼©æ”¾å’Œé¢„çƒ­ç­–ç•¥ï¼Œè€Œå•æœºè®­ç»ƒåˆ™ä¸éœ€è¦è¿™äº›ç­–ç•¥ï¼ˆ[Goyalç­‰äºº
    2017](ch058.xhtml#ref-goyal2017accurate)ï¼‰ã€‚
- en: Single-Machine to Distributed Transition
  id: totrans-758
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä»å•æœºåˆ°åˆ†å¸ƒå¼è¿‡æ¸¡
- en: The systematic optimization methodology established for single-machine training
    extends to distributed environments with important adaptations. Profiling must
    now capture inter-device communication patterns and synchronization overhead in
    addition to computation and memory metrics. Tools like NVIDIA Nsight Systems and
    PyTorchâ€™s distributed profiler reveal whether training is communication-bound
    or computation-bound, guiding the choice between parallelization strategies. The
    solution space expands from single-machine techniques to include data parallelism
    (distributing training examples), model parallelism (distributing model parameters),
    pipeline parallelism (distributing model layers), and hybrid approaches that combine
    multiple strategies. The principles remain consistentâ€”identify bottlenecks, select
    appropriate techniques, compose solutionsâ€”but the implementation complexity increases
    substantially.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºå•æœºè®­ç»ƒå»ºç«‹çš„ç³»ç»Ÿä¼˜åŒ–æ–¹æ³•ç»è¿‡é‡è¦è°ƒæ•´åæ‰©å±•åˆ°åˆ†å¸ƒå¼ç¯å¢ƒã€‚ç°åœ¨ï¼Œæ€§èƒ½åˆ†æå¿…é¡»æ•æ‰è®¾å¤‡é—´çš„é€šä¿¡æ¨¡å¼å’ŒåŒæ­¥å¼€é”€ï¼Œé™¤äº†è®¡ç®—å’Œå†…å­˜æŒ‡æ ‡ã€‚åƒNVIDIA Nsight
    Systemså’ŒPyTorchçš„åˆ†å¸ƒå¼åˆ†æå·¥å…·å¯ä»¥æ­ç¤ºè®­ç»ƒæ˜¯å—é€šä¿¡é™åˆ¶è¿˜æ˜¯å—è®¡ç®—é™åˆ¶ï¼Œä»è€ŒæŒ‡å¯¼é€‰æ‹©å¹¶è¡ŒåŒ–ç­–ç•¥ã€‚è§£å†³æ–¹æ¡ˆç©ºé—´ä»å•æœºæŠ€æœ¯æ‰©å±•åˆ°åŒ…æ‹¬æ•°æ®å¹¶è¡Œï¼ˆåˆ†å‘è®­ç»ƒç¤ºä¾‹ï¼‰ã€æ¨¡å‹å¹¶è¡Œï¼ˆåˆ†å‘æ¨¡å‹å‚æ•°ï¼‰ã€æµæ°´çº¿å¹¶è¡Œï¼ˆåˆ†å‘æ¨¡å‹å±‚ï¼‰ä»¥åŠç»“åˆå¤šç§ç­–ç•¥çš„æ··åˆæ–¹æ³•ã€‚åŸåˆ™ä¿æŒä¸€è‡´â€”â€”è¯†åˆ«ç“¶é¢ˆã€é€‰æ‹©é€‚å½“çš„æŠ€æœ¯ã€ç»„åˆè§£å†³æ–¹æ¡ˆï¼Œä½†å®ç°å¤æ‚æ€§æ˜¾è‘—å¢åŠ ã€‚
- en: Distributed Systems
  id: totrans-760
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼ç³»ç»Ÿ
- en: Building upon our single-machine optimization foundation, distributed training
    extends our systematic optimization framework to multiple machines. When single-machine
    techniques have been exhaustedâ€”prefetching eliminates data loading bottlenecks,
    mixed-precision maximizes memory efficiency, and gradient accumulation reaches
    practical limitsâ€”distributed approaches provide the next level of scaling capability.
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹åœ¨å•æœºä¼˜åŒ–åŸºç¡€ä¹‹ä¸Šï¼Œåˆ†å¸ƒå¼è®­ç»ƒå°†æˆ‘ä»¬çš„ç³»ç»Ÿä¼˜åŒ–æ¡†æ¶æ‰©å±•åˆ°å¤šå°æœºå™¨ã€‚å½“å•æœºæŠ€æœ¯å·²ç»ç”¨å°½â€”â€”é¢„å–æ¶ˆé™¤äº†æ•°æ®åŠ è½½ç“¶é¢ˆï¼Œæ··åˆç²¾åº¦æœ€å¤§åŒ–äº†å†…å­˜æ•ˆç‡ï¼Œæ¢¯åº¦ç´¯ç§¯è¾¾åˆ°å®ç”¨æé™æ—¶â€”â€”åˆ†å¸ƒå¼æ–¹æ³•æä¾›äº†ä¸‹ä¸€çº§åˆ«çš„æ‰©å±•èƒ½åŠ›ã€‚
- en: '***Distributed Training*** is the parallelization of model training across
    *multiple compute devices* through coordinated *data partitioning* and *gradient
    synchronization*, enabling training of models that exceed single-device memory
    or time constraints.'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '***åˆ†å¸ƒå¼è®­ç»ƒ***æ˜¯é€šè¿‡åè°ƒçš„æ•°æ®åˆ†åŒºå’Œæ¢¯åº¦åŒæ­¥ï¼Œåœ¨å¤šä¸ªè®¡ç®—è®¾å¤‡ä¸Šå¹¶è¡ŒåŒ–æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ï¼Œä½¿å¾—èƒ½å¤Ÿè®­ç»ƒè¶…å‡ºå•æœºå†…å­˜æˆ–æ—¶é—´é™åˆ¶çš„æ¨¡å‹ã€‚'
- en: 'The progression from single-machine to distributed training follows a natural
    scaling path: optimize locally first, then scale horizontally. This progression
    ensures that distributed systems operate efficiently at each node while adding
    the coordination mechanisms necessary for multi-machine training. Training machine
    learning models often requires scaling beyond a single machine due to increasing
    model complexity and dataset sizes. The demand for computational power, memory,
    and storage can exceed the capacity of individual devices, especially in domains
    like natural language processing, computer vision, and scientific computing. Distributed
    training[31](#fn31) addresses this challenge by spreading the workload across
    multiple machines, which coordinate to train a single model efficiently.'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å•æœºè®­ç»ƒåˆ°åˆ†å¸ƒå¼è®­ç»ƒçš„æ¼”è¿›éµå¾ªä¸€ä¸ªè‡ªç„¶çš„æ‰©å±•è·¯å¾„ï¼šé¦–å…ˆæœ¬åœ°ä¼˜åŒ–ï¼Œç„¶åæ¨ªå‘æ‰©å±•ã€‚è¿™ç§æ¼”è¿›ç¡®ä¿äº†åˆ†å¸ƒå¼ç³»ç»Ÿåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šé«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶å¢åŠ äº†å¤šæœºè®­ç»ƒæ‰€éœ€çš„åè°ƒæœºåˆ¶ã€‚ç”±äºæ¨¡å‹å¤æ‚æ€§å’Œæ•°æ®é›†å¤§å°çš„å¢åŠ ï¼Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸éœ€è¦è¶…å‡ºå•æœºçš„æ‰©å±•ã€‚è®¡ç®—èƒ½åŠ›ã€å†…å­˜å’Œå­˜å‚¨çš„éœ€æ±‚å¯èƒ½ä¼šè¶…è¿‡å•ä¸ªè®¾å¤‡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œç§‘å­¦è®¡ç®—ç­‰é¢†åŸŸã€‚åˆ†å¸ƒå¼è®­ç»ƒ[31](#fn31)é€šè¿‡å°†å·¥ä½œè´Ÿè½½åˆ†æ•£åˆ°å¤šå°æœºå™¨ä¸Šæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™äº›æœºå™¨åè°ƒèµ·æ¥ä»¥é«˜æ•ˆåœ°è®­ç»ƒå•ä¸ªæ¨¡å‹ã€‚
- en: This coordination relies on consensus protocols and synchronization primitives
    to ensure parameter updates remain consistent across nodes. While basic barrier
    synchronization suffices for research, production deployments require careful
    fault tolerance, checkpointing, and recovery mechanisms covered in later chapters
    on operational practices.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åè°ƒä¾èµ–äºå…±è¯†åè®®å’ŒåŒæ­¥åŸè¯­ï¼Œä»¥ç¡®ä¿å‚æ•°æ›´æ–°åœ¨å„ä¸ªèŠ‚ç‚¹ä¸Šä¿æŒä¸€è‡´ã€‚è™½ç„¶åŸºæœ¬çš„å±éšœåŒæ­¥å¯¹äºç ”ç©¶æ¥è¯´è¶³å¤Ÿäº†ï¼Œä½†ç”Ÿäº§éƒ¨ç½²éœ€è¦ä»”ç»†çš„å®¹é”™æ€§ã€æ£€æŸ¥ç‚¹å’Œæ¢å¤æœºåˆ¶ï¼Œè¿™äº›å°†åœ¨åç»­ç« èŠ‚å…³äºæ“ä½œå®è·µçš„ç« èŠ‚ä¸­ä»‹ç»ã€‚
- en: The path from single-device to distributed training involves distinct complexity
    stages, each building upon the previous levelâ€™s challenges. Single GPU training
    requires only local memory management and straightforward forward/backward passes,
    establishing the baseline computational patterns. Scaling to multiple GPUs within
    a single node introduces high-bandwidth communication requirements, typically
    handled through NVLink[32](#fn32) or PCIe connections with NCCL[33](#fn33) optimization,
    while preserving the single-machine simplicity of fault tolerance and scheduling.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å•è®¾å¤‡åˆ°åˆ†å¸ƒå¼è®­ç»ƒçš„è·¯å¾„æ¶‰åŠä¸åŒçš„å¤æ‚é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½å»ºç«‹åœ¨ä¹‹å‰çº§åˆ«çš„æŒ‘æˆ˜ä¹‹ä¸Šã€‚å•GPUè®­ç»ƒåªéœ€è¦æœ¬åœ°å†…å­˜ç®¡ç†å’Œç›´æ¥çš„æ­£å‘/åå‘ä¼ é€’ï¼Œä»è€Œç¡®ç«‹äº†åŸºæœ¬çš„è®¡ç®—æ¨¡å¼ã€‚æ‰©å±•åˆ°å•ä¸ªèŠ‚ç‚¹å†…çš„å¤šä¸ªGPUå¼•å…¥äº†é«˜å¸¦å®½é€šä¿¡éœ€æ±‚ï¼Œé€šå¸¸é€šè¿‡NVLink[32](#fn32)æˆ–å¸¦æœ‰NCCL[33](#fn33)ä¼˜åŒ–çš„PCIeè¿æ¥æ¥å¤„ç†ï¼ŒåŒæ—¶ä¿æŒäº†å•æœºçš„å®¹é”™æ€§å’Œè°ƒåº¦ç®€å•æ€§ã€‚
- en: '**Practical Distributed Training Complexity**'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®é™…åˆ†å¸ƒå¼è®­ç»ƒçš„å¤æ‚æ€§**'
- en: While frameworks like PyTorch (FSDP) and DeepSpeed abstract away much of the
    complexity, implementing distributed training efficiently remains a significant
    engineering challenge. Production deployments require careful network configuration
    (e.g., InfiniBand), infrastructure management (e.g., Kubernetes, Slurm), and debugging
    of complex, non-local issues like synchronization hangs or communication bottlenecks.
    For most teams, leveraging managed cloud services is often more practical than
    building and maintaining this infrastructure from scratch.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åƒPyTorch (FSDP)å’ŒDeepSpeedè¿™æ ·çš„æ¡†æ¶æŠ½è±¡æ‰äº†å¤§éƒ¨åˆ†å¤æ‚æ€§ï¼Œä½†é«˜æ•ˆå®ç°åˆ†å¸ƒå¼è®­ç»ƒä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚ç”Ÿäº§éƒ¨ç½²éœ€è¦ä»”ç»†çš„ç½‘ç»œé…ç½®ï¼ˆä¾‹å¦‚ï¼ŒInfiniBandï¼‰ã€åŸºç¡€è®¾æ–½ç®¡ç†ï¼ˆä¾‹å¦‚ï¼ŒKubernetesã€Slurmï¼‰ä»¥åŠè°ƒè¯•å¤æ‚çš„éæœ¬åœ°é—®é¢˜ï¼Œå¦‚åŒæ­¥æŒ‚èµ·æˆ–é€šä¿¡ç“¶é¢ˆã€‚å¯¹äºå¤§å¤šæ•°å›¢é˜Ÿæ¥è¯´ï¼Œåˆ©ç”¨æ‰˜ç®¡äº‘æœåŠ¡é€šå¸¸æ¯”ä»å¤´å¼€å§‹æ„å»ºå’Œç»´æŠ¤è¿™ç§åŸºç¡€è®¾æ–½æ›´ä¸ºå®é™…ã€‚
- en: The distributed training process itself involves splitting the dataset into
    non-overlapping subsets, assigning each subset to a different GPU, and performing
    forward and backward passes independently on each device. Once gradients are computed
    on each GPU, they are synchronized and aggregated before updating the model parameters,
    ensuring that all devices learn in a consistent manner. [FigureÂ 8.12](ch014.xhtml#fig-distributed-training)
    illustrates this process, showing how input data is divided, assigned to multiple
    GPUs for computation, and later synchronized to update the model collectively.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹æœ¬èº«æ¶‰åŠå°†æ•°æ®é›†åˆ†å‰²æˆéé‡å çš„å­é›†ï¼Œå°†æ¯ä¸ªå­é›†åˆ†é…ç»™ä¸åŒçš„GPUï¼Œå¹¶åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šç‹¬ç«‹æ‰§è¡Œæ­£å‘å’Œåå‘ä¼ é€’ã€‚ä¸€æ—¦åœ¨æ¯ä¸ªGPUä¸Šè®¡ç®—äº†æ¢¯åº¦ï¼Œå®ƒä»¬å°±ä¼šåœ¨æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹å‰è¿›è¡ŒåŒæ­¥å’Œèšåˆï¼Œç¡®ä¿æ‰€æœ‰è®¾å¤‡ä»¥ä¸€è‡´çš„æ–¹å¼å­¦ä¹ ã€‚[å›¾8.12](ch014.xhtml#fig-distributed-training)å±•ç¤ºäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œæ˜¾ç¤ºäº†å¦‚ä½•åˆ†å‰²è¾“å…¥æ•°æ®ï¼Œå°†å…¶åˆ†é…ç»™å¤šä¸ªGPUè¿›è¡Œè®¡ç®—ï¼Œç„¶ååŒæ­¥ä»¥é›†ä½“æ›´æ–°æ¨¡å‹ã€‚
- en: '![](../media/file119.svg)'
  id: totrans-769
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file119.svg)'
- en: 'FigureÂ 8.12: **Data-Parallel Training**: Distributed machine learning scales
    model training by partitioning datasets across multiple gpus, enabling concurrent
    computation of gradients, and then aggregating these gradients to update shared
    model parameters. This approach accelerates training by leveraging parallel processing
    while maintaining a consistent model across all devices.'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.12ï¼š**æ•°æ®å¹¶è¡Œè®­ç»ƒ**ï¼šåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ é€šè¿‡å°†æ•°æ®é›†åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šï¼Œå®ç°æ¢¯åº¦çš„å¹¶å‘è®¡ç®—ï¼Œç„¶åå°†è¿™äº›æ¢¯åº¦èšåˆä»¥æ›´æ–°å…±äº«æ¨¡å‹å‚æ•°ï¼Œä»è€Œé€šè¿‡åˆ©ç”¨å¹¶è¡Œå¤„ç†åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒæ‰€æœ‰è®¾å¤‡ä¸Šæ¨¡å‹çš„ä¸€è‡´æ€§ã€‚
- en: 'This coordination introduces several key challenges that distributed training
    systems must address. A distributed training system must orchestrate multi-machine
    computation by splitting up the work, managing communication between machines,
    and maintaining synchronization throughout the training process. Understanding
    these basic requirements provides the foundation for examining the main approaches
    to distributed training: data parallelism, which divides the training data across
    machines; model parallelism, which splits the model itself; pipeline parallelism,
    which combines aspects of both; and hybrid approaches that integrate multiple
    strategies.'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åè°ƒå¼•å…¥äº†åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå¿…é¡»è§£å†³çš„å‡ ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå¿…é¡»é€šè¿‡åˆ†å‰²å·¥ä½œã€ç®¡ç†æœºå™¨é—´çš„é€šä¿¡ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒåŒæ­¥æ¥ç¼–æ’å¤šæœºè®¡ç®—ã€‚äº†è§£è¿™äº›åŸºæœ¬éœ€æ±‚ä¸ºè€ƒå¯Ÿåˆ†å¸ƒå¼è®­ç»ƒçš„ä¸»è¦æ–¹æ³•æä¾›äº†åŸºç¡€ï¼šæ•°æ®å¹¶è¡Œï¼Œå®ƒå°†è®­ç»ƒæ•°æ®åˆ†å‰²åˆ°æœºå™¨ä¸Šï¼›æ¨¡å‹å¹¶è¡Œï¼Œå®ƒå°†æ¨¡å‹æœ¬èº«åˆ†å‰²ï¼›æµæ°´çº¿å¹¶è¡Œï¼Œå®ƒç»“åˆäº†ä¸¤è€…çš„ä¸€äº›æ–¹é¢ï¼›ä»¥åŠé›†æˆå¤šç§ç­–ç•¥çš„æ··åˆæ–¹æ³•ã€‚
- en: Distributed Training Efficiency Metrics
  id: totrans-772
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡æŒ‡æ ‡
- en: Before examining specific parallelism strategies, understanding the quantitative
    metrics that govern distributed training efficiency is essential. These metrics
    provide the foundation for making informed decisions about scaling strategies,
    hardware selection, and optimization approaches.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒå¯Ÿå…·ä½“çš„å¹¶è¡Œç­–ç•¥ä¹‹å‰ï¼Œäº†è§£æ§åˆ¶åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡çš„å®šé‡æŒ‡æ ‡æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›æŒ‡æ ‡ä¸ºåšå‡ºå…³äºæ‰©å±•ç­–ç•¥ã€ç¡¬ä»¶é€‰æ‹©å’Œä¼˜åŒ–æ–¹æ³•çš„æ˜æ™ºå†³ç­–æä¾›äº†åŸºç¡€ã€‚
- en: Communication overhead represents the primary bottleneck in distributed training
    systems. AllReduce operations consume 10-40% of total training time in data parallel
    systems, with this overhead increasing significantly at scale. For BERT-Large
    on 128 GPUs, communication overhead reaches 35% of total runtime, while GPT-3
    scale models experience 55% overhead on 1,024 GPUs. The communication time scales
    as O(n) for ring-AllReduce and O(log n) for tree-based reduction, making interconnect
    selection critical for large-scale deployments.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: é€šä¿¡å¼€é”€æ˜¯åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚åœ¨æ•°æ®å¹¶è¡Œç³»ç»Ÿä¸­ï¼ŒAllReduceæ“ä½œæ¶ˆè€—äº†10-40%çš„æ€»è®­ç»ƒæ—¶é—´ï¼Œéšç€è§„æ¨¡çš„å¢åŠ ï¼Œè¿™ç§å¼€é”€æ˜¾è‘—å¢åŠ ã€‚å¯¹äºåœ¨128ä¸ªGPUä¸Šçš„BERT-Largeï¼Œé€šä¿¡å¼€é”€è¾¾åˆ°æ€»è¿è¡Œæ—¶é—´çš„35%ï¼Œè€ŒGPT-3è§„æ¨¡æ¨¡å‹åœ¨1,024ä¸ªGPUä¸Šç»å†äº†55%çš„é€šä¿¡å¼€é”€ã€‚é€šä¿¡æ—¶é—´éšç€O(n)çš„ç¯-AllReduceå’ŒO(log
    n)çš„åŸºäºæ ‘çš„å‡å°‘è€Œæ‰©å±•ï¼Œè¿™ä½¿å¾—äº’è¿é€‰æ‹©å¯¹äºå¤§è§„æ¨¡éƒ¨ç½²è‡³å…³é‡è¦ã€‚
- en: The bandwidth requirements for efficient distributed training are substantial,
    particularly for transformer models. Efficient systems require 100-400 GB/s aggregate
    bandwidth per node for transformer architectures. BERT-Base needs 8 GB parameter
    synchronization per iteration across 64 GPUs, demanding 200 GB/s sustained bandwidth
    for <50ms synchronization latency. Language models with 175B parameters require
    700 GB/s aggregate bandwidth to maintain 80% parallel efficiency, necessitating
    InfiniBand HDR or equivalent interconnects.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒçš„å¸¦å®½éœ€æ±‚å¾ˆå¤§ï¼Œå°¤å…¶æ˜¯å¯¹äºTransformeræ¨¡å‹ã€‚é«˜æ•ˆçš„ç³»ç»Ÿéœ€è¦æ¯ä¸ªèŠ‚ç‚¹100-400 GB/sçš„æ€»å¸¦å®½æ¥æ”¯æŒTransformeræ¶æ„ã€‚BERT-Baseåœ¨æ¯ä¸ªè¿­ä»£ä¸­éœ€è¦8
    GBçš„å‚æ•°åŒæ­¥ï¼Œè·¨è¶Š64ä¸ªGPUï¼Œè¦æ±‚åœ¨<50msçš„åŒæ­¥å»¶è¿Ÿä¸‹ä¿æŒ200 GB/sçš„æŒç»­å¸¦å®½ã€‚å…·æœ‰175Bå‚æ•°çš„è¯­è¨€æ¨¡å‹éœ€è¦700 GB/sçš„æ€»å¸¦å®½æ¥ç»´æŒ80%çš„å¹¶è¡Œæ•ˆç‡ï¼Œè¿™éœ€è¦InfiniBand
    HDRæˆ–ç­‰æ•ˆçš„äº’è¿ã€‚
- en: Synchronization frequency presents a trade-off between communication efficiency
    and convergence behavior. Gradient accumulation reduces synchronization frequency
    but increases memory requirements and may impact convergence. Synchronizing every
    4 steps reduces communication overhead by 60% while increasing memory usage by
    3x for gradient storage. Asynchronous methods eliminate synchronization costs
    entirely but introduce staleness that degrades convergence by 15-30% for large
    learning rates.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ­¥é¢‘ç‡åœ¨é€šä¿¡æ•ˆç‡å’Œæ”¶æ•›è¡Œä¸ºä¹‹é—´æ„æˆäº†ä¸€ä¸ªæƒè¡¡ã€‚æ¢¯åº¦ç´¯ç§¯é™ä½äº†åŒæ­¥é¢‘ç‡ï¼Œä½†å¢åŠ äº†å†…å­˜éœ€æ±‚ï¼Œå¯èƒ½ä¼šå½±å“æ”¶æ•›ã€‚æ¯4æ­¥åŒæ­¥å¯ä»¥å‡å°‘60%çš„é€šä¿¡å¼€é”€ï¼Œä½†å°†æ¢¯åº¦å­˜å‚¨çš„å†…å­˜ä½¿ç”¨é‡å¢åŠ 3å€ã€‚å¼‚æ­¥æ–¹æ³•å®Œå…¨æ¶ˆé™¤äº†åŒæ­¥æˆæœ¬ï¼Œä½†å¼•å…¥äº†å¯èƒ½å¯¼è‡´å¤§å­¦ä¹ ç‡æ”¶æ•›é™ä½15-30%çš„é™ˆæ—§æ€§ã€‚
- en: Scaling efficiency follows predictable patterns across different GPU counts.
    In the linear scaling regime of 2-32 GPUs, systems typically achieve 85-95% parallel
    efficiency as communication overhead remains minimal. The communication bound
    regime emerges at 64-256 GPUs, where efficiency drops to 60-80% even with optimal
    interconnects. Beyond 512 GPUs, coordination overhead becomes dominant, limiting
    efficiency to 40-60% due to collective operation latency.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•æ•ˆç‡åœ¨ä¸åŒæ•°é‡çš„GPUä¹‹é—´éµå¾ªå¯é¢„æµ‹çš„æ¨¡å¼ã€‚åœ¨2-32ä¸ªGPUçš„çº¿æ€§æ‰©å±•èŒƒå›´å†…ï¼Œç³»ç»Ÿé€šå¸¸è¾¾åˆ°85-95%çš„å¹¶è¡Œæ•ˆç‡ï¼Œå› ä¸ºé€šä¿¡å¼€é”€ä»ç„¶å¾ˆå°ã€‚åœ¨64-256ä¸ªGPUçš„é€šä¿¡é™åˆ¶èŒƒå›´å†…ï¼Œå³ä½¿æœ‰æœ€ä¼˜çš„äº’è¿ï¼Œæ•ˆç‡ä¹Ÿä¼šä¸‹é™åˆ°60-80%ã€‚è¶…è¿‡512ä¸ªGPUæ—¶ï¼Œåè°ƒå¼€é”€å˜å¾—å ä¸»å¯¼åœ°ä½ï¼Œç”±äºé›†ä½“æ“ä½œçš„å»¶è¿Ÿï¼Œæ•ˆç‡é™åˆ¶åœ¨40-60%ã€‚
- en: Hardware selection critically impacts these scaling characteristics. NVIDIA
    DGX systems with NVLink achieve 600 GB/s bisection bandwidth, enabling 90% parallel
    efficiency up to 8 GPUs per node. Multi-node scaling requires InfiniBand networks,
    where EDR (100 Gbps) supports efficient training up to 64 nodes, while HDR (200
    Gbps) enables scaling to 256+ nodes with >70% efficiency.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶é€‰æ‹©å¯¹è¿™äº›æ‰©å±•ç‰¹æ€§æœ‰é‡è¦å½±å“ã€‚é…å¤‡NVLinkçš„NVIDIA DGXç³»ç»Ÿå®ç°äº†600 GB/sçš„åˆ†å‰²å¸¦å®½ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæ”¯æŒ8ä¸ªGPUè¾¾åˆ°90%çš„å¹¶è¡Œæ•ˆç‡ã€‚å¤šèŠ‚ç‚¹æ‰©å±•éœ€è¦InfiniBandç½‘ç»œï¼Œå…¶ä¸­EDRï¼ˆ100
    Gbpsï¼‰æ”¯æŒå¤šè¾¾64ä¸ªèŠ‚ç‚¹çš„æœ‰æ•ˆè®­ç»ƒï¼Œè€ŒHDRï¼ˆ200 Gbpsï¼‰åˆ™ä½¿æ‰©å±•åˆ°256ä¸ªä»¥ä¸ŠèŠ‚ç‚¹æˆä¸ºå¯èƒ½ï¼Œæ•ˆç‡è¶…è¿‡70%ã€‚
- en: These efficiency metrics directly influence the choice of parallelism strategy.
    Data parallelism works well in the linear scaling regime but becomes communication-bound
    at scale. Model parallelism addresses memory constraints but introduces sequential
    dependencies that limit efficiency. Pipeline parallelism reduces device idle time
    but introduces complexity in managing microbatches. Understanding these trade-offs
    enables architects to select appropriate strategies for their specific workloads.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ•ˆç‡æŒ‡æ ‡ç›´æ¥å½±å“äº†å¹¶è¡Œç­–ç•¥çš„é€‰æ‹©ã€‚æ•°æ®å¹¶è¡Œåœ¨çº¿æ€§æ‰©å±•èŒƒå›´å†…è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤§è§„æ¨¡ä¸‹ä¼šå—é™äºé€šä¿¡ã€‚æ¨¡å‹å¹¶è¡Œè§£å†³äº†å†…å­˜é™åˆ¶é—®é¢˜ï¼Œä½†å¼•å…¥äº†é™åˆ¶æ•ˆç‡çš„é¡ºåºä¾èµ–æ€§ã€‚æµæ°´çº¿å¹¶è¡Œå‡å°‘äº†è®¾å¤‡ç©ºé—²æ—¶é—´ï¼Œä½†å¼•å…¥äº†ç®¡ç†å¾®æ‰¹æ¬¡çš„å¤æ‚æ€§ã€‚ç†è§£è¿™äº›æƒè¡¡ä½¿å¾—æ¶æ„å¸ˆèƒ½å¤Ÿä¸ºä»–ä»¬çš„ç‰¹å®šå·¥ä½œè´Ÿè½½é€‰æ‹©åˆé€‚çš„ç­–ç•¥ã€‚
- en: Data Parallelism
  id: totrans-780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œ
- en: Building on the efficiency characteristics outlined above, data parallelism
    represents the most straightforward distributed approach, particularly effective
    in the linear scaling regime where communication overhead remains manageable.
    This method distributes the training process across multiple devices by splitting
    the dataset into smaller subsets. Each device trains a complete copy of the model
    using its assigned subset of the data. For example, when training an image classification
    model on 1 million images using 4 GPUs, each GPU would process 250,000 images
    while maintaining an identical copy of the model architecture.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¸Šè¿°æ•ˆç‡ç‰¹æ€§ï¼Œæ•°æ®å¹¶è¡ŒåŒ–ä»£è¡¨äº†æœ€ç›´æ¥çš„åˆ†å¸ƒå¼æ–¹æ³•ï¼Œåœ¨é€šä¿¡å¼€é”€å¯ç®¡ç†çš„çº¿æ€§æ‰©å±•åŒºåŸŸå°¤å…¶æœ‰æ•ˆã€‚è¿™ç§æ–¹æ³•é€šè¿‡å°†æ•°æ®é›†åˆ†å‰²æˆæ›´å°çš„å­é›†æ¥åœ¨å¤šä¸ªè®¾å¤‡ä¸Šåˆ†é…è®­ç»ƒè¿‡ç¨‹ã€‚æ¯ä¸ªè®¾å¤‡ä½¿ç”¨åˆ†é…ç»™å®ƒçš„æ•°æ®å­é›†çš„å®Œæ•´æ¨¡å‹å‰¯æœ¬è¿›è¡Œè®­ç»ƒã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨4ä¸ªGPUåœ¨100ä¸‡å¼ å›¾åƒä¸Šè®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹æ—¶ï¼Œæ¯ä¸ªGPUå°†å¤„ç†25ä¸‡å¼ å›¾åƒï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ¶æ„çš„ç›¸åŒå‰¯æœ¬ã€‚
- en: It is particularly effective when the dataset size is large but the model size
    is manageable, as each device must store a full copy of the model in memory. This
    method is widely used in scenarios such as image classification and natural language
    processing, where the dataset can be processed in parallel without dependencies
    between data samples. For instance, when training a ResNet model on ImageNet,
    each GPU can independently process its portion of images since the classification
    of one image doesnâ€™t depend on the results of another.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ•°æ®é›†å¾ˆå¤§ä½†æ¨¡å‹å¤§å°å¯ç®¡ç†æ—¶ï¼Œè¿™ç§æ–¹æ³•å°¤å…¶æœ‰æ•ˆï¼Œå› ä¸ºæ¯ä¸ªè®¾å¤‡å¿…é¡»åœ¨å†…å­˜ä¸­å­˜å‚¨æ¨¡å‹çš„å®Œæ•´å‰¯æœ¬ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ•°æ®é›†å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œè€Œæ•°æ®æ ·æœ¬ä¹‹é—´æ²¡æœ‰ä¾èµ–å…³ç³»ã€‚ä¾‹å¦‚ï¼Œå½“åœ¨ImageNetä¸Šè®­ç»ƒResNetæ¨¡å‹æ—¶ï¼Œæ¯ä¸ªGPUå¯ä»¥ç‹¬ç«‹å¤„ç†å…¶å›¾åƒéƒ¨åˆ†ï¼Œå› ä¸ºä¸€å¼ å›¾åƒçš„åˆ†ç±»ä¸ä¾èµ–äºå¦ä¸€å¼ å›¾åƒçš„ç»“æœã€‚
- en: The effectiveness of data parallelism stems from a property of stochastic gradient
    descent established in our optimization foundations. Gradients computed on different
    minibatches can be averaged while preserving mathematical equivalence to single-device
    training. This property enables parallel computation across devices, with the
    mathematical foundation following directly from the linearity of expectation.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡ŒåŒ–çš„æœ‰æ•ˆæ€§æºäºæˆ‘ä»¬åœ¨ä¼˜åŒ–åŸºç¡€ä¸­ç¡®ç«‹çš„éšæœºæ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªå±æ€§ã€‚åœ¨ä¸åŒçš„minibatchä¸Šè®¡ç®—çš„æ¢¯åº¦å¯ä»¥åœ¨ä¿æŒä¸å•è®¾å¤‡è®­ç»ƒçš„æ•°å­¦ç­‰ä»·æ€§çš„åŒæ—¶è¿›è¡Œå¹³å‡ã€‚è¿™ä¸€å±æ€§ä½¿å¾—è·¨è®¾å¤‡è¿›è¡Œå¹¶è¡Œè®¡ç®—æˆä¸ºå¯èƒ½ï¼Œå…¶æ•°å­¦åŸºç¡€ç›´æ¥æ¥æºäºæœŸæœ›çš„çº¿æ€§ã€‚
- en: 'Consider a model with parameters <semantics><mi>Î¸</mi><annotation encoding="application/x-tex">Î¸</annotation></semantics>
    training on a dataset <semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>.
    The loss function for a single data point <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> is <semantics><mrow><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(Î¸,
    x_i)</annotation></semantics>. In standard SGD with batch size <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics>, the gradient update for
    a minibatch is: <semantics><mrow><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g
    = \frac{1}{B} \sum_{i=1}^B \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªåœ¨æ•°æ®é›†<semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics>ä¸Šè®­ç»ƒçš„å‚æ•°<semantics><mi>Î¸</mi><annotation
    encoding="application/x-tex">Î¸</annotation></semantics>çš„æ¨¡å‹ã€‚å•ä¸ªæ•°æ®ç‚¹<semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics>çš„æŸå¤±å‡½æ•°æ˜¯<semantics><mrow><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(Î¸,
    x_i)</annotation></semantics>ã€‚åœ¨æ ‡å‡†SGDï¼ˆæ‰¹é‡å¤§å°ä¸º<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>ï¼‰ä¸­ï¼Œminibatchçš„æ¢¯åº¦æ›´æ–°ä¸ºï¼š<semantics><mrow><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g
    = \frac{1}{B} \sum_{i=1}^B \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>
- en: 'In data parallelism with <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    devices, each device <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    computes gradients on its own minibatch <semantics><msub><mi>B</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">B_k</annotation></semantics>: <semantics><mrow><msub><mi>g</mi><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">|</mo></mrow></mfrac><munder><mo>âˆ‘</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>âˆˆ</mo><msub><mi>B</mi><mi>k</mi></msub></mrow></munder><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g_k
    = \frac{1}{|B_k|} \sum_{x_i \in B_k} \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…·æœ‰ <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    ä¸ªè®¾å¤‡çš„æ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ªè®¾å¤‡ <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    éƒ½ä¼šè®¡ç®—å…¶è‡ªå·±çš„å°æ‰¹é‡ <semantics><msub><mi>B</mi><mi>k</mi></msub><annotation encoding="application/x-tex">B_k</annotation></semantics>
    çš„æ¢¯åº¦ï¼š<semantics><mrow><msub><mi>g</mi><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">|</mo></mrow></mfrac><munder><mo>âˆ‘</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>âˆˆ</mo><msub><mi>B</mi><mi>k</mi></msub></mrow></munder><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g_k
    = \frac{1}{|B_k|} \sum_{x_i \in B_k} \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>
- en: 'The global update averages these local gradients: <semantics><mrow><msub><mi>g</mi><mtext
    mathvariant="normal">global</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>g</mi><mi>k</mi></msub></mrow>
    <annotation encoding="application/x-tex">g_{\text{global}} = \frac{1}{N} \sum_{k=1}^N
    g_k</annotation></semantics>'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨å±€æ›´æ–°å¹³å‡è¿™äº›å±€éƒ¨æ¢¯åº¦ï¼š<semantics><mrow><msub><mi>g</mi><mtext mathvariant="normal">global</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>g</mi><mi>k</mi></msub></mrow>
    <annotation encoding="application/x-tex">g_{\text{global}} = \frac{1}{N} \sum_{k=1}^N
    g_k</annotation></semantics>
- en: 'This averaging is mathematically equivalent to computing the gradient on the
    combined batch <semantics><mrow><msub><mi>B</mi><mtext mathvariant="normal">total</mtext></msub><mo>=</mo><msubsup><mo>â‹ƒ</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>B</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">B_{\text{total}} = \bigcup_{k=1}^N B_k</annotation></semantics>:
    <semantics><mrow><msub><mi>g</mi><mtext mathvariant="normal">global</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mtext mathvariant="normal">total</mtext></msub><mo
    stretchy="true" form="postfix">|</mo></mrow></mfrac><munder><mo>âˆ‘</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>âˆˆ</mo><msub><mi>B</mi><mtext
    mathvariant="normal">total</mtext></msub></mrow></munder><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g_{\text{global}}
    = \frac{1}{|B_{\text{total}}|} \sum_{x_i \in B_{\text{total}}} \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™ç§å¹³å‡åœ¨æ•°å­¦ä¸Šç­‰åŒäºåœ¨åˆå¹¶åçš„æ‰¹æ¬¡ä¸Šè®¡ç®—æ¢¯åº¦ <semantics><mrow><msub><mi>B</mi><mtext mathvariant="normal">total</mtext></msub><mo>=</mo><msubsup><mo>â‹ƒ</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>B</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">B_{\text{total}} = \bigcup_{k=1}^N B_k</annotation></semantics>:
    <semantics><mrow><msub><mi>g</mi><mtext mathvariant="normal">global</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>B</mi><mtext mathvariant="normal">total</mtext></msub><mo
    stretchy="true" form="postfix">|</mo></mrow></mfrac><munder><mo>âˆ‘</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>âˆˆ</mo><msub><mi>B</mi><mtext
    mathvariant="normal">total</mtext></msub></mrow></munder><msub><mi>âˆ‡</mi><mi>Î¸</mi></msub><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>Î¸</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">g_{\text{global}}
    = \frac{1}{|B_{\text{total}}|} \sum_{x_i \in B_{\text{total}}} \nabla_Î¸ L(Î¸, x_i)</annotation></semantics>'
- en: This equivalence shows why data parallelism maintains the statistical properties
    of SGD training. The approach distributes distinct data subsets across devices,
    computes local gradients independently, and averages these gradients to approximate
    the full-batch gradient.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç­‰ä»·æ€§è¯´æ˜äº†ä¸ºä»€ä¹ˆæ•°æ®å¹¶è¡Œèƒ½å¤Ÿä¿æŒ SGD è®­ç»ƒçš„ç»Ÿè®¡ç‰¹æ€§ã€‚è¯¥æ–¹æ³•å°†ä¸åŒçš„æ•°æ®å­é›†åˆ†é…åˆ°å„ä¸ªè®¾å¤‡ä¸Šï¼Œç‹¬ç«‹è®¡ç®—å±€éƒ¨æ¢¯åº¦ï¼Œå¹¶å°†è¿™äº›æ¢¯åº¦å¹³å‡ä»¥è¿‘ä¼¼å…¨æ‰¹æ¢¯åº¦ã€‚
- en: The method parallels gradient accumulation, where a single device accumulates
    gradients over multiple forward passes before updating parameters. Both techniques
    use the additive properties of gradients to process large batches efficiently.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•ä¸æ¢¯åº¦ç´¯ç§¯å¹³è¡Œï¼Œå…¶ä¸­å•ä¸ªè®¾å¤‡åœ¨æ›´æ–°å‚æ•°ä¹‹å‰é€šè¿‡å¤šä¸ªæ­£å‘ä¼ é€’ç´¯ç§¯æ¢¯åº¦ã€‚è¿™ä¸¤ç§æŠ€æœ¯éƒ½ä½¿ç”¨æ¢¯åº¦çš„åŠ æ€§å±æ€§æ¥é«˜æ•ˆå¤„ç†å¤§å‹æ‰¹é‡ã€‚
- en: '**Production Reality: Data Parallelism at Scale**'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç”Ÿäº§ç°å®ï¼šå¤§è§„æ¨¡æ•°æ®å¹¶è¡Œ**'
- en: 'Data parallelism in production environments involves several operational considerations
    beyond the theoretical framework:'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®å¹¶è¡Œæ¶‰åŠè¶…å‡ºç†è®ºæ¡†æ¶çš„å‡ ä¸ªæ“ä½œè€ƒè™‘å› ç´ ï¼š
- en: '**Communication efficiency**: AllReduce operations for gradient synchronization
    become the bottleneck at scale. Production systems use optimized libraries like
    NCCL with ring or tree communication patterns to minimize overhead'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€šä¿¡æ•ˆç‡**ï¼šåœ¨è§„æ¨¡ä¸Šï¼Œæ¢¯åº¦åŒæ­¥çš„AllReduceæ“ä½œæˆä¸ºç“¶é¢ˆã€‚ç”Ÿäº§ç³»ç»Ÿä½¿ç”¨å¦‚NCCLè¿™æ ·çš„ä¼˜åŒ–åº“ï¼Œé‡‡ç”¨ç¯å½¢æˆ–æ ‘çŠ¶é€šä¿¡æ¨¡å¼ä»¥æœ€å°åŒ–å¼€é”€'
- en: '**Fault tolerance**: Node failures during large-scale training require checkpoint/restart
    strategies. Production systems implement hierarchical checkpointing with both
    local and distributed storage'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®¹é”™æ€§**ï¼šå¤§è§„æ¨¡è®­ç»ƒä¸­çš„èŠ‚ç‚¹æ•…éšœéœ€è¦æ£€æŸ¥ç‚¹/é‡å¯ç­–ç•¥ã€‚ç”Ÿäº§ç³»ç»Ÿå®æ–½å…·æœ‰æœ¬åœ°å’Œåˆ†å¸ƒå¼å­˜å‚¨çš„åˆ†å±‚æ£€æŸ¥ç‚¹'
- en: '**Dynamic scaling**: Cloud environments require elastic scaling capabilities
    to add/remove workers based on demand and cost constraints, complicated by the
    need to maintain gradient synchronization'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŠ¨æ€æ‰©å±•**ï¼šäº‘ç¯å¢ƒéœ€è¦å¼¹æ€§æ‰©å±•èƒ½åŠ›ï¼Œæ ¹æ®éœ€æ±‚å’Œæˆæœ¬é™åˆ¶æ·»åŠ /åˆ é™¤å·¥ä½œè€…ï¼ŒåŒæ—¶éœ€è¦ç»´æŠ¤æ¢¯åº¦åŒæ­¥ã€‚'
- en: '**Cost optimization**: Production data parallelism considers cost per GPU-hour
    across different instance types and preemptible instances, balancing training
    time against infrastructure costs'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬ä¼˜åŒ–**ï¼šç”Ÿäº§æ•°æ®å¹¶è¡Œè€ƒè™‘ä¸åŒå®ä¾‹ç±»å‹å’Œå¯æŠ¢å å®ä¾‹çš„æ¯GPUå°æ—¶æˆæœ¬ï¼Œåœ¨è®­ç»ƒæ—¶é—´å’ŒåŸºç¡€è®¾æ–½æˆæœ¬ä¹‹é—´è¿›è¡Œå¹³è¡¡'
- en: '**Network bandwidth requirements**: Large models require careful network topology
    planning as gradient communication can consume 10-50% of training time depending
    on model size and batch size'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç½‘ç»œå¸¦å®½éœ€æ±‚**ï¼šå¤§å‹æ¨¡å‹éœ€è¦ä»”ç»†çš„ç½‘ç»œæ‹“æ‰‘è§„åˆ’ï¼Œå› ä¸ºæ¢¯åº¦é€šä¿¡å¯èƒ½æ¶ˆè€—10-50%çš„è®­ç»ƒæ—¶é—´ï¼Œå…·ä½“å–å†³äºæ¨¡å‹å¤§å°å’Œæ‰¹é‡å¤§å°'
- en: Production teams typically benchmark communication patterns and scaling efficiency
    before deploying large distributed training jobs to identify optimal configurations.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿäº§å›¢é˜Ÿé€šå¸¸åœ¨éƒ¨ç½²å¤§å‹åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šä¹‹å‰å¯¹é€šä¿¡æ¨¡å¼å’Œæ‰©å±•æ•ˆç‡è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥ç¡®å®šæœ€ä½³é…ç½®ã€‚
- en: Data Parallelism Implementation
  id: totrans-798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œå®ç°
- en: The process of data parallelism can be broken into a series of distinct steps,
    each with its role in ensuring the system operates efficiently. These steps are
    illustrated in [FigureÂ 8.13](ch014.xhtml#fig-train-data-parallelism).
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œçš„è¿‡ç¨‹å¯ä»¥åˆ†è§£ä¸ºä¸€ç³»åˆ—ä¸åŒçš„æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½åœ¨ç¡®ä¿ç³»ç»Ÿé«˜æ•ˆè¿è¡Œä¸­æ‰®æ¼”ç€å…¶è§’è‰²ã€‚è¿™äº›æ­¥éª¤åœ¨[å›¾8.13](ch014.xhtml#fig-train-data-parallelism)ä¸­è¿›è¡Œäº†è¯´æ˜ã€‚
- en: '![](../media/file120.svg)'
  id: totrans-800
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file120.svg)'
- en: 'FigureÂ 8.13: **Data Parallelism**: Distributed training replicates the model
    across multiple devices, each processing a subset of the data before aggregating
    gradients to update model parameters, thereby accelerating the training process.
    This approach contrasts with model parallelism, where the model itself is partitioned
    across devices.'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.13ï¼š**æ•°æ®å¹¶è¡Œ**ï¼šåˆ†å¸ƒå¼è®­ç»ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¤åˆ¶æ¨¡å‹ï¼Œæ¯ä¸ªè®¾å¤‡åœ¨èšåˆæ¢¯åº¦ä»¥æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹å‰å¤„ç†æ•°æ®çš„ä¸€ä¸ªå­é›†ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸æ¨¡å‹å¹¶è¡Œç›¸å¯¹ç«‹ï¼Œæ¨¡å‹æœ¬èº«åœ¨è®¾å¤‡ä¹‹é—´è¿›è¡Œåˆ†åŒºã€‚
- en: Dataset Splitting
  id: totrans-802
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ•°æ®é›†æ‹†åˆ†
- en: The first step in data parallelism involves dividing the dataset into smaller,
    non-overlapping subsets. This ensures that each device processes a unique portion
    of the data, avoiding redundancy and enabling efficient utilization of available
    hardware. For instance, with a dataset of 100,000 training examples and 4 GPUs,
    each GPU would be assigned 25,000 examples. Modern frameworks like PyTorchâ€™s DistributedSampler
    handle this distribution automatically, implementing prefetching and caching mechanisms
    to ensure data is readily available for processing.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œçš„ç¬¬ä¸€æ­¥æ˜¯å°†æ•°æ®é›†åˆ’åˆ†ä¸ºæ›´å°çš„ã€éé‡å çš„å­é›†ã€‚è¿™ç¡®ä¿æ¯ä¸ªè®¾å¤‡å¤„ç†æ•°æ®çš„ä¸€ä¸ªç‹¬ç‰¹éƒ¨åˆ†ï¼Œé¿å…å†—ä½™å¹¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¯ç”¨ç¡¬ä»¶ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªåŒ…å«100,000ä¸ªè®­ç»ƒç¤ºä¾‹å’Œ4ä¸ªGPUçš„æ•°æ®é›†ï¼Œæ¯ä¸ªGPUå°†è¢«åˆ†é…25,000ä¸ªç¤ºä¾‹ã€‚ç°ä»£æ¡†æ¶å¦‚PyTorchçš„DistributedSamplerä¼šè‡ªåŠ¨å¤„ç†è¿™ç§åˆ†é…ï¼Œå®ç°é¢„å–å’Œç¼“å­˜æœºåˆ¶ä»¥ç¡®ä¿æ•°æ®å¯ä»¥éšæ—¶ç”¨äºå¤„ç†ã€‚
- en: Device Forward Pass
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è®¾å¤‡æ­£å‘ä¼ é€’
- en: Once the data subsets are distributed, each device performs the forward pass
    independently. During this stage, the model processes its assigned batch of data,
    generating predictions and calculating the loss. For example, in a ResNet-50 model,
    each GPU would independently compute the convolutions, activations, and final
    loss for its batch. The forward pass is computationally intensive and benefits
    from hardware accelerators like NVIDIA V100 GPUs or Google TPUs, which are optimized
    for matrix operations.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ•°æ®å­é›†è¢«åˆ†é…ï¼Œæ¯ä¸ªè®¾å¤‡ç‹¬ç«‹æ‰§è¡Œæ­£å‘ä¼ æ’­ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡å‹å¤„ç†å…¶åˆ†é…çš„æ‰¹æ¬¡æ•°æ®ï¼Œç”Ÿæˆé¢„æµ‹å¹¶è®¡ç®—æŸå¤±ã€‚ä¾‹å¦‚ï¼Œåœ¨ResNet-50æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªGPUä¼šç‹¬ç«‹è®¡ç®—å…¶æ‰¹æ¬¡çš„å·ç§¯ã€æ¿€æ´»å’Œæœ€ç»ˆæŸå¤±ã€‚æ­£å‘ä¼ æ’­æ˜¯è®¡ç®—å¯†é›†å‹çš„ï¼Œå¹¶å—ç›Šäºç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œå¦‚NVIDIA
    V100 GPUæˆ–Google TPUsï¼Œè¿™äº›åŠ é€Ÿå™¨é’ˆå¯¹çŸ©é˜µè¿ç®—è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: Backward Pass and Calculation
  id: totrans-806
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­å’Œè®¡ç®—
- en: Following the forward pass, each device computes the gradients of the loss with
    respect to the modelâ€™s parameters during the backward pass. Modern frameworks
    like PyTorch and TensorFlow handle this automatically through their autograd systems.
    For instance, if a model has 50 million parameters, each device calculates gradients
    for all parameters but based only on its local data subset.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å‘ä¼ æ’­ä¹‹åï¼Œæ¯ä¸ªè®¾å¤‡åœ¨åå‘ä¼ æ’­æœŸé—´è®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚ç°ä»£æ¡†æ¶å¦‚PyTorchå’ŒTensorFlowé€šè¿‡å®ƒä»¬çš„autogradç³»ç»Ÿè‡ªåŠ¨å¤„ç†è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹æœ‰5000ä¸‡ä¸ªå‚æ•°ï¼Œæ¯ä¸ªè®¾å¤‡è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼Œä½†ä»…åŸºäºå…¶æœ¬åœ°æ•°æ®å­é›†ã€‚
- en: Gradient Synchronization
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åŒæ­¥
- en: 'To maintain consistency across the distributed system, the gradients computed
    by each device must be synchronized. This coordination represents a distributed
    systems challenge: achieving global consensus while minimizing communication complexity.
    The ring all-reduce algorithm exemplifies this trade-off, organizing devices in
    a logical ring where each GPU communicates only with its neighbors. The algorithm
    complexity is O(n) in communication rounds but requires sequential dependencies
    that can limit parallelism.'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿æŒåˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€è‡´æ€§ï¼Œæ¯ä¸ªè®¾å¤‡è®¡ç®—å‡ºçš„æ¢¯åº¦å¿…é¡»è¿›è¡ŒåŒæ­¥ã€‚è¿™ç§åè°ƒä»£è¡¨äº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€ä¸ªæŒ‘æˆ˜ï¼šåœ¨æœ€å°åŒ–é€šä¿¡å¤æ‚æ€§çš„åŒæ—¶å®ç°å…¨å±€å…±è¯†ã€‚ç¯å½¢å…¨å½’çº¦ç®—æ³•æ˜¯è¿™ç§æƒè¡¡çš„ä¸€ä¸ªä¾‹å­ï¼Œå®ƒå°†è®¾å¤‡ç»„ç»‡åœ¨ä¸€ä¸ªé€»è¾‘ç¯ä¸­ï¼Œå…¶ä¸­æ¯ä¸ªGPUåªä¸å…¶é‚»å±…é€šä¿¡ã€‚ç®—æ³•çš„é€šä¿¡è½®æ¬¡å¤æ‚åº¦ä¸ºO(n)ï¼Œä½†éœ€è¦åºåˆ—ä¾èµ–æ€§ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¹¶è¡Œæ€§ã€‚
- en: 'For example, with 8 GPUs sharing gradients for a 100 MB model, ring all-reduce
    requires only 7 communication steps instead of the 56 steps needed for naive all-to-all
    synchronization. The ring topology creates bottlenecks: the slowest link in the
    ring determines the overall synchronization time, and network partitions can halt
    the entire training process. Alternative algorithms like tree-reduce achieve O(log
    n) latency at the cost of increased bandwidth requirements on root nodes. Modern
    systems often implement hierarchical topologies, using high-speed links within
    nodes and lower-bandwidth connections between nodes to optimize these trade-offs.'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºå…±äº«100 MBæ¨¡å‹æ¢¯åº¦çš„8ä¸ªGPUï¼Œç¯å½¢å…¨å½’çº¦åªéœ€è¦7ä¸ªé€šä¿¡æ­¥éª¤ï¼Œè€Œä¸æ˜¯åŸå§‹å…¨å…¨åŒæ­¥æ‰€éœ€çš„56ä¸ªæ­¥éª¤ã€‚ç¯å½¢æ‹“æ‰‘ç»“æ„ä¼šåˆ›å»ºç“¶é¢ˆï¼šç¯å½¢ä¸­æœ€æ…¢çš„é“¾è·¯å†³å®šäº†æ•´ä½“åŒæ­¥æ—¶é—´ï¼Œå¹¶ä¸”ç½‘ç»œåˆ†åŒºå¯èƒ½ä¼šåœæ­¢æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚åƒtree-reduceè¿™æ ·çš„æ›¿ä»£ç®—æ³•ä»¥å¢åŠ æ ¹èŠ‚ç‚¹å¸¦å®½éœ€æ±‚ä¸ºä»£ä»·ï¼Œå®ç°äº†O(log
    n)çš„å»¶è¿Ÿã€‚ç°ä»£ç³»ç»Ÿé€šå¸¸å®ç°åˆ†å±‚æ‹“æ‰‘ï¼Œåœ¨èŠ‚ç‚¹å†…ä½¿ç”¨é«˜é€Ÿé“¾è·¯ï¼Œåœ¨èŠ‚ç‚¹ä¹‹é—´ä½¿ç”¨ä½å¸¦å®½è¿æ¥æ¥ä¼˜åŒ–è¿™äº›æƒè¡¡ã€‚
- en: Parameter Updating
  id: totrans-811
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°
- en: After gradient aggregation, each device independently updates model parameters
    using the chosen optimization algorithm, such as SGD with momentum or Adam. This
    decentralized update strategy, implemented in frameworks like PyTorchâ€™s DistributedDataParallel
    (DDP), enables efficient parameter updates without requiring a central coordination
    server. Since all devices have identical gradient values after synchronization,
    they perform mathematically equivalent updates to maintain model consistency across
    the distributed system.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¢¯åº¦èšåˆä¹‹åï¼Œæ¯ä¸ªè®¾å¤‡ä½¿ç”¨é€‰å®šçš„ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚å¸¦æœ‰åŠ¨é‡çš„SGDæˆ–Adamï¼‰ç‹¬ç«‹æ›´æ–°æ¨¡å‹å‚æ•°ã€‚è¿™ç§å»ä¸­å¿ƒåŒ–çš„æ›´æ–°ç­–ç•¥ï¼Œåœ¨PyTorchçš„DistributedDataParallel
    (DDP)ç­‰æ¡†æ¶ä¸­å®ç°ï¼Œèƒ½å¤Ÿåœ¨ä¸è¦æ±‚ä¸­å¤®åè°ƒæœåŠ¡å™¨çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„å‚æ•°æ›´æ–°ã€‚ç”±äºæ‰€æœ‰è®¾å¤‡åœ¨åŒæ­¥åéƒ½å…·æœ‰ç›¸åŒçš„æ¢¯åº¦å€¼ï¼Œå› æ­¤å®ƒä»¬æ‰§è¡Œæ•°å­¦ä¸Šç­‰æ•ˆçš„æ›´æ–°ï¼Œä»¥ä¿æŒåˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ¨¡å‹ä¸€è‡´æ€§ã€‚
- en: For example, in a system with 8 GPUs training a ResNet model, each GPU computes
    local gradients based on its data subset. After gradient averaging via ring all-reduce,
    every GPU has the same global gradient values. Each device then independently
    applies these gradients using the optimizerâ€™s update rule. If using SGD with learning
    rate 0.1, the update would be `weights = weights - 0.1 * gradients`. This process
    maintains mathematical equivalence to single-device training while enabling distributed
    computation.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªæœ‰8ä¸ªGPUçš„ç³»ç»Ÿä¸Šè®­ç»ƒResNetæ¨¡å‹æ—¶ï¼Œæ¯ä¸ªGPUæ ¹æ®å…¶æ•°æ®å­é›†è®¡ç®—å±€éƒ¨æ¢¯åº¦ã€‚é€šè¿‡ç¯å…¨å½’ä¸€åŒ–å¹³å‡æ¢¯åº¦åï¼Œæ¯ä¸ªGPUéƒ½æœ‰ç›¸åŒçš„å…¨å±€æ¢¯åº¦å€¼ã€‚ç„¶åæ¯ä¸ªè®¾å¤‡ç‹¬ç«‹ä½¿ç”¨ä¼˜åŒ–å™¨çš„æ›´æ–°è§„åˆ™åº”ç”¨è¿™äº›æ¢¯åº¦ã€‚å¦‚æœä½¿ç”¨å­¦ä¹ ç‡ä¸º0.1çš„SGDï¼Œæ›´æ–°å°†æ˜¯`weights
    = weights - 0.1 * gradients`ã€‚è¿™ä¸ªè¿‡ç¨‹ä¿æŒäº†ä¸å•è®¾å¤‡è®­ç»ƒçš„æ•°å­¦ç­‰ä»·æ€§ï¼ŒåŒæ—¶å®ç°äº†åˆ†å¸ƒå¼è®¡ç®—ã€‚
- en: This process, which involves splitting data, performing computations, synchronizing
    results, and updating parameters, repeats for each batch of data. Modern frameworks
    automate this cycle, allowing developers to focus on model architecture and hyperparameter
    tuning rather than distributed computing logistics.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠæ•°æ®æ‹†åˆ†ã€æ‰§è¡Œè®¡ç®—ã€åŒæ­¥ç»“æœå’Œæ›´æ–°å‚æ•°ï¼Œå¯¹äºæ¯ä¸€æ‰¹æ•°æ®éƒ½ä¼šé‡å¤è¿›è¡Œã€‚ç°ä»£æ¡†æ¶è‡ªåŠ¨åŒ–äº†è¿™ä¸ªå¾ªç¯ï¼Œä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿä¸“æ³¨äºæ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°è°ƒæ•´ï¼Œè€Œä¸æ˜¯åˆ†å¸ƒå¼è®¡ç®—çš„ç‰©æµã€‚
- en: Data Parallelism Advantages
  id: totrans-815
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œä¼˜åŠ¿
- en: Data parallelism offers several key benefits that make it the predominant approach
    for distributed training. By splitting the dataset across multiple devices and
    allowing each device to train an identical copy of the model, this approach effectively
    addresses the core challenges in modern AI training systems.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œæä¾›äº†å‡ ä¸ªå…³é”®ä¼˜åŠ¿ï¼Œä½¿å…¶æˆä¸ºåˆ†å¸ƒå¼è®­ç»ƒçš„ä¸»è¦æ–¹æ³•ã€‚é€šè¿‡å°†æ•°æ®é›†æ‹†åˆ†åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå¹¶å…è®¸æ¯ä¸ªè®¾å¤‡è®­ç»ƒæ¨¡å‹çš„ä¸€ä¸ªç›¸åŒå‰¯æœ¬ï¼Œè¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç°ä»£AIè®­ç»ƒç³»ç»Ÿä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚
- en: The primary advantage of data parallelism is its linear scaling capability with
    large datasets. As datasets grow into the terabyte range, processing them on a
    single machine becomes prohibitively time-consuming. For example, training a vision
    transformer on ImageNet (1.2 million images) might take weeks on a single GPU,
    but only days when distributed across 8 GPUs. This scalability is particularly
    valuable in domains like language modeling, where datasets can exceed billions
    of tokens.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œçš„ä¸»è¦ä¼˜åŠ¿æ˜¯å…¶ä¸å¤§æ•°æ®é›†çš„çº¿æ€§æ‰©å±•èƒ½åŠ›ã€‚éšç€æ•°æ®é›†å¢é•¿åˆ°åƒå…†å­—èŠ‚èŒƒå›´ï¼Œå•æœºå¤„ç†å®ƒä»¬å˜å¾—è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¾‹å¦‚ï¼Œåœ¨å•ä¸ªGPUä¸Šè®­ç»ƒImageNetï¼ˆ120ä¸‡å¼ å›¾ç‰‡ï¼‰çš„è§†è§‰è½¬æ¢å™¨å¯èƒ½éœ€è¦å‡ å‘¨æ—¶é—´ï¼Œä½†åˆ†å¸ƒåœ¨8ä¸ªGPUä¸Šåªéœ€å‡ å¤©ã€‚è¿™ç§å¯æ‰©å±•æ€§åœ¨è¯­è¨€å»ºæ¨¡ç­‰é¢†åŸŸçš„ä»·å€¼å°¤ä¸ºçªå‡ºï¼Œå…¶ä¸­æ•°æ®é›†å¯ä»¥è¶…è¿‡æ•°åäº¿ä¸ªæ ‡è®°ã€‚
- en: Hardware utilization efficiency represents another important benefit. Data parallelism
    maintains high GPU utilization rates, typically, above 85%, by ensuring each device
    actively processes its data portion. Modern implementations achieve this through
    asynchronous data loading and gradient computation overlapping with communication.
    For instance, while one batch computes gradients, the next batchâ€™s data is already
    being loaded and preprocessed.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶åˆ©ç”¨ç‡æ•ˆç‡ä»£è¡¨å¦ä¸€ä¸ªé‡è¦å¥½å¤„ã€‚æ•°æ®å¹¶è¡Œé€šè¿‡ç¡®ä¿æ¯ä¸ªè®¾å¤‡ç§¯æå¤„ç†å…¶æ•°æ®éƒ¨åˆ†ï¼Œé€šå¸¸ä¿æŒé«˜äº85%çš„é«˜GPUåˆ©ç”¨ç‡ã€‚ç°ä»£å®ç°é€šè¿‡å¼‚æ­¥æ•°æ®åŠ è½½å’Œæ¢¯åº¦è®¡ç®—ä¸é€šä¿¡é‡å æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªæ‰¹æ¬¡è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä¸‹ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®å·²ç»è¢«åŠ è½½å¹¶é¢„å¤„ç†ã€‚
- en: Implementation simplicity sets data parallelism apart from other distribution
    strategies. Modern frameworks have reduced complex distributed training to just
    a few lines of code. For example, converting a PyTorch model to use data parallelism
    often requires only wrapping it in `DistributedDataParallel` and initializing
    a distributed environment. This accessibility has contributed significantly to
    its widespread adoption in both research and industry.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°ç®€å•æ€§ä½¿æ•°æ®å¹¶è¡Œä¸å…¶ä»–åˆ†å¸ƒå¼ç­–ç•¥åŒºåˆ†å¼€æ¥ã€‚ç°ä»£æ¡†æ¶å°†å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒç®€åŒ–ä¸ºå‡ è¡Œä»£ç ã€‚ä¾‹å¦‚ï¼Œå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºä½¿ç”¨æ•°æ®å¹¶è¡Œé€šå¸¸åªéœ€è¦å°†å…¶åŒ…è£¹åœ¨`DistributedDataParallel`ä¸­å¹¶åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒã€‚è¿™ç§æ˜“ç”¨æ€§æå¤§åœ°ä¿ƒè¿›äº†å…¶åœ¨ç ”ç©¶å’Œå·¥ä¸šç•Œçš„å¹¿æ³›åº”ç”¨ã€‚
- en: The approach also offers flexibility across model architectures. Whether training
    a ResNet (vision), BERT (language), or Graph Neural Network (graph data), the
    same data parallelism principles apply without modification. This universality
    makes it particularly valuable as a default choice for distributed training.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•è¿˜æä¾›äº†è·¨æ¨¡å‹æ¶æ„çš„çµæ´»æ€§ã€‚æ— è®ºæ˜¯è®­ç»ƒResNetï¼ˆè§†è§‰ï¼‰ã€BERTï¼ˆè¯­è¨€ï¼‰è¿˜æ˜¯å›¾ç¥ç»ç½‘ç»œï¼ˆå›¾æ•°æ®ï¼‰ï¼Œç›¸åŒçš„å¹¶è¡Œæ•°æ®åŸåˆ™éƒ½é€‚ç”¨ï¼Œæ— éœ€ä¿®æ”¹ã€‚è¿™ç§é€šç”¨æ€§ä½¿å…¶æˆä¸ºåˆ†å¸ƒå¼è®­ç»ƒçš„é»˜è®¤é€‰æ‹©ï¼Œå°¤å…¶æœ‰ä»·å€¼ã€‚
- en: Training time reduction is perhaps the most immediate benefit. Given proper
    implementation, data parallelism can achieve near-linear speedup with additional
    devices. Training that takes 100 hours on a single GPU might complete in roughly
    13 hours on 8 GPUs, assuming efficient gradient synchronization and minimal communication
    overhead.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ—¶é—´å‡å°‘å¯èƒ½æ˜¯æœ€ç›´æ¥çš„å¥½å¤„ã€‚åœ¨é€‚å½“çš„å®ç°ä¸‹ï¼Œæ•°æ®å¹¶è¡Œå¯ä»¥å®ç°æ¥è¿‘çº¿æ€§çš„åŠ é€Ÿï¼Œå¹¶éšç€é¢å¤–è®¾å¤‡çš„å¢åŠ ã€‚åœ¨å•ä¸ªGPUä¸Šéœ€è¦100å°æ—¶çš„è®­ç»ƒå¯èƒ½åœ¨8ä¸ªGPUä¸Šå¤§çº¦éœ€è¦13å°æ—¶å®Œæˆï¼Œå‡è®¾é«˜æ•ˆçš„æ¢¯åº¦åŒæ­¥å’Œæœ€å°çš„é€šä¿¡å¼€é”€ã€‚
- en: While these benefits make data parallelism compelling, achieving these advantages
    requires careful system design. Several challenges must be addressed to fully
    realize these benefits.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›å¥½å¤„ä½¿æ•°æ®å¹¶è¡Œå…·æœ‰å¸å¼•åŠ›ï¼Œä½†è¦å®ç°è¿™äº›ä¼˜åŠ¿éœ€è¦ä»”ç»†çš„ç³»ç»Ÿè®¾è®¡ã€‚å¿…é¡»è§£å†³å‡ ä¸ªæŒ‘æˆ˜æ‰èƒ½å®Œå…¨å®ç°è¿™äº›å¥½å¤„ã€‚
- en: '**GPT-2 Data Parallel Scaling: 1â†’8â†’32 GPUs**'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2æ•°æ®å¹¶è¡Œæ‰©å±•ï¼š1â†’8â†’32 GPU**'
- en: This example demonstrates how data parallelism scales in practice, including
    efficiency degradation.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­å±•ç¤ºäº†æ•°æ®å¹¶è¡Œåœ¨å®é™…ä¸­çš„æ‰©å±•æƒ…å†µï¼ŒåŒ…æ‹¬æ•ˆç‡ä¸‹é™ã€‚
- en: '**Single GPU Baseline**'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '**å•ä¸ªGPUåŸºçº¿**'
- en: 'Batch size: 16 (with gradient checkpointing, fits in 32GB)'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†å¤§å°ï¼š16ï¼ˆä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé€‚åˆ32GBï¼‰
- en: 'Time per step: 1.8 seconds'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ­¥æ—¶é—´ï¼š1.8ç§’
- en: 'Training throughput: ~9 samples/second'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒååé‡ï¼šçº¦9ä¸ªæ ·æœ¬/ç§’
- en: 'Time to 50K steps: **25 hours**'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ°50Kæ­¥çš„æ—¶é—´ï¼š**25å°æ—¶**
- en: '**8 GPUs: Single Node with NVLink**'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '**8ä¸ªGPUï¼šå•ä¸ªèŠ‚ç‚¹ä½¿ç”¨NVLink**'
- en: 'Configuration:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®ï¼š
- en: 'Per-GPU batch: 16, global batch: 128'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯GPUæ‰¹å¤„ç†ï¼š16ï¼Œå…¨å±€æ‰¹å¤„ç†ï¼š128
- en: 'Gradient synchronization: 3GB @ 600 GB/s (NVLink) = 5ms'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åŒæ­¥ï¼š3GB @ 600 GB/sï¼ˆNVLinkï¼‰= 5ms
- en: 'Performance results:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½ç»“æœï¼š
- en: 'Computation: 180ms per step'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—ï¼šæ¯æ­¥180ms
- en: 'Communication: 5ms per step'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šä¿¡ï¼šæ¯æ­¥5ms
- en: 'Total: 185ms per step'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»è®¡ï¼šæ¯æ­¥185ms
- en: 'Speedup: 1.8s Ã· 0.185s = 9.7Ã— (not quite 8Ã—)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿï¼š1.8s Ã· 0.185s = 9.7Ã—ï¼ˆå¹¶é8Ã—ï¼‰
- en: 'Parallel efficiency: 9.7 Ã· 8 = 121%'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶è¡Œæ•ˆç‡ï¼š9.7 Ã· 8 = 121%
- en: Why over 100% efficiency? Larger global batch (128 vs 16) improves GPU utilization
    from 72% to 89%. This â€œsuper-linearâ€ speedup is common in ML at small scales when
    the baseline has poor utilization.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¶…è¿‡100%çš„æ•ˆç‡ï¼Ÿæ›´å¤§çš„å…¨å±€æ‰¹å¤„ç†ï¼ˆ128æ¯”16ï¼‰å°†GPUåˆ©ç”¨ç‡ä»72%æé«˜åˆ°89%ã€‚è¿™ç§â€œè¶…çº¿æ€§â€åŠ é€Ÿåœ¨MLçš„å°è§„æ¨¡ä¸­å¾ˆå¸¸è§ï¼Œå½“åŸºçº¿åˆ©ç”¨ç‡è¾ƒå·®æ—¶ã€‚
- en: 'Training time: 25 hours Ã· 9.7 = **2.6 hours**'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ—¶é—´ï¼š25å°æ—¶ Ã· 9.7 = **2.6å°æ—¶**
- en: '**32 GPUs: 4 Nodes with InfiniBand**'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '**32ä¸ªGPUï¼š4ä¸ªèŠ‚ç‚¹ä½¿ç”¨InfiniBand**'
- en: 'Configuration:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®ï¼š
- en: 'Per-GPU batch: 16, global batch: 512'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯GPUæ‰¹å¤„ç†ï¼š16ï¼Œå…¨å±€æ‰¹å¤„ç†ï¼š512
- en: 'Intra-node communication: 5ms (NVLink)'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹å†…é€šä¿¡ï¼š5msï¼ˆNVLinkï¼‰
- en: 'Inter-node communication: 3GB @ 12.5 GB/s (InfiniBand) = 240ms'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹é—´é€šä¿¡ï¼š3GB @ 12.5 GB/sï¼ˆInfiniBandï¼‰= 240ms
- en: 'Performance results:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½ç»“æœï¼š
- en: 'Computation: 180ms (42% of time)'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—ï¼š180msï¼ˆå æ—¶é—´çš„42%ï¼‰
- en: 'Communication: 245ms (58% of time)'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šä¿¡ï¼š245msï¼ˆå æ—¶é—´çš„58%ï¼‰
- en: 'Total: 425ms per step'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»è®¡ï¼šæ¯æ­¥425ms
- en: 'Speedup: 1.8s Ã· 0.425s = 4.2Ã— faster â†’ 5.9 hours'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ é€Ÿï¼š1.8s Ã· 0.425s = 4.2Ã—æ›´å¿« â†’ 5.9å°æ—¶
- en: 'Parallel efficiency: 4.2 Ã· 32 = 13%'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶è¡Œæ•ˆç‡ï¼š4.2 Ã· 32 = 13%
- en: Communication dominates and becomes the bottleneck.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: é€šä¿¡æˆä¸ºä¸»å¯¼å¹¶æˆä¸ºç“¶é¢ˆã€‚
- en: '**Better Approach: 8 GPUs with Gradient Accumulation**'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´å¥½çš„æ–¹æ³•ï¼š8ä¸ªGPUä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**'
- en: 'Configuration: 8 GPUs Ã— batch 16 Ã— 4 accumulation steps = 512 effective batch'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é…ç½®ï¼š8ä¸ªGPU Ã— æ‰¹é‡16 Ã— 4ç´¯ç§¯æ­¥éª¤ = 512ä¸ªæœ‰æ•ˆæ‰¹é‡
- en: 'Communication overhead: 5ms Ã· (4 Ã— 180ms) = 0.7%'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šä¿¡å¼€é”€ï¼š5ms Ã· (4 Ã— 180ms) = 0.7%
- en: 'Training time: 3.8 hours'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ—¶é—´ï¼š3.8å°æ—¶
- en: 'Cost: $128/hour Ã— 3.8 hours = $486 vs.Â $3,021 for 32 GPUs'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆæœ¬ï¼š$128/å°æ—¶ Ã— 3.8å°æ—¶ = $486ï¼Œä¸32ä¸ªGPUçš„$3,021ç›¸æ¯”
- en: 'Savings: $2,535 (84% reduction) with only 1 hour longer training'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èŠ‚çœï¼š$2,535ï¼ˆå‡å°‘84%ï¼‰ï¼Œä»…å¤š1å°æ—¶è®­ç»ƒæ—¶é—´
- en: '**Key Insights**'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®è§è§£**'
- en: NVLink enables efficient scaling within single nodes (97% efficiency)
  id: totrans-861
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NVLinkä½¿å•ä¸ªèŠ‚ç‚¹å†…é«˜æ•ˆæ‰©å±•æˆä¸ºå¯èƒ½ï¼ˆ97%æ•ˆç‡ï¼‰
- en: Inter-node communication kills efficiency (drops to 13%)
  id: totrans-862
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹é—´é€šä¿¡æ‘§æ¯äº†æ•ˆç‡ï¼ˆé™è‡³13%ï¼‰
- en: Gradient accumulation beats naive scaling for memory-bound models
  id: totrans-863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯ä¼˜äºæœ´ç´ ç¼©æ”¾ï¼Œé€‚ç”¨äºå†…å­˜å—é™çš„æ¨¡å‹
- en: 'Sweet spot for GPT-2: 8 GPUs per node with gradient accumulation, not naive
    scaling to 32+ GPUs'
  id: totrans-864
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-2çš„æœ€ä½³ç‚¹ï¼šæ¯ä¸ªèŠ‚ç‚¹8ä¸ªGPUï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œè€Œä¸æ˜¯æœ´ç´ æ‰©å±•åˆ°32+ä¸ªGPU
- en: OpenAIâ€™s GPT-2 paper reports training on 32 V100s across 4 nodes using optimized
    communication (likely gradient accumulation combined with pipeline parallelism),
    not pure data parallelism.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„GPT-2è®ºæ–‡æŠ¥å‘Šäº†åœ¨4ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨ä¼˜åŒ–é€šä¿¡ï¼ˆå¯èƒ½æ˜¯æ¢¯åº¦ç´¯ç§¯ä¸æµæ°´çº¿å¹¶è¡Œç›¸ç»“åˆï¼‰åœ¨32ä¸ªV100ä¸Šè¿›è¡Œçš„è®­ç»ƒï¼Œè€Œä¸æ˜¯çº¯æ•°æ®å¹¶è¡Œã€‚
- en: Data Parallelism Limitations
  id: totrans-866
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œå±€é™æ€§
- en: While data parallelism is an effective approach for distributed training, it
    introduces several challenges that must be addressed to achieve efficient and
    scalable training systems. These challenges stem from the inherent trade-offs
    between computation and communication, as well as the limitations imposed by hardware
    and network infrastructures.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ•°æ®å¹¶è¡Œæ˜¯åˆ†å¸ƒå¼è®­ç»ƒçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å®ƒå¼•å…¥äº†å‡ ä¸ªæŒ‘æˆ˜ï¼Œå¿…é¡»è§£å†³è¿™äº›æŒ‘æˆ˜æ‰èƒ½å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•çš„è®­ç»ƒç³»ç»Ÿã€‚è¿™äº›æŒ‘æˆ˜æºäºè®¡ç®—å’Œé€šä¿¡ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ï¼Œä»¥åŠç¡¬ä»¶å’Œç½‘ç»œåŸºç¡€è®¾æ–½æ–½åŠ çš„é™åˆ¶ã€‚
- en: Communication overhead represents the most significant bottleneck in data parallelism.
    During gradient synchronization, each device must exchange gradient updates, often
    hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter
    model, each synchronization step might require transferring several gigabytes
    of data across the network. While high-speed interconnects like NVLink (300 GB/s)
    or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCLâ€™s ring-allreduce
    algorithm[34](#fn34) reduces this burden by organizing devices in a ring topology,
    but communication costs still grow with model size and device count.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: é€šä¿¡å¼€é”€æ˜¯æ•°æ®å¹¶è¡Œä¸­æœ€æ˜¾è‘—çš„ç“¶é¢ˆã€‚åœ¨æ¢¯åº¦åŒæ­¥æœŸé—´ï¼Œæ¯ä¸ªè®¾å¤‡å¿…é¡»äº¤æ¢æ¢¯åº¦æ›´æ–°ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼Œæ¯ä¸€æ­¥å¯èƒ½éœ€è¦ä¼ è¾“æ•°ç™¾å…†å­—èŠ‚çš„æ•°æ®ã€‚ä½¿ç”¨8ä¸ªGPUè®­ç»ƒä¸€ä¸ª10äº¿å‚æ•°çš„æ¨¡å‹æ—¶ï¼Œæ¯ä¸ªåŒæ­¥æ­¥éª¤å¯èƒ½éœ€è¦åœ¨ç½‘ç»œä¸Šä¼ è¾“æ•°å‰å­—èŠ‚çš„æ•°æ®ã€‚è™½ç„¶é«˜é€Ÿäº’è¿å¦‚NVLinkï¼ˆ300
    GB/sï¼‰æˆ–InfiniBandï¼ˆ200 Gb/sï¼‰æœ‰æ‰€å¸®åŠ©ï¼Œä½†å¼€é”€ä»ç„¶å¾ˆå¤§ã€‚NCCLçš„ç¯-allreduceç®—æ³•[34](#fn34)é€šè¿‡å°†è®¾å¤‡ç»„ç»‡æˆç¯å½¢æ‹“æ‰‘æ¥å‡è½»è¿™ç§è´Ÿæ‹…ï¼Œä½†é€šä¿¡æˆæœ¬ä»ç„¶éšç€æ¨¡å‹å¤§å°å’Œè®¾å¤‡æ•°é‡çš„å¢åŠ è€Œå¢é•¿ã€‚
- en: Scalability limitations become apparent as device count increases. While 8 GPUs
    might achieve <semantics><mrow><mn>7</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">7\times</annotation></semantics>
    speedup (87.5% scaling efficiency), scaling to 64 GPUs typically yields only 45-50<semantics><mi>Ã—</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> speedup (70-78% efficiency)
    due to growing synchronization costs. Scaling efficiency, calculated as speedup
    divided by the number of devicesâ€”quantifies how effectively additional hardware
    translates to reduced training time. Perfect linear scaling would yield 100% efficiency,
    but communication overhead and synchronization barriers typically degrade efficiency
    as device count grows. This non-linear scaling means that doubling the number
    of devices rarely halves the training time, particularly in configurations exceeding
    16-32 devices.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è®¾å¤‡æ•°é‡çš„å¢åŠ ï¼Œå¯æ‰©å±•æ€§é™åˆ¶å˜å¾—æ˜æ˜¾ã€‚è™½ç„¶8ä¸ªGPUå¯èƒ½å®ç°<semantics><mrow><mn>7</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">7\times</annotation></semantics>çš„é€Ÿåº¦æå‡ï¼ˆ87.5%çš„æ‰©å±•æ•ˆç‡ï¼‰ï¼Œä½†æ‰©å±•åˆ°64ä¸ªGPUé€šå¸¸åªèƒ½æä¾›45-50<semantics><mi>Ã—</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>çš„é€Ÿåº¦æå‡ï¼ˆ70-78%çš„æ•ˆç‡ï¼‰ï¼Œè¿™æ˜¯å› ä¸ºåŒæ­¥æˆæœ¬ä¸æ–­å¢åŠ ã€‚æ‰©å±•æ•ˆç‡ï¼Œå³é€Ÿåº¦æå‡é™¤ä»¥è®¾å¤‡æ•°é‡ï¼Œè¡¡é‡äº†é¢å¤–ç¡¬ä»¶å¦‚ä½•æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºå‡å°‘è®­ç»ƒæ—¶é—´ã€‚å®Œç¾çš„çº¿æ€§æ‰©å±•å°†äº§ç”Ÿ100%çš„æ•ˆç‡ï¼Œä½†é€šä¿¡å¼€é”€å’ŒåŒæ­¥éšœç¢é€šå¸¸éšç€è®¾å¤‡æ•°é‡çš„å¢åŠ è€Œé™ä½æ•ˆç‡ã€‚è¿™ç§éçº¿æ€§æ‰©å±•æ„å‘³ç€è®¾å¤‡æ•°é‡åŠ å€å¾ˆå°‘èƒ½å°†è®­ç»ƒæ—¶é—´å‡åŠï¼Œå°¤å…¶æ˜¯åœ¨è¶…è¿‡16-32ä¸ªè®¾å¤‡çš„é…ç½®ä¸­ã€‚
- en: Memory constraints present a hard limit for data parallelism. Consider a transformer
    model with 175 billion parameters, which requires approximately 350 GB just to
    store model parameters in FP32\. When accounting for optimizer states and activation
    memories, the total requirement often exceeds 1 TB per device. Since even high-end
    GPUs typically offer 80 GB or less, such models cannot use pure data parallelism.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜é™åˆ¶å¯¹æ•°æ®å¹¶è¡Œæ„æˆäº†ç¡¬æ€§é™åˆ¶ã€‚è€ƒè™‘ä¸€ä¸ªå…·æœ‰1750äº¿å‚æ•°çš„Transformeræ¨¡å‹ï¼Œä»…å­˜å‚¨æ¨¡å‹å‚æ•°åœ¨FP32æ ¼å¼ä¸‹å°±éœ€è¦å¤§çº¦350 GBã€‚è€ƒè™‘åˆ°ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¿€æ´»å†…å­˜ï¼Œæ€»éœ€æ±‚é€šå¸¸è¶…è¿‡æ¯ä¸ªè®¾å¤‡1
    TBã€‚ç”±äºå³ä½¿æ˜¯é«˜ç«¯GPUé€šå¸¸ä¹Ÿåªæä¾›80 GBæˆ–æ›´å°‘ï¼Œå› æ­¤æ­¤ç±»æ¨¡å‹ä¸èƒ½ä½¿ç”¨çº¯æ•°æ®å¹¶è¡Œã€‚
- en: Workload imbalance affects heterogeneous systems significantly. In a cluster
    mixing A100 and V100 GPUs, the A100s might process batches <semantics><mrow><mn>1.7</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">1.7\times</annotation></semantics> faster, forcing
    them to wait for the V100s to catch up. This idle time can reduce cluster utilization
    by 20-30% without proper load balancing mechanisms.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œè´Ÿè½½ä¸å¹³è¡¡å¯¹å¼‚æ„ç³»ç»Ÿå½±å“æ˜¾è‘—ã€‚åœ¨ä¸€ä¸ªæ··åˆA100å’ŒV100 GPUçš„é›†ç¾¤ä¸­ï¼ŒA100så¯èƒ½å¤„ç†æ‰¹æ¬¡çš„é€Ÿåº¦æ¯”V100så¿«<semantics><mrow><mn>1.7</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">1.7\times</annotation></semantics>ï¼Œè¿«ä½¿å®ƒä»¬ç­‰å¾…V100sèµ¶ä¸Šã€‚è¿™ç§ç©ºé—²æ—¶é—´åœ¨æ²¡æœ‰é€‚å½“çš„è´Ÿè½½å¹³è¡¡æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šé™ä½é›†ç¾¤åˆ©ç”¨ç‡20-30%ã€‚
- en: 'Finally, there are critical challenges related to fault tolerance and reliability
    in distributed training systems. Node failures become inevitable at scale: with
    100 GPUs running continuously, hardware failures occur multiple times per week
    as detailed in [ChapterÂ 16](ch022.xhtml#sec-robust-ai). A training run that costs
    millions of dollars cannot restart from scratch each time a single GPU fails.
    Modern distributed training systems implement sophisticated checkpointing strategies,
    storing model state every N iterations to minimize lost computation. Checkpoint
    frequency creates trade-offs: frequent checkpointing reduces the potential loss
    from failures but increases storage I/O overhead and training latency. Production
    systems typically checkpoint every 100-1000 iterations, balancing fault tolerance
    against performance.'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ä¸å®¹é”™å’Œå¯é æ€§ç›¸å…³çš„å…³é”®æŒ‘æˆ˜ã€‚åœ¨è§„æ¨¡æ‰©å¤§æ—¶ï¼ŒèŠ‚ç‚¹æ•…éšœå˜å¾—ä¸å¯é¿å…ï¼šå½“æœ‰100ä¸ªGPUæŒç»­è¿è¡Œæ—¶ï¼Œæ¯å‘¨ä¼šå‘ç”Ÿå¤šæ¬¡ç¡¬ä»¶æ•…éšœï¼Œå¦‚[ç¬¬16ç« ](ch022.xhtml#sec-robust-ai)ä¸­è¯¦ç»†æ‰€è¿°ã€‚ä¸€æ¬¡èŠ±è´¹æ•°ç™¾ä¸‡ç¾å…ƒçš„è®­ç»ƒè¿è¡Œä¸èƒ½æ¯æ¬¡å•ä¸ªGPUæ•…éšœæ—¶éƒ½ä»å¤´å¼€å§‹ã€‚ç°ä»£åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå®ç°äº†å¤æ‚çš„æ£€æŸ¥ç‚¹ç­–ç•¥ï¼Œæ¯Næ¬¡è¿­ä»£å­˜å‚¨ä¸€æ¬¡æ¨¡å‹çŠ¶æ€ä»¥æœ€å°åŒ–ä¸¢å¤±çš„è®¡ç®—ã€‚æ£€æŸ¥ç‚¹é¢‘ç‡ä¼šäº§ç”Ÿæƒè¡¡ï¼šé¢‘ç¹çš„æ£€æŸ¥ç‚¹å‡å°‘äº†æ•…éšœçš„æ½œåœ¨æŸå¤±ï¼Œä½†å¢åŠ äº†å­˜å‚¨I/Oå¼€é”€å’Œè®­ç»ƒå»¶è¿Ÿã€‚ç”Ÿäº§ç³»ç»Ÿé€šå¸¸æ¯100-1000æ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼Œåœ¨å®¹é”™å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚
- en: Implementation complexity compounds these reliability challenges. While modern
    frameworks abstract much of the complexity, implementing robust distributed training
    systems requires significant engineering expertise. Graceful degradation when
    subsets of nodes fail, consistent gradient synchronization despite network partitions,
    and automatic recovery from transient failures demand deep understanding of both
    machine learning and distributed systems principles.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å¤æ‚æ€§åŠ å‰§äº†è¿™äº›å¯é æ€§æŒ‘æˆ˜ã€‚è™½ç„¶ç°ä»£æ¡†æ¶æŠ½è±¡äº†è®¸å¤šå¤æ‚æ€§ï¼Œä½†å®ç°å¥å£®çš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿéœ€è¦æ˜¾è‘—çš„å·¥ç¨‹ä¸“ä¸šçŸ¥è¯†ã€‚å½“èŠ‚ç‚¹å­é›†å¤±è´¥æ—¶å®ç°ä¼˜é›…é™çº§ã€åœ¨ç½‘ç»œåˆ†åŒºçš„æƒ…å†µä¸‹ä¿æŒæ¢¯åº¦åŒæ­¥ï¼Œä»¥åŠä»ç¬æ—¶æ•…éšœä¸­è‡ªåŠ¨æ¢å¤ï¼Œéƒ½éœ€è¦å¯¹æœºå™¨å­¦ä¹ å’Œåˆ†å¸ƒå¼ç³»ç»ŸåŸç†æœ‰æ·±å…¥çš„ç†è§£ã€‚
- en: Despite these challenges, data parallelism remains an important technique for
    distributed training, with many strategies available to address its limitations.
    Monitoring these distributed systems requires specialized tooling for tracking
    gradient norms, communication patterns, and hardware utilization across nodesâ€”production
    monitoring strategies are covered in [ChapterÂ 13](ch019.xhtml#sec-ml-operations),
    while system-level failure handling and training reliability are addressed in
    [ChapterÂ 16](ch022.xhtml#sec-robust-ai). Model parallelism provides another strategy
    for scaling training that is particularly well-suited for handling extremely large
    models that cannot fit on a single device.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ•°æ®å¹¶è¡Œæ€§ä»ç„¶æ˜¯åˆ†å¸ƒå¼è®­ç»ƒçš„é‡è¦æŠ€æœ¯ï¼Œæœ‰è®¸å¤šç­–ç•¥å¯ä»¥è§£å†³å…¶å±€é™æ€§ã€‚ç›‘æ§è¿™äº›åˆ†å¸ƒå¼ç³»ç»Ÿéœ€è¦ä¸“é—¨çš„å·¥å…·æ¥è·Ÿè¸ªèŠ‚ç‚¹é—´çš„æ¢¯åº¦èŒƒæ•°ã€é€šä¿¡æ¨¡å¼å’Œç¡¬ä»¶åˆ©ç”¨ç‡â€”â€”ç”Ÿäº§ç›‘æ§ç­–ç•¥åœ¨[ç¬¬13ç« ](ch019.xhtml#sec-ml-operations)ä¸­æœ‰ä»‹ç»ï¼Œè€Œç³»ç»Ÿçº§æ•…éšœå¤„ç†å’Œè®­ç»ƒå¯é æ€§åœ¨[ç¬¬16ç« ](ch022.xhtml#sec-robust-ai)ä¸­æœ‰æ‰€è®¨è®ºã€‚æ¨¡å‹å¹¶è¡Œæ€§ä¸ºæ‰©å±•è®­ç»ƒæä¾›äº†å¦ä¸€ç§ç­–ç•¥ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†æ— æ³•é€‚åº”å•ä¸ªè®¾å¤‡çš„æå¤§å‹æ¨¡å‹ã€‚
- en: Model Parallelism
  id: totrans-875
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§
- en: While data parallelism scales dataset processing, some models themselves exceed
    the memory capacity of individual devices. Model parallelism splits neural networks
    across multiple computing devices when the modelâ€™s parameters exceed single-device
    memory limits. Unlike data parallelism, where each device contains a complete
    model copy, model parallelism assigns different model components to different
    devices ([Shazeer, Mirhoseini, Maziarz, Davis, et al. 2017](ch058.xhtml#ref-shazeer_mixture_of_experts_2017)).
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ•°æ®å¹¶è¡Œæ€§å¯ä»¥æ‰©å±•æ•°æ®é›†çš„å¤„ç†ï¼Œä½†æŸäº›æ¨¡å‹æœ¬èº«è¶…è¿‡äº†å•ä¸ªè®¾å¤‡çš„å†…å­˜å®¹é‡ã€‚å½“æ¨¡å‹çš„å‚æ•°è¶…è¿‡å•ä¸ªè®¾å¤‡çš„å†…å­˜é™åˆ¶æ—¶ï¼Œæ¨¡å‹å¹¶è¡Œæ€§å°†ç¥ç»ç½‘ç»œåˆ†å‰²åˆ°å¤šä¸ªè®¡ç®—è®¾å¤‡ä¸Šã€‚ä¸æ•°æ®å¹¶è¡Œæ€§ä¸åŒï¼Œå…¶ä¸­æ¯ä¸ªè®¾å¤‡éƒ½åŒ…å«å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ï¼Œæ¨¡å‹å¹¶è¡Œæ€§å°†ä¸åŒçš„æ¨¡å‹ç»„ä»¶åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ä¸Šï¼ˆ[Shazeer,
    Mirhoseini, Maziarz, Davis, ç­‰äºº 2017](ch058.xhtml#ref-shazeer_mixture_of_experts_2017)ï¼‰ã€‚
- en: Several implementations of model parallelism exist. In layer-based splitting,
    devices process distinct groups of layers sequentially. For instance, the first
    device might compute layers 1-4 while the second handles layers 5-8\. Channel-based
    splitting divides the channels within each layer across devices, such as the first
    device processing 512 channels while the second manages the remaining ones. For
    transformer architectures, attention head splitting distributes different attention
    heads to separate devices.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨å¤šç§æ¨¡å‹å¹¶è¡Œå®ç°ã€‚åŸºäºå±‚çš„åˆ’åˆ†ä¸­ï¼Œè®¾å¤‡æŒ‰é¡ºåºå¤„ç†ä¸åŒçš„å±‚ç»„ã€‚ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªè®¾å¤‡å¯èƒ½è®¡ç®—å±‚1-4ï¼Œè€Œç¬¬äºŒä¸ªè®¾å¤‡å¤„ç†å±‚5-8ã€‚åŸºäºé€šé“çš„åˆ’åˆ†å°†æ¯ä¸ªå±‚å†…çš„é€šé“åˆ†å¸ƒåœ¨è®¾å¤‡ä¹‹é—´ï¼Œä¾‹å¦‚ç¬¬ä¸€ä¸ªè®¾å¤‡å¤„ç†512ä¸ªé€šé“ï¼Œè€Œç¬¬äºŒä¸ªè®¾å¤‡ç®¡ç†å‰©ä½™çš„é€šé“ã€‚å¯¹äºTransformeræ¶æ„ï¼Œæ³¨æ„åŠ›å¤´åˆ’åˆ†å°†ä¸åŒçš„æ³¨æ„åŠ›å¤´åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ä¸Šã€‚
- en: This distribution method enables training of large-scale models. GPT-3, with
    175 billion parameters, relies on model parallelism for training. Vision transformers
    processing high-resolution 16k <semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    16k pixel images use model parallelism to manage memory constraints. Mixture-of-Expert
    architectures use this approach to distribute their conditional computation paths
    across hardware.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åˆ†å¸ƒæ–¹æ³•ä½¿å¾—å¯ä»¥è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ã€‚æ‹¥æœ‰1750äº¿å‚æ•°çš„GPT-3ä¾èµ–æ¨¡å‹å¹¶è¡Œè¿›è¡Œè®­ç»ƒã€‚å¤„ç†é«˜åˆ†è¾¨ç‡16k <semantics><mi>Ã—</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> 16kåƒç´ å›¾åƒçš„è§†è§‰Transformerä½¿ç”¨æ¨¡å‹å¹¶è¡Œæ¥ç®¡ç†å†…å­˜é™åˆ¶ã€‚æ··åˆä¸“å®¶æ¶æ„ä½¿ç”¨è¿™ç§æ–¹æ³•åœ¨å…¶æ¡ä»¶è®¡ç®—è·¯å¾„ä¸Šè·¨ç¡¬ä»¶è¿›è¡Œåˆ†å¸ƒã€‚
- en: Device coordination follows a specific pattern during training. In the forward
    pass, data flows sequentially through model segments on different devices. The
    backward pass propagates gradients in reverse order through these segments. During
    parameter updates, each device modifies only its assigned portion of the model.
    This coordination ensures mathematical equivalence to training on a single device
    while enabling the handling of models that exceed individual device memory capacities.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®¾å¤‡åè°ƒéµå¾ªç‰¹å®šçš„æ¨¡å¼ã€‚åœ¨æ­£å‘ä¼ æ’­ä¸­ï¼Œæ•°æ®æŒ‰é¡ºåºé€šè¿‡ä¸åŒè®¾å¤‡ä¸Šçš„æ¨¡å‹æ®µã€‚åå‘ä¼ æ’­é€šè¿‡è¿™äº›æ®µä»¥ç›¸åçš„é¡ºåºä¼ æ’­æ¢¯åº¦ã€‚åœ¨å‚æ•°æ›´æ–°æœŸé—´ï¼Œæ¯ä¸ªè®¾å¤‡ä»…ä¿®æ”¹å…¶åˆ†é…çš„æ¨¡å‹éƒ¨åˆ†ã€‚è¿™ç§åè°ƒç¡®ä¿äº†ä¸å•ä¸ªè®¾å¤‡ä¸Šè®­ç»ƒçš„æ•°å­¦ç­‰ä»·æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿå¤„ç†è¶…è¿‡å•ä¸ªè®¾å¤‡å†…å­˜å®¹é‡çš„æ¨¡å‹ã€‚
- en: Model Parallelism Implementation
  id: totrans-880
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œå®ç°
- en: 'Model parallelism divides neural networks across multiple computing devices,
    with each device computing a distinct portion of the modelâ€™s operations. This
    division allows training of models whose parameter counts exceed single-device
    memory capacity. The technique encompasses device coordination, data flow management,
    and gradient computation across distributed model segments. The mechanics of model
    parallelism are illustrated in [FigureÂ 8.14](ch014.xhtml#fig-model-parallelism).
    These steps are described next:'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œå°†ç¥ç»ç½‘ç»œåˆ†å¸ƒåœ¨å¤šä¸ªè®¡ç®—è®¾å¤‡ä¸Šï¼Œæ¯ä¸ªè®¾å¤‡è®¡ç®—æ¨¡å‹æ“ä½œçš„ç‰¹å®šéƒ¨åˆ†ã€‚è¿™ç§åˆ’åˆ†å…è®¸è®­ç»ƒå‚æ•°æ•°é‡è¶…è¿‡å•ä¸ªè®¾å¤‡å†…å­˜å®¹é‡çš„æ¨¡å‹ã€‚è¯¥æŠ€æœ¯åŒ…æ‹¬è®¾å¤‡åè°ƒã€æ•°æ®æµç®¡ç†å’Œåˆ†å¸ƒå¼æ¨¡å‹æ®µé—´çš„æ¢¯åº¦è®¡ç®—ã€‚æ¨¡å‹å¹¶è¡Œçš„æœºåˆ¶åœ¨[å›¾8.14](ch014.xhtml#fig-model-parallelism)ä¸­å±•ç¤ºã€‚ä»¥ä¸‹æ­¥éª¤å°†è¿›è¡Œæè¿°ï¼š
- en: '![](../media/file121.svg)'
  id: totrans-882
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file121.svg)'
- en: 'FigureÂ 8.14: **Model Partitioning**: Distributing a neural network across multiple
    devices enables training models larger than the memory capacity of a single device.
    This approach requires careful coordination of data flow and gradient computation
    between devices to maintain training efficiency.'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.14ï¼š**æ¨¡å‹åˆ’åˆ†**ï¼šå°†ç¥ç»ç½‘ç»œåˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šï¼Œä½¿å¾—å¯ä»¥è®­ç»ƒæ¯”å•ä¸ªè®¾å¤‡å†…å­˜å®¹é‡æ›´å¤§çš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•éœ€è¦ä»”ç»†åè°ƒè®¾å¤‡é—´çš„æ•°æ®æµå’Œæ¢¯åº¦è®¡ç®—ï¼Œä»¥ä¿æŒè®­ç»ƒæ•ˆç‡ã€‚
- en: Model Partitioning
  id: totrans-884
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¨¡å‹åˆ’åˆ†
- en: The first step in model parallelism is dividing the model into smaller segments.
    For instance, in a deep neural network, layers are often divided among devices.
    In a system with two GPUs, the first half of the layers might reside on GPU 1,
    while the second half resides on GPU 2\. Another approach is to split computations
    within a single layer, such as dividing matrix multiplications in transformer
    models across devices.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œç¬¬ä¸€æ­¥æ˜¯å°†æ¨¡å‹åˆ’åˆ†ä¸ºæ›´å°çš„æ®µã€‚ä¾‹å¦‚ï¼Œåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œå±‚é€šå¸¸åˆ†å¸ƒåœ¨è®¾å¤‡ä¹‹é—´ã€‚åœ¨å…·æœ‰ä¸¤ä¸ªGPUçš„ç³»ç»Ÿä¸Šï¼Œå‰åŠéƒ¨åˆ†å±‚å¯èƒ½ä½äºGPU 1ä¸Šï¼Œè€ŒååŠéƒ¨åˆ†å±‚ä½äºGPU
    2ä¸Šã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åˆ†å‰²å•ä¸ªå±‚å†…çš„è®¡ç®—ï¼Œä¾‹å¦‚åœ¨Transformeræ¨¡å‹ä¸­å°†çŸ©é˜µä¹˜æ³•åˆ†å¸ƒåœ¨è®¾å¤‡ä¹‹é—´ã€‚
- en: Model Forward Pass
  id: totrans-886
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ­£å‘ä¼ æ’­
- en: During the forward pass, data flows sequentially through the partitions. For
    example, data processed by the first set of layers on GPU 1 is sent to GPU 2 for
    processing by the next set of layers. This sequential flow ensures that the entire
    model is used, even though it is distributed across multiple devices. Efficient
    inter-device communication is important to minimize delays during this step ([Research
    2021](ch058.xhtml#ref-deepspeed_training_system_2021)).
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å‘ä¼ æ’­æœŸé—´ï¼Œæ•°æ®æŒ‰é¡ºåºé€šè¿‡åˆ†åŒºæµåŠ¨ã€‚ä¾‹å¦‚ï¼ŒGPU 1ä¸Šç¬¬ä¸€ç»„å±‚å¤„ç†çš„æ•°æ®è¢«å‘é€åˆ°GPU 2ï¼Œä»¥ä¾¿ä¸‹ä¸€ç»„å±‚è¿›è¡Œå¤„ç†ã€‚è¿™ç§é¡ºåºæµåŠ¨ç¡®ä¿äº†æ•´ä¸ªæ¨¡å‹è¢«ä½¿ç”¨ï¼Œå³ä½¿å®ƒåˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šã€‚é«˜æ•ˆçš„è·¨è®¾å¤‡é€šä¿¡å¯¹äºæœ€å°åŒ–æ­¤æ­¥éª¤ä¸­çš„å»¶è¿Ÿå¾ˆé‡è¦ï¼ˆ[ç ”ç©¶2021](ch058.xhtml#ref-deepspeed_training_system_2021)ï¼‰ã€‚
- en: Backward Pass and Calculation
  id: totrans-888
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­å’Œè®¡ç®—
- en: The backward pass computes gradients through the distributed model segments
    in reverse order. Each device calculates local gradients for its parameters and
    propagates necessary gradient information to previous devices. In transformer
    models, this means backpropagating through attention computations and feed-forward
    networks across device boundaries.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­é€šè¿‡åˆ†å¸ƒå¼æ¨¡å‹æ®µä»¥ç›¸åçš„é¡ºåºè®¡ç®—æ¢¯åº¦ã€‚æ¯ä¸ªè®¾å¤‡è®¡ç®—å…¶å‚æ•°çš„å±€éƒ¨æ¢¯åº¦ï¼Œå¹¶å°†å¿…è¦çš„æ¢¯åº¦ä¿¡æ¯ä¼ æ’­åˆ°ä¹‹å‰çš„è®¾å¤‡ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼Œè¿™æ„å‘³ç€åœ¨è®¾å¤‡è¾¹ç•Œä¹‹é—´åå‘ä¼ æ’­é€šè¿‡æ³¨æ„åŠ›è®¡ç®—å’Œå‰é¦ˆç½‘ç»œã€‚
- en: 'For example, in a two-device setup with attention mechanisms split between
    devices, the backward computation works as follows: The second device computes
    gradients for the final feed-forward layers and attention heads. It then sends
    the gradient tensors for the attention output to the first device. The first device
    uses these received gradients to compute updates for its attention parameters
    and earlier layer weights.'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªå…·æœ‰æ³¨æ„åŠ›æœºåˆ¶åœ¨è®¾å¤‡ä¹‹é—´åˆ†å‰²çš„ä¸¤ä¸ªè®¾å¤‡è®¾ç½®ä¸­ï¼Œåå‘è®¡ç®—çš„å·¥ä½œæ–¹å¼å¦‚ä¸‹ï¼šç¬¬äºŒä¸ªè®¾å¤‡è®¡ç®—æœ€ç»ˆå‰é¦ˆå±‚å’Œæ³¨æ„åŠ›å¤´çš„æ¢¯åº¦ã€‚ç„¶åï¼Œå®ƒå°†æ³¨æ„åŠ›è¾“å‡ºçš„æ¢¯åº¦å¼ é‡å‘é€åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡ã€‚ç¬¬ä¸€ä¸ªè®¾å¤‡ä½¿ç”¨è¿™äº›æ¥æ”¶åˆ°çš„æ¢¯åº¦æ¥è®¡ç®—å…¶æ³¨æ„åŠ›å‚æ•°å’Œæ—©æœŸå±‚æƒé‡çš„æ›´æ–°ã€‚
- en: Parameter Updates
  id: totrans-891
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°
- en: Parameter updates occur independently on each device using the computed gradients
    and an optimization algorithm. A device holding attention layer parameters applies
    updates using only the gradients computed for those specific parameters. This
    localized update approach differs from data parallelism, which requires gradient
    averaging across devices.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šç‹¬ç«‹å‘ç”Ÿï¼Œä½¿ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦å’Œä¼˜åŒ–ç®—æ³•ã€‚æŒæœ‰æ³¨æ„åŠ›å±‚å‚æ•°çš„è®¾å¤‡ä»…ä½¿ç”¨ä¸ºè¿™äº›ç‰¹å®šå‚æ•°è®¡ç®—å‡ºçš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚è¿™ç§å±€éƒ¨æ›´æ–°æ–¹æ³•ä¸éœ€è¦è·¨è®¾å¤‡æ¢¯åº¦å¹³å‡çš„æ•°æ®å¹¶è¡Œä¸åŒã€‚
- en: 'The optimization step proceeds as follows: Each device applies its chosen optimizer
    (such as Adam or AdaFactor) to update its portion of the model parameters. A device
    holding the first six transformer layers updates only those layersâ€™ weights and
    biases. This local parameter update eliminates the need for cross-device synchronization
    during the optimization step, reducing communication overhead.'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–æ­¥éª¤å¦‚ä¸‹ï¼šæ¯ä¸ªè®¾å¤‡åº”ç”¨å…¶é€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamæˆ–AdaFactorï¼‰æ¥æ›´æ–°å…¶æ¨¡å‹å‚æ•°çš„éƒ¨åˆ†ã€‚æŒæœ‰å‰å…­ä¸ªTransformerå±‚çš„è®¾å¤‡ä»…æ›´æ–°è¿™äº›å±‚çš„æƒé‡å’Œåå·®ã€‚è¿™ç§å±€éƒ¨å‚æ•°æ›´æ–°æ¶ˆé™¤äº†åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è·¨è®¾å¤‡åŒæ­¥çš„éœ€æ±‚ï¼Œä»è€Œå‡å°‘äº†é€šä¿¡å¼€é”€ã€‚
- en: Iterative Process
  id: totrans-894
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è¿­ä»£è¿‡ç¨‹
- en: Like other training strategies, model parallelism repeats these steps for every
    batch of data. As the dataset is processed over multiple iterations, the distributed
    model converges toward optimal performance.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–è®­ç»ƒç­–ç•¥ä¸€æ ·ï¼Œæ¨¡å‹å¹¶è¡Œå¯¹æ¯ä¸ªæ•°æ®æ‰¹æ¬¡é‡å¤è¿™äº›æ­¥éª¤ã€‚éšç€æ•°æ®é›†åœ¨å¤šæ¬¡è¿­ä»£ä¸­å¤„ç†ï¼Œåˆ†å¸ƒå¼æ¨¡å‹è¶‹å‘äºæœ€ä½³æ€§èƒ½ã€‚
- en: Parallelism Variations
  id: totrans-896
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å¹¶è¡Œå˜ä½“
- en: Model parallelism can be implemented through different strategies for dividing
    the model across devices. The three primary approaches are layer-wise partitioning,
    operator-level partitioning, and pipeline parallelism, each suited to different
    model structures and computational needs.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œå¯ä»¥é€šè¿‡ä¸åŒçš„ç­–ç•¥æ¥å®ç°ï¼Œè¿™äº›ç­–ç•¥ç”¨äºåœ¨è®¾å¤‡ä¹‹é—´åˆ’åˆ†æ¨¡å‹ã€‚ä¸‰ç§ä¸»è¦æ–¹æ³•ä¸ºå±‚åˆ’åˆ†ã€æ“ä½œçº§åˆ’åˆ†å’Œç®¡é“å¹¶è¡Œï¼Œæ¯ç§æ–¹æ³•éƒ½é€‚åˆä¸åŒçš„æ¨¡å‹ç»“æ„å’Œè®¡ç®—éœ€æ±‚ã€‚
- en: Layer-wise Partitioning
  id: totrans-898
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å±‚åˆ’åˆ†
- en: Layer-wise partitioning assigns distinct model layers to separate computing
    devices. In transformer architectures, this translates to specific devices managing
    defined sets of attention and feed-forward blocks. As illustrated in [FigureÂ 8.15](ch014.xhtml#fig-layers-blocks),
    a 24-layer transformer model distributed across four devices assigns six consecutive
    transformer blocks to each device. Device 1 processes blocks 1-6, device 2 handles
    blocks 7-12, and so forth.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚çº§åˆ’åˆ†å°†ä¸åŒçš„æ¨¡å‹å±‚åˆ†é…åˆ°ç‹¬ç«‹çš„è®¡ç®—è®¾å¤‡ä¸Šã€‚åœ¨è½¬æ¢å™¨æ¶æ„ä¸­ï¼Œè¿™æ„å‘³ç€ç‰¹å®šçš„è®¾å¤‡ç®¡ç†å®šä¹‰å¥½çš„æ³¨æ„åŠ›å’Œå‰é¦ˆå—é›†åˆã€‚å¦‚å›¾8.15æ‰€ç¤ºï¼Œä¸€ä¸ªåˆ†å¸ƒåœ¨å››ä¸ªè®¾å¤‡ä¸Šçš„24å±‚è½¬æ¢å™¨æ¨¡å‹å°†å…­ä¸ªè¿ç»­çš„è½¬æ¢å™¨å—åˆ†é…ç»™æ¯ä¸ªè®¾å¤‡ã€‚è®¾å¤‡1å¤„ç†å—1-6ï¼Œè®¾å¤‡2å¤„ç†å—7-12ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- en: '![](../media/file122.svg)'
  id: totrans-900
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file122.svg)'
- en: 'FigureÂ 8.15: **Layer-Wise Model Parallelism**: Distributing a transformer model
    across multiple gpus assigns consecutive layers to each device, enabling parallel
    processing of input data and accelerating training. This partitioning strategy
    allows each GPU to operate on a subset of the modelâ€™s layers, reducing the memory
    footprint and computational load per device.'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.15ï¼š**åˆ†å±‚æ¨¡å‹å¹¶è¡Œ**ï¼šå°†è½¬æ¢å™¨æ¨¡å‹åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œå°†è¿ç»­çš„å±‚åˆ†é…ç»™æ¯ä¸ªè®¾å¤‡ï¼Œä»è€Œå®ç°è¾“å…¥æ•°æ®çš„å¹¶è¡Œå¤„ç†å¹¶åŠ é€Ÿè®­ç»ƒã€‚è¿™ç§åˆ†åŒºç­–ç•¥å…è®¸æ¯ä¸ªGPUæ“ä½œæ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ï¼Œå‡å°‘æ¯ä¸ªè®¾å¤‡çš„å†…å­˜å ç”¨å’Œè®¡ç®—è´Ÿè½½ã€‚
- en: This sequential processing introduces device idle time, as each device must
    wait for the previous device to complete its computation before beginning work.
    For example, while device 1 processes the initial blocks, devices 2, 3, and 4
    remain inactive. Similarly, when device 2 begins its computation, device 1 sits
    idle. This pattern of waiting and idle time reduces hardware utilization efficiency
    compared to other parallelization strategies.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é¡ºåºå¤„ç†å¼•å…¥äº†è®¾å¤‡ç©ºé—²æ—¶é—´ï¼Œå› ä¸ºæ¯ä¸ªè®¾å¤‡å¿…é¡»ç­‰å¾…å‰ä¸€ä¸ªè®¾å¤‡å®Œæˆå…¶è®¡ç®—åæ‰èƒ½å¼€å§‹å·¥ä½œã€‚ä¾‹å¦‚ï¼Œå½“è®¾å¤‡1å¤„ç†åˆå§‹å—æ—¶ï¼Œè®¾å¤‡2ã€3å’Œ4ä¿æŒä¸æ´»è·ƒã€‚åŒæ ·ï¼Œå½“è®¾å¤‡2å¼€å§‹è®¡ç®—æ—¶ï¼Œè®¾å¤‡1å¤„äºç©ºé—²çŠ¶æ€ã€‚è¿™ç§ç­‰å¾…å’Œç©ºé—²æ—¶é—´çš„æ¨¡å¼ä¸å…¶å®ƒå¹¶è¡ŒåŒ–ç­–ç•¥ç›¸æ¯”ï¼Œé™ä½äº†ç¡¬ä»¶åˆ©ç”¨ç‡æ•ˆç‡ã€‚
- en: Layer-wise partitioning assigns distinct model layers to separate computing
    devices. In transformer architectures, this translates to specific devices managing
    defined sets of attention and feed-forward blocks. A 24-layer transformer model
    distributed across four devices assigns six consecutive transformer blocks to
    each device. Device 1 processes blocks 1-6, device 2 handles blocks 7-12, and
    so forth.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚çº§åˆ’åˆ†å°†ä¸åŒçš„æ¨¡å‹å±‚åˆ†é…åˆ°ç‹¬ç«‹çš„è®¡ç®—è®¾å¤‡ä¸Šã€‚åœ¨è½¬æ¢å™¨æ¶æ„ä¸­ï¼Œè¿™æ„å‘³ç€ç‰¹å®šçš„è®¾å¤‡ç®¡ç†å®šä¹‰å¥½çš„æ³¨æ„åŠ›å’Œå‰é¦ˆå—é›†åˆã€‚ä¸€ä¸ªåˆ†å¸ƒåœ¨å››ä¸ªè®¾å¤‡ä¸Šçš„24å±‚è½¬æ¢å™¨æ¨¡å‹å°†å…­ä¸ªè¿ç»­çš„è½¬æ¢å™¨å—åˆ†é…ç»™æ¯ä¸ªè®¾å¤‡ã€‚è®¾å¤‡1å¤„ç†å—1-6ï¼Œè®¾å¤‡2å¤„ç†å—7-12ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- en: Pipeline Parallelism
  id: totrans-904
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: ç®¡é“å¹¶è¡Œ
- en: Pipeline parallelism extends layer-wise partitioning by introducing microbatching
    to minimize device idle time, as illustrated in [FigureÂ 8.16](ch014.xhtml#fig-pipline-parallelism).
    Instead of waiting for an entire batch to sequentially pass through all devices,
    the computation is divided into smaller segments called microbatches ([D. Narayanan
    et al. 2019](ch058.xhtml#ref-harlap2018pipedream)). Each device, as represented
    by the rows in the drawing, processes its assigned model layers for different
    microbatches simultaneously. For example, the forward pass involves devices passing
    activations to the next stage (e.g., <semantics><msub><mi>F</mi><mrow><mn>0</mn><mo>,</mo><mn>0</mn></mrow></msub><annotation
    encoding="application/x-tex">F_{0,0}</annotation></semantics> to <semantics><msub><mi>F</mi><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><annotation
    encoding="application/x-tex">F_{1,0}</annotation></semantics>). The backward pass
    transfers gradients back through the pipeline (e.g., <semantics><msub><mi>B</mi><mrow><mn>3</mn><mo>,</mo><mn>3</mn></mrow></msub><annotation
    encoding="application/x-tex">B_{3,3}</annotation></semantics> to <semantics><msub><mi>B</mi><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub><annotation
    encoding="application/x-tex">B_{2,3}</annotation></semantics>). This overlapping
    computation reduces idle time and increases throughput while maintaining the logical
    sequence of operations across devices.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“å¹¶è¡Œé€šè¿‡å¼•å…¥å¾®æ‰¹æ¬¡æ‰©å±•äº†å±‚çº§çš„åˆ†åŒºï¼Œä»¥æœ€å°åŒ–è®¾å¤‡ç©ºé—²æ—¶é—´ï¼Œå¦‚å›¾8.16æ‰€ç¤ºã€‚[å›¾8.16](ch014.xhtml#fig-pipline-parallelism)ã€‚ä¸æ˜¯ç­‰å¾…æ•´ä¸ªæ‰¹æ¬¡é¡ºåºé€šè¿‡æ‰€æœ‰è®¾å¤‡ï¼Œè€Œæ˜¯å°†è®¡ç®—åˆ†æˆæ›´å°çš„æ®µï¼Œç§°ä¸ºå¾®æ‰¹æ¬¡([D.
    Narayananç­‰äºº2019](ch058.xhtml#ref-harlap2018pipedream))ã€‚å¦‚å›¾ä¸­è¡Œæ‰€ç¤ºï¼Œæ¯ä¸ªè®¾å¤‡åŒæ—¶å¤„ç†å…¶åˆ†é…çš„ä¸åŒå¾®æ‰¹æ¬¡çš„æ¨¡å‹å±‚ã€‚ä¾‹å¦‚ï¼Œæ­£å‘ä¼ æ’­æ¶‰åŠè®¾å¤‡å°†æ¿€æ´»ä¼ é€’åˆ°ä¸‹ä¸€é˜¶æ®µï¼ˆä¾‹å¦‚ï¼Œ<semantics><msub><mi>F</mi><mrow><mn>0</mn><mo>,</mo><mn>0</mn></mrow></msub><annotation
    encoding="application/x-tex">F_{0,0}</annotation></semantics>åˆ°<semantics><msub><mi>F</mi><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><annotation
    encoding="application/x-tex">F_{1,0}</annotation></semantics>ï¼‰ã€‚åå‘ä¼ æ’­å°†æ¢¯åº¦é€šè¿‡ç®¡é“åå‘ä¼ é€’ï¼ˆä¾‹å¦‚ï¼Œ<semantics><msub><mi>B</mi><mrow><mn>3</mn><mo>,</mo><mn>3</mn></mrow></msub><annotation
    encoding="application/x-tex">B_{3,3}</annotation></semantics>åˆ°<semantics><msub><mi>B</mi><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub><annotation
    encoding="application/x-tex">B_{2,3}</annotation></semantics>ï¼‰ã€‚è¿™ç§é‡å è®¡ç®—å‡å°‘äº†ç©ºé—²æ—¶é—´ï¼Œæé«˜äº†ååé‡ï¼ŒåŒæ—¶åœ¨è®¾å¤‡é—´ä¿æŒæ“ä½œé€»è¾‘çš„é¡ºåºã€‚
- en: '![](../media/file123.svg)'
  id: totrans-906
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file123.svg)'
- en: 'FigureÂ 8.16: **Pipeline Parallelism**: Microbatching distributes model layers
    across devices, enabling concurrent computation and minimizing idle time during
    both forward and backward passes to accelerate training. Activations flow sequentially
    between devices during the forward pass, while gradients propagate in reverse
    during backpropagation, effectively creating a pipeline for efficient resource
    utilization.'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.16ï¼š**ç®¡é“å¹¶è¡Œ**ï¼šå¾®æ‰¹æ¬¡å°†æ¨¡å‹å±‚åˆ†å¸ƒåœ¨è®¾å¤‡ä¸Šï¼Œå…è®¸å¹¶å‘è®¡ç®—ï¼Œåœ¨æ­£å‘å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­æœ€å°åŒ–ç©ºé—²æ—¶é—´ï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚æ­£å‘ä¼ æ’­æœŸé—´ï¼Œæ¿€æ´»åœ¨è®¾å¤‡ä¹‹é—´é¡ºåºæµåŠ¨ï¼Œè€Œåœ¨åå‘ä¼ æ’­æœŸé—´ï¼Œæ¢¯åº¦åå‘ä¼ æ’­ï¼Œæœ‰æ•ˆåœ°åˆ›å»ºäº†ä¸€ä¸ªç”¨äºé«˜æ•ˆèµ„æºåˆ©ç”¨çš„ç®¡é“ã€‚
- en: In a transformer model distributed across four devices, device 1 would process
    blocks 1-6 for microbatch <semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">N+1</annotation></semantics> while device 2 computes
    blocks 7-12 for microbatch <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>.
    Simultaneously, device 3 executes blocks 13-18 for microbatch <semantics><mrow><mi>N</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">N-1</annotation></semantics>, and device 4 processes
    blocks 19-24 for microbatch <semantics><mrow><mi>N</mi><mo>âˆ’</mo><mn>2</mn></mrow><annotation
    encoding="application/x-tex">N-2</annotation></semantics>. Each device maintains
    its assigned transformer blocks but operates on a different microbatch, creating
    a continuous flow of computation.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªåˆ†å¸ƒåœ¨å››ä¸ªè®¾å¤‡ä¸Šçš„å˜å‹å™¨æ¨¡å‹ä¸­ï¼Œè®¾å¤‡1å°†å¤„ç†å¾®æ‰¹æ¬¡<semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">N+1</annotation></semantics>çš„å—1-6ï¼Œè€Œè®¾å¤‡2è®¡ç®—å¾®æ‰¹æ¬¡<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>çš„å—7-12ã€‚åŒæ—¶ï¼Œè®¾å¤‡3æ‰§è¡Œå¾®æ‰¹æ¬¡<semantics><mrow><mi>N</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">N-1</annotation></semantics>çš„å—13-18ï¼Œè®¾å¤‡4å¤„ç†å¾®æ‰¹æ¬¡<semantics><mrow><mi>N</mi><mo>âˆ’</mo><mn>2</mn></mrow><annotation
    encoding="application/x-tex">N-2</annotation></semantics>çš„å—19-24ã€‚æ¯ä¸ªè®¾å¤‡ç»´æŠ¤å…¶åˆ†é…çš„å˜å‹å™¨å—ï¼Œä½†æ“ä½œä¸åŒçš„å¾®æ‰¹æ¬¡ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªè¿ç»­çš„è®¡ç®—æµã€‚
- en: The transfer of hidden states between devices occurs continuously rather than
    in distinct phases. When device 1 completes processing a microbatch, it immediately
    transfers the output tensor of shape [microbatch_size, sequence_length, hidden_dimension]
    to device 2 and begins processing the next microbatch. This overlapping computation
    pattern maintains full hardware utilization while preserving the modelâ€™s mathematical
    properties.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾å¤‡é—´éšè—çŠ¶æ€è½¬ç§»æ˜¯æŒç»­å‘ç”Ÿçš„ï¼Œè€Œä¸æ˜¯åœ¨ç‰¹å®šé˜¶æ®µå‘ç”Ÿã€‚å½“è®¾å¤‡1å®Œæˆå¤„ç†ä¸€ä¸ªå¾®æ‰¹æ¬¡åï¼Œå®ƒç«‹å³å°†å½¢çŠ¶ä¸º[microbatch_size, sequence_length,
    hidden_dimension]çš„è¾“å‡ºå¼ é‡ä¼ è¾“åˆ°è®¾å¤‡2ï¼Œå¹¶å¼€å§‹å¤„ç†ä¸‹ä¸€ä¸ªå¾®æ‰¹æ¬¡ã€‚è¿™ç§é‡å è®¡ç®—æ¨¡å¼åœ¨ä¿æŒæ¨¡å‹æ•°å­¦å±æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†ç¡¬ä»¶çš„å……åˆ†åˆ©ç”¨ã€‚
- en: Operator-level Parallelism
  id: totrans-910
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ“ä½œçº§å¹¶è¡Œæ€§
- en: 'Operator-level parallelism divides individual neural network operations across
    devices. In transformer models, this often means splitting attention computations.
    Consider a transformer with 64 attention heads and a hidden dimension of 4096\.
    Two devices might split this computation as follows: Device 1 processes attention
    heads 1-32, computing queries, keys, and values for its assigned heads. Device
    2 simultaneously processes heads 33-64\. Each device handles attention computations
    for [batch_size, sequence_length, 2048] dimensional tensors.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: æ“ä½œçº§å¹¶è¡Œæ€§å°†å•ä¸ªç¥ç»ç½‘ç»œæ“ä½œåˆ†æ•£åˆ°è®¾å¤‡ä¸Šã€‚åœ¨å˜å‹å™¨æ¨¡å‹ä¸­ï¼Œè¿™é€šå¸¸æ„å‘³ç€æ‹†åˆ†æ³¨æ„åŠ›è®¡ç®—ã€‚è€ƒè™‘ä¸€ä¸ªå…·æœ‰64ä¸ªæ³¨æ„åŠ›å¤´å’Œéšè—ç»´åº¦ä¸º4096çš„å˜å‹å™¨ã€‚ä¸¤ä¸ªè®¾å¤‡å¯èƒ½å¦‚ä¸‹æ‹†åˆ†æ­¤è®¡ç®—ï¼šè®¾å¤‡1å¤„ç†æ³¨æ„åŠ›å¤´1-32ï¼Œä¸ºå…¶åˆ†é…çš„å¤´è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚è®¾å¤‡2åŒæ—¶å¤„ç†å¤´33-64ã€‚æ¯ä¸ªè®¾å¤‡å¤„ç†[batch_size,
    sequence_length, 2048]ç»´åº¦çš„å¼ é‡çš„æ³¨æ„åŠ›è®¡ç®—ã€‚
- en: Matrix multiplication operations in feed-forward networks also benefit from
    operator-level splitting. A feed-forward layer with input dimension 4096 and intermediate
    dimension 16384 can split across devices along the intermediate dimension. Device
    1 computes the first 8192 intermediate features, while device 2 computes the remaining
    8192 features. This division reduces peak memory usage while maintaining mathematical
    equivalence to the original computation.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¦ˆç½‘ç»œä¸­çš„çŸ©é˜µä¹˜æ³•æ“ä½œä¹Ÿå—ç›Šäºæ“ä½œçº§æ‹†åˆ†ã€‚ä¸€ä¸ªè¾“å…¥ç»´åº¦ä¸º4096ï¼Œä¸­é—´ç»´åº¦ä¸º16384çš„å‰é¦ˆå±‚å¯ä»¥åœ¨ä¸­é—´ç»´åº¦ä¸Šè·¨è®¾å¤‡æ‹†åˆ†ã€‚è®¾å¤‡1è®¡ç®—å‰8192ä¸ªä¸­é—´ç‰¹å¾ï¼Œè€Œè®¾å¤‡2è®¡ç®—å‰©ä½™çš„8192ä¸ªç‰¹å¾ã€‚è¿™ç§åˆ’åˆ†å‡å°‘äº†å³°å€¼å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸå§‹è®¡ç®—çš„æ•°å­¦ç­‰ä»·æ€§ã€‚
- en: Summary
  id: totrans-913
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Each of these partitioning methods addresses specific challenges in training
    large models, and their applicability depends on the model architecture and the
    resources available. By selecting the appropriate strategy, practitioners can
    train models that exceed the limits of individual devices, enabling the development
    of advanced machine learning systems.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç§åˆ†åŒºæ–¹æ³•éƒ½é’ˆå¯¹è®­ç»ƒå¤§å‹æ¨¡å‹ä¸­çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œå…¶é€‚ç”¨æ€§å–å†³äºæ¨¡å‹æ¶æ„å’Œå¯ç”¨èµ„æºã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„ç­–ç•¥ï¼Œå®è·µè€…å¯ä»¥è®­ç»ƒè¶…å‡ºå•ä¸ªè®¾å¤‡é™åˆ¶çš„æ¨¡å‹ï¼Œä»è€Œä¿ƒè¿›é«˜çº§æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å‘å±•ã€‚
- en: Model Parallelism Advantages
  id: totrans-915
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§ä¼˜åŠ¿
- en: Model parallelism offers several significant benefits, making it an essential
    strategy for training large-scale models that exceed the capacity of individual
    devices. These advantages stem from its ability to partition the workload across
    multiple devices, enabling the training of more complex and resource-intensive
    architectures.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§æä¾›äº†å‡ ä¸ªæ˜¾è‘—çš„å¥½å¤„ï¼Œä½¿å…¶æˆä¸ºè®­ç»ƒè¶…å‡ºå•ä¸ªè®¾å¤‡å®¹é‡çš„å¤§è§„æ¨¡æ¨¡å‹çš„å…³é”®ç­–ç•¥ã€‚è¿™äº›ä¼˜åŠ¿æºäºå…¶èƒ½å¤Ÿåœ¨å¤šä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…å·¥ä½œè´Ÿè½½çš„èƒ½åŠ›ï¼Œä»è€Œä½¿å¾—è®­ç»ƒæ›´å¤æ‚å’Œèµ„æºå¯†é›†å‹çš„æ¶æ„æˆä¸ºå¯èƒ½ã€‚
- en: Memory scaling represents the primary advantage of model parallelism. Current
    transformer architectures contain up to hundreds of billions of parameters. A
    175 billion parameter model with 32-bit floating point precision requires 700
    GB of memory just to store its parameters. When accounting for activations, optimizer
    states, and gradients during training, the memory requirement multiplies several
    fold. Model parallelism makes training such architectures feasible by distributing
    these memory requirements across devices.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜æ‰©å±•æ˜¯æ¨¡å‹å¹¶è¡Œæ€§çš„ä¸»è¦ä¼˜åŠ¿ã€‚å½“å‰çš„å˜å‹å™¨æ¶æ„åŒ…å«å¤šè¾¾æ•°ç™¾äº¿ä¸ªå‚æ•°ã€‚ä¸€ä¸ªåŒ…å«1750äº¿ä¸ªå‚æ•°ä¸”ä»¥32ä½æµ®ç‚¹ç²¾åº¦å­˜å‚¨çš„æ¨¡å‹ï¼Œä»…å­˜å‚¨å…¶å‚æ•°å°±éœ€è¦700 GBçš„å†…å­˜ã€‚åœ¨è€ƒè™‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¿€æ´»ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ—¶ï¼Œå†…å­˜éœ€æ±‚ä¼šæˆå€å¢åŠ ã€‚æ¨¡å‹å¹¶è¡Œæ€§é€šè¿‡å°†è¿™äº›å†…å­˜éœ€æ±‚åˆ†é…åˆ°è®¾å¤‡ä¸Šï¼Œä½¿å¾—è®­ç»ƒæ­¤ç±»æ¶æ„æˆä¸ºå¯è¡Œã€‚
- en: Another key advantage is the efficient utilization of device memory and compute
    power. Since each device only needs to store and process a portion of the model,
    memory usage is distributed across the system. This allows practitioners to work
    with larger batch sizes or more complex layers without exceeding memory limits,
    which can also improve training stability and convergence.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯é«˜æ•ˆåˆ©ç”¨è®¾å¤‡å†…å­˜å’Œè®¡ç®—èƒ½åŠ›ã€‚ç”±äºæ¯ä¸ªè®¾å¤‡åªéœ€è¦å­˜å‚¨å’Œå¤„ç†æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå†…å­˜ä½¿ç”¨åœ¨ç³»ç»Ÿå†…éƒ¨åˆ†å¸ƒã€‚è¿™ä½¿å¾—ä»ä¸šè€…å¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹é‡å¤§å°æˆ–æ›´å¤æ‚çš„å±‚ï¼Œè€Œä¸ä¼šè¶…è¿‡å†…å­˜é™åˆ¶ï¼Œè¿™ä¹Ÿå¯ä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚
- en: Model parallelism also provides flexibility for different model architectures.
    Whether the model is sequential, as in many natural language processing tasks,
    or composed of computationally intensive operations, as in attention-based models
    or convolutional networks, there is a partitioning strategy that fits the architecture.
    This adaptability makes model parallelism applicable to a wide variety of tasks
    and domains.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§ä¹Ÿä¸ºä¸åŒçš„æ¨¡å‹æ¶æ„æä¾›äº†çµæ´»æ€§ã€‚æ— è®ºæ˜¯æ¨¡å‹æ˜¯é¡ºåºçš„ï¼Œå¦‚è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­é‚£æ ·ï¼Œè¿˜æ˜¯ç”±è®¡ç®—å¯†é›†å‹æ“ä½œç»„æˆï¼Œå¦‚åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹æˆ–å·ç§¯ç½‘ç»œï¼Œéƒ½å­˜åœ¨ä¸€ç§é€‚åˆè¯¥æ¶æ„çš„åˆ†åŒºç­–ç•¥ã€‚è¿™ç§é€‚åº”æ€§ä½¿å¾—æ¨¡å‹å¹¶è¡Œæ€§é€‚ç”¨äºå¹¿æ³›çš„ä»»åŠ¡å’Œé¢†åŸŸã€‚
- en: Finally, model parallelism is a natural complement to other distributed training
    strategies, such as data parallelism and pipeline parallelism. By combining these
    approaches, it becomes possible to train models that are both large in size and
    require extensive data. This hybrid flexibility is especially valuable in advanced
    research and production environments, where scaling models and datasets simultaneously
    is critical for achieving optimal performance.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ¨¡å‹å¹¶è¡Œæ€§æ˜¯å…¶ä»–åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥çš„è‡ªç„¶è¡¥å……ï¼Œä¾‹å¦‚æ•°æ®å¹¶è¡Œæ€§å’Œæµæ°´çº¿å¹¶è¡Œæ€§ã€‚é€šè¿‡ç»“åˆè¿™äº›æ–¹æ³•ï¼Œå¯ä»¥è®­ç»ƒå‡ºæ—¢å¤§åˆéœ€è¦å¤§é‡æ•°æ®çš„æ¨¡å‹ã€‚è¿™ç§æ··åˆçµæ´»æ€§åœ¨é«˜çº§ç ”ç©¶å’Œç”Ÿäº§ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰ä»·å€¼ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼ŒåŒæ—¶æ‰©å±•æ¨¡å‹å’Œæ•°æ®é›†å¯¹äºå®ç°æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ã€‚
- en: While model parallelism offers these benefits, its effectiveness depends on
    careful partitioning strategy design, with specific challenges addressed in the
    following sections and the trade-offs involved in its use.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¨¡å‹å¹¶è¡Œæ€§æä¾›äº†è¿™äº›å¥½å¤„ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºä»”ç»†çš„åˆ†åŒºç­–ç•¥è®¾è®¡ï¼Œä»¥ä¸‹ç« èŠ‚å°†è§£å†³å…·ä½“æŒ‘æˆ˜ï¼Œå¹¶è®¨è®ºå…¶ä½¿ç”¨ä¸­çš„æƒè¡¡ã€‚
- en: Model Parallelism Limitations
  id: totrans-922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§å±€é™æ€§
- en: While model parallelism provides an effective approach for training large-scale
    models, it also introduces unique challenges. These challenges arise from the
    complexity of partitioning the model and the dependencies between partitions during
    training. Addressing these issues requires careful system design and optimization.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¨¡å‹å¹¶è¡Œæ€§ä¸ºè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜æºäºæ¨¡å‹åˆ†åŒºçš„å¤æ‚æ€§ä»¥åŠè®­ç»ƒæœŸé—´åˆ†åŒºä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è§£å†³è¿™äº›é—®é¢˜éœ€è¦ä»”ç»†çš„ç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–ã€‚
- en: One major challenge in model parallelism is balancing the workload across devices.
    Not all parts of a model require the same amount of computation. For instance,
    in layer-wise partitioning, some layers may perform significantly more operations
    than others, leading to an uneven distribution of work. Devices responsible for
    the heavier computations may become bottlenecks, leaving others underutilized.
    This imbalance reduces overall efficiency and slows down training. Identifying
    optimal partitioning strategies is critical to ensuring all devices contribute
    evenly.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ€§ä¸­çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯åœ¨è®¾å¤‡ä¹‹é—´å¹³è¡¡å·¥ä½œè´Ÿè½½ã€‚æ¨¡å‹çš„æ‰€æœ‰éƒ¨åˆ†å¹¶ä¸éœ€è¦ç›¸åŒæ•°é‡çš„è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œåœ¨å±‚çŠ¶åˆ†åŒºä¸­ï¼ŒæŸäº›å±‚å¯èƒ½æ¯”å…¶ä»–å±‚æ‰§è¡Œçš„æ“ä½œæ˜¾è‘—æ›´å¤šï¼Œå¯¼è‡´å·¥ä½œåˆ†å¸ƒä¸å‡ã€‚è´Ÿè´£æ›´é‡è®¡ç®—ä»»åŠ¡çš„è®¾å¤‡å¯èƒ½æˆä¸ºç“¶é¢ˆï¼Œè€Œå…¶ä»–è®¾å¤‡åˆ™æœªè¢«å……åˆ†åˆ©ç”¨ã€‚è¿™ç§ä¸å¹³è¡¡é™ä½äº†æ•´ä½“æ•ˆç‡å¹¶å‡æ…¢äº†è®­ç»ƒé€Ÿåº¦ã€‚ç¡®å®šæœ€ä¼˜åˆ†åŒºç­–ç•¥å¯¹äºç¡®ä¿æ‰€æœ‰è®¾å¤‡å‡åŒ€è´¡çŒ®è‡³å…³é‡è¦ã€‚
- en: Another challenge is data dependency between devices. During the forward pass,
    activation tensors of shape [batch_size, sequence_length, hidden_dimension] must
    transfer between devices. For a typical transformer model with batch size 32,
    sequence length 2048, and hidden dimension 2048, each transfer moves approximately
    512 MB of data at float32 precision. With gradient transfers in the backward pass,
    a single training step can require several gigabytes of inter-device communication.
    On systems using PCIe interconnects with 64 GB/s theoretical bandwidth, these
    transfers introduce significant latency.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯è®¾å¤‡ä¹‹é—´çš„æ•°æ®ä¾èµ–æ€§ã€‚åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå½¢çŠ¶ä¸º[batch_size, sequence_length, hidden_dimension]çš„æ¿€æ´»å¼ é‡å¿…é¡»åœ¨è®¾å¤‡ä¹‹é—´ä¼ è¾“ã€‚å¯¹äºä¸€ä¸ªå…¸å‹çš„å…·æœ‰æ‰¹æ¬¡å¤§å°32ã€åºåˆ—é•¿åº¦2048å’Œéšè—ç»´åº¦2048çš„transformeræ¨¡å‹ï¼Œæ¯æ¬¡ä¼ è¾“å¤§çº¦ç§»åŠ¨512
    MBçš„float32ç²¾åº¦æ•°æ®ã€‚åœ¨åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦ä¼ è¾“ï¼Œå•ä¸ªè®­ç»ƒæ­¥éª¤å¯èƒ½éœ€è¦å‡ ä¸ªGBçš„è·¨è®¾å¤‡é€šä¿¡ã€‚åœ¨ç†è®ºå¸¦å®½ä¸º64 GB/sçš„PCIeäº’è¿ç³»ç»Ÿä¸­ï¼Œè¿™äº›ä¼ è¾“å¼•å…¥äº†æ˜¾è‘—çš„å»¶è¿Ÿã€‚
- en: Model parallelism also increases the complexity of implementation and debugging.
    Partitioning the model, ensuring proper data flow, and synchronizing gradients
    across devices require detailed coordination. Errors in any of these steps can
    lead to incorrect gradient updates or even model divergence. Debugging such errors
    is often more difficult in distributed systems, as issues may arise only under
    specific conditions or workloads.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡ŒåŒ–ä¹Ÿå¢åŠ äº†å®ç°å’Œè°ƒè¯•çš„å¤æ‚æ€§ã€‚å¯¹æ¨¡å‹è¿›è¡Œåˆ†åŒºã€ç¡®ä¿é€‚å½“çš„æ•°æ®æµä»¥åŠè·¨è®¾å¤‡åŒæ­¥æ¢¯åº¦éœ€è¦è¯¦ç»†çš„åè°ƒã€‚ä»»ä½•è¿™äº›æ­¥éª¤ä¸­çš„é”™è¯¯éƒ½å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°ä¸æ­£ç¡®ï¼Œç”šè‡³æ¨¡å‹å‘æ•£ã€‚åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œè°ƒè¯•æ­¤ç±»é”™è¯¯é€šå¸¸æ›´å›°éš¾ï¼Œå› ä¸ºé—®é¢˜å¯èƒ½ä»…åœ¨ç‰¹å®šæ¡ä»¶æˆ–å·¥ä½œè´Ÿè½½ä¸‹å‡ºç°ã€‚
- en: A further challenge is pipeline bubbles in pipeline parallelism. With m pipeline
    stages, the first <semantics><mrow><mi>m</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">m-1</annotation></semantics> steps operate at reduced
    efficiency as the pipeline fills. For example, in an 8-device pipeline, the first
    device begins processing immediately, but the eighth device remains idle for 7
    steps. This warmup period reduces hardware utilization by approximately <semantics><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>/</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">(m-1)/b</annotation></semantics>
    percent, where <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    is the number of batches in the training step.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç®¡é“å¹¶è¡Œä¸­çš„ç®¡é“æ°”æ³¡ã€‚åœ¨å…·æœ‰mä¸ªç®¡é“é˜¶æ®µçš„ç³»ç»Ÿä¸­ï¼Œéšç€ç®¡é“çš„å¡«å……ï¼Œå‰<semantics><mrow><mi>m</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">m-1</annotation></semantics>ä¸ªæ­¥éª¤çš„æ•ˆç‡ä¼šé™ä½ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ª8è®¾å¤‡ç®¡é“ä¸­ï¼Œç¬¬ä¸€ä¸ªè®¾å¤‡ç«‹å³å¼€å§‹å¤„ç†ï¼Œä½†ç¬¬å…«ä¸ªè®¾å¤‡åœ¨7ä¸ªæ­¥éª¤ä¸­ä¿æŒç©ºé—²ã€‚è¿™ä¸ªé¢„çƒ­æœŸå¤§çº¦å‡å°‘äº†çº¦<semantics><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>/</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">(m-1)/b</annotation></semantics>ç™¾åˆ†æ¯”çš„ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œå…¶ä¸­<semantics><mi>b</mi><annotation
    encoding="application/x-tex">b</annotation></semantics>æ˜¯è®­ç»ƒæ­¥éª¤ä¸­çš„æ‰¹æ¬¡æ•°é‡ã€‚
- en: Finally, model parallelism may be less effective for certain architectures,
    such as models with highly interdependent operations. In these cases, splitting
    the model may lead to excessive communication overhead, outweighing the benefits
    of parallel computation. For such models, alternative strategies like data parallelism
    or hybrid approaches might be more suitable.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¯¹äºæŸäº›æ¶æ„ï¼Œå¦‚é«˜åº¦ç›¸äº’ä¾èµ–çš„æ“ä½œçš„æ¨¡å‹ï¼Œæ¨¡å‹å¹¶è¡Œå¯èƒ½æ•ˆæœè¾ƒå·®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ†å‰²æ¨¡å‹å¯èƒ½å¯¼è‡´è¿‡åº¦çš„é€šä¿¡å¼€é”€ï¼Œè¶…è¿‡äº†å¹¶è¡Œè®¡ç®—çš„å¥½å¤„ã€‚å¯¹äºæ­¤ç±»æ¨¡å‹ï¼Œæ•°æ®å¹¶è¡Œæˆ–æ··åˆæ–¹æ³•ç­‰æ›¿ä»£ç­–ç•¥å¯èƒ½æ›´åˆé€‚ã€‚
- en: Despite these challenges, model parallelism remains an indispensable tool for
    training large models. With careful optimization and the use of modern frameworks,
    many of these issues can be mitigated, enabling efficient distributed training
    at scale.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ¨¡å‹å¹¶è¡ŒåŒ–ä»ç„¶æ˜¯è®­ç»ƒå¤§å‹æ¨¡å‹ä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚é€šè¿‡ä»”ç»†ä¼˜åŒ–å’Œç°ä»£æ¡†æ¶çš„ä½¿ç”¨ï¼Œè®¸å¤šè¿™äº›é—®é¢˜å¯ä»¥å¾—åˆ°ç¼“è§£ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚
- en: Hybrid Parallelism
  id: totrans-930
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡ŒåŒ–
- en: Recognizing that both data and model constraints can occur simultaneously, hybrid
    parallelism combines model parallelism and data parallelism when training neural
    networks ([D. Narayanan et al. 2021b](ch058.xhtml#ref-narayanan_pipeline_parallelism_2021)).
    A model might be too large to store on one GPU (requiring model parallelism) while
    simultaneously needing to process large batches of data efficiently (requiring
    data parallelism).
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: è®¤è¯†åˆ°æ•°æ®å’Œæ¨¡å‹çº¦æŸå¯èƒ½åŒæ—¶å‘ç”Ÿï¼Œæ··åˆå¹¶è¡ŒåŒ–åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ç»“åˆäº†æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œï¼ˆ[D. Narayanan ç­‰äºº 2021b](ch058.xhtml#ref-narayanan_pipeline_parallelism_2021)ï¼‰ã€‚ä¸€ä¸ªæ¨¡å‹å¯èƒ½å¤ªå¤§ï¼Œæ— æ³•å­˜å‚¨åœ¨ä¸€ä¸ªGPUä¸Šï¼ˆéœ€è¦æ¨¡å‹å¹¶è¡Œï¼‰ï¼ŒåŒæ—¶è¿˜éœ€è¦é«˜æ•ˆåœ°å¤„ç†å¤§é‡æ•°æ®ï¼ˆéœ€è¦æ•°æ®å¹¶è¡Œï¼‰ã€‚
- en: Training a 175-billion parameter language model on a dataset of 300 billion
    tokens demonstrates hybrid parallelism in practice. The neural network layers
    distribute across multiple GPUs through model parallelism, while data parallelism
    enables different GPU groups to process separate batches. The hybrid approach
    coordinates these two forms of parallelization.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŒ…å«3000äº¿ä¸ªæ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ª1750äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å±•ç¤ºäº†æ··åˆå¹¶è¡Œåœ¨å®é™…ä¸­çš„åº”ç”¨ã€‚ç¥ç»ç½‘ç»œå±‚é€šè¿‡æ¨¡å‹å¹¶è¡Œåˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œè€Œæ•°æ®å¹¶è¡Œä½¿å¾—ä¸åŒçš„GPUç»„èƒ½å¤Ÿå¤„ç†ä¸åŒçš„æ‰¹æ¬¡ã€‚æ··åˆæ–¹æ³•åè°ƒè¿™ä¸¤ç§å¹¶è¡ŒåŒ–å½¢å¼ã€‚
- en: This strategy addresses two key constraints. First, memory constraints arise
    when model parameters exceed single-device memory capacity. Second, computational
    demands increase when dataset size necessitates distributed processing.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç­–ç•¥è§£å†³äº†ä¸¤ä¸ªå…³é”®çº¦æŸã€‚é¦–å…ˆï¼Œå½“æ¨¡å‹å‚æ•°è¶…è¿‡å•ä¸ªè®¾å¤‡å†…å­˜å®¹é‡æ—¶ï¼Œä¼šå‡ºç°å†…å­˜çº¦æŸã€‚å…¶æ¬¡ï¼Œå½“æ•°æ®é›†å¤§å°éœ€è¦åˆ†å¸ƒå¼å¤„ç†æ—¶ï¼Œè®¡ç®—éœ€æ±‚å¢åŠ ã€‚
- en: Hybrid Parallelism Implementation
  id: totrans-934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œå®ç°
- en: Hybrid parallelism operates by combining the processes of model partitioning
    and dataset splitting, ensuring efficient utilization of both memory and computation
    across devices. This integration allows large-scale machine learning systems to
    overcome the constraints imposed by individual parallelism strategies.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œé€šè¿‡ç»“åˆæ¨¡å‹åˆ†åŒºå’Œæ•°æ®é›†æ‹†åˆ†çš„è¿‡ç¨‹ï¼Œç¡®ä¿äº†è·¨è®¾å¤‡å†…å­˜å’Œè®¡ç®—çš„æ•ˆç‡åˆ©ç”¨ã€‚è¿™ç§é›†æˆä½¿å¾—å¤§è§„æ¨¡æœºå™¨å­¦ä¹ ç³»ç»Ÿèƒ½å¤Ÿå…‹æœå•ä¸ªå¹¶è¡Œç­–ç•¥å¼ºåŠ çš„çº¦æŸã€‚
- en: Model and Data Partitioning
  id: totrans-936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¨¡å‹å’Œæ•°æ®åˆ†åŒº
- en: Hybrid parallelism divides both model architecture and training data across
    devices. The model divides through layer-wise or operator-level partitioning,
    where GPUs process distinct neural network segments. Simultaneously, the dataset
    splits into subsets, allowing each device group to train on different batches.
    A transformer model might distribute its attention layers across four GPUs, while
    each GPU group processes a unique 1,000-example batch. This dual partitioning
    distributes memory requirements and computational workload.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œå°†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ•°æ®åˆ†å‰²åˆ°è®¾å¤‡ä¸Šã€‚æ¨¡å‹é€šè¿‡å±‚æˆ–æ“ä½œçº§åˆ«çš„åˆ†åŒºæ¥åˆ†å‰²ï¼Œå…¶ä¸­GPUå¤„ç†ä¸åŒçš„ç¥ç»ç½‘ç»œæ®µã€‚åŒæ—¶ï¼Œæ•°æ®é›†æ‹†åˆ†æˆå­é›†ï¼Œå…è®¸æ¯ä¸ªè®¾å¤‡ç»„åœ¨ä¸åŒçš„æ‰¹æ¬¡ä¸Šè®­ç»ƒã€‚ä¸€ä¸ªè½¬æ¢å™¨æ¨¡å‹å¯èƒ½å°†å…¶æ³¨æ„åŠ›å±‚åˆ†å¸ƒåœ¨å››ä¸ªGPUä¸Šï¼Œè€Œæ¯ä¸ªGPUç»„å¤„ç†ä¸€ä¸ªç‹¬ç‰¹çš„1000ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚è¿™ç§åŒé‡åˆ†åŒºåˆ†é…äº†å†…å­˜éœ€æ±‚å’Œè®¡ç®—å·¥ä½œé‡ã€‚
- en: Forward Pass
  id: totrans-938
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ­£å‘ä¼ æ’­
- en: During the forward pass, input data flows through the distributed model. Each
    device processes its assigned portion of the model using the data subset it holds.
    For example, in a hybrid system with four devices, two devices might handle different
    layers of the model (model parallelism) while simultaneously processing distinct
    data batches (data parallelism). Communication between devices ensures that intermediate
    outputs from model partitions are passed seamlessly to subsequent partitions.
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥æ•°æ®æµç»åˆ†å¸ƒå¼æ¨¡å‹ã€‚æ¯ä¸ªè®¾å¤‡ä½¿ç”¨å…¶æŒæœ‰çš„æ•°æ®å­é›†å¤„ç†åˆ†é…ç»™å®ƒçš„æ¨¡å‹éƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªåŒ…å«å››ä¸ªè®¾å¤‡çš„æ··åˆç³»ç»Ÿä¸­ï¼Œä¸¤ä¸ªè®¾å¤‡å¯èƒ½å¤„ç†æ¨¡å‹çš„ä¸åŒçš„å±‚ï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰ï¼ŒåŒæ—¶å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰ã€‚è®¾å¤‡ä¹‹é—´çš„é€šä¿¡ç¡®ä¿æ¨¡å‹åˆ†åŒºä¸­é—´è¾“å‡ºæ— ç¼ä¼ é€’åˆ°åç»­åˆ†åŒºã€‚
- en: Backward Pass and Gradient Calculation
  id: totrans-940
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—
- en: During the backward pass, gradients are calculated for the model partitions
    stored on each device. Data-parallel devices that process the same subset of the
    model but different data batches aggregate their gradients, ensuring that updates
    reflect contributions from the entire dataset. For model-parallel devices, gradients
    are computed locally and passed to the next layer in reverse order. In a two-device
    model-parallel configuration, for example, the first device computes gradients
    for layers 1-3, then transmits these to the second device for layers 4-6\. This
    combination of gradient synchronization and inter-device communication ensures
    consistency across the distributed system.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œä¸ºæ¯ä¸ªè®¾å¤‡ä¸Šå­˜å‚¨çš„æ¨¡å‹åˆ†åŒºè®¡ç®—æ¢¯åº¦ã€‚å¤„ç†ç›¸åŒæ¨¡å‹å­é›†ä½†ä¸åŒæ•°æ®æ‰¹æ¬¡çš„å¹¶è¡Œæ•°æ®è®¾å¤‡æ±‡æ€»å…¶æ¢¯åº¦ï¼Œç¡®ä¿æ›´æ–°åæ˜ äº†æ•´ä¸ªæ•°æ®é›†çš„è´¡çŒ®ã€‚å¯¹äºæ¨¡å‹å¹¶è¡Œè®¾å¤‡ï¼Œæ¢¯åº¦åœ¨æœ¬åœ°è®¡ç®—å¹¶æŒ‰åå‘é¡ºåºä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸¤ä¸ªè®¾å¤‡çš„æ¨¡å‹å¹¶è¡Œé…ç½®ä¸­ï¼Œç¬¬ä¸€ä¸ªè®¾å¤‡è®¡ç®—ç¬¬1-3å±‚çš„æ¢¯åº¦ï¼Œç„¶åå°†è¿™äº›æ¢¯åº¦ä¼ è¾“åˆ°ç¬¬äºŒä¸ªè®¾å¤‡ä»¥è®¡ç®—ç¬¬4-6å±‚ã€‚è¿™ç§æ¢¯åº¦åŒæ­¥å’Œè®¾å¤‡é—´é€šä¿¡çš„ç»„åˆç¡®ä¿äº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€è‡´æ€§ã€‚
- en: Parameter Updates
  id: totrans-942
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å‚æ•°æ›´æ–°
- en: After gradient synchronization, model parameters are updated using the chosen
    optimization algorithm. Devices working in data parallelism update their shared
    model partitions consistently, while model-parallel devices apply updates to their
    local segments. Efficient communication is critical in this step to minimize delays
    and ensure that updates are correctly propagated across all devices.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¢¯åº¦åŒæ­¥ä¹‹åï¼Œæ¨¡å‹å‚æ•°ä½¿ç”¨é€‰å®šçš„ä¼˜åŒ–ç®—æ³•è¿›è¡Œæ›´æ–°ã€‚åœ¨æ•°æ®å¹¶è¡Œå·¥ä½œæ¨¡å¼ä¸‹çš„è®¾å¤‡ä¼šä¸€è‡´åœ°æ›´æ–°å®ƒä»¬å…±äº«çš„æ¨¡å‹åˆ†åŒºï¼Œè€Œæ¨¡å‹å¹¶è¡Œè®¾å¤‡åˆ™å¯¹å…¶å±€éƒ¨æ®µåº”ç”¨æ›´æ–°ã€‚åœ¨è¿™ä¸€æ­¥éª¤ä¸­ï¼Œé«˜æ•ˆçš„é€šä¿¡è‡³å…³é‡è¦ï¼Œä»¥æœ€å°åŒ–å»¶è¿Ÿå¹¶ç¡®ä¿æ›´æ–°èƒ½å¤Ÿæ­£ç¡®åœ°ä¼ æ’­åˆ°æ‰€æœ‰è®¾å¤‡ã€‚
- en: Iterative Process
  id: totrans-944
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è¿­ä»£è¿‡ç¨‹
- en: Hybrid parallelism follows an iterative process similar to other training strategies.
    The combination of model and data distribution allows the system to process large
    datasets and complex models efficiently over multiple training epochs. By balancing
    the computational workload and memory requirements, hybrid parallelism enables
    the training of advanced machine learning models that would otherwise be infeasible.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œéµå¾ªä¸å…¶ä»–è®­ç»ƒç­–ç•¥ç±»ä¼¼çš„è¿­ä»£è¿‡ç¨‹ã€‚æ¨¡å‹å’Œæ•°æ®åˆ†å¸ƒçš„ç»“åˆå…è®¸ç³»ç»Ÿåœ¨å¤šä¸ªè®­ç»ƒå‘¨æœŸå†…æœ‰æ•ˆåœ°å¤„ç†å¤§å‹æ•°æ®é›†å’Œå¤æ‚æ¨¡å‹ã€‚é€šè¿‡å¹³è¡¡è®¡ç®—å·¥ä½œè´Ÿè½½å’Œå†…å­˜éœ€æ±‚ï¼Œæ··åˆå¹¶è¡Œä½¿å¾—è®­ç»ƒåŸæœ¬ä¸å¯è¡Œçš„å…ˆè¿›æœºå™¨å­¦ä¹ æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚
- en: Parallelism Variations
  id: totrans-946
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å¹¶è¡Œå˜åŒ–
- en: Hybrid parallelism can be implemented in different configurations, depending
    on the model architecture, dataset characteristics, and available hardware. These
    variations allow for tailored solutions that optimize performance and scalability
    for specific training requirements.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œå¯ä»¥åœ¨ä¸åŒçš„é…ç½®ä¸­å®ç°ï¼Œè¿™å–å†³äºæ¨¡å‹æ¶æ„ã€æ•°æ®é›†ç‰¹å¾å’Œå¯ç”¨çš„ç¡¬ä»¶ã€‚è¿™äº›å˜åŒ–å…è®¸é’ˆå¯¹ç‰¹å®šçš„è®­ç»ƒéœ€æ±‚å®šåˆ¶è§£å†³æ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚
- en: Hierarchical Parallelism
  id: totrans-948
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å±‚æ¬¡å¹¶è¡Œ
- en: Hierarchical hybrid parallelism applies model parallelism to divide the model
    across devices first and then layers data parallelism on top to handle the dataset
    distribution. For example, in a system with eight devices, four devices may hold
    different partitions of the model, while each partition is replicated across the
    other four devices for data parallel processing. This approach is well-suited
    for large models with billions of parameters, where memory constraints are a primary
    concern.
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚æ¬¡æ··åˆå¹¶è¡Œé¦–å…ˆå°†æ¨¡å‹å¹¶è¡Œåº”ç”¨äºè·¨è®¾å¤‡åˆ’åˆ†æ¨¡å‹ï¼Œç„¶ååœ¨æ•°æ®é›†åˆ†å¸ƒä¹‹ä¸Šåº”ç”¨å±‚æ•°æ®å¹¶è¡Œã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªåŒ…å«å…«ä¸ªè®¾å¤‡çš„ç³»ç»Ÿä¸­ï¼Œå››ä¸ªè®¾å¤‡å¯èƒ½æŒæœ‰æ¨¡å‹çš„ä¸åŒåˆ†åŒºï¼Œè€Œæ¯ä¸ªåˆ†åŒºåœ¨å…¶ä»–å››ä¸ªè®¾å¤‡ä¸Šå¤åˆ¶ä»¥è¿›è¡Œæ•°æ®å¹¶è¡Œå¤„ç†ã€‚è¿™ç§æ–¹æ³•éå¸¸é€‚åˆå…·æœ‰æ•°åäº¿å‚æ•°çš„å¤§å‹æ¨¡å‹ï¼Œå…¶ä¸­å†…å­˜é™åˆ¶æ˜¯ä¸€ä¸ªä¸»è¦å…³æ³¨ç‚¹ã€‚
- en: Hierarchical hybrid parallelism ensures that the model size is distributed across
    devices, reducing memory requirements, while data parallelism ensures that multiple
    data samples are processed simultaneously, improving throughput. This dual-layered
    approach is particularly effective for models like transformers, where each layer
    may have a significant memory footprint.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚æ¬¡æ··åˆå¹¶è¡Œç¡®ä¿æ¨¡å‹å¤§å°åœ¨è®¾å¤‡ä¹‹é—´åˆ†å¸ƒï¼Œå‡å°‘å†…å­˜éœ€æ±‚ï¼Œè€Œæ•°æ®å¹¶è¡Œç¡®ä¿å¤šä¸ªæ•°æ®æ ·æœ¬å¯ä»¥åŒæ—¶å¤„ç†ï¼Œæé«˜ååé‡ã€‚è¿™ç§åŒå±‚æ–¹æ³•å¯¹äºåƒå˜å‹å™¨è¿™æ ·çš„æ¨¡å‹å°¤å…¶æœ‰æ•ˆï¼Œå› ä¸ºæ¯ä¸€å±‚å¯èƒ½éƒ½æœ‰ä¸€ä¸ªæ˜¾è‘—çš„å†…å­˜å ç”¨ã€‚
- en: Intra-layer Parallelism
  id: totrans-951
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å±‚å†…å¹¶è¡Œ
- en: Intra-layer hybrid parallelism combines model and data parallelism within individual
    layers of the model. For instance, in a transformer architecture, the attention
    mechanism can be split across multiple devices (model parallelism), while each
    device processes distinct batches of data (data parallelism). This fine-grained
    integration allows the system to optimize resource usage at the level of individual
    operations, enabling training for models with extremely large intermediate computations.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚å†…æ··åˆå¹¶è¡Œå°†æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œç»“åˆåˆ°æ¨¡å‹çš„å„ä¸ªå±‚ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨å˜å‹å™¨æ¶æ„ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è·¨å¤šä¸ªè®¾å¤‡åˆ†å‰²ï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰ï¼Œè€Œæ¯ä¸ªè®¾å¤‡å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰ã€‚è¿™ç§ç»†ç²’åº¦é›†æˆå…è®¸ç³»ç»Ÿåœ¨å•ä¸ªæ“ä½œçº§åˆ«ä¼˜åŒ–èµ„æºä½¿ç”¨ï¼Œä»è€Œèƒ½å¤Ÿè®­ç»ƒå…·æœ‰æå…¶å¤§é‡ä¸­é—´è®¡ç®—çš„æ¨¡å‹ã€‚
- en: This variation is particularly useful in scenarios where specific layers, such
    as attention or feedforward layers, have computationally intensive operations
    that are difficult to distribute effectively using model or data parallelism alone.
    Intra-layer hybrid parallelism addresses this challenge by applying both strategies
    simultaneously.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å˜åŒ–åœ¨ç‰¹å®šå±‚ï¼Œå¦‚æ³¨æ„åŠ›æˆ–å‰é¦ˆå±‚ï¼Œå…·æœ‰è®¡ç®—å¯†é›†å‹æ“ä½œä¸”éš¾ä»¥ä»…ä½¿ç”¨æ¨¡å‹æˆ–æ•°æ®å¹¶è¡Œæœ‰æ•ˆåˆ†é…çš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚å±‚å†…æ··åˆå¹¶è¡Œé€šè¿‡åŒæ—¶åº”ç”¨è¿™ä¸¤ç§ç­–ç•¥æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚
- en: Inter-layer Parallelism
  id: totrans-954
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å±‚é—´å¹¶è¡Œ
- en: Inter-layer hybrid parallelism focuses on distributing the workload between
    model and data parallelism at the level of distinct model layers. For example,
    early layers of a neural network may be distributed using model parallelism, while
    later layers use data parallelism. This approach aligns with the observation that
    certain layers in a model may be more memory-intensive, while others benefit from
    increased data throughput.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚é—´æ··åˆå¹¶è¡Œä¸»ä¹‰ä¸“æ³¨äºåœ¨ç‰¹å®šæ¨¡å‹å±‚çº§åˆ«ä¹‹é—´åˆ†é…æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œçš„å·¥ä½œè´Ÿè½½ã€‚ä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œçš„å‰å‡ å±‚å¯èƒ½ä½¿ç”¨æ¨¡å‹å¹¶è¡Œè¿›è¡Œåˆ†å¸ƒï¼Œè€Œåç»­å±‚åˆ™ä½¿ç”¨æ•°æ®å¹¶è¡Œã€‚è¿™ç§æ–¹æ³•ä¸è§‚å¯Ÿåˆ°çš„æŸäº›æ¨¡å‹å±‚å¯èƒ½æ›´å†…å­˜å¯†é›†ï¼Œè€Œå…¶ä»–å±‚åˆ™ä»å¢åŠ çš„æ•°æ®ååé‡ä¸­å—ç›Šçš„è§‚å¯Ÿç»“æœç›¸ä¸€è‡´ã€‚
- en: This configuration allows for dynamic allocation of resources, adapting to the
    specific demands of different layers in the model. By tailoring the parallelism
    strategy to the unique characteristics of each layer, inter-layer hybrid parallelism
    achieves an optimal balance between memory usage and computational efficiency.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é…ç½®å…è®¸åŠ¨æ€åˆ†é…èµ„æºï¼Œé€‚åº”æ¨¡å‹ä¸­ä¸åŒå±‚çš„ç‰¹å®šéœ€æ±‚ã€‚é€šè¿‡å°†å¹¶è¡Œç­–ç•¥å®šåˆ¶åˆ°æ¯ä¸€å±‚çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œå±‚é—´æ··åˆå¹¶è¡Œä¸»ä¹‰åœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ã€‚
- en: Hybrid Parallelism Advantages
  id: totrans-957
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œä¸»ä¹‰ä¼˜åŠ¿
- en: The adoption of hybrid parallelism in machine learning systems addresses some
    of the most significant challenges posed by the ever-growing scale of models and
    datasets. By blending the strengths of model parallelism and data parallelism,
    this approach provides a solution to scaling modern machine learning workloads.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­é‡‡ç”¨æ··åˆå¹¶è¡Œä¸»ä¹‰è§£å†³äº†ç”±æ¨¡å‹å’Œæ•°æ®é›†è§„æ¨¡ä¸æ–­å¢é•¿æ‰€æå‡ºçš„æŸäº›æœ€é‡å¤§çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆæ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œçš„ä¼˜åŠ¿ï¼Œè¿™ç§æ–¹æ³•ä¸ºç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½çš„æ‰©å±•æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚
- en: One of the most prominent benefits of hybrid parallelism is its ability to scale
    seamlessly across both the model and the dataset. Modern neural networks, particularly
    transformers used in natural language processing and vision applications, often
    contain billions of parameters. These models, paired with massive datasets, make
    training on a single device impractical or even impossible. Hybrid parallelism
    enables the division of the model across multiple devices to manage memory constraints
    while simultaneously distributing the dataset to process vast amounts of data
    efficiently. This dual capability ensures that training systems can handle the
    computational and memory demands of the largest models and datasets without compromise.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œä¸»ä¹‰æœ€çªå‡ºçš„å¥½å¤„ä¹‹ä¸€æ˜¯å®ƒèƒ½å¤Ÿåœ¨æ¨¡å‹å’Œæ•°æ®é›†ä¹‹é—´æ— ç¼æ‰©å±•ã€‚ç°ä»£ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰åº”ç”¨ä¸­ä½¿ç”¨çš„å˜å‹å™¨ï¼Œé€šå¸¸åŒ…å«æ•°åäº¿ä¸ªå‚æ•°ã€‚è¿™äº›æ¨¡å‹ä¸åºå¤§çš„æ•°æ®é›†ç›¸ç»“åˆï¼Œä½¿å¾—åœ¨å•ä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®­ç»ƒå˜å¾—ä¸åˆ‡å®é™…ï¼Œç”šè‡³ä¸å¯èƒ½ã€‚æ··åˆå¹¶è¡Œä¸»ä¹‰ä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨å¤šä¸ªè®¾å¤‡ä¹‹é—´åˆ’åˆ†ï¼Œä»¥ç®¡ç†å†…å­˜é™åˆ¶ï¼ŒåŒæ—¶å°†æ•°æ®é›†åˆ†å¸ƒåˆ°å¤„ç†å¤§é‡æ•°æ®çš„è®¾å¤‡ä¸Šï¼Œä»è€Œæœ‰æ•ˆåœ°è¿›è¡Œæ•°æ®åˆ†å‘ã€‚è¿™ç§åŒé‡èƒ½åŠ›ç¡®ä¿äº†è®­ç»ƒç³»ç»Ÿå¯ä»¥æ— å¦¥ååœ°å¤„ç†æœ€å¤§æ¨¡å‹å’Œæ•°æ®é›†çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚
- en: Another critical advantage lies in hardware utilization. In many distributed
    training systems, inefficiencies can arise when devices sit idle during different
    stages of computation or synchronization. Hybrid parallelism mitigates this issue
    by ensuring that all devices are actively engaged. Whether a device is computing
    forward passes through its portion of the model or processing data batches, hybrid
    strategies maximize resource usage, leading to faster training times and improved
    throughput.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå…³é”®ä¼˜åŠ¿åœ¨äºç¡¬ä»¶åˆ©ç”¨ç‡ã€‚åœ¨è®¸å¤šåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ï¼Œå½“è®¾å¤‡åœ¨è®¡ç®—æˆ–åŒæ­¥çš„ä¸åŒé˜¶æ®µå¤„äºç©ºé—²çŠ¶æ€æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æ··åˆå¹¶è¡Œä¸»ä¹‰é€šè¿‡ç¡®ä¿æ‰€æœ‰è®¾å¤‡éƒ½å¤„äºæ´»è·ƒçŠ¶æ€æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æ— è®ºè®¾å¤‡æ˜¯åœ¨é€šè¿‡å…¶æ¨¡å‹éƒ¨åˆ†è¿›è¡Œå‰å‘è®¡ç®—è¿˜æ˜¯å¤„ç†æ•°æ®æ‰¹æ¬¡ï¼Œæ··åˆç­–ç•¥éƒ½æœ€å¤§åŒ–äº†èµ„æºä½¿ç”¨ï¼Œä»è€Œç¼©çŸ­äº†è®­ç»ƒæ—¶é—´å¹¶æé«˜äº†ååé‡ã€‚
- en: Flexibility is another hallmark of hybrid parallelism. Machine learning models
    vary widely in architecture and computational demands. For instance, convolutional
    neural networks prioritize spatial data processing, while transformers require
    intensive operations like matrix multiplications in attention mechanisms. Hybrid
    parallelism adapts to these diverse needs by allowing practitioners to apply model
    and data parallelism selectively. This adaptability ensures that hybrid approaches
    can be tailored to the specific requirements of a given model, making it a versatile
    solution for diverse training scenarios.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: çµæ´»æ€§æ˜¯æ··åˆå¹¶è¡Œä¸»ä¹‰çš„å¦ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ¶æ„å’Œè®¡ç®—éœ€æ±‚ä¸Šå·®å¼‚å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¼˜å…ˆå¤„ç†ç©ºé—´æ•°æ®å¤„ç†ï¼Œè€Œå˜å‹å™¨åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­éœ€è¦æ‰§è¡Œå¦‚çŸ©é˜µä¹˜æ³•ç­‰å¯†é›†æ“ä½œã€‚æ··åˆå¹¶è¡Œä¸»ä¹‰é€šè¿‡å…è®¸å®è·µè€…æœ‰é€‰æ‹©åœ°åº”ç”¨æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œæ¥é€‚åº”è¿™äº›å¤šæ ·åŒ–çš„éœ€æ±‚ã€‚è¿™ç§é€‚åº”æ€§ç¡®ä¿äº†æ··åˆæ–¹æ³•å¯ä»¥æ ¹æ®ç‰¹å®šæ¨¡å‹çš„ç‰¹å®šè¦æ±‚è¿›è¡Œå®šåˆ¶ï¼Œä½¿å…¶æˆä¸ºå¤šæ ·åŒ–è®­ç»ƒåœºæ™¯çš„å¤šåŠŸèƒ½è§£å†³æ–¹æ¡ˆã€‚
- en: Hybrid parallelism reduces communication bottlenecks, a common issue in distributed
    systems. By striking a balance between distributing model computations and spreading
    data processing, hybrid strategies minimize the amount of inter-device communication
    required during training. This efficient coordination not only speeds up the training
    process but also enables the effective use of large-scale distributed systems
    where network latency might otherwise limit performance.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œè®¡ç®—å¯ä»¥å‡å°‘é€šä¿¡ç“¶é¢ˆï¼Œè¿™æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¸¸è§çš„é—®é¢˜ã€‚é€šè¿‡åœ¨åˆ†å¸ƒå¼æ¨¡å‹è®¡ç®—å’Œæ•°æ®å¤„ç†ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæ··åˆç­–ç•¥æœ€å°åŒ–äº†è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€éœ€çš„è®¾å¤‡é—´é€šä¿¡é‡ã€‚è¿™ç§é«˜æ•ˆçš„åè°ƒä¸ä»…åŠ å¿«äº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜ä½¿å¾—åœ¨å¯èƒ½é™åˆ¶æ€§èƒ½çš„ç½‘ç»œå»¶è¿Ÿæƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä½¿ç”¨å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿã€‚
- en: Finally, hybrid parallelism supports the ambitious scale of modern AI research
    and development. It provides a framework for leveraging advanced hardware infrastructures,
    including clusters of GPUs or TPUs, to train models that push the boundaries of
    whatâ€™s possible. Without hybrid parallelism, many of the breakthroughs in AI,
    including large language models and advanced vision systems, would remain unattainable
    due to resource limitations.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ··åˆå¹¶è¡Œè®¡ç®—æ”¯æŒç°ä»£äººå·¥æ™ºèƒ½ç ”ç©¶å¼€å‘çš„é›„å¿ƒå£®å¿—è§„æ¨¡ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºåˆ©ç”¨é«˜çº§ç¡¬ä»¶åŸºç¡€è®¾æ–½ï¼ŒåŒ…æ‹¬GPUæˆ–TPUé›†ç¾¤ï¼Œæ¥è®­ç»ƒæ¨åŠ¨å¯èƒ½æ€§çš„è¾¹ç•Œæ¨¡å‹ã€‚æ²¡æœ‰æ··åˆå¹¶è¡Œè®¡ç®—ï¼Œç”±äºèµ„æºé™åˆ¶ï¼Œè®¸å¤šäººå·¥æ™ºèƒ½çš„çªç ´ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œé«˜çº§è§†è§‰ç³»ç»Ÿï¼Œå°†æ— æ³•å®ç°ã€‚
- en: By enabling scalability, maximizing hardware efficiency, and offering flexibility,
    hybrid parallelism has become an essential strategy for training the most complex
    machine learning systems. It is not just a solution to todayâ€™s challenges but
    also a foundation for the future of AI, where models and datasets will continue
    to grow in complexity and size.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å®ç°å¯æ‰©å±•æ€§ã€æœ€å¤§åŒ–ç¡¬ä»¶æ•ˆç‡å’Œæä¾›çµæ´»æ€§ï¼Œæ··åˆå¹¶è¡Œè®¡ç®—å·²æˆä¸ºè®­ç»ƒæœ€å¤æ‚æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„åŸºæœ¬ç­–ç•¥ã€‚å®ƒä¸ä»…æ˜¯è§£å†³å½“å‰æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆï¼Œä¹Ÿæ˜¯äººå·¥æ™ºèƒ½æœªæ¥çš„åŸºç¡€ï¼Œå…¶ä¸­æ¨¡å‹å’Œæ•°æ®é›†å°†ç»§ç»­åœ¨å¤æ‚æ€§å’Œè§„æ¨¡ä¸Šå¢é•¿ã€‚
- en: Hybrid Parallelism Limitations
  id: totrans-965
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œè®¡ç®—å±€é™æ€§
- en: While hybrid parallelism provides a robust framework for scaling machine learning
    training, it also introduces complexities that require careful consideration.
    These challenges stem from the intricate coordination needed to integrate both
    model and data parallelism effectively. Understanding these obstacles is crucial
    for designing efficient hybrid systems and avoiding potential bottlenecks.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ··åˆå¹¶è¡Œè®¡ç®—ä¸ºæœºå™¨å­¦ä¹ è®­ç»ƒæä¾›äº†å¼ºå¤§çš„æ¡†æ¶ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†éœ€è¦ä»”ç»†è€ƒè™‘çš„å¤æ‚æ€§ã€‚è¿™äº›æŒ‘æˆ˜æºäºæœ‰æ•ˆé›†æˆæ¨¡å‹å’Œæ•°æ®å¹¶è¡Œæ‰€éœ€çš„å¤æ‚åè°ƒã€‚ç†è§£è¿™äº›éšœç¢å¯¹äºè®¾è®¡é«˜æ•ˆçš„æ··åˆç³»ç»Ÿå¹¶é¿å…æ½œåœ¨çš„ç“¶é¢ˆè‡³å…³é‡è¦ã€‚
- en: One of the primary challenges of hybrid parallelism is communication overhead.
    Both model and data parallelism involve significant inter-device communication.
    In model parallelism, devices must exchange intermediate outputs and gradients
    to maintain the sequential flow of computation. In data parallelism, gradients
    computed on separate data subsets must be synchronized across devices. Hybrid
    parallelism compounds these demands, as it requires efficient communication for
    both processes simultaneously. If not managed properly, the resulting overhead
    can negate the benefits of parallelization, particularly in large-scale systems
    with slower interconnects or high network latency.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆå¹¶è¡Œè®¡ç®—çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯é€šä¿¡å¼€é”€ã€‚æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œéƒ½æ¶‰åŠå¤§é‡çš„è®¾å¤‡é—´é€šä¿¡ã€‚åœ¨æ¨¡å‹å¹¶è¡Œä¸­ï¼Œè®¾å¤‡å¿…é¡»äº¤æ¢ä¸­é—´è¾“å‡ºå’Œæ¢¯åº¦ä»¥ä¿æŒè®¡ç®—çš„é¡ºåºæµç¨‹ã€‚åœ¨æ•°æ®å¹¶è¡Œä¸­ï¼Œè®¡ç®—åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šçš„æ¢¯åº¦å¿…é¡»åœ¨è®¾å¤‡é—´åŒæ­¥ã€‚æ··åˆå¹¶è¡Œè®¡ç®—åŒæ—¶è¦æ±‚è¿™ä¸¤ä¸ªè¿‡ç¨‹éƒ½è¿›è¡Œé«˜æ•ˆçš„é€šä¿¡ã€‚å¦‚æœç®¡ç†ä¸å½“ï¼Œäº§ç”Ÿçš„å¼€é”€å¯èƒ½ä¼šæŠµæ¶ˆå¹¶è¡ŒåŒ–çš„å¥½å¤„ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰è¾ƒæ…¢äº’è¿æˆ–é«˜ç½‘ç»œå»¶è¿Ÿçš„å¤§è§„æ¨¡ç³»ç»Ÿä¸­ã€‚
- en: Another critical challenge is the complexity of implementation. Hybrid parallelism
    demands a nuanced understanding of both model and data parallelism techniques,
    as well as the underlying hardware and software infrastructure. Designing efficient
    hybrid strategies involves making decisions about how to partition the model,
    how to distribute data, and how to synchronize computations across devices. This
    process often requires extensive experimentation and optimization, particularly
    for custom architectures or non-standard hardware setups. While modern frameworks
    like PyTorch and TensorFlow provide tools for distributed training, implementing
    hybrid parallelism at scale still requires significant engineering expertise.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å®ç°çš„å¤æ‚æ€§ã€‚æ··åˆå¹¶è¡Œéœ€è¦æ·±å…¥ç†è§£æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡ŒæŠ€æœ¯ï¼Œä»¥åŠåº•å±‚ç¡¬ä»¶å’Œè½¯ä»¶åŸºç¡€è®¾æ–½ã€‚è®¾è®¡æœ‰æ•ˆçš„æ··åˆç­–ç•¥æ¶‰åŠå†³å®šå¦‚ä½•åˆ†åŒºæ¨¡å‹ã€å¦‚ä½•åˆ†é…æ•°æ®ä»¥åŠå¦‚ä½•åœ¨è®¾å¤‡é—´åŒæ­¥è®¡ç®—ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸éœ€è¦å¤§é‡çš„å®éªŒå’Œä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®šåˆ¶æ¶æ„æˆ–éæ ‡å‡†ç¡¬ä»¶é…ç½®ã€‚è™½ç„¶ç°ä»£æ¡†æ¶å¦‚PyTorchå’ŒTensorFlowæä¾›äº†åˆ†å¸ƒå¼è®­ç»ƒçš„å·¥å…·ï¼Œä½†åœ¨è§„æ¨¡ä¸Šå®ç°æ··åˆå¹¶è¡Œä»ç„¶éœ€è¦æ˜¾è‘—çš„å·¥ç¨‹ä¸“ä¸šçŸ¥è¯†ã€‚
- en: Workload balancing also presents a challenge in hybrid parallelism. In a distributed
    system, not all devices may have equal computational capacity. Some devices may
    process data or compute gradients faster than others, leading to inefficiencies
    as faster devices wait for slower ones to complete their tasks. Additionally,
    certain model layers or operations may require more resources than others, creating
    imbalances in computational load. Managing this disparity requires careful tuning
    of partitioning strategies and the use of dynamic workload distribution techniques.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œè´Ÿè½½å‡è¡¡åœ¨æ··åˆå¹¶è¡Œä¸­ä¹Ÿæå‡ºäº†æŒ‘æˆ˜ã€‚åœ¨ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå¹¶éæ‰€æœ‰è®¾å¤‡éƒ½æ‹¥æœ‰ç›¸åŒçš„è®¡ç®—èƒ½åŠ›ã€‚ä¸€äº›è®¾å¤‡å¯èƒ½æ¯”å…¶ä»–è®¾å¤‡æ›´å¿«åœ°å¤„ç†æ•°æ®æˆ–è®¡ç®—æ¢¯åº¦ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºè¾ƒå¿«çš„è®¾å¤‡éœ€è¦ç­‰å¾…è¾ƒæ…¢çš„è®¾å¤‡å®Œæˆå…¶ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒæŸäº›æ¨¡å‹å±‚æˆ–æ“ä½œå¯èƒ½æ¯”å…¶ä»–æ“ä½œéœ€è¦æ›´å¤šçš„èµ„æºï¼Œä»è€Œåœ¨è®¡ç®—è´Ÿè½½ä¸Šé€ æˆä¸å¹³è¡¡ã€‚ç®¡ç†è¿™ç§å·®å¼‚éœ€è¦ä»”ç»†è°ƒæ•´åˆ†åŒºç­–ç•¥å’Œé‡‡ç”¨åŠ¨æ€å·¥ä½œè´Ÿè½½åˆ†é…æŠ€æœ¯ã€‚
- en: Memory constraints remain a concern, even in hybrid setups. While model parallelism
    addresses the issue of fitting large models into device memory, the additional
    memory requirements for data parallelism, such as storing multiple data batches
    and gradient buffers, can still exceed available capacity. This is especially
    true for models with extremely large intermediate computations, such as transformers
    with high-dimensional attention mechanisms. Balancing memory usage across devices
    is essential to prevent resource exhaustion during training.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿åœ¨æ··åˆé…ç½®ä¸­ï¼Œå†…å­˜é™åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è™½ç„¶æ¨¡å‹å¹¶è¡Œè§£å†³äº†å°†å¤§å‹æ¨¡å‹æ”¾å…¥è®¾å¤‡å†…å­˜çš„é—®é¢˜ï¼Œä½†æ•°æ®å¹¶è¡Œï¼ˆå¦‚å­˜å‚¨å¤šä¸ªæ•°æ®æ‰¹æ¬¡å’Œæ¢¯åº¦ç¼“å†²åŒºï¼‰çš„é¢å¤–å†…å­˜éœ€æ±‚ä»ç„¶å¯èƒ½è¶…è¿‡å¯ç”¨å®¹é‡ã€‚è¿™å¯¹äºå…·æœ‰æç«¯å¤§å‹ä¸­é—´è®¡ç®—ï¼ˆå¦‚å…·æœ‰é«˜ç»´æ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨ï¼‰çš„æ¨¡å‹å°¤å…¶å¦‚æ­¤ã€‚åœ¨è®¾å¤‡é—´å¹³è¡¡å†…å­˜ä½¿ç”¨å¯¹äºé˜²æ­¢è®­ç»ƒè¿‡ç¨‹ä¸­èµ„æºè€—å°½è‡³å…³é‡è¦ã€‚
- en: Lastly, hybrid parallelism poses challenges related to fault tolerance and debugging.
    Distributed systems are inherently more prone to hardware failures and synchronization
    errors. Debugging issues in hybrid setups can be significantly more complex than
    in standalone model or data parallelism systems, as errors may arise from interactions
    between the two approaches. Ensuring robust fault-tolerance mechanisms and designing
    tools for monitoring and debugging distributed systems are essential for maintaining
    reliability.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ··åˆå¹¶è¡Œè¿˜å¸¦æ¥äº†ä¸å®¹é”™å’Œè°ƒè¯•ç›¸å…³çš„é—®é¢˜ã€‚åˆ†å¸ƒå¼ç³»ç»Ÿå¤©ç”Ÿæ›´å®¹æ˜“å—åˆ°ç¡¬ä»¶æ•…éšœå’ŒåŒæ­¥é”™è¯¯çš„å½±å“ã€‚åœ¨æ··åˆé…ç½®ä¸­è°ƒè¯•é—®é¢˜å¯èƒ½æ¯”åœ¨ç‹¬ç«‹æ¨¡å‹æˆ–æ•°æ®å¹¶è¡Œç³»ç»Ÿä¸­è¦å¤æ‚å¾—å¤šï¼Œå› ä¸ºé”™è¯¯å¯èƒ½æºäºä¸¤ç§æ–¹æ³•ä¹‹é—´çš„äº¤äº’ã€‚ç¡®ä¿å¼ºå¤§çš„å®¹é”™æœºåˆ¶å’Œè®¾è®¡ç”¨äºç›‘æ§å’Œè°ƒè¯•åˆ†å¸ƒå¼ç³»ç»Ÿçš„å·¥å…·å¯¹äºä¿æŒå¯é æ€§è‡³å…³é‡è¦ã€‚
- en: Despite these challenges, hybrid parallelism remains an indispensable strategy
    for training large-scale machine learning models. By addressing these obstacles
    through optimized communication protocols, intelligent partitioning strategies,
    and robust fault-tolerance systems, practitioners can unlock the full potential
    of hybrid parallelism and drive innovation in AI research and applications.
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œæ··åˆå¹¶è¡Œä»ç„¶æ˜¯è®­ç»ƒå¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹ä¸å¯æˆ–ç¼ºçš„ç­–ç•¥ã€‚é€šè¿‡ä¼˜åŒ–é€šä¿¡åè®®ã€æ™ºèƒ½åˆ†åŒºç­–ç•¥å’Œå¼ºå¤§çš„å®¹é”™ç³»ç»Ÿæ¥å…‹æœè¿™äº›éšœç¢ï¼Œä»ä¸šè€…å¯ä»¥å……åˆ†å‘æŒ¥æ··åˆå¹¶è¡Œçš„æ½œåŠ›ï¼Œå¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½ç ”ç©¶å’Œåº”ç”¨çš„åˆ›æ–°ã€‚
- en: Parallelism Strategy Comparison
  id: totrans-973
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¶è¡Œç­–ç•¥æ¯”è¾ƒ
- en: The features of data parallelism, model parallelism, pipeline parallelism, and
    hybrid parallelism are summarized in [TableÂ 8.7](ch014.xhtml#tbl-parallelism-compare).
    This comparison highlights their respective focuses, memory requirements, communication
    overheads, scalability, implementation complexity, and ideal use cases. By examining
    these factors, practitioners can determine the most suitable approach for their
    training needs.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œæ··åˆå¹¶è¡Œçš„ç‰¹ç‚¹æ€»ç»“åœ¨[è¡¨8.7](ch014.xhtml#tbl-parallelism-compare)ä¸­ã€‚æ­¤æ¯”è¾ƒçªå‡ºäº†å®ƒä»¬å„è‡ªçš„ç„¦ç‚¹ã€å†…å­˜éœ€æ±‚ã€é€šä¿¡å¼€é”€ã€å¯æ‰©å±•æ€§ã€å®ç°å¤æ‚æ€§å’Œç†æƒ³ç”¨ä¾‹ã€‚é€šè¿‡åˆ†æè¿™äº›å› ç´ ï¼Œä»ä¸šè€…å¯ä»¥ç¡®å®šæœ€é€‚åˆå…¶è®­ç»ƒéœ€æ±‚çš„æ–¹æ³•ã€‚
- en: 'TableÂ 8.7: **Parallel Training Strategies**: Data, model, pipeline, and hybrid
    parallelism each address the challenges of scaling machine learning training by
    distributing workload across devices, differing in how they partition data and
    model parameters to optimize memory usage, communication, and scalability. Understanding
    these trade-offs enables practitioners to select the most effective approach for
    their specific model and infrastructure.'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.7ï¼š**å¹¶è¡Œè®­ç»ƒç­–ç•¥**ï¼šæ•°æ®ã€æ¨¡å‹ã€æµæ°´çº¿å’Œæ··åˆå¹¶è¡Œç­–ç•¥é€šè¿‡åœ¨è®¾å¤‡é—´åˆ†é…å·¥ä½œè´Ÿè½½æ¥è§£å†³æœºå™¨å­¦ä¹ è®­ç»ƒæ‰©å±•æ€§çš„æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨å¦‚ä½•åˆ’åˆ†æ•°æ®å’Œæ¨¡å‹å‚æ•°ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€é€šä¿¡å’Œå¯æ‰©å±•æ€§æ–¹é¢æœ‰æ‰€ä¸åŒã€‚ç†è§£è¿™äº›æƒè¡¡æœ‰åŠ©äºä»ä¸šè€…é€‰æ‹©æœ€é€‚åˆå…¶ç‰¹å®šæ¨¡å‹å’ŒåŸºç¡€è®¾æ–½çš„æœ€æœ‰æ•ˆæ–¹æ³•ã€‚
- en: '| **Aspect** | **Data Parallelism** | **Model Parallelism** | **Pipeline Parallelism**
    | **Hybrid Parallelism** |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '| **æ–¹é¢** | **æ•°æ®å¹¶è¡Œ** | **æ¨¡å‹å¹¶è¡Œ** | **æµæ°´çº¿å¹¶è¡Œ** | **æ··åˆå¹¶è¡Œ** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Focus** | Distributes dataset across devices, each with a full model copy
    | Distributes the model across devices, each handling a portion of the model |
    Distributes model stages in pipeline, processing microbatches concurrently | Combines
    multiple parallelism strategies for balanced scalability |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '| **ç„¦ç‚¹** | åœ¨è®¾å¤‡é—´åˆ†é…æ•°æ®é›†ï¼Œæ¯ä¸ªè®¾å¤‡æ‹¥æœ‰å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ | åœ¨è®¾å¤‡é—´åˆ†é…æ¨¡å‹ï¼Œæ¯ä¸ªè®¾å¤‡å¤„ç†æ¨¡å‹çš„ä¸€éƒ¨åˆ† | åœ¨æµæ°´çº¿ä¸­åˆ†é…æ¨¡å‹é˜¶æ®µï¼ŒåŒæ—¶å¤„ç†å¾®æ‰¹å¤„ç†
    | ç»“åˆå¤šç§å¹¶è¡Œç­–ç•¥ä»¥å®ç°å¹³è¡¡çš„å¯æ‰©å±•æ€§ |'
- en: '| **Memory Requirement per Device** | High (entire model on each device) |
    Low (model split across devices) | Low to Moderate (stages split across devices)
    | Moderate (splits model and dataset across devices) |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
  zh: '| **æ¯è®¾å¤‡çš„å†…å­˜éœ€æ±‚** | é«˜ï¼ˆæ¯ä¸ªè®¾å¤‡ä¸Šéƒ½æœ‰æ•´ä¸ªæ¨¡å‹ï¼‰ | ä½ï¼ˆæ¨¡å‹åœ¨è®¾å¤‡é—´åˆ†å‰²ï¼‰ | ä½åˆ°ä¸­ç­‰ï¼ˆé˜¶æ®µåœ¨è®¾å¤‡é—´åˆ†å‰²ï¼‰ | ä¸­ç­‰ï¼ˆåœ¨è®¾å¤‡é—´åˆ†å‰²æ¨¡å‹å’Œæ•°æ®é›†ï¼‰
    |'
- en: '| **Communication Overhead** | Moderate to High (gradient synchronization across
    devices) | High (communication for intermediate activations and gradients) | Moderate
    (activation passing between stages) | Very High (requires synchronization for
    both model and data) |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '| **é€šä¿¡å¼€é”€** | ä¸­ç­‰åˆ°é«˜ï¼ˆè®¾å¤‡é—´çš„æ¢¯åº¦åŒæ­¥ï¼‰ | é«˜ï¼ˆä¸­é—´æ¿€æ´»å’Œæ¢¯åº¦çš„é€šä¿¡ï¼‰ | ä¸­ç­‰ï¼ˆé˜¶æ®µé—´çš„æ¿€æ´»ä¼ é€’ï¼‰ | éå¸¸é«˜ï¼ˆéœ€è¦æ¨¡å‹å’Œæ•°æ®çš„åŒæ­¥ï¼‰
    |'
- en: '| **Scalability** | Good for large datasets with moderate model sizes | Good
    for very large models with smaller datasets | Good for deep models with many layers
    | Excellent for extremely large models and datasets |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '| **å¯æ‰©å±•æ€§** | é€‚ç”¨äºå¤§å‹æ•°æ®é›†ï¼Œæ¨¡å‹å¤§å°é€‚ä¸­ | é€‚ç”¨äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œæ•°æ®é›†è¾ƒå° | é€‚ç”¨äºå…·æœ‰è®¸å¤šå±‚çš„æ·±åº¦æ¨¡å‹ | é€‚ç”¨äºæå…¶å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†
    |'
- en: '| **Implementation Complexity** | Low to Moderate (relatively straightforward
    with existing tools) | Moderate to High (requires careful partitioning and coordination)
    | Moderate to High (requires pipeline scheduling and microbatch management) |
    High (complex integration of multiple parallelism strategies) |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
  zh: '| **å®ç°å¤æ‚åº¦** | ä½åˆ°ä¸­ç­‰ï¼ˆä½¿ç”¨ç°æœ‰å·¥å…·ç›¸å¯¹ç®€å•ï¼‰ | ä¸­ç­‰åˆ°é«˜ï¼ˆéœ€è¦ä»”ç»†åˆ’åˆ†å’Œåè°ƒï¼‰ | ä¸­ç­‰åˆ°é«˜ï¼ˆéœ€è¦æµæ°´çº¿è°ƒåº¦å’Œå¾®æ‰¹å¤„ç†ç®¡ç†ï¼‰ | é«˜ï¼ˆå¤šä¸ªå¹¶è¡Œç­–ç•¥çš„å¤æ‚é›†æˆï¼‰
    |'
- en: '| **Ideal Use Case** | Large datasets where model fits within a single device
    | Extremely large models that exceed single-device memory limits | Deep models
    with sequential stages that can tolerate microbatch latency | Training massive
    models on vast datasets in large-scale systems |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| **ç†æƒ³ç”¨ä¾‹** | å¤§å‹æ•°æ®é›†ï¼Œæ¨¡å‹é€‚åˆå•ä¸ªè®¾å¤‡ | è¶…å¤§æ¨¡å‹ï¼Œè¶…å‡ºå•ä¸ªè®¾å¤‡çš„å†…å­˜é™åˆ¶ | å…·æœ‰é¡ºåºé˜¶æ®µçš„æ·±åº¦æ¨¡å‹ï¼Œå¯ä»¥å®¹å¿å¾®æ‰¹å¤„ç†å»¶è¿Ÿ | åœ¨å¤§è§„æ¨¡ç³»ç»Ÿä¸­è®­ç»ƒå¤§é‡æ¨¡å‹å’Œå¤§æ•°æ®é›†
    |'
- en: '[FigureÂ 8.17](ch014.xhtml#fig-parallelism-flowchart) provides a general guideline
    for selecting parallelism strategies in distributed training systems. While the
    chart offers a structured decision-making process based on model size, dataset
    size, and scaling constraints, it is intentionally simplified. Real-world scenarios
    often involve additional complexities such as hardware heterogeneity, communication
    bandwidth, and workload imbalance, which may influence the choice of parallelism
    techniques. The chart is best viewed as a foundational tool for understanding
    the trade-offs and decision points in parallelism strategy selection. Practitioners
    should consider this guideline as a starting point and adapt it to the specific
    requirements and constraints of their systems to achieve optimal performance.'
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾8.17](ch014.xhtml#fig-parallelism-flowchart) æä¾›äº†åœ¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­é€‰æ‹©å¹¶è¡Œç­–ç•¥çš„ä¸€èˆ¬æŒ‡å—ã€‚è™½ç„¶è¯¥å›¾è¡¨åŸºäºæ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°å’Œæ‰©å±•çº¦æŸæä¾›äº†ä¸€ä¸ªç»“æ„åŒ–çš„å†³ç­–è¿‡ç¨‹ï¼Œä½†å®ƒæ˜¯æœ‰æ„ç®€åŒ–çš„ã€‚ç°å®åœºæ™¯é€šå¸¸æ¶‰åŠé¢å¤–çš„å¤æ‚æ€§ï¼Œå¦‚ç¡¬ä»¶å¼‚æ„æ€§ã€é€šä¿¡å¸¦å®½å’Œå·¥ä½œè´Ÿè½½ä¸å¹³è¡¡ï¼Œè¿™äº›éƒ½å¯èƒ½å½±å“å¹¶è¡ŒæŠ€æœ¯é€‰æ‹©ã€‚è¯¥å›¾è¡¨æœ€å¥½è¢«è§†ä¸ºç†è§£å¹¶è¡Œç­–ç•¥é€‰æ‹©ä¸­çš„æƒè¡¡å’Œå†³ç­–ç‚¹çš„åŸºçŸ³å·¥å…·ã€‚ä»ä¸šè€…åº”å°†æ­¤æŒ‡å—è§†ä¸ºèµ·ç‚¹ï¼Œå¹¶æ ¹æ®å…¶ç³»ç»Ÿçš„å…·ä½“éœ€æ±‚å’Œçº¦æŸè¿›è¡Œè°ƒæ•´ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚'
- en: '![](../media/file124.svg)'
  id: totrans-985
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file124.svg)'
- en: 'FigureÂ 8.17: **Parallelism Strategy Selection**: Distributed training systems
    use data, model, or hybrid parallelism based on model size, dataset size, and
    scaling constraints to accelerate training and efficiently utilize resources.
    This flowchart guides practitioners through a decision process, recognizing that
    real-world deployments often require adaptation due to factors like hardware heterogeneity
    and workload imbalance.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.17ï¼š**å¹¶è¡Œç­–ç•¥é€‰æ‹©**ï¼šåˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿæ ¹æ®æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°å’Œæ‰©å±•çº¦æŸä½¿ç”¨æ•°æ®ã€æ¨¡å‹æˆ–æ··åˆå¹¶è¡Œæ¥åŠ é€Ÿè®­ç»ƒå’Œé«˜æ•ˆåˆ©ç”¨èµ„æºã€‚æ­¤æµç¨‹å›¾æŒ‡å¯¼ä»ä¸šè€…é€šè¿‡å†³ç­–è¿‡ç¨‹ï¼Œè®¤è¯†åˆ°ç°å®ä¸–ç•Œçš„éƒ¨ç½²é€šå¸¸éœ€è¦æ ¹æ®ç¡¬ä»¶å¼‚æ„æ€§å’Œå·¥ä½œè´Ÿè½½ä¸å¹³è¡¡ç­‰å› ç´ è¿›è¡Œè°ƒæ•´ã€‚
- en: Framework Integration
  id: totrans-987
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¡†æ¶é›†æˆ
- en: While the theoretical foundations of distributed training establish the mathematical
    principles for scaling across multiple devices, modern frameworks provide abstractions
    that make these concepts accessible to practitioners. Understanding how frameworks
    like PyTorch translate distributed training theory into practical APIs bridges
    the gap between mathematical concepts and implementation.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åˆ†å¸ƒå¼è®­ç»ƒçš„ç†è®ºåŸºç¡€ç¡®ç«‹äº†è·¨å¤šä¸ªè®¾å¤‡çš„æ‰©å±•çš„æ•°å­¦åŸç†ï¼Œä½†ç°ä»£æ¡†æ¶æä¾›äº†æŠ½è±¡ï¼Œä½¿è¿™äº›æ¦‚å¿µå¯¹ä»ä¸šè€…å¯è®¿é—®ã€‚ç†è§£æ¡†æ¶å¦‚PyTorchå¦‚ä½•å°†åˆ†å¸ƒå¼è®­ç»ƒç†è®ºè½¬åŒ–ä¸ºå®é™…APIï¼Œå¼¥åˆäº†æ•°å­¦æ¦‚å¿µå’Œå®ç°ä¹‹é—´çš„å·®è·ã€‚
- en: Data Parallel Framework APIs
  id: totrans-989
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®å¹¶è¡Œæ¡†æ¶API
- en: The data parallelism mechanisms we explored earlierâ€”gradient averaging, AllReduce
    communication, and parameter synchronizationâ€”are abstracted through framework
    APIs that handle the complex coordination automatically. PyTorch provides two
    primary approaches that demonstrate different trade-offs between simplicity and
    performance.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰æ¢ç´¢çš„æ•°æ®å¹¶è¡Œæœºåˆ¶â€”â€”æ¢¯åº¦å¹³å‡ã€AllReduceé€šä¿¡å’Œå‚æ•°åŒæ­¥â€”â€”é€šè¿‡æ¡†æ¶APIè¿›è¡ŒæŠ½è±¡ï¼Œè¿™äº›APIè‡ªåŠ¨å¤„ç†å¤æ‚çš„åè°ƒã€‚PyTorchæä¾›äº†ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œå±•ç¤ºäº†ç®€å•æ€§å’Œæ€§èƒ½ä¹‹é—´çš„ä¸åŒæƒè¡¡ã€‚
- en: '`torch.nn.DataParallel` represents the simpler approach, automatically replicating
    the model across available GPUs within a single node. This API abstracts the gradient
    collection and averaging process, requiring minimal code changes to existing single-GPU
    training scripts. However, this simplicity comes with performance limitations,
    as the implementation uses a parameter server approach that can create communication
    bottlenecks when scaling beyond 4-8 GPUs.'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.DataParallel` ä»£è¡¨äº†ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œè‡ªåŠ¨åœ¨å•ä¸ªèŠ‚ç‚¹å†…å¯ç”¨çš„GPUä¸Šå¤åˆ¶æ¨¡å‹ã€‚æ­¤APIæŠ½è±¡äº†æ¢¯åº¦æ”¶é›†å’Œå¹³å‡è¿‡ç¨‹ï¼Œéœ€è¦æœ€å°åŒ–å¯¹ç°æœ‰å•GPUè®­ç»ƒè„šæœ¬çš„ä»£ç æ›´æ”¹ã€‚ç„¶è€Œï¼Œè¿™ç§ç®€å•æ€§ä¼´éšç€æ€§èƒ½é™åˆ¶ï¼Œå› ä¸ºå®ç°ä½¿ç”¨å‚æ•°æœåŠ¡å™¨æ–¹æ³•ï¼Œå½“æ‰©å±•åˆ°4-8ä¸ªGPUä»¥ä¸Šæ—¶å¯èƒ½ä¼šåˆ›å»ºé€šä¿¡ç“¶é¢ˆã€‚'
- en: '[PRE7]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For production scale training, `torch.distributed` provides the high-performance
    alternative that implements the efficient AllReduce communication patterns discussed
    earlier. This API requires explicit initialization of process groups and distributed
    coordination but enables the linear scaling characteristics essential for large-scale
    training.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç”Ÿäº§è§„æ¨¡çš„è®­ç»ƒï¼Œ`torch.distributed`æä¾›äº†ä¹‹å‰è®¨è®ºè¿‡çš„æœ‰æ•ˆAllReduceé€šä¿¡æ¨¡å¼çš„æ€§èƒ½æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤APIéœ€è¦æ˜¾å¼åˆå§‹åŒ–è¿›ç¨‹ç»„å’Œåˆ†å¸ƒå¼åè°ƒï¼Œä½†èƒ½å¤Ÿå®ç°å¯¹äºå¤§è§„æ¨¡è®­ç»ƒè‡³å…³é‡è¦çš„çº¿æ€§æ‰©å±•ç‰¹æ€§ã€‚
- en: '[PRE8]'
  id: totrans-994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The key insight is that `DistributedDataParallel` implements the efficient ring
    AllReduce algorithm automatically, transforming the O(n) communication complexity
    we discussed into practical code that achieves 90%+ parallel efficiency at scale.
    The framework handles device placement, gradient bucketing for efficient communication,
    and overlapping computation with communication.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®çš„æ´å¯Ÿæ˜¯`DistributedDataParallel`è‡ªåŠ¨å®ç°äº†é«˜æ•ˆçš„ç¯çŠ¶AllReduceç®—æ³•ï¼Œå°†æˆ‘ä»¬è®¨è®ºçš„O(n)é€šä¿¡å¤æ‚åº¦è½¬åŒ–ä¸ºå®é™…ä»£ç ï¼Œåœ¨è§„æ¨¡ä¸Šå®ç°äº†90%ä»¥ä¸Šçš„å¹¶è¡Œæ•ˆç‡ã€‚æ¡†æ¶å¤„ç†è®¾å¤‡æ”¾ç½®ã€æ¢¯åº¦æ¡¶åŒ–ä»¥å®ç°é«˜æ•ˆçš„é€šä¿¡ï¼Œä»¥åŠè®¡ç®—ä¸é€šä¿¡çš„é‡å ã€‚
- en: Model Parallel Framework Support
  id: totrans-996
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œæ¡†æ¶æ”¯æŒ
- en: Model parallelism requires more explicit coordination since frameworks must
    manage cross-device tensor placement and data flow. PyTorch addresses this through
    manual device placement and the emerging `torch.distributed.pipeline` API for
    pipeline parallelism.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¹¶è¡Œéœ€è¦æ›´æ˜ç¡®çš„åè°ƒï¼Œå› ä¸ºæ¡†æ¶å¿…é¡»ç®¡ç†è·¨è®¾å¤‡çš„å¼ é‡æ”¾ç½®å’Œæ•°æ®æµã€‚PyTorché€šè¿‡æ‰‹åŠ¨è®¾å¤‡æ”¾ç½®å’Œæ–°å…´çš„`torch.distributed.pipeline`
    APIæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚
- en: '[PRE9]'
  id: totrans-998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This manual approach exposes the sequential dependencies and communication overhead
    inherent in model parallelism, requiring careful management of tensor movement
    between devices. The framework automatically handles the backward pass gradient
    flow across device boundaries, but practitioners must consider the performance
    implications of frequent device transfers.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ‰‹åŠ¨æ–¹æ³•æ­ç¤ºäº†æ¨¡å‹å¹¶è¡Œä¸­å›ºæœ‰çš„é¡ºåºä¾èµ–å’Œé€šä¿¡å¼€é”€ï¼Œéœ€è¦ä»”ç»†ç®¡ç†è®¾å¤‡é—´çš„å¼ é‡ç§»åŠ¨ã€‚æ¡†æ¶è‡ªåŠ¨å¤„ç†è®¾å¤‡è¾¹ç•Œä¹‹é—´çš„åå‘ä¼ æ’­æ¢¯åº¦æµï¼Œä½†å®è·µè€…å¿…é¡»è€ƒè™‘é¢‘ç¹è®¾å¤‡è½¬ç§»çš„æ€§èƒ½å½±å“ã€‚
- en: Communication Primitives
  id: totrans-1000
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é€šä¿¡åŸè¯­
- en: 'Modern frameworks expose the communication operations that enable distributed
    training through high-level APIs. These primitives abstract the low-level NCCL
    operations while maintaining performance:'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ¡†æ¶é€šè¿‡é«˜çº§APIæš´éœ²äº†æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡æ“ä½œã€‚è¿™äº›åŸè¯­æŠ½è±¡äº†ä½çº§çš„NCCLæ“ä½œï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ï¼š
- en: '[PRE10]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These APIs translate directly to the NCCL collective operations that implement
    the efficient communication patterns discussed earlier, demonstrating how frameworks
    provide accessible interfaces to complex distributed systems concepts while maintaining
    the performance characteristics essential for production training.
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›APIç›´æ¥è½¬æ¢ä¸ºä¹‹å‰è®¨è®ºçš„NCCLé›†ä½“æ“ä½œï¼Œå®ç°äº†é«˜æ•ˆçš„é€šä¿¡æ¨¡å¼ï¼Œå±•ç¤ºäº†æ¡†æ¶å¦‚ä½•æä¾›æ˜“äºè®¿é—®çš„æ¥å£æ¥å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿæ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒç”Ÿäº§è®­ç»ƒæ‰€å¿…éœ€çš„æ€§èƒ½ç‰¹å¾ã€‚
- en: The framework abstractions enable practitioners to focus on model architecture
    and training dynamics while leveraging sophisticated distributed systems optimizations.
    This separation of concernsâ€”mathematical foundations handled by the framework,
    model design controlled by the practitionerâ€”exemplifies how modern ML systems
    balance accessibility with performance.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: æ¡†æ¶æŠ½è±¡ä½¿å®è·µè€…èƒ½å¤Ÿä¸“æ³¨äºæ¨¡å‹æ¶æ„å’Œè®­ç»ƒåŠ¨æ€ï¼ŒåŒæ—¶åˆ©ç”¨å¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿä¼˜åŒ–ã€‚è¿™ç§å…³æ³¨ç‚¹çš„åˆ†ç¦»â€”â€”æ•°å­¦åŸºç¡€ç”±æ¡†æ¶å¤„ç†ï¼Œæ¨¡å‹è®¾è®¡ç”±å®è·µè€…æ§åˆ¶â€”â€”å±•ç¤ºäº†ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿå¦‚ä½•åœ¨æ˜“ç”¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚
- en: Performance Optimization
  id: totrans-1005
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½ä¼˜åŒ–
- en: Building upon our understanding of pipeline optimizations and distributed training
    approaches, efficient training of machine learning models relies on identifying
    and addressing the factors that limit performance and scalability. This section
    explores a range of optimization techniques designed to improve the efficiency
    of training systems. By targeting specific bottlenecks, optimizing hardware and
    software interactions, and employing scalable training strategies, these methods
    help practitioners build systems that effectively utilize resources while minimizing
    training time.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹åœ¨æˆ‘ä»¬å¯¹ç®¡é“ä¼˜åŒ–å’Œåˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•çš„ç†è§£ä¹‹ä¸Šï¼Œé«˜æ•ˆè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ä¾èµ–äºè¯†åˆ«å’Œè§£å†³é™åˆ¶æ€§èƒ½å’Œå¯æ‰©å±•æ€§çš„å› ç´ ã€‚æœ¬èŠ‚æ¢è®¨äº†å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒç³»ç»Ÿçš„æ•ˆç‡ã€‚é€šè¿‡é’ˆå¯¹ç‰¹å®šç“¶é¢ˆã€ä¼˜åŒ–è½¯ç¡¬ä»¶äº¤äº’ä»¥åŠé‡‡ç”¨å¯æ‰©å±•çš„è®­ç»ƒç­–ç•¥ï¼Œè¿™äº›æ–¹æ³•å¸®åŠ©å®è·µè€…æ„å»ºæœ‰æ•ˆåˆ©ç”¨èµ„æºåŒæ—¶æœ€å°åŒ–è®­ç»ƒæ—¶é—´çš„ç³»ç»Ÿã€‚
- en: Bottleneck Analysis
  id: totrans-1007
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç“¶é¢ˆåˆ†æ
- en: Effective optimization of training systems requires a systematic approach to
    identifying and addressing performance bottlenecks. Bottlenecks can arise at various
    levels, including computation, memory, and data handling, and they directly impact
    the efficiency and scalability of the training process.
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆä¼˜åŒ–è®­ç»ƒç³»ç»Ÿéœ€è¦ä¸€ç§ç³»ç»Ÿæ€§çš„æ–¹æ³•æ¥è¯†åˆ«å’Œè§£å†³æ€§èƒ½ç“¶é¢ˆã€‚ç“¶é¢ˆå¯èƒ½å‡ºç°åœ¨å¤šä¸ªå±‚é¢ï¼ŒåŒ…æ‹¬è®¡ç®—ã€å†…å­˜å’Œæ•°æ®å¤„ç†ï¼Œå®ƒä»¬ç›´æ¥å½±å“è®­ç»ƒè¿‡ç¨‹çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
- en: Computational bottlenecks can significantly impact training efficiency. One
    common bottleneck occurs when computational resources, such as GPUs or TPUs, are
    underutilized. This can happen due to imbalanced workloads or inefficient parallelization
    strategies. For example, if one device completes its assigned computation faster
    than others, it remains idle while waiting for the slower devices to catch up.
    Such inefficiencies reduce the overall training throughput.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç“¶é¢ˆå¯èƒ½ä¼šæ˜¾è‘—å½±å“è®­ç»ƒæ•ˆç‡ã€‚ä¸€ä¸ªå¸¸è§ç“¶é¢ˆå‘ç”Ÿåœ¨è®¡ç®—èµ„æºï¼Œå¦‚GPUæˆ–TPUï¼Œåˆ©ç”¨ç‡ä½æ—¶ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå·¥ä½œè´Ÿè½½ä¸å¹³è¡¡æˆ–ä¸é«˜æ•ˆçš„å¹¶è¡ŒåŒ–ç­–ç•¥é€ æˆçš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªè®¾å¤‡æ¯”å…¶ä»–è®¾å¤‡æ›´å¿«åœ°å®Œæˆå…¶åˆ†é…çš„è®¡ç®—ï¼Œå®ƒå°†ä¿æŒç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…è¾ƒæ…¢çš„è®¾å¤‡èµ¶ä¸Šã€‚è¿™ç§ä½æ•ˆé™ä½äº†æ•´ä½“è®­ç»ƒååé‡ã€‚
- en: Memory-related bottlenecks are particularly challenging when dealing with large
    models. Insufficient memory can lead to frequent swapping of data between device
    memory and slower storage, significantly slowing down the training process. In
    some cases, the memory required to store intermediate activations during the forward
    and backward passes can exceed the available capacity, forcing the system to employ
    techniques such as gradient checkpointing, which trade off computational efficiency
    for memory savings.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†å¤§å‹æ¨¡å‹æ—¶ï¼Œä¸å†…å­˜ç›¸å…³çš„ç“¶é¢ˆå°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å†…å­˜ä¸è¶³å¯èƒ½å¯¼è‡´æ•°æ®åœ¨è®¾å¤‡å†…å­˜å’Œè¾ƒæ…¢çš„å­˜å‚¨ä¹‹é—´é¢‘ç¹äº¤æ¢ï¼Œä»è€Œæ˜¾è‘—å‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå­˜å‚¨æ­£å‘å’Œåå‘ä¼ é€’ä¸­ä¸­é—´æ¿€æ´»æ‰€éœ€çš„å†…å­˜å¯èƒ½è¶…è¿‡å¯ç”¨å®¹é‡ï¼Œè¿«ä½¿ç³»ç»Ÿé‡‡ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯ä»¥ç‰ºç‰²è®¡ç®—æ•ˆç‡æ¥èŠ‚çœå†…å­˜ã€‚
- en: Data handling bottlenecks can severely limit the utilization of computational
    resources. Training systems often rely on a continuous supply of data to keep
    computational resources fully utilized. If data loading and preprocessing are
    not optimized, computational devices may sit idle while waiting for new batches
    of data to arrive. This issue is particularly prevalent when training on large
    datasets stored on networked file systems or remote storage solutions. As illustrated
    in [FigureÂ 8.18](ch014.xhtml#fig-tf-bottleneck-trace), profiling traces can reveal
    cases where the GPU remains underutilized due to slow data loading, highlighting
    the importance of efficient input pipelines.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¤„ç†ç“¶é¢ˆå¯èƒ½ä¼šä¸¥é‡é™åˆ¶è®¡ç®—èµ„æºçš„åˆ©ç”¨ç‡ã€‚è®­ç»ƒç³»ç»Ÿé€šå¸¸ä¾èµ–äºè¿ç»­çš„æ•°æ®ä¾›åº”æ¥ä¿æŒè®¡ç®—èµ„æºå……åˆ†åˆ©ç”¨ã€‚å¦‚æœæ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ²¡æœ‰å¾—åˆ°ä¼˜åŒ–ï¼Œè®¡ç®—è®¾å¤‡å¯èƒ½ä¼šç©ºé—²ç­‰å¾…æ–°æ‰¹æ¬¡æ•°æ®åˆ°è¾¾ã€‚å½“åœ¨å­˜å‚¨åœ¨ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿæˆ–è¿œç¨‹å­˜å‚¨è§£å†³æ–¹æ¡ˆä¸Šçš„å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶ï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºæ™®éã€‚å¦‚å›¾8.18[å›¾](ch014.xhtml#fig-tf-bottleneck-trace)æ‰€ç¤ºï¼Œåˆ†æè·Ÿè¸ªå¯ä»¥æ­ç¤ºç”±äºæ•°æ®åŠ è½½ç¼“æ…¢å¯¼è‡´çš„GPUåˆ©ç”¨ç‡ä½çš„æƒ…å†µï¼Œçªå‡ºäº†é«˜æ•ˆè¾“å…¥ç®¡é“çš„é‡è¦æ€§ã€‚
- en: '![](../media/file125.png)'
  id: totrans-1012
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file125.png)'
- en: 'FigureÂ 8.18: **GPU Underutilization**: Profiling reveals identify data loading
    as a bottleneck, preventing full GPU utilization during training and increasing
    overall training time. The gaps in GPU activity indicate the device frequently
    waits for input data, suggesting optimization of the data pipeline is necessary
    to maximize computational throughput.'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.18ï¼š**GPUåˆ©ç”¨ç‡ä½**ï¼šåˆ†ææ˜¾ç¤ºï¼Œæ•°æ®åŠ è½½æˆä¸ºç“¶é¢ˆï¼Œé˜»ç¢äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„GPUå……åˆ†åˆ©ç”¨ï¼Œå¹¶å¢åŠ äº†æ•´ä½“è®­ç»ƒæ—¶é—´ã€‚GPUæ´»åŠ¨ä¸­çš„é—´éš™è¡¨æ˜è®¾å¤‡ç»å¸¸ç­‰å¾…è¾“å…¥æ•°æ®ï¼Œè¿™è¡¨æ˜ä¼˜åŒ–æ•°æ®ç®¡é“å¯¹äºæœ€å¤§åŒ–è®¡ç®—ååé‡æ˜¯å¿…è¦çš„ã€‚
- en: Identifying these bottlenecks typically involves using profiling tools to analyze
    the performance of the training system. Tools integrated into machine learning
    frameworks, such as PyTorchâ€™s `torch.profiler` or TensorFlowâ€™s `tf.data` analysis
    utilities, can provide detailed insights into where time and resources are being
    spent during training. By pinpointing the specific stages or operations that are
    causing delays, practitioners can design targeted optimizations to address these
    issues effectively.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: è¯†åˆ«è¿™äº›ç“¶é¢ˆé€šå¸¸æ¶‰åŠä½¿ç”¨åˆ†æå·¥å…·æ¥åˆ†æè®­ç»ƒç³»ç»Ÿçš„æ€§èƒ½ã€‚é›†æˆåˆ°æœºå™¨å­¦ä¹ æ¡†æ¶ä¸­çš„å·¥å…·ï¼Œå¦‚PyTorchçš„`torch.profiler`æˆ–TensorFlowçš„`tf.data`åˆ†æå·¥å…·ï¼Œå¯ä»¥æä¾›æœ‰å…³è®­ç»ƒæœŸé—´æ—¶é—´å’Œèµ„æºæ¶ˆè€—çš„è¯¦ç»†ä¿¡æ¯ã€‚é€šè¿‡ç¡®å®šå¯¼è‡´å»¶è¿Ÿçš„å…·ä½“é˜¶æ®µæˆ–æ“ä½œï¼Œä»ä¸šè€…å¯ä»¥è®¾è®¡æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–æ¥æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚
- en: System-Level Techniques
  id: totrans-1015
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿçº§æŠ€æœ¯
- en: After identifying the bottlenecks in a training system, the next step is to
    implement optimizations at the system level. These optimizations target the underlying
    hardware, data flow, and resource allocation to improve overall performance and
    scalability.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯†åˆ«è®­ç»ƒç³»ç»Ÿä¸­çš„ç“¶é¢ˆåï¼Œä¸‹ä¸€æ­¥æ˜¯åœ¨ç³»ç»Ÿçº§åˆ«å®æ–½ä¼˜åŒ–ã€‚è¿™äº›ä¼˜åŒ–é’ˆå¯¹åº•å±‚ç¡¬ä»¶ã€æ•°æ®æµå’Œèµ„æºåˆ†é…ï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚
- en: One essential technique is profiling training workloads[35](#fn35). Profiling
    involves collecting detailed metrics about the systemâ€™s performance during training,
    such as computation times, memory usage, and communication overhead. These metrics
    help reveal inefficiencies, such as imbalanced resource usage or excessive time
    spent in specific stages of the training pipeline. Profiling tools such as NVIDIA
    Nsight Systems or TensorFlow Profiler can provide actionable insights, enabling
    developers to make informed adjustments to their training configurations.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§åŸºæœ¬æŠ€æœ¯æ˜¯åˆ†æè®­ç»ƒå·¥ä½œè´Ÿè½½[35](#fn35)ã€‚åˆ†ææ¶‰åŠæ”¶é›†å…³äºç³»ç»Ÿåœ¨è®­ç»ƒæœŸé—´æ€§èƒ½çš„è¯¦ç»†æŒ‡æ ‡ï¼Œä¾‹å¦‚è®¡ç®—æ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œé€šä¿¡å¼€é”€ã€‚è¿™äº›æŒ‡æ ‡æœ‰åŠ©äºæ­ç¤ºä½æ•ˆä¹‹å¤„ï¼Œä¾‹å¦‚èµ„æºä½¿ç”¨ä¸å¹³è¡¡æˆ–è®­ç»ƒç®¡é“ç‰¹å®šé˜¶æ®µçš„è¿‡åº¦æ—¶é—´æ¶ˆè€—ã€‚NVIDIA
    Nsight Systemsæˆ–TensorFlow Profilerç­‰åˆ†æå·¥å…·å¯ä»¥æä¾›å¯æ“ä½œçš„è§è§£ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿå¯¹ä»–ä»¬çš„è®­ç»ƒé…ç½®åšå‡ºæ˜æ™ºçš„è°ƒæ•´ã€‚
- en: Leveraging hardware-specific features is another critical aspect of system-level
    optimization. Modern accelerators, such as GPUs and TPUs, include specialized
    capabilities that can significantly enhance performance when utilized effectively.
    For instance, mixed precision training, which uses lower-precision floating-point
    formats like FP16 or bfloat16 for computations, can dramatically reduce memory
    usage and improve throughput without sacrificing model accuracy. Similarly, tensor
    cores in NVIDIA GPUs are designed to accelerate matrix operations, a common computational
    workload in deep learning, making them ideal for optimizing forward and backward
    passes.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ç‰¹å®šç¡¬ä»¶çš„åŠŸèƒ½æ˜¯ç³»ç»Ÿçº§ä¼˜åŒ–çš„å¦ä¸€ä¸ªå…³é”®æ–¹é¢ã€‚ç°ä»£åŠ é€Ÿå™¨ï¼Œå¦‚GPUå’ŒTPUï¼ŒåŒ…æ‹¬å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½çš„ä¸“ç”¨åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œæ··åˆç²¾åº¦è®­ç»ƒï¼Œä½¿ç”¨FP16æˆ–bfloat16ç­‰ä½ç²¾åº¦æµ®ç‚¹æ ¼å¼è¿›è¡Œè®¡ç®—ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜ååé‡ï¼Œè€Œä¸ä¼šç‰ºç‰²æ¨¡å‹ç²¾åº¦ã€‚åŒæ ·ï¼ŒNVIDIA
    GPUä¸­çš„å¼ é‡æ ¸å¿ƒæ—¨åœ¨åŠ é€ŸçŸ©é˜µè¿ç®—ï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¸¸è§çš„è®¡ç®—å·¥ä½œè´Ÿè½½ï¼Œä½¿å®ƒä»¬éå¸¸é€‚åˆä¼˜åŒ–æ­£å‘å’Œåå‘ä¼ æ’­ã€‚
- en: Data pipeline optimization is also an important consideration at the system
    level. Ensuring that data is loaded, preprocessed, and delivered to the training
    devices efficiently can eliminate potential bottlenecks caused by slow data delivery.
    Techniques such as caching frequently used data, prefetching batches to overlap
    computation and data loading, and using efficient data storage formats like TFRecord
    or RecordIO can help maintain a steady flow of data to computational devices.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ä¼˜åŒ–ä¹Ÿæ˜¯ç³»ç»Ÿçº§çš„ä¸€ä¸ªé‡è¦è€ƒè™‘å› ç´ ã€‚ç¡®ä¿æ•°æ®è¢«é«˜æ•ˆåœ°åŠ è½½ã€é¢„å¤„ç†å¹¶äº¤ä»˜ç»™è®­ç»ƒè®¾å¤‡ï¼Œå¯ä»¥æ¶ˆé™¤ç”±ç¼“æ…¢çš„æ•°æ®ä¼ è¾“å¼•èµ·çš„æ½œåœ¨ç“¶é¢ˆã€‚ä¾‹å¦‚ï¼Œç¼“å­˜å¸¸ç”¨æ•°æ®ã€é¢„å–æ‰¹æ¬¡ä»¥é‡å è®¡ç®—å’Œæ•°æ®åŠ è½½ï¼Œä»¥åŠä½¿ç”¨TFRecordæˆ–RecordIOç­‰é«˜æ•ˆæ•°æ®å­˜å‚¨æ ¼å¼ç­‰æŠ€æœ¯ï¼Œå¯ä»¥å¸®åŠ©ä¿æŒæ•°æ®æµå‘è®¡ç®—è®¾å¤‡çš„ä¸€è‡´æ€§ã€‚
- en: Software-Level Techniques
  id: totrans-1020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è½¯ä»¶çº§æŠ€æœ¯
- en: In addition to system-level adjustments, software-level optimizations focus
    on improving the efficiency of training algorithms and their implementation within
    machine learning frameworks.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç³»ç»Ÿçº§è°ƒæ•´ä¹‹å¤–ï¼Œè½¯ä»¶çº§ä¼˜åŒ–è¿˜å…³æ³¨æé«˜è®­ç»ƒç®—æ³•åŠå…¶åœ¨æœºå™¨å­¦ä¹ æ¡†æ¶ä¸­çš„å®ç°æ•ˆç‡ã€‚
- en: One effective software-level optimization is the use of fused kernels. In traditional
    implementations, operations like matrix multiplications, activation functions,
    and gradient calculations are often executed as separate steps. Fused kernels
    combine these operations into a single optimized routine, reducing the overhead
    associated with launching multiple operations and improving cache utilization.
    Many frameworks, such as PyTorch and TensorFlow, automatically apply kernel fusion
    where possible, but developers can further optimize custom operations by explicitly
    using libraries like cuBLAS or cuDNN.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æœ‰æ•ˆçš„è½¯ä»¶çº§ä¼˜åŒ–æ˜¯ä½¿ç”¨èåˆå†…æ ¸ã€‚åœ¨ä¼ ç»Ÿå®ç°ä¸­ï¼ŒçŸ©é˜µä¹˜æ³•ã€æ¿€æ´»å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—ç­‰æ“ä½œé€šå¸¸ä½œä¸ºå•ç‹¬çš„æ­¥éª¤æ‰§è¡Œã€‚èåˆå†…æ ¸å°†è¿™äº›æ“ä½œç»„åˆæˆä¸€ä¸ªå•ä¸€çš„ä¼˜åŒ–ä¾‹ç¨‹ï¼Œå‡å°‘äº†å¯åŠ¨å¤šä¸ªæ“ä½œç›¸å…³çš„å¼€é”€ï¼Œå¹¶æé«˜äº†ç¼“å­˜åˆ©ç”¨ç‡ã€‚è®¸å¤šæ¡†æ¶ï¼Œå¦‚PyTorchå’ŒTensorFlowï¼Œä¼šè‡ªåŠ¨åœ¨å¯èƒ½çš„æƒ…å†µä¸‹åº”ç”¨å†…æ ¸èåˆï¼Œä½†å¼€å‘è€…å¯ä»¥é€šè¿‡æ˜¾å¼ä½¿ç”¨cuBLASæˆ–cuDNNç­‰åº“è¿›ä¸€æ­¥ä¼˜åŒ–è‡ªå®šä¹‰æ“ä½œã€‚
- en: Dynamic graph execution is another useful technique for software-level optimization.
    In frameworks that support dynamic computation graphs, such as PyTorch, the graph
    of operations is constructed on-the-fly during each training iteration. This flexibility
    allows for fine-grained optimizations based on the specific inputs and outputs
    of a given iteration. Dynamic graphs also enable more efficient handling of variable-length
    sequences, such as those encountered in natural language processing tasks.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€å›¾æ‰§è¡Œæ˜¯è½¯ä»¶çº§åˆ«ä¼˜åŒ–çš„å¦ä¸€ç§æœ‰ç”¨æŠ€æœ¯ã€‚åœ¨æ”¯æŒåŠ¨æ€è®¡ç®—å›¾æ¡†æ¶ä¸­ï¼Œå¦‚PyTorchï¼Œæ“ä½œå›¾åœ¨æ¯ä¸ªè®­ç»ƒè¿­ä»£ä¸­å®æ—¶æ„å»ºã€‚è¿™ç§çµæ´»æ€§å…è®¸æ ¹æ®ç‰¹å®šè¿­ä»£çš„ç‰¹å®šè¾“å…¥å’Œè¾“å‡ºè¿›è¡Œç»†ç²’åº¦ä¼˜åŒ–ã€‚åŠ¨æ€å›¾è¿˜ä½¿å¾—æ›´æœ‰æ•ˆåœ°å¤„ç†å¯å˜é•¿åº¦åºåˆ—æˆä¸ºå¯èƒ½ï¼Œä¾‹å¦‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­é‡åˆ°çš„æƒ…å†µã€‚
- en: Gradient accumulation is an additional strategy that can be implemented at the
    software level to address memory constraints. Instead of updating model parameters
    after every batch, gradient accumulation allows the system to compute gradients
    over multiple smaller batches and update parameters only after aggregating them.
    This approach effectively increases the batch size without requiring additional
    memory, enabling training on larger datasets or models.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯æ˜¯ä¸€ç§å¯ä»¥åœ¨è½¯ä»¶çº§åˆ«å®æ–½çš„é¢å¤–ç­–ç•¥ï¼Œç”¨äºè§£å†³å†…å­˜é™åˆ¶é—®é¢˜ã€‚ä¸æ˜¯åœ¨æ¯æ¬¡æ‰¹é‡æ›´æ–°åæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œæ¢¯åº¦ç´¯ç§¯å…è®¸ç³»ç»Ÿåœ¨å¤šä¸ªè¾ƒå°çš„æ‰¹é‡ä¸Šè®¡ç®—æ¢¯åº¦ï¼Œå¹¶åœ¨èšåˆåæ›´æ–°å‚æ•°ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å¢åŠ äº†æ‰¹é‡å¤§å°ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„å†…å­˜ï¼Œä½¿å¾—å¯ä»¥åœ¨æ›´å¤§çš„æ•°æ®é›†æˆ–æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: Scale-Up Strategies
  id: totrans-1025
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰©å±•ç­–ç•¥
- en: Scaling techniques aim to extend the capabilities of training systems to handle
    larger datasets and models by optimizing the training configuration and resource
    allocation.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©å±•æŠ€æœ¯æ—¨åœ¨é€šè¿‡ä¼˜åŒ–è®­ç»ƒé…ç½®å’Œèµ„æºåˆ†é…æ¥æ‰©å±•è®­ç»ƒç³»ç»Ÿçš„èƒ½åŠ›ï¼Œä»¥å¤„ç†æ›´å¤§çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚
- en: One common scaling technique is batch size scaling. Increasing the batch size
    can reduce the number of synchronization steps required during training, as fewer
    updates are needed to process the same amount of data. This approach contrasts
    with the dynamic batching strategies used in inference serving, where the goal
    is optimizing throughput for variable-length requests rather than training convergence.
    However, larger batch sizes may introduce challenges, such as slower convergence
    or reduced generalization. Techniques like learning rate scaling and warmup schedules[36](#fn36)
    can help mitigate these issues, ensuring stable and effective training even with
    large batches.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°æ‰©å±•æ˜¯ä¸€ç§å¸¸è§çš„æ‰©å±•æŠ€æœ¯ã€‚å¢åŠ æ‰¹é‡å¤§å°å¯ä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€éœ€çš„åŒæ­¥æ­¥éª¤æ•°é‡ï¼Œå› ä¸ºå¤„ç†ç›¸åŒæ•°é‡çš„æ•°æ®éœ€è¦çš„æ›´æ–°è¾ƒå°‘ã€‚è¿™ç§æ–¹æ³•ä¸æ¨ç†æœåŠ¡ä¸­ä½¿ç”¨çš„åŠ¨æ€æ‰¹é‡ç­–ç•¥å½¢æˆå¯¹æ¯”ï¼Œåè€…æ—¨åœ¨ä¼˜åŒ–å¯å˜é•¿åº¦è¯·æ±‚çš„ååé‡ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ”¶æ•›ã€‚ç„¶è€Œï¼Œè¾ƒå¤§çš„æ‰¹é‡å¤§å°å¯èƒ½ä¼šå¼•å…¥æŒ‘æˆ˜ï¼Œå¦‚æ”¶æ•›é€Ÿåº¦å˜æ…¢æˆ–æ³›åŒ–èƒ½åŠ›é™ä½ã€‚å­¦ä¹ ç‡ç¼©æ”¾å’Œé¢„çƒ­è®¡åˆ’ç­‰æŠ€æœ¯å¯ä»¥å¸®åŠ©ç¼“è§£è¿™äº›é—®é¢˜ï¼Œç¡®ä¿å³ä½¿åœ¨å¤§æ‰¹é‡æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œç¨³å®šæœ‰æ•ˆçš„è®­ç»ƒã€‚
- en: Layer-freezing strategies provide another method for scaling training systems
    efficiently. In many scenarios, particularly in transfer learning, the lower layers
    of a model capture general features and do not need frequent updates. By freezing
    these layers and allowing only the upper layers to train, memory and computational
    resources can be conserved, enabling the system to focus its efforts on fine-tuning
    the most critical parts of the model.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: å±‚å†»ç»“ç­–ç•¥ä¸ºé«˜æ•ˆæ‰©å±•è®­ç»ƒç³»ç»Ÿæä¾›äº†å¦ä¸€ç§æ–¹æ³•ã€‚åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨è¿ç§»å­¦ä¹ ä¸­ï¼Œæ¨¡å‹çš„åº•å±‚æ•è·äº†é€šç”¨ç‰¹å¾ï¼Œä¸éœ€è¦é¢‘ç¹æ›´æ–°ã€‚é€šè¿‡å†»ç»“è¿™äº›å±‚ï¼Œåªå…è®¸ä¸Šå±‚è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿé›†ä¸­ç²¾åŠ›å¾®è°ƒæ¨¡å‹çš„æœ€å…³é”®éƒ¨åˆ†ã€‚
- en: While distributed training techniques provide one dimension of scaling, the
    computational efficiency of individual devices within distributed systems determines
    overall performance. The optimization techniques and parallelization strategies
    we have explored achieve their full potential only when executed on hardware architectures
    designed to maximize throughput for machine learning workloads. This motivates
    our examination of specialized hardware platforms that accelerate the mathematical
    operations underlying all training scenarios.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯æä¾›äº†ä¸€ä¸ªæ‰©å±•ç»´åº¦ï¼Œä½†åˆ†å¸ƒå¼ç³»ç»Ÿå†…å•ä¸ªè®¾å¤‡çš„è®¡ç®—æ•ˆç‡å†³å®šäº†æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬æ¢ç´¢çš„ä¼˜åŒ–æŠ€æœ¯å’Œå¹¶è¡ŒåŒ–ç­–ç•¥åªæœ‰åœ¨æ‰§è¡Œäºæ—¨åœ¨æœ€å¤§åŒ–æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ååé‡çš„ç¡¬ä»¶æ¶æ„ä¸Šæ—¶æ‰èƒ½å‘æŒ¥å…¶å…¨éƒ¨æ½œåŠ›ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬ç ”ç©¶ä¸“é—¨åŒ–çš„ç¡¬ä»¶å¹³å°ï¼Œè¿™äº›å¹³å°å¯ä»¥åŠ é€Ÿæ‰€æœ‰è®­ç»ƒåœºæ™¯èƒŒåçš„æ•°å­¦è¿ç®—ã€‚
- en: Hardware Acceleration
  id: totrans-1030
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶åŠ é€Ÿ
- en: The optimization techniques we have discussed operate within the constraints
    imposed by underlying hardware architectures. The evolution of specialized machine
    learning hardware represents an important development in addressing the computational
    demands of modern training systems. Each hardware architecture, such as GPUs,
    TPUs, FPGAs, and ASICs, embodies distinct design philosophies and engineering
    trade-offs that optimize for specific aspects of the training process. These specialized
    processors have significantly altered the scalability and efficiency constraints
    of machine learning systems, enabling advances in model complexity and training
    speed. This hardware evolution builds upon the foundational understanding of ML
    system design principles established in [ChapterÂ 2](ch008.xhtml#sec-ml-systems).
    We briefly examine the architectural principles, performance characteristics,
    and practical applications of each hardware type, highlighting their important
    role in shaping the future capabilities of machine learning training systems.
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€è®¨è®ºçš„ä¼˜åŒ–æŠ€æœ¯æ˜¯åœ¨åº•å±‚ç¡¬ä»¶æ¶æ„çš„é™åˆ¶ä¸‹è¿›è¡Œçš„ã€‚ä¸“ç”¨æœºå™¨å­¦ä¹ ç¡¬ä»¶çš„å‘å±•ä»£è¡¨äº†åº”å¯¹ç°ä»£è®­ç»ƒç³»ç»Ÿè®¡ç®—éœ€æ±‚çš„é‡è¦è¿›å±•ã€‚æ¯ç§ç¡¬ä»¶æ¶æ„ï¼Œå¦‚GPUã€TPUã€FPGAå’ŒASICï¼Œéƒ½ä½“ç°äº†ç‹¬ç‰¹çš„è®¾è®¡å“²å­¦å’Œå·¥ç¨‹æƒè¡¡ï¼Œä»¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹çš„ç‰¹å®šæ–¹é¢ã€‚è¿™äº›ä¸“ç”¨å¤„ç†å™¨æ˜¾è‘—æ”¹å˜äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡çº¦æŸï¼Œä½¿å¾—æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒé€Ÿåº¦çš„è¿›æ­¥æˆä¸ºå¯èƒ½ã€‚è¿™ç§ç¡¬ä»¶æ¼”å˜å»ºç«‹åœ¨[ç¬¬2ç« ](ch008.xhtml#sec-ml-systems)ä¸­å»ºç«‹çš„MLç³»ç»Ÿè®¾è®¡åŸåˆ™çš„åŸºç¡€ä¹‹ä¸Šã€‚æˆ‘ä»¬ç®€è¦è€ƒå¯Ÿäº†æ¯ç§ç¡¬ä»¶ç±»å‹çš„æ¶æ„åŸåˆ™ã€æ€§èƒ½ç‰¹å¾å’Œå®é™…åº”ç”¨ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¡‘é€ æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿæœªæ¥èƒ½åŠ›æ–¹é¢çš„é‡è¦ä½œç”¨ã€‚
- en: GPUs
  id: totrans-1032
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPU
- en: Machine learning training systems demand immense computational power to process
    large datasets, perform gradient computations, and update model parameters efficiently.
    GPUs have emerged as a critical technology to meet these requirements ([FigureÂ 8.19](ch014.xhtml#fig-training-gpus)),
    primarily due to their highly parallelized architecture and ability to execute
    the dense linear algebra operations central to neural network training ([Dally,
    Keckler, and Kirk 2021](ch058.xhtml#ref-dally2021evolution)).
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿéœ€è¦å·¨å¤§çš„è®¡ç®—èƒ½åŠ›æ¥å¤„ç†å¤§é‡æ•°æ®é›†ï¼Œæ‰§è¡Œæ¢¯åº¦è®¡ç®—å’Œé«˜æ•ˆæ›´æ–°æ¨¡å‹å‚æ•°ã€‚GPUå·²æˆä¸ºæ»¡è¶³è¿™äº›éœ€æ±‚çš„å…³é”®æŠ€æœ¯([å›¾8.19](ch014.xhtml#fig-training-gpus))ï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶é«˜åº¦å¹¶è¡Œçš„æ¶æ„å’Œæ‰§è¡Œç¥ç»ç½‘ç»œè®­ç»ƒä¸­è‡³å…³é‡è¦çš„å¯†é›†çº¿æ€§ä»£æ•°è¿ç®—çš„èƒ½åŠ›([Dally,
    Keckler, å’Œ Kirk 2021](ch058.xhtml#ref-dally2021evolution))ã€‚
- en: '![](../media/file126.png)'
  id: totrans-1034
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file126.png)'
- en: 'FigureÂ 8.19: **GPU Acceleration Trends**: Successive GPU generations deliver
    exponential increases in FLOPS, enabling training of increasingly large and complex
    machine learning models and driving breakthroughs in areas like natural language
    processing. These advancements, spanning from pascal to blackwell, showcase the
    critical role of specialized hardware in overcoming the computational demands
    of modern AI.'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.19ï¼š**GPUåŠ é€Ÿè¶‹åŠ¿**ï¼šè¿ç»­çš„GPUä»£é™…å¸¦æ¥äº†FLOPSçš„æŒ‡æ•°çº§å¢é•¿ï¼Œä½¿å¾—è®­ç»ƒè¶Šæ¥è¶Šå¤§å‹å’Œå¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œå¹¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸæ¨åŠ¨äº†çªç ´ã€‚ä»pascalåˆ°blackwellçš„è¿™äº›è¿›æ­¥å±•ç¤ºäº†ä¸“ç”¨ç¡¬ä»¶åœ¨å…‹æœç°ä»£äººå·¥æ™ºèƒ½è®¡ç®—éœ€æ±‚ä¸­çš„å…³é”®ä½œç”¨ã€‚
- en: From the perspective of training pipeline architecture, GPUs address several
    key bottlenecks. The large number of cores in GPUs allows for simultaneous processing
    of thousands of matrix multiplications, accelerating the forward and backward
    passes of training. In systems where data throughput limits GPU utilization, prefetching
    and caching mechanisms help maintain a steady flow of data. These optimizations,
    previously discussed in training pipeline design, are critical to unlocking the
    full potential of GPUs ([David A. Patterson and Hennessy 2021b](ch058.xhtml#ref-Patterson2021)).
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒç®¡é“æ¶æ„çš„è§’åº¦æ¥çœ‹ï¼ŒGPUè§£å†³äº†å‡ ä¸ªå…³é”®ç“¶é¢ˆã€‚GPUä¸­å¤§é‡æ ¸å¿ƒçš„å­˜åœ¨å…è®¸åŒæ—¶å¤„ç†æ•°åƒä¸ªçŸ©é˜µä¹˜æ³•ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒçš„å‰å‘å’Œåå‘ä¼ é€’ã€‚åœ¨æ•°æ®ååé‡é™åˆ¶GPUåˆ©ç”¨ç‡çš„ç³»ç»Ÿä¸­ï¼Œé¢„å–å’Œç¼“å­˜æœºåˆ¶æœ‰åŠ©äºä¿æŒæ•°æ®æµçš„ç¨³å®šã€‚è¿™äº›ä¼˜åŒ–ï¼Œåœ¨è®­ç»ƒç®¡é“è®¾è®¡ä¸­å·²è®¨è®ºè¿‡ï¼Œå¯¹äºé‡Šæ”¾GPUçš„å…¨éƒ¨æ½œåŠ›è‡³å…³é‡è¦([David
    A. Patterson å’Œ Hennessy 2021b](ch058.xhtml#ref-Patterson2021))ã€‚
- en: In distributed training systems, GPUs enable scalable strategies such as data
    parallelism and model parallelism. NVIDIAâ€™s ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[37](#fn37)
    for multi-GPU communication, facilitates efficient parameter synchronization,
    a frequent challenge in large-scale setups. For example, in training large models
    like GPT-3[38](#fn38), GPUs were used in tandem with distributed frameworks to
    split computations across thousands of devices while addressing memory and compute
    scaling issues ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)).
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ï¼ŒGPUä½¿å¾—å¯æ‰©å±•ç­–ç•¥å¦‚æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œæˆä¸ºå¯èƒ½ã€‚NVIDIAçš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬[NCCL](https://developer.nvidia.com/nccl)[37](#fn37)ç­‰ç”¨äºå¤šGPUé€šä¿¡çš„å·¥å…·ï¼Œä¿ƒè¿›äº†é«˜æ•ˆçš„å‚æ•°åŒæ­¥ï¼Œè¿™æ˜¯å¤§è§„æ¨¡è®¾ç½®ä¸­å¸¸è§çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒåƒGPT-3[38](#fn38)è¿™æ ·çš„å¤§å‹æ¨¡å‹æ—¶ï¼ŒGPUä¸åˆ†å¸ƒå¼æ¡†æ¶ååŒä½¿ç”¨ï¼Œå°†è®¡ç®—åˆ†å¸ƒåœ¨æ•°åƒä¸ªè®¾å¤‡ä¸Šï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—æ‰©å±•é—®é¢˜([T.
    Brownç­‰äºº 2020](ch058.xhtml#ref-brown2020language))ã€‚
- en: Hardware-specific features further enhance GPU performance. NVIDIAâ€™s tensor
    cores[39](#fn39), for instance, are optimized for mixed-precision training, which
    reduces memory usage while maintaining numerical stability ([Micikevicius et al.
    2017](ch058.xhtml#ref-micikevicius2017mixed)). This directly addresses memory
    constraints, a common bottleneck in training massive models. Combined with software-level
    optimizations like fused kernels, GPUs deliver substantial speedups in both single-device
    and multi-device configurations.
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶ç‰¹å®šçš„ç‰¹æ€§è¿›ä¸€æ­¥æå‡äº†GPUçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒNVIDIAçš„tensor cores[39](#fn39)é’ˆå¯¹æ··åˆç²¾åº¦è®­ç»ƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿™å‡å°‘äº†å†…å­˜ä½¿ç”¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ•°å€¼ç¨³å®šæ€§([Micikeviciusç­‰äºº
    2017](ch058.xhtml#ref-micikevicius2017mixed))ã€‚è¿™ç›´æ¥è§£å†³äº†è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ—¶å¸¸è§çš„å†…å­˜é™åˆ¶ç“¶é¢ˆã€‚ç»“åˆè½¯ä»¶å±‚é¢çš„ä¼˜åŒ–ï¼Œå¦‚èåˆå†…æ ¸ï¼ŒGPUåœ¨å•è®¾å¤‡å’Œå¤šè®¾å¤‡é…ç½®ä¸­éƒ½èƒ½æä¾›æ˜¾è‘—çš„åŠ é€Ÿã€‚
- en: A case study that exemplifies the role of GPUs in machine learning training
    is OpenAIâ€™s use of NVIDIA hardware for large language models. Training GPT-3,
    with its 175 billion parameters, required distributed processing across thousands
    of V100 GPUs. The combination of GPU-optimized frameworks, advanced communication
    protocols, and hardware features enabled OpenAI to achieve this ambitious scale
    efficiently ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)). The privacy
    and security implications of such large-scale trainingâ€”including data governance
    and model securityâ€”are addressed integratedly in [ChapterÂ 15](ch021.xhtml#sec-security-privacy).
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä½“ç°GPUåœ¨æœºå™¨å­¦ä¹ è®­ç»ƒä¸­ä½œç”¨çš„æ¡ˆä¾‹ç ”ç©¶æ˜¯OpenAIä½¿ç”¨NVIDIAç¡¬ä»¶è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚è®­ç»ƒæ‹¥æœ‰1750äº¿å‚æ•°çš„GPT-3éœ€è¦æ•°åƒä¸ªV100
    GPUçš„åˆ†å¸ƒå¼å¤„ç†ã€‚GPUä¼˜åŒ–æ¡†æ¶ã€é«˜çº§é€šä¿¡åè®®å’Œç¡¬ä»¶ç‰¹æ€§çš„ç»“åˆä½¿å¾—OpenAIèƒ½å¤Ÿé«˜æ•ˆåœ°å®ç°è¿™ä¸€é›„å¿ƒå‹ƒå‹ƒçš„è§„æ¨¡([T. Brownç­‰äºº 2020](ch058.xhtml#ref-brown2020language))ã€‚è¿™ç§å¤§è§„æ¨¡è®­ç»ƒçš„éšç§å’Œå®‰å…¨å½±å“ï¼ŒåŒ…æ‹¬æ•°æ®æ²»ç†å’Œæ¨¡å‹å®‰å…¨ï¼Œåœ¨[ç¬¬15ç« ](ch021.xhtml#sec-security-privacy)ä¸­ç»¼åˆè®¨è®ºã€‚
- en: Despite their advantages, GPUs are not without challenges. Effective utilization
    of GPUs demands careful attention to workload balancing and inter-device communication.
    Training systems must also consider the cost implications, as GPUs are resource-intensive
    and require optimized data centers to operate at scale. However, with innovations
    like [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) and [CUDA-X libraries](https://developer.nvidia.com/cuda-zone)[40](#fn40),
    these challenges are continually being addressed.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡GPUå…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬å¹¶éæ²¡æœ‰æŒ‘æˆ˜ã€‚æœ‰æ•ˆåˆ©ç”¨GPUéœ€è¦ä»”ç»†å…³æ³¨å·¥ä½œè´Ÿè½½å¹³è¡¡å’Œè®¾å¤‡é—´é€šä¿¡ã€‚è®­ç»ƒç³»ç»Ÿè¿˜å¿…é¡»è€ƒè™‘æˆæœ¬å½±å“ï¼Œå› ä¸ºGPUèµ„æºå¯†é›†ï¼Œéœ€è¦ä¼˜åŒ–çš„æ•°æ®ä¸­å¿ƒæ‰èƒ½å¤§è§„æ¨¡è¿è¡Œã€‚ç„¶è€Œï¼Œéšç€[NVLink](https://www.nvidia.com/en-us/data-center/nvlink/)å’Œ[CUDA-Xåº“](https://developer.nvidia.com/cuda-zone)[40](#fn40)ç­‰åˆ›æ–°çš„å‡ºç°ï¼Œè¿™äº›æŒ‘æˆ˜æ­£åœ¨ä¸æ–­å¾—åˆ°è§£å†³ã€‚
- en: GPUs are indispensable for modern machine learning training systems due to their
    versatility, scalability, and integration with advanced software frameworks. The
    architectural principles discussed here extend beyond training to influence inference
    deployment strategies, as detailed in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration),
    where similar parallelization concepts apply to production environments. By addressing
    key bottlenecks in computation, memory, and distribution, GPUs play a foundational
    role in enabling large-scale training pipelines.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå…¶å¤šåŠŸèƒ½æ€§ã€å¯æ‰©å±•æ€§å’Œä¸é«˜çº§è½¯ä»¶æ¡†æ¶çš„é›†æˆï¼ŒGPUå¯¹äºç°ä»£æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿæ¥è¯´æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚è¿™é‡Œè®¨è®ºçš„æ¶æ„åŸåˆ™ä¸ä»…é™äºè®­ç»ƒï¼Œè¿˜å½±å“äº†æ¨ç†éƒ¨ç½²ç­–ç•¥ï¼Œå¦‚[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­è¯¦ç»†æ‰€è¿°ï¼Œå…¶ä¸­ç±»ä¼¼çš„å¹¶è¡ŒåŒ–æ¦‚å¿µé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚é€šè¿‡è§£å†³è®¡ç®—ã€å†…å­˜å’Œåˆ†å¸ƒä¸­çš„å…³é”®ç“¶é¢ˆï¼ŒGPUåœ¨å®ç°å¤§è§„æ¨¡è®­ç»ƒç®¡é“ä¸­å‘æŒ¥ç€åŸºç¡€æ€§ä½œç”¨ã€‚
- en: '**GPT-2 GPU Hardware Comparison**'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2 GPUç¡¬ä»¶æ¯”è¾ƒ**'
- en: Hardware selection significantly impacts GPT-2 training economics and timeline.
    This comparison shows real-world performance differences.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶é€‰æ‹©å¯¹GPT-2çš„è®­ç»ƒç»æµæ€§å’Œæ—¶é—´è¡¨æœ‰é‡å¤§å½±å“ã€‚æ­¤æ¯”è¾ƒæ˜¾ç¤ºäº†å®é™…çš„æ€§èƒ½å·®å¼‚ã€‚
- en: '**Training Throughput (samples/second)**'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒååé‡ï¼ˆæ ·æœ¬/ç§’ï¼‰**'
- en: '| GPU Generation | FP32 | FP16 (Mixed Precision) | Memory | Cost/hour |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '| GPUä»£æ•° | FP32 | FP16ï¼ˆæ··åˆç²¾åº¦ï¼‰ | å†…å­˜ | æ¯å°æ—¶æˆæœ¬ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| V100 (2017) | 90 | 220 | 32GB | $3.06 |'
  id: totrans-1047
  prefs: []
  type: TYPE_TB
  zh: '| V100 (2017) | 90 | 220 | 32GB | $3.06 |'
- en: '| A100 (2020) | 180 | 450 | 80GB | $4.10 |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
  zh: '| A100 (2020) | 180 | 450 | 80GB | $4.10 |'
- en: '| H100 (2022) | 320 | 820 | 80GB | $8.00 |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '| H100 (2022) | 320 | 820 | 80GB | $8.00 |'
- en: '**Training Time to 50K Steps (8 GPUs)**'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾¾åˆ°50Kæ­¥çš„è®­ç»ƒæ—¶é—´ï¼ˆ8ä¸ªGPUï¼‰**'
- en: 'V100: 14 days, cost: approximately $10,252'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'V100: 14å¤©ï¼Œè´¹ç”¨ï¼šçº¦$10,252'
- en: 'A100: 7 days, cost: approximately $6,877'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'A100: 7å¤©ï¼Œè´¹ç”¨ï¼šçº¦$6,877'
- en: 'H100: 3.5 days, cost: approximately $6,720'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'H100: 3.5å¤©ï¼Œè´¹ç”¨ï¼šçº¦$6,720'
- en: '*Note: Cloud pricing varies significantly and changes frequently by provider.*'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šäº‘æœåŠ¡å™¨çš„å®šä»·å·®å¼‚å¾ˆå¤§ï¼Œå¹¶ä¸”ç»å¸¸å› ä¾›åº”å•†è€Œå¼‚ã€‚*'
- en: '**Key Hardware-Driven Tradeoffs**'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®ç¡¬ä»¶é©±åŠ¨çš„æƒè¡¡**'
- en: 'Memory capacity enables larger batches: V100â€™s 32GB limits batch_size=16, while
    A100â€™s 80GB allows batch_size=32 â†’ faster convergence'
  id: totrans-1056
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†…å­˜å®¹é‡å…è®¸æ›´å¤§çš„æ‰¹é‡ï¼šV100çš„32GBé™åˆ¶äº†batch_size=16ï¼Œè€ŒA100çš„80GBå…è®¸batch_size=32 â†’ æ›´å¿«çš„æ”¶æ•›
- en: 'Tensor Core generations: H100â€™s 4th-gen Tensor Cores provide 3.7Ã— speedup over
    V100 for FP16 operations'
  id: totrans-1057
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tensor Coreä»£æ•°ï¼šH100çš„ç¬¬å››ä»£Tensor Coreåœ¨FP16æ“ä½œä¸Šæ¯”V100å¿«3.7å€
- en: 'NVLink bandwidth: H100â€™s 900 GB/s (vs V100â€™s 300 GB/s) reduces gradient synchronization
    time by 65%'
  id: totrans-1058
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NVLinkå¸¦å®½ï¼šH100çš„900 GB/sï¼ˆä¸V100çš„300 GB/sç›¸æ¯”ï¼‰å°†æ¢¯åº¦åŒæ­¥æ—¶é—´å‡å°‘äº†65%
- en: '**Why H100 Wins Despite Higher $/hour**'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆH100å°½ç®¡æ¯å°æ—¶è´¹ç”¨æ›´é«˜ä»èƒ½è·èƒœ**'
- en: Total cost lower due to 4Ã— faster training
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äº4å€å¿«çš„è®­ç»ƒï¼Œæ€»æˆæœ¬æ›´ä½
- en: Frees GPUs for other workloads sooner
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ›´å¿«åœ°ä¸ºå…¶ä»–å·¥ä½œè´Ÿè½½é‡Šæ”¾GPU
- en: Reduced energy consumption (3.5 vs 14 days runtime)
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘èƒ½è€—ï¼ˆ3.5å¤©ä¸14å¤©è¿è¡Œæ—¶é—´ç›¸æ¯”ï¼‰
- en: '**Hardware Selection Heuristic:** For models like GPT-2 where training runs
    take days/weeks, newer GPUs with higher throughput typically offer better total
    cost of ownership despite higher hourly rates. For quick experiments (<1 hour),
    older GPUs may be more cost-effective.'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¡¬ä»¶é€‰æ‹©å¯å‘å¼æ–¹æ³•ï¼š** å¯¹äºåƒGPT-2è¿™æ ·çš„æ¨¡å‹ï¼Œå…¶è®­ç»ƒéœ€è¦æ•°å¤©/æ•°å‘¨ï¼Œé€šå¸¸å…·æœ‰æ›´é«˜ååé‡çš„æ–°GPUå°½ç®¡æ¯å°æ—¶è´¹ç”¨æ›´é«˜ï¼Œä½†é€šå¸¸æä¾›æ›´å¥½çš„æ€»æ‹¥æœ‰æˆæœ¬ã€‚å¯¹äºå¿«é€Ÿå®éªŒï¼ˆ<1å°æ—¶ï¼‰ï¼Œè¾ƒæ—§çš„GPUå¯èƒ½æ›´ç»æµã€‚'
- en: TPUs
  id: totrans-1064
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPUs
- en: Tensor Processing Units (TPUs) and other custom accelerators have been purpose-built
    to address the unique challenges of large-scale machine learning training. Unlike
    GPUs, which are versatile and serve a wide range of applications, TPUs are specifically
    optimized for the computational patterns found in deep learning, such as matrix
    multiplications and convolutional operations ([Norman P. Jouppi et al. 2017b](ch058.xhtml#ref-jouppi2017tpu)).
    These devices mitigate training bottlenecks by offering high throughput, specialized
    memory handling, and tight integration with machine learning frameworks.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUsï¼‰å’Œå…¶ä»–å®šåˆ¶åŠ é€Ÿå™¨æ—¨åœ¨è§£å†³å¤§è§„æ¨¡æœºå™¨å­¦ä¹ è®­ç»ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¸é€šç”¨æ€§å¼ºã€é€‚ç”¨äºå¹¿æ³›åº”ç”¨çš„GPUä¸åŒï¼ŒTPUsä¸“é—¨é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¸­å‘ç°çš„è®¡ç®—æ¨¡å¼è¿›è¡Œä¼˜åŒ–ï¼Œå¦‚çŸ©é˜µä¹˜æ³•å’Œå·ç§¯æ“ä½œï¼ˆ[Norman
    P. Jouppiç­‰äºº 2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰ã€‚è¿™äº›è®¾å¤‡é€šè¿‡æä¾›é«˜ååé‡ã€ä¸“é—¨çš„å†…å­˜å¤„ç†å’Œä¸æœºå™¨å­¦ä¹ æ¡†æ¶çš„ç´§å¯†é›†æˆæ¥ç¼“è§£è®­ç»ƒç“¶é¢ˆã€‚
- en: As illustrated in [FigureÂ 8.20](ch014.xhtml#fig-training-tpus), TPUs have undergone
    significant architectural evolution, with each generation introducing enhancements
    tailored for increasingly demanding AI workloads. The first-generation TPU, introduced
    in 2015, was designed for internal inference acceleration. Subsequent iterations
    have focused on large-scale distributed training, memory optimizations, and efficiency
    improvements, culminating in the most recent Trillium architecture. These advancements
    illustrate how domain-specific accelerators continue to push the boundaries of
    AI performance and efficiency.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾8.20](ch014.xhtml#fig-training-tpus)æ‰€ç¤ºï¼ŒTPUsç»å†äº†é‡å¤§çš„æ¶æ„æ¼”å˜ï¼Œæ¯ä¸€ä»£éƒ½å¼•å…¥äº†é’ˆå¯¹æ—¥ç›Šå¢é•¿çš„AIå·¥ä½œè´Ÿè½½çš„ä¼˜åŒ–ã€‚ç¬¬ä¸€ä»£TPUäº2015å¹´æ¨å‡ºï¼Œæ—¨åœ¨ç”¨äºå†…éƒ¨æ¨ç†åŠ é€Ÿã€‚éšåçš„è¿­ä»£ä¸“æ³¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒã€å†…å­˜ä¼˜åŒ–å’Œæ•ˆç‡æå‡ï¼Œæœ€ç»ˆåœ¨æœ€æ–°çš„Trilliumæ¶æ„ä¸­è¾¾åˆ°é¡¶å³°ã€‚è¿™äº›è¿›æ­¥å±•ç¤ºäº†ç‰¹å®šé¢†åŸŸåŠ é€Ÿå™¨å¦‚ä½•ç»§ç»­æ¨åŠ¨AIæ€§èƒ½å’Œæ•ˆç‡çš„è¾¹ç•Œã€‚
- en: '![](../media/file127.png)'
  id: totrans-1067
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file127.png)'
- en: 'FigureÂ 8.20: **TPU Evolution**: Successive generations of tensor processing
    units demonstrate architectural advancements optimized for deep learning workloads,
    transitioning from inference acceleration to large-scale distributed training
    and culminating in the trillium architecture. These specialized accelerators address
    the computational demands of modern AI by enhancing memory handling, increasing
    throughput, and integrating tightly with machine learning frameworks.'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.20ï¼š**TPU å‘å±•å†ç¨‹**ï¼šè¿ç»­å‡ ä»£å¼ é‡å¤„ç†å•å…ƒå±•ç¤ºäº†é’ˆå¯¹æ·±åº¦å­¦ä¹ å·¥ä½œè´Ÿè½½çš„æ¶æ„è¿›æ­¥ï¼Œä»æ¨ç†åŠ é€Ÿè¿‡æ¸¡åˆ°å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œæœ€ç»ˆ culminating
    in the trillium architectureã€‚è¿™äº›ä¸“ç”¨åŠ é€Ÿå™¨é€šè¿‡å¢å¼ºå†…å­˜å¤„ç†ã€æé«˜ååé‡å’Œä¸æœºå™¨å­¦ä¹ æ¡†æ¶ç´§å¯†é›†æˆï¼Œæ»¡è¶³äº†ç°ä»£ AI çš„è®¡ç®—éœ€æ±‚ã€‚
- en: Machine learning frameworks can achieve substantial gains in training efficiency
    through purpose-built AI accelerators such as TPUs. However, maximizing these
    benefits requires careful attention to hardware-aware optimizations, including
    memory layout, dataflow orchestration, and computational efficiency.
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸“é—¨è®¾è®¡çš„ AI åŠ é€Ÿå™¨ï¼Œå¦‚ TPUsï¼Œæœºå™¨å­¦ä¹ æ¡†æ¶å¯ä»¥åœ¨è®­ç»ƒæ•ˆç‡ä¸Šå®ç°æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¦æœ€å¤§åŒ–è¿™äº›å¥½å¤„ï¼Œéœ€è¦ä»”ç»†å…³æ³¨ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–ï¼ŒåŒ…æ‹¬å†…å­˜å¸ƒå±€ã€æ•°æ®æµç¼–æ’å’Œè®¡ç®—æ•ˆç‡ã€‚
- en: 'Google developed TPUs with a primary goal: to accelerate machine learning workloads
    at scale while reducing the energy and infrastructure costs associated with traditional
    hardware. Their architecture is optimized for tasks that benefit from batch processing,
    making them particularly effective in distributed training systems where large
    datasets are split across multiple devices. A key feature of TPUs is their systolic
    array architecture[41](#fn41), which performs efficient matrix multiplications
    by streaming data through a network of processing elements. This design minimizes
    data movement overhead, reducing latency and energy consumptionâ€”critical factors
    for training large-scale models like transformers ([Norman P. Jouppi et al. 2017b](ch058.xhtml#ref-jouppi2017tpu)).'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: Google å¼€å‘äº† TPUsï¼Œå…¶ä¸»è¦ç›®æ ‡æ˜¯ï¼šåœ¨é™ä½ä¸ä¼ ç»Ÿç¡¬ä»¶ç›¸å…³çš„èƒ½æºå’ŒåŸºç¡€è®¾æ–½æˆæœ¬çš„åŒæ—¶ï¼Œå¤§è§„æ¨¡åŠ é€Ÿæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ã€‚å…¶æ¶æ„é’ˆå¯¹ä»æ‰¹å¤„ç†ä¸­å—ç›Šçš„ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå› æ­¤åœ¨å°†å¤§é‡æ•°æ®é›†åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šçš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚TPU
    çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯å…¶æ”¶ç¼©é˜µåˆ—æ¶æ„[41](#fn41)ï¼Œé€šè¿‡å°†æ•°æ®æµé€šè¿‡å¤„ç†å…ƒç´ ç½‘ç»œè¿›è¡Œé«˜æ•ˆçŸ©é˜µä¹˜æ³•ã€‚è¿™ç§è®¾è®¡æœ€å°åŒ–äº†æ•°æ®ç§»åŠ¨å¼€é”€ï¼Œé™ä½äº†å»¶è¿Ÿå’Œèƒ½è€—â€”â€”è¿™å¯¹äºè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚å˜å‹å™¨ï¼‰è‡³å…³é‡è¦ï¼ˆ[Norman
    P. Jouppi ç­‰äºº 2017b](ch058.xhtml#ref-jouppi2017tpu)ï¼‰ã€‚
- en: From the perspective of training pipeline optimization, TPUs simplify integration
    with data pipelines in the TensorFlow ecosystem. Features such as the TPU runtime
    and TensorFlowâ€™s [`tf.data` API](https://www.tensorflow.org/guide/data) enable
    seamless preprocessing, caching, and batching of data to feed the accelerators
    efficiently ([MartÃ­n Abadi et al. 2016](ch058.xhtml#ref-abadi2016tensorflow)).
    TPUs are designed to work in podsâ€”clusters of interconnected TPU devices that
    allow for massive parallelism. In such setups, TPU pods enable hybrid parallelism
    strategies by combining data parallelism across devices with model parallelism
    within devices, addressing memory and compute constraints simultaneously.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒæµç¨‹ä¼˜åŒ–çš„è§’åº¦æ¥çœ‹ï¼ŒTPUs ç®€åŒ–äº†ä¸ TensorFlow ç”Ÿæ€ç³»ç»Ÿä¸­æ•°æ®ç®¡é“çš„é›†æˆã€‚TPU è¿è¡Œæ—¶å’Œ TensorFlow çš„ [tf.data
    API](https://www.tensorflow.org/guide/data) ç­‰åŠŸèƒ½ä½¿å¾—æ•°æ®é¢„å¤„ç†ã€ç¼“å­˜å’Œæ‰¹å¤„ç†æ— ç¼ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¸ºåŠ é€Ÿå™¨æä¾›æ•°æ®ï¼ˆ[MartÃ­n
    Abadi ç­‰äºº 2016](ch058.xhtml#ref-abadi2016tensorflow)ï¼‰ã€‚TPUs è®¾è®¡ä¸ºåœ¨ pod ä¸­å·¥ä½œâ€”â€”ç›¸äº’è¿æ¥çš„
    TPU è®¾å¤‡é›†ç¾¤ï¼Œå…è®¸å®ç°å¤§è§„æ¨¡å¹¶è¡Œæ€§ã€‚åœ¨è¿™ç§é…ç½®ä¸­ï¼ŒTPU pod é€šè¿‡ç»“åˆè®¾å¤‡é—´çš„æ•°æ®å¹¶è¡Œæ€§å’Œè®¾å¤‡å†…çš„æ¨¡å‹å¹¶è¡Œæ€§ï¼Œå®ç°äº†æ··åˆå¹¶è¡Œç­–ç•¥ï¼ŒåŒæ—¶è§£å†³å†…å­˜å’Œè®¡ç®—é™åˆ¶ã€‚
- en: TPUs have been instrumental in training large-scale models, such as BERT and
    T5\. For example, Googleâ€™s use of TPUs to train BERT demonstrates their ability
    to handle both the memory-intensive requirements of large transformer models and
    the synchronization challenges of distributed setups ([Devlin et al. 2018a](ch058.xhtml#ref-Devlin2019)).
    By splitting the model across TPU cores and optimizing communication patterns,
    Google achieved excellent results while significantly reducing training time compared
    to traditional hardware.
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs åœ¨è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚ BERT å’Œ T5ï¼‰æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ä¾‹å¦‚ï¼ŒGoogle ä½¿ç”¨ TPUs è®­ç»ƒ BERT çš„åšæ³•å±•ç¤ºäº†å®ƒä»¬å¤„ç†å¤§å‹å˜å‹å™¨æ¨¡å‹çš„å†…å­˜å¯†é›†å‹éœ€æ±‚ä»¥åŠåˆ†å¸ƒå¼è®¾ç½®ä¸­çš„åŒæ­¥æŒ‘æˆ˜çš„èƒ½åŠ›ï¼ˆ[Devlin
    ç­‰äºº 2018a](ch058.xhtml#ref-Devlin2019)ï¼‰ã€‚é€šè¿‡å°†æ¨¡å‹åˆ†å‰²åˆ° TPU å†…æ ¸å¹¶ä¼˜åŒ–é€šä¿¡æ¨¡å¼ï¼ŒGoogle å®ç°äº†ä¼˜å¼‚çš„ç»“æœï¼ŒåŒæ—¶ä¸ä¼ ç»Ÿç¡¬ä»¶ç›¸æ¯”æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€‚
- en: Beyond TPUs, custom accelerators such as [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)
    and [Intel Gaudi](https://www.intel.com/content/www/us/en/artificial-intelligence/gaudi-deep-learning.html)
    chips are also gaining traction in the machine learning ecosystem. These devices
    are designed to compete with TPUs by offering similar performance benefits while
    catering to diverse cloud and on-premise environments. For example, AWS Trainium
    provides deep integration with the AWS ecosystem, allowing users to seamlessly
    scale their training pipelines with services like [Amazon SageMaker](https://aws.amazon.com/sagemaker/).
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†TPUsä¹‹å¤–ï¼Œå¦‚[AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)å’Œ[Intel
    Gaudi](https://www.intel.com/content/www/us/en/artificial-intelligence/gaudi-deep-learning.html)è¿™æ ·çš„å®šåˆ¶åŠ é€Ÿå™¨èŠ¯ç‰‡ä¹Ÿåœ¨æœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿä¸­è·å¾—äº†å…³æ³¨ã€‚è¿™äº›è®¾å¤‡æ—¨åœ¨é€šè¿‡æä¾›ç±»ä¼¼æ€§èƒ½ä¼˜åŠ¿çš„åŒæ—¶æ»¡è¶³å¤šæ ·åŒ–çš„äº‘å’Œæœ¬åœ°ç¯å¢ƒéœ€æ±‚ï¼Œä¸TPUsç«äº‰ã€‚ä¾‹å¦‚ï¼ŒAWS
    Trainiumæä¾›äº†ä¸AWSç”Ÿæ€ç³»ç»Ÿçš„æ·±åº¦é›†æˆï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡[Amazon SageMaker](https://aws.amazon.com/sagemaker/)ç­‰æœåŠ¡æ— ç¼æ‰©å±•å…¶è®­ç»ƒç®¡é“ã€‚
- en: While TPUs and custom accelerators excel in throughput and energy efficiency,
    their specialized nature introduces limitations. The trade-offs between specialized
    hardware performance and deployment flexibility become particularly important
    when considering edge deployment scenarios, as explored in [ChapterÂ 14](ch020.xhtml#sec-ondevice-learning).
    TPUs, for example, are tightly coupled with Googleâ€™s ecosystem, making them less
    accessible to practitioners using alternative frameworks. Similarly, the high
    upfront investment required for TPU pods may deter smaller organizations or those
    with limited budgets. Despite these challenges, the performance gains offered
    by custom accelerators make them a compelling choice for large-scale training
    tasks.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶TPUså’Œå®šåˆ¶åŠ é€Ÿå™¨åœ¨ååé‡å’Œèƒ½æ•ˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„ä¸“ç”¨æ€§è´¨å¼•å…¥äº†é™åˆ¶ã€‚åœ¨è€ƒè™‘è¾¹ç¼˜éƒ¨ç½²åœºæ™¯æ—¶ï¼Œä¸“ç”¨ç¡¬ä»¶æ€§èƒ½ä¸éƒ¨ç½²çµæ´»æ€§ä¹‹é—´çš„æƒè¡¡å˜å¾—å°¤ä¸ºé‡è¦ï¼Œæ­£å¦‚åœ¨ç¬¬14ç« [ç¬¬14ç« ](ch020.xhtml#sec-ondevice-learning)ä¸­æ¢è®¨çš„é‚£æ ·ã€‚ä¾‹å¦‚ï¼ŒTPUsä¸è°·æ­Œçš„ç”Ÿæ€ç³»ç»Ÿç´§å¯†è€¦åˆï¼Œä½¿å¾—å®ƒä»¬å¯¹ä½¿ç”¨æ›¿ä»£æ¡†æ¶çš„ä»ä¸šè€…æ¥è¯´ä¸å¤ªå®¹æ˜“è·å¾—ã€‚åŒæ ·ï¼ŒTPUé›†ç¾¤æ‰€éœ€çš„é«˜å‰æœŸæŠ•èµ„å¯èƒ½ä¼šé˜»ç¢å°å‹ç»„ç»‡æˆ–é¢„ç®—æœ‰é™çš„ç»„ç»‡ã€‚å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œä½†å®šåˆ¶åŠ é€Ÿå™¨æä¾›çš„æ€§èƒ½æå‡ä½¿å®ƒä»¬æˆä¸ºå¤§è§„æ¨¡è®­ç»ƒä»»åŠ¡çš„æœ‰åŠ›é€‰æ‹©ã€‚
- en: TPUs and custom accelerators address many of the key challenges in machine learning
    training systems, from handling massive datasets to optimizing distributed training.
    Their unique architectures and deep integration with specific ecosystems make
    them powerful tools for organizations seeking to scale their training workflows.
    As machine learning models and datasets continue to grow, these accelerators are
    likely to play an increasingly central role in shaping the future of AI training.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: TPUså’Œå®šåˆ¶åŠ é€Ÿå™¨è§£å†³äº†æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿä¸­è®¸å¤šå…³é”®æŒ‘æˆ˜ï¼Œä»å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†åˆ°ä¼˜åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚å®ƒä»¬ç‹¬ç‰¹çš„æ¶æ„å’Œä¸ç‰¹å®šç”Ÿæ€ç³»ç»Ÿçš„æ·±åº¦é›†æˆï¼Œä½¿å®ƒä»¬æˆä¸ºå¯»æ±‚æ‰©å±•å…¶è®­ç»ƒå·¥ä½œæµç¨‹çš„ç»„ç»‡çš„æœ‰åŠ›å·¥å…·ã€‚éšç€æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ•°æ®é›†çš„ä¸æ–­å¢é•¿ï¼Œè¿™äº›åŠ é€Ÿå™¨å¾ˆå¯èƒ½åœ¨å¡‘é€ AIè®­ç»ƒçš„æœªæ¥ä¸­æ‰®æ¼”è¶Šæ¥è¶Šæ ¸å¿ƒçš„è§’è‰²ã€‚
- en: FPGAs
  id: totrans-1076
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAsï¼‰
- en: Field-Programmable Gate Arrays (FPGAs) are versatile hardware solutions that
    allow developers to tailor their architecture for specific machine learning workloads.
    Unlike GPUs or TPUs, which are designed with fixed architectures, FPGAs can be
    reconfigured dynamically, offering a unique level of flexibility. This adaptability
    makes them particularly valuable for applications that require customized optimizations,
    low-latency processing, or experimentation with novel algorithms.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAsï¼‰æ˜¯çµæ´»çš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆï¼Œå…è®¸å¼€å‘è€…é’ˆå¯¹ç‰¹å®šçš„æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½å®šåˆ¶å…¶æ¶æ„ã€‚ä¸è®¾è®¡æœ‰å›ºå®šæ¶æ„çš„GPUæˆ–TPUä¸åŒï¼ŒFPGAså¯ä»¥åŠ¨æ€é‡æ–°é…ç½®ï¼Œæä¾›ç‹¬ç‰¹çº§åˆ«çš„çµæ´»æ€§ã€‚è¿™ç§é€‚åº”æ€§ä½¿å®ƒä»¬å¯¹äºéœ€è¦å®šåˆ¶ä¼˜åŒ–ã€ä½å»¶è¿Ÿå¤„ç†æˆ–å°è¯•æ–°ç®—æ³•çš„åº”ç”¨ç¨‹åºç‰¹åˆ«æœ‰ä»·å€¼ã€‚
- en: Microsoft had been exploring the use of FPGAs for a while, as seen in [FigureÂ 8.21](ch014.xhtml#fig-inference-fpgas),
    with one prominent example being [Project Brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave/).
    This initiative uses FPGAs to accelerate machine learning workloads in the Azure
    cloud. Microsoft chose FPGAs for their ability to provide low-latency inference
    (not training) while maintaining high throughput. This approach benefits scenarios
    where real-time predictions are critical, such as search engine queries or language
    translation services. By integrating FPGAs directly into their data center network[42](#fn42),
    Microsoft has achieved significant performance gains while minimizing power consumption.
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾8.21](ch014.xhtml#fig-inference-fpgas)æ‰€ç¤ºï¼Œå¾®è½¯å·²ç»æ¢ç´¢äº†ä¸€æ®µæ—¶é—´ä½¿ç”¨FPGAï¼Œä¸€ä¸ªçªå‡ºçš„ä¾‹å­æ˜¯[Project
    Brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave/)ã€‚è¿™ä¸ªé¡¹ç›®åˆ©ç”¨FPGAåŠ é€ŸAzureäº‘ä¸­çš„æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ã€‚å¾®è½¯é€‰æ‹©FPGAæ˜¯å› ä¸ºå®ƒä»¬èƒ½å¤Ÿåœ¨ä¿æŒé«˜ååé‡çš„åŒæ—¶æä¾›ä½å»¶è¿Ÿæ¨ç†ï¼ˆè€Œéè®­ç»ƒï¼‰ã€‚è¿™ç§æ–¹æ³•å¯¹äºå®æ—¶é¢„æµ‹è‡³å…³é‡è¦çš„åœºæ™¯æœ‰åˆ©ï¼Œå¦‚æœç´¢å¼•æ“æŸ¥è¯¢æˆ–è¯­è¨€ç¿»è¯‘æœåŠ¡ã€‚é€šè¿‡å°†FPGAç›´æ¥é›†æˆåˆ°æ•°æ®ä¸­å¿ƒç½‘ç»œä¸­[42](#fn42)ï¼Œå¾®è½¯å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æœ€å°åŒ–äº†èƒ½è€—ã€‚
- en: '![](../media/file128.png)'
  id: totrans-1079
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file128.png)'
- en: 'FigureÂ 8.21: **FPGA Evolution for Inference**: Microsoft progressively developed
    field-programmable gate arrays (fpgas) to accelerate machine learning inference
    in cloud services, shifting from initial project catapult designs to more advanced
    iterations and ultimately project brainwave. These reconfigurable hardware solutions
    offer low-latency processing and high throughput, particularly valuable for real-time
    applications like search and language translation.'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.21ï¼š**FPGAæ¨ç†æ¼”è¿›**ï¼šå¾®è½¯é€æ­¥å¼€å‘äº†ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰ä»¥åŠ é€Ÿäº‘æœåŠ¡ä¸­çš„æœºå™¨å­¦ä¹ æ¨ç†ï¼Œä»æœ€åˆçš„Catapulté¡¹ç›®è®¾è®¡è½¬å‘æ›´é«˜çº§çš„è¿­ä»£ï¼Œæœ€ç»ˆåˆ°Brainwaveé¡¹ç›®ã€‚è¿™äº›å¯é‡æ„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆæä¾›äº†ä½å»¶è¿Ÿå¤„ç†å’Œé«˜ååé‡ï¼Œå¯¹äºå®æ—¶åº”ç”¨å¦‚æœç´¢å’Œè¯­è¨€ç¿»è¯‘ç‰¹åˆ«æœ‰ä»·å€¼ã€‚
- en: From a training perspective, FPGAs offer unique advantages in optimizing training
    pipelines. Their reconfigurability allows them to implement custom dataflow architectures
    tailored to specific model requirements. While this training-focused customization
    differs from the inference-oriented FPGA applications more commonly deployed,
    both approaches use the flexibility that distinguishes FPGAs from fixed-function
    accelerators. For instance, data preprocessing and augmentation steps, which can
    often become bottlenecks in GPU-based systems, can be offloaded to FPGAs, freeing
    up GPUs for core training tasks. FPGAs can be programmed to perform operations
    such as sparse matrix multiplications, which are common in recommendation systems
    and graph-based models but are less efficient on traditional accelerators ([Putnam
    et al. 2014](ch058.xhtml#ref-Putnam2014)).
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒçš„è§’åº¦æ¥çœ‹ï¼ŒFPGAåœ¨ä¼˜åŒ–è®­ç»ƒç®¡é“æ–¹é¢æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚å®ƒä»¬çš„å¯é‡æ„æ€§å…è®¸å®ƒä»¬å®ç°é’ˆå¯¹ç‰¹å®šæ¨¡å‹è¦æ±‚å®šåˆ¶çš„è‡ªå®šä¹‰æ•°æ®æµæ¶æ„ã€‚è™½ç„¶è¿™ç§ä»¥è®­ç»ƒä¸ºå¯¼å‘çš„å®šåˆ¶ä¸æ›´å¸¸è§çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„FPGAåº”ç”¨ä¸åŒï¼Œä½†ä¸¤ç§æ–¹æ³•éƒ½ä½¿ç”¨äº†åŒºåˆ†FPGAä¸å›ºå®šåŠŸèƒ½åŠ é€Ÿå™¨çš„çµæ´»æ€§ã€‚ä¾‹å¦‚ï¼Œæ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤åœ¨åŸºäºGPUçš„ç³»ç»Ÿä¸­å¾€å¾€æˆä¸ºç“¶é¢ˆï¼Œå¯ä»¥å¸è½½åˆ°FPGAä¸Šï¼Œä»è€Œé‡Šæ”¾GPUç”¨äºæ ¸å¿ƒè®­ç»ƒä»»åŠ¡ã€‚FPGAå¯ä»¥è¢«ç¼–ç¨‹æ‰§è¡Œè¯¸å¦‚ç¨€ç–çŸ©é˜µä¹˜æ³•ç­‰æ“ä½œï¼Œè¿™åœ¨æ¨èç³»ç»Ÿå’ŒåŸºäºå›¾çš„æ¨¡å‹ä¸­å¾ˆå¸¸è§ï¼Œä½†åœ¨ä¼ ç»ŸåŠ é€Ÿå™¨ä¸Šæ•ˆç‡è¾ƒä½([Putnamç­‰äºº2014](ch058.xhtml#ref-Putnam2014))ã€‚
- en: In distributed training systems, FPGAs provide fine-grained control over communication
    patterns. This control allows developers to optimize inter-device communication
    and memory access, addressing challenges such as parameter synchronization overheads.
    For example, FPGAs can be configured to implement custom all-reduce algorithms
    for gradient aggregation, reducing latency compared to general-purpose hardware.
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­ï¼ŒFPGAæä¾›äº†å¯¹é€šä¿¡æ¨¡å¼çš„ç²¾ç»†æ§åˆ¶ã€‚è¿™ç§æ§åˆ¶å…è®¸å¼€å‘è€…ä¼˜åŒ–è®¾å¤‡é—´é€šä¿¡å’Œå†…å­˜è®¿é—®ï¼Œè§£å†³è¯¸å¦‚å‚æ•°åŒæ­¥å¼€é”€ç­‰æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼ŒFPGAå¯ä»¥è¢«é…ç½®ä¸ºå®æ–½å®šåˆ¶çš„all-reduceç®—æ³•ä»¥å®ç°æ¢¯åº¦èšåˆï¼Œä¸é€šç”¨ç¡¬ä»¶ç›¸æ¯”ï¼Œè¿™å¯ä»¥é™ä½å»¶è¿Ÿã€‚
- en: Despite their benefits, FPGAs come with challenges. Programming FPGAs requires
    expertise in hardware description languages (HDLs) like Verilog or VHDL, which
    can be a barrier for many machine learning practitioners. To address this, frameworks
    like [Xilinxâ€™s Vitis AI](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html)
    and [Intelâ€™s OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)
    have simplified FPGA programming by providing tools and libraries tailored for
    AI workloads. However, the learning curve remains steep compared to the well-established
    ecosystems of GPUs and TPUs.
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡FPGAå…·æœ‰ä¼˜åŠ¿ï¼Œä½†ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ã€‚ç¼–ç¨‹FPGAéœ€è¦æŒæ¡ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLsï¼‰å¦‚Verilogæˆ–VHDLçš„ä¸“é•¿ï¼Œè¿™å¯¹è®¸å¤šæœºå™¨å­¦ä¹ ä»ä¸šè€…æ¥è¯´å¯èƒ½æ˜¯ä¸€ä¸ªéšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåƒ[Xilinxçš„Vitis
    AI](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html)å’Œ[è‹±ç‰¹å°”çš„å¼€æºVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)è¿™æ ·çš„æ¡†æ¶é€šè¿‡æä¾›é’ˆå¯¹AIå·¥ä½œè´Ÿè½½å®šåˆ¶çš„å·¥å…·å’Œåº“ï¼Œç®€åŒ–äº†FPGAç¼–ç¨‹ã€‚ç„¶è€Œï¼Œä¸GPUå’ŒTPUç­‰æˆç†Ÿçš„ç”Ÿæ€ç³»ç»Ÿç›¸æ¯”ï¼Œå­¦ä¹ æ›²çº¿ä»ç„¶å¾ˆé™¡å³­ã€‚
- en: Microsoftâ€™s use of FPGAs highlights their potential to integrate seamlessly
    into existing machine learning workflows. This approach demonstrates the versatility
    of FPGAs, which serve different but complementary roles in training acceleration
    compared to their more common application in inference optimization, particularly
    in edge deployments. By incorporating FPGAs into Azure, Microsoft has demonstrated
    how these devices can complement other accelerators, optimizing end-to-end pipelines
    for both training and inference. This hybrid approach uses the strengths of FPGAs
    for specific tasks while relying on GPUs or CPUs for others, creating a balanced
    and efficient system.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è½¯å¯¹FPGAçš„ä½¿ç”¨çªå‡ºäº†å®ƒä»¬æ— ç¼é›†æˆåˆ°ç°æœ‰æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸­çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•å±•ç¤ºäº†FPGAçš„é€šç”¨æ€§ï¼Œä¸å®ƒä»¬åœ¨æ¨ç†ä¼˜åŒ–ä¸­æ›´ä¸ºå¸¸è§çš„åº”ç”¨ç›¸æ¯”ï¼Œåœ¨è®­ç»ƒåŠ é€Ÿä¸­æ‰®æ¼”ç€ä¸åŒä½†äº’è¡¥çš„è§’è‰²ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜éƒ¨ç½²ä¸­ã€‚é€šè¿‡å°†FPGAæ•´åˆåˆ°Azureä¸­ï¼Œå¾®è½¯å±•ç¤ºäº†è¿™äº›è®¾å¤‡å¦‚ä½•è¡¥å……å…¶ä»–åŠ é€Ÿå™¨ï¼Œä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†çš„ç«¯åˆ°ç«¯ç®¡é“ã€‚è¿™ç§æ··åˆæ–¹æ³•åˆ©ç”¨FPGAåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¾èµ–GPUæˆ–CPUå¤„ç†å…¶ä»–ä»»åŠ¡ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¹³è¡¡ä¸”é«˜æ•ˆçš„ç³»ç»Ÿã€‚
- en: FPGAs offer a compelling solution for machine learning training systems that
    require customization, low latency, or novel optimizations. While their adoption
    may be limited by programming complexity, advancements in tooling and real-world
    implementations like Microsoftâ€™s Project Brainwave demonstrate their growing relevance
    in the AI hardware ecosystem.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: FPGAsä¸ºéœ€è¦å®šåˆ¶ã€ä½å»¶è¿Ÿæˆ–æ–°é¢–ä¼˜åŒ–çš„æœºå™¨å­¦ä¹ è®­ç»ƒç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶å®ƒä»¬çš„é‡‡ç”¨å¯èƒ½å—åˆ°ç¼–ç¨‹å¤æ‚æ€§çš„é™åˆ¶ï¼Œä½†å·¥å…·å’Œç°å®ä¸–ç•Œçš„å®æ–½ï¼Œå¦‚å¾®è½¯çš„Project
    Brainwaveé¡¹ç›®ï¼Œè¡¨æ˜å®ƒä»¬åœ¨AIç¡¬ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­çš„ç›¸å…³æ€§æ­£åœ¨å¢é•¿ã€‚
- en: ASICs
  id: totrans-1086
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ASICs
- en: Application-Specific Integrated Circuits (ASICs) represent a class of hardware
    designed for specific tasks, offering unparalleled efficiency and performance
    by eschewing the general-purpose flexibility of GPUs or FPGAs. Among the most
    innovative examples of ASICs for machine learning training is the [Cerebras Wafer-Scale
    Engine (WSE)](https://www.cerebras.net/), as shown in [FigureÂ 8.22](ch014.xhtml#fig-training-wse),
    which stands apart for its unique approach to addressing the computational and
    memory challenges of training massive machine learning models.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ç‰¹å®šé›†æˆç”µè·¯ï¼ˆASICsï¼‰ä»£è¡¨ä¸€ç±»ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ç¡¬ä»¶ï¼Œé€šè¿‡æ‘’å¼ƒé€šç”¨çµæ´»æ€§ï¼Œå¦‚GPUæˆ–FPGAï¼Œæä¾›äº†æ— ä¸ä¼¦æ¯”çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚åœ¨æœºå™¨å­¦ä¹ è®­ç»ƒä¸­ï¼Œæœ€åˆ›æ–°çš„ASICç¤ºä¾‹ä¹‹ä¸€æ˜¯[Cerebrasæ™¶åœ†çº§å¼•æ“ï¼ˆWSEï¼‰](https://www.cerebras.net/)ï¼Œå¦‚[å›¾8.22](ch014.xhtml#fig-training-wse)æ‰€ç¤ºï¼Œå®ƒå› å…¶ç‹¬ç‰¹çš„è§£å†³è®­ç»ƒå¤§è§„æ¨¡æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜çš„æ–¹æ³•è€Œç‹¬æ ‘ä¸€å¸œã€‚
- en: '![](../media/file129.png)'
  id: totrans-1088
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file129.png)'
- en: 'FigureÂ 8.22: **Wafer-Scale Integration**: This 300mm silicon wafer contains
    2.6 trillion transistors, enabling a single chip to house an entire AI training
    system and overcome memory bandwidth limitations common in distributed training
    setups. By integrating massive computational resources onto a single die, the
    WSE significantly reduces data transfer bottlenecks and accelerates model training
    for large-scale machine learning applications.'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.22ï¼š**æ™¶åœ†çº§é›†æˆ**ï¼šè¿™å—300mmçš„ç¡…æ™¶åœ†åŒ…å«260äº¿ä¸ªæ™¶ä½“ç®¡ï¼Œä½¿å•ä¸ªèŠ¯ç‰‡èƒ½å¤Ÿå®¹çº³æ•´ä¸ªAIè®­ç»ƒç³»ç»Ÿï¼Œå¹¶å…‹æœåˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®ä¸­å¸¸è§çš„å†…å­˜å¸¦å®½é™åˆ¶ã€‚é€šè¿‡å°†åºå¤§çš„è®¡ç®—èµ„æºé›†æˆåˆ°å•ä¸ªæ™¶åœ†ä¸Šï¼ŒWSEæ˜¾è‘—å‡å°‘äº†æ•°æ®ä¼ è¾“ç“¶é¢ˆï¼Œå¹¶åŠ é€Ÿäº†å¤§è§„æ¨¡æœºå™¨å­¦ä¹ åº”ç”¨æ¨¡å‹è®­ç»ƒã€‚
- en: The Cerebras WSE is unlike traditional chips in that it is a single wafer-scale
    processor, spanning the entire silicon wafer rather than being cut into smaller
    chips. This architecture enables Cerebras to pack 2.6 trillion transistors and
    850,000 cores onto a single device[43](#fn43). These cores are connected via a
    high-bandwidth, low-latency interconnect, allowing data to move across the chip
    without the bottlenecks associated with external communication between discrete
    GPUs or TPUs ([Feldman et al. 2020](ch058.xhtml#ref-Feldman2020)).
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: Cerebras WSEä¸ä¼ ç»ŸèŠ¯ç‰‡ä¸åŒï¼Œå®ƒæ˜¯ä¸€ä¸ªå•ç‰‡çº§å¤„ç†å™¨ï¼Œè¦†ç›–æ•´ä¸ªç¡…æ™¶åœ†è€Œä¸æ˜¯åˆ‡å‰²æˆæ›´å°çš„èŠ¯ç‰‡ã€‚è¿™ç§æ¶æ„ä½¿å¾—Cerebrasèƒ½å¤Ÿåœ¨å•ä¸ªè®¾å¤‡ä¸Šé›†æˆ260äº¿ä¸ªæ™¶ä½“ç®¡å’Œ85ä¸‡ä¸ªæ ¸å¿ƒ[43](#fn43)ã€‚è¿™äº›æ ¸å¿ƒé€šè¿‡é«˜å¸¦å®½ã€ä½å»¶è¿Ÿçš„äº’è¿è¿æ¥ï¼Œå…è®¸æ•°æ®åœ¨èŠ¯ç‰‡å†…éƒ¨ç§»åŠ¨ï¼Œè€Œä¸å—ç¦»æ•£GPUæˆ–TPUä¹‹é—´å¤–éƒ¨é€šä¿¡ç“¶é¢ˆçš„å½±å“([Feldmanç­‰äºº
    2020](ch058.xhtml#ref-Feldman2020))ã€‚
- en: 'From a machine learning training perspective, the WSE addresses several critical
    bottlenecks:'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœºå™¨å­¦ä¹ è®­ç»ƒçš„è§’åº¦æ¥çœ‹ï¼ŒWSEè§£å†³äº†å‡ ä¸ªå…³é”®ç“¶é¢ˆï¼š
- en: '**Data Movement**: In traditional distributed systems, significant time is
    spent transferring data between devices. The WSE eliminates this by keeping all
    computations and memory on a single wafer, drastically reducing communication
    overhead.'
  id: totrans-1092
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®ç§»åŠ¨**ï¼šåœ¨ä¼ ç»Ÿçš„åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå¤§é‡æ—¶é—´ç”¨äºåœ¨è®¾å¤‡ä¹‹é—´ä¼ è¾“æ•°æ®ã€‚WSEé€šè¿‡å°†æ‰€æœ‰è®¡ç®—å’Œå†…å­˜ä¿æŒåœ¨å•ä¸ªæ™¶åœ†ä¸Šï¼Œæ¶ˆé™¤äº†è¿™ä¸€ç‚¹ï¼Œå¤§å¤§å‡å°‘äº†é€šä¿¡å¼€é”€ã€‚'
- en: '**Memory Bandwidth**: The WSE integrates 40 GB of high-speed on-chip memory
    directly adjacent to its processing cores. This proximity allows for near-instantaneous
    access to data, overcoming the latency challenges that GPUs often face when accessing
    off-chip memory.'
  id: totrans-1093
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å†…å­˜å¸¦å®½**ï¼šWSEé›†æˆäº†40 GBçš„é«˜é€Ÿç‰‡ä¸Šå†…å­˜ï¼Œç›´æ¥ä½äºå…¶å¤„ç†æ ¸å¿ƒé™„è¿‘ã€‚è¿™ç§é‚»è¿‘æ€§ä½¿å¾—æ•°æ®è®¿é—®å‡ ä¹ç¬é—´å®Œæˆï¼Œå…‹æœäº†GPUåœ¨è®¿é—®ç‰‡å¤–å†…å­˜æ—¶ç»å¸¸é‡åˆ°çš„å»¶è¿ŸæŒ‘æˆ˜ã€‚'
- en: '**Scalability**: While traditional distributed systems rely on complex software
    frameworks to manage multiple devices, the WSE simplifies scaling by consolidating
    all resources into one massive chip. This design is particularly well-suited for
    training large language models and other deep learning architectures that require
    significant parallelism.'
  id: totrans-1094
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¯æ‰©å±•æ€§**ï¼šè™½ç„¶ä¼ ç»Ÿçš„åˆ†å¸ƒå¼ç³»ç»Ÿä¾èµ–äºå¤æ‚çš„è½¯ä»¶æ¡†æ¶æ¥ç®¡ç†å¤šä¸ªè®¾å¤‡ï¼Œä½†WSEé€šè¿‡å°†æ‰€æœ‰èµ„æºæ•´åˆåˆ°ä¸€ä¸ªå·¨å¤§çš„èŠ¯ç‰‡ä¸Šï¼Œç®€åŒ–äº†å¯æ‰©å±•æ€§ã€‚è¿™ç§è®¾è®¡ç‰¹åˆ«é€‚åˆäºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œå…¶ä»–éœ€è¦å¤§é‡å¹¶è¡Œçš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚'
- en: A key example of Cerebrasâ€™ impact is its application in natural language processing.
    Organizations using the WSE have demonstrated substantial speedups in training
    transformer models, which are notoriously compute-intensive due to their reliance
    on attention mechanisms. The responsible deployment of such powerful training
    capabilitiesâ€”including considerations of energy consumption, accessibility, and
    societal impactâ€”is explored in [ChapterÂ 17](ch023.xhtml#sec-responsible-ai). By
    leveraging the chipâ€™s massive parallelism and memory bandwidth, training times
    for models like BERT have been significantly reduced compared to GPU-based systems
    ([T. Brown et al. 2020](ch058.xhtml#ref-brown2020language)).
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: Cerebraså…¬å¸çš„å½±å“åŠ›çš„ä¸€ä¸ªå…³é”®ä¾‹å­æ˜¯å…¶è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨ã€‚ä½¿ç”¨WSEçš„ç»„ç»‡å·²ç»å±•ç¤ºäº†åœ¨è®­ç»ƒTransformeræ¨¡å‹æ–¹é¢çš„æ˜¾è‘—åŠ é€Ÿï¼Œè¿™äº›æ¨¡å‹å› å…¶ä¾èµ–æ³¨æ„åŠ›æœºåˆ¶è€Œè®¡ç®—å¯†é›†ã€‚è¿™ç§å¼ºå¤§è®­ç»ƒèƒ½åŠ›çš„è´Ÿè´£ä»»éƒ¨ç½²ï¼ŒåŒ…æ‹¬å¯¹èƒ½è€—ã€å¯è®¿é—®æ€§å’Œç¤¾ä¼šå½±å“çš„è€ƒè™‘ï¼Œåœ¨[ç¬¬17ç« ](ch023.xhtml#sec-responsible-ai)ä¸­è¿›è¡Œäº†æ¢è®¨ã€‚é€šè¿‡åˆ©ç”¨èŠ¯ç‰‡çš„å·¨å¤§å¹¶è¡Œæ€§å’Œå†…å­˜å¸¦å®½ï¼Œä¸åŸºäºGPUçš„ç³»ç»Ÿç›¸æ¯”ï¼ŒBERTç­‰æ¨¡å‹çš„è®­ç»ƒæ—¶é—´å·²ç»æ˜¾è‘—å‡å°‘([T.
    Brownç­‰äºº 2020](ch058.xhtml#ref-brown2020language))ã€‚
- en: However, the Cerebras WSE also comes with limitations. Its single-chip design
    is optimized for specific use cases, such as dense matrix computations in deep
    learning, but may not be as versatile as multi-purpose hardware like GPUs or FPGAs.
    The cost of acquiring and integrating such a specialized device can be prohibitive
    for smaller organizations or those with diverse workloads.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒCerebras WSEä¹Ÿå­˜åœ¨å±€é™æ€§ã€‚å…¶å•ç‰‡è®¾è®¡é’ˆå¯¹ç‰¹å®šç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¾‹å¦‚æ·±åº¦å­¦ä¹ ä¸­çš„å¯†é›†çŸ©é˜µè®¡ç®—ï¼Œä½†å¯èƒ½ä¸å¦‚GPUæˆ–FPGAç­‰å¤šç”¨é€”ç¡¬ä»¶çµæ´»ã€‚è·å–å’Œæ•´åˆæ­¤ç±»ä¸“ç”¨è®¾å¤‡çš„æˆæœ¬å¯èƒ½å¯¹å°å‹ç»„ç»‡æˆ–å…·æœ‰å¤šæ ·åŒ–å·¥ä½œè´Ÿè½½çš„ç»„ç»‡æ„æˆéšœç¢ã€‚
- en: Cerebrasâ€™ strategy of targeting the largest models aligns with previously discussed
    trends, such as the growing emphasis on scaling techniques and hybrid parallelism
    strategies. The WSEâ€™s unique design addresses challenges like memory bottlenecks
    and inter-device communication overhead, making it a pioneering solution for next-generation
    AI workloads.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: Cerebrasé’ˆå¯¹æœ€å¤§è§„æ¨¡æ¨¡å‹çš„ç­–ç•¥ä¸ä¹‹å‰è®¨è®ºçš„è¶‹åŠ¿ç›¸ä¸€è‡´ï¼Œä¾‹å¦‚å¯¹æ‰©å±•æŠ€æœ¯å’Œæ··åˆå¹¶è¡Œç­–ç•¥çš„æ—¥ç›Šé‡è§†ã€‚WSEçš„ç‹¬ç‰¹è®¾è®¡è§£å†³äº†å†…å­˜ç“¶é¢ˆå’Œè®¾å¤‡é—´é€šä¿¡å¼€é”€ç­‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸‹ä¸€ä»£AIå·¥ä½œè´Ÿè½½çš„å…ˆé©±æ€§è§£å†³æ–¹æ¡ˆã€‚
- en: The Cerebras Wafer-Scale Engine exemplifies how ASICs can push the boundaries
    of what is possible in machine learning training. By addressing key bottlenecks
    in computation and data movement, the WSE offers a glimpse into the future of
    specialized hardware for AI, where the integration of highly optimized, task-specific
    architectures unlocks unprecedented performance.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: Cerebras Wafer-Scale Engine ä¸¾ä¾‹è¯´æ˜äº†ASICså¦‚ä½•æ¨åŠ¨æœºå™¨å­¦ä¹ è®­ç»ƒå¯èƒ½æ€§çš„è¾¹ç•Œã€‚é€šè¿‡è§£å†³è®¡ç®—å’Œæ•°æ®ç§»åŠ¨ä¸­çš„å…³é”®ç“¶é¢ˆï¼ŒWSEä¸ºAIä¸“ç”¨ç¡¬ä»¶çš„æœªæ¥æä¾›äº†ä¸€ä¸ªçª—å£ï¼Œå…¶ä¸­é«˜åº¦ä¼˜åŒ–çš„ã€ç‰¹å®šä»»åŠ¡çš„æ¶æ„é›†æˆè§£é”äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚
- en: Fallacies and Pitfalls
  id: totrans-1099
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°¬è¯¯å’Œé™·é˜±
- en: Training represents the most computationally intensive phase of machine learning
    system development, where complex optimization algorithms, distributed computing
    challenges, and resource management constraints intersect. The scale and complexity
    of modern training workloads create numerous opportunities for misconceptions
    about performance optimization, resource utilization, and system design choices.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿå¼€å‘ä¸­æœ€è®¡ç®—å¯†é›†çš„é˜¶æ®µï¼Œå…¶ä¸­å¤æ‚çš„ä¼˜åŒ–ç®—æ³•ã€åˆ†å¸ƒå¼è®¡ç®—æŒ‘æˆ˜å’Œèµ„æºç®¡ç†çº¦æŸç›¸äº¤ã€‚ç°ä»£è®­ç»ƒå·¥ä½œè´Ÿè½½çš„è§„æ¨¡å’Œå¤æ‚æ€§ä¸ºæ€§èƒ½ä¼˜åŒ–ã€èµ„æºåˆ©ç”¨å’Œç³»ç»Ÿè®¾è®¡é€‰æ‹©æ–¹é¢çš„è¯¯è§£æä¾›äº†è®¸å¤šæœºä¼šã€‚
- en: '**Fallacy:** *Training larger models always yields better performance.*'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: '**è°¬è¯¯ï¼š** *è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æ€»æ˜¯èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚*'
- en: This widespread belief drives teams to continuously scale model size without
    considering the relationship between model capacity and available data. While
    larger models can capture more complex patterns, they also require exponentially
    more data and computation to train effectively. Beyond certain thresholds, increasing
    model size leads to overfitting on limited datasets, diminishing returns in performance
    improvements, and unsustainable computational costs. Effective training requires
    matching model capacity to data availability and computational resources rather
    than pursuing size for its own sake.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ™®éçš„ä¿¡å¿µé©±ä½¿å›¢é˜Ÿä¸æ–­æ‰©å±•æ¨¡å‹å¤§å°ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æ¨¡å‹å®¹é‡å’Œå¯ç”¨æ•°æ®ä¹‹é—´çš„å…³ç³»ã€‚è™½ç„¶æ›´å¤§çš„æ¨¡å‹å¯ä»¥æ•æ‰æ›´å¤æ‚çš„æ¨¡å¼ï¼Œä½†å®ƒä»¬ä¹Ÿéœ€è¦æŒ‡æ•°çº§æ›´å¤šçš„æ•°æ®å’Œè®¡ç®—æ¥æœ‰æ•ˆè®­ç»ƒã€‚è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼Œå¢åŠ æ¨¡å‹å¤§å°ä¼šå¯¼è‡´åœ¨æœ‰é™æ•°æ®é›†ä¸Šçš„è¿‡æ‹Ÿåˆã€æ€§èƒ½æ”¹è¿›çš„è¾¹é™…æ•ˆç›Šé€’å‡å’Œä¸å¯æŒç»­çš„è®¡ç®—æˆæœ¬ã€‚æœ‰æ•ˆçš„è®­ç»ƒéœ€è¦å°†æ¨¡å‹å®¹é‡ä¸æ•°æ®å¯ç”¨æ€§å’Œè®¡ç®—èµ„æºç›¸åŒ¹é…ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿½æ±‚è§„æ¨¡æœ¬èº«ã€‚
- en: '**Pitfall:** *Assuming that distributed training automatically accelerates
    model development.*'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š** *å‡è®¾åˆ†å¸ƒå¼è®­ç»ƒä¼šè‡ªåŠ¨åŠ é€Ÿæ¨¡å‹å¼€å‘ã€‚*'
- en: Many practitioners expect that adding more devices will proportionally reduce
    training time without considering communication overhead and synchronization costs.
    Distributed training introduces coordination complexity, gradient aggregation
    bottlenecks, and potential convergence issues that can actually slow down training.
    Small models or datasets might train faster on single devices than distributed
    systems due to communication overhead. Successful distributed training requires
    careful analysis of model size, batch size requirements, and communication patterns
    to achieve actual speedup benefits.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šä»ä¸šè€…æœŸæœ›å¢åŠ æ›´å¤šè®¾å¤‡å°†æˆæ¯”ä¾‹åœ°å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°é€šä¿¡å¼€é”€å’ŒåŒæ­¥æˆæœ¬ã€‚åˆ†å¸ƒå¼è®­ç»ƒå¼•å…¥äº†åè°ƒå¤æ‚æ€§ã€æ¢¯åº¦èšåˆç“¶é¢ˆå’Œå¯èƒ½å‡ç¼“è®­ç»ƒçš„æ”¶æ•›é—®é¢˜ã€‚å°å‹æ¨¡å‹æˆ–æ•°æ®é›†åœ¨å•è®¾å¤‡ä¸Šè®­ç»ƒå¯èƒ½æ¯”åˆ†å¸ƒå¼ç³»ç»Ÿæ›´å¿«ï¼Œå› ä¸ºé€šä¿¡å¼€é”€ã€‚æˆåŠŸçš„åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦ä»”ç»†åˆ†ææ¨¡å‹å¤§å°ã€æ‰¹é‡å¤§å°éœ€æ±‚å’Œé€šä¿¡æ¨¡å¼ï¼Œä»¥å®ç°å®é™…çš„é€Ÿåº¦æå‡æ•ˆç›Šã€‚
- en: '**Fallacy:** *Learning rate schedules that work for small models apply directly
    to large-scale training.*'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '**è°¬è¯¯ï¼š** *é€‚ç”¨äºå°æ¨¡å‹çš„å­¦ä¹ ç‡è°ƒåº¦å¯ä»¥ç›´æ¥åº”ç”¨äºå¤§è§„æ¨¡è®­ç»ƒã€‚*'
- en: This misconception assumes that hyperparameters, particularly learning rates,
    scale linearly with model size or dataset size. Large-scale training often requires
    different optimization dynamics due to gradient noise characteristics, batch size
    effects, and convergence behavior changes. Learning rate schedules optimized for
    small-scale experiments frequently cause instability or poor convergence when
    applied to distributed training scenarios. Effective large-scale training requires
    hyperparameter adaptation specific to the scale and distributed nature of the
    training environment.
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¯¯è§£å‡è®¾è¶…å‚æ•°ï¼Œå°¤å…¶æ˜¯å­¦ä¹ ç‡ï¼Œä¼šéšç€æ¨¡å‹å¤§å°æˆ–æ•°æ®é›†å¤§å°çš„çº¿æ€§å¢é•¿ã€‚ç”±äºæ¢¯åº¦å™ªå£°ç‰¹æ€§ã€æ‰¹é‡å¤§å°æ•ˆåº”å’Œæ”¶æ•›è¡Œä¸ºå˜åŒ–ï¼Œå¤§è§„æ¨¡è®­ç»ƒé€šå¸¸éœ€è¦ä¸åŒçš„ä¼˜åŒ–åŠ¨æ€ã€‚é’ˆå¯¹å°è§„æ¨¡å®éªŒä¼˜åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦åœ¨åº”ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯æ—¶ï¼Œå¾€å¾€ä¼šå¼•èµ·ä¸ç¨³å®šæ€§æˆ–æ”¶æ•›ä¸è‰¯ã€‚æœ‰æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦é’ˆå¯¹è®­ç»ƒç¯å¢ƒçš„è§„æ¨¡å’Œåˆ†å¸ƒå¼ç‰¹æ€§è¿›è¡Œç‰¹å®šçš„è¶…å‚æ•°è°ƒæ•´ã€‚
- en: '**Pitfall:** *Neglecting training reproducibility and experimental tracking.*'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š** *å¿½è§†è®­ç»ƒçš„å¯é‡å¤æ€§å’Œå®éªŒè·Ÿè¸ªã€‚*'
- en: Under pressure to achieve quick results, teams often sacrifice training reproducibility
    by using random seeds inconsistently, failing to track hyperparameters, or running
    experiments without proper versioning. This approach makes it impossible to reproduce
    successful results, compare experiments fairly, or debug training failures. Complex
    distributed training setups amplify these issues, where subtle differences in
    device configuration, data loading order, or software versions can create significant
    result variations. Systematic experiment tracking and reproducibility practices
    are essential engineering disciplines, not optional overhead.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿½æ±‚å¿«é€Ÿç»“æœçš„å‹åŠ›ä¸‹ï¼Œå›¢é˜Ÿå¸¸å¸¸é€šè¿‡ä¸ä¸€è‡´åœ°ä½¿ç”¨éšæœºç§å­ã€æœªèƒ½è·Ÿè¸ªè¶…å‚æ•°æˆ–åœ¨æ²¡æœ‰é€‚å½“ç‰ˆæœ¬æ§åˆ¶çš„æƒ…å†µä¸‹è¿è¡Œå®éªŒæ¥ç‰ºç‰²è®­ç»ƒçš„å¯é‡å¤æ€§ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ— æ³•é‡ç°æˆåŠŸçš„ç»“æœã€å…¬å¹³åœ°æ¯”è¾ƒå®éªŒæˆ–è°ƒè¯•è®­ç»ƒå¤±è´¥ã€‚å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®æ”¾å¤§äº†è¿™äº›é—®é¢˜ï¼Œå…¶ä¸­è®¾å¤‡é…ç½®ã€æ•°æ®åŠ è½½é¡ºåºæˆ–è½¯ä»¶ç‰ˆæœ¬ä¸­çš„ç»†å¾®å·®å¼‚å¯ä»¥é€ æˆæ˜¾è‘—çš„ç»“æœå˜åŒ–ã€‚ç³»ç»Ÿæ€§çš„å®éªŒè·Ÿè¸ªå’Œå¯é‡å¤æ€§å®è·µæ˜¯è‡³å…³é‡è¦çš„å·¥ç¨‹å­¦ç§‘ï¼Œè€Œä¸æ˜¯å¯é€‰çš„é¢å¤–å¼€é”€ã€‚
- en: '**Pitfall:** *Underestimating infrastructure complexity and failure modes in
    distributed training systems.*'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š***ä½ä¼°åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸­çš„åŸºç¡€è®¾æ–½å¤æ‚æ€§å’Œæ•…éšœæ¨¡å¼ã€‚*'
- en: Many teams approach distributed training as a straightforward scaling exercise
    without adequately planning for the infrastructure challenges that emerge at scale.
    Distributed training systems introduce complex failure modes including node failures,
    network partitions, memory pressure from unbalanced load distribution, and synchronization
    deadlocks that can cause entire training runs to fail hours or days into execution.
    Hardware heterogeneity across training clusters creates performance imbalances
    where slower nodes become bottlenecks, while network topology and bandwidth limitations
    can make communication costs dominate computation time. Effective distributed
    training requires robust checkpoint and recovery mechanisms, load balancing strategies,
    health monitoring systems, and fallback procedures for handling partial failures.
    The infrastructure must also account for dynamic resource allocation, spot instance
    interruptions in cloud environments, and the operational complexity of maintaining
    consistent software environments across distributed workers.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šå›¢é˜Ÿå°†åˆ†å¸ƒå¼è®­ç»ƒè§†ä¸ºä¸€ä¸ªç®€å•çš„æ‰©å±•ç»ƒä¹ ï¼Œè€Œæ²¡æœ‰å……åˆ†è§„åˆ’åœ¨è§„æ¨¡æ‰©å¤§æ—¶å‡ºç°çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ã€‚åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿå¼•å…¥äº†å¤æ‚çš„æ•…éšœæ¨¡å¼ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹æ•…éšœã€ç½‘ç»œåˆ†åŒºã€ç”±ä¸å¹³è¡¡è´Ÿè½½åˆ†å¸ƒå¼•èµ·çš„å†…å­˜å‹åŠ›ä»¥åŠå¯èƒ½å¯¼è‡´æ•´ä¸ªè®­ç»ƒè¿è¡Œåœ¨æ‰§è¡Œæ•°å°æ—¶æˆ–æ•°å¤©åå¤±è´¥çš„åŒæ­¥æ­»é”ã€‚è®­ç»ƒé›†ç¾¤ä¸­çš„ç¡¬ä»¶å¼‚æ„æ€§é€ æˆäº†æ€§èƒ½ä¸å¹³è¡¡ï¼Œå…¶ä¸­è¾ƒæ…¢çš„èŠ‚ç‚¹æˆä¸ºç“¶é¢ˆï¼Œè€Œç½‘ç»œæ‹“æ‰‘å’Œå¸¦å®½é™åˆ¶å¯èƒ½ä½¿é€šä¿¡æˆæœ¬ä¸»å¯¼è®¡ç®—æ—¶é—´ã€‚æœ‰æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦å¼ºå¤§çš„æ£€æŸ¥ç‚¹å’Œæ¢å¤æœºåˆ¶ã€è´Ÿè½½å¹³è¡¡ç­–ç•¥ã€å¥åº·ç›‘æ§ç³»ç»Ÿä»¥åŠå¤„ç†éƒ¨åˆ†æ•…éšœçš„å›é€€ç¨‹åºã€‚åŸºç¡€è®¾æ–½è¿˜å¿…é¡»è€ƒè™‘åˆ°åŠ¨æ€èµ„æºåˆ†é…ã€äº‘ç¯å¢ƒä¸­çš„spotå®ä¾‹ä¸­æ–­ä»¥åŠç»´æŠ¤åˆ†å¸ƒå¼å·¥ä½œè€…ä¹‹é—´ä¸€è‡´è½¯ä»¶ç¯å¢ƒçš„æ“ä½œå¤æ‚æ€§ã€‚
- en: Summary
  id: totrans-1111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Training represents the computational heart of machine learning systems, where
    mathematical algorithms, memory management strategies, and distributed computing
    architectures converge to transform data into intelligent models. Throughout this
    chapter, we have seen how the seemingly simple concept of iterative parameter
    optimization requires careful engineering solutions to handle the scale and complexity
    of modern machine learning workloads. The operations of forward and backward propagation
    become orchestrations of matrix operations, memory allocations, and gradient computations
    that must be carefully balanced against hardware constraints and performance requirements.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ˜¯æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è®¡ç®—æ ¸å¿ƒï¼Œåœ¨è¿™é‡Œæ•°å­¦ç®—æ³•ã€å†…å­˜ç®¡ç†ç­–ç•¥å’Œåˆ†å¸ƒå¼è®¡ç®—æ¶æ„æ±‡èšåœ¨ä¸€èµ·ï¼Œå°†æ•°æ®è½¬åŒ–ä¸ºæ™ºèƒ½æ¨¡å‹ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†çœ‹ä¼¼ç®€å•çš„è¿­ä»£å‚æ•°ä¼˜åŒ–æ¦‚å¿µéœ€è¦ä»”ç»†çš„å·¥ç¨‹è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½çš„è§„æ¨¡å’Œå¤æ‚æ€§ã€‚å‰å‘å’Œåå‘ä¼ æ’­çš„æ“ä½œå˜æˆäº†çŸ©é˜µè¿ç®—ã€å†…å­˜åˆ†é…å’Œæ¢¯åº¦è®¡ç®—çš„ç¼–æ’ï¼Œè¿™äº›å¿…é¡»ä»”ç»†å¹³è¡¡ä»¥ç¬¦åˆç¡¬ä»¶çº¦æŸå’Œæ€§èƒ½è¦æ±‚ã€‚
- en: Our exploration from single-device training to distributed systems demonstrates
    how computational bottlenecks drive architectural innovation rather than simply
    limiting capabilities. Data parallelism enables scaling across multiple devices
    by distributing training examples, while model parallelism addresses memory limitations
    by partitioning model parameters across hardware resources. Advanced techniques
    like gradient accumulation, mixed precision training, and pipeline parallelism
    showcase how training systems can optimize memory usage, computational throughput,
    and convergence stability simultaneously. The interplay between these strategies
    reveals that effective training system design requires deep understanding of both
    algorithmic properties and hardware characteristics to achieve optimal resource
    utilization.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å•è®¾å¤‡è®­ç»ƒåˆ°åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¢ç´¢å±•ç¤ºäº†è®¡ç®—ç“¶é¢ˆå¦‚ä½•æ¨åŠ¨æ¶æ„åˆ›æ–°ï¼Œè€Œä¸ä»…ä»…æ˜¯é™åˆ¶èƒ½åŠ›ã€‚æ•°æ®å¹¶è¡Œé€šè¿‡åˆ†å¸ƒè®­ç»ƒç¤ºä¾‹å®ç°è·¨å¤šä¸ªè®¾å¤‡çš„æ‰©å±•ï¼Œè€Œæ¨¡å‹å¹¶è¡Œé€šè¿‡åœ¨ç¡¬ä»¶èµ„æºé—´åˆ’åˆ†æ¨¡å‹å‚æ•°æ¥è§£å†³å†…å­˜é™åˆ¶ã€‚æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒå’Œæµæ°´çº¿å¹¶è¡Œç­‰é«˜çº§æŠ€æœ¯å±•ç¤ºäº†è®­ç»ƒç³»ç»Ÿå¦‚ä½•åŒæ—¶ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€è®¡ç®—ååé‡å’Œæ”¶æ•›ç¨³å®šæ€§ã€‚è¿™äº›ç­–ç•¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨è¡¨æ˜ï¼Œæœ‰æ•ˆçš„è®­ç»ƒç³»ç»Ÿè®¾è®¡éœ€è¦æ·±å…¥ç†è§£ç®—æ³•ç‰¹æ€§å’Œç¡¬ä»¶ç‰¹æ€§ï¼Œä»¥å®ç°æœ€ä½³èµ„æºåˆ©ç”¨ã€‚
- en: This co-design principleâ€”where algorithms, software frameworks, and hardware
    architectures evolve togetherâ€”shapes modern training infrastructure. Matrix operation
    patterns drove GPU Tensor Core development, which frameworks exposed through mixed-precision
    APIs, enabling algorithmic techniques like FP16 training that further influenced
    next-generation hardware design. Understanding this feedback loop between computational
    requirements and system capabilities enables practitioners to make informed architectural
    decisions that leverage the full potential of modern training systems.
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ååŒè®¾è®¡åŸåˆ™â€”â€”ç®—æ³•ã€è½¯ä»¶æ¡†æ¶å’Œç¡¬ä»¶æ¶æ„å…±åŒè¿›åŒ–â€”â€”å¡‘é€ äº†ç°ä»£è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚çŸ©é˜µè¿ç®—æ¨¡å¼æ¨åŠ¨äº†GPU Tensor Coreçš„å‘å±•ï¼Œæ¡†æ¶é€šè¿‡æ··åˆç²¾åº¦APIæš´éœ²å‡ºæ¥ï¼Œä½¿å¾—ç®—æ³•æŠ€æœ¯å¦‚FP16è®­ç»ƒå¾—ä»¥å®ç°ï¼Œè¿™è¿›ä¸€æ­¥å½±å“äº†ä¸‹ä¸€ä»£ç¡¬ä»¶è®¾è®¡ã€‚ç†è§£è®¡ç®—éœ€æ±‚ä¸ç³»ç»Ÿèƒ½åŠ›ä¹‹é—´çš„åé¦ˆå¾ªç¯ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿåšå‡ºåˆ©ç”¨ç°ä»£è®­ç»ƒç³»ç»Ÿå…¨éƒ¨æ½œåŠ›çš„æ˜æ™ºçš„æ¶æ„å†³ç­–ã€‚
- en: The training optimizations explored throughout this chapter provide the foundation
    for the model-level efficiency techniques and deployment strategies examined in
    subsequent chapters. These systems principles extend naturally from training infrastructure
    to production inference systems, demonstrating how the engineering insights gained
    from optimizing training workflows inform the broader machine learning system
    lifecycle.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­æ¢è®¨çš„è®­ç»ƒä¼˜åŒ–ä¸ºåç»­ç« èŠ‚ä¸­æ£€æŸ¥çš„æ¨¡å‹çº§æ•ˆç‡æŠ€æœ¯å’Œéƒ¨ç½²ç­–ç•¥æä¾›äº†åŸºç¡€ã€‚è¿™äº›ç³»ç»ŸåŸåˆ™è‡ªç„¶åœ°ä»è®­ç»ƒåŸºç¡€è®¾æ–½æ‰©å±•åˆ°ç”Ÿäº§æ¨ç†ç³»ç»Ÿï¼Œå±•ç¤ºäº†ä»ä¼˜åŒ–è®­ç»ƒå·¥ä½œæµç¨‹ä¸­è·å¾—çš„ç»éªŒå¦‚ä½•å½±å“æ›´å¹¿æ³›çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿç”Ÿå‘½å‘¨æœŸã€‚
- en: '**Key Takeaways**'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®è¦ç‚¹**'
- en: Training efficiency depends on optimizing the entire pipeline from data loading
    through gradient computation and parameter updates
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•ˆç‡å–å†³äºä¼˜åŒ–ä»æ•°æ®åŠ è½½åˆ°æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°çš„æ•´ä¸ªæµç¨‹
- en: Distributed training strategies must balance communication overhead against
    computational parallelism to achieve scaling benefits
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥å¿…é¡»å¹³è¡¡é€šä¿¡å¼€é”€å’Œè®¡ç®—å¹¶è¡Œæ€§ï¼Œä»¥å®ç°æ‰©å±•å¸¦æ¥çš„å¥½å¤„
- en: Memory management techniques like gradient checkpointing and mixed precision
    are essential for training large models within hardware constraints
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦ç­‰å†…å­˜ç®¡ç†æŠ€æœ¯å¯¹äºåœ¨ç¡¬ä»¶çº¦æŸä¸‹è®­ç»ƒå¤§å‹æ¨¡å‹è‡³å…³é‡è¦
- en: Successful training systems require co-design of algorithms, software frameworks,
    and hardware architectures
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆåŠŸçš„è®­ç»ƒç³»ç»Ÿéœ€è¦ç®—æ³•ã€è½¯ä»¶æ¡†æ¶å’Œç¡¬ä»¶æ¶æ„çš„ååŒè®¾è®¡
- en: These principles and techniques provide the foundation for understanding how
    model optimization, hardware acceleration, and deployment strategies build upon
    training infrastructure to create complete machine learning systems. As models
    continue growing in size and complexity, these training techniques become increasingly
    critical for making advanced AI capabilities accessible and practical across diverse
    application domains and computational environments.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸåˆ™å’ŒæŠ€æœ¯ä¸ºç†è§£æ¨¡å‹ä¼˜åŒ–ã€ç¡¬ä»¶åŠ é€Ÿå’Œéƒ¨ç½²ç­–ç•¥å¦‚ä½•å»ºç«‹åœ¨è®­ç»ƒåŸºç¡€è®¾æ–½ä¹‹ä¸Šï¼Œä»¥åˆ›å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿæä¾›äº†åŸºç¡€ã€‚éšç€æ¨¡å‹åœ¨è§„æ¨¡å’Œå¤æ‚æ€§ä¸Šçš„æŒç»­å¢é•¿ï¼Œè¿™äº›è®­ç»ƒæŠ€æœ¯å¯¹äºä½¿é«˜çº§äººå·¥æ™ºèƒ½èƒ½åŠ›åœ¨å¤šæ ·åŒ–çš„åº”ç”¨é¢†åŸŸå’Œè®¡ç®—ç¯å¢ƒä¸­å˜å¾—å¯è®¿é—®å’Œå®ç”¨å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚
- en: '* * *'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
