- en: 8.1.2 Questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.1.2 问题
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html](https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html](https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html)
- en: '[E] What are the basic assumptions to be made for linear regression?'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 线性回归的基本假设是什么？'
- en: '[E] What happens if we don’t apply feature scaling to logistic regression?'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 如果我们不应用特征缩放到逻辑回归会发生什么？'
- en: '[E] What are the algorithms you’d use when developing the prototype of a fraud
    detection model?'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 在开发欺诈检测模型的原型时，你会使用哪些算法？'
- en: Feature selection.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征选择。
- en: '[E] Why do we use feature selection?'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么我们使用特征选择？'
- en: '[M] What are some of the algorithms for feature selection? Pros and cons of
    each.'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 特征选择有哪些算法？每种算法的优缺点。'
- en: k-means clustering.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-means聚类。
- en: '[E] How would you choose the value of k?'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你会如何选择k的值？'
- en: '[E] If the labels are known, how would you evaluate the performance of your
    k-means clustering algorithm?'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 如果标签已知，你会如何评估你的k-means聚类算法的性能？'
- en: '[M] How would you do it if the labels aren’t known?'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 如果标签未知，你会如何处理？'
- en: '[H] Given the following dataset, can you predict how K-means clustering works
    on it? Explain.'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 给定以下数据集，你能预测K-means聚类是如何工作的吗？解释。'
- en: '![k-means clustering](../Images/8ca03d741d2431482b90658ce75bae5b.png "image_tooltip")'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![k-means聚类](../Images/8ca03d741d2431482b90658ce75bae5b.png "image_tooltip")'
- en: k-nearest neighbor classification.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-最近邻分类。
- en: '[E] How would you choose the value of k?'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你会如何选择k的值？'
- en: '[E] What happens when you increase or decrease the value of k?'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 当你增加或减少k的值时会发生什么？'
- en: '[M] How does the value of k impact the bias and variance?'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] k的值如何影响偏差和方差？'
- en: k-means and GMM are both powerful clustering algorithms.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-means和GMM都是强大的聚类算法。
- en: '[M] Compare the two.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 比较这两个。'
- en: '[M] When would you choose one over another?'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你会在什么情况下选择其中一个而不是另一个？'
- en: '**Hint**: Here’s an example of how K-means and GMM algorithms perform on the
    artificial mouse dataset.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**提示**：以下是一个关于K-means和GMM算法在人工老鼠数据集上表现的例子。'
- en: '![k-means clustering vs. gaussian mixture model](../Images/defccceebaa2c20384ab102a0cb3cfbc.png
    "image_tooltip")'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![k-means聚类与高斯混合模型](../Images/defccceebaa2c20384ab102a0cb3cfbc.png "image_tooltip")'
- en: Image from [Mohamad Ghassany’s course on Machine Learning](https://www.mghassany.com/MLcourse/gaussian-mixture-models-em.html)
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图片来自[Mohamad Ghassany的机器学习课程](https://www.mghassany.com/MLcourse/gaussian-mixture-models-em.html)
- en: Bagging and boosting are two popular ensembling methods. Random forest is a
    bagging example while XGBoost is a boosting example.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bagging和boosting是两种流行的集成方法。随机森林是bagging的一个例子，而XGBoost是boosting的一个例子。
- en: '[M] What are some of the fundamental differences between bagging and boosting
    algorithms?'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 两种集成算法（bagging和boosting）之间有哪些基本区别？'
- en: '[M] How are they used in deep learning?'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 它们在深度学习中是如何使用的？'
- en: Given this directed graph.![Adjacency matrix](../Images/7a269af62de8405ddc2ba8f041e3d690.png
    "image_tooltip")
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定这个有向图。![邻接矩阵](../Images/7a269af62de8405ddc2ba8f041e3d690.png "image_tooltip")
- en: '[E] Construct its adjacency matrix.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 构建它的邻接矩阵。'
- en: '[E] How would this matrix change if the graph is now undirected?'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 如果图现在是无向的，这个矩阵会如何变化？'
- en: '[M] What can you say about the adjacency matrices of two isomorphic graphs?'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你能说些什么关于两个同构图的邻接矩阵？'
- en: Imagine we build a user-item collaborative filtering system to recommend to
    each user items similar to the items they’ve bought before.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想象我们构建一个用户-项目协同过滤系统，向每个用户推荐他们之前购买过的类似项目。
- en: '[M] You can build either a user-item matrix or an item-item matrix. What are
    the pros and cons of each approach?'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你可以构建用户-项目矩阵或项目-项目矩阵。每种方法的优缺点是什么？'
- en: '[E] How would you handle a new user who hasn’t made any purchases in the past?'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 如果一个新用户过去没有进行过任何购买，你会如何处理？'
- en: '[E] Is feature scaling necessary for kernel methods?'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 特征缩放对于核方法是否必要？'
- en: Naive Bayes classifier.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器。
- en: '[E] How is Naive Bayes classifier naive?'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么朴素贝叶斯分类器被称为朴素？'
- en: '[M] Let’s try to construct a Naive Bayes classifier to classify whether a tweet
    has a positive or negative sentiment. We have four training samples:'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 让我们尝试构建一个朴素贝叶斯分类器来分类推文是正面还是负面情绪。我们有四个训练样本：'
- en: '| **Tweet** | **Label** |'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| **推文** | **标签** |'
- en: '| This makes me so upset | Negative |'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 这让我非常生气 | 负面 |'
- en: '| This puppy makes me happy | Positive |'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 这只小狗让我很高兴 | 正面 |'
- en: '| Look at this happy hamster | Positive |'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 看看这只快乐的仓鼠 | 正面 |'
- en: '| No hamsters allowed in my house | Negative |'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 我家里不允许有仓鼠 | 负面 |'
- en: According to your classifier, what's sentiment of the sentence `The hamster
    is upset with the puppy`?
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据您的分类器，句子“仓鼠对小狗感到不满”的情感是什么？
- en: Two popular algorithms for winning Kaggle solutions are Light GBM and XGBoost.
    They are both gradient boosting algorithms.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个在Kaggle解决方案中常用的算法是Light GBM和XGBoost。它们都是梯度提升算法。
- en: '[E] What is gradient boosting?'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么是梯度提升？'
- en: '[M] What problems is gradient boosting good for?'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 梯度提升适用于哪些问题？'
- en: SVM.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVM。
- en: '[E] What’s linear separation? Why is it desirable when we use SVM?'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么是线性分离？为什么当我们使用SVM时，线性分离是可取的？'
- en: '[M] How well would vanilla SVM work on this dataset?'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 在这个数据集上，原始的SVM会表现如何？'
- en: '![Adjacency matrix](../Images/28ebf3793aeb63ffb66fbd2ea3b9166b.png "image_tooltip")'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![邻接矩阵](../Images/28ebf3793aeb63ffb66fbd2ea3b9166b.png "image_tooltip")'
- en: '[M] How well would vanilla SVM work on this dataset?'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 在这个数据集上，原始的SVM会表现如何？'
- en: '![Adjacency matrix](../Images/7e618f095e074e364a89f262b97d5855.png "image_tooltip")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![邻接矩阵](../Images/7e618f095e074e364a89f262b97d5855.png "image_tooltip")'
- en: '[M] How well would vanilla SVM work on this dataset?'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 在这个数据集上，原始的SVM会表现如何？'
- en: '![Adjacency matrix](../Images/22476012f7e6c58e01c258ca4feff488.png "image_tooltip")'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![邻接矩阵](../Images/22476012f7e6c58e01c258ca4feff488.png "image_tooltip")'
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright ©2021 Chip Huyen.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*这本书是由[Chip Huyen](https://huyenchip.com)在众多朋友的帮助下创作的。对于反馈、勘误和建议，作者可以通过[这里](https://huyenchip.com/communication/)联系。版权©2021
    Chip Huyen。*'
