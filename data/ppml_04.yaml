- en: Chapter 2 Hardware Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 硬件架构
- en: 原文：[https://ppml.dev/hardware.html](https://ppml.dev/hardware.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://ppml.dev/hardware.html](https://ppml.dev/hardware.html)
- en: Building a compute system to run machine learning software requires careful
    planning. How well it will perform depends on choosing the right hardware; a set
    of machine learning algorithms that can attack efficiently and effectively the
    task we want to perform; and how to represent the data we will use, the models
    and their outputs. For the purpose of this book, we define a “compute system”
    as a computer system, not necessarily server-class, that will perform one or more
    of the tasks required to learn or use machine learning models. We will often call
    it a “machine learning system” as well to highlight its purpose. Other types of
    systems, such as those focusing on storage (database servers, data lakes, object
    storage) or delivery (human-readable dashboards, computer-readable API endpoints)
    will be mentioned only in passing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个运行机器学习软件的计算系统需要周密的规划。其性能如何取决于选择正确的硬件；一套能够高效有效地执行我们想要执行的任务的机器学习算法；以及如何表示我们将使用的数据、模型及其输出。为了本书的目的，我们将“计算系统”定义为一种计算机系统，不一定是服务器级别的，它将执行学习或使用机器学习模型所需的一个或多个任务。我们通常会称之为“机器学习系统”，以突出其目的。其他类型的系统，如专注于存储（数据库服务器、数据湖、对象存储）或交付（可读性仪表板、计算机可读API端点）的系统，将仅作简要提及。
- en: 'In this chapter we focus on the hardware, moving to the data in Chapter [3](types-structures.html#types-structures)
    and to the algorithms in Chapter [4](algorithms.html#algorithms). After covering
    the key aspects of a compute system (Section [2.1](hardware.html#hardware-types)),
    we discuss how to use it to the best of its potential (Section [2.2](hardware.html#hardware-using)),
    the trade-offs involved in integrating remote systems (Section [2.3](hardware.html#hardware-cloud))
    and how to design it based on our requirements (Section [2.4](hardware.html#hardware-choice)).
    Modern machine learning libraries try to make these decisions for us, but they
    have limits that become apparent in many real-world applications: in such cases,
    being able to reason about the hardware is an invaluable skill.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于硬件，第[3](types-structures.html#types-structures)章将转向数据，第[4](algorithms.html#algorithms)章将转向算法。在介绍计算系统的关键方面（第[2.1](hardware.html#hardware-types)节）之后，我们讨论如何最大限度地发挥其潜力（第[2.2](hardware.html#hardware-using)节），集成远程系统所涉及的权衡（第[2.3](hardware.html#hardware-cloud)节）以及如何根据我们的需求进行设计（第[2.4](hardware.html#hardware-choice)节）。现代机器学习库试图为我们做出这些决定，但它们在许多实际应用中都有局限性：在这种情况下，能够对硬件进行推理是一项无价的技能。
- en: '![A schematic view of the key components that may appear in a modern compute
    system (not necessarily all at the same time). Sizes and distances are not to
    scale.](../Images/547fdc3726b6a903570f2227ea79c1de.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![现代计算系统中可能出现的关键组件的示意图（不一定同时出现）。尺寸和距离仅供参考。](../Images/547fdc3726b6a903570f2227ea79c1de.png)'
- en: 'Figure 2.1: A schematic view of the key components that may appear in a modern
    compute system (not necessarily all at the same time). Sizes and distances are
    not to scale.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：现代计算系统中可能出现的关键组件的示意图（不一定同时出现）。尺寸和距离仅供参考。
- en: 2.1 Types of Hardware
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 硬件类型
- en: 'Compute systems come in a variety of configurations whose components are summarised
    in Figure [2.1](hardware.html#fig:schematic). Broadly speaking, we can say that
    they vary along three axes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算系统有多种配置，其组件在图[2.1](hardware.html#fig:schematic)中总结。总的来说，我们可以认为它们沿着三个轴进行变化：
- en: '*compute*, the processors that perform the operations required to learn and
    run our machine learning models;'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计算*，执行学习并运行我们的机器学习模型所需的处理器。'
- en: '*memory* to store the data, the models and their outputs as variables and data
    structures; and the'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*内存*，用于存储数据、模型及其输出作为变量和数据结构；以及'
- en: '*connections* that we use to move the data and the models around.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*连接*，我们使用它来移动数据和模型。'
- en: 'These dimensions are admittedly somewhat arbitrary, but they are well-suited
    to discuss the topics we will cover in this book. In this context, choosing the
    right hardware means choosing a trade-off in terms of compute, memory and connections
    that allows the machine learning system to achieve the goals it is designed for
    while fitting the available budget. To quote one of the universal truths from
    RFC 1925 (Callon [1996](#ref-rfc1925)):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些维度虽然可以说是有些任意，但非常适合讨论本书中将要涉及的主题。在这种情况下，选择合适的硬件意味着在计算、内存和连接方面做出权衡，从而使机器学习系统能够实现其设计目标，同时适应可用的预算。引用RFC
    1925（Callon [1996](#ref-rfc1925)）中的一个普遍真理：
- en: '“(7a) Good, Fast, Cheap: Pick any two (you can’t have all three).”'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “（7a）好、快、便宜：你可以选择任意两个（你不能三者兼得）。”
- en: 2.1.1 Compute
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 计算
- en: 'The three types of processors we can commonly find in machine learning systems
    are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在机器学习系统中通常可以找到的三种处理器类型是：
- en: '*Central processing units* (CPUs), usually either an x86-64 processor from
    AMD or Intel or an ARM processor.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*中央处理器*（CPUs），通常是来自AMD或Intel的x86-64处理器，或者ARM处理器。'
- en: '*Graphics processing units* (GPUs) from NVidia or AMD.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自NVidia或AMD的*图形处理器*（GPUs）。
- en: '*Tensor processing units* (TPUs), usually from Google. Other specialised hardware
    to accelerate machine learning certainly exists (Reuther et al. [2020](#ref-ai-accelerators))
    but, for practical purposes, fulfils the same role as TPUs.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*张量处理器*（TPUs），通常来自Google。当然，还有其他专门用于加速机器学习的硬件（Reuther et al. [2020](#ref-ai-accelerators)），但就实际用途而言，与TPUs发挥着相同的作用。'
- en: 'CPUs, GPUs and TPUs represent different trade-offs in terms of speed, capabilities
    and versatility. Trading one off for another is unavoidable: the end of the “easy”
    performance improvements granted by Moore’s law (transistors per chip double each
    year or two) and by Dennard’s law (power density is constant as transistors get
    smaller, that is, we get more transistors) mean that we cannot expect general-purpose
    processors to become faster at the rate we were used to. Transistors cannot get
    any smaller without breaking the laws of physics. Current and voltage cannot drop
    any further while keeping transistors dependable, so we cannot easily double transistors
    per chip anymore. The only way out of this conundrum is domain-specific architectures
    that use their transistor- and power-budgets to the fullest for specific types
    of operations (Jouppi et al. [2018](#ref-patterson)). Hence the rise of GPUs and,
    more recently, TPUs in machine learning applications.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CPU、GPU和TPU在速度、功能和通用性方面代表了不同的权衡。为了另一个而牺牲一个是无法避免的：摩尔定律（芯片上的晶体管数量每年或每两年翻倍）和丹纳德定律（随着晶体管变得更小，功率密度保持不变，也就是说，我们得到更多的晶体管）赋予的“简单”性能提升的终结意味着我们不能再期望通用处理器以我们习惯的速度变快。晶体管不能无限制地变得更小，否则会违反物理定律。电流和电压不能进一步降低，同时保持晶体管的可靠性，所以我们不能再轻易地将芯片上的晶体管数量翻倍。摆脱这一困境的唯一方法是使用特定领域的架构，这些架构利用它们的晶体管和电力预算来最大化特定类型的操作（Jouppi
    et al. [2018](#ref-patterson)）。因此，GPU和，更近期的，TPU在机器学习应用中的兴起。
- en: 'CPUs are the most versatile type of compute: they can perform a wide range
    of operations by means of the instructions they implement; they can perform multiple
    operations in parallel to some extent (whether on different cores or on different
    threads on the same core); and they can efficiently handle any type of data. At
    the same time, CPUs contain *logical units* that implement specialised single-instruction
    multiple-data (SIMD) instruction sets such as the Streaming SIMD Extensions (SSE1
    to SSE4 on x86, Neon on ARM) and the Advanced Vector Extensions (AVX, AVX2, AVX512
    on x86, SVM on ARM) to perform numerical computations efficiently and simultaneously
    on multiple variables.[²](#fn2) The speed-ups that can be obtained by their use
    can be substantial, ranging from 10–15% to a factor of 10 (see, for instance,
    Williams-Young and Li [2019](#ref-young); Fortin et al. [2021](#ref-monagan)).
    The main limitation of SIMD instructions is that they can only operate on the
    contents of the *registers*, the smallest and fastest memory inside the CPU. For
    instance, on x86-64 CPUs each register can hold 2-16 floating point numbers, and
    there are 16 (SSE, AVX, AVX2) or 32 (AVX512) registers for each core.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CPU是最通用的计算类型：它们可以通过它们实现的指令执行广泛的操作；它们可以在一定程度上并行执行多个操作（无论是在不同的核心上还是在同一核心的不同线程上）；并且它们可以高效地处理任何类型的数据。同时，CPU包含*逻辑单元*，这些单元实现了专门的单一指令多数据（SIMD）指令集，如流式SIMD扩展（x86上的SSE1到SSE4，ARM上的Neon）和高级向量扩展（x86上的AVX、AVX2、AVX512，ARM上的SVM），以高效且同时地在多个变量上执行数值计算。[²](#fn2)通过使用这些指令可以获得显著的加速效果，从10-15%到10倍不等（例如，参见Williams-Young和Li
    [2019](#ref-young)；Fortin等人 [2021](#ref-monagan)）。SIMD指令的主要限制是它们只能操作CPU内部最小型、最快的内存*寄存器*的内容。例如，在x86-64
    CPU上，每个寄存器可以存储2-16个浮点数，每个核心有16个（SSE、AVX、AVX2）或32个（AVX512）寄存器。
- en: Another type of instruction that performs multiple operations in parallel are
    *fused operations* such as FMA (“fused add and multiply”), which perform predefined
    sets of operations on multiple variables and in approximately the same time it
    would take to perform a single operation. They also have the added benefit of
    performing just one floating point rounding to precision after the last operation,
    thus eliminating many rounding issues (more on this in Section [3.1.2](types-structures.html#floating-point)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种并行执行多个操作的指令类型是*融合操作*，例如FMA（“融合加法和乘法”），它对多个变量执行预定义的操作集，并且所需时间大约与执行单个操作的时间相同。它们还具有额外的优势，即在最后一个操作之后只需进行一次浮点数四舍五入到精度，从而消除了许多舍入问题（更多内容请参阅第[3.1.2](types-structures.html#floating-point)节）。
- en: 'GPUs specialise in parallel computations over large amounts of data. They are
    fundamentally different from CPUs: they behave like asynchronous devices in which
    we load data, we wait for data to be processed and then we collect the results.
    For practical purposes, modern GPUs can be viewed as multithreaded, multicore
    vector processors (Volkov and Demmel [2008](#ref-benchmarking-gpus)) designed
    to operate on one-dimensional arrays of data. The data is internally subdivided
    into blocks that will be processed by hundreds of independent, identical *units*.[³](#fn3)
    Each unit is extremely simple: it is close to a CPU’s SIMD/FMA logical unit in
    terms of functionality. It has its own set of registers and a local memory cache,
    but it can only apply a single type of operation at a time and is completely driven
    by the GPU scheduler. The GPU scheduler takes care of keeping the units busy by
    assigning them tasks in such a way as to maximise *occupancy* (the overall load
    of the GPU) and by keeping them fed with data from the global memory of the GPU.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: GPU擅长处理大量数据的并行计算。它们与CPU在本质上不同：它们的行为类似于异步设备，其中我们加载数据，等待数据被处理，然后收集结果。从实用角度来看，现代GPU可以被视为多线程、多核向量处理器（Volkov和Demmel
    [2008](#ref-benchmarking-gpus)），旨在处理一维数据数组。数据在内部被细分为将被数百个独立、相同的*单元*处理的块。[³](#fn3)每个单元极其简单：在功能上接近CPU的SIMD/FMA逻辑单元。它有自己的寄存器集和局部内存缓存，但一次只能应用一种类型的操作，并且完全由GPU调度器驱动。GPU调度器通过以最大化*占用率*（GPU的整体负载）的方式分配任务，并确保它们从GPU的全局内存中获取数据，来确保单元忙碌。
- en: 'This level of parallelism makes them potentially much faster than CPUs: a CPU
    has at most a few tens of cores, whereas a GPU has hundreds of units that are
    equally capable of using SIMD instructions. This allows the GPU scheduler to handle
    tasks of unequal sizes and with different latencies, limiting their impact on
    the efficiency of parallel computations. Furthermore, each GPU unit has more registers[⁴](#fn4)
    than a CPU core and can work on a much larger amount of data at extremely low
    latencies.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种级别的并行性使它们在理论上比CPU快得多：CPU最多只有几十个核心，而GPU有数百个单元，这些单元都能同样有效地使用SIMD指令。这使得GPU调度器能够处理大小不等且具有不同延迟的任务，从而限制它们对并行计算效率的影响。此外，每个GPU单元比CPU核心拥有更多的寄存器[⁴](#fn4)并且可以在极低的延迟下处理大量数据。
- en: 'On the other hand, the hardware design that makes all of this possible restricts
    what a GPU can do. Units are optimised to work on 32-bit floating point and integer
    numbers; modern hardware also supports 16-bit and 64-bit floating point numbers
    well, but working with other types of variables is difficult. Getting data to
    the units requires copying them first to the GPU global memory, where they will
    be stored in one or more memory banks. Different units cannot read different data
    from the same memory bank at the same time, so we should carefully optimise the
    layout of the data in memory. Furthermore, data are assumed to be organised into
    one-dimensional arrays: further structure is disregarded. Support for branching
    (that is, if-then-else programming constructs) is limited to the GPU scheduler:
    units have no concept of conditional execution at all. Finally, units are organised
    in groups of 32 to 64, and the GPU scheduler can only allocate tasks to groups.
    Any task whose size is not a multiple of the group size will result in under-utilised
    groups and decrease occupancy.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，使所有这些成为可能的硬件设计限制了GPU的功能。单元被优化以处理32位浮点数和整数；现代硬件也很好地支持16位和64位浮点数，但处理其他类型的变量则比较困难。将数据传输到单元需要先将它们复制到GPU全局内存中，在那里它们将被存储在一个或多个内存银行中。不同的单元不能在同一时间从同一个内存银行中读取不同的数据，因此我们应该仔细优化内存中数据的布局。此外，数据被假定为组织成一维数组：进一步的结构被忽略。对分支（即if-then-else编程结构）的支持仅限于GPU调度器：单元根本不具备条件执行的概念。最后，单元被组织成32到64个一组，GPU调度器只能将任务分配给组。任何大小不是组大小倍数的任务都会导致组利用率不足并降低占用率。
- en: 'TPUs are even more specialised: they are expressly built for training and performing
    inference on deep neural network models with the best possible average and tail
    performance. The architecture of TPU cores[⁵](#fn5) rests on five design decisions
    (Jouppi et al. [2018](#ref-patterson)):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TPUs更加专业化：它们是专门为训练和执行深度神经网络模型的推理而构建的，以实现最佳的平均和尾部性能。TPU核心[⁵](#fn5)的架构基于五个设计决策（Jouppi等人[2018](#ref-patterson)）：
- en: including a single, very simple core per processor;
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个处理器包含一个非常简单的核心；
- en: concentrating most computing power in a large, two-dimensional matrix-multiply
    unit;
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大部分计算能力集中在一个大型的二维矩阵乘法单元中。
- en: organising memory in a mesh network that allows for asynchronous, lockless communications
    between cores;
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将内存组织成网状网络，允许核心之间进行异步、无锁通信；
- en: implementing hardware support for integers and floats with limited precision,
    which use less memory;
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现对整数和有限精度的浮点数的硬件支持，这可以节省内存；
- en: dropping all the features that are not strictly needed for working with deep
    neural networks.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃所有不是严格必需用于处理深度神经网络的功能；
- en: This single-minded focus on deep learning makes TPUs the best hardware to use
    for this kind of models, with documented speed-ups on the order of 20-30 times
    over GPUs in terms of performance per watt (Jouppi et al. [2018](#ref-patterson)).
    In particular, (Jouppi et al. [2020](#ref-patterson2)) reports that TPUs are 50
    times faster per watt for inference and 5 to 10 times faster for training. These
    improvements are largely driven by the fact that TPU cores are much smaller than
    GPU or CPU cores (38 times less area), so they consume (13 times) less energy
    and leave a larger share of the available transistors to the matrix-multiply unit.
    Another important factor is the memory layout, which allows deadlock-free communications
    between TPU cores at 500Gb per second and removes the need to synchronise them
    periodically. Intuitively, we can expect these performance improvements to carry
    over to other types of machine learning models that require similar patterns of
    mathematical operations, and particularly to those that can be completely formulated
    in terms of matrix manipulations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对深度学习的单一关注使TPU成为使用此类模型的最佳硬件，性能提升在每瓦性能方面达到20-30倍（Jouppi等人[2018](#ref-patterson)）。特别是，（Jouppi等人[2020](#ref-patterson2)）报告称，TPU在推理方面的每瓦性能比GPU快50倍，在训练方面快5到10倍。这些改进主要是由TPU核心比GPU或CPU核心小得多（面积小38倍）的事实驱动的，因此它们消耗的能量（少13倍）更少，并将更多的晶体管份额留给了矩阵乘法单元。另一个重要因素是内存布局，它允许TPU核心之间以每秒500Gb的速度进行无死锁通信，并消除了定期同步它们的需要。直观地，我们可以预期这些性能改进将转移到其他类型的机器学习模型上，这些模型需要类似的数学运算模式，尤其是那些可以完全用矩阵操作来表述的模型。
- en: The price we pay for this level of performance is the complete lack of flexibility
    and versatility of TPUs, which are really good only at multiplying matrices. TPU
    cores cannot perform any instruction scheduling, do not support multithreading,
    and in general have none of the sophisticated features we can find in CPUs and
    GPUs. They are completely driven by the CPU of the compute system they are attached
    to. To make up for that, code can be compiled with Google’s XLA compiler (Tensorflow
    [2021](#ref-xla)) to require no dynamic scheduling and to maximise data- and instruction-level
    parallelism, combining operations to use SIMD/FMA instructions and to ensure that
    the matrix-multiplication unit is always busy. XLA has complete visibility into
    the structure of Tensorflow and PyTorch models and can optimise across operations
    much better than a traditional compiler or the CPU and GPU schedulers. It is effective
    to the point that it can achieve sustained 70% peak performance (Jouppi et al.
    [2020](#ref-patterson2)). TPUs are also heavily optimised for a single type of
    variable, Google’s “brain” floating point format (“bfloat”), and are slower for
    variables in the industry-standard IEEE 754 floating point format (Overton [2001](#ref-overton)).
    (More in Section [3.1.2](types-structures.html#floating-point).) Furthermore,
    they are designed specifically for 16-bit (“bfloat16”) floating point operations
    over 32-bit operations. Both formats are empirically good enough for working deep
    neural networks and are eight times more efficient to operate on than IEEE formats
    (Jouppi et al. [2020](#ref-patterson2)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为此级别的性能付出的代价是TPUs（张量处理单元）完全缺乏灵活性和多功能性，它们实际上只擅长矩阵乘法。TPU核心无法执行任何指令调度，不支持多线程，并且总体上没有我们在CPU和GPU中可以找到的任何复杂功能。它们完全由它们所连接的计算系统的CPU驱动。为了弥补这一点，可以使用Google的XLA编译器（Tensorflow
    [2021](#ref-xla)）来编译代码，以实现无需动态调度并最大化数据级和指令级并行性，将操作组合起来使用SIMD/FMA指令，并确保矩阵乘法单元始终处于忙碌状态。XLA对Tensorflow和PyTorch模型的结构具有完全的可见性，并且比传统的编译器或CPU和GPU调度器更好地跨操作进行优化。它有效到可以实现持续70%的峰值性能（Jouppi等人[2020](#ref-patterson2)）。TPUs还针对Google的“大脑”浮点格式（“bfloat”）进行了大量优化，对于工业标准IEEE
    754浮点格式的变量来说较慢（Overton [2001](#ref-overton)）。（更多内容请参阅第[3.1.2](types-structures.html#floating-point)节。）此外，它们专门设计用于16位（“bfloat16”）浮点运算，而不是32位运算。这两种格式在处理深度神经网络方面经验上足够好，并且比IEEE格式操作效率高八倍（Jouppi等人[2020](#ref-patterson2)）。
- en: 2.1.2 Memory
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 内存
- en: 'The practical performance of CPUs, GPUs and TPUs is limited by the fact that
    we need to provide them data to operate on. Each processor can only access the
    memory it is directly attached to: the registers and the internal cache for CPUs,
    the on-board global and local memory for GPUs and TPUs (see Figure [2.1](hardware.html#fig:schematic)).
    This translates to delivering input data from system RAM to their dedicated memory
    and copying the outputs they produce back to system RAM for further processing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CPU、GPU和TPU的实际性能受限于我们需要提供它们操作的数据。每个处理器只能访问其直接连接的内存：CPU的寄存器和内部缓存，GPU和TPU的板载全局和局部内存（见图[2.1](hardware.html#fig:schematic)）。这意味着将输入数据从系统RAM传输到它们的专用内存，并将它们产生的输出复制回系统RAM以进行进一步处理。
- en: 'Moving data between different memories costs time, as does moving them within
    each type of memory, although less so. Ideally, we want to have as much data as
    close as possible to the processor that will work on it. Furthermore, we want
    that processor to keep working on that data in place for as long as possible to
    amortise the cost of moving data over a large number of operations. This aspiration
    is limited by three factors:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同内存之间移动数据需要时间，在每种类型的内存中移动数据也是如此，尽管较少。理想情况下，我们希望尽可能多的数据尽可能靠近将要处理它的处理器。此外，我们希望该处理器尽可能长时间地处理这些数据，以分摊在大量操作中移动数据成本。这种愿望受到三个因素的影响：
- en: the *amount* of memory available to and directly accessible by each type of
    processor;
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每种处理器可用的和直接可访问的内存**数量**；
- en: the *latency* of accessing different types of memory;
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问不同类型内存的**延迟**；
- en: and the *bandwidth* of our connections to different types of memory, which determines
    how quickly data can be transferred after it is accessed.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以及我们连接到不同类型内存的**带宽**，这决定了数据在访问后可以多快地传输。
- en: In other words, latency is the time spent waiting for the memory copy to start,
    and the bandwidth is the maximum amount of memory we can copy per second.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，延迟是等待内存复制开始所花费的时间，而带宽是我们每秒可以复制的最大内存量。
- en: '![A schematic view of the different types of memory, their size and their latency
    (the time it takes for the CPU to access them). Times are expressed as nanoseconds
    (1ns = $10^{-9}$s) or microseconds (1μs = $10^{-6}$s).](../Images/57ea91307361ffaf0282f31cde38e80e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型内存的示意图，它们的大小和它们的延迟（CPU访问它们所需的时间）。时间以纳秒（1ns = $10^{-9}$s）或微秒（1μs = $10^{-6}$s）表示。](../Images/57ea91307361ffaf0282f31cde38e80e.png)'
- en: 'Figure 2.2: A schematic view of the different types of memory, their size and
    their latency (the time it takes for the CPU to access them). Times are expressed
    as nanoseconds (1ns = \(10^{-9}\)s) or microseconds (1μs = \(10^{-6}\)s).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：不同类型内存的示意图，它们的大小和它们的延迟（CPU访问它们所需的时间）。时间以纳秒（1ns = \(10^{-9}\)s）或微秒（1μs =
    \(10^{-6}\)s）表示。
- en: 'Clearly, each type of processor will be fastest in accessing its dedicated
    memory because it is physically located next to or inside it. Distance plays a
    key role in determining the latency of memory accesses: the propagation speed
    of electrical signals limits how quickly we can reach for that memory. The need
    to process these signals along the way, for instance, to translate the addresses
    of memory locations in different formats, may further delay accesses as well.
    CPU registers, the local memory of GPU units and the local memory of TPUs are
    directly attached to the respective processors to minimise distance and to make
    them directly addressable. This reduces latency from microseconds (\(10^{-6}\)
    seconds) or tens of microseconds (\(10^{-5}\) seconds) to a few hundred or tens
    of microseconds (\(10^{-9}\) to \(10^{-7}\) seconds). To quote RFC 1925 once more:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，每种处理器在访问其专用内存时都会最快，因为它们在物理上位于其旁边或内部。距离在确定内存访问延迟方面起着关键作用：电信号的传播速度限制了我们可以多快地访问该内存。例如，在途中处理这些信号的需求，例如，将不同格式的内存位置地址进行转换，可能会进一步延迟访问。CPU寄存器、GPU单元的本地内存和TPU的本地内存直接连接到相应的处理器，以最小化距离并使它们可以直接寻址。这将延迟从微秒（\(10^{-6}\)秒）或数十微秒（\(10^{-5}\)秒）减少到数百或数十微秒（\(10^{-9}\)到\(10^{-7}\)秒）。再次引用RFC
    1925：
- en: “(2) No matter how hard you push and no matter what the priority, you can’t
    increase the speed of light.”
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “(2) 无论你多么努力，无论优先级如何，你都无法提高光速。”
- en: 'The latency of accessing particular types of memory is usually inversely proportional
    to their size and is bound below by the frequency of the processor accessing it.
    We illustrate this point in Figure [2.2](hardware.html#fig:memory-hierarchy),
    focusing on the CPU, but our considerations hold for GPUs and TPUs as well. CPU
    registers and the various CPU caches are the smallest because their size is limited
    by the physical size of the CPU. For instance, the three levels of cache (L1,
    L2, L3) on a Sandy Bridge Intel CPU are 32kB (L1), 256kB (L2) and 20MB (L3) in
    size and can be accessed in 4, 12 and 29 cycles respectively (Williams-Young and
    Li [2019](#ref-young)). In contrast, registers can only store a few hundreds of
    bytes, but it only takes a single cycle to access them. For practical purposes,
    we can take a CPU cycle to be the reciprocal of the CPU’s clock frequency. Say
    that we have a 2GHz CPU: \[\begin{equation*} \text{$1$ cycle} \operatorname{/}
    (2 \times 10^9 \mbox{Hz}) = 5 \times 10^{-10}\mathrm{s} = 0.5\mathrm{ns}. \end{equation*}\]
    With this equation we can derive the latencies shown in Figure [2.2](hardware.html#fig:memory-hierarchy):
    accessing registers takes 0.5ns and accessing the CPU cache takes between 2ns
    and 14.5ns. It is easy to see that performance degrades quickly if the CPU is
    forced to wait for several nanoseconds to fetch the data from the CPU cache for
    every 0.5ns it spends doing computations. The degradation may be less noticeable
    for instructions that take longer than 1 cycle to complete, such as division,
    trigonometric and transcendental functions, simply because the time spent on the
    computations is larger compared to that spent waiting.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 访问特定类型内存的延迟通常与它们的尺寸成反比，并且受限于访问它的处理器的频率。我们在图[2.2](hardware.html#fig:memory-hierarchy)中说明了这一点，重点关注CPU，但我们的考虑也适用于GPU和TPU。CPU寄存器和各种CPU缓存是最小的，因为它们的大小受CPU的物理尺寸限制。例如，Sandy
    Bridge Intel CPU上的三级缓存（L1、L2、L3）的大小分别为32kB（L1）、256kB（L2）和20MB（L3），分别可以在4、12和29个周期内访问（Williams-Young和Li
    [2019](#ref-young)）。相比之下，寄存器只能存储几百字节，但访问它们只需要一个周期。为了实际目的，我们可以将CPU周期视为CPU时钟频率的倒数。假设我们有一个2GHz的CPU：\[\begin{equation*}
    \text{$1$ cycle} \operatorname{/} (2 \times 10^9 \mbox{Hz}) = 5 \times 10^{-10}\mathrm{s}
    = 0.5\mathrm{ns}. \end{equation*}\]使用这个公式，我们可以推导出图[2.2](hardware.html#fig:memory-hierarchy)中显示的延迟：访问寄存器需要0.5ns，访问CPU缓存需要2ns到14.5ns。很容易看出，如果CPU被迫在每0.5ns的计算过程中等待几个纳秒来从CPU缓存中获取数据，性能会迅速下降。对于需要超过1个周期才能完成的指令（如除法、三角函数和超越函数），这种下降可能不太明显，因为计算所花费的时间与等待时间相比更大。
- en: 'Next in the memory hierarchy are different sets of RAM: the system RAM accessible
    from CPUs, the video RAM on the GPU boards and that in TPU boards. As we can see
    from Figure [2.2](hardware.html#fig:memory-hierarchy), the latency of accessing
    RAM can be in the hundreds of nanoseconds, making it slower than CPU caches by
    a factor of at least 10\. The GPU and TPU RAM, called “global memory” in Figure
    [2.1](hardware.html#fig:schematic), can be even slower because GPUs and TPUs are
    connected to the CPU through a PCI Express bus (PCIe),[⁶](#fn6) which adds to
    the latency. However, RAM is much larger than CPU caches, ranging from a few gigabytes
    (GPU and TPU RAM) to a few terabytes (for system RAM).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存层次结构中接下来是不同的RAM集合：从CPU可访问的系统RAM，GPU板上的视频RAM以及TPU板上的RAM。如图[2.2](hardware.html#fig:memory-hierarchy)所示，访问RAM的延迟可以达到数百纳秒，这使得它比CPU缓存慢至少10倍。图[2.1](hardware.html#fig:schematic)中称为“全局内存”的GPU和TPU
    RAM可能甚至更慢，因为GPU和TPU通过PCI Express总线（PCIe）连接到CPU，[⁶](#fn6)这增加了延迟。然而，RAM比CPU缓存大得多，从几个GB（GPU和TPU
    RAM）到几个TB（系统RAM）不等。
- en: The latency of RAM is such that we want to read data from it as few times as
    possible, and to read as much data as possible each time. For example, consider
    accessing 10MB of data in RAM to apply a set of 10 FMA instructions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: RAM的延迟如此之高，以至于我们希望尽可能少地从中读取数据，并且每次读取尽可能多的数据。例如，考虑在RAM中访问10MB的数据以应用一组10个FMA指令。
- en: If we transfer the data to the CPU as a single batch, we have to wait 60–100ns
    in order to access it, and then 5ns performing computations.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将数据作为一个单独的批次传输到CPU，我们必须等待60–100ns才能访问它，然后花费5ns进行计算。
- en: If we transfer data in 200 50kB batches, we have to wait 12000–10000ns (12-20μs)
    to spend the same 5ns on computations.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们以200个50kB的批次传输数据，我们必须等待12000–10000ns（12-20μs）来花费同样的5ns进行计算。
- en: 'The transfer itself takes the same time since it only depends on the bandwidth
    of the PCIe connection between the CPU and the RAM: 10MB take 216μs at 64GB/s.
    However, in the first case the latency introduced by the memory transfer is negligible,
    while in the second case it increases the overall time by about 20%. This is why
    both GPUs and TPUs are initialised by copying all the data from system RAM in
    a single batch, making the memory transfer (often called “kernel launch”) a fixed
    overhead cost that will be amortised over the whole computation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 传输本身花费相同的时间，因为它只取决于 CPU 和 RAM 之间 PCIe 连接的带宽：在 64GB/s 的速度下，10MB 需要 216μs。然而，在第一种情况下，内存传输引入的延迟可以忽略不计，而在第二种情况下，它将总时间增加了大约
    20%。这就是为什么 GPU 和 TPU 都是通过一次性从系统 RAM 复制所有数据来初始化的，这使得内存传输（通常称为“内核启动”）成为固定开销成本，该成本将在整个计算过程中分摊。
- en: 'At the bottom of the memory hierarchy we have hot and cold storage. Hot storage
    is meant to contain data that we need to access often and right away, and will
    comprise hard drives (mostly solid-state drives) that are locally attached to
    the compute system. Cold storage is for data that we access less frequently and
    that do not require fast access times. It comprises a combination of tape, slower
    hard drives and network-attached storage. Hot storage usually has a size of several
    tens of terabytes, with less redundancy; cold storage can potentially scale to
    the petabytes, and often has more redundancy because it contains data that should
    be preserved in the long term. Hot storage is local, so it is limited by the latency
    and the bandwidth of PCIe; cold storage may be remote, so it is limited by network
    latencies and bandwidth. The storage medium is rarely a limiting factor: it almost
    always has more bandwidth than the connection we use to access it, which becomes
    the bottleneck.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存层次结构的底部，我们有热存储和冷存储。热存储旨在包含我们经常需要立即访问的数据，它将包括连接到计算系统的本地硬盘（主要是固态硬盘）。冷存储用于我们访问频率较低且不需要快速访问时间的数据。它包括磁带、较慢的硬盘和网络附加存储。热存储通常有数十个
    terabytes 的大小，冗余度较低；冷存储可能扩展到 petabytes，并且通常具有更高的冗余度，因为它包含应该长期保存的数据。热存储是本地的，因此受
    PCIe 延迟和带宽的限制；冷存储可能是远程的，因此受网络延迟和带宽的限制。存储介质很少成为限制因素：它几乎总是比我们用来访问它的连接具有更多的带宽，这成为瓶颈。
- en: 2.1.3 Connections
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 连接
- en: The last, crucial part of a compute system is the connections that allow the
    data to move between different processors and types of memory. The performance
    of memory is necessarily limited by how it is connected to various processors.
    Memory directly connected to a particular processor (CPU caches and registers,
    the memory built in GPU and TPU boards) is always the fastest to access for that
    particular processor because it works at its full native speed. This means that
    latency is minimised and that bandwidth is maximised. For example, TPU memory
    has a throughput of 500Gb/s (Jouppi et al. [2020](#ref-patterson2)) and GPU memory
    has a throughput of 500-1000Gb/s (Nvidia Quadro cards (Mujtaba [2018](#ref-gddr6))).
    The latency is negligible for both.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 计算系统最后、最重要的部分是允许数据在不同处理器和内存类型之间移动的连接。内存的性能必然受到其连接到各种处理器的方式的限制。直接连接到特定处理器的内存（CPU
    缓存和寄存器、集成在 GPU 和 TPU 板上的内存）总是对该处理器访问速度最快的，因为它以全速运行。这意味着延迟最小化，带宽最大化。例如，TPU 内存的数据吞吐量为
    500Gb/s（Jouppi 等人 [2020](#ref-patterson2)），而 GPU 内存的数据吞吐量为 500-1000Gb/s（Nvidia
    Quadro 显卡（Mujtaba [2018](#ref-gddr6)））。两者的延迟都可以忽略不计。
- en: However, GPUs cannot access the system RAM directly; nor can CPUs access the
    memory on the GPU boards. This means that any data that is transferred to a GPU
    for processing must be copied from the system RAM to the on-board memory. Speed
    is then limited by the bandwidth of the PCIe bus that connects the GPU to the
    system and latency increases to the levels shown in Figure [2.2](hardware.html#fig:memory-hierarchy).
    The same is true for TPUs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPU 不能直接访问系统 RAM；CPU 也不能访问 GPU 板上的内存。这意味着任何要传输到 GPU 进行处理的数据都必须从系统 RAM 复制到板载内存。速度因此受连接
    GPU 和系统 PCIe 总线的带宽限制，延迟增加到图 [2.2](hardware.html#fig:memory-hierarchy) 中所示的水平。对于
    TPU 也是如此。
- en: 'This is the reason why *data locality*, keeping the data “close” to the processor
    that will work on it, matters: direct connections have the best possible latency
    and bandwidth, while indirect ones are limited by the PCIe bus. Furthermore, transferring
    data between different types of memory typically involves copying it to system
    RAM as an intermediate step, which degrades performance even further.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么**数据局部性**，保持数据“靠近”将要处理它的处理器，很重要的原因：直接连接具有最佳可能的延迟和带宽，而间接连接则受PCIe总线限制。此外，在不同类型的内存之间传输数据通常涉及将其复制到系统RAM作为中间步骤，这进一步降低了性能。
- en: 'Hot and cold storage are different from other types of memory in several respects.
    Firstly, they do not have any compute capabilities and therefore we cannot avoid
    transferring the data they contain to system RAM to work on it. Secondly, neither
    type of storage will necessarily saturate its connection to the system RAM: the
    connection does not introduce any bottleneck in itself. Hot storage is typically
    connected to the compute system via PCIe, but its sustained read-write speed (8GB/s
    for SATA 3 to 4GB/s for NVMe) is comfortably smaller than PCIe. Cold storage is
    even slower, or is only available through a network connection such as 100Gb/s
    Ethernet.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 热存储和冷存储在几个方面与其他类型的内存不同。首先，它们没有任何计算能力，因此我们无法避免将包含的数据传输到系统RAM以进行处理。其次，任何一种存储类型都不一定会饱和其与系统RAM的连接：连接本身不会引入任何瓶颈。热存储通常通过PCIe连接到计算系统，但其持续读写速度（SATA
    3为8GB/s，NVMe为4GB/s）舒适地小于PCIe。冷存储甚至更慢，或者只能通过网络连接（如100Gb/s以太网）获得。
- en: 2.2 Making Hardware Live Up to Expectations
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 让硬件满足期望
- en: All these hardware types have powerful capabilities, each in their own way,
    but in order to use them effectively we need either compilers that can leverage
    them (if we can compile software from source) or libraries that have been built
    to do so (if we cannot). This means using compilers that understand the memory
    layout of the system and what specialised hardware instructions are available,
    or software built on optimised libraries like CUDA (Nvidia [2021](#ref-cuda))
    (for NVidia GPUs) or Intel’s Math Kernel Library (MKL) (Intel [2021](#ref-mkl))
    (for CPUs). Some popular machine learning frameworks and libraries such as PyTorch
    (Paszke et al. [2019](#ref-pytorch)) go even further and abstract all hardware-specific
    optimisations away, adapting automatically to the characteristics of the hardware
    they run on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些硬件类型都有强大的能力，每种都有其独特之处，但为了有效地使用它们，我们需要能够利用它们的编译器（如果我们能从源代码编译软件）或者已经构建来这样做库（如果我们不能）。这意味着使用理解系统内存布局和可用专用硬件指令的编译器，或者基于优化库（如CUDA（Nvidia
    [2021](#ref-cuda)））（用于NVIDIA GPU）或Intel的数学内核库（MKL）（Intel [2021](#ref-mkl)）（用于CPU）构建的软件。一些流行的机器学习框架和库，如PyTorch（Paszke等人
    [2019](#ref-pytorch)），甚至更进一步，抽象出所有硬件特定的优化，自动适应它们运行的硬件特性。
- en: 'The key to getting the best possible performance out of modern compute systems
    is to recognise that they have many processors (specialised or otherwise) and
    that we want to keep all those processors busy as much as possible. In other words,
    we need *parallelism*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从现代计算系统中获得最佳性能的关键是认识到它们有许多处理器（专用或其他），并且我们希望尽可能多地保持所有这些处理器忙碌。换句话说，我们需要**并行性**：
- en: At the *instruction level*, we want software to use hardware instructions that
    can be executed simultaneously.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**指令级别**上，我们希望软件使用可以同时执行的硬件指令。
- en: At the *data level*, we want tasks with modular inputs and outputs so that we
    can operate on each of their elements independently, without having to wait for
    other operations to complete.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**数据级别**上，我们希望任务具有模块化的输入和输出，这样我们就可以独立地对它们的每个元素进行操作，而无需等待其他操作完成。
- en: At the *thread level*, we want different parts of our machine learning software
    to depend on each other’s outputs as little as possible so that we can run them
    in separate threads or processes across different cores, processors or even systems.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**线程级别**上，我们希望我们的机器学习软件的不同部分尽可能少地相互依赖，这样我们就可以在不同的核心、处理器甚至系统上分别运行它们。
- en: 'To what extent thread-level parallelism is possible depends on what algorithms
    we are using and on how they are implemented (see Section [4.6](algorithms.html#bigO-performance)).
    The same is true for data-level parallelism: whether data points and random variables
    can be considered to be independent, whether parameters can be estimated independently,
    and whether predictions can be computed independently depends on what machine
    learning model we are using and on how we learned it. Instruction-level parallelism,
    on the other hand, depends crucially on the software using the appropriate hardware
    instructions (SIMD and FMA in CPUs and GPUs, matrix-multiplication units in TPUs).
    This is true for data-level parallelism as well because being able to operate
    on multiple data points simultaneously is useless if the software does not tell
    various processors to do that.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线程级并行化的程度取决于我们使用什么算法以及它们的实现方式（参见第[4.6](algorithms.html#bigO-performance)节）。数据级并行化也是如此：数据点和随机变量是否可以被认为是独立的，参数是否可以独立估计，以及预测是否可以独立计算，这取决于我们使用的机器学习模型以及我们如何学习它。另一方面，指令级并行化则关键取决于使用适当硬件指令的软件（CPU和GPU中的SIMD和FMA，TPU中的矩阵乘法单元）。这对于数据级并行化也是正确的，因为如果软件没有告诉各个处理器同时操作多个数据点，那么能够同时操作多个数据点是无用的。
- en: 'Taking advantage of parallelism requires us to feed data to all the processors
    involved in the computations so that they have something to operate on. How we
    do that determines the *operational intensity* of the software: the number of
    operations per byte of RAM accessed during execution. Data locality is then key
    to improving that: loading data has a much higher latency than operating on data
    that are already in the local memory of the processor, so the processor will end
    up sitting idle while waiting for the data to arrive. This is bound to happen
    every time we load data from a different level in the hierarchy in Figure [2.2](hardware.html#fig:memory-hierarchy),
    as we discussed in Section [2.1.3](hardware.html#hardware-connections). It is
    also bound to happen, to some extent, as we get close to running processors at
    full capacity. For instance, the CPU will often sit idle while waiting to receive
    results from GPUs and TPUs. And the closer we get to full occupancy, the less
    room we have for optimising load across and within processors. By the law of diminishing
    returns, we eventually end up decreasing their overall performance as the gains
    from increasing occupancy are outweighed by the overhead of managing different
    threads and processes contending for resources.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 利用并行化需要我们向所有参与计算的处理器提供数据，以便它们有东西可以操作。我们如何做到这一点决定了软件的*操作强度*：在执行过程中每访问一个RAM字节数的操作数。数据局部性然后是提高这一点的关键：加载数据比在处理器本地内存中操作数据具有更高的延迟，因此处理器最终会空闲等待数据到达。正如我们在第[2.1.3](hardware.html#hardware-connections)节中讨论的那样，每次我们从图[2.2](hardware.html#fig:memory-hierarchy)中的不同级别加载数据时，这种情况都会发生。在某种程度上，当我们接近以全容量运行处理器时，这种情况也会发生。例如，CPU经常会空闲等待从GPU和TPU接收结果。而且，我们越接近全占用，我们在处理器之间和内部优化加载的空间就越少。根据边际效用递减定律，我们最终会降低它们的整体性能，因为增加占用的收益被管理不同线程和进程争夺资源的开销所抵消。
- en: 'In other words, increasing operational intensity means reducing the number
    of memory accesses. Performing data transformations in place is a way to do that:
    it reduces the number and the volume of high-latency data transfers to and from
    RAM while maximising the usage of faster local memory. In doing so, we prevent
    the processors from stalling while waiting for data (they are “starving”) and
    we allow them to operate continuously (we “keep them fed” with data). The price
    is that memory use is likely to increase because we need to rearrange the data
    in memory and possibly to keep multiple copies around. Depending on the algorithm,
    it is sometimes possible to get most of the intensity without sacrificing space
    complexity, as in (Fortin et al. [2021](#ref-monagan)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，增加操作强度意味着减少内存访问次数。在原地执行数据转换是实现这一目标的方法：它减少了从RAM到和从RAM到的高延迟数据传输的数量和体积，同时最大化了更快本地内存的使用。这样做，我们防止了处理器在等待数据时停滞（它们处于“饥饿”状态），并允许它们持续运行（我们“用数据喂饱”它们）。代价是内存使用可能会增加，因为我们可能需要在内存中重新排列数据，并且可能需要保留多个副本。根据算法的不同，有时在不牺牲空间复杂度的情况下，可以获得大部分强度，就像（Fortin等人[2021](#ref-monagan)）中那样。
- en: 'When we are eventually forced to read from RAM, large RAM reads are better
    than many small RAM reads: we should lay out data continuously in RAM to allow
    for that. If we do not do that, most algorithms will become memory-bound. (More
    on that in Chapter [3](types-structures.html#types-structures).) Limiting memory
    usage in the first place will also help in this respect. Hence the interest in
    numeric formats with smaller precisions such as 16-bit floating point numbers
    and integers (Jouppi et al. [2018](#ref-patterson), [2020](#ref-patterson2));
    and in reducing the number of parameters of machine learning models by compressing
    them or by making the models sparser (Hazelwood et al. [2018](#ref-facebook-infra)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们最终被迫从 RAM 中读取时，大容量 RAM 读取比多次小容量 RAM 读取要好：我们应该在 RAM 中连续布局数据以允许这样做。如果我们不这样做，大多数算法将变得内存受限。（关于这一点，请参阅第
    [3](types-structures.html#types-structures) 章。）首先限制内存使用也将有助于这一点。因此，对具有较小精度的数值格式（如
    16 位浮点数和整数）以及通过压缩或使模型更稀疏来减少机器学习模型参数数量的兴趣增加（Jouppi 等人 [2018](#ref-patterson)，[2020](#ref-patterson2)）；以及通过压缩或使模型更稀疏来减少机器学习模型参数数量的兴趣增加（Hazelwood
    等人 [2018](#ref-facebook-infra)）。
- en: 2.3 Local and Remote Hardware
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 本地和远程硬件
- en: 'The discussion of the key aspects of compute systems in Sections [2.1](hardware.html#hardware-types)
    and [2.2](hardware.html#hardware-using) implicitly assumes that all hardware is
    part of a single system. That is unlikely to be the case: machine learning systems
    typically comprise different systems with specific purposes because different
    tasks run best on different hardware, and it is expensive to maximise memory,
    storage and compute in a single system all at the same time. Having different
    systems makes it possible to specialise them and to make them perform better while
    reducing costs. We can think of them as *remote storage* and *remote compute*,
    as they are labelled in Figure [2.1](hardware.html#fig:schematic), connected by
    either a local or a geographical network.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [2.1](hardware.html#hardware-types) 和 [2.2](hardware.html#hardware-using)
    节中讨论计算系统的关键方面时，隐含地假设所有硬件都是单个系统的一部分。这种情况不太可能发生：机器学习系统通常由具有特定目的的不同系统组成，因为不同的任务在不同的硬件上运行效果最佳，同时在单个系统中最大化内存、存储和计算成本是昂贵的。拥有不同的系统使得它们可以专业化，并提高性能同时降低成本。我们可以将它们视为
    *远程存储* 和 *远程计算*，正如它们在图 [2.1](hardware.html#fig:schematic) 中的标签所示，通过本地或地理网络连接。
- en: 'Remote systems that are in the same local network are typically connected by
    a high-speed Ethernet connection. 50Gb Ethernet is good enough even at the scale
    of Facebook operations (Hazelwood et al. [2018](#ref-facebook-infra)), so throughput
    is not a limiting factor for smaller machine learning systems. Latencies are more
    of a problem: the networking equipment that routes the traffic in the network
    is likely to introduce several microseconds of delay in establishing a new connection.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同一本地网络中的远程系统通常通过高速以太网连接连接。即使在 Facebook 运营的规模上，50Gb 以太网也足够好（Hazelwood 等人 [2018](#ref-facebook-infra)），因此吞吐量不是较小机器学习系统的限制因素。延迟则是更大的问题：在网络中路由流量的网络设备可能会在建立新连接时引入几个微秒的延迟。
- en: 'For remote systems that are in a different geographical location, both latency
    and bandwidth are limiting factors. A prime example is *cloud instances*, virtual
    servers that we can quickly create (“provision”) or destroy (“decommission”) and
    that run on hardware that we own (a private cloud) or that we rent from a public
    cloud provider such as Amazon Web Services (AWS), Microsoft Azure or Google Cloud
    Computing Services (GCP). Latency arises from the physical time it takes for signals
    to go through several layers of networking equipment to reach a system that is
    possibly in a different country. For instance, if we are located on the west coast
    of the United States the latency of a connection to the east coast is 40ms, to
    the United Kingdom is 81ms, and to Australia is 183ms (Gregg [2021](#ref-gregg)).
    If the remote system is activated on demand, we must also wait for it to boot
    before we can start processing any data: this can take between 1s and 40s depending
    on the type of virtualisation underlying the instances (Sections [7.1.3](deploying-code.html#vm-packaging)
    and [7.1.4](deploying-code.html#container-packaging)). Compared to the latencies
    in Figure [2.2](hardware.html#fig:memory-hierarchy), accessing data on a remote
    system is therefore 3-6 orders of magnitude slower than the storage of a local
    system. This is the case, for instance, for AWS spot instances: while they are
    cheaper to run, they must be booted up every time they are used and they may be
    shut down without warning.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于位于不同地理位置的远程系统，延迟和带宽都是限制因素。一个典型的例子是*云实例*，这些虚拟服务器我们可以快速创建（“提供”）或销毁（“退役”），并且运行在我们拥有的硬件上（私有云）或我们从公共云提供商如亚马逊网络服务（AWS）、微软Azure或谷歌云计算服务（GCP）租用的硬件上。延迟源于信号穿越多层网络设备所需的时间，以到达可能位于不同国家的系统。例如，如果我们位于美国西海岸，连接到东海岸的延迟是40ms，到英国的延迟是81ms，到澳大利亚的延迟是183ms（Gregg
    [2021](#ref-gregg)）。如果远程系统是按需激活的，我们还需要等待它启动，我们才能开始处理任何数据：这需要1秒到40秒不等，具体取决于实例底层虚拟化的类型（第7.1.3节[deploying-code.html#vm-packaging]和第7.1.4节[deploying-code.html#container-packaging]）。因此，与图2.2[hardware.html#fig:memory-hierarchy]中的延迟相比，访问远程系统上的数据比本地系统的存储慢3-6个数量级。例如，对于AWS
    spot实例：虽然它们运行成本较低，但每次使用时都必须启动，并且可能会在没有警告的情况下关闭。
- en: 'On the one hand, we want to preserve locality as much as possible: colocating
    the data and all the compute systems that will work on it to avoid large, repeated
    data transfers across different locations. Designing the topology of the local
    network connecting different systems can minimise the impact of data transfers
    within the network and can make it feasible to spread the load of training complex
    models across different systems (Hazelwood et al. [2018](#ref-facebook-infra)).
    This approach is known as *distributed* or *federated learning* (Li et al. [2021](#ref-federated)):
    as an example, see the research done at DeepMind for distributed deep reinforcement
    learning (Espeholt et al. [2018](#ref-impala)) and Google’s systems architecture
    for federated learning from mobile devices (Bonawitz et al. [2019](#ref-bonawitz)).
    The latter is an instance of *edge computing* (Khan et al. [2019](#ref-edge)),
    which addresses data privacy, security, and latency constraints by pushing data
    processing to low-power devices closer to where the data originates (Section [5.3.1](design-code.html#scoping-pipeline)).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们希望尽可能保持局部性：将数据和所有将处理它的计算系统放置在同一位置，以避免在不同地点之间进行大量重复的数据传输。设计连接不同系统的本地网络拓扑可以最小化网络内部数据传输的影响，并使将训练复杂模型的负载分散到不同系统成为可能（Hazelwood等人[2018](#ref-facebook-infra)）。这种方法被称为*分布式*或*联邦学习*（Li等人[2021](#ref-federated)）：例如，参见DeepMind进行的分布式深度强化学习研究（Espeholt等人[2018](#ref-impala)）和谷歌为从移动设备进行联邦学习设计的系统架构（Bonawitz等人[2019](#ref-bonawitz)）。后者是*边缘计算*（Khan等人[2019](#ref-edge)）的一个实例，通过将数据处理推向靠近数据源的低功耗设备来解决数据隐私、安全和延迟限制（第5.3.1节[design-code.html#scoping-pipeline]）。
- en: 'On the other hand, it is desirable to keep geographical spread for the purpose
    of disaster recovery. Keeping multiple copies of the data and of the models in
    different locations makes it unlikely that a hardware failure will result in the
    loss of crucial resources: a time-honoured strategy to achieve that is the “3-2-1
    backup rule” (3 copies of your data, your production data and 2 backup copies,
    on 2 different types of storage with 1 copy off-site). It also helps with locality,
    but requires some care in synchronising the data and the models at different locations
    to ensure that the correct version is used everywhere.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，出于灾难恢复的目的，保持地理分布是可取的。在不同位置保留数据和模型的多份副本，使得硬件故障不太可能导致关键资源的丢失：实现这一目标的一个传统策略是“3-2-1备份规则”（3份数据副本，2份生产数据备份，2种不同存储类型的备份，以及1份异地备份）。这也有助于本地化，但需要小心同步不同位置的数据和模型，以确保正确版本在所有地方都得到使用。
- en: 2.4 Choosing the Right Hardware for the Job
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 为任务选择合适的硬件
- en: 'No single compute system(s) configuration is best overall: practical performance
    is the result of complex interactions between the type(s) of algorithms, the models
    and the type(s) of hardware. Engineering the best possible performance should
    then begin by defining what the objectives of the machine learning system are
    (Section [5.3.1](design-code.html#scoping-pipeline)) and then choosing the software
    and the hardware required to achieve them. A comprehensive discussion on this
    topic can be found in (Gregg [2021](#ref-gregg)), which explores all the different
    aspects of hardware, operating systems, protocols, benchmarking and profiling.
    In what follows, we will focus on the interactions between the machine learning
    models, the software stack that underlies them and the hardware. Numeric libraries
    such as BLAS (Blackford et al. [2002](#ref-blas)), LAPACK (Anderson et al. [1999](#ref-lapack))
    and GSL (Galassi et al. [2021](#ref-gsl)), frameworks like TensorFlow (TensorFlow
    [2021](#ref-tensorflow)[a](#ref-tensorflow)) and PyTorch, and low-level libraries
    such as XLA and CUDA essentially act as compilers for the models and translate
    them into the best available set of hardware instructions for the most suitable
    processor(s).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何单一的计算机系统配置是整体上最佳的：实际性能是算法类型、模型和硬件类型之间复杂交互的结果。因此，要实现最佳性能，首先应该定义机器学习系统的目标（见第[5.3.1节](design-code.html#scoping-pipeline)），然后选择实现这些目标的软件和硬件。关于这个主题的全面讨论可以在(Gregg
    [2021](#ref-gregg))中找到，它探讨了硬件、操作系统、协议、基准测试和配置文件的所有不同方面。在接下来的内容中，我们将关注机器学习模型、支撑它们的软件栈以及硬件之间的交互。数值库，如BLAS
    (Blackford et al. [2002](#ref-blas))、LAPACK (Anderson et al. [1999](#ref-lapack))和GSL
    (Galassi et al. [2021](#ref-gsl))，框架如TensorFlow (TensorFlow [2021](#ref-tensorflow)[a](#ref-tensorflow))和PyTorch，以及底层库如XLA和CUDA，本质上充当模型的编译器，并将它们转换为最适合处理器(们)的最佳可用硬件指令集。
- en: 'Some tasks are better suited to particular types of hardware. Consider, for
    instance, deep neural networks. There are important computational differences
    between training and inference: the latter is less parallelisable, has higher
    memory requirements, and requires wider data to keep enough precision and avoid
    catastrophic errors in the final model (Jouppi et al. [2020](#ref-patterson2)).
    Hence training is best performed on compute systems generously equipped by GPUs
    and TPUs, while inference performs well even on CPUs (Hazelwood et al. [2018](#ref-facebook-infra)).
    After all, GPUs and TPUs are not magic “go fast” devices! They only benefit certain
    classes of problems that are embarrassingly parallel and mostly consist of vector
    (GPU) or matrix (TPU) operations. How many of each should we buy? That depends
    on the relative scale and frequency with which we perform each task. Typically,
    model training is performed every few days, while inference runs in real-time,
    possibly millions of times per day. For instance, 90% of the overall compute cost
    at Amazon is inference (Amazon Web Services [2022](#ref-trainium)[c](#ref-trainium)):
    at that scale, using GPUs becomes a necessity again. But since GPUs are poorly
    suited for inference, an ad hoc software scheduler (Jain et al. [2018](#ref-gpu-scheduling))
    is required to use them efficiently and with consistent, predictable performance.
    At smaller scales, compute systems with many CPU cores will be sufficient and
    simpler to set up.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一些任务更适合特定的硬件类型。例如，考虑深度神经网络。训练和推理之间存在重要的计算差异：后者更难以并行化，对内存的需求更高，并且需要更广泛的数据来保持足够的精度，避免在最终模型中发生灾难性错误（Jouppi
    等人 [2020](#ref-patterson2)）。因此，训练最好在配备有大量 GPU 和 TPUs 的计算系统上执行，而推理即使在 CPU 上也能良好运行（Hazelwood
    等人 [2018](#ref-facebook-infra)）。毕竟，GPU 和 TPUs 并非魔法“加速”设备！它们仅对某些类型的问题有益，这些问题具有明显的并行性，并且主要由向量（GPU）或矩阵（TPU）运算组成。我们应该购买多少个呢？这取决于我们执行每个任务的相对规模和频率。通常，模型训练每隔几天进行一次，而推理是实时进行的，每天可能进行数百万次。例如，亚马逊的整体计算成本中90%是推理（亚马逊网络服务
    [2022](#ref-trainium)[c](#ref-trainium))：在那个规模下，使用 GPU 再次成为必要。但由于 GPU 不适合推理，需要一个专门的软件调度器（Jain
    等人 [2018](#ref-gpu-scheduling))来高效且一致、可预测地使用它们。在较小的规模上，具有许多 CPU 内核的计算系统将足够且易于设置。
- en: 'Specific machine learning models may be feasible to use only on some types
    of hardware. Models that make heavy use of matrix operations naturally perform
    better on GPUs and TPUs but they are limited in size by the amount of on-board
    memory that the GPUs and TPUs can access. The whole model must fit and, at the
    same time, there must be enough memory left to store the data the model will operate
    on. Furthermore, we want to operate on data in batches as large as possible to
    increase occupancy: performance may be disappointing if we are forced to process
    data in small batches because the model uses up most of the on-board memory. This
    problem is not mitigated by putting multiple GPUs or TPUs in the same compute
    system because models are not shared between them. If memory requirements are
    beyond the capabilities of GPUs and TPUs, we are limited to running models on
    CPUs and system RAM, which has a much larger capacity but is slower. CPUs, however,
    may perform better for models or tasks that are not very parallelisable because
    different cores can perform completely different operations.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的机器学习模型可能只能在某些类型的硬件上使用。那些大量使用矩阵运算的模型在 GPU 和 TPUs 上表现更好，但它们的大小受到 GPU 和 TPUs
    可以访问的板载内存量的限制。整个模型必须适应，同时，还必须有足够的内存来存储模型将要操作的数据。此外，我们希望以尽可能大的批次操作数据以增加占用率：如果被迫以小批量处理数据，因为模型占用了大部分板载内存，性能可能会令人失望。将多个
    GPU 或 TPUs 放在同一计算系统中并不能减轻这个问题，因为模型之间不会共享。如果内存需求超出了 GPU 和 TPUs 的能力，我们只能将模型运行在 CPU
    和系统 RAM 上，尽管其容量更大但速度较慢。然而，对于不太容易并行化的模型或任务，CPU 可能会表现得更好，因为不同的核心可以执行完全不同的操作。
- en: 'Finally, a note from our future selves: we should plan for more hardware than
    we strictly need right now to accommodate what are likely to be growing compute,
    memory and storage needs (*capacity planning*). Compute requirements for training
    machine learning models grew by a factor of 10 between 2012 and 2018 (Jouppi et
    al. [2020](#ref-patterson2)). In addition, automated model selection techniques
    (also known as “hyperparameter tuning” for some types of models) such as AutoML
    (He, Zhao, and Chu [2021](#ref-automl)) are becoming increasingly common and use,
    on average, 50 times more compute power than what is needed to learn the type
    of model they select. The amount of data (Katal, Wazid, and Goudar [2013](#ref-bigdata))
    and the size of models (Zellers et al. [2019](#ref-grover); Cohen, Pavlick, and
    Tellex [2019](#ref-opengpt2)) are likewise constantly growing over time, requiring
    more hot storage and more memory (of all types) to store and use them (Beam, Manrai,
    and Ghassemi [2020](#ref-repro-health)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，来自我们未来自己的一个备注：我们应该为比我们现在严格需要的更多硬件做出规划，以适应可能增长的计算、内存和存储需求（*容量规划*）。训练机器学习模型的计算需求在
    2012 年到 2018 年之间增长了 10 倍（Jouppi 等人 [2020](#ref-patterson2)）。此外，自动模型选择技术（对于某些类型的模型也称为“超参数调整”）如
    AutoML（He，Zhao 和 Chu [2021](#ref-automl)）变得越来越普遍，并且平均使用比学习它们选择的模型类型所需的计算能力多 50
    倍。数据量（Katal，Wazid 和 Goudar [2013](#ref-bigdata)）和模型大小（Zellers 等人 [2019](#ref-grover)；Cohen，Pavlick
    和 Tellex [2019](#ref-opengpt2)）随着时间的推移也在不断增长，需要更多的热存储和更多类型的内存来存储和使用它们（Beam，Manrai
    和 Ghassemi [2020](#ref-repro-health)）。
- en: 'Cloud computing may reduce the need for capacity planning: if instances are
    relatively inexpensive and if they can be quickly created and destroyed, we may
    buy less hardware up-front and scale it as needed. In fact, dynamic scaling algorithms
    can do that automatically in most cloud services. Furthermore, all major cloud
    providers offer instances with GPUs and, in the case of Google, TPUs for use in
    applications that require them. However, cloud computing is not a universal solution
    to capacity planning. Firstly, the cloud instances we rent from public cloud providers
    are billed based on how long they are in use and on how much/how quickly they
    allow us to scale in response to sudden changes in our needs. Therefore, it may
    be cheaper to buy the hardware outright if we foresee using them often enough
    or for long enough periods of time and if we have predictable workloads and network
    traffic patterns. Secondly, cloud computing can only give us *horizontal scalability*
    (increasing the number of systems we can use) and is ill-suited to achieve *vertical
    scalability* (increasing the computing power in individual systems). Horizontal
    scalability may not improve the performance of machine learning models that are
    not modular or parallelisable to at least some extent, and may not help at all
    if we need to work with large blocks of data that must be kept in memory. Thirdly,
    cloud instances are more difficult to profile and trace, making it more difficult
    to understand their behaviour (the *observability* of the system is limited) and
    to diagnose any issue they may have. (More on this in Section [5.3.6](design-code.html#monitoring-pipeline).)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算可能会减少对容量规划的需求：如果实例相对便宜，并且可以快速创建和销毁，我们可能一开始购买更少的硬件，并根据需要扩展。事实上，动态扩展算法可以在大多数云服务中自动完成这一点。此外，所有主要的云服务提供商都提供带有
    GPU 的实例，在谷歌的情况下，还提供 TPUs 以供需要这些技术的应用程序使用。然而，云计算并不是容量规划的万能解决方案。首先，我们从公共云提供商租用的云实例的计费是基于它们的使用时间和它们允许我们如何以及多快地根据我们需求的突然变化进行扩展。因此，如果我们预见经常或长时间使用它们，并且我们有可预测的工作负载和网络流量模式，那么直接购买硬件可能更便宜。其次，云计算只能提供*水平可扩展性*（增加我们可以使用的系统数量），而不适合实现*垂直可扩展性*（增加单个系统的计算能力）。对于至少在一定程度上不是模块化或可并行化的机器学习模型，水平可扩展性可能不会提高其性能，如果我们需要处理必须保存在内存中的大量数据，它可能根本无法提供帮助。第三，云实例更难以进行性能分析和追踪，这使得理解它们的行为（系统的*可观察性*有限）和诊断它们可能存在的问题变得更加困难。（关于这一点，请参阅第
    [5.3.6](design-code.html#monitoring-pipeline) 节。）
- en: References
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: Amazon Web Services. 2022c. *AWS Trainium*. [https://aws.amazon.com/machine-learning/trainium/](https://aws.amazon.com/machine-learning/trainium/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务。2022c。《AWS Trainium》[https://aws.amazon.com/machine-learning/trainium/](https://aws.amazon.com/machine-learning/trainium/)。
- en: Anderson, E., Z. Bai, C. Bishof, S. Blackford, J. Demmel, J. Dongarra, J. Du
    Croz, et al. 1999\. *LAPACK Users’ Guide*. 3rd ed. SIAM.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 安德森，E.，白，Z.，比肖夫，C.，布莱福特，S.，德梅尔，J.，东加拉，J.，杜克罗兹，J.，等人。1999。*LAPACK 用户指南*。第3版。SIAM。
- en: 'Beam, A. L., A. K. Manrai, and M. Ghassemi. 2020\. “Challenges to the Reproducibility
    of Machine Learning Models in Health Care.” *Journal of the American Medical Association*
    323 (4): 305–6.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 'Beam, A. L., A. K. Manrai, 和 M. Ghassemi. 2020\. “在医疗保健中机器学习模型可重复性的挑战.” *美国医学会杂志*
    323 (4): 305–6.'
- en: 'Blackford, L. S., J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry,.
    Heroux, et al. 2002\. “An Updated Set of Basic Linear Algebra Subprograms (BLAS).”
    *ACM Transactions on Mathematical Software* 28 (2): 135–51.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'Blackford, L. S., J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry,
    Heroux, 等人. 2002\. “更新后的基本线性代数子程序（BLAS）集.” *ACM数学软件事务* 28 (2): 135–51.'
- en: 'Bonawitz, K., H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C.
    Kiddon, et al. 2019\. “Towards Federated Learning at Scale: System Design.” In
    *Proceedings of Machine Learning and Systems*, 374–88.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Bonawitz, K., H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C.
    Kiddon, 等人. 2019\. “迈向大规模联邦学习：系统设计.” *机器学习与系统会议论文集*，第374–88页.
- en: Callon, Ross. 1996\. *The Twelve Networking Truths*. [https://rfc-editor.org/rfc/rfc1925.txt](https://rfc-editor.org/rfc/rfc1925.txt).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Callon, Ross. 1996\. *《十二个网络真理》*. [https://rfc-editor.org/rfc/rfc1925.txt](https://rfc-editor.org/rfc/rfc1925.txt).
- en: 'Cohen, A. Gokaslan V., E. Pavlick, and S. Tellex. 2019\. *OpenGPT-2: We Replicated
    GPT-2 Because You Can Too*. [https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc](https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen, A. Gokaslan V., E. Pavlick, 和 S. Tellex. 2019\. *《OpenGPT-2：因为你可以，所以我们复现了GPT-2》*.
    [https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc](https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc).
- en: 'Espeholt, L., H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    et al. 2018\. “IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner
    Architectures.” In *Proceedings of the 35th International Conference on Machine
    Learning (ICML)*, 1407–16.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Espeholt, L., H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, 等人.
    2018\. “IMPALA：基于重要性加权演员-学习架构的可扩展分布式深度强化学习.” *第35届国际机器学习会议（ICML）论文集*，第1407–16页.
- en: 'Fortin, P, A. Fleury, F. Lemaire, and M. Monagan. 2021\. “High-Performance
    SIMD Modular Arithmetic for Polynomial Evaluation.” *Concurrency and Computation:
    Practice and Experience* 33 (16): e6270.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fortin, P, A. Fleury, F. Lemaire, 和 M. Monagan. 2021\. “用于多项式评估的高性能SIMD模数运算.”
    *并发计算：实践与经验* 33 (16): e6270.'
- en: Galassi, M., J. Davies, J. Theiler, B. Gough, G. Jungman, P. Alken, M. Booth,
    F. Rossi, and R. Ulerich. 2021\. *GNU Scientific Library*. [https://www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf](https://www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Galassi, M., J. Davies, J. Theiler, B. Gough, G. Jungman, P. Alken, M. Booth,
    F. Rossi, 和 R. Ulerich. 2021\. *《GNU科学库》*. [https://www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf](https://www.gnu.org/software/gsl/doc/latex/gsl-ref.pdf).
- en: 'Gregg, B. 2021\. *Systems Performance: Enterprise and the Cloud*. 2nd ed. Addison-Wesley.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Gregg, B. 2021\. *《系统性能：企业和云》*. 2nd ed. Addison-Wesley.
- en: 'Hazelwood, K., S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M.
    Fawzy, et al. 2018\. “Applied Machine Learning at Facebook: A Datacenter Infrastructure
    Perspective.” In *Proceedings of the IEEE International Symposium on High Performance
    Computer Architecture (HPCA)*, 620–29.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Hazelwood, K., S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M.
    Fawzy, 等人. 2018\. “从数据中心基础设施的角度看Facebook的应用机器学习.” *IEEE国际高性能计算机架构会议（HPCA）论文集*，第620–29页.
- en: 'He, X., K. Zhao, and X. Chu. 2021\. “AutoML: A Survey of the State-of-the-Art.”
    *Knowledge-Based Systems* 212: 106622.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'He, X., K. Zhao, 和 X. Chu. 2021\. “AutoML：现状调查.” *基于知识的系统* 212: 106622.'
- en: Intel. 2021\. *Intel oneAPI Math Kernel Library*. [https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Intel. 2021\. *《Intel oneAPI Math Kernel Library》*. [https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html).
- en: Jain, P., X. Mo, A. Jain, H. Subbaraj, R. Durrani, A. Tumanov, J. Gonzalez,
    and I. Stoica. 2018\. “Dynamic Space-Time Scheduling for GPU Inference.” In *Workshop
    on Systems for ML and Open Source Software, NeurIPS 2018*, 1–9.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Jain, P., X. Mo, A. Jain, H. Subbaraj, R. Durrani, A. Tumanov, J. Gonzalez,
    和 I. Stoica. 2018\. “GPU推理的动态时空调度.” *NeurIPS 2018系统与开源软件研讨会论文集*，第1–9页.
- en: 'Jouppi, N. P., D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young,
    and D. Patterson. 2020\. “A Domain-Specific Supercomputer for Training Deep Neural
    Networks.” *Communications of the ACM* 63 (7): 67–78.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jouppi, N. P., D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young,
    和 D. Patterson. 2020\. “用于训练深度神经网络的专用超级计算机.” *ACM通讯* 63 (7): 67–78.'
- en: 'Jouppi, N. P., C. Young, N. Patil, and D. Patterson. 2018\. “A Domain-Specific
    Architexture for Deep Neural Networks.” *Communications of the ACM* 61 (9): 50–59.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jouppi, N. P., C. Young, N. Patil, and D. Patterson. 2018. “深度神经网络专用架构。” *ACM通讯*
    61 (9): 50–59.'
- en: 'Katal, A., M. Wazid, and R. H. Goudar. 2013\. “Big Data: Issues, Challenges,
    Tools and Good Practices.” In *Proceedings of the International Conference on
    Contemporary Computing*, 404–9.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Katal, A., M. Wazid, and R. H. Goudar. 2013. “大数据：问题、挑战、工具和良好实践。” 在 *当代计算国际会议论文集*
    中，404–9页.
- en: 'Khan, W. Z., E. Ahmed, S. Hakak, I. Yaqoob, and A. Ahmed. 2019\. “Edge Computing:
    A Survey.” *Future Generation Computer Systems* 97: 219–35.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'Khan, W. Z., E. Ahmed, S. Hakak, I. Yaqoob, and A. Ahmed. 2019. “边缘计算：综述。”
    *未来一代计算机系统* 97: 219–35.'
- en: 'Li, Q., Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He. 2021\. “A
    Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy
    and Protection.” *IEEE Transactions on Knowledge and Data Engineering* Advance
    publication.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Li, Q., Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He. 2021. “联邦学习系统综述：数据隐私和保护的前景、炒作和现实。”
    *IEEE知识数据工程 Transactions* 提前出版.
- en: Mujtaba, Hassan. 2018\. *Samsung Powers NVIDIA Quadro RTX Graphics Cards with
    16Gb GDDR6 Memory*. [https://wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/](https://wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Mujtaba, Hassan. 2018. *三星为NVIDIA Quadro RTX显卡配备16Gb GDDR6内存*. [https://wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/](https://wccftech.com/nvidia-quadro-rtx-turing-gpu-samsung-gddr6-memory/).
- en: 'Nvidia. 2018\. *Nvidia Turing GPU Architecture: Graphics Reinvented*. [https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia. 2018. *Nvidia Turing GPU架构：图形革新*. [https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf).
- en: Nvidia. 2021\. *CUDA Toolkit Documentation*. [https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia. 2021. *CUDA Toolkit Documentation*. [https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/).
- en: Overton, M. L. 2001\. *Numerical Computing with IEEE Floating Point Arithmetic*.
    SIAM.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Overton, M. L. 2001. *使用IEEE浮点算术进行数值计算*。 SIAM.
- en: 'Paszke, A., S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    et al. 2019\. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.”
    In *Advances in Neural Information Processing Systems (Nips)*, 32:8026–37.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Paszke, A., S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    et al. 2019. “PyTorch：一种命令式风格、高性能深度学习库。” 在 *神经信息处理系统（Nips）进展* 中，32:8026–37.
- en: Reuther, A., P. Michaleas, M. Jones, V. Gadepally, S. Samsi, and J. Kepner.
    2020\. “Survey of Machine Learning Accelerators.” In *Proceedings of the 2020
    Ieee High Performance Extreme Computing Conference (Hpec)*, 1–12.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Reuther, A., P. Michaleas, M. Jones, V. Gadepally, S. Samsi, and J. Kepner.
    2020. “机器学习加速器综述。” 在 *2020年IEEE高性能极端计算会议（Hpec）论文集* 中，1–12页.
- en: 'Tensorflow. 2021\. *XLA: Optimizing Compiler for Machine Learning*. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow. 2021. *XLA：机器学习优化编译器*. [https://www.tensorflow.org/xla](https://www.tensorflow.org/xla).
- en: TensorFlow. 2021a. *TensorFlow*. [https://www.tensorflow.org/overview/](https://www.tensorflow.org/overview/).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow. 2021a. *TensorFlow*. [https://www.tensorflow.org/overview/](https://www.tensorflow.org/overview/).
- en: Volkov, V., and J. W. Demmel. 2008\. “Benchmarking GPUs to Tune Dense Linear
    Algebra.” In *Proceedings of the 2008 ACM/IEEE Conference on Supercomputing*,
    1–11.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Volkov, V., and J. W. Demmel. 2008. “基准测试GPU以调整密集线性代数。” 在 *2008年ACM/IEEE超级计算会议论文集*
    中，1–11页.
- en: Williams-Young, D. B., and X. Li. 2019\. *On the Efficacy and High-Performance
    Implementation of Quaternion Matrix Multiplication*. [https://arxiv.org/abs/1903.05575](https://arxiv.org/abs/1903.05575).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Williams-Young, D. B., and X. Li. 2019. *关于四元数矩阵乘法的有效性和高性能实现*。[https://arxiv.org/abs/1903.05575](https://arxiv.org/abs/1903.05575).
- en: Zellers, R., A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y.
    Choi. 2019\. “Defending against Neural Fake News.” In *Advances in Neural Information
    Processing Systems (NeurIPS)*, 9054–65.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Zellers, R., A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and Y.
    Choi. 2019. “防御神经网络假新闻。” 在 *神经信息处理系统（NeurIPS）进展* 中，9054–65页.
- en: '* * *'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For the moment, we will use “data” and “variables” interchangeably. How data
    are actually represented in different types of variables will be the topic of
    Chapter [3](types-structures.html#types-structures).[↩︎](hardware.html#fnref2)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们将“数据”和“变量”互换使用。不同类型变量中数据的实际表示将是第 [3](types-structures.html#types-structures)
    章的主题。[↩︎](hardware.html#fnref2)
- en: Naming conventions vary by vendor. In Nvidia GPUs, they are called “streaming
    units” organised in “streaming multiprocessors”; in AMD GPUs they are called “compute
    units” and “workgroup processors”; in Intel GPUs “execution units” and “execution
    cores”.[↩︎](hardware.html#fnref3)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名规范因供应商而异。在 Nvidia GPU 中，它们被称为“流单元”，组织在“流多处理器”中；在 AMD GPU 中，它们被称为“计算单元”和“工作组处理器”；在
    Intel GPU 中，“执行单元”和“执行核心”。[↩︎](hardware.html#fnref3)
- en: For instance, each unit in an Nvidia RTX 2060 has 256kB of registers (Nvidia
    [2018](#ref-turing-architecture)), while a CPU only has 32 \(\times\) 16 = 512
    bytes worth of AVX512 registers for each core (hence the name of the instruction
    set).[↩︎](hardware.html#fnref4)
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，Nvidia RTX 2060 中的每个单元都有 256kB 的寄存器（Nvidia [2018](#ref-turing-architecture)），而
    CPU 每个核心只有 32 \(\times\) 16 = 512 字节的 AVX512 寄存器（因此得名指令集）。[↩︎](hardware.html#fnref4)
- en: In describing TPUs, we follow Google’s naming conventions because, at the time
    of this writing, that is the only TPU in wide use in machine learning.[↩︎](hardware.html#fnref5)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在描述 TPUs 时，我们遵循 Google 的命名规范，因为在撰写本文时，那是在机器学习中广泛使用的唯一 TPU。[↩︎](hardware.html#fnref5)
- en: PCIe is in use in both x86-64 and ARM systems, and comes in several revisions
    and speeds. At the time of this writing, the current one is PCIe 4.0 which uses
    up to 16 channels in parallel to transfer up to 64GB/s.[↩︎](hardware.html#fnref6)
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCIe 在 x86-64 和 ARM 系统中都在使用，并且有多种版本和速度。在撰写本文时，当前版本是 PCIe 4.0，它使用多达 16 个并行通道来传输高达
    64GB/s 的数据。[↩︎](hardware.html#fnref6)
