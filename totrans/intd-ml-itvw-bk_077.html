<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>5.1.4 Calculus and convex optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>5.1.4 Calculus and convex optimization</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html">https://huyenchip.com/ml-interviews-book/contents/5.1.4-calculus-and-convex-optimization.html</a></blockquote>
                                
                                
<p><em>If some characters seem to be missing, it's because MathJax is not loaded correctly. Refreshing the page should fix it.</em></p>
<ol>
<li>Differentiable functions<ol>
<li>[E] What does it mean when a function is differentiable?</li>
<li>[E] Give an example of when a function doesn’t have a derivative at a point.</li>
<li>[M] Give an example of non-differentiable functions that are frequently used in machine learning. How do we do backpropagation if those functions aren’t differentiable?</li>
</ol>
</li>
<li>Convexity<ol>
<li>[E] What does it mean for a function to be convex or concave? Draw it.</li>
<li>[E] Why is convexity desirable in an optimization problem?</li>
<li>[M] Show that the cross-entropy loss function is convex.</li>
</ol>
</li>
<li><p>Given a logistic discriminant classifier:</p>
<p> <script type="math/tex; mode=display">
        p(y=1|x) = \sigma (w^Tx)
    </script></p>
<p> where the sigmoid function is given by:</p>
<p> <script type="math/tex; mode=display">
        \sigma(z) = (1 + \exp(-z))^{-1}
    </script></p>
<p> The logistic loss for a training sample <script type="math/tex; ">x_i</script> with class label <script type="math/tex; ">y_i</script> is given by:</p>
<p> <script type="math/tex; mode=display">
        L(y_i, x_i;w) = -\log p(y_i|x_i)
    </script></p>
<ol>
<li>Show that <script type="math/tex; ">p(y=-1|x) = \sigma(-w^Tx)</script>.</li>
<li>Show that <script type="math/tex; ">\Delta_wL(y_i, x_i; w) = -y_i(1-p(y_i|x_i))x_i</script>.</li>
<li>Show that <script type="math/tex; ">\Delta_wL(y_i, x_i; w)</script> is convex.</li>
</ol>
</li>
<li><p>Most ML algorithms we use nowadays use first-order derivatives (gradients) to construct the next training iteration.</p>
<ol>
<li>[E] How can we use second-order derivatives for training models?</li>
<li>[M] Pros and cons of second-order optimization.</li>
<li>[M] Why don’t we see more second-order optimization in practice?</li>
</ol>
</li>
<li>[M] How can we use the Hessian (second derivative matrix) to test for critical points? </li>
<li>[E] Jensen’s inequality forms the basis for many algorithms for probabilistic inference, including Expectation-Maximization and variational inference.. Explain what Jensen’s inequality is.</li>
<li>[E] Explain the chain rule.</li>
<li>[M] Let <script type="math/tex; ">x \in R_n</script>, <script type="math/tex; ">L = crossentropy(softmax(x), y)</script> in which <script type="math/tex; ">y</script> is a one-hot vector. Take the derivative of <script type="math/tex; ">L</script> with respect to <script type="math/tex; ">x</script>.</li>
<li>[M] Given the function <script type="math/tex; ">f(x, y) = 4x^2 - y</script> with the constraint <script type="math/tex; ">x^2 + y^2 =1</script>. Find the function’s maximum and minimum values.</li>
</ol>
<hr/>
<blockquote>
<p>On convex optimization</p>
</blockquote>
<p>Convex optimization is important because it's the only type of optimization that we more or less understand. Some might argue that since many of the common objective functions in deep learning aren't convex, we don't need to know about convex optimization. However, even when the functions aren't convex, analyzing them as if they were convex often gives us meaningful bounds. If an algorithm doesn't work assuming that a loss function is convex, it definitely doesn't work when the loss function is non-convex.</p>
<p>Convexity is the exception, not the rule. If you're asked whether a function is convex and it isn't already in the list of commonly known convex functions, there's a good chance that it isn't convex. If you want to learn about convex optimization, check out <a href="http://cs229.stanford.edu/section/cs229-cvxopt.pdf" target="_blank">Stephen Boyd's textbook</a>.</p>
<hr/>
<blockquote>
<p>On Hessian matrix</p>
</blockquote>
<p>The Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function. </p>
<p>Given a function <script type="math/tex; ">f : ℝn → ℝ</script>. If all second partial derivatives of f exist and are continuous over the domain of the function, then the Hessian matrix H of f is a square nn matrix such that: <script type="math/tex; ">H_{ij}=\frac{\delta f}{\delta x_i\delta x_j}</script>.</p>
<center>
    <img src="../Images/b8a01737fcf984b213c641a1a67c9bde.png" width="40%" alt="Hessian matrix" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image18.png"/>
</center>

<p>The Hessian is used for large-scale optimization problems within Newton-type methods and quasi-Newton methods. It is also commonly used for expressing image processing operators in image processing and computer vision for tasks such as blob detection and multi-scale signal representation.</p>

                                
                                    
</body>
</html>