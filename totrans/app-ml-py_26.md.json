["```py\nif porosity > 0.15:\n    if brittleness < 20:\n        initial_production = 1000\n    else:\n        initial_production = 7000\nelse:\n    if brittleness < 40:\n        initial_production = 500\n    else:\n        initial_production = 3000 \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True                                      # toggle to supress warnings\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn import tree                                      # tree program from scikit learn (package for machine learning)\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef extract_rules(tree_model, feature_names):                 # recursive method to extract rules, from paulkernfeld Stack Overflow (?)\n    rules = []\n    def traverse(node, depth, prev_rule):\n        if tree_model.tree_.children_left[node] == -1:        # Leaf node\n            class_label = np.argmax(tree_model.tree_.value[node])\n            rule = f\"{prev_rule} => Class {class_label}\"\n            rules.append(rule)\n        else:  # Split node\n            feature = feature_names[tree_model.tree_.feature[node]]\n            threshold = tree_model.tree_.threshold[node]\n            left_child = tree_model.tree_.children_left[node]\n            right_child = tree_model.tree_.children_right[node]\n            traverse(left_child, depth + 1, f\"{prev_rule} & {feature} <= {threshold}\") # Recursively traverse left and right subtrees\n            traverse(right_child, depth + 1, f\"{prev_rule} & {feature} > {threshold}\")\n    traverse(0, 0, \"Root\")\n    return rules\n\ndef plot_decision_tree_regions(tree_model, feature_names,X_min,X_max,annotate=True):\n    rules = extract_rules(tree_model, feature_names)\n    for irule, ____ in enumerate(rules):\n        rule = rules[irule].split()[2:]\n        X_min = Xmin[0]; X_max = Xmax[0]; Y_min = Xmin[1]; Y_max = Xmax[1];\n        index = [i for i,val in enumerate(rule) if val==feature_names[0]]\n        for i in index:\n            if rule[i+1] == '<=':\n                X_max = min(float(rule[i+2]),X_max)\n            else:\n                X_min = max(float(rule[i+2]),X_min)\n        index = [i for i,val in enumerate(rule) if val==feature_names[1]]\n        for i in index:\n            if rule[i+1] == '<=':\n                Y_max = min(float(rule[i+2]),Y_max)\n            else:\n                Y_min = max(float(rule[i+2]),Y_min) \n        plt.gca().add_patch(plt.Rectangle((X_min,Y_min),X_max-X_min,Y_max-Y_min, lw=2,ec='black',fc=\"none\"))\n        cx = (X_min + X_max)*0.5; cy = (Y_min + Y_max)*0.5; loc = np.array((cx,cy)).reshape(1, -1)\n        if annotate == True:\n            plt.annotate(text = str(f'{np.round(tree_model.predict(loc)[0],2):,.0f}'),xy=(cx,cy),ha='center',\n                         weight='bold',c='white',zorder=100)\n\ndef visualize_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,\n                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction \n    cmap = plt.cm.inferno\n    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization\n    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh\n                     np.arange(Xmax[1], Xmin[1], X2plot_step))\n    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh\n    y_hat = y_hat.reshape(XX1.shape)\n\n    plt.imshow(y_hat,interpolation=None, aspect=\"auto\", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], \n        vmin=ymin,vmax=ymax,alpha = 0.2,cmap=cmap,zorder=1)\n    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors=\"black\", label = 'Train',zorder=10)\n    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors=\"black\", label = 'Test',zorder=10)\n\n    plot_decision_tree_regions(model,Xname,Xmin,Xmax,annotate)\n    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])\n    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    cbar.set_label(ylabel, rotation=270, labelpad=20)\n    return y_hat\n\ndef check_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title): # plots the estimated vs. the actual \n    y_hat_train = model.predict(np.c_[X1_train,X2_train]); y_hat_test = model.predict(np.c_[X1_test,X2_test])\n\n    df_cross = pd.DataFrame(np.c_[y_test,y_hat_test],columns=['y_test','y_hat_test'])\n    df_cross_train = pd.DataFrame(np.c_[y_train,y_hat_train],columns=['y_train','y_hat_train'])\n\n    plt.scatter(y_train,y_hat_train,s=15, c='blue',marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, \n                linewidths=0.3, edgecolors=\"black\",label='Train',zorder=10)\n    plt.scatter(y_test,y_hat_test,s=15, c='red',marker='s', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, \n                linewidths=0.3, edgecolors=\"black\",label='Test',zorder=10)\n\n    unique_y_hat_all = set(np.concatenate([y_hat_test,y_hat_train]))\n    for y_hat in unique_y_hat_all:\n        plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.2,ls='--',zorder=1)\n\n    unique_y_hat_test = set(y_hat_test)\n    for y_hat in unique_y_hat_test:\n        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)\n        cond_mean_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].mean()\n        cond_P75_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.75)\n        cond_P25_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.25)\n        plt.scatter(cond_mean_y_hat,y_hat-0.02*(ymax-ymin),color='red',edgecolor='black',s=60,marker='^',zorder=100)\n        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat-0.025*(ymax-ymin),y_hat-0.025*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)\n\n    unique_y_hat_train = set(y_hat_train)\n    for y_hat in unique_y_hat_train:\n        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)\n        cond_mean_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].mean()\n        cond_P75_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.75)\n        cond_P25_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.25)\n        plt.scatter(cond_mean_y_hat,y_hat+0.02*(ymax-ymin),color='blue',edgecolor='black',s=60,marker='v',zorder=100)\n        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat+0.025*(ymax-ymin),y_hat+0.025*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)\n\n    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n    plt.xlim([ymin,ymax]); plt.ylim([ymin,ymax]); plt.legend(loc='upper left')\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\n    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)\n    MSE_train = metrics.mean_squared_error(y_train,y_hat_train); MSE_test = metrics.mean_squared_error(y_test,y_hat_test)\n    plt.gca().add_patch(plt.Rectangle((ymin+0.6*(ymax-ymin),ymin+0.1*(ymax-ymin)),0.40*(ymax-ymin),0.12*(ymax-ymin),\n        lw=0.5,ec='black',fc=\"white\",zorder=100))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.annotate('MSE Testing:  ' + str(f'{np.round(MSE_test,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.18*(ymax-ymin)),zorder=1000)\n    plt.annotate('MSE Training: ' + str(f'{np.round(MSE_train,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.12*(ymax-ymin)),zorder=1000)\n\ndef tree_tuning(node_max,cnode,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title,seed):\n    MSE_test_mat = np.zeros(node_max-1); MSE_train_mat = np.zeros(node_max-1);\n\n    for imax_leaf_node, max_leaf_node in enumerate(range(2,node_max+1)):\n        np.random.seed(seed = seed)\n        tree_temp = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_node)\n        tree_temp = tree_temp.fit(X_train.values, y_train.values)\n        y_hat_train = tree_temp.predict(np.c_[X1_train,X2_train]); y_hat_test = tree_temp.predict(np.c_[X1_test,X2_test])  \n        MSE_train_mat[imax_leaf_node] = metrics.mean_squared_error(y_train,y_hat_train)\n        MSE_test_mat[imax_leaf_node] = metrics.mean_squared_error(y_test,y_hat_test)\n        if max_leaf_node == cnode:\n            plt.scatter(cnode,MSE_train_mat[imax_leaf_node],color='blue',edgecolor='black',s=20,marker='o',zorder=1000)\n            plt.scatter(cnode,MSE_test_mat[imax_leaf_node],color='red',edgecolor='black',s=20,marker='o',zorder=1000)\n    maxcheck = max(np.max(MSE_train_mat),np.max(MSE_test_mat))\n\n    plt.vlines(cnode,0,maxcheck,color='black',ls='--',lw=1,zorder=1) \n    plt.plot(range(2,node_max+1),MSE_train_mat,color='blue',zorder=100,label='Train')\n    plt.plot(range(2,node_max+1),MSE_test_mat,color='red',zorder=100,label='Test')\n\n    plt.title(title); plt.xlabel('Maximum Number of Leaf Nodes'); plt.ylabel('Means Square Error')\n    plt.xlim([0,node_max]); plt.ylim([0,maxcheck]); plt.legend(loc='upper right')\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef tree_to_code(tree, feature_names):                        # code from StackOverFlow by paulkernfeld\n    tree_ = tree.tree_                                        # convert tree object to portable code to use anywhere\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print(\"def tree({}):\".format(\", \".join(feature_names)))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n            recurse(tree_.children_left[node], depth + 1)\n            print(\"{}elif {} > {}\".format(indent, name, threshold))\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print(\"{}return {}\".format(indent, tree_.value[node]))\n    recurse(0, 1) \n\ndef get_lineage(tree, feature_names):                         # code from StackOverFlow by Zelanzny7\n    left      = tree.tree_.children_left                      # track the decision path for any set of inputs\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    # get ids of child nodes\n    idx = np.argwhere(left == -1)[:,0]     \n    def recurse(left, right, child, lineage=None):          \n        if lineage is None:\n            lineage = [child]\n        if child in left:\n            parent = np.where(left == child)[0].item()\n            split = 'l'\n        else:\n            parent = np.where(right == child)[0].item()\n            split = 'r'\n        lineage.append((parent, split, threshold[parent], features[parent]))\n        if parent == 0:\n            lineage.reverse()\n            return lineage\n        else:\n            return recurse(left, right, parent, lineage)\n    for child in idx:\n        for node in recurse(left, right, child):\n            print(node) \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1000.0; ymax = 9000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(221)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[0] + ' Train and Test CDFs')\n\nplt.subplot(222)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[1] + ' Train and Test CDFs')\n\nplt.subplot(223)                                              # response feature CDF\nplot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(ylabel + ' Train and Test CDFs')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors=\"black\", label = 'Test')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_list = [2,3,4,10,20,100]\n\nfor inode,leaf_nodes in enumerate(leaf_nodes_list):\n\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)\n    tree_model = tree_model.fit(X_train.values, y_train.values)\n\n    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions\n    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_viz = 2\n\ntree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio\n\nax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions \nvisualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,\n        Xlabelunit,ylabelunit,annotate=False)   \n\nax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions\n_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,\n                  proportion=True,max_depth=4,fontsize=15)\n\nplt.tight_layout()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\ntrees = []; MSE_CV = []; node_CV = []\n\ninode = 2\nwhile inode < len(X_train):                                   # loop over the hyperparameter, train with training and test with testing\n    tree_model = tree.DecisionTreeRegressor(min_samples_leaf=1,max_leaf_nodes=inode).fit(X_train.values, y_train.values)\n    trees.append(tree_model)\n    predict_train = tree_model.predict(np.c_[X_test[Xname[0]],X_test[Xname[1]]]) \n    MSE_CV.append(metrics.mean_squared_error(y_test[yname],predict_train))   \n    all_nodes = tree_model.tree_.node_count             \n    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes\n    node_CV.append(terminal_nodes); inode+=1\n\nplt.subplot(111)\nplt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,linewidths=0.3,\n            edgecolors=\"black\",zorder=20)\ntuned_node = node_CV[np.argmin(MSE_CV)]; max_MSE_CV = np.max(MSE_CV)\nplt.vlines(tuned_node,0,1.05*max_MSE_CV,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node),(tuned_node-2,3.5e5),rotation=90,zorder=30)\nplt.title('Decision Tree Cross Validation Testing Error vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); plt.ylabel('Mean Square Error')\nplt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_CV); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nMSE_kF = []; node_kF = []                                     # k-fold iteration code modified from StackOverFlow by Dimosthenis\n\ninode = 2\nwhile inode < len(X_train):\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode).fit(X_train.values, y_train.values)\n    scores = cross_val_score(estimator=tree_model, X= np.c_[df[Xname[0]],df[Xname[1]]],y=df[yname], cv=5, n_jobs=4,\n        scoring = \"neg_mean_squared_error\")                   # perform 4-fold cross validation\n    MSE_kF.append(abs(scores.mean()))\n    all_nodes = tree_model.tree_.node_count   \n    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes\n    node_kF.append(terminal_nodes); inode+=1\n\ntuned_node_kF = node_kF[np.argmin(MSE_kF)]; max_MSE_kF = np.max(MSE_kF)  \nplt.subplot(111)\nplt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)\nplt.scatter(node_kF,MSE_kF,s=None,c=\"red\",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,\n            linewidths=0.5, edgecolors=\"black\",zorder=40,label='k-Fold')\nplt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.4,linewidths=0.3,\n            edgecolors=\"black\",zorder=20,label='Cross Validation')\nplt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); \nplt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.legend(loc='upper right')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npruned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF)\npruned_tree_model = pruned_tree_model.fit(X, y)               # re-train\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model, Tuned Leaf Nodes: ' + str(tuned_node_kF),Xname,yname,\n                    Xlabelunit,ylabelunit) # plots the data points and the decision tree prediction \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot, Tuned Leaf Nodes: ' + \n                    str(tuned_node_kF),)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nx1 = 7.0; x2 = 10.0                                          # the predictor feature values for the decision path\n\ndecision_path = pruned_tree_model.decision_path(np.c_[x1,x2])\nprint(decision_path) \n```", "```py\n (0, 0)\t1\n  (0, 1)\t1\n  (0, 3)\t1\n  (0, 13)\t1 \n```", "```py\ntree_to_code(pruned_tree_model, list(Xname))                  # convert a decision tree to Python code, nested if statements \n```", "```py\ndef tree(Por, Brittle):\n  if Por <= 14.789999961853027:\n    if Por <= 12.425000190734863:\n      if Por <= 8.335000038146973:\n        return [[1879.19091537]]\n      elif Por > 8.335000038146973\n        if Brittle <= 39.125:\n          return [[2551.00021508]]\n        elif Brittle > 39.125\n          return [[3369.12903299]]\n    elif Por > 12.425000190734863\n      if Brittle <= 39.26500129699707:\n        return [[3160.11022857]]\n      elif Brittle > 39.26500129699707\n        return [[4154.18334527]]\n  elif Por > 14.789999961853027\n    if Por <= 18.015000343322754:\n      if Brittle <= 33.25:\n        return [[3883.19381758]]\n      elif Brittle > 33.25\n        if Por <= 16.434999465942383:\n          return [[4544.69777089]]\n        elif Por > 16.434999465942383\n          return [[5240.84146117]]\n    elif Por > 18.015000343322754\n      if Brittle <= 31.5600004196167:\n        return [[4353.11874206]]\n      elif Brittle > 31.5600004196167\n        return [[5868.56369869]] \n```", "```py\nplt.subplot(111)                                              # plot the feature importance \nplt.title(\"Decision Tree Feature Importance\")\nplt.bar(Xlabel, pruned_tree_model.feature_importances_,edgecolor = 'black',\n       color=\"darkorange\",alpha = 0.6, align=\"center\")\nplt.xlim([-0.5,len(Xname)-0.5]); plt.ylim([0.,1.0])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1., top=0.8, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nfig = plt.figure(figsize=(15,10))\n\n_ = tree.plot_tree(pruned_tree_model,                         # plot the decision tree for model visualization\n                   feature_names=list(Xname),  \n                   class_names=list(yname),\n                   filled=True) \n```", "```py\nfrom sklearn import tree                                      # import decision tree from scikit-learn\nXname = ['Por','Brittle']; yname='Production'                 # predictor features and response feature\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\nmy_data = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load subsurface data table\nmy_tree = tree.DecisionTreeRegressor(max_leaf_nodes=26)       # instantiate tree with hyperparameters\nmy_tree = my_tree.fit(X.values,y.values)                      # train tree with training data\nestimate = my_tree.predict([[x1,x2]])[0]                      # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD \n```", "```py\npipe_tree = Pipeline([                                        # the machine learning workflow as a pipeline object\n\n    ('tree', tree.DecisionTreeRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'tree__max_leaf_nodes': np.arange(2,len(X),1,dtype = int),\n}\n\nKF_tuned_tree = GridSearchCV(pipe_tree,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             cv=KFold(n_splits=5,shuffle=False),refit = True)\nKF_tuned_tree.fit(X,y)                                        # tune and train the model\n\nprint('Tuned hyperparameter: max_leaf_nodes = ' + str(KF_tuned_tree.best_params_))\n\nestimate = KF_tuned_tree.predict([[x1,x2]])[0]                # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nTuned hyperparameter: max_leaf_nodes = {'tree__max_leaf_nodes': 10}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD \n```", "```py\nidata = 2                                                    # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nMSE_kF = []; node_kF = []                                     \nkf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification \n\ninode = 2\nwhile inode < len(X_train):\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)\n    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = \"neg_mean_squared_error\") # perform 5-fold cross validation\n    MSE_kF.append(abs(scores.mean()))\n    node_kF.append(inode); inode+=1\n\ntuned_node_kF = node_kF[np.argmin(MSE_kF)]\ntuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data\n\nplt.subplot(121)\nplt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)\nplt.scatter(node_kF,MSE_kF,s=None,c=\"red\",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,\n            linewidths=0.5, edgecolors=\"black\",zorder=40,label='k-Fold')\nplt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); \nplt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.legend(loc='upper right')\n\ny_hat = tuned_tree_model.predict(X)\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Decision Tree, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nif porosity > 0.15:\n    if brittleness < 20:\n        initial_production = 1000\n    else:\n        initial_production = 7000\nelse:\n    if brittleness < 40:\n        initial_production = 500\n    else:\n        initial_production = 3000 \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True                                      # toggle to supress warnings\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn import tree                                      # tree program from scikit learn (package for machine learning)\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef extract_rules(tree_model, feature_names):                 # recursive method to extract rules, from paulkernfeld Stack Overflow (?)\n    rules = []\n    def traverse(node, depth, prev_rule):\n        if tree_model.tree_.children_left[node] == -1:        # Leaf node\n            class_label = np.argmax(tree_model.tree_.value[node])\n            rule = f\"{prev_rule} => Class {class_label}\"\n            rules.append(rule)\n        else:  # Split node\n            feature = feature_names[tree_model.tree_.feature[node]]\n            threshold = tree_model.tree_.threshold[node]\n            left_child = tree_model.tree_.children_left[node]\n            right_child = tree_model.tree_.children_right[node]\n            traverse(left_child, depth + 1, f\"{prev_rule} & {feature} <= {threshold}\") # Recursively traverse left and right subtrees\n            traverse(right_child, depth + 1, f\"{prev_rule} & {feature} > {threshold}\")\n    traverse(0, 0, \"Root\")\n    return rules\n\ndef plot_decision_tree_regions(tree_model, feature_names,X_min,X_max,annotate=True):\n    rules = extract_rules(tree_model, feature_names)\n    for irule, ____ in enumerate(rules):\n        rule = rules[irule].split()[2:]\n        X_min = Xmin[0]; X_max = Xmax[0]; Y_min = Xmin[1]; Y_max = Xmax[1];\n        index = [i for i,val in enumerate(rule) if val==feature_names[0]]\n        for i in index:\n            if rule[i+1] == '<=':\n                X_max = min(float(rule[i+2]),X_max)\n            else:\n                X_min = max(float(rule[i+2]),X_min)\n        index = [i for i,val in enumerate(rule) if val==feature_names[1]]\n        for i in index:\n            if rule[i+1] == '<=':\n                Y_max = min(float(rule[i+2]),Y_max)\n            else:\n                Y_min = max(float(rule[i+2]),Y_min) \n        plt.gca().add_patch(plt.Rectangle((X_min,Y_min),X_max-X_min,Y_max-Y_min, lw=2,ec='black',fc=\"none\"))\n        cx = (X_min + X_max)*0.5; cy = (Y_min + Y_max)*0.5; loc = np.array((cx,cy)).reshape(1, -1)\n        if annotate == True:\n            plt.annotate(text = str(f'{np.round(tree_model.predict(loc)[0],2):,.0f}'),xy=(cx,cy),ha='center',\n                         weight='bold',c='white',zorder=100)\n\ndef visualize_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,\n                         ymax,title,Xname,yname,Xlabel,ylabel,annotate=True):# plots the data points and the decision tree prediction \n    cmap = plt.cm.inferno\n    X1plot_step = (Xmax[0] - Xmin[0])/300.0; X2plot_step = -1*(Xmax[1] - Xmin[1])/300.0 # resolution of the model visualization\n    XX1, XX2 = np.meshgrid(np.arange(Xmin[0], Xmax[0], X1plot_step), # set up the mesh\n                     np.arange(Xmax[1], Xmin[1], X2plot_step))\n    y_hat = model.predict(np.c_[XX1.ravel(), XX2.ravel()])    # predict with our trained model over the mesh\n    y_hat = y_hat.reshape(XX1.shape)\n\n    plt.imshow(y_hat,interpolation=None, aspect=\"auto\", extent=[Xmin[0],Xmax[0],Xmin[1],Xmax[1]], \n        vmin=ymin,vmax=ymax,alpha = 0.2,cmap=cmap,zorder=1)\n    sp = plt.scatter(X1_train,X2_train,s=None, c=y_train, marker='o', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.6, linewidths=0.3, edgecolors=\"black\", label = 'Train',zorder=10)\n    plt.scatter(X1_test,X2_test,s=None, c=y_test, marker='s', cmap=cmap, \n        norm=None, vmin=ymin, vmax=ymax, alpha=0.3, linewidths=0.3, edgecolors=\"black\", label = 'Test',zorder=10)\n\n    plot_decision_tree_regions(model,Xname,Xmin,Xmax,annotate)\n    plt.title(title); plt.xlabel(Xlabel[0]); plt.ylabel(Xlabel[1])\n    plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n    cbar = plt.colorbar(sp, orientation = 'vertical')         # add the color bar\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    cbar.set_label(ylabel, rotation=270, labelpad=20)\n    return y_hat\n\ndef check_tree_model(model,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title): # plots the estimated vs. the actual \n    y_hat_train = model.predict(np.c_[X1_train,X2_train]); y_hat_test = model.predict(np.c_[X1_test,X2_test])\n\n    df_cross = pd.DataFrame(np.c_[y_test,y_hat_test],columns=['y_test','y_hat_test'])\n    df_cross_train = pd.DataFrame(np.c_[y_train,y_hat_train],columns=['y_train','y_hat_train'])\n\n    plt.scatter(y_train,y_hat_train,s=15, c='blue',marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, \n                linewidths=0.3, edgecolors=\"black\",label='Train',zorder=10)\n    plt.scatter(y_test,y_hat_test,s=15, c='red',marker='s', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.7, \n                linewidths=0.3, edgecolors=\"black\",label='Test',zorder=10)\n\n    unique_y_hat_all = set(np.concatenate([y_hat_test,y_hat_train]))\n    for y_hat in unique_y_hat_all:\n        plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.2,ls='--',zorder=1)\n\n    unique_y_hat_test = set(y_hat_test)\n    for y_hat in unique_y_hat_test:\n        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)\n        cond_mean_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].mean()\n        cond_P75_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.75)\n        cond_P25_y_hat = df_cross.loc[df_cross['y_hat_test'] == y_hat, 'y_test'].quantile(0.25)\n        plt.scatter(cond_mean_y_hat,y_hat-0.02*(ymax-ymin),color='red',edgecolor='black',s=60,marker='^',zorder=100)\n        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat-0.025*(ymax-ymin),y_hat-0.025*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat-0.032*(ymax-ymin),y_hat-0.018*(ymax-ymin)],c='black',lw=0.7)\n\n    unique_y_hat_train = set(y_hat_train)\n    for y_hat in unique_y_hat_train:\n        #plt.plot([ymin,ymax],[y_hat,y_hat],c='black',alpha=0.3,ls='--',zorder=1)\n        cond_mean_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].mean()\n        cond_P75_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.75)\n        cond_P25_y_hat = df_cross_train.loc[df_cross_train['y_hat_train'] == y_hat, 'y_train'].quantile(0.25)\n        plt.scatter(cond_mean_y_hat,y_hat+0.02*(ymax-ymin),color='blue',edgecolor='black',s=60,marker='v',zorder=100)\n        plt.plot([cond_P25_y_hat,cond_P75_y_hat],[y_hat+0.025*(ymax-ymin),y_hat+0.025*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P25_y_hat,cond_P25_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)\n        plt.plot([cond_P75_y_hat,cond_P75_y_hat],[y_hat+0.032*(ymax-ymin),y_hat+0.018*(ymax-ymin)],c='black',lw=0.7)\n\n    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n    plt.xlim([ymin,ymax]); plt.ylim([ymin,ymax]); plt.legend(loc='upper left')\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\n    plt.arrow(ymin,ymin,ymax,ymax,width=0.02,color='black',head_length=0.0,head_width=0.0)\n    MSE_train = metrics.mean_squared_error(y_train,y_hat_train); MSE_test = metrics.mean_squared_error(y_test,y_hat_test)\n    plt.gca().add_patch(plt.Rectangle((ymin+0.6*(ymax-ymin),ymin+0.1*(ymax-ymin)),0.40*(ymax-ymin),0.12*(ymax-ymin),\n        lw=0.5,ec='black',fc=\"white\",zorder=100))\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.annotate('MSE Testing:  ' + str(f'{np.round(MSE_test,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.18*(ymax-ymin)),zorder=1000)\n    plt.annotate('MSE Training: ' + str(f'{np.round(MSE_train,2):,.0f}'),(ymin+0.62*(ymax-ymin),ymin+0.12*(ymax-ymin)),zorder=1000)\n\ndef tree_tuning(node_max,cnode,X1_train,X1_test,X2_train,X2_test,Xmin,Xmax,y_train,y_test,ymin,ymax,title,seed):\n    MSE_test_mat = np.zeros(node_max-1); MSE_train_mat = np.zeros(node_max-1);\n\n    for imax_leaf_node, max_leaf_node in enumerate(range(2,node_max+1)):\n        np.random.seed(seed = seed)\n        tree_temp = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_node)\n        tree_temp = tree_temp.fit(X_train.values, y_train.values)\n        y_hat_train = tree_temp.predict(np.c_[X1_train,X2_train]); y_hat_test = tree_temp.predict(np.c_[X1_test,X2_test])  \n        MSE_train_mat[imax_leaf_node] = metrics.mean_squared_error(y_train,y_hat_train)\n        MSE_test_mat[imax_leaf_node] = metrics.mean_squared_error(y_test,y_hat_test)\n        if max_leaf_node == cnode:\n            plt.scatter(cnode,MSE_train_mat[imax_leaf_node],color='blue',edgecolor='black',s=20,marker='o',zorder=1000)\n            plt.scatter(cnode,MSE_test_mat[imax_leaf_node],color='red',edgecolor='black',s=20,marker='o',zorder=1000)\n    maxcheck = max(np.max(MSE_train_mat),np.max(MSE_test_mat))\n\n    plt.vlines(cnode,0,maxcheck,color='black',ls='--',lw=1,zorder=1) \n    plt.plot(range(2,node_max+1),MSE_train_mat,color='blue',zorder=100,label='Train')\n    plt.plot(range(2,node_max+1),MSE_test_mat,color='red',zorder=100,label='Test')\n\n    plt.title(title); plt.xlabel('Maximum Number of Leaf Nodes'); plt.ylabel('Means Square Error')\n    plt.xlim([0,node_max]); plt.ylim([0,maxcheck]); plt.legend(loc='upper right')\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef tree_to_code(tree, feature_names):                        # code from StackOverFlow by paulkernfeld\n    tree_ = tree.tree_                                        # convert tree object to portable code to use anywhere\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n    print(\"def tree({}):\".format(\", \".join(feature_names)))\n\n    def recurse(node, depth):\n        indent = \"  \" * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n            recurse(tree_.children_left[node], depth + 1)\n            print(\"{}elif {} > {}\".format(indent, name, threshold))\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print(\"{}return {}\".format(indent, tree_.value[node]))\n    recurse(0, 1) \n\ndef get_lineage(tree, feature_names):                         # code from StackOverFlow by Zelanzny7\n    left      = tree.tree_.children_left                      # track the decision path for any set of inputs\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    # get ids of child nodes\n    idx = np.argwhere(left == -1)[:,0]     \n    def recurse(left, right, child, lineage=None):          \n        if lineage is None:\n            lineage = [child]\n        if child in left:\n            parent = np.where(left == child)[0].item()\n            split = 'l'\n        else:\n            parent = np.where(right == child)[0].item()\n            split = 'r'\n        lineage.append((parent, split, threshold[parent], features[parent]))\n        if parent == 0:\n            lineage.reverse()\n            return lineage\n        else:\n            return recurse(left, right, parent, lineage)\n    for child in idx:\n        for node in recurse(left, right, child):\n            print(node) \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1000.0; ymax = 9000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame')    # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(221)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[0]); plt.xlim(Xmin[0],Xmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[0] + ' Train and Test CDFs')\n\nplt.subplot(222)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xlabelunit[1]); plt.xlim(Xmin[1],Xmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xlabel[1] + ' Train and Test CDFs')\n\nplt.subplot(223)                                              # response feature CDF\nplot_CDF(y_train[yname],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(y_test[yname],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(ylabelunit); plt.xlim(ymin,ymax); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(ylabel + ' Train and Test CDFs')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=None, c=y_train[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=None, c=y_test[yname], marker='s', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.5, linewidths=0.3, edgecolors=\"black\", label = 'Test')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_list = [2,3,4,10,20,100]\n\nfor inode,leaf_nodes in enumerate(leaf_nodes_list):\n\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)\n    tree_model = tree_model.fit(X_train.values, y_train.values)\n\n    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions\n    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_viz = 2\n\ntree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio\n\nax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions \nvisualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,\n        Xlabelunit,ylabelunit,annotate=False)   \n\nax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions\n_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,\n                  proportion=True,max_depth=4,fontsize=15)\n\nplt.tight_layout()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 5; max_depth =99; min_samples_leaf = 1      # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nmax_leaf_nodes = 50; max_depth = 9; min_samples_leaf = 1     # hyperparameters\n\ntree_model = tree.DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes,max_depth = max_depth,min_samples_leaf = min_samples_leaf)\ntree_model = tree_model.fit(X_train.values, y_train.values)\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model',Xname,yname,Xlabelunit,ylabelunit) \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot',)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_list = [2,3,4,10,20,100]\n\nfor inode,leaf_nodes in enumerate(leaf_nodes_list):\n\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes)\n    tree_model = tree_model.fit(X_train.values, y_train.values)\n\n    plt.subplot(3,2,inode+1)                                         # visualize, data, and decision tree regions and predictions\n    visualize_tree_model(tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,Xlabelunit,ylabelunit,annotate=False)   \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=3.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nleaf_nodes_viz = 2\n\ntree_model_viz = tree.DecisionTreeRegressor(max_leaf_nodes = leaf_nodes_viz).fit(X_train.values, y_train.values)\n\nfig = plt.figure(figsize=(10, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[1, 2])  # 1 row, 3 columns with 1:2 width ratio\n\nax1 = fig.add_subplot(gs[0])                         # visualize, data, and decision tree regions and predictions \nvisualize_tree_model(tree_model_viz,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n        y_train[yname],y_test[yname],1000,9000,'Decision Tree Model, Number of Leaf Nodes: ' + str(leaf_nodes),Xname,yname,\n        Xlabelunit,ylabelunit,annotate=False)   \n\nax2 = fig.add_subplot(gs[1:])                                  # visualize, data, and decision tree regions and predictions\n_ = tree.plot_tree(tree_model_viz,ax = ax2,feature_names=list(Xname),class_names=list(yname),filled=False,label='none',rounded=True,precision=0,\n                  proportion=True,max_depth=4,fontsize=15)\n\nplt.tight_layout()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\ntrees = []; MSE_CV = []; node_CV = []\n\ninode = 2\nwhile inode < len(X_train):                                   # loop over the hyperparameter, train with training and test with testing\n    tree_model = tree.DecisionTreeRegressor(min_samples_leaf=1,max_leaf_nodes=inode).fit(X_train.values, y_train.values)\n    trees.append(tree_model)\n    predict_train = tree_model.predict(np.c_[X_test[Xname[0]],X_test[Xname[1]]]) \n    MSE_CV.append(metrics.mean_squared_error(y_test[yname],predict_train))   \n    all_nodes = tree_model.tree_.node_count             \n    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes\n    node_CV.append(terminal_nodes); inode+=1\n\nplt.subplot(111)\nplt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,linewidths=0.3,\n            edgecolors=\"black\",zorder=20)\ntuned_node = node_CV[np.argmin(MSE_CV)]; max_MSE_CV = np.max(MSE_CV)\nplt.vlines(tuned_node,0,1.05*max_MSE_CV,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node),(tuned_node-2,3.5e5),rotation=90,zorder=30)\nplt.title('Decision Tree Cross Validation Testing Error vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); plt.ylabel('Mean Square Error')\nplt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_CV); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nMSE_kF = []; node_kF = []                                     # k-fold iteration code modified from StackOverFlow by Dimosthenis\n\ninode = 2\nwhile inode < len(X_train):\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode).fit(X_train.values, y_train.values)\n    scores = cross_val_score(estimator=tree_model, X= np.c_[df[Xname[0]],df[Xname[1]]],y=df[yname], cv=5, n_jobs=4,\n        scoring = \"neg_mean_squared_error\")                   # perform 4-fold cross validation\n    MSE_kF.append(abs(scores.mean()))\n    all_nodes = tree_model.tree_.node_count   \n    decision_nodes = len([x for x in tree_model.tree_.feature if x != _tree.TREE_UNDEFINED]); terminal_nodes = all_nodes - decision_nodes\n    node_kF.append(terminal_nodes); inode+=1\n\ntuned_node_kF = node_kF[np.argmin(MSE_kF)]; max_MSE_kF = np.max(MSE_kF)  \nplt.subplot(111)\nplt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)\nplt.scatter(node_kF,MSE_kF,s=None,c=\"red\",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,\n            linewidths=0.5, edgecolors=\"black\",zorder=40,label='k-Fold')\nplt.scatter(node_CV,MSE_CV,s=None,c='red',marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.4,linewidths=0.3,\n            edgecolors=\"black\",zorder=20,label='Cross Validation')\nplt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); \nplt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.legend(loc='upper right')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npruned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF)\npruned_tree_model = pruned_tree_model.fit(X, y)               # re-train\n\nplt.subplot(121)                                              # visualize, data, and decision tree regions and predictions\nvisualize_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model, Tuned Leaf Nodes: ' + str(tuned_node_kF),Xname,yname,\n                    Xlabelunit,ylabelunit) # plots the data points and the decision tree prediction \n\nplt.subplot(122)                                              # cross validation with conditional statistics plot\ncheck_tree_model(pruned_tree_model,X_train[Xname[0]],X_test[Xname[0]],X_train[Xname[1]],X_test[Xname[1]],Xmin,Xmax,\n                    y_train[yname],y_test[yname],ymin,ymax,'Decision Tree Model Cross Validation Plot, Tuned Leaf Nodes: ' + \n                    str(tuned_node_kF),)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.25, hspace=0.2); plt.show() \n```", "```py\nx1 = 7.0; x2 = 10.0                                          # the predictor feature values for the decision path\n\ndecision_path = pruned_tree_model.decision_path(np.c_[x1,x2])\nprint(decision_path) \n```", "```py\n (0, 0)\t1\n  (0, 1)\t1\n  (0, 3)\t1\n  (0, 13)\t1 \n```", "```py\ntree_to_code(pruned_tree_model, list(Xname))                  # convert a decision tree to Python code, nested if statements \n```", "```py\ndef tree(Por, Brittle):\n  if Por <= 14.789999961853027:\n    if Por <= 12.425000190734863:\n      if Por <= 8.335000038146973:\n        return [[1879.19091537]]\n      elif Por > 8.335000038146973\n        if Brittle <= 39.125:\n          return [[2551.00021508]]\n        elif Brittle > 39.125\n          return [[3369.12903299]]\n    elif Por > 12.425000190734863\n      if Brittle <= 39.26500129699707:\n        return [[3160.11022857]]\n      elif Brittle > 39.26500129699707\n        return [[4154.18334527]]\n  elif Por > 14.789999961853027\n    if Por <= 18.015000343322754:\n      if Brittle <= 33.25:\n        return [[3883.19381758]]\n      elif Brittle > 33.25\n        if Por <= 16.434999465942383:\n          return [[4544.69777089]]\n        elif Por > 16.434999465942383\n          return [[5240.84146117]]\n    elif Por > 18.015000343322754\n      if Brittle <= 31.5600004196167:\n        return [[4353.11874206]]\n      elif Brittle > 31.5600004196167\n        return [[5868.56369869]] \n```", "```py\nplt.subplot(111)                                              # plot the feature importance \nplt.title(\"Decision Tree Feature Importance\")\nplt.bar(Xlabel, pruned_tree_model.feature_importances_,edgecolor = 'black',\n       color=\"darkorange\",alpha = 0.6, align=\"center\")\nplt.xlim([-0.5,len(Xname)-0.5]); plt.ylim([0.,1.0])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1., top=0.8, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nfig = plt.figure(figsize=(15,10))\n\n_ = tree.plot_tree(pruned_tree_model,                         # plot the decision tree for model visualization\n                   feature_names=list(Xname),  \n                   class_names=list(yname),\n                   filled=True) \n```", "```py\nfrom sklearn import tree                                      # import decision tree from scikit-learn\nXname = ['Por','Brittle']; yname='Production'                 # predictor features and response feature\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\nmy_data = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load subsurface data table\nmy_tree = tree.DecisionTreeRegressor(max_leaf_nodes=26)       # instantiate tree with hyperparameters\nmy_tree = my_tree.fit(X.values,y.values)                      # train tree with training data\nestimate = my_tree.predict([[x1,x2]])[0]                      # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD \n```", "```py\npipe_tree = Pipeline([                                        # the machine learning workflow as a pipeline object\n\n    ('tree', tree.DecisionTreeRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'tree__max_leaf_nodes': np.arange(2,len(X),1,dtype = int),\n}\n\nKF_tuned_tree = GridSearchCV(pipe_tree,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             cv=KFold(n_splits=5,shuffle=False),refit = True)\nKF_tuned_tree.fit(X,y)                                        # tune and train the model\n\nprint('Tuned hyperparameter: max_leaf_nodes = ' + str(KF_tuned_tree.best_params_))\n\nestimate = KF_tuned_tree.predict([[x1,x2]])[0]                # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nTuned hyperparameter: max_leaf_nodes = {'tree__max_leaf_nodes': 10}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1879.2 MCFPD \n```", "```py\nidata = 2                                                    # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nMSE_kF = []; node_kF = []                                     \nkf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification \n\ninode = 2\nwhile inode < len(X_train):\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)\n    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = \"neg_mean_squared_error\") # perform 5-fold cross validation\n    MSE_kF.append(abs(scores.mean()))\n    node_kF.append(inode); inode+=1\n\ntuned_node_kF = node_kF[np.argmin(MSE_kF)]\ntuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data\n\nplt.subplot(121)\nplt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)\nplt.scatter(node_kF,MSE_kF,s=None,c=\"red\",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,\n            linewidths=0.5, edgecolors=\"black\",zorder=40,label='k-Fold')\nplt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); \nplt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.legend(loc='upper right')\n\ny_hat = tuned_tree_model.predict(X)\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Decision Tree, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nidata = 2                                                    # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nMSE_kF = []; node_kF = []                                     \nkf = KFold(n_splits=5, shuffle=True, random_state=seed)       # k-fold specification \n\ninode = 2\nwhile inode < len(X_train):\n    tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=inode,random_state=seed)\n    scores = cross_val_score(estimator=tree_model,X=X,y=y,cv=kf,n_jobs=4,scoring = \"neg_mean_squared_error\") # perform 5-fold cross validation\n    MSE_kF.append(abs(scores.mean()))\n    node_kF.append(inode); inode+=1\n\ntuned_node_kF = node_kF[np.argmin(MSE_kF)]\ntuned_tree_model = tree.DecisionTreeRegressor(max_leaf_nodes=tuned_node_kF).fit(X.values, y.values) # retrain on all the data\n\nplt.subplot(121)\nplt.vlines(tuned_node_kF,0,1.05*max_MSE_kF,lw=1.0,ls='--',color='red',zorder=10)\nplt.annotate('Tuned Max Nodes = ' + str(tuned_node_kF),(tuned_node_kF-2,3.5e5),rotation=90,zorder=30)\nplt.scatter(node_kF,MSE_kF,s=None,c=\"red\",marker=None,cmap=None,norm=None,vmin=None,vmax=None,alpha=0.8,\n            linewidths=0.5, edgecolors=\"black\",zorder=40,label='k-Fold')\nplt.title('Decision Tree k-Fold Cross Validation Error (MSE) vs. Complexity'); plt.xlabel('Number of Terminal Nodes'); \nplt.ylabel('Mean Square Error'); plt.xlim(0,len(X_train)); plt.ylim(0,1.05*max_MSE_kF); add_grid()\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.legend(loc='upper right')\n\ny_hat = tuned_tree_model.predict(X)\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Decision Tree, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```"]