- en: Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的AI
- en: '*DALL·E 3 Prompt: Illustration of responsible AI in a futuristic setting with
    the universe in the backdrop: A human hand or hands nurturing a seedling that
    grows into an AI tree, symbolizing a neural network. The tree has digital branches
    and leaves, resembling a neural network, to represent the interconnected nature
    of AI. The background depicts a future universe where humans and animals with
    general intelligence collaborate harmoniously. The scene captures the initial
    nurturing of the AI as a seedling, emphasizing the ethical development of AI technology
    in harmony with humanity and the universe.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：描绘一个未来场景中的负责任AI插图，背景是宇宙：一只或几只人类手在培育一株幼苗，这株幼苗长成了一棵AI树，象征着神经网络。树上有数字分支和叶子，类似于神经网络，以表示AI的互联性。背景描绘了一个未来宇宙，其中人类和具有通用智能的动物和谐合作。场景捕捉了AI作为幼苗的初始培育，强调AI技术与人类和宇宙的和谐发展。*'
- en: '![](../media/file289.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file289.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why have responsible AI practices evolved from optional ethical considerations
    into mandatory engineering requirements that determine system reliability, legal
    compliance, and commercial viability?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么负责任的AI实践已经从可选的伦理考量转变为强制性的工程要求，这些要求决定了系统的可靠性、法律合规性和商业可行性？*'
- en: Machine learning systems deployed in real-world environments face stringent
    reliability requirements extending beyond algorithmic accuracy. Biased predictions
    trigger legal liability, opaque decision-making prevents regulatory approval,
    unaccountable systems fail audits, and unexplainable outputs undermine user trust.
    These operational realities transform responsible AI from philosophical ideals
    into concrete engineering constraints determining whether systems can be deployed,
    maintained, and scaled in production environments. Responsible AI practices provide
    systematic methodologies for building robust systems meeting regulatory requirements,
    passing third-party audits, maintaining user confidence, and operating reliably
    across diverse populations and contexts. Modern ML engineers must integrate bias
    detection, explainability mechanisms, accountability frameworks, and oversight
    systems as core architectural components. Understanding responsible AI as an engineering
    discipline enables building systems achieving both technical performance and operational
    sustainability in increasingly regulated deployment environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界环境中部署的机器学习系统面临严格的可靠性要求，这些要求超出了算法准确性。有偏见的预测会引发法律责任，不透明的决策会阻止监管批准，不可问责的系统会通不过审计，不可解释的输出会损害用户信任。这些运营现实将负责任的AI从哲学理想转变为具体的工程约束，决定了系统是否可以在生产环境中部署、维护和扩展。负责任的AI实践提供了系统性的方法，用于构建符合监管要求、通过第三方审计、保持用户信心并在不同人群和环境中可靠运行的系统。现代机器学习工程师必须将偏差检测、可解释性机制、问责制框架和监督系统作为核心架构组件进行整合。将负责任的AI视为一个工程学科，能够构建在日益监管的部署环境中既实现技术性能又具有运营可持续性的系统。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Define fairness, transparency, accountability, privacy, and safety as measurable
    engineering requirements that constrain system architecture, data handling, and
    deployment decisions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将公平性、透明度、问责制、隐私和安全定义为可衡量的工程要求，这些要求限制了系统架构、数据处理和部署决策
- en: Implement bias detection techniques using tools like Fairlearn to identify disparities
    in model performance across demographic groups and evaluate fairness metrics including
    demographic parity and equalized odds
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fairlearn等工具实施偏差检测技术，以识别模型性能在不同人口群体中的差异，并评估包括人口比例和均衡机会在内的公平性指标
- en: Apply privacy preservation methods including differential privacy, federated
    learning, and machine unlearning to protect sensitive data while maintaining model
    utility
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用隐私保护方法，包括差分隐私、联邦学习和机器反学习，以保护敏感数据同时保持模型效用
- en: Generate explanations for model predictions using post-hoc techniques such as
    SHAP, LIME, and GradCAM, while evaluating their computational costs and reliability
    for different model architectures
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SHAP、LIME和GradCAM等后处理技术生成模型预测的解释，同时评估它们在不同模型架构中的计算成本和可靠性
- en: Analyze how deployment contexts (cloud, edge, mobile, TinyML) impose architectural
    constraints that limit which responsible AI protections are technically feasible
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析部署环境（云、边缘、移动、TinyML）如何施加架构限制，限制了哪些负责任的AI保护在技术上可行
- en: Evaluate tradeoffs between competing responsible AI principles, recognizing
    when mathematical impossibility theorems prevent simultaneous optimization of
    all fairness criteria
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估相互竞争的负责任的AI原则之间的权衡，认识到当数学不可能定理阻止同时优化所有公平性标准时
- en: Design organizational structures and governance processes that translate responsible
    AI principles into operational practices, including role definitions, escalation
    pathways, and accountability mechanisms
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计组织结构和治理流程，将负责任的AI原则转化为操作实践，包括角色定义、升级路径和问责机制
- en: Assess computational overhead of responsible AI techniques and identify resource
    barriers to implementation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估负责任的AI技术的计算开销，并确定实施资源障碍
- en: Introduction to Responsible AI
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任的AI简介
- en: In 2019, Amazon scrapped a hiring algorithm trained on historical resume data
    after discovering it systematically penalized female candidates([Dastin 2022](ch058.xhtml#ref-dastin2018amazon)).
    While the system appeared technically sophisticated, it had learned that past
    successful applicants were predominantly male, reflecting historical gender bias
    rather than merit-based qualifications. The model was statistically optimal yet
    ethically disastrous, demonstrating that technical correctness can coexist with
    profound social harm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，亚马逊在发现其基于历史简历数据的招聘算法系统地惩罚女性候选人后，取消了该算法([Dastin 2022](ch058.xhtml#ref-dastin2018amazon))。虽然该系统在技术上看似复杂，但它已经学会了过去成功的申请者主要是男性，这反映了历史性别偏见，而不是基于能力的资格。该模型在统计上是最优的，但在伦理上却是灾难性的，这表明技术正确性可以与技术严重的社会危害共存。
- en: 'This incident illustrates the central challenge of responsible AI: systems
    can be algorithmically sound while perpetuating injustice, optimizing objectives
    while undermining values, and satisfying performance benchmarks while failing
    society. The problem extends beyond individual bias to encompass systemic questions
    about transparency, accountability, privacy, and safety in systems affecting billions
    of lives daily.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个事件说明了负责任的AI的核心挑战：系统可能在算法上合理，同时持续加剧不公，优化目标的同时破坏价值观，满足性能基准的同时却未能满足社会需求。这个问题不仅超越了个人偏见，还涵盖了影响数十亿人日常生活的系统中的透明度、问责制、隐私和安全等系统性问题。
- en: 'The discipline of machine learning systems engineering has evolved to address
    this critical juncture where technical excellence intersects with profound societal
    implications. The algorithmic foundations from [Chapter 3](ch009.xhtml#sec-dl-primer),
    optimization techniques from [Chapter 8](ch014.xhtml#sec-ai-training), and deployment
    architectures from [Chapter 2](ch008.xhtml#sec-ml-systems) establish the computational
    infrastructure necessary for systems of extraordinary capability and reach. However,
    as these systems assume increasingly consequential roles in healthcare diagnosis,
    judicial decision-making, employment screening, and financial services, the sufficiency
    of technical performance metrics alone comes into question. Contemporary machine
    learning systems create a fundamental challenge: they may achieve optimal statistical
    performance while producing outcomes that conflict with fairness, transparency,
    and social justice.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统工程的学科已经发展到解决这一关键转折点，即技术卓越与社会深远影响相交。从[第3章](ch009.xhtml#sec-dl-primer)的算法基础，从[第8章](ch014.xhtml#sec-ai-training)的优化技术，以及从[第2章](ch008.xhtml#sec-ml-systems)的部署架构，建立了为具有非凡能力和范围的系统所需的计算基础设施。然而，随着这些系统在医疗诊断、司法决策、就业筛选和金融服务中承担越来越重要的角色，仅仅技术性能指标是否足够的问题引起了质疑。当代机器学习系统提出了一个基本挑战：它们可能在统计性能上达到最优，同时产生与公平性、透明度和社会正义相冲突的结果。
- en: 'This chapter begins Part V: Trustworthy Systems by expanding our analytical
    framework from technical correctness to include the normative question of whether
    systems merit societal trust and acceptance. The progression from resilient systems
    establishes an important distinction: resilient AI addresses threats to system
    integrity through adversarial attacks and hardware failures, while responsible
    AI ensures that properly functioning systems generate outcomes consistent with
    human values and collective welfare.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从第五部分“可信系统”开始，通过扩展我们的分析框架，从技术正确性扩展到包括系统是否值得社会信任和接受这一规范性问题。从弹性系统的发展中建立了一个重要的区别：弹性AI通过对抗性攻击和硬件故障来应对系统完整性的威胁，而负责任的AI确保正常运行的系统产生符合人类价值观和集体福祉的结果。
- en: The discipline addressing this challenge transforms abstract ethical principles
    into concrete engineering constraints and design requirements. Similar to how
    security protocols require specific architectural decisions and monitoring infrastructure,
    responsible AI requires implementing fairness, transparency, and accountability
    through quantifiable technical mechanisms and verifiable system properties. This
    represents an expansion of engineering methodology to incorporate normative requirements
    as first-class design considerations, not merely applying philosophical concepts
    to engineering practice.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这一挑战的学科将抽象的伦理原则转化为具体的技术约束和设计要求。类似于安全协议需要特定的架构决策和监控基础设施，负责任的AI需要通过可量化的技术机制和可验证的系统属性来实施公平性、透明度和问责制。这代表了工程方法论的扩展，将规范性要求作为一级设计考虑因素，而不仅仅是将哲学概念应用于工程实践。
- en: Software engineering provides precedent for this disciplinary evolution. Early
    computational systems prioritized functional correctness, focusing on whether
    programs generated accurate outputs for given inputs. As systems increased in
    complexity and societal integration, the field developed methodologies for reliability
    engineering, security assurance, and maintainability analysis. Contemporary responsible
    AI practices represent parallel disciplinary maturation, extending systematic
    engineering approaches to include the social and ethical dimensions of algorithmic
    decision-making.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程为此学科的发展提供了先例。早期的计算系统优先考虑功能正确性，关注程序是否为给定的输入生成准确的输出。随着系统复杂性和社会整合度的提高，该领域发展了可靠性工程、安全保证和可维护性分析的方法。当代负责任的AI实践代表了平行的学科成熟，将系统化的工程方法扩展到包括算法决策的社会和伦理维度。
- en: This extension reflects the unprecedented scale of contemporary machine learning
    deployment. These systems now mediate decisions affecting billions of individuals
    across domains including credit allocation, medical diagnosis, educational assessment,
    and criminal justice proceedings. Unlike conventional software failures that manifest
    as system crashes or data corruption, responsible AI failures can perpetuate systemic
    discrimination, compromise democratic institutions, and erode public confidence
    in beneficial technologies. The field requires systems that demonstrate technical
    proficiency alongside ethical accountability and social responsibility.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种扩展反映了当代机器学习部署前所未有的规模。这些系统现在调解着影响数十亿人的决策，包括信贷分配、医疗诊断、教育评估和刑事司法程序。与表现为系统崩溃或数据损坏的传统软件故障不同，负责任的AI故障可能会持续系统性的歧视，损害民主机构，并侵蚀公众对有益技术的信心。该领域需要展示技术专业性和伦理问责制以及社会责任的系统。
- en: '***Responsible AI*** is the engineering discipline that systematically transforms
    *ethical principles* into *concrete design requirements* and *measurable system
    properties*, establishing them as *first-class constraints* in machine learning
    systems development.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**负责任的AI**是系统工程学科，系统地转化**伦理原则**为**具体的设计要求**和**可衡量的系统属性**，将其确立为机器学习系统开发中的**一级约束**。'
- en: Responsible AI constitutes a systematic engineering discipline with four interconnected
    dimensions. This chapter examines how ethical principles translate into measurable
    system requirements, analyzes technical methods for detecting and mitigating harmful
    algorithmic behaviors, explains why responsible AI extends beyond individual systems
    to include broader sociotechnical dynamics, and addresses practical implementation
    challenges within organizational and regulatory contexts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI构成一个具有四个相互关联维度的系统化工程学科。本章探讨了伦理原则如何转化为可衡量的系统需求，分析了检测和减轻有害算法行为的技术方法，解释了为什么负责任的AI不仅限于个别系统，还包括更广泛的社会技术动态，并针对组织和管理环境中的实际实施挑战进行了讨论。
- en: Students must develop both technical competency and contextual understanding.
    Students will learn to implement bias detection algorithms and privacy preservation
    mechanisms while understanding why technical solutions require organizational
    governance structures and stakeholder engagement processes. This covers methodologies
    for enhancing system explainability and accountability while examining tensions
    between competing normative values that no algorithmic approach can definitively
    resolve.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 学生必须培养技术能力和情境理解。学生将学习实施偏差检测算法和隐私保护机制，同时理解为什么技术解决方案需要组织治理结构和利益相关者参与流程。这包括增强系统可解释性和问责制的方法，同时考察无法由算法方法明确解决的竞争性规范性价值之间的紧张关系。
- en: The chapter develops the analytical framework necessary for engineering systems
    that simultaneously address immediate functional requirements and long term societal
    considerations. This framework treats responsible AI not as supplementary constraints
    applied to existing systems, but as fundamental principles integral to sound engineering
    practice in contemporary artificial intelligence development.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本章构建了同时解决即时功能需求和长期社会考量的工程系统所需的解析框架。这个框架将负责任的AI视为不是应用于现有系统的补充约束，而是当代人工智能发展中良好工程实践的基本原则。
- en: '**Navigating This Chapter**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**导航本章**'
- en: 'Responsible AI approaches from four complementary perspectives, each essential
    for building trustworthy ML systems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从四个互补的视角探讨负责任的AI方法，每个视角对于构建可信赖的机器学习系统都是必不可少的：
- en: '**1\. Principles and Foundations** ([Section 17.2](ch023.xhtml#sec-responsible-ai-core-principles-1bd7)
    through [Section 17.4](ch023.xhtml#sec-responsible-ai-responsible-ai-across-deployment-environments-e828)):
    Defines the objectives responsible AI systems should achieve. Introduces fairness,
    transparency, accountability, privacy, and safety as engineering requirements.
    Examines how these principles manifest differently across cloud, edge, mobile,
    and TinyML deployments, revealing tensions between ideals and operational constraints.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 原则与基础** ([第17.2节](ch023.xhtml#sec-responsible-ai-core-principles-1bd7)
    至 [第17.4节](ch023.xhtml#sec-responsible-ai-responsible-ai-across-deployment-environments-e828))：定义了负责任AI系统应实现的目标。介绍了公平性、透明度、问责制、隐私和安全作为工程要求。考察了这些原则如何在云、边缘、移动和TinyML部署中有所不同，揭示了理想与运营约束之间的紧张关系。'
- en: '**2\. Technical Implementation** ([Section 17.5](ch023.xhtml#sec-responsible-ai-technical-foundations-3436)):
    Presents concrete techniques that enable responsible AI. Covers detection methods
    for identifying bias and drift, mitigation techniques including privacy preservation
    and adversarial defenses, and validation approaches for explainability and monitoring.
    These methods operationalize abstract principles into measurable system behaviors.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 技术实施** ([第17.5节](ch023.xhtml#sec-responsible-ai-technical-foundations-3436))：介绍使负责任的AI成为可能的具体技术。包括识别偏差和漂移的检测方法、包括隐私保护和对抗性防御在内的缓解技术，以及用于可解释性和监控的验证方法。这些方法将抽象原则转化为可衡量的系统行为。'
- en: '**3\. Sociotechnical Dynamics** ([Section 17.6](ch023.xhtml#sec-responsible-ai-sociotechnical-dynamics-4938)):
    Demonstrates why technical correctness alone is insufficient. Examines feedback
    loops between systems and environments, human-AI collaboration challenges, competing
    stakeholder values, contestability mechanisms, and institutional governance structures.
    Responsible AI exists at the intersection of algorithms, organizations, and society.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 社会技术动态** ([第17.6节](ch023.xhtml#sec-responsible-ai-sociotechnical-dynamics-4938))：说明仅技术正确性是不够的。考察系统与环境之间的反馈循环、人机协作挑战、竞争性利益相关者价值、可争议机制和制度治理结构。负责任的AI存在于算法、组织和社会的交汇点。'
- en: '**4\. Implementation Realities** ([Section 17.7](ch023.xhtml#sec-responsible-ai-implementation-challenges-9173)
    through [Section 17.8](ch023.xhtml#sec-responsible-ai-ai-safety-value-alignment-8c93)):
    Examines how principles translate to practice. Addresses organizational barriers,
    data quality constraints, competing objectives, scalability challenges, and evaluation
    gaps. Concludes with AI safety and value alignment considerations for autonomous
    systems.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 实施现实** ([第17.7节](ch023.xhtml#sec-responsible-ai-implementation-challenges-9173)
    至 [第17.8节](ch023.xhtml#sec-responsible-ai-ai-safety-value-alignment-8c93))：考察原则如何转化为实践。解决组织障碍、数据质量约束、竞争性目标、可扩展性挑战和评估差距。以自主系统的AI安全和价值对齐考虑作为结论。'
- en: The chapter is comprehensive because responsible AI touches engineering, ethics,
    policy, and organizational design. Use the section structure to navigate to topics
    most relevant to your immediate needs, but recognize that effective responsible
    AI implementation requires integrating all four perspectives. Technical solutions
    alone cannot resolve value conflicts; ethical principles without technical implementation
    remain aspirational; and individual interventions fail without organizational
    support.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容全面，因为负责任的人工智能涉及工程、伦理、政策和组织设计。使用章节结构导航到与您当前需求最相关的主题，但请认识到，有效的负责任人工智能实施需要整合所有四个视角。仅技术解决方案无法解决价值冲突；没有技术实施的伦理原则仍然只是愿望；没有组织支持的个人干预将失败。
- en: These principles and practices establish the foundation for building AI systems
    that serve both current needs and long term societal wellbeing. By treating fairness,
    transparency, accountability, privacy, and safety as engineering requirements
    rather than afterthoughts, practitioners develop the technical skills and organizational
    approaches necessary to ensure ML systems benefit society while minimizing harm.
    This systematic approach to responsible AI transforms abstract ethical principles
    into concrete design constraints that guide every stage of the machine learning
    lifecycle.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则和实践为构建既满足当前需求又促进长期社会福祉的人工智能系统奠定了基础。通过将公平、透明度、问责制、隐私和安全视为工程要求而不是事后考虑，从业者发展了确保机器学习系统造福社会同时最小化危害所需的技术技能和组织方法。这种负责任人工智能的系统方法将抽象的伦理原则转化为具体的设计约束，指导机器学习生命周期的每个阶段。
- en: Core Principles
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核心原则
- en: Responsible AI refers to the development and deployment of machine learning
    systems that intentionally uphold ethical principles and promote socially beneficial
    outcomes. These principles serve not only as policy ideals but as concrete constraints
    on system design, implementation, and governance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能指的是开发和部署旨在维护伦理原则并促进社会有益成果的机器学习系统。这些原则不仅作为政策理想，而且作为对系统设计、实施和治理的具体约束。
- en: Fairness refers to the expectation that machine learning systems do not discriminate
    against individuals or groups on the basis of protected attributes[1](#fn1) such
    as race, gender, or socioeconomic status. This principle encompasses both statistical
    metrics and broader normative concerns about equity, justice, and structural bias.
    Formal mathematical definitions of fairness criteria are examined in detail in
    [Section 17.3.2](ch023.xhtml#sec-responsible-ai-fairness-machine-learning-2ba4).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指的是机器学习系统不因种族、性别或社会经济地位等受保护属性[1](#fn1)歧视个人或群体的期望。这一原则既包括统计指标，也包括更广泛的关于公平、正义和结构偏见的规范性关切。公平性标准的正式数学定义在[第17.3.2节](ch023.xhtml#sec-responsible-ai-fairness-machine-learning-2ba4)中进行了详细探讨。
- en: The computational resource requirements for implementing responsible AI systems
    create significant equity considerations that extend beyond individual system
    design. These challenges encompass both access barriers and environmental justice
    concerns examined in deployment constraints and implementation barriers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实施负责任人工智能系统的计算资源需求产生了重大的公平性考虑，这些考虑超越了个人系统设计。这些挑战包括部署限制和实施障碍中考察的进入障碍和环境正义关切。
- en: Explainability concerns the ability of stakeholders to interpret how a model
    produces its outputs. This involves understanding both how individual decisions
    are made and the model’s overall behavior patterns. Explanations may be generated
    after a decision is made (called post hoc explanations[2](#fn2)) to detail the
    reasoning process, or they may be built into the model’s design for transparent
    operation. The neural network architectures discussed in [Chapter 4](ch010.xhtml#sec-dnn-architectures)
    vary significantly in their inherent interpretability, with deeper networks generally
    being more difficult to explain. Explainability is important for error analysis,
    regulatory compliance, and building user trust.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性涉及利益相关者解释模型如何产生其输出的能力。这包括理解单个决策是如何做出的以及模型的整体行为模式。解释可以在决策之后生成（称为事后解释[2](#fn2)）以详细说明推理过程，或者它们可以内置到模型的设计中以实现透明操作。在第4章（ch010.xhtml#sec-dnn-architectures）中讨论的神经网络架构在固有的可解释性方面存在显著差异，深度网络通常更难以解释。可解释性对于错误分析、合规性和建立用户信任至关重要。
- en: Transparency refers to openness about how AI systems are built, trained, validated,
    and deployed. It includes disclosure of data sources, design assumptions, system
    limitations, and performance characteristics. While explainability focuses on
    understanding outputs, transparency addresses the broader lifecycle of the system.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度指的是关于人工智能系统构建、训练、验证和部署的开放性。它包括数据来源、设计假设、系统限制和性能特征的披露。虽然可解释性侧重于理解输出，但透明度涉及系统的更广泛生命周期。
- en: Accountability denotes the mechanisms by which individuals or organizations
    are held responsible for the outcomes of AI systems. It involves traceability,
    documentation, auditing, and the ability to remedy harms. Accountability ensures
    that AI failures are not treated as abstract malfunctions but as consequences
    with real-world impact.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 问责制指的是个人或组织因人工智能系统的结果而承担责任的方式。它涉及可追溯性、文档记录、审计和纠正伤害的能力。问责制确保人工智能故障不被视为抽象的故障，而是具有现实世界影响的后果。
- en: Value alignment[3](#fn3) is the principle that AI systems should pursue goals
    that are consistent with human intent and ethical norms. In practice, this involves
    both technical challenges, including reward design and constraint specification,
    and broader questions about whose values are represented and enforced.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 价值一致性[3](#fn3) 是指人工智能系统应追求与人类意图和道德规范一致的目标。在实践中，这涉及技术挑战，包括奖励设计和约束规范，以及更广泛的问题，即哪些价值观被代表和执行。
- en: Human oversight emphasizes the role of human judgment in supervising, correcting,
    or halting automated decisions. This includes humans-in-the-loop[4](#fn4) during
    operation, as well as organizational structures that ensure AI use remains accountable
    to societal values and real-world complexity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人类监督强调人类判断在监督、纠正或停止自动化决策中的作用。这包括操作期间的人类在回路[4](#fn4)，以及确保人工智能使用始终符合社会价值观和现实世界复杂性的组织结构。
- en: Other important principles such as privacy and robustness require specialized
    technical implementations that intersect with security and reliability considerations
    throughout system design.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其他重要原则，如隐私和鲁棒性，需要专门的技術实现，这些实现与系统设计中的安全和可靠性考虑相交。
- en: Integrating Principles Across the ML Lifecycle
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在机器学习生命周期中整合原则
- en: Responsible machine learning begins with a set of foundational principles, including
    fairness, transparency, accountability, privacy, and safety, that define what
    it means for an AI system to behave ethically and predictably. These principles
    are not abstract ideals or afterthoughts; they must be translated into concrete
    constraints that guide how models are trained, evaluated, deployed, and maintained.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习始于一组基础原则，包括公平性、透明度、问责制、隐私和安全，这些原则定义了人工智能系统如何以道德和可预测的方式行为。这些原则不是抽象的理想或事后想法；它们必须转化为具体的约束，以指导模型的训练、评估、部署和维护。
- en: Implementing these principles in practice requires understanding how each sets
    specific expectations for system behavior. Fairness addresses how models treat
    different subgroups and respond to historical biases. Explainability ensures that
    model decisions can be understood by developers, auditors, and end users. Privacy
    governs what data is collected and how it is used. Accountability defines how
    responsibilities are assigned, tracked, and enforced throughout the system lifecycle.
    Safety requires that models behave reliably even in uncertain or shifting environments.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中实施这些原则需要理解每个原则如何为系统行为设定具体期望。公平性关注模型如何对待不同的子群体以及如何应对历史偏见。可解释性确保模型决策可以被开发者、审计员和最终用户理解。隐私管理收集的数据以及如何使用这些数据。问责制定义了在整个系统生命周期中如何分配、跟踪和执行责任。安全性要求模型即使在不确定或不断变化的环境中也能可靠地运行。
- en: 'Table 17.1: **Responsible AI Lifecycle**: Embedding fairness, explainability,
    privacy, accountability, and robustness throughout the ML system lifecycle, from
    data collection to monitoring, ensures these principles become architectural commitments
    rather than post hoc considerations. The table maps these principles to specific
    development phases, revealing how proactive integration addresses potential risks
    and promotes trustworthy AI systems.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.1：**负责任的人工智能生命周期**：在整个机器学习系统生命周期中嵌入公平性、可解释性、隐私、问责制和鲁棒性，从数据收集到监控，确保这些原则成为架构承诺而不是事后考虑。该表将这些原则映射到特定的开发阶段，揭示了主动集成如何解决潜在风险并促进可信赖的人工智能系统。
- en: '| **Principle** | **Data Collection** | **Model Training** | **Evaluation**
    | **Deployment** | **Monitoring** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **原则** | **数据收集** | **模型训练** | **评估** | **部署** | **监控** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Fairness** | Representative sampling | Bias-aware algorithms | Group-level
    metrics | Threshold adjustment | Subgroup performance |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **公平性** | 代表性抽样 | 偏差感知算法 | 组级指标 | 阈值调整 | 子组性能 |'
- en: '| **Explainability** | Documentation standards | Interpretable architecture
    | Model behavior analysis | User-facing explanations | Explanation quality logs
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **可解释性** | 文档标准 | 可解释架构 | 模型行为分析 | 面向用户的解释 | 解释质量日志 |'
- en: '| **Transparency** | Data source tracking | Training documentation | Performance
    reporting | Model cards | Change tracking |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **透明度** | 数据源跟踪 | 训练文档 | 性能报告 | 模型卡片 | 变更跟踪 |'
- en: '| **Privacy** | Consent mechanisms | Privacy-preserving methods | Privacy impact
    assessment | Secure deployment | Access audit logs |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **隐私** | 同意机制 | 隐私保护方法 | 隐私影响评估 | 安全部署 | 访问审计日志 |'
- en: '| **Accountability** | Governance frameworks | Decision logging | Audit trail
    creation | Override mechanisms | Incident tracking |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **问责制** | 管理框架 | 决策日志 | 审计跟踪创建 | 覆盖机制 | 事件跟踪 |'
- en: '| **Robustness** | Quality assurance | Robust training methods | Stress testing
    | Failure handling | Performance monitoring |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **鲁棒性** | 质量保证 | 鲁棒训练方法 | 压力测试 | 故障处理 | 性能监控 |'
- en: 'These principles work in concert to define what it means for a machine learning
    system to behave responsibly, not as isolated features but as system-level constraints
    that are embedded across the lifecycle. [Table 17.1](ch023.xhtml#tbl-principles-lifecycle)
    provides a structured view of how key principles, including fairness, explainability,
    transparency, privacy, accountability, and robustness, map to the major phases
    of ML system development: data collection, model training, evaluation, deployment,
    and monitoring. Some principles (like fairness and privacy) begin with data, while
    others (like robustness and accountability) become most important during deployment
    and oversight. Explainability, though often emphasized during evaluation and user
    interaction, also supports model debugging and design-time validation. This comprehensive
    mapping reinforces that responsible AI is not a post hoc consideration but a multiphase
    architectural commitment.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则共同定义了机器学习系统如何表现出负责任的行为，不是作为孤立的功能，而是作为嵌入在整个生命周期中的系统级约束。[表17.1](ch023.xhtml#tbl-principles-lifecycle)提供了一个结构化的视角，展示了关键原则，包括公平性、可解释性、透明度、隐私、问责制和鲁棒性，如何映射到ML系统开发的各个主要阶段：数据收集、模型训练、评估、部署和监控。一些原则（如公平性和隐私）从数据开始，而其他原则（如鲁棒性和问责制）在部署和监督期间最为重要。尽管可解释性通常在评估和用户交互期间被强调，但它也支持模型调试和设计时验证。这种全面的映射强化了负责任的AI不是事后考虑，而是一个多阶段架构承诺的观点。
- en: Resource Requirements and Equity Implications
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源需求和公平影响
- en: Implementing responsible AI principles requires computational resources that
    vary significantly across techniques and deployment contexts. These resource requirements
    create multifaceted equity considerations that extend beyond individual organizations
    to encompass broader social and environmental justice concerns. Organizations
    with limited computing budgets may be unable to implement comprehensive responsible
    AI protections, potentially creating disparate access to ethical safeguards. State-of-the-art
    AI systems increasingly require specialized hardware and high-bandwidth connectivity
    that systematically exclude rural communities, developing regions, and resource-constrained
    users from accessing advanced AI capabilities.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实施负责任的AI原则需要计算资源，这些资源在技术和部署环境中差异很大。这些资源需求产生了多方面的公平考虑，不仅超越了单个组织，还涵盖了更广泛的社会和环境正义问题。计算预算有限的组织可能无法实施全面的负责任AI保护，这可能导致对道德保障的访问不平等。最先进的AI系统越来越需要专用硬件和高带宽连接，这系统地排除了农村社区、发展中国家和资源受限用户访问先进AI能力。
- en: 'Environmental justice concerns compound these access barriers through the engineering
    reality that responsible AI techniques impose significant energy costs. Training
    differential privacy models requires 15-30% additional compute cycles; real time
    fairness monitoring adds 10-20 ms latency and continuous CPU overhead; SHAP explanations
    demand 50-1000x normal inference compute. These computational requirements translate
    directly into infrastructure demands: a high-traffic system serving responsible
    AI features to 10 million users requires substantial additional datacenter capacity
    compared to unconstrained models.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 环境正义问题通过工程现实加剧了这些访问障碍，即负责任的AI技术需要巨大的能源成本。训练差分隐私模型需要额外的15-30%计算周期；实时公平监控增加了10-20毫秒的延迟和持续的CPU开销；SHAP解释需要50-1000倍的正常推理计算。这些计算需求直接转化为基础设施需求：一个为1000万用户提供负责任AI功能的繁忙系统，与不受限制的模型相比，需要大量的数据中心容量。
- en: The geographic distribution of this computational infrastructure creates systematic
    inequities that engineers must consider in system design. Data centers supporting
    AI workloads concentrate in regions with low electricity costs and favorable regulations,
    areas that often correlate with lower-income communities that experience increased
    pollution, heat generation, and electrical grid strain while frequently lacking
    the high-bandwidth connectivity needed to access the AI services these facilities
    enable. This creates a feedback loop where computational equity depends not only
    on algorithmic design but on infrastructure placement decisions that affect both
    system performance and community welfare. The detailed performance characteristics
    of specific techniques are examined in [Section 17.5](ch023.xhtml#sec-responsible-ai-technical-foundations-3436).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算基础设施的地理分布创造了工程师在设计系统时必须考虑的系统不平等。支持人工智能工作负载的数据中心集中在电力成本较低且法规有利的地区，这些地区通常与低收入社区相关，这些社区在污染、热量产生和电网压力增加的同时，往往缺乏访问这些设施所提供的AI服务所需的高带宽连接。这形成了一个反馈循环，其中计算公平不仅取决于算法设计，还取决于影响系统性能和社区福利的基础设施位置决策。具体技术的详细性能特征在[第17.5节](ch023.xhtml#sec-responsible-ai-technical-foundations-3436)中进行了考察。
- en: Transparency and Explainability
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 透明性和可解释性
- en: This section examines specific principles in detail. Machine learning systems
    are frequently criticized for their lack of interpretability. In many cases, models
    operate as opaque “black boxes,” producing outputs that are difficult for users,
    developers, and regulators to understand or scrutinize. This opacity presents
    a significant barrier to trust, particularly in high stakes domains such as criminal
    justice, healthcare, and finance, where accountability and the right to recourse
    are important. For example, the [COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx)
    algorithm, used in the United States to assess recidivism risk, was found to exhibit
    racial bias[5](#fn5). However, the proprietary nature of the system, combined
    with limited access to interpretability tools, hindered efforts to investigate
    or address the issue.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细探讨了具体原则。机器学习系统经常因缺乏可解释性而受到批评。在许多情况下，模型作为不透明的“黑箱”运行，产生用户、开发者和监管者难以理解或审查的输出。这种不透明性构成了信任的巨大障碍，尤其是在高风险领域，如刑事司法、医疗保健和金融，在这些领域，问责制和申诉权非常重要。例如，在美国用于评估再犯风险的[COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx)算法被发现存在种族偏见[5](#fn5)。然而，系统的专有性质以及有限的解释性工具访问权限阻碍了调查或解决该问题的努力。
- en: Explainability is the capacity to understand how a model produces its predictions.
    It includes both *local explanations*[6](#fn6), which clarify individual predictions,
    and *global explanations*[7](#fn7), which describe the models general behavior.
    Transparency, by contrast, encompasses openness about the broader system design
    and operation. This includes disclosure of data sources, feature engineering[8](#fn8),
    model architectures, training procedures, evaluation protocols, and known limitations.
    Transparency also involves documentation of intended use cases, system boundaries,
    and governance structures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是指理解模型如何产生预测的能力。它包括两种解释：*局部解释*[6](#fn6)，它阐明个别预测；以及*全局解释*[7](#fn7)，它描述模型的一般行为。相比之下，透明性涵盖了关于更广泛系统设计和操作的开放性。这包括数据来源、特征工程[8](#fn8)、模型架构、训练程序、评估协议和已知限制的披露。透明性还涉及记录预期用例、系统边界和治理结构。
- en: The importance of explainability and transparency extends beyond technical considerations
    to legal requirements. In many jurisdictions, these principles are legal obligations
    rather than merely best practices. For instance, the European Unions [General
    Data Protection Regulation (GDPR)](https://gdpr.eu/tag/gdpr/) requires that individuals
    receive meaningful information about the logic of automated decisions that significantly
    affect them[9](#fn9). Similar regulatory pressures are emerging in other domains,
    reinforcing the need to treat explainability and transparency as core architectural
    requirements.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性和透明度的重要性不仅限于技术考虑，还涉及法律要求。在许多司法管辖区，这些原则是法律义务，而不仅仅是最佳实践。例如，欧盟的[通用数据保护条例（GDPR）](https://gdpr.eu/tag/gdpr/)要求个人获得有关对其产生重大影响的自动化决策逻辑的有意义信息[9](#fn9)。在其他领域也出现了类似的监管压力，这加强了将可解释性和透明度视为核心架构要求的需求。
- en: Implementing these principles requires anticipating the needs of different stakeholders,
    whose competing values and priorities are examined comprehensively in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f).
    Designing for explainability and transparency therefore necessitates decisions
    about how and where to surface relevant information across the system lifecycle.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些原则需要预见不同利益相关者的需求，其相互竞争的价值和优先事项在[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中得到全面审查。因此，为了可解释性和透明度而进行设计，需要决定如何在系统生命周期中呈现相关信息。
- en: These principles also support system reliability over time. As models are retrained
    or updated, mechanisms for interpretability and traceability allow the detection
    of unexpected behavior, enable root cause analysis, and support governance. Transparency
    and explainability, when embedded into the structure and operation of a system,
    provide the foundation for trust, oversight, and alignment with institutional
    and societal expectations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则也支持系统在时间上的可靠性。随着模型的重训练或更新，可解释性和可追溯性的机制允许检测到意外行为，进行根本原因分析，并支持治理。透明度和可解释性，当嵌入到系统的结构和操作中时，为信任、监督和与机构和社会期望的协调提供了基础。
- en: Fairness in Machine Learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习中的公平性
- en: Fairness in machine learning presents complex challenges. As established in
    [Section 17.2](ch023.xhtml#sec-responsible-ai-core-principles-1bd7), fairness
    requires that automated systems not disproportionately disadvantage protected
    groups. Because these systems are trained on historical data, they are susceptible
    to reproducing and amplifying patterns of systemic bias[10](#fn10) embedded in
    that data. Without careful design, machine learning systems may unintentionally
    reinforce social inequities rather than mitigate them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的公平性提出了复杂挑战。如[第17.2节](ch023.xhtml#sec-responsible-ai-core-principles-1bd7)所确立的，公平性要求自动化系统不会不成比例地损害受保护群体。因为这些系统是在历史数据上训练的，它们容易复制和放大数据中嵌入的系统偏见[10](#fn10)。如果没有仔细设计，机器学习系统可能会无意中加强社会不平等，而不是减轻它们。
- en: A widely studied example comes from the healthcare domain. An algorithm used
    to allocate care management resources in U.S. hospitals was found to systematically
    underestimate the health needs of Black patients ([Obermeyer et al. 2019](ch058.xhtml#ref-obermeyer2019dissecting))[11](#fn11).
    The model used healthcare expenditures as a proxy for health status, but due to
    longstanding disparities in access and spending, Black patients were less likely
    to incur high costs. As a result, the model inferred that they were less sick,
    despite often having equal or greater medical need. This case illustrates how
    seemingly neutral design choices such as proxy variable selection can yield discriminatory
    outcomes when historical inequities are not properly accounted for.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个广泛研究过的例子来自医疗保健领域。一种用于在美国医院分配护理管理资源的算法被发现系统地低估了黑人患者的健康需求([Obermeyer等人2019](ch058.xhtml#ref-obermeyer2019dissecting))[11](#fn11)。该模型将医疗支出作为健康状况的代理，但由于长期存在的获取和支出差异，黑人患者不太可能产生高额费用。因此，该模型推断他们病情较轻，尽管他们通常有相等或更大的医疗需求。这一案例说明了看似中性的设计选择，如代理变量选择，如何在历史不平等没有得到适当考虑的情况下产生歧视性结果。
- en: Practitioners need formal methods to evaluate fairness given these risks of
    perpetuating bias. A range of formal criteria have been developed that quantify
    how models perform across groups defined by sensitive attributes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实践者需要正式的方法来评估这些持续偏见风险下的公平性。已经开发了一系列正式标准，这些标准量化了模型在由敏感属性定义的群体中的表现。
- en: '**Mathematical Content Ahead**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学内容先行**'
- en: 'Before examining formal definitions, consider the fundamental challenge: what
    does it mean for an algorithm to be fair? Should it treat everyone identically,
    or account for different baseline conditions? Should it optimize for equal outcomes,
    equal opportunities, or equal treatment? These questions lead to different mathematical
    criteria, each capturing different aspects of fairness.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察正式定义之前，考虑一下基本挑战：一个算法要公平意味着什么？它应该对每个人都同等对待，还是应该考虑不同的基线条件？它应该优化平等的结果、平等的机会还是平等的治疗？这些问题导致不同的数学标准，每个标准都捕捉到公平的不同方面。
- en: 'The following subsections introduce formal fairness definitions using probability
    notation. These metrics (demographic parity, equalized odds, equality of opportunity)
    appear throughout ML fairness literature and shape regulatory frameworks. Focus
    on understanding the intuition: what each metric measures and why it matters,
    rather than mathematical proofs. The concrete examples following each definition
    illustrate practical application. If probability notation is unfamiliar, start
    with the verbal descriptions and return to the formal definitions later.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将使用概率符号介绍正式的公平性定义。这些指标（人口比例、均衡机会、机会平等）贯穿于机器学习公平性文献中，并塑造了监管框架。关注理解直觉：每个指标衡量的是什么以及为什么它很重要，而不是数学证明。每个定义之后的具体例子说明了实际应用。如果概率符号不熟悉，从口头描述开始，稍后再回到正式定义。
- en: 'Suppose a model <semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)</annotation></semantics>
    predicts a binary outcome, such as loan repayment, and let <semantics><mi>S</mi><annotation
    encoding="application/x-tex">S</annotation></semantics> represent a sensitive
    attribute with subgroups <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>
    and <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>.
    Several widely used fairness definitions are:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个模型 <semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)</annotation></semantics>
    预测一个二元结果，例如贷款偿还，让 <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    代表一个敏感属性，具有子组 <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics>
    和 <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>。一些广泛使用的公平性定义包括：
- en: Demographic Parity
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人口比例平等
- en: 'This criterion requires that the probability of receiving a positive prediction
    is independent of group membership. Formally, the model satisfies demographic
    parity if: <semantics><mrow><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">P\big(h(x) = 1 \mid S = a\big) = P\big(h(x) = 1 \mid
    S = b\big)</annotation></semantics>'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此标准要求接收积极预测的概率与群体成员资格无关。形式上，如果模型满足以下条件，则认为模型满足人口比例平等：<semantics><mrow><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">P\big(h(x) = 1 \mid S = a\big) = P\big(h(x) = 1 \mid
    S = b\big)</annotation></semantics>
- en: This means the model assigns favorable outcomes, such as loan approval or treatment
    referral, at equal rates across subgroups defined by a sensitive attribute <semantics><mi>S</mi><annotation
    encoding="application/x-tex">S</annotation></semantics>.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型在由敏感属性 <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    定义的子组中，以相同的比率分配有利的结果，例如贷款批准或治疗推荐。
- en: In the healthcare example, demographic parity would ask whether Black and white
    patients were referred for care at the same rate, regardless of their underlying
    health needs. While this might seem fair in terms of equal access, it ignores
    real differences in medical status and risk, potentially overcorrecting in situations
    where needs are not evenly distributed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健的例子中，人口统计学上的平等会询问黑人患者和白人患者是否以相同的比率被推荐接受护理，无论他们的潜在健康需求如何。虽然从平等获取的角度来看这似乎是公平的，但它忽略了医疗状况和风险的实际差异，可能在需求分布不均匀的情况下过度纠正。
- en: This limitation motivates more nuanced fairness criteria.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制促使人们提出更细致的公平性标准。
- en: Equalized Odds
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平等机会
- en: 'This definition requires that the model’s predictions are conditionally independent
    of group membership given the true label. Specifically, the true positive and
    false positive rates must be equal across groups: <semantics><mrow><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mtext
    mathvariant="normal">for</mtext></mrow> <mi>y</mi><mo>∈</mo><mo stretchy="false"
    form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = y\big)
    = P\big(h(x) = 1 \mid S = b, Y = y\big), \quad \text{for } y \in \{0, 1\}.</annotation></semantics>'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此定义要求模型在给定真实标签的情况下，其预测与组别条件独立。具体来说，真实阳性和假阳性率必须在组间相等：<semantics><mrow><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mtext
    mathvariant="normal">for</mtext></mrow> <mi>y</mi><mo>∈</mo><mo stretchy="false"
    form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = y\big)
    = P\big(h(x) = 1 \mid S = b, Y = y\big), \quad \text{for } y \in \{0, 1\}.</annotation></semantics>
- en: That is, for each true outcome <semantics><mrow><mi>Y</mi><mo>=</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">Y = y</annotation></semantics>, the model should
    produce the same prediction distribution across groups <semantics><mrow><mi>S</mi><mo>=</mo><mi>a</mi></mrow><annotation
    encoding="application/x-tex">S = a</annotation></semantics> and <semantics><mrow><mi>S</mi><mo>=</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">S = b</annotation></semantics>. This means the model
    should behave similarly across groups for individuals with the same true outcome,
    whether they qualify for a positive result or not. It ensures that errors (both
    missed and incorrect positives) are distributed equally.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 即，对于每个真实的输出 <semantics><mrow><mi>Y</mi><mo>=</mo><mi>y</mi></mrow><annotation
    encoding="application/x-tex">Y = y</annotation></semantics>，模型应在组 <semantics><mrow><mi>S</mi><mo>=</mo><mi>a</mi></mrow><annotation
    encoding="application/x-tex">S = a</annotation></semantics> 和 <semantics><mrow><mi>S</mi><mo>=</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">S = b</annotation></semantics> 之间产生相同的预测分布。这意味着对于具有相同真实输出的个体，无论他们是否获得阳性结果，模型在组间应表现出相似的行为。这确保了错误（包括漏检和错误的阳性）在组间均匀分布。
- en: Applied to the medical case, equalized odds would ensure that patients with
    the same actual health needs (the true label <semantics><mi>Y</mi><annotation
    encoding="application/x-tex">Y</annotation></semantics>) are equally likely to
    be correctly or incorrectly referred, regardless of race. The original algorithm
    violated this by under-referring Black patients who were equally or more sick
    than their white counterparts, highlighting unequal true positive rates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗案例中应用，等概率确保具有相同实际健康需求（真实标签<semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>）的患者，无论种族如何，都有同等的机会被正确或错误地推荐。原始算法通过低估与白人患者健康状况相当或更差的黑人患者的推荐，违反了这一准则，突显了真实阳性率的不平等。
- en: A less stringent criterion focuses specifically on positive outcomes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不那么严格的准则专注于积极的成果。
- en: Equality of Opportunity
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机会均等
- en: 'A relaxation of equalized odds, this criterion focuses only on the true positive
    rate. It requires that, among individuals who should receive a positive outcome,
    the probability of receiving one is equal across groups: <semantics><mrow><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = 1\big)
    = P\big(h(x) = 1 \mid S = b, Y = 1\big).</annotation></semantics>'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 等概率的放宽，这一准则仅关注真实阳性率。它要求，在应获得积极结果的个人中，获得一个的概率在各个群体中是相等的：<semantics><mrow><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mi><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = 1\big)
    = P\big(h(x) = 1 \mid S = b, Y = 1\big).</annotation></semantics>
- en: This ensures that qualified individuals, who have <semantics><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">Y = 1</annotation></semantics>, are treated equally
    by the model regardless of group membership.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了具有资格的个人，无论其群体成员身份如何，模型都会平等对待，即<semantics><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">Y = 1</annotation></semantics>。
- en: 'In our running example, this measure would ensure that among patients who do
    require care, both Black and white individuals have an equal chance of being identified
    by the model. In the case of the U.S. hospital system, the algorithm’s use of
    healthcare expenditure as a proxy variable led to a failure in meeting this criterion:
    Black patients with significant health needs were less likely to receive care
    due to their lower historical spending.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的运行示例中，这一措施将确保在需要护理的患者中，黑人和白人个体都有同等的机会被模型识别。在美国医院系统中，算法使用医疗保健支出作为代理变量导致未能满足这一标准：由于历史支出较低，具有重大健康需求的黑人患者不太可能获得护理。
- en: 'Consider a simplified loan approval model evaluated on 200 applicants, evenly
    split between two demographic groups (Group A and Group B). The model makes predictions,
    and we later observe actual repayment outcomes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个在200名申请人中评估的简化贷款审批模型，这些申请人平均分为两个人口群体（A组和B组）。该模型做出预测，我们后来观察到实际的还款结果：
- en: '**Group A (100 applicants):**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**A组（100名申请人）：**'
- en: 'Model approved: 70 applicants (40 actually repaid, 30 defaulted)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型批准：70名申请人（40名实际上会还款，30名会违约）
- en: 'Model rejected: 30 applicants (5 actually would have repaid, 25 would have
    defaulted)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型拒绝：30名申请人（5名实际上会还款，25名会违约）
- en: '**Group B (100 applicants):**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**B组（100名申请人）：**'
- en: 'Model approved: 40 applicants (30 actually repaid, 10 defaulted)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型批准：40名申请人（30名实际上会还款，10名会违约）
- en: 'Model rejected: 60 applicants (20 actually would have repaid, 40 would have
    defaulted)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型拒绝：60名申请人（20名实际上会还款，40名会违约）
- en: '**Calculating Demographic Parity:** <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>70</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.70</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.40</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A) = \frac{70}{100}
    = 0.70 \\ P(h(x) = 1 \mid S = B) = \frac{40}{100} = 0.40 \end{gather*}</annotation></semantics>'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算人口统计学上的平等：** <semantics><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>70</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.70</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.40</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A) = \frac{70}{100}
    = 0.70 \\ P(h(x) = 1 \mid S = B) = \frac{40}{100} = 0.40 \end{gather*}</annotation></semantics>'
- en: '**Disparity:** <semantics><mrow><mn>0.70</mn><mo>−</mo><mn>0.40</mn><mo>=</mo><mn>0.30</mn></mrow><annotation
    encoding="application/x-tex">0.70 - 0.40 = 0.30</annotation></semantics> (30 percentage
    point gap)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**差异：** <semantics><mrow><mn>0.70</mn><mo>−</mo><mn>0.40</mn><mo>=</mo><mn>0.30</mn></mrow><annotation
    encoding="application/x-tex">0.70 - 0.40 = 0.30</annotation></semantics>（30个百分点的差距）'
- en: The model violates demographic parity by approving Group A applicants at substantially
    higher rates, regardless of actual repayment ability.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型通过以显著更高的比率批准A组申请人，无论其实际还款能力如何，违反了人口统计学上的平等。
- en: '**Calculating Equality of Opportunity (True Positive Rate):**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机会平等（真正率）：**'
- en: 'Among applicants who *would actually repay* (Y=1): <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mrow><mn>40</mn><mo>+</mo><mn>5</mn></mrow></mfrac><mo>=</mo><mfrac><mn>40</mn><mn>45</mn></mfrac><mo>≈</mo><mn>0.89</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>20</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.60</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 1) = \frac{40}{40
    + 5} = \frac{40}{45} \approx 0.89 \\ P(h(x) = 1 \mid S = B, Y = 1) = \frac{30}{30
    + 20} = \frac{30}{50} = 0.60 \end{gather*}</annotation></semantics>'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际会还款的申请人中（Y=1）：<semantics><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mrow><mn>40</mn><mo>+</mo><mn>5</mn></mrow></mfrac><mo>=</mo><mfrac><mn>40</mn><mn>45</mn></mfrac><mo>≈</mo><mn>0.89</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>20</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.60</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 1) = \frac{40}{40
    + 5} = \frac{40}{45} \approx 0.89 \\ P(h(x) = 1 \mid S = B, Y = 1) = \frac{30}{30
    + 20} = \frac{30}{50} = 0.60 \end{gather*}</annotation></semantics>'
- en: '**Disparity:** <semantics><mrow><mn>0.89</mn><mo>−</mo><mn>0.60</mn><mo>=</mo><mn>0.29</mn></mrow><annotation
    encoding="application/x-tex">0.89 - 0.60 = 0.29</annotation></semantics> (29 percentage
    point gap in TPR)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**差异：** <semantics><mrow><mn>0.89</mn><mo>−</mo><mn>0.60</mn><mo>=</mo><mn>0.29</mn></mrow><annotation
    encoding="application/x-tex">0.89 - 0.60 = 0.29</annotation></semantics>（真正例率29个百分点差距）'
- en: 'The model violates equality of opportunity: among qualified applicants who
    would repay, Group A members are correctly approved 89% of the time while Group
    B members are only approved 60% of the time.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型违反了机会平等原则：在那些会还款的合格申请人中，A组成员被正确批准的比率是89%，而B组成员只有60%被批准。
- en: '**Calculating Equalized Odds (True Positive Rate + False Positive Rate):**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算均衡概率（真正例率 + 假正例率）：**'
- en: 'We already calculated TPR above. Now for false positive rates among applicants
    who would *not* repay (Y=0): <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>25</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>55</mn></mfrac><mo>≈</mo><mn>0.55</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>10</mn><mrow><mn>10</mn><mo>+</mo><mn>40</mn></mrow></mfrac><mo>=</mo><mfrac><mn>10</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.20</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 0) = \frac{30}{30
    + 25} = \frac{30}{55} \approx 0.55 \\ P(h(x) = 1 \mid S = B, Y = 0) = \frac{10}{10
    + 40} = \frac{10}{50} = 0.20 \end{gather*}</annotation></semantics>'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经计算了真正例率（TPR）。现在计算那些不会还款（Y=0）的申请人的假正例率：<semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>25</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>55</mn></mfrac><mo>≈</mo><mn>0.55</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>10</mn><mrow><mn>10</mn><mo>+</mo><mn>40</mn></mrow></mfrac><mo>=</mo><mfrac><mn>10</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.20</mn></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 0) = \frac{30}{30
    + 25} = \frac{30}{55} \approx 0.55 \\ P(h(x) = 1 \mid S = B, Y = 0) = \frac{10}{10
    + 40} = \frac{10}{50} = 0.20 \end{gather*}</annotation></semantics>'
- en: 'The model also has unequal false positive rates: it incorrectly approves 55%
    of Group A applicants who will default, but only 20% of Group B applicants who
    will default. This reveals the model is more “generous” with Group A even when
    they won’t repay.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还存在着不等的假正例率：它错误地批准了55%的A组成员将会违约，但只有20%的B组成员将会违约。这表明，即使A组成员不会还款，模型对A组也更“慷慨”。
- en: '**Key Insight:** This model violates all three fairness criteria. Addressing
    one criterion doesn’t automatically satisfy others. In fact, they can conflict,
    as we’ll see next.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键洞见：** 该模型违反了所有三个公平性标准。解决一个标准并不意味着自动满足其他标准。事实上，它们可能存在冲突，正如我们接下来将要看到的。'
- en: These fairness criteria highlight tensions in defining algorithmic fairness.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公平性标准突显了在定义算法公平性时的紧张关系。
- en: '**Advanced Topic: Impossibility Results**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级主题：不可能性结果**'
- en: 'The impossibility theorems discussed below represent active research in fairness
    theory. Understanding that multiple fairness criteria cannot be simultaneously
    satisfied is more important than the mathematical proofs. The key insight: fairness
    is fundamentally a value-laden engineering decision requiring stakeholder deliberation,
    not a technical optimization problem with a single correct solution. This conceptual
    understanding suffices for most practitioners.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下面讨论的不可能性定理代表了公平性理论中的活跃研究。理解多个公平性标准不能同时满足比数学证明更重要。关键洞见：公平性本质上是一个充满价值判断的工程决策，需要利益相关者的审议，而不是一个具有单一正确解决方案的技术优化问题。这种概念理解对大多数从业者来说就足够了。
- en: These definitions capture different aspects of fairness and are generally incompatible[12](#fn12).
    To understand this intuitively, imagine a university wants to be fair in its admissions.
    What does that mean? Goal 1 (Demographic Parity) would be to admit students so
    that the admitted class reflects the demographics of the applicant pool, perhaps
    50% from Group A and 50% from Group B. Goal 2 (Equal Opportunity) would be to
    ensure that among all qualified applicants, the admission rate is the same across
    groups, so that 80% of qualified Group A applicants get in and 80% of qualified
    Group B applicants get in. The impossibility theorem shows you cannot always have
    both. If one group has a higher proportion of qualified applicants, achieving
    demographic parity (Goal 1) would require rejecting some of their qualified applicants,
    thus violating equal opportunity (Goal 2). There is no mathematical fix for this;
    it is a value judgment about which definition of fairness to prioritize. Satisfying
    one criterion may preclude satisfying another, reflecting the reality that fairness
    involves tradeoffs between competing normative goals. Determining which metric
    to prioritize requires careful consideration of the application context, potential
    harms, and stakeholder values as detailed in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义捕捉了公平性的不同方面，通常是不兼容的[12](#fn12)。为了直观理解这一点，想象一所大学希望在招生过程中保持公平。这意味着什么？目标1（人口比例平等）将是录取学生，使录取班级反映申请者池的统计数据，例如A组和B组各占50%。目标2（平等机会）将是确保在所有合格的申请者中，各群体之间的录取率相同，因此80%的合格A组申请者被录取，80%的合格B组申请者被录取。不可能定理表明，您并不总是可以两者兼得。如果一个群体有更多合格的申请者，实现人口比例平等（目标1）将需要拒绝一些他们的合格申请者，从而违反平等机会（目标2）。这个问题没有数学上的解决方案；这是关于优先考虑哪种公平性定义的价值判断。满足一个标准可能阻止满足另一个标准，反映了公平性涉及在竞争性规范性目标之间进行权衡的现实。确定优先考虑哪个指标需要仔细考虑应用背景、潜在危害和利益相关者的价值观，如[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中详细所述。
- en: Recognizing these tensions, operational systems must treat fairness as a constraint
    that informs decisions throughout the machine learning lifecycle. It is shaped
    by how data are collected and represented, how objectives and proxies are selected,
    how model predictions are thresholded, and how feedback mechanisms are structured.
    For example, a choice between ranking versus classification models can yield different
    patterns of access across groups, even when using the same underlying data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这些紧张关系，操作系统必须将公平性视为一个约束条件，在整个机器学习生命周期中指导决策。它受到数据收集和表示方式、目标和代理的选择、模型预测的阈值以及反馈机制结构的影响。例如，在排名模型和分类模型之间进行选择可能会在不同群体中产生不同的访问模式，即使使用相同的基础数据。
- en: Fairness metrics help formalize equity goals but are often limited to predefined
    demographic categories. In practice, these categories may be too coarse to capture
    the full range of disparities present in real-world data. A principled approach
    to fairness must account for overlapping and intersectional identities, ensuring
    that model behavior remains consistent across subgroups that may not be explicitly
    labeled in advance. Recent work in this area emphasizes the need for predictive
    reliability across a wide range of population slices ([Hébert-Johnson et al. 2018](ch058.xhtml#ref-hebert2018multicalibration)),
    reinforcing the idea that fairness must be considered a system-level requirement,
    not a localized adjustment. This expanded view of fairness highlights the importance
    of designing architectures, evaluation protocols, and monitoring strategies that
    support more nuanced, context-sensitive assessments of model behavior.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指标有助于正式化公平目标，但通常仅限于预定义的种群类别。在实践中，这些类别可能过于粗略，无法捕捉到现实世界数据中存在的全部差异。对公平性的原则性方法必须考虑到重叠和交叉身份，确保模型行为在可能事先未明确标记的子群体中保持一致。这一领域最近的研究强调了在广泛的人口切片中预测可靠性的必要性([Hébert-Johnson
    等人 2018](ch058.xhtml#ref-hebert2018multicalibration))，强化了公平性必须被视为系统级要求的观点，而不是局部调整。这种对公平性的扩展观点突出了设计架构、评估协议和监控策略的重要性，这些策略支持对模型行为进行更细致、更敏感的评估。
- en: Fairness considerations extend beyond algorithmic outcomes to encompass the
    computational resources and infrastructure required to deploy responsible AI systems.
    These broader equity implications, including environmental justice concerns, arise
    when energy-intensive AI infrastructure is concentrated in already disadvantaged
    communities[13](#fn13).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性考虑不仅限于算法结果，还包括部署负责任AI系统所需的计算资源和基础设施。当能源密集型AI基础设施集中在已经处于不利地位的社区时，这些更广泛的公平影响，包括环境正义问题，就会出现[13](#fn13)。
- en: The computational intensity of responsible AI techniques creates a form of digital
    divide where access to fair, transparent, and accountable AI systems becomes contingent
    on economic resources. Implementing fairness constraints, differential privacy
    mechanisms, and comprehensive explainability tools typically increases computational
    costs by 15-40% compared to unconstrained models. This creates a troubling dynamic
    where only organizations with substantial computational budgets can afford to
    deploy genuinely responsible AI systems, while resource-constrained deployments
    may sacrifice ethical safeguards for efficiency. The result is a two-tiered system
    where responsible AI becomes a privilege available primarily to well-resourced
    users and applications, potentially exacerbating existing inequalities rather
    than addressing them. These resource constraints create democratization challenges,
    while the broader implications create digital divide and access barriers affecting
    underserved communities.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任AI技术的计算强度创造了一种数字鸿沟，其中对公平、透明和可问责AI系统的访问成为经济资源的附属品。实施公平性约束、差分隐私机制和全面的可解释性工具通常会增加计算成本，比无约束模型高出15-40%。这创造了一种令人担忧的动态，只有拥有大量计算预算的组织才能负担得起部署真正负责任的AI系统，而资源受限的部署可能为了效率而牺牲道德保障。结果是，出现了一个双层系统，其中负责任的AI成为主要对资源充足的用户和应用可用的特权，可能加剧而不是解决现有的不平等。这些资源限制创造了民主化挑战，而更广泛的影响则创造了数字鸿沟和访问障碍，影响了服务不足的社区。
- en: 'These considerations point to a fundamental conclusion: fairness is a system-wide
    property that arises from the interaction of data engineering practices, modeling
    choices, evaluation procedures, and decision policies. It cannot be isolated to
    a single model component or resolved through post hoc adjustments alone. Responsible
    machine learning design requires treating fairness as a foundational constraint,
    one that informs architectural choices, workflows, and governance mechanisms throughout
    the entire lifecycle of the system. This system-wide view extends to all responsible
    AI principles, which translate into concrete engineering requirements across the
    ML lifecycle: fairness demands group-level performance metrics and different decision
    thresholds across populations; explainability requires runtime compute budgets
    with costs varying from 10-50 ms for gradient methods to 50-1000x overhead for
    SHAP analysis; privacy encompasses data governance, consent mechanisms, and lifecycle-aware
    retention policies; and accountability requires traceability infrastructure including
    model registries, audit logs, and human override mechanisms.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑因素指向一个基本结论：公平性是一个系统级属性，它源于数据工程实践、建模选择、评估程序和决策政策的相互作用。它不能孤立于单个模型组件，也不能仅通过事后调整来解决。负责任的机器学习设计需要将公平性视为一个基础约束，这一约束在整个系统的生命周期中指导架构选择、工作流程和治理机制。这种系统级观点扩展到所有负责任的AI原则，这些原则在ML生命周期中转化为具体的工程要求：公平性要求群体级性能指标和不同人群中的决策阈值；可解释性需要运行时计算预算，梯度方法的成本从10-50毫秒到SHAP分析的50-1000倍不等；隐私包括数据治理、同意机制和生命周期感知的保留策略；问责制需要可追溯性基础设施，包括模型注册、审计日志和人工覆盖机制。
- en: 'These principles interact and create tensions throughout system development.
    Privacy-preserving techniques may reduce explainability; fairness constraints
    may conflict with personalization; robust monitoring increases computational costs.
    The table in [Table 17.1](ch023.xhtml#tbl-principles-lifecycle) showed how each
    principle manifests across data collection, training, evaluation, deployment,
    and monitoring phases, reinforcing that responsible AI is not a post-deployment
    consideration but an architectural commitment. However, the feasibility of implementing
    these principles depends critically on deployment context: cloud, edge, mobile,
    and TinyML environments each impose different constraints that shape which responsible
    AI features are practically achievable.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则在整个系统开发过程中相互作用并产生张力。隐私保护技术可能会降低可解释性；公平性约束可能与个性化相冲突；稳健的监控会增加计算成本。《表 17.1》中的表格显示了每个原则如何在数据收集、训练、评估、部署和监控阶段体现，强化了负责任
    AI 不是部署后的考虑，而是一种架构承诺的观点。然而，实施这些原则的可行性严重依赖于部署环境：云、边缘、移动和 TinyML 环境各自施加不同的约束，这些约束塑造了哪些负责任
    AI 功能在实践中是可实现的。
- en: Privacy and Data Governance
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私和数据治理
- en: 'Privacy and data governance present complex challenges that extend beyond the
    threat-model perspective developed in [Chapter 15](ch021.xhtml#sec-security-privacy),
    while creating fundamental tensions with the fairness and transparency principles
    examined above. Security-focused privacy asks “how do we prevent unauthorized
    access?” Responsible privacy asks “should we collect this data at all, and if
    so, how do we minimize exposure throughout the system lifecycle?” This broader
    perspective creates inherent tensions: fairness monitoring requires collecting
    and analyzing sensitive demographic data, explainability methods may reveal information
    about training examples, and comprehensive transparency can conflict with individual
    privacy rights. Responsible AI systems must navigate these competing requirements
    through careful design choices that balance protection, accountability, and utility.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和数据治理带来的挑战超越了第 15 章中提出的威胁模型视角，同时与上述考察的公平性和透明度原则产生了根本性的张力。以安全为重点的隐私询问“我们如何防止未经授权的访问？”负责任的隐私询问“我们是否应该收集这些数据，如果是的话，我们如何在整个系统生命周期中最大限度地减少暴露？”这种更广泛的视角产生了固有的张力：公平性监控需要收集和分析敏感的人口统计数据，可解释性方法可能会揭示训练示例的信息，而全面的透明度可能与个人隐私权相冲突。负责任的
    AI 系统必须通过谨慎的设计选择来平衡保护、问责制和效用，以应对这些相互竞争的要求。
- en: Machine learning systems often rely on extensive collections of personal data
    to support model training and allow personalized functionality. This reliance
    introduces significant responsibilities related to user privacy, data protection,
    and ethical data stewardship. The quality and governance of this data, covered
    in [Chapter 6](ch012.xhtml#sec-data-engineering), directly impacts the ability
    to implement responsible AI principles. Responsible AI design treats privacy not
    as an ancillary feature, but as a core constraint that must inform decisions across
    the entire system lifecycle.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统通常依赖于大量的个人数据集来支持模型训练并允许个性化功能。这种依赖引入了与用户隐私、数据保护和道德数据管理相关的重大责任。数据的质量和管理，在第
    6 章中介绍，直接影响着实施负责任 AI 原则的能力。负责任的 AI 设计将隐私视为不是辅助功能，而是必须在整个系统生命周期中指导决策的核心约束。
- en: One of the core challenges in supporting privacy is the inherent tension between
    data utility and individual protection. Rich, high-resolution datasets can enhance
    model accuracy and adaptability but also heighten the risk of exposing sensitive
    information, particularly when datasets are aggregated or linked with external
    sources. For example, models trained on conversational data or medical records
    have been shown to memorize specific details that can later be retrieved through
    model queries or adversarial interaction ([Ippolito et al. 2023](ch058.xhtml#ref-carlini2023extractingllm))[14](#fn14).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持隐私方面，一个核心挑战是数据效用与个人保护之间的固有张力。丰富、高分辨率的数据库可以增强模型的准确性和适应性，但同时也增加了暴露敏感信息的风险，尤其是在数据集汇总或与外部来源关联时。例如，基于对话数据或医疗记录训练的模型已被证明会记住特定细节，这些细节可以通过模型查询或对抗性交互后来检索到（[Ippolito
    等人 2023](ch058.xhtml#ref-carlini2023extractingllm))[14](#fn14)。
- en: The privacy challenges extend beyond obvious sensitive data to seemingly innocuous
    information. Wearable devices that track physiological and behavioral signals,
    including heart rate, movement, or location, may individually seem benign but
    can jointly reveal detailed user profiles. These risks are further exacerbated
    when users have limited visibility or control over how their data is processed,
    retained, or transmitted.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私挑战不仅限于明显的敏感数据，还包括看似无害的信息。追踪生理和行为信号的可穿戴设备，包括心率、运动或位置，单个来看可能看似无害，但联合起来可以揭示详细的用户档案。当用户对他们的数据处理、保留或传输的可见性或控制有限时，这些风险会进一步加剧。
- en: Addressing these challenges requires understanding privacy as a system principle
    that entails robust data governance. This includes defining what data is collected,
    under what conditions, and with what degree of consent and transparency. The foundational
    data engineering practices discussed in [Chapter 6](ch012.xhtml#sec-data-engineering)
    provide the technical infrastructure for implementing these governance requirements.
    Responsible governance requires attention to labeling practices, access controls,
    logging infrastructure, and compliance with jurisdictional requirements. These
    mechanisms serve to constrain how data flows through a system and to document
    accountability for its use.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要将隐私视为一个系统原则，它包括强大的数据治理。这包括定义收集的数据、在什么条件下收集以及以何种程度的同意和透明度收集。在第6章（ch012.xhtml#sec-data-engineering）中讨论的基础数据工程实践为实施这些治理要求提供了技术基础设施。负责任的治理需要关注标签实践、访问控制、日志基础设施以及符合司法管辖要求。这些机制旨在限制数据在系统中的流动，并记录其使用的问责制。
- en: To support structured decision-making in this space, [Figure 17.1](ch023.xhtml#fig-privacy-risk-flow)
    shows a simplified flowchart outlining key privacy checkpoints in the early stages
    of a data pipeline. It highlights where core safeguards, such as consent acquisition,
    encryption, and differential privacy, should be applied. Actual implementations
    often involve more nuanced tradeoffs and context-sensitive decisions, but this
    diagram provides a scaffold for identifying where privacy risks arise and how
    they can be mitigated through responsible design choices.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持该领域的结构化决策，[图17.1](ch023.xhtml#fig-privacy-risk-flow) 展示了一个简化的流程图，概述了数据管道早期阶段的关键隐私检查点。它突出了核心保护措施，如同意获取、加密和差分隐私应该应用的地方。实际的实现往往涉及更微妙的权衡和情境敏感的决策，但此图提供了一个识别隐私风险出现位置以及如何通过负责任的设计选择来缓解这些风险的框架。
- en: '![](../media/file290.svg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file290.svg)'
- en: 'Figure 17.1: **Privacy-Aware Data Flow**: Responsible data governance requires
    proactive safeguards throughout a machine learning pipeline, including consent
    acquisition, encryption, and differential privacy mechanisms applied at key decision
    points to mitigate privacy risks and ensure accountability. This diagram structures
    these considerations, enabling designers to identify potential vulnerabilities
    and implement appropriate controls during data collection, processing, and storage.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：**隐私感知数据流**：负责任的数据治理需要在机器学习管道的整个过程中采取主动保护措施，包括在关键决策点应用同意获取、加密和差分隐私机制，以减轻隐私风险并确保问责制。此图结构化了这些考虑因素，使设计者能够识别潜在漏洞并在数据收集、处理和存储期间实施适当的控制。
- en: The consequences of weak data governance are well documented. Systems trained
    on poorly understood or biased datasets may perpetuate structural inequities or
    expose sensitive attributes unintentionally. In the COMPAS example introduced
    earlier, the lack of transparency surrounding data provenance and usage precluded
    effective evaluation or redress. In clinical applications, datasets frequently
    reflect artifacts such as missing values or demographic skew that compromise both
    performance and privacy. Without clear standards for data quality and documentation,
    such vulnerabilities become systemic.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 弱数据治理的后果已经得到了充分记录。在理解不充分或存在偏差的数据集上训练的系统可能会持续存在结构性不平等或无意中暴露敏感属性。在前面介绍的COMPAS示例中，数据来源和使用方面的不透明性阻止了有效的评估或纠正。在临床应用中，数据集通常反映诸如缺失值或人口统计偏差等伪影，这些伪影会损害性能和隐私。没有明确的数据质量和文档标准，这些漏洞会变得系统化。
- en: Privacy is not solely the concern of isolated algorithms or data processors;
    it must be addressed as a structural property of the system. Decisions about consent
    collection, data retention, model design, and auditability all contribute to the
    privacy posture of a machine learning pipeline. This includes the need to anticipate
    risks not only during training, but also during inference and ongoing operation.
    Threats such as membership inference attacks[15](#fn15) underscore the importance
    of embedding privacy safeguards into both model architecture and interface behavior.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私不仅仅是孤立算法或数据处理者的关注点；它必须作为系统结构属性来处理。关于同意收集、数据保留、模型设计和可审计性的决策都有助于机器学习管道的隐私状况。这包括在训练期间以及推理和持续运行期间预见风险的需要。如成员推理攻击[15](#fn15)
    等威胁强调了将隐私保障嵌入到模型架构和界面行为中的重要性。
- en: Legal frameworks increasingly reflect this understanding. Regulations such as
    the [GDPR](https://gdpr.eu), [CCPA](https://oag.ca.gov/privacy/ccpa)[16](#fn16),
    and [APPI](https://www.dataguidance.com/notes/japan-data-protection-overview)
    impose specific obligations regarding data minimization, purpose limitation, user
    consent, and the right to deletion. These requirements translate ethical expectations
    into enforceable design constraints, reinforcing the need to treat privacy as
    a core principle in system development.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 法律框架越来越多地反映了这种理解。例如，[GDPR](https://gdpr.eu)、[CCPA](https://oag.ca.gov/privacy/ccpa)[16](#fn16)
    和 [APPI](https://www.dataguidance.com/notes/japan-data-protection-overview) 等法规对数据最小化、目的限制、用户同意和删除权等方面提出了具体义务。这些要求将道德期望转化为可执行的设计约束，强化了在系统开发中将隐私视为核心原则的必要性。
- en: 'These privacy considerations culminate in a comprehensive approach: privacy
    in machine learning is a system-wide commitment. It requires coordination across
    technical and organizational domains to ensure that data usage aligns with user
    expectations, legal mandates, and societal norms. Rather than viewing privacy
    as a constraint to be balanced against functionality, responsible system design
    integrates privacy from the outset by informing architecture, shaping interfaces,
    and constraining how models are built, updated, and deployed.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些隐私考虑最终汇聚成一个全面的方法：机器学习中的隐私是一个系统级的承诺。它需要技术和组织领域的协调，以确保数据使用与用户期望、法律要求和社会规范相一致。而不是将隐私视为与功能平衡的约束，负责任的设计从一开始就通过告知架构、塑造界面和限制模型构建、更新和部署的方式将隐私整合进去。
- en: Safety and robustness represent additional critical dimensions of responsible
    AI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和鲁棒性是负责任人工智能的另外两个关键维度。
- en: Safety and Robustness
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性和鲁棒性
- en: Safety and robustness, introduced in [Chapter 16](ch022.xhtml#sec-robust-ai)
    as technical properties addressing hardware faults, adversarial attacks, and distribution
    shifts, also serve as responsible AI principles that extend beyond threat mitigation.
    Technical robustness ensures systems survive adversarial conditions; responsible
    robustness ensures systems behave in ways aligned with human expectations and
    values, even when technically functional. A model may be robust to bit flips and
    adversarial perturbations yet still exhibit behavior that is unsafe for deployment
    if it fails unpredictably in edge cases or optimizes objectives misaligned with
    user welfare.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和鲁棒性，在第16章（ch022.xhtml#sec-robust-ai）中作为解决硬件故障、对抗攻击和分布变化的特性被引入，也作为负责任人工智能原则，其作用不仅限于威胁缓解。技术鲁棒性确保系统在对抗条件下能够生存；负责任鲁棒性确保系统即使在技术上功能正常的情况下，也能以符合人类期望和价值观的方式行事。一个模型可能对位翻转和对抗性扰动具有鲁棒性，但如果它在边缘情况下不可预测地失败或优化与用户福祉不一致的目标，那么它可能仍然表现出不适合部署的不安全行为。
- en: Safety in machine learning refers to the assurance that models behave predictably
    under normal conditions and fail in controlled, non-catastrophic ways under stress
    or uncertainty. Closely related, robustness concerns a model’s ability to maintain
    stable and consistent performance in the presence of variation, whether in inputs,
    environments, or system configurations. Together, these properties are foundational
    for responsible deployment in safety critical domains, where machine learning
    outputs directly affect physical or high-stakes decisions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的安全性指的是在正常条件下模型行为可预测，以及在压力或不确定性下以可控、非灾难性的方式失败的保证。与安全性密切相关的是鲁棒性，它关注模型在输入、环境或系统配置变化的情况下维持稳定和一致性能的能力。这些属性共同构成了在安全关键领域负责任部署的基础，在这些领域，机器学习输出直接影响到物理或高风险决策。
- en: Ensuring safety and robustness in practice requires anticipating the full range
    of conditions a system may encounter and designing for behavior that remains reliable
    beyond the training distribution. This includes not only managing the variability
    of inputs but also addressing how models respond to unexpected correlations, rare
    events, and deliberate attempts to induce failure. For example, widely publicized
    failures in autonomous vehicle systems have revealed how limitations in object
    detection or overreliance on automation can result in harmful outcomes, even when
    models perform well under nominal test conditions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中确保安全和鲁棒性需要预测系统可能遇到的所有条件范围，并设计出在训练分布之外仍能保持可靠性的行为。这包括不仅管理输入的变异性，还要解决模型如何应对意外相关性、罕见事件和故意诱导失败尝试的问题。例如，广泛报道的自动驾驶系统故障揭示了在对象检测方面的局限性或过度依赖自动化如何导致有害后果，即使模型在正常测试条件下表现良好。
- en: 'One illustrative failure mode arises from adversarial inputs[17](#fn17): carefully
    constructed perturbations that appear benign to humans but cause a model to output
    incorrect or harmful predictions ([Szegedy et al. 2013b](ch058.xhtml#ref-szegedy2013intriguing)).
    Such vulnerabilities are not limited to image classification; they have been observed
    across modalities including audio, text, and structured data, and they reveal
    the brittleness of learned representations in high-dimensional spaces. Addressing
    these vulnerabilities requires specialized approaches including adversarial defenses
    and robustness techniques. These behaviors highlight that robustness must be considered
    not only during training but as a global property of how systems interact with
    real-world complexity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一种典型的失败模式源于对抗性输入[17](#fn17)：精心构造的扰动对人类看似无害，但会导致模型输出错误或有害的预测 ([Szegedy 等人 2013b](ch058.xhtml#ref-szegedy2013intriguing))。这种漏洞不仅限于图像分类；它们在包括音频、文本和结构化数据在内的多种模态中都有观察到，并揭示了在高维空间中学习表示的脆弱性。解决这些漏洞需要包括对抗性防御和鲁棒性技术在内的专门方法。这些行为凸显了鲁棒性不仅需要在训练期间考虑，而且作为系统与真实世界复杂性交互的全球属性。
- en: 'A related challenge is distribution shift: the inevitable mismatch between
    training data and conditions encountered in deployment. Whether due to seasonality,
    demographic changes, sensor degradation, or environmental variability, such shifts
    can degrade model reliability even in the absence of adversarial manipulation.
    Addressing distribution shift challenges requires systematic approaches to detecting
    and adapting to changing conditions. Failures under distribution shift may propagate
    through downstream decisions, introducing safety risks that extend beyond model
    accuracy alone. In domains such as healthcare, finance, or transportation, these
    risks are not hypothetical; they carry real consequences for individuals and institutions.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的挑战是分布偏移：训练数据与部署中遇到的条件之间不可避免的错配。无论是因为季节性、人口统计变化、传感器退化还是环境变化，这种偏移甚至在没有对抗性操作的情况下也可能降低模型可靠性。解决分布偏移挑战需要系统地检测和适应变化条件的方法。在分布偏移下的失败可能会通过下游决策传播，引入超出模型准确度本身的安全风险。在医疗保健、金融或交通等领域，这些风险并非假设性的；它们对个人和机构都有真实的影响。
- en: Responsible machine learning design treats robustness as a systemic requirement.
    Addressing it requires more than improving individual model performance. It involves
    designing systems that anticipate uncertainty, surface their limitations, and
    support fallback behavior when predictive confidence is low. This includes practices
    such as setting confidence thresholds, supporting abstention from decision-making,
    and integrating human oversight into operational workflows. These mechanisms are
    important for building systems that degrade gracefully rather than failing silently
    or unpredictably.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习设计将鲁棒性视为系统要求。解决它需要不仅仅是提高单个模型性能。它涉及设计能够预测不确定性、揭示其局限性并在预测信心低时支持回退行为的系统。这包括设置信心阈值、支持放弃决策和将人工监督集成到操作流程中的做法。这些机制对于构建能够优雅退化而不是无声或不可预测地失败的系统至关重要。
- en: These individual-model considerations extend to broader system requirements.
    Safety and robustness also impose requirements at the architectural and organizational
    level. Decisions about how models are monitored, how failures are detected, and
    how updates are governed all influence whether a system can respond effectively
    to changing conditions. Responsible design demands that robustness be treated
    not as a property of isolated models but as a constraint that shapes the overall
    behavior of machine learning systems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些针对单个模型的考虑延伸到更广泛的系统要求。安全和鲁棒性也在架构和组织层面提出了要求。关于如何监控模型、如何检测失败以及如何管理更新的决策都会影响系统是否能够有效地应对变化条件。负责任的设计要求将鲁棒性视为孤立模型的一个属性，而不是塑造机器学习系统整体行为的约束。
- en: This system-level perspective on safety and robustness leads to questions of
    accountability and governance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关于安全和鲁棒性的系统级视角引发了责任和治理的问题。
- en: Accountability and Governance
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 责任与治理
- en: Accountability in machine learning refers to the capacity to identify, attribute,
    and address the consequences of automated decisions. It extends beyond diagnosing
    failures to ensuring that responsibility for system behavior is clearly assigned,
    that harms can be remedied, and that ethical standards are maintained through
    oversight and institutional processes. Without such mechanisms, even well intentioned
    systems can generate significant harm without recourse, undermining public trust
    and eroding legitimacy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的责任是指识别、归因和解决自动化决策后果的能力。它不仅超越了诊断失败，还确保系统行为的责任明确分配，损害可以得到补救，并且通过监督和机构程序维护道德标准。没有这样的机制，即使是有良好意图的系统也可能在没有补救措施的情况下造成重大伤害，损害公众信任并侵蚀合法性。
- en: Unlike traditional software systems, where responsibility often lies with a
    clearly defined developer or operator, accountability in machine learning is distributed.
    Model outputs are shaped by upstream data collection, training objectives, pipeline
    design, interface behavior, and post-deployment feedback. These interconnected
    components often involve multiple actors across technical, legal, and organizational
    domains. For example, if a hiring platform produces biased outcomes, accountability
    may rest not only with the model developer but also with data providers, interface
    designers, and deploying institutions. Responsible system design requires that
    these relationships be explicitly mapped and governed.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与责任通常由明确定义的开发者或操作者承担的传统软件系统不同，机器学习中的责任是分散的。模型输出受到上游数据收集、训练目标、管道设计、界面行为和部署后反馈的影响。这些相互关联的组件通常涉及技术、法律和组织领域的多个参与者。例如，如果招聘平台产生有偏见的成果，责任可能不仅在于模型开发者，还在于数据提供者、界面设计师和部署机构。负责任的设计要求这些关系被明确映射和治理。
- en: Inadequate governance can prevent institutions from recognizing or correcting
    harmful model behavior. The failure of Google Flu Trends to anticipate distribution
    shift and feedback loops illustrates how opacity in model assumptions and update
    policies can inhibit corrective action. Without visibility into the system’s design
    and data curation, external stakeholders lacked the means to evaluate its validity,
    contributing to the model’s eventual discontinuation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 管理不善可能会阻碍机构识别或纠正有害的模型行为。谷歌流感趋势未能预测分布变化和反馈循环的失败，说明了模型假设和更新政策的透明度不足如何会阻碍纠正措施。在没有对系统设计和数据整理的可见性情况下，外部利益相关者缺乏评估其有效性的手段，这导致了模型最终被停止使用。
- en: Legal frameworks increasingly reflect the necessity of accountable design. Regulations
    such as the [Illinois Artificial Intelligence Video Interview Act](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=4015&ChapterID=68)
    and the [EU AI Act](https://artificialintelligenceact.eu/the-act/) impose requirements
    for transparency, consent, documentation, and oversight in high-risk applications.
    These policies embed accountability not only in the outcomes a system produces,
    but in the operational procedures and documentation that support its use. Internal
    organizational changes, including the introduction of fairness audits and the
    imposition of usage restrictions in targeted advertising systems, demonstrate
    how regulatory pressure can catalyze structural reforms in governance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 法律框架越来越反映了对可问责设计的必要性。例如，[伊利诺伊州人工智能视频面试法案](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=4015&ChapterID=68)和[欧盟人工智能法案](https://artificialintelligenceact.eu/the-act/)等法规对高风险应用中的透明度、同意、文档和监督提出了要求。这些政策不仅将问责制嵌入到系统产生的结果中，还嵌入到支持其使用的操作程序和文档中。内部组织变革，包括引入公平审计和在定向广告系统中实施使用限制，展示了监管压力如何催化治理结构的结构性改革。
- en: Designing for accountability entails supporting traceability at every stage
    of the system lifecycle. This includes documenting data provenance, recording
    model versioning, enabling human overrides, and retaining sufficient logs for
    retrospective analysis. Tools such as [model cards](https://arxiv.org/abs/1810.03993)[18](#fn18)
    and [datasheets for datasets](https://arxiv.org/abs/1803.09010)[19](#fn19) exemplify
    practices that make system behavior interpretable and reviewable. However, accountability
    is not reducible to documentation alone; it also requires mechanisms for feedback,
    contestation, and redress.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为可问责设计而设计意味着在系统生命周期的每个阶段都支持可追溯性。这包括记录数据来源、记录模型版本、启用人工干预，并保留足够的日志以供事后分析。例如，[模型卡片](https://arxiv.org/abs/1810.03993)[18](#fn18)和[数据集数据表](https://arxiv.org/abs/1803.09010)[19](#fn19)等工具展示了使系统行为可解释和可审查的实践。然而，问责制并不仅仅是文档；它还需要反馈、争议和纠正的机制。
- en: Within organizations, governance structures help formalize this responsibility.
    Ethics review processes, cross-functional audits, and model risk committees provide
    forums for anticipating downstream impact and responding to emerging concerns.
    These structures must be supported by infrastructure that allows users to contest
    decisions and developers to respond with corrections. For instance, systems that
    allow explanations or user-initiated reviews help bridge the gap between model
    logic and user experience, especially in domains where the impact of error is
    significant.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织内部，治理结构有助于正式化这一责任。伦理审查流程、跨职能审计和模型风险委员会为预测下游影响和应对新兴问题提供了论坛。这些结构必须由允许用户提出争议和开发者提供更正的基础设施来支持。例如，允许解释或用户发起审查的系统有助于弥合模型逻辑与用户体验之间的差距，尤其是在错误影响重大的领域。
- en: Architectural decisions also play a role. Interfaces can be designed to surface
    uncertainty, allow escalation, or suspend automated actions when appropriate.
    Logging and monitoring pipelines must be configured to detect signs of ethical
    drift, such as performance degradation across subpopulations or unanticipated
    feedback loops. In distributed systems, where uniform observability is difficult
    to maintain, accountability must be embedded through architectural safeguards
    such as secure protocols, update constraints, or trusted components.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 架构决策也发挥着作用。接口可以设计成在适当的时候揭示不确定性、允许升级或暂停自动化操作。日志和监控管道必须配置为检测道德漂移的迹象，如子群体间的性能下降或未预见的反馈循环。在分布式系统中，由于难以保持统一的可观察性，问责制必须通过如安全协议、更新约束或可信组件等架构保障来嵌入。
- en: Governance does not imply centralized control. Instead, it involves distributing
    responsibility in ways that are transparent, actionable, and sustainable. Technical
    teams, legal experts, end users, and institutional leaders must all have access
    to the tools and information necessary to evaluate system behavior and intervene
    when necessary. As machine learning systems become more complex and embedded in
    important infrastructure, accountability must scale accordingly by becoming a
    foundational consideration in both architecture and process, not a reactive layer
    added after deployment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 治理并不意味着集中控制。相反，它涉及以透明、可操作和可持续的方式分配责任。技术团队、法律专家、最终用户和机构领导者都必须能够访问评估系统行为和必要时进行干预所需的工具和信息。随着机器学习系统变得更加复杂并嵌入重要基础设施，问责制必须相应地扩展，成为架构和流程中的基础性考虑，而不是部署后的反应性层。
- en: 'Despite these governance mechanisms, meaningful accountability faces a challenge:
    distinguishing between decisions based on legitimate factors versus spurious correlations
    that may perpetuate historical biases. This challenge requires careful attention
    to data quality, feature selection, and ongoing monitoring to ensure that automated
    decisions reflect fair and justified reasoning rather than problematic patterns
    from biased historical data.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些治理机制，有意义的问责制仍面临挑战：区分基于合法因素的决策与可能延续历史偏见的虚假相关性。这一挑战需要仔细关注数据质量、特征选择和持续监控，以确保自动化决策反映公平和合理的推理，而不是来自有偏历史数据的有问题模式。
- en: The principles and techniques examined above provide the conceptual and technical
    foundation for responsible AI, but their practical implementation depends critically
    on deployment architecture. Cloud systems can support complex SHAP explanations
    and real time fairness monitoring, but TinyML devices must rely on static interpretability
    and compile-time privacy guarantees. Edge deployments enable local privacy preservation
    but limit global fairness assessment. These architectural constraints are not
    mere implementation details; they fundamentally shape which responsible AI protections
    are accessible to different users and applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 上述原则和技术为负责任的AI提供了概念和技术基础，但它们的实际实施在很大程度上取决于部署架构。云系统可以支持复杂的SHAP解释和实时公平性监控，但TinyML设备必须依赖于静态可解释性和编译时隐私保证。边缘部署可以实现本地隐私保护，但限制了全球公平性评估。这些架构限制不仅仅是实现细节；它们从根本上塑造了不同用户和应用可访问的负责任的AI保护。
- en: Responsible AI Across Deployment Environments
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署环境中的负责任AI
- en: Responsible AI principles manifest differently across deployment environments
    due to varying constraints on computation, connectivity, and governance. Cloud
    systems support comprehensive monitoring and complex explainability methods but
    introduce privacy risks through data aggregation. Edge and mobile deployments
    offer stronger data locality but limit post-deployment observability. TinyML systems
    face the most severe constraints, requiring static validation with no opportunity
    for runtime adjustment. Understanding these deployment-specific tradeoffs enables
    engineers to design systems that maximize responsible AI protections within architectural
    constraints.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算、连接和治理方面的不同限制，负责任的AI原则在不同部署环境中表现出不同的形态。云系统支持全面的监控和复杂的可解释性方法，但通过数据聚合引入了隐私风险。边缘和移动部署提供了更强的数据本地性，但限制了部署后的可观察性。TinyML系统面临最严重的限制，需要静态验证，没有运行时调整的机会。理解这些特定于部署的权衡，使工程师能够设计在架构约束内最大化负责任的AI保护的系统。
- en: These architectural differences introduce tradeoffs that affect not only what
    is technically feasible, but also how responsibilities are distributed across
    system components. Resource availability, latency constraints, user interface
    design, and the presence or absence of connectivity all play a role in determining
    whether responsible AI principles can be enforced consistently across deployment
    contexts. The deployment strategies and system architectures discussed in [Chapter 2](ch008.xhtml#sec-ml-systems)
    provide the foundation for understanding how to implement responsible AI across
    these diverse environments.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构差异引入了权衡，不仅影响技术上的可行性，还影响系统组件间责任分配的方式。资源可用性、延迟限制、用户界面设计以及连接性的存在与否，都在决定是否能在部署环境中一致地实施负责任的AI原则中发挥作用。第2章中讨论的部署策略和系统架构为理解如何在这些多样化的环境中实施负责任的AI提供了基础。
- en: Beyond these technical constraints, the geographic and economic distribution
    of computational resources creates additional layers of equity concerns in responsible
    AI deployment. High-performance AI systems typically require proximity to major
    data centers or high-bandwidth internet connections, creating service quality
    disparities that map closely to existing socioeconomic inequalities. Rural communities,
    developing regions, and economically disadvantaged areas often experience degraded
    AI service quality due to network latency, limited bandwidth, and distance from
    computational infrastructure[20](#fn20). This infrastructure gap means that responsible
    AI principles like real time explainability, continuous fairness monitoring, and
    privacy-preserving computation may be practically unavailable to users in these
    contexts.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些技术约束之外，计算资源的地理和经济分布还在负责任的AI部署中创造了额外的公平性关注层。高性能AI系统通常需要靠近主要数据中心或高速互联网连接，这导致了服务质量差异，这些差异与现有的社会经济不平等密切相关。农村社区、发展中地区和经济不发达地区往往由于网络延迟、带宽有限和距离计算基础设施较远，而体验到AI服务质量下降[20](#fn20)。这种基础设施差距意味着，实时可解释性、持续公平性监控和隐私保护计算等负责任的AI原则在这些环境中可能实际上无法为用户所获得。
- en: Understanding how deployment shapes the operational landscape for fairness,
    explainability, safety, privacy, and accountability is important for designing
    machine learning systems that are robust, aligned, and sustainable across real
    world settings.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 理解部署如何影响公平性、可解释性、安全性、隐私性和责任性在操作层面的布局，对于设计在现实世界环境中稳健、一致和可持续的机器学习系统非常重要。
- en: System Explainability
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统可解释性
- en: The first principle to examine in detail is explainability, whose feasibility
    in machine learning systems is deeply shaped by deployment context. While model
    architecture and explanation technique are important factors, system-level constraints,
    including computational capacity, latency requirements, interface design, and
    data accessibility, determine whether interpretability can be supported in a given
    environment. These constraints vary significantly across cloud platforms, mobile
    devices, edge systems, and deeply embedded deployments, affecting both the form
    and timing of explanations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 需要详细考察的第一个原则是可解释性，其在机器学习系统中的可行性深受部署环境的影响。虽然模型架构和解释技术是重要因素，但系统级约束，包括计算能力、延迟要求、界面设计和数据可访问性，决定了在特定环境中是否可以支持可解释性。这些约束在云平台、移动设备、边缘系统和深度嵌入式部署中差异很大，影响着解释的形式和时机。
- en: In high-resource environments, such as centralized cloud systems, techniques
    like SHAP and LIME can be used to generate detailed post hoc explanations, even
    if they require multiple forward passes or sampling procedures. These methods
    are often impractical in latency-sensitive or resource-constrained settings, where
    explanation must be lightweight and fast. On mobile devices or embedded systems,
    methods based on saliency maps[21](#fn21) or input gradients are more feasible,
    as they typically involve a single backward pass. In TinyML deployments, runtime
    explanation may be infeasible altogether, making development-time inspection the
    primary opportunity for ensuring interpretability. Model compression and optimization
    techniques often create tension with explainability requirements, as simplified
    models may be less interpretable than their full-scale counterparts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源丰富的环境中，例如集中式云系统，可以使用SHAP和LIME等技术生成详细的事后解释，即使它们需要多次正向传递或采样过程。这些方法在延迟敏感或资源受限的环境中通常不切实际，在这些环境中，解释必须是轻量级且快速的。在移动设备或嵌入式系统中，基于显著性图[21](#fn21)或输入梯度的方法更为可行，因为它们通常只涉及一次反向传递。在TinyML部署中，运行时解释可能根本不可行，这使得开发时检查成为确保可解释性的主要机会。模型压缩和优化技术通常与可解释性要求产生矛盾，因为简化模型可能不如其全规模模型可解释。
- en: Latency and interactivity also influence the delivery of explanations. In real-time
    systems, such as drones or automated industrial control loops, there may be no
    opportunity to present or compute explanations during operation. Logging internal
    signals or confidence scores for later analysis becomes the primary strategy.
    In contrast, systems with asynchronous interactions, such as financial risk scoring
    or medical diagnosis, allow for deeper and delayed explanations to be rendered
    after the decision has been made.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟性和交互性也会影响解释的传递。在实时系统中，例如无人机或自动化工业控制回路，在运行过程中可能没有机会展示或计算解释。记录内部信号或置信度分数以供后续分析成为主要策略。相比之下，具有异步交互的系统，如金融风险评估或医疗诊断，允许在做出决策后提供更深入和延迟的解释。
- en: Audience requirements further shape design choices. End users typically require
    explanations that are concise, intuitive, and contextually meaningful. For instance,
    a mobile health app might summarize a prediction as “elevated heart rate during
    sleep,” rather than referencing abstract model internals. By contrast, developers,
    auditors, and regulators often need access to attribution maps, concept activations,
    or decision traces to perform debugging, validation, or compliance review. These
    internal explanations must be exposed through developer-facing interfaces or embedded
    within the model development workflow.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 受众需求进一步塑造设计选择。最终用户通常需要简洁、直观且具有情境意义的解释。例如，一个移动健康应用可能会将预测总结为“睡眠期间心率升高”，而不是引用抽象的模型内部。相比之下，开发者、审计员和监管者通常需要访问归因图、概念激活或决策跟踪以进行调试、验证或合规审查。这些内部解释必须通过面向开发者的接口或嵌入到模型开发工作流程中公开。
- en: Explainability also varies across the system lifecycle. During model development,
    interpretability supports diagnostics, feature auditing, and concept verification.
    After deployment, explainability shifts toward runtime behavior monitoring, user
    communication, and post hoc analysis of failure cases. In systems where runtime
    explanation is infeasible, such as in TinyML, design-time validation becomes especially
    important, requiring models to be constructed in a way that anticipates and mitigates
    downstream interpretability failures.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性也因系统生命周期而异。在模型开发期间，可解释性支持诊断、特征审计和概念验证。部署后，可解释性转向运行时行为监控、用户沟通和事后分析失败案例。在运行时解释不可行的系统中，例如在TinyML中，设计时验证变得尤为重要，需要以预测和减轻下游可解释性失败的方式构建模型。
- en: Treating explainability as a system design constraint means planning for interpretability
    from the outset. It must be balanced alongside other deployment requirements,
    including latency budgets, energy constraints, and interface limitations. Responsible
    system design allocates sufficient resources not only for predictive performance,
    but for ensuring that stakeholders can meaningfully understand and evaluate model
    behavior within the operational limits of the deployment environment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将可解释性视为系统设计约束意味着从一开始就计划可解释性。它必须与其他部署要求相平衡，包括延迟预算、能源约束和界面限制。负责任的设计分配了足够的资源，不仅用于预测性能，还确保利益相关者可以在部署环境的操作限制内有意义地理解和评估模型行为。
- en: Fairness presents a parallel set of deployment-specific challenges.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性带来了一系列与部署相关的并行挑战。
- en: Fairness Constraints
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性约束
- en: While fairness can be formally defined, its operationalization is shaped by
    deployment-specific constraints that mirror and extend the challenges seen with
    explainability. Differences in data access, model personalization, computational
    capacity, and infrastructure for monitoring or retraining affect how fairness
    can be evaluated, enforced, and sustained across diverse system architectures.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然公平性可以正式定义，但其实施受到特定于部署的约束的影响，这些约束反映了并扩展了与可解释性相关的挑战。数据访问、模型个性化、计算能力、监控或重新训练的基础设施等方面的差异影响了公平性如何在不同系统架构中得到评估、执行和维持。
- en: A key determinant is data visibility. In centralized environments, such as cloud-hosted
    platforms, developers often have access to large datasets with demographic annotations.
    This allows the use of group-level fairness metrics, fairness-aware training procedures,
    and post hoc auditing. In contrast, decentralized deployments, such as federated
    learning[22](#fn22) clients or mobile applications, typically lack access to global
    statistics due to privacy constraints or fragmented data. On-device learning approaches
    present unique challenges for fairness assessment, as individual devices may have
    limited visibility into global demographic distributions. In such settings, fairness
    interventions must often be embedded during training or dataset curation, as post-deployment
    evaluation may be infeasible.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键决定因素是数据可见性。在集中式环境中，例如云托管平台，开发者通常可以访问带有人口统计注释的大型数据集。这允许使用组级公平性指标、公平性感知训练程序和事后审计。相比之下，去中心化部署，如联邦学习[22](#fn22)客户端或移动应用程序，通常由于隐私约束或数据碎片化而缺乏访问全局统计数据的权限。设备上的学习方法在公平性评估方面提出了独特的挑战，因为单个设备可能对全球人口分布的可见性有限。在这种情况下，公平性干预措施通常必须在训练或数据集整理期间嵌入，因为部署后的评估可能不可行。
- en: Personalization and adaptation mechanisms also influence fairness tradeoffs.
    Systems that deliver a global model to all users may target parity across demographic
    groups. In contrast, locally adapted models such as those embedded in health monitoring
    apps or on-device recommendation engines may aim for individual fairness, ensuring
    consistent treatment of similar users. However, enforcing this is challenging
    in the absence of clear similarity metrics or representative user data. Personalized
    systems that retrain based on local behavior may drift toward reinforcing existing
    disparities, particularly when data from marginalized users is sparse or noisy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化适应机制也影响公平性权衡。向所有用户提供全局模型的系统可能旨在实现人口统计群体间的平等。相比之下，嵌入在健康监测应用程序或设备推荐引擎中的本地适应模型可能旨在实现个体公平，确保对类似用户的一致待遇。然而，在没有明确的相似性指标或代表性用户数据的情况下，执行这一点具有挑战性。基于本地行为重新训练的个性化系统可能会偏向于强化现有的差异，尤其是在边缘化用户的数据稀疏或噪声时。
- en: Real-time and resource-constrained environments impose additional limitations.
    Embedded systems, wearables, or real-time control platforms often cannot support
    runtime fairness monitoring or dynamic threshold adjustment. In these scenarios,
    fairness must be addressed proactively through conservative design choices, including
    balanced training objectives and static evaluation of subgroup performance prior
    to deployment. For example, a speech recognition system deployed on a low-power
    wearable may need to ensure robust performance across different accents at design
    time, since post-deployment recalibration is not possible.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 实时和资源受限的环境带来了额外的限制。嵌入式系统、可穿戴设备或实时控制平台通常无法支持运行时公平性监控或动态阈值调整。在这些情况下，必须通过保守的设计选择来积极解决公平性问题，包括平衡的训练目标和在部署前的静态子群性能评估。例如，部署在低功耗可穿戴设备上的语音识别系统可能需要在设计时确保跨不同口音的稳健性能，因为部署后的重新校准是不可能的。
- en: Decision thresholds and system policies also affect realized fairness. Even
    when a model performs similarly across groups, applying a uniform threshold across
    all users may lead to disparate impacts if score distributions differ. A mobile
    loan approval system, for instance, may systematically under-approve one group
    unless group-specific thresholds are considered. Such decisions must be explicitly
    reasoned about, justified, and embedded into the systems policy logic in advance
    of deployment.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 决策阈值和系统策略也会影响实现的公平性。即使模型在各个群体中的表现相似，如果分数分布不同，在整个用户中应用统一的阈值可能会导致不同的影响。例如，一个移动贷款审批系统，除非考虑特定群体的阈值，否则可能会系统地低估某一群体。这样的决策必须明确推理、证明，并在部署之前嵌入到系统策略逻辑中。
- en: Long-term fairness is further shaped by feedback dynamics. Systems that retrain
    on user behavior, including ranking models, recommender systems, and automated
    decision pipelines, may reinforce historical biases unless feedback loops are
    carefully managed. For example, a hiring platform that disproportionately favors
    candidates from specific institutions may amplify existing inequalities when retrained
    on biased historical outcomes. Mitigating such effects requires governance mechanisms
    that span not only training but also deployment monitoring, data logging, and
    impact evaluation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 长期公平性还受到反馈动态的影响。包括排名模型、推荐系统和自动化决策流程在内的系统，如果反馈循环没有得到妥善管理，可能会加强历史偏见。例如，一个在偏见历史结果上重新训练时，不成比例地偏爱来自特定机构的候选人的招聘平台可能会放大现有的不平等。减轻这种影响需要治理机制，不仅涵盖训练，还包括部署监控、数据记录和影响评估。
- en: 'Fairness, like other responsible AI principles, is not confined to model parameters
    or training scripts. It emerges from a series of decisions across the full system
    lifecycle: data acquisition, model design, policy thresholds, retraining infrastructure,
    and user feedback handling. Treating fairness as a system-level constraint, particularly
    in constrained or decentralized deployments, requires anticipating where tradeoffs
    may arise and ensuring that fairness objectives are embedded into architecture,
    decision rules, and lifecycle management from the outset.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性，像其他负责任的AI原则一样，不仅限于模型参数或训练脚本。它源于整个系统生命周期中的一系列决策：数据采集、模型设计、策略阈值、重新训练基础设施和用户反馈处理。将公平性视为系统级约束，特别是在受限或去中心化部署中，需要预测可能出现权衡的地方，并确保公平性目标从一开始就嵌入到架构、决策规则和生命周期管理中。
- en: The deployment challenges faced by fairness extend to privacy architectures,
    where similar tensions arise between centralized control and distributed constraints.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性面临的部署挑战也扩展到隐私架构，其中在集中控制和分布式约束之间出现了类似的紧张关系。
- en: Privacy Architectures
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私架构
- en: 'Privacy in machine learning systems extends the pattern observed with fairness:
    it is not confined to protecting individual records; it is shaped by how data
    is collected, stored, transmitted, and integrated into system behavior. These
    decisions are tightly coupled to deployment architecture. System-level privacy
    constraints vary widely depending on whether a model is hosted in the cloud, embedded
    on-device, or distributed across user-controlled environments, each presenting
    different challenges for minimizing risk while maintaining functionality.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统中的隐私扩展了公平性的观察模式：它不仅限于保护个人记录；它还受到数据收集、存储、传输以及集成到系统行为中的方式的影响。这些决策与部署架构紧密相连。系统级隐私约束因模型是否托管在云端、嵌入在设备上或分布在使用者控制的多个环境中而大相径庭，每种部署方式都面临着在保持功能的同时最小化风险的不同挑战。
- en: A key architectural distinction is between centralized and decentralized data
    handling. Centralized cloud systems typically aggregate data at scale, enabling
    high-capacity modeling and monitoring. However, this aggregation increases exposure
    to breaches and surveillance, making strong encryption, access control, and auditability
    important. In decentralized deployments, including mobile applications, federated
    learning clients, and TinyML systems, data remains local, reducing central risk
    but limiting global observability. These environments often prevent developers
    from accessing the demographic or behavioral statistics needed to monitor system
    performance or enforce compliance, requiring privacy safeguards to be embedded
    during development.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的架构区别在于集中式和去中心化数据处理。集中式云系统通常在规模上聚合数据，从而实现高容量的建模和监控。然而，这种聚合增加了数据泄露和监控的风险，使得强大的加密、访问控制和可审计性变得重要。在去中心化部署中，包括移动应用程序、联邦学习客户端和TinyML系统，数据保持本地化，降低了中央风险，但限制了全局可观察性。这些环境通常阻止开发者访问用于监控系统性能或执行合规性所需的人口统计或行为统计数据，因此在开发期间需要嵌入隐私保护措施。
- en: Privacy challenges are especially pronounced in systems that personalize behavior
    over time. Applications such as smart keyboards, fitness trackers, or voice assistants
    continuously adapt to users by processing sensitive signals like location, typing
    patterns, or health metrics. Even when raw data is discarded, trained models may
    retain user-specific patterns that can be recovered via inference-time queries.
    In architectures where memory is persistent and interaction is frequent, managing
    long-term privacy requires tight integration of protective mechanisms into the
    model lifecycle.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在随时间个性化行为的系统中，隐私挑战尤为突出。例如，智能键盘、健身追踪器或语音助手等应用程序会通过处理敏感信号（如位置、打字模式或健康指标）来持续适应用户。即使原始数据被丢弃，训练好的模型也可能保留用户特定的模式，这些模式可以通过推理时间查询恢复。在内存持久且交互频繁的架构中，管理长期隐私需要将保护机制紧密集成到模型的生命周期中。
- en: Connectivity assumptions further shape privacy design. Cloud-connected systems
    allow centralized enforcement of encryption protocols and remote deletion policies,
    but may introduce latency, energy overhead, or increased exposure during data
    transmission. In contrast, edge systems typically operate offline or intermittently,
    making privacy enforcement dependent on architectural constraints such as feature
    minimization, local data retention, and compile-time obfuscation. On TinyML devices,
    which often lack persistent storage or update channels, privacy must be engineered
    into the static firmware and model binaries, leaving no opportunity for post-deployment
    adjustment.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 连接性假设进一步塑造了隐私设计。云连接的系统允许集中执行加密协议和远程删除策略，但可能会引入延迟、能源开销或在数据传输过程中增加暴露。相比之下，边缘系统通常在离线或间歇性运行，使得隐私执行依赖于架构约束，如特征最小化、本地数据保留和编译时混淆。在缺乏持久存储或更新通道的TinyML设备上，隐私必须被设计到静态固件和模型二进制文件中，从而在部署后没有调整的机会。
- en: Privacy risks also extend to the serving and monitoring layers. A model with
    logging allowed, or one that updates through active learning, may inadvertently
    expose sensitive information if logging infrastructure is not privacy-aware. For
    example, membership inference attacks can reveal whether a users data was included
    in training by analyzing model outputs. Defending against such attacks requires
    that privacy-preserving measures extend beyond training and into interface design,
    rate limiting, and access control.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私风险也扩展到服务和监控层。如果日志基础设施没有隐私意识，允许日志记录或通过主动学习进行更新的模型可能会无意中泄露敏感信息。例如，成员推理攻击可以通过分析模型输出来揭示用户数据是否包含在训练中。防御此类攻击需要隐私保护措施不仅扩展到训练，还要扩展到界面设计、速率限制和访问控制。
- en: 'Privacy is not determined solely by technical mechanisms but by how users experience
    the system. A model may meet formal privacy definitions and still violate user
    expectations if data collection is opaque or explanations are lacking. Interface
    design plays a central role: systems must clearly communicate what data is collected,
    how it is used, and how users can opt out or revoke consent. In privacy-sensitive
    applications, failure to align with user norms can erode trust even in technically
    compliant systems.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私不是由技术机制决定的，而是由用户如何体验系统决定的。一个模型可能符合正式的隐私定义，但如果数据收集不透明或缺乏解释，仍然可能违反用户期望。界面设计起着核心作用：系统必须清楚地传达收集的数据、如何使用这些数据以及用户如何选择退出或撤销同意。在隐私敏感的应用中，如果与用户规范不一致，即使在技术上合规的系统也可能损害信任。
- en: Architectural decisions thus influence privacy at every stage of the data lifecycle,
    from acquisition and preprocessing to inference and monitoring. Designing for
    privacy involves not only choosing secure algorithms, but also making principled
    tradeoffs based on deployment constraints, user needs, and legal obligations.
    In high-resource settings, this may involve centralized enforcement and policy
    tooling. In constrained environments, privacy must be embedded statically in model
    design and system behavior, often without the possibility of dynamic oversight.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑决策因此影响数据生命周期的每个阶段，从获取和预处理到推理和监控的隐私。为了隐私而设计不仅涉及选择安全的算法，还需要根据部署约束、用户需求和法律义务做出原则性的权衡。在高资源环境中，这可能涉及集中执行和政策工具。在受限环境中，隐私必须静态地嵌入到模型设计和系统行为中，通常没有动态监督的可能性。
- en: Privacy is not a feature to be appended after deployment. It is a system-level
    property that must be planned, implemented, and validated in concert with the
    architectural realities of the deployment environment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私不是部署后附加的功能。它是一个系统级属性，必须与部署环境的架构现实一起规划、实施和验证。
- en: Complementing privacy’s focus on data protection, safety and robustness architectures
    ensure systems behave predictably even when privacy mechanisms cannot prevent
    all risks. While privacy prevents unauthorized data exposure, safety ensures that
    system outputs remain reliable and aligned with human expectations under stress.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 补充隐私对数据保护的关注，安全性和鲁棒性架构确保系统即使在隐私机制无法防止所有风险的情况下也能表现出可预测的行为。虽然隐私防止未经授权的数据泄露，但安全性确保在压力下系统输出保持可靠并与人类期望一致。
- en: Safety and Robustness
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性和鲁棒性
- en: The implementation of safety and robustness in machine learning systems is closely
    shaped by deployment architecture. Systems deployed in dynamic, unpredictable
    environments, including autonomous vehicles, healthcare robotics, and smart infrastructure,
    must manage real-time uncertainty and mitigate the risk of high-impact failures.
    Others, such as embedded controllers or on-device ML systems, require stable and
    predictable operation under resource constraints, limited observability, and restricted
    opportunities for recovery. In all cases, safety and robustness are system-level
    properties that depend not only on model quality, but on how failures are detected,
    contained, and managed in deployment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中实现安全性和鲁棒性受到部署架构的紧密影响。在动态、不可预测的环境中部署的系统，包括自动驾驶汽车、医疗机器人和智能基础设施，必须管理实时不确定性并减轻高影响故障的风险。其他系统，如嵌入式控制器或设备上的机器学习系统，需要在资源受限、可观察性有限和恢复机会受限的情况下实现稳定和可预测的操作。在所有情况下，安全性和鲁棒性都是系统级属性，不仅取决于模型质量，还取决于在部署中如何检测、控制和处理故障。
- en: 'One recurring challenge is distribution shift: when conditions at deployment
    diverge from those encountered during training. Even modest shifts in input characteristics,
    including lighting, sensor noise, or environmental variability, can significantly
    degrade performance if uncertainty is not modeled or monitored. In architectures
    lacking runtime monitoring or fallback mechanisms, such degradation may go undetected
    until failure occurs. Systems intended for real-world variability must be architected
    to recognize when inputs fall outside expected distributions and to either recalibrate
    or defer decisions accordingly.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个反复出现的问题是分布偏移：当部署条件与训练期间遇到的条件不同时。即使输入特征（包括照明、传感器噪声或环境变化）的微小变化，如果没有对不确定性进行建模或监控，也可能显著降低性能。在缺乏运行时监控或回退机制的架构中，这种退化可能直到发生故障才被发现。旨在处理现实世界变化的系统必须设计成能够识别输入是否超出预期分布，并根据情况相应地重新校准或推迟决策。
- en: Adversarial robustness introduces an additional set of architectural considerations.
    In systems that make security-sensitive decisions, including fraud detection,
    content moderation, and biometric verification, adversarial inputs can compromise
    reliability. Mitigating these threats may involve both model-level defenses (e.g.,
    adversarial training, input filtering) and deployment-level strategies, such as
    API[23](#fn23) access control, rate limiting, or redundancy in input validation.
    These protections often impose latency and complexity tradeoffs that must be carefully
    balanced against real-time performance requirements.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗鲁棒性引入了额外的架构考虑因素。在涉及安全敏感决策的系统（如欺诈检测、内容审查和生物识别验证）中，对抗性输入可能会损害可靠性。缓解这些威胁可能涉及模型级别的防御（例如，对抗性训练、输入过滤）和部署级别的策略，如API[23](#fn23)访问控制、速率限制或输入验证的冗余。这些保护通常需要在延迟和复杂性之间进行权衡，必须仔细平衡与实时性能要求。
- en: Latency-sensitive deployments further constrain robustness strategies. In autonomous
    navigation, real-time monitoring, or control systems, decisions must be made within
    strict temporal budgets. Heavyweight robustness mechanisms may be infeasible,
    and fallback actions must be defined in advance. Many such systems rely on confidence
    thresholds, abstention[24](#fn24) logic, or rule-based overrides to reduce risk.
    For example, a delivery robot may proceed only when pedestrian detection confidence
    is high enough; otherwise, it pauses or defers to human oversight. These control
    strategies often reside outside the learned model, but must be tightly integrated
    into the systems safety logic.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对延迟敏感的部署进一步限制了鲁棒性策略。在自主导航、实时监控或控制系统，必须在严格的时序预算内做出决策。重型鲁棒性机制可能不可行，并且必须预先定义回退操作。许多此类系统依赖于置信度阈值、弃权逻辑或基于规则的覆盖来降低风险。例如，送货机器人只有在行人检测置信度足够高时才会继续前进；否则，它会暂停或转交给人工监督。这些控制策略通常位于学习模型之外，但必须紧密集成到系统的安全逻辑中。
- en: TinyML deployments introduce additional constraints. Deployed on microcontrollers
    with minimal memory, no operating system, and no connectivity, these systems cannot
    rely on runtime monitoring or remote updates. Safety and robustness must be engineered
    statically through conservative design, extensive pre-deployment testing, and
    the use of models that are inherently simple and predictable. Once deployed, the
    system must operate reliably under conditions such as sensor degradation, power
    fluctuations, or environmental variation without external intervention or dynamic
    correction.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML部署引入了额外的约束。这些系统部署在具有最小内存、没有操作系统和没有连接性的微控制器上，不能依赖于运行时监控或远程更新。安全和鲁棒性必须通过保守的设计、广泛的预部署测试以及使用本质上简单和可预测的模型来静态地设计。一旦部署，系统必须在传感器退化、电源波动或环境变化等条件下可靠地运行，而不需要外部干预或动态校正。
- en: 'Across all deployment contexts, monitoring and escalation mechanisms are important
    for sustaining robust behavior over time. In cloud or high-resource settings,
    systems may include uncertainty estimators, distributional change detectors, or
    human-in-the-loop feedback loops to detect failure conditions and trigger recovery.
    In more constrained settings, these mechanisms must be simplified or precomputed,
    but the principle remains: robustness is not achieved once, but maintained through
    the ongoing ability to recognize and respond to emerging risks.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有部署环境中，监控和升级机制对于在长时间内维持稳健行为至关重要。在云或高资源设置中，系统可能包括不确定性估计器、分布变化检测器或人工反馈循环，以检测故障条件并触发恢复。在更受限的环境中，这些机制必须简化或预先计算，但原则依然不变：稳健性不是一次性实现的，而是通过持续识别和应对新兴风险的能力来维持的。
- en: Safety and robustness must be treated as emergent system properties. They depend
    on how inputs are sensed and verified, how outputs are acted upon, how failure
    conditions are recognized, and how corrective measures are initiated. A robust
    system is not one that avoids all errors, but one that fails visibly, controllably,
    and safely. In safety-important applications, designing for this behavior is not
    optional; it is a foundational requirement.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和稳健性必须被视为新兴的系统属性。它们取决于输入是如何被感知和验证的，输出是如何被处理的，故障条件是如何被识别的，以及纠正措施是如何被启动的。一个稳健的系统不是避免所有错误的系统，而是一个能够明显、可控、安全地失败的系统。在安全至关重要的应用中，设计这种行为是强制性的，而不是可选的。
- en: These safety and robustness considerations lead to questions of governance and
    accountability, which must also adapt to deployment constraints.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些安全和稳健性考虑导致了对治理和问责制的疑问，这些疑问也必须适应部署限制。
- en: Governance Structures
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 治理结构
- en: Accountability in machine learning systems must be realized through concrete
    architectural choices, interface designs, and operational procedures. Governance
    structures make responsibility actionable by defining who is accountable for system
    outcomes, under what conditions, and through what mechanisms. These structures
    are deeply influenced by deployment architecture. The degree to which accountability
    can be traced, audited, and enforced varies across centralized, mobile, edge,
    and embedded environments, each posing distinct challenges for maintaining system
    oversight and integrity.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，问责制必须通过具体的架构选择、界面设计和操作程序来实现。治理结构通过定义谁对系统结果负责、在什么条件下以及通过什么机制负责，使责任变得可执行。这些结构深受部署架构的影响。问责制可追溯、审计和执行的程度在集中式、移动、边缘和嵌入式环境中各不相同，每个环境都为维持系统监督和完整性提出了独特的挑战。
- en: In centralized systems, such as cloud-hosted platforms, governance is typically
    supported by robust infrastructure for logging, version control, and real-time
    monitoring. Model registries, telemetry[25](#fn25) dashboards, and structured
    event pipelines allow teams to trace predictions to specific models, data inputs,
    or configuration states.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在集中式系统中，例如云托管平台，治理通常由强大的日志记录、版本控制和实时监控基础设施支持。模型注册表、遥测[25](#fn25)仪表板和结构化事件管道允许团队追踪预测到特定的模型、数据输入或配置状态。
- en: 'In contrast, edge deployments distribute intelligence to devices that may operate
    independently from centralized infrastructure. Embedded models in vehicles, factories,
    or homes must support localized mechanisms for detecting abnormal behavior, triggering
    alerts, and escalating issues. For example, an industrial sensor might flag anomalies
    when its prediction confidence drops, initiating a predefined escalation process.
    Designing for such autonomy requires forethought: engineers must determine what
    signals to capture, how to store them locally, and how to reassign responsibility
    when connectivity is intermittent or delayed.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，边缘部署将智能分布到可能独立于集中式基础设施运行的设备。在车辆、工厂或家庭中的嵌入式模型必须支持本地机制来检测异常行为、触发警报和升级问题。例如，一个工业传感器可能会在其预测置信度下降时标记异常，启动预定义的升级流程。为这种自主性设计需要深思熟虑：工程师必须确定要捕获哪些信号，如何本地存储它们，以及当连接断开或延迟时如何重新分配责任。
- en: Mobile deployments, such as personal finance apps or digital health tools, exist
    at the intersection of user interfaces and backend systems. When something goes
    wrong, it is often unclear whether the issue lies with a local model, a remote
    service, or the broader design of the user interaction. Governance in these settings
    must account for this ambiguity. Effective accountability requires clear documentation,
    accessible recourse pathways, and mechanisms for surfacing, explaining, and contesting
    automated decisions at the user level. The ability to understand and appeal outcomes
    must be embedded into both the interface and the surrounding service architecture.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 移动部署，如个人金融应用或数字健康工具，存在于用户界面和后端系统之间的交叉点。当出现问题的时候，往往不清楚问题出在本地模型、远程服务还是更广泛的用户交互设计上。在这些环境中，治理必须考虑到这种不确定性。有效的责任需要清晰的文档、可访问的救济途径，以及向用户层面展示、解释和质疑自动化决策的机制。理解和申诉结果的能力必须嵌入到界面和周围的服务架构中。
- en: In TinyML deployments, governance is especially constrained. Devices may lack
    connectivity, persistent storage, or runtime configurability, limiting opportunities
    for dynamic oversight or intervention. Here, accountability must be embedded statically
    through mechanisms such as cryptographic firmware signatures, fixed audit trails,
    and pre-deployment documentation of training data and model parameters. In some
    cases, governance must be enforced during manufacturing or provisioning, since
    no post-deployment correction is possible. These constraints make the design of
    governance structures inseparable from early-stage architectural decisions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在TinyML部署中，治理受到特别限制。设备可能缺乏连接性、持久存储或运行时可配置性，这限制了动态监督或干预的机会。在这里，责任必须通过诸如加密固件签名、固定审计跟踪和部署前训练数据和模型参数的文档等机制静态嵌入。在某些情况下，治理必须在制造或配置期间强制执行，因为部署后无法进行纠正。这些限制使得治理结构的设计与早期架构决策不可分割。
- en: Interfaces also play a important role in enabling accountability. Systems that
    surface explanations, expose uncertainty estimates, or allow users to query decision
    histories make it possible for developers, auditors, or users to understand both
    what occurred and why. By contrast, opaque APIs, undocumented thresholds, or closed-loop
    decision systems inhibit oversight. Effective governance requires that information
    flows be aligned with stakeholder needs, including technical, regulatory, and
    user-facing aspects, so that failure modes are observable and remediable.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 界面在启用责任方面也发挥着重要作用。能够展示解释、暴露不确定性估计或允许用户查询决策历史的系统，使得开发者、审计员或用户能够理解发生了什么以及为什么发生。相比之下，不透明的API、未记录的阈值或闭环决策系统阻碍了监督。有效的治理需要信息流与利益相关者的需求对齐，包括技术、监管和面向用户方面，以便失败模式是可观察和可修复的。
- en: Governance approaches must also adapt to domain-specific risks and institutional
    norms. High-stakes applications, such as healthcare or criminal justice, often
    involve legally mandated impact assessments and audit trails. Lower-risk domains
    may rely more heavily on internal practices, shaped by customer expectations,
    reputational concerns, or technical conventions. Regardless of the setting, governance
    must be treated as a system-level design property, not an external policy overlay.
    It is implemented through the structure of codebases, deployment pipelines, data
    flows, and decision interfaces.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 治理方法还必须适应特定领域的风险和制度规范。高风险应用，如医疗保健或刑事司法，通常涉及法律规定的影响评估和审计跟踪。低风险领域可能更多地依赖内部实践，这些实践由客户期望、声誉担忧或技术惯例塑造。无论在何种情况下，治理都必须被视为系统级设计属性，而不是外部政策叠加。它是通过代码库的结构、部署管道、数据流和决策接口来实现的。
- en: 'Sustaining accountability across diverse deployment environments requires planning
    not only for success, but for failure. This includes defining how anomalies are
    detected, how roles are assigned, how records are maintained, and how remediation
    occurs. These processes must be embedded in infrastructure: traceable in logs,
    enforceable through interfaces, and resilient to the architectural constraints
    of the systems deployment context.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 维护不同部署环境中的责任需要规划不仅针对成功，也针对失败。这包括定义如何检测异常、如何分配角色、如何维护记录以及如何进行补救。这些流程必须嵌入到基础设施中：在日志中可追踪、通过接口可强制执行，并能够抵御系统部署环境中的架构限制。
- en: Responsible AI governance increasingly must account for the environmental and
    distributional impacts of computational infrastructure choices. Organizations
    deploying AI systems bear responsibility not only for algorithmic outcomes but
    for the broader systemic impacts of their resource utilization patterns on environmental
    justice and equitable access, as discussed in the context of resource requirements
    and equity implications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI治理越来越必须考虑计算基础设施选择的环境和分配影响。部署AI系统的组织不仅对算法结果负责，还要对其资源利用模式对环境正义和公平获取的更广泛系统性影响负责，正如在资源需求和公平影响背景下所讨论的。
- en: Design Tradeoffs
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计权衡
- en: 'The governance challenges examined across different deployment contexts reveal
    a fundamental truth: deployment environments impose fundamental constraints that
    create tradeoffs in responsible AI implementation. Machine learning systems do
    not operate in idealized silos; they must navigate competing objectives under
    finite resources, strict latency requirements, evolving user behavior, and regulatory
    complexity.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的部署环境中考察的治理挑战揭示了基本真理：部署环境施加了基本约束，这些约束在负责任的AI实施中创造了权衡。机器学习系统不在理想化的孤岛中运行；它们必须在有限资源、严格的延迟要求、不断变化的使用行为和监管复杂性下，在相互竞争的目标之间进行导航。
- en: Cloud based systems often support extensive monitoring, fairness audits, interpretability
    services, and privacy preserving tools due to ample computational and storage
    resources. However, these benefits typically come with centralized data handling,
    which introduces risks related to surveillance, data breaches, and complex governance.
    In contrast, on device systems such as mobile applications, edge platforms, or
    TinyML deployments provide stronger data locality and user control, but limit
    post deployment visibility, fairness instrumentation, and model adaptation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于充足的计算和存储资源，基于云的系统通常支持广泛的监控、公平性审计、可解释性服务和隐私保护工具。然而，这些好处通常伴随着集中式数据处理，这引入了与监控、数据泄露和复杂治理相关的风险。相比之下，移动应用程序、边缘平台或TinyML部署等设备系统提供了更强的数据本地性和用户控制，但限制了部署后的可见性、公平性工具和模型适应性。
- en: Tensions between goals often become apparent at the architectural level. For
    example, systems with real time response requirements, such as wearable gesture
    recognition or autonomous braking, cannot afford to compute detailed interpretability
    explanations during inference. Designers must choose whether to precompute simplified
    outputs, defer explanation to asynchronous analysis, or omit interpretability
    altogether in runtime settings.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 目标之间的紧张关系通常在架构层面变得明显。例如，对于需要实时响应的系统，如可穿戴手势识别或自动制动，在推理过程中无法承担计算详细的可解释性解释。设计者必须选择是否预先计算简化的输出，将解释推迟到异步分析，或在运行时设置中完全省略可解释性。
- en: Conflicts also emerge between personalization and fairness. Systems that adapt
    to individuals based on local usage data often lack the global context necessary
    to assess disparities across population subgroups. Ensuring that personalized
    predictions do not result in systematic exclusion requires careful architectural
    design, balancing user level adaptation with mechanisms for group level equity
    and auditability.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化与公平性之间也出现了冲突。基于本地使用数据对个人进行适应的系统通常缺乏评估人口子群体间差异所需的全球背景。确保个性化预测不会导致系统性的排斥，需要仔细的架构设计，平衡用户级别的适应性与群体级别的公平性和可审计性机制。
- en: Privacy and robustness objectives can also conflict. Robust systems often benefit
    from logging rare events or user outliers to improve reliability. However, recording
    such data may conflict with privacy goals or violate legal constraints on data
    minimization. In settings where sensitive behavior must remain local or encrypted,
    robustness must be designed into the model architecture and training procedure
    in advance, since post hoc refinement may not be feasible.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私性和鲁棒性目标也可能存在冲突。鲁棒的系统通常从记录罕见事件或用户异常数据中受益，以提高可靠性。然而，记录此类数据可能与隐私目标相冲突，或违反数据最小化的法律限制。在敏感行为必须保持本地或加密的环境中，鲁棒性必须预先设计到模型架构和训练过程中，因为事后的改进可能不可行。
- en: The computational demands of responsible AI create tensions that extend beyond
    technical optimization to questions of environmental justice and equitable access.
    Energy-efficient deployment often requires simplified models with reduced fairness
    monitoring capabilities, creating a tradeoff between environmental sustainability
    and ethical safeguards. For example, implementing differential privacy in federated
    learning can increase per-device energy consumption by 25-40%, potentially making
    such privacy protections prohibitive for battery-constrained devices[26](#fn26).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI的计算需求产生了超越技术优化的紧张关系，涉及到环境正义和公平获取的问题。节能部署通常需要简化模型并减少公平性监控能力，这就在环境可持续性和道德保障之间产生了权衡。例如，在联邦学习中实施差分隐私可能会使每台设备的能耗增加25-40%，这可能会使这种隐私保护对电池受限的设备来说变得不可承受[26](#fn26)。
- en: These examples illustrate a broader systems level challenge. Responsible AI
    principles cannot be considered in isolation. They interact, and optimizing for
    one may constrain another. The appropriate balance depends on deployment architecture,
    stakeholder priorities, domain specific risks, the consequences of error, and
    increasingly, the environmental and distributional impacts of computational resource
    requirements.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子说明了更广泛的系统级挑战。负责任的AI原则不能孤立考虑。它们相互作用，优化一个可能会限制另一个。适当的平衡取决于部署架构、利益相关者的优先级、特定领域的风险、错误的后果，以及越来越重要的是计算资源需求的环境和分配影响。
- en: What distinguishes responsible machine learning design is not the elimination
    of tradeoffs, but the clarity and deliberateness with which they are navigated.
    Design decisions must be made transparently, with a full understanding of the
    limitations imposed by the deployment environment and the impacts of those decisions
    on system behavior.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习设计区别于其他设计的不是消除权衡，而是在于清晰和深思熟虑地处理这些权衡。设计决策必须透明化，全面理解部署环境带来的限制以及这些决策对系统行为的影响。
- en: To synthesize these insights, [Table 17.2](ch023.xhtml#tbl-ml-principles-comparison)
    summarizes the architectural tensions by comparing how responsible AI principles
    manifest across cloud, mobile, edge, and TinyML systems. Each setting imposes
    different constraints on explainability, fairness, privacy, safety, and accountability,
    based on factors such as compute capacity, connectivity, data access, and governance
    feasibility.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了综合这些见解，[表17.2](ch023.xhtml#tbl-ml-principles-comparison)通过比较负责任的AI原则如何在云、移动、边缘和TinyML系统中体现，总结了架构上的紧张关系。每个设置都根据计算能力、连接性、数据访问和治理可行性等因素，对可解释性、公平性、隐私、安全性和问责制施加不同的限制。
- en: As [Table 17.2](ch023.xhtml#tbl-ml-principles-comparison) reveals, no deployment
    context dominates across all principles; each makes different compromises. Cloud
    systems support complex explainability methods (SHAP, LIME) and centralized fairness
    monitoring but introduce privacy risks through data aggregation. Edge and mobile
    deployments offer stronger data locality but limit post-deployment observability
    and global fairness assessment. TinyML systems face the most severe constraints,
    requiring static validation and compile-time privacy guarantees with no opportunity
    for runtime adjustment. These constraints are not merely technical limitations
    but shape which responsible AI features are accessible to different users and
    applications, creating equity implications where only well-resourced deployments
    can afford comprehensive safeguards. Understanding these deployment constraints
    provides necessary context for the technical methods that operationalize responsible
    AI principles in practice.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表17.2](ch023.xhtml#tbl-ml-principles-comparison)所示，没有哪个部署环境在所有原则上都占主导地位；每个都做出了不同的妥协。云系统支持复杂的可解释性方法（SHAP、LIME）和集中式公平性监控，但通过数据聚合引入了隐私风险。边缘和移动部署提供了更强的数据本地性，但限制了部署后的可观察性和全球公平性评估。TinyML系统面临最严重的限制，需要静态验证和编译时隐私保证，没有运行时调整的机会。这些限制不仅仅是技术限制，它们决定了不同用户和应用可访问的负责任AI功能，创造了只有资源充足的部署才能负担得起全面保障的公平性问题。理解这些部署限制为在实践中实现负责任的AI原则的技术方法提供了必要的背景。
- en: 'Table 17.2: **Deployment Trade-Offs**: Responsible AI principles manifest differently
    across deployment contexts due to varying constraints on compute, connectivity,
    and governance; cloud deployments support complex explainability methods, while
    TinyML severely limits them. Prioritizing certain principles like explainability,
    fairness, privacy, safety, and accountability requires careful consideration of
    these constraints when designing machine learning systems for cloud, edge, mobile,
    and TinyML environments.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.2：**部署权衡**：由于计算、连接和治理的限制不同，负责任的AI原则在不同部署环境中表现出不同的形式；云部署支持复杂的可解释性方法，而微型机器学习则严重限制了这些方法。优先考虑某些原则，如可解释性、公平性、隐私、安全性和问责制，在设计云、边缘、移动和微型机器学习环境中的机器学习系统时，需要仔细考虑这些限制。
- en: '| **Principle** | **Cloud ML** | **Edge ML** | **Mobile ML** | **TinyML** |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **原则** | **云机器学习** | **边缘机器学习** | **移动机器学习** | **微型机器学习** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Explainability** | Supports complex models and methods like SHAP and sampling
    approaches | Needs lightweight, low-latency methods like saliency maps | Requires
    interpretable outputs for users, often defers deeper analysis to the cloud | Severely
    limited due to constrained hardware; mostly static or compile-time only |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **可解释性** | 支持复杂模型和方法，如SHAP和采样方法 | 需要轻量级、低延迟的方法，如显著性图 | 需要用户可解释的输出，通常将更深入的分析推迟到云端
    | 由于硬件限制，严重受限；大多数情况下仅限于静态或编译时 |'
- en: '| **Fairness** | Large datasets allow bias detection and mitigation | Localized
    biases harder to detect but allows on-device adjustments | High personalization
    complicates group-level fairness tracking | Minimal data limits bias analysis
    and mitigation |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **公平性** | 大数据集允许检测和缓解偏差 | 本地偏差难以检测，但允许设备上的调整 | 高度个性化使群体层面的公平性跟踪复杂化 | 数据量最小限制了偏差分析和缓解
    |'
- en: '| **Privacy** | Centralized data at risk of breaches but can utilize strong
    encryption and differential privacy methods | Sensitive personal data on-device
    requires on-device protections | Tight coupling to user identity requires consent-aware
    design and local processing | Distributed data reduces centralized risks but poses
    challenges for anonymization |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **隐私** | 集中的数据可能遭受泄露，但可以利用强大的加密和差分隐私方法 | 设备上的敏感个人数据需要设备保护 | 与用户身份的紧密耦合需要知情同意的设计和本地处理
    | 分布式数据减少了集中化风险，但给匿名化带来了挑战 |'
- en: '| **Safety** | Vulnerable to hacking and large-scale attacks | Real-world interactions
    make reliability important | Operates under user supervision, but still requires
    graceful failure | Needs distributed safety mechanisms due to autonomy |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **安全性** | 易受黑客攻击和大规模攻击 | 现实世界的交互使可靠性变得重要 | 在用户监督下运行，但仍需要优雅的失败处理 | 由于自主性，需要分布式安全机制
    |'
- en: '| **Accountability** | Corporate policies and audits allow traceability and
    oversight | Fragmented supply chains complicate accountability | Requires clear
    user-facing disclosures and feedback paths | Traceability required across long,
    complex hardware chains |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **问责制** | 公司政策和审计允许追溯和监督 | 分散的供应链使问责制复杂化 | 需要清晰的面向用户的披露和反馈路径 | 需要在长而复杂的硬件链中实现可追溯性
    |'
- en: '| **Governance** | External oversight and regulations like GDPR or CCPA are
    feasible | Requires self-governance by developers and integrators | Balances platform
    policy with app developer choices | Relies on built-in protocols and cryptographic
    assurances |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **治理** | 外部监督和法规，如GDPR或CCPA是可行的 | 需要开发者和集成商的自我治理 | 平衡平台政策与应用开发者选择 | 依赖于内置协议和加密保证
    |'
- en: Technical Foundations
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术基础
- en: 'Responsible machine learning requires technical methods that translate ethical
    principles into concrete system behaviors. These methods address practical challenges:
    detecting bias, preserving privacy, ensuring robustness, and providing interpretability.
    Success depends on how well these techniques work within real system constraints
    including data quality, computational resources, and deployment requirements.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习需要将伦理原则转化为具体系统行为的技术方法。这些方法解决实际挑战：检测偏差、保护隐私、确保鲁棒性，并提供可解释性。成功取决于这些技术在包括数据质量、计算资源和部署要求在内的实际系统约束下的工作效果。
- en: Understanding why these methods are necessary begins with recognizing how machine
    learning systems can develop problematic behaviors. Models learn patterns from
    training data, including historical biases and unfair associations. For example,
    a hiring algorithm trained on biased historical data will learn to replicate discriminatory
    patterns, associating certain demographic characteristics with success.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 了解为什么这些方法是必要的，首先要认识到机器学习系统如何发展出问题行为。模型从训练数据中学习模式，包括历史偏见和不公平的关联。例如，在具有偏见的历史数据上训练的招聘算法将学会复制歧视性模式，将某些人口统计特征与成功相关联。
- en: This happens because machine learning models learn correlations rather than
    understanding causation. They identify statistical patterns that may reflect unfair
    social structures instead of meaningful relationships. This systematic bias favors
    groups that were historically advantaged in the training data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为机器学习模型学习的是相关性而不是因果关系。它们识别的统计模式可能反映了不公平的社会结构，而不是有意义的关系。这种系统性偏差有利于在训练数据中历史上处于优势地位的群体。
- en: Addressing these issues requires more than simple corrections after training.
    Traditional machine learning optimizes only for accuracy, creating tension with
    fairness goals. Effective solutions must integrate fairness considerations directly
    into the learning process rather than treating them as secondary concerns.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题需要训练之后的不仅仅是简单的修正。传统的机器学习仅优化准确性，与公平性目标产生冲突。有效的解决方案必须将公平性考虑直接整合到学习过程中，而不是将其视为次要问题。
- en: Each technical approach involves specific tradeoffs between accuracy, computational
    cost, and implementation complexity. These methods are not universally applicable
    and must be chosen based on system requirements and constraints. Framework selection
    affects which responsible AI techniques can be practically implemented.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 每种技术方法都涉及准确性、计算成本和实现复杂度之间的特定权衡。这些方法并非普遍适用，必须根据系统需求和约束来选择。框架选择影响哪些负责任的人工智能技术可以实际实施。
- en: This section examines practical techniques for implementing responsible AI principles.
    Each method serves specific purposes within the system and comes with particular
    requirements and performance impacts. These tools work together to create trustworthy
    machine learning systems.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了实施负责任人工智能原则的实用技术。每种方法在系统中都有特定的目的，并带有特定的要求和性能影响。这些工具共同工作，以创建值得信赖的机器学习系统。
- en: The technical approaches to responsible AI can be organized into three complementary
    categories. Detection methods identify when systems exhibit problematic behaviors,
    providing early warning systems for bias, drift, and performance issues. Mitigation
    techniques actively prevent harmful outcomes through algorithmic interventions
    and robustness enhancements. Validation approaches provide mechanisms for understanding
    and explaining system behavior to stakeholders who evaluate automated decisions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任人工智能的技术方法可以分为三个互补的类别。检测方法识别系统何时表现出问题行为，为偏差、漂移和性能问题提供早期预警系统。缓解技术通过算法干预和鲁棒性增强积极预防有害结果。验证方法为理解并解释系统行为提供机制，以便评估自动化决策的利益相关者。
- en: Computational Overhead of Responsible AI Techniques
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负责任人工智能技术的计算开销
- en: Implementing responsible AI principles incurs quantifiable computational costs
    that must be considered during system design. Understanding these performance
    impacts enables engineers to make informed decisions about which techniques to
    implement based on available computational resources and quality requirements.
    [Table 17.3](ch023.xhtml#tbl-responsible-ai-overhead) provides a systematic comparison
    of the computational overhead introduced by different responsible AI techniques.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 实施负责任的人工智能原则会带来可量化的计算成本，这些成本必须在系统设计阶段考虑。了解这些性能影响使工程师能够根据可用的计算资源和质量要求，做出关于实施哪些技术的明智决策。[表17.3](ch023.xhtml#tbl-responsible-ai-overhead)提供了不同负责任人工智能技术引入的计算开销的系统比较。
- en: 'Table 17.3: **Performance Impact of Responsible AI Techniques**: Quantitative
    analysis reveals that responsible AI techniques impose measurable computational
    overhead across training and inference phases. Differential privacy and fairness
    constraints add modest overhead while explainability methods can significantly
    increase inference costs. These metrics help engineers optimize responsible AI
    implementations for production constraints.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.3：**负责任人工智能技术性能影响**：定量分析显示，负责任人工智能技术在训练和推理阶段都产生了可衡量的计算开销。差分隐私和公平性约束增加了适度的开销，而可解释性方法可以显著增加推理成本。这些指标有助于工程师针对生产约束优化负责任人工智能的实现。
- en: '| **Technique** | **Accuracy Impact** | **Training Overhead** | **Inference
    Cost** | **Memory Overhead** |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **准确度影响** | **训练开销** | **推理成本** | **内存开销** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Differential Privacy** | -2% to -5% | +15% to +30% | Minimal | +10% to
    +20% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| **差分隐私** | -2%到-5% | +15%到+30% | 最小 | +10%到+20% |'
- en: '| **(DP-SGD)** |  |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| **（DP-SGD）** |  |  |  |  |'
- en: '| **Fairness-Aware Training** | -1% to -3% | +5% to +15% | Minimal | +5% to
    +10% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| **公平性感知训练** | -1%到-3% | +5%到+15% | 最小 | +5%到+10% |'
- en: '| **(Reweighting/Constraints)** |  |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| **（重新加权/约束）** |  |  |  |  |'
- en: '| **SHAP Explanations** | N/A | N/A | +50% to +200% | +20% to +100% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| **SHAP解释** | N/A | N/A | +50%到+200% | +20%到+100% |'
- en: '| **Adversarial Training** | +2% to +5% | +100% to +300% | Minimal | +50% to
    +100% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| **对抗训练** | +2%到+5% | +100%到+300% | 最小 | +50%到+100% |'
- en: '| **Federated Learning** | -5% to -15% | +200% to +500% | Minimal | +100% to
    +300% |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **联邦学习** | -5%到-15% | +200%到+500% | 最小 | +100%到+300% |'
- en: 'The performance numbers in [Table 17.3](ch023.xhtml#tbl-responsible-ai-overhead)
    represent typical ranges across published benchmarks and production systems.[27](#fn27)
    Actual overhead varies significantly based on model architecture, dataset size,
    and implementation quality. For example, SHAP on linear models adds approximately
    10 ms, while SHAP on deep ensembles can add over 1000 ms. Adversarial training
    overhead depends on attack strength: PGD-7 adds roughly 150% overhead, while PGD-50
    adds approximately 300%. Federated learning overhead is dominated by communication
    rounds and client heterogeneity.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[表17.3](ch023.xhtml#tbl-responsible-ai-overhead)中的性能数字代表了已发布基准和实际系统中的典型范围。[27](#fn27)
    实际开销根据模型架构、数据集大小和实现质量而有很大差异。例如，SHAP在线性模型上增加大约10 ms，而在深度集成模型上可以增加超过1000 ms。对抗训练的开销取决于攻击强度：PGD-7增加大约150%的开销，而PGD-50增加大约300%。联邦学习的开销主要由通信轮次和客户端异构性主导。'
- en: These computational costs create significant equity considerations examined
    in multiple contexts. Organizations with limited resources may be unable to implement
    responsible AI techniques, potentially creating disparate access to ethical AI
    protections, a theme that emerges repeatedly in deployment contexts, implementation
    challenges, and organizational barriers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算成本在多个背景下引发了重大的公平性考虑。资源有限的组织可能无法实施负责任人工智能技术，这可能导致对道德人工智能保护的差异访问，这是一个在部署环境、实施挑战和组织障碍中反复出现的话题。
- en: Detection methods form the foundation for all other responsible AI interventions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 检测方法是所有其他负责任人工智能干预措施的基础。
- en: Bias and Risk Detection Methods
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差和风险检测方法
- en: Detection methods provide the foundational capability to identify when machine
    learning systems exhibit problematic behaviors that compromise responsible AI
    principles. These techniques serve as the early warning systems that alert practitioners
    to bias, drift, and performance degradation before they cause significant harm.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 检测方法为识别机器学习系统出现损害负责任人工智能原则的问题行为提供了基础能力。这些技术作为早期预警系统，在它们造成重大损害之前，提醒从业者注意偏差、漂移和性能退化。
- en: Bias Detection and Mitigation
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏差检测与缓解
- en: 'Operationalizing fairness in deployed systems requires more than principled
    objectives or theoretical metrics; it demands system-aware methods that detect,
    measure, and mitigate bias across the machine learning lifecycle. Practical bias
    detection can be implemented using tools like Fairlearn[28](#fn28) ([Bird et al.
    2020](ch058.xhtml#ref-bird2020fairlearn)):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署系统中实现公平性需要不仅仅是原则性目标或理论指标；它需要系统感知的方法，这些方法可以在机器学习生命周期中检测、测量和缓解偏差。实际偏差检测可以使用Fairlearn[28](#fn28)
    ([Bird等，2020](ch058.xhtml#ref-bird2020fairlearn))等工具实现：
- en: 'Listing 17.1: **Bias Detection with Fairlearn**: Systematic evaluation of loan
    approval model performance across demographic groups reveals potential disparities
    in approval rates and false positive rates that could indicate discriminatory
    patterns requiring intervention.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1：**使用Fairlearn进行偏差检测**：对贷款审批模型性能在人口群体间的系统性评估揭示了潜在的批准率和假阳性率差异，这些差异可能表明需要干预的歧视性模式。
- en: '[PRE0]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As demonstrated in [Listing 17.1](ch023.xhtml#lst-bias-detection), this approach
    enables systematic monitoring of fairness across demographic groups during deployment,
    revealing concerning disparities where loan approval rates vary dramatically by
    ethnicity: from 94% for Asian applicants to 68% for Black applicants. Building
    on the system-level constraints discussed earlier, fairness must be treated as
    an architectural consideration that intersects with data engineering, model training,
    inference design, monitoring infrastructure, and policy governance. While fairness
    metrics such as demographic parity, equalized odds, and equality of opportunity
    formalize different normative goals, their realization depends on the architecture’s
    ability to measure subgroup performance, support adaptive decision boundaries,
    and store or surface group-specific metadata during runtime.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表17.1](ch023.xhtml#lst-bias-detection)所示，这种方法使部署期间对人口群体间的公平性进行系统性监控成为可能，揭示了令人担忧的差异，其中贷款批准率因种族而异：亚洲申请人的批准率为94%，而黑人申请人的批准率为68%。基于之前讨论的系统级约束，公平性必须被视为一个与数据工程、模型训练、推理设计、监控基础设施和政策治理交叉的架构考虑因素。虽然人口统计平等、均衡机会和机会均等等公平性指标形式化了不同的规范性目标，但它们的实现取决于架构测量子群体性能、支持自适应决策边界以及在运行时存储或呈现特定于组的元数据的能力。
- en: Practical implementation is often shaped by limitations in data access and system
    instrumentation. In many real-world environments, especially in mobile, federated,
    or embedded systems, sensitive attributes such as gender, age, or race may not
    be available at inference time, making it difficult to track or audit model performance
    across demographic groups. The data collection and labeling strategies discussed
    in [Chapter 6](ch012.xhtml#sec-data-engineering) are essential for fairness assessment
    throughout the model lifecycle. In such contexts, fairness interventions must
    occur upstream during data curation or training, as post-deployment recalibration
    may not be feasible. Even when data is available, continuous retraining pipelines
    that incorporate user feedback can reinforce existing disparities unless explicitly
    monitored for fairness degradation. For example, an on-device recommendation model
    that adapts to user behavior may amplify prior biases if it lacks the infrastructure
    to detect demographic imbalances in user interactions or outputs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实际实施往往受到数据访问和系统仪表化的限制。在许多现实世界环境中，尤其是在移动、联邦或嵌入式系统中，敏感属性如性别、年龄或种族可能在推理时不可用，这使得跟踪或审计模型性能跨人口群体变得困难。第6章中讨论的数据收集和标注策略对于整个模型生命周期的公平性评估至关重要。在这种情况下，公平干预必须在数据整理或训练阶段上游进行，因为部署后的重新校准可能不可行。即使数据可用，结合用户反馈的持续重新训练管道也可能加剧现有差异，除非明确监控公平性下降。例如，一个适应用户行为的设备上推荐模型，如果缺乏检测用户交互或输出中人口统计不平衡的基础设施，可能会放大先前的偏见。
- en: '[Figure 17.2](ch023.xhtml#fig-fairness-example) illustrates how fairness constraints
    can introduce tension with deployment choices. In a binary loan approval system,
    two subgroups, Subgroup A, represented in blue, and Subgroup B, represented in
    red, require different decision thresholds to achieve equal true positive rates.
    Using a single threshold across groups leads to disparate outcomes, potentially
    disadvantaging Subgroup B. Addressing this imbalance by adjusting thresholds per
    group may improve fairness, but doing so requires support for conditional logic
    in the model serving stack, access to sensitive attributes at inference time,
    and a governance framework for explaining and justifying differential treatment
    across groups.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.2](ch023.xhtml#fig-fairness-example)说明了公平约束如何与部署选择产生紧张关系。在一个二元贷款审批系统中，两个子组，子组A用蓝色表示，子组B用红色表示，需要不同的决策阈值来实现相同的真正阳性率。在组间使用单个阈值会导致不同的结果，可能对子组B不利。通过调整每个组的阈值来解决这种不平衡可能提高公平性，但这需要支持模型服务堆栈中的条件逻辑，在推理时访问敏感属性，以及一个治理框架来解释和证明跨组差异处理的合理性。'
- en: '![](../media/file291.svg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file291.svg)'
- en: 'Figure 17.2: **Threshold-Dependent Fairness**: Varying classification thresholds
    across subgroups allows equal true positive rates but introduces complexity in
    model serving and necessitates access to sensitive attributes at inference time.
    Achieving fairness requires careful consideration of subgroup-specific performance,
    as a single threshold may disproportionately impact certain groups, highlighting
    the tension between accuracy and equitable outcomes in machine learning systems.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：**阈值依赖公平性**：在子群体中变化分类阈值允许相同的真正阳性率，但引入了模型服务的复杂性，并在推理时需要访问敏感属性。实现公平性需要对特定子群体的性能进行仔细考虑，因为单个阈值可能不成比例地影响某些群体，突显了机器学习系统中准确性和公平结果之间的紧张关系。
- en: Fairness interventions may be applied at different points in the pipeline, but
    each comes with system-level implications. Preprocessing methods, which rebalance
    training data through sampling, reweighting, or augmentation, require access to
    raw features and group labels, often through a feature store or data lake that
    preserves lineage. These methods are well-suited to systems with centralized training
    pipelines and high-quality labeled data. In contrast, in-processing approaches
    embed fairness constraints directly into the optimization objective. These require
    training infrastructure that can support custom loss functions or constrained
    solvers and may demand longer training cycles or additional regularization validation.
    The training techniques and optimization methods discussed in [Chapter 8](ch014.xhtml#sec-ai-training)
    provide the foundation for implementing these fairness-aware training approaches.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性干预可以在管道的不同点应用，但每个都伴随着系统级的影响。预处理方法，通过采样、重新加权或增强来重新平衡训练数据，需要访问原始特征和群体标签，通常通过一个保留谱系的特征存储或数据湖。这些方法非常适合具有集中式训练管道和高质量标记数据的系统。相比之下，内处理方法将公平性约束直接嵌入到优化目标中。这些需要能够支持自定义损失函数或约束求解器的训练基础设施，可能需要更长的训练周期或额外的正则化验证。第8章中讨论的训练技术和优化方法为实施这些公平性感知的训练方法提供了基础。
- en: Post-processing methods, including the application of group-specific thresholds
    or the adjustment of scores to equalize outcomes, require inference systems that
    can condition on sensitive attributes or reference external policy rules. This
    demands coordination between model serving infrastructure, access control policies,
    and logging pipelines to ensure that differential treatment is both auditable
    and legally defensible. The model serving architectures covered in [Chapter 2](ch008.xhtml#sec-ml-systems)
    detail the infrastructure requirements for implementing such conditional logic
    in production systems. Any post-processing strategy must be carefully validated
    to ensure that it does not compromise user experience, model stability, or compliance
    with jurisdictional regulations on attribute use.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理方法，包括应用特定群体的阈值或调整分数以实现结果均衡，需要推理系统能够根据敏感属性或参考外部政策规则进行条件化。这要求在模型服务基础设施、访问控制策略和日志管道之间进行协调，以确保差异化的处理既可审计又合法。第2章中涵盖的模型服务架构详细说明了在生产系统中实现这种条件逻辑所需的基础设施要求。任何后处理策略都必须经过仔细验证，以确保它不会损害用户体验、模型稳定性或遵守关于属性使用的司法管辖区法规。
- en: Scalable fairness enforcement often requires more advanced strategies, such
    as multicalibration[29](#fn29), which ensures that model predictions remain calibrated
    across a wide range of intersecting subgroups ([Hébert-Johnson et al. 2018](ch058.xhtml#ref-hebert2018multicalibration)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展的公平性执行通常需要更高级的策略，例如多校准[29](#fn29)，这确保模型预测在广泛的相交子群体中保持校准（[Hébert-Johnson等人2018](ch058.xhtml#ref-hebert2018multicalibration)）。
- en: Implementing multicalibration at scale requires infrastructure for dynamically
    generating subgroup partitions, computing per-group calibration error, and integrating
    fairness audits into automated monitoring systems. These capabilities are typically
    only available in large-scale, cloud-based deployments with mature observability
    and metrics pipelines. In constrained environments such as embedded or TinyML
    systems, where telemetry is limited and model logic is fixed, such techniques
    are not feasible and fairness must be validated entirely at design time.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模实施多校准需要动态生成子群划分的基础设施、计算每组校准误差以及将公平性审计集成到自动化监控系统。这些能力通常仅在具有成熟可观察性和度量管道的大规模、云基础部署中可用。在嵌入式或TinyML系统等受限环境中，由于遥测有限且模型逻辑固定，这些技术不可行，公平性必须在设计时完全验证。
- en: Across deployment environments, maintaining fairness requires lifecycle-aware
    mechanisms. Model updates, feedback loops, and interface designs all affect how
    fairness evolves over time. A fairness-aware model may degrade if retraining pipelines
    do not include fairness checks, if logging systems cannot track subgroup outcomes,
    or if user feedback introduces subtle biases not captured by training distributions.
    Monitoring systems must be equipped to surface fairness regressions, and retraining
    protocols must have access to subgroup-labeled validation data, which may require
    data governance policies and ethical review. Implementation of these monitoring
    systems requires production infrastructure for MLOps practices, while privacy-preserving
    techniques are essential for federated fairness assessment.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的部署环境中，保持公平性需要生命周期感知机制。模型更新、反馈循环和界面设计都会影响公平性随时间的变化。如果重新训练管道不包括公平性检查，如果日志系统无法跟踪子群结果，或者如果用户反馈引入了训练分布未捕捉到的微妙偏差，则公平感知模型可能会退化。监控系统必须能够揭示公平性退化，重新训练协议必须能够访问带有子群标签的验证数据，这可能需要数据治理政策和伦理审查。实施这些监控系统需要MLOps实践的生产基础设施，而隐私保护技术对于联邦公平性评估至关重要。
- en: Fairness is not a one-time optimization, nor is it a property of the model in
    isolation. It emerges from coordinated decisions across data acquisition, feature
    engineering, model design, thresholding, feedback handling, and system monitoring.
    Embedding fairness into machine learning systems requires architectural foresight,
    operational discipline, and tooling that spans the full deployment stack—from
    training workflows to serving infrastructure to user-facing interfaces.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性不是一次性的优化，也不是模型独立属性。它源于数据采集、特征工程、模型设计、阈值设置、反馈处理和系统监控等方面的协调决策。将公平性嵌入机器学习系统需要架构前瞻性、操作纪律以及贯穿整个部署堆栈的工具，从训练工作流到服务基础设施再到面向用户的界面。
- en: The sociotechnical implications of bias detection extend far beyond technical
    measurement. When fairness metrics identify disparities, organizations must navigate
    complex stakeholder deliberation processes as examined in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f).
    These decisions involve competing stakeholder interests, legal compliance requirements,
    and value trade-offs that cannot be resolved through technical means alone.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见检测的社会技术影响远远超出了技术测量的范畴。当公平性指标识别出差异时，组织必须像在[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中考察的那样，导航复杂的利益相关者审议过程。这些决策涉及相互竞争的利益相关者利益、法律合规要求和价值权衡，这些不能仅通过技术手段解决。
- en: Real-Time Fairness Monitoring Architecture
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实时公平性监控架构
- en: Implementing responsible AI principles in production systems requires architectural
    patterns that integrate fairness monitoring, explainability, and privacy controls
    directly into the model serving infrastructure. [Figure 17.3](ch023.xhtml#fig-responsible-ai-architecture)
    illustrates a reference architecture that demonstrates how responsible AI components
    integrate with existing ML systems infrastructure.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产系统中实施负责任的AI原则需要将公平性监控、可解释性和隐私控制直接集成到模型服务基础设施中的架构模式。[图17.3](ch023.xhtml#fig-responsible-ai-architecture)展示了如何将负责任的AI组件与现有的ML系统基础设施集成的参考架构。
- en: '![](../media/file292.svg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file292.svg)'
- en: 'Figure 17.3: **Production Responsible AI Architecture:** Real-time fairness
    monitoring requires integrated components that process each inference request
    through data anonymization, bias detection, and explanation generation while maintaining
    audit trails and triggering alerts when fairness thresholds are violated. The
    dashed line shows the feedback loop for model updates based on detected bias patterns.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：**生产负责任AI架构**：实时公平性监控需要集成组件，这些组件通过数据处理匿名化、偏差检测和解释生成来处理每个推理请求，同时维护审计跟踪，并在公平性阈值被违反时触发警报。虚线显示了基于检测到的偏差模式进行模型更新的反馈循环。
- en: 'This architecture addresses the production realities identified by experts
    through several key components that work together to implement responsible AI
    at scale:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构通过几个关键组件解决了专家确定的生产现实，这些组件协同工作以实现大规模的负责任AI：
- en: The data anonymization layer implements privacy-preserving transformations before
    model inference, using techniques like k-anonymity[30](#fn30) or differential
    privacy noise injection. This component adds 2-5 ms latency per request but provides
    formal privacy guarantees. Memory overhead is typically 15-25% due to encryption
    and noise generation requirements.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 数据匿名化层在模型推理之前实现隐私保护转换，使用诸如 k-匿名性[30](#fn30) 或差分隐私噪声注入等技术。该组件为每个请求增加 2-5 毫秒的延迟，但提供正式的隐私保证。由于加密和噪声生成需求，内存开销通常为
    15-25%。
- en: Real-time fairness monitoring tracks demographic parity and equalized odds metrics
    for each prediction, maintaining rolling statistics across protected groups. The
    system flags disparities exceeding configurable thresholds (e.g., >5% difference
    in approval rates). This monitoring adds 10-20 ms latency and requires 100-500 MB
    additional memory for metric storage and computation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 实时公平性监控跟踪每个预测的种群统计和均衡概率指标，在受保护群体中维护滚动统计。系统标记超过可配置阈值的差异（例如，批准率差异超过 5%）。这种监控增加了
    10-20 毫秒的延迟，并需要 100-500 MB 的额外内存用于指标存储和计算。
- en: The explanation engine generates SHAP or LIME explanations for model decisions,
    particularly for negative outcomes requiring user recourse. Fast approximation
    methods reduce explanation latency from 200-500 ms (full SHAP) to 20-50 ms (streaming
    SHAP) with 90% fidelity. Memory requirements increase by 50-100% due to gradient
    computation and feature importance caching.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 解释引擎为模型决策生成 SHAP 或 LIME 解释，特别是对于需要用户干预的负面结果。快速近似方法将解释延迟从 200-500 毫秒（完整 SHAP）减少到
    20-50 毫秒（流式 SHAP），同时保持 90% 的保真度。由于梯度计算和特征重要性缓存，内存需求增加 50-100%。
- en: '**Implementation Deep Dive**'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现深度解析**'
- en: 'The following code example demonstrates production-ready fairness monitoring
    with real-time bias detection. This represents a reference implementation showing
    architectural patterns rather than code to memorize. Focus on understanding: (1)
    how fairness metrics integrate into serving infrastructure, (2) what performance
    trade-offs the implementation manages, (3) how alerts trigger when thresholds
    are exceeded. You can return to implementation details when building similar systems.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了生产就绪的公平性监控和实时偏差检测。这代表了一个参考实现，展示了架构模式而不是需要记忆的代码。重点理解：（1）公平性指标如何集成到服务基础设施中，（2）实现管理了哪些性能权衡，（3）当阈值超过时如何触发警报。在构建类似系统时，您可以返回到实现细节。
- en: '[Listing 17.2](ch023.xhtml#lst-production-fairness-monitoring) demonstrates
    a production implementation that integrates these components into a real-time
    monitoring system:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 17.2](ch023.xhtml#lst-production-fairness-monitoring) 展示了一个将组件集成到实时监控系统中的生产实现：'
- en: 'Listing 17.2: **Production Fairness Monitoring Implementation**: Real-time
    bias detection system that processes inference requests, computes fairness metrics,
    and triggers alerts when disparities exceed thresholds, showing how responsible
    AI integrates with production ML serving infrastructure.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.2：**生产公平性监控实现**：实时偏差检测系统，处理推理请求，计算公平性指标，并在差异超过阈值时触发警报，展示了负责任的AI如何与生产机器学习服务基础设施集成。
- en: '[PRE1]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This production implementation demonstrates how responsible AI principles translate
    into concrete system architecture with quantifiable performance impacts. The fairness
    monitoring adds 10-20 ms latency per request and requires 100-500 MB additional
    memory, while the explanation engine increases response time by 20-50 ms and memory
    usage by 50-100%. These overheads must be balanced against reliability and compliance
    requirements when designing production systems.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这种生产实现展示了负责任的AI原则如何转化为具有可衡量性能影响的具体系统架构。公平性监控为每个请求增加了10-20 ms的延迟，并需要额外100-500 MB的内存，而解释引擎将响应时间增加了20-50 ms，内存使用量增加了50-100%。在设计生产系统时，必须将这些开销与可靠性和合规性要求相平衡。
- en: Detection capabilities must be coupled with mitigation techniques that actively
    prevent harmful outcomes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 检测能力必须与积极防止有害结果的缓解技术相结合。
- en: Risk Mitigation Techniques
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险缓解技术
- en: Mitigation techniques actively intervene in system design and operation to prevent
    harmful outcomes and reduce risks to users and society. These approaches range
    from privacy-preserving methods that protect sensitive data, to adversarial defenses
    that maintain system reliability under attack, to machine unlearning[31](#fn31)
    techniques that support data governance and user rights.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解技术积极干预系统设计和操作，以防止有害结果并降低对用户和社会的风险。这些方法包括保护敏感数据的隐私保护方法，到在攻击下保持系统可靠性的对抗性防御，再到支持数据治理和用户权利的机器反学习[31](#fn31)技术。
- en: Privacy Preservation
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 隐私保护
- en: Recall that privacy is a foundational principle of responsible machine learning,
    with implications that extend across data collection, model behavior, and user
    interaction. Privacy constraints are shaped not only by ethical and legal obligations,
    but also by the architectural properties of the system and the context in which
    it is deployed. Technical methods for privacy preservation aim to prevent data
    leakage, limit memorization, and uphold user rights such as consent, opt-out,
    and data deletion—particularly in systems that learn from personalized or sensitive
    information.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，隐私是负责任机器学习的基础原则，其影响延伸到数据收集、模型行为和用户交互。隐私约束不仅由伦理和法律义务塑造，还由系统的架构属性和部署的上下文所决定。隐私保护的技术方法旨在防止数据泄露，限制记忆，并维护用户权利，如同意、退出和数据删除——尤其是在从个性化或敏感信息中学习的系统中。
- en: Modern machine learning models, especially large-scale neural networks, are
    known to memorize individual training examples, including names, locations, or
    excerpts of private communication ([Ippolito et al. 2023](ch058.xhtml#ref-carlini2023extractingllm)).
    This memorization presents significant risks in privacy-sensitive applications
    such as smart assistants, wearables, or healthcare platforms, where training data
    may encode protected or regulated content. For example, a voice assistant that
    adapts to user speech may inadvertently retain specific phrases, which could later
    be extracted through carefully designed prompts or queries.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型，尤其是大规模神经网络，已知会记住单个训练示例，包括姓名、位置或私人通信的摘录（[Ippolito 等人 2023](ch058.xhtml#ref-carlini2023extractingllm)）。这种记忆在隐私敏感的应用中存在显著风险，例如智能助手、可穿戴设备或医疗平台，其中训练数据可能包含受保护或受监管的内容。例如，一个适应用户语音的语音助手可能会无意中保留特定的短语，这些短语可能后来通过精心设计的提示或查询被提取出来。
- en: 'This risk is not limited to language models. Diffusion models trained on image
    datasets have also been observed to regenerate visual instances from the training
    set, as illustrated in [Figure 17.4](ch023.xhtml#fig-diffusion-model-example).
    Such behavior highlights a more general vulnerability: many contemporary model
    architectures can internalize and reproduce training data, often without explicit
    signals or intent, and without easy detection or control.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这种风险不仅限于语言模型。在图像数据集上训练的扩散模型也被观察到可以从训练集中重新生成视觉实例，如图[图17.4](ch023.xhtml#fig-diffusion-model-example)所示。这种行为突显了一个更普遍的漏洞：许多当代模型架构可以内化和再现训练数据，通常没有明确的信号或意图，也没有容易检测或控制。
- en: '![](../media/file293.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file293.png)'
- en: 'Figure 17.4: **Diffusion Model Memorization**: Image diffusion models can reproduce
    training samples, revealing a risk of unintended memorization beyond language
    models and highlighting a general vulnerability in contemporary neural architectures.
    This memorization occurs despite the absence of explicit instructions and poses
    privacy concerns when training on sensitive datasets. Source: ([Ippolito et al.
    2023](ch058.xhtml#ref-carlini2023extractingllm)).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.4：**扩散模型记忆化**：图像扩散模型可以重现训练样本，揭示了除语言模型之外意外记忆化的风险，并突出了当代神经网络架构的一般性漏洞。这种记忆化即使在缺乏明确指令的情况下也会发生，当在敏感数据集上训练时，会引发隐私问题。来源：([Ippolito
    等人 2023](ch058.xhtml#ref-carlini2023extractingllm))。
- en: Models are also susceptible to membership inference attacks, in which adversaries
    attempt to determine whether a specific datapoint was part of the training set
    ([Shokri et al. 2017](ch058.xhtml#ref-shokri2017membership)). These attacks exploit
    subtle differences in model behavior between seen and unseen inputs. In high-stakes
    applications such as healthcare or legal prediction, the mere knowledge that an
    individuals record was used in training may violate privacy expectations or regulatory
    requirements.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 模型也容易受到成员推理攻击的影响，攻击者试图确定特定数据点是否是训练集的一部分（[Shokri 等人 2017](ch058.xhtml#ref-shokri2017membership)）。这些攻击利用了模型在已见和未见输入之间的行为上的微妙差异。在医疗保健或法律预测等高风险应用中，仅仅知道个人的记录被用于训练，就可能违反隐私期望或监管要求。
- en: To mitigate such vulnerabilities, a range of privacy-preserving techniques have
    been developed. Among the most widely adopted is differential privacy[32](#fn32),
    which provides formal guarantees that the inclusion or exclusion of a single datapoint
    has a statistically bounded effect on the models output. Algorithms such as differentially
    private stochastic gradient descent (DP-SGD) enforce these guarantees by clipping
    gradients and injecting noise during training ([Martin Abadi et al. 2016](ch058.xhtml#ref-abadi2016deep)).
    When implemented correctly, these methods prevent the model from memorizing individual
    datapoints and reduce the risk of inference attacks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些漏洞，已经开发了一系列的隐私保护技术。其中最广泛采用的一种是差分隐私[32](#fn32)，它提供了正式的保证，即单个数据点的包含或排除对模型输出的影响在统计上是有限的。例如，差分隐私随机梯度下降（DP-SGD）算法通过剪枝和训练过程中注入噪声来强制执行这些保证（[Martin
    Abadi 等人 2016](ch058.xhtml#ref-abadi2016deep)）。当正确实施时，这些方法可以防止模型记住单个数据点，并降低推理攻击的风险。
- en: However, differential privacy introduces significant system-level tradeoffs.
    The noise added during training can degrade model accuracy, increase the number
    of training iterations, and require access to larger datasets to maintain performance.
    These constraints are especially pronounced in resource-limited deployments such
    as mobile, edge, or embedded systems, where memory, compute, and power budgets
    are tightly constrained. In such settings, it may be necessary to combine lightweight
    privacy techniques (e.g., feature obfuscation, local differential privacy) with
    architectural strategies that limit data collection, shorten retention, or enforce
    strict access control at the edge.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，差分隐私引入了显著的系统级权衡。训练过程中添加的噪声可能会降低模型精度，增加训练迭代次数，并需要访问更大的数据集以维持性能。这些限制在资源受限的部署中尤为明显，如移动、边缘或嵌入式系统，在这些系统中，内存、计算和电力预算都受到严格限制。在这种情况下，可能需要结合轻量级隐私技术（例如，特征混淆、本地差分隐私）与限制数据收集、缩短保留期或在边缘实施严格访问控制的架构策略。
- en: Privacy enforcement also depends on infrastructure beyond the model itself.
    Data collection interfaces must support informed consent and transparency. Logging
    systems must avoid retaining sensitive inputs unless strictly necessary, and must
    support access controls, expiration policies, and auditability. Model serving
    infrastructure must be designed to prevent overexposure of outputs that could
    leak internal model behavior or allow reconstruction of private data. These system-level
    mechanisms require close coordination between ML engineering, platform security,
    and organizational governance.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私执行还依赖于模型本身之外的基础设施。数据收集接口必须支持知情同意和透明度。日志系统必须避免保留敏感输入，除非绝对必要，并且必须支持访问控制、过期策略和可审计性。模型服务基础设施必须设计成防止输出过度暴露，这可能泄露内部模型行为或允许重建私有数据。这些系统级机制需要机器学习工程、平台安全和组织治理之间的紧密协调。
- en: Privacy must be enforced not only during training but throughout the machine
    learning lifecycle. Retraining pipelines must account for deleted or revoked data,
    especially in jurisdictions with data deletion mandates. Monitoring infrastructure
    must avoid recording personally identifiable information in logs or dashboards.
    Privacy-aware telemetry collection, secure enclave deployment, and per-user audit
    trails are increasingly used to support these goals, particularly in applications
    with strict legal oversight.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私不仅需要在训练期间得到强制执行，在整个机器学习生命周期中都需要得到强制执行。再训练管道必须考虑到已删除或撤销的数据，特别是在有数据删除命令的司法管辖区。监控基础设施必须避免在日志或仪表板上记录个人信息。隐私感知遥测收集、安全区域部署以及按用户审计跟踪越来越多地被用于支持这些目标，尤其是在受到严格法律监管的应用中。
- en: Architectural decisions also vary by deployment context. Cloud-based systems
    may rely on centralized enforcement of differential privacy, encryption, and access
    control, supported by telemetry and retraining infrastructure. In contrast, edge
    and TinyML systems must build privacy constraints into the deployed model itself,
    often with no runtime configurability or feedback channel. In such cases, static
    analysis, conservative design, and embedded privacy guarantees must be implemented
    at compile time, with validation performed prior to deployment.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 架构决策也因部署环境而异。基于云的系统可能依赖于差分隐私、加密和访问控制的集中式执行，这些由遥测和再训练基础设施支持。相比之下，边缘和TinyML系统必须在部署的模型本身中构建隐私约束，通常没有运行时配置性或反馈通道。在这种情况下，必须在编译时实现静态分析、保守设计和嵌入式隐私保证，并在部署前进行验证。
- en: Privacy is not an attribute of a model in isolation but a system-level property
    that emerges from design decisions across the pipeline. Responsible privacy preservation
    requires that technical safeguards, interface controls, infrastructure policies,
    and regulatory compliance mechanisms work together to minimize risk throughout
    the lifecycle of a deployed machine learning system.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私不是一个模型独立属性，而是一个从整个管道的设计决策中产生的系统级属性。负责任的隐私保护要求技术保障、界面控制、基础设施政策和法规遵从机制共同工作，以在整个部署的机器学习系统生命周期中降低风险。
- en: Privacy preservation techniques create complex sociotechnical tensions that
    extend well beyond technical implementation. Differential privacy mechanisms may
    reduce model accuracy in ways that disproportionately affect underrepresented
    groups, creating conflicts between privacy and fairness objectives. These challenges
    require ongoing stakeholder engagement as detailed in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f),
    where organizations must navigate competing values around data control, personalization,
    and regulatory compliance.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护技术创造了复杂的社会技术紧张关系，这些关系远远超出了技术实施的范畴。差分隐私机制可能会以不成比例地影响少数群体的方式降低模型准确性，从而在隐私和公平性目标之间产生冲突。这些挑战需要持续的利益相关者参与，如[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中详细所述，其中组织必须在大数据控制、个性化以及法规遵从等相互冲突的价值之间进行导航。
- en: These privacy challenges become even more complex when considering the dynamic
    nature of user rights and data governance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑到用户权利和数据治理的动态性质时，这些隐私挑战变得更加复杂。
- en: Machine Unlearning
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机器反学习
- en: 'Privacy preservation does not end at training time. In many real-world systems,
    users must retain the right to revoke consent or request the deletion of their
    data, even after a model has been trained and deployed. Supporting this requirement
    introduces a core technical challenge: how can a model “forget” the influence
    of specific datapoints without requiring full retraining—a task that is often
    infeasible in edge, mobile, or embedded deployments with constrained compute,
    storage, and connectivity?'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护并不在训练时间结束。在许多现实世界的系统中，用户必须保留撤销同意或请求删除其数据的权利，即使模型已经训练并部署。支持这一要求引入了一个核心技术挑战：如何在不需要全面重新训练的情况下，让模型“忘记”特定数据点的影響——这项任务在计算、存储和连接受限的边缘、移动或嵌入式部署中通常是不切实际的。
- en: Traditional approaches to data deletion assume that the full training dataset
    remains accessible and that models can be retrained from scratch after removing
    the targeted records. [Figure 17.5](ch023.xhtml#fig-machine-unlearning) contrasts
    traditional model retraining with emerging machine unlearning approaches. While
    retraining involves reconstructing the model from scratch using a modified dataset,
    unlearning aims to remove a specific datapoint’s influence without repeating the
    entire learning process.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据删除方法假设完整的训练数据集仍然可访问，并且可以在删除目标记录后从头开始重新训练模型。[图 17.5](ch023.xhtml#fig-machine-unlearning)
    对传统模型重新训练与新兴机器反学习方法进行了对比。虽然重新训练涉及使用修改后的数据集从头开始重建模型，但反学习旨在去除特定数据点的影响，而不重复整个学习过程。
- en: '![](../media/file294.svg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file294.svg)'
- en: 'Figure 17.5: **Model Update Strategies**: Retraining reconstructs a model from
    scratch, while machine unlearning modifies an existing model to remove the influence
    of specific data points without complete reconstruction—a important distinction
    for resource-constrained deployments. This approach minimizes computational cost
    and allows privacy-preserving data deletion after initial model training.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5：**模型更新策略**：重新训练从零开始重建模型，而机器反学习则修改现有模型，以去除特定数据点的影响而不进行完全重建——这对于资源受限的部署来说是一个重要的区别。这种方法最小化了计算成本，并允许在初始模型训练后进行隐私保护的数据删除。
- en: This distinction becomes important in systems with tight latency, compute, or
    privacy constraints. These assumptions rarely hold in practice. Many deployed
    machine learning systems do not retain raw training data due to security, compliance,
    or cost constraints. In such environments, full retraining is often impractical
    and operationally disruptive, especially when data deletion must be verifiable,
    repeatable, and audit-ready.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有严格延迟、计算或隐私约束的系统中，这种区别变得很重要。在实践中，这些假设很少成立。由于安全、合规或成本限制，许多部署的机器学习系统不保留原始训练数据。在这样的环境中，完全重新训练通常不切实际且具有破坏性，尤其是在数据删除必须可验证、可重复且可审计的情况下。
- en: 'Machine unlearning aims to address this limitation by removing the influence
    of individual datapoints from an already trained model without retraining it entirely.
    Current approaches approximate this behavior by adjusting internal parameters,
    modifying gradient paths, or isolating and pruning components of the model so
    that the resulting predictions reflect what would have been learned without the
    deleted data ([Bourtoule et al. 2021](ch058.xhtml#ref-bourtoule2021machine)).
    These techniques are still maturing and may require simplified model architectures,
    additional tracking metadata, or compromise on model accuracy and stability. They
    also introduce new burdens around verification: how to prove that deletion has
    occurred in a meaningful way, especially when internal model state is not fully
    interpretable.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 机器反学习旨在通过移除已训练模型中个别数据点的 影响，而不完全重新训练模型，来解决这个问题。当前的方法通过调整内部参数、修改梯度路径或隔离和修剪模型的部分组件来近似这种行为，从而使结果预测反映在没有删除数据的情况下可能学到的内容
    ([Bourtoule 等人 2021](ch058.xhtml#ref-bourtoule2021machine))。这些技术仍在成熟中，可能需要简化的模型架构、额外的跟踪元数据，或者在模型精度和稳定性上做出妥协。它们还引入了新的验证负担：如何以有意义的方式证明已发生删除，尤其是在内部模型状态不完全可解释的情况下。
- en: The motivation for machine unlearning is reinforced by regulatory frameworks.
    Laws such as the General Data Protection Regulation (GDPR), the California Consumer
    Privacy Act (CCPA), and similar statutes in Canada and Japan codify the right
    to be forgotten, including for data used in model training. These laws increasingly
    require not just prevention of unauthorized data access, but proactive revocation—empowering
    users to request that their information cease to influence downstream system behavior.
    High-profile incidents in which generative models have reproduced personal content
    or copyrighted data highlight the practical urgency of integrating unlearning
    mechanisms into responsible system design.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 机器反学习的动机得到了监管框架的加强。例如，通用数据保护条例 (GDPR)、加利福尼亚消费者隐私法案 (CCPA) 以及加拿大和日本的类似法律将“被遗忘权”编纂成法典，包括用于模型训练的数据。这些法律越来越要求不仅防止未经授权的数据访问，而且要求主动撤销——赋予用户请求其信息停止影响下游系统行为的能力。一些知名事件，其中生成模型复制了个人内容或受版权保护的数据，突出了将反学习机制整合到负责任系统设计中的实际紧迫性。
- en: From a systems perspective, machine unlearning introduces nontrivial architectural
    and operational requirements. Systems must be able to track data lineage, including
    which datapoints contributed to a given model version. This often requires structured
    metadata capture and training pipeline instrumentation. Additionally, systems
    must support user-facing deletion workflows, including authentication, submission,
    and feedback on deletion status. Verification may require maintaining versioned
    model registries, along with mechanisms for confirming that the updated model
    exhibits no residual influence from the deleted data. These operations must span
    data storage, training orchestration, model deployment, and auditing infrastructure,
    and they must be robust to failure or rollback.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，机器反学习引入了非平凡的架构和运营需求。系统必须能够跟踪数据血缘，包括哪些数据点贡献了给定的模型版本。这通常需要结构化元数据捕获和训练管道的仪器化。此外，系统必须支持面向用户的删除工作流程，包括身份验证、提交和删除状态的反馈。验证可能需要维护版本化的模型注册表，以及确认更新后的模型没有残留的删除数据影响的机制。这些操作必须跨越数据存储、训练编排、模型部署和审计基础设施，并且必须能够应对故障或回滚。
- en: These challenges are amplified in resource-constrained deployments. TinyML systems
    typically run on devices with no persistent storage, no connectivity, and highly
    compressed models. Once deployed, they cannot be updated or retrained in response
    to deletion requests. In such settings, machine unlearning is effectively infeasible
    post-deployment and must be enforced during initial model development through
    static data minimization and conservative generalization strategies. Even in cloud-based
    systems, where retraining is more tractable, unlearning must contend with distributed
    training pipelines, replication across services, and the difficulty of synchronizing
    deletion across model snapshots and logs.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战在资源受限的部署中更为突出。TinyML系统通常运行在没有持久存储、没有连接性和高度压缩模型的设备上。一旦部署，它们就不能根据删除请求进行更新或重新训练。在这种设置下，机器反学习在部署后实际上是不可行的，必须在初始模型开发期间通过静态数据最小化和保守的泛化策略来强制执行。即使在基于云的系统，重新训练更为可行的情况下，反学习也必须应对分布式训练管道、跨服务的复制以及同步模型快照和日志中删除的困难。
- en: Machine unlearning is becoming important for responsible system design despite
    these challenges. As machine learning systems become more embedded, personalized,
    and adaptive, the ability to revoke training influence becomes central to maintaining
    user trust and meeting legal requirements. Critically, unlearning cannot be retrofitted
    after deployment. It must be considered during the architecture and policy design
    phases, with support for lineage tracking, re-training orchestration, and deployment
    roll-forward built into the system from the beginning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，机器反学习对于负责任系统设计正变得越来越重要。随着机器学习系统变得更加嵌入式、个性化和自适应，撤销训练影响的能力对于维护用户信任和满足法律要求变得至关重要。关键的是，反学习不能在部署后进行改造。它必须在架构和政策设计阶段予以考虑，并且从系统一开始就必须支持血缘跟踪、重新训练编排和部署回滚。
- en: Machine unlearning represents a shift in privacy thinking—from protecting what
    data is collected, to controlling how long that data continues to affect system
    behavior. This lifecycle-oriented perspective introduces new challenges for model
    design, infrastructure planning, and regulatory compliance, while also providing
    a foundation for more user-controllable, transparent, and adaptable machine learning
    systems.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 机器反学习代表了隐私思考的转变——从保护收集的数据，到控制这些数据持续影响系统行为的时间。这种以生命周期为导向的视角为模型设计、基础设施规划和法规遵从引入了新的挑战，同时也为更用户可控、透明和自适应的机器学习系统提供了基础。
- en: Responsible AI systems must also maintain reliable behavior under challenging
    conditions, including deliberate attacks.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI系统还必须在包括故意攻击在内的困难条件下保持可靠的行为。
- en: Adversarial Robustness
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗鲁棒性
- en: Adversarial robustness, examined in [Chapter 16](ch022.xhtml#sec-robust-ai)
    and [Chapter 15](ch021.xhtml#sec-security-privacy) as a defense against deliberate
    attacks, also serves as a foundation for responsible AI deployment. Beyond protecting
    against malicious adversaries, adversarial robustness ensures models behave reliably
    when encountering naturally occurring variations, edge cases, and inputs that
    deviate from training distributions. A model vulnerable to adversarial perturbations
    reveals fundamental brittleness in its learned representations—brittleness that
    compromises trustworthiness even in non-adversarial contexts.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 16 章 ([ch022.xhtml#sec-robust-ai]) 和第 15 章 ([ch021.xhtml#sec-security-privacy])
    中探讨的对抗鲁棒性，作为针对故意攻击的防御手段，也构成了负责任 AI 部署的基础。除了保护免受恶意对手的攻击之外，对抗鲁棒性确保模型在遇到自然发生的变异、边缘情况和偏离训练分布的输入时仍能可靠地表现。易受对抗性扰动影响的模型揭示了其学习表示中的基本脆弱性——这种脆弱性即使在非对抗性环境中也会损害可信度。
- en: Machine learning models, particularly deep neural networks, are known to be
    vulnerable to small, carefully crafted perturbations that significantly alter
    their predictions. These vulnerabilities, first formalized through the concept
    of adversarial examples ([Szegedy et al. 2013b](ch058.xhtml#ref-szegedy2013intriguing)),
    highlight a gap between model performance on curated training data and behavior
    under real-world variability. A model that performs reliably on clean inputs may
    fail when exposed to inputs that differ only slightly from its training distribution—differences
    imperceptible to humans, but sufficient to change the model’s output.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，尤其是深度神经网络，已知容易受到微小、精心设计的扰动的影响，这些扰动会显著改变它们的预测。这些漏洞最初是通过对抗性示例的概念 ([Szegedy
    等人 2013b](ch058.xhtml#ref-szegedy2013intriguing)) 得到的形式化，突显了模型在精心挑选的训练数据上的性能与在现实世界变量下的行为之间的差距。一个在干净输入上表现可靠的模型，当暴露于与其训练分布略有不同的输入时可能会失败——这些差异对人类来说是不可察觉的，但足以改变模型输出。
- en: This phenomenon is not limited to theory. Adversarial examples have been used
    to manipulate real systems, including content moderation pipelines ([Bhagoji et
    al. 2018](ch058.xhtml#ref-bhagoji2018practical)), ad-blocking detection ([Tramèr
    et al. 2019](ch058.xhtml#ref-tramer2019adversarial)), and voice recognition models
    ([Carlini et al. 2016](ch058.xhtml#ref-carlini2016hidden)). In safety-important
    domains such as autonomous driving or medical diagnostics, even rare failures
    can have high-consequence outcomes, compromising user trust or opening attack
    surfaces for malicious exploitation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象不仅限于理论。对抗性示例已被用于操纵真实系统，包括内容审核管道 ([Bhagoji 等人 2018](ch058.xhtml#ref-bhagoji2018practical))、广告拦截检测
    ([Tramèr 等人 2019](ch058.xhtml#ref-tramer2019adversarial)) 和语音识别模型 ([Carlini 等人
    2016](ch058.xhtml#ref-carlini2016hidden))。在自动驾驶或医疗诊断等安全至关重要的领域，即使是罕见的故障也可能产生高后果的结果，损害用户信任或为恶意利用打开攻击面。
- en: '[Figure 17.6](ch023.xhtml#fig-adversarial-example) illustrates a visually negligible
    perturbation that causes a confident misclassification—underscoring how subtle
    changes can produce disproportionately harmful effects.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.6](ch023.xhtml#fig-adversarial-example) 展示了一个视觉上微不足道的扰动，它导致了一个自信的错误分类——强调了微妙的变化如何产生不成比例的有害影响。'
- en: '![](../media/file295.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file295.png)'
- en: 'Figure 17.6: **Adversarial Perturbation**: Subtle, intentionally crafted noise
    can cause machine learning models to misclassify inputs with high confidence,
    even though the change is imperceptible to humans. This example shows how a small
    perturbation to an image of a pig causes a misclassification, highlighting the
    vulnerability of deep learning systems to adversarial attacks. Source: Microsoft.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6：**对抗性扰动**：精心制作的微妙噪声可能导致机器学习模型以高置信度错误分类输入，尽管这种变化对人类来说是不可察觉的。这个例子展示了图像的微小扰动如何导致错误分类，突显了深度学习系统对对抗性攻击的脆弱性。来源：微软。
- en: At its core, adversarial vulnerability stems from an architectural mismatch
    between model assumptions and deployment conditions. Many training pipelines assume
    data is clean, independent, and identically distributed. In contrast, deployed
    systems must operate under uncertainty, noise, domain shift, and possible adversarial
    tampering. Robustness, in this context, encompasses not only the ability to resist
    attack but also the ability to maintain consistent behavior under degraded or
    unpredictable conditions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，对抗性漏洞源于模型假设与部署条件之间的架构不匹配。许多训练管道假设数据是干净的、独立的且同分布的。相比之下，部署系统必须在不确定性、噪声、领域偏移和可能的对抗性篡改下运行。在这种情况下，鲁棒性不仅包括抵抗攻击的能力，还包括在退化或不可预测条件下保持一致行为的能力。
- en: Improving robustness begins at training. Adversarial training, one of the most
    widely used techniques, augments training data with perturbed examples. This helps
    the model learn more stable decision boundaries but typically increases training
    time and reduces clean-data accuracy. Implementing adversarial training at scale
    also places demands on data preprocessing pipelines, model checkpointing infrastructure,
    and validation protocols that can accommodate perturbed inputs.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 提高鲁棒性始于训练阶段。对抗性训练是最广泛使用的技术之一，通过添加扰动示例来增强训练数据。这有助于模型学习更稳定的决策边界，但通常会增加训练时间并降低干净数据的准确性。在规模上实施对抗性训练也对数据预处理管道、模型检查点基础设施和可以容纳扰动输入的验证协议提出了要求。
- en: Architectural modifications can also promote robustness. Techniques that constrain
    a models Lipschitz constant, regularize gradient sensitivity, or enforce representation
    smoothness can make predictions more stable. These design changes must be compatible
    with the models expressive needs and the underlying training framework. For example,
    smooth models may be preferred for embedded systems with limited input precision
    or where safety-important thresholds must be respected.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 架构修改也可以促进鲁棒性。限制模型Lipschitz常数的技巧、正则化梯度敏感性或强制执行表示平滑性可以使预测更稳定。这些设计更改必须与模型的表达需求以及底层训练框架兼容。例如，平滑模型可能更适合输入精度有限或必须遵守安全重要阈值的嵌入式系统。
- en: At inference time, systems may implement uncertainty-aware decision-making.
    Models can abstain from making predictions when confidence is low, or route uncertain
    inputs to fallback mechanisms—such as rule-based components or human-in-the-loop
    systems. These strategies require deployment infrastructure that supports fallback
    logic, user escalation workflows, or configurable abstention policies. For instance,
    a mobile diagnostic app might return “inconclusive” if model confidence falls
    below a specified threshold, rather than issuing a potentially harmful prediction.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时间，系统可能会实施不确定性感知的决策制定。当置信度低时，模型可以避免做出预测，或者将不确定的输入路由到回退机制——例如基于规则的组件或人机交互系统。这些策略需要部署基础设施来支持回退逻辑、用户升级工作流程或可配置的弃权策略。例如，一个移动诊断应用可能会在模型置信度低于指定阈值时返回“不确定”，而不是发布可能有害的预测。
- en: Monitoring infrastructure plays a important role in maintaining robustness post-deployment.
    Distribution shift detection, anomaly tracking, and behavior drift analytics allow
    systems to identify when robustness is degrading over time. Implementing these
    capabilities requires persistent logging of model inputs, predictions, and contextual
    metadata, as well as secure channels for triggering retraining or escalation.
    These tools introduce their own systems overhead and must be integrated with telemetry
    services, alerting frameworks, and model versioning workflows.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 监控基础设施在部署后保持鲁棒性方面发挥着重要作用。分布偏移检测、异常跟踪和行为漂移分析允许系统识别鲁棒性随时间退化的情况。实现这些功能需要持续记录模型输入、预测和上下文元数据，以及触发重新训练或升级的安全通道。这些工具引入了它们自己的系统开销，必须与遥测服务、警报框架和模型版本控制工作流程集成。
- en: Beyond empirical defenses, formal approaches offer stronger guarantees. Certified
    defenses, such as randomized smoothing, provide probabilistic assurances that
    a models output will remain stable within a bounded input region. These methods
    require multiple forward passes per inference and are computationally intensive,
    making them suitable primarily for high-assurance, resource-rich environments.
    Their integration into production workflows also demands compatibility with model
    serving infrastructure and probabilistic verification tooling.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 除了经验性防御之外，形式化方法提供了更强的保证。例如，经过认证的防御，如随机平滑，提供了概率保证，即模型在有限输入区域内输出的稳定性。这些方法在每个推理过程中需要多次正向传递，计算密集，因此主要适用于高保证、资源丰富的环境。它们的生产工作流程集成也要求与模型服务基础设施和概率验证工具兼容。
- en: Simpler defenses, such as input preprocessing, filter inputs through denoising,
    compression, or normalization steps to remove adversarial noise. These transformations
    must be lightweight enough for real-time execution, especially in edge deployments,
    and robust enough to preserve task-relevant features. Another approach is ensemble
    modeling, in which predictions are aggregated across multiple diverse models.
    This increases robustness but adds complexity to inference pipelines, increases
    memory footprint, and complicates deployment and maintenance workflows.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的防御，例如输入预处理，通过去噪、压缩或归一化步骤过滤输入以去除对抗噪声。这些转换必须足够轻量，以便实时执行，特别是在边缘部署中，并且足够鲁棒以保留任务相关的特征。另一种方法是集成建模，其中预测是在多个不同的模型之间汇总的。这增加了鲁棒性，但增加了推理管道的复杂性，增加了内存占用，并复杂化了部署和维护工作流程。
- en: System constraints such as latency, memory, power budget, and model update cadence
    strongly shape which robustness strategies are feasible. Adversarial training
    increases model size and training duration, which may challenge CI/CD pipelines
    and increase retraining costs. Certified defenses demand computational headroom
    and inference time tolerance. Monitoring requires logging infrastructure, data
    retention policies, and access control. On-device and TinyML deployments, in particular,
    often cannot accommodate runtime checks or dynamic updates. In such cases, robustness
    must be validated statically and embedded at compile time.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 系统约束，如延迟、内存、电源预算和模型更新周期，强烈影响着哪些鲁棒性策略是可行的。对抗训练增加了模型大小和训练时间，这可能会挑战CI/CD管道并增加重新训练成本。经过认证的防御需要计算余量和推理时间容忍度。监控需要日志记录基础设施、数据保留策略和访问控制。特别是，在设备上和TinyML部署中，通常无法容纳运行时检查或动态更新。在这种情况下，鲁棒性必须通过静态验证和编译时嵌入。
- en: Adversarial robustness is not a standalone model attribute. It is a system-level
    property that emerges from coordination across training, model architecture, inference
    logic, logging, and fallback pathways. A model that appears robust in isolation
    may still fail if deployed in a system that lacks monitoring or interface safeguards.
    Conversely, even a partially robust model can contribute to overall system reliability
    if embedded within an architecture that detects uncertainty, limits exposure to
    untrusted inputs, and supports recovery when things go wrong.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗鲁棒性不是一个独立的模型属性。它是一个从训练、模型架构、推理逻辑、日志记录和回退路径协调中出现的系统级属性。一个在孤立情况下看似鲁棒的模型，如果部署在一个缺乏监控或接口保护的系统中，仍然可能会失败。相反，即使是一个部分鲁棒的模型，如果嵌入到一个能够检测不确定性、限制对不受信任的输入的暴露，并在出错时支持恢复的架构中，也可以有助于整体系统的可靠性。
- en: Robustness, like privacy and fairness, must be engineered not just into the
    model, but into the system surrounding it. Responsible ML system design requires
    anticipating the ways in which models might fail under real-world stress—and building
    infrastructure that makes those failures detectable, recoverable, and safe.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性，就像隐私和公平性一样，必须不仅被设计到模型中，还要被设计到其周围的系统中。负责任的机器学习系统设计需要预测模型在现实世界压力下可能失败的方式——并构建使这些失败可检测、可恢复和安全的基础设施。
- en: Validation approaches enable stakeholders to understand and audit system behavior.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 验证方法使利益相关者能够理解和审计系统行为。
- en: Validation Approaches
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证方法
- en: Validation approaches provide mechanisms for understanding, auditing, and explaining
    system behavior to stakeholders who must evaluate whether automated decisions
    align with ethical and operational requirements. These techniques enable transparency,
    support regulatory compliance, and build trust between users and automated systems.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 验证方法为理解、审计和向必须评估自动化决策是否符合道德和运营要求的利益相关者解释系统行为提供了机制。这些技术促进了透明度，支持法规遵从，并在用户和自动化系统之间建立信任。
- en: Explainability and Interpretability
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可解释性与可解释性
- en: As machine learning systems are deployed in increasingly consequential domains,
    the ability to understand and interpret model predictions becomes important. Explainability
    and interpretability refer to the technical and design mechanisms that make a
    models behavior intelligible to human stakeholders—whether developers, domain
    experts, auditors, regulators, or end users. While the terms are often used interchangeably,
    interpretability typically refers to the inherent transparency of a model, such
    as a decision tree or linear classifier. Explainability, in contrast, encompasses
    techniques for generating post hoc justifications for predictions made by complex
    or opaque models.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统在越来越重要的领域得到部署，理解和解释模型预测的能力变得重要。可解释性和可解释性指的是使模型行为对人类利益相关者（无论是开发者、领域专家、审计员、监管者还是最终用户）可理解的技术和设计机制。虽然这两个术语经常互换使用，但可解释性通常指模型的内在透明度，例如决策树或线性分类器。相比之下，可解释性包括为复杂或模糊模型做出的预测生成事后合理化的技术。
- en: Explainability plays a central role in system validation, error analysis, user
    trust, regulatory compliance, and incident investigation. In high-stakes domains
    such as healthcare, financial services, and autonomous decision systems, explanations
    help determine whether a model is making decisions for legitimate reasons or relying
    on spurious correlations. For instance, an explainability tool might reveal that
    a diagnostic model is overly sensitive to image artifacts rather than medical
    features, which is a failure mode that could otherwise go undetected. Regulatory
    frameworks in many sectors now mandate that AI systems provide “meaningful information”
    about how decisions are made, reinforcing the need for systematic support for
    explanation.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性在系统验证、错误分析、用户信任、法规遵从和事件调查中扮演着核心角色。在医疗保健、金融服务和自主决策系统等高风险领域，解释有助于确定模型是否基于合法原因做出决策，或者依赖于虚假的相关性。例如，一个可解释性工具可能会揭示诊断模型对图像伪影过于敏感，而不是对医疗特征敏感，这是一种可能未被发现的故障模式。现在，许多行业的监管框架都要求人工智能系统提供关于决策如何做出的“有意义的信息”，这强化了对系统解释的系统性支持的需求。
- en: Explainability methods can be broadly categorized based on when they operate
    and how they relate to model structure. Post hoc methods are applied after training
    and treat the model as a black box. These methods do not require access to internal
    model weights and instead infer influence patterns or feature contributions from
    model behavior. Common post hoc techniques include feature attribution methods
    such as input gradients, Integrated Gradients[33](#fn33), GradCAM[34](#fn34) ([Selvaraju
    et al. 2017](ch058.xhtml#ref-selvaraju2017grad)), LIME ([Ribeiro, Singh, and Guestrin
    2016](ch058.xhtml#ref-ribeiro2016should)), and SHAP ([Lundberg and Lee 2017](ch058.xhtml#ref-lundberg2017unified)).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性方法可以根据它们操作的时间和它们与模型结构的关系进行广泛分类。事后方法在训练后应用，并将模型视为黑盒。这些方法不需要访问内部模型权重，而是从模型行为中推断影响模式或特征贡献。常见的事后技术包括特征归因方法，如输入梯度、集成梯度[33](#fn33)、GradCAM[34](#fn34)
    ([Selvaraju等人 2017](ch058.xhtml#ref-selvaraju2017grad))、LIME ([Ribeiro, Singh,
    and Guestrin 2016](ch058.xhtml#ref-ribeiro2016should)) 和 SHAP ([Lundberg and Lee
    2017](ch058.xhtml#ref-lundberg2017unified))。
- en: 'These approaches are widely used in image and tabular domains, where explanations
    can be rendered as saliency maps or feature rankings. To illustrate how SHAP attribution
    works in practice, consider a trained random forest model predicting loan approval
    (approve=1, deny=0) based on three features: `income`, `debt_ratio`, and `credit_score`.
    For a specific applicant who was denied—with income of $45,000, debt ratio of
    0.55 (55% of income goes to debt), and credit score of 620—the model predicts
    denial with probability 0.72\. SHAP values, based on Shapley values from cooperative
    game theory, measure each feature’s contribution to moving the prediction from
    a baseline (average prediction across all training data, P(approve) = 0.50) to
    this individual prediction.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在图像和表格领域被广泛使用，其中解释可以表示为显著性图或特征排名。为了说明SHAP归因在实际中的工作原理，考虑一个基于三个特征（`income`，`debt_ratio`，和`credit_score`）预测贷款批准（approve=1，deny=0）的已训练随机森林模型。对于一个具体申请者，其收入为$45,000，债务比率为0.55（55%的收入用于债务），信用评分为620，模型预测拒绝的概率为0.72。基于合作博弈论中的Shapley值的SHAP值，衡量每个特征对将预测从基线（所有训练数据的平均预测，P(approve)
    = 0.50）移动到这个个体预测的贡献。
- en: 'The SHAP framework computes each feature’s contribution by evaluating the model
    on all possible feature subsets. Starting from the baseline prediction of 0.50,
    adding income ($45K, slightly below average) decreases approval probability by
    0.05\. Adding debt_ratio (0.55, high) strongly decreases approval by an additional
    0.25\. Adding credit_score (620, below threshold) moderately decreases approval
    by 0.12\. The final prediction becomes 0.50 - 0.05 - 0.25 - 0.12 = 0.08, corresponding
    to P(deny) = 0.72\. This reveals that the high debt ratio contributed most strongly
    to the denial (-0.25), followed by the below-average credit score (-0.12), while
    income had minimal impact (-0.05). Such explanations are actionable: reducing
    debt ratio below 40% would likely flip the decision.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP框架通过评估所有可能的特征子集来计算每个特征的贡献。从基线预测0.50开始，增加收入（$45K，略低于平均水平）会降低批准概率0.05。增加债务比率（0.55，高）会额外降低批准概率0.25。增加信用评分（620，低于阈值）会适度降低批准概率0.12。最终预测变为0.50
    - 0.05 - 0.25 - 0.12 = 0.08，对应于P(deny) = 0.72。这表明高债务比率对拒绝贡献最大（-0.25），其次是低于平均水平的信用评分（-0.12），而收入的影响最小（-0.05）。这样的解释是可操作的：将债务比率降低到40%以下可能会改变决策。
- en: However, this rigor comes at significant computational cost. This 3-feature
    example requires evaluating 2³ = 8 feature subsets. For a model with 20 features,
    SHAP requires 2²⁰ ≈ 1 million subset evaluations, explaining the 50-1000x computational
    overhead compared to simple gradient methods. Tree-based SHAP implementations
    exploit model structure to reduce this to polynomial time, but deep learning models
    typically require approximation algorithms (KernelSHAP, DeepSHAP) with sampling-based
    estimation. While SHAP provides theoretically grounded, additive feature attribution
    that satisfies desirable properties (local accuracy, missingness, consistency),
    these costs make SHAP impractical for real-time explanation in high-throughput
    systems without approximation or caching strategies.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种严谨性伴随着巨大的计算成本。这个包含3个特征的示例需要评估2³ = 8个特征子集。对于一个包含20个特征的模型，SHAP需要评估2²⁰ ≈ 1百万个特征子集，这解释了与简单梯度方法相比，计算开销高达50-1000倍。基于树的SHAP实现通过利用模型结构将计算时间降低到多项式时间，但深度学习模型通常需要近似算法（KernelSHAP，DeepSHAP）以及基于采样的估计。虽然SHAP提供了理论上基于的、可加的特征归因，这些归因满足了一些理想属性（局部精度、缺失值、一致性），但这些成本使得SHAP在没有近似或缓存策略的情况下，对于高吞吐量系统的实时解释来说并不实用。
- en: Another post hoc approach involves counterfactual explanations, which describe
    how a models output would change if the input were modified in specific ways.
    These are especially relevant for decision-facing applications such as credit
    or hiring systems. For example, a counterfactual explanation might state that
    an applicant would have received a loan approval if their reported income were
    higher or their debt lower ([Wachter, Mittelstadt, and Russell 2017](ch058.xhtml#ref-wachter2017counterfactual)).
    Counterfactual generation requires access to domain-specific constraints and realistic
    data manifolds, making integration into real-time systems challenging.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种事后方法涉及反事实解释，它描述了如果输入以特定方式修改，模型输出会如何变化。这些解释对于面向决策的应用程序，如信贷或招聘系统，尤其相关。例如，一种反事实解释可能声明，如果申请人的报告收入更高或债务更低，他们将获得贷款批准（[Wachter,
    Mittelstadt, and Russell 2017](ch058.xhtml#ref-wachter2017counterfactual)）。反事实生成需要访问特定领域的约束和现实数据流形，这使得将其集成到实时系统中具有挑战性。
- en: A third class of techniques relies on concept-based explanations, which attempt
    to align learned model features with human-interpretable concepts. For example,
    a convolutional network trained to classify indoor scenes might activate filters
    associated with “lamp,” “bed,” or “bookshelf” ([C. J. Cai et al. 2019](ch058.xhtml#ref-kim2018interpretability)).
    These methods are especially useful in domains where subject matter experts expect
    explanations in familiar semantic terms. However, they require training data with
    concept annotations or auxiliary models for concept detection, which introduces
    additional infrastructure dependencies.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类技术依赖于基于概念的解释，这些解释试图将学习到的模型特征与人类可解释的概念相一致。例如，一个用于分类室内场景的卷积网络可能会激活与“灯”、“床”或“书架”相关的过滤器（[C.
    J. Cai 等人 2019](ch058.xhtml#ref-kim2018interpretability)）。这些方法在领域专家期望以熟悉的语义术语进行解释的领域特别有用。然而，它们需要具有概念注释的训练数据或用于概念检测的辅助模型，这引入了额外的基础设施依赖。
- en: While post hoc methods are flexible and broadly applicable, they come with limitations.
    Because they approximate reasoning after the fact, they may produce plausible
    but misleading rationales. Their effectiveness depends on model smoothness, input
    structure, and the fidelity of the explanation technique. These methods are often
    most useful for exploratory analysis, debugging, or user-facing summaries—not
    as definitive accounts of internal logic.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然事后方法具有灵活性和广泛的应用性，但它们也存在局限性。因为它们在事后近似推理，可能会产生看似合理但具有误导性的理由。它们的有效性取决于模型的平滑度、输入结构和解释技术的保真度。这些方法通常在探索性分析、调试或面向用户的总结中最为有用，而不是作为内部逻辑的最终解释。
- en: In contrast, inherently interpretable models are transparent by design. Examples
    include decision trees, rule lists, linear models with monotonicity constraints,
    and k-nearest neighbor classifiers. These models expose their reasoning structure
    directly, enabling stakeholders to trace predictions through a set of interpretable
    rules or comparisons. In regulated or safety-important domains such as recidivism
    prediction or medical triage, inherently interpretable models may be preferred,
    even at the cost of some accuracy ([Rudin 2019](ch058.xhtml#ref-rudin2019stop)).
    However, these models generally do not scale well to high-dimensional or unstructured
    data, and their simplicity can limit performance in complex tasks.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，本质上可解释的模型在设计上就是透明的。例如，决策树、规则列表、具有单调性约束的线性模型和k-最近邻分类器。这些模型直接暴露其推理结构，使得利益相关者可以通过一系列可解释的规则或比较来追踪预测。在监管或安全至关重要的领域，如再犯预测或医疗分诊，本质上可解释的模型可能更受欢迎，即使这要以一些准确性为代价（[Rudin
    2019](ch058.xhtml#ref-rudin2019stop)）。然而，这些模型通常难以扩展到高维或非结构化数据，并且它们的简单性可能会限制在复杂任务中的性能。
- en: The relative interpretability of different model types can be visualized along
    a spectrum. As shown in [Figure 17.7](ch023.xhtml#fig-interpretability-spectrum),
    models such as decision trees and linear regression offer transparency by design,
    whereas more complex architectures like neural networks and convolutional models
    require external techniques to explain their behavior. This distinction is central
    to choosing an appropriate model for a given application—particularly in settings
    where regulatory scrutiny or stakeholder trust is paramount.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型类型的相对可解释性可以沿着一个谱系进行可视化。如图[图17.7](ch023.xhtml#fig-interpretability-spectrum)所示，决策树和线性回归等模型通过设计提供透明度，而像神经网络和卷积模型这样更复杂的架构则需要外部技术来解释其行为。这种区别在选择适用于特定应用的适当模型时至关重要——尤其是在监管审查或利益相关者信任至关重要的环境中。
- en: '![](../media/file296.svg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file296.svg)'
- en: 'Figure 17.7: **Model Interpretability Spectrum**: Inherently interpretable
    models, such as linear regression and decision trees, offer transparent reasoning,
    while complex models like neural networks require post-hoc explanation techniques
    to understand their predictions. This distinction guides model selection based
    on application needs, prioritizing transparency in regulated domains or when stakeholder
    trust is important.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7：**模型可解释性谱系**：内禀可解释的模型，如线性回归和决策树，提供透明的推理，而复杂的模型如神经网络则需要事后解释技术来理解其预测。这种区别指导了基于应用需求进行模型选择，优先考虑在受监管领域或当利益相关者信任重要时的透明度。
- en: Hybrid approaches aim to combine the representational capacity of deep models
    with the transparency of interpretable components. Concept bottleneck models ([Koh
    et al. 2020](ch058.xhtml#ref-koh2020concept)), for example, first predict intermediate,
    interpretable variables and then use a simple classifier to produce the final
    prediction. ProtoPNet models ([C. Chen et al. 2019](ch058.xhtml#ref-chen2019looks))
    classify examples by comparing them to learned prototypes, offering visual analogies
    for users to understand predictions. These hybrid methods are attractive in domains
    that demand partial transparency, but they introduce new system design considerations,
    such as the need to store and index learned prototypes and surface them at inference
    time.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法旨在结合深度模型的表示能力和可解释组件的透明度。例如，概念瓶颈模型([Koh等人2020](ch058.xhtml#ref-koh2020concept))首先预测中间的可解释变量，然后使用简单的分类器生成最终预测。ProtoPNet模型([C.
    Chen等人2019](ch058.xhtml#ref-chen2019looks))通过将示例与学习到的原型进行比较来分类，为用户提供理解预测的视觉类比。这些混合方法在需要部分透明度的领域中具有吸引力，但它们引入了新的系统设计考虑因素，例如需要存储和索引学习到的原型，并在推理时呈现它们。
- en: A more recent research direction is mechanistic interpretability, which seeks
    to reverse-engineer the internal operations of neural networks. This line of work,
    inspired by program analysis and neuroscience, attempts to map neurons, layers,
    or activation patterns to specific computational functions ([Olah et al. 2020](ch058.xhtml#ref-olah2020zoom);
    [Geiger et al. 2021](ch058.xhtml#ref-geiger2021causal)). Although promising, this
    field remains exploratory and is currently most relevant to the analysis of large
    foundation models where traditional interpretability tools are insufficient.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更近期的研究方向是机制可解释性，它试图逆向工程神经网络的内部分工。这一工作受到程序分析和神经科学的启发，试图将神经元、层或激活模式映射到特定的计算功能([Olah等人2020](ch058.xhtml#ref-olah2020zoom);
    [Geiger等人2021](ch058.xhtml#ref-geiger2021causal))。尽管前景广阔，但这一领域仍然是探索性的，目前主要与大型基础模型的分析相关，在这些模型中，传统的可解释性工具是不够的。
- en: From a systems perspective, explainability introduces a number of architectural
    dependencies. Explanations must be generated, stored, surfaced, and evaluated
    within system constraints. The required infrastructure may include explanation
    APIs, memory for storing attribution maps, visualization libraries, and logging
    mechanisms that capture intermediate model behavior. Models must often be instrumented
    with hooks or configured to support repeated evaluations—particularly for explanation
    methods that require sampling, perturbation, or backpropagation.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统视角来看，可解释性引入了许多架构依赖。解释必须在系统约束内生成、存储、呈现和评估。所需的基础设施可能包括解释API、存储归因图的内存、可视化库以及捕获中间模型行为的日志机制。模型通常需要通过钩子进行仪器化或配置以支持重复评估——尤其是对于需要采样、扰动或反向传播的解释方法。
- en: 'These requirements interact directly with deployment constraints and impose
    quantifiable performance costs that must be factored into system design. SHAP
    explanations typically require 50-1000x additional forward passes compared to
    standard inference, with computational overhead ranging from 200 ms to 5+ seconds
    per explanation depending on model complexity. LIME similarly requires training
    surrogate models that add 100-500 ms per explanation. In production deployments,
    these costs translate to significant infrastructure overhead: a high-traffic system
    serving 10,000 predictions per second with 10% explanation rate would require
    50-500x additional compute capacity solely for explainability.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这些需求直接与部署约束相互作用，并施加可量化的性能成本，这些成本必须纳入系统设计。SHAP解释通常需要比标准推理多50-1000倍的向前传递，每解释的计算开销从200
    ms到5+秒不等，具体取决于模型复杂性。LIME类似地需要训练代理模型，每解释增加100-500 ms。在生产部署中，这些成本转化为显著的基础设施开销：一个每秒处理10,000次预测且解释率为10%的高流量系统，仅为了可解释性就需要50-500倍额外的计算能力。
- en: For resource-constrained environments, gradient-based attribution methods offer
    more efficient alternatives, typically adding only 10-50 ms overhead per explanation
    by leveraging backpropagation infrastructure already present for training. However,
    these methods are less reliable for complex models and may produce inconsistent
    explanations across model updates. Edge deployments often implement explainability
    through precomputed rule approximations or simplified decision boundaries, sacrificing
    explanation fidelity for feasible latency profiles under 100 ms.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对于资源受限的环境，基于梯度的归因方法提供了更有效的替代方案，通常通过利用已经存在的反向传播基础设施，每解释增加10-50 ms的开销。然而，这些方法对于复杂模型来说可靠性较低，可能在模型更新之间产生不一致的解释。边缘部署通常通过预计算的规则近似或简化的决策边界来实现可解释性，牺牲了解释的精确性以换取低于100
    ms的可行延迟。
- en: Storage requirements also scale significantly with explanation needs. Storing
    SHAP values for tabular data requires approximately 4-8 bytes per feature per
    prediction, while gradient attribution maps for images can require 1-10 MB per
    explanation depending on resolution. A production system maintaining explanation
    logs for 1 million predictions daily would require 50 GB-10 TB of additional storage
    capacity monthly, necessitating careful data lifecycle management and retention
    policies.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 存储需求也随着解释需求而显著增加。存储表格数据的SHAP值大约需要每个特征每预测4-8字节，而图像的梯度归因图可能需要根据分辨率每解释1-10 MB。一个每天维护100万次预测的解释日志的生产系统，每月需要额外的50
    GB-10 TB存储容量，这需要仔细的数据生命周期管理和保留策略。
- en: Explainability spans the full machine learning lifecycle. During development,
    interpretability tools are used for dataset auditing, concept validation, and
    early debugging. At inference time, they support accountability, decision verification,
    and user communication. Post-deployment, explanations may be logged, surfaced
    in audits, or queried during error investigations. System design must support
    each of these phases—ensuring that explanation tools are integrated into training
    frameworks, model serving infrastructure, and user-facing applications.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性贯穿整个机器学习生命周期。在开发阶段，可解释性工具用于数据集审计、概念验证和早期调试。在推理时间，它们支持问责制、决策验证和用户沟通。部署后，解释可能被记录、在审计中呈现或在错误调查期间查询。系统设计必须支持这些所有阶段——确保解释工具集成到训练框架、模型服务基础设施和面向用户的应用程序中。
- en: Compression and optimization techniques also affect explainability. Pruning,
    quantization, and architectural simplifications often used in TinyML or mobile
    settings can distort internal representations or disable gradient flow, degrading
    the reliability of attribution-based explanations. In such cases, interpretability
    must be validated post-optimization to ensure that it remains meaningful and trustworthy.
    If explanation quality is important, these transformations must be treated as
    part of the design constraint space.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩和优化技术也会影响可解释性。在TinyML或移动设置中常用到的剪枝、量化和架构简化可能会扭曲内部表示或禁用梯度流，降低基于归因的解释的可靠性。在这种情况下，必须在优化后验证可解释性，以确保其仍然有意义和可信。如果解释质量很重要，这些变换必须被视为设计约束空间的一部分。
- en: Explainability is not an add-on feature but a system-wide concern. Designing
    for interpretability requires careful decisions about who needs explanations,
    what kind of explanations are meaningful, and how those explanations can be delivered
    given the systems latency, compute, and interface budget. As machine learning
    becomes embedded in important workflows, the ability to explain becomes a core
    requirement for safe, trustworthy, and accountable systems.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性不是一个附加功能，而是一个系统级关注点。为了可解释性而设计需要谨慎地决定谁需要解释，什么样的解释是有意义的，以及如何在系统的延迟、计算和界面预算限制下提供这些解释。随着机器学习嵌入到重要的工作流程中，解释能力成为安全、可信和可问责系统的核心要求。
- en: The sociotechnical challenges of explainability center on the gap between technical
    explanations and human understanding. While algorithms can generate feature attributions
    and gradient maps, stakeholders often need explanations that align with their
    mental models, domain expertise, and decision-making processes. A radiologist
    reviewing an AI-generated diagnosis needs explanations that reference medical
    concepts and visual patterns, not abstract neural network activations. This translation
    challenge requires ongoing collaboration between technical teams and domain experts
    to develop explanation formats that are both technically accurate and practically
    meaningful. Explanations can shape human decision-making in unexpected ways, creating
    new responsibilities for how explanatory information is presented and interpreted.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的社会技术挑战集中在技术解释和人类理解之间的差距。虽然算法可以生成特征归因和梯度图，但利益相关者通常需要与他们的心理模型、领域专业知识和决策过程相一致的解释。一位审查AI生成的诊断的放射科医生需要参考医学概念和视觉模式的解释，而不是抽象的神经网络激活。这种翻译挑战需要技术团队和领域专家之间的持续合作，以开发既技术准确又实际有意义的解释格式。解释可以在意想不到的方式上塑造人类决策，从而为解释信息的呈现和解释方式带来新的责任。
- en: Model Performance Monitoring
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型性能监控
- en: 'Training-time evaluations, no matter how rigorous, do not guarantee reliable
    model performance once a system is deployed. Real-world environments are dynamic:
    input distributions shift due to seasonality, user behavior evolves in response
    to system outputs, and contextual expectations change with policy or regulation.
    These factors can cause predictive performance, and even more importantly, system
    trustworthiness, to degrade over time. A model that performs well under training
    or validation conditions may still make unreliable or harmful decisions in production.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 无论训练时的评估多么严格，一旦系统部署，都不能保证可靠的模型性能。现实环境是动态的：由于季节性，输入分布会发生变化；用户行为会随着系统输出而演变；政策或法规的变化会导致上下文期望发生变化。这些因素可能导致预测性能下降，甚至更重要的是，系统可信度随时间退化。在训练或验证条件下表现良好的模型，在生产中仍可能做出不可靠或有害的决定。
- en: The implications of such drift extend beyond raw accuracy. Fairness guarantees
    may break down if subgroup distributions shift relative to the training set, or
    if features that previously correlated with outcomes become unreliable in new
    contexts. Interpretability demands may also evolve—for instance, as new stakeholder
    groups seek explanations, or as regulators introduce new transparency requirements.
    Trustworthiness, therefore, is not a static property conferred at training time,
    but a dynamic system attribute shaped by deployment context and operational feedback.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这种漂移的影响不仅限于原始准确率。如果子群体分布相对于训练集发生变化，或者先前与结果相关的特征在新环境中变得不可靠，公平性保证可能会失效。可解释性的需求也可能发生变化——例如，随着新的利益相关者群体寻求解释，或者随着监管机构引入新的透明度要求。因此，可信度不是一个在训练时授予的静态属性，而是一个由部署环境和操作反馈塑造的动态系统属性。
- en: To ensure responsible behavior over time, machine learning systems must incorporate
    mechanisms for continual monitoring, evaluation, and corrective action. Monitoring
    involves more than tracking aggregate accuracy—it requires surfacing performance
    metrics across relevant subgroups, detecting shifts in input distributions, identifying
    anomalous outputs, and capturing meaningful user feedback. These signals must
    then be compared to predefined expectations around fairness, robustness, and transparency,
    and linked to actionable system responses such as model retraining, recalibration,
    or rollback.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保长期的责任行为，机器学习系统必须纳入持续监控、评估和纠正行动的机制。监控不仅涉及跟踪总体准确性，还需要在相关的子组中呈现性能指标，检测输入分布的变化，识别异常输出，并捕捉有意义的用户反馈。然后，这些信号必须与关于公平性、鲁棒性和透明度的预定义期望进行比较，并链接到可操作的系统响应，如模型重新训练、重新校准或回滚。
- en: Implementing effective monitoring depends on robust infrastructure. Systems
    must log inputs, outputs, and contextual metadata in a structured and secure manner.
    This requires telemetry pipelines that capture model versioning, input characteristics,
    prediction confidence, and post-inference feedback. These logs support drift detection
    and provide evidence for retrospective audits of fairness and robustness. Monitoring
    systems must also be integrated with alerting, update scheduling, and policy review
    processes to support timely and traceable intervention.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 实施有效的监控依赖于稳健的基础设施。系统必须以结构化和安全的方式记录输入、输出和上下文元数据。这需要能够捕获模型版本、输入特征、预测置信度和后推理反馈的遥测管道。这些日志支持漂移检测，并为公平性和鲁棒性的事后审计提供证据。监控系统还必须与警报、更新调度和政策审查流程集成，以支持及时和可追溯的干预。
- en: 'Monitoring also supports feedback-driven improvement. For example, repeated
    user disagreement, correction requests, or operator overrides can signal problematic
    behavior. This feedback must be aggregated, validated, and translated into updates
    to training datasets, data labeling processes, or model architecture. However,
    such feedback loops carry risks: biased user responses can introduce new inequities,
    and excessive logging can compromise privacy. Designing these loops requires careful
    coordination between user experience design, system security, and ethical governance.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 监控还支持反馈驱动的改进。例如，重复的用户不同意、更正请求或操作员覆盖可以表明存在问题的行为。这种反馈必须被汇总、验证并转化为更新训练数据集、数据标注过程或模型架构。然而，这样的反馈循环存在风险：有偏的用户响应可能会引入新的不平等，而过度的日志记录可能会损害隐私。设计这些循环需要用户体验设计、系统安全和道德治理之间的仔细协调。
- en: Monitoring mechanisms vary by deployment architecture. In cloud-based systems,
    rich logging and compute capacity allow for real-time telemetry, scheduled fairness
    audits, and continuous integration of new data into retraining pipelines. These
    environments support dynamic reconfiguration and centralized policy enforcement.
    However, the volume of telemetry may introduce its own challenges in terms of
    cost, privacy risk, and regulatory compliance.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 监控机制因部署架构而异。在基于云的系统，丰富的日志记录和计算能力允许实时遥测、计划中的公平性审计以及将新数据持续集成到重新训练管道中。这些环境支持动态重新配置和集中式政策执行。然而，遥测数据的量可能会在成本、隐私风险和合规性方面带来自己的挑战。
- en: In mobile systems, connectivity is intermittent and data storage is limited.
    Monitoring must be lightweight and resilient to synchronization delays. Local
    inference systems may collect performance data asynchronously and transmit it
    in aggregate to backend systems. Privacy constraints are often stricter, particularly
    when personal data must remain on-device. These systems require careful data minimization
    and local aggregation techniques to preserve privacy while maintaining observability.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动系统中，连接是间歇性的，数据存储有限。监控必须轻量级且能够抵御同步延迟。本地推理系统可能会异步收集性能数据，并将其汇总传输到后端系统。隐私约束通常更严格，尤其是在个人数据必须保留在设备上时。这些系统需要仔细的数据最小化和本地聚合技术，以在保持可观察性的同时保护隐私。
- en: Edge deployments, such as those in autonomous vehicles, smart factories, or
    real-time control systems, demand low-latency responses and operate with minimal
    external supervision. Monitoring in these systems must be embedded within the
    runtime, with internal checks on sensor integrity, prediction confidence, and
    behavior deviation. These checks often require low-overhead implementations of
    uncertainty estimation, anomaly detection, or consistency validation. System designers
    must anticipate failure conditions and ensure that anomalous behavior triggers
    safe fallback procedures or human intervention.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘部署，如自动驾驶汽车、智能工厂或实时控制系统，需要低延迟响应，并以最小外部监督运行。在这些系统中的监控必须嵌入到运行时，对传感器完整性、预测置信度和行为偏差进行内部检查。这些检查通常需要低开销的不确定性估计、异常检测或一致性验证的实现。系统设计者必须预测故障条件，并确保异常行为触发安全回退程序或人工干预。
- en: TinyML systems, which operate on deeply embedded hardware with no connectivity,
    persistent storage, or dynamic update path, present the most constrained monitoring
    scenario. In these environments, monitoring must be designed and compiled into
    the system prior to deployment. Common strategies include input range checking,
    built-in redundancy, static failover logic, or conservative validation thresholds.
    Once deployed, these models operate independently, and any post-deployment failure
    may require physical device replacement or firmware-level reset.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML系统，在无连接、持久存储或动态更新路径的深度嵌入式硬件上运行，呈现了最受限的监控场景。在这些环境中，监控必须在部署前设计和编译到系统中。常见的策略包括输入范围检查、内置冗余、静态故障转移逻辑或保守的验证阈值。一旦部署，这些模型独立运行，任何部署后的故障可能需要物理设备更换或固件级别的重置。
- en: 'The core challenge is universal: deployed ML systems must not only perform
    well initially, but continue to behave responsibly as the environment changes.
    Monitoring provides the observability layer that links system performance to ethical
    goals and accountability structures. Without monitoring, fairness and robustness
    become invisible. Without feedback, misalignment cannot be corrected. Monitoring,
    therefore, is the operational foundation that allows machine learning systems
    to remain adaptive, auditable, and aligned with their intended purpose over time.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 核心挑战是普遍的：部署的机器学习系统不仅最初必须表现良好，而且随着环境的变化必须继续负责任地行为。监控提供了将系统性能与道德目标和问责结构联系起来的可观察性层。没有监控，公平性和鲁棒性将变得不可见。没有反馈，偏差无法得到纠正。因此，监控是操作基础，使机器学习系统能够保持适应性、可审计性和与其预期目的的一致性。
- en: 'The technical methods explored in this section—bias detection algorithms, differential
    privacy mechanisms, adversarial training procedures, and explainability frameworks—provide
    essential capabilities for responsible AI implementation. However, these tools
    reveal a fundamental limitation: technical correctness alone cannot guarantee
    beneficial outcomes. Consider three concrete examples that illustrate this challenge:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨的技术方法——偏差检测算法、差分隐私机制、对抗性训练程序和可解释性框架——为负责任的AI实现提供了基本能力。然而，这些工具揭示了根本性的限制：仅仅技术正确性不能保证有益的结果。考虑三个具体例子来说明这一挑战：
- en: A fairness auditing system detects racial bias in a loan approval model, but
    the organization lacks processes for interpreting results or implementing corrections.
    The technical capability exists, but organizational inertia prevents remediation.
    Differential privacy preserves formal mathematical guarantees about data protection,
    but users do not understand these protections and continue to share sensitive
    information inappropriately. The privacy method works as designed, but behavioral
    context undermines its effectiveness. An explainability system generates technically
    accurate feature importance scores, but affected individuals cannot access or
    interpret these explanations due to interface design and literacy barriers.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 公平审计系统检测贷款审批模型中的种族偏见，但该组织缺乏解释结果或实施纠正措施的过程。技术能力存在，但组织惯性阻碍了补救。差分隐私保护了关于数据保护的正式数学保证，但用户不理解这些保护，并继续不适当地共享敏感信息。隐私方法按设计工作，但行为上下文削弱了其有效性。可解释性系统生成技术准确的特征重要性分数，但受影响的人由于界面设计和识字障碍无法访问或解释这些解释。
- en: These examples demonstrate that responsible AI implementation depends on alignment
    between technical capabilities and sociotechnical contexts—organizational incentives,
    human behavior, stakeholder values, and institutional governance structures.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子表明，负责任的人工智能实施取决于技术能力和社会技术环境之间的协调——组织激励、人类行为、利益相关者价值观和制度治理结构。
- en: Sociotechnical Dynamics
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会技术动态
- en: Responsible AI systems operate within complex sociotechnical environments where
    technical methods interact with human behavior, organizational practices, and
    competing stakeholder values. The bias detection tools, privacy preservation techniques,
    and explainability methods examined above provide necessary capabilities, but
    their effectiveness depends entirely on how they integrate with decision-making
    processes, user interfaces, feedback mechanisms, and governance structures. Understanding
    these interactions is essential for sustainable responsible AI deployment that
    achieves beneficial outcomes in practice.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能系统在复杂的社会技术环境中运行，其中技术方法与人类行为、组织实践和竞争的利益相关者价值观相互作用。上述检测偏差的工具、保护隐私的技术和可解释性方法提供了必要的功能，但它们的有效性完全取决于它们如何与决策过程、用户界面、反馈机制和治理结构整合。理解这些相互作用对于实现实际有益结果的可持续负责任人工智能部署至关重要。
- en: '**Cognitive Shift: From Pure Engineering to Sociotechnical Engineering**'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**认知转变：从纯工程到社会技术工程**'
- en: 'The previous section focused on technical tools for solving well-defined problems:
    algorithms for detecting bias, methods for preserving privacy, and techniques
    for generating explanations. We now shift our analytical perspective to address
    challenges that cannot be solved with algorithms alone.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节重点介绍了解决定义明确问题的技术工具：检测偏差的算法、保护隐私的方法和生成解释的技术。我们现在转变分析视角，以解决仅用算法无法解决的问题。
- en: 'The following sections examine how responsible AI systems interact with people,
    organizations, and competing values. This transition requires different reasoning
    skills: instead of optimizing objective functions, we analyze stakeholder conflicts;
    instead of tuning hyperparameters, we navigate ethical tradeoffs; instead of measuring
    technical performance, we assess social impact. These are the challenges of sociotechnical
    engineering—designing systems that must satisfy both computational constraints
    and human values.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将探讨负责任的人工智能系统如何与人类、组织和竞争价值观互动。这种转变需要不同的推理技能：不是优化目标函数，而是分析利益相关者冲突；不是调整超参数，而是导航伦理权衡；不是衡量技术性能，而是评估社会影响。这些都是社会技术工程面临的挑战——设计必须满足计算约束和人类价值观的系统。
- en: Understanding these sociotechnical dynamics is crucial for sustainable responsible
    AI implementation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些社会技术动态对于可持续的负责任人工智能实施至关重要。
- en: Responsible machine learning system design extends beyond technical correctness
    and algorithmic safeguards. Once deployed, these systems operate within complex
    sociotechnical environments where their outputs influence, and are influenced
    by, human behavior, institutional practices, and evolving societal norms. Over
    time, machine learning systems become part of the environments they are intended
    to model, creating feedback dynamics that affect future data collection, model
    retraining, and downstream decision-making.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能系统设计不仅超越了技术正确性和算法保障。一旦部署，这些系统将在复杂的社会技术环境中运行，其输出会影响并受到人类行为、制度实践和不断变化的社会规范的影响。随着时间的推移，机器学习系统成为它们旨在模拟的环境的一部分，从而产生反馈动态，影响未来的数据收集、模型再训练和下游决策。
- en: This section addresses the broader ethical and systemic challenges associated
    with the deployment of machine learning technologies. It examines how feedback
    loops between models and environments can reinforce bias, how human-AI collaboration
    introduces new risks and responsibilities, and how conflicts between stakeholder
    values complicate the operationalization of fairness and accountability. It considers
    the role of contestability and institutional governance in sustaining responsible
    system behavior. These considerations highlight that responsibility is not a static
    property of an algorithm, but a dynamic outcome of system design, usage, and oversight
    over time.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了与机器学习技术部署相关的更广泛的伦理和系统性挑战。它探讨了模型和环境之间的反馈循环如何强化偏见，人类-人工智能合作如何引入新的风险和责任，以及利益相关者价值观之间的冲突如何复杂化公平性和问责制的实施。它考虑了可争议性和制度治理在维持负责任系统行为中的作用。这些考虑表明，责任不是算法的静态属性，而是系统设计、使用和长期监督的动态结果。
- en: System Feedback Loops
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统反馈循环
- en: Machine learning systems do not merely observe and model the world; they also
    shape it. Once deployed, their predictions and decisions often influence the environments
    they are intended to analyze. This feedback alters future data distributions,
    modifies user behavior, and affects institutional practices, creating a recursive
    loop between model outputs and system inputs. Over time, such dynamics can amplify
    biases, entrench disparities, or unintentionally shift the objectives a model
    was designed to serve.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统不仅仅是观察和模拟世界；它们也塑造世界。一旦部署，它们的预测和决策往往会影响它们旨在分析的环境。这种反馈会改变未来的数据分布，修改用户行为，并影响制度实践，在模型输出和系统输入之间形成一个递归循环。随着时间的推移，这种动态可能会放大偏见，加剧不平等，或无意中改变模型旨在服务的目标。
- en: A well-documented example of this phenomenon is predictive policing. When a
    model trained on historical arrest data predicts higher crime rates in a particular
    neighborhood, law enforcement may allocate more patrols to that area. This increased
    presence leads to more recorded incidents, which are then used as input for future
    model training, further reinforcing the model’s original prediction. Even if the
    model was not explicitly biased at the outset, its integration into a feedback
    loop results in a self-fulfilling pattern that disproportionately affects already
    over-policed communities.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象的一个很好的例子是预测警务。当一个基于历史逮捕数据的模型预测某个特定地区的犯罪率较高时，执法部门可能会向该地区分配更多的巡逻警力。这种增加的警力导致更多记录的事件，然后这些事件被用作未来模型训练的输入，进一步强化了模型最初的预测。即使模型在开始时没有明显的偏见，但其融入反馈循环会导致一个自我实现的模式，这种模式不成比例地影响已经过度警力的社区。
- en: Recommender systems exhibit similar dynamics in digital environments. A content
    recommendation model that prioritizes engagement may gradually narrow the range
    of content a user is exposed to, leading to feedback loops that reinforce existing
    preferences or polarize opinions. These effects can be difficult to detect using
    conventional performance metrics, as the system continues to optimize its training
    objective even while diverging from broader social or epistemic goals.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统在数字环境中表现出类似的动态。一个优先考虑参与度的内容推荐模型可能会逐渐缩小用户接触到的内容范围，导致强化现有偏好或极端化意见的反馈循环。这些影响可能难以使用传统的性能指标检测，因为系统在偏离更广泛的社会或认识论目标的同时，仍在优化其训练目标。
- en: From a systems perspective, feedback loops present a core challenge to responsible
    AI. They undermine the assumption of independently and identically distributed
    data and complicate the evaluation of fairness, robustness, and generalization.
    Standard validation methods, which rely on static test sets, may fail to capture
    the evolving impact of the model on the data-generating process. Once such loops
    are established, interventions aimed at improving fairness or accuracy may have
    limited effect unless the underlying data dynamics are addressed.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，反馈循环对负责任的AI构成了核心挑战。它们破坏了独立同分布数据的假设，并复杂化了公平性、鲁棒性和泛化的评估。依赖于静态测试集的标准验证方法可能无法捕捉模型对数据生成过程影响的演变。一旦建立这样的循环，旨在提高公平性或准确性的干预措施可能效果有限，除非解决潜在的数据动态。
- en: Designing for responsibility in the presence of feedback loops requires a lifecycle
    view of machine learning systems. It entails not only monitoring model performance
    over time, but also understanding how the systems outputs influence the environment,
    how these changes are captured in new data, and how retraining practices either
    mitigate or exacerbate these effects.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在反馈循环的情况下设计责任需要机器学习系统的生命周期视图。这不仅包括监控模型性能随时间的变化，还包括理解系统输出如何影响环境，这些变化如何在新数据中捕捉到，以及再训练实践如何减轻或加剧这些效应。
- en: In cloud-based systems, these updates may occur frequently and at scale, with
    extensive telemetry available to detect behavior drift. In contrast, edge and
    embedded deployments often operate offline or with limited observability. A smart
    home system that adapts thermostat behavior based on user interactions may reinforce
    energy consumption patterns or comfort preferences in ways that alter the home
    environment—and subsequently affect future inputs to the model. Without connectivity
    or centralized oversight, these loops may go unrecognized, despite their impact
    on both user behavior and system performance. The operational monitoring practices
    detailed in [Chapter 13](ch019.xhtml#sec-ml-operations) are crucial for detecting
    and managing these feedback dynamics in production systems.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于云的系统中，这些更新可能频繁发生且规模庞大，有广泛的遥测数据可用于检测行为漂移。相比之下，边缘和嵌入式部署通常在离线或有限的可观察性下运行。一个根据用户交互调整恒温器行为的智能家居系统可能会以改变家庭环境的方式强化能源消耗模式或舒适偏好——从而影响模型未来的输入。在没有连接或集中监督的情况下，这些循环可能未被识别，尽管它们对用户行为和系统性能都有影响。第13章中详细说明的操作监控实践对于在生产系统中检测和管理这些反馈动态至关重要。
- en: Systems must be equipped with mechanisms to detect distributional drift, identify
    behavior shaping effects, and support corrective updates that align with the systems
    intended goals. Feedback loops are not inherently harmful, but they must be recognized
    and managed. When left unexamined, they introduce systemic risk; when thoughtfully
    addressed, they provide an opportunity for learning systems to adapt responsibly
    in complex, dynamic environments.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 系统必须配备机制来检测分布漂移、识别行为塑造效应，并支持与系统预期目标一致的纠正更新。反馈循环本身并非有害，但它们必须被识别和管理。当未被审查时，它们会引入系统性风险；当经过深思熟虑地处理时，它们为学习系统在复杂、动态环境中负责任地适应提供了机会。
- en: These system-level feedback dynamics become even more complex when human operators
    are integrated into the decision-making process.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类操作员集成到决策过程中时，这些系统级反馈动态变得更加复杂。
- en: Human-AI Collaboration
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人机协作
- en: Machine learning systems are increasingly deployed not as standalone agents,
    but as components in larger workflows that involve human decision-makers. In many
    domains, such as healthcare, finance, and transportation, models serve as decision-support
    tools, offering predictions, risk scores, or recommendations that are reviewed
    and acted upon by human operators. This collaborative configuration raises important
    questions about how responsibility is shared between humans and machines, how
    trust is calibrated, and how oversight mechanisms are implemented in practice.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统越来越多地被部署为不仅仅是独立代理，而是作为涉及人类决策者的更大工作流程中的组件。在许多领域，如医疗保健、金融和交通，模型作为决策支持工具，提供预测、风险评分或推荐，这些由人类操作员审查并采取行动。这种协作配置提出了关于责任在人类和机器之间如何共享、信任如何校准以及如何在实践中实施监督机制的重要问题。
- en: Human-AI collaboration introduces both opportunities and risks. When designed
    appropriately, systems can augment human judgment, reduce cognitive burden, and
    enhance consistency in decision-making. However, when poorly designed, they may
    lead to automation bias, where users over-rely on model outputs even in the presence
    of clear errors. Conversely, excessive distrust can result in algorithm aversion,
    where users disregard useful model predictions due to a lack of transparency or
    perceived credibility. The effectiveness of collaborative systems depends not
    only on the model’s performance, but on how the system communicates uncertainty,
    provides explanations, and allows for human override or correction.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 人机协作既带来了机遇也带来了风险。当设计得当，系统可以增强人类判断，减轻认知负担，并提高决策的一致性。然而，当设计不当，它们可能导致自动化偏差，即用户即使在存在明显错误的情况下也过度依赖模型输出。相反，过度不信任可能导致算法厌恶，即用户由于缺乏透明度或感知可信度而忽视有用的模型预测。协作系统的有效性不仅取决于模型的表现，还取决于系统如何传达不确定性，提供解释，以及允许人类覆盖或纠正。
- en: Oversight mechanisms must be tailored to the deployment context. In high-stakes
    domains, such as medical triage or autonomous driving, humans may be expected
    to supervise automated decisions in real time. This configuration places cognitive
    and temporal demands on the human operator and assumes that intervention will
    occur quickly and reliably when needed. In practice, however, continuous human
    supervision is often impractical or ineffective, particularly when the operator
    must monitor multiple systems or lacks clear criteria for intervention.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机制必须针对部署环境量身定制。在高风险领域，如医疗分级或自动驾驶，人们可能期望人类实时监督自动化决策。这种配置对人类操作员提出了认知和时间上的要求，并假设在需要时干预将迅速且可靠地发生。然而，在实践中，持续的人类监督往往是不可行或无效的，尤其是当操作员必须监控多个系统或缺乏明确的干预标准时。
- en: From a systems design perspective, supporting effective oversight requires more
    than providing access to raw model outputs. Interfaces must be constructed to
    surface relevant information at the right time, in the right format, and with
    appropriate context. Confidence scores, uncertainty estimates, explanations, and
    change alerts can all play a role in enabling human oversight. Workflows must
    define when and how intervention is possible, who is authorized to override model
    outputs, and how such overrides are logged, audited, and incorporated into future
    system updates.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统设计角度来看，支持有效的监督需要不仅仅是提供对原始模型输出的访问。界面必须构建以在正确的时间、正确的格式和适当的上下文中呈现相关信息。置信度分数、不确定性估计、解释和变更警报都可以在使人类监督成为可能方面发挥作用。工作流程必须定义何时以及如何进行干预，谁有权覆盖模型输出，以及如何记录、审计和将这些覆盖纳入未来的系统更新。
- en: Consider a hospital triage system that uses a machine learning model to prioritize
    patients in the emergency department. The model generates a risk score for each
    incoming patient, which is presented alongside a suggested triage category. In
    principle, a human nurse is responsible for confirming or overriding the suggestion.
    However, if the model’s outputs are presented without sufficient justification,
    such as an explanation of the contributing features or the context for uncertainty,
    the nurse may defer to the model even in borderline cases. Over time, the models
    outputs may become the de facto triage decision, especially under time pressure.
    If a distribution shift occurs (for instance, due to a new illness or change in
    patient demographics), the nurse may lack both the situational awareness and the
    interface support needed to detect that the model is underperforming. In such
    cases, the appearance of human oversight masks a system in which responsibility
    has effectively shifted to the model without clear accountability or recourse.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个使用机器学习模型来优先处理急诊科患者的医院分级系统。该模型为每位进入的患者生成一个风险评分，并将其与建议的分级类别一起展示。原则上，护士负责确认或覆盖该建议。然而，如果模型的输出没有充分的理由，例如解释贡献特征或不确定性的背景，护士甚至可能在边缘情况下依赖模型。随着时间的推移，模型的输出可能成为事实上的分级决策，尤其是在时间压力下。如果发生分布变化（例如，由于新疾病或患者人口统计学的变化），护士可能缺乏必要的情境意识和界面支持，以检测模型表现不佳。在这种情况下，人类监督的表面现象掩盖了一个责任实际上已转移到模型而没有明确问责或追索权的系统。
- en: 'In such systems, human oversight is not merely a matter of policy declaration,
    but a function of infrastructure design: how predictions are surfaced, what information
    is retained, how intervention is enacted, and how feedback loops connect human
    decisions to system updates. Without integration across these components, oversight
    becomes fragmented, and responsibility may shift invisibly from human to machine.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的系统中，人类监督不仅仅是政策声明的问题，而是基础设施设计的问题：如何呈现预测，保留什么信息，如何实施干预，以及如何将反馈循环连接到系统更新。如果没有这些组件之间的整合，监督就会变得支离破碎，责任可能从人类无形地转移到机器。
- en: The boundary between decision support and automation is often fluid. Systems
    initially designed to assist human decision-makers may gradually assume greater
    autonomy as trust increases or organizational incentives shift. This transition
    can occur without explicit policy changes, resulting in de facto automation without
    appropriate accountability structures. Responsible system design must therefore
    anticipate changes in use over time and ensure that appropriate checks remain
    in place even as reliance on automation grows.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 决策支持与自动化之间的界限通常很模糊。最初设计用于协助人类决策者的系统可能会随着信任的增加或组织激励的变化而逐渐获得更大的自主性。这种转变可能在没有明确政策变化的情况下发生，导致事实上自动化，但没有适当的问责结构。因此，负责任的设计必须预见使用随时间的变化，并确保即使在依赖自动化增长的情况下，适当的检查仍然存在。
- en: Human-AI collaboration requires careful integration of model capabilities, interface
    design, operational policy, and institutional oversight. Collaboration is not
    simply a matter of inserting a “human-in-the-loop”; it is a systems challenge
    that spans technical, organizational, and ethical dimensions. Designing for oversight
    entails embedding mechanisms that allow intervention, support informed trust,
    and support shared responsibility between human operators and machine learning
    systems.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 人机协作需要仔细整合模型能力、界面设计、运营政策和制度监督。协作不仅仅是插入“人机结合”；这是一个跨越技术、组织和伦理维度的系统挑战。为监督而设计意味着嵌入允许干预、支持有信息信任和支持人类操作员与机器学习系统之间共享责任的机制。
- en: The complexity of human-AI collaboration is further compounded by the reality
    that different stakeholders often hold conflicting values and priorities.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 人类-人工智能合作的复杂性进一步加剧，因为不同的利益相关者往往持有相互冲突的价值观和优先事项。
- en: Normative Pluralism and Value Conflicts
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范多元主义和价值冲突
- en: '**Philosophical Content**'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**哲学内容**'
- en: 'This section examines competing value systems and their implications for ML
    design—a departure from primarily technical content. The key insight: technical
    excellence is necessary but insufficient for trustworthy AI because stakeholders
    hold legitimately different conceptions of fairness, privacy, and accountability
    that cannot be reconciled through better algorithms. Understanding these value
    tensions is essential for navigating design decisions that affect people’s lives.
    This perspective complements, rather than replaces, technical skills.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了相互竞争的价值体系和它们对机器学习设计的影响——这偏离了主要的技术内容。关键见解：技术卓越对于可信的人工智能是必要的，但不足以保证，因为利益相关者持有关于公平、隐私和问责的不同合法概念，这些概念不能通过更好的算法来调和。理解这些价值紧张关系对于导航影响人们生活的设计决策至关重要。这种观点补充，而不是取代，技术技能。
- en: Responsible machine learning cannot be reduced to the optimization of a single
    objective. In real-world settings, machine learning systems are deployed into
    environments shaped by diverse, and often conflicting, human values.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习不能仅仅归结为单一目标的优化。在现实世界的环境中，机器学习系统被部署到由多种、通常相互冲突的人类价值观塑造的环境中。
- en: 'Consider a team building a mental health chatbot for adolescents that uses
    ML to detect crisis situations and recommend interventions. The system must balance
    multiple legitimate but incompatible objectives:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个团队正在构建一个用于青少年的心理健康聊天机器人，该机器人使用机器学习来检测危机情况并推荐干预措施。该系统必须在多个合法但不兼容的目标之间取得平衡：
- en: '**Medical Efficacy**: Optimize for best clinical outcomes based on evidence-based
    practices. This suggests aggressive intervention—alerting parents, counselors,
    or emergency services whenever the model detects potential self-harm risk, even
    with low confidence, because false negatives could be fatal.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**医疗功效**：基于循证实践优化最佳临床结果。这意味着采取积极的干预措施——当模型检测到潜在的自伤风险时，即使信心较低，也要提醒父母、顾问或紧急服务，因为假阴性可能导致致命后果。'
- en: '**Patient Autonomy**: Respect adolescent privacy and agency. Many teenagers
    seek mental health support specifically because they cannot talk to parents or
    authority figures. Aggressive notification policies may deter vulnerable teens
    from using the system at all, leaving them without any support.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**患者自主权**：尊重青少年的隐私和自主权。许多青少年寻求心理健康支持，正是因为他们无法与父母或权威人士交谈。激进的通报政策可能会阻止脆弱的青少年使用该系统，使他们得不到任何支持。'
- en: '**Privacy Protection**: Minimize data collection and retention to protect sensitive
    mental health information. This suggests local processing, no conversation logging,
    and no sharing with third parties—but also prevents the system from improving
    through learning from interactions or enabling human review when the model is
    uncertain.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私保护**：最小化数据收集和保留，以保护敏感的心理健康信息。这表明本地处理、不记录对话、不与第三方共享——但也阻止了系统通过学习互动来改进，或当模型不确定时无法启用人工审查。'
- en: '**Resource Efficiency**: Operate within computational and human oversight budgets.
    Involving human counselors for every flagged interaction provides better care
    but is prohibitively expensive at scale. Fully automated responses reduce costs
    but may provide inappropriate guidance in complex situations.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源效率**：在计算和人工监督预算内运行。对于每个标记的互动都涉及人类顾问可以提供更好的护理，但在规模上成本过高。完全自动化的响应可以降低成本，但在复杂情况下可能会提供不适当的指导。'
- en: '**Legal Compliance**: Meet mandatory reporting requirements and liability standards.
    In many jurisdictions, systems that detect imminent harm must notify authorities—overriding
    patient autonomy and privacy regardless of clinical judgment about whether notification
    helps or harms the patient.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**法律合规性**：满足强制报告要求和责任标准。在许多司法管辖区，检测到即将到来的伤害的系统必须通知当局——无论临床判断通知是否有助于或伤害患者，都应优先考虑患者自主权和隐私。'
- en: These values are not poorly specified requirements that can be reconciled through
    better engineering. They reflect fundamentally different conceptions of what the
    system should achieve and whom it should prioritize. Optimizing for medical efficacy
    (aggressive intervention) directly conflicts with patient autonomy (minimal intervention).
    Privacy protection (no data retention) conflicts with resource efficiency (learning
    from interactions). Legal compliance (mandatory reporting) may conflict with clinical
    efficacy (therapeutic relationship based on trust).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这些价值观不是定义不明确的、可以通过更好的工程来调和的要求。它们反映了关于系统应实现什么以及应优先考虑谁的根本不同的概念。优化医疗效果（积极干预）与患者自主权（最小干预）直接冲突。隐私保护（不保留数据）与资源效率（从互动中学习）冲突。法律合规性（强制报告）可能与临床效果（基于信任的治疗关系）冲突。
- en: '**No algorithm determines which value should dominate.** Different stakeholders
    hold legitimately different positions: clinicians may prioritize efficacy, teenagers
    may prioritize autonomy, lawyers may prioritize compliance, and budget officers
    may prioritize efficiency. The technical team must facilitate stakeholder deliberation
    to determine which trade-offs are acceptable in this specific context—a fundamentally
    normative decision that precedes and constrains technical optimization.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '**没有算法决定哪个价值应该占主导地位**。不同的利益相关者持有合法的不同立场：临床医生可能优先考虑效果，青少年可能优先考虑自主权，律师可能优先考虑合规性，预算官员可能优先考虑效率。技术团队必须促进利益相关者的审议，以确定在特定背景下哪些权衡是可以接受的——这是一个根本性的规范性决策，它先于并限制了技术优化。'
- en: What constitutes a fair outcome for one stakeholder may be perceived as inequitable
    by another. Similarly, decisions that prioritize accuracy or efficiency may conflict
    with goals such as transparency, individual autonomy, or harm reduction. These
    tensions are not incidental—they are structural. They reflect the pluralistic
    nature of the societies in which machine learning systems are embedded and the
    institutional settings in which they are deployed.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个利益相关者来说，什么是公平的结果，可能被另一个利益相关者视为不公平。同样，优先考虑准确度或效率的决定可能与透明度、个人自主权或减少伤害等目标相冲突。这些紧张关系并非偶然——它们是结构性的。它们反映了机器学习系统嵌入的社会的多元性以及它们部署的机构环境。
- en: Fairness is a particularly prominent site of value conflict. Fairness can be
    formalized in multiple, often incompatible ways. A model that satisfies demographic
    parity may violate equalized odds; a model that prioritizes individual fairness
    may undermine group-level parity. Choosing among these definitions is not purely
    a technical decision but a normative one, informed by domain context, historical
    patterns of discrimination, and the perspectives of those affected by model outcomes.
    In practice, multiple stakeholders, including engineers, users, auditors, and
    regulators, may hold conflicting views on which definitions are most appropriate
    and why.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性是价值冲突特别突出的领域。公平性可以通过多种方式形式化，通常是不兼容的。一个满足人口比例的模型可能会违反平等机会；一个优先考虑个体公平性的模型可能会破坏组级平等。在这些定义之间进行选择不仅仅是技术决策，而是规范性决策，受领域背景、歧视的历史模式以及受模型结果影响的人的视角所指导。在实践中，包括工程师、用户、审计员和监管机构在内的多个利益相关者可能对哪些定义最合适以及为什么持有相互冲突的观点。
- en: These tensions are not confined to fairness alone. Conflicts also arise between
    interpretability and predictive performance, privacy and personalization, or short-term
    utility and long-term consequences. These tradeoffs manifest differently depending
    on the systems deployment architecture, revealing how deeply value conflicts are
    tied to the design and operation of ML systems.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 这些紧张关系并不仅限于公平性。在可解释性和预测性能、隐私和个人化、或短期效用和长期后果之间也出现冲突。这些权衡在不同的系统部署架构中表现不同，揭示了价值冲突如何与ML系统的设计和操作紧密相连。
- en: 'Consider a voice-based assistant deployed on a mobile device. To enhance personalization,
    the system may learn user preferences locally, without sending raw data to the
    cloud. This design improves privacy and reduces latency, but it may also lead
    to performance disparities if users with underrepresented usage patterns receive
    less accurate or responsive predictions. One way to improve fairness would be
    to centralize updates using group-level statistics—but doing so introduces new
    privacy risks and may violate user expectations around local data handling. Here,
    the design must navigate among valid but competing values: privacy, fairness,
    and personalization.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在移动设备上部署的基于语音的助手。为了提高个性化，系统可能在本地学习用户偏好，而不将原始数据发送到云端。这种设计提高了隐私性并减少了延迟，但可能会导致使用模式代表性不足的用户收到不准确或反应迟钝的预测。提高公平性的方法之一是使用组级统计信息集中更新——但这样做会引入新的隐私风险，并可能违反用户对本地数据处理方面的期望。在这里，设计必须在有效但相互竞争的价值之间进行权衡：隐私、公平性和个性化。
- en: In cloud-based deployments, such as credit scoring platforms or recommendation
    engines, tensions often arise between transparency and proprietary protection.
    End users or regulators may demand clear explanations of why a decision was made,
    particularly in situations with significant consequences, but the models in use
    may rely on complex ensembles or proprietary training data. Revealing these internals
    may be commercially sensitive or technically infeasible. In such cases, the system
    must reconcile competing pressures for institutional accountability and business
    confidentiality.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于云的部署中，例如信用评分平台或推荐引擎，透明度和专有保护之间常常出现紧张关系。最终用户或监管机构可能要求对决策原因进行明确解释，尤其是在具有重大后果的情况下，但使用的模型可能依赖于复杂的集成或专有训练数据。透露这些内部信息可能具有商业敏感性或技术上不可行。在这种情况下，系统必须协调机构问责制和商业机密之间的竞争压力。
- en: In edge systems, such as home security cameras or autonomous drones, resource
    constraints often dictate model selection and update frequency. Prioritizing low
    latency and energy efficiency may require deploying compressed or quantized models
    that are less robust to distribution shift or adversarial perturbations. More
    resilient models could improve safety, but they may exceed the systems memory
    budget or violate power constraints. Here, safety, efficiency, and maintainability
    must be balanced under hardware-imposed tradeoffs. Efficiency techniques and optimization
    methods are essential for implementing responsible AI in resource-constrained
    environments.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘系统中，例如家庭安全摄像头或自主无人机，资源限制通常决定了模型选择和更新频率。优先考虑低延迟和能源效率可能需要部署压缩或量化的模型，这些模型对分布变化或对抗性扰动的鲁棒性较低。更健壮的模型可以提高安全性，但它们可能超出系统内存预算或违反功率限制。在这里，必须在硬件强加的权衡下平衡安全性、效率和可维护性。在资源受限的环境中实施负责任的AI，效率技术和优化方法是必不可少的。
- en: On TinyML platforms, where models are deployed to microcontrollers with no persistent
    connectivity, tradeoffs are even more pronounced. A system may be optimized for
    static performance on a fixed dataset, but unable to incorporate new fairness
    constraints, retrain on updated inputs, or generate explanations once deployed.
    Hardware constraints fundamentally shape what responsible AI practices are feasible
    on resource-limited devices. The value conflict lies not just in what the model
    optimizes, but in what the system is able to support post-deployment.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在TinyML平台上，由于模型被部署到没有持久连接的微控制器上，权衡变得更加明显。一个系统可能在固定数据集上的静态性能得到优化，但无法纳入新的公平约束、在更新的输入上进行重新训练或一旦部署就生成解释。硬件约束从根本上塑造了在资源受限设备上可行的负责任AI实践。价值冲突不仅在于模型优化了什么，还在于系统在部署后能够支持什么。
- en: These examples make clear that normative pluralism is not an abstract philosophical
    challenge; it is a recurring systems constraint. Technical approaches such as
    multi-objective optimization, constrained training, and fairness-aware evaluation
    can help surface and formalize tradeoffs, but they do not eliminate the need for
    judgment. Decisions about whose values to represent, which harms to mitigate,
    and how to balance competing objectives cannot be made algorithmically. They require
    deliberation, stakeholder input, and governance structures that extend beyond
    the model itself.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子清楚地表明，规范性多元主义不是一个抽象的哲学挑战；它是一个反复出现的系统约束。诸如多目标优化、约束训练和公平感知评估等技术方法可以帮助揭示和形式化权衡，但它们并不能消除对判断的需求。关于代表哪些价值观、减轻哪些伤害以及如何平衡相互冲突的目标的决定，不能通过算法来做出。这些决策需要深思熟虑、利益相关者的输入以及超越模型本身的治理结构。
- en: Participatory and value-sensitive design methodologies offer potential paths
    forward. Rather than treating values as parameters to be optimized after deployment,
    these approaches seek to engage stakeholders during the requirements phase, define
    ethical tradeoffs explicitly, and trace how they are instantiated in system architecture.
    While no design process can satisfy all values simultaneously, systems that are
    transparent about their tradeoffs and open to revision are better positioned to
    sustain trust and accountability over time.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 参与式和价值观敏感的设计方法提供了前进的潜在途径。这些方法不是将价值观视为部署后要优化的参数，而是在需求阶段就寻求与利益相关者互动，明确定义伦理权衡，并追踪它们如何在系统架构中实现。虽然没有任何设计过程能够同时满足所有价值观，但那些对其权衡透明并开放修订的系统更有可能随着时间的推移维持信任和问责制。
- en: Machine learning systems are not neutral tools. They embed and enact value judgments,
    whether explicitly specified or implicitly assumed. A commitment to responsible
    AI requires acknowledging this fact and building systems that reflect and respond
    to the ethical and social pluralism of their operational contexts.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统不是中立的工具。它们嵌入并实施价值判断，无论是明确指定的还是隐含假设的。对负责任的AI的承诺需要承认这一事实，并构建能够反映和响应其操作环境中的伦理和社会多元主义的系统。
- en: Addressing these value conflicts requires more than technical solutions—it demands
    transparency and mechanisms for contestability that allow stakeholders to understand
    and challenge system decisions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些价值冲突需要不仅仅是技术解决方案——它需要透明度和可争议性的机制，允许利益相关者理解和挑战系统决策。
- en: Transparency and Contestability
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 透明度和可争议性
- en: Transparency is widely recognized as a foundational principle of responsible
    machine learning. It allows users, developers, auditors, and regulators to understand
    how a system functions, assess its limitations, and identify sources of harm.
    Yet transparency alone is not sufficient. In high-stakes domains, individuals
    and institutions must not only understand system behavior—they must also be able
    to challenge, correct, or reverse it when necessary. This capacity for contestability,
    which refers to the ability to interrogate and contest a system’s decisions, is
    a important feature of accountability.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度被广泛认为是负责任机器学习的基础原则。它使用户、开发者、审计员和监管机构能够理解系统的工作方式，评估其局限性，并确定伤害的来源。然而，透明度本身并不足够。在高风险领域，个人和机构不仅需要理解系统行为，还必须在必要时能够挑战、纠正或逆转它。这种可争议性，即质疑和挑战系统决策的能力，是问责制的一个重要特征。
- en: 'Transparency in machine learning systems typically focuses on disclosure: revealing
    how models are trained, what data they rely on, what assumptions are embedded
    in their design, and what known limitations affect their use. Documentation tools
    such as model cards and datasheets for datasets support this goal by formalizing
    system metadata in a structured, reproducible format. These resources can improve
    governance, support compliance, and inform user expectations. However, transparency
    as disclosure does not guarantee meaningful control. Even when technical details
    are available, users may lack the institutional use, interface tools, or procedural
    access to contest a decision that adversely affects them.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的透明度通常关注于披露：揭示模型是如何训练的，它们依赖哪些数据，它们设计中嵌入的假设是什么，以及哪些已知的限制影响了它们的使用。模型卡片和数据集的数据表等文档工具通过将系统元数据正式化在结构化、可重复的格式中，支持这一目标。这些资源可以提高治理水平，支持合规性，并告知用户期望。然而，作为披露的透明度并不能保证有意义的控制。即使技术细节可用，用户可能缺乏机构的使用、界面工具或程序性访问来质疑对他们产生不利影响的决策。
- en: To move from transparency to contestability, machine learning systems must be
    designed with mechanisms for explanation, recourse, and feedback. Explanation
    refers to the capacity of the system to provide understandable reasons for its
    outputs, tailored to the needs and context of the person receiving them. Recourse
    refers to the ability of individuals to alter their circumstances and receive
    a different outcome. Feedback refers to the ability of users to report errors,
    dispute outcomes, or signal concerns—and to have those signals incorporated into
    system updates or oversight processes.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 要从透明度过渡到可质疑性，机器学习系统必须设计有解释、救济和反馈的机制。解释指的是系统提供可理解的理由以支持其输出的能力，这些理由针对接收者的需求和情境量身定制。救济指的是个人改变其情况并获得不同结果的能力。反馈指的是用户报告错误、质疑结果或发出关注，并且这些信号被纳入系统更新或监督流程的能力。
- en: These mechanisms are often lacking in practice, particularly in systems deployed
    at scale or embedded in low-resource devices. For example, in mobile loan application
    systems, users may receive a rejection without explanation and have no opportunity
    to provide additional information or appeal the decision. The lack of transparency
    at the interface level, even if documentation exists elsewhere, makes the system
    effectively unchallengeable. Similarly, a predictive model deployed in a clinical
    setting may generate a risk score that guides treatment decisions without surfacing
    the underlying reasoning to the physician. If the model underperforms for a specific
    patient subgroup, and this behavior is not observable or contestable, the result
    may be unintentional harm that cannot be easily diagnosed or corrected.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制在实践中往往缺乏，尤其是在大规模部署或嵌入低资源设备中的系统。例如，在移动贷款申请系统中，用户可能会收到没有解释的拒绝，并且没有机会提供额外信息或上诉决策。即使其他地方存在文档，界面级别的透明度不足使得系统实际上无法被质疑。同样，在临床环境中部署的预测模型可能会生成一个指导治疗决策的风险评分，而不向医生展示其背后的推理。如果模型对特定患者群体表现不佳，而这种行为不可观察或不可质疑，结果可能是无法轻易诊断或纠正的无意伤害。
- en: From a systems perspective, enabling contestability requires coordination across
    technical and institutional components. Models must expose sufficient information
    to support explanation. Interfaces must surface this information in a usable and
    timely way. Organizational processes must be in place to review feedback, respond
    to appeals, and update system behavior. Logging and auditing infrastructure must
    track not only model outputs, but user interventions and override decisions. In
    some cases, technical safeguards, including human-in-the-loop overrides and decision
    abstention thresholds, may also serve contestability by ensuring that ambiguous
    or high-risk decisions defer to human judgment.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，实现可质疑性需要跨技术和管理组件的协调。模型必须提供足够的信息以支持解释。界面必须以可用和及时的方式展示这些信息。必须建立组织流程来审查反馈、回应上诉和更新系统行为。日志和审计基础设施必须跟踪不仅模型输出，还包括用户干预和覆盖决策。在某些情况下，包括人工干预覆盖和决策放弃阈值在内的一些技术保障措施，也可能通过确保模糊或高风险决策推迟到人类判断来服务于可质疑性。
- en: The degree of contestability that is feasible varies by deployment context.
    In centralized cloud platforms, it may be possible to offer full explanation APIs,
    user dashboards, and appeal workflows. In contrast, in edge and TinyML deployments,
    contestability may be limited to logging and periodic updates based on batch-synchronized
    feedback. In all cases, the design of machine learning systems must acknowledge
    that transparency is not simply a matter of technical disclosure. It is a structural
    property of systems that determines whether users and institutions can meaningfully
    question, correct, and govern the behavior of automated decision-making.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 可行的可竞争程度因部署环境而异。在集中式云平台上，可能可以提供完整的解释API、用户仪表板和申诉工作流程。相比之下，在边缘和TinyML部署中，可竞争性可能仅限于基于批量同步反馈的日志记录和定期更新。在所有情况下，机器学习系统设计都必须承认透明性不仅仅是技术披露的问题。它是系统的一个结构属性，决定了用户和机构是否能够有意义地质疑、纠正和治理自动化决策的行为。
- en: Implementing effective transparency and contestability mechanisms requires institutional
    support and governance structures that extend beyond individual technical teams.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 实施有效的透明度和可竞争性机制需要机构支持和治理结构，这些结构超越了单个技术团队。
- en: Institutional Embedding of Responsibility
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 责任的机构嵌入
- en: Machine learning systems do not operate in isolation. Their development, deployment,
    and ongoing management are embedded within institutional environments that include
    technical teams, legal departments, product owners, compliance officers, and external
    stakeholders. Responsibility in such systems is not the property of a single actor
    or component—it is distributed across roles, workflows, and governance processes.
    Designing for responsible AI therefore requires attention to the institutional
    settings in which these systems are built and used.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统并非独立运作。它们的发展、部署和持续管理嵌入在包括技术团队、法律部门、产品所有者、合规官员和外部利益相关者的机构环境中。在这样的系统中，责任不是单个行为者或组件的属性——它是分布在不同角色、工作流程和治理过程中的。因此，设计负责任的AI需要关注这些系统构建和使用的机构环境。
- en: This distributed nature of responsibility introduces both opportunities and
    challenges. On the one hand, the involvement of multiple stakeholders provides
    checks and balances that can help prevent harmful outcomes. On the other hand,
    the diffusion of responsibility can lead to accountability gaps, where no individual
    or team has clear authority or incentive to intervene when problems arise. When
    harm occurs, it may be unclear whether the fault lies with the data pipeline,
    the model architecture, the deployment configuration, the user interface, or the
    surrounding organizational context.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这种责任分布的特性既带来了机遇也带来了挑战。一方面，多个利益相关者的参与提供了制衡，有助于防止有害结果。另一方面，责任的扩散可能导致问责制差距，当出现问题时，可能不清楚问题出在数据管道、模型架构、部署配置、用户界面或周围的组织环境中。
- en: One illustrative case is Google Flu Trends, a widely cited example of failure
    due to institutional misalignment. The system, which attempted to predict flu
    outbreaks from search data, initially performed well but gradually diverged from
    reality due to changes in user behavior and shifts in the data distribution. These
    issues went uncorrected for years, in part because there were no established processes
    for system validation, external auditing, or escalation when model performance
    declined. The failure was not due to a single technical flaw, but to the absence
    of an institutional framework that could respond to drift, uncertainty, and feedback
    from outside the development team.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的案例是谷歌流感趋势，这是由于机构不匹配而广泛引用的失败例子。该系统试图从搜索数据中预测流感爆发，最初表现良好，但由于用户行为的变化和数据分布的转移，逐渐与现实脱节。这些问题多年未得到纠正，部分原因是因为没有建立系统验证、外部审计或模型性能下降时的升级流程。失败并非由于单个技术缺陷，而是由于缺乏能够应对漂移、不确定性和来自开发团队外部的反馈的机构框架。
- en: Embedding responsibility institutionally requires more than assigning accountability.
    It requires the design of processes, tools, and incentives that allow responsible
    action. Technical infrastructure such as versioned model registries, model cards,
    and audit logs must be coupled with organizational structures such as ethics review
    boards, model risk committees, and red-teaming procedures. These mechanisms ensure
    that technical insights are actionable, that feedback is integrated across teams,
    and that concerns raised by users, developers, or regulators are addressed systematically
    rather than ad hoc.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在机构中嵌入责任需要不仅仅是分配问责制。它需要设计流程、工具和激励措施，以允许负责任的行为。如版本化模型注册表、模型卡片和审计日志等技术基础设施必须与如伦理审查委员会、模型风险委员会和红队程序等组织结构相结合。这些机制确保技术见解是可操作的，反馈在团队间得到整合，并且用户、开发者或监管机构提出的问题得到系统性的解决，而不是临时性的。
- en: The level of institutional support required varies across deployment contexts.
    In large-scale cloud platforms, governance structures may include internal accountability
    audits, compliance workflows, and dedicated teams responsible for monitoring system
    behavior. In smaller-scale deployments, including edge or mobile systems embedded
    in healthcare devices or public infrastructure, governance may rely on cross-functional
    engineering practices and external certification or regulation. In TinyML deployments,
    where connectivity and observability are limited, institutional responsibility
    may be exercised through upstream controls such as safety-important validation,
    embedded security constraints, and lifecycle tracking of deployed firmware.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的机构支持水平因部署环境而异。在大规模云平台上，治理结构可能包括内部问责审计、合规工作流程以及负责监控系统行为的专门团队。在较小规模的部署中，包括嵌入医疗设备或公共基础设施中的边缘或移动系统，治理可能依赖于跨职能的工程实践和外部认证或监管。在TinyML部署中，由于连接性和可观察性有限，机构责任可能通过上游控制来行使，例如安全重要的验证、嵌入式安全约束和部署固件的整个生命周期跟踪。
- en: In all cases, responsible machine learning requires coordination between technical
    and institutional systems. This coordination must extend across the entire model
    lifecycle—from initial data acquisition and model training to deployment, monitoring,
    update, and eventual decommissioning. It must also incorporate external actors,
    including domain experts, civil society organizations, and regulatory authorities,
    to ensure that responsibility is exercised not only within the development team
    but across the broader ecosystem in which machine learning systems operate.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，负责任的机器学习都需要技术系统和机构系统之间的协调。这种协调必须扩展到整个模型生命周期——从最初的数据采集和模型训练到部署、监控、更新以及最终的退役。它还必须纳入外部参与者，包括领域专家、民间社会组织和监管机构，以确保责任不仅限于开发团队，而且在整个机器学习系统运行的更广泛生态系统中得到行使。
- en: Responsibility is not a static attribute of a model or a team; it is a dynamic
    property of how systems are governed, maintained, and contested over time. Embedding
    that responsibility within institutions, by means of policy, infrastructure, and
    accountability mechanisms, is important for aligning machine learning systems
    with the social values and operational realities they are meant to serve.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 责任不是一个模型或团队的静态属性；它是系统如何治理、维护和随时间争议的动态属性。通过政策、基础设施和问责机制将这种责任嵌入机构中，对于使机器学习系统与它们旨在服务的社交价值和运营现实相一致至关重要。
- en: These considerations of institutional responsibility and value conflicts highlight
    that responsible AI implementation extends beyond technical solutions to encompass
    broader questions of access, participation, and environmental impact. The computational
    resource requirements explored in the previous section create systemic barriers
    that determine who can develop, deploy, and benefit from responsible AI capabilities—transforming
    responsible AI from an individual system property into a collective social challenge.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关于机构责任和价值冲突的考虑表明，负责任的AI实施不仅超越了技术解决方案，还包括更广泛的关于访问、参与和环境影响的议题。前一小节中探讨的计算资源需求创造了系统性障碍，决定了谁可以开发、部署并从负责任的AI能力中受益——将负责任的AI从个体系统属性转变为集体社会挑战。
- en: The sociotechnical considerations explored in this section—system feedback loops
    that create self-reinforcing disparities, human-AI collaboration challenges like
    automation bias and algorithm aversion, normative pluralism across stakeholder
    values, and computational equity gaps—reveal why the technical foundations from
    [Section 17.5](ch023.xhtml#sec-responsible-ai-technical-foundations-3436) alone
    cannot ensure responsible AI. These dynamics operate at the intersection of algorithms,
    humans, organizations, and society, where static fairness metrics prove insufficient
    and competing values cannot be reconciled algorithmically. Yet even with clear
    principles and sound technical methods, translating responsible AI into operational
    practice faces substantial implementation challenges.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了本节中探讨的社会技术考虑因素——创建自我强化的差异的系统反馈循环、自动化偏见和算法厌恶等人类-人工智能协作挑战、利益相关者价值观的规范性多元主义以及计算公平性差距——揭示了为什么仅凭[第17.5节](ch023.xhtml#sec-responsible-ai-technical-foundations-3436)中的技术基础无法确保负责任的人工智能。这些动态在算法、人类、组织和社会的交汇处运作，静态的公平性指标证明是不够的，而竞争性价值观无法通过算法调和。然而，即使在明确的原则和可靠的技术方法下，将负责任的人工智能转化为实际操作实践也面临着巨大的实施挑战。
- en: Implementation Challenges
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施挑战
- en: 'The technical foundations and sociotechnical dynamics examined above establish
    what responsible AI systems should achieve, but substantial barriers prevent these
    capabilities from operating effectively in practice. Consider how the methods
    explored earlier encounter organizational obstacles: bias detection algorithms
    like Fairlearn require ongoing data collection and monitoring infrastructure,
    but many organizations lack processes for acting on fairness metrics. Differential
    privacy mechanisms demand careful parameter tuning and performance monitoring,
    yet teams may lack expertise in privacy-utility tradeoffs. Explainability frameworks
    generate feature attribution scores, but without design systems that make explanations
    accessible to affected users, these technical capabilities provide no practical
    benefit.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 上述考察的技术基础和社会技术动态确立了负责任的人工智能系统应实现的目标，但实质性的障碍阻止了这些能力在实际操作中有效运行。考虑一下之前探讨的方法如何遇到组织障碍：例如，公平性检测算法如Fairlearn需要持续的数据收集和监控基础设施，但许多组织缺乏针对公平性指标的执行流程。差分隐私机制需要仔细的参数调整和性能监控，然而团队可能缺乏在隐私与效用权衡方面的专业知识。可解释性框架生成特征归因分数，但如果没有设计出使解释对受影响用户可访问的系统，这些技术能力无法提供实际效益。
- en: These examples illustrate a fundamental gap between technical capability and
    operational implementation. While responsible AI methods provide necessary tools,
    their effectiveness depends entirely on organizational structures, data infrastructure,
    evaluation processes, and sustained commitment that extends far beyond algorithm
    development. Understanding these implementation challenges is essential for building
    systems that maintain responsible behavior over time rather than achieving it
    only during initial deployment.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子说明了技术能力与实际操作实施之间的基本差距。虽然负责任的人工智能方法提供了必要的工具，但它们的有效性完全取决于组织结构、数据基础设施、评估流程以及超越算法开发的持续承诺。理解这些实施挑战对于构建能够在长时间内保持负责任行为的系统至关重要，而不仅仅是实现初始部署时的目标。
- en: This section examines the practical challenges that arise when embedding responsible
    AI practices into production ML systems using the classical People-Process-Technology
    framework that provides structure for analyzing implementation barriers systematically.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过使用提供分析实施障碍系统结构的经典人-流程-技术框架，探讨了将负责任的人工智能实践嵌入到生产机器学习系统中的实际挑战。
- en: '**People challenges** encompass organizational structures, role definitions,
    incentive alignment, and stakeholder coordination that determine whether responsible
    AI principles translate into sustained organizational behavior. **Process challenges**
    involve standardization gaps, lifecycle maintenance procedures, competing optimization
    objectives, and evaluation methodologies that affect how responsible AI practices
    integrate with development workflows. **Technology challenges** include data quality
    constraints, computational resource limitations, scalability bottlenecks, and
    infrastructure gaps that determine whether responsible AI techniques can operate
    effectively at production scale.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**人员挑战**包括组织结构、角色定义、激励一致性和利益相关者协调，这些因素决定了负责任的人工智能原则是否能够转化为持续的组织行为。**流程挑战**涉及标准化差距、生命周期维护程序、竞争优化目标和评估方法，这些因素影响负责任的人工智能实践如何与开发工作流程相结合。**技术挑战**包括数据质量限制、计算资源限制、可扩展性瓶颈和基础设施缺口，这些因素决定了负责任的人工智能技术是否能够在生产规模上有效运行。'
- en: Collectively, these challenges illustrate the friction between idealized principles
    and operational reality. Understanding their interconnections is essential for
    developing systems-level strategies that embed responsibility into the architecture,
    infrastructure, and workflows of machine learning deployment.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战共同展示了理想化原则与实际操作现实之间的摩擦。理解它们之间的相互联系对于开发将责任嵌入到机器学习部署的架构、基础设施和工作流程的系统级策略至关重要。
- en: The following analysis examines implementation barriers through three interconnected
    lenses, recognizing that effective responsible AI requires coordinated solutions
    addressing all three dimensions simultaneously.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 以下分析通过三个相互关联的视角来审视实施障碍，认识到有效的负责任人工智能需要协调解决方案，同时解决这三个维度。
- en: Organizational Structures and Incentives
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组织结构及激励机制
- en: The implementation of responsible machine learning is shaped not only by technical
    feasibility but by the organizational context in which systems are developed and
    deployed. Within companies, research labs, and public institutions, responsibility
    must be translated into concrete roles, workflows, and incentives. In practice,
    however, organizational structures often fragment responsibility, making it difficult
    to coordinate ethical objectives across engineering, product, legal, and operational
    teams.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任机器学习的实施不仅受技术可行性的影响，还受系统开发和部署的组织环境的影响。在公司、研究实验室和公共机构中，责任必须转化为具体的角色、工作流程和激励机制。然而，在实践中，组织结构往往分散责任，使得在工程、产品、法律和运营团队之间协调道德目标变得困难。
- en: Responsible AI requires sustained investment in practices such as subgroup performance
    evaluation, explainability analysis, adversarial robustness testing, and the integration
    of privacy-preserving techniques like differential privacy or federated training.
    These activities can be time-consuming and resource-intensive, yet they often
    fall outside the formal performance metrics used to evaluate team productivity.
    For example, teams may be incentivized to ship features quickly or meet performance
    benchmarks, even when doing so undermines fairness or overlooks potential harms.
    When ethical diligence is treated as a discretionary task, instead of being an
    integrated component of the system lifecycle, it becomes vulnerable to deprioritization
    under deadline pressure or organizational churn.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能需要持续投资于诸如子群性能评估、可解释性分析、对抗性鲁棒性测试以及集成差分隐私或联邦训练等隐私保护技术的实践。这些活动可能耗时且资源密集，但它们往往超出了用于评估团队生产力的正式绩效指标。例如，团队可能会被激励快速发布功能或达到性能基准，即使这样做会损害公平性或忽视潜在的危害。当道德尽职调查被视为一项可选择的任务，而不是系统生命周期的集成组成部分时，它就会在截止日期压力或组织动荡下变得容易受到优先级降低的影响。
- en: Responsibility is further complicated by ambiguity over ownership. In many organizations,
    no single team is responsible for ensuring that a system behaves ethically over
    time. Model performance may be owned by one team, user experience by another,
    data infrastructure by a third, and compliance by a fourth. When issues arise,
    including disparate impact in predictions or insufficient explanation quality,
    there may be no clear protocol for identifying root causes or coordinating mitigation.
    As a result, concerns raised by developers, users, or auditors may go unaddressed,
    not because of malicious intent, but due to lack of process and cross-functional
    alignment.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有权的不确定性，责任问题进一步复杂化。在许多组织中，没有单一团队负责确保系统在一段时间内表现出道德行为。模型性能可能由一个团队拥有，用户体验由另一个团队拥有，数据基础设施由第三个团队拥有，合规性由第四个团队拥有。当出现问题时，包括预测中的不同影响或解释质量不足，可能没有明确的协议来识别根本原因或协调缓解措施。因此，开发者、用户或审计员提出的问题可能得不到解决，这不是因为恶意意图，而是由于缺乏流程和跨职能协调。
- en: 'Establishing effective organizational structures for responsible AI requires
    more than policy declarations. It demands operational mechanisms: designated roles
    with responsibility for ethical oversight, clearly defined escalation pathways,
    accountability for post-deployment monitoring, and incentives that reward teams
    for ethical foresight and system maintainability. In some organizations, this
    may take the form of Responsible AI committees, cross-functional review boards,
    or model risk teams that work alongside developers throughout the model lifecycle.
    In others, domain experts or user advocates may be embedded into product teams
    to anticipate downstream impacts and evaluate value tradeoffs in context.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 建立有效的组织结构以负责人工智能不仅需要政策声明。它需要操作机制：指定具有道德监督责任的职位，明确定义的升级途径，对部署后监控的责任，以及奖励团队进行道德前瞻性和系统可维护性的激励措施。在一些组织中，这可能采取负责任的人工智能委员会、跨职能审查委员会或与开发者一起在整个模型生命周期中工作的模型风险团队的形式。在其他组织中，领域专家或用户倡导者可能被嵌入到产品团队中，以预测下游影响并评估情境中的价值权衡。
- en: As shown in [Figure 17.8](ch023.xhtml#fig-human-centered-ai), the responsibility
    for ethical system behavior is distributed across multiple constituencies, including
    industry, academia, civil society, and government. Within organizations, this
    distribution must be mirrored by mechanisms that connect technical design with
    strategic oversight and operational control. Without these linkages, responsibility
    becomes diffuse, and well-intentioned efforts may be undermined by systemic misalignment.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图17.8](ch023.xhtml#fig-human-centered-ai)所示，道德系统行为的责任分布在多个利益相关者之间，包括行业、学术界、民间社会和政府。在组织中，这种分布必须通过将技术设计与技术监督和操作控制相连接的机制来反映。没有这些联系，责任就会变得模糊，良好的意图可能会被系统性的不协调所破坏。
- en: '![](../media/file297.svg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file297.svg)'
- en: 'Figure 17.8: **Stakeholder Responsibility**: Effective human-centered AI implementation
    requires shared accountability across industry, academia, civil society, and government
    to address ethical considerations and systemic risks. These diverse groups shape
    technical design, strategic oversight, and operational control, ensuring responsible
    AI development and deployment throughout the model lifecycle. Source: ([Shneiderman
    2020](ch058.xhtml#ref-schneiderman2020)).'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8：**利益相关者责任**：有效的人本人工智能实施需要行业、学术界、民间社会和政府之间的共同责任，以解决道德考虑和系统性风险。这些不同的群体塑造技术设计、战略监督和操作控制，确保在整个模型生命周期中负责任的人工智能开发和部署。来源：([Shneiderman
    2020](ch058.xhtml#ref-schneiderman2020))。
- en: Responsible AI is not merely a question of technical excellence or regulatory
    compliance. It is a systems-level challenge that requires aligning ethical objectives
    with the institutional structures through which machine learning systems are designed,
    deployed, and maintained. Creating and sustaining these structures is important
    for ensuring that responsibility is embedded not only in the model, but in the
    organization that governs its use.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的人工智能不仅仅是技术卓越或监管合规的问题。这是一个系统级挑战，需要将道德目标与机器学习系统设计、部署和维护的机构结构相一致。创建和维持这些结构对于确保责任不仅嵌入到模型中，还嵌入到管理其使用的组织中至关重要。
- en: Beyond organizational challenges, teams face significant technical barriers
    related to data quality and availability.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 除去组织挑战之外，团队还面临着与数据质量和可用性相关的重大技术障碍。
- en: Data Constraints and Quality Gaps
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据限制和质量差距
- en: Improving data pipelines remains one of the most difficult implementation challenges
    in practice despite broad recognition that data quality is important for responsible
    machine learning. Developers and researchers often understand the importance of
    representative data, accurate labeling, and mitigation of historical bias. Yet
    even when intentions are clear, structural and organizational barriers frequently
    prevent meaningful intervention. Responsibility for data is often distributed
    across teams, governed by legacy systems, or embedded in broader institutional
    processes that are difficult to change.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管广泛认识到数据质量对于负责任的机器学习的重要性，但在实践中，提高数据管道仍然是实施中最困难的问题之一。开发人员和研究人员通常理解代表性数据、准确标注和历史偏差缓解的重要性。然而，即使意图明确，结构和组织障碍经常阻止有意义的干预。数据责任通常分散在各个团队之间，受制于遗留系统，或者嵌入在难以改变更广泛的机构流程中。
- en: 'The data engineering principles covered in [Chapter 6](ch012.xhtml#sec-data-engineering)—including
    data validation, schema management, versioning, lineage tracking, and quality
    monitoring—provide the technical foundation for addressing these challenges. However,
    applying these principles to responsible AI introduces additional complexity:
    fairness requires assessing representativeness across demographic groups, bias
    mitigation demands understanding historical data collection practices, and privacy
    preservation constrains which validation techniques are permissible. The organizational
    challenges described here reflect the gap between having robust data engineering
    infrastructure and using it effectively to support responsible AI objectives.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch012.xhtml#sec-data-engineering)中涵盖的数据工程原则——包括数据验证、模式管理、版本控制、血缘跟踪和质量监控——为解决这些挑战提供了技术基础。然而，将这些原则应用于负责任的AI引入了额外的复杂性：公平性需要评估人口统计群体中的代表性，偏差缓解需要理解历史数据收集实践，隐私保护则限制了哪些验证技术是允许的。这里描述的组织挑战反映了拥有强大的数据工程基础设施和使用它来有效支持负责任AI目标之间的差距。'
- en: Subgroup imbalance, label ambiguity, and distribution shift, each of which affect
    generalization and performance across domains, are well-established concerns in
    responsible ML. These issues often manifest in the form of poor calibration, out-of-distribution
    failures, or demographic disparities in evaluation metrics. However, addressing
    them in real-world settings requires more than technical knowledge. It requires
    access to relevant data, institutional support for remediation, and sufficient
    time and resources to iterate on the dataset itself. In many machine learning
    pipelines, once the data is collected and the training set defined, the data pipeline
    becomes effectively frozen. Teams may lack both the authority and the infrastructure
    to modify or extend the dataset midstream, even if performance disparities are
    discovered. Even in modern data pipelines with automated validation and feature
    stores, retroactively correcting training distributions remains difficult once
    dataset versioning and data lineage have been locked into production.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 子群不平衡、标签模糊性和分布偏移，这些问题都会影响跨领域的泛化能力和性能，在负责任的机器学习中是众所周知的问题。这些问题通常以校准不良、分布外失败或在评估指标中的人口统计差异的形式出现。然而，在现实环境中解决这些问题需要不仅仅是技术知识。它需要访问相关数据、机构对补救措施的支持，以及足够的时间和资源来迭代数据集本身。在许多机器学习流程中，一旦数据收集完成并定义了训练集，数据管道实际上就变得冻结了。团队可能既缺乏修改或扩展数据集的权威性，也缺乏基础设施，即使发现了性能差异也是如此。即使在具有自动化验证和特征存储的现代数据管道中，一旦数据集版本和数据血缘被锁定到生产中，事后纠正训练分布仍然困难。
- en: In domains like healthcare, education, and social services, these challenges
    are especially pronounced. Data acquisition may be subject to legal constraints,
    privacy regulations, or cross-organizational coordination. For example, a team
    developing a triage model may discover that their training data underrepresents
    patients from smaller or rural hospitals. Correcting this imbalance would require
    negotiating data access with external partners, aligning on feature standards,
    and resolving inconsistencies in labeling practices. The logistical and operational
    costs can be prohibitive even when all parties agree on the need for improvement.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健、教育和社会福利等领域，这些挑战尤为突出。数据获取可能受到法律约束、隐私法规或跨组织协调的限制。例如，一个开发分级模型的团队可能会发现他们的训练数据未能充分代表来自较小或农村医院的病人。纠正这种不平衡需要与外部合作伙伴协商数据访问、统一特征标准以及解决标签实践中的不一致性。即使所有各方都同意需要改进，这些后勤和运营成本也可能具有威慑力。
- en: Efforts to collect more representative data may also run into ethical and political
    concerns. In some cases, additional data collection could expose marginalized
    populations to new risks. This paradox of exposure, in which the individuals most
    harmed by exclusion are also those most vulnerable to misuse, complicates efforts
    to improve fairness through dataset expansion. For example, gathering more data
    on non-binary individuals to support fairness in gender-sensitive applications
    may improve model coverage, but it also raises serious concerns around consent,
    identifiability, and downstream use. Teams must navigate these tensions carefully,
    often without clear institutional guidance.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 收集更具代表性数据的努力也可能遇到伦理和政治方面的担忧。在某些情况下，额外的数据收集可能会使边缘化群体面临新的风险。这种暴露的悖论，即那些最被排除在外的人也是最容易受到滥用伤害的人，使得通过数据集扩展来提高公平性的努力复杂化。例如，为了支持性别敏感应用中的公平性，收集更多关于非二元个体的数据可能会提高模型覆盖范围，但也引发了关于同意、可识别性和下游使用的严重担忧。团队必须谨慎地处理这些紧张关系，通常在没有明确机构指导的情况下。
- en: Upstream biases in data collection systems can persist unchecked even when data
    is plentiful. Many organizations rely on third-party data vendors, external APIs,
    or operational databases that were not designed with fairness or interpretability
    in mind. For instance, Electronic Health Records, which are commonly used in clinical
    machine learning, often reflect systemic disparities in care, as well as documentation
    habits that encode racial or socioeconomic bias ([Himmelstein, Bates, and Zhou
    2022](ch058.xhtml#ref-himmelstein2022examination)). Teams working downstream may
    have little visibility into how these records were created, and few levers for
    addressing embedded harms.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据量充足，数据收集系统中的上游偏见也可能无法得到控制。许多组织依赖第三方数据供应商、外部API或运营数据库，这些数据库在设计时并未考虑公平性或可解释性。例如，在临床机器学习中常用的电子健康记录，通常反映了护理中的系统性差异，以及编码种族或社会经济偏见的记录习惯（[Himmelstein,
    Bates, and Zhou 2022](ch058.xhtml#ref-himmelstein2022examination)）。下游工作的团队可能对如何创建这些记录几乎没有任何了解，并且几乎没有解决嵌入的损害的杠杆。
- en: Improving dataset quality is often not the responsibility of any one team. Data
    pipelines may be maintained by infrastructure or analytics groups that operate
    independently of the ML engineering or model evaluation teams. This organizational
    fragmentation makes it difficult to coordinate data audits, track provenance,
    or implement feedback loops that connect model behavior to underlying data issues.
    In practice, responsibility for dataset quality tends to fall through the cracks—recognized
    as important, but rarely prioritized or resourced.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 提高数据集质量通常不是任何一团队的职责。数据管道可能由基础设施或分析团队维护，这些团队独立于机器学习工程或模型评估团队运作。这种组织上的碎片化使得协调数据审计、追踪数据来源或实施将模型行为与潜在数据问题相连接的反馈循环变得困难。在实践中，数据集质量的责任往往被忽视——虽然认识到其重要性，但很少被优先考虑或分配资源。
- en: Addressing these challenges requires long-term investment in infrastructure,
    workflows, and cross-functional communication. Technical tools such as data validation,
    automated audits, and dataset documentation frameworks (e.g., model cards, datasheets,
    or the [Data Nutrition Project](https://datanutrition.org/)) can help, but only
    when they are embedded within teams that have the mandate and support to act on
    their findings. Improving data quality is not just a matter of better tooling
    but a question of how responsibility for data is assigned, shared, and sustained
    across the system lifecycle.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战需要长期投资于基础设施、工作流程和跨职能沟通。技术工具，如数据验证、自动审计和数据集文档框架（例如，模型卡片、数据表或[数据营养项目](https://datanutrition.org/)）可以帮助，但只有当它们嵌入到拥有行动权限和支持的团队中时才能发挥作用。提高数据质量不仅仅是更好的工具问题，而是关于如何在整个系统生命周期中分配、共享和维持数据责任的问题。
- en: Even when data quality challenges are addressed, teams face additional complexity
    in balancing multiple competing objectives.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 即使解决了数据质量挑战，团队在平衡多个竞争目标时仍面临额外的复杂性。
- en: Balancing Competing Objectives
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平衡竞争目标
- en: Machine learning system design is often framed as a process of optimization—improving
    accuracy, reducing loss, or maximizing utility. Yet in responsible ML practice,
    optimization must be balanced against a range of competing objectives, including
    fairness, interpretability, robustness, privacy, and resource efficiency. These
    objectives are not always aligned, and improvements in one dimension may entail
    tradeoffs in another. While these tensions are well understood in theory, managing
    them in real-world systems is a persistent and unresolved challenge.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统设计通常被描述为一个优化过程——提高准确性、减少损失或最大化效用。然而，在负责任的机器学习实践中，优化必须与一系列竞争目标相平衡，包括公平性、可解释性、鲁棒性、隐私和资源效率。这些目标并不总是协调一致，一个维度的改进可能涉及另一个维度的权衡。虽然这些紧张关系在理论上得到了很好的理解，但在现实世界系统中管理它们是一个持续存在且未解决的挑战。
- en: Consider the tradeoff between model accuracy and interpretability. In many cases,
    more interpretable models, including shallow decision trees and linear models,
    achieve lower predictive performance than complex ensemble methods or deep neural
    networks. In low-stakes applications, this tradeoff may be acceptable, or even
    preferred. But in high-stakes domains such as healthcare or finance, where decisions
    affect individuals well-being or access to opportunity, teams are often caught
    between the demand for performance and the need for transparent reasoning. Even
    when interpretability is prioritized during development, it may be overridden
    at deployment in favor of marginal gains in model accuracy.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑模型准确性与可解释性之间的权衡。在许多情况下，包括浅层决策树和线性模型在内的更可解释的模型，其预测性能低于复杂的集成方法或深度神经网络。在低风险应用中，这种权衡可能是可接受的，甚至更受欢迎。但在高风险领域，如医疗保健或金融，决策会影响个人的福祉或机会获取，团队往往陷入对性能的需求和透明推理的需求之间。即使开发过程中优先考虑可解释性，在部署时也可能为了模型准确性的微小提升而牺牲可解释性。
- en: Similar tensions emerge between personalization and fairness. A recommendation
    system trained to maximize user engagement may personalize aggressively, using
    fine-grained behavioral data to tailor outputs to individual users. While this
    approach can improve satisfaction for some users, it may entrench disparities
    across demographic groups, particularly if personalization draws on features correlated
    with race, gender, or socioeconomic status. Adding fairness constraints may reduce
    disparities at the group level, but at the cost of reducing perceived personalization
    for some users. These effects are often difficult to measure, and even more difficult
    to explain to product teams under pressure to optimize engagement metrics.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在个性化与公平性之间也存在类似的紧张关系。一个旨在最大化用户参与度的推荐系统可能会采取激进的个性化策略，使用细粒度的行为数据来调整输出以适应单个用户。虽然这种方法可能提高某些用户的满意度，但它可能会加剧不同人口群体之间的差异，尤其是如果个性化依赖于与种族、性别或社会经济地位相关的特征。添加公平性约束可能会在群体层面上减少差异，但代价是降低某些用户感知到的个性化程度。这些影响往往难以衡量，而且在压力下优化参与度指标的产品团队中，解释这些影响更加困难。
- en: Privacy introduces another set of constraints. Techniques such as differential
    privacy, federated learning, or local data minimization can meaningfully reduce
    privacy risks. But they also introduce noise, limit model capacity, or reduce
    access to training data. In centralized systems, these costs may be absorbed through
    infrastructure scaling or hybrid training architectures. In edge or TinyML deployments,
    however, the tradeoffs are more acute. A wearable device tasked with local inference
    must often balance model complexity, energy consumption, latency, and privacy
    guarantees simultaneously. Supporting one constraint typically weakens another,
    forcing system designers to prioritize among equally important goals. These tensions
    are further amplified by deployment-specific design decisions such as quantization
    levels, activation clipping, or compression strategies that affect how effectively
    models can support multiple objectives at once.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私引入了另一组约束。诸如差分隐私、联邦学习或本地数据最小化等技术可以有意义地降低隐私风险。但它们也引入了噪声，限制了模型容量，或减少了训练数据的访问。在集中式系统中，这些成本可能通过基础设施扩展或混合训练架构来吸收。然而，在边缘或TinyML部署中，这些权衡更为尖锐。一个负责本地推理的可穿戴设备必须经常在模型复杂性、能耗、延迟和隐私保证之间进行权衡。支持一个约束通常会削弱另一个，迫使系统设计者在同等重要的目标之间进行优先级排序。这些紧张关系通过特定部署的设计决策进一步放大，例如量化级别、激活裁剪或压缩策略，这些都影响模型同时支持多个目标的有效性。
- en: These tradeoffs are not purely technical—they reflect deeper normative judgments
    about what a system is designed to achieve and for whom, as explored in detail
    in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f).
    Responsible ML development requires making these judgments explicit, evaluating
    them in context, and subjecting them to stakeholder input and institutional oversight.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权衡并非纯粹的技术问题——它们反映了关于系统设计目标及其面向对象的更深层次的规范性判断，这在[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中已有详细探讨。负责任的机器学习开发需要明确这些判断，在特定情境中评估它们，并使它们接受利益相关者和机构的监督。
- en: What makes this challenge particularly difficult in implementation is that these
    competing objectives are rarely owned by a single team or function. Performance
    may be optimized by the modeling team, fairness monitored by a responsible AI
    group, and privacy handled by legal or compliance departments. Without deliberate
    coordination, system-level tradeoffs can be made implicitly, piecemeal, or without
    visibility into long-term consequences. Over time, the result may be a model that
    appears well-behaved in isolation but fails to meet its ethical goals when embedded
    in production infrastructure.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 使这一挑战在实施中尤其困难的是，这些相互竞争的目标很少由单一团队或功能拥有。性能可能由建模团队优化，公平性由负责任的AI团队监控，隐私由法律或合规部门处理。没有明确的协调，系统级别的权衡可能被隐性地、零散地做出，或者没有对长期后果的可见性。随着时间的推移，结果可能是一个在孤立状态下表现良好的模型，但嵌入到生产基础设施中时却未能达到其伦理目标。
- en: Balancing competing objectives requires not only technical fluency but a commitment
    to transparency, deliberation, and alignment across teams. Systems must be designed
    to surface tradeoffs rather than obscure them, to make room for constraint-aware
    development rather than pursue narrow optimization. In practice, this may require
    redefining what “success” looks like—not as performance on a single metric, but
    as sustained alignment between system behavior and its intended role in a broader
    social or operational context.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡相互竞争的目标不仅需要技术熟练，还需要对透明度、深思熟虑和团队间的协调一致做出承诺。系统必须设计成揭示权衡而不是掩盖它们，为约束感知的开发留出空间，而不是追求狭隘的优化。在实践中，这可能需要重新定义“成功”的含义——不是指单一指标上的表现，而是指系统行为与其在更广泛的社会或运营环境中的预期角色之间的持续一致。
- en: 'Across these first three challenges—organizational structures, data quality,
    and competing objectives—a pattern emerges: responsible AI failure rarely stems
    from technical ignorance. Teams understand fairness metrics, privacy techniques,
    and bias mitigation methods. Instead, failure occurs at the intersection of organizational
    fragmentation that distributes responsibility without accountability, data constraints
    that create technical barriers even with clear intentions, and competing objectives
    that force normative tradeoffs disguised as technical problems. When modeling
    teams optimize performance, compliance teams address privacy, and product teams
    prioritize engagement independently, system-level ethical behavior emerges by
    accident rather than design. These are fundamentally sociotechnical governance
    problems requiring clear ownership structures that span organizational boundaries,
    data infrastructure designed for ethical auditing, and deliberative processes
    for making value tradeoffs explicit. These challenges become even more acute when
    systems must maintain responsible behavior at scale over time.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些前三个挑战——组织结构、数据质量和竞争目标中——出现了一种模式：负责任的人工智能失败很少源于技术无知。团队了解公平性指标、隐私技术和偏差缓解方法。相反，失败发生在组织碎片化的交汇处，这种碎片化将责任分配出去但没有问责制，数据约束在明确意图的情况下甚至创造了技术障碍，以及竞争目标迫使规范权衡，这些权衡被伪装成技术问题。当建模团队优化性能，合规团队处理隐私，产品团队独立优先考虑参与度时，系统级别的道德行为是偶然出现的而不是设计出来的。这些问题本质上是跨组织边界的社会技术治理问题，需要明确的所有权结构，设计用于道德审计的数据基础设施，以及明确价值权衡的审议过程。当系统必须随着时间的推移在规模上维持负责任的行为时，这些挑战变得更加尖锐。
- en: Scalability and Maintenance
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性和维护
- en: 'Responsible machine learning practices are often introduced during the early
    phases of model development: fairness audits are conducted during initial evaluation,
    interpretability methods are applied during model selection, and privacy-preserving
    techniques are considered during training. However, as systems transition from
    research prototypes to production deployments, these practices frequently degrade
    or disappear. The gap between what is possible in principle and what is sustainable
    in production is a core implementation challenge for responsible AI.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的机器学习实践通常在模型开发的早期阶段被引入：在初始评估期间进行公平性审计，在模型选择期间应用可解释性方法，并在训练期间考虑隐私保护技术。然而，随着系统从研究原型过渡到生产部署，这些实践往往退化或消失。在原则上可能实现与在生产中可持续实现之间的差距是负责任人工智能的核心实施挑战。
- en: Many responsible AI interventions are not designed with scalability in mind.
    Fairness checks may be performed on a static dataset, but not integrated into
    ongoing data ingestion pipelines. Explanation methods may be developed using development-time
    tools but never translated into deployable user-facing interfaces. Privacy constraints
    may be enforced during training, but overlooked during post-deployment monitoring
    or model updates. In each case, what begins as a responsible design intention
    fails to persist across system scaling and lifecycle changes.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 许多负责任的人工智能干预措施并没有考虑到可扩展性。公平性检查可能在一个静态数据集上执行，但并未整合到持续的数据摄入管道中。解释方法可能使用开发时间工具开发，但从未转化为可部署的用户界面。隐私约束可能在训练期间实施，但在部署后监控或模型更新期间被忽视。在每种情况下，开始时作为负责任设计意图的东西，在系统扩展和生命周期变化中未能持续。
- en: Production environments introduce new pressures that reshape system priorities.
    Models must operate across diverse hardware configurations, interface with evolving
    APIs, serve millions of users with low latency, and maintain availability under
    operational stress. For instance, maintaining consistent behavior across CPU,
    GPU, and edge accelerators requires tight integration between framework abstractions,
    runtime schedulers, and hardware-specific compilers. These constraints demand
    continuous adaptation and rapid iteration, often deprioritizing activities that
    are difficult to automate or measure. Responsible AI practices, especially those
    that involve human review, stakeholder consultation, or post-hoc evaluation, may
    not be easily incorporated into fast-paced DevOps[35](#fn35) pipelines.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境引入了新的压力，这些压力重塑了系统优先级。模型必须在不同的硬件配置上运行，与不断发展的API接口，以低延迟为百万用户提供服务，并在操作压力下保持可用性。例如，在CPU、GPU和边缘加速器之间保持一致行为需要框架抽象、运行时调度器和硬件特定编译器之间的紧密集成。这些限制要求持续适应和快速迭代，通常会导致难以自动化或衡量的活动被降级。负责任的AI实践，特别是涉及人工审查、利益相关者咨询或事后评估的实践，可能难以轻松地融入快速发展的DevOps[35](#fn35)管道。
- en: Maintenance introduces further complexity. Machine learning systems are rarely
    static. New data is ingested, retraining is performed, features are deprecated
    or added, and usage patterns shift over time. In the absence of rigorous version
    control, changelogs, and impact assessments, it can be difficult to trace how
    system behavior evolves or whether responsibility-related properties such as fairness
    or robustness are being preserved. Organizational turnover and team restructuring
    can erode institutional memory. Teams responsible for maintaining a deployed model
    may not be the ones who originally developed or audited it, leading to unintentional
    misalignment between system goals and current implementation. These issues are
    especially acute in continual or streaming learning scenarios, where concept drift
    and shifting data distributions demand active monitoring and real-time updates.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 维护引入了更多的复杂性。机器学习系统很少是静态的。新数据被摄入，重新训练被执行，功能被弃用或添加，使用模式随时间变化。在没有严格的版本控制、变更日志和影响评估的情况下，很难追踪系统行为如何演变，或者与责任相关的属性，如公平性或鲁棒性是否得到了保留。组织的人员流动和团队重组可能会侵蚀机构记忆。负责维护已部署模型的团队可能不是最初开发和审计它的团队，这可能导致系统目标与当前实施之间出现无意的不一致。这些问题在持续学习或流式学习场景中尤其严重，因为概念漂移和数据分布的变化需要积极的监控和实时更新。
- en: These challenges are magnified in multi-model systems and cross-platform deployments.
    A recommendation engine may consist of dozens of interacting models, each optimized
    for a different subtask or user segment. A voice assistant deployed across mobile
    and edge environments may maintain different versions of the same model, tuned
    to local hardware constraints. Coordinating updates, ensuring consistency, and
    sustaining responsible behavior in such distributed systems requires infrastructure
    that tracks not only code and data, but also values and constraints.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战在多模型系统和跨平台部署中更加突出。推荐引擎可能包含数十个相互作用的模型，每个模型都针对不同的子任务或用户群体进行了优化。在移动和边缘环境中部署的语音助手可能维护同一模型的不同版本，这些版本针对本地硬件限制进行了调整。在如此分布式的系统中协调更新、确保一致性和维持负责任的行为，需要的基础设施不仅要跟踪代码和数据，还要跟踪价值和约束。
- en: Addressing scalability and maintenance challenges requires treating responsible
    AI as a lifecycle property, not a one-time evaluation. This means embedding audit
    hooks, metadata tracking, and monitoring protocols into system infrastructure.
    It also means creating documentation that persists across team transitions, defining
    accountability structures that survive project handoffs, and ensuring that system
    updates do not inadvertently erase hard-won improvements in fairness, transparency,
    or safety. While such practices can be difficult to implement retroactively, they
    can be integrated into system design from the outset through responsible-by-default
    tooling and workflows.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 解决可扩展性和维护挑战需要将负责任的AI视为生命周期属性，而不是一次性评估。这意味着将审计钩子、元数据跟踪和监控协议嵌入到系统基础设施中。这也意味着创建跨团队过渡持续存在的文档，定义在项目移交中存活的问责结构，并确保系统更新不会无意中抹去在公平性、透明度或安全性方面取得的来之不易的改进。虽然这些做法可能难以事后实施，但它们可以通过默认负责任的工具和工作流程从系统设计之初就集成进去。
- en: Responsibility must scale with the system. Machine learning models deployed
    in real-world environments must not only meet ethical standards at launch, but
    continue to do so as they grow in complexity, user reach, and operational scope.
    Achieving this requires sustained organizational investment and architectural
    planning—not simply technical correctness at a single point in time.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 责任必须与系统规模相匹配。在实际环境中部署的机器学习模型不仅必须在启动时符合伦理标准，而且随着其复杂性、用户范围和运营范围的扩大，必须继续符合这些标准。实现这一点需要持续的机构投资和架构规划——而不仅仅是某一时间点的技术正确性。
- en: Standardization and Evaluation Gaps
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化和评估差距
- en: While the field of responsible machine learning has produced a wide range of
    tools, metrics, and evaluation frameworks, there is still little consensus on
    how to systematically assess whether a system is responsible in practice. Many
    teams recognize the importance of fairness, privacy, interpretability, and robustness,
    yet they often struggle to translate these principles into consistent, measurable
    standards. Benchmarking methodologies provide valuable frameworks for standardized
    evaluation, though adapting these approaches to responsible AI metrics remains
    an active area of development. The lack of formalized evaluation criteria, combined
    with the fragmentation of tools and frameworks, poses a significant barrier to
    implementing responsible AI at scale.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管负责任的机器学习领域已经产生了大量工具、指标和评估框架，但在如何系统地评估系统在实际操作中是否负责任的问题上，仍然缺乏共识。许多团队认识到公平性、隐私、可解释性和鲁棒性的重要性，但他们往往难以将这些原则转化为一致、可衡量的标准。基准测试方法为标准化评估提供了有价值的框架，尽管将这些方法适应负责任的AI指标仍然是发展的一个活跃领域。缺乏正式的评估标准，加上工具和框架的碎片化，对大规模实施负责任的AI构成了重大障碍。
- en: This fragmentation is evident both across and within institutions. Academic
    research frequently introduces new metrics for fairness or robustness that are
    difficult to reproduce outside experimental settings. Industrial teams, by contrast,
    must prioritize metrics that integrate cleanly with production infrastructure,
    are interpretable by non-specialists, and can be monitored over time. As a result,
    practices developed in one context may not transfer well to another, and performance
    comparisons across systems may be unreliable or misleading. For instance, a model
    evaluated for fairness on one benchmark dataset using demographic parity may not
    meet the requirements of equalized odds in another domain or jurisdiction. Without
    shared standards, these evaluations remain ad hoc, making it difficult to establish
    confidence in a systems responsible behavior across contexts.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这种碎片化现象在机构之间以及机构内部都十分明显。学术研究经常引入新的公平性或鲁棒性指标，这些指标在实验设置之外难以复制。相比之下，工业团队必须优先考虑与生产基础设施无缝集成、非专业人士可解释且可以随时间监控的指标。因此，在一个环境中开发的实践可能无法很好地转移到另一个环境中，并且跨系统性能比较可能不可靠或具有误导性。例如，在一个基准数据集上使用人口统计学平等性评估的公平性模型，可能不符合另一个领域或司法管辖区中均衡机会的要求。没有共享标准，这些评估仍然是临时的，这使得在跨情境中建立对系统负责任行为的信心变得困难。
- en: Responsible AI evaluation also suffers from a mismatch between the unit of analysis,
    which is frequently the individual model or batch job, and the level of deployment,
    which includes end-to-end system components such as data ingestion pipelines,
    feature transformations, inference APIs, caching layers, and human-in-the-loop
    workflows. A system that appears fair or interpretable in isolation may fail to
    uphold those properties once integrated into a broader application. Tools that
    support holistic, system-level evaluation remain underdeveloped, and there is
    little guidance on how to assess responsibility across interacting components
    in modern ML stacks.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI评估也面临着分析单元与部署水平之间的不匹配问题，分析单元通常是单个模型或批量作业，而部署水平包括数据摄取管道、特征转换、推理API、缓存层以及人机交互工作流程等端到端系统组件。一个在孤立状态下看似公平或可解释的系统，一旦集成到更广泛的应用中，可能就无法保持这些特性。支持整体、系统级评估的工具仍处于发展阶段，而且关于如何评估现代机器学习堆栈中交互组件的责任几乎没有指导。
- en: Further complicating matters is the lack of lifecycle-aware metrics. Most evaluation
    tools are applied at a single point in time—often just before deployment. Yet
    responsible AI properties such as fairness and robustness are dynamic. They depend
    on how data distributions evolve, how models are updated, and how users interact
    with the system. Without continuous or periodic evaluation, it is difficult to
    determine whether a system remains aligned with its intended ethical goals after
    deployment. Post-deployment monitoring tools exist, but they are rarely integrated
    with the development-time metrics used to assess initial model quality. This disconnect
    makes it hard to detect drift in ethical performance, or to trace observed harms
    back to their upstream sources.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，缺乏生命周期感知的指标也使问题复杂化。大多数评价工具都是在单一时间点应用——通常是在部署之前。然而，负责任的AI属性，如公平性和鲁棒性，是动态的。它们取决于数据分布如何演变，模型如何更新，以及用户如何与系统互动。没有持续或定期的评价，很难确定系统在部署后是否仍然与其预期的道德目标保持一致。部署后监控工具存在，但它们很少与用于评估初始模型质量的开发时间指标集成。这种脱节使得检测道德性能的漂移或追踪观察到的伤害回溯到其上游来源变得困难。
- en: Tool fragmentation further contributes to these challenges. Responsible AI tooling
    is often distributed across disconnected packages, dashboards, or internal systems,
    each designed for a specific task or metric. A team may use one tool for explainability,
    another for bias detection, and a third for compliance reporting—with no unified
    interface for reasoning about system-level tradeoffs. The lack of interoperability
    hinders collaboration between teams, complicates documentation, and increases
    the risk that important evaluations will be skipped or performed inconsistently.
    These challenges are compounded by missing hooks for metadata propagation or event
    logging across components like feature stores, inference gateways, and model registries.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 工具碎片化进一步加剧了这些挑战。负责任的AI工具通常分布在分离的包、仪表板或内部系统中，每个系统都针对特定的任务或指标进行设计。一个团队可能使用一个工具进行可解释性分析，另一个工具进行偏差检测，第三个工具进行合规性报告——没有统一的界面来推理系统级别的权衡。缺乏互操作性阻碍了团队之间的协作，复杂了文档，并增加了重要评价被跳过或执行不一致的风险。这些挑战因缺少在特征存储、推理网关和模型注册库等组件之间传播元数据或事件记录的钩子而加剧。
- en: Addressing these gaps requires progress on multiple fronts. First, shared evaluation
    frameworks must be developed that define what it means for a system to behave
    responsibly—not just in abstract terms, but in measurable, auditable criteria
    that are meaningful across domains. Second, evaluation must be extended beyond
    individual models to cover full system pipelines, including user-facing interfaces,
    update policies, and feedback mechanisms. Finally, evaluation must become a recurring
    lifecycle activity, supported by infrastructure that tracks system behavior over
    time and alerts developers when ethical properties degrade.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些差距需要从多个方面取得进展。首先，必须开发共享的评价框架，这些框架定义了系统如何负责任地行为——不仅是在抽象的术语中，而且是在可衡量、可审计的标准中，这些标准在各个领域都有意义。其次，评价必须扩展到超越单个模型，涵盖完整的系统管道，包括面向用户的界面、更新策略和反馈机制。最后，评价必须成为一种持续的生命周期活动，由能够跟踪系统行为随时间变化并提醒开发者当道德属性下降的基础设施支持。
- en: Without standardized, system-aware evaluation methods, responsible AI remains
    a moving target—described in principles but difficult to verify in practice. Building
    confidence in machine learning systems requires not only better models and tools,
    but shared norms, durable metrics, and evaluation practices that reflect the operational
    realities of deployed AI.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 没有标准化的、系统感知的评价方法，负责任的AI仍然是一个移动的目标——在原则中被描述，但在实践中难以验证。建立对机器学习系统的信心不仅需要更好的模型和工具，还需要共享规范、持久指标以及反映部署AI操作现实性的评价实践。
- en: Responsible AI cannot be achieved through isolated interventions or static compliance
    checks. It requires architectural planning, infrastructure support, and institutional
    processes that sustain ethical goals across the system lifecycle. As ML systems
    scale, diversify, and embed themselves into sensitive domains, the ability to
    enforce properties like fairness, robustness, and privacy must be supported not
    only at model selection time, but across retraining, quantization, serving, and
    monitoring stages. Without persistent oversight, responsible practices degrade
    as systems evolve—especially when tooling, metrics, and documentation are not
    designed to track and preserve them through deployment and beyond.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI不能通过孤立干预或静态合规性检查来实现。它需要架构规划、基础设施支持和制度流程，以在整个系统生命周期内维持道德目标。随着机器学习系统规模的扩大、多样化以及嵌入敏感领域，必须支持在模型选择时间以及重新训练、量化、服务和监控阶段强制执行公平性、鲁棒性和隐私等属性。如果没有持续的监督，负责任的做法会随着系统的发展而退化——特别是当工具、指标和文档没有设计用来在部署及之后跟踪和保存它们时。
- en: Meeting this challenge will require greater standardization, deeper integration
    of responsibility-aware practices into CI/CD pipelines, and long-term investment
    in system infrastructure that supports ethical foresight. The goal is not to perfect
    ethical decision-making in code, but to make responsibility an operational property—traceable,
    testable, and aligned with the constraints and affordances of machine learning
    systems at scale.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 应对这一挑战需要更高的标准化、将责任意识实践更深入地集成到CI/CD管道中，以及长期投资于支持道德远见的系统基础设施。目标不是在代码中完善道德决策，而是使责任成为一个操作属性——可追踪、可测试，并与大规模机器学习系统的约束和功能相一致。
- en: Implementation Decision Framework
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施决策框架
- en: Given these implementation challenges, practitioners need systematic approaches
    to prioritize responsible AI principles based on deployment context and stakeholder
    needs. When designing ML systems, practitioners must navigate trade-offs between
    competing objectives while maintaining ethical safeguards appropriate to system
    stakes and constraints. [Table 17.4](ch023.xhtml#tbl-practitioner-decision-framework)
    provides a decision framework for making these context-sensitive choices.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些实施挑战，实践者需要系统的方法来根据部署上下文和利益相关者的需求优先考虑负责任的AI原则。在设计机器学习系统时，实践者必须在维护与系统风险和约束相适应的道德保障的同时，在相互竞争的目标之间进行权衡。[表17.4](ch023.xhtml#tbl-practitioner-decision-framework)提供了一个决策框架，用于做出这些上下文敏感的选择。
- en: '**Decision Heuristics:**'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策启发式方法**：'
- en: '**When multiple principles conflict**: Engage stakeholders to determine which
    harms are most severe. The mental health chatbot example examined in [Section 17.6.3](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)
    showed such conflicts require deliberation, not algorithmic resolution.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当多个原则冲突时**：与利益相关者合作，确定哪些危害最为严重。在[第17.6.3节](ch023.xhtml#sec-responsible-ai-normative-pluralism-value-conflicts-d61f)中考察的心理健康聊天机器人示例表明，此类冲突需要深思熟虑，而不是算法解决。'
- en: '**When computational budgets are constrained**: Prioritize principles by risk.
    High-stakes decisions demand fairness/explainability even at significant cost.
    Low-stakes applications can use lightweight methods.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当计算预算受限时**：根据风险优先级原则。高风险决策即使在重大成本下也要求公平性和可解释性。低风险应用可以使用轻量级方法。'
- en: '**When deployment context changes**: Re-evaluate principle priorities. A cloud
    model moved to edge loses centralized monitoring capability—compensate with pre-deployment
    validation and local safeguards.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当部署上下文发生变化时**：重新评估原则优先级。从云端模型迁移到边缘会失去集中监控能力——通过部署前的验证和本地安全措施来补偿。'
- en: '**When stakeholder values differ**: Document trade-offs explicitly and create
    contestability mechanisms allowing affected users to challenge decisions.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当利益相关者的价值观不同时**：明确记录权衡并创建可争议机制，允许受影响的用户挑战决策。'
- en: 'Table 17.4: **Practitioner Decision Framework**: Prioritizing responsible AI
    principles based on deployment context, showing primary principles, implementation
    priorities, and acceptable trade-offs for different system types. This framework
    guides practitioners in making context-appropriate decisions when principles conflict
    or resources are constrained.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.4：**实践者决策框架**：根据部署上下文优先考虑负责任的AI原则，显示主要原则、实施优先级和不同系统类型的可接受权衡。此框架指导实践者在原则冲突或资源受限时做出适合上下文的决策。
- en: '| **Deployment Context** | **Primary** **Principles** | **Implementation Priority**
    | **Acceptable Trade-offs** |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| **部署环境** | **主要原则** | **实施优先级** | **可接受的权衡** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **High-Stakes Individual Decisions** **(healthcare diagnosis, credit/loans,**
    **criminal justice, employment)** | Fairness, Explainability, Accountability |
    Mandatory fairness metrics across protected groups; explainability for negative
    outcomes; human oversight for edge cases | Accept 2-5% accuracy reduction for
    interpretability; 20-100 ms latency for explanations; higher computational costs
    |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| **高风险个人决策** **(医疗诊断、信贷/贷款、刑事司法、就业)** | 公平性、可解释性、问责制 | 在受保护群体中强制执行公平性指标；对负面结果的解释；边缘案例的人类监督
    | 接受2-5%的准确性降低以实现可解释性；解释的延迟为20-100毫秒；更高的计算成本 |'
- en: '| **Safety-Critical Systems** **(autonomous vehicles, medical** **devices,
    industrial control)** | Safety, Robustness, Accountability | Certified adversarial
    defenses; formal validation; failsafe mechanisms; comprehensive logging | Accept
    significant training overhead (100-300% for adversarial training); conservative
    confidence thresholds; redundant inference |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| **关键安全系统** **(自动驾驶汽车、医疗设备、工业控制)** | 安全性、鲁棒性、问责制 | 认证的对抗性防御；形式验证；安全机制；全面的日志记录
    | 接受显著的训练开销（对抗性训练为100-300%）；保守的置信阈值；冗余推理 |'
- en: '| **Privacy-Sensitive Applications** **(health records, financial data,** **personal
    communications)** | Privacy, Security, Transparency | Differential privacy (ε≤1.0);
    local processing; data minimization; user consent mechanisms | Accept 2-5% accuracy
    loss for DP; higher client-side compute; limited model updates; reduced personalization
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| **隐私敏感应用** **(健康记录、财务数据、个人通讯)** | 隐私、安全、透明度 | 差分隐私（ε≤1.0）；本地处理；数据最小化；用户同意机制
    | 接受2-5%的准确性损失以实现DP；更高的客户端计算；有限的模型更新；减少个性化 |'
- en: '| **Large-Scale Consumer Systems** **(content recommendation, search,** **advertising)**
    | Fairness, Transparency, Safety | Bias monitoring across demographics; explanation
    mechanisms; content policy enforcement; feedback loops detection | Balance explainability
    costs against scale (streaming SHAP vs. full SHAP); accept 5-15 ms latency for
    fairness checks; invest in monitoring infrastructure |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| **大规模消费者系统** **(内容推荐、搜索、广告)** | 公平性、透明度、安全性 | 在人口统计学上监测偏差；解释机制；内容政策执行；反馈循环检测
    | 平衡可解释性成本与规模（流式SHAP与完整SHAP）；接受5-15毫秒的公平性检查延迟；投资于监控基础设施 |'
- en: '| **Resource-Constrained Deployments** **(mobile, edge, TinyML)** | Privacy,
    Efficiency, Safety | Local inference; data locality; input validation; graceful
    degradation | Sacrifice real-time fairness monitoring; use lightweight explainability
    (gradients over SHAP); pre-deployment validation only; limited model complexity
    |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| **资源受限部署** **(移动、边缘、TinyML)** | 隐私、效率、安全性 | 本地推理；数据本地化；输入验证；优雅降级 | 放弃实时公平性监控；使用轻量级可解释性（SHAP上的梯度）；仅进行部署前的验证；有限的模型复杂性
    |'
- en: '| **Research/Exploratory Systems** **(internal tools, prototypes,** **A/B tests)**
    | Transparency, Safety (harm prevention) | Documentation of known limitations;
    restricted user populations; monitoring for unintended harms | Can deprioritize
    sophisticated fairness/explainability for internal use; focus on observability
    and rapid iteration |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| **研究/探索性系统** **(内部工具、原型、A/B 测试)** | 透明度、安全性（防止伤害） | 已知限制的文档；限制用户群体；监测意外伤害
    | 可以在内部使用时降低复杂的公平性/可解释性；关注可观察性和快速迭代 |'
- en: This framework provides starting guidance. Responsible AI implementation requires
    ongoing assessment as systems, contexts, and societal expectations evolve.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供起始指导。负责任的AI实施需要持续评估，因为系统、环境和公众期望都在不断演变。
- en: These implementation challenges become even more complex as AI systems increase
    in autonomy and capability. The value alignment principle introduced in [Section 17.2](ch023.xhtml#sec-responsible-ai-core-principles-1bd7)—ensuring
    AI systems pursue goals consistent with human intent and ethical norms—takes on
    heightened importance when systems operate with greater independence. While the
    responsible AI techniques examined above address bias, privacy, and explainability
    in supervised contexts, autonomous systems require additional safety mechanisms
    to prevent misalignment between system objectives and human values.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能系统自主性和能力的提升，这些实施挑战变得更加复杂。在[第17.2节](ch023.xhtml#sec-responsible-ai-core-principles-1bd7)中引入的价值对齐原则——确保人工智能系统追求与人类意图和伦理规范一致的目标——在系统以更高独立性运行时显得尤为重要。虽然上述负责的人工智能技术解决了监督环境中的偏差、隐私和可解释性问题，但自主系统需要额外的安全机制来防止系统目标与人类价值观之间的不一致。
- en: AI Safety and Value Alignment
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能安全与价值对齐
- en: 'Value alignment challenges scale dramatically as machine learning systems gain
    autonomy and capability. The responsible AI techniques examined above—bias detection,
    explainability, privacy preservation—provide essential capabilities but reveal
    fundamental limitations when systems operate with greater independence. Consider
    how these established methods break down in autonomous contexts:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统获得自主性和能力，价值对齐挑战急剧扩大。上述负责的人工智能技术——偏差检测、可解释性、隐私保护——提供了基本能力，但当系统以更高独立性运行时，这些技术揭示了根本性的局限性。考虑这些既定方法在自主环境中的失效情况：
- en: Bias detection algorithms like those implemented in Fairlearn require ongoing
    human interpretation and corrective action. An autonomous vehicle’s perception
    system might exhibit systematic bias against detecting pedestrians with mobility
    aids, but without human oversight, the bias detection metrics become just logged
    statistics with no remediation pathway. The technical capability to measure bias
    exists, but autonomous systems lack the judgment to determine appropriate responses.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Fairlearn中实施的方法的偏差检测算法需要持续的人类解释和纠正行动。一辆自动驾驶汽车的感觉系统可能会表现出对检测有行动辅助设备的行人的系统性偏差，但如果没有人类监督，偏差检测指标就只是记录的统计数据，没有补救途径。虽然存在测量偏差的技术能力，但自主系统缺乏判断力来确定适当的反应。
- en: Explainability frameworks assume human audiences who can interpret and act on
    explanations. An autonomous trading system might generate perfectly accurate SHAP
    explanations for its decisions, but these explanations become meaningless if no
    human reviews them before the system executes thousands of trades per second.
    The system optimizes its objective (profit) through methods its designers never
    anticipated, making explanations a post-hoc record rather than a decision-making
    aid.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性框架假设有能够解释并采取行动的人类受众。一个自主交易系统可能会为其决策生成完全准确的SHAP解释，但如果在系统每秒执行数千笔交易之前没有人审查这些解释，那么这些解释就变得毫无意义。系统通过其设计者从未预料到的方法优化其目标（利润），使得解释成为事后记录而不是决策辅助。
- en: Privacy preservation techniques like differential privacy protect individual
    data points but cannot address broader value misalignment. An autonomous content
    recommendation system might preserve user privacy through local differential privacy
    while simultaneously optimizing for engagement metrics that promote misinformation
    or harmful content. Technical privacy compliance becomes insufficient when the
    system’s fundamental objectives conflict with user welfare.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护技术，如差分隐私，保护个体数据点，但不能解决更广泛的价值不一致问题。一个自主内容推荐系统可能通过局部差分隐私保护用户隐私，同时优化促进错误信息或有害内容的参与度指标。当系统的基本目标与用户福利冲突时，技术隐私合规性变得不足。
- en: These examples illustrate why responsible AI frameworks, while necessary, become
    insufficient as systems gain autonomy. The techniques assume human oversight,
    constrained objectives, and relatively predictable operating environments. AI
    safety extends these concerns to systems that may optimize objectives misaligned
    with human intentions, operate in unpredictable environments, or pursue goals
    through methods their designers never anticipated.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子说明了为什么负责的人工智能框架，虽然必要，但随着系统获得自主性而变得不足。这些技术假设了人类的监督、受约束的目标和相对可预测的运行环境。人工智能安全将这些担忧扩展到可能优化与人类意图不一致的目标、在不可预测的环境中运行或通过其设计者未曾预料到的方法追求目标的系统。
- en: As machine learning systems increase in autonomy, scale, and deployment complexity,
    the nature of responsibility expands beyond model-level fairness or privacy concerns.
    It includes ensuring that systems pursue the right objectives, behave safely in
    uncertain environments, and remain aligned with human intentions over time. These
    concerns fall under the domain of AI safety[36](#fn36), which focuses on preventing
    unintended or harmful outcomes from capable AI systems. A central challenge is
    that today’s ML models often optimize proxy metrics[37](#fn37), such as loss functions,
    reward functions, or engagement signals, that do not fully capture human values.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统在自主性、规模和部署复杂性方面的增加，责任的本质超越了模型层面的公平性或隐私问题。它包括确保系统追求正确的目标，在不确定的环境中安全地行为，并且随着时间的推移保持与人类意图的一致。这些问题属于人工智能安全[36](#fn36)
    的领域，该领域专注于防止有能力的AI系统产生意外或有害的结果。一个核心挑战是，今天的机器学习模型通常优化代理指标[37](#fn37)，例如损失函数、奖励函数或参与度信号，这些指标并不能完全捕捉到人类价值观。
- en: One concrete example comes from recommendation systems, where a model trained
    to maximize click-through rate (CTR)[38](#fn38) may end up promoting content that
    increases engagement but diminishes user satisfaction, including clickbait, misinformation,
    and emotionally manipulative material. This behavior is aligned with the proxy,
    but misaligned with the actual goal, resulting in a feedback loop that reinforces
    undesirable outcomes. As shown in [Figure 17.9](ch023.xhtml#fig-reward-hacking-loop),
    the system learns to optimize for a measurable reward (clicks) rather than the
    intended human-centered outcome (satisfaction). The result is emergent behavior
    that reflects specification gaming or reward hacking[39](#fn39)—a central concern
    in value alignment and AI safety.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的例子来自推荐系统，一个被训练以最大化点击率（CTR）[38](#fn38) 的模型可能会最终推广那些增加用户参与度但降低用户满意度的内容，包括点击诱饵、错误信息和情感操纵材料。这种行为与代理指标一致，但与实际目标不一致，导致一个强化不理想结果的反馈循环。如图
    17.9[图 17.9](ch023.xhtml#fig-reward-hacking-loop) 所示，系统学会优化可衡量的奖励（点击量）而不是预期以人为中心的成果（满意度）。结果是出现反映规范游戏或奖励黑客[39](#fn39)
    的行为——这是价值对齐和人工智能安全中的核心关注点。
- en: '![](../media/file298.svg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file298.svg)'
- en: 'Figure 17.9: **Reward Hacking Loop**: Maximizing measurable rewards—like clicks—can
    incentivize unintended model behaviors that undermine the intended goal of user
    satisfaction. optimizing for proxy metrics creates misalignment between a system’s
    objective and desired outcomes, posing challenges for value alignment in AI safety.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9：**奖励黑客循环**：最大化可衡量的奖励（如点击量）可能会激励出非预期的模型行为，这些行为会损害用户满意度的预期目标。针对代理指标进行优化会导致系统目标与期望结果之间的不一致，对人工智能安全中的价值对齐构成挑战。
- en: In 1960, Norbert Wiener wrote, “if we use, to achieve our purposes, a mechanical
    agency with whose operation we cannot interfere effectively… we had better be
    quite sure that the purpose put into the machine is the purpose which we desire”
    ([Wiener 1960](ch058.xhtml#ref-wiener1960some)).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 1960 年，诺伯特·维纳写道：“如果我们使用一个我们无法有效干预其操作的机械机构来实现我们的目的……我们最好确保放入机器中的目的是我们想要的” ([Wiener
    1960](ch058.xhtml#ref-wiener1960some))。
- en: 'As the capabilities of deep learning models have increasingly approached, and,
    in certain instances, exceeded, human performance, the concern that such systems
    may pursue unintended or undesirable goals has become more pressing ([S. Russell
    2021](ch058.xhtml#ref-russell2021human)). Within the field of AI safety, a central
    focus is the problem of value alignment: how to ensure that machine learning systems
    act in accordance with broad human intentions, rather than optimizing misaligned
    proxies or exhibiting emergent behavior that undermines social goals. As Russell
    argues in Human-Compatible Artificial Intelligence, much of current AI research
    presumes that the objectives to be optimized are known and fixed, focusing instead
    on the effectiveness of optimization rather than the design of objectives themselves.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习模型的能力越来越接近，并在某些情况下超过人类性能，这样的系统可能追求非预期或不希望的目标的担忧变得更加紧迫 ([S. Russell 2021](ch058.xhtml#ref-russell2021human))。在人工智能安全领域，一个核心关注点是价值对齐问题：如何确保机器学习系统能够按照广泛的人类意图行事，而不是优化不一致的代理指标或表现出损害社会目标的涌现行为。正如
    Russell 在《人类兼容的人工智能》一书中所论证的，当前的大部分人工智能研究都假设要优化的目标是已知且固定的，而将重点放在优化效果上，而不是目标本身的设计。
- en: Yet defining “the right purpose” for intelligent systems is especially difficult
    in real-world deployment settings. ML systems often operate within dynamic environments,
    interact with multiple stakeholders, and adapt over time. These conditions make
    it challenging to encode human values in static objective functions or reward
    signals. Frameworks like Value Sensitive Design aim to address this challenge
    by providing formal processes for eliciting and integrating stakeholder values
    during system design.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界的部署环境中定义“正确的目的”对于智能系统来说尤其困难。机器学习系统通常在动态环境中运行，与多个利益相关者互动，并随着时间的推移进行适应。这些条件使得在静态目标函数或奖励信号中编码人类价值观变得具有挑战性。像价值敏感设计（Value
    Sensitive Design）这样的框架旨在通过在系统设计过程中提供正式的过程来激发和整合利益相关者的价值观，以应对这一挑战。
- en: Taking a holistic sociotechnical perspective, which accounts for both the algorithmic
    mechanisms and the contexts in which systems operate, is important for ensuring
    alignment. Without this, intelligent systems may pursue narrow performance objectives
    (e.g., accuracy, engagement, or throughput) while producing socially undesirable
    outcomes. Achieving robust alignment under such conditions remains an open and
    important area of research in ML systems.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 从整体的社会技术视角出发，这既考虑了算法机制，也考虑了系统运行的环境，对于确保对齐至关重要。没有这一点，智能系统可能会追求狭窄的性能目标（例如，准确性、参与度或吞吐量），同时产生社会不希望看到的结果。在这样条件下实现稳健的对齐仍然是机器学习系统中一个开放且重要的研究领域。
- en: The absence of alignment can give rise to well-documented failure modes, particularly
    in systems that optimize complex objectives. In reinforcement learning (RL), for
    example, models often learn to exploit unintended aspects of the reward function—a
    phenomenon known as specification gaming or reward hacking. Such failures arise
    when variables not explicitly included in the objective are manipulated in ways
    that maximize reward while violating human intent.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐的缺失可能导致已记录的失败模式，尤其是在优化复杂目标的系统中。例如，在强化学习（RL）中，模型经常学会利用奖励函数的非预期方面——这种现象被称为规范游戏或奖励黑客攻击。当未明确包含在目标中的变量以最大化奖励的方式被操纵，同时违反人类意图时，就会发生此类失败。
- en: 'A particularly influential approach in recent years has been reinforcement
    learning from human feedback (RLHF), where large pre-trained models are fine-tuned
    using human-provided preference signals ([Christiano et al. 2017](ch058.xhtml#ref-christiano2017deep)).
    While this method improves alignment over standard RL, it also introduces new
    risks. Ngo ([Ngo, Chan, and Mindermann 2022](ch058.xhtml#ref-ngo2022alignment))
    identifies three potential failure modes introduced by RLHF: (1) situationally
    aware reward hacking, where models exploit human fallibility; (2) the emergence
    of misaligned internal goals that generalize beyond the training distribution;
    and (3) the development of power-seeking behavior that preserves reward maximization
    capacity, even at the expense of human oversight.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，一种特别有影响力的方法是从人类反馈中进行强化学习（RLHF），其中大型预训练模型通过人类提供的偏好信号进行微调（[Christiano 等人 2017](ch058.xhtml#ref-christiano2017deep)）。虽然这种方法在标准强化学习的基础上提高了对齐度，但也引入了新的风险。Ngo
    ([Ngo, Chan, and Mindermann 2022](ch058.xhtml#ref-ngo2022alignment)) 指出了 RLHF
    引入的三种潜在失败模式：（1）情境感知奖励黑客攻击，其中模型利用人类的易错性；（2）出现与训练分布不一致的内部目标；（3）发展出寻求权力的行为，即使牺牲人类监督，也能保持奖励最大化能力。
- en: 'These concerns are not limited to speculative scenarios. Amodei et al. ([2016](ch058.xhtml#ref-amodei2016concrete))
    outline six concrete challenges for AI safety: (1) avoiding negative side effects
    during policy execution, (2) mitigating reward hacking, (3) ensuring scalable
    oversight when ground-truth evaluation is expensive or infeasible, (4) designing
    safe exploration strategies that promote creativity without increasing risk, (5)
    achieving robustness to distributional shift in testing environments, and (6)
    maintaining alignment across task generalization. Each of these challenges becomes
    more acute as systems are scaled up, deployed across diverse settings, and integrated
    with real-time feedback or continual learning.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 这些担忧并不仅限于推测性场景。Amodei 等人 ([2016](ch058.xhtml#ref-amodei2016concrete)) 概述了六个具体的
    AI 安全挑战：（1）在策略执行期间避免负面副作用；（2）减轻奖励黑客攻击；（3）在地面真实评估昂贵或不可行时确保可扩展的监督；（4）设计安全的探索策略，以促进创造力而不增加风险；（5）在测试环境中对分布变化具有鲁棒性；（6）在任务泛化中保持对齐。随着系统规模的扩大、在多样化的环境中部署以及与实时反馈或持续学习集成，这些挑战变得更加尖锐。
- en: These safety challenges are particularly evident in autonomous systems that
    operate with reduced human oversight.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这些安全挑战在需要减少人类监督的自主系统中尤为明显。
- en: Autonomous Systems and Trust
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自主系统与信任
- en: The consequences of autonomous systems that act independently of human oversight
    and often outside the bounds of human judgment have been widely documented across
    multiple industries. A prominent recent example is the suspension of Cruises deployment
    and testing permits by the California Department of Motor Vehicles due to [“unreasonable
    risks to public safety”](https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html).
    One such [incident](https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html)
    involved a pedestrian who entered a crosswalk just as the stoplight turned green—an
    edge case in perception and decision-making that led to a collision. A more tragic
    example occurred in 2018, when a self-driving Uber vehicle in autonomous mode
    [failed to classify a pedestrian pushing a bicycle](https://www.bbc.com/news/technology-54175359)
    as an object requiring avoidance, resulting in a fatality.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 独立于人类监督并经常超出人类判断范围的行为自主系统的后果已在多个行业中得到了广泛记录。一个突出的近期例子是加利福尼亚州机动车辆管理局因[“对公共安全构成不合理风险”](https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html)而暂停了Cruises的部署和测试许可。其中一起[事件](https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html)涉及一名行人刚刚进入人行横道，此时交通灯变绿——这是一个感知和决策的边缘案例，导致了碰撞。一个更悲惨的例子发生在2018年，当时一辆自动驾驶优步车辆在自动驾驶模式下[未能将推自行车的行人分类为需要避让的对象](https://www.bbc.com/news/technology-54175359)，导致了一起致命事故。
- en: While autonomous driving systems are often the focal point of public concern,
    similar risks arise in other domains. Remotely piloted drones and autonomous military
    systems are already [reshaping modern warfare](https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/),
    raising not only safety and effectiveness concerns but also difficult questions
    about ethical oversight, rules of engagement, and responsibility. When autonomous
    systems fail, the question of [who should be held accountable](https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/)
    remains both legally and ethically unresolved.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动驾驶系统通常是公众关注的焦点，但类似的风险也出现在其他领域。遥控无人机和自主军事系统已经[重塑了现代战争](https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/)，不仅引发了安全和有效性问题，还提出了关于道德监督、交战规则和责任等棘手问题。当自主系统失败时，[谁应该承担责任](https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/)的问题在法律和道德上仍然悬而未决。
- en: At its core, this challenge reflects a deeper tension between human and machine
    autonomy. Engineering and computer science disciplines have historically emphasized
    machine autonomy—improving system performance, minimizing human intervention,
    and maximizing automation. A bibliometric analysis of the ACM Digital Library
    found that, as of 2019, 90% of the most cited papers referencing “autonomy” focused
    on machine, rather than human, autonomy ([Calvo et al. 2020](ch058.xhtml#ref-calvo2020supporting)).
    Productivity, efficiency, and automation have been widely treated as default objectives,
    often without interrogating the assumptions or tradeoffs they entail for human
    agency and oversight.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，这一挑战反映了人类与机器自主性之间的更深层次的紧张关系。工程和计算机科学学科历史上强调机器自主性——提高系统性能、最小化人类干预和最大化自动化。对ACM数字图书馆的计量分析发现，截至2019年，90%引用“自主性”的最受关注论文都集中在机器自主性，而不是人类自主性（[Calvo等人，2020](ch058.xhtml#ref-calvo2020supporting)）。生产力、效率和自动化被广泛视为默认目标，通常没有质疑它们所涉及的人类能动性和监督的假设或权衡。
- en: 'However, these goals can place human interests at risk when systems operate
    in dynamic, uncertain environments where full specification of safe behavior is
    infeasible. This difficulty is formally captured by the frame problem and qualification
    problem, both of which highlight the impossibility of enumerating all the preconditions
    and contingencies needed for real-world action to succeed ([McCarthy 1981](ch058.xhtml#ref-mccarthy1981epistemological)).
    In practice, such limitations manifest as brittle autonomy: systems that appear
    competent under nominal conditions but fail silently or dangerously when faced
    with ambiguity or distributional shift.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当系统在动态、不确定的环境中运行，且无法完全指定安全行为时，这些目标可能会使人类利益处于风险之中。这种困难被形式化为框架问题和资格问题，两者都突出了列举所有现实行动成功所需的先决条件和偶然事件的不可能性（[麦卡锡
    1981](ch058.xhtml#ref-mccarthy1981epistemological)）。在实践中，这种限制表现为脆弱的自主性：在名义条件下看似有能力的系统，在面临模糊性或分布变化时可能会无声或危险地失败。
- en: 'To address this, researchers have proposed formal safety frameworks such as
    Responsibility-Sensitive Safety (RSS) ([Shalev-Shwartz, Shammah, and Shashua 2017](ch058.xhtml#ref-shalev2017formal)),
    which decompose abstract safety goals into mathematically defined constraints
    on system behavior—such as minimum distances, braking profiles, and right-of-way
    conditions. These formulations allow safety properties to be verified under specific
    assumptions and scenarios. However, such approaches remain vulnerable to the same
    limitations they aim to solve: they are only as good as the assumptions encoded
    into them and often require extensive domain modeling that may not generalize
    well to unanticipated edge cases.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，研究人员提出了形式化的安全框架，如责任敏感安全（RSS）([沙莱夫-施瓦茨、沙玛和沙舒亚 2017](ch058.xhtml#ref-shalev2017formal)），它将抽象的安全目标分解为对系统行为的数学定义约束——例如最小距离、制动配置文件和优先通行条件。这些公式允许在特定的假设和场景下验证安全属性。然而，这种方法仍然容易受到它们旨在解决的问题的相同限制：它们的效果取决于编码到其中的假设，并且通常需要广泛的领域建模，这可能无法很好地推广到未预见的边缘情况。
- en: An alternative approach emphasizes human-centered system design, ensuring that
    human judgment and oversight remain central to autonomous decision-making. Value-Sensitive
    Design ([Friedman 1996](ch058.xhtml#ref-friedman1996value)) proposes incorporating
    user values into system design by explicitly considering factors like capability,
    complexity, misrepresentation, and the fluidity of user control. More recently,
    the METUX model (Motivation, Engagement, and Thriving in the User Experience)
    extends this thinking by identifying six “spheres of technology experience”—Adoption,
    Interface, Tasks, Behavior, Life, and Society, which affect how technology supports
    or undermines human flourishing ([Peters, Calvo, and Ryan 2018](ch058.xhtml#ref-peters2018designing)).
    These ideas are rooted in Self-Determination Theory (SDT), which defines autonomy
    not as control in a technical sense, but as the ability to act in accordance with
    ones values and goals ([Ryan and Deci 2000](ch058.xhtml#ref-ryan2000self)).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法强调以人为中心的系统设计，确保人类的判断和监督在自主决策中始终处于核心地位。价值敏感设计（[弗里德曼 1996](ch058.xhtml#ref-friedman1996value)）建议通过明确考虑能力、复杂性、误表示和用户控制的流动性等因素，将用户价值观纳入系统设计。最近，METUX模型（用户体验中的动机、参与和繁荣）通过识别六个“技术体验领域”——采用、界面、任务、行为、生活和社区，来扩展这种思考，这些领域影响技术如何支持或破坏人类的繁荣（[彼得斯、卡洛沃和瑞恩
    2018](ch058.xhtml#ref-peters2018designing)）。这些想法根植于自我决定理论（SDT），该理论将自主性定义为在技术意义上的控制，而不是按照个人的价值观和目标行动的能力（[瑞恩和德西
    2000](ch058.xhtml#ref-ryan2000self)）。
- en: In the context of ML systems, these perspectives underscore the importance of
    designing architectures, interfaces, and feedback mechanisms that preserve human
    agency. For instance, recommender systems that optimize engagement metrics may
    interfere with behavioral autonomy by shaping user preferences in opaque ways.
    By evaluating systems across METUXs six spheres, designers can anticipate and
    mitigate downstream effects that compromise meaningful autonomy, even in cases
    where short-term system performance appears optimal.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统的背景下，这些观点强调了设计架构、界面和反馈机制以保持人类能动性的重要性。例如，优化参与度指标的推荐系统可能会通过不透明的方式塑造用户偏好，从而干扰行为自主性。通过在METUX的六个领域内评估系统，设计师可以预测并减轻损害有意义自主性的下游影响，即使短期系统性能看似最优。
- en: Beyond technical safety considerations, the deployment of autonomous AI systems
    raises broader societal concerns about economic disruption.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 除去技术安全考虑之外，自主人工智能系统的部署引发了更广泛的社会担忧，即经济动荡。
- en: Economic Implications of AI Automation
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能自动化经济影响
- en: A recurring concern in the adoption of AI technologies is the potential for
    widespread job displacement. As machine learning systems become capable of performing
    increasingly complex cognitive and physical tasks, there is growing fear that
    they may replace existing workers and reduce the availability of alternative employment
    opportunities across industries. These concerns are particularly acute in sectors
    with well-structured tasks, including logistics, manufacturing, and customer service,
    where AI-based automation appears both technically feasible and economically incentivized.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用人工智能技术的过程中，一个反复出现的问题是广泛的工作岗位转移的可能性。随着机器学习系统能够执行越来越复杂的认知和物理任务，人们越来越担心它们可能会取代现有工人，并减少整个行业可替代就业机会的可用性。这些担忧在具有良好结构化任务的行业中尤为严重，包括物流、制造和客户服务，在这些行业中，基于人工智能的自动化在技术上可行且具有经济激励。
- en: However, the economic implications of automation are not historically unprecedented.
    Prior waves of technological change, including industrial mechanization and computerization,
    have tended to result in job displacement rather than absolute job loss ([Shneiderman
    2022](ch058.xhtml#ref-shneiderman2022human)). Automation often reduces the cost
    and increases the quality of goods and services, thereby expanding access and
    driving demand. This demand, in turn, creates new forms of production, distribution,
    and support work—sometimes in adjacent sectors, sometimes in roles that did not
    previously exist.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自动化的经济影响在历史上并非前所未有。包括工业机械化和计算机化在内的先前技术变革往往导致就业岗位的转移而不是绝对数量的岗位损失（[Shneiderman
    2022](ch058.xhtml#ref-shneiderman2022human)）。自动化通常降低商品和服务的成本并提高质量，从而扩大了获取渠道并推动了需求。这种需求反过来又创造了新的生产、分销和支持工作形式——有时在相邻行业，有时在以前不存在的新角色中。
- en: Empirical studies of industrial robotics and process automation further challenge
    the feasibility of “lights-out” factories, systems that are designed for fully
    autonomous operation without human oversight. Despite decades of effort, most
    attempts to achieve this level of automation have been unsuccessful. According
    to the MIT Work of the Future task force ([Work of the Future 2020](ch058.xhtml#ref-work_of_the_future_2020)),
    such efforts often lead to zero-sum automation, where productivity increases come
    at the expense of system flexibility, adaptability, and fault tolerance. Human
    workers remain important for tasks that require contextual judgment, cross-domain
    generalization, or system-level debugging—capabilities that are still difficult
    to encode in machine learning models or automation frameworks.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 工业机器人和流程自动化的实证研究进一步挑战了“无人工厂”的可行性，这种系统旨在在没有人类监督的情况下完全自主运行。尽管数十年来一直在努力，但大多数实现这一自动化水平的尝试都未能成功。根据麻省理工学院未来工作工作组([Work
    of the Future 2020](ch058.xhtml#ref-work_of_the_future_2020))的研究，这些努力往往导致零和自动化，其中生产力的提高是以系统灵活性、适应性和容错性为代价的。人类工作者对于需要情境判断、跨领域泛化或系统级调试的任务仍然很重要——这些能力在机器学习模型或自动化框架中仍然难以编码。
- en: Instead, the task force advocates for a positive-sum automation approach that
    augments human work rather than replacing it. This strategy emphasizes the integration
    of AI systems into workflows where humans retain oversight and control, such as
    semi-autonomous assembly lines or collaborative robotics. It also recommends bottom-up
    identification of automatable tasks, with priority given to those that reduce
    cognitive load or eliminate hazardous work, alongside the selection of appropriate
    metrics that capture both efficiency and resilience. Metrics rooted solely in
    throughput or cost minimization may inadvertently penalize human-in-the-loop designs,
    whereas broader metrics tied to safety, maintainability, and long-term adaptability
    provide a more comprehensive view of system performance.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，工作组倡导一种正和自动化方法，这种方法增强人类工作而不是取代它。该策略强调将人工智能系统整合到工作流程中，其中人类保持监督和控制，例如半自主装配线或协作机器人。它还建议自下而上地识别可自动化任务，优先考虑那些减少认知负荷或消除危险工作，同时选择适当的指标，这些指标可以捕捉效率和弹性。仅基于吞吐量或成本最小化的指标可能会无意中惩罚需要人工介入的设计，而与安全、可维护性和长期适应性相关的更广泛的指标则提供了对系统性能的更全面看法。
- en: Nonetheless, the long-run economic trajectory does not eliminate the reality
    of near-term disruption. Workers whose skills are rendered obsolete by automation
    may face wage stagnation, reduced bargaining power, or long-term displacement—especially
    in the absence of retraining opportunities or labor market mobility. Public and
    legislative efforts will play a important role in shaping this transition, including
    policies that promote equitable access to the benefits of automation. Positive
    applications of AI demonstrate how responsible deployment can create beneficial
    economic opportunities while addressing social challenges. These may include upskilling
    initiatives, social safety nets, minimum wage increases, and corporate accountability
    frameworks that ensure the distributional impacts of AI are monitored and addressed
    over time.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，长期的经济轨迹并不能消除近期中断的现实。那些技能被自动化取代的工人可能会面临工资停滞、谈判能力下降或长期失业——尤其是在缺乏再培训机会或劳动力市场流动性时。公共和立法努力将在塑造这一转型中发挥重要作用，包括促进自动化利益公平获取的政策。人工智能的积极应用展示了负责任地部署如何创造有益的经济机会，同时解决社会挑战。这些可能包括提升技能计划、社会安全网、最低工资增加以及确保人工智能的分配影响得到监测和解决的企业问责框架。
- en: Addressing these economic concerns requires not only thoughtful policy but also
    effective public communication about AI capabilities and limitations.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些经济问题不仅需要深思熟虑的政策，还需要关于人工智能能力和局限性的有效公众沟通。
- en: AI Literacy and Communication
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能素养与沟通
- en: 'A 1993 survey of 3,000 North American adults’ beliefs about the “electronic
    thinking machine” revealed two dominant perspectives on early computing: the “beneficial
    tool of man” and the “awesome thinking machine” ([Martin 1993](ch058.xhtml#ref-martin1993myth)).
    The latter reflects a perception of computers as mysterious, intelligent, and
    potentially uncontrollable—“smarter than people, unlimited, fast, and frightening.”
    These perceptions, though decades old, remain relevant in the age of machine learning
    systems. As the pace of innovation accelerates, responsible AI development must
    be accompanied by clear and accurate scientific communication, especially concerning
    the capabilities, limitations, and uncertainties of AI technologies.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 1993年对3000名北美成年人关于“电子思维机器”信念的调查揭示了早期计算的两个主导观点：“人类的有益工具”和“令人敬畏的思维机器” ([Martin
    1993](ch058.xhtml#ref-martin1993myth))。后者反映了人们对计算机的神秘、智能和可能无法控制的看法——“比人聪明、无限、快速且令人恐惧。”这些看法虽然已经过去了几十年，但在机器学习系统时代仍然相关。随着创新步伐的加快，负责任的人工智能发展必须伴随着清晰和准确的科学沟通，特别是关于人工智能技术的能力、局限性和不确定性的沟通。
- en: 'As modern AI systems surpass layperson understanding and begin to influence
    high-stakes decisions, public narratives tend to polarize between utopian and
    dystopian extremes. This is not merely a result of media framing, but of a more
    core difficulty: in technologically advanced societies, the outputs of scientific
    systems are often perceived as magical—“understandable only in terms of what it
    did, not how it worked” ([Handlin 1965](ch058.xhtml#ref-handlin1965science)).
    Without scaffolding for technical comprehension, systems like generative models,
    autonomous agents, or large-scale recommender platforms can be misunderstood or
    mistrusted, impeding informed public discourse.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代人工智能系统超越普通人的理解并开始影响高风险决策，公众叙事往往在乌托邦和反乌托邦的极端之间两极分化。这不仅仅是因为媒体框架，更是由于一个更根本的困难：在技术先进的社会中，科学系统的输出往往被视为神奇的——“只能从它所做的事情来理解，而不是它是如何工作的”
    ([Handlin 1965](ch058.xhtml#ref-handlin1965science))。没有技术理解的支撑，像生成模型、自主代理或大规模推荐平台这样的系统可能会被误解或不受信任，阻碍有信息的公众讨论。
- en: Tech companies bear responsibility in this landscape. Overstated claims, anthropomorphic
    marketing, or opaque product launches contribute to cycles of hype and disappointment,
    eroding public trust. But improving AI literacy requires more than restraint in
    corporate messaging. It demands systematic research on scientific communication
    in the context of AI. Despite the societal impact of modern machine learning,
    an analysis of the Scopus scholarly database found only a small number of papers
    that intersect the domains of “artificial intelligence” and “science communication”
    ([Schäfer 2023](ch058.xhtml#ref-schafer2023notorious)).
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，科技公司负有责任。夸大的声明、拟人化的营销或模糊的产品发布加剧了炒作和失望的循环，侵蚀了公众的信任。但提高AI素养需要的不只是企业信息的克制，还需要对AI背景下的科学传播进行系统研究。尽管现代机器学习对社会有影响，但Scopus学术数据库的分析发现，只有少数论文涉及“人工智能”和“科学传播”这两个领域([Schäfer
    2023](ch058.xhtml#ref-schafer2023notorious))。
- en: Addressing this gap requires attention to how narratives about AI are shaped—not
    just by companies, but also by academic institutions, regulators, journalists,
    non-profits, and policy advocates. The frames and metaphors used by these actors
    significantly influence how the public perceives agency, risk, and control in
    AI systems ([Lindgren 2023](ch058.xhtml#ref-lindgren2023handbook)). These perceptions,
    in turn, affect adoption, oversight, and resistance, particularly in domains such
    as education, healthcare, and employment, where AI deployment intersects directly
    with lived experience.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一差距需要关注关于AI的叙事是如何形成的——不仅由公司，还包括学术机构、监管机构、记者、非营利组织和政策倡导者。这些行为者使用的框架和隐喻极大地影响了公众对AI系统中代理、风险和控制的认识([Lindgren
    2023](ch058.xhtml#ref-lindgren2023handbook))。这些认识反过来又影响了对AI的采用、监督和抵制，尤其是在教育、医疗保健和就业等领域，AI的部署直接与生活经验相交。
- en: From a systems perspective, public understanding is not an externality—it is
    part of the deployment context. Misinformation about how AI systems function can
    lead to overreliance, misplaced blame, or underutilization of safety mechanisms.
    Equally, a lack of understanding of model uncertainty, data bias, or decision
    boundaries can exacerbate the risks of automation-induced harm. For individuals
    whose jobs are impacted by AI, targeted efforts to build domain-specific literacy
    can also support reskilling and adaptation ([Ng et al. 2021](ch058.xhtml#ref-ng2021ai)).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，公众的理解不是外部性——它是部署背景的一部分。关于AI系统如何工作的错误信息可能导致过度依赖、错误的指责或安全机制的低利用率。同样，对模型不确定性、数据偏差或决策边界的缺乏理解可能会加剧自动化引起的伤害风险。对于受AI影响的个人，针对特定领域的素养建设努力也可以支持再培训和适应([Ng
    et al. 2021](ch058.xhtml#ref-ng2021ai))。
- en: 'AI literacy is not just about technical fluency. It is about building public
    confidence that the goals of system designers are aligned with societal welfare—and
    that those building AI systems are not removed from public values, but accountable
    to them. As Handlin observed in 1965: *“Even those who never acquire that understanding
    need assurance that there is a connection between the goals of science and their
    welfare, and above all, that the scientist is not a man altogether apart but one
    who shares some of their value.”*'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: AI素养不仅仅是技术流利。它关乎建立公众信心，即系统设计者的目标与社会福利相一致——并且那些构建AI系统的人不是脱离公众价值观的，而是对他们负责。正如Handlin在1965年观察到的那样：*“即使那些从未获得这种理解的人也需要确信，科学的目标与他们的福祉之间有联系，最重要的是，科学家不是完全脱离他们的人，而是与他们共享一些价值观的人。”*
- en: Fallacies and Pitfalls
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Responsible AI intersects technical engineering with complex ethical and social
    considerations, creating opportunities for misconceptions about the nature of
    bias, fairness, and accountability in machine learning systems. The appeal of
    technical solutions to ethical problems can obscure the deeper institutional and
    societal changes required to create truly responsible AI systems.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI将技术工程与复杂的伦理和社会考量相结合，从而在机器学习系统的偏差、公平和问责制性质方面产生误解的机会。对技术解决方案解决伦理问题的吸引力可能会掩盖创建真正负责任的AI系统所需的更深刻的制度和社会变革。
- en: '**Fallacy:** *Bias can be eliminated from AI systems through better algorithms
    and more data.*'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *通过更好的算法和更多数据，可以从AI系统中消除偏差。*'
- en: This misconception assumes that bias is a technical problem with purely technical
    solutions. Bias in AI systems often reflects deeper societal inequalities and
    historical injustices embedded in data collection processes, labeling decisions,
    and problem formulations. Even perfect algorithms trained on comprehensive datasets
    can perpetuate or amplify social biases if those biases are present in the underlying
    data or evaluation frameworks. Algorithmic fairness requires ongoing human judgment
    about values and trade-offs rather than one-time technical fixes. Effective bias
    mitigation involves continuous monitoring, stakeholder engagement, and institutional
    changes rather than relying solely on algorithmic interventions.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解假设偏见是一个纯粹的技术问题，需要纯粹的技术解决方案。AI系统中的偏见往往反映了数据收集过程、标注决策和问题表述中嵌入的更深层次的社会不平等和历史不公正。即使是在综合数据集上训练的完美算法，如果底层数据或评估框架中存在偏见，也可能持续或放大社会偏见。算法公平性需要关于价值观和权衡的持续人类判断，而不是一次性的技术修复。有效的偏见缓解涉及持续的监控、利益相关者参与和制度变革，而不是仅仅依赖算法干预。
- en: '**Pitfall:** *Treating explainability as an optional feature rather than a
    system requirement.*'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 将可解释性视为可选功能而不是系统要求。'
- en: Many teams view explainability as a nice-to-have capability that can be added
    after models are developed and deployed. This approach fails to account for how
    explainability requirements significantly shape model design, evaluation frameworks,
    and deployment strategies. Post-hoc explanation methods often provide misleading
    or incomplete insights that fail to support actual decision-making needs. High-stakes
    applications require explainability to be designed into the system architecture
    from the beginning, influencing choices about model complexity, feature engineering,
    and evaluation metrics rather than being retrofitted as an afterthought.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队将可解释性视为一个锦上添花的特性，可以在模型开发和部署后添加。这种方法未能考虑到可解释性要求如何显著影响模型设计、评估框架和部署策略。事后解释方法往往提供误导或不完整的见解，无法支持实际的决策需求。高风险应用需要从一开始就将可解释性设计到系统架构中，影响关于模型复杂性、特征工程和评估指标的选择，而不是作为事后考虑的补充。
- en: '**Fallacy:** *Ethical AI guidelines and principles automatically translate
    to responsible implementation.*'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** 伦理AI指南和原则自动转化为负责任的实施。'
- en: This belief assumes that establishing ethical principles or guidelines ensures
    responsible AI development without considering implementation challenges. High-level
    principles like fairness, transparency, and accountability often conflict with
    each other and with technical requirements in practice. Organizations that focus
    on principle articulation without investing in operationalization mechanisms often
    end up with ethical frameworks that have little impact on actual system behavior.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设，建立伦理原则或指南可以确保负责任的AI发展，而不考虑实施挑战。在实践中，高级原则如公平性、透明度和问责制往往相互冲突，并且与技术要求相冲突。那些只关注原则阐述而不投资于操作化机制的组织，最终往往得到对实际系统行为影响甚微的伦理框架。
- en: '**Pitfall:** *Assuming that responsible AI practices impose only costs without
    providing business value.*'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 假设负责任的AI实践只带来成本而不提供商业价值。'
- en: Teams often view responsible AI as regulatory compliance overhead that necessarily
    conflicts with performance and efficiency goals. This perspective misses the significant
    business value that responsible AI practices can provide through improved system
    reliability, enhanced user trust, reduced legal risk, and expanded market access.
    Responsible AI techniques can improve model generalization, reduce maintenance
    costs, and prevent costly failures in deployment. Organizations that treat responsibility
    as pure cost rather than strategic capability miss opportunities to build competitive
    advantages through trustworthy AI systems.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 团队通常将负责任的AI视为监管合规的额外负担，这必然与性能和效率目标相冲突。这种观点忽略了负责任的AI实践通过提高系统可靠性、增强用户信任、降低法律风险和扩大市场准入所能提供的重大商业价值。负责任的AI技术可以提高模型泛化能力，降低维护成本，并防止部署中的高昂失败。那些将责任视为纯粹的成本而不是战略能力的组织，错失了通过可信赖的AI系统建立竞争优势的机会。
- en: '**Pitfall:** *Implementing fairness and explainability features without considering
    their system-level performance and scalability implications.*'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** 在不考虑其系统级性能和可扩展性影响的情况下实施公平性和可解释性功能。'
- en: Many teams add fairness constraints or explainability methods to existing systems
    without analyzing how these features affect overall system architecture, performance,
    and maintainability. Real-time fairness monitoring can introduce significant computational
    overhead that degrades system responsiveness, while storing explanations for complex
    models can create substantial storage and bandwidth requirements. Effective responsible
    AI systems require careful co-design of fairness and explainability requirements
    with system architecture, considering trade-offs between responsible AI features
    and system performance from the initial design phase.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队在分析这些功能如何影响整体系统架构、性能和可维护性之前，就在现有系统中添加了公平性约束或可解释性方法。实时公平性监控可能会引入显著的计算开销，从而降低系统响应速度；而存储复杂模型的可解释性可能会产生大量的存储和带宽需求。有效的负责任AI系统需要仔细地与系统架构共同设计公平性和可解释性要求，从最初的设计阶段就考虑负责任AI功能和系统性能之间的权衡。
- en: Summary
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored responsible AI through four complementary perspectives
    that together define a comprehensive engineering approach to building trustworthy
    machine learning systems. The foundational principles of fairness, transparency,
    accountability, privacy, and safety establish what responsible AI systems should
    achieve, but these principles only become operational through integration with
    technical capabilities, sociotechnical dynamics, and implementation practices.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过四个互补的视角探讨了负责任的AI，这些视角共同定义了构建值得信赖的机器学习系统的全面工程方法。公平性、透明度、问责制、隐私和安全性的基础原则确立了负责任AI系统应实现的目标，但这些原则只有与技术能力、社会技术动态和实施实践相结合才能成为操作性的。
- en: The technical foundations we examined translate abstract principles into concrete
    system behaviors through bias detection algorithms, privacy preservation mechanisms,
    explainability frameworks, and robustness enhancements. However, the computational
    overhead analysis revealed that these techniques create significant equity considerations.
    Not all organizations can afford comprehensive responsible AI protections, potentially
    creating disparate access to ethical safeguards.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察的技术基础通过偏见检测算法、隐私保护机制、可解释性框架和鲁棒性增强将抽象原则转化为具体系统行为。然而，计算开销分析揭示了这些技术创造了重大的公平性考虑。并非所有组织都能承担全面的负责任AI保护，这可能会造成对道德保障的差异化访问。
- en: 'Yet technical correctness alone cannot guarantee beneficial outcomes. Sociotechnical
    dynamics shape whether capabilities translate into real-world impact: organizational
    incentives, human behavior, stakeholder values, and governance structures determine
    outcomes. Perfect bias detection algorithms are useless without organizational
    processes for acting on their findings; privacy preservation methods fail if users
    don’t understand their protections.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 仅技术正确性本身并不能保证有益的结果。社会技术动态决定了能力是否转化为现实世界的影响：组织激励、人类行为、利益相关者价值观和治理结构决定了结果。没有组织采取行动对其发现采取行动的完美偏见检测算法是无用的；如果用户不理解他们的保护，隐私保护方法也会失败。
- en: Implementation challenges further highlight the gap between principles and practice,
    showing how organizational structures, data constraints, competing objectives,
    and evaluation gaps prevent even well-intentioned responsible AI efforts from
    succeeding. The transformation of standalone fallacies into contextual warnings
    reinforces that responsible AI requires ongoing vigilance against common misconceptions
    rather than one-time technical fixes.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 实施挑战进一步突显了原则与实践之间的差距，展示了组织结构、数据约束、竞争目标和评估差距如何阻止甚至有良好意图的负责任AI努力取得成功。将孤立谬误转化为情境警告强化了负责任AI需要持续警惕常见误解，而不是一次性的技术修复。
- en: '**Key Takeaways**'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: '**Integration is essential**: Responsible AI emerges from alignment between
    principles, technical capabilities, sociotechnical dynamics, and implementation
    practices—none alone is sufficient'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整合至关重要**：负责任的AI源于原则、技术能力、社会技术动态和实施实践之间的协调一致——任何单一因素都不足以实现'
- en: '**Technical methods enable but don’t guarantee responsibility**: Bias detection,
    privacy preservation, and explainability tools provide necessary capabilities,
    but their effectiveness depends entirely on organizational and social context'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**技术方法能够实现但并不保证责任**：偏见检测、隐私保护和可解释性工具提供了必要的功能，但它们的有效性完全取决于组织和社交环境'
- en: '**Equity extends beyond algorithms**: Computational resource requirements create
    systematic barriers that determine who can access responsible AI protections,
    transforming ethics from individual system properties into collective social challenges'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平性超越算法**：计算资源需求创造了系统性障碍，决定了谁可以访问负责任的AI保护，将伦理从个体系统属性转变为集体社会挑战'
- en: '**Deployment context shapes possibility**: Cloud systems support comprehensive
    monitoring while TinyML devices require static validation—responsible AI must
    adapt to architectural constraints rather than imposing uniform requirements'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署环境塑造可能性**：云系统支持全面的监控，而TinyML设备则需要静态验证——负责任的AI必须适应架构限制，而不是强加统一的要求'
- en: '**Value conflicts require deliberation**: Fairness impossibility theorems demonstrate
    that competing principles cannot be reconciled algorithmically but require stakeholder
    engagement and explicit trade-off decisions'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值冲突需要深思熟虑**：公平性不可能定理表明，竞争性原则不能通过算法来调和，而需要利益相关者的参与和明确的权衡决策'
- en: As machine learning systems become increasingly embedded in critical social
    infrastructure, responsible AI frameworks establish essential foundations for
    trustworthy systems. However, responsibility extends beyond the algorithmic fairness,
    explainability, and safety concerns examined here. The computational demands of
    responsible AI techniques—requiring 15-30% more training resources, 50-1000x inference
    compute for explanations, and substantial monitoring infrastructure—raise critical
    questions about environmental impact and resource consumption.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统越来越多地嵌入关键社会基础设施，负责任的AI框架为可信赖的系统建立了基本的基础。然而，责任不仅限于这里所考察的算法公平性、可解释性和安全性问题。负责任的AI技术的计算需求——需要15-30%更多的训练资源、50-1000倍的解释推理计算和大量的监控基础设施——对环境影响和资源消耗提出了关键问题。
- en: The next chapter explores how responsibility encompasses sustainability, examining
    the carbon footprint of training large models, the environmental costs of datacenter
    operations, and the imperative to develop energy-efficient AI systems. Just as
    responsible AI asks whether our systems treat people fairly, sustainable AI asks
    whether our systems treat the planet responsibly. The principles of trustworthy
    systems ultimately require balancing technical performance, social responsibility,
    and environmental stewardship—ensuring AI development enhances human welfare without
    compromising our collective future.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章探讨了责任如何涵盖可持续性，考察了训练大型模型的碳足迹、数据中心运营的环境成本以及开发节能AI系统的必要性。正如负责任的AI询问我们的系统是否公平对待人们一样，可持续AI询问我们的系统是否负责任地对待地球。可信赖系统的原则最终需要平衡技术性能、社会责任和环境管理——确保AI发展增强人类福祉，而不损害我们的共同未来。
- en: '* * *'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
