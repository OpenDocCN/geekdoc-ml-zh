- en: DL Primer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习入门
- en: '*DALL·E 3 Prompt: A rectangular illustration divided into two halves on a clean
    white background. The left side features a detailed and colorful depiction of
    a biological neural network, showing interconnected neurons with glowing synapses
    and dendrites. The right side displays a sleek and modern artificial neural network,
    represented by a grid of interconnected nodes and edges resembling a digital circuit.
    The transition between the two sides is distinct but harmonious, with each half
    clearly illustrating its respective theme: biological on the left and artificial
    on the right.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：一个在干净白色背景上分为两半的矩形插图。左侧详细且多彩地描绘了一个生物神经网络，展示了相互连接的神经元、发光的突触和树突。右侧展示了一个时尚现代的人工神经网络，由相互连接的节点和边组成的网格表示，类似于数字电路。两边的过渡既明显又和谐，每半部分清楚地说明了其各自的主题：左侧是生物的，右侧是人工的.*'
- en: '![](../media/file32.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file32.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why do deep learning systems engineers need deep mathematical understanding
    of neural network operations rather than treating them as black-box components?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么深度学习系统工程师需要对神经网络操作有深入数学理解，而不是将它们视为黑盒组件？*'
- en: 'Modern deep learning systems rely on neural networks as their core computational
    engine, but successful engineering requires understanding the mathematics that
    governs their behavior. Neural network mathematics determines memory requirements,
    computational complexity, and optimization landscapes that directly impact system
    design decisions. Without grasping concepts like gradient flow, activation functions,
    and backpropagation mechanics, engineers cannot predict system behavior, diagnose
    training failures, or optimize resource allocation. Each mathematical operation
    translates to specific hardware requirements: matrix multiplication demands gigabytes
    per second of memory bandwidth, while activation function choices determine mobile
    processor compatibility. Understanding these operations transforms neural networks
    from opaque components into predictable, engineerable systems.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习系统依赖于神经网络作为其核心计算引擎，但成功的工程需要理解控制其行为的数学。神经网络数学决定了内存需求、计算复杂度和优化景观，这些直接影响系统设计决策。如果不能掌握梯度流、激活函数和反向传播机制等概念，工程师无法预测系统行为、诊断训练失败或优化资源分配。每个数学运算都对应特定的硬件要求：矩阵乘法需要每秒数GB的内存带宽，而激活函数的选择决定了移动处理器的兼容性。理解这些操作将神经网络从不可见组件转变为可预测、可工程化的系统。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Trace AI evolution from rule-based systems to neural networks and identify driving
    engineering challenges
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪人工智能从基于规则的系统到神经网络的演变，并确定驱动工程挑战
- en: Analyze neural network operations (matrix multiplication, activations, gradients)
    and their hardware implications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析神经网络操作（矩阵乘法、激活、梯度）及其硬件影响
- en: Design neural network architectures by selecting appropriate layer configurations,
    activation functions, and connection patterns based on computational constraints
    and task requirements
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择适当的层配置、激活函数和连接模式，根据计算约束和任务需求设计神经网络架构
- en: Implement forward propagation through multi-layer networks, computing weighted
    sums and applying activation functions to transform raw inputs into hierarchical
    feature representations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过多层网络实现前向传播，计算加权总和并应用激活函数将原始输入转换为层次特征表示
- en: Execute backpropagation algorithms to compute gradients and update network weights,
    demonstrating how prediction errors propagate backward through network layers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行反向传播算法来计算梯度并更新网络权重，展示预测误差如何通过网络层反向传播
- en: Compare training and inference operational phases, analyzing their distinct
    computational demands, resource requirements, and optimization strategies for
    different deployment scenarios
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较训练和推理操作阶段，分析它们不同的计算需求、资源需求和针对不同部署场景的优化策略
- en: Evaluate loss functions and optimization algorithms, explaining how these choices
    affect training dynamics, convergence behavior, and final model performance
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估损失函数和优化算法，解释这些选择如何影响训练动态、收敛行为和最终模型性能
- en: Assess the deep learning pipeline to identify computational bottlenecks and
    optimization opportunities
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估深度学习管道以识别计算瓶颈和优化机会
- en: Deep Learning Systems Engineering Foundation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习系统工程基础
- en: 'Consider the seemingly simple task of identifying cats in photographs. Using
    traditional programming, you would need to write explicit rules: look for triangular
    ears, check for whiskers, verify the presence of four legs, examine fur patterns,
    and handle countless variations in lighting, angles, poses, and breeds. Each edge
    case demands additional rules, creating increasingly complex decision trees that
    still fail when encountering unexpected variations. This limitation, the impossibility
    of manually encoding all patterns for complex real-world problems, drove the evolution
    from rule-based programming to machine learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到看似简单的任务，比如在照片中识别猫。使用传统的编程方法，你需要编写显式规则：寻找三角形的耳朵，检查是否有胡须，验证是否有四条腿，检查毛皮图案，并处理无数的光照、角度、姿势和品种的变化。每个边缘情况都需要额外的规则，从而创建越来越复杂的决策树，但仍然会在遇到意外变化时失败。这种限制，即无法手动编码复杂现实世界问题的所有模式，推动了从基于规则的编程到机器学习的演变。
- en: Deep learning represents the culmination of this evolution, solving the cat
    identification problem by learning directly from millions of cat and non-cat images.
    Instead of programming rules, we provide examples and let the system discover
    patterns automatically. This shift from explicit programming to learned representations
    has implications for how we design and engineer computational systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习代表了这一演变的顶峰，通过直接从数百万只猫和非猫图像中学习来解决猫识别问题。我们不是编写规则，而是提供示例，让系统自动发现模式。这种从显式编程到学习表示的转变对我们设计和工程计算系统的方式产生了影响。
- en: Deep learning systems present an engineering challenge that distinguishes them
    from conventional software. While traditional systems execute deterministic algorithms
    based on explicit rules, deep learning systems operate through mathematical processes
    that learn data representations. This shift requires understanding the mathematical
    operations underlying these systems for engineers responsible for their design,
    implementation, and maintenance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统提出了一个工程挑战，这使得它们与传统软件区别开来。虽然传统系统执行基于显式规则的确定性算法，但深度学习系统通过数学过程学习数据表示来运行。这种转变要求工程师理解这些系统背后的数学操作，以便于它们的设计、实施和维护。
- en: The engineering implications of this mathematical complexity are important.
    When production systems exhibit degraded performance characteristics, conventional
    debugging methodologies prove inadequate. Performance anomalies may originate
    from gradient instabilities[1](#fn1) during optimization, numerical precision
    limitations in activation computations, or memory access patterns inherent to
    tensor operations[2](#fn2). Without foundational mathematical literacy, systems
    engineers cannot effectively differentiate between implementation failures and
    algorithmic constraints, accurately predict computational resource requirements,
    or systematically optimize performance bottlenecks that emerge from the underlying
    mathematical operations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学复杂性的工程影响非常重要。当生产系统表现出降低的性能特征时，传统的调试方法证明是不够的。性能异常可能源于优化过程中的梯度不稳定性[1](#fn1)，激活计算中的数值精度限制，或者张量操作固有的内存访问模式[2](#fn2)。没有基础数学素养，系统工程师无法有效地区分实现失败和算法约束，准确预测计算资源需求，或系统地优化由底层数学操作产生的性能瓶颈。
- en: '***Deep Learning*** is a subfield of machine learning that employs *neural
    networks with multiple layers* to *automatically learn hierarchical representations*
    from data, eliminating the need for *explicit feature engineering*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是机器学习的一个子领域，它使用具有多层**神经网络**来自动从数据中学习层次化表示，从而消除了**显式特征工程**的需求。'
- en: Deep learning has become the dominant approach in modern artificial intelligence
    by addressing the limitations that constrained earlier methods. While rule-based
    systems required exhaustive manual specification of decision pathways and conventional
    machine learning techniques demanded feature engineering expertise, neural network
    architectures discover pattern representations directly from raw data. This capability
    enables applications previously considered intractable, though it introduces computational
    complexity that requires reconsideration of system architecture design principles.
    As illustrated in [Figure 3.1](ch009.xhtml#fig-ai-ml-dl), neural networks form
    a foundational component within the broader hierarchy of machine learning and
    artificial intelligence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通过解决限制早期方法的局限性，已成为现代人工智能的主导方法。虽然基于规则的系统需要详尽的手动指定决策路径，而传统的机器学习技术需要特征工程专业知识，但神经网络架构可以直接从原始数据中发现模式表示。这种能力使得以前被认为难以处理的应用成为可能，尽管它引入了需要重新考虑系统架构设计原则的计算复杂性。如图3.1[图](ch009.xhtml#fig-ai-ml-dl)所示，神经网络在机器学习和人工智能更广泛层次结构中构成了一个基础组成部分。
- en: '![](../media/file33.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file33.png)'
- en: 'Figure 3.1: **AI Hierarchy**: Neural networks form a core component of deep
    learning within machine learning and artificial intelligence by modeling patterns
    in large datasets. Machine learning algorithms enable systems to learn from data
    as a subset of the broader AI field.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：**AI层次结构**：神经网络通过在大数据集中建模模式，成为机器学习和人工智能领域深度学习的一个核心组成部分。机器学习算法使系统能够作为更广泛人工智能领域的一部分从数据中学习。
- en: The transition to neural network architectures represents a shift that goes
    beyond algorithmic evolution, requiring reconceptualization of system design methods.
    Neural networks execute computations through massively parallel matrix operations
    that work well with specialized hardware architectures. These systems learn through
    iterative optimization processes that generate distinctive memory access patterns
    and impose strict numerical precision requirements. The computational characteristics
    of inference differ substantially from training phases, requiring distinct optimization
    strategies for each operational mode.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 向神经网络架构的转变代表了一种超越算法演变的转变，需要重新概念化系统设计方法。神经网络通过大量并行矩阵运算执行计算，这些运算与专用硬件架构配合良好。这些系统通过迭代优化过程学习，这些过程产生独特的内存访问模式并施加严格的数值精度要求。推理的计算特征与训练阶段大不相同，需要为每种操作模式制定不同的优化策略。
- en: 'This chapter establishes the mathematical literacy needed for engineering neural
    network systems effectively. Rather than treating these architectures as opaque
    abstractions, we examine the mathematical operations that determine system behavior
    and performance. We investigate how biological neural processes inspired artificial
    neuron models, analyze how individual neurons compose into complex network topologies,
    and explore how these networks acquire knowledge through mathematical optimization.
    Each concept connects directly to practical system engineering considerations:
    understanding matrix multiplication operations illuminates memory bandwidth requirements,
    comprehending gradient computation mechanisms explains numerical precision constraints,
    and recognizing optimization dynamics informs resource allocation decisions.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章建立了有效工程神经网络系统所需的数学素养。我们不是将这些架构视为不透明的抽象，而是检查决定系统行为和性能的数学运算。我们研究生物神经网络过程如何启发人工神经元模型，分析单个神经元如何组成复杂的网络拓扑，以及这些网络如何通过数学优化获取知识。每个概念都直接与实际系统工程考虑相关：理解矩阵乘法运算可以阐明内存带宽需求，理解梯度计算机制可以解释数值精度约束，而识别优化动态可以指导资源分配决策。
- en: We begin by examining how artificial intelligence methods evolved from explicit
    rule-based programming to adaptive learning systems. We then investigate the biological
    neural processes that inspired artificial neuron models, establish the mathematical
    framework governing neural network operations, and analyze the optimization processes
    that enable these systems to extract patterns from complex datasets. Throughout
    this exploration, we focus on the system engineering implications of each mathematical
    principle, constructing the theoretical foundation needed for designing, implementing,
    and optimizing production-scale deep learning systems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考察人工智能方法是如何从基于规则的编程发展到自适应学习系统的。然后，我们研究启发人工神经元模型的生物神经网络过程，建立控制神经网络操作的数学框架，并分析使这些系统能够从复杂数据集中提取模式的优化过程。在整个探索过程中，我们关注每个数学原理的系统工程意义，构建设计、实施和优化生产规模深度学习系统所需的理论基础。
- en: Upon completion of this chapter, students will understand neural networks not
    as opaque algorithmic constructs, but as engineerable computational systems whose
    mathematical operations provide direct guidance for their practical implementation
    and operational deployment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本章学习后，学生将不再将神经网络视为晦涩的算法结构，而是将其视为可工程化的计算系统，其数学运算为其实际实施和操作部署提供直接指导。
- en: Evolution of ML Paradigms
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习范式的演变
- en: To understand why deep learning emerged as the dominant approach requiring specialized
    computational infrastructure, we examine how AI methods evolved over time. The
    current era of AI represents the latest stage in evolution from rule-based programming
    through classical machine learning to modern neural networks. Understanding this
    progression reveals how each approach builds upon and addresses the limitations
    of its predecessors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么深度学习成为需要专门计算基础设施的主导方法，我们考察了人工智能方法随时间的发展。当前的AI时代代表了从基于规则的编程通过经典机器学习到现代神经网络的最新进化阶段。理解这一进展揭示了每种方法是如何建立在前辈的基础上并解决其局限性的。
- en: Traditional Rule-Based Programming Limitations
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统基于规则的编程局限性
- en: 'Traditional programming requires developers to explicitly define rules that
    tell computers how to process inputs and produce outputs. Consider a simple game
    like Breakout[3](#fn3), shown in [Figure 3.2](ch009.xhtml#fig-breakout). The program
    needs explicit rules for every interaction: when the ball hits a brick, the code
    must specify that the brick should be removed and the ball’s direction should
    be reversed. While this approach works effectively for games with clear physics
    and limited states, it demonstrates a limitation of rule based systems.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统编程要求开发者明确定义规则，告诉计算机如何处理输入并产生输出。考虑一个简单的游戏，如Breakout[3]，如图3.2[2]所示。程序需要对每个交互进行明确的规则定义：当球击中砖块时，代码必须指定砖块应该被移除，球的运动方向应该反转。虽然这种方法对于具有明确物理和有限状态的游戏是有效的，但它展示了基于规则系统的局限性。
- en: '![](../media/file34.svg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file34.svg)'
- en: 'Figure 3.2: **Rule-Based System**: Traditional programming relies on explicitly
    defined rules to map inputs to outputs, limiting adaptability to complex or uncertain
    environments as every possible scenario must be anticipated and coded. This approach
    contrasts with deep learning, where systems learn patterns from data instead of
    relying on pre-programmed logic.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：**基于规则的系统**：传统编程依赖于明确定义的规则来将输入映射到输出，这限制了其在复杂或不确定环境中的适应性，因为必须预测并编码所有可能的场景。这种方法与深度学习形成对比，在深度学习中，系统从数据中学习模式，而不是依赖于预编程的逻辑。
- en: Beyond individual applications, this rule based paradigm extends to all traditional
    programming, as illustrated in [Figure 3.3](ch009.xhtml#fig-traditional). The
    program takes both rules for processing and input data to produce outputs. Early
    artificial intelligence research explored whether this approach could scale to
    solve complex problems by encoding sufficient rules to capture intelligent behavior.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅局限于单个应用，这种基于规则的范式扩展到了所有传统编程，如图3.3[1]所示。程序需要同时处理规则和输入数据以产生输出。早期的人工智能研究探讨了这种方法是否可以通过编码足够的规则来捕捉智能行为，从而扩展到解决复杂问题。
- en: '![](../media/file35.svg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file35.svg)'
- en: 'Figure 3.3: **Rule-Based Programming**: Traditional programs operate on data
    using explicitly defined rules, forming the basis for early AI systems but lacking
    the adaptability of modern machine learning approaches. This approach contrasts
    with deep learning, where the system infers rules from examples rather than relying
    on pre-programmed logic.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：**基于规则的编程**：传统程序通过明确定义的规则操作数据，构成了早期人工智能系统的基础，但缺乏现代机器学习方法的适应性。这种方法与深度学习形成对比，在深度学习中，系统从示例中推断规则，而不是依赖于预编程的逻辑。
- en: 'Despite their apparent simplicity, rule-based limitations become evident with
    complex real-world tasks. Recognizing human activities ([Figure 3.4](ch009.xhtml#fig-activity-rules))
    illustrates this challenge: classifying movement below 4 mph as walking seems
    straightforward until real-world complexity emerges. Speed variations, transitions
    between activities, and boundary cases each demand additional rules, creating
    unwieldy decision trees. Computer vision tasks compound these difficulties: detecting
    cats requires rules about ears, whiskers, and body shapes, while accounting for
    viewing angles, lighting, occlusions, and natural variations. Early systems achieved
    success only in controlled environments with well-defined constraints.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们表面上看起来很简单，但在复杂的现实世界任务中，基于规则的局限性变得明显。识别人类活动（[图3.4](ch009.xhtml#fig-activity-rules)）说明了这一挑战：将低于4英里/小时的速度归类为行走似乎很简单，直到现实世界的复杂性出现。速度变化、活动之间的转换和边界情况都需要额外的规则，从而产生难以管理的决策树。计算机视觉任务加剧了这些困难：检测猫需要关于耳朵、胡须和身体形状的规则，同时还要考虑观察角度、光照、遮挡和自然变化。早期系统只在具有明确约束的受控环境中取得了成功。
- en: '![](../media/file36.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file36.png)'
- en: 'Figure 3.4: **Rule-Based Programming**: Traditional programs rely on explicitly
    defined rules to operate on data, forming the basis for early AI systems but lacking
    adaptability in complex tasks.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：**基于规则的编程**：传统程序依赖于明确定义的规则来操作数据，构成了早期人工智能系统的基础，但在复杂任务中缺乏适应性。
- en: 'Recognizing these limitations, the knowledge engineering approach that characterized
    artificial intelligence research in the 1970s and 1980s attempted to systematize
    rule creation. Expert systems[4](#fn4) encoded domain knowledge as explicit rules,
    showing promise in specific domains with well defined parameters but struggling
    with tasks humans perform naturally, such as object recognition, speech understanding,
    or natural language interpretation. These limitations highlighted a challenge:
    many aspects of intelligent behavior rely on implicit knowledge that resists explicit
    rule based representation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这些局限性，20世纪70年代和80年代人工智能研究中的知识工程方法试图系统地创建规则。专家系统[4](#fn4)将领域知识编码为明确规则，在具有明确参数的特定领域显示出希望，但在人类自然执行的任务，如物体识别、语音理解或自然语言解释方面却遇到了困难。这些局限性凸显了一个挑战：许多智能行为的方面依赖于难以用基于规则的显式表示的隐式知识。
- en: Classical Machine Learning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典机器学习
- en: 'Confronting the scalability barriers of rule based systems, researchers began
    exploring approaches that could learn from data. Machine learning offered a promising
    direction: instead of writing rules for every situation, researchers could write
    programs that identified patterns in examples. However, the success of these methods
    still depended heavily on human insight to define relevant patterns, a process
    known as feature engineering.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 面对基于规则的系统的可扩展性障碍，研究人员开始探索可以从数据中学习的方法。机器学习提供了一个有希望的方向：而不是为每种情况编写规则，研究人员可以编写识别示例中模式的程序。然而，这些方法的成功仍然在很大程度上依赖于人类洞察力来定义相关模式，这个过程被称为特征工程。
- en: 'This approach introduced feature engineering: transforming raw data into representations
    that expose patterns to learning algorithms. The Histogram of Oriented Gradients
    (HOG) ([Dalal and Triggs, n.d.](ch058.xhtml#ref-dalal2005histograms))[5](#fn5)
    method ([Figure 3.5](ch009.xhtml#fig-hog)) exemplifies this approach, identifying
    edges where brightness changes sharply, dividing images into cells, and measuring
    edge orientations within each cell. This transforms raw pixels into shape descriptors
    robust to lighting variations and small positional changes.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法引入了特征工程：将原始数据转换为学习算法可以揭示模式的表示。方向梯度直方图（HOG）方法（[Dalal和Triggs，未注明日期](ch058.xhtml#ref-dalal2005histograms))[5](#fn5)（[图3.5](ch009.xhtml#fig-hog)）是这种方法的例证，它识别亮度急剧变化的边缘，将图像划分为单元格，并测量每个单元格内的边缘方向。这把原始像素转换成对光照变化和微小位置变化具有鲁棒性的形状描述符。
- en: '![](../media/file37.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file37.png)'
- en: 'Figure 3.5: **HOG Method**: Identifies edges in images to create a histogram
    of gradients, transforming pixel values into shape descriptors that are invariant
    to lighting changes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：**HOG方法**：通过识别图像中的边缘来创建梯度直方图，将像素值转换为形状描述符，这些描述符对光照变化不变。
- en: Complementary methods like SIFT ([Lowe 1999](ch058.xhtml#ref-lowe1999object))[6](#fn6)
    (Scale-Invariant Feature Transform) and Gabor filters[7](#fn7) captured different
    visual patterns—SIFT detected keypoints stable across scale and orientation changes,
    while Gabor filters identified textures and frequencies. Each encoded domain expertise
    about visual pattern recognition.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 补充方法如SIFT ([Lowe 1999](ch058.xhtml#ref-lowe1999object))[6](#fn6)（尺度不变特征变换）和Gabor滤波器[7](#fn7)捕捉了不同的视觉模式——SIFT检测出在不同尺度和方向变化中保持稳定的特征点，而Gabor滤波器识别纹理和频率。每个都编码了关于视觉模式识别的领域专业知识。
- en: These engineering efforts enabled advances in computer vision during the 2000s.
    Systems could now recognize objects with some robustness to real world variations,
    leading to applications in face detection, pedestrian detection, and object recognition.
    Despite these successes, the approach had limitations. Experts needed to carefully
    design feature extractors for each new problem, and the resulting features might
    miss important patterns that were not anticipated in their design.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工程努力在2000年代推动了计算机视觉的进步。系统现在能够以一定的鲁棒性识别对象，以应对现实世界的各种变化，从而在人脸检测、行人检测和物体识别等领域得到应用。尽管取得了这些成功，但这种方法也有局限性。专家需要为每个新问题仔细设计特征提取器，而结果的特征可能会错过在设计时未预料到的重要模式。
- en: 'Deep Learning: Automatic Pattern Discovery'
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习：自动模式发现
- en: Neural networks represent a shift in how we approach problem solving with computers,
    establishing a new programming approach that learns from data rather than following
    explicit rules. This shift becomes particularly evident when considering tasks
    like computer vision, specifically identifying objects in images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络代表了我们在计算机上解决问题的方法的一次转变，确立了一种新的编程方法，它从数据中学习而不是遵循明确的规则。这种转变在考虑诸如计算机视觉等任务时尤其明显，特别是识别图像中的对象。
- en: Deep learning differs by learning directly from raw data. Traditional programming,
    as we saw earlier in [Figure 3.3](ch009.xhtml#fig-traditional), required both
    rules and data as inputs to produce answers. Machine learning inverts this relationship,
    as shown in [Figure 3.6](ch009.xhtml#fig-deeplearning). Instead of writing rules,
    we provide examples (data) and their correct answers to discover the underlying
    rules automatically. This shift eliminates the need for humans to specify what
    patterns are important.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通过直接从原始数据中学习而有所不同。如我们之前在[图3.3](ch009.xhtml#fig-traditional)中看到的，传统的编程需要规则和数据作为输入来产生答案。机器学习颠倒了这种关系，如[图3.6](ch009.xhtml#fig-deeplearning)所示。我们不是编写规则，而是提供示例（数据）及其正确答案，以自动发现潜在的规则。这种转变消除了人类指定哪些模式重要的需求。
- en: '![](../media/file38.svg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file38.svg)'
- en: 'Figure 3.6: **Data-Driven Rule Discovery**: Deep learning models learn patterns
    and relationships directly from data, eliminating the need for manually specified
    rules and enabling automated feature extraction from raw inputs. This contrasts
    with traditional programming, where both rules and data are required to generate
    outputs, and classical machine learning, where rules are inferred from labeled
    data.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：**数据驱动规则发现**：深度学习模型直接从数据中学习模式和关系，消除了手动指定规则的需求，并能够从原始输入中自动提取特征。这与传统编程不同，在传统编程中，规则和数据都是生成输出的必要条件，以及与经典机器学习不同，在经典机器学习中，规则是从标记数据中推断出来的。
- en: Through this automated process, the system discovers these patterns from examples.
    When shown millions of images of cats, the system learns to identify increasingly
    complex visual patterns, from simple edges to more complex combinations that make
    up cat like features. This parallels how human visual systems operate, building
    understanding from basic visual elements to complex objects.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个自动化过程，系统从示例中发现了这些模式。当展示数百万张猫的图片时，系统学会识别越来越复杂的视觉模式，从简单的边缘到更复杂的组合，这些组合构成了猫的特征。这与人眼视觉系统的运作方式相平行，从基本的视觉元素到复杂物体构建理解。
- en: 'Building on this hierarchical learning principle, deep networks learn hierarchical
    representations where complex patterns emerge from simpler ones. Each layer learns
    increasingly abstract features: edges → shapes → objects → concepts. Deeper networks
    can express exponentially more functions with only polynomially more parameters,
    which is why “deep” matters theoretically. The compositionality principle explains
    why deep learning works: complex real-world patterns often have hierarchical structure
    that matches the network’s representational bias.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种分层学习原理，深度网络学习分层表示，其中复杂模式从简单模式中产生。每一层学习越来越抽象的特征：边缘 → 形状 → 对象 → 概念。更深的网络可以用多项式级更多的参数表达指数级更多的函数，这就是为什么“深度”在理论上很重要的原因。组合性原则解释了为什么深度学习有效：复杂的现实世界模式通常具有与网络表示偏差相匹配的分层结构。
- en: 'This hierarchical structure creates an advantage: unlike traditional approaches
    where performance plateaus, deep learning models continue improving with additional
    data (recognizing more variations) and computation (discovering subtler patterns).
    This scalability drove dramatic performance gains. Image recognition accuracy
    improved from 74% in 2012 to over 95% today[8](#fn8).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层结构带来了一种优势：与传统方法中性能达到平台期不同，深度学习模型随着额外数据的增加（识别更多变化）和计算的进行（发现更细微的模式）而持续改进。这种可扩展性推动了性能的显著提升。图像识别的准确性从2012年的74%提高到了今天的95%以上[8](#fn8)。
- en: 'Neural network performance follows predictable scaling relationships that directly
    impact system design. These scaling laws explain why modern AI systems prioritize
    larger models over longer training: GPT-4 has ~1000× more parameters than GPT-1
    but uses similar training time. Memory bandwidth and storage capacity consequently
    become the primary constraints rather than raw computational power. The detailed
    mathematical formulations of these scaling laws and their quantitative analysis
    are covered in [Chapter 8](ch014.xhtml#sec-ai-training), while [Chapter 10](ch016.xhtml#sec-model-optimizations)
    explores their practical implementation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的性能遵循可预测的缩放关系，这些关系直接影响系统设计。这些缩放定律解释了为什么现代人工智能系统优先考虑更大的模型而不是更长的训练时间：GPT-4的参数比GPT-1多约1000倍，但训练时间却相似。因此，内存带宽和存储容量成为主要的限制因素，而不是原始的计算能力。这些缩放定律的详细数学公式及其定量分析在[第8章](ch014.xhtml#sec-ai-training)中有所介绍，而[第10章](ch016.xhtml#sec-model-optimizations)探讨了它们的实际应用。
- en: Beyond performance improvements, this approach has implications for AI system
    construction. Deep learning’s ability to learn directly from raw data eliminates
    the need for manual feature engineering while introducing new demands. Advanced
    infrastructure is required to handle massive datasets, powerful computers to process
    this data, and specialized hardware to perform complex mathematical calculations
    efficiently. The computational requirements of deep learning have driven the development
    of specialized computer chips optimized for these calculations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能提升之外，这种方法对人工智能系统的构建也有影响。深度学习直接从原始数据中学习的能力消除了手动特征工程的需求，同时也带来了新的要求。需要高级基础设施来处理大量数据集，强大的计算机来处理这些数据，以及专门的硬件来高效地执行复杂的数学计算。深度学习的计算需求推动了专用计算机芯片的发展，这些芯片针对这些计算进行了优化。
- en: The empirical evidence strongly supports these claims. The success of deep learning
    in computer vision exemplifies how this approach, when given sufficient data and
    computation, can surpass traditional methods. This pattern has repeated across
    many domains, from speech recognition to game playing, establishing deep learning
    as a transformative approach to artificial intelligence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实证证据强烈支持这些说法。深度学习在计算机视觉中的成功证明了当给予足够的数据和计算时，这种方法如何超越传统方法。这种模式在许多领域重复出现，从语音识别到游戏，确立了深度学习作为人工智能变革性方法的地位。
- en: 'However, this transformation comes with trade-offs: deep learning’s computational
    demands reshape system requirements. Understanding these requirements provides
    context for the technical details of neural networks that follow.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种转变也伴随着权衡：深度学习的计算需求重塑了系统需求。了解这些需求为以下关于神经网络的技术细节提供了背景。
- en: Computational Infrastructure Requirements
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算基础设施需求
- en: The progression from traditional programming to deep learning represents not
    just a shift in how we solve problems, but a transformation in computing system
    requirements that directly impacts every aspect of ML systems design. This transformation
    becomes important when we consider the full spectrum of ML systems, from massive
    cloud deployments to resource constrained Tiny ML devices.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统编程到深度学习的转变不仅代表了我们在解决问题方式上的转变，而且代表了计算系统需求的变化，这直接影响着机器学习系统设计的各个方面。当我们考虑机器学习系统的完整范围时，这种转变变得尤为重要，从大规模云部署到资源受限的Tiny
    ML设备。
- en: Traditional programs follow predictable patterns. They execute sequential instructions,
    access memory in regular patterns, and use computing resources in well understood
    ways. A typical rule based image processing system might scan through pixels methodically,
    applying fixed operations with modest and predictable computational and memory
    requirements. These characteristics made traditional programs relatively straightforward
    to deploy across different computing platforms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 传统程序遵循可预测的模式。它们执行顺序指令，以常规模式访问内存，并以理解良好的方式使用计算资源。一个典型的基于规则的图像处理系统可能会系统地扫描像素，应用固定操作，具有适度和可预测的计算和内存需求。这些特征使得传统程序在不同计算平台上的部署相对简单。
- en: 'Table 3.1: **System Resource Evolution**: Programming paradigms shift system
    demands from sequential computation to structured parallelism with feature engineering,
    and finally to massive matrix operations and complex memory hierarchies in deep
    learning. This table clarifies how deep learning fundamentally alters system requirements
    compared to traditional programming and machine learning with engineered features,
    impacting computation and memory access patterns.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1：**系统资源演变**：编程范式从顺序计算转向具有特征工程的有序并行，最终转向深度学习中的大规模矩阵运算和复杂内存层次结构。此表阐明了深度学习如何从根本上改变系统需求，与传统编程和具有工程特征的机器学习相比，影响计算和内存访问模式。
- en: '| **System Aspect** | **Traditional Programming** | **ML with Features** |
    **Deep Learning** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **系统方面** | **传统编程** | **具有特征的机器学习** | **深度学习** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Computation** | Sequential, predictable paths | Structured parallel operations
    | Massive matrix parallelism |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **计算** | 顺序、可预测的路径 | 结构化并行操作 | 大规模矩阵并行 |'
- en: '| **Memory Access** | Small, predictable patterns | Medium, batch-oriented
    | Large, complex hierarchical patterns |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **内存访问** | 小型、可预测的模式 | 中型、批量导向 | 大型、复杂分层模式 |'
- en: '| **Data Movement** | Simple input/output flows | Structured batch processing
    | Intensive cross-system movement |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **数据移动** | 简单的输入/输出流 | 结构化批量处理 | 交叉系统间的密集移动 |'
- en: '| **Hardware Needs** | CPU-centric | CPU with vector units | Specialized accelerators
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **硬件需求** | 以CPU为中心 | 带有向量单元的CPU | 专用加速器 |'
- en: '| **Resource Scaling** | Fixed requirements | Linear with data size | Exponential
    with complexity |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **资源扩展** | 固定需求 | 与数据大小成线性关系 | 与复杂性成指数关系 |'
- en: As we moved toward data-driven approaches, classical machine learning with engineered
    features introduced new complexities. Feature extraction algorithms required more
    intensive computation and structured data movement. The HOG feature extractor
    discussed earlier, for instance, requires multiple passes over image data, computing
    gradients and constructing histograms. While this increased both computational
    demands and memory complexity, the resource requirements remained predictable
    and scalable across platforms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们向数据驱动方法迈进，具有工程特征的经典机器学习引入了新的复杂性。特征提取算法需要更密集的计算和结构化数据移动。例如，前面讨论过的HOG特征提取器需要多次遍历图像数据，计算梯度并构建直方图。虽然这增加了计算需求和内存复杂性，但资源需求仍然可预测且可扩展。
- en: Deep learning, however, reshapes system requirements across multiple dimensions,
    as illustrated in [Table 3.1](ch009.xhtml#tbl-evolution). Understanding these
    evolutionary changes is important as differences manifest in several ways, with
    implications across the entire ML systems spectrum.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度学习在多个维度上重塑了系统需求，如[表3.1](ch009.xhtml#tbl-evolution)所示。理解这些演变变化很重要，因为差异以多种方式显现，影响整个机器学习系统范围。
- en: Parallel Matrix Operation Patterns
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行矩阵运算模式
- en: The computational paradigm shift becomes immediately apparent when comparing
    these approaches. Traditional programs follow sequential logic flows. In stark
    contrast, deep learning requires massive parallel operations on matrices. This
    shift explains why conventional CPUs, designed for sequential processing, prove
    inefficient for neural network computations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较这些方法时，计算范式转变立即变得明显。传统的程序遵循顺序逻辑流程。相比之下，深度学习需要在矩阵上进行大规模并行操作。这种转变解释了为什么专为顺序处理设计的传统CPU在神经网络计算中效率低下。
- en: 'This parallel computational model creates new bottlenecks. The fundamental
    challenge is the memory wall: while computational capacity can be increased by
    adding more processing units, memory bandwidth to feed those units doesn’t scale
    as favorably[9](#fn9). Modern accelerators address this through hierarchical memory
    systems with multiple cache levels and specialized memory architectures that enable
    data reuse. The key insight is that keeping data close to where it’s processed—in
    faster, smaller caches rather than slower, larger main memory—dramatically improves
    performance.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种并行计算模型创造了新的瓶颈。基本挑战是内存墙：虽然可以通过添加更多处理单元来增加计算能力，但为这些单元提供内存带宽的增长并不那么有利[9](#fn9)。现代加速器通过具有多个缓存级别和专门内存架构的分层内存系统来解决这个问题，这些架构能够实现数据重用。关键洞察是保持数据靠近其处理位置——在更快、更小的缓存中，而不是在较慢、较大的主内存中——可以显著提高性能。
- en: These memory hierarchy challenges explain why neural network accelerators focus
    on maximizing data reuse. Rather than repeatedly fetching the same weights from
    slow main memory, successful designs keep frequently accessed data in fast local
    storage and carefully schedule operations to minimize data movement. The detailed
    quantitative analysis of these memory systems and their performance characteristics
    is covered in [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内存层次结构的挑战解释了为什么神经网络加速器专注于最大化数据重用。成功的架构不是反复从慢速的主内存中获取相同的权重，而是将频繁访问的数据保存在快速的本地存储中，并仔细安排操作以最小化数据移动。这些内存系统及其性能特性的详细定量分析在[第11章](ch017.xhtml#sec-ai-acceleration)中有所介绍。
- en: The need for parallel processing has driven the adoption of specialized hardware
    architectures, ranging from powerful cloud GPUs to specialized mobile processors
    to Tiny ML accelerators. The specific hardware architectures and their trade-offs
    for ML workloads are explored in [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理的需求推动了专用硬件架构的采用，从强大的云GPU到专用移动处理器再到Tiny ML加速器。第11章[sec-ai-acceleration](ch017.xhtml#sec-ai-acceleration)中探讨了特定硬件架构及其在机器学习工作负载中的权衡。
- en: Hierarchical Memory Architecture
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分层内存架构
- en: The memory requirements present another shift. Traditional programs typically
    maintain small, fixed memory footprints. In contrast, deep learning models must
    manage parameters across complex memory hierarchies. Memory bandwidth often becomes
    the primary performance bottleneck, creating challenges for resource-constrained
    systems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 内存需求带来了另一个转变。传统的程序通常保持较小的、固定的内存占用。相比之下，深度学习模型必须管理复杂内存层次结构中的参数。内存带宽往往成为主要的性能瓶颈，给资源受限的系统带来挑战。
- en: This memory-intensive nature creates performance bottlenecks unique to neural
    computing. Matrix multiplication—the core neural network operation—is often memory
    bandwidth-bound rather than compute-bound[10](#fn10). The fundamental issue is
    that processors can perform computations faster than they can fetch data from
    memory. Each weight must be loaded from memory to perform a multiplication, and
    if the memory system can’t supply data fast enough, computational units sit idle
    waiting for values to arrive. This imbalance between computational capability
    and memory bandwidth explains why simply adding more processing units doesn’t
    proportionally improve performance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种内存密集型特性为神经网络计算创造了独特的性能瓶颈。矩阵乘法——神经网络的核心操作——通常是内存带宽限制的，而不是计算限制的[10](#fn10)。基本问题是处理器可以比从内存中获取数据更快地执行计算。每个权重都必须从内存中加载以执行乘法，如果内存系统不能快速提供数据，计算单元就会空闲等待值到来。这种计算能力和内存带宽之间的不平衡解释了为什么仅仅增加更多的处理单元并不能成比例地提高性能。
- en: 'GPUs address this challenge through both higher memory bandwidth and massive
    parallelism, achieving better utilization than traditional CPUs. However, the
    underlying constraint remains: energy consumption in neural networks is dominated
    by data movement, not computation. Moving data from main memory to processing
    units consumes more energy than the actual mathematical operations. This energy
    hierarchy explains why specialized processors focus on techniques that reduce
    data movement, keeping data closer to where it’s processed.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GPU通过更高的内存带宽和巨大的并行性来应对这一挑战，实现了比传统CPU更好的利用率。然而，根本的约束仍然存在：神经网络中的能耗主要由数据移动而非计算主导。将数据从主内存移动到处理单元比实际的数学运算消耗更多的能量。这种能量层次结构解释了为什么专用处理器专注于减少数据移动的技术，使数据更接近处理的地方。
- en: This fundamental memory-computation tradeoff manifests differently across deployment
    scenarios. Cloud servers can afford more memory and power to maximize throughput,
    while mobile devices must carefully optimize to operate within strict power budgets.
    Training systems prioritize computational throughput even at higher energy costs,
    while inference systems emphasize energy efficiency. These different constraints
    drive different optimization strategies across the ML systems spectrum, ranging
    from memory-rich cloud deployments to heavily optimized Tiny ML implementations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本的内存-计算权衡在不同部署场景中表现出不同的特点。云服务器可以承担更多的内存和电力以最大化吞吐量，而移动设备必须仔细优化以在严格的电力预算内运行。训练系统即使在更高的能源成本下也优先考虑计算吞吐量，而推理系统则强调能源效率。这些不同的限制驱使着机器学习系统范围内的不同优化策略，从内存丰富的云部署到高度优化的Tiny
    ML实现。
- en: Memory optimization strategies like quantization and pruning are detailed in
    [Chapter 10](ch016.xhtml#sec-model-optimizations), while hardware architectures
    and their memory systems are explored in [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章（[第10章](ch016.xhtml#sec-model-optimizations)）详细介绍了内存优化策略，如量化剪枝，而第11章（[第11章](ch017.xhtml#sec-ai-acceleration)）探讨了硬件架构及其内存系统。
- en: Distributed Computing Requirements
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布式计算需求
- en: Researchers discovered deep learning changes how systems scale and the importance
    of efficiency. Traditional programs have relatively fixed resource requirements
    with predictable performance characteristics. Deep learning models can consume
    exponentially more resources as they grow in complexity. This relationship between
    model capability and resource consumption makes system efficiency a concern. [Chapter 9](ch015.xhtml#sec-efficient-ai)
    provides coverage of techniques to optimize this relationship, including methods
    to reduce computational requirements while maintaining model performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，深度学习改变了系统的扩展方式以及效率的重要性。传统的程序具有相对固定的资源需求，具有可预测的性能特征。随着复杂性的增加，深度学习模型可以消耗指数级更多的资源。这种模型能力与资源消耗之间的关系使得系统效率成为一个关注点。[第9章](ch015.xhtml#sec-efficient-ai)涵盖了优化这种关系的技巧，包括在保持模型性能的同时减少计算需求的方法。
- en: 'Bridging algorithmic concepts with hardware realities becomes essential. While
    traditional programs map relatively straightforwardly to standard computer architectures,
    deep learning requires careful consideration of:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将算法概念与硬件现实联系起来变得至关重要。虽然传统的程序可以相对直接地映射到标准计算机架构，但深度学习需要仔细考虑：
- en: How to efficiently map matrix operations to physical hardware ([Chapter 11](ch017.xhtml#sec-ai-acceleration)
    covers hardware-specific optimization strategies)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何有效地将矩阵运算映射到物理硬件上（[第11章](ch017.xhtml#sec-ai-acceleration)涵盖了针对特定硬件的优化策略）
- en: Ways to minimize data movement across memory hierarchies
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化跨内存层次结构数据移动的方法
- en: Methods to balance computational capability with resource constraints ([Chapter 9](ch015.xhtml#sec-efficient-ai)
    explores scaling laws and efficiency trade-offs)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡计算能力与资源限制的方法（[第9章](ch015.xhtml#sec-efficient-ai)探讨了扩展定律和效率权衡）
- en: Techniques to optimize both algorithm and system-level efficiency ([Chapter 10](ch016.xhtml#sec-model-optimizations)
    provides model compression techniques)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法和系统级效率的技术（[第10章](ch016.xhtml#sec-model-optimizations)提供了模型压缩技术）
- en: These shifts explain why deep learning has spurred innovations across the entire
    computing stack. From specialized hardware accelerators to new memory architectures
    to sophisticated software frameworks, the demands of deep learning continue to
    reshape computer system design.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转变解释了为什么深度学习激发了整个计算堆栈的创新。从专用硬件加速器到新的内存架构，再到复杂的软件框架，深度学习的需求持续重塑计算机系统设计。
- en: Having established both the historical progression from rule-based systems to
    neural networks and the computational infrastructure this evolution demands, we
    now examine the foundational inspiration behind these systems. The answer to what
    neural networks compute begins not with silicon and software, but with biology—specifically,
    the neural networks in our brains that inspired the artificial neural networks
    powering modern AI systems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了从基于规则的系统到神经网络的历史演变以及这一演变所要求的计算基础设施之后，我们现在考察这些系统的根本灵感。神经网络计算答案的起点不是硅和软件，而是生物学——具体来说，是我们大脑中的神经网络，这些神经网络启发了现代人工智能系统所依赖的人工神经网络。
- en: From Biology to Silicon
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从生物学到硅
- en: 'Having examined how programming approaches evolved from rules to data-driven
    learning, and how this evolution drives the computational infrastructure requirements
    we see today, we now turn to the question: what are these neural networks actually
    computing? The answer begins not with silicon, but with biology.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察了编程方法如何从规则发展到数据驱动学习，以及这种演变如何推动我们今天看到的计算基础设施需求之后，我们现在转向一个问题：这些神经网络实际上在计算什么？答案不是从硅开始，而是从生物学开始。
- en: 'The massive computational requirements we just examined (specialized processors,
    hierarchical memory systems, high-bandwidth data movement) all trace back to a
    simple inspiration: the biological neuron. Understanding how nature solves information
    processing problems with 20 watts of power reveals both the potential and the
    challenges of artificial neural systems. As we examine biological neurons and
    their artificial counterparts, watch for a pattern: each biological feature that
    we choose to implement or approximate creates specific computational demands,
    linking the dendrite-and-synapse model directly to the processing power and memory
    bandwidth requirements we just discussed.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚考察的巨大计算需求（专用处理器、分层内存系统、高带宽数据移动）都源于一个简单的灵感：生物神经元。了解自然界如何用20瓦的功率解决信息处理问题，揭示了人工神经网络系统的潜力和挑战。当我们考察生物神经元及其人工对应物时，注意寻找一个模式：我们选择实现或近似的每个生物特征都会产生特定的计算需求，将树突和突触模型直接与刚刚讨论的处理能力和内存带宽需求联系起来。
- en: 'This section bridges biological inspiration and systems implementation by examining
    three key transformations: how biological neurons inspire artificial neuron design,
    how neural principles translate into mathematical operations, and how these operations
    drive the system requirements we outlined earlier. By the end, you’ll understand
    why implementing even simplified neural computation requires the specialized hardware
    infrastructure modern ML systems demand.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过考察三个关键转换来连接生物灵感和系统实现：生物神经元如何启发人工神经元设计，神经网络原理如何转化为数学运算，以及这些运算如何推动我们之前概述的系统需求。到最后，你将理解为什么即使实现简化的神经网络计算也需要现代机器学习系统所要求的专用硬件基础设施。
- en: Biological Neural Processing Principles
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生物神经网络处理原理
- en: 'From a systems perspective, biological neural networks offer solutions to the
    computational challenges we’ve just discussed: they achieve massive parallelism,
    efficient memory usage, and adaptive learning while consuming minimal energy.
    Four key principles from biological intelligence directly inform artificial neural
    network design:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统角度来看，生物神经网络为我们刚刚讨论的计算挑战提供了解决方案：它们实现了大规模并行处理、高效的内存使用和自适应学习，同时消耗最少的能量。四个来自生物智能的关键原则直接影响了人工神经网络的设计：
- en: '**Adaptive Learning**: The brain continuously modifies neural connections based
    on experience, refining responses through interaction with the environment. This
    biological capability inspired machine learning’s core principle: improving from
    data rather than following fixed, pre-programmed rules.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应学习**：大脑根据经验持续修改神经网络连接，通过与环境的互动来细化响应。这种生物能力启发了机器学习的核心原则：从数据中改进，而不是遵循固定的、预先编程的规则。'
- en: '**Parallel Processing**: The brain processes vast amounts of information simultaneously,
    with different regions specializing in specific functions while working in concert.
    This distributed, parallel architecture contrasts with traditional sequential
    computing and has influenced modern AI system design.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行处理**：大脑同时处理大量信息，不同区域专注于特定功能，同时协同工作。这种分布式、并行架构与传统顺序计算形成对比，并影响了现代人工智能系统设计。'
- en: '**Pattern Recognition**: Biological systems excel at identifying patterns in
    complex, noisy data—recognizing faces in crowds, understanding speech in noisy
    environments, identifying objects from partial information. This capability has
    inspired applications in computer vision and speech recognition, though artificial
    systems still strive to match the brain’s efficiency.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式识别**：生物系统在识别复杂、嘈杂数据中的模式方面表现出色——在人群中识别面孔，在嘈杂环境中理解语音，从部分信息中识别物体。这种能力启发了计算机视觉和语音识别的应用，尽管人工系统仍在努力匹配大脑的效率。'
- en: '**Energy Efficiency**: Biological systems achieve processing with exceptional
    energy efficiency. The human brain’s 20-watt power consumption[11](#fn11) creates
    a stark efficiency gap that artificial systems are still striving to bridge. Understanding
    and replicating this efficiency is explored in [Chapter 18](ch024.xhtml#sec-sustainable-ai)
    through environmental impact analysis and energy-efficient optimization strategies.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**能源效率**：生物系统以非凡的能源效率进行处理。人脑的20瓦功率消耗[11](#fn11) 形成了一个显著的效率差距，人工系统仍在努力弥合这一差距。理解和复制这种效率在[第18章](ch024.xhtml#sec-sustainable-ai)中通过环境影响分析和能源效率优化策略进行了探讨。'
- en: 'These biological principles suggest key requirements for artificial neural
    systems: simple processing units integrating multiple inputs, adjustable connection
    strengths, nonlinear activation based on input thresholds, parallel processing
    architecture, and learning through connection strength modification. The following
    sections examine how we translate these biological insights into mathematical
    operations and into silicon implementations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些生物学原理为人工神经网络提出了关键要求：简单处理单元整合多个输入，可调节的连接强度，基于输入阈值的非线性激活，并行处理架构，以及通过连接强度修改进行学习。以下章节将探讨我们如何将这些生物学洞察转化为数学运算和硅基实现。
- en: These biological principles have shaped two approaches in artificial intelligence.
    The first attempts to directly mimic neural structure and function, creating artificial
    neural networks that structurally resemble biological networks. The second takes
    a more abstract approach, adapting biological principles to work efficiently within
    computer hardware constraints without copying biological structures exactly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些生物学原理塑造了人工智能中的两种方法。第一种试图直接模仿神经网络的结构和功能，创建了结构上类似于生物网络的的人工神经网络。第二种采取更抽象的方法，将生物学原理适应于在计算机硬件约束下高效工作，而不必精确复制生物结构。
- en: 'To understand how either approach works in practice, we must first examine
    the basic unit that makes neural computation possible: the individual neuron.
    By understanding how biological neurons process information, we can then see how
    this process translates into the mathematical operations that drive artificial
    neural networks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这两种方法在实际中的工作原理，我们首先必须考察使神经网络计算成为可能的基本单元：单个神经元。通过理解生物神经元如何处理信息，我们就可以看到这一过程如何转化为驱动人工神经网络的数学运算。
- en: Biological Neuron Structure
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生物神经元结构
- en: 'Translating these high-level principles into practical implementation requires
    examining the basic unit of biological information processing: the neuron. This
    cellular building block provides the blueprint for its artificial counterpart
    and reveals how complex neural networks emerge from simple components working
    together.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些高级原理转化为实际实现需要考察生物学信息处理的基本单元：神经元。这个细胞构建块为其人工对应物提供了蓝图，并揭示了复杂神经网络如何从简单的协同工作组件中产生。
- en: In biological systems, the neuron (or cell) represents the basic functional
    unit of the nervous system. Understanding its structure is crucial for drawing
    parallels to artificial systems. [Figure 3.7](ch009.xhtml#fig-bio_nn2ai_nn) illustrates
    the structure of a biological neuron.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物系统中，神经元（或细胞）代表神经系统的基本功能单元。理解其结构对于将人工系统与之类比至关重要。[图3.7](ch009.xhtml#fig-bio_nn2ai_nn)展示了生物神经元的结构。
- en: '![](../media/file39.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file39.png)'
- en: 'Figure 3.7: **Biological Neuron Mapping**: Artificial neurons abstract key
    functions from their biological counterparts, receiving weighted inputs at dendrites,
    summing them in the cell body, and producing an output via the axon, analogous
    to activation functions in artificial neural networks. This abstraction enables
    the construction of complex artificial neural networks capable of sophisticated
    information processing. Source: geeksforgeeks.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：**生物神经元映射**：人工神经元从其生物对应物中抽象出关键功能，在树突接收加权输入，在细胞体中求和，并通过轴突产生输出，类似于人工神经网络中的激活函数。这种抽象使得能够构建复杂的人工神经网络，能够进行复杂的信息处理。来源：geeksforgeeks。
- en: A biological neuron consists of several key components. The central part is
    the cell body, or soma, which contains the nucleus and performs the cell’s basic
    life processes. Extending from the soma are branch-like structures called dendrites,
    which act as receivers for incoming signals from other neurons. The connections
    between neurons occur at synapses[12](#fn12), which modulate the strength of the
    transmitted signals. Finally, a long, slender projection called the axon conducts
    electrical impulses away from the cell body to other neurons.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元由几个关键组件组成。中心部分是细胞体，或称为细胞核，其中包含细胞核并执行细胞的基本生命过程。从细胞体延伸出分支状结构，称为树突，它们作为接收器接收来自其他神经元的信号。神经元之间的连接发生在突触[12](#fn12)，它们调节传输信号的强度。最后，一个细长的突起称为轴突，将电脉冲从细胞体传导到其他神经元。
- en: 'Integrating these structural components, the neuron functions as follows: Dendrites
    act as receivers, collecting input signals from other neurons. Synapses at these
    connections modulate the strength of each signal, determining how much influence
    each input has. The soma integrates these weighted signals and decides whether
    to trigger an output signal. If triggered, the axon transmits this signal to other
    neurons.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些结构组件整合，神经元的功能如下：树突作为接收器，从其他神经元收集输入信号。这些连接处的突触调节每个信号的强度，决定每个输入的影响程度。细胞体将这些加权信号综合起来，并决定是否触发输出信号。如果触发，轴突将此信号传输到其他神经元。
- en: Each element of a biological neuron has a computational analog in artificial
    systems, reflecting the principles of learning, adaptability, and efficiency found
    in nature. To better understand how biological intelligence informs artificial
    systems, [Table 3.2](ch009.xhtml#tbl-bio_nn2ai_nn) captures the mapping between
    the components of biological and artificial neurons. This should be viewed alongside
    [Figure 3.7](ch009.xhtml#fig-bio_nn2ai_nn) for a complete picture. Together, they
    show the biological-to-artificial neuron mapping.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元的每个元素在人工系统中都有一个计算上的类似物，反映了自然界中发现的原理，如学习、适应性和效率。为了更好地理解生物智能如何影响人工系统，[表3.2](ch009.xhtml#tbl-bio_nn2ai_nn)捕捉了生物神经元和人工神经元组件之间的映射。这应该与[图3.7](ch009.xhtml#fig-bio_nn2ai_nn)一起查看，以获得完整的图景。它们共同展示了从生物到人工神经元的映射。
- en: 'Table 3.2: **Neuron Correspondence**: Biological neurons inspire artificial
    neuron design through analogous components—dendrites map to inputs (receiving
    signals), synapses map to weights (modulating connection strength), the soma to
    net input, and the axon to output—establishing a foundation for computational
    modeling of intelligence. This table clarifies how key functions of biological
    neurons are abstracted and implemented in artificial neural networks, enabling
    learning and information processing.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：**神经元对应关系**：生物神经元通过类似组件启发人工神经元的设计——树突对应输入（接收信号），突触对应权重（调节连接强度），细胞体对应净输入，轴突对应输出——为智能计算建模奠定基础。此表阐明了生物神经元的哪些关键功能被抽象并在人工神经网络中实现，从而实现学习和信息处理。
- en: '| **Biological Neuron** | **Artificial Neuron** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **生物神经元** | **人工神经元** |'
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Cell** | Neuron / Node |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **细胞** | 神经元/节点 |'
- en: '| **Dendrites** | Inputs |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **树突** | 输入 |'
- en: '| **Synapses** | Weights |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **突触** | 权重 |'
- en: '| **Soma** | Net Input |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **细胞体** | 净输入 |'
- en: '| **Axon** | Output |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **轴突** | 输出 |'
- en: Understanding these correspondences proves crucial for grasping how artificial
    systems approximate biological intelligence. Each component serves a similar function
    through different mechanisms, with specific implications for artificial neural
    networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些对应关系对于掌握人工系统如何近似生物智能至关重要。每个组件通过不同的机制执行类似的功能，对人工神经网络具有特定的含义。
- en: '**Cell <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    Neuron/Node**: The artificial neuron or node serves as the basic computational
    unit, mirroring the cell’s role in biological systems.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**细胞 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    神经元/节点**：人工神经元或节点作为基本的计算单元，反映了生物系统中细胞的作用。'
- en: '**Dendrites <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    Inputs**: Dendrites in biological neurons receive incoming signals from other
    neurons, analogous to how inputs feed into artificial neurons. They act as the
    signal receivers, like antennas collecting information.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**树突 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    输入**：生物神经元的树突接收来自其他神经元的信号，这与输入如何进入人工神经元的方式类似。它们充当信号接收器，就像收集信息的天线一样。'
- en: '**Synapses <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    Weights**: Synapses modulate the strength of connections between neurons, directly
    analogous to weights in artificial neurons. These weights are adjustable, enabling
    learning and optimization over time by controlling how much influence each input
    has.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**突触 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    权重**：突触调节神经元之间连接的强度，这与人工神经元中的权重直接相似。这些权重是可以调整的，通过控制每个输入的影响程度，使得学习与优化可以在时间上得到实现。'
- en: '**Soma <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    Net Input**: The net input in artificial neurons sums weighted inputs to determine
    activation, similar to how the soma integrates signals in biological neurons.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**胞体 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    净输入**：人工神经元中的净输入将加权输入相加以确定激活，这与生物神经元中胞体整合信号的方式相似。'
- en: '**Axon <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    Output**: The output of an artificial neuron passes processed information to subsequent
    network layers, much like an axon transmits signals to other neurons.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轴突 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics>
    输出**：人工神经元的输出将处理过的信息传递给后续的网络层，这与轴突将信号传递给其他神经元的方式非常相似。'
- en: This mapping illustrates how artificial neural networks simplify and abstract
    biological processes while preserving their essential computational principles.
    Understanding individual neurons represents only the beginning. The true power
    of neural networks emerges from how these basic units work together in larger
    systems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种映射展示了人工神经网络如何简化并抽象生物过程，同时保留其基本的计算原理。理解单个神经元只是开始。神经网络的真实力量来自于这些基本单元如何在更大的系统中协同工作。
- en: From a systems engineering perspective, this biological-to-artificial translation
    reveals why neural networks have such demanding computational requirements. Each
    simple biological process maps to intensive mathematical operations that must
    be executed millions or billions of times in parallel.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从系统工程的角度来看，这种从生物到人工的翻译揭示了为什么神经网络具有如此高的计算要求。每个简单的生物过程都映射到密集的数学运算，这些运算必须并行执行数百万或数十亿次。
- en: Artificial Neural Network Design Principles
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经网络设计原则
- en: Bridging the gap from biological inspiration to practical implementation, the
    translation from biological principles to artificial computation requires a deep
    appreciation of what makes biological neural networks so effective at both the
    cellular and network levels, and why replicating these capabilities in silicon
    presents such significant systems challenges. The brain processes information
    through distributed computation across billions of neurons, each operating relatively
    slowly compared to silicon transistors. A biological neuron fires at approximately
    200 Hz, while modern processors operate at gigahertz frequencies. Despite this
    speed limitation, the brain’s parallel architecture enables sophisticated real-time
    processing of complex sensory input, decision-making, and control of behavior.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从生物灵感到实际实施的桥梁，将生物原理转换为人工计算需要深刻理解生物神经网络在细胞和网络层面上的有效性，以及为什么在硅中复制这些能力会带来如此重大的系统挑战。大脑通过在数十亿个神经元之间进行分布式计算来处理信息，每个神经元的运行速度相对于硅晶体管来说相对较慢。尽管存在速度限制，但大脑的并行架构能够实现对复杂感官输入、决策和行为控制的复杂实时处理。
- en: Despite the apparent speed disadvantage, this computational efficiency emerges
    from the brain’s basic organizational principles. Each neuron acts as a simple
    processing unit, integrating inputs from thousands of other neurons and producing
    a binary output signal based on whether this integrated input exceeds a threshold.
    The connection strengths between neurons, mediated by synapses, are continuously
    modified through experience. This synaptic plasticity forms the basis for learning
    and adaptation in biological neural networks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在速度上存在明显劣势，但这种计算效率源于大脑的基本组织原则。每个神经元作为一个简单的处理单元，整合来自数千个其他神经元的输入，并根据这个综合输入是否超过阈值产生一个二进制输出信号。神经元之间的连接强度，通过突触介导，会通过经验不断修改。这种突触可塑性构成了生物神经网络学习和适应的基础。
- en: Replicating biological efficiency in artificial systems requires navigating
    fundamental trade-offs. While the brain achieves remarkable efficiency with only
    20 watts (as noted earlier), comparable artificial neural networks require orders
    of magnitude more power. Large language models, for example, can consume megawatts
    during training and kilowatts during inference—thousands to hundreds of thousands
    of times more power than the brain. This substantial efficiency gap drives the
    engineering focus on specialized hardware, quantization techniques, and architectural
    innovations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工系统中复制生物效率需要克服基本权衡。虽然大脑仅用20瓦（如前所述）就能实现非凡的效率，但与之相当的的人工神经网络则需要数量级更多的电力。例如，大型语言模型在训练期间可能消耗兆瓦电力，在推理期间消耗千瓦电力——比大脑的电力多出数千到数百万倍。这种显著的效率差距推动了工程领域对专用硬件、量化技术和架构创新的关注。
- en: 'Drawing from these organizational insights, biological systems suggest several
    key computational elements needed in artificial neural systems:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些组织洞察中汲取灵感，生物系统提出了人工神经网络中所需的一些关键计算元素：
- en: Simple processing units that integrate multiple inputs
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成多个输入的简单处理单元
- en: Adjustable connection strengths between units
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元之间可调节的连接强度
- en: Nonlinear activation based on input thresholds
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输入阈值的非线性激活
- en: Parallel processing architecture
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行处理架构
- en: Learning through modification of connection strengths
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过修改连接强度进行学习
- en: 'The question now becomes: how do we translate these abstract biological principles
    into concrete mathematical operations that computers can execute?'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题变成了：我们如何将这些抽象的生物原理转化为计算机可以执行的具体数学操作？
- en: Mathematical Translation of Neural Concepts
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经概念数学翻译
- en: 'Translating biological insights into practical systems, we face the challenge
    of capturing the essence of neural computation within the rigid framework of digital
    systems. As established in our neuron model analysis (see [Table 3.2](ch009.xhtml#tbl-bio_nn2ai_nn)),
    artificial neurons simplify biological processes into three key operations: weighted
    input processing (synaptic strength), summation (signal integration), and activation
    functions (threshold-based firing).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将生物洞察力转化为实际系统，我们面临着在数字系统的刚性框架内捕捉神经计算本质的挑战。正如我们在神经元模型分析中确立的（参见[表3.2](ch009.xhtml#tbl-bio_nn2ai_nn)），人工神经元将生物过程简化为三个关键操作：加权输入处理（突触强度）、求和（信号整合）和激活函数（基于阈值的触发）。
- en: '[Table 3.3](ch009.xhtml#tbl-bio2comp) provides a systematic view of how these
    biological features map to their computational counterparts, revealing both the
    possibilities and limitations of digital neural implementation.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3.3](ch009.xhtml#tbl-bio2comp)提供了一个系统性的视角，展示了这些生物特征如何映射到它们的计算对应物，揭示了数字神经实现的可能性与局限性。'
- en: 'Table 3.3: **Biological-Computational Analogies**: Artificial neurons abstract
    key principles of biological neural systems, mapping neuron firing to activation
    functions, synaptic strength to weighted connections, and signal integration to
    summation operations—establishing a foundation for digital neural implementation.
    Distributed memory and parallel processing in biological systems find computational
    counterparts in weight matrices and concurrent computation, respectively, highlighting
    both the power and limitations of this abstraction.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3：**生物-计算类比**：人工神经元抽象了生物神经网络系统的关键原则，将神经元触发映射到激活函数，突触强度映射到加权连接，将信号整合映射到求和操作——为数字神经实现奠定了基础。生物系统中的分布式记忆和并行处理在权重矩阵和并发计算中找到计算对应物，突显了这种抽象的强大和局限性。
- en: '| **Biological Feature** | **Computational Translation** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **生物特征** | **计算翻译** |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Neuron firing** | Activation function |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **神经元激发** | 激活函数 |'
- en: '| **Synaptic strength** | Weighted connections |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **突触强度** | 加权连接 |'
- en: '| **Signal integration** | Summation operation |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **信号整合** | 求和操作 |'
- en: '| **Distributed memory** | Weight matrices |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **分布式内存** | 权重矩阵 |'
- en: '| **Parallel processing** | Concurrent computation |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **并行处理** | 并发计算 |'
- en: Using the biological-to-artificial mapping principles outlined earlier, this
    mathematical abstraction preserves key computational principles while enabling
    efficient digital implementation. The weighting, summation, and activation operations
    directly correspond to the synaptic strength, signal integration, and threshold
    firing mechanisms identified in our neuron correspondence analysis.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面概述的生物到人工映射原理，这种数学抽象保留了关键的计算原理，同时实现了高效的数字实现。权重、求和和激活操作直接对应于我们在神经元对应分析中确定的突触强度、信号整合和阈值激发机制。
- en: This abstraction has a computational cost. What happens effortlessly in biology
    requires intensive mathematical computation in artificial systems. As discussed
    in the Memory Systems section, these operations create significant computational
    demands due to memory bandwidth limitations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象具有计算成本。在生物学中轻松完成的事情，在人工系统中需要密集的数学计算。正如在记忆系统部分所讨论的，这些操作由于内存带宽限制而产生了显著的计算需求。
- en: Memory in artificial neural networks takes a markedly different form from biological
    systems. While biological memories are distributed across synaptic connections
    and neural patterns, artificial networks store information in discrete weights
    and parameters. This architectural difference reflects the constraints of current
    computing hardware, where memory and processing are physically separated rather
    than integrated as in biological systems. Despite these implementation differences,
    artificial neural networks achieve similar functional capabilities in pattern
    recognition and learning.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络中的记忆与生物系统有着显著不同的形式。虽然生物记忆分布在突触连接和神经网络模式中，但人工网络将信息存储在离散的权重和参数中。这种架构差异反映了当前计算硬件的限制，其中内存和处理在物理上是分离的，而不是像生物系统那样集成。尽管这些实现方式不同，但人工神经网络在模式识别和学习方面实现了类似的功能能力。
- en: The brain’s massive parallelism represents a challenge in artificial implementation.
    While biological neural networks process information through billions of neurons
    operating simultaneously, artificial systems approximate this parallelism through
    specialized hardware like GPUs and tensor processing units. These devices efficiently
    compute the matrix operations that form the mathematical foundation of artificial
    neural networks, achieving parallel processing at a different scale and granularity
    than biological systems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑的巨大并行性在人工实现中是一个挑战。虽然生物神经网络通过数亿个同时工作的神经元处理信息，但人工系统通过如GPU和张量处理单元等专用硬件来近似这种并行性。这些设备有效地计算构成人工神经网络数学基础矩阵运算，以不同于生物系统的规模和粒度实现并行处理。
- en: Hardware and Software Requirements
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件和软件需求
- en: The computational translation of neural principles creates infrastructure demands
    that emerge from key differences between biological and artificial implementations,
    directly shaping system design.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 神经原理的计算翻译产生了来自生物和人工实现之间关键差异的基础设施需求，这直接塑造了系统设计。
- en: '[Table 3.4](ch009.xhtml#tbl-comp2sys) shows how each computational element
    drives particular system requirements. This mapping shows how the choices made
    in computational translation directly influence the hardware and system architecture
    needed for implementation.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3.4](ch009.xhtml#tbl-comp2sys)展示了每个计算元素如何驱动特定的系统需求。这种映射显示了在计算翻译中做出的选择如何直接影响实施所需的硬件和系统架构。'
- en: 'Table 3.4: **Computational Demands**: Artificial neural network design directly
    translates into specific system requirements; for example, efficient activation
    functions necessitate fast nonlinear operation units and large-scale weight storage
    demands high-bandwidth memory access. Understanding this mapping guides hardware
    and system architecture choices for effective implementation of artificial intelligence.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.4：**计算需求**：人工神经网络的设计直接转化为特定的系统需求；例如，高效的激活函数需要快速的非线性操作单元，而大规模的权重存储需求需要高带宽的内存访问。理解这种映射有助于指导硬件和系统架构的选择，以有效地实施人工智能。
- en: '| **Computational Element** | **System Requirements** |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **计算元素** | **系统要求** |'
- en: '| --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Activation functions** | Fast nonlinear operation units |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| **激活函数** | 快速非线性操作单元 |'
- en: '| **Weight operations** | High-bandwidth memory access |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **权重操作** | 高带宽内存访问 |'
- en: '| **Parallel computation** | Specialized parallel processors |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **并行计算** | 专用并行处理器 |'
- en: '| **Weight storage** | Large-scale memory systems |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **权重存储** | 大规模内存系统 |'
- en: '| **Learning algorithms** | Gradient computation hardware |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **学习算法** | 梯度计算硬件 |'
- en: Storage architecture represents a critical requirement, driven by the key difference
    in how biological and artificial systems handle memory. In biological systems,
    memory and processing are intrinsically integrated—synapses both store connection
    strengths and process signals. Artificial systems, however, must maintain a clear
    separation between processing units and memory. This creates a need for both high-capacity
    storage to hold millions or billions of connection weights and high-bandwidth
    pathways to move this data quickly between storage and processing units. The efficiency
    of this data movement often becomes a critical bottleneck that biological systems
    do not face.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 存储架构代表了一个关键需求，由生物系统和人工系统处理内存的关键差异所驱动。在生物系统中，记忆和处理是内在集成的——突触既存储连接强度又处理信号。然而，人工系统必须在处理单元和内存之间保持清晰的分离。这需要既有高容量存储来存储数百万或数十亿个连接权重，又有高带宽路径来快速在存储和处理单元之间移动这些数据。这种数据移动的效率通常成为生物系统不面临的临界瓶颈。
- en: The learning process itself imposes distinct requirements on artificial systems.
    While biological networks modify synaptic strengths through local chemical processes,
    artificial networks must coordinate weight updates across the entire network.
    This creates computational and memory demands during training, as systems must
    not only store current weights but also maintain space for gradients and intermediate
    calculations. The requirement to backpropagate error signals, with no real biological
    analog, complicates the system architecture. Securing these large models and protecting
    sensitive training data introduces complex requirements addressed in [Chapter 16](ch022.xhtml#sec-robust-ai).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程本身对人工系统提出了独特的要求。虽然生物网络通过局部化学过程修改突触强度，但人工网络必须在整个网络中协调权重更新。这会在训练期间产生计算和内存需求，因为系统不仅必须存储当前权重，还必须为梯度和中途计算保留空间。反向传播错误信号的需求，没有真正的生物类似物，使得系统架构复杂化。确保这些大型模型和保护敏感训练数据引入了复杂的要求，这些要求在[第16章](ch022.xhtml#sec-robust-ai)中得到了解决。
- en: Energy efficiency emerges as a final critical requirement, highlighting perhaps
    the starkest contrast between biological and artificial implementations. The human
    brain’s remarkable energy efficiency, which operates on approximately 20 watts,
    stands in sharp contrast to the substantial power demands of artificial neural
    networks. Current systems often require orders of magnitude more energy to implement
    similar capabilities. This gap drives ongoing research in more efficient hardware
    architectures and has profound implications for the practical deployment of neural
    networks, particularly in resource-constrained environments like mobile devices
    or edge computing systems. The environmental impact of this energy consumption
    and strategies for sustainable AI development are explored in [Chapter 18](ch024.xhtml#sec-sustainable-ai).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 能效成为最后一个关键需求，突显了生物和人工实现之间可能的最鲜明对比。人脑惊人的能效，大约20瓦特，与人工神经网络的大量电力需求形成鲜明对比。当前系统通常需要数量级更多的能量来实现类似的功能。这一差距推动了更高效硬件架构的研究，并对神经网络的实际部署产生了深远影响，尤其是在资源受限的环境，如移动设备或边缘计算系统中。这一能源消耗的环境影响和可持续AI发展的策略在[第18章](ch024.xhtml#sec-sustainable-ai)中进行了探讨。
- en: These system requirements directly drive the architectural choices we make in
    building ML systems, from the specialized hardware accelerators covered in [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    to the distributed training systems discussed in [Chapter 8](ch014.xhtml#sec-ai-training).
    Understanding why these requirements exist, rooted in the key differences between
    biological and artificial computation, is essential for making informed decisions
    about system design and optimization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统需求直接推动了我们在构建机器学习系统时所做的架构选择，从第11章（ch017.xhtml#sec-ai-acceleration）中涵盖的专用硬件加速器到第8章（ch014.xhtml#sec-ai-training）中讨论的分布式训练系统。理解这些需求存在的原因，即根植于生物计算和人工计算之间的关键差异，对于做出关于系统设计和优化的明智决策至关重要。
- en: Evolution of Neural Network Computing
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络计算的发展
- en: We can appreciate how the field of deep learning evolved to meet these challenges
    through advances in hardware and algorithms. This journey began with early artificial
    neural networks in the 1950s, marked by the introduction of the Perceptron ([Rosenblatt
    1958](ch058.xhtml#ref-rosenblatt1958perceptron))[13](#fn13). While groundbreaking
    in concept, these early systems were severely limited by the computational capabilities
    of their era, primarily mainframe computers that lacked both the processing power
    and memory capacity needed for complex networks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过硬件和算法的进步来欣赏深度学习领域是如何演变以应对这些挑战的。这一旅程始于20世纪50年代的早期人工神经网络，以感知器（[Rosenblatt
    1958](ch058.xhtml#ref-rosenblatt1958perceptron)）[13](#fn13)的引入为标志。虽然这些早期系统在概念上具有突破性，但它们受到了其时代计算能力的严重限制，主要是缺乏处理能力和内存容量的大型计算机，这些能力对于复杂网络来说是必需的。
- en: The development of backpropagation algorithms in the 1980s ([Rumelhart, Hinton,
    and Williams 1986](ch058.xhtml#ref-rumelhart1986learning)) was a theoretical breakthrough[14](#fn14)
    and provided a systematic way to train multi-layer networks. The computational
    demands of this algorithm far exceeded available hardware capabilities. Training
    even modest networks could take weeks, making experimentation and practical applications
    challenging. This mismatch between algorithmic requirements and hardware capabilities
    contributed to a period of reduced interest in neural networks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪80年代反向传播算法的发展（[Rumelhart, Hinton, and Williams 1986](ch058.xhtml#ref-rumelhart1986learning)）是一个理论上的突破[14](#fn14)，并为训练多层网络提供了一种系统性的方法。该算法的计算需求远远超过了可用的硬件能力。即使是训练一个适度的网络也可能需要数周时间，这使得实验和实际应用变得具有挑战性。算法需求与硬件能力之间的这种不匹配导致了对神经网络兴趣的下降期。
- en: '![](../media/file40.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file40.png)'
- en: 'Figure 3.8: **Computational Growth**: Exponential increases in computational
    power—initially at a 1.4× rate from 1952–2010, then accelerating to a doubling
    every 3.4 months from 2012–2022—enabled the scaling of deep learning models. this
    trend, coupled with a 10-month doubling cycle for large-scale models after 2015,
    directly addresses the historical bottleneck of training complex neural networks
    and fueled the recent advances in the field. Source: ([Sardanelli et al. 2023](ch058.xhtml#ref-epochai2023trends)).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：**计算增长**：计算能力的指数级增长——最初从1952年到2010年以1.4倍的速度增长，然后从2012年到2022年加速到每3.4个月翻一番——使得深度学习模型的扩展成为可能。这种趋势，加上2015年后大型模型10个月翻一番的周期，直接解决了训练复杂神经网络的历史瓶颈，并推动了该领域的近期进展。来源：([Sardanelli
    et al. 2023](ch058.xhtml#ref-epochai2023trends))。
- en: 'While we’ve established the technical foundations of deep learning in earlier
    sections, the term itself gained prominence in the 2010s, coinciding with significant
    advances in computational power and data accessibility. The field has grown exponentially,
    as illustrated in [Figure 3.8](ch009.xhtml#fig-trends). The graph reveals two
    remarkable trends: computational capabilities measured in floating-point operations
    per second (FLOPS) initially followed a <semantics><mrow><mn>1.4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">1.4\times</annotation></semantics> improvement pattern
    from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to
    2022\. Perhaps more striking is the emergence of large-scale models between 2015
    and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to
    3 orders of magnitude faster than the general trend, following an aggressive 10-month
    doubling cycle.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在前面的章节中已经建立了深度学习的技术基础，但这个术语本身在2010年代获得了显著的关注，这与计算能力的显著进步和数据可访问性的提高相吻合。该领域呈指数级增长，如图[图3.8](ch009.xhtml#fig-trends)所示。该图表揭示了两个显著的趋势：以每秒浮点运算次数（FLOPS）衡量的计算能力最初从1952年到2010年遵循了<semantics><mrow><mn>1.4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">1.4\times</annotation></semantics>的改进模式，然后从2012年到2022年加速到3.4个月的翻倍周期。也许更引人注目的是，2015年到2022年间大规模模型的出现（在图中未明确显示或容易看到），其增长速度比一般趋势快2到3个数量级，遵循了激进的10个月翻倍周期。
- en: 'The evolutionary trends were driven by parallel advances across three dimensions:
    data availability, algorithmic innovations, and computing infrastructure. These
    three factors reinforced each other in a virtuous cycle that continues to drive
    progress in the field today. As [Figure 3.9](ch009.xhtml#fig-virtuous-cycle) shows,
    more powerful computing infrastructure enabled processing larger datasets. Larger
    datasets drove algorithmic innovations. Better algorithms demanded more sophisticated
    computing systems.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 进化趋势是由三个维度的并行进步驱动的：数据可用性、算法创新和计算基础设施。这三个因素在良性循环中相互加强，至今仍在推动该领域的进步。如图[图3.9](ch009.xhtml#fig-virtuous-cycle)所示，更强大的计算基础设施使得处理更大的数据集成为可能。更大的数据集推动了算法创新。更好的算法需要更复杂的计算系统。
- en: '![](../media/file41.svg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file41.svg)'
- en: Figure 3.9
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9
- en: The data revolution transformed what was possible with neural networks. The
    rise of the internet and digital devices created unprecedented access to training
    data. Image sharing platforms provided millions of labeled images. Digital text
    collections enabled language processing at scale. Sensor networks and IoT devices
    generated continuous streams of real-world data. This abundance of data provided
    the raw material needed for neural networks to learn complex patterns effectively.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 数据革命改变了神经网络所能实现的可能性。互联网和数字设备的兴起为训练数据提供了前所未有的访问。图像分享平台提供了数百万个标记图像。数字文本集合实现了大规模的语言处理。传感器网络和物联网设备产生了连续的实时数据流。这种数据丰富性为神经网络有效地学习复杂模式提供了所需的原始材料。
- en: Algorithmic innovations made it possible to use this data effectively. New methods
    for initializing networks and controlling learning rates made training more stable.
    Techniques for preventing overfitting[15](#fn15) allowed models to generalize
    better to new data. Researchers discovered that neural network performance scaled
    predictably with model size, computation, and data quantity, leading to increasingly
    ambitious architectures.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 算法创新使得有效地使用这些数据成为可能。新的网络初始化方法和控制学习率的方法使训练更加稳定。防止过拟合[15](#fn15)的技术使模型能够更好地泛化到新的数据。研究人员发现，神经网络性能与模型大小、计算和数据量呈可预测的比例关系，导致越来越雄心勃勃的架构。
- en: Computing infrastructure evolved to meet these growing demands. On the hardware
    side, graphics processing units (GPUs) provided the parallel processing capabilities
    needed for efficient neural network computation. Specialized AI accelerators like
    TPUs[16](#fn16) ([Norman P. Jouppi et al. 2017d](ch058.xhtml#ref-jouppi2017datacenter))
    pushed performance further. High-bandwidth memory systems and fast interconnects
    addressed data movement challenges. Equally important were software advances—frameworks
    and libraries[17](#fn17) that made it easier to build and train networks, distributed
    computing systems that enabled training at scale, and tools for optimizing model
    deployment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 计算基础设施的发展以满足这些不断增长的需求。在硬件方面，图形处理单元（GPU）提供了高效神经网络计算所需的并行处理能力。像TPU[16](#fn16)（[Norman
    P. Jouppi等人，2017d](ch058.xhtml#ref-jouppi2017datacenter)）这样的专用AI加速器进一步提升了性能。高带宽内存系统和快速互连解决了数据移动挑战。同样重要的是软件的进步——使构建和训练网络更容易的框架和库[17](#fn17)，能够实现大规模训练的分布式计算系统，以及优化模型部署的工具。
- en: The convergence of data availability, algorithmic innovation, and computational
    infrastructure created the foundation for modern deep learning. Building effective
    ML systems requires understanding the computational operations that drive infrastructure
    requirements. Simple mathematical operations, when scaled across millions of parameters
    and billions of training examples, create the massive computational demands that
    shaped this evolution.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可用性、算法创新和计算基础设施的融合为现代深度学习奠定了基础。构建有效的机器学习系统需要理解驱动基础设施需求的计算操作。简单的数学运算，当扩展到数百万个参数和数十亿个训练示例时，创造了塑造这一演变的巨大计算需求。
- en: Neural Network Fundamentals
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: Having traced neural networks’ evolution from biological inspiration through
    historical milestones to modern systems, we now shift focus from “why deep learning
    succeeded” to “how neural networks actually compute.” This section develops the
    mathematical and architectural foundations essential for ML systems engineering.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在追踪了神经网络从生物启发到历史里程碑再到现代系统的演变之后，我们现在将重点从“为什么深度学习成功”转移到“神经网络实际上是如何计算的”。本节发展了机器学习系统工程所必需的数学和架构基础。
- en: 'We take a bottom-up approach, building from simple to complex: individual neurons
    that perform weighted summations → layers that organize parallel computation →
    complete networks that transform raw inputs into predictions. Each concept introduces
    both mathematical principles and their systems implications. As you read, notice
    how each seemingly simple operation—a dot product here, an activation function
    there—compounds into the computational requirements we discussed earlier: millions
    of parameters demanding gigabytes of memory, billions of operations requiring
    specialized hardware, massive datasets necessitating distributed training.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取自下而上的方法，从简单到复杂：执行加权求和的单个神经元 → 组织并行计算的层 → 将原始输入转换为预测的完整网络。每个概念都引入了数学原理及其系统影响。在阅读时，请注意每个看似简单的操作——这里的点积，那里的激活函数——如何累积成我们之前讨论的计算需求：数百万个参数需要数吉字节内存，数十亿个操作需要专用硬件，大量数据集需要分布式训练。
- en: The latest developments in neural architectures and emerging paradigms that
    build upon these foundations are explored in [Chapter 20](ch026.xhtml#sec-agi-systems).
    For now, we establish the foundational concepts that all neural networks share,
    from simple classifiers to large language models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构的最新发展和建立在这些基础之上的新兴范例在[第20章](ch026.xhtml#sec-agi-systems)中进行了探讨。目前，我们建立所有神经网络共有的基础概念，从简单的分类器到大型语言模型。
- en: Network Architecture Fundamentals
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络架构基础
- en: The architecture of a neural network determines how information flows through
    the system, from input to output. While modern networks can be tremendously complex,
    they all build upon a few key organizational principles that directly impact system
    design. Understanding these principles is necessary for both implementing neural
    networks and appreciating why they require the computational infrastructure we’ve
    discussed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构决定了信息如何从输入流向输出，通过系统流动。虽然现代网络可以非常复杂，但它们都建立在几个关键的组织原则之上，这些原则直接影响系统设计。理解这些原则对于实现神经网络和欣赏为什么它们需要我们讨论的计算基础设施至关重要。
- en: To ground these concepts in a concrete example, we’ll use handwritten digit
    recognition throughout this section—specifically, the task of classifying images
    from the MNIST dataset ([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient)).
    This seemingly simple task reveals all the fundamental principles of neural networks
    while providing intuition for more complex applications.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些概念具体化，我们将在本节中使用手写数字识别作为例子——具体来说，是来自MNIST数据集（[Lecun等人，1998](ch058.xhtml#ref-lecun1998gradient)）的图像分类任务。这个看似简单的任务揭示了神经网络的所有基本原理，同时为更复杂的应用提供了直观感受。
- en: '**The Task**: Given a 28×28 pixel grayscale image of a handwritten digit, classify
    it as one of the ten digits (0-9).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务**：给定一个28×28像素的手写数字灰度图像，将其分类为十个数字之一（0-9）。'
- en: '**Input Representation**: Each image contains 784 pixels (28×28), with values
    ranging from 0 (white) to 255 (black). We normalize these to the range [0,1] by
    dividing by 255\. When fed to a neural network, these 784 values form our input
    vector <semantics><mrow><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>784</mn></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{x} \in \mathbb{R}^{784}</annotation></semantics>.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入表示**：每个图像包含784个像素（28×28），值从0（白色）到255（黑色）。我们通过除以255将这些值归一化到[0,1]的范围内。当输入到神经网络时，这784个值形成我们的输入向量
    <semantics><mrow><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>784</mn></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{x} \in \mathbb{R}^{784}</annotation></semantics>。'
- en: '**Output Representation**: The network produces 10 values, one for each possible
    digit. These values represent the network’s confidence that the input image contains
    each digit. The digit with the highest confidence becomes the prediction.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出表示**：网络产生10个值，每个可能的数字一个。这些值代表网络对输入图像包含每个数字的置信度。置信度最高的数字成为预测结果。'
- en: '**Why This Example**: MNIST is small enough to understand completely (784 inputs,
    ~100K parameters for a simple network) yet large enough to be realistic. The task
    is intuitive—everyone understands what “recognize a handwritten 7” means—making
    it ideal for learning neural network principles that scale to much larger problems.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么选择这个例子**：MNIST数据集足够小，可以完全理解（784个输入，一个简单网络大约有10万个参数），但又足够大，具有现实性。这个任务直观易懂——每个人都知道“识别一个手写的7”是什么意思——使其成为学习可以扩展到更大问题的神经网络原理的理想选择。'
- en: '**Network Architecture Preview**: A typical MNIST classifier might use: 784
    input neurons (one per pixel) → 128 hidden neurons → 64 hidden neurons → 10 output
    neurons (one per digit class). As we develop concepts, we’ll reference this specific
    architecture.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络架构预览**：一个典型的MNIST分类器可能使用：784个输入神经元（每个像素一个）→ 128个隐藏神经元 → 64个隐藏神经元 → 10个输出神经元（每个数字类别一个）。随着我们开发概念，我们将参考这个特定的架构。'
- en: Driving practical system design, each architectural choice—from how neurons
    are connected to how layers are organized—creates specific computational patterns
    that must be efficiently mapped to hardware. This mapping between network architecture
    and computational requirements is crucial for building scalable ML systems.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动实际系统设计，每个架构选择——从神经元如何连接到层如何组织——都创造了必须高效映射到硬件的特定计算模式。这种网络架构与计算需求之间的映射对于构建可扩展的机器学习系统至关重要。
- en: Nonlinear Activation Functions
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非线性激活函数
- en: 'At the heart of all neural architectures lies a basic building block: the artificial
    neuron or perceptron, which implements the biological-to-artificial translation
    principles established earlier. From a systems perspective, understanding the
    perceptron’s mathematical operations is crucial because these simple operations,
    when replicated millions of times across a network, create the computational bottlenecks
    we discussed earlier.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所有神经网络架构的核心都是一个基本构建块：人工神经元或感知器，它实现了之前建立的生物到人工的翻译原则。从系统角度来看，理解感知器的数学操作至关重要，因为这些简单的操作在网络上重复数百万次时，就形成了我们之前讨论的计算瓶颈。
- en: Consider our MNIST digit recognition task. Each pixel in a 28×28 image becomes
    an input to our network. A single neuron in the first hidden layer might learn
    to detect a specific pattern—perhaps a vertical edge that appears in digits like
    “1” or “7.” This neuron must somehow combine all 784 pixel values into a single
    output that indicates whether its pattern is present.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们的MNIST数字识别任务。28×28图像中的每个像素都成为我们网络的输入。第一隐藏层中的一个神经元可能学会检测特定的模式——比如在数字“1”或“7”中出现的垂直边缘。这个神经元必须以某种方式将所有784个像素值组合成一个输出，以指示其模式是否存在。
- en: The perceptron accomplishes this through weighted summation. It takes multiple
    inputs <semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation
    encoding="application/x-tex">x_1, x_2, ..., x_n</annotation></semantics> (in our
    case, <semantics><mrow><mi>n</mi><mo>=</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">n=784</annotation></semantics>
    pixel values), each representing a feature of the object under analysis. For digit
    recognition, these features are simply the raw pixel intensities, though for other
    tasks they might be the characteristics of a home for predicting its price or
    the attributes of a song to forecast its popularity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机通过加权求和来完成这项任务。它接受多个输入 <semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation
    encoding="application/x-tex">x_1, x_2, ..., x_n</annotation></semantics>（在我们的例子中，<semantics><mrow><mi>n</mi><mo>=</mo><mn>784</mn></mrow><annotation
    encoding="application/x-tex">n=784</annotation></semantics>个像素值），每个输入代表分析对象的一个特征。对于数字识别，这些特征仅仅是原始像素强度，而对于其他任务，它们可能是预测房价的房屋特征或预测歌曲流行度的歌曲属性。
- en: This multiplication process reveals the computational complexity beneath apparently
    simple operations. From a computational standpoint, each input requires storage
    in memory and retrieval during processing. When multiplied across millions of
    neurons in a deep network, these memory access patterns become a primary performance
    bottleneck. This is why the memory hierarchy and bandwidth considerations we discussed
    earlier are so critical to neural network performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个乘法过程揭示了看似简单的操作背后的计算复杂性。从计算的角度来看，每个输入都需要在内存中存储并在处理过程中检索。当在深度网络中的数百万个神经元上乘法时，这些内存访问模式成为主要的性能瓶颈。这就是为什么我们之前讨论的内存层次结构和带宽考虑对神经网络性能如此关键的原因。
- en: Understanding this weighted summation process, a perceptron can be configured
    to perform either regression or classification tasks. For regression, the actual
    numerical output <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> is used. For classification,
    the output depends on whether <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> crosses a certain
    threshold. If <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> exceeds this threshold,
    the perceptron might output one class (e.g., ‘yes’), and if it does not, another
    class (e.g., ‘no’).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个加权求和过程后，感知机可以被配置为执行回归或分类任务。对于回归，实际数值输出 <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> 被使用。对于分类，输出取决于 <semantics><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics>
    是否超过某个阈值。如果 <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> 超过这个阈值，感知机可能会输出一个类别（例如，“是”），如果没有，则输出另一个类别（例如，“否”）。
- en: '![](../media/file42.svg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file42.svg)'
- en: 'Figure 3.10: **Weighted Input Summation**: Perceptrons compute a weighted sum
    of multiple inputs, representing feature values, and pass the result to an activation
    function to produce an output. each input <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> is multiplied by a corresponding
    weight <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics> before being aggregated,
    forming the basis for learning complex patterns from data. using this figure.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：**加权输入求和**：感知机计算多个输入的加权总和，这些输入代表特征值，并将结果传递给激活函数以产生输出。每个输入 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> 在聚合之前被乘以相应的权重 <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics>，这构成了从数据中学习复杂模式的基础。使用此图。
- en: Visualizing these mathematical concepts, [Figure 3.10](ch009.xhtml#fig-perceptron)
    illustrates the core building blocks of a perceptron, which serves as the foundation
    for more complex neural networks. Scaling beyond individual units, layers of perceptrons
    work in concert, with each layer’s output serving as the input for the subsequent
    layer. This hierarchical arrangement creates deep learning models capable of tackling
    increasingly sophisticated tasks, from image recognition to natural language processing.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化这些数学概念，[图3.10](ch009.xhtml#fig-perceptron) 展示了感知器的基本构建块，它是更复杂神经网络的基础。超越单个单元的扩展，感知器层协同工作，每一层的输出作为下一层的输入。这种层次结构创建了能够处理越来越复杂任务的深度学习模型，从图像识别到自然语言处理。
- en: 'Breaking down the computational mechanics, each input <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> has a corresponding
    weight <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics>, and the perceptron
    simply multiplies each input by its matching weight. The intermediate output,
    <semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics>,
    is computed as the weighted sum of inputs: <semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">z
    = \sum (x_i \cdot w_{ij})</annotation></semantics>'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 拆解计算机制，每个输入 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics>
    都有一个相应的权重 <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics>，感知器只是将每个输入与其匹配的权重相乘。中间输出
    <semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics>
    是作为输入的加权总和来计算的：<semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">z
    = \sum (x_i \cdot w_{ij})</annotation></semantics>
- en: The apparent simplicity of this mathematical expression masks its computational
    complexity. When scaled across millions of neurons and billions of parameters,
    these memory access patterns become the dominant performance bottleneck in neural
    network computation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学表达式的表面简单性掩盖了其计算复杂性。当扩展到数百万个神经元和数十亿个参数时，这些内存访问模式成为神经网络计算中的主要性能瓶颈。
- en: 'Enhancing the model’s flexibility, to this intermediate calculation, a bias
    term <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    is added, allowing the model to better fit the data by shifting the linear output
    function up or down. Thus, the intermediate linear combination computed by the
    perceptron including the bias becomes: <semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow> <annotation
    encoding="application/x-tex">z = \sum (x_i \cdot w_{ij}) + b</annotation></semantics>'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强模型的灵活性，在这个中间计算中，添加了一个偏置项 <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>，允许模型通过上下移动线性输出函数来更好地拟合数据。因此，感知器计算出的中间线性组合包括偏置项变为：<semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow> <annotation
    encoding="application/x-tex">z = \sum (x_i \cdot w_{ij}) + b</annotation></semantics>
- en: This mathematical formulation directly drives the hardware requirements we discussed
    earlier. The summation requires accumulator units, the multiplications demand
    high-throughput arithmetic units, and the memory accesses necessitate high-bandwidth
    memory systems. Understanding this connection between mathematical operations
    and hardware requirements is crucial for designing efficient ML systems.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数学公式直接决定了我们之前讨论的硬件需求。求和操作需要累加器单元，乘法操作需要高吞吐量的算术单元，而内存访问则需要高带宽的内存系统。理解数学运算与硬件需求之间的这种联系对于设计高效的机器学习系统至关重要。
- en: Beyond linear transformations, activation functions are critical nonlinear transformations
    that enable neural networks to learn complex patterns by converting linear weighted
    sums into nonlinear outputs. Without activation functions, multiple linear layers
    would collapse into a single linear transformation, severely limiting the network’s
    expressive power. [Figure 3.11](ch009.xhtml#fig-activation-functions) illustrates
    the four most commonly used activation functions and their characteristic shapes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了线性变换之外，激活函数是非线性变换的关键，它通过将线性加权求和转换为非线性输出，使神经网络能够学习复杂模式。没有激活函数，多层线性层会塌缩成一个单一的线性变换，严重限制了网络的表达能力。[图3.11](ch009.xhtml#fig-activation-functions)展示了最常用的四种激活函数及其特征形状。
- en: '![](../media/file43.svg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file43.svg)'
- en: 'Figure 3.11: **Common Activation Functions**: Neural networks rely on nonlinear
    activation functions to approximate complex relationships. Each function exhibits
    distinct characteristics: sigmoid maps inputs to <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(0,1)</annotation></semantics> with smooth gradients,
    tanh provides zero-centered outputs in <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>,
    ReLU introduces sparsity by outputting zero for negative inputs, and softmax converts
    logits into probability distributions. These different behaviors enable networks
    to learn different types of patterns and relationships.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：**常见激活函数**：神经网络依赖于非线性激活函数来近似复杂关系。每个函数都表现出独特的特征：sigmoid将输入映射到<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>，具有平滑的梯度，tanh提供以零为中心的输出<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>，ReLU通过对于负输入输出零来引入稀疏性，softmax将logits转换为概率分布。这些不同的行为使得网络能够学习不同类型的模式和关系。
- en: 'The choice of activation function profoundly impacts both learning effectiveness
    and computational efficiency. Understanding the mathematical properties of each
    function is essential for designing effective neural networks. The most commonly
    used activation functions include:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择对学习效果和计算效率有深远的影响。理解每个函数的数学特性对于设计有效的神经网络至关重要。最常用的激活函数包括：
- en: Sigmoid
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The sigmoid function maps any input value to a bounded range between 0 and
    1: <semantics><mrow><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}}</annotation></semantics>'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数将任何输入值映射到0和1之间的有界范围：<semantics><mrow><mi>σ</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mrow>
    <annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}}</annotation></semantics>
- en: This S-shaped curve (visible in [Figure 3.11](ch009.xhtml#fig-activation-functions),
    top-left) produces outputs that can be interpreted as probabilities, making sigmoid
    particularly useful for binary classification tasks. For very large positive inputs,
    the function approaches 1; for very large negative inputs, it approaches 0\. The
    smooth, continuous nature of sigmoid makes it differentiable everywhere, which
    is necessary for gradient-based learning.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这种S形曲线（如图3.11（ch009.xhtml#fig-activation-functions）左上角所示）产生的输出可以解释为概率，这使得sigmoid特别适用于二元分类任务。对于非常大的正输入，函数趋近于1；对于非常大的负输入，它趋近于0。sigmoid平滑、连续的特性使其在所有地方都是可微的，这对于基于梯度的学习是必要的。
- en: 'However, sigmoid has a significant limitation: for inputs with large absolute
    values (far from zero), the gradient becomes extremely small—a phenomenon called
    the **vanishing gradient problem**[18](#fn18). During backpropagation, these small
    gradients are multiplied together across layers, causing gradients in early layers
    to become exponentially tiny. This effectively prevents learning in deep networks,
    as weight updates become negligible.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，sigmoid 函数有一个显著的局限性：对于绝对值较大的输入（远离零），梯度变得极其小——这种现象称为**梯度消失问题**[18](#fn18)。在反向传播过程中，这些小的梯度在层之间相乘，导致早期层的梯度呈指数级减小。这实际上阻止了深度网络的学习，因为权重更新变得微不足道。
- en: Sigmoid outputs are not zero-centered (all outputs are positive). This asymmetry
    can cause inefficient weight updates during optimization, as gradients for weights
    connected to sigmoid units will all have the same sign.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 的输出不是零中心（所有输出都是正数）。这种不对称性可能导致优化过程中的权重更新效率低下，因为连接到 sigmoid 单元的权重的梯度都将具有相同的符号。
- en: Tanh
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: 'The hyperbolic tangent function addresses sigmoid’s zero-centering limitation
    by mapping inputs to the range <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,
    1)</annotation></semantics>: <semantics><mrow><mo>tanh</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x +
    e^{-x}}</annotation></semantics>'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数通过将输入映射到范围 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,
    1)</annotation></semantics> 来解决 sigmoid 的零中心限制： <semantics><mrow><mo>tanh</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x +
    e^{-x}}</annotation></semantics>
- en: As shown in [Figure 3.11](ch009.xhtml#fig-activation-functions) (top-right),
    tanh produces an S-shaped curve similar to sigmoid but centered at zero. Negative
    inputs map to negative outputs, while positive inputs map to positive outputs.
    This symmetry helps balance gradient flow during training, often leading to faster
    convergence than sigmoid.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 3.11](ch009.xhtml#fig-activation-functions)（右上角）所示，tanh 产生一个类似于 sigmoid
    的 S 形曲线，但中心在零点。负输入映射到负输出，而正输入映射到正输出。这种对称性有助于在训练期间平衡梯度流动，通常比 sigmoid 导致更快的收敛。
- en: Like sigmoid, tanh is smooth and differentiable everywhere. It still suffers
    from the vanishing gradient problem for inputs with large magnitudes. When the
    function saturates (approaches -1 or 1), gradients become very small. Despite
    this limitation, tanh’s zero-centered outputs make it preferable to sigmoid for
    hidden layers in many architectures, particularly in recurrent neural networks
    where maintaining balanced activations across time steps is important.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与 sigmoid 类似，tanh 在任何地方都是平滑且可微的。它对于具有大数值的输入仍然受到梯度消失问题的困扰。当函数饱和（接近 -1 或 1）时，梯度变得非常小。尽管存在这种局限性，但
    tanh 的零中心输出使其在许多架构中比 sigmoid 更适合隐藏层，特别是在循环神经网络中，保持时间步之间的激活平衡非常重要。
- en: ReLU
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReLU
- en: 'The Rectified Linear Unit (ReLU) revolutionized deep learning by providing
    a simple solution to the vanishing gradient problem ([Nair and Hinton 2010](ch058.xhtml#ref-nair2010rectified))[19](#fn19):
    <semantics><mrow><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd
    columnalign="left" style="text-align: left"><mi>x</mi></mtd><mtd columnalign="left"
    style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>></mo><mn>0</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mn>0</mn></mtd><mtd columnalign="left"
    style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">\text{ReLU}(x) = \max(0, x) = \begin{cases}
    x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}</annotation></semantics>'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '矩形线性单元（ReLU）通过提供一个简单的解决方案来解决梯度消失问题（[Nair和Hinton 2010](ch058.xhtml#ref-nair2010rectified))[19](#fn19)：<semantics><mrow><mtext
    mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>max</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><mi>x</mi></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext
    mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>></mo><mn>0</mn></mtd></mtr><mtr><mtd
    columnalign="left" style="text-align: left"><mn>0</mn></mtd><mtd columnalign="left"
    style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">\text{ReLU}(x) = \max(0, x) = \begin{cases}
    x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}</annotation></semantics>'
- en: '[Figure 3.11](ch009.xhtml#fig-activation-functions) (bottom-left) shows ReLU’s
    characteristic shape: a straight line for positive inputs and zero for negative
    inputs. This simplicity provides several advantages:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.11](ch009.xhtml#fig-activation-functions)（左下角）显示了ReLU的特征形状：对于正输入是直线，对于负输入是零。这种简单性提供了几个优点：'
- en: '**Gradient Flow**: For positive inputs, ReLU’s gradient is exactly 1, allowing
    gradients to flow unchanged through the network. This prevents the vanishing gradient
    problem that plagues sigmoid and tanh in deep architectures.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度流**：对于正输入，ReLU的梯度正好是1，允许梯度在网络中不变地流动。这防止了sigmoid和tanh在深层架构中遇到的梯度消失问题。'
- en: '**Sparsity**: By setting all negative activations to zero, ReLU introduces
    natural sparsity in the network. Typically, about 50% of neurons in a ReLU network
    output zero for any given input. This sparsity can help reduce overfitting and
    makes the network more interpretable.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏性**：通过将所有负激活设置为零，ReLU在网络上引入了自然稀疏性。通常，ReLU网络中的大约50%的神经元对于任何给定的输入都会输出零。这种稀疏性有助于减少过拟合，并使网络更具可解释性。'
- en: '**Computational Efficiency**: Unlike sigmoid and tanh, which require expensive
    exponential calculations, ReLU is computed with a simple comparison and conditional
    operation: `output = (input > 0) ? input : 0`. This simplicity translates to faster
    computation and lower energy consumption, particularly important for deployment
    on resource-constrained devices.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算效率**：与需要昂贵的指数计算的sigmoid和tanh函数不同，ReLU通过简单的比较和条件操作进行计算：`output = (input
    > 0) ? input : 0`。这种简单性转化为更快的计算和更低的能耗，这对于在资源受限的设备上部署尤为重要。'
- en: ReLU is not without drawbacks. The **dying ReLU problem** occurs when neurons
    become “stuck” outputting zero. If a neuron’s weights are updated such that its
    weighted input is consistently negative, the neuron outputs zero and contributes
    zero gradient during backpropagation. This neuron effectively becomes non-functional
    and can never recover. Careful initialization and learning rate selection help
    mitigate this issue.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU并非没有缺点。**死亡ReLU问题**发生在神经元“卡住”并输出零的情况下。如果一个神经元的权重更新使得其加权输入始终为负，该神经元将输出零，并在反向传播期间贡献零梯度。这个神经元实际上变得非功能性，并且永远无法恢复。仔细初始化和学习率选择有助于减轻这个问题。
- en: Softmax
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Softmax
- en: 'Unlike the previous activation functions that operate independently on each
    value, softmax considers all values simultaneously to produce a probability distribution:
    <semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K
    e^{z_j}}</annotation></semantics>'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前独立作用于每个值的激活函数不同，softmax同时考虑所有值以产生一个概率分布：<semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K
    e^{z_j}}</annotation></semantics>
- en: For a vector of <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>
    values (often called logits), softmax transforms them into <semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics> probabilities that sum
    to 1\. [Figure 3.11](ch009.xhtml#fig-activation-functions) (bottom-right) shows
    one component of the softmax output; in practice, softmax processes entire vectors
    where each element’s output depends on all input values.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个包含<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>个值（通常称为logits）的向量，softmax将其转换为<semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics>个概率，这些概率之和为1。[图3.11](ch009.xhtml#fig-activation-functions)（右下角）显示了softmax输出的一个组成部分；在实践中，softmax处理整个向量，其中每个元素的输出都取决于所有输入值。
- en: Softmax is almost exclusively used in the output layer for multi-class classification
    problems. By converting arbitrary real-valued logits into probabilities, softmax
    enables the network to express confidence across multiple classes. The class with
    the highest probability becomes the predicted class. The exponential function
    ensures that larger logits receive disproportionately higher probabilities, creating
    clear distinctions between classes when the network is confident.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax几乎仅用于多类分类问题的输出层。通过将任意实值logits转换为概率，softmax使网络能够在多个类别中表达信心。概率最高的类别成为预测类别。指数函数确保较大的logits获得不成比例的高概率，当网络有信心时，这会在类别之间创建清晰的区分。
- en: The mathematical relationship between input logits and output probabilities
    is differentiable, allowing gradients to flow back through softmax during training.
    When combined with cross-entropy loss (discussed in [Chapter 8](ch014.xhtml#sec-ai-training)),
    softmax produces particularly clean gradient expressions that guide learning effectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 输入logits与输出概率之间的数学关系是可微分的，这使得在训练期间梯度可以通过softmax反向传播。当与交叉熵损失（在第8章中讨论）结合使用时，softmax产生特别干净的梯度表达式，有效地指导学习。
- en: '**Systems Perspective: Activation Functions and Hardware**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统视角：激活函数与硬件**'
- en: '**Why ReLU Dominates in Practice**: Beyond its mathematical benefits like avoiding
    vanishing gradients, ReLU’s hardware efficiency explains its widespread adoption.
    Computing <semantics><mrow><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>
    requires a single comparison operation, while sigmoid and tanh require computing
    exponentials—operations that are orders of magnitude more expensive in both time
    and energy. This computational simplicity means ReLU can be executed faster on
    any processor and consumes significantly less power, a critical consideration
    for battery-powered devices. The computational and hardware implications of activation
    functions, including performance benchmarks and implementation strategies for
    modern accelerators, are explored in [Chapter 8](ch014.xhtml#sec-ai-training).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么ReLU在实践中的统治地位**：除了避免梯度消失等数学优势之外，ReLU的硬件效率解释了其广泛的应用。计算<semantics><mrow><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>只需要一个比较操作，而sigmoid和tanh需要计算指数——这些操作在时间和能量上都要昂贵得多。这种计算简单性意味着ReLU可以在任何处理器上更快地执行，并且消耗的电量显著减少，这对于电池供电设备来说是一个关键考虑因素。激活函数的计算和硬件影响，包括性能基准和现代加速器的实现策略，将在第8章[Chapter 8](ch014.xhtml#sec-ai-training)中探讨。'
- en: '![](../media/file44.svg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file44.svg)'
- en: 'Figure 3.12: **Non-Linear Activation**: Neural networks model complex relationships
    by applying non-linear activation functions to weighted sums of inputs, enabling
    the representation of non-linear decision boundaries. These functions transform
    input values, creating the capacity to learn intricate patterns beyond linear
    combinations via the arrangement of points. Source: Medium, sachin kaushik.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：**非线性激活**：神经网络通过将输入的加权和应用非线性激活函数来模拟复杂关系，从而能够表示非线性决策边界。这些函数通过点的排列转换输入值，从而具有学习复杂模式的能力，这些模式超出了线性组合的范围。来源：Medium，sachin
    kaushik。
- en: 'As detailed in the activation function section above, these nonlinear transformations
    convert the linear input sum into a non-linear output: <semantics><mrow><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = \sigma(z)</annotation></semantics>'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的激活函数部分详细说明，这些非线性变换将线性输入和转换成非线性输出：<semantics><mrow><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = \sigma(z)</annotation></semantics>
- en: 'Thus, the final output of the perceptron, including the activation function,
    can be expressed as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，感知器的最终输出，包括激活函数，可以表示为：
- en: '[Figure 3.12](ch009.xhtml#fig-nonlinear) shows an example where data exhibit
    a nonlinear pattern that could not be adequately modeled with a linear approach,
    demonstrating why the nonlinear activation functions discussed earlier are essential
    for complex pattern recognition.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.12](ch009.xhtml#fig-nonlinear)展示了数据表现出非线性模式，而线性方法无法充分模拟的例子，这说明了为什么前面讨论的非线性激活函数对于复杂模式识别是至关重要的。'
- en: The universal approximation theorem[20](#fn20) establishes that neural networks
    with activation functions can approximate arbitrary functions. This theoretical
    foundation, combined with the computational and optimization characteristics of
    specific activation functions like ReLU and sigmoid discussed above, explains
    neural networks’ practical effectiveness in complex tasks.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化逼近定理[20](#fn20)确立了具有激活函数的神经网络可以逼近任意函数。这一理论基础，结合上述讨论的ReLU和sigmoid等特定激活函数的计算和优化特性，解释了神经网络在复杂任务中的实际有效性。
- en: 'Combining the linear combination with the activation function, the complete
    perceptron computation is: <semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mo>∑</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = \sigma\left(\sum (x_i \cdot w_{ij}) + b\right)</annotation></semantics>'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 将线性组合与激活函数相结合，完整的感知器计算如下：<semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mo>∑</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = \sigma\left(\sum (x_i \cdot w_{ij}) + b\right)</annotation></semantics>
- en: Layers and Connections
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层和连接
- en: While a single perceptron can model simple decisions, the power of neural networks
    comes from combining multiple neurons into layers. A layer is a collection of
    neurons that process information in parallel. Each neuron in a layer operates
    independently on the same input but with its own set of weights and bias, allowing
    the layer to learn different features or patterns from the same input data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单个感知器可以模拟简单的决策，但神经网络的力量来自于将多个神经元组合成层。层是一组并行处理信息的神经元。层中的每个神经元独立地对相同的输入进行操作，但具有自己的一套权重和偏置，这使得层能够从相同的数据中学习不同的特征或模式。
- en: 'In a typical neural network, we organize these layers hierarchically:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的神经网络中，我们以层次化的方式组织这些层：
- en: '**Input Layer**: Receives the raw data features'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入层**：接收原始数据特征'
- en: '**Hidden Layers**: Process and transform the data through multiple stages'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐藏层**：通过多个阶段处理和转换数据'
- en: '**Output Layer**: Produces the final prediction or decision'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出层**：生成最终的预测或决策'
- en: '[Figure 3.13](ch009.xhtml#fig-layers) illustrates this layered architecture.
    When data flows through these layers, each successive layer transforms the representation
    of the data, gradually building more complex and abstract features. This hierarchical
    processing is what gives deep neural networks their remarkable ability to learn
    complex patterns.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.13](ch009.xhtml#fig-layers)展示了这种分层架构。当数据通过这些层流动时，每一层都会转换数据的表示，逐渐构建更复杂和抽象的特征。这种层次化处理赋予了深度神经网络学习复杂模式非凡的能力。'
- en: '![](../media/file45.svg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file45.svg)'
- en: 'Figure 3.13: **Layered Network Architecture**: Deep neural networks transform
    data through successive layers, enabling the extraction of increasingly complex
    features and patterns. each layer applies non-linear transformations to the outputs
    of the previous layer, ultimately mapping raw inputs to desired outputs. Source:
    brunellon.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：**分层网络架构**：深度神经网络通过连续的层转换数据，使提取越来越复杂的特征和模式成为可能。每一层都对前一层的输出应用非线性转换，最终将原始输入映射到期望的输出。来源：brunellon。
- en: Data Flow Through Network Layers
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据在网络层中的流动
- en: 'As data flows through the network, it is transformed at each layer to extract
    meaningful patterns. The weighted summation and activation process we established
    for individual neurons scales up: each layer applies these operations in parallel
    across all its neurons, with outputs from one layer becoming inputs to the next.
    This creates a hierarchical pipeline where simple features detected in early layers
    combine into increasingly complex patterns in deeper layers—enabling neural networks
    to learn sophisticated representations from raw data.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据通过网络流动时，在每一层都会进行转换以提取有意义的模式。我们为单个神经元建立的加权求和和激活过程扩展到整个网络：每一层都对其所有神经元并行应用这些操作，一个层的输出成为下一层的输入。这创建了一个层次化的管道，其中早期层检测到的简单特征在深层层中组合成越来越复杂的模式——使神经网络能够从原始数据中学习复杂的表示。
- en: Parameters and Connections
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数和连接
- en: The learnable parameters of neural networks consist primarily of weights and
    biases, which together determine how information flows through the network and
    how transformations are applied to input data. This section examines how these
    parameters are organized and structured within neural networks. We explore weight
    matrices that connect layers, connection patterns that define network topology,
    bias terms that provide flexibility in transformations, and parameter organization
    strategies that enable efficient computation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的可学习参数主要由权重和偏置组成，它们共同决定了信息在网络中的流动方式以及如何对输入数据进行变换。本节探讨了这些参数在神经网络中的组织和结构。我们探讨了连接层的权重矩阵，定义网络拓扑的连接模式，提供变换灵活性的偏置项，以及实现高效计算的参数组织策略。
- en: Weight Matrices
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重矩阵
- en: Weights determine how strongly inputs influence neuron outputs. In larger networks,
    these organize into matrices for efficient computation across layers. For example,
    in a layer with <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    input features and <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    neurons, the weights form a matrix <semantics><mrow><mi>𝐖</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{n \times m}</annotation></semantics>.
    Each column in this matrix represents the weights for a single neuron in the layer.
    This organization allows the network to process multiple inputs simultaneously,
    an essential feature for handling real-world data efficiently.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 权重决定了输入如何强烈地影响神经元输出。在较大的网络中，这些权重会组织成矩阵，以便在层之间进行高效的计算。例如，在一个具有<semantics><mi>n</mi><annotation
    encoding="application/x-tex">n</annotation></semantics>个输入特征和<semantics><mi>m</mi><annotation
    encoding="application/x-tex">m</annotation></semantics>个神经元的层中，权重形成一个矩阵<semantics><mrow><mi>𝐖</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{n \times m}</annotation></semantics>。这个矩阵中的每一列代表层中单个神经元的权重。这种组织方式使得网络能够同时处理多个输入，这对于有效地处理现实世界数据是一个基本特征。
- en: 'Recall that for a single neuron, we computed <semantics><mrow><mi>z</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">z = \sum_{i=1}^n (x_i \cdot w_{ij}) + b</annotation></semantics>.
    When we have a layer of <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    neurons, we could compute each neuron’s output separately, but matrix operations
    provide a much more efficient approach. Rather than computing each neuron individually,
    matrix multiplication enables us to compute all <semantics><mi>m</mi><annotation
    encoding="application/x-tex">m</annotation></semantics> outputs simultaneously:
    <semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow>
    <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} +
    \mathbf{b}</annotation></semantics>'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，对于一个单个神经元，我们计算了<semantics><mrow><mi>z</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation
    encoding="application/x-tex">z = \sum_{i=1}^n (x_i \cdot w_{ij}) + b</annotation></semantics>。当我们有一个具有<semantics><mi>m</mi><annotation
    encoding="application/x-tex">m</annotation></semantics>个神经元的层时，我们可以分别计算每个神经元的输出，但矩阵运算提供了一种更有效的方法。而不是单独计算每个神经元，矩阵乘法使我们能够同时计算所有<semantics><mi>m</mi><annotation
    encoding="application/x-tex">m</annotation></semantics>个输出：<semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow>
    <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} +
    \mathbf{b}</annotation></semantics>
- en: This matrix organization is more than just mathematical convenience; it reflects
    how modern neural networks are implemented for efficiency. Each weight <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics> represents the strength
    of the connection between input feature <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>
    and neuron <semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics>
    in the layer.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种矩阵组织不仅仅是数学上的便利，它反映了现代神经网络为了效率而实施的实现方式。每个权重<semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation
    encoding="application/x-tex">w_{ij}</annotation></semantics>代表了输入特征<semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>和层中神经元<semantics><mi>j</mi><annotation
    encoding="application/x-tex">j</annotation></semantics>之间连接的强度。
- en: Network Connectivity Architectures
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络连接架构
- en: In the simplest and most common case, each neuron in a layer is connected to
    every neuron in the previous layer, forming what we call a “dense” or “fully-connected”
    layer. This pattern means that each neuron has the opportunity to learn from all
    available features from the previous layer. While this chapter focuses on fully-connected
    layers to establish foundational principles, alternative connectivity patterns
    (explored in [Chapter 4](ch010.xhtml#sec-dnn-architectures)) can dramatically
    improve efficiency for structured data by restricting connections based on problem
    characteristics.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单和最常见的情况下，一个层中的每个神经元都与前一层的每个神经元相连，形成了我们所说的“密集”或“全连接”层。这种模式意味着每个神经元都有机会从前一层的所有可用特征中学习。虽然本章专注于全连接层以建立基础原理，但替代的连接模式（在第4章[第4章](ch010.xhtml#sec-dnn-architectures)中探讨）可以通过根据问题特征限制连接来显著提高结构化数据的效率。
- en: '[Figure 3.14](ch009.xhtml#fig-connections) illustrates these dense connections
    between layers. For a network with layers of sizes <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>3</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1,
    n_2, n_3)</annotation></semantics>, the weight matrices would have dimensions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.14](ch009.xhtml#fig-connections)展示了这些层之间的密集连接。对于一个具有大小为<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>3</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1,
    n_2, n_3)</annotation></semantics>的网络的权重矩阵将具有以下维度：'
- en: 'Between first and second layer: <semantics><mrow><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics>'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一层和第二层之间：<semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics>
- en: 'Between second and third layer: <semantics><mrow><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二层和第三层之间：<semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>
- en: '![](../media/file46.svg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file46.svg)'
- en: 'Figure 3.14: **Fully-Connected Layers**: Multilayer perceptrons (MLPs) utilize
    dense connections between layers, enabling each neuron to integrate information
    from all neurons in the preceding layer. The weight matrices defining these connections—<semantics><mrow><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics>
    and <semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>—determine
    the strength of these integrations and facilitate learning complex patterns from
    input data. Source: J. McCaffrey.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：**全连接层**：多层感知器（MLPs）利用层之间的密集连接，使每个神经元能够整合前一层的所有神经元的信 息。定义这些连接的权重矩阵—<semantics><mrow><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics>
    和 <semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>—决定了这些整合的强度，并有助于从输入数据中学习复杂的模式。来源：J.
    McCaffrey。
- en: Bias Terms
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏置项
- en: Each neuron in a layer also has an associated bias term. While weights determine
    the relative importance of inputs, biases allow neurons to shift their activation
    functions. This shifting is crucial for learning, as it gives the network flexibility
    to fit more complex patterns.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 层中的每个神经元也都有一个相关的偏置项。虽然权重决定了输入的相对重要性，但偏置允许神经元移动其激活函数。这种移动对于学习至关重要，因为它为网络提供了适应更复杂模式的灵活性。
- en: 'For a layer with <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    neurons, the bias terms form a vector <semantics><mrow><mi>𝐛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{b} \in \mathbb{R}^m</annotation></semantics>.
    When we compute the layer’s output, this bias vector is added to the weighted
    sum of inputs: <semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow>
    <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} +
    \mathbf{b}</annotation></semantics>'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>
    个神经元的层，偏置项形成一个向量 <semantics><mrow><mi>𝐛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{b} \in \mathbb{R}^m</annotation></semantics>。当我们计算层的输出时，这个偏置向量被添加到输入的加权和：<semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow>
    <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} +
    \mathbf{b}</annotation></semantics>
- en: The bias terms[21](#fn21) effectively allow each neuron to have a different
    “threshold” for activation, making the network more expressive.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置项[21](#fn21)有效地允许每个神经元具有不同的“阈值”以激活，这使得网络更具表现力。
- en: Weight and Bias Storage Organization
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重和偏置存储组织
- en: 'The organization of weights and biases across a neural network follows a systematic
    pattern. For a network with <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    layers, we maintain:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中权重和偏置的组织遵循一种系统模式。对于一个具有 <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    层的网络，我们保持：
- en: A weight matrix <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>
    for each layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层的权重矩阵 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>
- en: A bias vector <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
    for each layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    的偏置向量 <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
- en: Activation functions <semantics><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">f^{(l)}</annotation></semantics>
    for each layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    的激活函数 <semantics><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">f^{(l)}</annotation></semantics>
- en: 'This gives us the complete layer computation: <semantics><mrow><msup><mi>𝐡</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐡</mi><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{h}^{(l)}
    = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})</annotation></semantics>
    Where <semantics><msup><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics>
    represents the layer’s output after applying the activation function.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了完整的层计算：<semantics><mrow><msup><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐡</mi><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{h}^{(l)}
    = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})</annotation></semantics>
    其中 <semantics><msup><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics>
    表示应用激活函数后的层的输出。
- en: '**Checkpoint: Neural Network Architecture Fundamentals**'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查点：神经网络架构基础**'
- en: 'Before proceeding to network topology and training, verify your understanding
    of the foundational concepts we’ve covered:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续到网络拓扑和训练之前，验证你对我们已经涵盖的基础概念的理解：
- en: '**Core Concepts:**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**核心概念**：'
- en: '**Systems Implications:**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统影响**：'
- en: '**Self-Test Example**: For a digit recognition network with layers 784→100→10,
    calculate: (1) parameters in each weight matrix, (2) total parameter count, (3)
    activations stored during inference for a single image.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**自测示例**：对于一个具有784→100→10层的数字识别网络，计算：(1)每个权重矩阵中的参数数量，(2)总参数数量，(3)在推理过程中存储的单个图像的激活。'
- en: '*If any of these feel unclear, review [Section 3.4](ch009.xhtml#sec-dl-primer-neural-network-fundamentals-68cd)
    (Neural Network Fundamentals), [Section 3.4.1.1](ch009.xhtml#sec-dl-primer-nonlinear-activation-functions-868a)
    (Neurons and Activations), or [Section 3.4.2](ch009.xhtml#sec-dl-primer-parameters-connections-6c54)
    (Weights and Biases) before continuing. The upcoming sections on training and
    optimization build directly on these foundations.*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果这些内容有任何不清楚的地方，请在继续之前回顾[第3.4节](ch009.xhtml#sec-dl-primer-neural-network-fundamentals-68cd)（神经网络基础）、[第3.4.1.1节](ch009.xhtml#sec-dl-primer-nonlinear-activation-functions-868a)（神经元和激活）或[第3.4.2节](ch009.xhtml#sec-dl-primer-parameters-connections-6c54)（权重和偏差）。接下来的关于训练和优化的章节直接建立在这些基础上。*'
- en: Architecture Design
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构设计
- en: Network topology describes how individual neurons organize into layers and connect
    to form complete neural networks. Building intuition begins with a simple problem
    that became famous in AI history[22](#fn22).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑描述了单个神经元如何组织成层并连接形成完整的神经网络。构建直觉从AI历史中一个简单的问题开始，这个问题变得闻名[22](#fn22)。
- en: Consider a network learning the XOR function—a classic problem that requires
    non-linearity. With inputs <semantics><msub><mi>x</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">x_1</annotation></semantics> and <semantics><msub><mi>x</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">x_2</annotation></semantics> that can be 0 or 1,
    XOR outputs 1 when inputs differ and 0 when they’re the same.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个学习XOR函数的网络——这是一个需要非线性的经典问题。输入 <semantics><msub><mi>x</mi><mn>1</mn></msub><annotation
    encoding="application/x-tex">x_1</annotation></semantics> 和 <semantics><msub><mi>x</mi><mn>2</mn></msub><annotation
    encoding="application/x-tex">x_2</annotation></semantics> 可以是0或1，当输入不同时XOR输出1，当它们相同时输出0。
- en: '**Network Structure**: 2 inputs → 2 hidden neurons → 1 output'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络结构**：2个输入 → 2个隐藏神经元 → 1个输出'
- en: '**Forward Pass Example**: For inputs <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1,
    0)</annotation></semantics>:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传递示例**：对于输入 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1,
    0)</annotation></semantics>:'
- en: 'Hidden neuron 1: <semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>=</mo><mtext
    mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>12</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_1
    = \text{ReLU}(1 \cdot w_{11} + 0 \cdot w_{12} + b_1)</annotation></semantics>'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏神经元1：<semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>12</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_1
    = \text{ReLU}(1 \cdot w_{11} + 0 \cdot w_{12} + b_1)</annotation></semantics>
- en: 'Hidden neuron 2: <semantics><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>=</mo><mtext
    mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>21</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>22</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_2
    = \text{ReLU}(1 \cdot w_{21} + 0 \cdot w_{22} + b_2)</annotation></semantics>'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏神经元2：<semantics><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>21</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>22</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_2
    = \text{ReLU}(1 \cdot w_{21} + 0 \cdot w_{22} + b_2)</annotation></semantics>
- en: 'Output: <semantics><mrow><mi>y</mi><mo>=</mo><mtext mathvariant="normal">sigmoid</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>31</mn></msub><mo>+</mo><msub><mi>h</mi><mn>2</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>32</mn></msub><mo>+</mo><msub><mi>b</mi><mn>3</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y
    = \text{sigmoid}(h_1 \cdot w_{31} + h_2 \cdot w_{32} + b_3)</annotation></semantics>'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：<semantics><mrow><mi>y</mi><mo>=</mo><mtext mathvariant="normal">sigmoid</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>31</mn></msub><mo>+</mo><msub><mi>h</mi><mn>2</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>32</mn></msub><mo>+</mo><msub><mi>b</mi><mn>3</mn></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y
    = \text{sigmoid}(h_1 \cdot w_{31} + h_2 \cdot w_{32} + b_3)</annotation></semantics>
- en: This simple network demonstrates how hidden layers enable learning non-linear
    patterns—something a single layer cannot achieve.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的网络展示了隐藏层如何使学习非线性模式成为可能——这是单层无法实现的。
- en: The XOR example established the fundamental three-layer architecture, but real-world
    networks require systematic consideration of design constraints and computational
    scale[23](#fn23). Recognizing handwritten digits using the MNIST ([Lecun et al.
    1998](ch058.xhtml#ref-lecun1998gradient))[24](#fn24) dataset illustrates how problem
    structure determines network dimensions while hidden layer configuration remains
    a critical design decision.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: XOR示例建立了基本的三层架构，但现实世界的网络需要系统地考虑设计约束和计算规模[23](#fn23)。使用MNIST([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient))[24](#fn24)数据集识别手写数字说明了问题结构如何决定网络维度，同时隐藏层配置仍然是一个关键的设计决策。
- en: Feedforward Network Architecture
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前馈网络架构
- en: Applying the three-layer architecture to MNIST reveals how data characteristics
    and task requirements constrain network design. As shown in [Figure 3.15](ch009.xhtml#fig-mnist-topology-1)<semantics><mtext
    mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics>,
    a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times
    28</annotation></semantics> pixel grayscale image of a handwritten digit must
    be processed through input, hidden, and output layers to produce a classification
    output.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 将三层架构应用于MNIST揭示了数据特性和任务需求如何限制网络设计。如图[图3.15](ch009.xhtml#fig-mnist-topology-1)<semantics><mtext
    mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics>所示，一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素的手写数字灰度图像必须通过输入、隐藏和输出层进行处理，以产生分类输出。
- en: The input layer’s width is directly determined by our data format. As shown
    in [Figure 3.15](ch009.xhtml#fig-mnist-topology-1)<semantics><mtext mathvariant="normal">b)</mtext><annotation
    encoding="application/x-tex">\text{b)}</annotation></semantics>, for a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel image,
    each pixel becomes an input feature, requiring 784 input neurons <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>28</mn><mo>×</mo><mn>28</mn><mo>=</mo><mn>784</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times
    28 = 784)</annotation></semantics>. We can think of this either as a 2D grid of
    pixels or as a flattened vector of 784 values, where each value represents the
    intensity of one pixel.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的宽度直接由我们的数据格式决定。如图[图3.15](ch009.xhtml#fig-mnist-topology-1)<semantics><mtext
    mathvariant="normal">b)</mtext><annotation encoding="application/x-tex">\text{b)}</annotation></semantics>所示，对于一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素的图像，每个像素成为一个输入特征，需要784个输入神经元<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>28</mn><mo>×</mo><mn>28</mn><mo>=</mo><mn>784</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times
    28 = 784)</annotation></semantics>。我们可以将其视为一个2D像素网格，或者是一个784个值的扁平向量，其中每个值代表一个像素的强度。
- en: The output layer’s structure is determined by our task requirements. For digit
    classification, we use 10 output neurons, one for each possible digit (0-9). When
    presented with an image, the network produces a value for each output neuron,
    where higher values indicate greater confidence that the image represents that
    particular digit.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的结构由我们的任务需求决定。对于数字分类，我们使用10个输出神经元，每个可能的数字（0-9）一个。当呈现一个图像时，网络为每个输出神经元产生一个值，其中更高的值表示图像代表该特定数字的可能性更大。
- en: Between these fixed input and output layers, we have flexibility in designing
    the hidden layer topology. The choice of hidden layer structure, including the
    number of layers to use and their respective widths, represents one of the key
    design decisions in neural networks. Additional layers increase the network’s
    depth, allowing it to learn more abstract features through successive transformations.
    The width of each layer provides capacity for learning different features at each
    level of abstraction.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些固定的输入和输出层之间，我们在设计隐藏层拓扑结构方面具有灵活性。隐藏层结构的选择，包括要使用的层数及其相应的宽度，是神经网络中的关键设计决策之一。增加额外的层增加了网络的深度，使其能够通过连续的转换学习更抽象的特征。每一层的宽度提供了在每个抽象层次上学习不同特征的能力。
- en: '![](../media/file47.svg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file47.svg)'
- en: 'Figure 3.15: <semantics><mtext mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics>
    A neural network topology for classifying MNIST digits, showing how a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel image
    is processed. The image on the left shows the original digit, with dimensions
    labeled. The network on the right shows how each pixel connects to the hidden
    layers, ultimately producing 10 outputs for digit classification. <semantics><mtext
    mathvariant="normal">b)</mtext><annotation encoding="application/x-tex">\text{b)}</annotation></semantics>
    Alternative visualization of the MNIST network topology, showing how the 2D image
    is flattened into a 784-dimensional vector before being processed by the network.
    This representation emphasizes how spatial data is transformed into a format suitable
    for neural network processing.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.15: <semantics><mtext mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics>
    用于分类MNIST数字的神经网络拓扑，展示了如何处理一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素图像。左侧的图像显示了原始数字，并标注了尺寸。右侧的网络显示了每个像素如何连接到隐藏层，最终产生10个输出以进行数字分类。
    <semantics><mtext mathvariant="normal">b)</mtext><annotation encoding="application/x-tex">\text{b)}</annotation></semantics>
    MNIST网络拓扑的另一种可视化，展示了二维图像在网络处理之前被展平成一个784维向量。这种表示强调了空间数据是如何转换成适合神经网络处理格式的。'
- en: These basic topological choices have significant implications for both the network’s
    capabilities and its computational requirements. Each additional layer or neuron
    increases the number of parameters that must be stored and computed during both
    training and inference. However, without sufficient depth or width, the network
    may lack the capacity to learn complex patterns in the data.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本的拓扑选择对网络的性能和计算需求都有重大影响。每一层或神经元的增加都会增加在训练和推理过程中必须存储和计算的数量。然而，如果没有足够的深度或宽度，网络可能缺乏学习数据中复杂模式的能力。
- en: 'Design Trade-offs: Depth vs Width vs Performance'
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计权衡：深度 vs 宽度 vs 性能
- en: 'The design of neural network topology centers on three key decisions: the number
    of layers (depth), the size of each layer (width), and how these layers connect.
    Each choice affects both the network’s learning capability and its computational
    requirements.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络拓扑的设计集中在三个关键决策上：层数（深度）、每层的大小（宽度）以及这些层如何连接。每个选择都会影响网络的学习能力及其计算需求。
- en: 'Network depth determines achievable abstraction: stacked layers build increasingly
    complex features through successive transformations. For MNIST, shallow layers
    detect edges, intermediate layers combine edges into strokes, and deep layers
    assemble complete digit patterns. However, additional depth increases computational
    cost, training difficulty (vanishing gradients), and architectural complexity
    without guaranteed benefits.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 网络深度决定了可达到的抽象程度：堆叠的层通过连续的转换构建越来越复杂的特征。对于MNIST，浅层检测边缘，中间层将边缘组合成笔画，深层组装成完整的数字模式。然而，增加深度会增加计算成本、训练难度（梯度消失）和架构复杂性，而不会带来保证的好处。
- en: The width of each layer, which is determined by the number of neurons it contains,
    controls how much information the network can process in parallel at each stage.
    Wider layers can learn more features simultaneously but require proportionally
    more parameters and computation. For instance, if a hidden layer is processing
    edge features in our digit recognition task, its width determines how many different
    edge patterns it can detect simultaneously.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的宽度，由它包含的神经元数量决定，控制了网络在每一阶段可以并行处理多少信息。更宽的层可以同时学习更多特征，但需要成比例更多的参数和计算。例如，如果隐藏层在我们的数字识别任务中处理边缘特征，其宽度决定了它可以同时检测多少不同的边缘模式。
- en: A very important consideration in topology design is the total parameter count.
    For a network with layers of size <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>n</mi><mi>L</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1,
    n_2, \ldots, n_L)</annotation></semantics>, each pair of adjacent layers <semantics><mi>l</mi><annotation
    encoding="application/x-tex">l</annotation></semantics> and <semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">l+1</annotation></semantics> requires <semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">n_l \times n_{l+1}</annotation></semantics> weight
    parameters, plus <semantics><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation
    encoding="application/x-tex">n_{l+1}</annotation></semantics> bias parameters.
    These parameters must be stored in memory and updated during training, making
    the parameter count a key constraint in practical applications.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在拓扑设计中的一个非常重要的考虑因素是总参数数。对于一个具有大小为<semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>n</mi><mi>L</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1,
    n_2, \ldots, n_L)</annotation></semantics>的层的网络，每一对相邻层<semantics><mi>l</mi><annotation
    encoding="application/x-tex">l</annotation></semantics>和<semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">l+1</annotation></semantics>需要<semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">n_l \times n_{l+1}</annotation></semantics>个权重参数，以及<semantics><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation
    encoding="application/x-tex">n_{l+1}</annotation></semantics>个偏置参数。这些参数必须在内存中存储并在训练过程中更新，这使得参数数成为实际应用中的一个关键约束。
- en: Network design requires balancing learning capacity, computational efficiency,
    and training tractability. While the basic approach connects every neuron to every
    neuron in the next layer (fully connected), this does not always represent the
    most effective strategy. The fully-connected approach assumes every input element
    may interact with every other—yet real-world data rarely exhibits such unconstrained
    relationships.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 网络设计需要平衡学习容量、计算效率和训练可追踪性。虽然基本方法是将每个神经元连接到下一层的每个神经元（全连接），但这并不总是最有效的策略。全连接方法假设每个输入元素都可能与其他每个元素交互——然而，现实世界的数据很少表现出这种无约束的关系。
- en: 'Consider the MNIST example: a 28×28 image has 784 pixels, creating 306,936
    possible pixel pairs (<semantics><mfrac><mrow><mn>784</mn><mo>×</mo><mn>783</mn></mrow><mn>2</mn></mfrac><annotation
    encoding="application/x-tex">\frac{784 \times 783}{2}</annotation></semantics>).
    A fully-connected first layer with 100 neurons learns 78,400 weights, effectively
    examining every possible pixel relationship. Neighboring pixels (forming edges
    of digits) interact more than pixels at opposite corners. Fully-connected layers
    spend parameters and computation learning that pixel (1,1) doesn’t interact strongly
    with pixel (28,28), relationships we could encode structurally. Specialized architectures
    (explored in [Chapter 4](ch010.xhtml#sec-dnn-architectures)) address this inefficiency
    by restricting connections based on problem structure, achieving superior results
    with 10-100× fewer parameters by exploiting spatial locality, temporal ordering,
    or other domain-specific patterns.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑MNIST示例：一个28×28的图像有784个像素，可以形成306,936个可能的像素对（<semantics><mfrac><mrow><mn>784</mn><mo>×</mo><mn>783</mn></mrow><mn>2</mn></mfrac><annotation
    encoding="application/x-tex">\frac{784 \times 783}{2}</annotation></semantics>）。一个包含100个神经元的全连接第一层学习78,400个权重，实际上检查了每个可能的像素关系。相邻像素（形成数字的边缘）之间的交互比相对角上的像素更多。全连接层通过参数和计算学习像素（1,1）与像素（28,28）之间没有强烈的交互，这些关系我们可以通过结构编码。专门的架构（在第4章中探讨）通过根据问题结构限制连接来解决这个问题的不效率，通过利用空间局部性、时间顺序或其他特定领域的模式，以10-100×更少的参数实现更优的结果。
- en: Information flow through the network represents another important consideration.
    While the basic flow proceeds from input to output, some network designs include
    additional paths such as skip connections or residual connections. These alternative
    paths facilitate training and improve effectiveness at learning complex patterns
    by functioning as shortcuts that enable more direct information flow when needed,
    analogous to how the human brain combines detailed and general impressions during
    object recognition.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 信息在网络中的流动代表了另一个重要的考虑因素。虽然基本流程是从输入到输出，但一些网络设计包括额外的路径，如跳跃连接或残差连接。这些替代路径通过作为在需要时允许更直接信息流的捷径来促进训练并提高学习复杂模式的有效性，类似于人类大脑在物体识别过程中结合详细和一般印象的方式。
- en: These design decisions have significant practical implications including memory
    usage for storing network parameters, computational costs during both training
    and inference, training behavior and convergence, and the network’s ability to
    generalize to new examples. The optimal balance of these trade-offs depends heavily
    on the specific problem, available computational resources, and dataset characteristics.
    Successful network design requires careful consideration of these factors against
    practical constraints.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设计决策具有重大的实际意义，包括存储网络参数的内存使用、训练和推理过程中的计算成本、训练行为和收敛性，以及网络泛化到新示例的能力。这些权衡的最佳平衡在很大程度上取决于具体问题、可用的计算资源以及数据集特征。成功的网络设计需要对这些因素进行仔细考虑，同时考虑实际约束。
- en: 'With our understanding of network architecture established—how neurons connect
    into layers, how layers stack into networks, and how design choices affect computational
    requirements—we can now address the central question: how do these networks learn?
    The architecture provides the structure, but the learning process brings that
    structure to life by discovering the weight values that enable accurate predictions.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们建立了对网络架构的理解——神经元如何连接到层，层如何堆叠成网络，以及设计选择如何影响计算需求之后——我们现在可以解决核心问题：这些网络是如何学习的？架构提供了结构，但学习过程通过发现使准确预测成为可能的权重值，使这种结构变得生动。
- en: '**Systems Perspective: Architecture Shapes Deployment Feasibility**'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统视角：架构决定部署可行性**'
- en: '**From Design to Deployment**: Every architectural decision—number of layers,
    layer widths, connection patterns—directly determines memory requirements and
    computational cost. A network with 1 million parameters requires roughly 4MB of
    memory just to store weights, before considering activations during inference.
    As models grow deeper and wider, their memory footprint and computational demands
    grow quadratically, not linearly. This mathematical relationship between architecture
    and resource requirements explains why the same architectural patterns cannot
    deploy uniformly across all platforms. Systems engineering insight emerges: architectural
    design must consider target deployment constraints from the outset, as post-hoc
    compression only partially recovers from architecture-resource mismatches.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**从设计到部署**：每一个架构决策——层数、层宽、连接模式——直接决定了内存需求和计算成本。一个拥有100万个参数的网络，仅为了存储权重就需要大约4MB的内存，在考虑推理过程中的激活之前。随着模型变得更深更宽，它们的内存占用和计算需求呈平方增长，而不是线性增长。这种架构与资源需求之间的数学关系解释了为什么相同的架构模式不能在所有平台上统一部署。系统工程的洞察：架构设计必须从一开始就考虑目标部署约束，因为事后压缩只能部分恢复架构与资源不匹配的问题。'
- en: Layer Connectivity Design Patterns
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层连接设计模式
- en: Neural networks can be structured with different connection patterns between
    layers, each offering distinct advantages for learning and computation. Understanding
    these patterns provides insight into how networks process information and learn
    representations from data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以通过层之间的不同连接模式进行结构化，每种模式都为学习和计算提供了独特的优势。理解这些模式可以提供对网络如何处理信息和从数据中学习表示的见解。
- en: Dense connectivity represents the standard pattern where each neuron connects
    to every neuron in the subsequent layer. In our MNIST example, connecting our
    784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight
    parameters. This full connectivity enables the network to learn arbitrary relationships
    between inputs and outputs, but the number of parameters scales quadratically
    with layer width.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接表示每个神经元都与后续层中的每个神经元相连的标准模式。在我们的MNIST示例中，将我们的784维输入层连接到100个神经元的隐藏层需要78,400个权重参数。这种完全连接使网络能够学习输入和输出之间的任意关系，但参数数量与层宽度的平方成正比。
- en: Sparse connectivity patterns introduce purposeful restrictions in how neurons
    connect between layers. Rather than maintaining all possible connections, neurons
    connect to only a subset of neurons in the adjacent layer. This approach draws
    inspiration from biological neural systems, where neurons typically form connections
    with a limited number of other neurons. In visual processing tasks like our MNIST
    example, neurons might connect only to inputs representing nearby pixels, reflecting
    the local nature of visual features.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏连接模式在神经元层间连接的方式上引入了有意的限制。而不是保持所有可能的连接，神经元只连接到相邻层中的一部分神经元。这种方法从生物神经网络系统中汲取灵感，其中神经元通常只与有限数量的其他神经元形成连接。在我们的MNIST示例等视觉处理任务中，神经元可能只连接到表示附近像素的输入，反映了视觉特征的局部性质。
- en: As networks grow deeper, the path from input to output becomes longer, potentially
    complicating the learning process. Skip connections address this by adding direct
    paths between non-adjacent layers. These connections provide alternative routes
    for information flow, supplementing the standard layer-by-layer progression. In
    our digit recognition example, skip connections might allow later layers to reference
    both high-level patterns and the original pixel values directly.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络的加深，从输入到输出的路径变长，可能会使学习过程复杂化。跳跃连接通过在非相邻层之间添加直接路径来解决这个问题。这些连接提供了信息流的替代路径，补充了标准的层间逐层进展。在我们的数字识别示例中，跳跃连接可能允许后续层直接引用高级模式和原始像素值。
- en: These connection patterns have significant implications for both the theoretical
    capabilities and practical implementation of neural networks. Dense connections
    maximize learning flexibility at the cost of computational efficiency. Sparse
    connections can reduce computational requirements while potentially improving
    the network’s ability to learn structured patterns. Skip connections help maintain
    effective information flow in deeper networks.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些连接模式对神经网络的理沦能力和实际应用都有重大影响。密集连接以牺牲计算效率为代价最大化学习灵活性。稀疏连接可以减少计算需求，同时可能提高网络学习结构化模式的能力。跳跃连接有助于在更深的网络中保持有效的信息流。
- en: Model Size and Computational Complexity
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型大小和计算复杂度
- en: The arrangement of parameters (weights and biases) in a neural network determines
    both its learning capacity and computational requirements. While topology defines
    the network’s structure, the initialization and organization of parameters plays
    a crucial role in learning and performance.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中参数（权重和偏置）的排列决定了其学习能力和计算需求。虽然拓扑结构定义了网络的结构，但参数的初始化和组织在学习和性能中起着至关重要的作用。
- en: Parameter count grows with network width and depth. For our MNIST example, consider
    a network with a 784-dimensional input layer, two hidden layers of 100 neurons
    each, and a 10-neuron output layer. The first layer requires 78,400 weights and
    100 biases, the second layer 10,000 weights and 100 biases, and the output layer
    1,000 weights and 10 biases, totaling 89,610 parameters. Each must be stored in
    memory and updated during learning.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量随着网络宽度和深度的增加而增长。对于我们的MNIST示例，考虑一个具有784维输入层、两个各有100个神经元的隐藏层和一个10个神经元的输出层的网络。第一层需要78,400个权重和100个偏置，第二层需要10,000个权重和100个偏置，输出层需要1,000个权重和10个偏置，总共89,610个参数。每个参数都必须在内存中存储并在学习过程中更新。
- en: Parameter initialization is critical to network behavior. Setting all parameters
    to zero would cause neurons in a layer to behave identically, preventing diverse
    feature learning. Instead, weights are typically initialized randomly, while biases
    often start at small constant values or even zeros. The scale of these initial
    values matters significantly, as values that are too large or too small can lead
    to poor learning dynamics.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 参数初始化对网络行为至关重要。将所有参数设置为零会导致同一层的神经元行为相同，从而阻止多样化的特征学习。相反，权重通常随机初始化，而偏差通常从小的常数值或甚至零开始。这些初始值的规模至关重要，因为过大或过小的值可能导致学习动态不佳。
- en: The distribution of parameters affects information flow through layers. In digit
    recognition, if weights are too small, important input details might not propagate
    to later layers. If too large, the network might amplify noise. Biases help adjust
    the activation threshold of each neuron, enabling the network to learn optimal
    decision boundaries.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的分布影响信息通过层的流动。在数字识别中，如果权重太小，重要的输入细节可能无法传播到后续层。如果太大，网络可能会放大噪声。偏差有助于调整每个神经元的激活阈值，使网络能够学习最佳决策边界。
- en: Different architectures may impose specific constraints on parameter organization.
    Some share weights across network regions to encode position-invariant pattern
    recognition. Others might restrict certain weights to zero, implementing sparse
    connectivity patterns.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的架构可能对参数组织施加特定的约束。一些在网络区域之间共享权重，以编码位置不变的模式识别。其他可能将某些权重限制为零，实现稀疏连接模式。
- en: 'With our understanding of network architecture, neurons, and parameters established,
    we can now address the fundamental question: how do these randomly initialized
    parameters become useful? The answer lies in the learning process that transforms
    a network from its initial random state into a system capable of making accurate
    predictions.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解了网络架构、神经元和参数之后，我们现在可以解决一个基本问题：这些随机初始化的参数如何变得有用？答案在于将网络从初始随机状态转换成能够做出准确预测的系统这一学习过程。
- en: Learning Process
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习过程
- en: Neural networks learn to perform tasks through a process of training on examples.
    This process transforms the network from its initial state, where weights are
    randomly initialized as we just discussed, to a trained state where these same
    weights encode meaningful patterns from the training data. Understanding this
    process is essential to both the theoretical foundations and practical implementations
    of deep learning models.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过在示例上进行训练的过程来学习执行任务。这个过程将网络从初始状态（正如我们刚才讨论的，权重随机初始化）转换到训练状态，在这些状态下，相同的权重编码了来自训练数据的具有意义的模式。理解这一过程对于深度学习模型的理论基础和实践实现都是至关重要的。
- en: Supervised Learning from Labeled Examples
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从标记示例中进行监督学习
- en: 'Building on our architectural foundation, the core principle of neural network
    training is supervised learning from labeled examples. Consider our MNIST digit
    recognition task: we have a dataset of 60,000 training images, each a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel grayscale
    image paired with its correct digit label. The network must learn the relationship
    between these images and their corresponding digits through an iterative process
    of prediction and weight adjustment. Ensuring the quality and integrity of training
    data is essential to model success, as covered in [Chapter 6](ch012.xhtml#sec-data-engineering).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在我们的建筑基础之上，神经网络训练的核心原则是从标记示例中进行监督学习。以我们的MNIST数字识别任务为例：我们有一个包含60,000个训练图像的数据集，每个图像都是一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素的灰度图像，并配对其正确的数字标签。网络必须通过预测和权重调整的迭代过程来学习这些图像与其对应数字之间的关系。确保训练数据的质量和完整性对于模型的成功至关重要，如第6章所述。
- en: 'This relationship between inputs and outputs drives the training methodology.
    Training operates as a loop, where each iteration involves processing a subset
    of training examples called a batch[25](#fn25). For each batch, the network performs
    several key operations:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出之间的关系驱动了训练方法。训练作为一个循环进行，其中每个迭代涉及处理一个称为批次的训练示例子集[25](#fn25)。对于每个批次，网络执行几个关键操作：
- en: Forward computation through the network layers to generate predictions
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过网络层进行正向计算以生成预测
- en: Evaluation of prediction accuracy using a loss function
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用损失函数评估预测精度
- en: Computation of weight adjustments based on prediction errors
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于预测误差计算权重调整
- en: Update of network weights to improve future predictions
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新网络权重以改善未来的预测
- en: 'Formalizing this iterative approach, this process can be expressed mathematically.
    Given an input image <semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>
    and its true label <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>,
    the network computes its prediction: <semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = f(x; \theta)</annotation></semantics> where <semantics><mi>f</mi><annotation
    encoding="application/x-tex">f</annotation></semantics> represents the neural
    network function and <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    represents all trainable parameters (weights and biases, which we discussed earlier).
    The network’s error is measured by a loss function <semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics>: <semantics><mrow><mtext
    mathvariant="normal">loss</mtext><mo>=</mo><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{loss}
    = L(\hat{y}, y)</annotation></semantics>'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种迭代方法形式化，这个过程可以用数学表达式表示。给定一个输入图像<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>及其真实标签<semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>，网络计算其预测：<semantics><mrow><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y}
    = f(x; \theta)</annotation></semantics>其中<semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>表示神经网络函数，<semantics><mi>θ</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>表示所有可训练参数（权重和偏置，我们之前讨论过）。网络的误差通过损失函数<semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics>来衡量：<semantics><mrow><mtext
    mathvariant="normal">loss</mtext><mo>=</mo><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{loss}
    = L(\hat{y}, y)</annotation></semantics>
- en: This quantification of prediction quality becomes the foundation for learning.
    This error measurement drives the adjustment of network parameters through a process
    called “backpropagation,” which we will examine in detail later.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预测质量的量化成为学习的基础。这种误差测量通过称为“反向传播”的过程驱动网络参数的调整，我们将在后面详细探讨。
- en: 'Scaling beyond individual examples, in practice, training operates on batches
    of examples rather than individual inputs. For the MNIST dataset, each training
    iteration might process, for example, 32, 64, or 128 images simultaneously. This
    batch processing serves two purposes: it enables efficient use of modern computing
    hardware through parallel processing, and it provides more stable parameter updates
    by averaging errors across multiple examples.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 超越单个示例的缩放，在实践中，训练是在示例批次上进行的，而不是单个输入。对于MNIST数据集，每个训练迭代可能会同时处理32、64或128个图像。这种批量处理有两个目的：它通过并行处理使现代计算硬件得到有效利用，并通过在多个示例中平均误差来提供更稳定的参数更新。
- en: This batch-based approach creates both computational efficiency and training
    stability. The training cycle continues until the network achieves sufficient
    accuracy or reaches a predetermined number of iterations. Throughout this process,
    the loss function serves as a guide, with its minimization indicating improved
    network performance. Establishing proper metrics and evaluation protocols is crucial
    for assessing training effectiveness, as discussed in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于批次的处理方法既提高了计算效率，又保证了训练的稳定性。训练周期会持续进行，直到网络达到足够的准确度或达到预定的迭代次数。在整个过程中，损失函数充当指南，其最小化表示网络性能的改善。正如在第12章[Chapter 12](ch018.xhtml#sec-benchmarking-ai)中讨论的那样，建立适当的指标和评估协议对于评估训练效果至关重要。
- en: Forward Pass Computation
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传递计算
- en: Forward propagation, as illustrated in [Figure 3.16](ch009.xhtml#fig-forward-propagation),
    is the core computational process in a neural network, where input data flows
    through the network’s layers to generate predictions. Understanding this process
    is important as it underlies both network inference and training. We examine how
    forward propagation works using our MNIST digit recognition example.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播，如图3.16[图](ch009.xhtml#fig-forward-propagation)所示，是神经网络中的核心计算过程，其中输入数据通过网络层流动以生成预测。理解这个过程很重要，因为它既是网络推理的基础，也是训练的基础。我们通过我们的MNIST数字识别示例来考察正向传播是如何工作的。
- en: '![](../media/file48.svg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file48.svg)'
- en: 'Figure 3.16: **Forward Propagation Process**: Neural networks transform input
    data into predictions by sequentially applying weighted sums and activation functions
    across interconnected layers, enabling complex pattern recognition. This layered
    computation forms the basis for both making inferences and updating model parameters
    during training.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16：**前向传播过程**：神经网络通过在相互连接的层中按顺序应用加权求和和激活函数，将输入数据转换为预测，从而实现复杂模式识别。这种分层计算构成了在训练过程中进行推理和更新模型参数的基础。
- en: When an image of a handwritten digit enters our network, it undergoes a series
    of transformations through the layers. Each transformation combines the weighted
    inputs with learned patterns to progressively extract relevant features. In our
    MNIST example, a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel image
    is processed through multiple layers to ultimately produce probabilities for each
    possible digit (0-9).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个手写数字的图像进入我们的网络时，它将通过层进行一系列的转换。每个转换将加权输入与学习到的模式相结合，逐步提取相关特征。在我们的MNIST示例中，一个28×28像素的图像通过多个层进行处理，最终为每个可能的数字（0-9）生成概率。
- en: The process begins with the input layer, where each pixel’s grayscale value
    becomes an input feature. For MNIST, this means 784 input values <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>28</mn><mo>×</mo><mn>28</mn><mo>=</mo><mn>784</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times
    28 = 784)</annotation></semantics>, each normalized between 0 and 1\. These values
    then propagate forward through the hidden layers, where each neuron combines its
    inputs according to its learned weights and applies a nonlinear activation function.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从输入层开始，其中每个像素的灰度值成为输入特征。对于MNIST来说，这意味着784个输入值（28×28=784），每个值在0到1之间归一化。然后这些值通过隐藏层向前传播，其中每个神经元根据其学习的权重组合其输入，并应用非线性激活函数。
- en: From a computational perspective, each forward pass through our MNIST network
    (784→128→64→10) requires substantial matrix operations. The first layer alone
    performs nearly 100,000 multiply-accumulate operations per sample. When processing
    multiple samples in a batch, these operations multiply accordingly, requiring
    careful management of memory bandwidth and computational resources. Specialized
    hardware like GPUs can execute these operations efficiently through parallel processing.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算角度来看，每次通过我们的MNIST网络（784→128→64→10）的前向传递都需要大量的矩阵运算。仅第一层就每个样本执行近10万个乘加运算。当批量处理多个样本时，这些运算相应地相乘，需要仔细管理内存带宽和计算资源。专门的硬件如GPU可以通过并行处理高效地执行这些运算。
- en: Individual Layer Processing
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单个层处理
- en: The forward computation through a neural network proceeds systematically, with
    each layer transforming its inputs into increasingly abstract representations.
    In our MNIST network, this transformation process occurs in distinct stages.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通过神经网络的正向计算是系统性的，每一层都将其输入转换为越来越抽象的表示。在我们的MNIST网络中，这个过程发生在不同的阶段。
- en: 'At each layer, the computation involves two key steps: a linear transformation
    of inputs followed by a nonlinear activation. The linear transformation applies
    the same weighted sum operation we’ve seen before, but now using notation that
    tracks which layer we’re in: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)}
    + \mathbf{b}^{(l)}</annotation></semantics>'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层，计算涉及两个关键步骤：输入的线性变换后跟一个非线性激活。线性变换应用我们之前见过的相同的加权求和操作，但现在使用跟踪我们所在层级的符号：<semantics><mrow><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)}
    + \mathbf{b}^{(l)}</annotation></semantics>
- en: Here, <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>
    represents the weight matrix for layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    <semantics><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{(l-1)}</annotation></semantics>
    contains the activations from the previous layer (the outputs after applying activation
    functions), and <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
    is the bias vector. The superscript <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l)</annotation></semantics>
    keeps track of which layer each parameter belongs to.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>
    代表第 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    层的权重矩阵，<semantics><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{(l-1)}</annotation></semantics>
    包含上一层的激活值（应用激活函数后的输出），而 <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
    是偏置向量。上标 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l)</annotation></semantics>
    用于追踪每个参数属于哪一层。
- en: 'Following this linear transformation, each layer applies a nonlinear activation
    function <semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>:
    <semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)}
    = f(\mathbf{Z}^{(l)})</annotation></semantics>'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在此线性变换之后，每一层应用一个非线性激活函数 <semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>：<semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)}
    = f(\mathbf{Z}^{(l)})</annotation></semantics>
- en: 'This process repeats at each layer, creating a chain of transformations:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程在每个层级上重复，形成一个变换链：
- en: Input → Linear Transform → Activation → Linear Transform → Activation → … →
    Output
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 → 线性变换 → 激活 → 线性变换 → 激活 → … → 输出
- en: In our MNIST example, the pixel values first undergo a transformation by the
    first hidden layer’s weights, converting the 784-dimensional input into an intermediate
    representation. Each subsequent layer further transforms this representation,
    ultimately producing a 10-dimensional output vector representing the network’s
    confidence in each possible digit.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST示例中，像素值首先通过第一隐藏层的权重进行变换，将784维输入转换为中间表示。每个后续层进一步变换这一表示，最终产生一个10维输出向量，表示网络对每个可能的数字的置信度。
- en: Matrix Multiplication Formulation
  id: totrans-355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵乘法公式
- en: The complete forward propagation process can be expressed as a composition of
    functions, each representing a layer’s transformation. Formalizing this mathematically
    builds on the MNIST example.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的前向传播过程可以表示为函数的组合，每个函数代表一层变换。将这一过程数学化是基于MNIST示例的。
- en: 'For a network with <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    layers, we can express the full forward computation as: <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false"
    form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mi>⋯</mi><mo minsize="1.2"
    maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>f</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">)</mo><mi>⋯</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\mathbf{A}^{(L)} = f^{(L)}\Big(\mathbf{W}^{(L)}f^{(L-1)}\Big(\mathbf{W}^{(L-1)}\cdots\big(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X}
    + \mathbf{b}^{(1)})\big)\cdots + \mathbf{b}^{(L-1)}\Big) + \mathbf{b}^{(L)}\Big)</annotation></semantics>'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>层的网络，我们可以将完整的正向计算表示为：<semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false"
    form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mi>⋯</mi><mo minsize="1.2"
    maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>f</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="postfix">)</mo><mi>⋯</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\mathbf{A}^{(L)} = f^{(L)}\Big(\mathbf{W}^{(L)}f^{(L-1)}\Big(\mathbf{W}^{(L-1)}\cdots\big(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X}
    + \mathbf{b}^{(1)})\big)\cdots + \mathbf{b}^{(L-1)}\Big) + \mathbf{b}^{(L)}\Big)</annotation></semantics>
- en: 'While this nested expression captures the complete process, we typically compute
    it step by step:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个嵌套表达式可以捕捉到整个过程，但我们通常是一步一步地计算它：
- en: 'First layer: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X}
    + \mathbf{b}^{(1)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(1)}
    = f^{(1)}(\mathbf{Z}^{(1)})</annotation></semantics>'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '第一层：`<semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X}
    + \mathbf{b}^{(1)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(1)}
    = f^{(1)}(\mathbf{Z}^{(1)})</annotation></semantics>` '
- en: 'Hidden layers <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l
    = 2,\ldots, L-1)</annotation></semantics>: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)}
    + \mathbf{b}^{(l)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)}
    = f^{(l)}(\mathbf{Z}^{(l)})</annotation></semantics>'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '隐藏层 `<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l
    = 2,\ldots, L-1)</annotation></semantics>`: `<semantics><mrow><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)}
    + \mathbf{b}^{(l)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)}
    = f^{(l)}(\mathbf{Z}^{(l)})</annotation></semantics>`'
- en: 'Output layer: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)}
    + \mathbf{b}^{(L)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(L)}
    = f^{(L)}(\mathbf{Z}^{(L)})</annotation></semantics>'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '输出层: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)}
    + \mathbf{b}^{(L)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(L)}
    = f^{(L)}(\mathbf{Z}^{(L)})</annotation></semantics>'
- en: 'In our MNIST example, if we have a batch of <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> images, the dimensions
    of these operations are:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST示例中，如果我们有一批<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>图像，这些操作的维度是：
- en: 'Input <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>:
    <semantics><mrow><mi>B</mi><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">B
    \times 784</annotation></semantics>'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '输入 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>:
    <semantics><mrow><mi>B</mi><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">B
    \times 784</annotation></semantics>'
- en: 'First layer weights <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics>:
    <semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><mn>784</mn></mrow><annotation
    encoding="application/x-tex">n_1\times 784</annotation></semantics>'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '第一层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics>:
    <semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><mn>784</mn></mrow><annotation
    encoding="application/x-tex">n_1\times 784</annotation></semantics>'
- en: 'Hidden layer weights <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>:
    <semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">n_l\times n_{l-1}</annotation></semantics>'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '隐藏层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>:
    <semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">n_l\times n_{l-1}</annotation></semantics>'
- en: 'Output layer weights <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(L)}</annotation></semantics>:
    <semantics><mrow><mn>10</mn><mo>×</mo><msub><mi>n</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">10 \times n_{L-1}</annotation></semantics>'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '输出层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(L)}</annotation></semantics>:
    <semantics><mrow><mn>10</mn><mo>×</mo><msub><mi>n</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">10 \times n_{L-1}</annotation></semantics>'
- en: Step-by-Step Computation Sequence
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤计算序列
- en: Understanding how these mathematical operations translate into actual computation
    requires examining the forward propagation process for a batch of MNIST images.
    This process illustrates how data transforms from raw pixel values to digit predictions.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些数学运算如何转化为实际计算需要检查一批 MNIST 图像的前向传播过程。这个过程说明了数据如何从原始像素值转换为数字预测。
- en: Consider a batch of 32 images entering our network. Each image starts as a <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> grid of pixel
    values, which we flatten into a 784-dimensional vector. For the entire batch,
    this gives us an input matrix <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>
    of size <semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">32\times
    784</annotation></semantics>, where each row represents one image. The values
    are typically normalized to lie between 0 and 1.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含 32 张图像的批次进入我们的网络。每张图像最初是一个 <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> 的像素值网格，我们将其展平成一个
    784 维的向量。对于整个批次，这给我们一个大小为 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>
    的输入矩阵 <semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">32\times
    784</annotation></semantics>，其中每一行代表一张图像。这些值通常被归一化，使其位于 0 和 1 之间。
- en: 'The transformation at each layer proceeds as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的转换过程如下：
- en: '**Input Layer Processing**: The network takes our input matrix <semantics><mi>𝐗</mi><annotation
    encoding="application/x-tex">\mathbf{X}</annotation></semantics> <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>32</mn><mo>×</mo><mn>784</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(32\times
    784)</annotation></semantics> and transforms it using the first layer’s weights.
    If our first hidden layer has 128 neurons, <semantics><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics> is a <semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">784\times 128</annotation></semantics> matrix. The
    resulting computation <semantics><mrow><mi>𝐗</mi><msup><mi>𝐖</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{X}\mathbf{W}^{(1)}</annotation></semantics>
    produces a <semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">32\times 128</annotation></semantics> matrix.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层处理**：网络接收我们的输入矩阵 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>32</mn><mo>×</mo><mn>784</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(32\times
    784)</annotation></semantics> 并使用第一层的权重进行转换。如果我们的第一个隐藏层有 128 个神经元，<semantics><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics> 是一个 <semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">784\times 128</annotation></semantics> 的矩阵。得到的计算
    <semantics><mrow><mi>𝐗</mi><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X}\mathbf{W}^{(1)}</annotation></semantics>
    产生一个 <semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">32\times
    128</annotation></semantics> 的矩阵。'
- en: '**Hidden Layer Transformations**: Each element in this matrix then has its
    corresponding bias added and passes through an activation function. For example,
    with a ReLU activation, any negative values become zero while positive values
    remain unchanged. This nonlinear transformation enables the network to learn complex
    patterns in the data.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层转换**：这个矩阵中的每个元素然后都会加上相应的偏置并通过激活函数。例如，使用ReLU激活函数时，任何负值变为零，而正值保持不变。这种非线性转换使网络能够学习数据中的复杂模式。'
- en: '**Output Generation**: The final layer transforms its inputs into a <semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">32\times 10</annotation></semantics> matrix, where
    each row contains 10 values corresponding to the network’s confidence scores for
    each possible digit. Often, these scores are converted to probabilities using
    a softmax function: <semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext
    mathvariant="normal">digit</mtext></mrow> <mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>k</mi></msub></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10}
    e^{z_k}}</annotation></semantics>'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出生成**：最后一层将其输入转换为 <semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">32\times 10</annotation></semantics> 矩阵，其中每一行包含10个值，对应于网络对每个可能数字的置信度分数。通常，这些分数会通过softmax函数转换为概率：<semantics><mrow><mi>P</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">digit</mtext></mrow>
    <mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>k</mi></msub></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10}
    e^{z_k}}</annotation></semantics>'
- en: For each image in the batch, this produces a probability distribution over the
    possible digits. The digit with the highest probability represents the network’s
    prediction.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批处理中的每一张图像，这会产生一个可能的数字的概率分布。概率最高的数字代表网络的预测。
- en: Implementation and Optimization Considerations
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现和优化考虑
- en: The implementation of forward propagation requires careful attention to several
    practical aspects that affect both computational efficiency and memory usage.
    These considerations become particularly important when processing large batches
    of data or working with deep networks.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播的实现需要仔细关注几个影响计算效率和内存使用的实际方面。当处理大量数据或使用深度网络时，这些考虑变得尤为重要。
- en: 'Memory management plays an important role during forward propagation. Each
    layer’s activations must be stored for potential use in the backward pass during
    training. For our MNIST example with a batch size of 32, if we have three hidden
    layers of sizes 128, 256, and 128, the activation storage requirements are:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理在前向传播期间起着重要作用。每个层的激活必须存储起来，以便在训练期间的逆传播中使用。对于我们的MNIST示例，批处理大小为32，如果我们有三个大小为128、256和128的隐藏层，激活存储需求为：
- en: 'First hidden layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation
    encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> values'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation
    encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> 个值
- en: 'Second hidden layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn><mo>=</mo><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation
    encoding="application/x-tex">32\times 256 = 8,192</annotation></semantics> values'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn><mo>=</mo><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation
    encoding="application/x-tex">32\times 256 = 8,192</annotation></semantics> 个值
- en: 'Third hidden layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation
    encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> values'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation
    encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> 个值
- en: 'Output layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">32\times 10 = 320</annotation></semantics> values'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">32\times 10 = 320</annotation></semantics> 个值
- en: This produces a total of 16,704 values that must be maintained in memory for
    each batch during training. The memory requirements scale linearly with batch
    size and become substantial for larger networks.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生总共16,704个值，在训练过程中必须为每个批次保留在内存中。内存需求与批次大小成线性关系，对于更大的网络来说，内存需求变得相当大。
- en: Batch processing introduces important trade-offs. Larger batches enable more
    efficient matrix operations and better hardware utilization but require more memory.
    For example, doubling the batch size to 64 would double the memory requirements
    for activations. This relationship between batch size, memory usage, and computational
    efficiency guides the choice of batch size in practice.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理引入了重要的权衡。更大的批次可以更有效地进行矩阵运算和更好的硬件利用率，但需要更多的内存。例如，将批次大小加倍到64将使激活的内存需求加倍。批次大小、内存使用和计算效率之间的关系指导了实际中批次大小的选择。
- en: The organization of computations also affects performance. Matrix operations
    can be optimized through careful memory layout and specialized libraries. The
    choice of activation functions affects both the network’s learning capabilities
    and computational efficiency, as some functions (like ReLU) require less computation
    than others (like tanh or sigmoid).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的组织也会影响性能。矩阵运算可以通过仔细的内存布局和专门的库进行优化。激活函数的选择会影响网络的学习能力以及计算效率，因为某些函数（如ReLU）的计算量比其他函数（如tanh或sigmoid）少。
- en: The computational characteristics of neural networks favor parallel processing
    architectures. While traditional CPUs can execute these operations, GPUs designed
    for parallel computation can achieve substantial speedups—often 10-100× faster
    for matrix operations. Specialized AI accelerators achieve even better efficiency
    through techniques like reduced precision arithmetic, specialized memory architectures,
    and dataflow optimizations tailored for neural network computation patterns.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的计算特性有利于并行处理架构。虽然传统的CPU可以执行这些操作，但专为并行计算设计的GPU可以在矩阵运算上实现显著的加速——通常比矩阵运算快10-100倍。专门的AI加速器通过降低精度算术、专门的内存架构以及针对神经网络计算模式的数据流优化等技术，实现了更高的效率。
- en: Energy consumption also varies significantly across hardware platforms. CPUs
    offer flexibility but consume more energy per operation. GPUs provide high throughput
    at higher power consumption. Specialized edge accelerators optimize for energy
    efficiency, achieving the same computations with orders of magnitude less power—a
    critical consideration for mobile and embedded deployments. This energy disparity
    stems from the fundamental memory hierarchy challenges where data movement dominates
    computation costs.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 能耗在硬件平台之间也有显著差异。CPU提供灵活性，但每个操作的能耗更高。GPU提供高吞吐量，但功耗也更高。专门的边缘加速器优化能耗效率，以更少的功率完成相同的计算——这对于移动和嵌入式部署来说是一个关键考虑因素。这种能耗差异源于基本的内存层次结构挑战，其中数据移动主导了计算成本。
- en: These considerations form the foundation for understanding the system requirements
    of neural networks, which we will explore in more detail in [Chapter 4](ch010.xhtml#sec-dnn-architectures).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑构成了理解神经网络系统需求的基础，我们将在[第4章](ch010.xhtml#sec-dnn-architectures)中更详细地探讨。
- en: 'Now that we understand how neural networks process inputs to generate predictions
    through forward propagation, a critical question emerges: how do we determine
    if these predictions are good? The answer lies in loss functions, which provide
    the mathematical framework for measuring prediction quality.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络如何通过正向传播处理输入以生成预测，一个关键问题随之出现：我们如何确定这些预测是否良好？答案在于损失函数，它为衡量预测质量提供了数学框架。
- en: Loss Functions
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: Neural networks learn by measuring and minimizing their prediction errors. Loss
    functions provide the algorithmic structure for quantifying these errors, serving
    as the essential feedback mechanism that guides the learning process. Through
    loss functions, we can convert the abstract goal of “making good predictions”
    into a concrete optimization problem.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过衡量和最小化预测误差来学习。损失函数提供了量化这些误差的算法结构，作为指导学习过程的必要反馈机制。通过损失函数，我们可以将“做出良好预测”的抽象目标转化为具体的优化问题。
- en: To understand the role of loss functions, let’s continue with our MNIST digit
    recognition example. When the network processes a handwritten digit image, it
    outputs ten numbers representing its confidence in each possible digit (0-9).
    The loss function measures how far these predictions deviate from the true answer.
    For instance, if an image displays a “7”, the network should exhibit high confidence
    for digit “7” and low confidence for all other digits. The loss function penalizes
    the network when its prediction deviates from this target.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解损失函数的作用，让我们继续使用我们的MNIST数字识别示例。当网络处理一个手写数字图像时，它输出十个数字，代表它对每个可能数字（0-9）的置信度。损失函数衡量这些预测与真实答案的偏差程度。例如，如果一个图像显示的是“7”，网络应该对数字“7”表现出高置信度，而对其他所有数字表现出低置信度。当网络的预测偏离这个目标时，损失函数会对网络进行惩罚。
- en: 'Consider a concrete example: if the network sees an image of “7” and outputs
    confidences: <semantics><mrow><mo stretchy="true" form="prefix" mathvariant="monospace">[</mo><mn
    mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.2</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.3</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.1</mn><mo stretchy="true" form="postfix" mathvariant="monospace">]</mo></mrow>
    <annotation encoding="application/x-tex">\mathtt{[0.1, 0.1, 0.1, 0.0, 0.0, 0.0,
    0.2, 0.3, 0.1, 0.1]}</annotation></semantics>'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具体的例子：如果网络看到一个“7”的图像，并输出置信度：<semantics><mrow><mo stretchy="true" form="prefix"
    mathvariant="monospace">[</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.2</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.3</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo stretchy="true"
    form="postfix" mathvariant="monospace">]</mo></mrow> <annotation encoding="application/x-tex">\mathtt{[0.1,
    0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]}</annotation></semantics>
- en: 'The highest confidence (0.3) is assigned to digit “7”, but this confidence
    is quite low, indicating uncertainty in the prediction. A good loss function would
    produce a high loss value here, signaling that the network needs significant improvement.
    Conversely, if the network outputs: <semantics><mrow><mo stretchy="true" form="prefix"
    mathvariant="monospace">[</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.9</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo stretchy="true"
    form="postfix" mathvariant="monospace">]</mo></mrow> <annotation encoding="application/x-tex">\mathtt{[0.0,
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]}</annotation></semantics>'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 最高的置信度（0.3）分配给了数字“7”，但这个置信度相当低，表明预测存在不确定性。一个好的损失函数会在这里产生一个高的损失值，表明网络需要显著改进。相反，如果网络输出：<semantics><mrow><mo
    stretchy="true" form="prefix" mathvariant="monospace">[</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.9</mn><mo
    mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn
    mathvariant="monospace">0.1</mn><mo stretchy="true" form="postfix" mathvariant="monospace">]</mo></mrow>
    <annotation encoding="application/x-tex">\mathtt{[0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
    0.0, 0.9, 0.0, 0.1]}</annotation></semantics>
- en: The loss function should produce a lower value, as this prediction is much closer
    to ideal. This illustrates how loss functions guide network improvement by providing
    feedback on prediction quality.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数应该产生一个较低的值，因为这样的预测更接近理想值。这说明了损失函数如何通过提供预测质量的反馈来指导网络改进。
- en: Error Measurement Fundamentals
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误测量基础
- en: 'A loss function measures how far the network’s predictions are from the correct
    answers. This difference is expressed as a single number: a lower loss means the
    predictions are more accurate, while a higher loss indicates the network needs
    improvement. During training, the loss function guides the network by helping
    it adjust its weights to make better predictions. For example, in recognizing
    handwritten digits, the loss will penalize predictions that assign low confidence
    to the correct digit.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数衡量网络的预测与正确答案之间的距离。这种差异用一个单一的数字表示：较低的损失意味着预测更准确，而较高的损失则表明网络需要改进。在训练过程中，损失函数通过帮助网络调整其权重以做出更好的预测来指导网络。例如，在识别手写数字时，损失将惩罚对正确数字信心较低的预测。
- en: 'Mathematically, a loss function <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>
    takes two inputs: the network’s predictions <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation
    encoding="application/x-tex">\hat{y}</annotation></semantics> and the true values
    <semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>.
    For a single training example in our MNIST task: <semantics><mrow><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">measure
    of discrepancy between prediction and truth</mtext></mrow> <annotation encoding="application/x-tex">L(\hat{y},
    y) = \text{measure of discrepancy between prediction and truth}</annotation></semantics>'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，损失函数<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>接受两个输入：网络的预测<semantics><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics>和真实值<semantics><mi>y</mi><annotation
    encoding="application/x-tex">y</annotation></semantics>。在我们的MNIST任务中，对于单个训练示例：<semantics><mrow><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">预测与真实值之间差异的度量</mtext></mrow>
    <annotation encoding="application/x-tex">L(\hat{y}, y) = \text{measure of discrepancy
    between prediction and truth}</annotation></semantics>
- en: 'When training with batches of data, we typically compute the average loss across
    all examples in the batch: <semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}}
    = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)</annotation></semantics> where <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> is the batch size and
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\hat{y}_i,
    y_i)</annotation></semantics> represents the prediction and truth for the <semantics><mi>i</mi><annotation
    encoding="application/x-tex">i</annotation></semantics>-th example.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用数据批次进行训练时，我们通常计算批次中所有示例的平均损失：<semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}}
    = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)</annotation></semantics> 其中 <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> 是批次大小，并且 <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\hat{y}_i,
    y_i)</annotation></semantics> 代表第 <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>
    个示例的预测和真实值。
- en: 'The choice of loss function depends on the type of task. For our MNIST classification
    problem, we need a loss function that can:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的选择取决于任务类型。对于我们的MNIST分类问题，我们需要一个能够：
- en: Handle probability distributions over multiple classes
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理多个类别的概率分布
- en: Provide meaningful gradients for learning
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供有意义的梯度以进行学习
- en: Penalize wrong predictions effectively
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效地惩罚错误预测
- en: Scale well with batch processing
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与批量处理具有良好的扩展性
- en: Cross-Entropy and Classification Loss Functions
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交叉熵和分类损失函数
- en: For classification tasks like MNIST digit recognition, “cross-entropy” ([Shannon
    1948](ch058.xhtml#ref-shannon1948mathematical))[26](#fn26) loss has emerged as
    the standard choice. This loss function is particularly well-suited for comparing
    predicted probability distributions with true class labels.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像MNIST数字识别这样的分类任务，“交叉熵”([香农1948](ch058.xhtml#ref-shannon1948mathematical))[26](#fn26)损失函数已经成为标准选择。这种损失函数特别适合于比较预测的概率分布与真实的类别标签。
- en: 'For a single digit image, our network outputs a probability distribution over
    the ten possible digits. We represent the true label as a one-hot vector where
    all entries are 0 except for a 1 at the correct digit’s position. For instance,
    if the true digit is “7”, the label would be: <semantics><mrow><mi>y</mi><mo>=</mo><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo></mrow> <annotation
    encoding="application/x-tex">y = \big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\big]</annotation></semantics>'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个数字图像，我们的网络输出的是十个可能数字的概率分布。我们将真实标签表示为一个one-hot向量，其中除了正确数字位置上的1之外，所有条目都是0。例如，如果真实数字是“7”，标签将是：<semantics><mrow><mi>y</mi><mo>=</mo><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo></mrow> <annotation
    encoding="application/x-tex">y = \big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\big]</annotation></semantics>
- en: 'The cross-entropy loss for this example is: <semantics><mrow><mi>L</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mi>j</mi></msub><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y},
    y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)</annotation></semantics> where <semantics><msub><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\hat{y}_j</annotation></semantics>
    represents the network’s predicted probability for digit j. Given our one-hot
    encoding, this simplifies to: <semantics><mrow><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>c</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y},
    y) = -\log(\hat{y}_c)</annotation></semantics> where <semantics><mi>c</mi><annotation
    encoding="application/x-tex">c</annotation></semantics> is the index of the correct
    class. This means the loss depends only on the predicted probability for the correct
    digit—the network is penalized based on how confident it is in the right answer.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 此例中的交叉熵损失为：<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mi>j</mi></msub><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y},
    y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)</annotation></semantics> 其中 <semantics><msub><mover><mi>y</mi><mo
    accent="true">̂</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\hat{y}_j</annotation></semantics>
    表示网络对数字 j 的预测概率。鉴于我们的独热编码，这简化为：<semantics><mrow><mi>L</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>c</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y},
    y) = -\log(\hat{y}_c)</annotation></semantics> 其中 <semantics><mi>c</mi><annotation
    encoding="application/x-tex">c</annotation></semantics> 是正确类别的索引。这意味着损失只取决于对正确数字的预测概率——网络根据其对正确答案的信心程度受到惩罚。
- en: 'For example, if our network predicts the following probabilities for an image
    of “7”:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的网络预测以下概率为“7”的图像：
- en: '[PRE0]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The loss would be <semantics><mrow><mi>−</mi><mo>log</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0.8</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">-\log(0.8)</annotation></semantics>, which is approximately
    0.223\. If the network were more confident and predicted 0.9 for the correct digit,
    the loss would decrease to approximately 0.105.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 损失将是 <semantics><mrow><mi>−</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.8</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log(0.8)</annotation></semantics>，这大约是
    0.223。如果网络更有信心，并预测正确的数字为 0.9，则损失将降低到大约 0.105。
- en: Batch Loss Calculation Methods
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量损失计算方法
- en: The practical computation of loss involves considerations for both numerical
    stability and batch processing. When working with batches of data, we compute
    the average loss across all examples in the batch.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的实际计算涉及对数值稳定性和批量处理的考虑。当处理数据批次时，我们计算批次中所有示例的平均损失。
- en: 'For a batch of B examples, the cross-entropy loss becomes: <semantics><mrow><msub><mi>L</mi><mtext
    mathvariant="normal">batch</mtext></msub><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}}
    = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})</annotation></semantics>'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 对于B个样本的批次，交叉熵损失变为：<semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}}
    = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})</annotation></semantics>
- en: Computing this loss efficiently requires careful consideration of numerical
    precision. Taking the logarithm of very small probabilities can lead to numerical
    instability. Consider a case where our network predicts a probability of 0.0001
    for the correct class. Computing <semantics><mrow><mo>log</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0.0001</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\log(0.0001)</annotation></semantics> directly might
    cause underflow or result in imprecise values.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 高效计算这个损失需要仔细考虑数值精度。对非常小的概率取对数可能导致数值不稳定性。考虑一种情况，我们的网络预测正确类别的概率为0.0001。直接计算<semantics><mrow><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0.0001</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\log(0.0001)</annotation></semantics>可能会造成下溢或导致结果不精确。
- en: 'To address this, we typically implement the loss computation with two key modifications:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们通常通过以下两个关键修改来实现损失计算：
- en: 'Add a small epsilon to prevent taking log of zero: <semantics><mrow><mi>L</mi><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>+</mo><mi>ϵ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L
    = -\log(\hat{y} + \epsilon)</annotation></semantics>'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个小的ε值以防止取零的对数：<semantics><mrow><mi>L</mi><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>+</mo><mi>ϵ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L
    = -\log(\hat{y} + \epsilon)</annotation></semantics>
- en: 'Apply the log-sum-exp trick for numerical stability: <semantics><mrow><mtext
    mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo>−</mo><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{\exp\big(z_i
    - \max(z)\big)}{\sum_j \exp\big(z_j - \max(z)\big)}</annotation></semantics>'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高数值稳定性，应用log-sum-exp技巧：<semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mo minsize="1.2"
    maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo>−</mo><mo>max</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{\exp\big(z_i
    - \max(z)\big)}{\sum_j \exp\big(z_j - \max(z)\big)}</annotation></semantics>
- en: 'For our MNIST example with a batch size of 32, this means:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的MNIST示例，批大小为32，这意味着：
- en: Processing 32 sets of 10 probabilities
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理32组10个概率值
- en: Computing 32 individual loss values
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算32个单独的损失值
- en: Averaging these values to produce the final batch loss
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均这些值以产生最终的批损失
- en: Impact on Learning Dynamics
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对学习动态的影响
- en: Understanding how loss functions influence training helps explain key implementation
    decisions in deep learning models.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 理解损失函数如何影响训练有助于解释深度学习模型中的关键实现决策。
- en: 'During each training iteration, the loss value serves multiple purposes:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，损失值具有多重作用：
- en: 'Performance Metric: It quantifies current network accuracy'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 性能指标：它量化了当前网络的准确性
- en: 'Optimization Target: Its gradients guide weight updates'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化目标：其梯度指导权重更新
- en: 'Convergence Signal: Its trend indicates training progress'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛信号：其趋势表示训练进度
- en: 'For our MNIST classifier, monitoring the loss during training reveals the network’s
    learning trajectory. A typical pattern might show:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的MNIST分类器，在训练过程中监控损失可以揭示网络的学习轨迹。典型的模式可能显示：
- en: Initial high loss (<semantics><mrow><mo>∼</mo><mn>2.3</mn></mrow><annotation
    encoding="application/x-tex">\sim 2.3</annotation></semantics>, equivalent to
    random guessing among 10 classes)
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始高损失（<semantics><mrow><mo>∼</mo><mn>2.3</mn></mrow><annotation encoding="application/x-tex">\sim
    2.3</annotation></semantics>，相当于在10个类别中的随机猜测）
- en: Rapid decrease in early training iterations
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在早期训练迭代中迅速下降
- en: Gradual improvement as the network fine-tunes its predictions
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着网络微调其预测，逐渐改进
- en: Eventually stabilizing at a lower loss (<semantics><mrow><mo>∼</mo><mn>0.1</mn></mrow><annotation
    encoding="application/x-tex">\sim 0.1</annotation></semantics>, indicating confident
    correct predictions)
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终稳定在较低的损失（<semantics><mrow><mo>∼</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\sim
    0.1</annotation></semantics>，表示有信心做出正确的预测）
- en: 'The loss function’s gradients with respect to the network’s outputs provide
    the initial error signal that drives backpropagation. For cross-entropy loss,
    these gradients have a particularly simple form: the difference between predicted
    and true probabilities. This mathematical property makes cross-entropy loss especially
    suitable for classification tasks, as it provides strong gradients even when predictions
    are very wrong.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数相对于网络输出的梯度提供了驱动反向传播的初始误差信号。对于交叉熵损失，这些梯度具有特别简单的形式：预测概率与真实概率之间的差异。这种数学特性使得交叉熵损失特别适合分类任务，因为它即使在预测非常错误的情况下也能提供强大的梯度。
- en: 'The choice of loss function also influences other training decisions:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的选择也会影响其他训练决策：
- en: Learning rate selection (larger loss gradients might require smaller learning
    rates)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率选择（较大的损失梯度可能需要较小的学习率）
- en: Batch size (loss averaging across batches affects gradient stability)
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批大小（批间损失平均影响梯度稳定性）
- en: Optimization algorithm behavior
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法行为
- en: Convergence criteria
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛标准
- en: Once we have quantified the network’s prediction errors through loss functions,
    the next critical step is determining how to adjust the network’s weights to reduce
    these errors. This brings us to backward propagation, the mechanism that enables
    neural networks to learn from their mistakes.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们通过损失函数量化了网络的预测误差，下一步关键步骤就是确定如何调整网络的权重以减少这些误差。这引出了反向传播，这是神经网络从错误中学习的一种机制。
- en: Gradient Computation and Backpropagation
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度计算和反向传播
- en: '***Backpropagation*** is an algorithm that efficiently computes *gradients*
    of a neural network’s *loss function* with respect to all parameters by systematically
    applying the *chain rule* backward through network layers.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '***反向传播*** 是一种算法，通过系统地应用链式法则反向通过网络层，高效地计算神经网络损失函数相对于所有参数的梯度。'
- en: Backward propagation, often called backpropagation, is the algorithmic cornerstone
    of neural network training that enables systematic weight adjustment through gradient-based
    optimization. While loss functions tell us how wrong our predictions are, backpropagation
    tells us exactly how to fix them.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播，通常称为反向传播，是神经网络训练的算法基石，它通过基于梯度的优化系统地调整权重。
- en: 'To build intuition for this complex process, consider the “credit assignment”
    problem through a factory assembly line analogy. Imagine a car factory where vehicles
    pass through multiple stations: Station A installs the frame, Station B adds the
    engine, Station C attaches the wheels, and Station D performs final assembly.
    When quality inspectors at the end of the line find a defective car, they face
    a critical question: which station contributed most to the problem, and how should
    each station adjust its process?'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这一复杂过程建立直观理解，可以通过工厂装配线类比来考虑“信用分配”问题。想象一个汽车工厂，其中车辆通过多个站点：站点A安装底盘，站点B添加发动机，站点C安装轮子，站点D进行最终装配。当生产线末端的质检员发现一辆有缺陷的汽车时，他们面临一个关键问题：哪个站点对问题贡献最大，每个站点应该如何调整其工艺？
- en: 'The solution works backward from the defect. The inspector first examines the
    final assembly (Station D) and determines how its work affected the quality issue.
    Station D then looks at what it received from Station C and calculates how much
    of the problem came from the wheels versus its own assembly work. This feedback
    flows backward: Station C examines the engine from Station B, and Station B reviews
    the frame from Station A. Each station receives an “adjustment signal” proportional
    to how much its work contributed to the defect. If Station B’s engine mounting
    was the primary cause, it receives a strong signal to change its process, while
    stations that performed correctly receive smaller or no adjustment signals.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案从缺陷开始逆向工作。检查员首先检查最终装配（站点D）并确定其工作如何影响质量问题。站点D随后查看它从站点C接收到的内容，并计算问题中有多少来自轮子与它自己的装配工作。这种反馈逆向流动：站点C检查站点B的发动机，站点B则审查站点A的底盘。每个站点都会收到一个与其工作对缺陷贡献成比例的“调整信号”。如果站点B的发动机安装是主要原因，它会收到一个强烈的信号来改变其工艺，而执行正确的站点则会收到较小或没有调整信号。
- en: Backpropagation solves this credit assignment problem in neural networks systematically.
    The output layer (like Station D) receives the most direct feedback about what
    went wrong. It calculates how its inputs from the previous layer contributed to
    the error and sends specific adjustment signals backward through the network.
    Each layer receives guidance proportional to its contribution to the prediction
    error and adjusts its weights accordingly. This process ensures that every layer
    learns from the mistake, with the most responsible connections making the largest
    adjustments.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播系统地解决了神经网络中的“信用分配”问题。输出层（如站点D）接收关于出了什么错的直接反馈最多。它计算其从前一层的输入对错误的影响，并通过网络发送特定的调整信号。每一层都会根据其对预测错误的贡献接收相应的指导，并相应地调整其权重。这个过程确保每一层都能从错误中学习，最有责任感的连接会做出最大的调整。
- en: In neural networks, each layer acts like a station on the assembly line, and
    backpropagation determines how much each connection contributed to the final prediction
    error. This systematic approach to learning from mistakes forms the foundation
    of how neural networks improve through experience.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，每一层就像装配线上的一个站点，反向传播确定每个连接对最终预测错误的贡献程度。这种从错误中学习的系统方法构成了神经网络通过经验改进的基础。
- en: This section presents the complete optimization framework, from gradient computation
    through practical training implementation.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了完整的优化框架，从梯度计算到实际训练实现的整个过程。
- en: Backpropagation Algorithm Steps
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向传播算法步骤
- en: While forward propagation computes predictions, backward propagation determines
    how to adjust the network’s weights to improve these predictions. To understand
    this process, consider our MNIST example where the network predicts a “3” for
    an image of “7”. Backward propagation provides a systematic way to adjust weights
    throughout the network to make this mistake less likely in the future by calculating
    how each weight contributed to the error.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 当正向传播计算预测时，反向传播确定如何调整网络的权重以改进这些预测。为了理解这个过程，可以考虑我们的MNIST示例，其中网络预测一个“3”来表示一个“7”的图像。反向传播提供了一种系统的方法来调整网络中的权重，通过计算每个权重对错误的影响，使这种错误在未来发生的可能性降低。
- en: The process begins at the network’s output, where we compare predicted digit
    probabilities with the true label. This error then flows backward through the
    network, with each layer’s weights receiving an update signal based on their contribution
    to the final prediction. The computation follows the chain rule of calculus, breaking
    down the complex relationship between weights and final error into manageable
    steps.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程从网络的输出层开始，我们比较预测的数字概率与真实标签。这个错误随后通过网络反向流动，每一层的权重根据其对最终预测的贡献接收一个更新信号。计算遵循微积分的链式法则，将权重与最终错误之间的复杂关系分解成可管理的步骤。
- en: The mathematical foundations of backpropagation provide the theoretical basis
    for training neural networks, but practical implementation requires sophisticated
    software frameworks. Modern frameworks like PyTorch and TensorFlow implement automatic
    differentiation systems that handle gradient computation automatically, eliminating
    the need for manual derivative implementation. The systems engineering aspects
    of these frameworks, including computation graphs and optimization strategies,
    are covered comprehensively in [Chapter 7](ch013.xhtml#sec-ai-frameworks).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的数学基础为训练神经网络提供了理论基础，但实际实现需要复杂的软件框架。现代框架如PyTorch和TensorFlow实现了自动微分系统，可以自动处理梯度计算，消除了手动导数实现的需求。这些框架的系统工程方面，包括计算图和优化策略，在[第7章](ch013.xhtml#sec-ai-frameworks)中得到了全面介绍。
- en: Error Signal Propagation
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误信号传播
- en: The flow of gradients through a neural network follows a path opposite to the
    forward propagation. Starting from the loss at the output layer, gradients propagate
    backwards, computing how each layer, and ultimately each weight, influenced the
    final prediction error.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度通过神经网络的流动遵循与正向传播相反的路径。从输出层的损失开始，梯度反向传播，计算每一层，最终每个权重如何影响了最终的预测误差。
- en: In our MNIST example, consider what happens when the network misclassifies a
    “7” as a “3”. The loss function generates an initial error signal at the output
    layer, essentially indicating that the probability for “7” should increase while
    the probability for “3” should decrease. This error signal then propagates backward
    through the network layers.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST示例中，考虑当网络错误地将一个“7”分类为“3”时会发生什么。损失函数在输出层生成一个初始错误信号，本质上表明“7”的概率应该增加，而“3”的概率应该减少。然后，这个错误信号通过网络层反向传播。
- en: 'For a network with L layers, the gradient flow can be expressed mathematically.
    At each layer l, we compute how the layer’s output affected the final loss: <semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l)}}
    = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial
    \mathbf{A}^{(l)}}</annotation></semantics>'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有L层的网络，梯度流可以用数学公式表示。在每一层l，我们计算该层的输出如何影响最终损失：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l)}}
    = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial
    \mathbf{A}^{(l)}}</annotation></semantics>
- en: This computation cascades backward through the network, with each layer’s gradients
    depending on the gradients computed in the layer previous to it. The process reveals
    how each layer’s transformation contributed to the final prediction error. For
    instance, if certain weights in an early layer strongly influenced a misclassification,
    they will receive larger gradient values, indicating a need for more substantial
    adjustment.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算过程通过网络向后级联，每一层的梯度依赖于它之前一层计算的梯度。这个过程揭示了每一层的转换是如何贡献到最终预测误差的。例如，如果早期层中的一些权重强烈影响了错误分类，它们将获得更大的梯度值，这表明需要更大幅度的调整。
- en: This process faces challenges in deep networks. As gradients flow backward through
    many layers, they can either vanish or explode. When gradients are repeatedly
    multiplied through many layers, they can become exponentially small, particularly
    with sigmoid or tanh activation functions. This causes early layers to learn very
    slowly or not at all, as they receive negligible updates. Conversely, if gradient
    values are consistently greater than 1, they can grow exponentially, leading to
    unstable training and destructive weight updates.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在深层网络中面临挑战。随着梯度通过许多层向后流动，它们可能消失或爆炸。当梯度在许多层中反复相乘时，它们可以变得指数级小，特别是在sigmoid或tanh激活函数的情况下。这导致早期层学习非常缓慢或根本不学习，因为它们接收到的更新微乎其微。相反，如果梯度值始终大于1，它们可以指数级增长，导致训练不稳定和权重更新破坏性。
- en: Derivative Calculation Process
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导数计算过程
- en: The actual computation of gradients involves calculating several partial derivatives
    at each layer. For each layer, we need to determine how changes in weights, biases,
    and activations affect the final loss. These computations follow directly from
    the chain rule of calculus but must be implemented efficiently for practical neural
    network training.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的实际计算涉及到在每一层计算几个偏导数。对于每一层，我们需要确定权重、偏差和激活的变化如何影响最终的损失。这些计算直接来自微积分的链式法则，但必须高效实现以适应实际的神经网络训练。
- en: 'At each layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    we compute three main gradient components:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>，我们计算三个主要的梯度分量：
- en: 'Weight Gradients: <semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\frac{\partial
    L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T</annotation></semantics>'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重梯度：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow></mfrac><msup><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\frac{\partial
    L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T</annotation></semantics>
- en: 'Bias Gradients: <semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{b}^{(l)}}
    = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏差梯度：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{b}^{(l)}}
    = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>
- en: 'Input Gradients (for propagating to previous layer): <semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l-1)}}
    = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>'
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入梯度（用于传播到前一层）：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l-1)}}
    = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>
- en: 'In our MNIST example, consider the final layer where the network outputs digit
    probabilities. If the network predicted <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mi>…</mi><mo>,</mo><mn>0.05</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.1,
    0.2, 0.5,\ldots, 0.05]</annotation></semantics> for an image of “7”, the gradient
    computation would:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST示例中，考虑网络输出数字概率的最后一层。如果网络预测了“7”图像的<semantics><mrow><mo stretchy="true"
    form="prefix">[</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mi>…</mi><mo>,</mo><mn>0.05</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.1,
    0.2, 0.5,\ldots, 0.05]</annotation></semantics>，梯度计算将：
- en: Start with the error in these probabilities
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些概率中的误差开始
- en: Compute how weight adjustments would affect this error
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算权重调整如何影响这个误差
- en: Propagate these gradients backward to help adjust earlier layer weights
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些梯度反向传播以帮助调整早期层的权重
- en: 'These mathematical formulations precisely describe gradient computation, but
    the systems breakthrough lies in how frameworks automatically implement these
    calculations. Consider a simple operation like matrix multiplication followed
    by ReLU activation: `output = torch.relu(input @ weight)`. The mathematical gradient
    involves computing the derivative of ReLU (0 for negative inputs, 1 for positive)
    and applying the chain rule for matrix multiplication. The framework handles this
    automatically by:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数学公式精确地描述了梯度计算，但系统突破的亮点在于框架如何自动实现这些计算。考虑一个简单的操作，如矩阵乘法后跟ReLU激活：`output = torch.relu(input
    @ weight)`。数学梯度涉及计算ReLU的导数（对于负输入为0，对于正输入为1）并应用矩阵乘法的链式法则。框架通过以下方式自动处理：
- en: Recording the operation in a computation graph during forward pass
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在正向传递期间将操作记录在计算图中
- en: Storing necessary intermediate values (pre-ReLU activations for gradient computation)
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储必要的中间值（用于梯度计算的预ReLU激活）
- en: Automatically generating the backward pass function for each operation
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动为每个操作生成反向传递函数
- en: Optimizing memory usage and computation order across the entire graph
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化整个图中的内存使用和计算顺序
- en: This automation transforms gradient computation from a manual, error-prone process
    requiring deep mathematical expertise into a reliable system capability that enables
    rapid experimentation and deployment. The framework ensures correctness while
    optimizing for computational efficiency, memory usage, and hardware utilization.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自动化将梯度计算从需要深厚数学专业知识且容易出错的手动过程转变为一种可靠的系统功能，它能够实现快速实验和部署。该框架在优化计算效率、内存使用和硬件利用率的同时确保正确性。
- en: Computational Implementation Details
  id: totrans-474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算实现细节
- en: The practical implementation of backward propagation requires careful consideration
    of computational resources and memory management. These implementation details
    significantly impact training efficiency and scalability.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的实际实现需要仔细考虑计算资源和内存管理。这些实现细节对训练效率和可扩展性有重大影响。
- en: 'Memory requirements during backward propagation stem from two main sources.
    First, we need to store the intermediate activations from the forward pass, as
    these are required for computing gradients. For our MNIST network with a batch
    size of 32, each layer’s activations must be maintained:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中的内存需求主要来自两个来源。首先，我们需要存储正向传播过程中的中间激活值，因为这些值是计算梯度所必需的。对于我们的MNIST网络，批大小为32，每一层的激活值必须被维护：
- en: 'Input layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation
    encoding="application/x-tex">32\times 784</annotation></semantics> values (~100KB
    using 32-bit numbers)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">32\times
    784</annotation></semantics> 个值 (~100KB使用32位数字)
- en: 'Hidden layer 1: <semantics><mrow><mn>32</mn><mo>×</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">32\times 512</annotation></semantics> values (~64KB)'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层1：<semantics><mrow><mn>32</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">32\times
    512</annotation></semantics> 个值 (~64KB)
- en: 'Hidden layer 2: <semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn></mrow><annotation
    encoding="application/x-tex">32\times 256</annotation></semantics> values (~32KB)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层2：<semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">32\times
    256</annotation></semantics> 个值 (~32KB)
- en: 'Output layer: <semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">32\times 10</annotation></semantics> values (~1.3KB)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">32\times
    10</annotation></semantics> 个值 (~1.3KB)
- en: Second, we must store gradients for each parameter during backward propagation.
    For our example network with approximately 500,000 parameters, this requires several
    megabytes of memory for gradients. Advanced optimizers like Adam[27](#fn27) require
    additional memory to store momentum terms, roughly doubling the gradient storage
    requirements.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们必须在反向传播过程中为每个参数存储梯度。对于我们的示例网络，该网络大约有500,000个参数，这需要几个兆字节的内存来存储梯度。像Adam[27](#fn27)这样的高级优化器需要额外的内存来存储动量项，这大约将梯度存储需求翻倍。
- en: The memory bandwidth requirements scale with model size and batch size. Each
    training step requires loading all parameters, storing gradients, and accessing
    activations—creating substantial memory traffic. For modest networks like our
    MNIST example, this traffic remains manageable within typical memory system capabilities.
    However, as models grow larger, memory bandwidth can become a significant bottleneck,
    with the largest models requiring specialized high-bandwidth memory systems to
    maintain training efficiency.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽需求与模型大小和批大小成比例。每个训练步骤都需要加载所有参数、存储梯度和访问激活值——这会产生大量的内存流量。对于像我们的MNIST示例这样的适度网络，这种流量在典型的内存系统能力范围内仍然是可管理的。然而，随着模型变得更大，内存带宽可能成为一个重要的瓶颈，最大的模型可能需要专用的高带宽内存系统来维持训练效率。
- en: 'Second, we need storage for the gradients themselves. For each layer, we must
    maintain gradients of similar dimensions to the weights and biases. Taking our
    previous example of a network with hidden layers of size 128, 256, and 128, this
    means storing:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们需要存储梯度本身的空间。对于每一层，我们必须维护与权重和偏置相似维度的梯度。以我们之前的例子，一个具有大小为128、256和128的隐藏层的网络为例，这意味着需要存储：
- en: 'First layer gradients: <semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">784\times 128</annotation></semantics> values'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层梯度：<semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">784\times 128</annotation></semantics> 个值
- en: 'Second layer gradients: <semantics><mrow><mn>128</mn><mo>×</mo><mn>256</mn></mrow><annotation
    encoding="application/x-tex">128\times 256</annotation></semantics> values'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层梯度：<semantics><mrow><mn>128</mn><mo>×</mo><mn>256</mn></mrow><annotation
    encoding="application/x-tex">128\times 256</annotation></semantics> 个值
- en: 'Third layer gradients: <semantics><mrow><mn>256</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">256\times 128</annotation></semantics> values'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层梯度：<semantics><mrow><mn>256</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">256\times 128</annotation></semantics> 个值
- en: 'Output layer gradients: <semantics><mrow><mn>128</mn><mo>×</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">128\times 10</annotation></semantics> values'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层梯度：<semantics><mrow><mn>128</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">128\times
    10</annotation></semantics> 个值
- en: 'The computational pattern of backward propagation follows a specific sequence:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的计算模式遵循特定的顺序：
- en: Compute gradients at current layer
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前层计算梯度
- en: Update stored gradients
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新存储的梯度
- en: Propagate error signal to previous layer
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将错误信号传播到前一层
- en: Repeat until input layer is reached
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到达到输入层
- en: For batch processing, these computations are performed simultaneously across
    all examples in the batch, enabling efficient use of matrix operations and parallel
    processing capabilities.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量处理，这些计算在批处理中的所有示例上同时执行，从而能够有效地使用矩阵运算和并行处理能力。
- en: Modern frameworks handle these computations through sophisticated autograd engines.
    When you call `loss.backward()` in PyTorch, the framework automatically manages
    memory allocation, operation scheduling, and gradient accumulation across the
    computation graph. The system tracks which tensors require gradients, optimizes
    memory usage through gradient checkpointing when needed, and schedules operations
    to maximize hardware utilization. This automated management allows practitioners
    to focus on model design rather than the intricate details of gradient computation
    implementation.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架通过复杂的自动微分引擎处理这些计算。当你调用 PyTorch 中的 `loss.backward()` 时，框架会自动管理内存分配、操作调度以及在计算图上的梯度累积。系统跟踪哪些张量需要梯度，并在需要时通过梯度检查点优化内存使用，并安排操作以最大化硬件利用率。这种自动管理允许实践者专注于模型设计，而不是梯度计算实现的复杂细节。
- en: Weight Update and Optimization
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重更新和优化
- en: Training neural networks requires systematic adjustment of weights and biases
    to minimize prediction errors through an iterative optimization process. Building
    on the computational foundations established in our biological-to-artificial translation,
    this section explores the core mechanisms of neural network optimization, from
    gradient-based parameter updates to practical training implementations.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络需要通过迭代优化过程系统地调整权重和偏差，以最小化预测误差。在本节中，基于我们在生物到人工翻译中建立的计算基础，我们将探讨神经网络优化的核心机制，从基于梯度的参数更新到实际训练实现。
- en: Parameter Update Algorithms
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数更新算法
- en: '***Gradient Descent*** is an iterative optimization algorithm that minimizes
    a *loss function* by repeatedly adjusting parameters in the direction of *steepest
    descent*, calculated from the *gradient* with respect to those parameters.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**是一种迭代优化算法，通过反复调整参数的方向来最小化*损失函数*，该方向由参数的*梯度*计算得出。'
- en: The optimization process adjusts network weights through gradient descent[28](#fn28),
    a systematic method that implements the learning principles derived from our biological
    neural network analysis. This iterative process calculates how each weight contributes
    to the error and updates parameters to reduce loss, gradually refining the network’s
    predictive ability.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程通过梯度下降[28](#fn28)调整网络权重，这是一种系统方法，它实现了从我们的生物神经网络分析中推导出的学习原则。这个迭代过程计算每个权重对误差的贡献，并更新参数以减少损失，逐渐提高网络的预测能力。
- en: 'The fundamental update rule combines backpropagation’s gradient computation
    with parameter adjustment: <semantics><mrow><msub><mi>θ</mi><mtext mathvariant="normal">new</mtext></msub><mo>=</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">old</mtext></msub><mo>−</mo><mi>α</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow>
    <annotation encoding="application/x-tex">\theta_{\text{new}} = \theta_{\text{old}}
    - \alpha \nabla_{\theta}L</annotation></semantics> where <semantics><mi>θ</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics> represents any network
    parameter (weights or biases), <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>
    is the learning rate, and <semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow><annotation
    encoding="application/x-tex">\nabla_{\theta}L</annotation></semantics> is the
    gradient computed through backpropagation.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 基本更新规则结合了反向传播的梯度计算与参数调整：<semantics><mrow><msub><mi>θ</mi><mtext mathvariant="normal">new</mtext></msub><mo>=</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">old</mtext></msub><mo>−</mo><mi>α</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow>
    <annotation encoding="application/x-tex">\theta_{\text{new}} = \theta_{\text{old}}
    - \alpha \nabla_{\theta}L</annotation></semantics> 其中 <semantics><mi>θ</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics> 代表任何网络参数（权重或偏差），<semantics><mi>α</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics> 是学习率，而 <semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow><annotation
    encoding="application/x-tex">\nabla_{\theta}L</annotation></semantics> 是通过反向传播计算出的梯度。
- en: For our MNIST example, this means adjusting weights to improve digit classification
    accuracy. If the network frequently confuses “7”s with “1”s, gradient descent
    will modify weights to better distinguish between these digits. The learning rate
    <semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>[29](#fn29)
    controls adjustment magnitude—too large values cause overshooting optimal parameters,
    while too small values result in slow convergence.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的MNIST示例，这意味着调整权重以提高数字分类的准确性。如果网络经常将“7”与“1”混淆，梯度下降将修改权重以更好地区分这些数字。学习率<semantics><mi>α</mi><annotation
    encoding="application/x-tex">\alpha</annotation></semantics>[29](#fn29)控制调整幅度——过大值会导致超过最佳参数，而太小值会导致收敛缓慢。
- en: Despite neural network loss landscapes being highly non-convex with multiple
    local minima, gradient descent reliably finds effective solutions in practice.
    The theoretical reasons—involving concepts like the lottery ticket hypothesis
    ([Frankle and Carbin 2018](ch058.xhtml#ref-frankle2018lottery)), implicit bias
    ([Neyshabur et al. 2017](ch058.xhtml#ref-neyshabur2017exploring)), and overparameterization
    benefits ([Nakkiran et al. 2019](ch058.xhtml#ref-nakkiran2019deep))—remain active
    research areas. For practical ML systems engineering, the key insight is that
    gradient descent with appropriate learning rates, initialization, and regularization
    consistently trains neural networks to high performance.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络的损失地形高度非凸，具有多个局部最小值，但在实践中梯度下降可靠地找到有效的解决方案。涉及彩票假设（Frankle和Carbin 2018）、隐含偏差（Neyshabur等人2017）和过度参数化优势（Nakkiran等人2019）等概念的理论原因仍然是活跃的研究领域。对于实际的机器学习系统工程，关键洞察是具有适当的学习率、初始化和正则化的梯度下降可以持续训练神经网络以达到高性能。
- en: Mini-Batch Gradient Updates
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小批量梯度更新
- en: Neural networks typically process multiple examples simultaneously during training,
    an approach known as mini-batch gradient descent. Rather than updating weights
    after each individual image, we compute the average gradient over a batch of examples
    before performing the update.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在训练过程中通常同时处理多个示例，这种方法被称为小批量梯度下降。我们不是在更新每个单独的图像后更新权重，而是在更新之前计算一批示例的平均梯度。
- en: 'For a batch of size <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>,
    the loss gradient becomes: <semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mtext
    mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B
    \nabla_{\theta}L_i</annotation></semantics>'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大小为<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>的一批，损失梯度变为：<semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mtext
    mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mi>i</mi></msub></mrow>
    <annotation encoding="application/x-tex">\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B
    \nabla_{\theta}L_i</annotation></semantics>
- en: 'In our MNIST training, with a typical batch size of 32, this means:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST训练中，典型的批大小为32，这意味着：
- en: Process 32 images through forward propagation
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过正向传播处理32个图像
- en: Compute loss for all 32 predictions
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有32个预测的损失
- en: Average the gradients across all 32 examples
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有32个示例的梯度平均
- en: Update weights using this averaged gradient
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个平均梯度更新权重
- en: '**Systems Perspective: Batch Size and Hardware Utilization**'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统视角：批大小和硬件利用率**'
- en: '**The Batch Size Trade-off**: Larger batches improve hardware efficiency because
    matrix operations can process multiple examples with similar computational cost
    to processing one. However, each example in the batch requires memory to store
    its activations, creating a fundamental trade-off: larger batches use hardware
    more efficiently but demand more memory. Available memory thus becomes a hard
    constraint on batch size, which in turn affects how efficiently the hardware can
    be utilized. This relationship between algorithm design (batch size) and hardware
    capability (memory) exemplifies why ML systems engineering requires thinking about
    both simultaneously.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量大小的权衡**：较大的批次可以提高硬件效率，因为矩阵运算可以以处理一个实例的计算成本处理多个实例。然而，批次中的每个实例都需要内存来存储其激活，这创造了一个基本的权衡：较大的批次更有效地使用硬件，但需要更多的内存。因此，可用内存成为批量大小的硬约束，反过来又影响硬件的利用效率。这种算法设计（批大小）与硬件能力（内存）之间的关系说明了为什么机器学习系统工程需要同时考虑这两个方面。'
- en: Iterative Learning Process
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代学习过程
- en: The complete training process combines forward propagation, backward propagation,
    and weight updates into a systematic training loop. This loop repeats until the
    network achieves satisfactory performance or reaches a predetermined number of
    iterations.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练过程将前向传播、反向传播和权重更新组合成一个系统的训练循环。这个循环会重复，直到网络达到令人满意的性能或达到预定的迭代次数。
- en: 'A single pass through the entire training dataset is called an epoch[30](#fn30).
    For MNIST, with 60,000 training images and a batch size of 32, each epoch consists
    of 1,875 batch iterations. The training loop structure is:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 整个训练数据集的一次遍历称为一个epoch[30](#fn30)。对于MNIST，有60,000个训练图像和32个批量大小的数据，每个epoch包含1,875个批次数迭代。训练循环结构如下：
- en: 'For each epoch:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个epoch：
- en: Shuffle training data to prevent learning order-dependent patterns
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱训练数据以防止学习顺序依赖的模式
- en: 'For each batch:'
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: Perform forward propagation
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行前向传播
- en: Compute loss
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Execute backward propagation
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行反向传播
- en: Update weights using gradient descent
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新权重
- en: Evaluate network performance
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估网络性能
- en: 'During training, we monitor several key metrics:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们监控几个关键指标：
- en: 'Training loss: average loss over recent batches'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失：最近批次的平均损失
- en: 'Validation accuracy: performance on held-out test data'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证准确率：在保留的测试数据上的性能
- en: 'Learning progress: how quickly the network improves'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习进度：网络改进的速度
- en: For our digit recognition task, we might observe the network’s accuracy improve
    from 10% (random guessing) to over 95% through multiple epochs of training.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数字识别任务，我们可能会观察到网络在多个epoch的训练后，准确率从10%（随机猜测）提高到超过95%。
- en: Convergence and Stability Considerations
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收敛和稳定性考虑
- en: The successful implementation of neural network training requires attention
    to several key practical aspects that significantly impact learning effectiveness.
    These considerations bridge the gap between theoretical understanding and practical
    implementation.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 成功实施神经网络训练需要关注几个关键的实际方面，这些方面对学习效果有显著影响。这些考虑因素架起了理论与实践之间的桥梁。
- en: '***Overfitting*** occurs when a machine learning model learns patterns specific
    to the *training data* that fail to generalize to *unseen data*, resulting in
    high training accuracy but poor test performance.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习模型学习到特定于*训练数据*的特定模式，而这些模式无法推广到*未见数据*时，就会发生**过拟合**，导致训练准确率高但测试性能差。
- en: Learning rate selection is perhaps the most critical parameter affecting training.
    For our MNIST network, the choice of learning rate dramatically influences the
    training dynamics. A large learning rate of 0.1 might cause unstable training
    where the loss oscillates or explodes as weight updates overshoot optimal values.
    Conversely, a very small learning rate of 0.0001 might result in extremely slow
    convergence, requiring many more epochs to achieve good performance. A moderate
    learning rate of 0.01 often provides a good balance between training speed and
    stability, allowing the network to make steady progress while maintaining stable
    learning.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率选择可能是影响训练的最关键参数。对于我们的MNIST网络，学习率的选择极大地影响了训练动态。一个大的学习率0.1可能会导致不稳定的训练，其中损失振荡或爆炸，因为权重更新超出了最优值。相反，一个非常小的学习率0.0001可能会导致收敛极其缓慢，需要更多的epoch才能达到良好的性能。一个适中的学习率0.01通常在训练速度和稳定性之间提供了良好的平衡，使网络能够稳步进步，同时保持稳定的学习。
- en: Convergence monitoring provides crucial feedback during the training process.
    As training progresses, we typically observe the loss value stabilizing around
    a particular value, indicating the network is approaching a local optimum. The
    validation accuracy often plateaus as well, suggesting the network has extracted
    most of the learnable patterns from the data. The gap between training and validation
    performance offers insights into whether the network is overfitting or generalizing
    well to new examples. The operational aspects of monitoring models in production
    environments, including detecting model degradation and performance drift, are
    comprehensively covered in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，收敛监控提供了关键反馈。随着训练的进行，我们通常观察到损失值稳定在某个特定值附近，表明网络正在接近局部最优。验证准确率也往往会达到平台期，这表明网络已经从数据中提取了大部分可学习的模式。训练和验证性能之间的差距可以揭示网络是否过拟合或对新示例泛化良好。在生产环境中监控模型的操作方面，包括检测模型退化性能漂移，这些方面在[第13章](ch019.xhtml#sec-ml-operations)中得到了全面覆盖。
- en: Resource requirements become increasingly important as we scale neural network
    training. The memory footprint must accommodate both model parameters and the
    intermediate computations needed for backpropagation. Computation scales linearly
    with batch size, affecting training speed and hardware utilization. Modern training
    often leverages GPU acceleration, making efficient use of parallel computing capabilities
    crucial for practical implementation.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们扩展神经网络训练，资源需求变得越来越重要。内存占用必须容纳模型参数和反向传播所需的中间计算。计算与批大小成线性关系，影响训练速度和硬件利用率。现代训练通常利用GPU加速，因此，对于实际实施来说，高效利用并行计算能力至关重要。
- en: Training neural networks also presents several challenges. Overfitting occurs
    when the network becomes too specialized to the training data, performing well
    on seen examples but poorly on new ones. Gradient instability can manifest as
    either vanishing or exploding gradients, making learning difficult. The interplay
    between batch size, available memory, and computational resources often requires
    careful balancing to achieve efficient training while working within hardware
    constraints.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络也带来了一些挑战。当网络对训练数据过于专业化时，就会发生过拟合，它在已见示例上表现良好，但在新示例上表现较差。梯度不稳定性可能表现为梯度消失或梯度爆炸，使得学习变得困难。批大小、可用内存和计算资源之间的相互作用通常需要仔细平衡，以在硬件约束内实现高效的训练。
- en: '**Checkpoint: Neural Network Learning Process**'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查点：神经网络学习过程**'
- en: 'You’ve now covered the complete training cycle—the mathematical machinery that
    enables neural networks to learn from data. Before moving to inference and deployment,
    verify your understanding:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经完成了完整的训练周期——数学工具，它使得神经网络能够从数据中学习。在转向推理和部署之前，验证你的理解：
- en: '**Forward Propagation:**'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '**正向传播：**'
- en: '**Loss Functions:**'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数：**'
- en: '**Backward Propagation:**'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播：**'
- en: '**Optimization:**'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化：**'
- en: '**The Complete Training Loop:**'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整的训练循环：**'
- en: '**Self-Test**: For our MNIST network (784→128→64→10), trace what happens during
    one training iteration with batch size 32: What matrices multiply? What gets stored?
    What memory is required? What gradients are computed?'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**自测**：对于我们的MNIST网络（784→128→64→10），追踪在批大小为32的一次训练迭代中发生了什么：哪些矩阵相乘？什么被存储？需要多少内存？计算了哪些梯度？'
- en: '*If any concepts feel unclear, review [Section 3.5.2](ch009.xhtml#sec-dl-primer-forward-pass-computation-a837)
    (Forward Propagation), [Section 3.5.3](ch009.xhtml#sec-dl-primer-loss-functions-d892)
    (Loss Functions), [Section 3.5.4](ch009.xhtml#sec-dl-primer-gradient-computation-backpropagation-e26a)
    (Backward Propagation), or [Section 3.5.5](ch009.xhtml#sec-dl-primer-weight-update-optimization-20af)
    (Optimization Process). These mechanisms form the foundation for understanding
    the training-vs-inference distinction we explore next.*'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果任何概念感觉不清楚，请回顾[第3.5.2节](ch009.xhtml#sec-dl-primer-forward-pass-computation-a837)（正向传播）、[第3.5.3节](ch009.xhtml#sec-dl-primer-loss-functions-d892)（损失函数）、[第3.5.4节](ch009.xhtml#sec-dl-primer-gradient-computation-backpropagation-e26a)（反向传播）或[第3.5.5节](ch009.xhtml#sec-dl-primer-weight-update-optimization-20af)（优化过程）。这些机制构成了理解训练与推理区别的下一个探索的基础。*'
- en: Inference Pipeline
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理管道
- en: 'Having explored the training process in detail, we now turn to the operational
    phase of neural networks. Neural networks serve two distinct purposes: learning
    from data during training and making predictions during inference. While we’ve
    explored how networks learn through forward propagation, backward propagation,
    and weight updates, the prediction phase operates differently. During inference,
    networks use their learned parameters to transform inputs into outputs without
    the need for learning mechanisms. This simpler computational process still requires
    careful consideration of how data flows through the network and how system resources
    are utilized. Understanding the prediction phase is crucial as it represents how
    neural networks are actually deployed to solve real-world problems, from classifying
    images to generating text predictions.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细探讨了训练过程之后，我们现在转向神经网络的运营阶段。神经网络有两个不同的目的：在训练期间从数据中学习，在推理期间进行预测。虽然我们已经探讨了网络通过前向传播、反向传播和权重更新来学习的方式，但预测阶段的工作方式不同。在推理期间，网络使用其学习到的参数将输入转换为输出，而不需要学习机制。这个更简单的计算过程仍然需要仔细考虑数据如何通过网络流动以及系统资源如何被利用。理解预测阶段至关重要，因为它代表了神经网络如何实际部署来解决现实世界问题，从分类图像到生成文本预测。
- en: Production Deployment and Prediction Pipeline
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产部署和预测流程
- en: The operational deployment of neural networks centers on inference, which is
    the process of using trained models to make predictions on new data. Unlike training,
    which requires iterative parameter updates and extensive computational resources,
    inference represents the production workload that delivers value in deployed systems.
    Understanding the fundamental differences between these two phases proves essential
    for designing efficient ML systems, as each phase imposes distinct requirements
    on hardware, memory, and software architecture. This section examines the core
    characteristics of inference, beginning with a systematic comparison to training
    before exploring the computational pipeline that transforms inputs into predictions.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的运营部署集中在推理上，这是使用训练好的模型对新数据进行预测的过程。与需要迭代参数更新和大量计算资源的训练不同，推理代表了在生产系统中提供价值的运营工作量。理解这两个阶段之间的基本差异对于设计高效的机器学习系统至关重要，因为每个阶段都对硬件、内存和软件架构提出了不同的要求。本节从系统性地比较训练开始，探讨将输入转换为预测的计算流程，来考察推理的核心特征。
- en: 'This phase transition introduces an important constraint regarding model adaptability
    that significantly impacts system design. While trained models demonstrate generalization
    capabilities across unseen inputs through learned statistical patterns, the learned
    parameters remain fixed throughout deployment. Once training concludes, the model
    applies its learned probability distributions without modification. When the operational
    data distribution diverges from training distributions, the model continues executing
    its fixed computational pathways regardless of this shift. Consider an autonomous
    vehicle perception system: if construction zone frequency increases substantially
    or novel vehicle configurations appear in deployment, the model’s responses reflect
    the statistical patterns learned during training rather than adapting to the evolved
    operational context. The capacity for adaptation in ML systems emerges not from
    runtime model modification but from systematic retraining with updated data, a
    deliberate engineering process detailed in [Chapter 8](ch014.xhtml#sec-ai-training).'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段转换引入了一个关于模型适应性的重要约束，这对系统设计产生了重大影响。虽然训练好的模型通过学习到的统计模式在未见过的输入上表现出泛化能力，但学习到的参数在整个部署过程中保持固定。一旦训练结束，模型就应用其学习到的概率分布而不做修改。当运营数据分布与训练分布不同时，模型继续执行其固定的计算路径，而不考虑这种变化。考虑一个自动驾驶车辆感知系统：如果施工区域频率显著增加或部署中出现新的车辆配置，模型的响应将反映训练期间学习的统计模式，而不是适应演变的运营环境。机器学习系统适应能力并非来自运行时模型的修改，而是来自使用更新数据进行的系统重训练，这是一个在[第8章](ch014.xhtml#sec-ai-training)中详细描述的故意工程过程。
- en: Operational Phase Differences
  id: totrans-550
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运营阶段差异
- en: Neural network operation divides into two fundamentally distinct phases that
    impose markedly different computational requirements and system constraints. Training
    requires both forward and backward passes through the network to compute gradients
    and update weights, while inference involves only forward pass computation. This
    architectural simplification means that each layer performs only one set of operations
    during inference, transforming inputs to outputs using learned weights without
    tracking intermediate values for gradient computation, as illustrated in [Figure 3.17](ch009.xhtml#fig-training-vs-inference).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络操作分为两个基本不同的阶段，这两个阶段对计算要求和系统约束产生了显著的不同。训练需要通过网络进行正向和反向传递来计算梯度并更新权重，而推理仅涉及正向传递计算。这种架构简化意味着在推理期间每个层只执行一组操作，使用学习到的权重将输入转换为输出，而不跟踪中间值以进行梯度计算，如图[图3.17](ch009.xhtml#fig-training-vs-inference)所示。
- en: 'These computational differences manifest directly in hardware requirements
    and deployment strategies. Training clusters typically employ high-memory GPUs[31](#fn31)
    with substantial cooling infrastructure. Inference deployments prioritize latency
    and energy efficiency across diverse platforms: mobile devices utilize low-power
    neural processors (typically 2-4W), edge servers deploy specialized inference
    accelerators[32](#fn32), and cloud services employ inference-optimized instances
    with reduced numerical precision for increased throughput[33](#fn33). Production
    inference systems serving millions of requests daily require sophisticated infrastructure
    including load balancing, auto-scaling, and failover mechanisms typically unnecessary
    in training environments.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算差异直接体现在硬件需求和部署策略中。训练集群通常使用高内存的GPU[31](#fn31)和大量的冷却基础设施。推理部署优先考虑不同平台上的延迟和能效：移动设备使用低功耗的神经网络处理器（通常为2-4W），边缘服务器部署专门的推理加速器[32](#fn32)，而云服务使用推理优化的实例，降低数值精度以提高吞吐量[33](#fn33)。每天处理数百万请求的生产推理系统需要复杂的基础设施，包括负载均衡、自动扩展和故障转移机制，这些在训练环境中通常是无需的。
- en: '![](../media/file49.svg)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file49.svg)'
- en: 'Figure 3.17: **Inference vs. Training Flow**: During inference, neural networks
    utilize learned weights for forward pass computation only, simplifying the data
    flow and reducing computational cost compared to training, which requires both
    forward and backward passes for weight updates. This streamlined process enables
    efficient deployment of trained models for real-time predictions.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17：**推理与训练流程**：在推理过程中，神经网络仅利用学习到的权重进行正向传递计算，简化了数据流并降低了与训练相比的计算成本，因为训练需要正向和反向传递来更新权重。这种简化的流程使得训练好的模型能够高效地部署以进行实时预测。
- en: Parameter freezing represents another major distinction between training and
    inference phases. During training, weights and biases continuously update to minimize
    the loss function. In inference, these parameters remain fixed, acting as static
    transformations learned from the training data. This freezing of parameters not
    only simplifies computation but also enables optimizations impossible during training,
    such as weight quantization or pruning.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 参数冻结代表了训练和推理阶段之间另一个主要的区别。在训练过程中，权重和偏差持续更新以最小化损失函数。在推理过程中，这些参数保持固定，作为从训练数据中学习到的静态转换。这种参数冻结不仅简化了计算，还使得在训练期间无法实现的优化成为可能，例如权重量化或剪枝。
- en: The structural difference between training loops and inference passes significantly
    impacts system design. Training operates in an iterative loop, processing multiple
    batches of data repeatedly across many epochs to refine the network’s parameters.
    Inference, in contrast, typically processes each input just once, generating predictions
    in a single forward pass. This shift from iterative refinement to single-pass
    prediction influences how we architect systems for deployment.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环与推理传递之间的结构差异对系统设计有显著影响。训练在迭代循环中运行，通过多个批次的数据重复处理多个epoch来细化网络的参数。相比之下，推理通常只处理每个输入一次，通过单次正向传递生成预测。这种从迭代细化到单次预测的转变影响了我们为部署而构建系统的方式。
- en: These structural differences create substantially different memory and computation
    requirements between training and inference. Training demands considerable memory
    to store intermediate activations for backpropagation, gradients for weight updates,
    and optimization states. Inference eliminates these memory-intensive requirements,
    needing only enough memory to store the model parameters and compute a single
    forward pass. This reduction in memory footprint, coupled with simpler computation
    patterns, enables inference to run efficiently on a broader range of devices,
    from powerful servers to resource-constrained edge devices.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构上的差异在训练和推理之间创造了显著不同的内存和计算需求。训练需要大量的内存来存储反向传播的中间激活、权重更新的梯度和优化状态。推理消除了这些内存密集型需求，只需要足够的内存来存储模型参数并计算单次前向传播。这种内存足迹的减少，加上更简单的计算模式，使得推理可以在从强大的服务器到资源受限的边缘设备更广泛的设备上高效运行。
- en: In general, the training phase requires more computational resources and memory
    for learning, while inference is streamlined for efficient prediction. [Table 3.5](ch009.xhtml#tbl-train-vs-inference)
    summarizes the key differences between training and inference.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，训练阶段需要更多的计算资源和内存来进行学习，而推理过程则被优化以实现高效的预测。[表3.5](ch009.xhtml#tbl-train-vs-inference)总结了训练和推理之间的关键差异。
- en: 'Table 3.5: **Training vs. Inference**: Neural networks transition from a computationally
    intensive training phase—requiring both forward and backward passes with updated
    parameters—to an efficient inference phase using fixed parameters and solely forward
    passes. This distinction enables deployment on resource-constrained devices by
    minimizing memory requirements and computational load during prediction.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.5：**训练与推理对比**：神经网络从计算密集的训练阶段——需要带有更新参数的前向和反向传播——过渡到使用固定参数和仅前向传播的效率推理阶段。这种区别使得在资源受限的设备上部署成为可能，通过最小化预测期间的内存需求和计算负载。
- en: '| **Aspect** | **Training** | **Inference** |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **训练** | **推理** |'
- en: '| --- | --- | --- |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Computation Flow** | Forward and backward passes, gradient computation
    | Forward pass only, direct input to output |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| **计算流程** | 前向和反向传播，梯度计算 | 仅前向传播，直接从输入到输出 |'
- en: '| **Parameters** | Continuously updated weights and biases | Fixed/frozen weights
    and biases |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | 持续更新的权重和偏差 | 固定/冻结的权重和偏差 |'
- en: '| **Processing Pattern** | Iterative loops over multiple epochs | Single pass
    through the network |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| **处理模式** | 多个epoch的迭代循环 | 网络的单次遍历 |'
- en: '| **Memory Requirements** | High – stores activations, gradients, optimizer
    state | Lower– stores only model parameters and current input |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| **内存需求** | 高 – 存储激活、梯度、优化器状态 | 低 – 仅存储模型参数和当前输入 |'
- en: '| **Computational Needs** | Heavy – gradient updates, backpropagation | Lighter
    – matrix multiplication only |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| **计算需求** | 重 – 梯度更新，反向传播 | 较轻 – 仅矩阵乘法 |'
- en: '| **Hardware Requirements** | GPUs/specialized hardware for efficient training
    | Can run on simpler devices, including mobile/edge |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| **硬件需求** | 用于高效训练的GPU/专用硬件 | 可在更简单的设备上运行，包括移动/边缘设备 |'
- en: This stark contrast between training and inference phases highlights why system
    architectures often differ significantly between development and deployment environments.
    While training requires substantial computational resources and specialized hardware,
    inference can be optimized for efficiency and deployed across a broader range
    of devices.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和推理阶段之间的这种鲜明对比突出了为什么系统架构在开发和部署环境之间往往存在显著差异。虽然训练需要大量的计算资源和专用硬件，但推理可以针对效率进行优化，并部署在更广泛的设备上。
- en: Training and inference enable different architectural optimizations. Training
    requires high-precision arithmetic and backward pass computation, driving specialized
    hardware adoption with flexible compute units. Inference allows for various efficiency
    optimizations and specialized architectures that take advantage of the simpler
    computational flow. These differences explain why specialized inference processors
    can achieve much higher energy efficiency compared to general-purpose training
    hardware.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和推理允许不同的架构优化。训练需要高精度的算术和反向传播计算，推动了具有灵活计算单元的专用硬件的采用。推理允许进行各种效率优化和利用更简单计算流的专用架构。这些差异解释了为什么专用推理处理器与通用训练硬件相比，可以实现更高的能效。
- en: 'Memory usage patterns also differ dramatically: training stores all activations
    for backpropagation (requiring 2-3x more memory), while inference can discard
    activations immediately after use.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用模式也差异很大：训练存储所有激活以进行反向传播（需要2-3倍更多的内存），而推理可以在使用后立即丢弃激活。
- en: End-to-End Prediction Workflow
  id: totrans-571
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 端到端预测工作流程
- en: The implementation of neural networks in practical applications requires a complete
    processing pipeline that extends beyond the network itself. This pipeline, which
    is illustrated in [Figure 3.18](ch009.xhtml#fig-inference-pipeline) transforms
    raw inputs into meaningful outputs through a series of distinct stages, each essential
    for the system’s operation. Understanding this complete pipeline provides critical
    insights into the design and deployment of deep learning systems.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中实现神经网络需要完整的处理管道，这个管道不仅限于网络本身。这个管道，如图3.18所示，通过一系列不同的阶段将原始输入转换为有意义的输出，每个阶段对于系统的运行都是必不可少的。理解这个完整的管道对于深度学习系统的设计和部署提供了关键见解。
- en: '![](../media/file50.svg)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file50.svg)'
- en: 'Figure 3.18: **Inference Pipeline**: Machine learning systems transform raw
    inputs into final outputs through a series of sequential stages—preprocessing,
    neural network computation, and post-processing—each critical for accurate prediction
    and deployment. This pipeline emphasizes the distinction between model architecture
    and the complete system required for real-world application.'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18：**推理管道**：机器学习系统通过一系列连续的阶段将原始输入转换为最终输出——预处理、神经网络计算和后处理——每个阶段对于准确预测和部署都至关重要。这个管道强调了模型架构与实际应用所需的完整系统之间的区别。
- en: The key thing to notice from the figure is that deep learning systems operate
    as hybrid architectures that combine conventional computing operations with neural
    network computations. The neural network component, focused on learned transformations
    through matrix operations, represents just one element within a broader computational
    framework. This framework encompasses both the preparation of input data and the
    interpretation of network outputs, processes that rely primarily on traditional
    computing methods.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，深度学习系统作为混合架构运行，结合了传统的计算操作和神经网络计算。神经网络组件，专注于通过矩阵操作学习到的转换，只是更广泛的计算框架中的一个元素。这个框架包括输入数据的准备和网络输出的解释，这些过程主要依赖于传统的计算方法。
- en: 'Consider how data flows through the pipeline in [Figure 3.18](ch009.xhtml#fig-inference-pipeline):'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑数据如何在[图3.18](ch009.xhtml#fig-inference-pipeline)中通过管道流动：
- en: Raw inputs arrive in their original form, which might be images, text, sensor
    readings, or other data types
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始输入以原始形式到达，可能是图像、文本、传感器读数或其他数据类型
- en: Pre-processing transforms these inputs into a format suitable for neural network
    consumption
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理将这些输入转换为神经网络可消费的格式
- en: The neural network performs its learned transformations
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络执行其学习到的转换
- en: Raw outputs emerge from the network, often in numerical form
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络的原始输出通常以数值形式出现
- en: Post-processing converts these outputs into meaningful, actionable results
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后处理将这些输出转换为有意义的、可操作的结果
- en: This pipeline structure reveals several key characteristics of deep learning
    systems. The neural network, despite its computational sophistication, functions
    as a component within a larger system. Performance bottlenecks may arise at any
    stage of the pipeline, not exclusively within the neural network computation.
    System optimization must therefore consider the entire pipeline rather than focusing
    solely on the neural network’s operation.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道结构揭示了深度学习系统的一些关键特征。尽管神经网络在计算上非常复杂，但它仍然是一个更大系统中的组件。性能瓶颈可能出现在管道的任何阶段，而不仅仅是神经网络计算中。因此，系统优化必须考虑整个管道，而不仅仅是关注神经网络的操作。
- en: The hybrid nature of this architecture has significant implications for system
    implementation. While neural network computations may benefit from specialized
    hardware accelerators, pre- and post-processing operations typically execute on
    conventional processors. This distribution of computation across heterogeneous
    hardware resources represents a fundamental consideration in system design.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的混合性质对系统实现有重大影响。虽然神经网络计算可能从专用硬件加速器中受益，但预处理和后处理操作通常在传统处理器上执行。这种计算在异构硬件资源上的分布是系统设计中的一个基本考虑因素。
- en: Data Preprocessing and Normalization
  id: totrans-584
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理和归一化
- en: 'The pre-processing stage transforms raw inputs into a format suitable for neural
    network computation. While often overlooked in theoretical discussions, this stage
    forms a critical bridge between real-world data and neural network operations.
    Consider our MNIST digit recognition example: before a handwritten digit image
    can be processed by the neural network we designed earlier, it must undergo several
    transformations. Raw images of handwritten digits arrive in various formats, sizes,
    and pixel value ranges. For instance, in [Figure 3.19](ch009.xhtml#fig-handwritten),
    we see that the digits are all of different sizes, and even the number 6 is written
    differently by the same person.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理阶段将原始输入转换为适合神经网络计算格式的数据。虽然在理论讨论中经常被忽视，但这一阶段在现实世界数据和神经网络操作之间形成了一个关键的桥梁。以我们的MNIST数字识别示例来说：在我们之前设计的神经网络处理手写数字图像之前，它必须经过几个转换。手写数字的原始图像以各种格式、大小和像素值范围到达。例如，在[图3.19](ch009.xhtml#fig-handwritten)中，我们可以看到数字的大小各不相同，甚至同一个人写的数字6也有不同的写法。
- en: '![](../media/file51.png)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file51.png)'
- en: 'Figure 3.19: **Handwritten Digit Variability**: Real-world data exhibits substantial
    variation in style, size, and orientation, necessitating robust pre-processing
    techniques for reliable machine learning performance. These images exemplify the
    challenges of digit recognition, where even seemingly simple inputs require normalization
    and feature extraction before they can be effectively processed by a neural network.
    Source: o. augereau.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19：**手写数字的可变性**：现实世界的数据在风格、大小和方向上表现出显著的差异，这需要强大的预处理技术来保证可靠的机器学习性能。这些图像展示了数字识别的挑战，即使看似简单的输入也需要进行归一化和特征提取，才能被神经网络有效处理。来源：o.
    augereau。
- en: 'The pre-processing stage standardizes these inputs through conventional computing
    operations:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理阶段通过传统的计算操作对这些输入进行标准化：
- en: Image scaling to the required <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel dimensions,
    camera images are usually large(r).
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像缩放到所需的<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素尺寸，相机图像通常较大(r)。
- en: Pixel value normalization from <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>255</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,255]</annotation></semantics>
    to <semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics>,
    most cameras generate colored images.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素值归一化从<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>255</mn><mo
    stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,255]</annotation></semantics>到<semantics><mrow><mo
    stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics>，大多数相机生成彩色图像。
- en: Flattening the 2D image array into a 784-dimensional vector, preparing it for
    the neural network.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将2D图像数组展平为784维向量，为神经网络做准备。
- en: Basic validation to ensure data integrity, making sure the network predicted
    correctly.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本验证以确保数据完整性，确保网络预测正确。
- en: 'What distinguishes pre-processing from neural network computation is its reliance
    on traditional computing operations rather than learned transformations. While
    the neural network learns to recognize digits through training, pre-processing
    operations remain fixed, deterministic transformations. This distinction has important
    system implications: pre-processing operates on conventional CPUs rather than
    specialized neural network hardware, and its performance characteristics follow
    traditional computing patterns.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理与神经网络计算的区别在于它依赖于传统的计算操作而不是学习到的转换。虽然神经网络通过训练学习识别数字，但预处理操作仍然是固定的、确定性的转换。这种区别对系统有重要的影响：预处理在传统的CPU上而不是在专门的神经网络硬件上运行，其性能特征遵循传统的计算模式。
- en: The effectiveness of pre-processing directly impacts system performance. Poor
    normalization can lead to reduced accuracy, inconsistent scaling can introduce
    artifacts, and inefficient implementation can create bottlenecks. Understanding
    these implications helps in designing robust deep learning systems that perform
    well in real-world conditions.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的有效性直接影响系统性能。不良的归一化可能导致准确性降低，不一致的缩放可能引入伪影，低效的实现可能造成瓶颈。理解这些影响有助于设计在现实条件下表现良好的鲁棒深度学习系统。
- en: Forward Pass Computation Pipeline
  id: totrans-595
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前向传递计算管道
- en: The inference phase represents the operational state of a neural network, where
    learned parameters are used to transform inputs into predictions. Unlike the training
    phase we discussed earlier, inference focuses solely on forward computation with
    fixed parameters.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 推理阶段代表神经网络的操作状态，其中学习的参数被用来将输入转换为预测。与之前讨论的训练阶段不同，推理仅关注具有固定参数的前向计算。
- en: Model Loading and Setup
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型加载和设置
- en: 'Before processing any inputs, the neural network must be properly initialized
    for inference. This initialization phase involves loading the model parameters
    learned during training into memory. For our MNIST digit recognition network,
    this means loading specific weight matrices and bias vectors for each layer. The
    exact memory requirements for our architecture are:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理任何输入之前，神经网络必须正确初始化以进行推理。这个初始化阶段包括将训练期间学习的模型参数加载到内存中。对于我们的MNIST数字识别网络，这意味着为每一层加载特定的权重矩阵和偏置向量。我们架构的确切内存需求如下：
- en: 'Input to first hidden layer:'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一隐藏层输入：
- en: 'Weight matrix: <semantics><mrow><mn>784</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>78</mn><mo>,</mo><mn>400</mn></mrow><annotation
    encoding="application/x-tex">784\times 100 = 78,400</annotation></semantics> parameters'
  id: totrans-600
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵：<semantics><mrow><mn>784</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>78</mn><mo>,</mo><mn>400</mn></mrow><annotation
    encoding="application/x-tex">784\times 100 = 78,400</annotation></semantics> 个参数
- en: 'Bias vector: 100 parameters'
  id: totrans-601
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量：100个参数
- en: 'First to second hidden layer:'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一到第二隐藏层：
- en: 'Weight matrix: <semantics><mrow><mn>100</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation
    encoding="application/x-tex">100\times 100 = 10,000</annotation></semantics> parameters'
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵：<semantics><mrow><mn>100</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation
    encoding="application/x-tex">100\times 100 = 10,000</annotation></semantics> 个参数
- en: 'Bias vector: 100 parameters'
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量：100个参数
- en: 'Second hidden layer to output:'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二隐藏层到输出：
- en: 'Weight matrix: <semantics><mrow><mn>100</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>1</mn><mo>,</mo><mn>000</mn></mrow><annotation
    encoding="application/x-tex">100\times 10 = 1,000</annotation></semantics> parameters'
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵：<semantics><mrow><mn>100</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>1</mn><mo>,</mo><mn>000</mn></mrow><annotation
    encoding="application/x-tex">100\times 10 = 1,000</annotation></semantics> 个参数
- en: 'Bias vector: 10 parameters'
  id: totrans-607
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量：10个参数
- en: 'This architecture’s complete parameter requirements are detailed in the Resource
    Requirements section below. For processing a single image, this means allocating
    space for:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的完整参数需求在下面的资源需求部分中详细说明。对于处理单个图像，这意味着为以下内容分配空间：
- en: 'First hidden layer activations: 100 values'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一隐藏层激活值：100个值
- en: 'Second hidden layer activations: 100 values'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二隐藏层激活值：100个值
- en: 'Output layer activations: 10 values'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层激活值：10个值
- en: This memory allocation pattern differs significantly from training, where additional
    memory was needed for gradients, optimizer states, and backpropagation computations.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 这种内存分配模式与训练阶段有显著差异，在训练阶段需要额外的内存用于梯度、优化器状态和反向传播计算。
- en: Real-world inference deployments employ various memory optimization techniques
    to reduce resource requirements while maintaining acceptable accuracy. Systems
    may combine multiple requests together to better utilize hardware capabilities
    while meeting response time requirements. For resource-constrained deployments,
    various model compression approaches help models fit within available memory while
    preserving functionality.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际推理部署中，采用各种内存优化技术以减少资源需求同时保持可接受的准确性。系统可能将多个请求组合在一起以更好地利用硬件能力并满足响应时间要求。对于资源受限的部署，各种模型压缩方法有助于模型适应可用内存同时保持功能。
- en: Inference Forward Pass Execution
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理前向传递执行
- en: During inference, data propagates through the network’s layers using the initialized
    parameters. This forward propagation process, while similar in structure to its
    training counterpart, operates with different computational constraints and optimizations.
    The computation follows a deterministic path from input to output, transforming
    the data at each layer using learned parameters.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，数据通过网络的层使用初始化的参数传播。这个前向传播过程，虽然结构与训练时的对应过程相似，但具有不同的计算约束和优化。计算遵循从输入到输出的确定性路径，在每个层使用学习到的参数转换数据。
- en: 'For our MNIST digit recognition network, consider the precise computations
    at each layer. The network processes a pre-processed image represented as a 784-dimensional
    vector through successive transformations:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的MNIST数字识别网络，考虑每一层的精确计算。网络通过连续的转换处理一个表示为784维向量的预处理图像：
- en: 'First Hidden Layer Computation:'
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一隐藏层计算：
- en: 'Input transformation: 784 inputs combine with 78,400 weights through matrix
    multiplication'
  id: totrans-618
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入转换：784个输入与78,400个权重通过矩阵乘法结合
- en: 'Linear computation: <semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>𝐱</mi><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(1)} = \mathbf{x}\mathbf{W}^{(1)} + \mathbf{b}^{(1)}</annotation></semantics>'
  id: totrans-619
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性计算：<semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>𝐱</mi><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(1)} = \mathbf{x}\mathbf{W}^{(1)} + \mathbf{b}^{(1)}</annotation></semantics>
- en: 'Activation: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(1)}
    = \text{ReLU}(\mathbf{z}^{(1)})</annotation></semantics>'
  id: totrans-620
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活：<semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(1)}
    = \text{ReLU}(\mathbf{z}^{(1)})</annotation></semantics>
- en: 'Output: 100-dimensional activation vector'
  id: totrans-621
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：100维激活向量
- en: 'Second Hidden Layer Computation:'
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二隐藏层计算：
- en: 'Input transformation: 100 values combine with 10,000 weights'
  id: totrans-623
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入转换：100个值与10,000个权重结合
- en: 'Linear computation: <semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(2)} = \mathbf{a}^{(1)}\mathbf{W}^{(2)}
    + \mathbf{b}^{(2)}</annotation></semantics>'
  id: totrans-624
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性计算：<semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(2)} = \mathbf{a}^{(1)}\mathbf{W}^{(2)}
    + \mathbf{b}^{(2)}</annotation></semantics>
- en: 'Activation: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(2)}
    = \text{ReLU}(\mathbf{z}^{(2)})</annotation></semantics>'
  id: totrans-625
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '激活函数: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(2)}
    = \text{ReLU}(\mathbf{z}^{(2)})</annotation></semantics>'
- en: 'Output: 100-dimensional activation vector'
  id: totrans-626
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '输出: 100维激活向量'
- en: 'Output Layer Computation:'
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '输出层计算:'
- en: 'Final transformation: 100 values combine with 1,000 weights'
  id: totrans-628
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终转换：100个值与1,000个权重结合
- en: 'Linear computation: <semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(3)} = \mathbf{a}^{(2)}\mathbf{W}^{(3)}
    + \mathbf{b}^{(3)}</annotation></semantics>'
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '线性计算: <semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{z}^{(3)} = \mathbf{a}^{(2)}\mathbf{W}^{(3)}
    + \mathbf{b}^{(3)}</annotation></semantics>'
- en: 'Activation: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(3)}
    = \text{softmax}(\mathbf{z}^{(3)})</annotation></semantics>'
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '激活函数: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(3)}
    = \text{softmax}(\mathbf{z}^{(3)})</annotation></semantics>'
- en: 'Output: 10 probability values'
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '输出: 10个概率值'
- en: '[Table 3.6](ch009.xhtml#tbl-forward-pass) shows how these computations, while
    mathematically identical to training-time forward propagation, show important
    operational differences:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3.6](ch009.xhtml#tbl-forward-pass)展示了这些计算，虽然从数学上与训练时的正向传播相同，但显示了重要的操作差异：'
- en: 'Table 3.6: **Forward Pass Optimization**: During inference, neural networks
    prioritize computational efficiency by retaining only current layer activations
    and releasing intermediate states, unlike training where complete activation history
    is maintained for backpropagation. This optimization streamlines output generation
    by focusing resources on immediate computations rather than gradient preparation.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '表3.6: **正向传播优化**：在推理过程中，神经网络通过仅保留当前层的激活并释放中间状态来优先考虑计算效率，这与训练时维护完整的激活历史以供反向传播不同。这种优化通过将资源集中在即时计算而不是梯度准备上，简化了输出生成过程。'
- en: '| **Characteristic** | **Training Forward Pass** | **Inference Forward Pass**
    |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **训练正向传播** | **推理正向传播** |'
- en: '| --- | --- | --- |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Activation Storage** | Maintains complete activation history for backpropagation
    | Retains only current layer activations |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| **激活存储** | 维护完整的激活历史以供反向传播 | 仅保留当前层的激活 |'
- en: '| **Memory Pattern** | Preserves intermediate states throughout forward pass
    | Releases memory after layer computation completes |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| **内存模式** | 在正向传播过程中保留中间状态 | 层计算完成后释放内存 |'
- en: '| **Computational Flow** | Structured for gradient computation preparation
    | Optimized for direct output generation |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| **计算流程** | 结构化以准备梯度计算 | 优化以直接生成输出 |'
- en: '| **Resource Profile** | Higher memory requirements for training operations
    | Minimized memory footprint for efficient execution |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| **资源配置文件** | 训练操作需要更高的内存要求 | 最小化内存占用以实现高效执行 |'
- en: This streamlined computation pattern enables efficient inference while maintaining
    the network’s learned capabilities. The reduction in memory requirements and simplified
    computational flow make inference particularly suitable for deployment in resource-constrained
    environments, such as Mobile ML and Tiny ML.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化的计算模式能够在保持网络学习能力的同时实现高效的推理。内存需求减少和计算流程简化使得推理特别适合部署在资源受限的环境中，如移动机器学习和微型机器学习。
- en: Memory and Computational Resources
  id: totrans-641
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存和计算资源
- en: 'Neural networks consume computational resources differently during inference
    compared to training. During inference, resource utilization focuses primarily
    on efficient forward pass computation and minimal memory overhead. Examining the
    specific requirements for the MNIST digit recognition network reveals:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练相比，神经网络在推理过程中消耗的计算资源不同。在推理过程中，资源利用率主要集中于高效的正向传递计算和最小的内存开销。检查 MNIST 数字识别网络的特定要求揭示：
- en: 'Memory requirements during inference can be precisely quantified:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程中的内存需求可以精确量化：
- en: 'Static Memory (Model Parameters):'
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态内存（模型参数）：
- en: 'Layer 1: 78,400 weights + 100 biases'
  id: totrans-645
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层：78,400 个权重 + 100 个偏置
- en: 'Layer 2: 10,000 weights + 100 biases'
  id: totrans-646
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层：10,000 个权重 + 100 个偏置
- en: 'Layer 3: 1,000 weights + 10 biases'
  id: totrans-647
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层：1,000 个权重 + 10 个偏置
- en: 'Total: 89,610 parameters (<semantics><mrow><mo>≈</mo><mn>358.44</mn></mrow><annotation
    encoding="application/x-tex">\approx 358.44</annotation></semantics> KB at 32-bit
    floating point precision[34](#fn34))'
  id: totrans-648
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总计：89,610 个参数 (<semantics><mrow><mo>≈</mo><mn>358.44</mn></mrow><annotation
    encoding="application/x-tex">\approx 358.44</annotation></semantics> KB 在 32 位浮点精度下[34](#fn34))
- en: 'Dynamic Memory (Activations):'
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态内存（激活值）：
- en: 'Layer 1 output: 100 values'
  id: totrans-650
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层输出：100 个值
- en: 'Layer 2 output: 100 values'
  id: totrans-651
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层输出：100 个值
- en: 'Layer 3 output: 10 values'
  id: totrans-652
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层输出：10 个值
- en: 'Total: 210 values (<semantics><mrow><mo>≈</mo><mn>0.84</mn></mrow><annotation
    encoding="application/x-tex">\approx 0.84</annotation></semantics> KB at 32-bit
    floating point precision)'
  id: totrans-653
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总计：210 个值 (<semantics><mrow><mo>≈</mo><mn>0.84</mn></mrow><annotation encoding="application/x-tex">\approx
    0.84</annotation></semantics> KB 在 32 位浮点精度下)
- en: 'Computational requirements follow a fixed pattern for each input:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个输入，计算需求遵循固定的模式：
- en: 'First layer: 78,400 multiply-adds'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层：78,400 次乘加操作
- en: 'Second layer: 10,000 multiply-adds'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层：10,000 次乘加操作
- en: 'Output layer: 1,000 multiply-adds'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层：1,000 次乘加操作
- en: 'Total: 89,400 multiply-add operations per inference'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总计：每次推理 89,400 次乘加操作
- en: This resource profile stands in stark contrast to training requirements, where
    additional memory for gradients and computational overhead for backpropagation
    significantly increase resource demands. The predictable, streamlined nature of
    inference computations enables various optimization opportunities and efficient
    hardware utilization.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 这种资源配置文件与训练需求形成鲜明对比，训练时需要额外的梯度内存和反向传播的计算开销显著增加了资源需求。推理计算的预测性和简化的特性使得各种优化机会和高效的硬件利用成为可能。
- en: Performance Enhancement Techniques
  id: totrans-660
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能提升技术
- en: The fixed nature of inference computation presents several opportunities for
    optimization that are not available during training. Once a neural network’s parameters
    are frozen, the predictable pattern of computation allows for systematic improvements
    in both memory usage and computational efficiency.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 推理计算的固定特性提供了在训练期间不可用的优化机会。一旦神经网络的参数被冻结，可预测的计算模式允许在内存使用和计算效率方面进行系统性的改进。
- en: 'Batch size selection represents a key trade-off in inference optimization.
    During training, large batches were necessary for stable gradient computation,
    but inference offers more flexibility. Processing single inputs minimizes latency,
    making it ideal for real-time applications where immediate responses are crucial.
    However, batch processing can significantly improve throughput by better utilizing
    parallel computing capabilities, particularly on GPUs. For our MNIST network,
    consider the memory implications: processing a single image requires storing 210
    activation values, while a batch of 32 images requires 6,720 activation values
    but can process images up to 32 times faster on parallel hardware.'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小选择是推理优化中的一个关键权衡。在训练过程中，为了稳定梯度计算，需要大批次，但推理提供了更多的灵活性。处理单个输入可以最小化延迟，使其对于需要即时响应的实时应用非常理想。然而，批处理可以通过更好地利用并行计算能力显著提高吞吐量，尤其是在GPU上。对于我们MNIST网络，考虑内存影响：处理单个图像需要存储210个激活值，而32个图像的批次需要6,720个激活值，但在并行硬件上可以快32倍处理图像。
- en: Memory management during inference can be significantly more efficient than
    during training. Since intermediate values are only needed for forward computation,
    memory buffers can be carefully managed and reused. The activation values from
    each layer need only exist until the next layer’s computation is complete. This
    enables in-place operations where possible, reducing the total memory footprint.
    The fixed nature of inference allows for precise memory alignment and access patterns
    optimized for the underlying hardware architecture.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程中的内存管理可以比训练过程中更加高效。由于中间值仅用于正向计算，内存缓冲区可以仔细管理并重复使用。每一层的激活值只需存在到下一层计算完成即可。这使可能的情况下可以进行原地操作，从而减少总的内存占用。推理的固定性质允许进行精确的内存对齐和访问模式，这些模式针对底层硬件架构进行了优化。
- en: Hardware-specific optimizations become particularly important during inference.
    On CPUs, computations can be organized to maximize cache utilization and take
    advantage of parallel processing capabilities where the same operation is applied
    to multiple data elements simultaneously. GPU deployments benefit from optimized
    matrix multiplication routines and efficient memory transfer patterns. These optimizations
    extend beyond pure computational efficiency, as they can significantly impact
    power consumption and hardware utilization, critical factors in real-world deployments.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，针对硬件的优化变得尤为重要。在CPU上，计算可以组织起来以最大化缓存利用率，并利用并行处理能力，即同时将相同的操作应用于多个数据元素。GPU部署得益于优化的矩阵乘法例程和高效的内存传输模式。这些优化不仅超越了纯计算效率，还可以显著影响功耗和硬件利用率，这在实际部署中是关键因素。
- en: The predictable nature of inference also enables optimizations like reduced
    numerical precision. While training typically requires full floating-point precision
    to maintain stable learning, inference can often operate with reduced precision
    while maintaining acceptable accuracy. For our MNIST network, such optimizations
    could significantly reduce the memory footprint with corresponding improvements
    in computational efficiency.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的可预测性也使得像降低数值精度这样的优化成为可能。虽然训练通常需要全浮点精度以保持稳定的学习，但推理通常可以在保持可接受精度的同时使用较低的精度。对于我们MNIST网络，这样的优化可以显著减少内存占用，同时提高计算效率。
- en: These optimization principles, while illustrated through our simple MNIST feedforward
    network, represent only the foundation of neural network optimization. More sophisticated
    architectures introduce additional considerations and opportunities, including
    specialized designs for spatial data processing, sequential computation, and attention-based
    computation patterns. These architectural variations and their optimizations are
    explored in [Chapter 4](ch010.xhtml#sec-dnn-architectures), [Chapter 10](ch016.xhtml#sec-model-optimizations),
    and [Chapter 9](ch015.xhtml#sec-efficient-ai).
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化原则，虽然通过我们简单的MNIST前馈网络进行说明，但仅代表了神经网络优化的基础。更复杂的架构引入了额外的考虑和机会，包括专门为空间数据处理、顺序计算和基于注意力的计算模式设计的架构。这些架构变化及其优化在[第4章](ch010.xhtml#sec-dnn-architectures)、[第10章](ch016.xhtml#sec-model-optimizations)和[第9章](ch015.xhtml#sec-efficient-ai)中进行了探讨。
- en: Output Interpretation and Decision Making
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出解释和决策制定
- en: The transformation of neural network outputs into actionable predictions requires
    a return to traditional computing paradigms. Just as pre-processing bridges real-world
    data to neural computation, post-processing bridges neural outputs back to conventional
    computing systems. This completes the hybrid computing pipeline we examined earlier,
    where neural and traditional computing operations work in concert to solve real-world
    problems.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络输出转换为可操作的预测需要回归到传统的计算范式。正如预处理将现实世界数据连接到神经计算一样，后处理将神经输出重新连接到传统的计算系统。这完成了我们之前检查的混合计算管道，其中神经计算和传统计算操作协同工作以解决现实世界问题。
- en: 'The complexity of post-processing extends beyond simple mathematical transformations.
    Real-world systems must handle uncertainty, validate outputs, and integrate with
    larger computing systems. In our MNIST example, a digit recognition system might
    require not just the most likely digit, but also confidence measures to determine
    when human intervention is needed. This introduces additional computational steps:
    confidence thresholds, secondary prediction checks, and error handling logic,
    all of which are implemented in traditional computing frameworks.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理的复杂性不仅限于简单的数学变换。现实世界的系统必须处理不确定性、验证输出，并与更大的计算系统集成。在我们的MNIST示例中，一个数字识别系统可能不仅需要最可能的数字，还需要置信度度量来确定何时需要人工干预。这引入了额外的计算步骤：置信度阈值、二级预测检查和错误处理逻辑，所有这些都在传统的计算框架中实现。
- en: The computational requirements of post-processing differ significantly from
    neural network inference. While inference benefits from parallel processing and
    specialized hardware, post-processing typically runs on conventional CPUs and
    follows sequential logic patterns. This return to traditional computing brings
    both advantages and constraints. Operations are more flexible and easier to modify
    than neural computations, but they may become bottlenecks if not carefully implemented.
    For instance, computing softmax probabilities for a batch of predictions requires
    different optimization strategies than the matrix multiplications of neural network
    layers.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理的计算需求与神经网络推理大相径庭。虽然推理受益于并行处理和专用硬件，但后处理通常在传统的CPU上运行，遵循顺序逻辑模式。这种回归到传统的计算既带来了优势也带来了限制。操作比神经计算更灵活且更容易修改，但如果不仔细实现，它们可能会成为瓶颈。例如，为一批预测计算softmax概率需要与神经网络层的矩阵乘法不同的优化策略。
- en: System integration considerations often dominate post-processing design. Output
    formats must match downstream system requirements, error handling must align with
    broader system protocols, and performance must meet system-level constraints.
    In a complete mail sorting system, the post-processing stage must not only identify
    digits but also format these predictions for the sorting machinery, handle uncertainty
    cases appropriately, and maintain processing speeds that match physical mail flow
    rates.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 系统集成考虑通常主导后处理设计。输出格式必须与下游系统要求匹配，错误处理必须与更广泛系统协议保持一致，性能必须满足系统级约束。在一个完整的邮件分拣系统中，后处理阶段不仅必须识别数字，还必须将这些预测格式化为分拣机械，适当地处理不确定性情况，并保持与物理邮件流动速度相匹配的处理速度。
- en: This return to traditional computing paradigms completes the hybrid nature of
    deep learning systems. Just as pre-processing prepared real-world data for neural
    computation, post-processing adapts neural outputs for real-world use. Understanding
    this hybrid nature, the interplay between neural and traditional computing, is
    essential for designing and implementing effective deep learning systems.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 这种回归到传统的计算范式完成了深度学习系统的混合特性。正如预处理为神经计算准备现实世界数据一样，后处理将神经输出调整为现实世界的应用。理解这种混合特性，即神经计算与传统计算之间的相互作用，对于设计和实现有效的深度学习系统至关重要。
- en: 'We’ve now covered the complete lifecycle of neural networks: from architectural
    design through training dynamics to inference deployment. Each concept—neurons,
    layers, forward propagation, backpropagation, loss functions, optimization—represents
    a piece of the puzzle. But how do these pieces fit together in practice? The following
    checkpoint helps you verify your understanding of how these components integrate
    into complete systems, after which we’ll examine a historical case study that
    brings all these principles to life in a real-world deployment.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了神经网络的完整生命周期：从架构设计到训练动态再到推理部署。每个概念——神经元、层、前向传播、反向传播、损失函数、优化——都代表了一个拼图的一部分。但在实践中，这些部分是如何组合在一起的？以下检查点帮助您验证对这些组件如何整合到完整系统中的理解，之后我们将考察一个历史案例研究，该研究将这些原则在实际部署中付诸实践。
- en: '**Checkpoint: Complete Neural Network System**'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查点：完整的神经网络系统**'
- en: 'Before examining how these concepts integrate in a real-world deployment, verify
    your understanding of the complete neural network lifecycle:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查这些概念如何在现实世界的部署中整合之前，验证您对完整神经网络生命周期的理解：
- en: '**Integration Across Phases:**'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '**跨阶段集成：**'
- en: '**Training to Deployment:**'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '**从训练到部署：**'
- en: '**Inference and Deployment:**'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理与部署：**'
- en: '**Systems Integration:**'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统集成：**'
- en: '**End-to-End Flow:**'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '**端到端流程：**'
- en: '**Self-Test**: For an MNIST digit classifier (784→128→64→10) deployed in production:
    (1) Explain why training this model requires ~12GB GPU memory while inference
    needs only ~400MB. (2) Trace a single digit image from camera capture through
    preprocessing, inference, and post-processing to final prediction. (3) Identify
    where bottlenecks might occur in a real-time system processing 100 images/second.
    (4) Describe how you would monitor for model degradation in production.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '**自我测试**：对于一个在生产中部署的MNIST数字分类器（784→128→64→10）：（1）解释为什么训练此模型需要约12GB GPU内存，而推理只需要约400MB。（2）追踪一个数字图像从相机捕获到预处理、推理和后处理的最终预测。（3）识别在每秒处理100个图像的实时系统中可能出现的瓶颈。（4）描述您如何在生产中监控模型退化。'
- en: '*The following case study demonstrates how these concepts integrate in a production
    system deployed at massive scale. Watch for how architectural choices, training
    strategies, and deployment constraints combine to create a working ML system.*'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下案例研究展示了这些概念如何在大规模部署的生产系统中整合。注意观察架构选择、训练策略和部署约束如何结合以创建一个工作的机器学习系统。*'
- en: 'Case Study: USPS Digit Recognition'
  id: totrans-683
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究：美国邮政服务数字识别
- en: 'We’ve explored neural networks from first principles—how neurons compute, how
    layers transform data, how training adjusts weights, and how inference makes predictions.
    These concepts might seem abstract, but they all came together in one of the first
    large-scale neural network deployments: the United States Postal Service’s handwritten
    digit recognition system. This historical example illustrates how the mathematical
    principles we’ve studied translate into practical engineering decisions, system
    trade-offs, and real-world performance constraints.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第一性原理探讨了神经网络——神经元如何计算，层如何转换数据，训练如何调整权重，以及推理如何做出预测。这些概念可能看起来很抽象，但它们都在第一个大规模神经网络部署中汇集在一起：美国邮政服务的手写数字识别系统。这个历史案例说明了我们所学的数学原理如何转化为实际工程决策、系统权衡和现实世界的性能约束。
- en: 'The theoretical foundations of neural networks find concrete expression in
    systems that solve real-world problems at scale. The USPS handwritten digit recognition
    system, deployed in the 1990s, exemplifies this translation from theory to practice.
    This early production deployment established many principles still relevant in
    modern ML systems: the importance of robust preprocessing pipelines, the need
    for confidence thresholds in automated decision-making, and the challenge of maintaining
    system performance under varying real-world conditions. While today’s systems
    deploy vastly more sophisticated architectures on more capable hardware, examining
    this foundational case study reveals how the optimization principles established
    earlier in this chapter combine to create production systems—lessons that scale
    from 1990s mail sorting to 2025’s edge AI deployments.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的理沦基础在解决大规模现实世界问题的系统中得到了具体体现。20世纪90年代部署的美国邮政服务手写数字识别系统就是这种从理论到实践的例证。这一早期的生产部署确立了现代机器学习系统中许多仍然相关的原则：稳健的预处理管道的重要性、自动化决策中置信度阈值的需要，以及在不断变化的现实世界条件下维持系统性能的挑战。虽然今天的系统在更强大的硬件上部署了更加复杂的架构，但研究这个基础案例研究揭示了本章早期建立的优化原则如何结合在一起创建生产系统——这些经验教训从20世纪90年代的邮件分类扩展到2025年的边缘人工智能部署。
- en: The Mail Sorting Challenge
  id: totrans-686
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 邮件分类挑战
- en: The United States Postal Service (USPS) processes over 100 million pieces of
    mail daily, each requiring accurate routing based on handwritten ZIP codes. In
    the early 1990s, human operators primarily performed this task, making it one
    of the largest manual data entry operations worldwide. The automation of this
    process through neural networks represents an early and successful large-scale
    deployment of artificial intelligence, embodying many core principles of neural
    computation.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 美国邮政服务（USPS）每天处理超过一亿件邮件，每件邮件都需要根据手写的ZIP代码进行准确的路线规划。在20世纪90年代初，人工操作员主要执行这项任务，使其成为世界上最大的手动数据录入操作之一。通过神经网络自动化这一过程代表了人工智能早期和成功的大规模部署，体现了许多神经计算的核心原则。
- en: 'The complexity of this task becomes evident: a ZIP code recognition system
    must process images of handwritten digits captured under varying conditions—different
    writing styles, pen types, paper colors, and environmental factors ([Figure 3.20](ch009.xhtml#fig-usps-digit-examples)).
    It must make accurate predictions within milliseconds to maintain mail processing
    speeds. Errors in recognition can lead to significant delays and costs from misrouted
    mail. This real-world constraint meant the system needed not just high accuracy,
    but also reliable measures of prediction confidence to identify when human intervention
    was necessary.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的复杂性变得显而易见：ZIP代码识别系统必须处理在多种条件下捕获的手写数字图像——不同的书写风格、笔的类型、纸张颜色和环境因素（[图3.20](ch009.xhtml#fig-usps-digit-examples)）。它必须在毫秒内做出准确的预测，以保持邮件处理速度。识别错误可能导致邮件误投，造成重大延误和成本。这个现实世界的限制意味着系统不仅需要高精度，还需要可靠的预测置信度度量，以确定何时需要人工干预。
- en: '![](../media/file52.jpg)'
  id: totrans-689
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file52.jpg)'
- en: 'Figure 3.20: **Handwritten Digit Variability**: Real-world handwritten digits
    exhibit significant variations in stroke width, slant, and character formation,
    posing challenges for automated recognition systems like those used by the USPS.
    These examples demonstrate the need for robust feature extraction and model generalization
    to achieve high accuracy in optical character recognition (OCR) tasks.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20：**手写数字的变异性**：现实世界中的手写数字在笔画宽度、倾斜度和字符形成方面存在显著差异，这对USPS使用的自动化识别系统等系统构成了挑战。这些示例说明了在光学字符识别（OCR）任务中实现高精度所需的鲁棒特征提取和模型泛化。
- en: This challenging environment presented requirements spanning every aspect of
    neural network implementation we’ve discussed, from biological inspiration to
    practical deployment considerations. The success or failure of the system would
    depend not just on the neural network’s accuracy, but on the entire pipeline from
    image capture through to final sorting decisions.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具有挑战性的环境提出了涵盖我们讨论的神经网络实现各个方面的要求，从生物启发到实际部署考虑。系统的成功或失败不仅取决于神经网络的准确性，还取决于从图像捕获到最终分类决策的整个流程。
- en: Engineering Process and Design Decisions
  id: totrans-692
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工程流程和设计决策
- en: The development of the USPS digit recognition system required careful consideration
    at every stage, from data collection to deployment. This process illustrates how
    theoretical principles of neural networks translate into practical engineering
    decisions.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: USPS数字识别系统的发展需要在每个阶段都进行仔细考虑，从数据收集到部署。这个过程说明了神经网络的理论原理如何转化为实际工程决策。
- en: Data collection presented the first major challenge. Unlike controlled laboratory
    environments, postal facilities needed to process mail pieces with tremendous
    variety. The training dataset had to capture this diversity. Digits written by
    people of different ages, educational backgrounds, and writing styles formed just
    part of the challenge. Envelopes came in varying colors and textures, and images
    were captured under different lighting conditions and orientations. This extensive
    data collection effort later contributed to the creation of the MNIST database
    we’ve used in our examples.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集带来了第一个主要挑战。与受控的实验室环境不同，邮政设施需要处理具有极大多样性的邮件件。训练数据集必须捕捉这种多样性。不同年龄、教育背景和书写风格的人所写的数字只是挑战的一部分。信封的颜色和质感各不相同，图像是在不同的光照条件和方向下捕获的。这项广泛的数据收集工作后来为我们在示例中使用的MNIST数据库的创建做出了贡献。
- en: The network architecture design required balancing multiple constraints. While
    deeper networks might achieve higher accuracy, they would also increase processing
    time and computational requirements. Processing <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel images
    of individual digits needed to complete within strict time constraints while running
    reliably on available hardware. The network had to maintain consistent accuracy
    across varying conditions, from well-written digits to hurried scrawls.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构设计需要平衡多个约束条件。虽然更深层的网络可能实现更高的准确性，但它们也会增加处理时间和计算需求。处理单个数字的<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>像素图像需要在严格的时间限制内完成，同时还要在可用的硬件上可靠运行。网络必须在不同的条件下保持一致的准确性，从书写良好的数字到匆忙的涂鸦。
- en: Training the network introduced additional complexity. The system needed to
    achieve high accuracy not just on a test dataset, but on the endless variety of
    real-world handwriting styles. Careful preprocessing normalized input images to
    account for variations in size and orientation. Data augmentation techniques increased
    the variety of training samples. The team validated performance across different
    demographic groups and tested under actual operating conditions to ensure robust
    performance.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络引入了额外的复杂性。系统不仅需要在测试数据集上实现高准确性，还需要在无尽多样的现实世界书写风格上实现高准确性。仔细的预处理将输入图像标准化，以考虑大小和方向的变化。数据增强技术增加了训练样本的多样性。团队在不同的人口群体中验证了性能，并在实际操作条件下进行了测试，以确保稳健的性能。
- en: The engineering team faced a critical decision regarding confidence thresholds.
    Setting these thresholds too high would route too many pieces to human operators,
    defeating the purpose of automation. Setting them too low would risk delivery
    errors. The solution emerged from analyzing the confidence distributions of correct
    versus incorrect predictions. This analysis established thresholds that optimized
    the tradeoff between automation rate and error rate, ensuring efficient operation
    while maintaining acceptable accuracy.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 工程团队面临着一个关于置信度阈值的关键决策。将这些阈值设置得太高会将太多邮件件路由给人工操作员，从而违背了自动化的目的。将这些阈值设置得太低则可能存在投递错误的风险。解决方案是通过分析正确与错误预测的置信度分布得出的。这种分析确定了优化自动化率和错误率之间权衡的阈值，确保高效运行的同时保持可接受的准确性。
- en: Production System Architecture
  id: totrans-698
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产系统架构
- en: Following a single piece of mail through the USPS recognition system illustrates
    how the concepts we’ve discussed integrate into a complete solution. The journey
    from physical mail piece to sorted letter demonstrates the interplay between traditional
    computing, neural network inference, and physical machinery.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随一封邮件通过USPS识别系统，可以说明我们讨论的概念如何整合成一个完整的解决方案。从物理邮件件到分拣信件的旅程展示了传统计算、神经网络推理和物理机械之间的相互作用。
- en: The process begins when an envelope reaches the imaging station. High-speed
    cameras capture the ZIP code region at rates exceeding several pieces of mail
    (e.g. 10) pieces per second. This image acquisition process must adapt to varying
    envelope colors, handwriting styles, and environmental conditions. The system
    must maintain consistent image quality despite the speed of operation, as motion
    blur and proper illumination present significant engineering challenges.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 当信封到达成像站时，过程开始。高速相机以每秒超过几件邮件（例如，10件）的速度捕捉ZIP代码区域。此图像获取过程必须适应不断变化的信封颜色、书写风格和环境条件。尽管操作速度很快，但系统必须保持一致的图像质量，因为运动模糊和适当的照明带来了重大的工程挑战。
- en: 'Pre-processing transforms these raw camera images into a format suitable for
    neural network analysis. The system must locate the ZIP code region, segment individual
    digits, and normalize each digit image. This stage employs traditional computer
    vision techniques: image thresholding adapts to envelope background color, connected
    component analysis identifies individual digits, and size normalization produces
    standard <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times
    28</annotation></semantics> pixel images. Speed remains critical; these operations
    must complete within milliseconds to maintain throughput.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理将这些原始相机图像转换为适合神经网络分析的形式。系统必须定位ZIP代码区域，分割单个数字，并对每个数字图像进行归一化。这一阶段采用传统的计算机视觉技术：图像阈值化适应信封背景颜色，连通分量分析识别单个数字，尺寸归一化产生标准的28×28像素图像。速度仍然至关重要；这些操作必须在毫秒内完成，以保持吞吐量。
- en: The neural network then processes each normalized digit image. The trained network,
    with its 89,610 parameters (as we detailed earlier), performs forward propagation
    to generate predictions. Each digit passes through two hidden layers of 100 neurons
    each, ultimately producing ten output values representing digit probabilities.
    This inference process, while computationally intensive, benefits from the optimizations
    we discussed in the previous section.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络随后处理每个归一化的数字图像。经过训练的网络，具有我们之前详细说明的89,610个参数，执行正向传播以生成预测。每个数字通过两个各有100个神经元的隐藏层，最终产生十个输出值，代表数字的概率。这一推理过程虽然计算密集，但得益于我们在上一节中讨论的优化。
- en: Post-processing converts these neural network outputs into sorting decisions.
    The system applies confidence thresholds to each digit prediction. A complete
    ZIP code requires high confidence in all five digits, a single uncertain digit
    flags the entire piece for human review. When confidence meets thresholds, the
    system transmits sorting instructions to mechanical systems that physically direct
    the mail piece to its appropriate bin.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理将这些神经网络输出转换为分类决策。系统对每个数字预测应用置信度阈值。一个完整的ZIP代码需要所有五个数字都高度置信，一个不确定的数字会将整个邮件标记为需要人工审查。当置信度达到阈值时，系统将分类指令传输到机械系统，这些系统将物理地将邮件件引导到其适当的分类箱中。
- en: The entire pipeline operates under strict timing constraints. From image capture
    to sorting decision, processing must complete before the mail piece reaches its
    sorting point. The system maintains multiple pieces in various pipeline stages
    simultaneously, requiring careful synchronization between computing and mechanical
    systems. This real-time operation illustrates why the optimizations we discussed
    in inference and post-processing become crucial in practical applications.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道在严格的时序约束下运行。从图像捕获到分类决策，处理必须在邮件件到达分类点之前完成。系统同时维护多个邮件件在各个管道阶段，需要计算和机械系统之间的仔细同步。这种实时操作说明了我们在推理和后处理中讨论的优化为什么在实际应用中变得至关重要。
- en: Performance Outcomes and Operational Impact
  id: totrans-705
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能结果和运营影响
- en: The implementation of neural network-based ZIP code recognition transformed
    USPS mail processing operations. By 2000, several facilities across the country
    utilized this technology, processing millions of mail pieces daily. This real-world
    deployment demonstrated both the potential and limitations of neural network systems
    in mission-critical applications.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的ZIP代码识别的实施改变了美国邮政服务（USPS）的邮件处理操作。到2000年，全国多个设施采用了这项技术，每天处理数百万件邮件。这一实际部署展示了神经网络系统在关键任务应用中的潜力和局限性。
- en: Performance metrics revealed interesting patterns that validate many of these
    fundamental principles. The system achieved its highest accuracy on clearly written
    digits, similar to those in the training data. However, performance varied significantly
    with real-world factors. Lighting conditions affected pre-processing effectiveness.
    Unusual writing styles occasionally confused the neural network. Environmental
    vibrations could also impact image quality. These challenges led to continuous
    refinements in both the physical system and the neural network pipeline.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标揭示了验证许多基本原理的有趣模式。系统在清晰书写的数字上达到了最高的准确率，这些数字与训练数据中的类似。然而，性能因现实世界因素而显著变化。光照条件影响了预处理的有效性。不寻常的书写风格有时会混淆神经网络。环境振动也可能影响图像质量。这些挑战导致了物理系统和神经网络管道的持续改进。
- en: The economic impact proved substantial. Prior to automation, manual sorting
    required operators to read and key in ZIP codes at an average rate of one piece
    per second. The neural network system processed pieces at ten times this rate
    while reducing labor costs and error rates. However, the system didn’t eliminate
    human operators entirely; their role shifted to handling uncertain cases and maintaining
    system performance. This hybrid approach, combining artificial and human intelligence,
    became a model for other automation projects.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 经济影响是显著的。在自动化之前，手动分拣需要操作员以平均每秒处理一件的速度读取和输入ZIP代码。神经网络系统以十倍于此的速度处理物品，同时降低了劳动力成本和错误率。然而，该系统并没有完全消除人工操作员；他们的角色转变为处理不确定案例和维护系统性能。这种结合人工和人类智能的混合方法成为其他自动化项目的典范。
- en: The system also revealed important lessons about deploying neural networks in
    production environments. Training data quality proved crucial; the network performed
    best on digit styles well-represented in its training set. Regular retraining
    helped adapt to evolving handwriting styles. Maintenance required both hardware
    specialists and deep learning experts, introducing new operational considerations.
    These insights influenced subsequent deployments of neural networks in other industrial
    applications.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 系统还揭示了在生产环境中部署神经网络的宝贵经验。训练数据质量至关重要；网络在训练集中代表良好的数字风格上表现最佳。定期重新训练有助于适应不断变化的书写风格。维护需要硬件专家和深度学习专家，引入了新的运营考虑。这些见解影响了神经网络在其他工业应用中的后续部署。
- en: 'Researchers discovered this implementation demonstrated how theoretical principles
    translate into practical constraints. The biological inspiration of neural networks
    provided the foundation for digit recognition, but successful deployment required
    careful consideration of system-level factors: processing speed, error handling,
    maintenance requirements, and integration with existing infrastructure. These
    lessons continue to inform modern deep learning deployments, where similar challenges
    of scale, reliability, and integration persist.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，这种实现展示了理论原则如何转化为实际约束。神经网络生物启发的数字识别提供了基础，但成功的部署需要仔细考虑系统级因素：处理速度、错误处理、维护需求和与现有基础设施的集成。这些经验教训继续为现代深度学习部署提供信息，其中类似的规模、可靠性和集成挑战仍然存在。
- en: Key Engineering Lessons and Design Principles
  id: totrans-711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键工程教训和设计原则
- en: The USPS ZIP code recognition system exemplifies the journey from biological
    inspiration to practical neural network deployment. It demonstrates how the basic
    principles of neural computation, from preprocessing through inference to postprocessing,
    combine to solve real-world problems.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 美国邮政服务ZIP代码识别系统展示了从生物启发到实际神经网络部署的历程。它展示了从预处理到推理再到后处理的神经网络基本原理如何结合解决现实世界问题。
- en: The system’s development shows why understanding both the theoretical foundations
    and practical considerations is crucial. While the biological visual system processes
    handwritten digits effortlessly, translating this capability into an artificial
    system required careful consideration of network architecture, training procedures,
    and system integration.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的开发展示了理解理论基础和实践考虑的重要性。虽然生物视觉系统可以轻松处理手写数字，但将这种能力转化为人工系统需要仔细考虑网络架构、训练程序和系统集成。
- en: 'The success of this early large-scale neural network deployment helped establish
    many practices we now consider standard: the importance of thorough training data,
    the need for confidence metrics, the role of pre- and post-processing, and the
    critical nature of system-level optimization.'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这种早期大规模神经网络部署的成功帮助确立了我们现在认为是标准的许多实践：详尽训练数据的重要性、信心指标的需求、预处理和后处理的作用以及系统级优化的关键性。
- en: 'The principles demonstrated by the USPS system—robust preprocessing, confidence-based
    decision making, and hybrid human-AI workflows—remain foundational in modern deployments,
    though the scale and sophistication have transformed dramatically. Where USPS
    deployed networks with ~100K parameters processing images at 10 pieces/second
    on specialized hardware consuming 50-100W, today’s mobile devices deploy models
    with 1-10M parameters processing 30+ frames/second for real-time vision tasks
    on neural processors consuming <2W. Edge AI systems in 2025—from smartphone face
    recognition to autonomous vehicle perception—face analogous challenges of balancing
    accuracy against computational constraints, but operate under far tighter power
    budgets (milliwatts vs watts) and stricter latency requirements (milliseconds
    vs tens of milliseconds). The core systems engineering principles remain constant:
    understanding the mathematical operations enables hardware-software co-design,
    preprocessing pipelines determine robustness to real-world variations, and confidence
    thresholding separates cases requiring human judgment from automated processing.
    This historical case study thus provides not merely historical context but a template
    for reasoning about modern ML systems deployment across the entire spectrum from
    cloud to edge to tiny devices.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: USPS系统所展示的原则——稳健的预处理、基于信心的决策以及人机混合工作流程——在现代部署中仍然是基础性的，尽管规模和复杂性发生了巨大变化。当时USPS部署的网络在专用硬件上以每秒10张的速度处理约10万个参数的图像，消耗50-100W的功率，而今天的移动设备部署的模型具有1-1000万个参数，每秒处理30多帧图像，用于实时视觉任务，在神经处理器上消耗的功率小于2W。2025年的边缘AI系统——从智能手机面部识别到自动驾驶车辆感知——面临着类似的挑战，即在准确性和计算约束之间取得平衡，但运行在远为紧张的功率预算（毫瓦与瓦特）和更严格的延迟要求（毫秒与数十毫秒）之下。核心的系统工程原则保持不变：理解数学运算使硬件-软件协同设计成为可能，预处理管道决定了对现实世界变化的鲁棒性，而信心阈值将需要人工判断的情况与自动化处理的情况分开。因此，这个历史案例研究不仅提供了历史背景，还为从云到边缘到微型设备的整个范围内现代机器学习系统部署的推理提供了一个模板。
- en: Deep Learning and the AI Triangle
  id: totrans-716
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习与AI三角形
- en: The neural network concepts we’ve explored throughout this chapter map directly
    onto the AI Triangle framework that governs all deep learning systems. This connection
    illuminates why deep learning requires such a fundamental rethinking of computational
    architectures and system design principles.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中探讨的神经网络概念直接映射到控制所有深度学习系统的AI三角形框架。这种联系阐明了为什么深度学习需要对计算架构和系统设计原则进行如此根本性的重新思考。
- en: '**Algorithms**: The mathematical foundations we’ve covered—forward propagation,
    activation functions, backpropagation, and gradient descent—define the algorithmic
    core of deep learning systems. The architecture choices we make (layer depths,
    neuron counts, connection patterns) directly determine the computational complexity,
    memory requirements, and training dynamics. Each activation function selection,
    from ReLU’s computational efficiency to sigmoid’s saturating gradients, represents
    an algorithmic decision with profound systems implications. The hierarchical feature
    learning that distinguishes neural networks from classical approaches emerges
    from these algorithmic building blocks, but success depends critically on the
    other two triangle components.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法**：我们已涵盖的数学基础——前向传播、激活函数、反向传播和梯度下降——定义了深度学习系统的算法核心。我们做出的架构选择（层深度、神经元数量、连接模式）直接决定了计算复杂度、内存需求和训练动态。每个激活函数的选择，从ReLU的计算效率到sigmoid的饱和梯度，都代表了具有深远系统影响的算法决策。区分神经网络与经典方法的层次特征学习正是源于这些算法构建块，但成功在很大程度上取决于其他两个三角形组件。'
- en: '**Data**: The learning process is entirely dependent on labeled data to calculate
    loss functions and guide weight updates through backpropagation. Our MNIST example
    demonstrated how data quality, distribution, and scale directly determine network
    performance—the algorithms remain identical, but data characteristics govern whether
    learning succeeds or fails. The shift from manual feature engineering to automatic
    representation learning doesn’t eliminate data dependency; it transforms the challenge
    from designing features to curating datasets that capture the full complexity
    of real-world patterns. Data preprocessing, augmentation, and validation strategies
    become algorithmic design decisions that shape the entire learning process.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据**：学习过程完全依赖于标记数据来计算损失函数并通过反向传播指导权重更新。我们的MNIST示例展示了数据质量、分布和规模如何直接决定网络性能——算法保持不变，但数据特征决定了学习是否成功。从手动特征工程到自动表示学习的转变并没有消除数据依赖性；它将挑战从设计特征转变为整理能够捕捉现实世界模式全部复杂性的数据集。数据预处理、增强和验证策略成为算法设计决策，这些决策塑造了整个学习过程。'
- en: '**Infrastructure**: The massive number of matrix multiplications required for
    forward and backward propagation reveals why specialized hardware infrastructure
    became essential for deep learning success. The memory bandwidth limitations we
    explored, the parallel computation patterns that favor GPU architectures, and
    the different computational demands of training versus inference all stem from
    the mathematical operations we’ve studied. The evolution from CPUs to GPUs to
    specialized AI accelerators directly responds to the computational patterns inherent
    in neural network algorithms. Understanding these mathematical foundations enables
    engineers to make informed decisions about hardware selection, memory hierarchy
    design, and distributed training strategies.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础设施**：前向传播和反向传播所需的巨大矩阵乘法数量揭示了为什么专门的硬件基础设施对于深度学习成功变得至关重要。我们探讨的内存带宽限制、有利于GPU架构的并行计算模式以及训练与推理的不同计算需求，都源自我们研究的数学运算。从CPU到GPU再到专门的AI加速器的演变，直接响应了神经网络算法固有的计算模式。理解这些数学基础使工程师能够就硬件选择、内存层次结构设计和分布式训练策略做出明智的决策。'
- en: 'The interdependence of these three components emerges through our chapter’s
    progression: algorithms define what computations are necessary, data determines
    whether those computations can learn meaningful patterns, and infrastructure determines
    whether the system can execute efficiently at scale. Neural networks succeeded
    not because any single component improved, but because advances in all three areas
    aligned—more sophisticated algorithms, larger datasets, and specialized hardware
    created a synergistic effect that transformed artificial intelligence.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个组件的相互依存性通过我们章节的进展而显现：算法定义了必要的计算，数据决定了这些计算是否能够学习有意义的模式，基础设施决定了系统是否能够在规模上高效执行。神经网络之所以成功，并不是因为任何单个组件有所改进，而是因为这三个领域的进步是一致的——更复杂的算法、更大的数据集和专门的硬件产生了一种协同效应，从而改变了人工智能。
- en: 'This AI Triangle perspective explains why deep learning engineering requires
    systems thinking that goes far beyond traditional software development. Optimizing
    any single component without considering the others leads to suboptimal outcomes:
    the most elegant algorithms fail without quality data, the best datasets remain
    unusable without adequate computational infrastructure, and the most powerful
    hardware achieves nothing without algorithms that can effectively learn from data.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 这种AI三角形的视角解释了为什么深度学习工程需要超越传统软件开发的系统思维。不考虑其他组件而优化任何单个组件会导致次优结果：最优雅的算法在没有高质量数据的情况下失败，最佳数据集在没有足够的计算基础设施的情况下仍然无法使用，最强大的硬件如果没有能够有效从数据中学习的算法，将一无所获。
- en: Fallacies and Pitfalls
  id: totrans-723
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误与陷阱
- en: Deep learning represents a paradigm shift from explicit programming to learning
    from data, which creates unique misconceptions about when and how to apply these
    powerful but complex systems. The mathematical foundations and statistical nature
    of neural networks often lead to misunderstandings about their capabilities, limitations,
    and appropriate use cases.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习代表了从显式编程到从数据学习的范式转变，这产生了关于何时以及如何应用这些强大但复杂的系统的独特误解。神经网络数学基础和统计性质往往导致对其能力、局限性和适当用例的误解。
- en: '**Fallacy:** *Neural networks are “black boxes” that cannot be understood or
    debugged.*'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *神经网络是“黑盒”，无法理解或调试。*'
- en: While neural networks lack the explicit rule-based transparency of traditional
    algorithms, multiple techniques enable understanding and debugging their behavior.
    Activation visualization reveals what patterns neurons respond to, gradient analysis
    shows how inputs affect outputs, and attention mechanisms highlight which features
    influence decisions. Layer-wise relevance propagation traces decision paths through
    the network, while ablation studies identify critical components. The perception
    of inscrutability often stems from attempting to understand neural networks through
    traditional programming paradigms rather than statistical and visual analysis
    methods. Modern interpretability tools provide insights into network behavior,
    though admittedly different from line-by-line code debugging.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络缺乏传统算法的显式规则基础透明度，但多种技术能够理解和调试其行为。激活可视化揭示了神经元对哪些模式做出响应，梯度分析显示了输入如何影响输出，而注意力机制突出了哪些特征影响决策。层级相关性传播追踪决策路径通过网络，而消融研究确定了关键组件。难以理解的感觉通常源于试图通过传统的编程范式而不是统计和可视化分析方法来理解神经网络。现代可解释性工具提供了对网络行为的洞察，尽管这确实不同于逐行代码调试。
- en: '**Fallacy:** *Deep learning eliminates the need for domain expertise and careful
    feature engineering.*'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *深度学习消除了对领域专业知识和仔细的特征工程的需求。*'
- en: The promise of automatic feature learning has led to the misconception that
    deep learning operates independently of domain knowledge. In reality, successful
    deep learning applications require extensive domain expertise to design appropriate
    architectures (convolutional layers for spatial data, recurrent structures for
    sequences), select meaningful training objectives, create representative datasets,
    and interpret model outputs within context. The USPS digit recognition system
    succeeded precisely because it incorporated postal service expertise about mail
    handling, digit writing patterns, and operational constraints. Domain knowledge
    guides critical decisions about data augmentation strategies, validation metrics,
    and deployment requirements that determine real-world success.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 自动特征学习的承诺导致了这样的误解，即深度学习可以独立于领域知识运作。实际上，成功的深度学习应用需要广泛的领域专业知识来设计适当的架构（例如，对于空间数据使用卷积层，对于序列使用循环结构），选择有意义的训练目标，创建代表性的数据集，并在特定情境下解释模型输出。USPS数字识别系统之所以成功，正是因为它结合了邮政服务关于邮件处理、数字书写模式和操作限制的专业知识。领域知识指导着关于数据增强策略、验证指标和部署要求的关键决策，这些决策决定了实际应用的成功。
- en: '**Pitfall:** *Using complex deep learning models for problems solvable with
    simpler methods.*'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *对于可以用更简单方法解决的问题使用复杂的深度学习模型。*'
- en: Teams frequently deploy sophisticated neural networks for tasks where linear
    models or decision trees would suffice, introducing unnecessary complexity, computational
    cost, and maintenance burden. A linear regression model requiring milliseconds
    to train may outperform a neural network requiring hours when data is limited
    or relationships are truly linear. Before employing deep learning, establish baseline
    performance with simple models. If a logistic regression achieves 95% accuracy
    on your classification task, the marginal improvement from a neural network rarely
    justifies the increased complexity. Reserve deep learning for problems exhibiting
    hierarchical patterns, non-linear relationships, or high-dimensional interactions
    that simpler models cannot capture.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 团队经常部署复杂的神经网络来完成可以用线性模型或决策树解决的问题，这引入了不必要的复杂性、计算成本和维护负担。一个训练时间只需毫秒的线性回归模型，在数据有限或关系真正线性时，可能比需要数小时训练的神经网络表现更好。在采用深度学习之前，使用简单模型建立基线性能。如果一个逻辑回归模型在分类任务上达到了95%的准确率，那么神经网络带来的边际改进很少能证明增加的复杂性是合理的。将深度学习保留用于那些表现出层次结构模式、非线性关系或高维交互的问题，这些是简单模型无法捕捉的。
- en: '**Pitfall:** *Training neural networks without understanding the underlying
    data distribution.*'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *在不理解潜在数据分布的情况下训练神经网络。*'
- en: Many practitioners treat neural network training as a mechanical process of
    feeding data through standard architectures, ignoring critical data characteristics
    that determine success. Networks trained on imbalanced datasets will exhibit poor
    performance on minority classes unless addressed through resampling or loss weighting.
    Non-stationary distributions require continuous retraining or adaptive mechanisms.
    Outliers can dominate gradient updates, preventing convergence. The USPS system
    required careful analysis of digit frequency distributions, writing style variations,
    and image quality factors before achieving production-ready performance. Successful
    training demands thorough exploratory data analysis, understanding of statistical
    properties, and continuous monitoring of data quality metrics throughout the training
    process.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者将神经网络训练视为一种机械过程，通过标准架构输入数据，忽略了决定成功的关键数据特征。在不平衡数据集上训练的网络，除非通过重采样或损失加权来解决，否则在少数类别上表现将不佳。非平稳分布需要持续的重训练或自适应机制。异常值可能会主导梯度更新，防止收敛。USPS系统在达到生产就绪性能之前，需要对数字频率分布、书写风格变化和图像质量因素进行仔细分析。成功的训练需要彻底的探索性数据分析、对统计特性的理解，以及在训练过程中持续监控数据质量指标。
- en: '**Pitfall:** *Assuming research-grade models can be deployed directly into
    production systems without system-level considerations.*'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *假设研究级模型可以直接部署到生产系统中，而不考虑系统级因素。*'
- en: Many teams treat model development as separate from system deployment, leading
    to failures when research prototypes encounter production constraints. A neural
    network achieving excellent accuracy on clean datasets may fail when integrated
    with real-time data pipelines, legacy databases, or distributed serving infrastructure.
    Production systems require consideration of latency budgets, memory constraints,
    concurrent user loads, and fault tolerance mechanisms that rarely appear in research
    environments. The transformation from research code to production systems demands
    careful attention to data preprocessing pipelines, model serialization formats,
    serving infrastructure scalability, and monitoring systems for detecting performance
    degradation. Successful deployment requires early collaboration between data science
    and systems engineering teams to align model requirements with operational constraints.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队将模型开发视为与系统部署分开，当研究原型遇到生产限制时会导致失败。一个在干净数据集上实现出色准确率的神经网络，当与实时数据管道、遗留数据库或分布式服务基础设施集成时可能会失败。生产系统需要考虑延迟预算、内存限制、并发用户负载和容错机制，这些在研究环境中很少出现。从研究代码到生产系统的转变需要仔细关注数据预处理管道、模型序列化格式、服务基础设施的可扩展性和监测系统，以检测性能下降。成功的部署需要数据科学和系统工程团队早期合作，以将模型需求与操作限制相一致。
- en: Summary
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Neural networks transform computational approaches by replacing rule-based programming
    with adaptive systems that learn patterns from data. Building on the biological-to-artificial
    neuron mappings explored throughout this chapter, these systems create practical
    implementations that process complex information and improve performance through
    experience.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过用从数据中学习模式的自适应系统取代基于规则的编程，改变了计算方法。在本章中探索的从生物到人工神经元映射的基础上，这些系统创建了处理复杂信息并通过经验提高性能的实用实现。
- en: Neural network architecture demonstrates hierarchical processing, where each
    layer extracts progressively more abstract patterns from raw data. Training adjusts
    connection weights through iterative optimization to minimize prediction errors,
    while inference applies learned knowledge to make predictions on new data. This
    separation between learning and application phases creates distinct system requirements
    for computational resources, memory usage, and processing latency that shape system
    design and deployment strategies.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构展示了层次化处理，其中每一层从原始数据中逐步提取更抽象的模式。训练通过迭代优化调整连接权重以最小化预测误差，而推理则将学习到的知识应用于对新数据的预测。这种学习阶段和应用阶段的分离，为计算资源、内存使用和处理延迟等系统设计部署策略创造了不同的系统需求。
- en: 'This chapter established mathematics and systems implications through fully-connected
    architectures. The multilayer perceptrons explored here demonstrate universal
    function approximation. With enough neurons and appropriate weights, such networks
    can theoretically learn any continuous function. This mathematical generality
    comes with computational costs. Consider our MNIST example: a 28×28 pixel image
    contains 784 input values, and a fully-connected network treats each pixel independently,
    learning 61,400 weights just in the first layer (784 inputs × 100 neurons). Neighboring
    pixels are highly correlated while distant pixels rarely interact. Fully-connected
    architectures expend computational resources learning irrelevant long-range relationships.'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过全连接架构建立了数学和系统含义。在这里探索的多层感知器展示了通用函数逼近。只要有足够的神经元和适当的权重，这样的网络理论上可以学习任何连续函数。这种数学的普遍性伴随着计算成本。以我们的MNIST示例来说：一个28×28像素的图像包含784个输入值，一个全连接网络独立地处理每个像素，仅在第一层就学习到61,400个权重（784个输入
    × 100个神经元）。相邻像素高度相关，而远距离像素很少交互。全连接架构在学习无关的长距离关系上消耗了计算资源。
- en: '**Key Takeaways**'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Neural networks replace hand-coded rules with adaptive patterns discovered from
    data through hierarchical processing architectures
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络用从数据中通过分层处理架构发现的自适应模式替换了手工编码的规则。
- en: Fully-connected networks provide universal approximation capability but sacrifice
    computational efficiency by treating all input relationships equally
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接网络提供了通用逼近能力，但通过平等对待所有输入关系而牺牲了计算效率。
- en: Training and inference represent distinct operational phases with different
    computational demands and system design requirements
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和推理代表了不同的操作阶段，具有不同的计算需求和系统设计要求。
- en: Complete processing pipelines integrate traditional computing with neural computation
    across preprocessing, inference, and postprocessing stages
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的处理管道将传统计算与神经计算集成到预处理、推理和后处理阶段。
- en: System-level considerations—from activation function selection to batch size
    configuration to network topology—directly determine deployment feasibility across
    cloud, edge, and tiny devices
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统级考虑——从激活函数的选择到批量大小配置再到网络拓扑——直接决定了在云、边缘和微型设备上的部署可行性。
- en: Specialized architectures (CNNs, RNNs, Transformers) encode problem structure
    into network design, achieving dramatic efficiency gains over fully-connected
    alternatives
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用架构（CNNs、RNNs、Transformers）将问题结构编码到网络设计中，在完全连接的替代方案上实现了显著的效率提升。
- en: 'Real-world problems exhibit structure that generic fully-connected networks
    cannot efficiently exploit: images have spatial locality, text has sequential
    dependencies, graphs have relational patterns, time-series data has temporal dynamics.
    This structural blindness creates three critical problems: computational waste
    (learning relationships that don’t exist), data inefficiency (requiring more training
    examples to learn patterns that could be encoded structurally), and poor scalability
    (parameter counts explode as input dimensions grow).'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 实际问题表现出通用全连接网络无法有效利用的结构：图像具有空间局部性，文本具有序列依赖性，图具有关系模式，时间序列数据具有时间动态性。这种结构盲点造成了三个关键问题：计算浪费（学习不存在的关联关系）、数据效率低下（需要更多的训练示例来学习可以结构化编码的模式），以及可扩展性差（随着输入维度的增加，参数数量激增）。
- en: The next chapter ([Chapter 4](ch010.xhtml#sec-dnn-architectures)) addresses
    these limitations by introducing specialized architectures that encode problem
    structure directly into network design. Convolutional Neural Networks exploit
    spatial locality for vision tasks, achieving state-of-the-art performance with
    10-100× fewer parameters through restricted connections and weight sharing. Recurrent
    Neural Networks capture temporal dependencies for sequential data through hidden
    states, though sequential processing creates parallelization challenges. Transformers
    enable parallel processing of sequences through attention mechanisms, revolutionizing
    natural language processing while introducing new memory scaling challenges.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章（[第4章](ch010.xhtml#sec-dnn-architectures)）通过引入直接将问题结构编码到网络设计中的专用架构来解决这些限制。卷积神经网络通过限制连接和权重共享，利用空间局部性进行视觉任务，通过更少的参数（10-100×）实现了最先进的性能。循环神经网络通过隐藏状态捕获序列数据的时间依赖性，尽管序列处理创造了并行化挑战。变换器通过注意力机制实现序列的并行处理，革命性地改变了自然语言处理，同时也引入了新的内存扩展挑战。
- en: Each architectural innovation brings systems engineering trade-offs that build
    directly on the foundations established in this chapter. Convolutional layers
    demand different memory access patterns than fully-connected layers, recurrent
    networks face different parallelization constraints, and attention mechanisms
    create new computational bottlenecks. The mathematical operations remain the same
    matrix multiplications and non-linear activations we’ve studied, but their organization
    changes systems requirements.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 每一项架构创新都带来了系统工程的权衡，这些权衡直接建立在本章建立的基础之上。卷积层需要与全连接层不同的内存访问模式，循环网络面临不同的并行化约束，而注意力机制创造了新的计算瓶颈。数学运算仍然是我们在研究中学习的矩阵乘法和非线性激活，但它们的组织方式改变了系统需求。
- en: Understanding these specialized architectures represents the natural next step
    in ML systems engineering—taking the principles of forward propagation, gradient
    descent, and activation functions we’ve mastered here and applying them within
    architectures designed for both computational efficiency and problem-specific
    structure. The journey from biological inspiration to mathematical formulation
    to systems implementation continues as we explore how to build neural networks
    that not only learn effectively but do so within the constraints of real-world
    computational systems.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些专业架构代表着在机器学习系统工程中的自然下一步——将我们已掌握的前向传播、梯度下降和激活函数的原则应用于旨在计算效率和特定问题结构的架构中。随着我们探索如何构建不仅学习效果良好，而且在现实世界计算系统约束下也能学习的神经网络，从生物灵感到数学公式再到系统实现的过程持续进行。
- en: '* * *'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
