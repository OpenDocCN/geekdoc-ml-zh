["```py\nfrom fairlearn.metrics import MetricFrame\nfrom sklearn.metrics import accuracy_score, precision_score\n\n# Loan approval model evaluation across demographic groups\nmf = MetricFrame(\n    metrics={\n        \"approval_rate\": accuracy_score,\n        \"precision\": precision_score,\n        \"false_positive_rate\": lambda y_true, y_pred: (\n            (y_pred == 1) & (y_true == 0)\n        ).sum()\n        / (y_true == 0).sum(),\n    },\n    y_true=loan_approvals_actual,\n    y_pred=loan_approvals_predicted,\n    sensitive_features=applicant_demographics[\"ethnicity\"],\n)\n\n# Display performance disparities across ethnic groups\nprint(\"Loan Approval Performance by Ethnic Group:\")\nprint(mf.by_group)\n# Output shows: Asian: 94% approval, White: 91% approval,\n# Hispanic: 73% approval, Black: 68% approval\n```", "```py\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n\n@dataclass\nclass FairnessMetrics:\n    demographic_parity_diff: float\n    equalized_odds_diff: float\n    equality_opportunity_diff: float\n    group_counts: Dict[str, int]\n\n\nclass RealTimeFairnessMonitor:\n    def __init__(\n        self, window_size: int = 1000, alert_threshold: float = 0.05\n    ):\n        self.window_size = window_size\n        self.alert_threshold = alert_threshold\n        self.predictions_buffer = []\n        self.demographics_buffer = []\n        # For actual outcomes when available\n        self.labels_buffer = []\n\n    async def process_prediction(\n        self,\n        prediction: int,\n        demographics: Dict[str, str],\n        actual_label: Optional[int] = None,\n    ) -> FairnessMetrics:\n        \"\"\"Process single prediction and update fairness metrics\"\"\"\n```", "```py\n        # Store in rolling window buffer\n        self.predictions_buffer.append(prediction)\n        self.demographics_buffer.append(demographics)\n        if actual_label is not None:\n            self.labels_buffer.append(actual_label)\n\n        # Maintain window size\n        if len(self.predictions_buffer) > self.window_size:\n            self.predictions_buffer.pop(0)\n            self.demographics_buffer.pop(0)\n            if self.labels_buffer:\n                self.labels_buffer.pop(0)\n\n        # Compute fairness metrics\n        metrics = self._compute_fairness_metrics()\n\n        # Check for bias alerts\n        if (\n            metrics.demographic_parity_diff > self.alert_threshold\n            or metrics.equalized_odds_diff > self.alert_threshold\n        ):\n            await self._trigger_bias_alert(metrics)\n\n        return metrics\n\n    def _compute_fairness_metrics(self) -> FairnessMetrics:\n        \"\"\"Compute demographic parity and equalized odds\"\"\"\n        \"\"\"across groups\"\"\"\n        if len(self.predictions_buffer) < 100:  # Minimum sample size\n            return FairnessMetrics(0.0, 0.0, 0.0, {})\n\n        # Group predictions by protected attribute\n        groups = {}\n        for i, demo in enumerate(self.demographics_buffer):\n            group = demo.get(\"ethnicity\", \"unknown\")\n            if group not in groups:\n                groups[group] = {\"predictions\": [], \"labels\": []}\n            groups[group][\"predictions\"].append(\n                self.predictions_buffer[i]\n            )\n            if i < len(self.labels_buffer):\n                groups[group][\"labels\"].append(self.labels_buffer[i])\n\n        # Compute demographic parity (approval rates)\n        approval_rates = {}\n        for group, data in groups.items():\n            if len(data[\"predictions\"]) > 0:\n                approval_rates[group] = np.mean(data[\"predictions\"])\n\n        demo_parity_diff = (\n            max(approval_rates.values())\n            - min(approval_rates.values())\n            if len(approval_rates) > 1\n            else 0.0\n        )\n\n        # Compute equalized odds (TPR/False Positive Rate\n        # differences) if labels available\n        eq_odds_diff = 0.0\n        eq_opp_diff = 0.0\n\n        if self.labels_buffer and len(groups) > 1:\n            tpr_by_group = {}\n            fpr_by_group = {}\n\n            for group, data in groups.items():\n                if (\n                    len(data[\"labels\"]) > 10\n                ):  # Minimum for reliable metrics\n                    tn, fp, fn, tp = confusion_matrix(\n                        data[\"labels\"], data[\"predictions\"]\n                    ).ravel()\n                    tpr_by_group[group] = (\n                        tp / (tp + fn) if (tp + fn) > 0 else 0\n                    )\n                    fpr_by_group[group] = (\n                        fp / (fp + tn) if (fp + tn) > 0 else 0\n                    )\n\n            if len(tpr_by_group) > 1:\n                eq_odds_diff = max(\n                    abs(tpr_by_group[g1] - tpr_by_group[g2])\n                    for g1 in tpr_by_group\n                    for g2 in tpr_by_group\n                )\n                eq_opp_diff = max(tpr_by_group.values()) - min(\n                    tpr_by_group.values()\n                )\n\n        group_counts = {\n            group: len(data[\"predictions\"])\n            for group, data in groups.items()\n        }\n\n        return FairnessMetrics(\n            demographic_parity_diff=demo_parity_diff,\n            equalized_odds_diff=eq_odds_diff,\n            equality_opportunity_diff=eq_opp_diff,\n            group_counts=group_counts,\n        )\n\n    async def _trigger_bias_alert(self, metrics: FairnessMetrics):\n        \"\"\"Trigger alert when bias threshold exceeded\"\"\"\n        alert_message = (\n          f\"BIAS ALERT: Demographic parity difference: \"\n          f\"{metrics.demographic_parity_diff:.3f}, \"\n        )\n        alert_message += (\n          f\"Equalized odds difference: \"\n          f\"{metrics.equalized_odds_diff:.3f}\"\n        )\n\n        # Log to audit system\n        print(f\"[AUDIT] {alert_message}\")\n\n        # Could trigger additional actions:\n        # - Send alert to monitoring dashboard\n        # - Temporarily enable manual review\n        # - Trigger model retraining pipeline\n        # - Adjust decision thresholds\n```"]