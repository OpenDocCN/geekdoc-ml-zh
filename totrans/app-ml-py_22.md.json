["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as stats                                   # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.patches import Rectangle                      # build a custom legend\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.linear_model import LinearRegression             # frequentist model for comparison\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef next_proposal(prev_theta, step_stdev = 0.5):              # assuming a Gaussian distribution centered on previous theta and step stdev \n    out_theta = stats.multivariate_normal(mean=prev_theta,cov=np.eye(3)*step_stdev**2).rvs(1)\n    return out_theta\n\ndef likelihood_density(x,y,theta):                            # likelihood - probability (density) of the data given the model\n    density = np.sum(stats.norm.logpdf(y, loc=theta[0]*x+theta[1],scale=theta[2])) # assume independence, sum is product in log space\n    return density\n\ndef prior_density(theta,prior):                               # prior - probability (density) of the model parameters given the prior \n    mean = np.array([prior[0,0],prior[1,0],prior[2,0]]); cov = np.zeros([3,3]); cov[0,0] = prior[0,1]; cov[1,1] = prior[1,1]; cov[2,2] = prior[2,1]\n    prior_out = stats.multivariate_normal.logpdf(theta,mean=mean,cov=cov,allow_singular=True)\n    return prior_out\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nnp.random.seed(seed = seed)                                   # set random number seed\n\ndata_b1 = 3; data_b0 = 20; data_sigma = 5; n = 100            # set data model parameters\n\nX = np.random.rand(n)*30                                      # random x values\ny = data_b1*X+data_b0 + np.random.normal(loc=0.0,scale=data_sigma,size=n) # y as a linear function of x + random noise \n\nXname = [r'$X_1$']; yname = [r'$y$']                          # specify the predictor features (x2) and response feature (x1)\nXmin = 0.0; Xmax = 30; ymin = 0.0; ymax = 120                 # set minimums and maximums for visualization \nXunit = ['none']; yunit = ['none'] \nXlabelunit = Xname[0] + ' (' + Xunit[0] + ')'\nylabelunit = yname[0] + ' (' + yunit[0] + ')'\n\nxhat = np.linspace(Xmin,Xmax,100)                             # set of x values to predict and visualize the model\nlinear_model = LinearRegression().fit(X.reshape(-1, 1),y)     # instantiate and train the frequentist linear regression model\nyhat = linear_model.predict(xhat.reshape(-1, 1))              # make predictions for model plotting\n\nslope = linear_model.coef_[0]\nintercept = linear_model.intercept_\n\nplt.subplot(111)                                              # plot the data and model\nplt.scatter(X, y,c='red',s=10,edgecolor='black')\nplt.plot(xhat,yhat,c='red'); add_grid()\nplt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel(Xlabelunit); plt.ylabel(ylabelunit)\nadd_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[18.0,38])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope,2)),[16.0,30])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept,2)),[16.0,20])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[21.0,30])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope,2)) + r' $\\times$ $z$ + (' + str(round(intercept,2)) + ')',[21.0,20])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprior = np.zeros([3,2])                                       # prior distributions\nprior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation\nprior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation\nprior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phi^2 \n\nplt.subplot(131)                                              # slope prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplot(132)                                              # intercept prior distribution\nplt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)\n\nplt.subplot(133)                                              # noise prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$\\sigma$'); plt.ylabel('Natural Log Density'); plt.title(\"Gamma Prior Distribution, $\\sigma$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() \n```", "```py\nnp.random.seed(seed = seed)\nstep_stdev = 0.2\n\nthetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step\naccepted = 0\n\nn = 10000                                                     # number of attempts, include accepted and rejected\n\nfor i in range(n):\n    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal\n\n    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density\n    log_like = likelihood_density(X,y,thetas[-1,:])\n\n    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density\n    log_prior = prior_density(thetas[-1,:],prior)\n\n    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio\n\n    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio\n        thetas = np.vstack((thetas,theta_new)); accepted += 1\n\nprint('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')\n\ndf = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) \n```", "```py\nAccepted 1603 samples of 10000 attempts \n```", "```py\nalpha = 0.1                                                   # alpha level for displayed confidence intervals \n\nplt.subplot(131)                                              # plot slope, b_1, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title(\"McMC Bayesian Linear Regression Slope\"); add_grid()\nplt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplot(132)                                              # plot intercept, b_0, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title(\"McMC Bayesian Linear Regression Intercept\"); add_grid()\nplt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,30])\n\nplt.subplot(133)                                              # plot noise, sigma, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\\sigma$'); plt.title(\"McMC Bayesian Linear Regression Sigma\"); add_grid()\nplt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)\nplt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nburn_chain = 250\nplt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)\nplt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)\nsns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)\nplt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)\nadd_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') \nplt.xlim([0,5]); plt.ylim([0,25])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as stats                                   # statistical methods\nimport pandas as pd                                           # DataFrames\nimport pandas.plotting as pd_plot\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.patches import Rectangle                      # build a custom legend\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.linear_model import LinearRegression             # frequentist model for comparison\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef next_proposal(prev_theta, step_stdev = 0.5):              # assuming a Gaussian distribution centered on previous theta and step stdev \n    out_theta = stats.multivariate_normal(mean=prev_theta,cov=np.eye(3)*step_stdev**2).rvs(1)\n    return out_theta\n\ndef likelihood_density(x,y,theta):                            # likelihood - probability (density) of the data given the model\n    density = np.sum(stats.norm.logpdf(y, loc=theta[0]*x+theta[1],scale=theta[2])) # assume independence, sum is product in log space\n    return density\n\ndef prior_density(theta,prior):                               # prior - probability (density) of the model parameters given the prior \n    mean = np.array([prior[0,0],prior[1,0],prior[2,0]]); cov = np.zeros([3,3]); cov[0,0] = prior[0,1]; cov[1,1] = prior[1,1]; cov[2,2] = prior[2,1]\n    prior_out = stats.multivariate_normal.logpdf(theta,mean=mean,cov=cov,allow_singular=True)\n    return prior_out\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nnp.random.seed(seed = seed)                                   # set random number seed\n\ndata_b1 = 3; data_b0 = 20; data_sigma = 5; n = 100            # set data model parameters\n\nX = np.random.rand(n)*30                                      # random x values\ny = data_b1*X+data_b0 + np.random.normal(loc=0.0,scale=data_sigma,size=n) # y as a linear function of x + random noise \n\nXname = [r'$X_1$']; yname = [r'$y$']                          # specify the predictor features (x2) and response feature (x1)\nXmin = 0.0; Xmax = 30; ymin = 0.0; ymax = 120                 # set minimums and maximums for visualization \nXunit = ['none']; yunit = ['none'] \nXlabelunit = Xname[0] + ' (' + Xunit[0] + ')'\nylabelunit = yname[0] + ' (' + yunit[0] + ')'\n\nxhat = np.linspace(Xmin,Xmax,100)                             # set of x values to predict and visualize the model\nlinear_model = LinearRegression().fit(X.reshape(-1, 1),y)     # instantiate and train the frequentist linear regression model\nyhat = linear_model.predict(xhat.reshape(-1, 1))              # make predictions for model plotting\n\nslope = linear_model.coef_[0]\nintercept = linear_model.intercept_\n\nplt.subplot(111)                                              # plot the data and model\nplt.scatter(X, y,c='red',s=10,edgecolor='black')\nplt.plot(xhat,yhat,c='red'); add_grid()\nplt.title('Linear Regression Model, Regression of ' + yname[0] + ' on ' + Xname[0])\nplt.xlabel(Xlabelunit); plt.ylabel(ylabelunit)\nadd_grid(); plt.xlim([Xmin,Xmax]); plt.ylim([ymin,ymax])\nplt.annotate('Linear Regression Model',[18.0,38])\nplt.annotate(r'    $\\beta_1$ :' + str(round(slope,2)),[16.0,30])\nplt.annotate(r'    $\\beta_0$ :' + str(round(intercept,2)),[16.0,20])\nplt.annotate(r'$N[\\phi] = \\beta_1 \\times z + \\beta_0$',[21.0,30])\nplt.annotate(r'$N[\\phi] = $' + str(round(slope,2)) + r' $\\times$ $z$ + (' + str(round(intercept,2)) + ')',[21.0,20])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprior = np.zeros([3,2])                                       # prior distributions\nprior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation\nprior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation\nprior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phi^2 \n\nplt.subplot(131)                                              # slope prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplot(132)                                              # intercept prior distribution\nplt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)\n\nplt.subplot(133)                                              # noise prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$\\sigma$'); plt.ylabel('Natural Log Density'); plt.title(\"Gamma Prior Distribution, $\\sigma$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() \n```", "```py\nnp.random.seed(seed = seed)\nstep_stdev = 0.2\n\nthetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step\naccepted = 0\n\nn = 10000                                                     # number of attempts, include accepted and rejected\n\nfor i in range(n):\n    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal\n\n    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density\n    log_like = likelihood_density(X,y,thetas[-1,:])\n\n    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density\n    log_prior = prior_density(thetas[-1,:],prior)\n\n    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio\n\n    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio\n        thetas = np.vstack((thetas,theta_new)); accepted += 1\n\nprint('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')\n\ndf = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) \n```", "```py\nAccepted 1603 samples of 10000 attempts \n```", "```py\nalpha = 0.1                                                   # alpha level for displayed confidence intervals \n\nplt.subplot(131)                                              # plot slope, b_1, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title(\"McMC Bayesian Linear Regression Slope\"); add_grid()\nplt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplot(132)                                              # plot intercept, b_0, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title(\"McMC Bayesian Linear Regression Intercept\"); add_grid()\nplt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,30])\n\nplt.subplot(133)                                              # plot noise, sigma, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\\sigma$'); plt.title(\"McMC Bayesian Linear Regression Sigma\"); add_grid()\nplt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)\nplt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nburn_chain = 250\nplt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)\nplt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)\nsns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)\nplt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)\nadd_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') \nplt.xlim([0,5]); plt.ylim([0,25])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nprior = np.zeros([3,2])                                       # prior distributions\nprior[0,:] = [5.0,1.0]                                        # Gaussian prior model for slope, mean and standard deviation\nprior[1,:] = [18.0,2.0]                                       # Gaussian prior model for intercept, mean and standard deviation\nprior[2,:] = [7.0,1.0]                                        # Gaussian prior model for sigma, k (shape) and phi (scale), recall mean = k x phi, var = k x phi^2 \n\nplt.subplot(131)                                              # slope prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[0,0],scale=prior[0,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_1$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplot(132)                                              # intercept prior distribution\nplt.plot(np.arange(0,100,0.01),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,100,0.01),np.full(10000,-1.0e20),stats.norm.logpdf(np.arange(0,100,0.01),loc=prior[1,0],scale=prior[1,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$b_0$'); plt.ylabel('Natural Log Density'); plt.title(\"Gaussian Prior Distribution, $b_1$\"); add_grid(); plt.xlim([0,100]); plt.ylim(ylim)\n\nplt.subplot(133)                                              # noise prior distribution\nplt.plot(np.arange(0,10,0.01),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),c='red'); ylim = plt.gca().get_ylim() \nplt.fill_between(np.arange(0,10,0.01),np.full(1000,-1.0e20),stats.norm.logpdf(np.arange(0,10,0.01),loc=prior[2,0],scale=prior[2,1]),color='black',alpha=0.05,zorder=1)\nplt.xlabel(r'$\\sigma$'); plt.ylabel('Natural Log Density'); plt.title(\"Gamma Prior Distribution, $\\sigma$\"); add_grid(); plt.xlim([0,10]); plt.ylim(ylim)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.35, hspace=0.5); plt.show() \n```", "```py\nnp.random.seed(seed = seed)\nstep_stdev = 0.2\n\nthetas = np.random.rand(3).reshape(1,-1)                      # seed a random first step\naccepted = 0\n\nn = 10000                                                     # number of attempts, include accepted and rejected\n\nfor i in range(n):\n    theta_new = next_proposal(thetas[-1,:],step_stdev=step_stdev) # next proposal\n\n    log_like_new = likelihood_density(X,y,theta_new)          # new and prior likelihoods, log of density\n    log_like = likelihood_density(X,y,thetas[-1,:])\n\n    log_prior_new = prior_density(theta_new,prior)            # new and prior, log of density\n    log_prior = prior_density(thetas[-1,:],prior)\n\n    likelihood_prior_proposal_ratio = np.exp((log_like_new + log_prior_new) - (log_like + log_prior)) # calculate log ratio\n\n    if likelihood_prior_proposal_ratio > np.random.rand(1):   # conditionally accept by likelihood ratio\n        thetas = np.vstack((thetas,theta_new)); accepted += 1\n\nprint('Accepted ' + str(accepted) + ' samples of ' + str(n) + ' attempts')\n\ndf = pd.DataFrame(np.vstack([thetas[:,0],thetas[:,1],thetas[:,2]]).T, columns= ['Slope','Intercept','Sigma']) \n```", "```py\nAccepted 1603 samples of 10000 attempts \n```", "```py\nalpha = 0.1                                                   # alpha level for displayed confidence intervals \n\nplt.subplot(131)                                              # plot slope, b_1, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,0],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_1$'); plt.title(\"McMC Bayesian Linear Regression Slope\"); add_grid()\nplt.plot([0,accepted],[linear_model.coef_[0],linear_model.coef_[0]],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.coef_[0]+0.05],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[0,0],prior[0,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[0,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[0,0],scale=prior[0,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplot(132)                                              # plot intercept, b_0, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,1],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$b_0$'); plt.title(\"McMC Bayesian Linear Regression Intercept\"); add_grid()\nplt.plot([0,accepted],[linear_model.intercept_,linear_model.intercept_],c='blue',zorder=200)\nplt.annotate('Linear Regression',[30,linear_model.intercept_+0.25],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[1,0],prior[1,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[1,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[1,0],scale=prior[1,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,30])\n\nplt.subplot(133)                                              # plot noise, sigma, samples\nplt.plot(np.arange(0,accepted+1,1),thetas[:,2],c='red',zorder=100) \nplt.xlabel('McMC Metropolis Sample'); plt.ylabel(r'$\\sigma$'); plt.title(\"McMC Bayesian Linear Regression Sigma\"); add_grid()\nplt.plot([0,accepted],[data_sigma,data_sigma],color='blue',zorder=200)\nplt.annotate('Synthetic Data Noise',[30,data_sigma+0.06],color='blue',zorder=200)\nplt.plot([0,accepted],[prior[2,0],prior[2,0]],c='black',zorder=10)\nplt.annotate('Prior Model [P' + str((int(alpha/2*100))) + ',P' + str((int((1.0-alpha/2)*100))) + ']',[30,prior[2,0]+0.07])\nlower = stats.norm.ppf(alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[lower,lower],c='black',ls='--',zorder=10)\nupper = stats.norm.ppf(1-alpha/2.0,loc=prior[2,0],scale=prior[2,1])\nplt.plot([0,accepted],[upper,upper],c='black',ls='--',zorder=10)\nplt.fill_between([0,accepted],[lower,lower],[upper,upper],color='black',alpha=0.05,zorder=1)\nplt.xlim([0,accepted]); plt.ylim([0,10])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5); plt.show() \n```", "```py\nburn_chain = 250\nplt.scatter(thetas[:burn_chain,0],thetas[:burn_chain,1],s=5,marker = 'x',c='black',alpha=0.8,linewidth=0.3,cmap=plt.cm.inferno,zorder=10)\nplt.scatter(thetas[burn_chain:,0],thetas[burn_chain:,1],s=5,c=np.arange(burn_chain,accepted+1,1),alpha=1.0,edgecolor='black',linewidth=0.1,cmap=plt.cm.inferno,zorder=10)\nsns.kdeplot(data=df[burn_chain:],x='Slope',y='Intercept',color='grey',linewidths=1.00,alpha=0.9,levels=np.logspace(-4,-0,3),zorder=100)\nplt.plot(thetas[:burn_chain,0],thetas[:burn_chain,1],color='black',linewidth=0.1,zorder=1)\nadd_grid(); plt.xlabel('Slope, $b_1$'); plt.ylabel('Intercept, $b_0$'); plt.title('McMC Metropolis Samples Bayesian Linear Regression') \nplt.xlim([0,5]); plt.ylim([0,25])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.2, hspace=0.5); plt.show() \n```"]