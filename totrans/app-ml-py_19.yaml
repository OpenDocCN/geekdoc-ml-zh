- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_linear_regression.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_linear_regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **Linear Regression**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression](https://youtu.be/0fzbyhWiP84)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ridge Regression](https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LASSO Regression](https://youtu.be/cVFYhlCCI_8?si=NbwIDaZj30vxezn2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Norms](https://youtu.be/JmxGlrurQp0?si=vuF1TXDbZkyRC1j-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael‚Äôs Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow, demonstration of linear regression for machine learning-based
    predictions. Why start with linear regression?
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is the simplest parametric predictive machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learn about training machine learning models with an analytical solution
    calculated from the derivative of training MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get‚Äôs us started with the concepts of loss functions and norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have access to analytics expressions for confidence intervals for model uncertainty,
    and hypothesis tests for parameter significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs some basic details about linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/806bf5f702f9bb5a63e30d6e1f7969d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada2fcc2740c48478e79404563c91061.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/835541b16e1038a4606f7d97b628c4f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytical Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the loss is \(L^2\) we have access to a non-iterative, analytics solution,
    i.e., optimization of the linear regression model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: we are looking for the model parameter(s) that minimize the loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when we have an analytical solution, we can use the 1st and 2nd derivatives
    of the loss function,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: A local or global loss minimum occurs where,
  prefs: []
  type: TYPE_NORMAL
- en: first derivative of the loss function is 0.0 - zero slope indicating a local
    minimum or maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L(y_{\alpha}, F(X_{\alpha}))}{\partial b_1} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: second derivative of the loss function is greater than 0.0 - positive curvature
    indicated a local minimum instead of maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 L(y_{\alpha}, F(X_{\alpha}))}{\partial b_1^2} > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs use this approach to derived the solution for linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the least squares (L2) norm the linear regression model parameters can be
    trained with training data with an analytical solution. Let‚Äôs derive it for linear
    regression analytical solution for the case of 1 predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: To calculated the slope coefficient, \(b_1\), we start with the loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - \left( b_{1} x_{i} + b_0 \right) \right)^2
    = \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0 \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we take the partial derivative with respect to the model parameter, \(b_1\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial b_1} \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0
    \right)^2 = \sum_{i=1}^n -2 \cdot x_i \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: to optimize, minimize the loss, we set the partical derivative with respect
    to the model parameter, \(b_1\) equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n -2 \cdot x_i \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can simplify (divide both sides by -2) and distribute multiply to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - b_0 \cdot x_i -b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can substitute in \(b_0 = \overline{y} - b_1 \cdot \overline{x}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \left(\overline{y} - b_1 \cdot \overline{x}
    \right) \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: once again we multiply to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + b_1 \cdot \overline{x}
    \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: we separate the sums,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + \sum_{i=1}^n b_1
    \cdot \overline{x} \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and we can pull out the \(b_1\) constant,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + b_1 \sum_{i=1}^n
    \overline{x} \cdot x_i - x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and reorder a little to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i - b_1 \sum_{i=1}^n
    x_i^2 - \overline{x} \cdot x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: move to the other side,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 \sum_{i=1}^n x_i^2 - \overline{x} \cdot x_i = \sum_{i=1}^n x_i \cdot
    y_i - \overline{y} \cdot x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: and divide both sizes by \(\sum_{i=1}^n x_i^2 - \overline{x} \cdot x_i\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 = \frac{\sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i}{\sum_{i=1}^n
    x_i^2 - \overline{x} \cdot x_i} \]
  prefs: []
  type: TYPE_NORMAL
- en: we now have an analytical solution for the \(b_1\) slope term for linear regression.
    It can be shown that this is equivalent to another form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 = \frac{\sum_{i=1}^n \left( x_i - \overline{x} \right) \cdot \left( y_i
    - \overline{y} \right)}{\sum_{i=1}^n \left( x_i - \overline{x} \right)^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: I prefer this form because it is easily interpreted as the covariance of \(X\)
    and \(Y\), \(C_{x,y}\) divided by the variance of \(X\), \(sigma_{x}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: Now for the calculation of the intercept term, \(b_0\), we return to the loss
    function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - \left( b_{1} x_{i} + b_0 \right) \right)^2
    = \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0 \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and this time we take the partial derivative with respect to \(b_0\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial b_0} \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0
    \right)^2 = \sum_{i=1}^n -2 \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: to optimize, minimize the loss, we set the partical derivative with respect
    to the model parameter, \(b_0\) equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n -2 \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: we divide both sides by -2,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - b_0 -b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: and break up the sum,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - \sum_{i=1}^n b_0 - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: simplify the simple term as it is a constant, \(\sum_{i=1}^n b_0 = n \cdot b_0\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - n \cdot b_0 - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: now move the \(b_0\) term to the left-hand side,
  prefs: []
  type: TYPE_NORMAL
- en: \[ n \cdot b_0 = \sum_{i=1}^n y_i - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: pull out the constant \(b_1\) from the sum,
  prefs: []
  type: TYPE_NORMAL
- en: \[ n \cdot b_0 = \sum_{i=1}^n y_i - b_1 \sum_{i=1}^n x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: divide both sides by \(n\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_0 = \frac{\sum_{i=1}^n y_i}{n} - b_1 \frac{\sum_{i=1}^n x_i}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is very interesting, we now see 2 arithmetic means!
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\sum_{i=1}^n y_i}{n} = \overline{y} \quad \quad \frac{\sum_{i=1}^n
    x_i}{n} = \overline{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: so our solution is simply,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_0 = \overline{y} - b_1 \cdot \overline{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For linear regression models we have access to a powerful metric to check our
    model, the coefficient of determination, also known as ‚Äúr-squared‚Äù, \(r^2\).
  prefs: []
  type: TYPE_NORMAL
- en: the proportion of variance explained by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculated as the explained variance, \(SS_{reg}\) divided by the total variance,
    \(SS_{tot}\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ SS_{reg} = \sum_{i=1}^n \left(\hat{y}_i - \overline{y} \right)^2 \quad \quad
    SS_{tot} = \sum_{i=1}^n \left(y_i - \overline{y} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{y}_i\) is the model prediction at the \(i\) training data, and
    $\overline{y} is the average of the sample data.
  prefs: []
  type: TYPE_NORMAL
- en: the \(r^2\) can be calculated from the correlation coefficient, so you can know
    the goodness of a linear regression model, before you train it!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ r^2 = \left(\rho_{x,y} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: note, \(r^2\) can only be used for linear models where,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^2_{tot} = \sigma^2_{reg} + \sigma^2_{res} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma^2_{tot}\) is total variance of the response feature, \(\sigma^2_{reg}\)
    is the variance of the model predictions, and \(\sigma^2_{res}\) is the variance
    of the error, i.e., the residuals, \(\Delta y_i = y_i - \hat{y}_i\).
  prefs: []
  type: TYPE_NORMAL
- en: How to interpret \(r^2\)? It would be dangerous to hard thresholds, but I can
    give some soft guidance,
  prefs: []
  type: TYPE_NORMAL
- en: \(r^2 \ge 0.98\) - the model is cheating or the problem is very easy, linear,
    noise-free and well sampled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(0.0 \le r^2 \le 0.6\) - the model is not working well, check the data and
    model choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(r^2 \lt 0.0\) - the model is going the wrong way! You are better off with
    estimating by the global mean, \(\overline{y}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To communicate model uncertainty we rely on confidence intervals for the model
    parameters, \(b_1\) and \(b_0\). Let‚Äôs define confidence interval here for convenience,
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Interval** - the uncertainty in a summary statistic / model /
    model parameter represented as a range, lower and upper bound, based on a specified
    probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We communicate confidence intervals like this:'
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that model slope is between
    0.5 and 0.7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover analytical methods here, but we could also use the more flexible Bootstrap
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôs enough here, let‚Äôs load data and explain more as we demonstrate linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a function to streamline the addition specified percentiles and
    major and minor gridlines to our plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time). Also, in this case make
    sure to place the required (see below) data file in this working directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This
    dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: density (\(g/cm^{3}\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (volume %)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, the dataset is synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.906634 | 12.845691 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.668073 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.795190 | 11.015021 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.705466 | 17.185360 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.821963 | 8.190405 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.708322 | 10.728462 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.897087 | 11.245838 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1.864561 | 11.357547 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.119652 | 8.614564 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1.301057 | 15.280571 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1.774021 | 9.489298 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1.410996 | 14.371990 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1.697005 | 10.495092 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.996736 | 20.964941 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 1.783736 | 13.393518 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 1.743519 | 14.758068 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 1.348847 | 15.877907 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 2.331653 | 4.968240 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 1.438900 | 16.529857 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 1.766823 | 10.485052 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1.802992 | 10.120258 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 1.750352 | 11.325941 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 1.885087 | 9.242607 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 2.044451 | 8.936061 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 1.778580 | 11.426343 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.552689 | 14.157303 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2.022877 | 10.672887 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 1.530699 | 16.476751 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 1.753578 | 10.826057 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 1.791432 | 14.506748 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1.830085 | 10.561222 |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | 1.479878 | 14.443138 |'
  prefs: []
  type: TYPE_TB
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the DataFrame is useful first check of the data.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: add parameter ‚Äòn=13‚Äô to see the first 13 rows of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.906634 | 12.845691 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.668073 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.795190 | 11.015021 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.705466 | 17.185360 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.821963 | 8.190405 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.708322 | 10.728462 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.897087 | 11.245838 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1.864561 | 11.357547 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.119652 | 8.614564 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1.301057 | 15.280571 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1.774021 | 9.489298 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1.410996 | 14.371990 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1.697005 | 10.495092 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table.
  prefs: []
  type: TYPE_NORMAL
- en: We use transpose just to flip the table so that features are on the rows and
    the statistics are on the columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Density | 32.0 | 1.719994 | 0.262314 | 0.996736 | 1.547192 | 1.770422 | 1.838704
    | 2.331653 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 32.0 | 12.317525 | 3.224611 | 4.968240 | 10.492582 | 11.341744
    | 14.459041 | 20.964941 |'
  prefs: []
  type: TYPE_TB
- en: Data Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should also take a look at the histograms.
  prefs: []
  type: TYPE_NORMAL
- en: get a sense of the range, modes, skew, outliers etc. for each feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/98de0f9e6b4fc55ef535855f27875a0c49893f41a34ad00f9df6b7568c083cdf.png](../Images/a2eda54f1271413aca71673e74f3a116.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first train a linear regression model to all our data with SciPy package,
    stats module.
  prefs: []
  type: TYPE_NORMAL
- en: we will develop more complicated cross validation training and tuning methods
    latter with training and testing data splits latter. For now all the data is used
    to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recall we imported the module as ‚Äòst‚Äô above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We instantiate, train the linear regression model and get model diagnostics
    for confidence intervals and hypothesis testing all in one line of code!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have 5 outputs when we instantiate and fit our model.
  prefs: []
  type: TYPE_NORMAL
- en: '**slope** - the slope of our linear model, the \(b_1\) in the model, \(y =
    b_1 x + b_0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intercept** - the intercept of our linear model, the \(b_0\) in the model,
    \(y = b_1 x + b_0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r_value** - the Pearson correlation, the square is the \(r^2\), the variance
    explained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_value** - the p-value for the hypothesis test for the slope of the model
    of zero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stderr** - the standard error of the slope parameter, \(SE_{b_1}\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs plot the data and the model, to get out estimates we substitute our predictor
    feature values into our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d3fdafa548372646c151694b18b0c75982244b0b0a186248b71ef376c8584bc8.png](../Images/54afba2518c03ba22364ced82320385f.png)'
  prefs: []
  type: TYPE_IMG
- en: The model looks reasonable. Let‚Äôs go beyond occular inspection.
  prefs: []
  type: TYPE_NORMAL
- en: Model Checking with \(r^2\) Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs first explain \(r^2\), proportion of variance explained. Here‚Äôs the variance
    explained by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùë†ùë†ùëüùëíùëî = \sum_{ùëñ=1}^{ùëõ}\left(\hat{y}_i - \overline{y}\right)^2
    \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: and the variance not explained by the model,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùë†ùë†ùëüùëísid = \sum_{ùëñ=1}^{ùëõ}\left(y_i - \hat{y}\right)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Now we can calculate the variance explained as,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùëü^2 = \frac{ùë†ùë†_{ùëüùëíùëî}}{ùë†ùë†_{ùëüùëíùëî}+ùë†ùë†_{ùëüùëíùë†ùëñùëë}} = \frac{\text{variance
    explained}}{\text{total variance}} \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: which is a common and intuitive metric for the goodness of a linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Checking with Hypothesis Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs test the slope, \(b_1\), with the following hypothesis test,
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{equation} H_0: b_{1} = 0.0 \end{equation}'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{equation} H_1: b_{1} \ne 0.0 \end{equation}'
  prefs: []
  type: TYPE_NORMAL
- en: and see if we can reject this hypothesis, \(H_{0}\) , that the slope parameter
    is equal to 0.0\. If we reject this null hypothesis, we show that the slope is
    meaningful, there is a linear relationship between density and porosity that we
    can use.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the \(linregress\) function from the \(stats\) package provides
    us with the two sided p-value for this test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Since the p-value is less than any reasonable \(\alpha\) value, we reject the
    null hypothesis and adopt the alternative hypothesis, \(H_1\), that the slope
    is not equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: We can also perform the entire hypothesis test by calculating the,
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{statistic} = \frac{b_1}{SE_{b_1}} \]
  prefs: []
  type: TYPE_NORMAL
- en: First we need the \(t_{critical}\) value, given \(\alpha\) and \(df = n-2\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We see a consistent result with the previous hypothesis test with the p-value,
    since the \(t_{statistic}\) is outside the \(t_{critical}\) lower and upper interval,
    we reject the null hypothesis, \(h_0\), that the slope, \(b_1\) is equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: We can also observe correlation coefficient, \(r\) value, and the \(r^2\) value
    that indicates the proportion of variance that is described for our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Confidence Intervals for Model Uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs calculate the 95% confidence interval for the slope parameter, \(b_1\)
    of our model. We just need our \(t_{critical}\) and the standard error in the
    slope, \(SE_{b_1}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs visualize the model uncertainty through confidence intervals in the slope.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0bbbe3bf373fd68d3772e3c17b50ebbc962bfa52dfe9696bbbcf88f2260be693.png](../Images/d1ca1ce80b88510090621b7b2f30e375.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Prediction Intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs calculate the prediction intervals.
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} \hat{y}*{n+1} ¬± t*{(\frac{\alpha}{2},n-2)} \sqrt{MSE}\ \times
    \sqrt{1+\frac{1}{n}+\frac{(x_{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Note, this is the standard error of the prediction,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} SE_{\hat{y}*{n+1}} = \sqrt{MSE}\ \times \sqrt{1+\frac{1}{n}+\frac{(x*{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where MSE, model mean square error calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} MSE = \sum_{i=1}^n\frac{(y_i - \hat{y}*i)^2}{n-2} = \sum*{i=1}^n
    \frac{\left(y_i - (b_1 x - b_0) \right)^2}{n-2} \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Note, that this indicates that prediction intervals are wider the further we
    estimate from the mean of the predictor feature values. We can substitute model
    MSE, MSE, and standard error of the estimate, \(SE_{\hat{y}_{n+1}}\) for the final
    form is,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} \hat{y}*{n+1} ¬± t*{(\frac{\alpha}{2},n-2)} \sqrt{\sum_{i=1}^n
    \frac{\left(y_i - (b_1 x - b_0) \right)^2}{n-2}}\sqrt{1+\frac{1}{n}+\frac{(x_{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs demonstrate a prediction interval.
  prefs: []
  type: TYPE_NORMAL
- en: select a X value, new_X below as the input, and the alpha level for the prediction
    interval, i.e., alpha = 0.05 results in the P025 and P975 results to define the
    interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5a6dd7728637e0e475487e0a80b773a4446a401a945fb86809c1c1febb8c575d.png](../Images/19bd5fc3ca00501a930e17fc3ebb1e40.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let‚Äôs use this model to make a prediction at all the data locations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/545490d4664e3ba1508005fe883dd6cea523f44e8aaadcd36f2307b474dab097.png](../Images/cfe742ca2d1f207ffbe6602e9f0b93b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Checking Prediction Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is useful to plot the predictions of porosity with the training porosity
    vs. density scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: From this plot we can observe the linear limitation of our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and get a sense of the unexplained variance \(\frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
    {n-1}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c0b53c83c8aa3c23ee1636d164da4a46e45d2410df17b6670670dd5d04980935.png](../Images/bf66aa77a321545828b548c07857ba83.png)'
  prefs: []
  type: TYPE_IMG
- en: See the plotted error residuals,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta y_i = y_i - \hat{y}_i \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) are the true response values and \(\hat{y}_i\) are the estimated
    response values.
  prefs: []
  type: TYPE_NORMAL
- en: It is good to check the error residual distribution that,
  prefs: []
  type: TYPE_NORMAL
- en: the average is close to 0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the shape is not skewed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are no outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs look at the error residual distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/1bc994acb07d204d76581a13525e09733c2f17f2105bbd67f0005fbc0001ac0e.png](../Images/3b7dc57b47ae0c681e96fc44088f5a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Next we will check the truth vs. estimated scatter plot, and cross validation
    residual plot, residual vs. the fitted value.
  prefs: []
  type: TYPE_NORMAL
- en: with these plots we check if the errors are consistent over the range of fitted
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, we could use this plot to identify higher error or systematic under-
    or overestimation over a specific range of fitted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8ad1779c8dd4b364ab69f60144c920c3837df8fd24c6ab9fa4bd6953f0c4bc9a.png](../Images/eb662574297fc05b3c1cced939dbb806.png)'
  prefs: []
  type: TYPE_IMG
- en: For the demonstration case, there is no apparent conditional bias in the estimates
    over the range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Practice on a New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ok, time to get to work. Let‚Äôs load up a dataset and build a linear regression
    model with,
  prefs: []
  type: TYPE_NORMAL
- en: compact code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: basic visaulizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can select any of these datasets or modify the code and add your own to
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 year cumulative oil production (Mbbl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR | Prod |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1695.360819 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3007.096063 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2531.938259 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5288.514854 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 2859.469624 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4017.374438 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 2952.812773 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 2670.933846 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 2474.048178 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2722.893266 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3828.247174 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 5095.810104 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4091.637316 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 195 | 11.95 | 3.13 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 196 | 17.99 | 9.87 | 3.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 197 | 12.12 | 2.27 | 3.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 198 | 15.55 | 4.48 | 2.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 199 | 20.89 | 7.54 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: 200 rows √ó 3 columns
  prefs: []
  type: TYPE_NORMAL
- en: Build and Check Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3a7d23260a411f4894d7f9270056b8ff28820241bf7e6859bae42ed471f22fa3.png](../Images/dcda56f7f818f02921fd7cfe954f1f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 19.55 | 8.41 | 3.08 | 35.49 | 1.34 | 1.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 11.34 | 2.72 | 3.43 | 58.03 | 0.57 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 179 | 7.22 | 1.42 | 3.60 | 63.09 | -0.03 | 1.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 109 | 15.19 | 5.05 | 3.11 | 57.97 | 1.15 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 134 | 12.83 | 2.69 | 3.67 | 17.20 | 0.61 | 2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 12.55 | 3.22 | 3.43 | 56.93 | 0.79 | 2.27 |'
  prefs: []
  type: TYPE_TB
- en: '| 182 | 9.88 | 2.72 | 3.64 | 55.19 | 0.52 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 71 | 14.17 | 3.94 | 2.92 | 59.30 | 0.91 | 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| 112 | 18.24 | 4.31 | 2.85 | 44.53 | 1.39 | 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 163 | 11.60 | 1.73 | 2.18 | 53.54 | 0.60 | 1.50 |'
  prefs: []
  type: TYPE_TB
- en: 160 rows √ó 6 columns
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of linear regression. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow, demonstration of linear regression for machine learning-based
    predictions. Why start with linear regression?
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is the simplest parametric predictive machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learn about training machine learning models with an analytical solution
    calculated from the derivative of training MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get‚Äôs us started with the concepts of loss functions and norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have access to analytics expressions for confidence intervals for model uncertainty,
    and hypothesis tests for parameter significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs some basic details about linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/806bf5f702f9bb5a63e30d6e1f7969d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada2fcc2740c48478e79404563c91061.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/835541b16e1038a4606f7d97b628c4f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytical Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the loss is \(L^2\) we have access to a non-iterative, analytics solution,
    i.e., optimization of the linear regression model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: we are looking for the model parameter(s) that minimize the loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when we have an analytical solution, we can use the 1st and 2nd derivatives
    of the loss function,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: A local or global loss minimum occurs where,
  prefs: []
  type: TYPE_NORMAL
- en: first derivative of the loss function is 0.0 - zero slope indicating a local
    minimum or maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial L(y_{\alpha}, F(X_{\alpha}))}{\partial b_1} = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: second derivative of the loss function is greater than 0.0 - positive curvature
    indicated a local minimum instead of maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \frac{\partial^2 L(y_{\alpha}, F(X_{\alpha}))}{\partial b_1^2} > 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs use this approach to derived the solution for linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the least squares (L2) norm the linear regression model parameters can be
    trained with training data with an analytical solution. Let‚Äôs derive it for linear
    regression analytical solution for the case of 1 predictor feature.
  prefs: []
  type: TYPE_NORMAL
- en: To calculated the slope coefficient, \(b_1\), we start with the loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - \left( b_{1} x_{i} + b_0 \right) \right)^2
    = \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0 \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we take the partial derivative with respect to the model parameter, \(b_1\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial b_1} \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0
    \right)^2 = \sum_{i=1}^n -2 \cdot x_i \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: to optimize, minimize the loss, we set the partical derivative with respect
    to the model parameter, \(b_1\) equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n -2 \cdot x_i \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can simplify (divide both sides by -2) and distribute multiply to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - b_0 \cdot x_i -b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can substitute in \(b_0 = \overline{y} - b_1 \cdot \overline{x}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \left(\overline{y} - b_1 \cdot \overline{x}
    \right) \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: once again we multiply to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + b_1 \cdot \overline{x}
    \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: we separate the sums,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + \sum_{i=1}^n b_1
    \cdot \overline{x} \cdot x_i - b_1 x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and we can pull out the \(b_1\) constant,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i + b_1 \sum_{i=1}^n
    \overline{x} \cdot x_i - x_i^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and reorder a little to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i - b_1 \sum_{i=1}^n
    x_i^2 - \overline{x} \cdot x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: move to the other side,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 \sum_{i=1}^n x_i^2 - \overline{x} \cdot x_i = \sum_{i=1}^n x_i \cdot
    y_i - \overline{y} \cdot x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: and divide both sizes by \(\sum_{i=1}^n x_i^2 - \overline{x} \cdot x_i\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 = \frac{\sum_{i=1}^n x_i \cdot y_i - \overline{y} \cdot x_i}{\sum_{i=1}^n
    x_i^2 - \overline{x} \cdot x_i} \]
  prefs: []
  type: TYPE_NORMAL
- en: we now have an analytical solution for the \(b_1\) slope term for linear regression.
    It can be shown that this is equivalent to another form,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_1 = \frac{\sum_{i=1}^n \left( x_i - \overline{x} \right) \cdot \left( y_i
    - \overline{y} \right)}{\sum_{i=1}^n \left( x_i - \overline{x} \right)^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: I prefer this form because it is easily interpreted as the covariance of \(X\)
    and \(Y\), \(C_{x,y}\) divided by the variance of \(X\), \(sigma_{x}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: Now for the calculation of the intercept term, \(b_0\), we return to the loss
    function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - \left( b_{1} x_{i} + b_0 \right) \right)^2
    = \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0 \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and this time we take the partial derivative with respect to \(b_0\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial b_0} \sum_{i=1}^n \left(y_i - b_{1} x_{i} - b_0
    \right)^2 = \sum_{i=1}^n -2 \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: to optimize, minimize the loss, we set the partical derivative with respect
    to the model parameter, \(b_0\) equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n -2 \left(y_i - b_0 -b_1 x_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: we divide both sides by -2,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - b_0 -b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: and break up the sum,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - \sum_{i=1}^n b_0 - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: simplify the simple term as it is a constant, \(\sum_{i=1}^n b_0 = n \cdot b_0\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \sum_{i=1}^n y_i - n \cdot b_0 - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: now move the \(b_0\) term to the left-hand side,
  prefs: []
  type: TYPE_NORMAL
- en: \[ n \cdot b_0 = \sum_{i=1}^n y_i - \sum_{i=1}^n b_1 x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: pull out the constant \(b_1\) from the sum,
  prefs: []
  type: TYPE_NORMAL
- en: \[ n \cdot b_0 = \sum_{i=1}^n y_i - b_1 \sum_{i=1}^n x_i \]
  prefs: []
  type: TYPE_NORMAL
- en: divide both sides by \(n\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_0 = \frac{\sum_{i=1}^n y_i}{n} - b_1 \frac{\sum_{i=1}^n x_i}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is very interesting, we now see 2 arithmetic means!
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\sum_{i=1}^n y_i}{n} = \overline{y} \quad \quad \frac{\sum_{i=1}^n
    x_i}{n} = \overline{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: so our solution is simply,
  prefs: []
  type: TYPE_NORMAL
- en: \[ b_0 = \overline{y} - b_1 \cdot \overline{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For linear regression models we have access to a powerful metric to check our
    model, the coefficient of determination, also known as ‚Äúr-squared‚Äù, \(r^2\).
  prefs: []
  type: TYPE_NORMAL
- en: the proportion of variance explained by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculated as the explained variance, \(SS_{reg}\) divided by the total variance,
    \(SS_{tot}\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ SS_{reg} = \sum_{i=1}^n \left(\hat{y}_i - \overline{y} \right)^2 \quad \quad
    SS_{tot} = \sum_{i=1}^n \left(y_i - \overline{y} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{y}_i\) is the model prediction at the \(i\) training data, and
    $\overline{y} is the average of the sample data.
  prefs: []
  type: TYPE_NORMAL
- en: the \(r^2\) can be calculated from the correlation coefficient, so you can know
    the goodness of a linear regression model, before you train it!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ r^2 = \left(\rho_{x,y} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: note, \(r^2\) can only be used for linear models where,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^2_{tot} = \sigma^2_{reg} + \sigma^2_{res} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma^2_{tot}\) is total variance of the response feature, \(\sigma^2_{reg}\)
    is the variance of the model predictions, and \(\sigma^2_{res}\) is the variance
    of the error, i.e., the residuals, \(\Delta y_i = y_i - \hat{y}_i\).
  prefs: []
  type: TYPE_NORMAL
- en: How to interpret \(r^2\)? It would be dangerous to hard thresholds, but I can
    give some soft guidance,
  prefs: []
  type: TYPE_NORMAL
- en: \(r^2 \ge 0.98\) - the model is cheating or the problem is very easy, linear,
    noise-free and well sampled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(0.0 \le r^2 \le 0.6\) - the model is not working well, check the data and
    model choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(r^2 \lt 0.0\) - the model is going the wrong way! You are better off with
    estimating by the global mean, \(\overline{y}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To communicate model uncertainty we rely on confidence intervals for the model
    parameters, \(b_1\) and \(b_0\). Let‚Äôs define confidence interval here for convenience,
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Interval** - the uncertainty in a summary statistic / model /
    model parameter represented as a range, lower and upper bound, based on a specified
    probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We communicate confidence intervals like this:'
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that model slope is between
    0.5 and 0.7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover analytical methods here, but we could also use the more flexible Bootstrap
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôs enough here, let‚Äôs load data and explain more as we demonstrate linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a function to streamline the addition specified percentiles and
    major and minor gridlines to our plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time). Also, in this case make
    sure to place the required (see below) data file in this working directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This
    dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: density (\(g/cm^{3}\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (volume %)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, the dataset is synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.906634 | 12.845691 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.668073 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.795190 | 11.015021 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.705466 | 17.185360 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.821963 | 8.190405 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.708322 | 10.728462 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.897087 | 11.245838 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1.864561 | 11.357547 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.119652 | 8.614564 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1.301057 | 15.280571 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1.774021 | 9.489298 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1.410996 | 14.371990 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1.697005 | 10.495092 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.996736 | 20.964941 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 1.783736 | 13.393518 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 1.743519 | 14.758068 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 1.348847 | 15.877907 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 2.331653 | 4.968240 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 1.438900 | 16.529857 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 1.766823 | 10.485052 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1.802992 | 10.120258 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 1.750352 | 11.325941 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 1.885087 | 9.242607 |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 2.044451 | 8.936061 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 1.778580 | 11.426343 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1.552689 | 14.157303 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2.022877 | 10.672887 |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | 1.530699 | 16.476751 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 1.753578 | 10.826057 |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | 1.791432 | 14.506748 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1.830085 | 10.561222 |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | 1.479878 | 14.443138 |'
  prefs: []
  type: TYPE_TB
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the DataFrame is useful first check of the data.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: add parameter ‚Äòn=13‚Äô to see the first 13 rows of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.906634 | 12.845691 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.668073 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.795190 | 11.015021 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.705466 | 17.185360 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.821963 | 8.190405 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.708322 | 10.728462 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.897087 | 11.245838 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1.864561 | 11.357547 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.119652 | 8.614564 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1.301057 | 15.280571 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1.774021 | 9.489298 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1.410996 | 14.371990 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1.697005 | 10.495092 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table.
  prefs: []
  type: TYPE_NORMAL
- en: We use transpose just to flip the table so that features are on the rows and
    the statistics are on the columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Density | 32.0 | 1.719994 | 0.262314 | 0.996736 | 1.547192 | 1.770422 | 1.838704
    | 2.331653 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 32.0 | 12.317525 | 3.224611 | 4.968240 | 10.492582 | 11.341744
    | 14.459041 | 20.964941 |'
  prefs: []
  type: TYPE_TB
- en: Data Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should also take a look at the histograms.
  prefs: []
  type: TYPE_NORMAL
- en: get a sense of the range, modes, skew, outliers etc. for each feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/98de0f9e6b4fc55ef535855f27875a0c49893f41a34ad00f9df6b7568c083cdf.png](../Images/a2eda54f1271413aca71673e74f3a116.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first train a linear regression model to all our data with SciPy package,
    stats module.
  prefs: []
  type: TYPE_NORMAL
- en: we will develop more complicated cross validation training and tuning methods
    latter with training and testing data splits latter. For now all the data is used
    to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recall we imported the module as ‚Äòst‚Äô above
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We instantiate, train the linear regression model and get model diagnostics
    for confidence intervals and hypothesis testing all in one line of code!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have 5 outputs when we instantiate and fit our model.
  prefs: []
  type: TYPE_NORMAL
- en: '**slope** - the slope of our linear model, the \(b_1\) in the model, \(y =
    b_1 x + b_0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intercept** - the intercept of our linear model, the \(b_0\) in the model,
    \(y = b_1 x + b_0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r_value** - the Pearson correlation, the square is the \(r^2\), the variance
    explained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p_value** - the p-value for the hypothesis test for the slope of the model
    of zero'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stderr** - the standard error of the slope parameter, \(SE_{b_1}\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs plot the data and the model, to get out estimates we substitute our predictor
    feature values into our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d3fdafa548372646c151694b18b0c75982244b0b0a186248b71ef376c8584bc8.png](../Images/54afba2518c03ba22364ced82320385f.png)'
  prefs: []
  type: TYPE_IMG
- en: The model looks reasonable. Let‚Äôs go beyond occular inspection.
  prefs: []
  type: TYPE_NORMAL
- en: Model Checking with \(r^2\) Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs first explain \(r^2\), proportion of variance explained. Here‚Äôs the variance
    explained by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùë†ùë†ùëüùëíùëî = \sum_{ùëñ=1}^{ùëõ}\left(\hat{y}_i - \overline{y}\right)^2
    \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: and the variance not explained by the model,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùë†ùë†ùëüùëísid = \sum_{ùëñ=1}^{ùëõ}\left(y_i - \hat{y}\right)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Now we can calculate the variance explained as,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} ùëü^2 = \frac{ùë†ùë†_{ùëüùëíùëî}}{ùë†ùë†_{ùëüùëíùëî}+ùë†ùë†_{ùëüùëíùë†ùëñùëë}} = \frac{\text{variance
    explained}}{\text{total variance}} \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: which is a common and intuitive metric for the goodness of a linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Checking with Hypothesis Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs test the slope, \(b_1\), with the following hypothesis test,
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{equation} H_0: b_{1} = 0.0 \end{equation}'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{equation} H_1: b_{1} \ne 0.0 \end{equation}'
  prefs: []
  type: TYPE_NORMAL
- en: and see if we can reject this hypothesis, \(H_{0}\) , that the slope parameter
    is equal to 0.0\. If we reject this null hypothesis, we show that the slope is
    meaningful, there is a linear relationship between density and porosity that we
    can use.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the \(linregress\) function from the \(stats\) package provides
    us with the two sided p-value for this test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Since the p-value is less than any reasonable \(\alpha\) value, we reject the
    null hypothesis and adopt the alternative hypothesis, \(H_1\), that the slope
    is not equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: We can also perform the entire hypothesis test by calculating the,
  prefs: []
  type: TYPE_NORMAL
- en: \[ t_{statistic} = \frac{b_1}{SE_{b_1}} \]
  prefs: []
  type: TYPE_NORMAL
- en: First we need the \(t_{critical}\) value, given \(\alpha\) and \(df = n-2\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We see a consistent result with the previous hypothesis test with the p-value,
    since the \(t_{statistic}\) is outside the \(t_{critical}\) lower and upper interval,
    we reject the null hypothesis, \(h_0\), that the slope, \(b_1\) is equal to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: We can also observe correlation coefficient, \(r\) value, and the \(r^2\) value
    that indicates the proportion of variance that is described for our model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Confidence Intervals for Model Uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs calculate the 95% confidence interval for the slope parameter, \(b_1\)
    of our model. We just need our \(t_{critical}\) and the standard error in the
    slope, \(SE_{b_1}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs visualize the model uncertainty through confidence intervals in the slope.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0bbbe3bf373fd68d3772e3c17b50ebbc962bfa52dfe9696bbbcf88f2260be693.png](../Images/d1ca1ce80b88510090621b7b2f30e375.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Prediction Intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs calculate the prediction intervals.
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} \hat{y}*{n+1} ¬± t*{(\frac{\alpha}{2},n-2)} \sqrt{MSE}\ \times
    \sqrt{1+\frac{1}{n}+\frac{(x_{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Note, this is the standard error of the prediction,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} SE_{\hat{y}*{n+1}} = \sqrt{MSE}\ \times \sqrt{1+\frac{1}{n}+\frac{(x*{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where MSE, model mean square error calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} MSE = \sum_{i=1}^n\frac{(y_i - \hat{y}*i)^2}{n-2} = \sum*{i=1}^n
    \frac{\left(y_i - (b_1 x - b_0) \right)^2}{n-2} \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Note, that this indicates that prediction intervals are wider the further we
    estimate from the mean of the predictor feature values. We can substitute model
    MSE, MSE, and standard error of the estimate, \(SE_{\hat{y}_{n+1}}\) for the final
    form is,
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation} \hat{y}*{n+1} ¬± t*{(\frac{\alpha}{2},n-2)} \sqrt{\sum_{i=1}^n
    \frac{\left(y_i - (b_1 x - b_0) \right)^2}{n-2}}\sqrt{1+\frac{1}{n}+\frac{(x_{n+1}-\overline{x})^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}
    } \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs demonstrate a prediction interval.
  prefs: []
  type: TYPE_NORMAL
- en: select a X value, new_X below as the input, and the alpha level for the prediction
    interval, i.e., alpha = 0.05 results in the P025 and P975 results to define the
    interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5a6dd7728637e0e475487e0a80b773a4446a401a945fb86809c1c1febb8c575d.png](../Images/19bd5fc3ca00501a930e17fc3ebb1e40.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let‚Äôs use this model to make a prediction at all the data locations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/545490d4664e3ba1508005fe883dd6cea523f44e8aaadcd36f2307b474dab097.png](../Images/cfe742ca2d1f207ffbe6602e9f0b93b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Checking Prediction Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is useful to plot the predictions of porosity with the training porosity
    vs. density scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: From this plot we can observe the linear limitation of our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and get a sense of the unexplained variance \(\frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
    {n-1}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c0b53c83c8aa3c23ee1636d164da4a46e45d2410df17b6670670dd5d04980935.png](../Images/bf66aa77a321545828b548c07857ba83.png)'
  prefs: []
  type: TYPE_IMG
- en: See the plotted error residuals,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta y_i = y_i - \hat{y}_i \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) are the true response values and \(\hat{y}_i\) are the estimated
    response values.
  prefs: []
  type: TYPE_NORMAL
- en: It is good to check the error residual distribution that,
  prefs: []
  type: TYPE_NORMAL
- en: the average is close to 0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the shape is not skewed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are no outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs look at the error residual distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/1bc994acb07d204d76581a13525e09733c2f17f2105bbd67f0005fbc0001ac0e.png](../Images/3b7dc57b47ae0c681e96fc44088f5a7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Next we will check the truth vs. estimated scatter plot, and cross validation
    residual plot, residual vs. the fitted value.
  prefs: []
  type: TYPE_NORMAL
- en: with these plots we check if the errors are consistent over the range of fitted
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, we could use this plot to identify higher error or systematic under-
    or overestimation over a specific range of fitted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8ad1779c8dd4b364ab69f60144c920c3837df8fd24c6ab9fa4bd6953f0c4bc9a.png](../Images/eb662574297fc05b3c1cced939dbb806.png)'
  prefs: []
  type: TYPE_IMG
- en: For the demonstration case, there is no apparent conditional bias in the estimates
    over the range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Practice on a New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ok, time to get to work. Let‚Äôs load up a dataset and build a linear regression
    model with,
  prefs: []
  type: TYPE_NORMAL
- en: compact code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: basic visaulizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can select any of these datasets or modify the code and add your own to
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 year cumulative oil production (Mbbl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR | Prod |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1695.360819 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3007.096063 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2531.938259 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5288.514854 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 2859.469624 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4017.374438 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 2952.812773 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 2670.933846 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 2474.048178 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2722.893266 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3828.247174 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 5095.810104 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4091.637316 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 195 | 11.95 | 3.13 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 196 | 17.99 | 9.87 | 3.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 197 | 12.12 | 2.27 | 3.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 198 | 15.55 | 4.48 | 2.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 199 | 20.89 | 7.54 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: 200 rows √ó 3 columns
  prefs: []
  type: TYPE_NORMAL
- en: Build and Check Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3a7d23260a411f4894d7f9270056b8ff28820241bf7e6859bae42ed471f22fa3.png](../Images/dcda56f7f818f02921fd7cfe954f1f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 19.55 | 8.41 | 3.08 | 35.49 | 1.34 | 1.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 11.34 | 2.72 | 3.43 | 58.03 | 0.57 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 179 | 7.22 | 1.42 | 3.60 | 63.09 | -0.03 | 1.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 109 | 15.19 | 5.05 | 3.11 | 57.97 | 1.15 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 134 | 12.83 | 2.69 | 3.67 | 17.20 | 0.61 | 2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 12.55 | 3.22 | 3.43 | 56.93 | 0.79 | 2.27 |'
  prefs: []
  type: TYPE_TB
- en: '| 182 | 9.88 | 2.72 | 3.64 | 55.19 | 0.52 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 71 | 14.17 | 3.94 | 2.92 | 59.30 | 0.91 | 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| 112 | 18.24 | 4.31 | 2.85 | 44.53 | 1.39 | 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 163 | 11.60 | 1.73 | 2.18 | 53.54 | 0.60 | 1.50 |'
  prefs: []
  type: TYPE_TB
- en: 160 rows √ó 6 columns
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 year cumulative oil production (Mbbl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR | Prod |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 | 81.40 | 1.16 | 2.31 | 1695.360819 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 | 46.17 | 0.89 | 1.88 | 3007.096063 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 | 72.80 | 0.89 | 2.72 | 2531.938259 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 | 39.81 | 1.08 | 1.88 | 5288.514854 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 | 10.94 | 1.51 | 1.90 | 2859.469624 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 14.53 | 4.81 | 2.69 | 53.60 | 0.94 | 1.67 | 4017.374438 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 13.49 | 3.60 | 2.93 | 63.71 | 0.80 | 1.85 | 2952.812773 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 11.58 | 3.03 | 3.25 | 53.00 | 0.69 | 1.93 | 2670.933846 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 12.52 | 2.72 | 2.43 | 65.77 | 0.95 | 1.98 | 2474.048178 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 13.25 | 3.94 | 3.71 | 66.20 | 1.14 | 2.65 | 2722.893266 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 15.04 | 4.39 | 2.22 | 61.11 | 1.08 | 1.77 | 3828.247174 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 16.19 | 6.30 | 2.29 | 49.10 | 1.53 | 1.86 | 5095.810104 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 16.82 | 5.42 | 2.80 | 66.65 | 1.17 | 1.98 | 4091.637316 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12.08 | 2.92 | 2.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.38 | 3.53 | 3.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.02 | 2.59 | 4.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.67 | 6.75 | 2.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 17.52 | 4.57 | 3.18 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 195 | 11.95 | 3.13 | 2.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 196 | 17.99 | 9.87 | 3.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 197 | 12.12 | 2.27 | 3.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 198 | 15.55 | 4.48 | 2.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 199 | 20.89 | 7.54 | 3.23 |'
  prefs: []
  type: TYPE_TB
- en: 200 rows √ó 3 columns
  prefs: []
  type: TYPE_NORMAL
- en: Build and Check Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3a7d23260a411f4894d7f9270056b8ff28820241bf7e6859bae42ed471f22fa3.png](../Images/dcda56f7f818f02921fd7cfe954f1f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 19.55 | 8.41 | 3.08 | 35.49 | 1.34 | 1.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 11.34 | 2.72 | 3.43 | 58.03 | 0.57 | 2.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 179 | 7.22 | 1.42 | 3.60 | 63.09 | -0.03 | 1.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 109 | 15.19 | 5.05 | 3.11 | 57.97 | 1.15 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 134 | 12.83 | 2.69 | 3.67 | 17.20 | 0.61 | 2.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 12.55 | 3.22 | 3.43 | 56.93 | 0.79 | 2.27 |'
  prefs: []
  type: TYPE_TB
- en: '| 182 | 9.88 | 2.72 | 3.64 | 55.19 | 0.52 | 2.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 71 | 14.17 | 3.94 | 2.92 | 59.30 | 0.91 | 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| 112 | 18.24 | 4.31 | 2.85 | 44.53 | 1.39 | 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 163 | 11.60 | 1.73 | 2.18 | 53.54 | 0.60 | 1.50 |'
  prefs: []
  type: TYPE_TB
- en: 160 rows √ó 6 columns
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of linear regression. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
