- en: 2 How Machine Learning Works
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 如何机器学习工作
- en: 原文：[https://mlbook.explained.ai/intro.html](https://mlbook.explained.ai/intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mlbook.explained.ai/intro.html](https://mlbook.explained.ai/intro.html)
- en: '[Terence Parr](http://parrt.cs.usfca.edu) and [Jeremy Howard](http://www.fast.ai/about/#jeremy)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[Terence Parr](http://parrt.cs.usfca.edu) 和 [Jeremy Howard](http://www.fast.ai/about/#jeremy)'
- en: Copyright © 2018-2019 Terence Parr. All rights reserved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 版权所有 © 2018-2019 Terence Parr。保留所有权利。
- en: '*Please don''t replicate on web or redistribute in any way.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*请勿在网络上复制或以任何方式分发。*'
- en: This book generated from markup+markdown+python+latex source with [Bookish](https://github.com/parrt/bookish).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是由 markup+markdown+python+latex 源代码生成的，使用 [Bookish](https://github.com/parrt/bookish)。
- en: You can make **comments or annotate** this page by going to the annotated version
    of this page. You'll see existing annotated bits highlighted in yellow. They are
    *PUBLICLY VISIBLE*. Or, you can send comments, suggestions, or fixes directly
    to [Terence](mailto:parrt@cs.usfca.edu).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问此页面的注释版本来对此页进行 **评论或注释**。您会看到现有的注释部分以黄色突出显示。它们是 *公开可见的*。或者，您可以直接向 [Terence](mailto:parrt@cs.usfca.edu)
    发送评论、建议或修正。
- en: Contents
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目录
- en: '[Learning to Learn about Apartment Data](#sec:2.1)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习关于公寓数据的知识](#sec:2.1)'
- en: '[Learning by rote](#sec:2.1.1)'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[死记硬背学习](#sec:2.1.1)'
- en: '[Getting to know the neighbors](#sec:kNN)'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解邻居](#sec:kNN)'
- en: '[Drawing the line](#mathy)'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[划清界限](#mathy)'
- en: '[Walking through the trees](#sec:2.1.4)'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在树林中漫步](#sec:2.1.4)'
- en: '[Random Forest Regressors](#sec:2.2)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林回归器](#sec:2.2)'
- en: '[Random Forest Classifiers](#sec:2.3)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林分类器](#sec:2.3)'
- en: '[The Big Picture](#sec:2.4)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[整体图景](#sec:2.4)'
- en: “*Without data you're just another person with an opinion*” — W. Edwards Deming
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “*没有数据，你只是另一个有意见的人*” — W. Edwards Deming
- en: In densely-populated cities, such as San Francisco and New York, everyone's
    favorite topic is the cost of housing. There's nothing quite as invigorating as
    writing a check for US$4200/month to rent a single bedroom apartment (as renters
    do down the street from Terence's place in San Francisco's Mission District).
    People are always comparing rent prices because they want to know if they're overpaying
    or getting a good deal. The idea is to collect information on similar apartments
    and then compare prices. Real estate agents tend to have more data and are, hopefully,
    able to provide more accurate rent estimates.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在人口密集的城市，如旧金山和纽约，人们最喜欢的主题是住房成本。没有什么比支付每月4200美元的租金来租一个单间公寓（就像在旧金山使命区的Terence住所附近租房的人一样）更让人振奋了。人们总是比较租金价格，因为他们想知道自己是否支付过高或得到了一个好交易。想法是收集类似公寓的信息，然后比较价格。房地产经纪人通常拥有更多数据，并有望提供更准确的租金估计。
- en: 1A nice alliteration by Navid Amini https://scholar.google.com/citations?user=tZTnipEAAAAJ
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1Navid Amini 的一次很好的押韵 https://scholar.google.com/citations?user=tZTnipEAAAAJ
- en: The problem, of course, is that large amounts of data quickly overwhelm the
    human mind and so we turn to computers for help. Unfortunately, basic statistics
    aren't sufficient to handle interesting problems like apartment rent prediction.
    Instead, we need *machine learning* to discover relationships and patterns in
    data, which is the subject of this book. Simply put, machine learning turns experience
    into expertise,1 generalizing from training data to make accurate predictions
    or classifications in new situations (for previously unseen data).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，问题是大量数据很快就会让人脑不堪重负，所以我们转向计算机寻求帮助。不幸的是，基本的统计学不足以处理像公寓租金预测这样的有趣问题。相反，我们需要 *机器学习*
    来发现数据中的关系和模式，这正是本书的主题。简单来说，机器学习将经验转化为专业知识，从训练数据中推广到新情况（对于以前未见过的数据）进行准确的预测或分类。
- en: The goal of this chapter is to get an idea of how machine learning works. To
    do that, let's try to invent a technique for predicting apartment rent prices
    in New York City. It'll highlight the difficulty of the problem and help us understand
    the machine learning approach. Without understanding the underlying algorithms,
    we can't successfully apply machine learning. We must be able to choose the right
    algorithm for a particular problem and be able to properly prepare data for that
    algorithm. By starting simply and going down a few dead ends, we'll also motivate
    the construction of more sophisticated techniques. Along the way, we'll define
    a number of important terms and concepts commonly used by machine learning practitioners
    and give a general overview of the machine learning process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是了解机器学习是如何工作的。为了做到这一点，让我们尝试发明一种预测纽约市公寓租金价格的技术。这将突出问题的难度，并帮助我们理解机器学习的方法。如果不理解底层算法，我们就无法成功应用机器学习。我们必须能够为特定问题选择正确的算法，并能够为该算法正确准备数据。通过从简单开始并走一些弯路，我们也将激发更复杂技术的构建。在这个过程中，我们将定义许多机器学习从业者常用的术语和概念，并对机器学习过程提供一个概述。
- en: 2.1 Learning to Learn about Apartment Data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 学习关于公寓数据
- en: As with any problem-solving exercise, it's a good idea to start by clearly defining
    the problem. Given four attributes of an apartment, the number of bedrooms, the
    number of bathrooms, and location (longitude, latitude) we want to *predict* (determine)
    the price. Those apartment attributes are called *features* and the price is called
    the *target*. We're usually given data in the form of a table like the following.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何问题解决练习一样，首先清楚地定义问题是明智的。给定公寓的四个属性，卧室数量、浴室数量和位置（经度、纬度），我们想要*预测*（确定）价格。这些公寓属性被称为*特征*，价格被称为*目标*。我们通常以表格的形式提供数据，如下所示。
- en: '![](../Images/dee21c75e7a9f4fed8d3e604f6fe7bb8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/dee21c75e7a9f4fed8d3e604f6fe7bb8.png)'
- en: This data is called *training data* or the *training set* because we, or a program,
    must learn from this “experience” in order to make predictions. (It's often convenient
    to treat all of the features for a single record as a single *feature vector*;
    a *vector* is a list of numbers.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这组数据被称为*训练数据*或*训练集*，因为我们或一个程序必须从这种“经验”中学习，以便做出预测。（通常将单个记录的所有特征视为单个*特征向量*是很方便的；*向量*是一系列数字。）
- en: The central problem of machine learning is to build a system that is accurate
    without being overly-specific to this training data (assuming the data has an
    underlying relationship to capture).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心问题是要构建一个系统，该系统能够准确预测，但又不至于过于特定于训练数据（假设数据具有一个基础关系来捕捉）。
- en: It's easy to build a system that makes accurate predictions for items in the
    training set. All we have to do is memorize the apartments and their prices (in
    this context) then look up the price for an apartment in the training data when
    asked to do so. At the other extreme, we could compute the average rent across
    all apartments and predict that price for any apartment look up, inside or outside
    of the training data. The overall average rent would not be super accurate for
    a specific apartment, but it would give a ballpark figure, easily distinguishing
    rent from, say, a hot pastrami sandwich at Katz's delicatessen on E. Houston Street
    ($21.45).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个系统，使其能够对训练集中的物品做出准确预测是很简单的。我们只需要记住公寓及其价格（在这个上下文中），然后在需要时查找训练数据中的公寓价格。在另一个极端，我们可以计算所有公寓的平均租金，并为任何公寓预测这个价格，无论是在训练数据内部还是外部。对于特定公寓来说，整体平均租金可能不会非常准确，但它会给出一个大致的数字，很容易区分租金和，比如说，在东休斯顿街的Katz熟食店的一个热培根三明治（21.45美元）。
- en: Memorization doesn't generalize beyond the training data but is precise. Blurring
    all apartments together obviously yields a prediction for any apartment we present
    but is not precise at all. Somewhere in between lies effective machine learning.
    Let's start with just memorizing the training data and work our way towards a
    system that properly generalizes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆化不能推广到训练数据之外，但非常精确。将所有公寓混合在一起显然可以预测我们提出的任何公寓的价格，但并不精确。在两者之间存在着有效的机器学习。让我们从仅仅记忆训练数据开始，逐步构建一个能够正确推广的系统。
- en: 2.1.1 Learning by rote
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 死记硬背学习
- en: 'Given the training data, we can reasonably predict a price of $5,465 for an
    apartment with two bedrooms and one bathroom at location coordinates 40.7947,-73.9667
    because that comes straight from the second row of the table. To get perfect accuracy,
    we can interpret the learning process conceptually as just filling up a dictionary
    that maps a four-element key to a single value (price), something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练数据，我们可以合理地预测一个位于坐标40.7947,-73.9667的带有两个卧室和一个浴室的公寓的价格为$5,465，因为这直接来自表格的第二行。为了获得完美的准确性，我们可以从概念上理解学习过程就是填充一个将四个元素键映射到单个值（价格）的字典，就像这样：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the vocabulary of machine learning, we are “*training* a *model*,” where
    the model here is a dictionary data structure. Training in this case simply means
    to remember all apartment data with perfect recall.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的术语中，我们是在“*训练*一个*模型*”，这里的模型是一个字典数据结构。在这个情况下，训练简单意味着记住所有公寓数据，并实现完美的回忆。
- en: 'But this training process assumes that all apartment records are unique, which
    is not a valid assumption. For example, here are four studio apartments with the
    same (bedrooms, bathrooms, latitude, longitude) feature vector but different (eye-popping!)
    prices:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个训练过程假设所有公寓记录都是唯一的，这并不是一个有效的假设。例如，这里有四个具有相同（卧室，卫生间，纬度，经度）特征向量但价格不同的（令人眼花缭乱的！）公寓：
- en: '|   | bedrooms | bathrooms | latitude | longitude | price |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | 卧室 | 卫生间 | 纬度 | 经度 | 价格 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| 1470 | 0 | 1.0000 | 40.7073 | -73.9664 | 2650 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 1470 | 0 | 1.0000 | 40.7073 | -73.9664 | 2650 |'
- en: '| 36509 | 0 | 1.0000 | 40.7073 | -73.9664 | 2850 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 36509 | 0 | 1.0000 | 40.7073 | -73.9664 | 2850 |'
- en: '| 39241 | 0 | 1.0000 | 40.7073 | -73.9664 | 2950 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 39241 | 0 | 1.0000 | 40.7073 | -73.9664 | 2950 |'
- en: '| 46405 | 0 | 1.0000 | 40.7073 | -73.9664 | 2850 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 46405 | 0 | 1.0000 | 40.7073 | -73.9664 | 2850 |'
- en: Having multiple prices for the same feature vector represents an uncertainty.
    Which of the four prices should the model return? There's likely a good reason
    for the difference in prices, such as view or square footage, but we don't have
    that data. Or, as we'll see in **Chapter 5** *Exploring and Denoising Your Data
    Set*, data is sometimes noisy or just plain wrong. Either way, we need to deal
    with this uncertainty because repeated keys cause our rudimentary training process
    to overwrite previous prices.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同一个特征向量存在多个价格表示存在不确定性。模型应该返回四个价格中的哪一个？价格差异可能有一个很好的原因，比如视野或面积，但我们没有这些数据。或者，正如我们在**第5章**
    *探索和去噪您的数据集*中将要看到的，数据有时是嘈杂的或者完全是错误的。无论如何，我们需要处理这种不确定性，因为重复的键会导致我们的基本训练过程覆盖先前的价格。
- en: Because our goal is to generalize, giving a good estimate for apartments not
    in our training data, we should aim for the expected rent value considering all
    apartments in the city with the same attributes. Another term for expected value
    is “average” so let's just record the average, which is what a human expert would
    do implicitly. In this case, we'd record an average price ($2,825) and yield that
    value when asked to predict the price of an apartment with those features. The
    more sample prices in our training data we have for a particular set of apartment
    attributes, the better the estimate of the true average price we'd get. This works
    well and is actually a kind of lossy compression because we have merged records,
    at the cost of less specific predictions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的目标是进行泛化，为训练数据中不存在的公寓提供一个良好的估计，我们应该考虑整个城市具有相同属性的所有公寓的预期租金值。预期值的另一个术语是“平均”，所以我们只记录平均值，这是人类专家会隐含做的事情。在这种情况下，我们会记录一个平均价格（$2,825）并在被要求预测具有这些特征的公寓价格时提供该值。我们训练数据中对于特定一组公寓属性拥有的样本价格越多，我们得到的真实平均价格的估计就越好。这效果很好，实际上是一种有损压缩，因为我们合并了记录，代价是预测的特异性降低。
- en: Aggregating records for identical apartment feature vectors and recording their
    average rent dips a toe into the as-yet murky waters of machine learning. We are
    creating an aggregate price for a prototypical apartment of a particular type,
    in a sense learning what the price should be or is expected to be for that type
    of apartment. This gives us a hint that machine learning is a just a sensible
    combination of data structures, algorithms, and statistics. As we continue, hopefully
    you'll see that machine learning is not some mysterious and arcane mechanism that
    takes forever to learn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对具有相同公寓特征向量的记录进行汇总并记录其平均租金，这涉足到机器学习尚且模糊的水域。我们在某种程度上学习了一个特定类型的典型公寓的价格，或者说是预期价格。这给我们一个暗示，机器学习只是数据结构、算法和统计学的合理组合。随着我们继续前进，希望你能看到机器学习并不是某种神秘且复杂的机制，需要花费很长时间才能学会。
- en: 2.1.2 Getting to know the neighbors
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 认识邻居
- en: The problem with the rudimentary dictionary model is that it's super rigid in
    that it can't deal with uncertainty in the apartment feature vectors themselves.
    (Previously-unseen apartment feature combinations raise a “key error” during look
    up.) How should we predict the price of an apartment whose features don't exactly
    match an entry in the training data?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基本字典模型的问题是它非常僵化，无法处理公寓特征向量本身的不确定性。（在查找时，以前未见过公寓特征组合会引发“键错误”）。我们应该如何预测那些特征与训练数据中的条目不完全匹配的公寓的价格？
- en: An interesting solution is to keep the original training data as-is and then
    scan for the apartment record whose features most closely match the features of
    the apartment for which we'd like a price. As before, there could be multiple
    prices for that closest matching record and so we'd want to average those prices
    to yield the prediction. Believe it or not, such a simple model is very powerful
    (for appropriate data sets) and is a variation on what's called a *k nearest neighbor*
    predictor (See [kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的解决方案是保留原始的训练数据不变，然后扫描寻找与我们要估算价格的公寓特征最接近的公寓记录。与之前一样，可能存在多个与最接近匹配记录的价格，因此我们想要对这些价格进行平均以得到预测值。信不信由你，这样一个简单的模型（对于适当的数据集来说）非常强大，并且是所谓的**k最近邻**预测器的一种变体（参见[kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)）。
- en: The only problem with the nearest neighbor model is performance. We have to
    keep the entire training data set around as the model and we have to linearly
    scan all of the records looking for the closest match for an apartment feature
    vector. In contrast to the dictionary model, there is no training process, but
    apartment lookup (prediction) is very slow.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻模型唯一的问题是性能。我们必须保留整个训练数据集作为模型，并且必须线性扫描所有记录以寻找与公寓特征向量最接近的匹配项。与字典模型相比，没有训练过程，但公寓查找（预测）非常慢。
- en: 'Another way to handle uncertainty in the apartment features, is to merge records
    as we did before. We can group apartment records by a combination of bedrooms
    and bathrooms and compute the average price for each group. This approach works
    in this case because there are only so many combinations of numbers of bedrooms
    and bathrooms; we can cover them all. For example, here are the first few average
    prices per bedrooms/bathrooms combination:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理公寓特征不确定性的方法是，像之前那样合并记录。我们可以通过卧室和浴室的组合来分组公寓记录，并计算每个组合的平均价格。这种方法在本例中有效，因为卧室和浴室的组合数量有限；我们可以覆盖所有这些组合。例如，以下是前几个卧室/浴室组合的平均价格：
- en: '|   | bedrooms | bathrooms | price |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|   | 卧室 | 浴室 | 价格 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| 0 | 0 | 0.0000 | 2872.9272 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0.0000 | 2872.9272 |'
- en: '| 1 | 0 | 1.0000 | 2442.5502 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1.0000 | 2442.5502 |'
- en: '| 2 | 0 | 1.5000 | 3391.1111 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 1.5000 | 3391.1111 |'
- en: '| 3 | 0 | 2.0000 | 5354.0714 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 2.0000 | 5354.0714 |'
- en: '| 4 | 0 | 4.0000 | 7995.0000 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 4.0000 | 7995.0000 |'
- en: '| 5 | 1 | 0.0000 | 2944.1918 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 0.0000 | 2944.1918 |'
- en: '| 6 | 1 | 1.0000 | 3015.5427 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 1.0000 | 3015.5427 |'
- en: (You gotta like New York City and its quirky apartments; apparently you can
    rent places with no bedroom but 4 bathrooms for an average of $7995/month!)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: （你必须喜欢纽约市及其古怪的公寓；显然，你可以租到没有卧室但有4个浴室的地方，平均每月租金为7995美元！）
- en: At the cost of specificity, merging dramatically reduces the size of the data
    set, from 48,266 records down to 51 records for this data set. A dictionary or
    linear scan could quickly find the bedrooms/bathrooms combination to make a prediction.
    Of course, this approach completely ignores location, which we know to be important.
    The group averages are hiding a lot of variability between apartments. We could
    make a secondary index that grouped apartments by latitude/longitude to get a
    second estimate based solely on the location. But it's unclear how we would merge
    the two rent estimates into a single prediction. Such an *ad hoc* approach sometimes
    works but requires a lot of thought and is highly dependent upon the data set.
    We need a more systematic approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以牺牲特定性为代价，合并大大减少了数据集的大小，从48,266条记录减少到这个数据集的51条记录。字典或线性扫描可以快速找到卧室/浴室组合以进行预测。当然，这种方法完全忽略了位置，我们知道位置很重要。组平均值隐藏了公寓之间的许多变异性。我们可以创建一个二级索引，按纬度/经度对公寓进行分组，以仅基于位置获得第二个估计。但
    unclear如何将两个租金估计合并为一个预测。这种*临时*方法有时有效，但需要大量思考，并且高度依赖于数据集。我们需要一个更系统的方法。
- en: 2.1.3 Drawing the line
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 绘制线条
- en: 'We could try a “mathy” approach where we weight each feature by how important
    it is then use a weighted sum to estimate rent prices:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试一种“数学”方法，即根据每个特征的重要性对其进行加权，然后使用加权总和来估计租金价格：
- en: '![](../Images/cf5f8ca7f767f0625a13d359e46ca12b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf5f8ca7f767f0625a13d359e46ca12b.png)'
- en: This equation boils all of our rent training data down to just five numbers
    that comprise our model (four weights, w[i], and a scalar for minimum rent). That's
    an amazing compression! Better yet, making a prediction is superfast because it's
    just four multiplies and four additions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程将所有我们的租金训练数据简化为仅由五个数字组成，这构成了我们的模型（四个权重，w[i]，以及一个最小租金的标量）。这是一个惊人的压缩！更好的是，做出预测非常快，因为它只需要四个乘法和四个加法。
- en: This approach often works well and is called a [linear model](https://en.wikipedia.org/wiki/Linear_regression)
    or *linear regression* because it tries to draw a line (or plane when given more
    than two dimensions) through the training data. (Recall the formula for a line
    from high school algebra, ![](../Images/0da1bff7e3f0ca018ba244103db34a3e.png).)
    The technique has been around for over 200 years and mathematicians have an elegant
    formula to conjure up suitable w[i] weights. *Regression* is the general term
    for predicting numerical values based upon training data and the associated model
    is called a *regressor*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通常效果很好，被称为[线性模型](https://en.wikipedia.org/wiki/Linear_regression)或*线性回归*，因为它试图在训练数据中绘制一条线（或当给定超过两个维度时为平面）。（回想一下高中代数中线的公式，![](../Images/0da1bff7e3f0ca018ba244103db34a3e.png)。）这项技术已经存在了200多年，数学家有一个优雅的公式来生成合适的w[i]权重。*回归*是预测基于训练数据和相关模型的通用术语，而该模型被称为*回归器*。
- en: '[![](../Images/a6a2d108e03a62c364bd3f83f859a623.png)](images/intro/intro_stats_4.svg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/a6a2d108e03a62c364bd3f83f859a623.png)](images/intro/intro_stats_4.svg)'
- en: '**Figure 2.1**. Price per number of bathrooms and linear regression fit line'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.1**. 每间浴室的价格和线性回归拟合线'
- en: For this data set, unfortunately, a linear model is not a good choice because
    such models treat every feature as a single trend with lower rent on one side
    and higher rent on the other, or vice versa. For example, it's reasonable to assume
    that rent prices would go up as the number of bathrooms goes up, but the data
    doesn't support that conclusion. **Figure 2.1** shows the average rent for all
    apartments with the same number of bathrooms with dots where we actually have
    data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，不幸的是，线性模型不是一个好的选择，因为这样的模型将每个特征视为一个单一趋势，一边租金较低，另一边租金较高，或者反之亦然。例如，假设租金价格会随着浴室数量的增加而上升是合理的，但数据并不支持这个结论。**图2.1**显示了具有相同浴室数量的所有公寓的平均租金，用点表示我们实际拥有的数据。
- en: The “best fit,” red line minimizes the difference between the line and the actual
    average price but is clearly a terrible predictor of price. In this case, there
    is something weird going on beyond 4 bathrooms. (10 bathrooms and only $3500/month?
    One can only imagine what those places look like.) Consequently, a single line
    is a poor fit and does not capture jagged relationships like this very well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳拟合”，红色线条最小化了线条与实际平均价格之间的差异，但显然是一个糟糕的价格预测器。在这种情况下，在4个浴室之后似乎有某种奇怪的事情发生。（10个浴室而每月只有3500美元？人们只能想象这些地方的样子。）因此，单一线条是一个较差的拟合，并且不能很好地捕捉这种锯齿状的关系。
- en: 2.1.4 Walking through the trees
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 遍历树
- en: A more sophisticated approach would treat different ranges of a feature's values
    separately, giving a different rent estimate per range. Each feature range would
    have a different prototypical apartment. But, we have to be careful not to create
    a model that is too specific to the training data because it won't generalize
    well. We don't want to go lurching back to the other extreme, towards a dictionary
    model that memorizes exact apartment feature vector to price relationships.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更复杂的方法是分别处理特征值的不同范围，为每个范围提供一个不同的租金估计。每个特征范围将有一个不同的典型公寓。但是，我们必须小心不要创建一个过于特定于训练数据的模型，因为它不会很好地泛化。我们不希望回到另一个极端，即回到一个记忆精确公寓特征向量到价格关系的字典模型。
- en: 'We want a model that gracefully throttles up, splitting a feature''s values
    into as many ranges as necessary to get decent accuracy but without creating so
    many tight ranges it kills generality. To see how such a model might work, let''s
    consider the rent prices for one-bath, one- and two-bedroom apartments in a very
    small rectangular region of New York:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一个模型能够优雅地调整速度，将一个特征值分成尽可能多的范围，以获得良好的准确性，但又不会创建太多紧密的范围而损害其通用性。为了了解这样的模型可能如何工作，让我们考虑纽约一个非常小的矩形区域中一室、一室和两室公寓的租金价格：
- en: '|   | bedrooms | bathrooms | latitude | longitude | price |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|   | 房间数 | 卫生间 | 纬度 | 经度 | 价格 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| 701 | 1 | 1.0000 | 40.6661 | -73.9882 | 2200 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 701 | 1 | 1.0000 | 40.6661 | -73.9882 | 2200 |'
- en: '| 41397 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 41397 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
- en: '| 41476 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 41476 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
- en: '| 42254 | 1 | 1.0000 | 40.6661 | -73.9882 | 2200 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 42254 | 1 | 1.0000 | 40.6661 | -73.9882 | 2200 |'
- en: '| 45510 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 45510 | 1 | 1.0000 | 40.6661 | -73.9882 | 2100 |'
- en: '| 408 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 408 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
- en: '| 16876 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 16876 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
- en: '| 40852 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 40852 | 2 | 1.0000 | 40.6661 | -73.9882 | 2500 |'
- en: '| 165 | 1 | 1.0000 | 40.6663 | -73.9402 | 1800 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 165 | 1 | 1.0000 | 40.6663 | -73.9402 | 1800 |'
- en: '| 9113 | 1 | 1.0000 | 40.6663 | -73.9402 | 1800 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 9113 | 1 | 1.0000 | 40.6663 | -73.9402 | 1800 |'
- en: '| 27085 | 2 | 1.0000 | 40.6663 | -73.9402 | 2350 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 27085 | 2 | 1.0000 | 40.6663 | -73.9402 | 2350 |'
- en: 'An easy but tedious way to capture the relationship between the feature values
    and the associated price would be to define some rules in Python:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单但繁琐的方法来捕捉特征值与相关价格之间的关系，就是在Python中定义一些规则：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With enough coffee, we should be able to come up with the rules to carve up
    the *feature space* (4-dimensional space of all possible bedrooms, bathrooms,
    latitude, longitude combinations) into clusters. Ideally, each cluster would contain
    apartments with similar attributes and similar rent, as is the case for this subsample.
    To make a rent prediction, we'd execute the rules until we get a match for the
    apartment features of interest.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有足够的咖啡，我们应该能够想出规则来划分*特征空间*（所有可能的房间、卫生间、纬度、经度组合的4维空间）成聚类。理想情况下，每个聚类都包含具有相似属性和相似租金的公寓，就像这个子样本的情况一样。为了进行租金预测，我们会执行这些规则，直到找到与感兴趣的公寓特征相匹配的规则。
- en: Unlike the dictionary model, these rules can handle previously unseen data.
    For example, imagine a one-bedroom, one-bathroom apartment at location 40.6612,-73.9800
    that does not exist in the training data. The first rule applies and so the model
    would predict rent of $2,143\. This model generalizes (at least somewhat) because
    it deals in ranges of feature values not exact feature values.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与字典模型不同，这些规则可以处理以前未见过的数据。例如，想象一个位于40.6612,-73.9800的一室一卫公寓，它不在训练数据中。第一条规则适用，因此模型会预测租金为$2,143。这个模型泛化（至少在一定程度上）因为它处理的是特征值的范围，而不是精确的特征值。
- en: The size and number of feature value ranges used by the model represent a kind
    of an accuracy “knob.” Turning the knob one way increases generality but makes
    the model potentially less accurate. In the opposite direction, we can make the
    ranges tighter and the model more accurate, but we potentially lose generality.
    A model that is overly-specific to the training data and not general enough is
    said to *overfit* the training data. The opposite, naturally, is an *underfit*
    model that doesn't capture the relationships in the training data well, which
    also means that it won't generalize well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用的特征值范围的大小和数量代表了一种准确性的“旋钮”。旋转旋钮一个方向可以增加泛化性，但会使模型可能更不准确。相反方向，我们可以使范围更紧，模型更准确，但可能失去泛化性。一个过度特定于训练数据且泛化性不足的模型被称为*过度拟合*训练数据。自然地，相反的是*欠拟合*模型，它没有很好地捕捉训练数据中的关系，这也意味着它不会很好地泛化。
- en: 'Okay, now we have a model that is potentially accurate and general but prediction
    through sequential execution of numerous `if`-statements would be pretty slow.
    The trick to making prediction efficient is to factor and nest the rules so they
    share comparisons to avoid repeated testing:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们有一个可能准确且通用的模型，但是通过执行多个`if`语句的顺序预测会相当慢。使预测变得高效的技巧是将规则进行因式分解和嵌套，以便它们共享比较以避免重复测试：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/0ac8fbdfaa83780d93a4e7555da541d2.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/0ac8fbdfaa83780d93a4e7555da541d2.png)'
- en: '**Figure 2.2**. Sample partial regressor decision tree'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.2**. 样本部分回归决策树'
- en: Another way to encode those nested rules is with a tree data structure, where
    each node performs a comparison. **Figure 2.2** is a visual representation of
    what such a tree might look like. Predicting rent using such a tree costs just
    four comparisons as we descend from root to the appropriate leaf, testing features
    as we go. The leaves of the tree contain the prices for all apartments fitting
    the criteria on the path from the root down to that leaf. Trees like this are
    called *decision trees* and, if we allow the same feature to be tested multiple
    times, decision trees can carve up feature spaces into arbitrarily tight clusters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用树数据结构编码嵌套规则的方法是，其中每个节点执行比较。**图2.2**是这样一个树可能看起来什么样的视觉表示。使用这样的树预测租金只需要进行四次比较，因为我们从根节点下降到适当的叶子节点，在下降过程中测试特征。树的叶子包含所有符合从根节点到该叶子节点路径上标准的价格。这种类型的树被称为*决策树*，如果我们允许同一特征被多次测试，决策树可以将特征空间分割成任意紧密的簇。
- en: The problem with decision trees is that they tend to get too specific; they
    overfit training data. For example, we could build a decision tree that carved
    up the feature space so that each leaf corresponded to a single apartment. That'd
    provide precise answers but, as with our rudimentary dictionary model, such a
    specific tree would not generalize.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的问题在于它们往往会变得过于具体；它们过度拟合训练数据。例如，我们可以构建一个决策树，将特征空间分割成每个叶子节点对应一个单独的公寓。这将提供精确的答案，但就像我们的原始字典模型一样，这样一个特定的树无法泛化。
- en: 2.2 Random Forest Regressors
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 随机森林回归器
- en: 'To prevent overfitting, we can weaken the decision tree by reducing its accuracy
    in a very specific way: By training the tree on a random selection of the training
    data instead of all members of the data set. (Technically, we are [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)),
    which randomly selects records but with replacement, meaning that a record can
    appear multiple times in the bootstrapped sample.) To reduce overfitting even
    further, we can sometimes forget that certain features exist, such as the number
    of bedrooms. Because not all elements from the original data set are present,
    we have a coarser view of the training data so the comparison ranges in our decision
    tree nodes will necessarily be broader.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过度拟合，我们可以通过以下非常具体的方式降低决策树的准确性：通过在训练数据的一个随机子集上训练树，而不是数据集的所有成员。（技术上，我们正在进行[自助法](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))，它随机选择记录，但可以进行替换，这意味着记录可以在自助样本中出现多次。）为了进一步减少过度拟合，我们有时可以忘记某些特征的存在，例如卧室数量。因为原始数据集中的不是所有元素都存在，所以我们有更粗糙的训练数据视图，因此决策树节点中的比较范围必然更广。
- en: To compensate for this weak learner, we can create lots of them and take the
    average of their individual predictions to make an overall rent prediction. We
    call this [ensemble learning](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)
    and it's an excellent general technique to increase accuracy without such a strong
    tendency to overfit. Introducing more randomness gives us a *Random Forest*™,
    which we'll abbreviate as *RF*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补偿这种弱学习器，我们可以创建很多这样的模型，并取它们各自预测的平均值来做出整体的租金预测。我们称之为[集成学习](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)，这是一种非常优秀的通用技术，可以提高准确性，而不会过度拟合。引入更多的随机性给我们带来了*随机森林*™，我们将它简称为*RF*。
- en: An RF behaves very much like a group of real estate agents looking for comparable
    apartments and cooperating to estimate an apartment's price (“crowdsourcing”).
    In the training process the agents would independently select and visit available
    apartments in New York City. The selection of apartments should be random to reduce
    the possibility that an agent only visits, say, one-bedroom apartments. Such an
    agent would be “overfit” in the sense that they could only give reasonable answers
    for one bedrooms. Randomly selecting apartments reduces the possibility of such
    sampling bias. Each agent would train on a different sample of apartments but
    with some overlap. Adding more agents improves our prediction accuracy and model
    generality, without increasing the probability of overfitting.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（RF）的行为非常类似于一群房地产经纪人寻找可比的公寓并合作估算公寓价格（“众包”）。在训练过程中，经纪人会独立选择并参观纽约市可用的公寓。公寓的选择应该是随机的，以减少经纪人只参观，比如说，一室公寓的可能性。这样的经纪人会“过度拟合”，因为他们只能对一室公寓给出合理的答案。随机选择公寓可以减少这种采样偏差。每个经纪人都会在不同的公寓样本上训练，但会有一些重叠。增加更多的经纪人可以提高我们的预测准确性和模型泛化能力，而不会增加过度拟合的概率。
- en: To predict the price of an apartment with a particular set of features, each
    agent would find comparable (similar or identical) apartments from their training
    set and report the average of those apartments. The overall prediction would then
    be the average of all agents' averages.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要预测具有特定特征的公寓价格，每个经纪人都会从他们的训练集中找到可比的（相似或相同的）公寓，并报告这些公寓的平均值。然后，整体预测将是所有经纪人平均值的平均值。
- en: We'll learn the details of random forests in **Chapter 17** *Forests of Randomized
    Decision Trees* and how to implement them in **Chapter 18** *Implementing Random
    Forests*. RFs are the Swiss Army Knife™ of the machine learning world and we recommend
    them as your model of choice for the majority of machine learning problems encountered
    in practice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第17章“随机决策树森林”中学习随机森林的细节，以及如何在第18章“实现随机森林”中实现它们。随机森林是机器学习世界的瑞士军刀™，我们建议将其作为你在实践中遇到的多数机器学习问题的首选模型。
- en: 'With a small tweak, we can use random forests for a related and equally-useful
    task: predicting discrete categories like cancer/not-cancer instead of continuous
    values such as prices.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个小调整，我们可以使用随机森林来完成一个相关且同样有用的任务：预测离散类别，如癌症/非癌症，而不是连续值，如价格。
- en: 2.3 Random Forest Classifiers
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 随机森林分类器
- en: We've all been to the doctor to present a set of symptoms and ask for a diagnosis.
    We'd like to know whether that rash is poison ivy, an abrasion, a virus, or skin
    cancer. In response, the doctor provides a diagnosis consisting of the name of
    the disease or condition. Or, we might simply want to know whether our symptoms
    are something to worry about, in which case the doctor provides a binary yes/no
    answer. A machine learning model providing such a diagnosis is called a *classifier*
    because it predicts a class or category (disease name) rather than predicting
    a numeric value like rent price.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都去过医生那里，展示一系列症状并请求诊断。我们想知道那个皮疹是毒葛、擦伤、病毒还是皮肤癌。作为回应，医生会提供由疾病或条件名称组成的诊断。或者，我们可能只是想知道我们的症状是否值得担心，在这种情况下，医生会提供一个二进制的是/否答案。提供这种诊断的机器学习模型被称为*分类器*，因为它预测一个类别或类别（疾病名称），而不是预测像租金价格这样的数值。
- en: 'Doctors implicitly compute disease likelihoods from their experience in order
    to make a diagnosis. Roughly speaking, a doctor analyzes the situation by thinking:
    “I''ve seen similar symptoms 10 times; 7 of those people had disease A, 2 had
    disease B, and 1 had disease C.” The diagnosis is then disease A if we apply a
    decision rule that picks the disease with the most “votes” (highest likelihood).
    That sounds just like the nearest neighbor (kNN) approach we used in **Section
    2.1.2** *Getting to know the neighbors* to predict rent prices.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 医生通过他们的经验隐式地计算疾病可能性，以便做出诊断。大致来说，医生通过思考：“我见过类似的症状10次；其中7人有疾病A，2人有疾病B，1人有疾病C。”然后，如果应用一个选择“投票”最多（最高可能性）的疾病的决策规则，诊断就是疾病A。这听起来就像我们在**第2.1.2节**“了解邻居”中使用的最近邻（kNN）方法来预测租金价格。
- en: A kNN classifier finds k similar patient case histories, per the feature vectors
    of symptoms, and takes a majority diagnosis “vote” among those k. The only difference
    between a kNN regressor and classifier is that the regressor yields the average
    of the target values for the k similar records and a kNN classifier yields the
    target category that is in the majority among the k similar records.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: kNN分类器根据症状的特征向量找到k个相似的病例历史，并在这些k个病例中采取多数诊断“投票”。kNN回归器和分类器之间的唯一区别在于，回归器为k个相似记录的目标值提供平均值，而kNN分类器为k个相似记录中占多数的目标类别提供目标类别。
- en: In the abstract, decision trees are carving up the feature space into groups
    of similar observations just like kNN models do. Each leaf represents one of these
    groups. The difference between regressors and classifiers is that leaves in regressor
    trees predict numeric values while leaves in classifier trees predict discrete
    categories. A regressor leaf predicts the average of the target values from the
    observations grouped in that leaf and a classifier leaf predicts the most common
    target category. An RF classifier is just an ensemble of classifier decision trees
    that yields the most likely category predicted among the trees. In that sense,
    it's kind of a meta-voting scheme because an RF classifier counts votes from the
    trees in the forest and the trees themselves count votes among observations in
    the leaves.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在摘要中，决策树将特征空间划分为与kNN模型类似的一组相似观测值。每个叶子节点代表这些组中的一个。回归树和分类树之间的区别在于，回归树的叶子节点预测数值，而分类树的叶子节点预测离散类别。回归树的叶子节点预测该叶子节点中分组观测值的平均目标值，而分类树的叶子节点预测最常见的目标类别。RF分类器只是一个分类决策树的集合，它从这些树中预测最可能的类别。从这个意义上说，它有点像一种元投票方案，因为RF分类器统计森林中树的意见，而树本身在叶子节点中统计观测值之间的意见。
- en: '![](../Images/07a2942d272a5b6105de9a587c6ec061.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07a2942d272a5b6105de9a587c6ec061.png)'
- en: '**Figure 2.3**. Sample partial classifier decision tree'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.3**. 样本部分分类决策树'
- en: Continuing with our apartment price example, consider the problem of predicting
    interest in the web page advertising an apartment instead of predicting the rent
    price. We can imagine that apartments with lots of bedrooms and a decent price
    would generate more interest than apartments with no bathrooms but very low price
    (although, it is New York City, so you never know). An RF can capture the relationship
    between apartment features and low, medium, high interest categories just as it
    can capture the relationship to prices. **Figure 2.3** depicts a partial decision
    tree whose leaves predict medium and high interest, depending on the number of
    bedrooms, bathrooms, and the price.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的公寓价格示例，考虑预测公寓广告页面的兴趣而不是预测租金价格的问题。我们可以想象，拥有多个卧室和合理价格的公寓会比没有浴室但价格非常低的公寓产生更多的兴趣（尽管，在纽约市，你永远不知道）。RF可以捕捉公寓特征与低、中、高兴趣类别之间的关系，就像它可以捕捉与价格之间的关系一样。**图2.3**展示了预测中等和高兴趣的叶子节点，这取决于卧室数量、浴室数量和价格。
- en: Now that we've got an idea of what's going on at a high level, let's pack the
    key concepts and terminology we've learned in this chapter into a few concentrated
    paragraphs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对高层次的情况有了了解，让我们将本章学到的关键概念和术语集中到几个段落中。
- en: 2.4 The Big Picture
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 大图景
- en: '![](../Images/a28cdc936882be72eabc12d4560c896a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a28cdc936882be72eabc12d4560c896a.png)'
- en: '**Figure 2.4**. Feature vectors X and targets y'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.4**. 特征向量X和目标y'
- en: Machine learning uses a model to capture the relationship between feature vectors
    and some target variables within a training data set. A feature vector is a set
    of features or attributes that characterize a particular object, such as the number
    of bedrooms, bathrooms, and location of an apartment. The target is either a scalar
    value like rent price, or it's an integer classification such as “creditworthy”
    or “it's not cancer.” Features and targets are presented to the model for training
    as a list of feature vectors and a list of target values in the form of an abstract
    matrix X and vector y (**Figure 2.4**). The ![](../Images/da9763e247beb7c653237d6841eb71e4.png)
    row of X is labelled x[i] and represents everything we know about a particular
    entity, such as an apartment. The ![](../Images/da9763e247beb7c653237d6841eb71e4.png)
    target value is labeled y[i]. (We'll capitalize matrices and bold vectors, kind
    of like declaring types in a programming language like Java or C++.) The vector
    of features, x[i], and target value, y[i], for one entity is called an *observation*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习使用模型来捕捉训练数据集中特征向量与某些目标变量之间的关系。特征向量是一组特征或属性，用于描述特定对象，例如公寓的卧室数量、浴室数量和位置。目标是标量值，如租金价格，或者是一个整数分类，例如“有信用”或“不是癌症”。特征和目标以特征向量列表和目标值列表的形式呈现给模型进行训练，形式为抽象矩阵
    X 和向量 y（**图 2.4**）。X 的 ![](../Images/da9763e247beb7c653237d6841eb71e4.png) 行被标记为
    x[i]，代表我们对特定实体的所有了解，例如一个公寓。目标值 ![](../Images/da9763e247beb7c653237d6841eb71e4.png)
    被标记为 y[i]。（我们将矩阵大写，粗体向量，类似于在 Java 或 C++ 等编程语言中声明类型。）一个实体的特征向量 x[i] 和目标值 y[i] 的向量被称为
    *观测值*。
- en: '![](../Images/02f131073a3557ca702a5ad69741ddee.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02f131073a3557ca702a5ad69741ddee.png)'
- en: '**Figure 2.5**. Predictor block diagram![](../Images/4267147f578e1fd6cde5f534c506c9b9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.5**. 预测器框图![](../Images/4267147f578e1fd6cde5f534c506c9b9.png)'
- en: '**Figure 2.6**. Classifier block diagram![](../Images/4687bc7d38b45bc969e4d047e85c99a7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.6**. 分类器框图![](../Images/4687bc7d38b45bc969e4d047e85c99a7.png)'
- en: '**Figure 2.7**. Predictors draw curves through data![](../Images/fc3c1a6f2523aeba87c95e7b0bd7d4b0.png)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.7**. 预测器通过数据绘制曲线![](../Images/fc3c1a6f2523aeba87c95e7b0bd7d4b0.png)'
- en: '**Figure 2.8**. Classifiers draw curves through the space separating classes'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.8**. 分类器在分隔类的空间中绘制曲线'
- en: If the target is a numeric value, we're building a predictor (also commonly
    called a regressor), as shown in **Figure 2.5**. If the target is a discrete category
    or class, we're building a classifier (**Figure 2.6**). We'll learn about classifiers
    in [foo], but a simple way to think about the difference between predictors and
    classifiers is illustrated in **Figure 2.7** and **Figure 2.8**. Predictors are
    usually fitting curves to data and classifiers are drawing decision boundaries
    in between data points associated with the various categories. There is a tendency
    to think of predictors and classifiers as totally different problems with different
    solutions, but they are really the same core problem and most models have both
    predictor and classifier variants. In fact, the classifier variant is sometimes
    nothing more than the predictor variant with an additional function that clips,
    scales, or tweaks the predictor's output.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是数值，我们正在构建一个预测器（也常称为回归器），如图 **图 2.5** 所示。如果目标是离散类别或类别，我们正在构建一个分类器（**图 2.6**）。我们将在
    [foo] 中学习关于分类器的知识，但简单来说，预测器和分类器之间的区别如图 **图 2.7** 和 **图 2.8** 所示。预测器通常是将曲线拟合到数据中，而分类器是在与各种类别相关的数据点之间绘制决策边界。有一种趋势认为预测器和分类器是两个完全不同的问题，具有不同的解决方案，但实际上它们是同一个核心问题，大多数模型都有预测器和分类器变体。实际上，分类器变体有时不过是预测器变体加上一个额外的函数，该函数可以剪辑、缩放或调整预测器的输出。
- en: Machine learning tasks that have both feature vectors `x` and known targets
    `y` fall into the *supervised learning* category and are the focus of this book.
    *Unsupervised learning* tasks involve just `x` and the target variable is unknown;
    we say the data is unlabeled. The most common unsupervised task is called *clustering*
    that attempts to cluster similar data points together very much like **Figure
    2.8**. In the supervised case, though, we know not only how many categories there
    are but we also know which records are associated with which category. The goal
    of clustering is to discover both the number of categories and assign records
    to categories. As we mentioned in **Chapter 1** *Welcome!*, the vast majority
    of machine learning problems are supervised and, besides, unsupervised techniques
    are straightforward to learn after mastering supervised techniques.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 具有特征向量 `x` 和已知目标 `y` 的机器学习任务属于**监督学习**类别，这也是本书的重点。**无监督学习**任务只涉及 `x`，目标变量是未知的；我们说数据是无标签的。最常见的无监督任务是**聚类**，它试图将相似的数据点聚集在一起，就像**图2.8**所示。然而，在监督的情况下，我们不仅知道有多少个类别，还知道哪些记录与哪个类别相关。聚类的目标是发现类别的数量并将记录分配到类别中。正如我们在**第一章**中提到的**欢迎**，绝大多数机器学习问题都是监督学习，而且，在掌握监督技术之后，无监督技术也容易学习。
- en: '![](../Images/a9ecfbe2a3265d918dabdbe871803c58.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/a9ecfbe2a3265d918dabdbe871803c58.png)'
- en: '**Figure 2.9**. Distilling model parameters from training data and'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.9**. 从训练数据中提取模型参数'
- en: A model is a combination of data structure, algorithm, and mathematics that
    captures the relationship described by a collection of (feature vector, target)
    pairs. The model records a condensation of the training data in its data structure,
    which can be anything from the unaltered training set (nearest neighbor model)
    to a set of decision trees (random forest model) to a handful of weights (linear
    model). This data structure comprises the parameters of the model and the parameters
    are computed from the training data. **Figure 2.9** illustrates the process.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是由数据结构、算法和数学组成，它捕捉了一组（特征向量，目标）对所描述的关系。模型在其数据结构中记录了训练数据的浓缩，这可以是未改变的训练集（最近邻模型）到一组决策树（随机森林模型）再到少量权重（线性模型）。这个数据结构构成了模型的参数，这些参数是从训练数据中计算得出的。**图2.9**展示了这个过程。
- en: The process of computing model parameters is called training the model or fitting
    a model to the data. If a model is unable to capture the relationship between
    feature vectors and targets, the model is underfitting (assuming there is a relationship
    to be had). At the other extreme, a model is overfitting if it is too specific
    to the training data and doesn't generalize well. To generalize means that we
    get accurate predictions for feature vectors not found in the training set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型参数的过程被称为训练模型或拟合数据。如果一个模型无法捕捉特征向量和目标之间的关系，那么这个模型就是欠拟合（假设存在可捕捉的关系）。在另一个极端，如果一个模型对训练数据过于具体，并且泛化能力不好，那么这个模型就是过拟合。泛化的意思是我们对训练集中未出现的特征向量也能得到准确的预测。
- en: Models also have hyper-parameters, which dictate the overall architecture or
    other aspects of the model. For example, the nearest neighbor model is more specifically
    called “k-nearest neighbor” because the model finds the nearest k objects then
    averages their target values to make a prediction. k is the model's hyper-parameter.
    In a random forest, the number of trees in the forest is the most common hyper-parameter.
    In a neural network, hyper-parameters typically include the number of layers and
    number of neurons. Hyper-parameters are specified by the programmer, not computed
    from the training data, and are often used to tune a model to improve accuracy
    for a particular data set.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 模型也有超参数，这些超参数决定了模型的总体架构或其他方面。例如，最近邻模型更具体地称为“k-最近邻”，因为模型找到最近的k个对象，然后平均它们的目标值来做出预测。k是模型的超参数。在随机森林中，森林中的树的数量是最常见的超参数。在神经网络中，超参数通常包括层数和神经元数量。超参数由程序员指定，而不是从训练数据中计算得出，并且通常用于调整模型以提高特定数据集的准确性。
- en: To test generality, we either need to be given a validation set as well as a
    training set, or we need to split the provided single data set into a training
    set and a validation set. The model is exposed only to the training set, reserving
    the validation set for measuring generality and tuning the model. (Later we'll
    discuss a *test set* that is used as a final test of generality; the test set
    is never used while training or tuning the model.) The validation set has both
    feature vectors and targets and so we can compare the model's prediction with
    the known correct targets to compute an accuracy metric.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试泛化能力，我们要么需要同时获得验证集和训练集，要么需要将提供的单个数据集拆分为训练集和验证集。模型仅暴露于训练集，将验证集保留用于测量泛化能力和调整模型。（稍后我们将讨论一个用作泛化最终测试的*测试集*；测试集在训练或调整模型时永远不会使用。）验证集包含特征向量和目标，因此我们可以将模型的预测与已知的正确目标进行比较，以计算准确度指标。
- en: That's a lot to take in, but it will crystallize more and more as we work through
    more examples. In the next chapter, we'll see how easy it is to train random forest
    models on some interesting and real data sets.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要吸收很多信息，但随着我们处理更多示例，它将越来越清晰。在下一章中，我们将看到在有趣且真实的数据集上训练随机森林模型是多么容易。
