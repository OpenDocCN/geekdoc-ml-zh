- en: 5.1.3 Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html](https://huyenchip.com/ml-interviews-book/contents/5.1.3-dimensionality-reduction.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*If some characters seem to be missing, it''s because MathJax is not loaded
    correctly. Refreshing the page should fix it.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[E] Why do we need dimensionality reduction?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Eigendecomposition is a common factorization technique used for dimensionality
    reduction. Is the eigendecomposition of a matrix always unique?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Name some applications of eigenvalues and eigenvectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] We want to do PCA on a dataset of multiple features in different ranges.
    For example, one is in the range 0-1 and one is in the range 10 - 1000\. Will
    PCA work on this dataset?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Under what conditions can one apply eigendecomposition? What about SVD?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the relationship between SVD and eigendecomposition?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s the relationship between PCA and SVD?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] How does t-SNE (T-distributed Stochastic Neighbor Embedding) work? Why
    do we need it?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**In case you need a refresh on PCA, here''s an explanation without any math.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Assume that your grandma likes wine and would like to find characteristics
    that best describe wine bottles sitting in her cellar. There are many characteristics
    we can use to describe a bottle of wine including age, price, color, alcoholic
    content, sweetness, acidity, etc. Many of these characteristics are related and
    therefore redundant. Is there a way we can choose fewer characteristics to describe
    our wine and answer questions such as: which two bottles of wine differ the most?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: PCA is a technique to construct new characteristics out of the existing characteristics.
    For example, a new characteristic might be computed as `age - acidity + price`
    or something like that, which we call a linear combination.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To differentiate our wines, we'd like to find characteristics that strongly
    differ across wines. If we find a new characteristic that is the same for most
    of the wines, then it wouldn't be very useful. PCA looks for characteristics that
    show as much variation across wines as possible, out of all linear combinations
    of existing characteristics. These constructed characteristics are principal components
    of our wines.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to see a more detailed, intuitive explanation of PCA with visualization,
    check out [amoeba's answer on StackOverflow](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579).
    This is possibly the best PCA explanation I've ever read.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
