# 负责任的 AI

*DALL·E 3 提示：描绘一个未来场景中的负责任 AI 插图，背景是宇宙：一只或几只人类手在培育一株幼苗，这株幼苗长成了一棵 AI 树，象征着神经网络。树上有数字分支和叶子，类似于神经网络，以表示 AI 的互联性。背景描绘了一个未来宇宙，其中人类和具有通用智能的动物和谐合作。场景捕捉了 AI 作为幼苗的初始培育，强调 AI 技术与人类和宇宙的和谐发展。*

![](img/file289.png)

## 目的

*为什么负责任的 AI 实践已经从可选的伦理考量转变为强制性的工程要求，这些要求决定了系统的可靠性、法律合规性和商业可行性？*

在现实世界环境中部署的机器学习系统面临严格的可靠性要求，这些要求超出了算法准确性。有偏见的预测会引发法律责任，不透明的决策会阻止监管批准，不可问责的系统会通不过审计，不可解释的输出会损害用户信任。这些运营现实将负责任的 AI 从哲学理想转变为具体的工程约束，决定了系统是否可以在生产环境中部署、维护和扩展。负责任的 AI 实践提供了系统性的方法，用于构建符合监管要求、通过第三方审计、保持用户信心并在不同人群和环境中可靠运行的系统。现代机器学习工程师必须将偏差检测、可解释性机制、问责制框架和监督系统作为核心架构组件进行整合。将负责任的 AI 视为一个工程学科，能够构建在日益监管的部署环境中既实现技术性能又具有运营可持续性的系统。

**学习目标**

+   将公平性、透明度、问责制、隐私和安全定义为可衡量的工程要求，这些要求限制了系统架构、数据处理和部署决策

+   使用 Fairlearn 等工具实施偏差检测技术，以识别模型性能在不同人口群体中的差异，并评估包括人口比例和均衡机会在内的公平性指标

+   应用隐私保护方法，包括差分隐私、联邦学习和机器反学习，以保护敏感数据同时保持模型效用

+   使用 SHAP、LIME 和 GradCAM 等后处理技术生成模型预测的解释，同时评估它们在不同模型架构中的计算成本和可靠性

+   分析部署环境（云、边缘、移动、TinyML）如何施加架构限制，限制了哪些负责任的 AI 保护在技术上可行

+   评估相互竞争的负责任的 AI 原则之间的权衡，认识到当数学不可能定理阻止同时优化所有公平性标准时

+   设计组织结构和治理流程，将负责任的 AI 原则转化为操作实践，包括角色定义、升级路径和问责机制

+   评估负责任的 AI 技术的计算开销，并确定实施资源障碍

## 负责任的 AI 简介

2019 年，亚马逊在发现其基于历史简历数据的招聘算法系统地惩罚女性候选人后，取消了该算法(Dastin 2022)。虽然该系统在技术上看似复杂，但它已经学会了过去成功的申请者主要是男性，这反映了历史性别偏见，而不是基于能力的资格。该模型在统计上是最优的，但在伦理上却是灾难性的，这表明技术正确性可以与技术严重的社会危害共存。

这个事件说明了负责任的 AI 的核心挑战：系统可能在算法上合理，同时持续加剧不公，优化目标的同时破坏价值观，满足性能基准的同时却未能满足社会需求。这个问题不仅超越了个人偏见，还涵盖了影响数十亿人日常生活的系统中的透明度、问责制、隐私和安全等系统性问题。

机器学习系统工程的学科已经发展到解决这一关键转折点，即技术卓越与社会深远影响相交。从第三章的算法基础，从第八章的优化技术，以及从第二章的部署架构，建立了为具有非凡能力和范围的系统所需的计算基础设施。然而，随着这些系统在医疗诊断、司法决策、就业筛选和金融服务中承担越来越重要的角色，仅仅技术性能指标是否足够的问题引起了质疑。当代机器学习系统提出了一个基本挑战：它们可能在统计性能上达到最优，同时产生与公平性、透明度和社会正义相冲突的结果。

本章从第五部分“可信系统”开始，通过扩展我们的分析框架，从技术正确性扩展到包括系统是否值得社会信任和接受这一规范性问题。从弹性系统的发展中建立了一个重要的区别：弹性 AI 通过对抗性攻击和硬件故障来应对系统完整性的威胁，而负责任的 AI 确保正常运行的系统产生符合人类价值观和集体福祉的结果。

处理这一挑战的学科将抽象的伦理原则转化为具体的技术约束和设计要求。类似于安全协议需要特定的架构决策和监控基础设施，负责任的 AI 需要通过可量化的技术机制和可验证的系统属性来实施公平性、透明度和问责制。这代表了工程方法论的扩展，将规范性要求作为一级设计考虑因素，而不仅仅是将哲学概念应用于工程实践。

软件工程为此学科的发展提供了先例。早期的计算系统优先考虑功能正确性，关注程序是否为给定的输入生成准确的输出。随着系统复杂性和社会整合度的提高，该领域发展了可靠性工程、安全保证和可维护性分析的方法。当代负责任的 AI 实践代表了平行的学科成熟，将系统化的工程方法扩展到包括算法决策的社会和伦理维度。

这种扩展反映了当代机器学习部署前所未有的规模。这些系统现在调解着影响数十亿人的决策，包括信贷分配、医疗诊断、教育评估和刑事司法程序。与表现为系统崩溃或数据损坏的传统软件故障不同，负责任的 AI 故障可能会持续系统性的歧视，损害民主机构，并侵蚀公众对有益技术的信心。该领域需要展示技术专业性和伦理问责制以及社会责任的系统。

**负责任的 AI**是系统工程学科，系统地转化**伦理原则**为**具体的设计要求**和**可衡量的系统属性**，将其确立为机器学习系统开发中的**一级约束**。

负责任的 AI 构成一个具有四个相互关联维度的系统化工程学科。本章探讨了伦理原则如何转化为可衡量的系统需求，分析了检测和减轻有害算法行为的技术方法，解释了为什么负责任的 AI 不仅限于个别系统，还包括更广泛的社会技术动态，并针对组织和管理环境中的实际实施挑战进行了讨论。

学生必须培养技术能力和情境理解。学生将学习实施偏差检测算法和隐私保护机制，同时理解为什么技术解决方案需要组织治理结构和利益相关者参与流程。这包括增强系统可解释性和问责制的方法，同时考察无法由算法方法明确解决的竞争性规范性价值之间的紧张关系。

本章构建了同时解决即时功能需求和长期社会考量的工程系统所需的解析框架。这个框架将负责任的 AI 视为不是应用于现有系统的补充约束，而是当代人工智能发展中良好工程实践的基本原则。

**导航本章**

从四个互补的视角探讨负责任的 AI 方法，每个视角对于构建可信赖的机器学习系统都是必不可少的：

**1. 原则与基础** (第 17.2 节 至 第 17.4 节)：定义了负责任 AI 系统应实现的目标。介绍了公平性、透明度、问责制、隐私和安全作为工程要求。考察了这些原则如何在云、边缘、移动和 TinyML 部署中有所不同，揭示了理想与运营约束之间的紧张关系。

**2. 技术实施** (第 17.5 节)：介绍使负责任的 AI 成为可能的具体技术。包括识别偏差和漂移的检测方法、包括隐私保护和对抗性防御在内的缓解技术，以及用于可解释性和监控的验证方法。这些方法将抽象原则转化为可衡量的系统行为。

**3. 社会技术动态** (第 17.6 节)：说明仅技术正确性是不够的。考察系统与环境之间的反馈循环、人机协作挑战、竞争性利益相关者价值、可争议机制和制度治理结构。负责任的 AI 存在于算法、组织和社会的交汇点。

**4. 实施现实** (第 17.7 节 至 第 17.8 节)：考察原则如何转化为实践。解决组织障碍、数据质量约束、竞争性目标、可扩展性挑战和评估差距。以自主系统的 AI 安全和价值对齐考虑作为结论。

本章内容全面，因为负责任的人工智能涉及工程、伦理、政策和组织设计。使用章节结构导航到与您当前需求最相关的主题，但请认识到，有效的负责任人工智能实施需要整合所有四个视角。仅技术解决方案无法解决价值冲突；没有技术实施的伦理原则仍然只是愿望；没有组织支持的个人干预将失败。

这些原则和实践为构建既满足当前需求又促进长期社会福祉的人工智能系统奠定了基础。通过将公平、透明度、问责制、隐私和安全视为工程要求而不是事后考虑，从业者发展了确保机器学习系统造福社会同时最小化危害所需的技术技能和组织方法。这种负责任人工智能的系统方法将抽象的伦理原则转化为具体的设计约束，指导机器学习生命周期的每个阶段。

## 核心原则

负责任的人工智能指的是开发和部署旨在维护伦理原则并促进社会有益成果的机器学习系统。这些原则不仅作为政策理想，而且作为对系统设计、实施和治理的具体约束。

公平性指的是机器学习系统不因种族、性别或社会经济地位等受保护属性 1 歧视个人或群体的期望。这一原则既包括统计指标，也包括更广泛的关于公平、正义和结构偏见的规范性关切。公平性标准的正式数学定义在第 17.3.2 节中进行了详细探讨。

实施负责任人工智能系统的计算资源需求产生了重大的公平性考虑，这些考虑超越了个人系统设计。这些挑战包括部署限制和实施障碍中考察的进入障碍和环境正义关切。

可解释性涉及利益相关者解释模型如何产生其输出的能力。这包括理解单个决策是如何做出的以及模型的整体行为模式。解释可以在决策之后生成（称为事后解释 2）以详细说明推理过程，或者它们可以内置到模型的设计中以实现透明操作。在第四章（ch010.xhtml#sec-dnn-architectures）中讨论的神经网络架构在固有的可解释性方面存在显著差异，深度网络通常更难以解释。可解释性对于错误分析、合规性和建立用户信任至关重要。

透明度指的是关于人工智能系统构建、训练、验证和部署的开放性。它包括数据来源、设计假设、系统限制和性能特征的披露。虽然可解释性侧重于理解输出，但透明度涉及系统的更广泛生命周期。

问责制指的是个人或组织因人工智能系统的结果而承担责任的方式。它涉及可追溯性、文档记录、审计和纠正伤害的能力。问责制确保人工智能故障不被视为抽象的故障，而是具有现实世界影响的后果。

价值一致性 3 是指人工智能系统应追求与人类意图和道德规范一致的目标。在实践中，这涉及技术挑战，包括奖励设计和约束规范，以及更广泛的问题，即哪些价值观被代表和执行。

人类监督强调人类判断在监督、纠正或停止自动化决策中的作用。这包括操作期间的人类在回路 4，以及确保人工智能使用始终符合社会价值观和现实世界复杂性的组织结构。

其他重要原则，如隐私和鲁棒性，需要专门的技術实现，这些实现与系统设计中的安全和可靠性考虑相交。

## 在机器学习生命周期中整合原则

负责任的机器学习始于一组基础原则，包括公平性、透明度、问责制、隐私和安全，这些原则定义了人工智能系统如何以道德和可预测的方式行为。这些原则不是抽象的理想或事后想法；它们必须转化为具体的约束，以指导模型的训练、评估、部署和维护。

在实践中实施这些原则需要理解每个原则如何为系统行为设定具体期望。公平性关注模型如何对待不同的子群体以及如何应对历史偏见。可解释性确保模型决策可以被开发者、审计员和最终用户理解。隐私管理收集的数据以及如何使用这些数据。问责制定义了在整个系统生命周期中如何分配、跟踪和执行责任。安全性要求模型即使在不确定或不断变化的环境中也能可靠地运行。

表 17.1：**负责任的人工智能生命周期**：在整个机器学习系统生命周期中嵌入公平性、可解释性、隐私、问责制和鲁棒性，从数据收集到监控，确保这些原则成为架构承诺而不是事后考虑。该表将这些原则映射到特定的开发阶段，揭示了主动集成如何解决潜在风险并促进可信赖的人工智能系统。

| **原则** | **数据收集** | **模型训练** | **评估** | **部署** | **监控** |
| --- | --- | --- | --- | --- | --- |
| **公平性** | 代表性抽样 | 偏差感知算法 | 组级指标 | 阈值调整 | 子组性能 |
| **可解释性** | 文档标准 | 可解释架构 | 模型行为分析 | 面向用户的解释 | 解释质量日志 |
| **透明度** | 数据源跟踪 | 训练文档 | 性能报告 | 模型卡片 | 变更跟踪 |
| **隐私** | 同意机制 | 隐私保护方法 | 隐私影响评估 | 安全部署 | 访问审计日志 |
| **问责制** | 管理框架 | 决策日志 | 审计跟踪创建 | 覆盖机制 | 事件跟踪 |
| **鲁棒性** | 质量保证 | 鲁棒训练方法 | 压力测试 | 故障处理 | 性能监控 |

这些原则共同定义了机器学习系统如何表现出负责任的行为，不是作为孤立的功能，而是作为嵌入在整个生命周期中的系统级约束。表 17.1 提供了一个结构化的视角，展示了关键原则，包括公平性、可解释性、透明度、隐私、问责制和鲁棒性，如何映射到 ML 系统开发的各个主要阶段：数据收集、模型训练、评估、部署和监控。一些原则（如公平性和隐私）从数据开始，而其他原则（如鲁棒性和问责制）在部署和监督期间最为重要。尽管可解释性通常在评估和用户交互期间被强调，但它也支持模型调试和设计时验证。这种全面的映射强化了负责任的 AI 不是事后考虑，而是一个多阶段架构承诺的观点。

#### 资源需求和公平影响

实施负责任的 AI 原则需要计算资源，这些资源在技术和部署环境中差异很大。这些资源需求产生了多方面的公平考虑，不仅超越了单个组织，还涵盖了更广泛的社会和环境正义问题。计算预算有限的组织可能无法实施全面的负责任 AI 保护，这可能导致对道德保障的访问不平等。最先进的 AI 系统越来越需要专用硬件和高带宽连接，这系统地排除了农村社区、发展中国家和资源受限用户访问先进 AI 能力。

环境正义问题通过工程现实加剧了这些访问障碍，即负责任的 AI 技术需要巨大的能源成本。训练差分隐私模型需要额外的 15-30%计算周期；实时公平监控增加了 10-20 毫秒的延迟和持续的 CPU 开销；SHAP 解释需要 50-1000 倍的正常推理计算。这些计算需求直接转化为基础设施需求：一个为 1000 万用户提供负责任 AI 功能的繁忙系统，与不受限制的模型相比，需要大量的数据中心容量。

这种计算基础设施的地理分布创造了工程师在设计系统时必须考虑的系统不平等。支持人工智能工作负载的数据中心集中在电力成本较低且法规有利的地区，这些地区通常与低收入社区相关，这些社区在污染、热量产生和电网压力增加的同时，往往缺乏访问这些设施所提供的 AI 服务所需的高带宽连接。这形成了一个反馈循环，其中计算公平不仅取决于算法设计，还取决于影响系统性能和社区福利的基础设施位置决策。具体技术的详细性能特征在第 17.5 节中进行了考察。

### 透明性和可解释性

本节详细探讨了具体原则。机器学习系统经常因缺乏可解释性而受到批评。在许多情况下，模型作为不透明的“黑箱”运行，产生用户、开发者和监管者难以理解或审查的输出。这种不透明性构成了信任的巨大障碍，尤其是在高风险领域，如刑事司法、医疗保健和金融，在这些领域，问责制和申诉权非常重要。例如，在美国用于评估再犯风险的[COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx)算法被发现存在种族偏见 5。然而，系统的专有性质以及有限的解释性工具访问权限阻碍了调查或解决该问题的努力。

可解释性是指理解模型如何产生预测的能力。它包括两种解释：*局部解释*6，它阐明个别预测；以及*全局解释*7，它描述模型的一般行为。相比之下，透明性涵盖了关于更广泛系统设计和操作的开放性。这包括数据来源、特征工程 8、模型架构、训练程序、评估协议和已知限制的披露。透明性还涉及记录预期用例、系统边界和治理结构。

可解释性和透明度的重要性不仅限于技术考虑，还涉及法律要求。在许多司法管辖区，这些原则是法律义务，而不仅仅是最佳实践。例如，欧盟的[通用数据保护条例（GDPR）](https://gdpr.eu/tag/gdpr/)要求个人获得有关对其产生重大影响的自动化决策逻辑的有意义信息 9。在其他领域也出现了类似的监管压力，这加强了将可解释性和透明度视为核心架构要求的需求。

实施这些原则需要预见不同利益相关者的需求，其相互竞争的价值和优先事项在第 17.6.3 节中得到全面审查。因此，为了可解释性和透明度而进行设计，需要决定如何在系统生命周期中呈现相关信息。

这些原则也支持系统在时间上的可靠性。随着模型的重训练或更新，可解释性和可追溯性的机制允许检测到意外行为，进行根本原因分析，并支持治理。透明度和可解释性，当嵌入到系统的结构和操作中时，为信任、监督和与机构和社会期望的协调提供了基础。

### 机器学习中的公平性

机器学习中的公平性提出了复杂挑战。如第 17.2 节所确立的，公平性要求自动化系统不会不成比例地损害受保护群体。因为这些系统是在历史数据上训练的，它们容易复制和放大数据中嵌入的系统偏见 10。如果没有仔细设计，机器学习系统可能会无意中加强社会不平等，而不是减轻它们。

一个广泛研究过的例子来自医疗保健领域。一种用于在美国医院分配护理管理资源的算法被发现系统地低估了黑人患者的健康需求(Obermeyer 等人 2019)11。该模型将医疗支出作为健康状况的代理，但由于长期存在的获取和支出差异，黑人患者不太可能产生高额费用。因此，该模型推断他们病情较轻，尽管他们通常有相等或更大的医疗需求。这一案例说明了看似中性的设计选择，如代理变量选择，如何在历史不平等没有得到适当考虑的情况下产生歧视性结果。

实践者需要正式的方法来评估这些持续偏见风险下的公平性。已经开发了一系列正式标准，这些标准量化了模型在由敏感属性定义的群体中的表现。

**数学内容先行**

在考察正式定义之前，考虑一下基本挑战：一个算法要公平意味着什么？它应该对每个人都同等对待，还是应该考虑不同的基线条件？它应该优化平等的结果、平等的机会还是平等的治疗？这些问题导致不同的数学标准，每个标准都捕捉到公平的不同方面。

以下小节将使用概率符号介绍正式的公平性定义。这些指标（人口比例、均衡机会、机会平等）贯穿于机器学习公平性文献中，并塑造了监管框架。关注理解直觉：每个指标衡量的是什么以及为什么它很重要，而不是数学证明。每个定义之后的具体例子说明了实际应用。如果概率符号不熟悉，从口头描述开始，稍后再回到正式定义。

假设一个模型 <semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)</annotation></semantics> 预测一个二元结果，例如贷款偿还，让 <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics> 代表一个敏感属性，具有子组 <semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics> 和 <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>。一些广泛使用的公平性定义包括：

#### 人口比例平等

此标准要求接收积极预测的概率与群体成员资格无关。形式上，如果模型满足以下条件，则认为模型满足人口比例平等：<semantics><mrow><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a\big) = P\big(h(x) = 1 \mid S = b\big)</annotation></semantics>

这意味着模型在由敏感属性 <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics> 定义的子组中，以相同的比率分配有利的结果，例如贷款批准或治疗推荐。

在医疗保健的例子中，人口统计学上的平等会询问黑人患者和白人患者是否以相同的比率被推荐接受护理，无论他们的潜在健康需求如何。虽然从平等获取的角度来看这似乎是公平的，但它忽略了医疗状况和风险的实际差异，可能在需求分布不均匀的情况下过度纠正。

这种限制促使人们提出更细致的公平性标准。

#### 平等机会

此定义要求模型在给定真实标签的情况下，其预测与组别条件独立。具体来说，真实阳性和假阳性率必须在组间相等：<semantics><mrow><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>,</mo><mrow><mtext mathvariant="normal">for</mtext></mrow> <mi>y</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow> <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = y\big) = P\big(h(x) = 1 \mid S = b, Y = y\big), \quad \text{for } y \in \{0, 1\}.</annotation></semantics>

即，对于每个真实的输出 <semantics><mrow><mi>Y</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">Y = y</annotation></semantics>，模型应在组 <semantics><mrow><mi>S</mi><mo>=</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">S = a</annotation></semantics> 和 <semantics><mrow><mi>S</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">S = b</annotation></semantics> 之间产生相同的预测分布。这意味着对于具有相同真实输出的个体，无论他们是否获得阳性结果，模型在组间应表现出相似的行为。这确保了错误（包括漏检和错误的阳性）在组间均匀分布。

在医疗案例中应用，等概率确保具有相同实际健康需求（真实标签<semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>）的患者，无论种族如何，都有同等的机会被正确或错误地推荐。原始算法通过低估与白人患者健康状况相当或更差的黑人患者的推荐，违反了这一准则，突显了真实阳性率的不平等。

一个不那么严格的准则专注于积极的成果。

#### 机会均等

等概率的放宽，这一准则仅关注真实阳性率。它要求，在应获得积极结果的个人中，获得一个的概率在各个群体中是相等的：<semantics><mrow><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>a</mi><mo>,</mi><mi>Y</mi><mo>=</mo><mn>1</mn><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>=</mo><mi>P</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>b</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mi>.</mi></mrow> <annotation encoding="application/x-tex">P\big(h(x) = 1 \mid S = a, Y = 1\big) = P\big(h(x) = 1 \mid S = b, Y = 1\big).</annotation></semantics>

这确保了具有资格的个人，无论其群体成员身份如何，模型都会平等对待，即<semantics><mrow><mi>Y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">Y = 1</annotation></semantics>。

在我们的运行示例中，这一措施将确保在需要护理的患者中，黑人和白人个体都有同等的机会被模型识别。在美国医院系统中，算法使用医疗保健支出作为代理变量导致未能满足这一标准：由于历史支出较低，具有重大健康需求的黑人患者不太可能获得护理。

考虑一个在 200 名申请人中评估的简化贷款审批模型，这些申请人平均分为两个人口群体（A 组和 B 组）。该模型做出预测，我们后来观察到实际的还款结果：

**A 组（100 名申请人）：**

+   模型批准：70 名申请人（40 名实际上会还款，30 名会违约）

+   模型拒绝：30 名申请人（5 名实际上会还款，25 名会违约）

**B 组（100 名申请人）：**

+   模型批准：40 名申请人（30 名实际上会还款，10 名会违约）

+   模型拒绝：60 名申请人（20 名实际上会还款，40 名会违约）

**计算人口统计学上的平等：** <semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>70</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.70</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mn>100</mn></mfrac><mo>=</mo><mn>0.40</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A) = \frac{70}{100} = 0.70 \\ P(h(x) = 1 \mid S = B) = \frac{40}{100} = 0.40 \end{gather*}</annotation></semantics>

**差异：** <semantics><mrow><mn>0.70</mn><mo>−</mo><mn>0.40</mn><mo>=</mo><mn>0.30</mn></mrow><annotation encoding="application/x-tex">0.70 - 0.40 = 0.30</annotation></semantics>（30 个百分点的差距）

该模型通过以显著更高的比率批准 A 组申请人，无论其实际还款能力如何，违反了人口统计学上的平等。

**计算机会平等（真正率）：**

在实际会还款的申请人中（Y=1）：<semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>40</mn><mrow><mn>40</mn><mo>+</mo><mn>5</mn></mrow></mfrac><mo>=</mo><mfrac><mn>40</mn><mn>45</mn></mfrac><mo>≈</mo><mn>0.89</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>20</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.60</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 1) = \frac{40}{40 + 5} = \frac{40}{45} \approx 0.89 \\ P(h(x) = 1 \mid S = B, Y = 1) = \frac{30}{30 + 20} = \frac{30}{50} = 0.60 \end{gather*}</annotation></semantics>

**差异：** <semantics><mrow><mn>0.89</mn><mo>−</mo><mn>0.60</mn><mo>=</mo><mn>0.29</mn></mrow><annotation encoding="application/x-tex">0.89 - 0.60 = 0.29</annotation></semantics>（真正例率 29 个百分点差距）

该模型违反了机会平等原则：在那些会还款的合格申请人中，A 组成员被正确批准的比率是 89%，而 B 组成员只有 60%被批准。

**计算均衡概率（真正例率 + 假正例率）：**

我们已经计算了真正例率（TPR）。现在计算那些不会还款（Y=0）的申请人的假正例率：<semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>A</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>30</mn><mrow><mn>30</mn><mo>+</mo><mn>25</mn></mrow></mfrac><mo>=</mo><mfrac><mn>30</mn><mn>55</mn></mfrac><mo>≈</mo><mn>0.55</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>∣</mo><mi>S</mi><mo>=</mo><mi>B</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>10</mn><mrow><mn>10</mn><mo>+</mo><mn>40</mn></mrow></mfrac><mo>=</mo><mfrac><mn>10</mn><mn>50</mn></mfrac><mo>=</mo><mn>0.20</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*} P(h(x) = 1 \mid S = A, Y = 0) = \frac{30}{30 + 25} = \frac{30}{55} \approx 0.55 \\ P(h(x) = 1 \mid S = B, Y = 0) = \frac{10}{10 + 40} = \frac{10}{50} = 0.20 \end{gather*}</annotation></semantics>

该模型还存在着不等的假正例率：它错误地批准了 55%的 A 组成员将会违约，但只有 20%的 B 组成员将会违约。这表明，即使 A 组成员不会还款，模型对 A 组也更“慷慨”。

**关键洞见：** 该模型违反了所有三个公平性标准。解决一个标准并不意味着自动满足其他标准。事实上，它们可能存在冲突，正如我们接下来将要看到的。

这些公平性标准突显了在定义算法公平性时的紧张关系。

**高级主题：不可能性结果**

下面讨论的不可能性定理代表了公平性理论中的活跃研究。理解多个公平性标准不能同时满足比数学证明更重要。关键洞见：公平性本质上是一个充满价值判断的工程决策，需要利益相关者的审议，而不是一个具有单一正确解决方案的技术优化问题。这种概念理解对大多数从业者来说就足够了。

这些定义捕捉了公平性的不同方面，通常是不兼容的 12。为了直观理解这一点，想象一所大学希望在招生过程中保持公平。这意味着什么？目标 1（人口比例平等）将是录取学生，使录取班级反映申请者池的统计数据，例如 A 组和 B 组各占 50%。目标 2（平等机会）将是确保在所有合格的申请者中，各群体之间的录取率相同，因此 80%的合格 A 组申请者被录取，80%的合格 B 组申请者被录取。不可能定理表明，您并不总是可以两者兼得。如果一个群体有更多合格的申请者，实现人口比例平等（目标 1）将需要拒绝一些他们的合格申请者，从而违反平等机会（目标 2）。这个问题没有数学上的解决方案；这是关于优先考虑哪种公平性定义的价值判断。满足一个标准可能阻止满足另一个标准，反映了公平性涉及在竞争性规范性目标之间进行权衡的现实。确定优先考虑哪个指标需要仔细考虑应用背景、潜在危害和利益相关者的价值观，如第 17.6.3 节中详细所述。

认识到这些紧张关系，操作系统必须将公平性视为一个约束条件，在整个机器学习生命周期中指导决策。它受到数据收集和表示方式、目标和代理的选择、模型预测的阈值以及反馈机制结构的影响。例如，在排名模型和分类模型之间进行选择可能会在不同群体中产生不同的访问模式，即使使用相同的基础数据。

公平性指标有助于正式化公平目标，但通常仅限于预定义的种群类别。在实践中，这些类别可能过于粗略，无法捕捉到现实世界数据中存在的全部差异。对公平性的原则性方法必须考虑到重叠和交叉身份，确保模型行为在可能事先未明确标记的子群体中保持一致。这一领域最近的研究强调了在广泛的人口切片中预测可靠性的必要性(Hébert-Johnson 等人 2018)，强化了公平性必须被视为系统级要求的观点，而不是局部调整。这种对公平性的扩展观点突出了设计架构、评估协议和监控策略的重要性，这些策略支持对模型行为进行更细致、更敏感的评估。

公平性考虑不仅限于算法结果，还包括部署负责任 AI 系统所需的计算资源和基础设施。当能源密集型 AI 基础设施集中在已经处于不利地位的社区时，这些更广泛的公平影响，包括环境正义问题，就会出现 13。

负责任 AI 技术的计算强度创造了一种数字鸿沟，其中对公平、透明和可问责 AI 系统的访问成为经济资源的附属品。实施公平性约束、差分隐私机制和全面的可解释性工具通常会增加计算成本，比无约束模型高出 15-40%。这创造了一种令人担忧的动态，只有拥有大量计算预算的组织才能负担得起部署真正负责任的 AI 系统，而资源受限的部署可能为了效率而牺牲道德保障。结果是，出现了一个双层系统，其中负责任的 AI 成为主要对资源充足的用户和应用可用的特权，可能加剧而不是解决现有的不平等。这些资源限制创造了民主化挑战，而更广泛的影响则创造了数字鸿沟和访问障碍，影响了服务不足的社区。

这些考虑因素指向一个基本结论：公平性是一个系统级属性，它源于数据工程实践、建模选择、评估程序和决策政策的相互作用。它不能孤立于单个模型组件，也不能仅通过事后调整来解决。负责任的机器学习设计需要将公平性视为一个基础约束，这一约束在整个系统的生命周期中指导架构选择、工作流程和治理机制。这种系统级观点扩展到所有负责任的 AI 原则，这些原则在 ML 生命周期中转化为具体的工程要求：公平性要求群体级性能指标和不同人群中的决策阈值；可解释性需要运行时计算预算，梯度方法的成本从 10-50 毫秒到 SHAP 分析的 50-1000 倍不等；隐私包括数据治理、同意机制和生命周期感知的保留策略；问责制需要可追溯性基础设施，包括模型注册、审计日志和人工覆盖机制。

这些原则在整个系统开发过程中相互作用并产生张力。隐私保护技术可能会降低可解释性；公平性约束可能与个性化相冲突；稳健的监控会增加计算成本。《表 17.1》中的表格显示了每个原则如何在数据收集、训练、评估、部署和监控阶段体现，强化了负责任 AI 不是部署后的考虑，而是一种架构承诺的观点。然而，实施这些原则的可行性严重依赖于部署环境：云、边缘、移动和 TinyML 环境各自施加不同的约束，这些约束塑造了哪些负责任 AI 功能在实践中是可实现的。

### 隐私和数据治理

隐私和数据治理带来的挑战超越了第十五章中提出的威胁模型视角，同时与上述考察的公平性和透明度原则产生了根本性的张力。以安全为重点的隐私询问“我们如何防止未经授权的访问？”负责任的隐私询问“我们是否应该收集这些数据，如果是的话，我们如何在整个系统生命周期中最大限度地减少暴露？”这种更广泛的视角产生了固有的张力：公平性监控需要收集和分析敏感的人口统计数据，可解释性方法可能会揭示训练示例的信息，而全面的透明度可能与个人隐私权相冲突。负责任的 AI 系统必须通过谨慎的设计选择来平衡保护、问责制和效用，以应对这些相互竞争的要求。

机器学习系统通常依赖于大量的个人数据集来支持模型训练并允许个性化功能。这种依赖引入了与用户隐私、数据保护和道德数据管理相关的重大责任。数据的质量和管理，在第六章中介绍，直接影响着实施负责任 AI 原则的能力。负责任的 AI 设计将隐私视为不是辅助功能，而是必须在整个系统生命周期中指导决策的核心约束。

在支持隐私方面，一个核心挑战是数据效用与个人保护之间的固有张力。丰富、高分辨率的数据库可以增强模型的准确性和适应性，但同时也增加了暴露敏感信息的风险，尤其是在数据集汇总或与外部来源关联时。例如，基于对话数据或医疗记录训练的模型已被证明会记住特定细节，这些细节可以通过模型查询或对抗性交互后来检索到（Ippolito 等人 2023)14。

隐私挑战不仅限于明显的敏感数据，还包括看似无害的信息。追踪生理和行为信号的可穿戴设备，包括心率、运动或位置，单个来看可能看似无害，但联合起来可以揭示详细的用户档案。当用户对他们的数据处理、保留或传输的可见性或控制有限时，这些风险会进一步加剧。

解决这些挑战需要将隐私视为一个系统原则，它包括强大的数据治理。这包括定义收集的数据、在什么条件下收集以及以何种程度的同意和透明度收集。在第六章（ch012.xhtml#sec-data-engineering）中讨论的基础数据工程实践为实施这些治理要求提供了技术基础设施。负责任的治理需要关注标签实践、访问控制、日志基础设施以及符合司法管辖要求。这些机制旨在限制数据在系统中的流动，并记录其使用的问责制。

为了支持该领域的结构化决策，图 17.1 展示了一个简化的流程图，概述了数据管道早期阶段的关键隐私检查点。它突出了核心保护措施，如同意获取、加密和差分隐私应该应用的地方。实际的实现往往涉及更微妙的权衡和情境敏感的决策，但此图提供了一个识别隐私风险出现位置以及如何通过负责任的设计选择来缓解这些风险的框架。

![图片](img/file290.svg)

图 17.1：**隐私感知数据流**：负责任的数据治理需要在机器学习管道的整个过程中采取主动保护措施，包括在关键决策点应用同意获取、加密和差分隐私机制，以减轻隐私风险并确保问责制。此图结构化了这些考虑因素，使设计者能够识别潜在漏洞并在数据收集、处理和存储期间实施适当的控制。

弱数据治理的后果已经得到了充分记录。在理解不充分或存在偏差的数据集上训练的系统可能会持续存在结构性不平等或无意中暴露敏感属性。在前面介绍的 COMPAS 示例中，数据来源和使用方面的不透明性阻止了有效的评估或纠正。在临床应用中，数据集通常反映诸如缺失值或人口统计偏差等伪影，这些伪影会损害性能和隐私。没有明确的数据质量和文档标准，这些漏洞会变得系统化。

隐私不仅仅是孤立算法或数据处理者的关注点；它必须作为系统结构属性来处理。关于同意收集、数据保留、模型设计和可审计性的决策都有助于机器学习管道的隐私状况。这包括在训练期间以及推理和持续运行期间预见风险的需要。如成员推理攻击 15 等威胁强调了将隐私保障嵌入到模型架构和界面行为中的重要性。

法律框架越来越多地反映了这种理解。例如，[GDPR](https://gdpr.eu)、[CCPA](https://oag.ca.gov/privacy/ccpa)16 和 [APPI](https://www.dataguidance.com/notes/japan-data-protection-overview) 等法规对数据最小化、目的限制、用户同意和删除权等方面提出了具体义务。这些要求将道德期望转化为可执行的设计约束，强化了在系统开发中将隐私视为核心原则的必要性。

这些隐私考虑最终汇聚成一个全面的方法：机器学习中的隐私是一个系统级的承诺。它需要技术和组织领域的协调，以确保数据使用与用户期望、法律要求和社会规范相一致。而不是将隐私视为与功能平衡的约束，负责任的设计从一开始就通过告知架构、塑造界面和限制模型构建、更新和部署的方式将隐私整合进去。

安全性和鲁棒性是负责任人工智能的另外两个关键维度。

### 安全性和鲁棒性

安全性和鲁棒性，在第十六章（ch022.xhtml#sec-robust-ai）中作为解决硬件故障、对抗攻击和分布变化的特性被引入，也作为负责任人工智能原则，其作用不仅限于威胁缓解。技术鲁棒性确保系统在对抗条件下能够生存；负责任鲁棒性确保系统即使在技术上功能正常的情况下，也能以符合人类期望和价值观的方式行事。一个模型可能对位翻转和对抗性扰动具有鲁棒性，但如果它在边缘情况下不可预测地失败或优化与用户福祉不一致的目标，那么它可能仍然表现出不适合部署的不安全行为。

机器学习中的安全性指的是在正常条件下模型行为可预测，以及在压力或不确定性下以可控、非灾难性的方式失败的保证。与安全性密切相关的是鲁棒性，它关注模型在输入、环境或系统配置变化的情况下维持稳定和一致性能的能力。这些属性共同构成了在安全关键领域负责任部署的基础，在这些领域，机器学习输出直接影响到物理或高风险决策。

在实践中确保安全和鲁棒性需要预测系统可能遇到的所有条件范围，并设计出在训练分布之外仍能保持可靠性的行为。这包括不仅管理输入的变异性，还要解决模型如何应对意外相关性、罕见事件和故意诱导失败尝试的问题。例如，广泛报道的自动驾驶系统故障揭示了在对象检测方面的局限性或过度依赖自动化如何导致有害后果，即使模型在正常测试条件下表现良好。

一种典型的失败模式源于对抗性输入 17：精心构造的扰动对人类看似无害，但会导致模型输出错误或有害的预测 (Szegedy 等人 2013b)。这种漏洞不仅限于图像分类；它们在包括音频、文本和结构化数据在内的多种模态中都有观察到，并揭示了在高维空间中学习表示的脆弱性。解决这些漏洞需要包括对抗性防御和鲁棒性技术在内的专门方法。这些行为凸显了鲁棒性不仅需要在训练期间考虑，而且作为系统与真实世界复杂性交互的全球属性。

相关的挑战是分布偏移：训练数据与部署中遇到的条件之间不可避免的错配。无论是因为季节性、人口统计变化、传感器退化还是环境变化，这种偏移甚至在没有对抗性操作的情况下也可能降低模型可靠性。解决分布偏移挑战需要系统地检测和适应变化条件的方法。在分布偏移下的失败可能会通过下游决策传播，引入超出模型准确度本身的安全风险。在医疗保健、金融或交通等领域，这些风险并非假设性的；它们对个人和机构都有真实的影响。

负责任的机器学习设计将鲁棒性视为系统要求。解决它需要不仅仅是提高单个模型性能。它涉及设计能够预测不确定性、揭示其局限性并在预测信心低时支持回退行为的系统。这包括设置信心阈值、支持放弃决策和将人工监督集成到操作流程中的做法。这些机制对于构建能够优雅退化而不是无声或不可预测地失败的系统至关重要。

这些针对单个模型的考虑延伸到更广泛的系统要求。安全和鲁棒性也在架构和组织层面提出了要求。关于如何监控模型、如何检测失败以及如何管理更新的决策都会影响系统是否能够有效地应对变化条件。负责任的设计要求将鲁棒性视为孤立模型的一个属性，而不是塑造机器学习系统整体行为的约束。

这种关于安全和鲁棒性的系统级视角引发了责任和治理的问题。

### 责任与治理

机器学习中的责任是指识别、归因和解决自动化决策后果的能力。它不仅超越了诊断失败，还确保系统行为的责任明确分配，损害可以得到补救，并且通过监督和机构程序维护道德标准。没有这样的机制，即使是有良好意图的系统也可能在没有补救措施的情况下造成重大伤害，损害公众信任并侵蚀合法性。

与责任通常由明确定义的开发者或操作者承担的传统软件系统不同，机器学习中的责任是分散的。模型输出受到上游数据收集、训练目标、管道设计、界面行为和部署后反馈的影响。这些相互关联的组件通常涉及技术、法律和组织领域的多个参与者。例如，如果招聘平台产生有偏见的成果，责任可能不仅在于模型开发者，还在于数据提供者、界面设计师和部署机构。负责任的设计要求这些关系被明确映射和治理。

管理不善可能会阻碍机构识别或纠正有害的模型行为。谷歌流感趋势未能预测分布变化和反馈循环的失败，说明了模型假设和更新政策的透明度不足如何会阻碍纠正措施。在没有对系统设计和数据整理的可见性情况下，外部利益相关者缺乏评估其有效性的手段，这导致了模型最终被停止使用。

法律框架越来越反映了对可问责设计的必要性。例如，[伊利诺伊州人工智能视频面试法案](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=4015&ChapterID=68)和[欧盟人工智能法案](https://artificialintelligenceact.eu/the-act/)等法规对高风险应用中的透明度、同意、文档和监督提出了要求。这些政策不仅将问责制嵌入到系统产生的结果中，还嵌入到支持其使用的操作程序和文档中。内部组织变革，包括引入公平审计和在定向广告系统中实施使用限制，展示了监管压力如何催化治理结构的结构性改革。

为可问责设计而设计意味着在系统生命周期的每个阶段都支持可追溯性。这包括记录数据来源、记录模型版本、启用人工干预，并保留足够的日志以供事后分析。例如，[模型卡片](https://arxiv.org/abs/1810.03993)18 和[数据集数据表](https://arxiv.org/abs/1803.09010)19 等工具展示了使系统行为可解释和可审查的实践。然而，问责制并不仅仅是文档；它还需要反馈、争议和纠正的机制。

在组织内部，治理结构有助于正式化这一责任。伦理审查流程、跨职能审计和模型风险委员会为预测下游影响和应对新兴问题提供了论坛。这些结构必须由允许用户提出争议和开发者提供更正的基础设施来支持。例如，允许解释或用户发起审查的系统有助于弥合模型逻辑与用户体验之间的差距，尤其是在错误影响重大的领域。

架构决策也发挥着作用。接口可以设计成在适当的时候揭示不确定性、允许升级或暂停自动化操作。日志和监控管道必须配置为检测道德漂移的迹象，如子群体间的性能下降或未预见的反馈循环。在分布式系统中，由于难以保持统一的可观察性，问责制必须通过如安全协议、更新约束或可信组件等架构保障来嵌入。

治理并不意味着集中控制。相反，它涉及以透明、可操作和可持续的方式分配责任。技术团队、法律专家、最终用户和机构领导者都必须能够访问评估系统行为和必要时进行干预所需的工具和信息。随着机器学习系统变得更加复杂并嵌入重要基础设施，问责制必须相应地扩展，成为架构和流程中的基础性考虑，而不是部署后的反应性层。

尽管有这些治理机制，有意义的问责制仍面临挑战：区分基于合法因素的决策与可能延续历史偏见的虚假相关性。这一挑战需要仔细关注数据质量、特征选择和持续监控，以确保自动化决策反映公平和合理的推理，而不是来自有偏历史数据的有问题模式。

上述原则和技术为负责任的 AI 提供了概念和技术基础，但它们的实际实施在很大程度上取决于部署架构。云系统可以支持复杂的 SHAP 解释和实时公平性监控，但 TinyML 设备必须依赖于静态可解释性和编译时隐私保证。边缘部署可以实现本地隐私保护，但限制了全球公平性评估。这些架构限制不仅仅是实现细节；它们从根本上塑造了不同用户和应用可访问的负责任的 AI 保护。

## 部署环境中的负责任 AI

由于计算、连接和治理方面的不同限制，负责任的 AI 原则在不同部署环境中表现出不同的形态。云系统支持全面的监控和复杂的可解释性方法，但通过数据聚合引入了隐私风险。边缘和移动部署提供了更强的数据本地性，但限制了部署后的可观察性。TinyML 系统面临最严重的限制，需要静态验证，没有运行时调整的机会。理解这些特定于部署的权衡，使工程师能够设计在架构约束内最大化负责任的 AI 保护的系统。

这些架构差异引入了权衡，不仅影响技术上的可行性，还影响系统组件间责任分配的方式。资源可用性、延迟限制、用户界面设计以及连接性的存在与否，都在决定是否能在部署环境中一致地实施负责任的 AI 原则中发挥作用。第二章中讨论的部署策略和系统架构为理解如何在这些多样化的环境中实施负责任的 AI 提供了基础。

除了这些技术约束之外，计算资源的地理和经济分布还在负责任的 AI 部署中创造了额外的公平性关注层。高性能 AI 系统通常需要靠近主要数据中心或高速互联网连接，这导致了服务质量差异，这些差异与现有的社会经济不平等密切相关。农村社区、发展中地区和经济不发达地区往往由于网络延迟、带宽有限和距离计算基础设施较远，而体验到 AI 服务质量下降 20。这种基础设施差距意味着，实时可解释性、持续公平性监控和隐私保护计算等负责任的 AI 原则在这些环境中可能实际上无法为用户所获得。

理解部署如何影响公平性、可解释性、安全性、隐私性和责任性在操作层面的布局，对于设计在现实世界环境中稳健、一致和可持续的机器学习系统非常重要。

### 系统可解释性

需要详细考察的第一个原则是可解释性，其在机器学习系统中的可行性深受部署环境的影响。虽然模型架构和解释技术是重要因素，但系统级约束，包括计算能力、延迟要求、界面设计和数据可访问性，决定了在特定环境中是否可以支持可解释性。这些约束在云平台、移动设备、边缘系统和深度嵌入式部署中差异很大，影响着解释的形式和时机。

在资源丰富的环境中，例如集中式云系统，可以使用 SHAP 和 LIME 等技术生成详细的事后解释，即使它们需要多次正向传递或采样过程。这些方法在延迟敏感或资源受限的环境中通常不切实际，在这些环境中，解释必须是轻量级且快速的。在移动设备或嵌入式系统中，基于显著性图 21 或输入梯度的方法更为可行，因为它们通常只涉及一次反向传递。在 TinyML 部署中，运行时解释可能根本不可行，这使得开发时检查成为确保可解释性的主要机会。模型压缩和优化技术通常与可解释性要求产生矛盾，因为简化模型可能不如其全规模模型可解释。

延迟性和交互性也会影响解释的传递。在实时系统中，例如无人机或自动化工业控制回路，在运行过程中可能没有机会展示或计算解释。记录内部信号或置信度分数以供后续分析成为主要策略。相比之下，具有异步交互的系统，如金融风险评估或医疗诊断，允许在做出决策后提供更深入和延迟的解释。

受众需求进一步塑造设计选择。最终用户通常需要简洁、直观且具有情境意义的解释。例如，一个移动健康应用可能会将预测总结为“睡眠期间心率升高”，而不是引用抽象的模型内部。相比之下，开发者、审计员和监管者通常需要访问归因图、概念激活或决策跟踪以进行调试、验证或合规审查。这些内部解释必须通过面向开发者的接口或嵌入到模型开发工作流程中公开。

可解释性也因系统生命周期而异。在模型开发期间，可解释性支持诊断、特征审计和概念验证。部署后，可解释性转向运行时行为监控、用户沟通和事后分析失败案例。在运行时解释不可行的系统中，例如在 TinyML 中，设计时验证变得尤为重要，需要以预测和减轻下游可解释性失败的方式构建模型。

将可解释性视为系统设计约束意味着从一开始就计划可解释性。它必须与其他部署要求相平衡，包括延迟预算、能源约束和界面限制。负责任的设计分配了足够的资源，不仅用于预测性能，还确保利益相关者可以在部署环境的操作限制内有意义地理解和评估模型行为。

公平性带来了一系列与部署相关的并行挑战。

### 公平性约束

虽然公平性可以正式定义，但其实施受到特定于部署的约束的影响，这些约束反映了并扩展了与可解释性相关的挑战。数据访问、模型个性化、计算能力、监控或重新训练的基础设施等方面的差异影响了公平性如何在不同系统架构中得到评估、执行和维持。

一个关键决定因素是数据可见性。在集中式环境中，例如云托管平台，开发者通常可以访问带有人口统计注释的大型数据集。这允许使用组级公平性指标、公平性感知训练程序和事后审计。相比之下，去中心化部署，如联邦学习 22 客户端或移动应用程序，通常由于隐私约束或数据碎片化而缺乏访问全局统计数据的权限。设备上的学习方法在公平性评估方面提出了独特的挑战，因为单个设备可能对全球人口分布的可见性有限。在这种情况下，公平性干预措施通常必须在训练或数据集整理期间嵌入，因为部署后的评估可能不可行。

个性化适应机制也影响公平性权衡。向所有用户提供全局模型的系统可能旨在实现人口统计群体间的平等。相比之下，嵌入在健康监测应用程序或设备推荐引擎中的本地适应模型可能旨在实现个体公平，确保对类似用户的一致待遇。然而，在没有明确的相似性指标或代表性用户数据的情况下，执行这一点具有挑战性。基于本地行为重新训练的个性化系统可能会偏向于强化现有的差异，尤其是在边缘化用户的数据稀疏或噪声时。

实时和资源受限的环境带来了额外的限制。嵌入式系统、可穿戴设备或实时控制平台通常无法支持运行时公平性监控或动态阈值调整。在这些情况下，必须通过保守的设计选择来积极解决公平性问题，包括平衡的训练目标和在部署前的静态子群性能评估。例如，部署在低功耗可穿戴设备上的语音识别系统可能需要在设计时确保跨不同口音的稳健性能，因为部署后的重新校准是不可能的。

决策阈值和系统策略也会影响实现的公平性。即使模型在各个群体中的表现相似，如果分数分布不同，在整个用户中应用统一的阈值可能会导致不同的影响。例如，一个移动贷款审批系统，除非考虑特定群体的阈值，否则可能会系统地低估某一群体。这样的决策必须明确推理、证明，并在部署之前嵌入到系统策略逻辑中。

长期公平性还受到反馈动态的影响。包括排名模型、推荐系统和自动化决策流程在内的系统，如果反馈循环没有得到妥善管理，可能会加强历史偏见。例如，一个在偏见历史结果上重新训练时，不成比例地偏爱来自特定机构的候选人的招聘平台可能会放大现有的不平等。减轻这种影响需要治理机制，不仅涵盖训练，还包括部署监控、数据记录和影响评估。

公平性，像其他负责任的 AI 原则一样，不仅限于模型参数或训练脚本。它源于整个系统生命周期中的一系列决策：数据采集、模型设计、策略阈值、重新训练基础设施和用户反馈处理。将公平性视为系统级约束，特别是在受限或去中心化部署中，需要预测可能出现权衡的地方，并确保公平性目标从一开始就嵌入到架构、决策规则和生命周期管理中。

公平性面临的部署挑战也扩展到隐私架构，其中在集中控制和分布式约束之间出现了类似的紧张关系。

### 隐私架构

机器学习系统中的隐私扩展了公平性的观察模式：它不仅限于保护个人记录；它还受到数据收集、存储、传输以及集成到系统行为中的方式的影响。这些决策与部署架构紧密相连。系统级隐私约束因模型是否托管在云端、嵌入在设备上或分布在使用者控制的多个环境中而大相径庭，每种部署方式都面临着在保持功能的同时最小化风险的不同挑战。

关键的架构区别在于集中式和去中心化数据处理。集中式云系统通常在规模上聚合数据，从而实现高容量的建模和监控。然而，这种聚合增加了数据泄露和监控的风险，使得强大的加密、访问控制和可审计性变得重要。在去中心化部署中，包括移动应用程序、联邦学习客户端和 TinyML 系统，数据保持本地化，降低了中央风险，但限制了全局可观察性。这些环境通常阻止开发者访问用于监控系统性能或执行合规性所需的人口统计或行为统计数据，因此在开发期间需要嵌入隐私保护措施。

在随时间个性化行为的系统中，隐私挑战尤为突出。例如，智能键盘、健身追踪器或语音助手等应用程序会通过处理敏感信号（如位置、打字模式或健康指标）来持续适应用户。即使原始数据被丢弃，训练好的模型也可能保留用户特定的模式，这些模式可以通过推理时间查询恢复。在内存持久且交互频繁的架构中，管理长期隐私需要将保护机制紧密集成到模型的生命周期中。

连接性假设进一步塑造了隐私设计。云连接的系统允许集中执行加密协议和远程删除策略，但可能会引入延迟、能源开销或在数据传输过程中增加暴露。相比之下，边缘系统通常在离线或间歇性运行，使得隐私执行依赖于架构约束，如特征最小化、本地数据保留和编译时混淆。在缺乏持久存储或更新通道的 TinyML 设备上，隐私必须被设计到静态固件和模型二进制文件中，从而在部署后没有调整的机会。

隐私风险也扩展到服务和监控层。如果日志基础设施没有隐私意识，允许日志记录或通过主动学习进行更新的模型可能会无意中泄露敏感信息。例如，成员推理攻击可以通过分析模型输出来揭示用户数据是否包含在训练中。防御此类攻击需要隐私保护措施不仅扩展到训练，还要扩展到界面设计、速率限制和访问控制。

隐私不是由技术机制决定的，而是由用户如何体验系统决定的。一个模型可能符合正式的隐私定义，但如果数据收集不透明或缺乏解释，仍然可能违反用户期望。界面设计起着核心作用：系统必须清楚地传达收集的数据、如何使用这些数据以及用户如何选择退出或撤销同意。在隐私敏感的应用中，如果与用户规范不一致，即使在技术上合规的系统也可能损害信任。

建筑决策因此影响数据生命周期的每个阶段，从获取和预处理到推理和监控的隐私。为了隐私而设计不仅涉及选择安全的算法，还需要根据部署约束、用户需求和法律义务做出原则性的权衡。在高资源环境中，这可能涉及集中执行和政策工具。在受限环境中，隐私必须静态地嵌入到模型设计和系统行为中，通常没有动态监督的可能性。

隐私不是部署后附加的功能。它是一个系统级属性，必须与部署环境的架构现实一起规划、实施和验证。

补充隐私对数据保护的关注，安全性和鲁棒性架构确保系统即使在隐私机制无法防止所有风险的情况下也能表现出可预测的行为。虽然隐私防止未经授权的数据泄露，但安全性确保在压力下系统输出保持可靠并与人类期望一致。

### 安全性和鲁棒性

在机器学习系统中实现安全性和鲁棒性受到部署架构的紧密影响。在动态、不可预测的环境中部署的系统，包括自动驾驶汽车、医疗机器人和智能基础设施，必须管理实时不确定性并减轻高影响故障的风险。其他系统，如嵌入式控制器或设备上的机器学习系统，需要在资源受限、可观察性有限和恢复机会受限的情况下实现稳定和可预测的操作。在所有情况下，安全性和鲁棒性都是系统级属性，不仅取决于模型质量，还取决于在部署中如何检测、控制和处理故障。

一个反复出现的问题是分布偏移：当部署条件与训练期间遇到的条件不同时。即使输入特征（包括照明、传感器噪声或环境变化）的微小变化，如果没有对不确定性进行建模或监控，也可能显著降低性能。在缺乏运行时监控或回退机制的架构中，这种退化可能直到发生故障才被发现。旨在处理现实世界变化的系统必须设计成能够识别输入是否超出预期分布，并根据情况相应地重新校准或推迟决策。

对抗鲁棒性引入了额外的架构考虑因素。在涉及安全敏感决策的系统（如欺诈检测、内容审查和生物识别验证）中，对抗性输入可能会损害可靠性。缓解这些威胁可能涉及模型级别的防御（例如，对抗性训练、输入过滤）和部署级别的策略，如 API23 访问控制、速率限制或输入验证的冗余。这些保护通常需要在延迟和复杂性之间进行权衡，必须仔细平衡与实时性能要求。

对延迟敏感的部署进一步限制了鲁棒性策略。在自主导航、实时监控或控制系统，必须在严格的时序预算内做出决策。重型鲁棒性机制可能不可行，并且必须预先定义回退操作。许多此类系统依赖于置信度阈值、弃权逻辑或基于规则的覆盖来降低风险。例如，送货机器人只有在行人检测置信度足够高时才会继续前进；否则，它会暂停或转交给人工监督。这些控制策略通常位于学习模型之外，但必须紧密集成到系统的安全逻辑中。

TinyML 部署引入了额外的约束。这些系统部署在具有最小内存、没有操作系统和没有连接性的微控制器上，不能依赖于运行时监控或远程更新。安全和鲁棒性必须通过保守的设计、广泛的预部署测试以及使用本质上简单和可预测的模型来静态地设计。一旦部署，系统必须在传感器退化、电源波动或环境变化等条件下可靠地运行，而不需要外部干预或动态校正。

在所有部署环境中，监控和升级机制对于在长时间内维持稳健行为至关重要。在云或高资源设置中，系统可能包括不确定性估计器、分布变化检测器或人工反馈循环，以检测故障条件并触发恢复。在更受限的环境中，这些机制必须简化或预先计算，但原则依然不变：稳健性不是一次性实现的，而是通过持续识别和应对新兴风险的能力来维持的。

安全和稳健性必须被视为新兴的系统属性。它们取决于输入是如何被感知和验证的，输出是如何被处理的，故障条件是如何被识别的，以及纠正措施是如何被启动的。一个稳健的系统不是避免所有错误的系统，而是一个能够明显、可控、安全地失败的系统。在安全至关重要的应用中，设计这种行为是强制性的，而不是可选的。

这些安全和稳健性考虑导致了对治理和问责制的疑问，这些疑问也必须适应部署限制。

### 治理结构

在机器学习系统中，问责制必须通过具体的架构选择、界面设计和操作程序来实现。治理结构通过定义谁对系统结果负责、在什么条件下以及通过什么机制负责，使责任变得可执行。这些结构深受部署架构的影响。问责制可追溯、审计和执行的程度在集中式、移动、边缘和嵌入式环境中各不相同，每个环境都为维持系统监督和完整性提出了独特的挑战。

在集中式系统中，例如云托管平台，治理通常由强大的日志记录、版本控制和实时监控基础设施支持。模型注册表、遥测 25 仪表板和结构化事件管道允许团队追踪预测到特定的模型、数据输入或配置状态。

相比之下，边缘部署将智能分布到可能独立于集中式基础设施运行的设备。在车辆、工厂或家庭中的嵌入式模型必须支持本地机制来检测异常行为、触发警报和升级问题。例如，一个工业传感器可能会在其预测置信度下降时标记异常，启动预定义的升级流程。为这种自主性设计需要深思熟虑：工程师必须确定要捕获哪些信号，如何本地存储它们，以及当连接断开或延迟时如何重新分配责任。

移动部署，如个人金融应用或数字健康工具，存在于用户界面和后端系统之间的交叉点。当出现问题的时候，往往不清楚问题出在本地模型、远程服务还是更广泛的用户交互设计上。在这些环境中，治理必须考虑到这种不确定性。有效的责任需要清晰的文档、可访问的救济途径，以及向用户层面展示、解释和质疑自动化决策的机制。理解和申诉结果的能力必须嵌入到界面和周围的服务架构中。

在 TinyML 部署中，治理受到特别限制。设备可能缺乏连接性、持久存储或运行时可配置性，这限制了动态监督或干预的机会。在这里，责任必须通过诸如加密固件签名、固定审计跟踪和部署前训练数据和模型参数的文档等机制静态嵌入。在某些情况下，治理必须在制造或配置期间强制执行，因为部署后无法进行纠正。这些限制使得治理结构的设计与早期架构决策不可分割。

界面在启用责任方面也发挥着重要作用。能够展示解释、暴露不确定性估计或允许用户查询决策历史的系统，使得开发者、审计员或用户能够理解发生了什么以及为什么发生。相比之下，不透明的 API、未记录的阈值或闭环决策系统阻碍了监督。有效的治理需要信息流与利益相关者的需求对齐，包括技术、监管和面向用户方面，以便失败模式是可观察和可修复的。

治理方法还必须适应特定领域的风险和制度规范。高风险应用，如医疗保健或刑事司法，通常涉及法律规定的影响评估和审计跟踪。低风险领域可能更多地依赖内部实践，这些实践由客户期望、声誉担忧或技术惯例塑造。无论在何种情况下，治理都必须被视为系统级设计属性，而不是外部政策叠加。它是通过代码库的结构、部署管道、数据流和决策接口来实现的。

维护不同部署环境中的责任需要规划不仅针对成功，也针对失败。这包括定义如何检测异常、如何分配角色、如何维护记录以及如何进行补救。这些流程必须嵌入到基础设施中：在日志中可追踪、通过接口可强制执行，并能够抵御系统部署环境中的架构限制。

负责任的 AI 治理越来越必须考虑计算基础设施选择的环境和分配影响。部署 AI 系统的组织不仅对算法结果负责，还要对其资源利用模式对环境正义和公平获取的更广泛系统性影响负责，正如在资源需求和公平影响背景下所讨论的。

### 设计权衡

在不同的部署环境中考察的治理挑战揭示了基本真理：部署环境施加了基本约束，这些约束在负责任的 AI 实施中创造了权衡。机器学习系统不在理想化的孤岛中运行；它们必须在有限资源、严格的延迟要求、不断变化的使用行为和监管复杂性下，在相互竞争的目标之间进行导航。

由于充足的计算和存储资源，基于云的系统通常支持广泛的监控、公平性审计、可解释性服务和隐私保护工具。然而，这些好处通常伴随着集中式数据处理，这引入了与监控、数据泄露和复杂治理相关的风险。相比之下，移动应用程序、边缘平台或 TinyML 部署等设备系统提供了更强的数据本地性和用户控制，但限制了部署后的可见性、公平性工具和模型适应性。

目标之间的紧张关系通常在架构层面变得明显。例如，对于需要实时响应的系统，如可穿戴手势识别或自动制动，在推理过程中无法承担计算详细的可解释性解释。设计者必须选择是否预先计算简化的输出，将解释推迟到异步分析，或在运行时设置中完全省略可解释性。

个性化与公平性之间也出现了冲突。基于本地使用数据对个人进行适应的系统通常缺乏评估人口子群体间差异所需的全球背景。确保个性化预测不会导致系统性的排斥，需要仔细的架构设计，平衡用户级别的适应性与群体级别的公平性和可审计性机制。

隐私性和鲁棒性目标也可能存在冲突。鲁棒的系统通常从记录罕见事件或用户异常数据中受益，以提高可靠性。然而，记录此类数据可能与隐私目标相冲突，或违反数据最小化的法律限制。在敏感行为必须保持本地或加密的环境中，鲁棒性必须预先设计到模型架构和训练过程中，因为事后的改进可能不可行。

负责任的 AI 的计算需求产生了超越技术优化的紧张关系，涉及到环境正义和公平获取的问题。节能部署通常需要简化模型并减少公平性监控能力，这就在环境可持续性和道德保障之间产生了权衡。例如，在联邦学习中实施差分隐私可能会使每台设备的能耗增加 25-40%，这可能会使这种隐私保护对电池受限的设备来说变得不可承受 26。

这些例子说明了更广泛的系统级挑战。负责任的 AI 原则不能孤立考虑。它们相互作用，优化一个可能会限制另一个。适当的平衡取决于部署架构、利益相关者的优先级、特定领域的风险、错误的后果，以及越来越重要的是计算资源需求的环境和分配影响。

负责任的机器学习设计区别于其他设计的不是消除权衡，而是在于清晰和深思熟虑地处理这些权衡。设计决策必须透明化，全面理解部署环境带来的限制以及这些决策对系统行为的影响。

为了综合这些见解，表 17.2 通过比较负责任的 AI 原则如何在云、移动、边缘和 TinyML 系统中体现，总结了架构上的紧张关系。每个设置都根据计算能力、连接性、数据访问和治理可行性等因素，对可解释性、公平性、隐私、安全性和问责制施加不同的限制。

如表 17.2 所示，没有哪个部署环境在所有原则上都占主导地位；每个都做出了不同的妥协。云系统支持复杂的可解释性方法（SHAP、LIME）和集中式公平性监控，但通过数据聚合引入了隐私风险。边缘和移动部署提供了更强的数据本地性，但限制了部署后的可观察性和全球公平性评估。TinyML 系统面临最严重的限制，需要静态验证和编译时隐私保证，没有运行时调整的机会。这些限制不仅仅是技术限制，它们决定了不同用户和应用可访问的负责任 AI 功能，创造了只有资源充足的部署才能负担得起全面保障的公平性问题。理解这些部署限制为在实践中实现负责任的 AI 原则的技术方法提供了必要的背景。

表 17.2：**部署权衡**：由于计算、连接和治理的限制不同，负责任的 AI 原则在不同部署环境中表现出不同的形式；云部署支持复杂的可解释性方法，而微型机器学习则严重限制了这些方法。优先考虑某些原则，如可解释性、公平性、隐私、安全性和问责制，在设计云、边缘、移动和微型机器学习环境中的机器学习系统时，需要仔细考虑这些限制。

| **原则** | **云机器学习** | **边缘机器学习** | **移动机器学习** | **微型机器学习** |
| --- | --- | --- | --- | --- |
| **可解释性** | 支持复杂模型和方法，如 SHAP 和采样方法 | 需要轻量级、低延迟的方法，如显著性图 | 需要用户可解释的输出，通常将更深入的分析推迟到云端 | 由于硬件限制，严重受限；大多数情况下仅限于静态或编译时 |
| **公平性** | 大数据集允许检测和缓解偏差 | 本地偏差难以检测，但允许设备上的调整 | 高度个性化使群体层面的公平性跟踪复杂化 | 数据量最小限制了偏差分析和缓解 |
| **隐私** | 集中的数据可能遭受泄露，但可以利用强大的加密和差分隐私方法 | 设备上的敏感个人数据需要设备保护 | 与用户身份的紧密耦合需要知情同意的设计和本地处理 | 分布式数据减少了集中化风险，但给匿名化带来了挑战 |
| **安全性** | 易受黑客攻击和大规模攻击 | 现实世界的交互使可靠性变得重要 | 在用户监督下运行，但仍需要优雅的失败处理 | 由于自主性，需要分布式安全机制 |
| **问责制** | 公司政策和审计允许追溯和监督 | 分散的供应链使问责制复杂化 | 需要清晰的面向用户的披露和反馈路径 | 需要在长而复杂的硬件链中实现可追溯性 |
| **治理** | 外部监督和法规，如 GDPR 或 CCPA 是可行的 | 需要开发者和集成商的自我治理 | 平衡平台政策与应用开发者选择 | 依赖于内置协议和加密保证 |

## 技术基础

负责任的机器学习需要将伦理原则转化为具体系统行为的技术方法。这些方法解决实际挑战：检测偏差、保护隐私、确保鲁棒性，并提供可解释性。成功取决于这些技术在包括数据质量、计算资源和部署要求在内的实际系统约束下的工作效果。

了解为什么这些方法是必要的，首先要认识到机器学习系统如何发展出问题行为。模型从训练数据中学习模式，包括历史偏见和不公平的关联。例如，在具有偏见的历史数据上训练的招聘算法将学会复制歧视性模式，将某些人口统计特征与成功相关联。

这是因为机器学习模型学习的是相关性而不是因果关系。它们识别的统计模式可能反映了不公平的社会结构，而不是有意义的关系。这种系统性偏差有利于在训练数据中历史上处于优势地位的群体。

解决这些问题需要训练之后的不仅仅是简单的修正。传统的机器学习仅优化准确性，与公平性目标产生冲突。有效的解决方案必须将公平性考虑直接整合到学习过程中，而不是将其视为次要问题。

每种技术方法都涉及准确性、计算成本和实现复杂度之间的特定权衡。这些方法并非普遍适用，必须根据系统需求和约束来选择。框架选择影响哪些负责任的人工智能技术可以实际实施。

本节探讨了实施负责任人工智能原则的实用技术。每种方法在系统中都有特定的目的，并带有特定的要求和性能影响。这些工具共同工作，以创建值得信赖的机器学习系统。

负责任人工智能的技术方法可以分为三个互补的类别。检测方法识别系统何时表现出问题行为，为偏差、漂移和性能问题提供早期预警系统。缓解技术通过算法干预和鲁棒性增强积极预防有害结果。验证方法为理解并解释系统行为提供机制，以便评估自动化决策的利益相关者。

#### 负责任人工智能技术的计算开销

实施负责任的人工智能原则会带来可量化的计算成本，这些成本必须在系统设计阶段考虑。了解这些性能影响使工程师能够根据可用的计算资源和质量要求，做出关于实施哪些技术的明智决策。表 17.3 提供了不同负责任人工智能技术引入的计算开销的系统比较。

表 17.3：**负责任人工智能技术性能影响**：定量分析显示，负责任人工智能技术在训练和推理阶段都产生了可衡量的计算开销。差分隐私和公平性约束增加了适度的开销，而可解释性方法可以显著增加推理成本。这些指标有助于工程师针对生产约束优化负责任人工智能的实现。

| **技术** | **准确度影响** | **训练开销** | **推理成本** | **内存开销** |
| --- | --- | --- | --- | --- |
| **差分隐私** | -2%到-5% | +15%到+30% | 最小 | +10%到+20% |
| **（DP-SGD）** |  |  |  |  |
| **公平性感知训练** | -1%到-3% | +5%到+15% | 最小 | +5%到+10% |
| **（重新加权/约束）** |  |  |  |  |
| **SHAP 解释** | N/A | N/A | +50%到+200% | +20%到+100% |
| **对抗训练** | +2%到+5% | +100%到+300% | 最小 | +50%到+100% |
| **联邦学习** | -5%到-15% | +200%到+500% | 最小 | +100%到+300% |

表 17.3 中的性能数字代表了已发布基准和实际系统中的典型范围。27 实际开销根据模型架构、数据集大小和实现质量而有很大差异。例如，SHAP 在线性模型上增加大约 10 ms，而在深度集成模型上可以增加超过 1000 ms。对抗训练的开销取决于攻击强度：PGD-7 增加大约 150%的开销，而 PGD-50 增加大约 300%。联邦学习的开销主要由通信轮次和客户端异构性主导。

这些计算成本在多个背景下引发了重大的公平性考虑。资源有限的组织可能无法实施负责任人工智能技术，这可能导致对道德人工智能保护的差异访问，这是一个在部署环境、实施挑战和组织障碍中反复出现的话题。

检测方法是所有其他负责任人工智能干预措施的基础。

### 偏差和风险检测方法

检测方法为识别机器学习系统出现损害负责任人工智能原则的问题行为提供了基础能力。这些技术作为早期预警系统，在它们造成重大损害之前，提醒从业者注意偏差、漂移和性能退化。

#### 偏差检测与缓解

在部署系统中实现公平性需要不仅仅是原则性目标或理论指标；它需要系统感知的方法，这些方法可以在机器学习生命周期中检测、测量和缓解偏差。实际偏差检测可以使用 Fairlearn28 (Bird 等，2020)等工具实现：

列表 17.1：**使用 Fairlearn 进行偏差检测**：对贷款审批模型性能在人口群体间的系统性评估揭示了潜在的批准率和假阳性率差异，这些差异可能表明需要干预的歧视性模式。

```py
from fairlearn.metrics import MetricFrame
from sklearn.metrics import accuracy_score, precision_score

# Loan approval model evaluation across demographic groups
mf = MetricFrame(
    metrics={
        "approval_rate": accuracy_score,
        "precision": precision_score,
        "false_positive_rate": lambda y_true, y_pred: (
            (y_pred == 1) & (y_true == 0)
        ).sum()
        / (y_true == 0).sum(),
    },
    y_true=loan_approvals_actual,
    y_pred=loan_approvals_predicted,
    sensitive_features=applicant_demographics["ethnicity"],
)

# Display performance disparities across ethnic groups
print("Loan Approval Performance by Ethnic Group:")
print(mf.by_group)
# Output shows: Asian: 94% approval, White: 91% approval,
# Hispanic: 73% approval, Black: 68% approval
```

如列表 17.1 所示，这种方法使部署期间对人口群体间的公平性进行系统性监控成为可能，揭示了令人担忧的差异，其中贷款批准率因种族而异：亚洲申请人的批准率为 94%，而黑人申请人的批准率为 68%。基于之前讨论的系统级约束，公平性必须被视为一个与数据工程、模型训练、推理设计、监控基础设施和政策治理交叉的架构考虑因素。虽然人口统计平等、均衡机会和机会均等等公平性指标形式化了不同的规范性目标，但它们的实现取决于架构测量子群体性能、支持自适应决策边界以及在运行时存储或呈现特定于组的元数据的能力。

实际实施往往受到数据访问和系统仪表化的限制。在许多现实世界环境中，尤其是在移动、联邦或嵌入式系统中，敏感属性如性别、年龄或种族可能在推理时不可用，这使得跟踪或审计模型性能跨人口群体变得困难。第六章中讨论的数据收集和标注策略对于整个模型生命周期的公平性评估至关重要。在这种情况下，公平干预必须在数据整理或训练阶段上游进行，因为部署后的重新校准可能不可行。即使数据可用，结合用户反馈的持续重新训练管道也可能加剧现有差异，除非明确监控公平性下降。例如，一个适应用户行为的设备上推荐模型，如果缺乏检测用户交互或输出中人口统计不平衡的基础设施，可能会放大先前的偏见。

![图 17.2](img/ch023.xhtml#fig-fairness-example)说明了公平约束如何与部署选择产生紧张关系。在一个二元贷款审批系统中，两个子组，子组 A 用蓝色表示，子组 B 用红色表示，需要不同的决策阈值来实现相同的真正阳性率。在组间使用单个阈值会导致不同的结果，可能对子组 B 不利。通过调整每个组的阈值来解决这种不平衡可能提高公平性，但这需要支持模型服务堆栈中的条件逻辑，在推理时访问敏感属性，以及一个治理框架来解释和证明跨组差异处理的合理性。

![图片](img/file291.svg)

图 17.2：**阈值依赖公平性**：在子群体中变化分类阈值允许相同的真正阳性率，但引入了模型服务的复杂性，并在推理时需要访问敏感属性。实现公平性需要对特定子群体的性能进行仔细考虑，因为单个阈值可能不成比例地影响某些群体，突显了机器学习系统中准确性和公平结果之间的紧张关系。

公平性干预可以在管道的不同点应用，但每个都伴随着系统级的影响。预处理方法，通过采样、重新加权或增强来重新平衡训练数据，需要访问原始特征和群体标签，通常通过一个保留谱系的特征存储或数据湖。这些方法非常适合具有集中式训练管道和高质量标记数据的系统。相比之下，内处理方法将公平性约束直接嵌入到优化目标中。这些需要能够支持自定义损失函数或约束求解器的训练基础设施，可能需要更长的训练周期或额外的正则化验证。第八章中讨论的训练技术和优化方法为实施这些公平性感知的训练方法提供了基础。

后处理方法，包括应用特定群体的阈值或调整分数以实现结果均衡，需要推理系统能够根据敏感属性或参考外部政策规则进行条件化。这要求在模型服务基础设施、访问控制策略和日志管道之间进行协调，以确保差异化的处理既可审计又合法。第二章中涵盖的模型服务架构详细说明了在生产系统中实现这种条件逻辑所需的基础设施要求。任何后处理策略都必须经过仔细验证，以确保它不会损害用户体验、模型稳定性或遵守关于属性使用的司法管辖区法规。

可扩展的公平性执行通常需要更高级的策略，例如多校准 29，这确保模型预测在广泛的相交子群体中保持校准（Hébert-Johnson 等人 2018）。

在大规模实施多校准需要动态生成子群划分的基础设施、计算每组校准误差以及将公平性审计集成到自动化监控系统。这些能力通常仅在具有成熟可观察性和度量管道的大规模、云基础部署中可用。在嵌入式或 TinyML 系统等受限环境中，由于遥测有限且模型逻辑固定，这些技术不可行，公平性必须在设计时完全验证。

在不同的部署环境中，保持公平性需要生命周期感知机制。模型更新、反馈循环和界面设计都会影响公平性随时间的变化。如果重新训练管道不包括公平性检查，如果日志系统无法跟踪子群结果，或者如果用户反馈引入了训练分布未捕捉到的微妙偏差，则公平感知模型可能会退化。监控系统必须能够揭示公平性退化，重新训练协议必须能够访问带有子群标签的验证数据，这可能需要数据治理政策和伦理审查。实施这些监控系统需要 MLOps 实践的生产基础设施，而隐私保护技术对于联邦公平性评估至关重要。

公平性不是一次性的优化，也不是模型独立属性。它源于数据采集、特征工程、模型设计、阈值设置、反馈处理和系统监控等方面的协调决策。将公平性嵌入机器学习系统需要架构前瞻性、操作纪律以及贯穿整个部署堆栈的工具，从训练工作流到服务基础设施再到面向用户的界面。

偏见检测的社会技术影响远远超出了技术测量的范畴。当公平性指标识别出差异时，组织必须像在第 17.6.3 节中考察的那样，导航复杂的利益相关者审议过程。这些决策涉及相互竞争的利益相关者利益、法律合规要求和价值权衡，这些不能仅通过技术手段解决。

#### 实时公平性监控架构

在生产系统中实施负责任的 AI 原则需要将公平性监控、可解释性和隐私控制直接集成到模型服务基础设施中的架构模式。图 17.3 展示了如何将负责任的 AI 组件与现有的 ML 系统基础设施集成的参考架构。

![图片](img/file292.svg)

图 17.3：**生产负责任 AI 架构**：实时公平性监控需要集成组件，这些组件通过数据处理匿名化、偏差检测和解释生成来处理每个推理请求，同时维护审计跟踪，并在公平性阈值被违反时触发警报。虚线显示了基于检测到的偏差模式进行模型更新的反馈循环。

此架构通过几个关键组件解决了专家确定的生产现实，这些组件协同工作以实现大规模的负责任 AI：

数据匿名化层在模型推理之前实现隐私保护转换，使用诸如 k-匿名性 30 或差分隐私噪声注入等技术。该组件为每个请求增加 2-5 毫秒的延迟，但提供正式的隐私保证。由于加密和噪声生成需求，内存开销通常为 15-25%。

实时公平性监控跟踪每个预测的种群统计和均衡概率指标，在受保护群体中维护滚动统计。系统标记超过可配置阈值的差异（例如，批准率差异超过 5%）。这种监控增加了 10-20 毫秒的延迟，并需要 100-500 MB 的额外内存用于指标存储和计算。

解释引擎为模型决策生成 SHAP 或 LIME 解释，特别是对于需要用户干预的负面结果。快速近似方法将解释延迟从 200-500 毫秒（完整 SHAP）减少到 20-50 毫秒（流式 SHAP），同时保持 90% 的保真度。由于梯度计算和特征重要性缓存，内存需求增加 50-100%。

**实现深度解析**

以下代码示例展示了生产就绪的公平性监控和实时偏差检测。这代表了一个参考实现，展示了架构模式而不是需要记忆的代码。重点理解：（1）公平性指标如何集成到服务基础设施中，（2）实现管理了哪些性能权衡，（3）当阈值超过时如何触发警报。在构建类似系统时，您可以返回到实现细节。

列表 17.2 展示了一个将组件集成到实时监控系统中的生产实现：

列表 17.2：**生产公平性监控实现**：实时偏差检测系统，处理推理请求，计算公平性指标，并在差异超过阈值时触发警报，展示了负责任的 AI 如何与生产机器学习服务基础设施集成。

```py
import asyncio
from dataclasses import dataclass
from typing import Dict, List, Optional
import numpy as np
from sklearn.metrics import confusion_matrix


@dataclass
class FairnessMetrics:
    demographic_parity_diff: float
    equalized_odds_diff: float
    equality_opportunity_diff: float
    group_counts: Dict[str, int]


class RealTimeFairnessMonitor:
    def __init__(
        self, window_size: int = 1000, alert_threshold: float = 0.05
    ):
        self.window_size = window_size
        self.alert_threshold = alert_threshold
        self.predictions_buffer = []
        self.demographics_buffer = []
        # For actual outcomes when available
        self.labels_buffer = []

    async def process_prediction(
        self,
        prediction: int,
        demographics: Dict[str, str],
        actual_label: Optional[int] = None,
    ) -> FairnessMetrics:
        """Process single prediction and update fairness metrics"""
```

```py
        # Store in rolling window buffer
        self.predictions_buffer.append(prediction)
        self.demographics_buffer.append(demographics)
        if actual_label is not None:
            self.labels_buffer.append(actual_label)

        # Maintain window size
        if len(self.predictions_buffer) > self.window_size:
            self.predictions_buffer.pop(0)
            self.demographics_buffer.pop(0)
            if self.labels_buffer:
                self.labels_buffer.pop(0)

        # Compute fairness metrics
        metrics = self._compute_fairness_metrics()

        # Check for bias alerts
        if (
            metrics.demographic_parity_diff > self.alert_threshold
            or metrics.equalized_odds_diff > self.alert_threshold
        ):
            await self._trigger_bias_alert(metrics)

        return metrics

    def _compute_fairness_metrics(self) -> FairnessMetrics:
        """Compute demographic parity and equalized odds"""
        """across groups"""
        if len(self.predictions_buffer) < 100:  # Minimum sample size
            return FairnessMetrics(0.0, 0.0, 0.0, {})

        # Group predictions by protected attribute
        groups = {}
        for i, demo in enumerate(self.demographics_buffer):
            group = demo.get("ethnicity", "unknown")
            if group not in groups:
                groups[group] = {"predictions": [], "labels": []}
            groups[group]["predictions"].append(
                self.predictions_buffer[i]
            )
            if i < len(self.labels_buffer):
                groups[group]["labels"].append(self.labels_buffer[i])

        # Compute demographic parity (approval rates)
        approval_rates = {}
        for group, data in groups.items():
            if len(data["predictions"]) > 0:
                approval_rates[group] = np.mean(data["predictions"])

        demo_parity_diff = (
            max(approval_rates.values())
            - min(approval_rates.values())
            if len(approval_rates) > 1
            else 0.0
        )

        # Compute equalized odds (TPR/False Positive Rate
        # differences) if labels available
        eq_odds_diff = 0.0
        eq_opp_diff = 0.0

        if self.labels_buffer and len(groups) > 1:
            tpr_by_group = {}
            fpr_by_group = {}

            for group, data in groups.items():
                if (
                    len(data["labels"]) > 10
                ):  # Minimum for reliable metrics
                    tn, fp, fn, tp = confusion_matrix(
                        data["labels"], data["predictions"]
                    ).ravel()
                    tpr_by_group[group] = (
                        tp / (tp + fn) if (tp + fn) > 0 else 0
                    )
                    fpr_by_group[group] = (
                        fp / (fp + tn) if (fp + tn) > 0 else 0
                    )

            if len(tpr_by_group) > 1:
                eq_odds_diff = max(
                    abs(tpr_by_group[g1] - tpr_by_group[g2])
                    for g1 in tpr_by_group
                    for g2 in tpr_by_group
                )
                eq_opp_diff = max(tpr_by_group.values()) - min(
                    tpr_by_group.values()
                )

        group_counts = {
            group: len(data["predictions"])
            for group, data in groups.items()
        }

        return FairnessMetrics(
            demographic_parity_diff=demo_parity_diff,
            equalized_odds_diff=eq_odds_diff,
            equality_opportunity_diff=eq_opp_diff,
            group_counts=group_counts,
        )

    async def _trigger_bias_alert(self, metrics: FairnessMetrics):
        """Trigger alert when bias threshold exceeded"""
        alert_message = (
          f"BIAS ALERT: Demographic parity difference: "
          f"{metrics.demographic_parity_diff:.3f}, "
        )
        alert_message += (
          f"Equalized odds difference: "
          f"{metrics.equalized_odds_diff:.3f}"
        )

        # Log to audit system
        print(f"[AUDIT] {alert_message}")

        # Could trigger additional actions:
        # - Send alert to monitoring dashboard
        # - Temporarily enable manual review
        # - Trigger model retraining pipeline
        # - Adjust decision thresholds
```

这种生产实现展示了负责任的 AI 原则如何转化为具有可衡量性能影响的具体系统架构。公平性监控为每个请求增加了 10-20 ms 的延迟，并需要额外 100-500 MB 的内存，而解释引擎将响应时间增加了 20-50 ms，内存使用量增加了 50-100%。在设计生产系统时，必须将这些开销与可靠性和合规性要求相平衡。

检测能力必须与积极防止有害结果的缓解技术相结合。

### 风险缓解技术

缓解技术积极干预系统设计和操作，以防止有害结果并降低对用户和社会的风险。这些方法包括保护敏感数据的隐私保护方法，到在攻击下保持系统可靠性的对抗性防御，再到支持数据治理和用户权利的机器反学习 31 技术。

#### 隐私保护

记住，隐私是负责任机器学习的基础原则，其影响延伸到数据收集、模型行为和用户交互。隐私约束不仅由伦理和法律义务塑造，还由系统的架构属性和部署的上下文所决定。隐私保护的技术方法旨在防止数据泄露，限制记忆，并维护用户权利，如同意、退出和数据删除——尤其是在从个性化或敏感信息中学习的系统中。

现代机器学习模型，尤其是大规模神经网络，已知会记住单个训练示例，包括姓名、位置或私人通信的摘录（Ippolito 等人 2023）。这种记忆在隐私敏感的应用中存在显著风险，例如智能助手、可穿戴设备或医疗平台，其中训练数据可能包含受保护或受监管的内容。例如，一个适应用户语音的语音助手可能会无意中保留特定的短语，这些短语可能后来通过精心设计的提示或查询被提取出来。

这种风险不仅限于语言模型。在图像数据集上训练的扩散模型也被观察到可以从训练集中重新生成视觉实例，如图图 17.4 所示。这种行为突显了一个更普遍的漏洞：许多当代模型架构可以内化和再现训练数据，通常没有明确的信号或意图，也没有容易检测或控制。

![图片](img/file293.png)

图 17.4：**扩散模型记忆化**：图像扩散模型可以重现训练样本，揭示了除语言模型之外意外记忆化的风险，并突出了当代神经网络架构的一般性漏洞。这种记忆化即使在缺乏明确指令的情况下也会发生，当在敏感数据集上训练时，会引发隐私问题。来源：(Ippolito 等人 2023)。

模型也容易受到成员推理攻击的影响，攻击者试图确定特定数据点是否是训练集的一部分（Shokri 等人 2017）。这些攻击利用了模型在已见和未见输入之间的行为上的微妙差异。在医疗保健或法律预测等高风险应用中，仅仅知道个人的记录被用于训练，就可能违反隐私期望或监管要求。

为了减轻这些漏洞，已经开发了一系列的隐私保护技术。其中最广泛采用的一种是差分隐私 32，它提供了正式的保证，即单个数据点的包含或排除对模型输出的影响在统计上是有限的。例如，差分隐私随机梯度下降（DP-SGD）算法通过剪枝和训练过程中注入噪声来强制执行这些保证（Martin Abadi 等人 2016）。当正确实施时，这些方法可以防止模型记住单个数据点，并降低推理攻击的风险。

然而，差分隐私引入了显著的系统级权衡。训练过程中添加的噪声可能会降低模型精度，增加训练迭代次数，并需要访问更大的数据集以维持性能。这些限制在资源受限的部署中尤为明显，如移动、边缘或嵌入式系统，在这些系统中，内存、计算和电力预算都受到严格限制。在这种情况下，可能需要结合轻量级隐私技术（例如，特征混淆、本地差分隐私）与限制数据收集、缩短保留期或在边缘实施严格访问控制的架构策略。

隐私执行还依赖于模型本身之外的基础设施。数据收集接口必须支持知情同意和透明度。日志系统必须避免保留敏感输入，除非绝对必要，并且必须支持访问控制、过期策略和可审计性。模型服务基础设施必须设计成防止输出过度暴露，这可能泄露内部模型行为或允许重建私有数据。这些系统级机制需要机器学习工程、平台安全和组织治理之间的紧密协调。

隐私不仅需要在训练期间得到强制执行，在整个机器学习生命周期中都需要得到强制执行。再训练管道必须考虑到已删除或撤销的数据，特别是在有数据删除命令的司法管辖区。监控基础设施必须避免在日志或仪表板上记录个人信息。隐私感知遥测收集、安全区域部署以及按用户审计跟踪越来越多地被用于支持这些目标，尤其是在受到严格法律监管的应用中。

架构决策也因部署环境而异。基于云的系统可能依赖于差分隐私、加密和访问控制的集中式执行，这些由遥测和再训练基础设施支持。相比之下，边缘和 TinyML 系统必须在部署的模型本身中构建隐私约束，通常没有运行时配置性或反馈通道。在这种情况下，必须在编译时实现静态分析、保守设计和嵌入式隐私保证，并在部署前进行验证。

隐私不是一个模型独立属性，而是一个从整个管道的设计决策中产生的系统级属性。负责任的隐私保护要求技术保障、界面控制、基础设施政策和法规遵从机制共同工作，以在整个部署的机器学习系统生命周期中降低风险。

隐私保护技术创造了复杂的社会技术紧张关系，这些关系远远超出了技术实施的范畴。差分隐私机制可能会以不成比例地影响少数群体的方式降低模型准确性，从而在隐私和公平性目标之间产生冲突。这些挑战需要持续的利益相关者参与，如第 17.6.3 节中详细所述，其中组织必须在大数据控制、个性化以及法规遵从等相互冲突的价值之间进行导航。

当考虑到用户权利和数据治理的动态性质时，这些隐私挑战变得更加复杂。

#### 机器反学习

隐私保护并不在训练时间结束。在许多现实世界的系统中，用户必须保留撤销同意或请求删除其数据的权利，即使模型已经训练并部署。支持这一要求引入了一个核心技术挑战：如何在不需要全面重新训练的情况下，让模型“忘记”特定数据点的影響——这项任务在计算、存储和连接受限的边缘、移动或嵌入式部署中通常是不切实际的。

传统数据删除方法假设完整的训练数据集仍然可访问，并且可以在删除目标记录后从头开始重新训练模型。图 17.5 对传统模型重新训练与新兴机器反学习方法进行了对比。虽然重新训练涉及使用修改后的数据集从头开始重建模型，但反学习旨在去除特定数据点的影响，而不重复整个学习过程。

![图片](img/file294.svg)

图 17.5：**模型更新策略**：重新训练从零开始重建模型，而机器反学习则修改现有模型，以去除特定数据点的影响而不进行完全重建——这对于资源受限的部署来说是一个重要的区别。这种方法最小化了计算成本，并允许在初始模型训练后进行隐私保护的数据删除。

在具有严格延迟、计算或隐私约束的系统中，这种区别变得很重要。在实践中，这些假设很少成立。由于安全、合规或成本限制，许多部署的机器学习系统不保留原始训练数据。在这样的环境中，完全重新训练通常不切实际且具有破坏性，尤其是在数据删除必须可验证、可重复且可审计的情况下。

机器反学习旨在通过移除已训练模型中个别数据点的 影响，而不完全重新训练模型，来解决这个问题。当前的方法通过调整内部参数、修改梯度路径或隔离和修剪模型的部分组件来近似这种行为，从而使结果预测反映在没有删除数据的情况下可能学到的内容 (Bourtoule 等人 2021)。这些技术仍在成熟中，可能需要简化的模型架构、额外的跟踪元数据，或者在模型精度和稳定性上做出妥协。它们还引入了新的验证负担：如何以有意义的方式证明已发生删除，尤其是在内部模型状态不完全可解释的情况下。

机器反学习的动机得到了监管框架的加强。例如，通用数据保护条例 (GDPR)、加利福尼亚消费者隐私法案 (CCPA) 以及加拿大和日本的类似法律将“被遗忘权”编纂成法典，包括用于模型训练的数据。这些法律越来越要求不仅防止未经授权的数据访问，而且要求主动撤销——赋予用户请求其信息停止影响下游系统行为的能力。一些知名事件，其中生成模型复制了个人内容或受版权保护的数据，突出了将反学习机制整合到负责任系统设计中的实际紧迫性。

从系统角度来看，机器反学习引入了非平凡的架构和运营需求。系统必须能够跟踪数据血缘，包括哪些数据点贡献了给定的模型版本。这通常需要结构化元数据捕获和训练管道的仪器化。此外，系统必须支持面向用户的删除工作流程，包括身份验证、提交和删除状态的反馈。验证可能需要维护版本化的模型注册表，以及确认更新后的模型没有残留的删除数据影响的机制。这些操作必须跨越数据存储、训练编排、模型部署和审计基础设施，并且必须能够应对故障或回滚。

这些挑战在资源受限的部署中更为突出。TinyML 系统通常运行在没有持久存储、没有连接性和高度压缩模型的设备上。一旦部署，它们就不能根据删除请求进行更新或重新训练。在这种设置下，机器反学习在部署后实际上是不可行的，必须在初始模型开发期间通过静态数据最小化和保守的泛化策略来强制执行。即使在基于云的系统，重新训练更为可行的情况下，反学习也必须应对分布式训练管道、跨服务的复制以及同步模型快照和日志中删除的困难。

尽管存在这些挑战，机器反学习对于负责任系统设计正变得越来越重要。随着机器学习系统变得更加嵌入式、个性化和自适应，撤销训练影响的能力对于维护用户信任和满足法律要求变得至关重要。关键的是，反学习不能在部署后进行改造。它必须在架构和政策设计阶段予以考虑，并且从系统一开始就必须支持血缘跟踪、重新训练编排和部署回滚。

机器反学习代表了隐私思考的转变——从保护收集的数据，到控制这些数据持续影响系统行为的时间。这种以生命周期为导向的视角为模型设计、基础设施规划和法规遵从引入了新的挑战，同时也为更用户可控、透明和自适应的机器学习系统提供了基础。

负责任的 AI 系统还必须在包括故意攻击在内的困难条件下保持可靠的行为。

#### 对抗鲁棒性

在第十六章 ([ch022.xhtml#sec-robust-ai]) 和第十五章 ([ch021.xhtml#sec-security-privacy]) 中探讨的对抗鲁棒性，作为针对故意攻击的防御手段，也构成了负责任 AI 部署的基础。除了保护免受恶意对手的攻击之外，对抗鲁棒性确保模型在遇到自然发生的变异、边缘情况和偏离训练分布的输入时仍能可靠地表现。易受对抗性扰动影响的模型揭示了其学习表示中的基本脆弱性——这种脆弱性即使在非对抗性环境中也会损害可信度。

机器学习模型，尤其是深度神经网络，已知容易受到微小、精心设计的扰动的影响，这些扰动会显著改变它们的预测。这些漏洞最初是通过对抗性示例的概念 (Szegedy 等人 2013b) 得到的形式化，突显了模型在精心挑选的训练数据上的性能与在现实世界变量下的行为之间的差距。一个在干净输入上表现可靠的模型，当暴露于与其训练分布略有不同的输入时可能会失败——这些差异对人类来说是不可察觉的，但足以改变模型输出。

这种现象不仅限于理论。对抗性示例已被用于操纵真实系统，包括内容审核管道 (Bhagoji 等人 2018)、广告拦截检测 (Tramèr 等人 2019) 和语音识别模型 (Carlini 等人 2016)。在自动驾驶或医疗诊断等安全至关重要的领域，即使是罕见的故障也可能产生高后果的结果，损害用户信任或为恶意利用打开攻击面。

![图 17.6](img/ch023.xhtml#fig-adversarial-example) 展示了一个视觉上微不足道的扰动，它导致了一个自信的错误分类——强调了微妙的变化如何产生不成比例的有害影响。

![图片](img/file295.png)

图 17.6：**对抗性扰动**：精心制作的微妙噪声可能导致机器学习模型以高置信度错误分类输入，尽管这种变化对人类来说是不可察觉的。这个例子展示了图像的微小扰动如何导致错误分类，突显了深度学习系统对对抗性攻击的脆弱性。来源：微软。

在其核心，对抗性漏洞源于模型假设与部署条件之间的架构不匹配。许多训练管道假设数据是干净的、独立的且同分布的。相比之下，部署系统必须在不确定性、噪声、领域偏移和可能的对抗性篡改下运行。在这种情况下，鲁棒性不仅包括抵抗攻击的能力，还包括在退化或不可预测条件下保持一致行为的能力。

提高鲁棒性始于训练阶段。对抗性训练是最广泛使用的技术之一，通过添加扰动示例来增强训练数据。这有助于模型学习更稳定的决策边界，但通常会增加训练时间并降低干净数据的准确性。在规模上实施对抗性训练也对数据预处理管道、模型检查点基础设施和可以容纳扰动输入的验证协议提出了要求。

架构修改也可以促进鲁棒性。限制模型 Lipschitz 常数的技巧、正则化梯度敏感性或强制执行表示平滑性可以使预测更稳定。这些设计更改必须与模型的表达需求以及底层训练框架兼容。例如，平滑模型可能更适合输入精度有限或必须遵守安全重要阈值的嵌入式系统。

在推理时间，系统可能会实施不确定性感知的决策制定。当置信度低时，模型可以避免做出预测，或者将不确定的输入路由到回退机制——例如基于规则的组件或人机交互系统。这些策略需要部署基础设施来支持回退逻辑、用户升级工作流程或可配置的弃权策略。例如，一个移动诊断应用可能会在模型置信度低于指定阈值时返回“不确定”，而不是发布可能有害的预测。

监控基础设施在部署后保持鲁棒性方面发挥着重要作用。分布偏移检测、异常跟踪和行为漂移分析允许系统识别鲁棒性随时间退化的情况。实现这些功能需要持续记录模型输入、预测和上下文元数据，以及触发重新训练或升级的安全通道。这些工具引入了它们自己的系统开销，必须与遥测服务、警报框架和模型版本控制工作流程集成。

除了经验性防御之外，形式化方法提供了更强的保证。例如，经过认证的防御，如随机平滑，提供了概率保证，即模型在有限输入区域内输出的稳定性。这些方法在每个推理过程中需要多次正向传递，计算密集，因此主要适用于高保证、资源丰富的环境。它们的生产工作流程集成也要求与模型服务基础设施和概率验证工具兼容。

简单的防御，例如输入预处理，通过去噪、压缩或归一化步骤过滤输入以去除对抗噪声。这些转换必须足够轻量，以便实时执行，特别是在边缘部署中，并且足够鲁棒以保留任务相关的特征。另一种方法是集成建模，其中预测是在多个不同的模型之间汇总的。这增加了鲁棒性，但增加了推理管道的复杂性，增加了内存占用，并复杂化了部署和维护工作流程。

系统约束，如延迟、内存、电源预算和模型更新周期，强烈影响着哪些鲁棒性策略是可行的。对抗训练增加了模型大小和训练时间，这可能会挑战 CI/CD 管道并增加重新训练成本。经过认证的防御需要计算余量和推理时间容忍度。监控需要日志记录基础设施、数据保留策略和访问控制。特别是，在设备上和 TinyML 部署中，通常无法容纳运行时检查或动态更新。在这种情况下，鲁棒性必须通过静态验证和编译时嵌入。

对抗鲁棒性不是一个独立的模型属性。它是一个从训练、模型架构、推理逻辑、日志记录和回退路径协调中出现的系统级属性。一个在孤立情况下看似鲁棒的模型，如果部署在一个缺乏监控或接口保护的系统中，仍然可能会失败。相反，即使是一个部分鲁棒的模型，如果嵌入到一个能够检测不确定性、限制对不受信任的输入的暴露，并在出错时支持恢复的架构中，也可以有助于整体系统的可靠性。

鲁棒性，就像隐私和公平性一样，必须不仅被设计到模型中，还要被设计到其周围的系统中。负责任的机器学习系统设计需要预测模型在现实世界压力下可能失败的方式——并构建使这些失败可检测、可恢复和安全的基础设施。

验证方法使利益相关者能够理解和审计系统行为。

### 验证方法

验证方法为理解、审计和向必须评估自动化决策是否符合道德和运营要求的利益相关者解释系统行为提供了机制。这些技术促进了透明度，支持法规遵从，并在用户和自动化系统之间建立信任。

#### 可解释性与可解释性

随着机器学习系统在越来越重要的领域得到部署，理解和解释模型预测的能力变得重要。可解释性和可解释性指的是使模型行为对人类利益相关者（无论是开发者、领域专家、审计员、监管者还是最终用户）可理解的技术和设计机制。虽然这两个术语经常互换使用，但可解释性通常指模型的内在透明度，例如决策树或线性分类器。相比之下，可解释性包括为复杂或模糊模型做出的预测生成事后合理化的技术。

可解释性在系统验证、错误分析、用户信任、法规遵从和事件调查中扮演着核心角色。在医疗保健、金融服务和自主决策系统等高风险领域，解释有助于确定模型是否基于合法原因做出决策，或者依赖于虚假的相关性。例如，一个可解释性工具可能会揭示诊断模型对图像伪影过于敏感，而不是对医疗特征敏感，这是一种可能未被发现的故障模式。现在，许多行业的监管框架都要求人工智能系统提供关于决策如何做出的“有意义的信息”，这强化了对系统解释的系统性支持的需求。

可解释性方法可以根据它们操作的时间和它们与模型结构的关系进行广泛分类。事后方法在训练后应用，并将模型视为黑盒。这些方法不需要访问内部模型权重，而是从模型行为中推断影响模式或特征贡献。常见的事后技术包括特征归因方法，如输入梯度、集成梯度 33、GradCAM34 (Selvaraju 等人 2017)、LIME (Ribeiro, Singh, and Guestrin 2016) 和 SHAP (Lundberg and Lee 2017)。

这些方法在图像和表格领域被广泛使用，其中解释可以表示为显著性图或特征排名。为了说明 SHAP 归因在实际中的工作原理，考虑一个基于三个特征（`income`，`debt_ratio`，和`credit_score`）预测贷款批准（approve=1，deny=0）的已训练随机森林模型。对于一个具体申请者，其收入为$45,000，债务比率为 0.55（55%的收入用于债务），信用评分为 620，模型预测拒绝的概率为 0.72。基于合作博弈论中的 Shapley 值的 SHAP 值，衡量每个特征对将预测从基线（所有训练数据的平均预测，P(approve) = 0.50）移动到这个个体预测的贡献。

SHAP 框架通过评估所有可能的特征子集来计算每个特征的贡献。从基线预测 0.50 开始，增加收入（$45K，略低于平均水平）会降低批准概率 0.05。增加债务比率（0.55，高）会额外降低批准概率 0.25。增加信用评分（620，低于阈值）会适度降低批准概率 0.12。最终预测变为 0.50 - 0.05 - 0.25 - 0.12 = 0.08，对应于 P(deny) = 0.72。这表明高债务比率对拒绝贡献最大（-0.25），其次是低于平均水平的信用评分（-0.12），而收入的影响最小（-0.05）。这样的解释是可操作的：将债务比率降低到 40%以下可能会改变决策。

然而，这种严谨性伴随着巨大的计算成本。这个包含 3 个特征的示例需要评估 2³ = 8 个特征子集。对于一个包含 20 个特征的模型，SHAP 需要评估 2²⁰ ≈ 1 百万个特征子集，这解释了与简单梯度方法相比，计算开销高达 50-1000 倍。基于树的 SHAP 实现通过利用模型结构将计算时间降低到多项式时间，但深度学习模型通常需要近似算法（KernelSHAP，DeepSHAP）以及基于采样的估计。虽然 SHAP 提供了理论上基于的、可加的特征归因，这些归因满足了一些理想属性（局部精度、缺失值、一致性），但这些成本使得 SHAP 在没有近似或缓存策略的情况下，对于高吞吐量系统的实时解释来说并不实用。

另一种事后方法涉及反事实解释，它描述了如果输入以特定方式修改，模型输出会如何变化。这些解释对于面向决策的应用程序，如信贷或招聘系统，尤其相关。例如，一种反事实解释可能声明，如果申请人的报告收入更高或债务更低，他们将获得贷款批准（Wachter, Mittelstadt, and Russell 2017）。反事实生成需要访问特定领域的约束和现实数据流形，这使得将其集成到实时系统中具有挑战性。

第三类技术依赖于基于概念的解释，这些解释试图将学习到的模型特征与人类可解释的概念相一致。例如，一个用于分类室内场景的卷积网络可能会激活与“灯”、“床”或“书架”相关的过滤器（C. J. Cai 等人 2019）。这些方法在领域专家期望以熟悉的语义术语进行解释的领域特别有用。然而，它们需要具有概念注释的训练数据或用于概念检测的辅助模型，这引入了额外的基础设施依赖。

虽然事后方法具有灵活性和广泛的应用性，但它们也存在局限性。因为它们在事后近似推理，可能会产生看似合理但具有误导性的理由。它们的有效性取决于模型的平滑度、输入结构和解释技术的保真度。这些方法通常在探索性分析、调试或面向用户的总结中最为有用，而不是作为内部逻辑的最终解释。

与之相反，本质上可解释的模型在设计上就是透明的。例如，决策树、规则列表、具有单调性约束的线性模型和 k-最近邻分类器。这些模型直接暴露其推理结构，使得利益相关者可以通过一系列可解释的规则或比较来追踪预测。在监管或安全至关重要的领域，如再犯预测或医疗分诊，本质上可解释的模型可能更受欢迎，即使这要以一些准确性为代价（Rudin 2019）。然而，这些模型通常难以扩展到高维或非结构化数据，并且它们的简单性可能会限制在复杂任务中的性能。

不同模型类型的相对可解释性可以沿着一个谱系进行可视化。如图图 17.7 所示，决策树和线性回归等模型通过设计提供透明度，而像神经网络和卷积模型这样更复杂的架构则需要外部技术来解释其行为。这种区别在选择适用于特定应用的适当模型时至关重要——尤其是在监管审查或利益相关者信任至关重要的环境中。

![图片](img/file296.svg)

图 17.7：**模型可解释性谱系**：内禀可解释的模型，如线性回归和决策树，提供透明的推理，而复杂的模型如神经网络则需要事后解释技术来理解其预测。这种区别指导了基于应用需求进行模型选择，优先考虑在受监管领域或当利益相关者信任重要时的透明度。

混合方法旨在结合深度模型的表示能力和可解释组件的透明度。例如，概念瓶颈模型(Koh 等人 2020)首先预测中间的可解释变量，然后使用简单的分类器生成最终预测。ProtoPNet 模型(C. Chen 等人 2019)通过将示例与学习到的原型进行比较来分类，为用户提供理解预测的视觉类比。这些混合方法在需要部分透明度的领域中具有吸引力，但它们引入了新的系统设计考虑因素，例如需要存储和索引学习到的原型，并在推理时呈现它们。

一个更近期的研究方向是机制可解释性，它试图逆向工程神经网络的内部分工。这一工作受到程序分析和神经科学的启发，试图将神经元、层或激活模式映射到特定的计算功能(Olah 等人 2020; Geiger 等人 2021)。尽管前景广阔，但这一领域仍然是探索性的，目前主要与大型基础模型的分析相关，在这些模型中，传统的可解释性工具是不够的。

从系统视角来看，可解释性引入了许多架构依赖。解释必须在系统约束内生成、存储、呈现和评估。所需的基础设施可能包括解释 API、存储归因图的内存、可视化库以及捕获中间模型行为的日志机制。模型通常需要通过钩子进行仪器化或配置以支持重复评估——尤其是对于需要采样、扰动或反向传播的解释方法。

这些需求直接与部署约束相互作用，并施加可量化的性能成本，这些成本必须纳入系统设计。SHAP 解释通常需要比标准推理多 50-1000 倍的向前传递，每解释的计算开销从 200 ms 到 5+秒不等，具体取决于模型复杂性。LIME 类似地需要训练代理模型，每解释增加 100-500 ms。在生产部署中，这些成本转化为显著的基础设施开销：一个每秒处理 10,000 次预测且解释率为 10%的高流量系统，仅为了可解释性就需要 50-500 倍额外的计算能力。

对于资源受限的环境，基于梯度的归因方法提供了更有效的替代方案，通常通过利用已经存在的反向传播基础设施，每解释增加 10-50 ms 的开销。然而，这些方法对于复杂模型来说可靠性较低，可能在模型更新之间产生不一致的解释。边缘部署通常通过预计算的规则近似或简化的决策边界来实现可解释性，牺牲了解释的精确性以换取低于 100 ms 的可行延迟。

存储需求也随着解释需求而显著增加。存储表格数据的 SHAP 值大约需要每个特征每预测 4-8 字节，而图像的梯度归因图可能需要根据分辨率每解释 1-10 MB。一个每天维护 100 万次预测的解释日志的生产系统，每月需要额外的 50 GB-10 TB 存储容量，这需要仔细的数据生命周期管理和保留策略。

可解释性贯穿整个机器学习生命周期。在开发阶段，可解释性工具用于数据集审计、概念验证和早期调试。在推理时间，它们支持问责制、决策验证和用户沟通。部署后，解释可能被记录、在审计中呈现或在错误调查期间查询。系统设计必须支持这些所有阶段——确保解释工具集成到训练框架、模型服务基础设施和面向用户的应用程序中。

压缩和优化技术也会影响可解释性。在 TinyML 或移动设置中常用到的剪枝、量化和架构简化可能会扭曲内部表示或禁用梯度流，降低基于归因的解释的可靠性。在这种情况下，必须在优化后验证可解释性，以确保其仍然有意义和可信。如果解释质量很重要，这些变换必须被视为设计约束空间的一部分。

可解释性不是一个附加功能，而是一个系统级关注点。为了可解释性而设计需要谨慎地决定谁需要解释，什么样的解释是有意义的，以及如何在系统的延迟、计算和界面预算限制下提供这些解释。随着机器学习嵌入到重要的工作流程中，解释能力成为安全、可信和可问责系统的核心要求。

可解释性的社会技术挑战集中在技术解释和人类理解之间的差距。虽然算法可以生成特征归因和梯度图，但利益相关者通常需要与他们的心理模型、领域专业知识和决策过程相一致的解释。一位审查 AI 生成的诊断的放射科医生需要参考医学概念和视觉模式的解释，而不是抽象的神经网络激活。这种翻译挑战需要技术团队和领域专家之间的持续合作，以开发既技术准确又实际有意义的解释格式。解释可以在意想不到的方式上塑造人类决策，从而为解释信息的呈现和解释方式带来新的责任。

#### 模型性能监控

无论训练时的评估多么严格，一旦系统部署，都不能保证可靠的模型性能。现实环境是动态的：由于季节性，输入分布会发生变化；用户行为会随着系统输出而演变；政策或法规的变化会导致上下文期望发生变化。这些因素可能导致预测性能下降，甚至更重要的是，系统可信度随时间退化。在训练或验证条件下表现良好的模型，在生产中仍可能做出不可靠或有害的决定。

这种漂移的影响不仅限于原始准确率。如果子群体分布相对于训练集发生变化，或者先前与结果相关的特征在新环境中变得不可靠，公平性保证可能会失效。可解释性的需求也可能发生变化——例如，随着新的利益相关者群体寻求解释，或者随着监管机构引入新的透明度要求。因此，可信度不是一个在训练时授予的静态属性，而是一个由部署环境和操作反馈塑造的动态系统属性。

为了确保长期的责任行为，机器学习系统必须纳入持续监控、评估和纠正行动的机制。监控不仅涉及跟踪总体准确性，还需要在相关的子组中呈现性能指标，检测输入分布的变化，识别异常输出，并捕捉有意义的用户反馈。然后，这些信号必须与关于公平性、鲁棒性和透明度的预定义期望进行比较，并链接到可操作的系统响应，如模型重新训练、重新校准或回滚。

实施有效的监控依赖于稳健的基础设施。系统必须以结构化和安全的方式记录输入、输出和上下文元数据。这需要能够捕获模型版本、输入特征、预测置信度和后推理反馈的遥测管道。这些日志支持漂移检测，并为公平性和鲁棒性的事后审计提供证据。监控系统还必须与警报、更新调度和政策审查流程集成，以支持及时和可追溯的干预。

监控还支持反馈驱动的改进。例如，重复的用户不同意、更正请求或操作员覆盖可以表明存在问题的行为。这种反馈必须被汇总、验证并转化为更新训练数据集、数据标注过程或模型架构。然而，这样的反馈循环存在风险：有偏的用户响应可能会引入新的不平等，而过度的日志记录可能会损害隐私。设计这些循环需要用户体验设计、系统安全和道德治理之间的仔细协调。

监控机制因部署架构而异。在基于云的系统，丰富的日志记录和计算能力允许实时遥测、计划中的公平性审计以及将新数据持续集成到重新训练管道中。这些环境支持动态重新配置和集中式政策执行。然而，遥测数据的量可能会在成本、隐私风险和合规性方面带来自己的挑战。

在移动系统中，连接是间歇性的，数据存储有限。监控必须轻量级且能够抵御同步延迟。本地推理系统可能会异步收集性能数据，并将其汇总传输到后端系统。隐私约束通常更严格，尤其是在个人数据必须保留在设备上时。这些系统需要仔细的数据最小化和本地聚合技术，以在保持可观察性的同时保护隐私。

边缘部署，如自动驾驶汽车、智能工厂或实时控制系统，需要低延迟响应，并以最小外部监督运行。在这些系统中的监控必须嵌入到运行时，对传感器完整性、预测置信度和行为偏差进行内部检查。这些检查通常需要低开销的不确定性估计、异常检测或一致性验证的实现。系统设计者必须预测故障条件，并确保异常行为触发安全回退程序或人工干预。

TinyML 系统，在无连接、持久存储或动态更新路径的深度嵌入式硬件上运行，呈现了最受限的监控场景。在这些环境中，监控必须在部署前设计和编译到系统中。常见的策略包括输入范围检查、内置冗余、静态故障转移逻辑或保守的验证阈值。一旦部署，这些模型独立运行，任何部署后的故障可能需要物理设备更换或固件级别的重置。

核心挑战是普遍的：部署的机器学习系统不仅最初必须表现良好，而且随着环境的变化必须继续负责任地行为。监控提供了将系统性能与道德目标和问责结构联系起来的可观察性层。没有监控，公平性和鲁棒性将变得不可见。没有反馈，偏差无法得到纠正。因此，监控是操作基础，使机器学习系统能够保持适应性、可审计性和与其预期目的的一致性。

本节探讨的技术方法——偏差检测算法、差分隐私机制、对抗性训练程序和可解释性框架——为负责任的 AI 实现提供了基本能力。然而，这些工具揭示了根本性的限制：仅仅技术正确性不能保证有益的结果。考虑三个具体例子来说明这一挑战：

公平审计系统检测贷款审批模型中的种族偏见，但该组织缺乏解释结果或实施纠正措施的过程。技术能力存在，但组织惯性阻碍了补救。差分隐私保护了关于数据保护的正式数学保证，但用户不理解这些保护，并继续不适当地共享敏感信息。隐私方法按设计工作，但行为上下文削弱了其有效性。可解释性系统生成技术准确的特征重要性分数，但受影响的人由于界面设计和识字障碍无法访问或解释这些解释。

这些例子表明，负责任的人工智能实施取决于技术能力和社会技术环境之间的协调——组织激励、人类行为、利益相关者价值观和制度治理结构。

## 社会技术动态

负责任的人工智能系统在复杂的社会技术环境中运行，其中技术方法与人类行为、组织实践和竞争的利益相关者价值观相互作用。上述检测偏差的工具、保护隐私的技术和可解释性方法提供了必要的功能，但它们的有效性完全取决于它们如何与决策过程、用户界面、反馈机制和治理结构整合。理解这些相互作用对于实现实际有益结果的可持续负责任人工智能部署至关重要。

**认知转变：从纯工程到社会技术工程**

上一节重点介绍了解决定义明确问题的技术工具：检测偏差的算法、保护隐私的方法和生成解释的技术。我们现在转变分析视角，以解决仅用算法无法解决的问题。

以下章节将探讨负责任的人工智能系统如何与人类、组织和竞争价值观互动。这种转变需要不同的推理技能：不是优化目标函数，而是分析利益相关者冲突；不是调整超参数，而是导航伦理权衡；不是衡量技术性能，而是评估社会影响。这些都是社会技术工程面临的挑战——设计必须满足计算约束和人类价值观的系统。

理解这些社会技术动态对于可持续的负责任人工智能实施至关重要。

负责任的人工智能系统设计不仅超越了技术正确性和算法保障。一旦部署，这些系统将在复杂的社会技术环境中运行，其输出会影响并受到人类行为、制度实践和不断变化的社会规范的影响。随着时间的推移，机器学习系统成为它们旨在模拟的环境的一部分，从而产生反馈动态，影响未来的数据收集、模型再训练和下游决策。

本节讨论了与机器学习技术部署相关的更广泛的伦理和系统性挑战。它探讨了模型和环境之间的反馈循环如何强化偏见，人类-人工智能合作如何引入新的风险和责任，以及利益相关者价值观之间的冲突如何复杂化公平性和问责制的实施。它考虑了可争议性和制度治理在维持负责任系统行为中的作用。这些考虑表明，责任不是算法的静态属性，而是系统设计、使用和长期监督的动态结果。

### 系统反馈循环

机器学习系统不仅仅是观察和模拟世界；它们也塑造世界。一旦部署，它们的预测和决策往往会影响它们旨在分析的环境。这种反馈会改变未来的数据分布，修改用户行为，并影响制度实践，在模型输出和系统输入之间形成一个递归循环。随着时间的推移，这种动态可能会放大偏见，加剧不平等，或无意中改变模型旨在服务的目标。

这种现象的一个很好的例子是预测警务。当一个基于历史逮捕数据的模型预测某个特定地区的犯罪率较高时，执法部门可能会向该地区分配更多的巡逻警力。这种增加的警力导致更多记录的事件，然后这些事件被用作未来模型训练的输入，进一步强化了模型最初的预测。即使模型在开始时没有明显的偏见，但其融入反馈循环会导致一个自我实现的模式，这种模式不成比例地影响已经过度警力的社区。

推荐系统在数字环境中表现出类似的动态。一个优先考虑参与度的内容推荐模型可能会逐渐缩小用户接触到的内容范围，导致强化现有偏好或极端化意见的反馈循环。这些影响可能难以使用传统的性能指标检测，因为系统在偏离更广泛的社会或认识论目标的同时，仍在优化其训练目标。

从系统角度来看，反馈循环对负责任的 AI 构成了核心挑战。它们破坏了独立同分布数据的假设，并复杂化了公平性、鲁棒性和泛化的评估。依赖于静态测试集的标准验证方法可能无法捕捉模型对数据生成过程影响的演变。一旦建立这样的循环，旨在提高公平性或准确性的干预措施可能效果有限，除非解决潜在的数据动态。

在存在反馈循环的情况下设计责任需要机器学习系统的生命周期视图。这不仅包括监控模型性能随时间的变化，还包括理解系统输出如何影响环境，这些变化如何在新数据中捕捉到，以及再训练实践如何减轻或加剧这些效应。

在基于云的系统中，这些更新可能频繁发生且规模庞大，有广泛的遥测数据可用于检测行为漂移。相比之下，边缘和嵌入式部署通常在离线或有限的可观察性下运行。一个根据用户交互调整恒温器行为的智能家居系统可能会以改变家庭环境的方式强化能源消耗模式或舒适偏好——从而影响模型未来的输入。在没有连接或集中监督的情况下，这些循环可能未被识别，尽管它们对用户行为和系统性能都有影响。第十三章中详细说明的操作监控实践对于在生产系统中检测和管理这些反馈动态至关重要。

系统必须配备机制来检测分布漂移、识别行为塑造效应，并支持与系统预期目标一致的纠正更新。反馈循环本身并非有害，但它们必须被识别和管理。当未被审查时，它们会引入系统性风险；当经过深思熟虑地处理时，它们为学习系统在复杂、动态环境中负责任地适应提供了机会。

当人类操作员集成到决策过程中时，这些系统级反馈动态变得更加复杂。

### 人机协作

机器学习系统越来越多地被部署为不仅仅是独立代理，而是作为涉及人类决策者的更大工作流程中的组件。在许多领域，如医疗保健、金融和交通，模型作为决策支持工具，提供预测、风险评分或推荐，这些由人类操作员审查并采取行动。这种协作配置提出了关于责任在人类和机器之间如何共享、信任如何校准以及如何在实践中实施监督机制的重要问题。

人机协作既带来了机遇也带来了风险。当设计得当，系统可以增强人类判断，减轻认知负担，并提高决策的一致性。然而，当设计不当，它们可能导致自动化偏差，即用户即使在存在明显错误的情况下也过度依赖模型输出。相反，过度不信任可能导致算法厌恶，即用户由于缺乏透明度或感知可信度而忽视有用的模型预测。协作系统的有效性不仅取决于模型的表现，还取决于系统如何传达不确定性，提供解释，以及允许人类覆盖或纠正。

监督机制必须针对部署环境量身定制。在高风险领域，如医疗分级或自动驾驶，人们可能期望人类实时监督自动化决策。这种配置对人类操作员提出了认知和时间上的要求，并假设在需要时干预将迅速且可靠地发生。然而，在实践中，持续的人类监督往往是不可行或无效的，尤其是当操作员必须监控多个系统或缺乏明确的干预标准时。

从系统设计角度来看，支持有效的监督需要不仅仅是提供对原始模型输出的访问。界面必须构建以在正确的时间、正确的格式和适当的上下文中呈现相关信息。置信度分数、不确定性估计、解释和变更警报都可以在使人类监督成为可能方面发挥作用。工作流程必须定义何时以及如何进行干预，谁有权覆盖模型输出，以及如何记录、审计和将这些覆盖纳入未来的系统更新。

考虑一个使用机器学习模型来优先处理急诊科患者的医院分级系统。该模型为每位进入的患者生成一个风险评分，并将其与建议的分级类别一起展示。原则上，护士负责确认或覆盖该建议。然而，如果模型的输出没有充分的理由，例如解释贡献特征或不确定性的背景，护士甚至可能在边缘情况下依赖模型。随着时间的推移，模型的输出可能成为事实上的分级决策，尤其是在时间压力下。如果发生分布变化（例如，由于新疾病或患者人口统计学的变化），护士可能缺乏必要的情境意识和界面支持，以检测模型表现不佳。在这种情况下，人类监督的表面现象掩盖了一个责任实际上已转移到模型而没有明确问责或追索权的系统。

在这样的系统中，人类监督不仅仅是政策声明的问题，而是基础设施设计的问题：如何呈现预测，保留什么信息，如何实施干预，以及如何将反馈循环连接到系统更新。如果没有这些组件之间的整合，监督就会变得支离破碎，责任可能从人类无形地转移到机器。

决策支持与自动化之间的界限通常很模糊。最初设计用于协助人类决策者的系统可能会随着信任的增加或组织激励的变化而逐渐获得更大的自主性。这种转变可能在没有明确政策变化的情况下发生，导致事实上自动化，但没有适当的问责结构。因此，负责任的设计必须预见使用随时间的变化，并确保即使在依赖自动化增长的情况下，适当的检查仍然存在。

人机协作需要仔细整合模型能力、界面设计、运营政策和制度监督。协作不仅仅是插入“人机结合”；这是一个跨越技术、组织和伦理维度的系统挑战。为监督而设计意味着嵌入允许干预、支持有信息信任和支持人类操作员与机器学习系统之间共享责任的机制。

人类-人工智能合作的复杂性进一步加剧，因为不同的利益相关者往往持有相互冲突的价值观和优先事项。

### 规范多元主义和价值冲突

**哲学内容**

本节探讨了相互竞争的价值体系和它们对机器学习设计的影响——这偏离了主要的技术内容。关键见解：技术卓越对于可信的人工智能是必要的，但不足以保证，因为利益相关者持有关于公平、隐私和问责的不同合法概念，这些概念不能通过更好的算法来调和。理解这些价值紧张关系对于导航影响人们生活的设计决策至关重要。这种观点补充，而不是取代，技术技能。

负责任的机器学习不能仅仅归结为单一目标的优化。在现实世界的环境中，机器学习系统被部署到由多种、通常相互冲突的人类价值观塑造的环境中。

考虑一个团队正在构建一个用于青少年的心理健康聊天机器人，该机器人使用机器学习来检测危机情况并推荐干预措施。该系统必须在多个合法但不兼容的目标之间取得平衡：

**医疗功效**：基于循证实践优化最佳临床结果。这意味着采取积极的干预措施——当模型检测到潜在的自伤风险时，即使信心较低，也要提醒父母、顾问或紧急服务，因为假阴性可能导致致命后果。

**患者自主权**：尊重青少年的隐私和自主权。许多青少年寻求心理健康支持，正是因为他们无法与父母或权威人士交谈。激进的通报政策可能会阻止脆弱的青少年使用该系统，使他们得不到任何支持。

**隐私保护**：最小化数据收集和保留，以保护敏感的心理健康信息。这表明本地处理、不记录对话、不与第三方共享——但也阻止了系统通过学习互动来改进，或当模型不确定时无法启用人工审查。

**资源效率**：在计算和人工监督预算内运行。对于每个标记的互动都涉及人类顾问可以提供更好的护理，但在规模上成本过高。完全自动化的响应可以降低成本，但在复杂情况下可能会提供不适当的指导。

**法律合规性**：满足强制报告要求和责任标准。在许多司法管辖区，检测到即将到来的伤害的系统必须通知当局——无论临床判断通知是否有助于或伤害患者，都应优先考虑患者自主权和隐私。

这些价值观不是定义不明确的、可以通过更好的工程来调和的要求。它们反映了关于系统应实现什么以及应优先考虑谁的根本不同的概念。优化医疗效果（积极干预）与患者自主权（最小干预）直接冲突。隐私保护（不保留数据）与资源效率（从互动中学习）冲突。法律合规性（强制报告）可能与临床效果（基于信任的治疗关系）冲突。

**没有算法决定哪个价值应该占主导地位**。不同的利益相关者持有合法的不同立场：临床医生可能优先考虑效果，青少年可能优先考虑自主权，律师可能优先考虑合规性，预算官员可能优先考虑效率。技术团队必须促进利益相关者的审议，以确定在特定背景下哪些权衡是可以接受的——这是一个根本性的规范性决策，它先于并限制了技术优化。

对于一个利益相关者来说，什么是公平的结果，可能被另一个利益相关者视为不公平。同样，优先考虑准确度或效率的决定可能与透明度、个人自主权或减少伤害等目标相冲突。这些紧张关系并非偶然——它们是结构性的。它们反映了机器学习系统嵌入的社会的多元性以及它们部署的机构环境。

公平性是价值冲突特别突出的领域。公平性可以通过多种方式形式化，通常是不兼容的。一个满足人口比例的模型可能会违反平等机会；一个优先考虑个体公平性的模型可能会破坏组级平等。在这些定义之间进行选择不仅仅是技术决策，而是规范性决策，受领域背景、歧视的历史模式以及受模型结果影响的人的视角所指导。在实践中，包括工程师、用户、审计员和监管机构在内的多个利益相关者可能对哪些定义最合适以及为什么持有相互冲突的观点。

这些紧张关系并不仅限于公平性。在可解释性和预测性能、隐私和个人化、或短期效用和长期后果之间也出现冲突。这些权衡在不同的系统部署架构中表现不同，揭示了价值冲突如何与 ML 系统的设计和操作紧密相连。

考虑在移动设备上部署的基于语音的助手。为了提高个性化，系统可能在本地学习用户偏好，而不将原始数据发送到云端。这种设计提高了隐私性并减少了延迟，但可能会导致使用模式代表性不足的用户收到不准确或反应迟钝的预测。提高公平性的方法之一是使用组级统计信息集中更新——但这样做会引入新的隐私风险，并可能违反用户对本地数据处理方面的期望。在这里，设计必须在有效但相互竞争的价值之间进行权衡：隐私、公平性和个性化。

在基于云的部署中，例如信用评分平台或推荐引擎，透明度和专有保护之间常常出现紧张关系。最终用户或监管机构可能要求对决策原因进行明确解释，尤其是在具有重大后果的情况下，但使用的模型可能依赖于复杂的集成或专有训练数据。透露这些内部信息可能具有商业敏感性或技术上不可行。在这种情况下，系统必须协调机构问责制和商业机密之间的竞争压力。

在边缘系统中，例如家庭安全摄像头或自主无人机，资源限制通常决定了模型选择和更新频率。优先考虑低延迟和能源效率可能需要部署压缩或量化的模型，这些模型对分布变化或对抗性扰动的鲁棒性较低。更健壮的模型可以提高安全性，但它们可能超出系统内存预算或违反功率限制。在这里，必须在硬件强加的权衡下平衡安全性、效率和可维护性。在资源受限的环境中实施负责任的 AI，效率技术和优化方法是必不可少的。

在 TinyML 平台上，由于模型被部署到没有持久连接的微控制器上，权衡变得更加明显。一个系统可能在固定数据集上的静态性能得到优化，但无法纳入新的公平约束、在更新的输入上进行重新训练或一旦部署就生成解释。硬件约束从根本上塑造了在资源受限设备上可行的负责任 AI 实践。价值冲突不仅在于模型优化了什么，还在于系统在部署后能够支持什么。

这些例子清楚地表明，规范性多元主义不是一个抽象的哲学挑战；它是一个反复出现的系统约束。诸如多目标优化、约束训练和公平感知评估等技术方法可以帮助揭示和形式化权衡，但它们并不能消除对判断的需求。关于代表哪些价值观、减轻哪些伤害以及如何平衡相互冲突的目标的决定，不能通过算法来做出。这些决策需要深思熟虑、利益相关者的输入以及超越模型本身的治理结构。

参与式和价值观敏感的设计方法提供了前进的潜在途径。这些方法不是将价值观视为部署后要优化的参数，而是在需求阶段就寻求与利益相关者互动，明确定义伦理权衡，并追踪它们如何在系统架构中实现。虽然没有任何设计过程能够同时满足所有价值观，但那些对其权衡透明并开放修订的系统更有可能随着时间的推移维持信任和问责制。

机器学习系统不是中立的工具。它们嵌入并实施价值判断，无论是明确指定的还是隐含假设的。对负责任的 AI 的承诺需要承认这一事实，并构建能够反映和响应其操作环境中的伦理和社会多元主义的系统。

解决这些价值冲突需要不仅仅是技术解决方案——它需要透明度和可争议性的机制，允许利益相关者理解和挑战系统决策。

### 透明度和可争议性

透明度被广泛认为是负责任机器学习的基础原则。它使用户、开发者、审计员和监管机构能够理解系统的工作方式，评估其局限性，并确定伤害的来源。然而，透明度本身并不足够。在高风险领域，个人和机构不仅需要理解系统行为，还必须在必要时能够挑战、纠正或逆转它。这种可争议性，即质疑和挑战系统决策的能力，是问责制的一个重要特征。

机器学习系统的透明度通常关注于披露：揭示模型是如何训练的，它们依赖哪些数据，它们设计中嵌入的假设是什么，以及哪些已知的限制影响了它们的使用。模型卡片和数据集的数据表等文档工具通过将系统元数据正式化在结构化、可重复的格式中，支持这一目标。这些资源可以提高治理水平，支持合规性，并告知用户期望。然而，作为披露的透明度并不能保证有意义的控制。即使技术细节可用，用户可能缺乏机构的使用、界面工具或程序性访问来质疑对他们产生不利影响的决策。

要从透明度过渡到可质疑性，机器学习系统必须设计有解释、救济和反馈的机制。解释指的是系统提供可理解的理由以支持其输出的能力，这些理由针对接收者的需求和情境量身定制。救济指的是个人改变其情况并获得不同结果的能力。反馈指的是用户报告错误、质疑结果或发出关注，并且这些信号被纳入系统更新或监督流程的能力。

这些机制在实践中往往缺乏，尤其是在大规模部署或嵌入低资源设备中的系统。例如，在移动贷款申请系统中，用户可能会收到没有解释的拒绝，并且没有机会提供额外信息或上诉决策。即使其他地方存在文档，界面级别的透明度不足使得系统实际上无法被质疑。同样，在临床环境中部署的预测模型可能会生成一个指导治疗决策的风险评分，而不向医生展示其背后的推理。如果模型对特定患者群体表现不佳，而这种行为不可观察或不可质疑，结果可能是无法轻易诊断或纠正的无意伤害。

从系统角度来看，实现可质疑性需要跨技术和管理组件的协调。模型必须提供足够的信息以支持解释。界面必须以可用和及时的方式展示这些信息。必须建立组织流程来审查反馈、回应上诉和更新系统行为。日志和审计基础设施必须跟踪不仅模型输出，还包括用户干预和覆盖决策。在某些情况下，包括人工干预覆盖和决策放弃阈值在内的一些技术保障措施，也可能通过确保模糊或高风险决策推迟到人类判断来服务于可质疑性。

可行的可竞争程度因部署环境而异。在集中式云平台上，可能可以提供完整的解释 API、用户仪表板和申诉工作流程。相比之下，在边缘和 TinyML 部署中，可竞争性可能仅限于基于批量同步反馈的日志记录和定期更新。在所有情况下，机器学习系统设计都必须承认透明性不仅仅是技术披露的问题。它是系统的一个结构属性，决定了用户和机构是否能够有意义地质疑、纠正和治理自动化决策的行为。

实施有效的透明度和可竞争性机制需要机构支持和治理结构，这些结构超越了单个技术团队。

### 责任的机构嵌入

机器学习系统并非独立运作。它们的发展、部署和持续管理嵌入在包括技术团队、法律部门、产品所有者、合规官员和外部利益相关者的机构环境中。在这样的系统中，责任不是单个行为者或组件的属性——它是分布在不同角色、工作流程和治理过程中的。因此，设计负责任的 AI 需要关注这些系统构建和使用的机构环境。

这种责任分布的特性既带来了机遇也带来了挑战。一方面，多个利益相关者的参与提供了制衡，有助于防止有害结果。另一方面，责任的扩散可能导致问责制差距，当出现问题时，可能不清楚问题出在数据管道、模型架构、部署配置、用户界面或周围的组织环境中。

一个典型的案例是谷歌流感趋势，这是由于机构不匹配而广泛引用的失败例子。该系统试图从搜索数据中预测流感爆发，最初表现良好，但由于用户行为的变化和数据分布的转移，逐渐与现实脱节。这些问题多年未得到纠正，部分原因是因为没有建立系统验证、外部审计或模型性能下降时的升级流程。失败并非由于单个技术缺陷，而是由于缺乏能够应对漂移、不确定性和来自开发团队外部的反馈的机构框架。

在机构中嵌入责任需要不仅仅是分配问责制。它需要设计流程、工具和激励措施，以允许负责任的行为。如版本化模型注册表、模型卡片和审计日志等技术基础设施必须与如伦理审查委员会、模型风险委员会和红队程序等组织结构相结合。这些机制确保技术见解是可操作的，反馈在团队间得到整合，并且用户、开发者或监管机构提出的问题得到系统性的解决，而不是临时性的。

所需的机构支持水平因部署环境而异。在大规模云平台上，治理结构可能包括内部问责审计、合规工作流程以及负责监控系统行为的专门团队。在较小规模的部署中，包括嵌入医疗设备或公共基础设施中的边缘或移动系统，治理可能依赖于跨职能的工程实践和外部认证或监管。在 TinyML 部署中，由于连接性和可观察性有限，机构责任可能通过上游控制来行使，例如安全重要的验证、嵌入式安全约束和部署固件的整个生命周期跟踪。

在所有情况下，负责任的机器学习都需要技术系统和机构系统之间的协调。这种协调必须扩展到整个模型生命周期——从最初的数据采集和模型训练到部署、监控、更新以及最终的退役。它还必须纳入外部参与者，包括领域专家、民间社会组织和监管机构，以确保责任不仅限于开发团队，而且在整个机器学习系统运行的更广泛生态系统中得到行使。

责任不是一个模型或团队的静态属性；它是系统如何治理、维护和随时间争议的动态属性。通过政策、基础设施和问责机制将这种责任嵌入机构中，对于使机器学习系统与它们旨在服务的社交价值和运营现实相一致至关重要。

这些关于机构责任和价值冲突的考虑表明，负责任的 AI 实施不仅超越了技术解决方案，还包括更广泛的关于访问、参与和环境影响的议题。前一小节中探讨的计算资源需求创造了系统性障碍，决定了谁可以开发、部署并从负责任的 AI 能力中受益——将负责任的 AI 从个体系统属性转变为集体社会挑战。

本节探讨了本节中探讨的社会技术考虑因素——创建自我强化的差异的系统反馈循环、自动化偏见和算法厌恶等人类-人工智能协作挑战、利益相关者价值观的规范性多元主义以及计算公平性差距——揭示了为什么仅凭第 17.5 节中的技术基础无法确保负责任的人工智能。这些动态在算法、人类、组织和社会的交汇处运作，静态的公平性指标证明是不够的，而竞争性价值观无法通过算法调和。然而，即使在明确的原则和可靠的技术方法下，将负责任的人工智能转化为实际操作实践也面临着巨大的实施挑战。

## 实施挑战

上述考察的技术基础和社会技术动态确立了负责任的人工智能系统应实现的目标，但实质性的障碍阻止了这些能力在实际操作中有效运行。考虑一下之前探讨的方法如何遇到组织障碍：例如，公平性检测算法如 Fairlearn 需要持续的数据收集和监控基础设施，但许多组织缺乏针对公平性指标的执行流程。差分隐私机制需要仔细的参数调整和性能监控，然而团队可能缺乏在隐私与效用权衡方面的专业知识。可解释性框架生成特征归因分数，但如果没有设计出使解释对受影响用户可访问的系统，这些技术能力无法提供实际效益。

这些例子说明了技术能力与实际操作实施之间的基本差距。虽然负责任的人工智能方法提供了必要的工具，但它们的有效性完全取决于组织结构、数据基础设施、评估流程以及超越算法开发的持续承诺。理解这些实施挑战对于构建能够在长时间内保持负责任行为的系统至关重要，而不仅仅是实现初始部署时的目标。

本节通过使用提供分析实施障碍系统结构的经典人-流程-技术框架，探讨了将负责任的人工智能实践嵌入到生产机器学习系统中的实际挑战。

**人员挑战**包括组织结构、角色定义、激励一致性和利益相关者协调，这些因素决定了负责任的人工智能原则是否能够转化为持续的组织行为。**流程挑战**涉及标准化差距、生命周期维护程序、竞争优化目标和评估方法，这些因素影响负责任的人工智能实践如何与开发工作流程相结合。**技术挑战**包括数据质量限制、计算资源限制、可扩展性瓶颈和基础设施缺口，这些因素决定了负责任的人工智能技术是否能够在生产规模上有效运行。

这些挑战共同展示了理想化原则与实际操作现实之间的摩擦。理解它们之间的相互联系对于开发将责任嵌入到机器学习部署的架构、基础设施和工作流程的系统级策略至关重要。

以下分析通过三个相互关联的视角来审视实施障碍，认识到有效的负责任人工智能需要协调解决方案，同时解决这三个维度。

### 组织结构及激励机制

负责任机器学习的实施不仅受技术可行性的影响，还受系统开发和部署的组织环境的影响。在公司、研究实验室和公共机构中，责任必须转化为具体的角色、工作流程和激励机制。然而，在实践中，组织结构往往分散责任，使得在工程、产品、法律和运营团队之间协调道德目标变得困难。

负责任的人工智能需要持续投资于诸如子群性能评估、可解释性分析、对抗性鲁棒性测试以及集成差分隐私或联邦训练等隐私保护技术的实践。这些活动可能耗时且资源密集，但它们往往超出了用于评估团队生产力的正式绩效指标。例如，团队可能会被激励快速发布功能或达到性能基准，即使这样做会损害公平性或忽视潜在的危害。当道德尽职调查被视为一项可选择的任务，而不是系统生命周期的集成组成部分时，它就会在截止日期压力或组织动荡下变得容易受到优先级降低的影响。

由于所有权的不确定性，责任问题进一步复杂化。在许多组织中，没有单一团队负责确保系统在一段时间内表现出道德行为。模型性能可能由一个团队拥有，用户体验由另一个团队拥有，数据基础设施由第三个团队拥有，合规性由第四个团队拥有。当出现问题时，包括预测中的不同影响或解释质量不足，可能没有明确的协议来识别根本原因或协调缓解措施。因此，开发者、用户或审计员提出的问题可能得不到解决，这不是因为恶意意图，而是由于缺乏流程和跨职能协调。

建立有效的组织结构以负责人工智能不仅需要政策声明。它需要操作机制：指定具有道德监督责任的职位，明确定义的升级途径，对部署后监控的责任，以及奖励团队进行道德前瞻性和系统可维护性的激励措施。在一些组织中，这可能采取负责任的人工智能委员会、跨职能审查委员会或与开发者一起在整个模型生命周期中工作的模型风险团队的形式。在其他组织中，领域专家或用户倡导者可能被嵌入到产品团队中，以预测下游影响并评估情境中的价值权衡。

如图 17.8 所示，道德系统行为的责任分布在多个利益相关者之间，包括行业、学术界、民间社会和政府。在组织中，这种分布必须通过将技术设计与技术监督和操作控制相连接的机制来反映。没有这些联系，责任就会变得模糊，良好的意图可能会被系统性的不协调所破坏。

![图片](img/file297.svg)

图 17.8：**利益相关者责任**：有效的人本人工智能实施需要行业、学术界、民间社会和政府之间的共同责任，以解决道德考虑和系统性风险。这些不同的群体塑造技术设计、战略监督和操作控制，确保在整个模型生命周期中负责任的人工智能开发和部署。来源：(Shneiderman 2020)。

负责任的人工智能不仅仅是技术卓越或监管合规的问题。这是一个系统级挑战，需要将道德目标与机器学习系统设计、部署和维护的机构结构相一致。创建和维持这些结构对于确保责任不仅嵌入到模型中，还嵌入到管理其使用的组织中至关重要。

除去组织挑战之外，团队还面临着与数据质量和可用性相关的重大技术障碍。

### 数据限制和质量差距

尽管广泛认识到数据质量对于负责任的机器学习的重要性，但在实践中，提高数据管道仍然是实施中最困难的问题之一。开发人员和研究人员通常理解代表性数据、准确标注和历史偏差缓解的重要性。然而，即使意图明确，结构和组织障碍经常阻止有意义的干预。数据责任通常分散在各个团队之间，受制于遗留系统，或者嵌入在难以改变更广泛的机构流程中。

第六章中涵盖的数据工程原则——包括数据验证、模式管理、版本控制、血缘跟踪和质量监控——为解决这些挑战提供了技术基础。然而，将这些原则应用于负责任的 AI 引入了额外的复杂性：公平性需要评估人口统计群体中的代表性，偏差缓解需要理解历史数据收集实践，隐私保护则限制了哪些验证技术是允许的。这里描述的组织挑战反映了拥有强大的数据工程基础设施和使用它来有效支持负责任 AI 目标之间的差距。

子群不平衡、标签模糊性和分布偏移，这些问题都会影响跨领域的泛化能力和性能，在负责任的机器学习中是众所周知的问题。这些问题通常以校准不良、分布外失败或在评估指标中的人口统计差异的形式出现。然而，在现实环境中解决这些问题需要不仅仅是技术知识。它需要访问相关数据、机构对补救措施的支持，以及足够的时间和资源来迭代数据集本身。在许多机器学习流程中，一旦数据收集完成并定义了训练集，数据管道实际上就变得冻结了。团队可能既缺乏修改或扩展数据集的权威性，也缺乏基础设施，即使发现了性能差异也是如此。即使在具有自动化验证和特征存储的现代数据管道中，一旦数据集版本和数据血缘被锁定到生产中，事后纠正训练分布仍然困难。

在医疗保健、教育和社会福利等领域，这些挑战尤为突出。数据获取可能受到法律约束、隐私法规或跨组织协调的限制。例如，一个开发分级模型的团队可能会发现他们的训练数据未能充分代表来自较小或农村医院的病人。纠正这种不平衡需要与外部合作伙伴协商数据访问、统一特征标准以及解决标签实践中的不一致性。即使所有各方都同意需要改进，这些后勤和运营成本也可能具有威慑力。

收集更具代表性数据的努力也可能遇到伦理和政治方面的担忧。在某些情况下，额外的数据收集可能会使边缘化群体面临新的风险。这种暴露的悖论，即那些最被排除在外的人也是最容易受到滥用伤害的人，使得通过数据集扩展来提高公平性的努力复杂化。例如，为了支持性别敏感应用中的公平性，收集更多关于非二元个体的数据可能会提高模型覆盖范围，但也引发了关于同意、可识别性和下游使用的严重担忧。团队必须谨慎地处理这些紧张关系，通常在没有明确机构指导的情况下。

即使数据量充足，数据收集系统中的上游偏见也可能无法得到控制。许多组织依赖第三方数据供应商、外部 API 或运营数据库，这些数据库在设计时并未考虑公平性或可解释性。例如，在临床机器学习中常用的电子健康记录，通常反映了护理中的系统性差异，以及编码种族或社会经济偏见的记录习惯（Himmelstein, Bates, and Zhou 2022）。下游工作的团队可能对如何创建这些记录几乎没有任何了解，并且几乎没有解决嵌入的损害的杠杆。

提高数据集质量通常不是任何一团队的职责。数据管道可能由基础设施或分析团队维护，这些团队独立于机器学习工程或模型评估团队运作。这种组织上的碎片化使得协调数据审计、追踪数据来源或实施将模型行为与潜在数据问题相连接的反馈循环变得困难。在实践中，数据集质量的责任往往被忽视——虽然认识到其重要性，但很少被优先考虑或分配资源。

解决这些挑战需要长期投资于基础设施、工作流程和跨职能沟通。技术工具，如数据验证、自动审计和数据集文档框架（例如，模型卡片、数据表或[数据营养项目](https://datanutrition.org/)）可以帮助，但只有当它们嵌入到拥有行动权限和支持的团队中时才能发挥作用。提高数据质量不仅仅是更好的工具问题，而是关于如何在整个系统生命周期中分配、共享和维持数据责任的问题。

即使解决了数据质量挑战，团队在平衡多个竞争目标时仍面临额外的复杂性。

### 平衡竞争目标

机器学习系统设计通常被描述为一个优化过程——提高准确性、减少损失或最大化效用。然而，在负责任的机器学习实践中，优化必须与一系列竞争目标相平衡，包括公平性、可解释性、鲁棒性、隐私和资源效率。这些目标并不总是协调一致，一个维度的改进可能涉及另一个维度的权衡。虽然这些紧张关系在理论上得到了很好的理解，但在现实世界系统中管理它们是一个持续存在且未解决的挑战。

考虑模型准确性与可解释性之间的权衡。在许多情况下，包括浅层决策树和线性模型在内的更可解释的模型，其预测性能低于复杂的集成方法或深度神经网络。在低风险应用中，这种权衡可能是可接受的，甚至更受欢迎。但在高风险领域，如医疗保健或金融，决策会影响个人的福祉或机会获取，团队往往陷入对性能的需求和透明推理的需求之间。即使开发过程中优先考虑可解释性，在部署时也可能为了模型准确性的微小提升而牺牲可解释性。

在个性化与公平性之间也存在类似的紧张关系。一个旨在最大化用户参与度的推荐系统可能会采取激进的个性化策略，使用细粒度的行为数据来调整输出以适应单个用户。虽然这种方法可能提高某些用户的满意度，但它可能会加剧不同人口群体之间的差异，尤其是如果个性化依赖于与种族、性别或社会经济地位相关的特征。添加公平性约束可能会在群体层面上减少差异，但代价是降低某些用户感知到的个性化程度。这些影响往往难以衡量，而且在压力下优化参与度指标的产品团队中，解释这些影响更加困难。

隐私引入了另一组约束。诸如差分隐私、联邦学习或本地数据最小化等技术可以有意义地降低隐私风险。但它们也引入了噪声，限制了模型容量，或减少了训练数据的访问。在集中式系统中，这些成本可能通过基础设施扩展或混合训练架构来吸收。然而，在边缘或 TinyML 部署中，这些权衡更为尖锐。一个负责本地推理的可穿戴设备必须经常在模型复杂性、能耗、延迟和隐私保证之间进行权衡。支持一个约束通常会削弱另一个，迫使系统设计者在同等重要的目标之间进行优先级排序。这些紧张关系通过特定部署的设计决策进一步放大，例如量化级别、激活裁剪或压缩策略，这些都影响模型同时支持多个目标的有效性。

这些权衡并非纯粹的技术问题——它们反映了关于系统设计目标及其面向对象的更深层次的规范性判断，这在第 17.6.3 节中已有详细探讨。负责任的机器学习开发需要明确这些判断，在特定情境中评估它们，并使它们接受利益相关者和机构的监督。

使这一挑战在实施中尤其困难的是，这些相互竞争的目标很少由单一团队或功能拥有。性能可能由建模团队优化，公平性由负责任的 AI 团队监控，隐私由法律或合规部门处理。没有明确的协调，系统级别的权衡可能被隐性地、零散地做出，或者没有对长期后果的可见性。随着时间的推移，结果可能是一个在孤立状态下表现良好的模型，但嵌入到生产基础设施中时却未能达到其伦理目标。

平衡相互竞争的目标不仅需要技术熟练，还需要对透明度、深思熟虑和团队间的协调一致做出承诺。系统必须设计成揭示权衡而不是掩盖它们，为约束感知的开发留出空间，而不是追求狭隘的优化。在实践中，这可能需要重新定义“成功”的含义——不是指单一指标上的表现，而是指系统行为与其在更广泛的社会或运营环境中的预期角色之间的持续一致。

在这些前三个挑战——组织结构、数据质量和竞争目标中——出现了一种模式：负责任的人工智能失败很少源于技术无知。团队了解公平性指标、隐私技术和偏差缓解方法。相反，失败发生在组织碎片化的交汇处，这种碎片化将责任分配出去但没有问责制，数据约束在明确意图的情况下甚至创造了技术障碍，以及竞争目标迫使规范权衡，这些权衡被伪装成技术问题。当建模团队优化性能，合规团队处理隐私，产品团队独立优先考虑参与度时，系统级别的道德行为是偶然出现的而不是设计出来的。这些问题本质上是跨组织边界的社会技术治理问题，需要明确的所有权结构，设计用于道德审计的数据基础设施，以及明确价值权衡的审议过程。当系统必须随着时间的推移在规模上维持负责任的行为时，这些挑战变得更加尖锐。

### 可扩展性和维护

负责任的机器学习实践通常在模型开发的早期阶段被引入：在初始评估期间进行公平性审计，在模型选择期间应用可解释性方法，并在训练期间考虑隐私保护技术。然而，随着系统从研究原型过渡到生产部署，这些实践往往退化或消失。在原则上可能实现与在生产中可持续实现之间的差距是负责任人工智能的核心实施挑战。

许多负责任的人工智能干预措施并没有考虑到可扩展性。公平性检查可能在一个静态数据集上执行，但并未整合到持续的数据摄入管道中。解释方法可能使用开发时间工具开发，但从未转化为可部署的用户界面。隐私约束可能在训练期间实施，但在部署后监控或模型更新期间被忽视。在每种情况下，开始时作为负责任设计意图的东西，在系统扩展和生命周期变化中未能持续。

生产环境引入了新的压力，这些压力重塑了系统优先级。模型必须在不同的硬件配置上运行，与不断发展的 API 接口，以低延迟为百万用户提供服务，并在操作压力下保持可用性。例如，在 CPU、GPU 和边缘加速器之间保持一致行为需要框架抽象、运行时调度器和硬件特定编译器之间的紧密集成。这些限制要求持续适应和快速迭代，通常会导致难以自动化或衡量的活动被降级。负责任的 AI 实践，特别是涉及人工审查、利益相关者咨询或事后评估的实践，可能难以轻松地融入快速发展的 DevOps35 管道。

维护引入了更多的复杂性。机器学习系统很少是静态的。新数据被摄入，重新训练被执行，功能被弃用或添加，使用模式随时间变化。在没有严格的版本控制、变更日志和影响评估的情况下，很难追踪系统行为如何演变，或者与责任相关的属性，如公平性或鲁棒性是否得到了保留。组织的人员流动和团队重组可能会侵蚀机构记忆。负责维护已部署模型的团队可能不是最初开发和审计它的团队，这可能导致系统目标与当前实施之间出现无意的不一致。这些问题在持续学习或流式学习场景中尤其严重，因为概念漂移和数据分布的变化需要积极的监控和实时更新。

这些挑战在多模型系统和跨平台部署中更加突出。推荐引擎可能包含数十个相互作用的模型，每个模型都针对不同的子任务或用户群体进行了优化。在移动和边缘环境中部署的语音助手可能维护同一模型的不同版本，这些版本针对本地硬件限制进行了调整。在如此分布式的系统中协调更新、确保一致性和维持负责任的行为，需要的基础设施不仅要跟踪代码和数据，还要跟踪价值和约束。

解决可扩展性和维护挑战需要将负责任的 AI 视为生命周期属性，而不是一次性评估。这意味着将审计钩子、元数据跟踪和监控协议嵌入到系统基础设施中。这也意味着创建跨团队过渡持续存在的文档，定义在项目移交中存活的问责结构，并确保系统更新不会无意中抹去在公平性、透明度或安全性方面取得的来之不易的改进。虽然这些做法可能难以事后实施，但它们可以通过默认负责任的工具和工作流程从系统设计之初就集成进去。

责任必须与系统规模相匹配。在实际环境中部署的机器学习模型不仅必须在启动时符合伦理标准，而且随着其复杂性、用户范围和运营范围的扩大，必须继续符合这些标准。实现这一点需要持续的机构投资和架构规划——而不仅仅是某一时间点的技术正确性。

### 标准化和评估差距

尽管负责任的机器学习领域已经产生了大量工具、指标和评估框架，但在如何系统地评估系统在实际操作中是否负责任的问题上，仍然缺乏共识。许多团队认识到公平性、隐私、可解释性和鲁棒性的重要性，但他们往往难以将这些原则转化为一致、可衡量的标准。基准测试方法为标准化评估提供了有价值的框架，尽管将这些方法适应负责任的 AI 指标仍然是发展的一个活跃领域。缺乏正式的评估标准，加上工具和框架的碎片化，对大规模实施负责任的 AI 构成了重大障碍。

这种碎片化现象在机构之间以及机构内部都十分明显。学术研究经常引入新的公平性或鲁棒性指标，这些指标在实验设置之外难以复制。相比之下，工业团队必须优先考虑与生产基础设施无缝集成、非专业人士可解释且可以随时间监控的指标。因此，在一个环境中开发的实践可能无法很好地转移到另一个环境中，并且跨系统性能比较可能不可靠或具有误导性。例如，在一个基准数据集上使用人口统计学平等性评估的公平性模型，可能不符合另一个领域或司法管辖区中均衡机会的要求。没有共享标准，这些评估仍然是临时的，这使得在跨情境中建立对系统负责任行为的信心变得困难。

负责任的 AI 评估也面临着分析单元与部署水平之间的不匹配问题，分析单元通常是单个模型或批量作业，而部署水平包括数据摄取管道、特征转换、推理 API、缓存层以及人机交互工作流程等端到端系统组件。一个在孤立状态下看似公平或可解释的系统，一旦集成到更广泛的应用中，可能就无法保持这些特性。支持整体、系统级评估的工具仍处于发展阶段，而且关于如何评估现代机器学习堆栈中交互组件的责任几乎没有指导。

更进一步，缺乏生命周期感知的指标也使问题复杂化。大多数评价工具都是在单一时间点应用——通常是在部署之前。然而，负责任的 AI 属性，如公平性和鲁棒性，是动态的。它们取决于数据分布如何演变，模型如何更新，以及用户如何与系统互动。没有持续或定期的评价，很难确定系统在部署后是否仍然与其预期的道德目标保持一致。部署后监控工具存在，但它们很少与用于评估初始模型质量的开发时间指标集成。这种脱节使得检测道德性能的漂移或追踪观察到的伤害回溯到其上游来源变得困难。

工具碎片化进一步加剧了这些挑战。负责任的 AI 工具通常分布在分离的包、仪表板或内部系统中，每个系统都针对特定的任务或指标进行设计。一个团队可能使用一个工具进行可解释性分析，另一个工具进行偏差检测，第三个工具进行合规性报告——没有统一的界面来推理系统级别的权衡。缺乏互操作性阻碍了团队之间的协作，复杂了文档，并增加了重要评价被跳过或执行不一致的风险。这些挑战因缺少在特征存储、推理网关和模型注册库等组件之间传播元数据或事件记录的钩子而加剧。

解决这些差距需要从多个方面取得进展。首先，必须开发共享的评价框架，这些框架定义了系统如何负责任地行为——不仅是在抽象的术语中，而且是在可衡量、可审计的标准中，这些标准在各个领域都有意义。其次，评价必须扩展到超越单个模型，涵盖完整的系统管道，包括面向用户的界面、更新策略和反馈机制。最后，评价必须成为一种持续的生命周期活动，由能够跟踪系统行为随时间变化并提醒开发者当道德属性下降的基础设施支持。

没有标准化的、系统感知的评价方法，负责任的 AI 仍然是一个移动的目标——在原则中被描述，但在实践中难以验证。建立对机器学习系统的信心不仅需要更好的模型和工具，还需要共享规范、持久指标以及反映部署 AI 操作现实性的评价实践。

负责任的 AI 不能通过孤立干预或静态合规性检查来实现。它需要架构规划、基础设施支持和制度流程，以在整个系统生命周期内维持道德目标。随着机器学习系统规模的扩大、多样化以及嵌入敏感领域，必须支持在模型选择时间以及重新训练、量化、服务和监控阶段强制执行公平性、鲁棒性和隐私等属性。如果没有持续的监督，负责任的做法会随着系统的发展而退化——特别是当工具、指标和文档没有设计用来在部署及之后跟踪和保存它们时。

应对这一挑战需要更高的标准化、将责任意识实践更深入地集成到 CI/CD 管道中，以及长期投资于支持道德远见的系统基础设施。目标不是在代码中完善道德决策，而是使责任成为一个操作属性——可追踪、可测试，并与大规模机器学习系统的约束和功能相一致。

### 实施决策框架

鉴于这些实施挑战，实践者需要系统的方法来根据部署上下文和利益相关者的需求优先考虑负责任的 AI 原则。在设计机器学习系统时，实践者必须在维护与系统风险和约束相适应的道德保障的同时，在相互竞争的目标之间进行权衡。表 17.4 提供了一个决策框架，用于做出这些上下文敏感的选择。

**决策启发式方法**：

+   **当多个原则冲突时**：与利益相关者合作，确定哪些危害最为严重。在第 17.6.3 节中考察的心理健康聊天机器人示例表明，此类冲突需要深思熟虑，而不是算法解决。

+   **当计算预算受限时**：根据风险优先级原则。高风险决策即使在重大成本下也要求公平性和可解释性。低风险应用可以使用轻量级方法。

+   **当部署上下文发生变化时**：重新评估原则优先级。从云端模型迁移到边缘会失去集中监控能力——通过部署前的验证和本地安全措施来补偿。

+   **当利益相关者的价值观不同时**：明确记录权衡并创建可争议机制，允许受影响的用户挑战决策。

表 17.4：**实践者决策框架**：根据部署上下文优先考虑负责任的 AI 原则，显示主要原则、实施优先级和不同系统类型的可接受权衡。此框架指导实践者在原则冲突或资源受限时做出适合上下文的决策。

| **部署环境** | **主要原则** | **实施优先级** | **可接受的权衡** |
| --- | --- | --- | --- |
| **高风险个人决策** **(医疗诊断、信贷/贷款、刑事司法、就业)** | 公平性、可解释性、问责制 | 在受保护群体中强制执行公平性指标；对负面结果的解释；边缘案例的人类监督 | 接受 2-5%的准确性降低以实现可解释性；解释的延迟为 20-100 毫秒；更高的计算成本 |
| **关键安全系统** **(自动驾驶汽车、医疗设备、工业控制)** | 安全性、鲁棒性、问责制 | 认证的对抗性防御；形式验证；安全机制；全面的日志记录 | 接受显著的训练开销（对抗性训练为 100-300%）；保守的置信阈值；冗余推理 |
| **隐私敏感应用** **(健康记录、财务数据、个人通讯)** | 隐私、安全、透明度 | 差分隐私（ε≤1.0）；本地处理；数据最小化；用户同意机制 | 接受 2-5%的准确性损失以实现 DP；更高的客户端计算；有限的模型更新；减少个性化 |
| **大规模消费者系统** **(内容推荐、搜索、广告)** | 公平性、透明度、安全性 | 在人口统计学上监测偏差；解释机制；内容政策执行；反馈循环检测 | 平衡可解释性成本与规模（流式 SHAP 与完整 SHAP）；接受 5-15 毫秒的公平性检查延迟；投资于监控基础设施 |
| **资源受限部署** **(移动、边缘、TinyML)** | 隐私、效率、安全性 | 本地推理；数据本地化；输入验证；优雅降级 | 放弃实时公平性监控；使用轻量级可解释性（SHAP 上的梯度）；仅进行部署前的验证；有限的模型复杂性 |
| **研究/探索性系统** **(内部工具、原型、A/B 测试)** | 透明度、安全性（防止伤害） | 已知限制的文档；限制用户群体；监测意外伤害 | 可以在内部使用时降低复杂的公平性/可解释性；关注可观察性和快速迭代 |

该框架提供起始指导。负责任的 AI 实施需要持续评估，因为系统、环境和公众期望都在不断演变。

随着人工智能系统自主性和能力的提升，这些实施挑战变得更加复杂。在第 17.2 节中引入的价值对齐原则——确保人工智能系统追求与人类意图和伦理规范一致的目标——在系统以更高独立性运行时显得尤为重要。虽然上述负责的人工智能技术解决了监督环境中的偏差、隐私和可解释性问题，但自主系统需要额外的安全机制来防止系统目标与人类价值观之间的不一致。

## 人工智能安全与价值对齐

随着机器学习系统获得自主性和能力，价值对齐挑战急剧扩大。上述负责的人工智能技术——偏差检测、可解释性、隐私保护——提供了基本能力，但当系统以更高独立性运行时，这些技术揭示了根本性的局限性。考虑这些既定方法在自主环境中的失效情况：

类似于 Fairlearn 中实施的方法的偏差检测算法需要持续的人类解释和纠正行动。一辆自动驾驶汽车的感觉系统可能会表现出对检测有行动辅助设备的行人的系统性偏差，但如果没有人类监督，偏差检测指标就只是记录的统计数据，没有补救途径。虽然存在测量偏差的技术能力，但自主系统缺乏判断力来确定适当的反应。

可解释性框架假设有能够解释并采取行动的人类受众。一个自主交易系统可能会为其决策生成完全准确的 SHAP 解释，但如果在系统每秒执行数千笔交易之前没有人审查这些解释，那么这些解释就变得毫无意义。系统通过其设计者从未预料到的方法优化其目标（利润），使得解释成为事后记录而不是决策辅助。

隐私保护技术，如差分隐私，保护个体数据点，但不能解决更广泛的价值不一致问题。一个自主内容推荐系统可能通过局部差分隐私保护用户隐私，同时优化促进错误信息或有害内容的参与度指标。当系统的基本目标与用户福利冲突时，技术隐私合规性变得不足。

这些例子说明了为什么负责的人工智能框架，虽然必要，但随着系统获得自主性而变得不足。这些技术假设了人类的监督、受约束的目标和相对可预测的运行环境。人工智能安全将这些担忧扩展到可能优化与人类意图不一致的目标、在不可预测的环境中运行或通过其设计者未曾预料到的方法追求目标的系统。

随着机器学习系统在自主性、规模和部署复杂性方面的增加，责任的本质超越了模型层面的公平性或隐私问题。它包括确保系统追求正确的目标，在不确定的环境中安全地行为，并且随着时间的推移保持与人类意图的一致。这些问题属于人工智能安全 36 的领域，该领域专注于防止有能力的 AI 系统产生意外或有害的结果。一个核心挑战是，今天的机器学习模型通常优化代理指标 37，例如损失函数、奖励函数或参与度信号，这些指标并不能完全捕捉到人类价值观。

一个具体的例子来自推荐系统，一个被训练以最大化点击率（CTR）38 的模型可能会最终推广那些增加用户参与度但降低用户满意度的内容，包括点击诱饵、错误信息和情感操纵材料。这种行为与代理指标一致，但与实际目标不一致，导致一个强化不理想结果的反馈循环。如图 17.9 图 17.9 所示，系统学会优化可衡量的奖励（点击量）而不是预期以人为中心的成果（满意度）。结果是出现反映规范游戏或奖励黑客 39 的行为——这是价值对齐和人工智能安全中的核心关注点。

![图片](img/file298.svg)

图 17.9：**奖励黑客循环**：最大化可衡量的奖励（如点击量）可能会激励出非预期的模型行为，这些行为会损害用户满意度的预期目标。针对代理指标进行优化会导致系统目标与期望结果之间的不一致，对人工智能安全中的价值对齐构成挑战。

1960 年，诺伯特·维纳写道：“如果我们使用一个我们无法有效干预其操作的机械机构来实现我们的目的……我们最好确保放入机器中的目的是我们想要的” (Wiener 1960)。

随着深度学习模型的能力越来越接近，并在某些情况下超过人类性能，这样的系统可能追求非预期或不希望的目标的担忧变得更加紧迫 (S. Russell 2021)。在人工智能安全领域，一个核心关注点是价值对齐问题：如何确保机器学习系统能够按照广泛的人类意图行事，而不是优化不一致的代理指标或表现出损害社会目标的涌现行为。正如 Russell 在《人类兼容的人工智能》一书中所论证的，当前的大部分人工智能研究都假设要优化的目标是已知且固定的，而将重点放在优化效果上，而不是目标本身的设计。

然而，在现实世界的部署环境中定义“正确的目的”对于智能系统来说尤其困难。机器学习系统通常在动态环境中运行，与多个利益相关者互动，并随着时间的推移进行适应。这些条件使得在静态目标函数或奖励信号中编码人类价值观变得具有挑战性。像价值敏感设计（Value Sensitive Design）这样的框架旨在通过在系统设计过程中提供正式的过程来激发和整合利益相关者的价值观，以应对这一挑战。

从整体的社会技术视角出发，这既考虑了算法机制，也考虑了系统运行的环境，对于确保对齐至关重要。没有这一点，智能系统可能会追求狭窄的性能目标（例如，准确性、参与度或吞吐量），同时产生社会不希望看到的结果。在这样条件下实现稳健的对齐仍然是机器学习系统中一个开放且重要的研究领域。

对齐的缺失可能导致已记录的失败模式，尤其是在优化复杂目标的系统中。例如，在强化学习（RL）中，模型经常学会利用奖励函数的非预期方面——这种现象被称为规范游戏或奖励黑客攻击。当未明确包含在目标中的变量以最大化奖励的方式被操纵，同时违反人类意图时，就会发生此类失败。

近年来，一种特别有影响力的方法是从人类反馈中进行强化学习（RLHF），其中大型预训练模型通过人类提供的偏好信号进行微调（Christiano 等人 2017）。虽然这种方法在标准强化学习的基础上提高了对齐度，但也引入了新的风险。Ngo (Ngo, Chan, and Mindermann 2022) 指出了 RLHF 引入的三种潜在失败模式：（1）情境感知奖励黑客攻击，其中模型利用人类的易错性；（2）出现与训练分布不一致的内部目标；（3）发展出寻求权力的行为，即使牺牲人类监督，也能保持奖励最大化能力。

这些担忧并不仅限于推测性场景。Amodei 等人 (2016) 概述了六个具体的 AI 安全挑战：（1）在策略执行期间避免负面副作用；（2）减轻奖励黑客攻击；（3）在地面真实评估昂贵或不可行时确保可扩展的监督；（4）设计安全的探索策略，以促进创造力而不增加风险；（5）在测试环境中对分布变化具有鲁棒性；（6）在任务泛化中保持对齐。随着系统规模的扩大、在多样化的环境中部署以及与实时反馈或持续学习集成，这些挑战变得更加尖锐。

这些安全挑战在需要减少人类监督的自主系统中尤为明显。

### 自主系统与信任

独立于人类监督并经常超出人类判断范围的行为自主系统的后果已在多个行业中得到了广泛记录。一个突出的近期例子是加利福尼亚州机动车辆管理局因[“对公共安全构成不合理风险”](https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html)而暂停了 Cruises 的部署和测试许可。其中一起[事件](https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html)涉及一名行人刚刚进入人行横道，此时交通灯变绿——这是一个感知和决策的边缘案例，导致了碰撞。一个更悲惨的例子发生在 2018 年，当时一辆自动驾驶优步车辆在自动驾驶模式下[未能将推自行车的行人分类为需要避让的对象](https://www.bbc.com/news/technology-54175359)，导致了一起致命事故。

尽管自动驾驶系统通常是公众关注的焦点，但类似的风险也出现在其他领域。遥控无人机和自主军事系统已经[重塑了现代战争](https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/)，不仅引发了安全和有效性问题，还提出了关于道德监督、交战规则和责任等棘手问题。当自主系统失败时，[谁应该承担责任](https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/)的问题在法律和道德上仍然悬而未决。

在其核心，这一挑战反映了人类与机器自主性之间的更深层次的紧张关系。工程和计算机科学学科历史上强调机器自主性——提高系统性能、最小化人类干预和最大化自动化。对 ACM 数字图书馆的计量分析发现，截至 2019 年，90%引用“自主性”的最受关注论文都集中在机器自主性，而不是人类自主性（Calvo 等人，2020）。生产力、效率和自动化被广泛视为默认目标，通常没有质疑它们所涉及的人类能动性和监督的假设或权衡。

然而，当系统在动态、不确定的环境中运行，且无法完全指定安全行为时，这些目标可能会使人类利益处于风险之中。这种困难被形式化为框架问题和资格问题，两者都突出了列举所有现实行动成功所需的先决条件和偶然事件的不可能性（麦卡锡 1981）。在实践中，这种限制表现为脆弱的自主性：在名义条件下看似有能力的系统，在面临模糊性或分布变化时可能会无声或危险地失败。

为了解决这个问题，研究人员提出了形式化的安全框架，如责任敏感安全（RSS）(沙莱夫-施瓦茨、沙玛和沙舒亚 2017），它将抽象的安全目标分解为对系统行为的数学定义约束——例如最小距离、制动配置文件和优先通行条件。这些公式允许在特定的假设和场景下验证安全属性。然而，这种方法仍然容易受到它们旨在解决的问题的相同限制：它们的效果取决于编码到其中的假设，并且通常需要广泛的领域建模，这可能无法很好地推广到未预见的边缘情况。

一种替代方法强调以人为中心的系统设计，确保人类的判断和监督在自主决策中始终处于核心地位。价值敏感设计（弗里德曼 1996）建议通过明确考虑能力、复杂性、误表示和用户控制的流动性等因素，将用户价值观纳入系统设计。最近，METUX 模型（用户体验中的动机、参与和繁荣）通过识别六个“技术体验领域”——采用、界面、任务、行为、生活和社区，来扩展这种思考，这些领域影响技术如何支持或破坏人类的繁荣（彼得斯、卡洛沃和瑞恩 2018）。这些想法根植于自我决定理论（SDT），该理论将自主性定义为在技术意义上的控制，而不是按照个人的价值观和目标行动的能力（瑞恩和德西 2000）。

在机器学习系统的背景下，这些观点强调了设计架构、界面和反馈机制以保持人类能动性的重要性。例如，优化参与度指标的推荐系统可能会通过不透明的方式塑造用户偏好，从而干扰行为自主性。通过在 METUX 的六个领域内评估系统，设计师可以预测并减轻损害有意义自主性的下游影响，即使短期系统性能看似最优。

除去技术安全考虑之外，自主人工智能系统的部署引发了更广泛的社会担忧，即经济动荡。

### 人工智能自动化经济影响

在采用人工智能技术的过程中，一个反复出现的问题是广泛的工作岗位转移的可能性。随着机器学习系统能够执行越来越复杂的认知和物理任务，人们越来越担心它们可能会取代现有工人，并减少整个行业可替代就业机会的可用性。这些担忧在具有良好结构化任务的行业中尤为严重，包括物流、制造和客户服务，在这些行业中，基于人工智能的自动化在技术上可行且具有经济激励。

然而，自动化的经济影响在历史上并非前所未有。包括工业机械化和计算机化在内的先前技术变革往往导致就业岗位的转移而不是绝对数量的岗位损失（Shneiderman 2022）。自动化通常降低商品和服务的成本并提高质量，从而扩大了获取渠道并推动了需求。这种需求反过来又创造了新的生产、分销和支持工作形式——有时在相邻行业，有时在以前不存在的新角色中。

工业机器人和流程自动化的实证研究进一步挑战了“无人工厂”的可行性，这种系统旨在在没有人类监督的情况下完全自主运行。尽管数十年来一直在努力，但大多数实现这一自动化水平的尝试都未能成功。根据麻省理工学院未来工作工作组(Work of the Future 2020)的研究，这些努力往往导致零和自动化，其中生产力的提高是以系统灵活性、适应性和容错性为代价的。人类工作者对于需要情境判断、跨领域泛化或系统级调试的任务仍然很重要——这些能力在机器学习模型或自动化框架中仍然难以编码。

相反，工作组倡导一种正和自动化方法，这种方法增强人类工作而不是取代它。该策略强调将人工智能系统整合到工作流程中，其中人类保持监督和控制，例如半自主装配线或协作机器人。它还建议自下而上地识别可自动化任务，优先考虑那些减少认知负荷或消除危险工作，同时选择适当的指标，这些指标可以捕捉效率和弹性。仅基于吞吐量或成本最小化的指标可能会无意中惩罚需要人工介入的设计，而与安全、可维护性和长期适应性相关的更广泛的指标则提供了对系统性能的更全面看法。

尽管如此，长期的经济轨迹并不能消除近期中断的现实。那些技能被自动化取代的工人可能会面临工资停滞、谈判能力下降或长期失业——尤其是在缺乏再培训机会或劳动力市场流动性时。公共和立法努力将在塑造这一转型中发挥重要作用，包括促进自动化利益公平获取的政策。人工智能的积极应用展示了负责任地部署如何创造有益的经济机会，同时解决社会挑战。这些可能包括提升技能计划、社会安全网、最低工资增加以及确保人工智能的分配影响得到监测和解决的企业问责框架。

解决这些经济问题不仅需要深思熟虑的政策，还需要关于人工智能能力和局限性的有效公众沟通。

### 人工智能素养与沟通

1993 年对 3000 名北美成年人关于“电子思维机器”信念的调查揭示了早期计算的两个主导观点：“人类的有益工具”和“令人敬畏的思维机器” (Martin 1993)。后者反映了人们对计算机的神秘、智能和可能无法控制的看法——“比人聪明、无限、快速且令人恐惧。”这些看法虽然已经过去了几十年，但在机器学习系统时代仍然相关。随着创新步伐的加快，负责任的人工智能发展必须伴随着清晰和准确的科学沟通，特别是关于人工智能技术的能力、局限性和不确定性的沟通。

随着现代人工智能系统超越普通人的理解并开始影响高风险决策，公众叙事往往在乌托邦和反乌托邦的极端之间两极分化。这不仅仅是因为媒体框架，更是由于一个更根本的困难：在技术先进的社会中，科学系统的输出往往被视为神奇的——“只能从它所做的事情来理解，而不是它是如何工作的” (Handlin 1965)。没有技术理解的支撑，像生成模型、自主代理或大规模推荐平台这样的系统可能会被误解或不受信任，阻碍有信息的公众讨论。

在这个领域，科技公司负有责任。夸大的声明、拟人化的营销或模糊的产品发布加剧了炒作和失望的循环，侵蚀了公众的信任。但提高 AI 素养需要的不只是企业信息的克制，还需要对 AI 背景下的科学传播进行系统研究。尽管现代机器学习对社会有影响，但 Scopus 学术数据库的分析发现，只有少数论文涉及“人工智能”和“科学传播”这两个领域(Schäfer 2023)。

解决这一差距需要关注关于 AI 的叙事是如何形成的——不仅由公司，还包括学术机构、监管机构、记者、非营利组织和政策倡导者。这些行为者使用的框架和隐喻极大地影响了公众对 AI 系统中代理、风险和控制的认识(Lindgren 2023)。这些认识反过来又影响了对 AI 的采用、监督和抵制，尤其是在教育、医疗保健和就业等领域，AI 的部署直接与生活经验相交。

从系统角度来看，公众的理解不是外部性——它是部署背景的一部分。关于 AI 系统如何工作的错误信息可能导致过度依赖、错误的指责或安全机制的低利用率。同样，对模型不确定性、数据偏差或决策边界的缺乏理解可能会加剧自动化引起的伤害风险。对于受 AI 影响的个人，针对特定领域的素养建设努力也可以支持再培训和适应(Ng et al. 2021)。

AI 素养不仅仅是技术流利。它关乎建立公众信心，即系统设计者的目标与社会福利相一致——并且那些构建 AI 系统的人不是脱离公众价值观的，而是对他们负责。正如 Handlin 在 1965 年观察到的那样：*“即使那些从未获得这种理解的人也需要确信，科学的目标与他们的福祉之间有联系，最重要的是，科学家不是完全脱离他们的人，而是与他们共享一些价值观的人。”*

## 谬误和陷阱

负责任的 AI 将技术工程与复杂的伦理和社会考量相结合，从而在机器学习系统的偏差、公平和问责制性质方面产生误解的机会。对技术解决方案解决伦理问题的吸引力可能会掩盖创建真正负责任的 AI 系统所需的更深刻的制度和社会变革。

**谬误：** *通过更好的算法和更多数据，可以从 AI 系统中消除偏差。*

这种误解假设偏见是一个纯粹的技术问题，需要纯粹的技术解决方案。AI 系统中的偏见往往反映了数据收集过程、标注决策和问题表述中嵌入的更深层次的社会不平等和历史不公正。即使是在综合数据集上训练的完美算法，如果底层数据或评估框架中存在偏见，也可能持续或放大社会偏见。算法公平性需要关于价值观和权衡的持续人类判断，而不是一次性的技术修复。有效的偏见缓解涉及持续的监控、利益相关者参与和制度变革，而不是仅仅依赖算法干预。

**陷阱：** 将可解释性视为可选功能而不是系统要求。

许多团队将可解释性视为一个锦上添花的特性，可以在模型开发和部署后添加。这种方法未能考虑到可解释性要求如何显著影响模型设计、评估框架和部署策略。事后解释方法往往提供误导或不完整的见解，无法支持实际的决策需求。高风险应用需要从一开始就将可解释性设计到系统架构中，影响关于模型复杂性、特征工程和评估指标的选择，而不是作为事后考虑的补充。

**谬误：** 伦理 AI 指南和原则自动转化为负责任的实施。

这种信念假设，建立伦理原则或指南可以确保负责任的 AI 发展，而不考虑实施挑战。在实践中，高级原则如公平性、透明度和问责制往往相互冲突，并且与技术要求相冲突。那些只关注原则阐述而不投资于操作化机制的组织，最终往往得到对实际系统行为影响甚微的伦理框架。

**陷阱：** 假设负责任的 AI 实践只带来成本而不提供商业价值。

团队通常将负责任的 AI 视为监管合规的额外负担，这必然与性能和效率目标相冲突。这种观点忽略了负责任的 AI 实践通过提高系统可靠性、增强用户信任、降低法律风险和扩大市场准入所能提供的重大商业价值。负责任的 AI 技术可以提高模型泛化能力，降低维护成本，并防止部署中的高昂失败。那些将责任视为纯粹的成本而不是战略能力的组织，错失了通过可信赖的 AI 系统建立竞争优势的机会。

**陷阱：** 在不考虑其系统级性能和可扩展性影响的情况下实施公平性和可解释性功能。

许多团队在分析这些功能如何影响整体系统架构、性能和可维护性之前，就在现有系统中添加了公平性约束或可解释性方法。实时公平性监控可能会引入显著的计算开销，从而降低系统响应速度；而存储复杂模型的可解释性可能会产生大量的存储和带宽需求。有效的负责任 AI 系统需要仔细地与系统架构共同设计公平性和可解释性要求，从最初的设计阶段就考虑负责任 AI 功能和系统性能之间的权衡。

## 摘要

本章通过四个互补的视角探讨了负责任的 AI，这些视角共同定义了构建值得信赖的机器学习系统的全面工程方法。公平性、透明度、问责制、隐私和安全性的基础原则确立了负责任 AI 系统应实现的目标，但这些原则只有与技术能力、社会技术动态和实施实践相结合才能成为操作性的。

我们考察的技术基础通过偏见检测算法、隐私保护机制、可解释性框架和鲁棒性增强将抽象原则转化为具体系统行为。然而，计算开销分析揭示了这些技术创造了重大的公平性考虑。并非所有组织都能承担全面的负责任 AI 保护，这可能会造成对道德保障的差异化访问。

仅技术正确性本身并不能保证有益的结果。社会技术动态决定了能力是否转化为现实世界的影响：组织激励、人类行为、利益相关者价值观和治理结构决定了结果。没有组织采取行动对其发现采取行动的完美偏见检测算法是无用的；如果用户不理解他们的保护，隐私保护方法也会失败。

实施挑战进一步突显了原则与实践之间的差距，展示了组织结构、数据约束、竞争目标和评估差距如何阻止甚至有良好意图的负责任 AI 努力取得成功。将孤立谬误转化为情境警告强化了负责任 AI 需要持续警惕常见误解，而不是一次性的技术修复。

**关键要点**

+   **整合至关重要**：负责任的 AI 源于原则、技术能力、社会技术动态和实施实践之间的协调一致——任何单一因素都不足以实现

+   **技术方法能够实现但并不保证责任**：偏见检测、隐私保护和可解释性工具提供了必要的功能，但它们的有效性完全取决于组织和社交环境

+   **公平性超越算法**：计算资源需求创造了系统性障碍，决定了谁可以访问负责任的 AI 保护，将伦理从个体系统属性转变为集体社会挑战

+   **部署环境塑造可能性**：云系统支持全面的监控，而 TinyML 设备则需要静态验证——负责任的 AI 必须适应架构限制，而不是强加统一的要求

+   **价值冲突需要深思熟虑**：公平性不可能定理表明，竞争性原则不能通过算法来调和，而需要利益相关者的参与和明确的权衡决策

随着机器学习系统越来越多地嵌入关键社会基础设施，负责任的 AI 框架为可信赖的系统建立了基本的基础。然而，责任不仅限于这里所考察的算法公平性、可解释性和安全性问题。负责任的 AI 技术的计算需求——需要 15-30%更多的训练资源、50-1000 倍的解释推理计算和大量的监控基础设施——对环境影响和资源消耗提出了关键问题。

下一章探讨了责任如何涵盖可持续性，考察了训练大型模型的碳足迹、数据中心运营的环境成本以及开发节能 AI 系统的必要性。正如负责任的 AI 询问我们的系统是否公平对待人们一样，可持续 AI 询问我们的系统是否负责任地对待地球。可信赖系统的原则最终需要平衡技术性能、社会责任和环境管理——确保 AI 发展增强人类福祉，而不损害我们的共同未来。

* * *
