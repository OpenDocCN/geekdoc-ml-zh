- en: 7.1 Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html](https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and
    active learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Empirical risk minimization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s the risk in empirical risk minimization?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why is it empirical?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How do we minimize that risk?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Occam''s razor states that when the simple explanation and complex explanation
    both work equally well, the simple explanation is usually correct. How do we apply
    this principle in ML?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What are the conditions that allowed deep learning to gain popularity in
    the last decade?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] If we have a wide NN and a deep NN with the same number of parameters,
    which one is more expressive and why?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] The Universal Approximation Theorem states that a neural network with 1
    hidden layer can approximate any continuous function for inputs within a specific
    range. Then why can’t a simple neural network reach an arbitrarily small positive
    error?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What are saddle points and local minima? Which are thought to cause more
    problems for training large NNs?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What are the differences between parameters and hyperparameters?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why is hyperparameter tuning important?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Explain algorithm for tuning hyperparameters.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Classification vs. regression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What makes a classification problem different from a regression problem?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Can a classification problem be turned into a regression problem and vice
    versa?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Parametric vs. non-parametric methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What’s the difference between parametric methods and non-parametric methods?
    Give an example of each method.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] When should we use one and when should we use the other?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why does ensembling independently trained models generally improve performance?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why does L1 regularization tend to lead to sparsity while L2 regularization
    pushes weights closer to 0?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why does an ML model’s performance degrade in production?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What problems might we run into when deploying large machine learning models?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your model performs really well on the test set but poorly in production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What are your hypotheses about the causes?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] How do you validate whether your hypotheses are correct?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Imagine your hypotheses about the causes are correct. What would you do
    to address them?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
