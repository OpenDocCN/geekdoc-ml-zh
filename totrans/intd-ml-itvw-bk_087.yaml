- en: 7.1 Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7.1 基础知识
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html](https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html](https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html)
- en: '[E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and
    active learning.'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 解释监督学习、无监督学习、弱监督学习、半监督学习和主动学习。'
- en: Empirical risk minimization.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经验风险最小化。
- en: '[E] What’s the risk in empirical risk minimization?'
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 经验风险最小化中的风险是什么？'
- en: '[E] Why is it empirical?'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么它是经验性的？'
- en: '[E] How do we minimize that risk?'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 我们如何最小化那个风险？'
- en: '[E] Occam''s razor states that when the simple explanation and complex explanation
    both work equally well, the simple explanation is usually correct. How do we apply
    this principle in ML?'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 奥卡姆剃刀原则指出，当简单解释和复杂解释都能同样好地工作时，简单的解释通常是正确的。我们如何在机器学习中应用这个原则？'
- en: '[E] What are the conditions that allowed deep learning to gain popularity in
    the last decade?'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 允许深度学习在过去十年中流行的条件是什么？'
- en: '[M] If we have a wide NN and a deep NN with the same number of parameters,
    which one is more expressive and why?'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 如果我们有一个具有相同参数数量的宽神经网络和深神经网络，哪一个更有表达力，为什么？'
- en: '[H] The Universal Approximation Theorem states that a neural network with 1
    hidden layer can approximate any continuous function for inputs within a specific
    range. Then why can’t a simple neural network reach an arbitrarily small positive
    error?'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 广义逼近定理指出，具有1个隐藏层的神经网络可以逼近特定范围内的任何连续函数。那么为什么简单的神经网络不能达到任意小的正误差？'
- en: '[E] What are saddle points and local minima? Which are thought to cause more
    problems for training large NNs?'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么是鞍点和局部最小值？它们被认为对训练大型神经网络造成更多问题的原因是什么？'
- en: Hyperparameters.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数。
- en: '[E] What are the differences between parameters and hyperparameters?'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 参数和超参数之间的区别是什么？'
- en: '[E] Why is hyperparameter tuning important?'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么超参数调优很重要？'
- en: '[M] Explain algorithm for tuning hyperparameters.'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 解释超参数调优的算法。'
- en: Classification vs. regression.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类与回归。
- en: '[E] What makes a classification problem different from a regression problem?'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么是分类问题和回归问题的区别？'
- en: '[E] Can a classification problem be turned into a regression problem and vice
    versa?'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 一个分类问题能否变成回归问题，反之亦然？'
- en: Parametric vs. non-parametric methods.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数方法与非参数方法。
- en: '[E] What’s the difference between parametric methods and non-parametric methods?
    Give an example of each method.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 参数方法和非参数方法之间的区别是什么？给出每种方法的例子。'
- en: '[H] When should we use one and when should we use the other?'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 我们应该在什么时候使用一个，什么时候使用另一个？'
- en: '[M] Why does ensembling independently trained models generally improve performance?'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么将独立训练的模型进行集成通常能提高性能？'
- en: '[M] Why does L1 regularization tend to lead to sparsity while L2 regularization
    pushes weights closer to 0?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 为什么L1正则化倾向于导致稀疏性，而L2正则化将权重推向0？'
- en: '[E] Why does an ML model’s performance degrade in production?'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 为什么机器学习模型在生产中的性能会下降？'
- en: '[M] What problems might we run into when deploying large machine learning models?'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 在部署大型机器学习模型时，我们可能会遇到哪些问题？'
- en: Your model performs really well on the test set but poorly in production.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的模型在测试集上表现很好，但在生产中表现很差。
- en: '[M] What are your hypotheses about the causes?'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 你对原因的假设是什么？'
- en: '[H] How do you validate whether your hypotheses are correct?'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 你如何验证你的假设是否正确？'
- en: '[M] Imagine your hypotheses about the causes are correct. What would you do
    to address them?'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 假设你对原因的假设是正确的。你会如何解决它们？'
