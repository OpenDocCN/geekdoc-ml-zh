- en: Training and Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œè°ƒæ•´
- en: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html)
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python
    åœ°ç»Ÿè®¡å­¦åº”ç”¨ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python
    åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
- en: 'Chapter of e-book â€œApplied Machine Learning in Python: a Hands-on Guide with
    Codeâ€.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­ä¹¦â€œã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹åŠ¨æŒ‡å—ã€‹â€çš„ç« èŠ‚ã€‚
- en: 'Cite this e-Book as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Pyrcz, M.J., 2024, *ã€ŠPython åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹åŠ¨æŒ‡å—ã€‹* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138
    [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)
- en: 'The workflows in this book and more are available here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹åœ¨æ­¤å¤„å¯ç”¨ï¼š
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶].
    Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
- en: By Michael J. Pyrcz
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ï¼šMichael J. Pyrcz
- en: Â© Copyright 2024.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚
- en: 'This chapter is a summary of **Machine Learning Training and Tuning** including
    essential concepts:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ˜¯å¯¹**æœºå™¨å­¦ä¹ è®­ç»ƒå’Œè°ƒæ•´**çš„æ€»ç»“ï¼ŒåŒ…æ‹¬åŸºæœ¬æ¦‚å¿µï¼š
- en: Model Parameter Training and Hyperparameter Tuning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°è®­ç»ƒå’Œè¶…å‚æ•°è°ƒæ•´
- en: Model Goodness Metrics
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹è‰¯å¥½æ€§æŒ‡æ ‡
- en: Cross Validation Workflows
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯å·¥ä½œæµç¨‹
- en: Limitations of Cross Validation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯çš„å±€é™æ€§
- en: '**YouTube Lecture**: check out my lectures on:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘åœ¨ä»¥ä¸‹æ–¹é¢çš„è®²åº§ï¼š'
- en: '[Training and Testing](https://youtu.be/owOSiKT3K8E?si=PrY5lL4Dbi2Ix7fu).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®­ç»ƒå’Œæµ‹è¯•](https://youtu.be/owOSiKT3K8E?si=PrY5lL4Dbi2Ix7fu)'
- en: '[Model Goodness Metrics](https://youtu.be/g38sEpFOX-0?si=XPC18zNMCxaIZCOF)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¨¡å‹è‰¯å¥½æ€§æŒ‡æ ‡](https://youtu.be/g38sEpFOX-0?si=XPC18zNMCxaIZCOF)'
- en: '[Cross Validation Considerations](https://youtu.be/FiX8IWPhTcg?si=K4A3W0zTaiypm7n7)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äº¤å‰éªŒè¯è€ƒè™‘å› ç´ ](https://youtu.be/FiX8IWPhTcg?si=K4A3W0zTaiypm7n7)'
- en: For your convenience hereâ€™s a summary of salient points.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‚¨çš„æ–¹ä¾¿ï¼Œä»¥ä¸‹æ˜¯å¯¹å…³é”®ç‚¹çš„æ€»ç»“ã€‚
- en: Training and Tuning Predictive Machine Learning Models
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œè°ƒæ•´é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹
- en: In predictive machine learning, we follow a standard model training and testing
    workflow. This process ensures that our model generalizes well to new data, rather
    than just fitting the training data perfectly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¢„æµ‹æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éµå¾ªæ ‡å‡†çš„æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å·¥ä½œæµç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„æ•°æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯å®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
- en: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
- en: Standard predictive machine learning modeling workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†é¢„æµ‹æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ã€‚
- en: Letâ€™s walk through the key steps,
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€ä¸€äº†è§£å…³é”®æ­¥éª¤ï¼Œ
- en: '**Train and Test Split** - divide the available data into mutually exclusive,
    exhaustive subsets: a training set and a testing set.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²** - å°†å¯ç”¨æ•°æ®åˆ’åˆ†ä¸ºäº’æ–¥ä¸”ç©·å°½çš„å­é›†ï¼šä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªæµ‹è¯•é›†ã€‚'
- en: typically, 15%â€“30% of the data is held out for testing
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œ15%â€“30%çš„æ•°æ®è¢«ä¿ç•™ç”¨äºæµ‹è¯•
- en: the remaining 70%â€“85% is used for training the model
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰©ä½™çš„70%â€“85%ç”¨äºè®­ç»ƒæ¨¡å‹
- en: '**Define a range of hyperparameter(s)** values to explore, ranging from,'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰è¦æ¢ç´¢çš„è¶…å‚æ•°å€¼èŒƒå›´**ï¼ŒèŒƒå›´ä»ï¼Œ'
- en: simple models with low flexibility
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çµæ´»æ€§ä½çš„ç®€å•æ¨¡å‹
- en: to complex models with high flexibility
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ°å…·æœ‰é«˜çµæ´»æ€§çš„å¤æ‚æ¨¡å‹
- en: \(\quad\) This step may involve tuning multiple hyperparameters, in which case
    efficient sampling methods (e.g., grid search, random search, or Bayesian optimization)
    are often used.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) è¿™ä¸€æ­¥å¯èƒ½æ¶‰åŠè°ƒæ•´å¤šä¸ªè¶…å‚æ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸ä¼šä½¿ç”¨é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œç½‘æ ¼æœç´¢ã€éšæœºæœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–ï¼‰ã€‚
- en: '**Train Model Parameters for Each Hyperparameter Setting** - for each set of
    hyperparameters, train a model on the training data. This yields:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¸ºæ¯ä¸ªè¶…å‚æ•°è®¾ç½®è®­ç»ƒæ¨¡å‹å‚æ•°** - å¯¹äºæ¯ç»„è¶…å‚æ•°ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚è¿™ä¼šäº§ç”Ÿï¼š'
- en: a suite of trained models, each with different complexity
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç³»åˆ—å…·æœ‰ä¸åŒå¤æ‚æ€§çš„è®­ç»ƒæ¨¡å‹
- en: each model has parameters optimized to minimize error on the training data
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å‚æ•°è¢«ä¼˜åŒ–ä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®
- en: '**Evaluate Each Model on the Withheld Testing Data** - using the testing data,'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨ä¿ç•™çš„æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¯ä¸ªæ¨¡å‹** - ä½¿ç”¨æµ‹è¯•æ•°æ®ï¼Œ'
- en: evaluate how each trained model performs on unseen data
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¯ä¸ªè®­ç»ƒæ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°
- en: summarize prediction error (for example, root mean square error (RMSE), mean
    absolute error (MAE), classification accuracy) for each model
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹è¯¯å·®ï¼ˆä¾‹å¦‚ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€åˆ†ç±»å‡†ç¡®ç‡ï¼‰
- en: '**Select the Hyperparameters That Minimize Test Error** - this is the hyperparameter
    tuning step:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„è¶…å‚æ•°** - è¿™å°±æ˜¯è¶…å‚æ•°è°ƒæ•´æ­¥éª¤ï¼š'
- en: choose the model hyperparaemter(s) that performs best on the test data
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹è¶…å‚æ•°
- en: these are your tuned hyperparameters
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯ä½ çš„è°ƒæ•´åçš„è¶…å‚æ•°
- en: '**Retrain the Final Model on All Data Using Tuned Hyperparameters** - now that
    the best model complexity has been identified,'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹** - ç°åœ¨å·²ç»ç¡®å®šäº†æœ€ä½³æ¨¡å‹å¤æ‚åº¦ï¼Œ'
- en: retrain the model using both the training and test sets
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†é‡æ–°è®­ç»ƒæ¨¡å‹
- en: this maximizes the amount of data used for final model parameter estimation
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æœ€å¤§åŒ–äº†ç”¨äºæœ€ç»ˆæ¨¡å‹å‚æ•°ä¼°è®¡çš„æ•°æ®é‡
- en: the resulting model is the one you deploy in real-world applications
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœæ¨¡å‹æ˜¯ä½ åœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½²çš„æ¨¡å‹
- en: Common Questions About the Model Training and Tuning Workflow
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§å…³äºæ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜å·¥ä½œæµç¨‹çš„é—®é¢˜
- en: As a professor, I often hear these questions when I introduce the above machine
    learning model training and tuning workflow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åæ•™æˆï¼Œå½“æˆ‘ä»‹ç»ä¸Šè¿°æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜å·¥ä½œæµç¨‹æ—¶ï¼Œæˆ‘ç»å¸¸å¬åˆ°è¿™äº›é—®é¢˜ã€‚
- en: '**What is the main outcome of steps 1â€“5?** - the only reliable outcome is the
    tuned hyperparameters.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1-5çš„ä¸»è¦ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ** - å”¯ä¸€å¯é çš„ç»“æœæ˜¯è°ƒæ•´åçš„è¶…å‚æ•°ã€‚'
- en: \(\quad\) we do not use the model trained in step 3 or 4 directly, because it
    was trained without access to all available data. Instead, we retrain the final
    model using all data with the selected hyperparameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) æˆ‘ä»¬ä¸ç›´æ¥ä½¿ç”¨æ­¥éª¤3æˆ–4ä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ˜¯åœ¨æ²¡æœ‰è®¿é—®æ‰€æœ‰å¯ç”¨æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒçš„ã€‚ç›¸åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰æ•°æ®å’Œé€‰å®šçš„è¶…å‚æ•°é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚
- en: '**Why not train the model on all the data from the beginning?** - because if
    we do that, we have no independent way to evaluate the modelâ€™s generalization.
    A very complex model can easily overfitâ€”fitting the training data perfectly, but
    performing poorly on new, unseen data.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆä¸€å¼€å§‹ä¸åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Ÿ** - å› ä¸ºå¦‚æœæˆ‘ä»¬é‚£æ ·åšï¼Œæˆ‘ä»¬å°±æ²¡æœ‰ç‹¬ç«‹çš„æ–¹å¼æ¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸€ä¸ªéå¸¸å¤æ‚çš„æ¨¡å‹å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆâ€”â€”å®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚'
- en: \(\quad\) overfitting happens when model flexibility is too highâ€”it captures
    noise instead of the underlying pattern. Without a withheld test set, we canâ€™t
    detect this.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) å½“æ¨¡å‹çµæ´»æ€§è¿‡é«˜æ—¶ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆâ€”â€”å®ƒæ•æ‰äº†å™ªå£°è€Œä¸æ˜¯æ½œåœ¨çš„æ¨¡å¼ã€‚å¦‚æœæ²¡æœ‰ä¿ç•™çš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å°±æ— æ³•æ£€æµ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: This workflow for training and tuning predictive machine learning models is,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç”¨äºè®­ç»ƒå’Œè°ƒæ•´é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹çš„å·¥ä½œæµç¨‹æ˜¯ï¼Œ
- en: an empirical, cross-validation-based process
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŸºäºç»éªŒã€äº¤å‰éªŒè¯çš„è¿‡ç¨‹
- en: a practical simulation of real-world model use
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ç°å®ä¸–ç•Œæ¨¡å‹ä½¿ç”¨çš„ä¸€ä¸ªå®é™…æ¨¡æ‹Ÿ
- en: a method to identify the model complexity that best balances fit and generalization
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§è¯†åˆ«æœ€ä½³æ¨¡å‹å¤æ‚åº¦çš„æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ‹Ÿåˆå’Œæ³›åŒ–
- en: Iâ€™ve said model parameters and model hyperparameters a bunch of times, so I
    owe you their definitions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²ç»å¤šæ¬¡æåˆ°æ¨¡å‹å‚æ•°å’Œæ¨¡å‹è¶…å‚æ•°ï¼Œæ‰€ä»¥æˆ‘æœ‰è´£ä»»ç»™å‡ºå®ƒä»¬çš„å®šä¹‰ã€‚
- en: Model Parameters and Model Hyperparameters
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°å’Œæ¨¡å‹è¶…å‚æ•°
- en: '**Model parameters** are fit during training phase to minimize error at the
    training data, i.e., model parameters are trained with training data and control
    model fit to the data. For example,'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å‚æ•°**åœ¨è®­ç»ƒé˜¶æ®µè¢«è°ƒæ•´ä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼Œå³æ¨¡å‹å‚æ•°æ˜¯ç”¨è®­ç»ƒæ•°æ®è®­ç»ƒçš„ï¼Œå¹¶æ§åˆ¶æ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆã€‚ä¾‹å¦‚ï¼Œ'
- en: for the polynomial predictive machine learning model from the machine learning
    workflow example above, the model parameters are the polynomial coefficients,
    e.g., \(b_3\), \(b_2\), \(b_1\) and \(c\) (often called \(b_0\)) for the third
    order polynomial model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸Šé¢æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ç¤ºä¾‹ä¸­çš„å¤šé¡¹å¼é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ˜¯å¤šé¡¹å¼ç³»æ•°ï¼Œä¾‹å¦‚ï¼Œä¸‰æ¬¡å¤šé¡¹å¼æ¨¡å‹çš„\(b_3\)ã€\(b_2\)ã€\(b_1\)å’Œ\(c\)ï¼ˆé€šå¸¸ç§°ä¸º\(b_0\)ï¼‰ã€‚
- en: '![](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
- en: Model parameters are adjusted to fit of the model to the data, i.e., model parameters
    are trained to minimize error over the training data (x markers).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°è¢«è°ƒæ•´ä»¥é€‚åº”æ¨¡å‹ä¸æ•°æ®çš„ä¸€è‡´æ€§ï¼Œå³æ¨¡å‹å‚æ•°è¢«è®­ç»ƒä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ï¼ˆxæ ‡è®°ï¼‰ä¸Šçš„è¯¯å·®ã€‚
- en: '**Model hyperparameters** are very different. They do not constrain the model
    fit to the data directly, instead they constrain the model complexity. The model
    hyperparameters are selected (call tuning) to minimize error at the withheld testing
    data. Going back to our polynomial predictive machine learning example, the choice
    of polynomial order is the model hyperparameter.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹è¶…å‚æ•°**éå¸¸ä¸åŒã€‚å®ƒä»¬ä¸ç›´æ¥çº¦æŸæ¨¡å‹ä¸æ•°æ®çš„ä¸€è‡´æ€§ï¼Œè€Œæ˜¯çº¦æŸæ¨¡å‹å¤æ‚æ€§ã€‚æ¨¡å‹è¶…å‚æ•°è¢«é€‰æ‹©ï¼ˆç§°ä¸ºè°ƒæ•´ï¼‰ä»¥æœ€å°åŒ–ä¿ç•™æµ‹è¯•æ•°æ®ä¸Šçš„è¯¯å·®ã€‚å›åˆ°æˆ‘ä»¬å¤šé¡¹å¼é¢„æµ‹æ€§æœºå™¨å­¦ä¹ ç¤ºä¾‹ï¼Œå¤šé¡¹å¼é˜¶æ•°çš„é€‰æ‹©æ˜¯æ¨¡å‹è¶…å‚æ•°ã€‚'
- en: '![](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
- en: Model hyperparameters are adjusted to change the model complexity / flexibility,
    i.e., model hyperparameters are tuned to minimize error over the withheld testing
    data (solid circles).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¶…å‚æ•°è¢«è°ƒæ•´ä»¥æ”¹å˜æ¨¡å‹å¤æ‚åº¦/çµæ´»æ€§ï¼Œå³æ¨¡å‹è¶…å‚æ•°è¢«è°ƒæ•´ä»¥æœ€å°åŒ–ä¿ç•™çš„æµ‹è¯•æ•°æ®ï¼ˆå®å¿ƒåœ†ï¼‰ä¸Šçš„è¯¯å·®ã€‚
- en: Model parameters vs. model hyperparameters
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°ä¸æ¨¡å‹è¶…å‚æ•°
- en: Model parameters control the model fit and are trained with training data. Model
    hyperparameters control the model complexity and are tuned with testing data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°æ§åˆ¶æ¨¡å‹æ‹Ÿåˆï¼Œå¹¶ä½¿ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ¨¡å‹è¶…å‚æ•°æ§åˆ¶æ¨¡å‹å¤æ‚æ€§ï¼Œå¹¶ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œè°ƒæ•´ã€‚
- en: Regression and Classification
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›å½’å’Œåˆ†ç±»
- en: Before we proceed, we need to define regression and classification.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å›å½’å’Œåˆ†ç±»ã€‚
- en: '**Regression** - a predictive machine learning model where the response feature(s)
    is continuous.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›å½’** - ä¸€ç§é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­å“åº”ç‰¹å¾æ˜¯è¿ç»­çš„ã€‚'
- en: '**Classification** - a predictive machine learning model where the response
    feature(s) is categorical.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†ç±»** - ä¸€ç§é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­å“åº”ç‰¹å¾æ˜¯åˆ†ç±»çš„ã€‚'
- en: It turns out that for each of these we need to build different models and use
    different methods to score these models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œå¯¹äºè¿™äº›ä¸­çš„æ¯ä¸€ä¸ªï¼Œæˆ‘ä»¬éƒ½éœ€è¦æ„å»ºä¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥è¯„ä¼°è¿™äº›æ¨¡å‹ã€‚
- en: for the remainder of this discussion we will focus on regression, but in later
    chapters we introduce classification models as well.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æœ¬è®¨è®ºçš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å…³æ³¨å›å½’ï¼Œä½†åœ¨åé¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä»‹ç»åˆ†ç±»æ¨¡å‹ã€‚
- en: Now, to better understand predictive machine learning model tuning, i.e., the
    empirical approach to tune model complexity to minimize testing error, we need
    to understand the sources of testing error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹è°ƒæ•´ï¼Œå³é€šè¿‡ç»éªŒæ–¹æ³•è°ƒæ•´æ¨¡å‹å¤æ‚æ€§ä»¥æœ€å°åŒ–æµ‹è¯•è¯¯å·®ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æµ‹è¯•è¯¯å·®çš„æ¥æºã€‚
- en: the causes of the thing that we are attempting to minimize!
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯•å›¾æœ€å°åŒ–çš„é‚£ä¸ªäº‹ç‰©çš„æˆå› ï¼
- en: Training and Testing Data
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
- en: For clarity, consider this is schematic of the flow of training and testing
    data in the predictive machine learning model parameter training and hyperparameter
    tuning workflow,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œè€ƒè™‘è¿™æ˜¯é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°è®­ç»ƒå’Œè¶…å‚æ•°è°ƒæ•´å·¥ä½œæµç¨‹ä¸­è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æµç¤ºæ„å›¾ï¼Œ
- en: '![](../Images/454aec4a3d657f072406100f036592b6.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/454aec4a3d657f072406100f036592b6.png)'
- en: The flow of training and testing data in the predictive machine learning model
    parameter training and hyperparameter tuning workflow.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°è®­ç»ƒå’Œè¶…å‚æ•°è°ƒæ•´å·¥ä½œæµç¨‹ä¸­è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„æµç¨‹ã€‚
- en: '**Training Data**,'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ•°æ®**,'
- en: trains model parameters
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: trains the final model for real world use
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒç”¨äºå®é™…åº”ç”¨çš„æœ€ç»ˆæ¨¡å‹
- en: '**Testing Data**,'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•æ•°æ®**,'
- en: withheld from training model parameters to avoid model overfit
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒæ¨¡å‹å‚æ•°ä¸­ä¿ç•™ä»¥é¿å…æ¨¡å‹è¿‡æ‹Ÿåˆ
- en: tunes model hyperparameters
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
- en: returned to train the final tuned model for deployment
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿”å›è®­ç»ƒä»¥éƒ¨ç½²æœ€ç»ˆè°ƒæ•´å¥½çš„æ¨¡å‹
- en: How Much Data Should be Withheld for Testing?
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åº”è¯¥ä¿ç•™å¤šå°‘æ•°æ®ç”¨äºæµ‹è¯•ï¼Ÿ
- en: The proportion in testing is recommended by various sources from 15% - 30% of
    the total dataset. This is a compromise,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ•°æ®æ‰€å çš„æ¯”ä¾‹ç”±å„ç§æ¥æºæ¨èï¼Œä»æ€»æ•°æ®é›†çš„15%åˆ°30%ã€‚è¿™æ˜¯ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆï¼Œ
- en: data withheld for testing reduces the data available for training; therefore,
    reduces the accuracy of the model.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºæµ‹è¯•ä¿ç•™çš„æ•°æ®å‡å°‘äº†å¯ç”¨äºè®­ç»ƒçš„æ•°æ®ï¼›å› æ­¤ï¼Œé™ä½äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
- en: data withheld for testing improves the accuracy of the assessment of the model
    performance.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºæµ‹è¯•ä¿ç•™çš„æ•°æ®æé«˜äº†å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
- en: Various authors have experimented on a variety of training and testing ratios
    and have recommended splits for their applications,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„ä½œè€…å·²ç»å¯¹å„ç§è®­ç»ƒå’Œæµ‹è¯•æ¯”ç‡è¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸ºå…¶åº”ç”¨æ¨èäº†åˆ†å‰²æ–¹æ¡ˆï¼Œ
- en: the optimum ratio of training and testing split depends on problem setting
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²çš„æœ€ä½³æ¯”ä¾‹å–å†³äºé—®é¢˜è®¾ç½®
- en: To determine the proportion of testing data to withheld we could consider the
    difficulty in model parameter training (e.g., the number of model parameters)
    and the difficulty in model hyperparameter tuning (e.g., number of hyperparameters,
    range of response feature outcomes).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®å®šæµ‹è¯•æ•°æ®ä¿ç•™çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ¨¡å‹å‚æ•°è®­ç»ƒçš„éš¾åº¦ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡ï¼‰å’Œæ¨¡å‹è¶…å‚æ•°è°ƒæ•´çš„éš¾åº¦ï¼ˆä¾‹å¦‚ï¼Œè¶…å‚æ•°çš„æ•°é‡ï¼Œå“åº”ç‰¹å¾ç»“æœçš„èŒƒå›´ï¼‰ã€‚
- en: Fair Train and Test Splits
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¬å¹³çš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²
- en: Dr. Julian Salazar suggests that for spatial prediction problems that random
    train and test data splits may not be fair.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ±åˆ©å®‰Â·è¨æ‹‰æ‰åšå£«å»ºè®®ï¼Œå¯¹äºç©ºé—´é¢„æµ‹é—®é¢˜ï¼Œéšæœºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²å¯èƒ½ä¸å…¬å¹³ã€‚
- en: proposed a [fair train and test split method](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    for spatial prediction models that splits the data based on the difficulty of
    the planned use of the model.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æå‡ºäº†ä¸€ç§[å…¬å¹³çš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²æ–¹æ³•](https://www.sciencedirect.com/science/article/pii/S0920410521015023)ï¼Œç”¨äºåŸºäºæ¨¡å‹è®¡åˆ’ç”¨é€”éš¾åº¦çš„ç©ºé—´é¢„æµ‹æ¨¡å‹æ•°æ®åˆ†å‰²ã€‚
- en: prediction difficulty is related to kriging variance that accounts for spatial
    continuity and distance offset, i.e., the difficulty of the estimate.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹éš¾åº¦ä¸å…‹é‡Œé‡‘æ–¹å·®ç›¸å…³ï¼Œå®ƒè€ƒè™‘äº†ç©ºé—´è¿ç»­æ€§å’Œè·ç¦»åç§»ï¼Œå³ä¼°è®¡çš„éš¾åº¦ã€‚
- en: the testing split is iterated to match the distribution of kriging variance
    for planned real world use of the model
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•åˆ†å‰²è¢«è¿­ä»£ä»¥åŒ¹é…æ¨¡å‹åœ¨è®¡åˆ’ç”¨äºç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å…‹é‡Œé‡‘æ–¹å·®åˆ†å¸ƒ
- en: To illustrate this concept of prediction difficulty, consider this set of well
    logs with both random assignment of testing data and withholding an entire contiguous
    region of the well log for testing data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜é¢„æµ‹éš¾åº¦è¿™ä¸€æ¦‚å¿µï¼Œè€ƒè™‘ä»¥ä¸‹è¿™ç»„äº•æ—¥å¿—ï¼Œå…¶ä¸­åŒ…å«äº†æµ‹è¯•æ•°æ®çš„éšæœºåˆ†é…å’Œä¿ç•™æ•´ä¸ªè¿ç»­åŒºåŸŸä½œä¸ºæµ‹è¯•æ•°æ®ã€‚
- en: '**easy prediction problem** - for random assignment, usually training data
    are available very close and very similar to the withheld testing data'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ˜“äºé¢„æµ‹çš„é—®é¢˜** - å¯¹äºéšæœºåˆ†é…ï¼Œé€šå¸¸è®­ç»ƒæ•°æ®éå¸¸æ¥è¿‘ä¸”ä¸ä¿ç•™çš„æµ‹è¯•æ•°æ®éå¸¸ç›¸ä¼¼'
- en: '**difficult prediction problem** - for removal of the contiguous region, there
    are no similar nor close training data to the withheld testing data'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éš¾ä»¥é¢„æµ‹çš„é—®é¢˜** - å¯¹äºç§»é™¤è¿ç»­åŒºåŸŸï¼Œæ²¡æœ‰ç›¸ä¼¼æˆ–æ¥è¿‘çš„è®­ç»ƒæ•°æ®ä¸ä¿ç•™çš„æµ‹è¯•æ•°æ®'
- en: '![](../Images/5aa194417f3fa3435f02717796814e62.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aa194417f3fa3435f02717796814e62.png)'
- en: Two cases for train and test data split, random (left) and by-region (right).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²çš„ä¸¤ä¸ªæ¡ˆä¾‹ï¼Œéšæœºï¼ˆå·¦ï¼‰å’ŒæŒ‰åŒºåŸŸï¼ˆå³ï¼‰ã€‚
- en: Consider the following prediction cases, i.e., planned real world use of the
    models, and some practical suggestions for fair train and test split.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹é¢„æµ‹æ¡ˆä¾‹ï¼Œå³æ¨¡å‹çš„è®¡åˆ’ç°å®ä¸–ç•Œåº”ç”¨ï¼Œä»¥åŠä¸€äº›å…³äºå…¬å¹³è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²çš„å®é™…å»ºè®®ã€‚
- en: If the model will be used to impute data with small offsets from available data
    then construct a train and test split with train data close to test data - random
    assignment of withheld testing data is likely sufficient.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹å°†ç”¨äºä½¿ç”¨å¯ç”¨æ•°æ®ä¸­çš„å°åç§»æ¥æ’è¡¥æ•°æ®ï¼Œåˆ™æ„å»ºä¸€ä¸ªé è¿‘æµ‹è¯•æ•°æ®çš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰² - éšæœºåˆ†é…ä¿ç•™çš„æµ‹è¯•æ•°æ®å¯èƒ½æ˜¯è¶³å¤Ÿçš„ã€‚
- en: if the model will be used to predict a large distance offsets then perform splits
    the result is large offsets between train and test data - withhold entire wells,
    drill holes or spatial regions.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹å°†ç”¨äºé¢„æµ‹å¤§è·ç¦»åç§»ï¼Œåˆ™è¿›è¡Œåˆ†å‰²ï¼Œç»“æœæ˜¯åœ¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¹‹é—´äº§ç”Ÿå¤§åç§» - ä¿ç•™æ•´ä¸ªäº•ç­’ã€é’»å­”æˆ–ç©ºé—´åŒºåŸŸã€‚
- en: Note, with fair train and test splits the tuned model may vary based on the
    planned use for the model.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨å…¬å¹³çš„è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²ä¸‹ï¼Œè°ƒæ•´åçš„æ¨¡å‹å¯èƒ½åŸºäºæ¨¡å‹çš„è®¡åˆ’ç”¨é€”è€Œæœ‰æ‰€ä¸åŒã€‚
- en: Use a simple method like withholding entire wells for a predrill prediction
    model, or use the Dr. Salazar workflow, but donâ€™t ignore this issue and just use
    random selection by default.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åƒä¸ºé¢„é’»é¢„æµ‹æ¨¡å‹ä¿ç•™æ•´ä¸ªäº•ç­’è¿™æ ·çš„ç®€å•æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨è¨æ‹‰æ‰åšå£«çš„å·¥ä½œæµç¨‹ï¼Œä½†ä¸è¦å¿½ç•¥è¿™ä¸ªé—®é¢˜ï¼Œè€Œé»˜è®¤ä½¿ç”¨éšæœºé€‰æ‹©ã€‚
- en: admittedly, throughout this e-book for demonstration workflow brevity and clarity
    I have just used random training and testing data assignments.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™½ç„¶å¦‚æ­¤ï¼Œåœ¨æ•´ä¸ªç”µå­ä¹¦ä¸­ï¼Œä¸ºäº†æ¼”ç¤ºå·¥ä½œæµç¨‹çš„ç®€æ´æ€§å’Œæ¸…æ™°æ€§ï¼Œæˆ‘ä»…ä»…ä½¿ç”¨äº†éšæœºçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†é…ã€‚
- en: Model Metrics
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº¦é‡
- en: Since we have covered the workflows for training and tuning, now we can specify
    the model metrics that are applied for,
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å·²ç»æ¶µç›–äº†åŸ¹è®­å’Œè°ƒæ•´çš„å·¥ä½œæµç¨‹ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥æŒ‡å®šåº”ç”¨è¿™äº›æ¨¡å‹åº¦é‡çš„æ¨¡å‹ã€‚
- en: training model parameters
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: tuning model hyperparameters
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
- en: model checking and comparison
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ£€æŸ¥å’Œæ¯”è¾ƒ
- en: Hereâ€™s an flowchart indicating how these metrics fit into the machine learning
    modeling workflow.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªæµç¨‹å›¾ï¼Œè¯´æ˜äº†è¿™äº›åº¦é‡å¦‚ä½•é€‚åˆæœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/d311f90f3b59dd6738b0ce7a12614995.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d311f90f3b59dd6738b0ce7a12614995.png)'
- en: Various applications for model metrics in machine learning modeling workflows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ä¸­ï¼Œæ¨¡å‹åº¦é‡æœ‰å¤šç§åº”ç”¨ã€‚
- en: Choice of model metric depends primarily on the context of the prediction problem,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº¦é‡çš„é€‰æ‹©ä¸»è¦å–å†³äºé¢„æµ‹é—®é¢˜çš„ä¸Šä¸‹æ–‡ï¼Œ
- en: classification vs. regression
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»ä¸å›å½’
- en: individual estimates vs. entire subsets in space (images) or time (signals)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä¸ªä¼°è®¡ä¸ç©ºé—´ï¼ˆå›¾åƒï¼‰æˆ–æ—¶é—´ï¼ˆä¿¡å·ï¼‰ä¸­çš„æ•´ä¸ªå­é›†
- en: estimation vs. uncertainty
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼°è®¡ä¸ä¸ç¡®å®šæ€§
- en: There are additional considerations, for example,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…¶ä»–ä¸€äº›è€ƒè™‘å› ç´ ï¼Œä¾‹å¦‚ï¼Œ
- en: \(L^1\) vs \(L^2\) norms with their differences, for example, in robustness
    with outliers, stability of solutions and solution sparsity
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(L^1\) ä¸ \(L^2\) èŒƒæ•°åŠå…¶å·®å¼‚ï¼Œä¾‹å¦‚ï¼Œåœ¨å¼‚å¸¸å€¼é²æ£’æ€§ã€è§£çš„ç¨³å®šæ€§å’Œè§£çš„ç¨€ç–æ€§æ–¹é¢
- en: consistency with model assumptions, for example, \(r^2\) is only valid for linear
    models
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹å‡è®¾çš„ä¸€è‡´æ€§ï¼Œä¾‹å¦‚ï¼Œ\(r^2\) åªå¯¹çº¿æ€§æ¨¡å‹æœ‰æ•ˆ
- en: Letâ€™s review some of the common model metrics for regression models and then
    for classification models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€äº›å›å½’æ¨¡å‹çš„å¸¸è§æ¨¡å‹åº¦é‡ï¼Œç„¶åæ˜¯åˆ†ç±»æ¨¡å‹ã€‚
- en: Mean Square Error (MSE)
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡æ–¹è¯¯å·® (MSE)
- en: Is sensitive to outliers, but is continuously differentiable, leading to a closed-form
    expression for model training. Since the error is squared the error units are
    squared and this may be less interpretable, for example, MSE of 23,543 \(mD^2\).
    The equation is,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œä½†è¿ç»­å¯å¾®ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒæœ‰å°é—­å½¢å¼è¡¨è¾¾å¼ã€‚ç”±äºè¯¯å·®æ˜¯å¹³æ–¹çš„ï¼Œè¯¯å·®å•ä½ä¹Ÿæ˜¯å¹³æ–¹çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´è§£é‡Šæ€§è¾ƒå·®ï¼Œä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®ä¸º 23,543 \(mD^2\)ã€‚è¯¥æ–¹ç¨‹æ˜¯ï¼Œ
- en: \[ \text{Test MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Test MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
- en: Mean Absolute Error (MAE)
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡å€¼ç»å¯¹è¯¯å·® (MAE)
- en: Is robust in the presence of outliers, but is not continuously differentiable;
    therefore, there is no closed-form expression for model training and training
    is generally accomplished by iterative optimization. The equation is,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­˜åœ¨å¼‚å¸¸å€¼çš„æƒ…å†µä¸‹å…·æœ‰é²æ£’æ€§ï¼Œä½†ä¸æ˜¯è¿ç»­å¯å¾®çš„ï¼›å› æ­¤ï¼Œæ²¡æœ‰æ¨¡å‹è®­ç»ƒçš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼Œæ¨¡å‹è®­ç»ƒé€šå¸¸é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¥å®Œæˆã€‚è¯¥æ–¹ç¨‹æ˜¯ï¼Œ
- en: \[ \text{Test MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    |y_i - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta
    y_i| \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Test MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    |y_i - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta
    y_i| \]
- en: Variance Explained
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£é‡Šçš„æ–¹å·®
- en: The proportion of variance of the response feature captured by the model. Assumes
    additivity of variance; therefore, we only use this model metric for linear models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ•è·çš„å“åº”ç‰¹å¾æ–¹å·®çš„æ¯”ç‡ã€‚å‡è®¾æ–¹å·®çš„å¯åŠ æ€§ï¼›å› æ­¤ï¼Œæˆ‘ä»¬åªå¯¹çº¿æ€§æ¨¡å‹ä½¿ç”¨æ­¤æ¨¡å‹åº¦é‡ã€‚
- en: First we calculate the variance explained by the model, simply as the variance
    of the model predictions,
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—æ¨¡å‹è§£é‡Šçš„æ–¹å·®ï¼Œç®€å•åœ°è¯´å°±æ˜¯æ¨¡å‹é¢„æµ‹çš„æ–¹å·®ï¼Œ
- en: \[ \sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    ( \hat{y}_i - \bar{y} )^2 \]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    ( \hat{y}_i - \bar{y} )^2 \]
- en: then we calculate the variance not explained by the model as the variance of
    the error over the model predictions,
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¡ç®—æ¨¡å‹æœªè§£é‡Šçš„æ–¹å·®ï¼Œä½œä¸ºæ¨¡å‹é¢„æµ‹è¯¯å·®çš„æ–¹å·®ï¼Œ
- en: \[ \sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 \]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 \]
- en: then under the assumption of additivity of variance, we calculate the ratio
    of variance explained over all variance, variance explained plus variance not
    explained,
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æ–¹å·®å¯åŠ æ€§çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—è§£é‡Šçš„æ–¹å·®ä¸æ‰€æœ‰æ–¹å·®ï¼ˆè§£é‡Šçš„æ–¹å·®åŠ ä¸Šæœªè§£é‡Šçš„æ–¹å·®ï¼‰çš„æ¯”ç‡ï¼Œ
- en: \[ r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not
    explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2} \]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not
    explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2} \]
- en: For linear regression, recall \(r^2 = \left( \rho_(X,y) \right)^2\); therefore,
    like correlation coefficients, \(r^2\),
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçº¿æ€§å›å½’ï¼Œå›å¿† \(r^2 = \left( \rho_(X,y) \right)^2\)ï¼›å› æ­¤ï¼Œåƒç›¸å…³ç³»æ•°ä¸€æ ·ï¼Œ\(r^2\)ï¼Œ
- en: has similar issues as correlation with respect to outliers and mixing multiple
    populations, e.g., Simpsonâ€™s Paradox
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸å¼‚å¸¸å€¼å’Œæ··åˆå¤šä¸ªæ€»ä½“ç›¸å…³çš„ç›¸å…³æ€§é—®é¢˜ç±»ä¼¼ï¼Œä¾‹å¦‚ï¼Œè¾›æ™®æ£®æ‚–è®º
- en: for nonlinear models consider pseudo-R-square methods
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºéçº¿æ€§æ¨¡å‹ï¼Œè€ƒè™‘ä¼ªRå¹³æ–¹æ–¹æ³•
- en: Also, even a linear model can have a negative \(r^2\) if the model trend contradicts
    the data trend, for example, if you fit data with a negative slope with a linear
    model with a positive slope!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå³ä½¿çº¿æ€§æ¨¡å‹ä¹Ÿå¯èƒ½æœ‰è´Ÿçš„ \(r^2\)ï¼Œå¦‚æœæ¨¡å‹è¶‹åŠ¿ä¸æ•°æ®è¶‹åŠ¿ç›¸çŸ›ç›¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ ç”¨å…·æœ‰æ­£æ–œç‡çš„çº¿æ€§æ¨¡å‹æ‹Ÿåˆå…·æœ‰è´Ÿæ–œç‡çš„æ•°æ®ï¼
- en: Inlier Ratio
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…ç‚¹æ¯”ç‡
- en: The proportion of testing data, \(y_i\) within a margin, \(\epsilon\), of the
    model, \(\hat{y}_i\), calculated as,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ•°æ®ä¸­ï¼Œ\(y_i\) åœ¨æ¨¡å‹ï¼Œ\(\hat{y}_i\) çš„è¾¹ç•Œï¼Œ\(\epsilon\) å†…çš„æ¯”ä¾‹ï¼Œè®¡ç®—å¦‚ä¸‹ï¼Œ
- en: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
- en: where the indicator transform, \(I_R\) is defined as,
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æŒ‡ç¤ºå˜æ¢ï¼Œ\(I_R\) è¢«å®šä¹‰ä¸ºï¼Œ
- en: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
- en: Hereâ€™s an illustration of the inlier ratio model metric, \(I_R\) model metric,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå†…ç‚¹æ¯”ç‡æ¨¡å‹åº¦é‡ï¼Œ\(I_R\) æ¨¡å‹åº¦é‡çš„æ’å›¾ï¼Œ
- en: '![](../Images/3ddaa66ff3998a59b318297be09c60b8.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ddaa66ff3998a59b318297be09c60b8.png)'
- en: Testing data, model with margin, \(\epsilon\), and outliers (white) and inliers
    (red) identified, 16 inliers out of 25 data samples, \(ğ¼ğ‘… = 0.64\).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ•°æ®ï¼Œå…·æœ‰è¾¹ç•Œçš„æ¨¡å‹ï¼Œ\(\epsilon\)ï¼Œä»¥åŠå¼‚å¸¸å€¼ï¼ˆç™½è‰²ï¼‰å’Œå†…ç‚¹ï¼ˆçº¢è‰²ï¼‰å·²è¯†åˆ«ï¼Œ25ä¸ªæ•°æ®æ ·æœ¬ä¸­æœ‰16ä¸ªå†…ç‚¹ï¼Œ\(ğ¼ğ‘… = 0.64\)ã€‚
- en: While the illustration is a linear model, this metric may be applied to any
    model. Although there is some subjectivity with the inlier ratio model metric,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ’å›¾æ˜¯çº¿æ€§æ¨¡å‹ï¼Œä½†æ­¤åº¦é‡å¯ä»¥åº”ç”¨äºä»»ä½•æ¨¡å‹ã€‚å°½ç®¡å†…ç‚¹æ¯”ç‡æ¨¡å‹åº¦é‡å­˜åœ¨ä¸€äº›ä¸»è§‚æ€§ï¼Œ
- en: what is the best selection for the margin, \(\epsilon\)?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æœ€ä¼˜çš„è¾¹ç•Œé€‰æ‹©ï¼Œ\(\epsilon\)ï¼Ÿ
- en: Common Classification Model Metrics
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§åˆ†ç±»æ¨¡å‹åº¦é‡
- en: Letâ€™s review some of the common model metrics for classification models. Classification
    is potentially more complicated than regression, since instead of a single model
    metric, we actually calculate an entire confusion matrix,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€äº›å¸¸è§çš„åˆ†ç±»æ¨¡å‹åº¦é‡ã€‚åˆ†ç±»å¯èƒ½æ¯”å›å½’æ›´å¤æ‚ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šè®¡ç®—çš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ··æ·†çŸ©é˜µï¼Œ
- en: a \(K \times K\) matrix with frequencies of predicted (x axis) vs. actual (y
    axis) categories to visualize the performance of a classification model, where
    \(K\) is the response feature cardinality, i.e., the number of possible categories
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª \(K \times K\) çš„çŸ©é˜µï¼ŒåŒ…å«é¢„æµ‹ï¼ˆxè½´ï¼‰ä¸å®é™…ï¼ˆyè½´ï¼‰ç±»åˆ«çš„é¢‘ç‡ï¼Œä»¥å¯è§†åŒ–åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶ä¸­ \(K\) æ˜¯å“åº”ç‰¹å¾çš„åŸºæ•°ï¼Œå³å¯èƒ½ç±»åˆ«çš„æ•°é‡
- en: visualize and diagnose all the combinations of correct and misclassification
    with the classification model, for example, category 1 is often misclassified
    as category 3,
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†ç±»æ¨¡å‹å¯è§†åŒ–å¹¶è¯Šæ–­æ‰€æœ‰æ­£ç¡®å’Œé”™è¯¯åˆ†ç±»çš„ç»„åˆï¼Œä¾‹å¦‚ï¼Œç±»åˆ«1é€šå¸¸è¢«é”™è¯¯åœ°åˆ†ç±»ä¸ºç±»åˆ«3ï¼Œ
- en: '![](../Images/0c2a856f882f5bcc7e9df675ea5afb21.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c2a856f882f5bcc7e9df675ea5afb21.png)'
- en: Example confusion matrix for a classification model, 2D matrix with the frequencies
    of all cases of truth and predicted categories.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»æ¨¡å‹ï¼Œä¸€ä¸ªç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œ2DçŸ©é˜µï¼ŒåŒ…å«æ‰€æœ‰çœŸå®å’Œé¢„æµ‹ç±»åˆ«çš„é¢‘ç‡ã€‚
- en: perfect accuracy is number of each class, \(n_1, n_2, \ldots, n_K\) on the diagonal,
    i.e., category 1 is always predicted as category 1, etc.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®Œç¾å‡†ç¡®åº¦æ˜¯æ¯ä¸ªç±»åˆ«çš„æ•°é‡ï¼Œ\(n_1, n_2, \ldots, n_K\) åœ¨å¯¹è§’çº¿ä¸Šï¼Œå³ç±»åˆ«1æ€»æ˜¯è¢«é¢„æµ‹ä¸ºç±»åˆ«1ç­‰ã€‚
- en: '![](../Images/34df964c246951e764fc3a7e8fecbf96.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34df964c246951e764fc3a7e8fecbf96.png)'
- en: Example confusion matrix for perfectly accurate classification model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œç¾å‡†ç¡®åº¦çš„åˆ†ç±»æ¨¡å‹ç¤ºä¾‹æ··æ·†çŸ©é˜µã€‚
- en: the confusion matrix is applied to calculate a single summary of categorical
    accuracy, for example, precision, recall, etc.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µè¢«åº”ç”¨äºè®¡ç®—åˆ†ç±»å‡†ç¡®æ€§çš„å•ä¸ªæ±‡æ€»ï¼Œä¾‹å¦‚ï¼Œç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰ã€‚
- en: model metrics are specific to the specific category and may significantly vary
    over categories, i.e., we can predict well for category \(k=1\) but not for category
    \(k=3\).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº¦é‡ç‰¹å®šäºç‰¹å®šç±»åˆ«ï¼Œå¹¶ä¸”å¯èƒ½åœ¨ç±»åˆ«ä¹‹é—´æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå³æˆ‘ä»¬å¯èƒ½å¯¹ç±»åˆ« \(k=1\) é¢„æµ‹å¾—å¾ˆå¥½ï¼Œä½†å¯¹ç±»åˆ« \(k=3\) åˆ™ä¸ç„¶ã€‚
- en: Precision
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç²¾ç¡®åº¦
- en: For category \(ğ‘˜\), precision is the ratio of true positive over all positives,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç±»åˆ« \(ğ‘˜\)ï¼Œç²¾ç¡®åº¦æ˜¯çœŸå®æ­£ä¾‹ä¸æ‰€æœ‰æ­£ä¾‹çš„æ¯”ç‡ï¼Œ
- en: \[ \text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true
    positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all
    positives}} \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true
    positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all
    positives}} \]
- en: we can intuitively describe precision as the conditional probability,
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°ç²¾ç¡®åº¦ä½œä¸ºæ¡ä»¶æ¦‚ç‡ï¼Œ
- en: \[ \text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says
    } k \text{ is happening}\right) \]![](../Images/020824af3f57f97af1ee2ba6ca6de684.png)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says
    } k \text{ is happening}\right) \]![](../Images/020824af3f57f97af1ee2ba6ca6de684.png)
- en: Example confusion matrix with illustration of the precision model metric for
    each category, \(k\).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„ç²¾ç¡®åº¦æ¨¡å‹æŒ‡æ ‡ã€‚
- en: For this example, we can calculate the precision for each category as,
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®åº¦ï¼Œ
- en: Category k=1
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k=1
- en: \[ \text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 \]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 \]
- en: Category k = 2,
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 2,
- en: \[ \text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42
    \]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42
    \]
- en: Category k = 3,
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 3,
- en: \[ \text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 \]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 \]
- en: Recall (called sensitivity in medical)
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¬å›ç‡ï¼ˆåœ¨åŒ»å­¦ä¸­ç§°ä¸ºçµæ•åº¦ï¼‰
- en: Recall for group \(ğ‘˜\) is the ratio of true positives over all cases of \(ğ‘˜\).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ \(ğ‘˜\) çš„å¬å›ç‡æ˜¯çœŸå®é˜³æ€§æ•°ä¸ \(ğ‘˜\) çš„æ‰€æœ‰æ¡ˆä¾‹æ•°çš„æ¯”ç‡ã€‚
- en: \[ \text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}}
    + n_{k \text{ false negative}}} \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}}
    + n_{k \text{ false negative}}} \]
- en: We can intuitively describe recall as, how many of group ğ‘˜ did we catch?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°å¬å›ç‡ï¼Œå³æˆ‘ä»¬æ•è·äº†å¤šå°‘ç»„ ğ‘˜ çš„æˆå‘˜ï¼Ÿ
- en: Note, recall does not account for false positives.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¬å›ç‡ä¸è€ƒè™‘å‡é˜³æ€§ã€‚
- en: '![](../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png)'
- en: Example confusion matrix with illustration of the recall model metric for each
    category, \(k\).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„å¬å›ç‡æ¨¡å‹æŒ‡æ ‡ã€‚
- en: For this example, we can calculate the recall for each category as,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¬å›ç‡ï¼Œ
- en: Category k=1
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k=1
- en: \[ \text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 \]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 \]
- en: Category k = 2,
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 2,
- en: \[ \text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 \]
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 \]
- en: Category k = 3,
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 3,
- en: \[ \text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 \]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 \]
- en: Specificity
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰¹å¼‚æ€§
- en: Specificity for group \(ğ‘˜\) is the ratio of true negatives over all negative
    cases of \(n \ne ğ‘˜\).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ \(ğ‘˜\) çš„ç‰¹å¼‚æ€§æ˜¯çœŸå®è´Ÿä¾‹æ•°ä¸ \(n \ne ğ‘˜\) çš„æ‰€æœ‰è´Ÿä¾‹æ•°çš„æ¯”ç‡ã€‚
- en: \[ \text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k
    \text{ true negative}} + n_{k \text{ false positive}}} \]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k
    \text{ true negative}} + n_{k \text{ false positive}}} \]
- en: We can intuitively describe specificity as, how many of not group \(k\) did
    we catch?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°ç‰¹å¼‚æ€§ï¼Œå³æˆ‘ä»¬æ•è·äº†å¤šå°‘éç»„ \(k\) çš„æˆå‘˜ï¼Ÿ
- en: Note, recall does not account for true positives.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¬å›ç‡ä¸è€ƒè™‘çœŸå®é˜³æ€§ã€‚
- en: '![](../Images/22a61637f89b9d1f446385497e2e9b65.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22a61637f89b9d1f446385497e2e9b65.png)'
- en: Example confusion matrix with illustration of the recall model metric for each
    category, \(k\).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„å¬å›ç‡æ¨¡å‹æŒ‡æ ‡ã€‚
- en: For this example, we can calculate the recall for each category as,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¬å›ç‡ï¼Œ
- en: Category k=1
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k=1
- en: \[ \text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5
    + 7)} = \frac{50}{62} = 0.81 \]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5
    + 7)} = \frac{50}{62} = 0.81 \]
- en: Category k = 2,
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 2,
- en: \[ \text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 +
    15)} = \frac{28}{58} = 0.48 \]
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 +
    15)} = \frac{28}{58} = 0.48 \]
- en: Category k = 3,
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 3,
- en: \[ \text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2
    + 9)} = \frac{57}{68} = 0.84 \]
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2
    + 9)} = \frac{57}{68} = 0.84 \]
- en: f1-score
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: f1-score
- en: f1-score is the harmonic mean of precision and recall for each \(k\) category,
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: f1-score æ˜¯æ¯ä¸ª \(k\) ç±»åˆ«ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ã€‚
- en: \[ \text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}}
    \]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}}
    \]
- en: The idea is to combine precision and recall into a single metric since they
    both see different aspects of the classification model accuracy.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ç†å¿µæ˜¯å°†ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆå¹¶æˆä¸€ä¸ªå•ä¸€æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒä»¬éƒ½çœ‹åˆ°äº†åˆ†ç±»æ¨¡å‹å‡†ç¡®æ€§çš„ä¸åŒæ–¹é¢ã€‚
- en: the harmonic mean is sensitive the to lowest score; therefore, good performance
    in one score cannot average out or make up for bad performance in the other
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒå’Œå¹³å‡æ•°å¯¹æœ€ä½åˆ†æ•æ„Ÿï¼›å› æ­¤ï¼Œä¸€ä¸ªåˆ†æ•°çš„è‰¯å¥½è¡¨ç°ä¸èƒ½å¹³å‡æˆ–å¼¥è¡¥å¦ä¸€ä¸ªåˆ†æ•°çš„ç³Ÿç³•è¡¨ç°
- en: Train and Test Hold Out Cross Validation
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯
- en: If only one train and test data split is applied to tune our machine learning
    model hyperparameters then we are applying the hold out cross validation approach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåªåº”ç”¨ä¸€ä¸ªè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²æ¥è°ƒæ•´æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¶…å‚æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ­£åœ¨åº”ç”¨ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯æ–¹æ³•ã€‚
- en: we split the data into training and testing data, these are exhaustive and mutually
    exclusive groups.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®åˆ†å‰²æˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼Œè¿™äº›æ˜¯è¯¦å°½ä¸”äº’æ–¥çš„ç»„ã€‚
- en: but this cross validation method is not exahustive, we only consider the one
    split for testing, most data are not tested. Also, we do not explore the full
    combinatorial of possible splits (more about this as we compare with other cross
    validation methods)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†è¿™ç§äº¤å‰éªŒè¯æ–¹æ³•å¹¶ä¸å…¨é¢ï¼Œæˆ‘ä»¬åªè€ƒè™‘ä¸€ä¸ªæµ‹è¯•åˆ†å‰²ï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½æ²¡æœ‰è¢«æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰æ¢ç´¢æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç»„åˆï¼ˆæˆ‘ä»¬å°†åœ¨ä¸å…¶ä»–äº¤å‰éªŒè¯æ–¹æ³•æ¯”è¾ƒæ—¶äº†è§£æ›´å¤šå…³äºè¿™ä¸€ç‚¹ï¼‰
- en: '![](../Images/ada2e0334323368c40690e596e78d464.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ada2e0334323368c40690e596e78d464.png)'
- en: The train and test data hold out cross validation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯ã€‚
- en: The workflow is,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹æ˜¯ï¼Œ
- en: withhold the testing data subset from model training
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ¨¡å‹è®­ç»ƒä¸­ä¿ç•™æµ‹è¯•æ•°æ®å­é›†
- en: train models with the remaining training data with various hyperparameters representing
    simple to complicated models
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‰©ä½™çš„è®­ç»ƒæ•°æ®ä»¥åŠä»£è¡¨ä»ç®€å•åˆ°å¤æ‚æ¨¡å‹çš„å¤šç§è¶…å‚æ•°æ¥è®­ç»ƒæ¨¡å‹
- en: then test the suite of simple to complicated trained models with withheld testing
    data
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨ä¿ç•™çš„æµ‹è¯•æ•°æ®æµ‹è¯•ä»ç®€å•åˆ°å¤æ‚çš„è®­ç»ƒæ¨¡å‹å¥—ä»¶
- en: select the model hyperparameters (complexity) with lowest testing error
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©å…·æœ‰æœ€ä½æµ‹è¯•é”™è¯¯çš„æ¨¡å‹è¶…å‚æ•°ï¼ˆå¤æ‚æ€§ï¼‰
- en: retrain the model with the turned hyperparameters and all of the data for deployment
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°å’Œæ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²
- en: The advantage of this approach is that we can readily evaluate the training
    and testing data split.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥è½»æ¾è¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²ã€‚
- en: since there is only one split, we can easily visualize and evaluate the train
    and test data cases, coverage and balance
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºåªæœ‰ä¸€ä¸ªåˆ†å‰²ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å¯è§†åŒ–å’Œè¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ¡ˆä¾‹ã€è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: The disadvantage is that this method may be sensitive to the specific selection
    of testing data
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯è¿™ç§æ–¹æ³•å¯èƒ½å¯¹æµ‹è¯•æ•°æ®çš„ç‰¹å®šé€‰æ‹©æ•æ„Ÿ
- en: as a result hold out cross validation may result in a noisy plot of testing
    error vs. the hyperparameter
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯å¯èƒ½ä¼šå¯¼è‡´æµ‹è¯•é”™è¯¯ä¸è¶…å‚æ•°çš„å™ªå£°å›¾
- en: Train, Validate and Test Hold Out Cross Validation
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯
- en: There is a more complete hold out cross validation workflow commonly applied,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸ç”¨çš„æ›´å®Œæ•´çš„ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯å·¥ä½œæµç¨‹æ˜¯
- en: '**Train with training data split** - models sees and learns from this data
    to train the model parameters.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨è®­ç»ƒæ•°æ®åˆ†å‰²è¿›è¡Œè®­ç»ƒ** - æ¨¡å‹ä»è¿™ä¸ªæ•°æ®ä¸­çœ‹åˆ°å¹¶å­¦ä¹ ä»¥è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚'
- en: '**Validate with the validation data split** - evaluation of model complexity
    vs. accuracy with data withheld from model parameter training to tune the model
    hyperparameters. The same as testing data in train and test workflow.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨éªŒè¯æ•°æ®åˆ†å‰²è¿›è¡ŒéªŒè¯** - è¯„ä¼°æ¨¡å‹å¤æ‚æ€§ç›¸å¯¹äºå‡†ç¡®æ€§çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯ä»æ¨¡å‹å‚æ•°è®­ç»ƒä¸­ä¿ç•™å‡ºæ¥çš„ï¼Œä»¥è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ã€‚è¿™ä¸è®­ç»ƒå’Œæµ‹è¯•å·¥ä½œæµç¨‹ä¸­çš„æµ‹è¯•æ•°æ®ç›¸åŒã€‚'
- en: '**Test model performance with testing data** - data withheld until the model
    is complete to provide a final evaluation of model performance. This data had
    no role in building the model and is commonly applied to compare multiple competing
    models.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨æµ‹è¯•æ•°æ®æµ‹è¯•æ¨¡å‹æ€§èƒ½** - æ•°æ®ä¿ç•™åˆ°æ¨¡å‹å®Œæˆï¼Œä»¥æä¾›å¯¹æ¨¡å‹æ€§èƒ½çš„æœ€ç»ˆè¯„ä¼°ã€‚è¿™äº›æ•°æ®åœ¨æ„å»ºæ¨¡å‹æ—¶æ²¡æœ‰å‘æŒ¥ä½œç”¨ï¼Œé€šå¸¸ç”¨äºæ¯”è¾ƒå¤šä¸ªç«äº‰æ¨¡å‹ã€‚'
- en: '![](../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png)'
- en: The train, validate and test hold out cross validation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¿æŒåˆ†ç¦»äº¤å‰éªŒè¯ã€‚
- en: I understand the motivation for the training, validation and testing cross validation
    workflow. It is an attempt to check our models, objectively, with cases that,
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç†è§£è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•äº¤å‰éªŒè¯å·¥ä½œæµç¨‹çš„åŠ¨æœºã€‚è¿™æ˜¯å°è¯•ç”¨æˆ‘ä»¬çŸ¥é“çœŸç›¸å¹¶èƒ½å‡†ç¡®è®¿é—®å‡†ç¡®æ€§çš„æ¡ˆä¾‹æ¥å®¢è§‚åœ°æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: we know the truth and can access accuracy accurately
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“çœŸç›¸å¹¶èƒ½å‡†ç¡®è®¿é—®å‡†ç¡®æ€§
- en: had nothing to do with the model construction, training model parameters nor
    tuning model hyperparameters
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹æ„å»ºã€è®­ç»ƒæ¨¡å‹å‚æ•°æˆ–è°ƒæ•´æ¨¡å‹è¶…å‚æ•°æ— å…³
- en: I appreciate this, but I have some concerns,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹æ­¤è¡¨ç¤ºèµèµï¼Œä½†æˆ‘æœ‰ä¸€äº›æ‹…å¿§ï¼Œ
- en: We are further reducing the number of samples available to training model parameters
    and to tuning model hyperparameters.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿›ä¸€æ­¥å‡å°‘äº†å¯ç”¨äºè®­ç»ƒæ¨¡å‹å‚æ•°å’Œè°ƒæ•´æ¨¡å‹è¶…å‚æ•°çš„æ ·æœ¬æ•°é‡ã€‚
- en: Eventually we will retrain the tuned model with all the data, so the model we
    test is not actually the final deployed model.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæˆ‘ä»¬å°†ä½¿ç”¨æ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒè°ƒæ•´åçš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬æµ‹è¯•çš„æ¨¡å‹å®é™…ä¸Šå¹¶ä¸æ˜¯æœ€ç»ˆéƒ¨ç½²çš„æ¨¡å‹ã€‚
- en: What do we do if the testing data is not accurately predicted? Do we include
    another round of testing with another withheld subset of the data? Ad infinitum?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæµ‹è¯•æ•°æ®æ²¡æœ‰è¢«å‡†ç¡®é¢„æµ‹ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿæˆ‘ä»¬æ˜¯å¦åŒ…æ‹¬å¦ä¸€è½®æµ‹è¯•ï¼Œä½¿ç”¨æ•°æ®é›†çš„å¦ä¸€éƒ¨åˆ†è¿›è¡Œæµ‹è¯•ï¼Ÿæ— ç©·æ— å°½ï¼Ÿ
- en: Load the Required Libraries
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ‰€éœ€çš„åº“
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç åŠ è½½æ‰€éœ€çš„åº“ã€‚è¿™äº›åº“åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: '[PRE0]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Declare Functions
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: I also added a convenience function to add major and minor gridlines to improve
    plot interpretability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜æ·»åŠ äº†ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥æ·»åŠ ä¸»ç½‘æ ¼çº¿å’Œå‰¯ç½‘æ ¼çº¿ï¼Œä»¥æé«˜ç»˜å›¾çš„å¯è§£é‡Šæ€§ã€‚
- en: '[PRE1]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load Data to Demonstration Cross Validation Methods
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æ•°æ®åŠ è½½åˆ°æ¼”ç¤ºäº¤å‰éªŒè¯æ–¹æ³•
- en: Letâ€™s load a spatial dataset and select 2 predictor features to visualize cross
    validation methods.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªç©ºé—´æ•°æ®é›†å¹¶é€‰æ‹©2ä¸ªé¢„æµ‹ç‰¹å¾æ¥å¯è§†åŒ–äº¤å‰éªŒè¯æ–¹æ³•ã€‚
- en: we will focus on the data splits and not the actual model training and tuning.
    Later when we cover predictive machine learning methods we will add the model
    component of the workflow.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…³æ³¨æ•°æ®æ‹†åˆ†ï¼Œè€Œä¸æ˜¯å®é™…çš„æ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´ã€‚ç¨åå½“æˆ‘ä»¬ä»‹ç»é¢„æµ‹æœºå™¨å­¦ä¹ æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬å°†æ·»åŠ å·¥ä½œæµç¨‹ä¸­çš„æ¨¡å‹ç»„ä»¶ã€‚
- en: '[PRE2]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Visualize Train and Test Hold Out Cross Validation
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¿ç•™çš„äº¤å‰éªŒè¯
- en: Letâ€™s compare the train and test with train, validate and test hold out data
    splits.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ä¿ç•™çš„æ‹†åˆ†ã€‚
- en: first we plot a train and test data split and then a train, validate and test
    split.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ‹†åˆ†ï¼Œç„¶åæ˜¯è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ‹†åˆ†ã€‚
- en: '[PRE4]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png](../Images/a54bde3cc01f9083714572540a961fb0.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png](../Images/a54bde3cc01f9083714572540a961fb0.png)'
- en: It is a good idea to visualize the train and test split,
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œ
- en: histograms for each predictor feature and the response feature to ensure that
    the train and test cover the range of possible outcomes and are balanced
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–äº†å¯èƒ½çš„èŒƒå›´å¹¶ä¸”æ˜¯å¹³è¡¡çš„
- en: if the number of predictor features is 2 then we can actually plot the predictor
    feature space to check coverage and balance of train and test data splits
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé¢„æµ‹ç‰¹å¾çš„æ•°é‡æ˜¯2ï¼Œé‚£ä¹ˆæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ç»˜åˆ¶é¢„æµ‹ç‰¹å¾ç©ºé—´æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ‹†åˆ†çš„è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: Now letâ€™s repeat this for the train, validate and test data split.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œé’ˆå¯¹è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®æ‹†åˆ†ã€‚
- en: '[PRE5]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png](../Images/8537d6caff0c1936ede31daca961765a.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png](../Images/8537d6caff0c1936ede31daca961765a.png)'
- en: Once again we can visualize the splits, now train, validate and test,
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ‹†åˆ†ï¼Œç°åœ¨åŒ…æ‹¬è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ï¼Œ
- en: histograms for each predictor feature and the response feature to ensure that
    the train and test cover the range of possible outcomes and are balanced
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œä»¥ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–äº†å¯èƒ½çš„èŒƒå›´å¹¶ä¸”æ˜¯å¹³è¡¡çš„
- en: if the number of predictor features is 2 then we can actually plot the predictor
    feature space to check coverage and balance of train and test data splits
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé¢„æµ‹ç‰¹å¾çš„æ•°é‡æ˜¯2ï¼Œé‚£ä¹ˆæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ç»˜åˆ¶é¢„æµ‹ç‰¹å¾ç©ºé—´æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ‹†åˆ†çš„è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: Leave-one-out Cross Validation (LOO CV)
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOO CV)
- en: Leave-one-out cross validation is an exhaustive cross validation method, i.e.,
    all data gets tested by loop over all the data.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯æ˜¯ä¸€ç§è¯¦å°½çš„äº¤å‰éªŒè¯æ–¹æ³•ï¼Œå³é€šè¿‡éå†æ‰€æœ‰æ•°æ®æ¥æµ‹è¯•æ‰€æœ‰æ•°æ®ã€‚
- en: we train and tune \(n\) models, for each model a single datum is withheld as
    testing and the \(n-1\) data are assigned as training data
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®­ç»ƒå’Œè°ƒæ•´ \(n\) ä¸ªæ¨¡å‹ï¼Œå¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªæ•°æ®ç‚¹è¢«ä¿ç•™ä½œä¸ºæµ‹è¯•ï¼Œè€Œ \(n-1\) ä¸ªæ•°æ®è¢«åˆ†é…ä¸ºè®­ç»ƒæ•°æ®
- en: we will calculate \(n\) training and testing errors that will be aggregated
    over all \(n\) models, for example, the average of the mean square error.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¡ç®— \(n\) ä¸ªè®­ç»ƒå’Œæµ‹è¯•è¯¯å·®ï¼Œè¿™äº›è¯¯å·®å°†åœ¨æ‰€æœ‰ \(n\) ä¸ªæ¨¡å‹ä¸Šæ±‡æ€»ï¼Œä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®çš„å¹³å‡å€¼ã€‚
- en: In the case of leave-one-out cross validation,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç•™ä¸€æ³•äº¤å‰éªŒè¯çš„æƒ…å†µä¸‹ï¼Œ
- en: we test at only one datum so the test error is just a single error at the single
    withheld datum, so we just use standard MSE over the \(n\) models
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæµ‹è¯•ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œå› æ­¤æµ‹è¯•è¯¯å·®åªæ˜¯å•ä¸ªä¿ç•™æ•°æ®ç‚¹çš„å•ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åªä½¿ç”¨æ ‡å‡†å‡æ–¹è¯¯å·®æ¥è¯„ä¼° \(n\) ä¸ªæ¨¡å‹
- en: \[ \text{Test MSE Aggregate} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{æµ‹è¯•å‡æ–¹è¯¯å·®æ€»å’Œ} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
- en: but, we have \(n-1\) training data for each model, so we aggregate, by averageing
    the mean square error of each model,
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬æœ‰ \(n-1\) ä¸ªè®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡å¹³å‡æ¯ä¸ªæ¨¡å‹çš„å‡æ–¹è¯¯å·®æ¥æ±‡æ€»ï¼Œ
- en: \[ \text{Train MSE Aggregate} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1}
    (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i \]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Train MSE Aggregate} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1}
    (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i \]
- en: Hereâ€™s the leave-one-out cross validation steps,
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç•™ä¸€æ³•äº¤å‰éªŒè¯çš„æ­¥éª¤ï¼Œ
- en: Loop over all \(n\) data, and withhold that data
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éå†æ‰€æœ‰ \(n\) ä¸ªæ•°æ®ï¼Œå¹¶ä¿ç•™è¯¥æ•°æ®
- en: Train on the remaining \(nâˆ’1\) data and test on the withheld single data
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å‰©ä½™çš„ \(nâˆ’1\) ä¸ªæ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨ä¿ç•™çš„å•ä¸ªæ•°æ®ä¸Šæµ‹è¯•
- en: Calculate model goodness metric, MSE for a single test data is the square error
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¨¡å‹è‰¯å¥½åº¦æŒ‡æ ‡ï¼Œå•ä¸ªæµ‹è¯•æ•°æ®çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ˜¯å¹³æ–¹è¯¯å·®
- en: Goto 1
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ° 1
- en: Aggregate model goodness metric over all data, \(n\)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ‰€æœ‰æ•°æ®èšåˆæ¨¡å‹è‰¯å¥½åº¦æŒ‡æ ‡ï¼Œ\(n\)
- en: Typically, leave-one-out cross validation is too easy of a prediction problem;
    therefore, it is not commonly used,
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œç•™ä¸€æ³•äº¤å‰éªŒè¯é¢„æµ‹é—®é¢˜è¿‡äºç®€å•ï¼›å› æ­¤ï¼Œå®ƒä¸å¸¸è¢«ä½¿ç”¨ï¼Œ
- en: but it introduces the concept of exhaustive cross validation, i.e., all data
    gets tested!
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†å®ƒå¼•å…¥äº†å…¨é¢äº¤å‰éªŒè¯çš„æ¦‚å¿µï¼Œå³æ‰€æœ‰æ•°æ®éƒ½è¿›è¡Œäº†æµ‹è¯•ï¼
- en: Leave-one-out cross validation is also exhaustive in the sense that the full
    combinatorial of \(n\) data choose \(p\) where \(p=1\) are explored,
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯åœ¨æ¢ç´¢ \(n\) ä¸ªæ•°æ®ä¸­é€‰æ‹© \(p\) çš„å®Œæ•´ç»„åˆæ–¹é¢ä¹Ÿæ˜¯å…¨é¢çš„ï¼Œå…¶ä¸­ \(p=1\)ï¼Œ
- en: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n
    - 1)!} = n \]
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n
    - 1)!} = n \]
- en: where the full combinatorial is the \(n\) models that we built above!
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œå®Œæ•´çš„ç»„åˆæ˜¯ä¸Šé¢æ„å»ºçš„ \(n\) ä¸ªæ¨¡å‹ï¼
- en: K-fold Cross Validation (k-fold CV)
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K æŠ˜äº¤å‰éªŒè¯ï¼ˆk-fold CVï¼‰
- en: K-fold is a more general, efficient and robust approach.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: K æŠ˜æ˜¯ä¸€ç§æ›´é€šç”¨ã€é«˜æ•ˆä¸”ç¨³å¥çš„æ–¹æ³•ã€‚
- en: a exhaustive cross validation approach (all data are tested), but it samples
    a limited set of the possible combinatorial of prediction problems, unlike Leave-one-out
    cross validation where we attempt every possible case on data withheld for testing
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§å…¨é¢äº¤å‰éªŒè¯æ–¹æ³•ï¼ˆæ‰€æœ‰æ•°æ®éƒ½è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œä½†å®ƒåªé‡‡æ ·äº†é¢„æµ‹é—®é¢˜å¯èƒ½ç»„åˆçš„æœ‰é™é›†ï¼Œä¸ç•™ä¸€æ³•äº¤å‰éªŒè¯ä¸åŒï¼Œåœ¨ç•™ä¸€æ³•äº¤å‰éªŒè¯ä¸­ï¼Œæˆ‘ä»¬å°è¯•å¯¹ä¿ç•™ç”¨äºæµ‹è¯•çš„æ•°æ®çš„æ¯ä¸€ç§å¯èƒ½æƒ…å†µè¿›è¡Œæµ‹è¯•
- en: for K-fold cross validation we assign a single set of K equal size splits and
    we loop over the splits, withholding the \(k\) split for testing data and using
    the data outside the split for training
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº K æŠ˜äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ç»„ K ä¸ªå¤§å°ç›¸ç­‰çš„åˆ†å‰²ï¼Œå¹¶éå†è¿™äº›åˆ†å‰²ï¼Œä¿ç•™ \(k\) ä¸ªåˆ†å‰²ä½œä¸ºæµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨åˆ†å‰²å¤–çš„æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®
- en: the testing proportion is \(\frac{1}{K}\), e.g., for \(K=3\), 33.3% is withheld
    for testing, for \(K=4\), 25% is withheld for testing and for \(K=5\), 20% is
    withheld for testing
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ¯”ä¾‹æ˜¯ \(\frac{1}{K}\)ï¼Œä¾‹å¦‚ï¼Œå¯¹äº \(K=3\)ï¼Œ33.3% è¢«ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œå¯¹äº \(K=4\)ï¼Œ25% è¢«ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œå¯¹äº
    \(K=5\)ï¼Œ20% è¢«ä¿ç•™ç”¨äºæµ‹è¯•
- en: We call it K-fold cross validation, because each of the splits is known as a
    fold. Hereâ€™s the steps for K-fold cross validation,
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°ä¹‹ä¸º K æŠ˜äº¤å‰éªŒè¯ï¼Œå› ä¸ºæ¯ä¸ªåˆ†å‰²éƒ½è¢«ç§°ä¸ºä¸€ä¸ªæŠ˜ã€‚ä»¥ä¸‹æ˜¯ K æŠ˜äº¤å‰éªŒè¯çš„æ­¥éª¤ï¼Œ
- en: Select \(K\), integer number of folds
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹© \(K\)ï¼Œæ•´æ•°ä¸ªæŠ˜
- en: Split the data into \(K\) equal size folds
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ•°æ®åˆ†æˆ \(K\) ä¸ªå¤§å°ç›¸ç­‰çš„æŠ˜
- en: Loop over each \(k = 1,\ldots,K\) fold
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éå†æ¯ä¸ª \(k = 1,\ldots,K\) æŠ˜
- en: Assign the data outside the \(k\) fold as training data and inside the \(k\)
    fold as testing data
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† \(k\) æŠ˜å¤–çš„æ•°æ®åˆ†é…ä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶å°† \(k\) æŠ˜å†…çš„æ•°æ®åˆ†é…ä¸ºæµ‹è¯•æ•°æ®
- en: Train and test the prediction model and calculated the testing model metric
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•é¢„æµ‹æ¨¡å‹ï¼Œå¹¶è®¡ç®—æµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: Goto 3
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ° 3
- en: Aggregate testing model metric over all K folds
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ‰€æœ‰ K æŠ˜è¿›è¡Œèšåˆæµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: As you can see above k-fold cross validation is exhaustive, since all data is
    tested, i.e., withheld as testing data, but the method is not exhaustive in that
    all possible \(\frac{n}{K}\) data subsets are not considered.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œk æŠ˜äº¤å‰éªŒè¯æ˜¯å…¨é¢çš„ï¼Œå› ä¸ºæ‰€æœ‰æ•°æ®éƒ½è¿›è¡Œäº†æµ‹è¯•ï¼Œå³ä½œä¸ºæµ‹è¯•æ•°æ®è¢«ä¿ç•™ï¼Œä½†è¿™ç§æ–¹æ³•å¹¶ä¸æ˜¯å…¨é¢çš„ï¼Œå› ä¸ºå¹¶æ²¡æœ‰è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„ \(\frac{n}{K}\)
    ä¸ªæ•°æ®å­é›†ã€‚
- en: To calculated the combinatorial for exhaustive K folds we used the multinomial
    coefficient,
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—å…¨é¢ K æŠ˜çš„ç»„åˆï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤šé¡¹å¼ç³»æ•°ï¼Œ
- en: \[ \frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!} \]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!} \]
- en: For example, if there are \(n=100\) data and \(K=4\) folds, there are \(6.72
    \times 10^55\) possible combinations. I vote that we stick with regular K-fold
    cross validation.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ \(n=100\) ä¸ªæ•°æ®å’Œ \(K=4\) ä¸ªæŠ˜ï¼Œé‚£ä¹ˆæœ‰ \(6.72 \times 10^55\) ç§å¯èƒ½çš„ç»„åˆã€‚æˆ‘å»ºè®®æˆ‘ä»¬åšæŒä½¿ç”¨å¸¸è§„çš„
    K æŠ˜äº¤å‰éªŒè¯ã€‚
- en: Letâ€™s visualize K-fold cross validation splits, for the case of \(K=4\).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ– K æŠ˜äº¤å‰éªŒè¯çš„åˆ†å‰²ï¼Œä»¥ \(K=4\) ä¸ºä¾‹ã€‚
- en: '[PRE6]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png](../Images/dd257beaa821a963fc15b3318a7bd9f2.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/dd257beaa821a963fc15b3318a7bd9f2.png)'
- en: Leave-p-out Cross Validation (LpO-CV)
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç•™å‡º \(p\) ä¸ªæ ·æœ¬äº¤å‰éªŒè¯ï¼ˆLpO-CVï¼‰
- en: This is the variant of K-fold cross validation that exhaustively samples the
    full combinatorial of withholding \(p\) testing data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ K æŠ˜äº¤å‰éªŒè¯çš„ä¸€ç§å˜ä½“ï¼Œå®ƒå½»åº•é‡‡æ ·äº†ä¿ç•™ \(p\) ä¸ªæµ‹è¯•æ•°æ®çš„å…¨éƒ¨ç»„åˆã€‚
- en: Select \(p\), integer number of testing data to withhold
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹© \(p\)ï¼Œä¿ç•™æµ‹è¯•æ•°æ®çš„æ•´æ•°æ•°é‡
- en: For all possible \(p\) subsets of \(n\),
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å¯èƒ½çš„ \(p\) ä¸ª \(n\) çš„å­é›†ï¼Œ
- en: Assign the data outside the \(p\) as training data and inside the \(p\) as testing
    data
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† \(p\) ä»¥å¤–çš„æ•°æ®åˆ†é…ä¸ºè®­ç»ƒæ•°æ®ï¼Œå°† \(p\) ä»¥å†…çš„æ•°æ®åˆ†é…ä¸ºæµ‹è¯•æ•°æ®
- en: Train and test the prediction model and calculated the testing model metric
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•é¢„æµ‹æ¨¡å‹å¹¶è®¡ç®—æµ‹è¯•æ¨¡å‹åº¦é‡
- en: Goto 2
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ° 2
- en: Aggregate testing model metric over the combinatorial
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ç»„åˆä¸Šèšåˆæµ‹è¯•æ¨¡å‹åº¦é‡
- en: For this case the combinatorial of cases is, \(n\) choose \(p\),
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ç§æƒ…å†µï¼Œæ¡ˆä¾‹çš„ç»„åˆæ˜¯ï¼Œä» \(n\) ä¸­é€‰æ‹© \(p\)ï¼Œ
- en: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} \]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} \]
- en: For \(n=100\) and \(p=20\), we have \(5.36 \times 10^{20}\) combinations to
    check!
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº \(n=100\) å’Œ \(p=20\)ï¼Œæˆ‘ä»¬æœ‰ \(5.36 \times 10^{20}\) ç§ç»„åˆéœ€è¦æ£€æŸ¥ï¼
- en: Limitations of Cross Validation
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯çš„é™åˆ¶
- en: Here are some additional issues with the model cross validation approach in
    general,
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›å…³äºæ¨¡å‹äº¤å‰éªŒè¯æ–¹æ³•çš„ä¸€èˆ¬é—®é¢˜ï¼Œ
- en: '**Peeking, Information Leakage** â€“ some information is transmitted from the
    withheld data into the model, some model decision(s) use all the data. Pipelines
    and wrappers help with this.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å·çœ‹ï¼Œä¿¡æ¯æ³„éœ²**â€”â€”ä¸€äº›ä¿¡æ¯ä»ä¿ç•™æ•°æ®ä¼ è¾“åˆ°æ¨¡å‹ä¸­ï¼Œä¸€äº›æ¨¡å‹å†³ç­–ï¼ˆä»¬ï¼‰ä½¿ç”¨æ‰€æœ‰æ•°æ®ã€‚ç®¡é“å’ŒåŒ…è£…å™¨æœ‰åŠ©äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚'
- en: '**Black Swans / Stationarity** â€“ the model cannot be tested for data events
    not available in the data. This is also known as the â€˜No Free Lunch Theoremâ€™ in
    machine learning'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é»‘å¤©é¹… / é™æ€æ€§**â€”â€”æ¨¡å‹æ— æ³•æµ‹è¯•æ•°æ®ä¸­ä¸å¯ç”¨çš„äº‹ä»¶ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºæœºå™¨å­¦ä¹ ä¸­çš„â€œæ— å…è´¹åˆé¤å®šç†â€'
- en: Consider the words of Hume,
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¼‘è°Ÿçš„è¯ï¼Œ
- en: â€œeven after the observation of the frequent or constant conjunction of objects,
    we have no reason to draw any inference concerning any object beyond those of
    which we have had experienceâ€ - Hume (1739â€“1740)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå³ä½¿è§‚å¯Ÿåˆ°äº†é¢‘ç¹æˆ–æ’å®šçš„å¯¹è±¡è”åˆï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰ç†ç”±å¯¹ä»»ä½•è¶…å‡ºæˆ‘ä»¬ç»éªŒèŒƒå›´çš„å¯¹è±¡åšå‡ºä»»ä½•æ¨è®ºâ€â€”â€”ä¼‘è°Ÿ (1739â€“1740)
- en: We cannot predict things that we have never seen in our data!
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•é¢„æµ‹æˆ‘ä»¬åœ¨æ•°æ®ä¸­ä»æœªè§è¿‡çš„äº‹ç‰©ï¼
- en: hereâ€™s a quote from the famous Oreskes et al. (1994) paper on subsurface validation
    and verification,
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»è‘—åçš„ Oreskes ç­‰äºº (1994) å…³äºåœ°ä¸‹éªŒè¯å’ŒéªŒè¯çš„è®ºæ–‡ä¸­æ‘˜å½•çš„ä¸€å¥è¯ï¼Œ
- en: â€œVerification and validation of numerical models of natural systems is impossible.
    This is because natural systems are never closed and because model results are
    always nonunique. Models can be confirmed by the demonstration of agreement between
    observation and prediction, but confirmation is inherently partial. Complete confirmation
    is logically precluded by the fallacy of affirming the consequent and by incomplete
    access to natural phenomena. Models can only be evaluated in relative terms, and
    their predictive value is always open to question. The primary value of models
    is heuristic.â€
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: â€œéªŒè¯å’ŒéªŒè¯è‡ªç„¶ç³»ç»Ÿçš„æ•°å€¼æ¨¡å‹æ˜¯ä¸å¯èƒ½çš„ã€‚è¿™æ˜¯å› ä¸ºè‡ªç„¶ç³»ç»Ÿæ°¸è¿œä¸ä¼šå°é—­ï¼Œå› ä¸ºæ¨¡å‹ç»“æœæ€»æ˜¯éå”¯ä¸€çš„ã€‚æ¨¡å‹å¯ä»¥é€šè¿‡è§‚å¯Ÿå’Œé¢„æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§æ¥è¯å®ï¼Œä½†è¯å®æœ¬è´¨ä¸Šæ˜¯ä¸å®Œå…¨çš„ã€‚ç”±äºè‚¯å®šåä»¶çš„è°¬è¯¯å’Œè‡ªç„¶ç°è±¡çš„ä¸å®Œå…¨è®¿é—®ï¼Œå®Œå…¨è¯å®åœ¨é€»è¾‘ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚æ¨¡å‹åªèƒ½ç›¸å¯¹è¯„ä¼°ï¼Œå®ƒä»¬çš„é¢„æµ‹ä»·å€¼æ€»æ˜¯å€¼å¾—æ€€ç–‘ã€‚æ¨¡å‹çš„ä¸»è¦ä»·å€¼æ˜¯å¯å‘æ€§çš„ã€‚â€
- en: Oreskes et al. (1994)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oreskes ç­‰äºº (1994)
- en: all of this is summed up very well with,
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›éƒ½å¾ˆå¥½åœ°æ€»ç»“ä¸ºï¼Œ
- en: â€˜All models are wrong, but some are usefulâ€™ â€“ George Box
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œä½†æœ‰äº›æ˜¯æœ‰ç”¨çš„â€â€”â€”ä¹”æ²»Â·åšå…‹æ–¯
- en: and a reminder of,
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶æé†’ï¼Œ
- en: '**Parsimony** â€“ since all models are wrong, an economical description of the
    system. Occamâ€™s Razor'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç®€çº¦æ€§**â€”â€”ç”±äºæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œå› æ­¤å¯¹ç³»ç»Ÿçš„ç»æµæè¿°ã€‚å¥¥å¡å§†å‰ƒåˆ€'
- en: resulting in a pragmatic approach of,
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è€Œå½¢æˆäº†ä¸€ç§å®ç”¨æ–¹æ³•ï¼Œ
- en: '**Worrying Selectively** â€“ since all models are wrong, figure out what is most
    importantly wrong.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©æ€§æ‹…å¿§**â€”â€”ç”±äºæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œæ‰¾å‡ºæœ€é‡è¦çš„é”™è¯¯æ˜¯ä»€ä¹ˆã€‚'
- en: finally, I add my own words,
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘åŠ å…¥äº†è‡ªå·±çš„è§‚ç‚¹ï¼Œ
- en: â€˜Be humble, the earth will surprise you!â€™ â€“ Michael Pyrcz
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè°¦é€Šï¼Œåœ°çƒä¼šç»™ä½ æƒŠå–œï¼â€â€”â€”è¿ˆå…‹å°”Â·çš®å°”å¥‡
- en: Comments
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„è®º
- en: This was a basic description of machine learning concepts. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹æœºå™¨å­¦ä¹ æ¦‚å¿µçš„åŸºæœ¬æè¿°ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„ [å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)
    ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§é“¾æ¥ã€‚
- en: I hope this was helpful,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„è´Ÿè´£äººï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚'
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®¡ç®—æœºä¸åœ°çƒç§‘å­¦](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[æ•°å­¦åœ°çƒç§‘å­¦](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚'
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿ˆå…‹å°”å·²ç»æ’°å†™äº†70å¤šç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€ŠGeostatistical
    Reservoir Modelingã€‹ï¼ˆ[äºšé©¬é€Šé“¾æ¥](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼‰ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€ŠApplied
    Geostatistics in Python: a Hands-on Guide with GeostatsPyã€‹ï¼ˆ[GitHubé“¾æ¥](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ï¼‰å’Œã€ŠApplied
    Machine Learning in Python: a Hands-on Guide with Codeã€‹ï¼ˆ[GitHubé“¾æ¥](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ï¼‰ã€‚'
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å«100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªGitHubè´¦æˆ·ä¸Šçš„è¯¦ç»†æ–‡æ¡£å·¥ä½œæµç¨‹ï¼Œè¿™äº›å·¥ä½œæµç¨‹åˆ†å¸ƒåœ¨40å¤šä¸ªå­˜å‚¨åº“ä¸­ï¼Œæ—¨åœ¨æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ æœºå™¨çš„äººæ¥è¯´æœ‰å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„Ÿå…´è¶£åˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººåŒ…æ‹¬Fosteræ•™æˆã€Torres-Verdinæ•™æˆå’Œvan Oortæ•™æˆï¼‰ï¼Ÿæˆ‘çš„ç ”ç©¶ç»“åˆæ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»åˆ°ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡Cockrellå·¥ç¨‹å­¦é™¢å’ŒJacksonåœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
- en: Training and Tuning Predictive Machine Learning Models
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œè°ƒæ•´é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹
- en: In predictive machine learning, we follow a standard model training and testing
    workflow. This process ensures that our model generalizes well to new data, rather
    than just fitting the training data perfectly.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é¢„æµ‹æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éµå¾ªæ ‡å‡†çš„æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å·¥ä½œæµç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æ–°æ•°æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯å®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
- en: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31abfecc21c246ba7144dfb847ab4378.png)'
- en: Standard predictive machine learning modeling workflow.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†é¢„æµ‹æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ã€‚
- en: Letâ€™s walk through the key steps,
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€ä¸€äº†è§£å…³é”®æ­¥éª¤ï¼Œ
- en: '**Train and Test Split** - divide the available data into mutually exclusive,
    exhaustive subsets: a training set and a testing set.'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²** - å°†å¯ç”¨çš„æ•°æ®åˆ’åˆ†ä¸ºäº’æ–¥çš„ã€ç©·å°½çš„å­é›†ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚'
- en: typically, 15%â€“30% of the data is held out for testing
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œ15%â€“30%çš„æ•°æ®è¢«ä¿ç•™ç”¨äºæµ‹è¯•
- en: the remaining 70%â€“85% is used for training the model
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰©ä½™çš„70%â€“85%ç”¨äºè®­ç»ƒæ¨¡å‹
- en: '**Define a range of hyperparameter(s)** values to explore, ranging from,'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å®šä¹‰è¦æ¢ç´¢çš„è¶…å‚æ•°å€¼èŒƒå›´**ï¼ŒèŒƒå›´ä»ï¼Œ'
- en: simple models with low flexibility
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®€å•æ¨¡å‹ï¼Œçµæ´»æ€§ä½
- en: to complex models with high flexibility
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ°å¤æ‚æ¨¡å‹ï¼Œçµæ´»æ€§é«˜
- en: \(\quad\) This step may involve tuning multiple hyperparameters, in which case
    efficient sampling methods (e.g., grid search, random search, or Bayesian optimization)
    are often used.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) æ­¤æ­¥éª¤å¯èƒ½æ¶‰åŠè°ƒæ•´å¤šä¸ªè¶…å‚æ•°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸ä½¿ç”¨é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œç½‘æ ¼æœç´¢ã€éšæœºæœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–ï¼‰ã€‚
- en: '**Train Model Parameters for Each Hyperparameter Setting** - for each set of
    hyperparameters, train a model on the training data. This yields:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä¸ºæ¯ä¸ªè¶…å‚æ•°è®¾ç½®è®­ç»ƒæ¨¡å‹å‚æ•°** - å¯¹äºæ¯ç»„è¶…å‚æ•°ï¼Œåœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚è¿™ä¼šäº§ç”Ÿï¼š'
- en: a suite of trained models, each with different complexity
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç³»åˆ—ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹å…·æœ‰ä¸åŒçš„å¤æ‚åº¦
- en: each model has parameters optimized to minimize error on the training data
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å‚æ•°ä¼˜åŒ–ä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®
- en: '**Evaluate Each Model on the Withheld Testing Data** - using the testing data,'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åœ¨ä¿ç•™çš„æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¯ä¸ªæ¨¡å‹** - ä½¿ç”¨æµ‹è¯•æ•°æ®ï¼Œ'
- en: evaluate how each trained model performs on unseen data
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¯ä¸ªè®­ç»ƒæ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°
- en: summarize prediction error (for example, root mean square error (RMSE), mean
    absolute error (MAE), classification accuracy) for each model
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹è¯¯å·®ï¼ˆä¾‹å¦‚ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€åˆ†ç±»å‡†ç¡®ç‡ï¼‰
- en: '**Select the Hyperparameters That Minimize Test Error** - this is the hyperparameter
    tuning step:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©æœ€å°åŒ–æµ‹è¯•é”™è¯¯çš„è¶…å‚æ•°** - è¿™æ˜¯è¶…å‚æ•°è°ƒæ•´æ­¥éª¤ï¼š'
- en: choose the model hyperparaemter(s) that performs best on the test data
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹è¶…å‚æ•°
- en: these are your tuned hyperparameters
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›å°±æ˜¯ä½ çš„è°ƒæ•´åçš„è¶…å‚æ•°
- en: '**Retrain the Final Model on All Data Using Tuned Hyperparameters** - now that
    the best model complexity has been identified,'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹** - ç°åœ¨å·²ç»ç¡®å®šäº†æœ€ä½³æ¨¡å‹å¤æ‚æ€§ï¼Œ'
- en: retrain the model using both the training and test sets
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†é‡æ–°è®­ç»ƒæ¨¡å‹
- en: this maximizes the amount of data used for final model parameter estimation
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æœ€å¤§åŒ–äº†ç”¨äºæœ€ç»ˆæ¨¡å‹å‚æ•°ä¼°è®¡çš„æ•°æ®é‡
- en: the resulting model is the one you deploy in real-world applications
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœæ¨¡å‹æ˜¯ä½ åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²çš„æ¨¡å‹
- en: Common Questions About the Model Training and Tuning Workflow
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºæ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´å·¥ä½œæµç¨‹çš„å¸¸è§é—®é¢˜
- en: As a professor, I often hear these questions when I introduce the above machine
    learning model training and tuning workflow.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åæ•™æˆï¼Œæˆ‘åœ¨ä»‹ç»ä¸Šè¿°æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´å·¥ä½œæµç¨‹æ—¶ï¼Œç»å¸¸å¬åˆ°è¿™äº›é—®é¢˜ã€‚
- en: '**What is the main outcome of steps 1â€“5?** - the only reliable outcome is the
    tuned hyperparameters.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1â€“5 çš„ä¸»è¦ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ** - å”¯ä¸€å¯é çš„è¾“å‡ºæ˜¯è°ƒæ•´åçš„è¶…å‚æ•°ã€‚'
- en: \(\quad\) we do not use the model trained in step 3 or 4 directly, because it
    was trained without access to all available data. Instead, we retrain the final
    model using all data with the selected hyperparameters.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) æˆ‘ä»¬ä¸ç›´æ¥ä½¿ç”¨æ­¥éª¤ 3 æˆ– 4 ä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ˜¯åœ¨æ— æ³•è®¿é—®æ‰€æœ‰å¯ç”¨æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒçš„ã€‚ç›¸åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰æ•°æ®å’Œé€‰å®šçš„è¶…å‚æ•°é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚
- en: '**Why not train the model on all the data from the beginning?** - because if
    we do that, we have no independent way to evaluate the modelâ€™s generalization.
    A very complex model can easily overfitâ€”fitting the training data perfectly, but
    performing poorly on new, unseen data.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆä¸€å¼€å§‹ä¸åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Ÿ** - å› ä¸ºå¦‚æœæˆ‘ä»¬é‚£æ ·åšï¼Œæˆ‘ä»¬å°±æ²¡æœ‰ç‹¬ç«‹çš„æ–¹å¼æ¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸€ä¸ªéå¸¸å¤æ‚çš„æ¨¡å‹å¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆâ€”â€”å®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚'
- en: \(\quad\) overfitting happens when model flexibility is too highâ€”it captures
    noise instead of the underlying pattern. Without a withheld test set, we canâ€™t
    detect this.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: \(\quad\) å½“æ¨¡å‹çµæ´»æ€§è¿‡é«˜æ—¶ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆâ€”â€”å®ƒæ•æ‰äº†å™ªå£°è€Œä¸æ˜¯æ½œåœ¨çš„æ¨¡å¼ã€‚å¦‚æœæ²¡æœ‰ä¿ç•™çš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬æ— æ³•æ£€æµ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: This workflow for training and tuning predictive machine learning models is,
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç”¨äºè®­ç»ƒå’Œè°ƒæ•´é¢„æµ‹æ€§æœºå™¨å­¦ä¹ æ¨¡å‹çš„å·¥ä½œæµç¨‹æ˜¯ï¼Œ
- en: an empirical, cross-validation-based process
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§åŸºäºç»éªŒã€äº¤å‰éªŒè¯çš„è¿‡ç¨‹
- en: a practical simulation of real-world model use
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ç°å®ä¸–ç•Œæ¨¡å‹ä½¿ç”¨çš„å®é™…æ¨¡æ‹Ÿ
- en: a method to identify the model complexity that best balances fit and generalization
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§è¯†åˆ«æœ€ä½³æ¨¡å‹å¤æ‚åº¦çš„æ–¹æ³•ï¼Œä»¥å¹³è¡¡æ‹Ÿåˆå’Œæ³›åŒ–
- en: Iâ€™ve said model parameters and model hyperparameters a bunch of times, so I
    owe you their definitions.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²ç»å¤šæ¬¡æåˆ°æ¨¡å‹å‚æ•°å’Œæ¨¡å‹è¶…å‚æ•°ï¼Œæ‰€ä»¥æˆ‘æœ‰è´£ä»»ç»™å‡ºå®ƒä»¬çš„å®šä¹‰ã€‚
- en: Model Parameters and Model Hyperparameters
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°å’Œæ¨¡å‹è¶…å‚æ•°
- en: '**Model parameters** are fit during training phase to minimize error at the
    training data, i.e., model parameters are trained with training data and control
    model fit to the data. For example,'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å‚æ•°**åœ¨è®­ç»ƒé˜¶æ®µè¿›è¡Œè°ƒæ•´ä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®çš„è¯¯å·®ï¼Œå³æ¨¡å‹å‚æ•°é€šè¿‡è®­ç»ƒæ•°æ®è®­ç»ƒï¼Œå¹¶æ§åˆ¶æ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆã€‚ä¾‹å¦‚ï¼Œ'
- en: for the polynomial predictive machine learning model from the machine learning
    workflow example above, the model parameters are the polynomial coefficients,
    e.g., \(b_3\), \(b_2\), \(b_1\) and \(c\) (often called \(b_0\)) for the third
    order polynomial model.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºä¸Šè¿°æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ç¤ºä¾‹ä¸­çš„å¤šé¡¹å¼é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ˜¯å¤šé¡¹å¼ç³»æ•°ï¼Œä¾‹å¦‚ï¼Œå¯¹äºä¸‰æ¬¡å¤šé¡¹å¼æ¨¡å‹ï¼Œ\(b_3\)ã€\(b_2\)ã€\(b_1\)
    å’Œ \(c\)ï¼ˆé€šå¸¸ç§°ä¸º \(b_0\)ï¼‰ã€‚
- en: '![](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85cce1589249c12a11ff52ea10e6c893.png)'
- en: Model parameters are adjusted to fit of the model to the data, i.e., model parameters
    are trained to minimize error over the training data (x markers).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°è¢«è°ƒæ•´ä»¥æ‹Ÿåˆæ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆï¼Œå³æ¨¡å‹å‚æ•°è¢«è®­ç»ƒä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®ï¼ˆx æ ‡è®°ï¼‰ä¸Šçš„è¯¯å·®ã€‚
- en: '**Model hyperparameters** are very different. They do not constrain the model
    fit to the data directly, instead they constrain the model complexity. The model
    hyperparameters are selected (call tuning) to minimize error at the withheld testing
    data. Going back to our polynomial predictive machine learning example, the choice
    of polynomial order is the model hyperparameter.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹è¶…å‚æ•°**éå¸¸ä¸åŒã€‚å®ƒä»¬å¹¶ä¸ç›´æ¥çº¦æŸæ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆï¼Œè€Œæ˜¯çº¦æŸæ¨¡å‹å¤æ‚åº¦ã€‚æ¨¡å‹è¶…å‚æ•°è¢«é€‰æ‹©ï¼ˆç§°ä¸ºè°ƒæ•´ï¼‰ä»¥æœ€å°åŒ–ä¿ç•™çš„æµ‹è¯•æ•°æ®çš„è¯¯å·®ã€‚å›åˆ°æˆ‘ä»¬å¤šé¡¹å¼é¢„æµ‹æœºå™¨å­¦ä¹ ç¤ºä¾‹ï¼Œå¤šé¡¹å¼é˜¶æ•°çš„é€‰æ‹©æ˜¯æ¨¡å‹è¶…å‚æ•°ã€‚'
- en: '![](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aef7a81bd36cd7f2b25e1574784b4932.png)'
- en: Model hyperparameters are adjusted to change the model complexity / flexibility,
    i.e., model hyperparameters are tuned to minimize error over the withheld testing
    data (solid circles).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¶…å‚æ•°è¢«è°ƒæ•´ä»¥æ”¹å˜æ¨¡å‹å¤æ‚åº¦/çµæ´»æ€§ï¼Œå³æ¨¡å‹è¶…å‚æ•°è¢«è°ƒæ•´ä»¥æœ€å°åŒ–ä¿ç•™æµ‹è¯•æ•°æ®ï¼ˆå®å¿ƒåœ†ï¼‰ä¸Šçš„é”™è¯¯ã€‚
- en: Model parameters vs. model hyperparameters
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°ä¸æ¨¡å‹è¶…å‚æ•°
- en: Model parameters control the model fit and are trained with training data. Model
    hyperparameters control the model complexity and are tuned with testing data.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°æ§åˆ¶æ¨¡å‹æ‹Ÿåˆï¼Œå¹¶ä½¿ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ¨¡å‹è¶…å‚æ•°æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œå¹¶ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œè°ƒæ•´ã€‚
- en: Regression and Classification
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›å½’å’Œåˆ†ç±»
- en: Before we proceed, we need to define regression and classification.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å›å½’å’Œåˆ†ç±»ã€‚
- en: '**Regression** - a predictive machine learning model where the response feature(s)
    is continuous.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›å½’** - ä¸€ä¸ªé¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­å“åº”ç‰¹å¾æ˜¯è¿ç»­çš„ã€‚'
- en: '**Classification** - a predictive machine learning model where the response
    feature(s) is categorical.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†ç±»** - ä¸€ä¸ªé¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­å“åº”ç‰¹å¾æ˜¯åˆ†ç±»çš„ã€‚'
- en: It turns out that for each of these we need to build different models and use
    different methods to score these models.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œå¯¹äºè¿™äº›ä¸­çš„æ¯ä¸€ä¸ªï¼Œæˆ‘ä»¬éƒ½éœ€è¦æ„å»ºä¸åŒçš„æ¨¡å‹å¹¶ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥è¯„ä¼°è¿™äº›æ¨¡å‹ã€‚
- en: for the remainder of this discussion we will focus on regression, but in later
    chapters we introduce classification models as well.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æœ¬è®¨è®ºçš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå›å½’ï¼Œä½†åœ¨åé¢çš„ç« èŠ‚ä¸­æˆ‘ä»¬ä¹Ÿä¼šä»‹ç»åˆ†ç±»æ¨¡å‹ã€‚
- en: Now, to better understand predictive machine learning model tuning, i.e., the
    empirical approach to tune model complexity to minimize testing error, we need
    to understand the sources of testing error.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹è°ƒæ•´ï¼Œå³è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ä»¥æœ€å°åŒ–æµ‹è¯•é”™è¯¯çš„ç»éªŒæ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æµ‹è¯•é”™è¯¯çš„æ¥æºã€‚
- en: the causes of the thing that we are attempting to minimize!
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯•å›¾æœ€å°åŒ–çš„ä¸œè¥¿çš„åŸå› ï¼
- en: Training and Testing Data
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸ¹è®­å’Œæµ‹è¯•æ•°æ®
- en: For clarity, consider this is schematic of the flow of training and testing
    data in the predictive machine learning model parameter training and hyperparameter
    tuning workflow,
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œè€ƒè™‘è¿™æ˜¯é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°è®­ç»ƒå’Œè¶…å‚æ•°è°ƒæ•´å·¥ä½œæµç¨‹ä¸­åŸ¹è®­å’Œæµ‹è¯•æ•°æ®æµåŠ¨çš„ç¤ºæ„å›¾ï¼Œ
- en: '![](../Images/454aec4a3d657f072406100f036592b6.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/454aec4a3d657f072406100f036592b6.png)'
- en: The flow of training and testing data in the predictive machine learning model
    parameter training and hyperparameter tuning workflow.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°è®­ç»ƒå’Œè¶…å‚æ•°è°ƒæ•´å·¥ä½œæµç¨‹ä¸­åŸ¹è®­å’Œæµ‹è¯•æ•°æ®çš„æµåŠ¨ã€‚
- en: '**Training Data**,'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒæ•°æ®**ï¼Œ'
- en: trains model parameters
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: trains the final model for real world use
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒç”¨äºå®é™…åº”ç”¨çš„æœ€ç»ˆæ¨¡å‹
- en: '**Testing Data**,'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•æ•°æ®**ï¼Œ'
- en: withheld from training model parameters to avoid model overfit
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒä¸­ä¿ç•™æ¨¡å‹å‚æ•°ä»¥é¿å…æ¨¡å‹è¿‡æ‹Ÿåˆ
- en: tunes model hyperparameters
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
- en: returned to train the final tuned model for deployment
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿”å›ä»¥è®­ç»ƒæœ€ç»ˆè°ƒæ•´æ¨¡å‹ä»¥éƒ¨ç½²
- en: How Much Data Should be Withheld for Testing?
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åº”è¯¥ä¿ç•™å¤šå°‘æ•°æ®ç”¨äºæµ‹è¯•ï¼Ÿ
- en: The proportion in testing is recommended by various sources from 15% - 30% of
    the total dataset. This is a compromise,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•çš„æ¯”ä¾‹ç”±å„ç§æ¥æºæ¨èï¼Œä»æ€»æ•°æ®é›†çš„15%åˆ°30%ã€‚è¿™æ˜¯ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆï¼Œ
- en: data withheld for testing reduces the data available for training; therefore,
    reduces the accuracy of the model.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿ç•™ç”¨äºæµ‹è¯•çš„æ•°æ®å‡å°‘äº†å¯ç”¨äºè®­ç»ƒçš„æ•°æ®ï¼›å› æ­¤ï¼Œé™ä½äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
- en: data withheld for testing improves the accuracy of the assessment of the model
    performance.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿ç•™ç”¨äºæµ‹è¯•çš„æ•°æ®æé«˜äº†å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚
- en: Various authors have experimented on a variety of training and testing ratios
    and have recommended splits for their applications,
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„ä½œè€…åœ¨å„ç§å„æ ·çš„åŸ¹è®­å’Œæµ‹è¯•æ¯”ä¾‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸ºå…¶åº”ç”¨æ¨èäº†åˆ†å‰²ã€‚
- en: the optimum ratio of training and testing split depends on problem setting
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸ¹è®­å’Œæµ‹è¯•åˆ†å‰²çš„æœ€ä½³æ¯”ä¾‹å–å†³äºé—®é¢˜è®¾ç½®
- en: To determine the proportion of testing data to withheld we could consider the
    difficulty in model parameter training (e.g., the number of model parameters)
    and the difficulty in model hyperparameter tuning (e.g., number of hyperparameters,
    range of response feature outcomes).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®å®šæµ‹è¯•æ•°æ®ä¿ç•™çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ¨¡å‹å‚æ•°è®­ç»ƒçš„éš¾åº¦ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡ï¼‰å’Œæ¨¡å‹è¶…å‚æ•°è°ƒæ•´çš„éš¾åº¦ï¼ˆä¾‹å¦‚ï¼Œè¶…å‚æ•°çš„æ•°é‡ï¼Œå“åº”ç‰¹å¾ç»“æœçš„èŒƒå›´ï¼‰ã€‚
- en: Fair Train and Test Splits
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¬å¹³çš„åŸ¹è®­å’Œæµ‹è¯•æ•°æ®åˆ†å‰²
- en: Dr. Julian Salazar suggests that for spatial prediction problems that random
    train and test data splits may not be fair.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: æœ±åˆ©å®‰Â·è¨æ‹‰æ‰åšå£«å»ºè®®ï¼Œå¯¹äºç©ºé—´é¢„æµ‹é—®é¢˜ï¼ŒéšæœºåŸ¹è®­å’Œæµ‹è¯•æ•°æ®åˆ†å‰²å¯èƒ½å¹¶ä¸å…¬å¹³ã€‚
- en: proposed a [fair train and test split method](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    for spatial prediction models that splits the data based on the difficulty of
    the planned use of the model.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æå‡ºäº†ä¸€ç§[å…¬å¹³è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²æ–¹æ³•](https://www.sciencedirect.com/science/article/pii/S0920410521015023)ï¼Œç”¨äºç©ºé—´é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æ ¹æ®æ¨¡å‹è®¡åˆ’ç”¨é€”çš„éš¾åº¦æ¥åˆ†å‰²æ•°æ®ã€‚
- en: prediction difficulty is related to kriging variance that accounts for spatial
    continuity and distance offset, i.e., the difficulty of the estimate.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹éš¾åº¦ä¸å…‹é‡Œé‡‘æ–¹å·®ç›¸å…³ï¼Œå…‹é‡Œé‡‘æ–¹å·®è€ƒè™‘äº†ç©ºé—´è¿ç»­æ€§å’Œè·ç¦»åç§»ï¼Œå³ä¼°è®¡çš„éš¾åº¦ã€‚
- en: the testing split is iterated to match the distribution of kriging variance
    for planned real world use of the model
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•åˆ†å‰²è¢«è¿­ä»£ä»¥åŒ¹é…æ¨¡å‹è®¡åˆ’å®é™…åº”ç”¨çš„å…‹é‡Œé‡‘æ–¹å·®åˆ†å¸ƒ
- en: To illustrate this concept of prediction difficulty, consider this set of well
    logs with both random assignment of testing data and withholding an entire contiguous
    region of the well log for testing data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜é¢„æµ‹éš¾åº¦è¿™ä¸€æ¦‚å¿µï¼Œè€ƒè™‘ä»¥ä¸‹ä¸€ç»„äº•æ—¥å¿—ï¼Œå…¶ä¸­æ—¢åŒ…å«äº†æµ‹è¯•æ•°æ®çš„éšæœºåˆ†é…ï¼Œåˆä¿ç•™äº†ä¸€ä¸ªè¿ç»­çš„æ•´ä¸ªäº•æ—¥å¿—åŒºåŸŸä½œä¸ºæµ‹è¯•æ•°æ®ã€‚
- en: '**easy prediction problem** - for random assignment, usually training data
    are available very close and very similar to the withheld testing data'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®¹æ˜“é¢„æµ‹çš„é—®é¢˜** - å¯¹äºéšæœºåˆ†é…ï¼Œé€šå¸¸è®­ç»ƒæ•°æ®éå¸¸æ¥è¿‘ä¸”ä¸ä¿ç•™çš„æµ‹è¯•æ•°æ®éå¸¸ç›¸ä¼¼'
- en: '**difficult prediction problem** - for removal of the contiguous region, there
    are no similar nor close training data to the withheld testing data'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éš¾ä»¥é¢„æµ‹çš„é—®é¢˜** - å¯¹äºç§»é™¤è¿ç»­åŒºåŸŸï¼Œæ²¡æœ‰ä¸ä¿ç•™çš„æµ‹è¯•æ•°æ®ç›¸ä¼¼æˆ–æ¥è¿‘çš„è®­ç»ƒæ•°æ®'
- en: '![](../Images/5aa194417f3fa3435f02717796814e62.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/5aa194417f3fa3435f02717796814e62.png)'
- en: Two cases for train and test data split, random (left) and by-region (right).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²çš„ä¸¤ç§æƒ…å†µï¼Œéšæœºï¼ˆå·¦ï¼‰å’ŒæŒ‰åŒºåŸŸï¼ˆå³ï¼‰ã€‚
- en: Consider the following prediction cases, i.e., planned real world use of the
    models, and some practical suggestions for fair train and test split.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹é¢„æµ‹æ¡ˆä¾‹ï¼Œå³æ¨¡å‹çš„è®¡åˆ’å®é™…åº”ç”¨ï¼Œä»¥åŠä¸€äº›å…³äºå…¬å¹³è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²çš„å®é™…å»ºè®®ã€‚
- en: If the model will be used to impute data with small offsets from available data
    then construct a train and test split with train data close to test data - random
    assignment of withheld testing data is likely sufficient.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹å°†ç”¨äºä½¿ç”¨å¯ç”¨æ•°æ®ä¸­çš„å°åç§»æ¥æ’è¡¥æ•°æ®ï¼Œåˆ™æ„å»ºä¸€ä¸ªè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®åˆ†å‰²ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ®é è¿‘æµ‹è¯•æ•°æ® - éšæœºåˆ†é…ä¿ç•™çš„æµ‹è¯•æ•°æ®å¯èƒ½æ˜¯è¶³å¤Ÿçš„ã€‚
- en: if the model will be used to predict a large distance offsets then perform splits
    the result is large offsets between train and test data - withhold entire wells,
    drill holes or spatial regions.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹å°†ç”¨äºé¢„æµ‹å¤§è·ç¦»åç§»ï¼Œåˆ™è¿›è¡Œåˆ†å‰²ï¼Œç»“æœæ˜¯åœ¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¹‹é—´å­˜åœ¨å¤§åç§» - ä¿ç•™æ•´ä¸ªäº•ã€é’»å­”æˆ–ç©ºé—´åŒºåŸŸã€‚
- en: Note, with fair train and test splits the tuned model may vary based on the
    planned use for the model.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨å…¬å¹³çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²ä¸‹ï¼Œè°ƒæ•´åçš„æ¨¡å‹å¯èƒ½æ ¹æ®æ¨¡å‹çš„è®¡åˆ’ç”¨é€”è€Œæœ‰æ‰€ä¸åŒã€‚
- en: Use a simple method like withholding entire wells for a predrill prediction
    model, or use the Dr. Salazar workflow, but donâ€™t ignore this issue and just use
    random selection by default.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé¢„é’»é¢„æµ‹æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä¿ç•™æ•´ä¸ªäº•çš„ç®€å•æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨Salazaråšå£«çš„å·¥ä½œæµç¨‹ï¼Œä½†ä¸è¦å¿½ç•¥è¿™ä¸ªé—®é¢˜ï¼Œè€Œé»˜è®¤ä½¿ç”¨éšæœºé€‰æ‹©ã€‚
- en: admittedly, throughout this e-book for demonstration workflow brevity and clarity
    I have just used random training and testing data assignments.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¿è®¤ï¼Œåœ¨æ•´ä¸ªç”µå­ä¹¦ä¸­ï¼Œä¸ºäº†æ¼”ç¤ºå·¥ä½œæµç¨‹çš„ç®€æ´æ€§å’Œæ¸…æ™°æ€§ï¼Œæˆ‘ä»…ä»…ä½¿ç”¨äº†éšæœºçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†é…ã€‚
- en: Model Metrics
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹æŒ‡æ ‡
- en: Since we have covered the workflows for training and tuning, now we can specify
    the model metrics that are applied for,
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å·²ç»æ¶µç›–äº†è®­ç»ƒå’Œè°ƒæ•´çš„å·¥ä½œæµç¨‹ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥æŒ‡å®šåº”ç”¨è¿™äº›æ¨¡å‹æŒ‡æ ‡çš„æƒ…å†µï¼Œ
- en: training model parameters
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: tuning model hyperparameters
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒæ•´æ¨¡å‹è¶…å‚æ•°
- en: model checking and comparison
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ£€æŸ¥å’Œæ¯”è¾ƒ
- en: Hereâ€™s an flowchart indicating how these metrics fit into the machine learning
    modeling workflow.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªæµç¨‹å›¾ï¼Œè¯´æ˜äº†è¿™äº›æŒ‡æ ‡å¦‚ä½•é€‚åº”æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/d311f90f3b59dd6738b0ce7a12614995.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/d311f90f3b59dd6738b0ce7a12614995.png)'
- en: Various applications for model metrics in machine learning modeling workflows.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æŒ‡æ ‡åœ¨æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥ä½œæµç¨‹ä¸­çš„å„ç§åº”ç”¨ã€‚
- en: Choice of model metric depends primarily on the context of the prediction problem,
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æŒ‡æ ‡çš„é€‰æ‹©ä¸»è¦å–å†³äºé¢„æµ‹é—®é¢˜çš„ä¸Šä¸‹æ–‡ï¼Œ
- en: classification vs. regression
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»ä¸å›å½’
- en: individual estimates vs. entire subsets in space (images) or time (signals)
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å•ä¸ªä¼°è®¡å€¼ä¸ç©ºé—´ï¼ˆå›¾åƒï¼‰æˆ–æ—¶é—´ï¼ˆä¿¡å·ï¼‰ä¸­çš„æ•´ä¸ªå­é›†
- en: estimation vs. uncertainty
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼°è®¡ä¸ä¸ç¡®å®šæ€§
- en: There are additional considerations, for example,
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…¶ä»–è€ƒè™‘å› ç´ ï¼Œä¾‹å¦‚ï¼Œ
- en: \(L^1\) vs \(L^2\) norms with their differences, for example, in robustness
    with outliers, stability of solutions and solution sparsity
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(L^1\) ä¸ \(L^2\) èŒƒæ•°åŠå…¶å·®å¼‚ï¼Œä¾‹å¦‚ï¼Œåœ¨å¼‚å¸¸å€¼é²æ£’æ€§ã€è§£çš„ç¨³å®šæ€§å’Œè§£çš„ç¨€ç–æ€§æ–¹é¢
- en: consistency with model assumptions, for example, \(r^2\) is only valid for linear
    models
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹å‡è®¾çš„ä¸€è‡´æ€§ï¼Œä¾‹å¦‚ï¼Œ\(r^2\) åªå¯¹çº¿æ€§æ¨¡å‹æœ‰æ•ˆ
- en: Letâ€™s review some of the common model metrics for regression models and then
    for classification models.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€äº›å›å½’æ¨¡å‹çš„å¸¸è§æ¨¡å‹åº¦é‡ï¼Œç„¶åæ˜¯åˆ†ç±»æ¨¡å‹ã€‚
- en: Mean Square Error (MSE)
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡æ–¹è¯¯å·® (MSE)
- en: Is sensitive to outliers, but is continuously differentiable, leading to a closed-form
    expression for model training. Since the error is squared the error units are
    squared and this may be less interpretable, for example, MSE of 23,543 \(mD^2\).
    The equation is,
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œä½†è¿ç»­å¯å¾®ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒçš„é—­å¼è¡¨è¾¾å¼ã€‚ç”±äºè¯¯å·®æ˜¯å¹³æ–¹çš„ï¼Œè¯¯å·®å•ä½ä¹Ÿæ˜¯å¹³æ–¹çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´è§£é‡Šæ€§è¾ƒå·®ï¼Œä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®ä¸º 23,543 \(mD^2\)ã€‚è¯¥æ–¹ç¨‹æ˜¯ï¼Œ
- en: \[ \text{Test MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{æµ‹è¯•å‡æ–¹è¯¯å·®} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i
    - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
- en: Mean Absolute Error (MAE)
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡å€¼ç»å¯¹è¯¯å·® (MAE)
- en: Is robust in the presence of outliers, but is not continuously differentiable;
    therefore, there is no closed-form expression for model training and training
    is generally accomplished by iterative optimization. The equation is,
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼‚å¸¸å€¼å­˜åœ¨çš„æƒ…å†µä¸‹ç¨³å¥ï¼Œä½†ä¸å¯è¿ç»­å¾®åˆ†ï¼›å› æ­¤ï¼Œæ²¡æœ‰æ¨¡å‹è®­ç»ƒçš„é—­å¼è¡¨è¾¾å¼ï¼Œè®­ç»ƒé€šå¸¸é€šè¿‡è¿­ä»£ä¼˜åŒ–å®Œæˆã€‚æ–¹ç¨‹æ˜¯ï¼Œ
- en: \[ \text{Test MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    |y_i - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta
    y_i| \]
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{æµ‹è¯•MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |y_i
    - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta
    y_i| \]
- en: Variance Explained
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£é‡Šæ–¹å·®
- en: The proportion of variance of the response feature captured by the model. Assumes
    additivity of variance; therefore, we only use this model metric for linear models.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ•è·çš„å“åº”ç‰¹å¾æ–¹å·®çš„æ¯”ç‡ã€‚å‡è®¾æ–¹å·®å¯åŠ æ€§ï¼›å› æ­¤ï¼Œæˆ‘ä»¬åªå¯¹çº¿æ€§æ¨¡å‹ä½¿ç”¨æ­¤æ¨¡å‹åº¦é‡ã€‚
- en: First we calculate the variance explained by the model, simply as the variance
    of the model predictions,
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—æ¨¡å‹è§£é‡Šçš„æ–¹å·®ï¼Œç®€å•åœ°è¯´å°±æ˜¯æ¨¡å‹é¢„æµ‹çš„æ–¹å·®ï¼Œ
- en: \[ \sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    ( \hat{y}_i - \bar{y} )^2 \]
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    ( \hat{y}_i - \bar{y} )^2 \]
- en: then we calculate the variance not explained by the model as the variance of
    the error over the model predictions,
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—æ¨¡å‹æœªè§£é‡Šçš„æ–¹å·®ä½œä¸ºè¯¯å·®æ–¹å·®çš„æ–¹å·®ï¼Œ
- en: \[ \sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 \]
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 \]
- en: then under the assumption of additivity of variance, we calculate the ratio
    of variance explained over all variance, variance explained plus variance not
    explained,
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æ–¹å·®å¯åŠ æ€§çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—è§£é‡Šæ–¹å·®ä¸æ€»æ–¹å·®çš„æ¯”ç‡ï¼Œè§£é‡Šæ–¹å·®åŠ ä¸Šæœªè§£é‡Šçš„æ–¹å·®ï¼Œ
- en: \[ r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not
    explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2} \]
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \[ r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not
    explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2} \]
- en: For linear regression, recall \(r^2 = \left( \rho_(X,y) \right)^2\); therefore,
    like correlation coefficients, \(r^2\),
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçº¿æ€§å›å½’ï¼Œå›å¿† \(r^2 = \left( \rho_(X,y) \right)^2\)ï¼›å› æ­¤ï¼Œåƒç›¸å…³ç³»æ•°ä¸€æ ·ï¼Œ\(r^2\)ï¼Œ
- en: has similar issues as correlation with respect to outliers and mixing multiple
    populations, e.g., Simpsonâ€™s Paradox
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ç›¸å…³ç³»æ•°åœ¨å¼‚å¸¸å€¼å’Œæ··åˆå¤šä¸ªæ€»ä½“æ–¹é¢æœ‰ç±»ä¼¼é—®é¢˜ï¼Œä¾‹å¦‚ï¼Œè¾›æ™®æ£®æ‚–è®º
- en: for nonlinear models consider pseudo-R-square methods
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºéçº¿æ€§æ¨¡å‹ï¼Œè€ƒè™‘ä¼ªå†³å®šç³»æ•°æ–¹æ³•
- en: Also, even a linear model can have a negative \(r^2\) if the model trend contradicts
    the data trend, for example, if you fit data with a negative slope with a linear
    model with a positive slope!
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå³ä½¿çº¿æ€§æ¨¡å‹ä¹Ÿå¯èƒ½æœ‰è´Ÿçš„ \(r^2\)ï¼Œå¦‚æœæ¨¡å‹è¶‹åŠ¿ä¸æ•°æ®è¶‹åŠ¿ç›¸çŸ›ç›¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ ç”¨æ­£æ–œç‡çš„çº¿æ€§æ¨¡å‹æ‹Ÿåˆè´Ÿæ–œç‡çš„æ•°æ®ï¼
- en: Inlier Ratio
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…ç‚¹æ¯”ç‡
- en: The proportion of testing data, \(y_i\) within a margin, \(\epsilon\), of the
    model, \(\hat{y}_i\), calculated as,
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ä¸ºæ¨¡å‹ \(\hat{y}_i\) åœ¨ä¸€ä¸ªè¾¹é™… \(\epsilon\) å†…çš„æµ‹è¯•æ•°æ® \(y_i\) çš„æ¯”ä¾‹ï¼Œ
- en: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
- en: where the indicator transform, \(I_R\) is defined as,
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼ŒæŒ‡ç¤ºå˜æ¢ \(I_R\) å®šä¹‰ä¸ºï¼Œ
- en: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
- en: Hereâ€™s an illustration of the inlier ratio model metric, \(I_R\) model metric,
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹å†…ç‚¹æ¯”ä¾‹æ¨¡å‹æŒ‡æ ‡ \(I_R\) çš„ä¸€ä¸ªè¯´æ˜ï¼Œ
- en: '![](../Images/3ddaa66ff3998a59b318297be09c60b8.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ddaa66ff3998a59b318297be09c60b8.png)'
- en: Testing data, model with margin, \(\epsilon\), and outliers (white) and inliers
    (red) identified, 16 inliers out of 25 data samples, \(ğ¼ğ‘… = 0.64\).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ•°æ®ï¼Œæ¨¡å‹å¸¦æœ‰è¾¹ç•Œï¼Œ\(\epsilon\)ï¼Œä»¥åŠè¯†åˆ«å‡ºçš„å¼‚å¸¸å€¼ï¼ˆç™½è‰²ï¼‰å’Œå†…ç‚¹ï¼ˆçº¢è‰²ï¼‰ï¼Œ25 ä¸ªæ•°æ®æ ·æœ¬ä¸­æœ‰ 16 ä¸ªå†…ç‚¹ï¼Œ\(ğ¼ğ‘… = 0.64\)ã€‚
- en: While the illustration is a linear model, this metric may be applied to any
    model. Although there is some subjectivity with the inlier ratio model metric,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸ªå›¾ç¤ºæ˜¯çº¿æ€§æ¨¡å‹ï¼Œä½†è¿™ä¸ªæŒ‡æ ‡å¯ä»¥åº”ç”¨äºä»»ä½•æ¨¡å‹ã€‚å°½ç®¡å†…ç‚¹æ¯”ä¾‹æ¨¡å‹æŒ‡æ ‡å­˜åœ¨ä¸€äº›ä¸»è§‚æ€§ï¼Œ
- en: what is the best selection for the margin, \(\epsilon\)?
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æœ€ä¼˜çš„è¾¹ç•Œé€‰æ‹©ï¼Œ\(\epsilon\)ï¼Ÿ
- en: Common Classification Model Metrics
  id: totrans-488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§åˆ†ç±»æ¨¡å‹æŒ‡æ ‡
- en: Letâ€™s review some of the common model metrics for classification models. Classification
    is potentially more complicated than regression, since instead of a single model
    metric, we actually calculate an entire confusion matrix,
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€äº›å¸¸è§çš„åˆ†ç±»æ¨¡å‹æŒ‡æ ‡ã€‚åˆ†ç±»å¯èƒ½æ¯”å›å½’æ›´å¤æ‚ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šè®¡ç®—çš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ··æ·†çŸ©é˜µï¼Œ
- en: a \(K \times K\) matrix with frequencies of predicted (x axis) vs. actual (y
    axis) categories to visualize the performance of a classification model, where
    \(K\) is the response feature cardinality, i.e., the number of possible categories
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª \(K \times K\) çš„çŸ©é˜µï¼Œè¡¨ç¤ºé¢„æµ‹ï¼ˆx è½´ï¼‰ä¸å®é™…ï¼ˆy è½´ï¼‰ç±»åˆ«é¢‘ç‡ï¼Œç”¨äºå¯è§†åŒ–åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶ä¸­ \(K\) æ˜¯å“åº”ç‰¹å¾çš„åŸºæ•°ï¼Œå³å¯èƒ½ç±»åˆ«çš„æ•°é‡
- en: visualize and diagnose all the combinations of correct and misclassification
    with the classification model, for example, category 1 is often misclassified
    as category 3,
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†ç±»æ¨¡å‹å¯è§†åŒ–å¹¶è¯Šæ–­æ‰€æœ‰æ­£ç¡®å’Œé”™è¯¯åˆ†ç±»çš„ç»„åˆï¼Œä¾‹å¦‚ï¼Œç±»åˆ« 1 å¸¸å¸¸è¢«é”™è¯¯åœ°åˆ†ç±»ä¸ºç±»åˆ« 3ï¼Œ
- en: '![](../Images/0c2a856f882f5bcc7e9df675ea5afb21.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c2a856f882f5bcc7e9df675ea5afb21.png)'
- en: Example confusion matrix for a classification model, 2D matrix with the frequencies
    of all cases of truth and predicted categories.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»æ¨¡å‹çš„ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œè¡¨ç¤ºæ‰€æœ‰çœŸå®å’Œé¢„æµ‹ç±»åˆ«çš„é¢‘ç‡ã€‚
- en: perfect accuracy is number of each class, \(n_1, n_2, \ldots, n_K\) on the diagonal,
    i.e., category 1 is always predicted as category 1, etc.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®Œç¾å‡†ç¡®åº¦æ˜¯æ¯ä¸ªç±»åˆ«çš„æ•°é‡ï¼Œ\(n_1, n_2, \ldots, n_K\) åœ¨å¯¹è§’çº¿ä¸Šï¼Œå³ç±»åˆ« 1 æ€»æ˜¯é¢„æµ‹ä¸ºç±»åˆ« 1ï¼Œç­‰ç­‰ã€‚
- en: '![](../Images/34df964c246951e764fc3a7e8fecbf96.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34df964c246951e764fc3a7e8fecbf96.png)'
- en: Example confusion matrix for perfectly accurate classification model.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œç¾å‡†ç¡®åº¦çš„ç¤ºä¾‹æ··æ·†çŸ©é˜µã€‚
- en: the confusion matrix is applied to calculate a single summary of categorical
    accuracy, for example, precision, recall, etc.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ··æ·†çŸ©é˜µåº”ç”¨äºè®¡ç®—å•ä¸ªåˆ†ç±»å‡†ç¡®åº¦çš„æ‘˜è¦ï¼Œä¾‹å¦‚ï¼Œç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰ã€‚
- en: model metrics are specific to the specific category and may significantly vary
    over categories, i.e., we can predict well for category \(k=1\) but not for category
    \(k=3\).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æŒ‡æ ‡ç‰¹å®šäºç‰¹å®šç±»åˆ«ï¼Œå¹¶ä¸”å¯èƒ½åœ¨ç±»åˆ«ä¹‹é—´æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå³æˆ‘ä»¬å¯èƒ½å¯¹ç±»åˆ« \(k=1\) é¢„æµ‹å¾—å¾ˆå¥½ï¼Œä½†å¯¹ç±»åˆ« \(k=3\) åˆ™ä¸ç„¶ã€‚
- en: Precision
  id: totrans-499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç²¾ç¡®åº¦
- en: For category \(ğ‘˜\), precision is the ratio of true positive over all positives,
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç±»åˆ« \(ğ‘˜\)ï¼Œç²¾ç¡®åº¦æ˜¯æ‰€æœ‰æ­£ä¾‹ä¸­çœŸæ­£ä¾‹çš„æ¯”ä¾‹ï¼Œ
- en: \[ \text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true
    positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all
    positives}} \]
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true
    positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all
    positives}} \]
- en: we can intuitively describe precision as the conditional probability,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°ç²¾ç¡®åº¦ä¸ºæ¡ä»¶æ¦‚ç‡ï¼Œ
- en: \[ \text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says
    } k \text{ is happening}\right) \]![](../Images/020824af3f57f97af1ee2ba6ca6de684.png)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says
    } k \text{ is happening}\right) \]![](../Images/020824af3f57f97af1ee2ba6ca6de684.png)
- en: Example confusion matrix with illustration of the precision model metric for
    each category, \(k\).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„ç²¾ç¡®åº¦æ¨¡å‹æŒ‡æ ‡ã€‚
- en: For this example, we can calculate the precision for each category as,
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®åº¦ï¼Œ
- en: Category k=1
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k=1
- en: \[ \text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 \]
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 \]
- en: Category k = 2,
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 2,
- en: \[ \text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42
    \]
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42
    \]
- en: Category k = 3,
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« k = 3,
- en: \[ \text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 \]
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 \]
- en: Recall (called sensitivity in medical)
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¬å›ç‡ï¼ˆåœ¨åŒ»å­¦ä¸­ç§°ä¸ºæ•æ„Ÿæ€§ï¼‰
- en: Recall for group \(ğ‘˜\) is the ratio of true positives over all cases of \(ğ‘˜\).
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»„ \(ğ‘˜\) çš„å¬å›ç‡æ˜¯çœŸæ­£ä¾‹ä¸ \(ğ‘˜\) çš„æ‰€æœ‰æ¡ˆä¾‹çš„æ¯”ä¾‹ã€‚
- en: \[ \text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}}
    + n_{k \text{ false negative}}} \]
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}}
    + n_{k \text{ false negative}}} \]
- en: We can intuitively describe recall as, how many of group ğ‘˜ did we catch?
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°å¬å›ç‡ï¼Œå³æˆ‘ä»¬æ•è·äº†ç»„ \(k\) ä¸­çš„å¤šå°‘ä¸ªï¼Ÿ
- en: Note, recall does not account for false positives.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¬å›ç‡ä¸è€ƒè™‘å‡é˜³æ€§ã€‚
- en: '![](../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png)'
- en: Example confusion matrix with illustration of the recall model metric for each
    category, \(k\).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„å¬å›æ¨¡å‹æŒ‡æ ‡ï¼Œ
- en: For this example, we can calculate the recall for each category as,
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¬å›ç‡å¦‚ä¸‹ï¼Œ
- en: Category k=1
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k=1\)
- en: \[ \text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 \]
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 \]
- en: Category k = 2,
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k = 2\)ï¼Œ
- en: \[ \text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 \]
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 \]
- en: Category k = 3,
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k = 3\)ï¼Œ
- en: \[ \text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 \]
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 \]
- en: Specificity
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰¹å¼‚æ€§
- en: Specificity for group \(ğ‘˜\) is the ratio of true negatives over all negative
    cases of \(n \ne ğ‘˜\).
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ \(ğ‘˜\) çš„ç‰¹å¼‚æ€§æ˜¯çœŸå®è´Ÿä¾‹ä¸æ‰€æœ‰é \(k\) è´Ÿä¾‹çš„æ¯”ç‡ã€‚
- en: \[ \text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k
    \text{ true negative}} + n_{k \text{ false positive}}} \]
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k
    \text{ true negative}} + n_{k \text{ false positive}}} \]
- en: We can intuitively describe specificity as, how many of not group \(k\) did
    we catch?
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æè¿°ç‰¹å¼‚æ€§ï¼Œå³æˆ‘ä»¬æ•è·äº†éç»„ \(k\) ä¸­çš„å¤šå°‘ä¸ªï¼Ÿ
- en: Note, recall does not account for true positives.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¬å›ç‡ä¸è€ƒè™‘çœŸé˜³æ€§ã€‚
- en: '![](../Images/22a61637f89b9d1f446385497e2e9b65.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22a61637f89b9d1f446385497e2e9b65.png)'
- en: Example confusion matrix with illustration of the recall model metric for each
    category, \(k\).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ··æ·†çŸ©é˜µï¼Œå±•ç¤ºäº†æ¯ä¸ªç±»åˆ« \(k\) çš„å¬å›æ¨¡å‹æŒ‡æ ‡ï¼Œ
- en: For this example, we can calculate the recall for each category as,
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¬å›ç‡å¦‚ä¸‹ï¼Œ
- en: Category k=1
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k=1\)
- en: \[ \text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5
    + 7)} = \frac{50}{62} = 0.81 \]
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5
    + 7)} = \frac{50}{62} = 0.81 \]
- en: Category k = 2,
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k = 2\)ï¼Œ
- en: \[ \text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 +
    15)} = \frac{28}{58} = 0.48 \]
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 +
    15)} = \frac{28}{58} = 0.48 \]
- en: Category k = 3,
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ« \(k = 3\)ï¼Œ
- en: \[ \text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2
    + 9)} = \frac{57}{68} = 0.84 \]
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2
    + 9)} = \frac{57}{68} = 0.84 \]
- en: f1-score
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: f1-score
- en: f1-score is the harmonic mean of precision and recall for each \(k\) category,
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: f1-score æ˜¯æ¯ä¸ª \(k\) ç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ï¼Œ
- en: \[ \text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}}
    \]
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}}
    \]
- en: The idea is to combine precision and recall into a single metric since they
    both see different aspects of the classification model accuracy.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ç†å¿µæ˜¯å°†ç²¾ç¡®ç‡å’Œå¬å›ç‡åˆå¹¶æˆä¸€ä¸ªå•ä¸€æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒä»¬éƒ½çœ‹åˆ°äº†åˆ†ç±»æ¨¡å‹å‡†ç¡®æ€§çš„ä¸åŒæ–¹é¢ã€‚
- en: the harmonic mean is sensitive the to lowest score; therefore, good performance
    in one score cannot average out or make up for bad performance in the other
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è°ƒå’Œå¹³å‡å€¼å¯¹æœ€ä½åˆ†æ•°æ•æ„Ÿï¼›å› æ­¤ï¼Œä¸€ä¸ªåˆ†æ•°çš„è‰¯å¥½è¡¨ç°ä¸èƒ½å¹³å‡æˆ–å¼¥è¡¥å¦ä¸€ä¸ªåˆ†æ•°çš„å·®åŠ£è¡¨ç°
- en: Train and Test Hold Out Cross Validation
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•ç•™å‡ºäº¤å‰éªŒè¯
- en: If only one train and test data split is applied to tune our machine learning
    model hyperparameters then we are applying the hold out cross validation approach.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåªåº”ç”¨ä¸€ä¸ªè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²æ¥è°ƒæ•´æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¶…å‚æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ­£åœ¨åº”ç”¨ç•™å‡ºäº¤å‰éªŒè¯æ–¹æ³•ã€‚
- en: we split the data into training and testing data, these are exhaustive and mutually
    exclusive groups.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼Œè¿™äº›æ˜¯è¯¦å°½ä¸”äº’æ–¥çš„ç»„ã€‚
- en: but this cross validation method is not exahustive, we only consider the one
    split for testing, most data are not tested. Also, we do not explore the full
    combinatorial of possible splits (more about this as we compare with other cross
    validation methods)
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†è¿™ç§äº¤å‰éªŒè¯æ–¹æ³•å¹¶ä¸å…¨é¢ï¼Œæˆ‘ä»¬åªè€ƒè™‘ä¸€ä¸ªç”¨äºæµ‹è¯•çš„åˆ†å‰²ï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½æ²¡æœ‰è¢«æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰æ¢ç´¢æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç»„åˆï¼ˆæ›´å¤šå…³äºè¿™ä¸€ç‚¹ï¼Œå½“æˆ‘ä»¬ä¸å…¶ä»–äº¤å‰éªŒè¯æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ—¶ï¼‰
- en: '![](../Images/ada2e0334323368c40690e596e78d464.png)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ada2e0334323368c40690e596e78d464.png)'
- en: The train and test data hold out cross validation.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®é‡‡ç”¨ç•™å‡ºäº¤å‰éªŒè¯ã€‚
- en: The workflow is,
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹æ˜¯ï¼Œ
- en: withhold the testing data subset from model training
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹è®­ç»ƒä¸­ä¿ç•™æµ‹è¯•æ•°æ®å­é›†
- en: train models with the remaining training data with various hyperparameters representing
    simple to complicated models
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‰©ä½™çš„è®­ç»ƒæ•°æ®ä»¥åŠä»£è¡¨ç®€å•åˆ°å¤æ‚æ¨¡å‹çš„å¤šç§è¶…å‚æ•°æ¥è®­ç»ƒæ¨¡å‹
- en: then test the suite of simple to complicated trained models with withheld testing
    data
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨ä¿ç•™çš„æµ‹è¯•æ•°æ®æµ‹è¯•ä»ç®€å•åˆ°å¤æ‚çš„è®­ç»ƒæ¨¡å‹å¥—ä»¶
- en: select the model hyperparameters (complexity) with lowest testing error
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©å…·æœ‰æœ€ä½æµ‹è¯•é”™è¯¯çš„æ¨¡å‹è¶…å‚æ•°ï¼ˆå¤æ‚åº¦ï¼‰
- en: retrain the model with the turned hyperparameters and all of the data for deployment
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è°ƒæ•´åçš„è¶…å‚æ•°å’Œæ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²
- en: The advantage of this approach is that we can readily evaluate the training
    and testing data split.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ‹†åˆ†ã€‚
- en: since there is only one split, we can easily visualize and evaluate the train
    and test data cases, coverage and balance
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºåªæœ‰ä¸€ä¸ªæ‹†åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å¯è§†åŒ–å’Œè¯„ä¼°è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ¡ˆä¾‹ã€è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: The disadvantage is that this method may be sensitive to the specific selection
    of testing data
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯å®ƒå¯èƒ½å¯¹æµ‹è¯•æ•°æ®çš„ç‰¹å®šé€‰æ‹©æ•æ„Ÿ
- en: as a result hold out cross validation may result in a noisy plot of testing
    error vs. the hyperparameter
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¿ç•™æ ·æœ¬äº¤å‰éªŒè¯å¯èƒ½ä¼šå¯¼è‡´æµ‹è¯•é”™è¯¯ä¸è¶…å‚æ•°çš„å™ªå£°å›¾
- en: Train, Validate and Test Hold Out Cross Validation
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¿ç•™æ ·æœ¬äº¤å‰éªŒè¯
- en: There is a more complete hold out cross validation workflow commonly applied,
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸ç”¨çš„æ›´å®Œæ•´çš„ä¿ç•™æ ·æœ¬äº¤å‰éªŒè¯å·¥ä½œæµç¨‹ï¼Œ
- en: '**Train with training data split** - models sees and learns from this data
    to train the model parameters.'
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨è®­ç»ƒæ•°æ®æ‹†åˆ†è¿›è¡Œè®­ç»ƒ** - æ¨¡å‹é€šè¿‡æ­¤æ•°æ®å­¦ä¹ å’Œè®­ç»ƒæ¨¡å‹å‚æ•°ã€‚'
- en: '**Validate with the validation data split** - evaluation of model complexity
    vs. accuracy with data withheld from model parameter training to tune the model
    hyperparameters. The same as testing data in train and test workflow.'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨éªŒè¯æ•°æ®æ‹†åˆ†è¿›è¡ŒéªŒè¯** - è¯„ä¼°æ¨¡å‹å¤æ‚åº¦ä¸å‡†ç¡®åº¦ï¼Œæ•°æ®æœªç”¨äºæ¨¡å‹å‚æ•°è®­ç»ƒä»¥è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ã€‚ä¸è®­ç»ƒå’Œæµ‹è¯•å·¥ä½œæµç¨‹ä¸­çš„æµ‹è¯•æ•°æ®ç›¸åŒã€‚'
- en: '**Test model performance with testing data** - data withheld until the model
    is complete to provide a final evaluation of model performance. This data had
    no role in building the model and is commonly applied to compare multiple competing
    models.'
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨æµ‹è¯•æ•°æ®æµ‹è¯•æ¨¡å‹æ€§èƒ½** - æ•°æ®ä¿ç•™ç›´åˆ°æ¨¡å‹å®Œæˆï¼Œä»¥æä¾›æ¨¡å‹æ€§èƒ½çš„æœ€ç»ˆè¯„ä¼°ã€‚è¿™äº›æ•°æ®åœ¨æ„å»ºæ¨¡å‹æ—¶æ²¡æœ‰å‘æŒ¥ä½œç”¨ï¼Œé€šå¸¸ç”¨äºæ¯”è¾ƒå¤šä¸ªç«äº‰æ¨¡å‹ã€‚'
- en: '![](../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png)'
- en: The train, validate and test hold out cross validation.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¿ç•™æ ·æœ¬äº¤å‰éªŒè¯ã€‚
- en: I understand the motivation for the training, validation and testing cross validation
    workflow. It is an attempt to check our models, objectively, with cases that,
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç†è§£è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•äº¤å‰éªŒè¯å·¥ä½œæµç¨‹çš„åŠ¨æœºã€‚è¿™æ˜¯å°è¯•ç”¨å®¢è§‚çš„æ–¹å¼æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™äº›æ¡ˆä¾‹ï¼Œ
- en: we know the truth and can access accuracy accurately
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“çœŸç›¸å¹¶èƒ½å‡†ç¡®è®¿é—®å‡†ç¡®æ€§
- en: had nothing to do with the model construction, training model parameters nor
    tuning model hyperparameters
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸æ¨¡å‹æ„å»ºã€è®­ç»ƒæ¨¡å‹å‚æ•°æˆ–è°ƒæ•´æ¨¡å‹è¶…å‚æ•°æ— å…³
- en: I appreciate this, but I have some concerns,
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹æ­¤è¡¨ç¤ºèµèµï¼Œä½†æˆ‘æœ‰ä¸€äº›é¡¾è™‘ï¼Œ
- en: We are further reducing the number of samples available to training model parameters
    and to tuning model hyperparameters.
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨è¿›ä¸€æ­¥å‡å°‘å¯ç”¨äºè®­ç»ƒæ¨¡å‹å‚æ•°å’Œè°ƒæ•´æ¨¡å‹è¶…å‚æ•°çš„æ ·æœ¬æ•°é‡ã€‚
- en: Eventually we will retrain the tuned model with all the data, so the model we
    test is not actually the final deployed model.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒè°ƒæ•´åçš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬æµ‹è¯•çš„æ¨¡å‹å®é™…ä¸Šå¹¶ä¸æ˜¯æœ€ç»ˆéƒ¨ç½²çš„æ¨¡å‹ã€‚
- en: What do we do if the testing data is not accurately predicted? Do we include
    another round of testing with another withheld subset of the data? Ad infinitum?
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœæµ‹è¯•æ•°æ®é¢„æµ‹ä¸å‡†ç¡®ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿæˆ‘ä»¬æ˜¯å¦åŒ…æ‹¬å¦ä¸€è½®æµ‹è¯•ï¼Œä½¿ç”¨æ•°æ®çš„ä¸€ä¸ªå¦ä¸€ä¸ªä¿ç•™å­é›†ï¼Ÿæ— ç©·æ— å°½ï¼Ÿ
- en: Load the Required Libraries
  id: totrans-575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ‰€éœ€çš„åº“
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç åŠ è½½æ‰€éœ€çš„åº“ã€‚è¿™äº›åº“åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: '[PRE7]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Declare Functions
  id: totrans-578
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: I also added a convenience function to add major and minor gridlines to improve
    plot interpretability.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜æ·»åŠ äº†ä¸€ä¸ªæ–¹ä¾¿çš„å‡½æ•°æ¥æ·»åŠ ä¸»ç½‘æ ¼çº¿å’Œå‰¯ç½‘æ ¼çº¿ï¼Œä»¥æé«˜ç»˜å›¾çš„å¯è§£é‡Šæ€§ã€‚
- en: '[PRE8]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Load Data to Demonstration Cross Validation Methods
  id: totrans-581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ•°æ®åˆ°æ¼”ç¤ºäº¤å‰éªŒè¯æ–¹æ³•
- en: Letâ€™s load a spatial dataset and select 2 predictor features to visualize cross
    validation methods.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªç©ºé—´æ•°æ®é›†å¹¶é€‰æ‹©2ä¸ªé¢„æµ‹ç‰¹å¾æ¥å¯è§†åŒ–äº¤å‰éªŒè¯æ–¹æ³•ã€‚
- en: we will focus on the data splits and not the actual model training and tuning.
    Later when we cover predictive machine learning methods we will add the model
    component of the workflow.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…³æ³¨æ•°æ®æ‹†åˆ†ï¼Œè€Œä¸æ˜¯å®é™…çš„æ¨¡å‹è®­ç»ƒå’Œè°ƒæ•´ã€‚ç¨åå½“æˆ‘ä»¬ä»‹ç»é¢„æµ‹æœºå™¨å­¦ä¹ æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬å°†æ·»åŠ å·¥ä½œæµç¨‹ä¸­çš„æ¨¡å‹ç»„ä»¶ã€‚
- en: '[PRE9]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Visualize Train and Test Hold Out Cross Validation
  id: totrans-586
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•ä¿ç•™äº¤å‰éªŒè¯
- en: Letâ€™s compare the train and test with train, validate and test hold out data
    splits.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•ä¸è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ä¿ç•™æ•°æ®åˆ†å‰²ã€‚
- en: first we plot a train and test data split and then a train, validate and test
    split.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²ï¼Œç„¶åæ˜¯è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åˆ†å‰²ã€‚
- en: '[PRE11]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png](../Images/a54bde3cc01f9083714572540a961fb0.png)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/a54bde3cc01f9083714572540a961fb0.png)'
- en: It is a good idea to visualize the train and test split,
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–è®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œ
- en: histograms for each predictor feature and the response feature to ensure that
    the train and test cover the range of possible outcomes and are balanced
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºèŒƒå›´å¹¶ä¿æŒå¹³è¡¡ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾ç»˜åˆ¶ç›´æ–¹å›¾
- en: if the number of predictor features is 2 then we can actually plot the predictor
    feature space to check coverage and balance of train and test data splits
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé¢„æµ‹ç‰¹å¾çš„æ•°é‡æ˜¯ 2ï¼Œé‚£ä¹ˆæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ç»˜åˆ¶é¢„æµ‹ç‰¹å¾ç©ºé—´æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²çš„è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: Now letâ€™s repeat this for the train, validate and test data split.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é‡å¤è¿™ä¸ªæ­¥éª¤ï¼Œé’ˆå¯¹è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åˆ†å‰²ã€‚
- en: '[PRE12]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png](../Images/8537d6caff0c1936ede31daca961765a.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/8537d6caff0c1936ede31daca961765a.png)'
- en: Once again we can visualize the splits, now train, validate and test,
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: å†æ¬¡å¯è§†åŒ–åˆ†å‰²ï¼Œç°åœ¨åŒ…æ‹¬è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ï¼Œ
- en: histograms for each predictor feature and the response feature to ensure that
    the train and test cover the range of possible outcomes and are balanced
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºèŒƒå›´å¹¶ä¿æŒå¹³è¡¡ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹ç‰¹å¾å’Œå“åº”ç‰¹å¾ç»˜åˆ¶ç›´æ–¹å›¾
- en: if the number of predictor features is 2 then we can actually plot the predictor
    feature space to check coverage and balance of train and test data splits
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœé¢„æµ‹ç‰¹å¾çš„æ•°é‡æ˜¯ 2ï¼Œé‚£ä¹ˆæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥ç»˜åˆ¶é¢„æµ‹ç‰¹å¾ç©ºé—´æ¥æ£€æŸ¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å‰²çš„è¦†ç›–ç‡å’Œå¹³è¡¡æ€§
- en: Leave-one-out Cross Validation (LOO CV)
  id: totrans-600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOO CV)
- en: Leave-one-out cross validation is an exhaustive cross validation method, i.e.,
    all data gets tested by loop over all the data.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯æ˜¯ä¸€ç§ç©·ä¸¾äº¤å‰éªŒè¯æ–¹æ³•ï¼Œå³é€šè¿‡éå†æ‰€æœ‰æ•°æ®æ¥æµ‹è¯•æ‰€æœ‰æ•°æ®ã€‚
- en: we train and tune \(n\) models, for each model a single datum is withheld as
    testing and the \(n-1\) data are assigned as training data
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®­ç»ƒå’Œè°ƒæ•´ \(n\) ä¸ªæ¨¡å‹ï¼Œå¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªæ•°æ®ç‚¹è¢«ä¿ç•™ä½œä¸ºæµ‹è¯•ï¼Œè€Œ \(n-1\) ä¸ªæ•°æ®ç‚¹è¢«åˆ†é…ä¸ºè®­ç»ƒæ•°æ®
- en: we will calculate \(n\) training and testing errors that will be aggregated
    over all \(n\) models, for example, the average of the mean square error.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®¡ç®— \(n\) ä¸ªè®­ç»ƒå’Œæµ‹è¯•é”™è¯¯ï¼Œè¿™äº›é”™è¯¯å°†æ±‡æ€»åˆ°æ‰€æœ‰ \(n\) ä¸ªæ¨¡å‹ä¸­ï¼Œä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®çš„å¹³å‡å€¼ã€‚
- en: In the case of leave-one-out cross validation,
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç•™ä¸€æ³•äº¤å‰éªŒè¯çš„æƒ…å†µä¸‹ï¼Œ
- en: we test at only one datum so the test error is just a single error at the single
    withheld datum, so we just use standard MSE over the \(n\) models
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªåœ¨å•ä¸ªæ•°æ®ç‚¹ä¸Šæµ‹è¯•ï¼Œå› æ­¤æµ‹è¯•è¯¯å·®åªæ˜¯å•ä¸ªä¿ç•™æ•°æ®ç‚¹çš„å•ä¸ªè¯¯å·®ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€åœ¨ \(n\) ä¸ªæ¨¡å‹ä¸Šä½¿ç”¨æ ‡å‡† MSE
- en: \[ \text{Test MSE Aggregate} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}}
    (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta
    y_i)^2 \]
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{æµ‹è¯• MSE æ€»å’Œ} = \frac{1}{n_{\text{æµ‹è¯•}}} \sum_{i=1}^{n_{\text{æµ‹è¯•}}} (y_i
    - \hat{y}_i)^2 = \frac{1}{n_{\text{æµ‹è¯•}}} \sum_{i=1}^{n_{\text{æµ‹è¯•}}} (\Delta y_i)^2
    \]
- en: but, we have \(n-1\) training data for each model, so we aggregate, by averageing
    the mean square error of each model,
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬æœ‰ \(n-1\) ä¸ªè®­ç»ƒæ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡å¹³å‡æ¯ä¸ªæ¨¡å‹çš„å‡æ–¹è¯¯å·®æ¥æ±‡æ€»ï¼Œ
- en: \[ \text{Train MSE Aggregate} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1}
    (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i \]
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \text{è®­ç»ƒ MSE æ€»å’Œ} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1}
    (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i \]
- en: Hereâ€™s the leave-one-out cross validation steps,
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ç•™ä¸€æ³•äº¤å‰éªŒè¯çš„æ­¥éª¤ï¼Œ
- en: Loop over all \(n\) data, and withhold that data
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éå†æ‰€æœ‰ \(n\) ä¸ªæ•°æ®ï¼Œå¹¶ä¿ç•™è¯¥æ•°æ®
- en: Train on the remaining \(nâˆ’1\) data and test on the withheld single data
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å‰©ä½™çš„ \(nâˆ’1\) ä¸ªæ•°æ®ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨ä¿ç•™çš„å•ä¸ªæ•°æ®ä¸Šæµ‹è¯•
- en: Calculate model goodness metric, MSE for a single test data is the square error
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¨¡å‹è‰¯å¥½åº¦æŒ‡æ ‡ï¼Œå•ä¸ªæµ‹è¯•æ•°æ®çš„å‡æ–¹è¯¯å·®æ˜¯å¹³æ–¹è¯¯å·®
- en: Goto 1
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ° 1
- en: Aggregate model goodness metric over all data, \(n\)
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰æ•°æ®ä¸Šæ±‡æ€»æ¨¡å‹è‰¯å¥½åº¦æŒ‡æ ‡ï¼Œ\(n\)
- en: Typically, leave-one-out cross validation is too easy of a prediction problem;
    therefore, it is not commonly used,
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œç•™ä¸€æ³•äº¤å‰éªŒè¯æ˜¯ä¸€ä¸ªå¤ªç®€å•çš„é¢„æµ‹é—®é¢˜ï¼›å› æ­¤ï¼Œå®ƒä¸å¤ªå¸¸ç”¨ï¼Œ
- en: but it introduces the concept of exhaustive cross validation, i.e., all data
    gets tested!
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½†å®ƒå¼•å…¥äº†ç©·ä¸¾äº¤å‰éªŒè¯çš„æ¦‚å¿µï¼Œå³æ‰€æœ‰æ•°æ®éƒ½å¾—åˆ°æµ‹è¯•ï¼
- en: Leave-one-out cross validation is also exhaustive in the sense that the full
    combinatorial of \(n\) data choose \(p\) where \(p=1\) are explored,
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: ç•™ä¸€æ³•äº¤å‰éªŒè¯åœ¨æŸç§æ„ä¹‰ä¸Šä¹Ÿæ˜¯å…¨é¢çš„ï¼Œå› ä¸ºå®ƒæ¢ç´¢äº† \(n\) ä¸ªæ•°æ®ä¸­é€‰æ‹© \(p\)ï¼ˆå…¶ä¸­ \(p=1\)ï¼‰çš„å®Œæ•´æ’åˆ—ç»„åˆï¼Œ
- en: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n
    - 1)!} = n \]
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n
    - 1)!} = n \]
- en: where the full combinatorial is the \(n\) models that we built above!
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œå®Œæ•´çš„æ’åˆ—ç»„åˆæ˜¯æˆ‘ä»¬ä¸Šé¢æ„å»ºçš„ \(n\) ä¸ªæ¨¡å‹ï¼
- en: K-fold Cross Validation (k-fold CV)
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KæŠ˜äº¤å‰éªŒè¯ï¼ˆk-fold CVï¼‰
- en: K-fold is a more general, efficient and robust approach.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: KæŠ˜æ˜¯ä¸€ç§æ›´é€šç”¨ã€é«˜æ•ˆä¸”ç¨³å¥çš„æ–¹æ³•ã€‚
- en: a exhaustive cross validation approach (all data are tested), but it samples
    a limited set of the possible combinatorial of prediction problems, unlike Leave-one-out
    cross validation where we attempt every possible case on data withheld for testing
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§å…¨é¢äº¤å‰éªŒè¯çš„æ–¹æ³•ï¼ˆæ‰€æœ‰æ•°æ®éƒ½è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œä½†å®ƒåªé‡‡æ ·äº†å¯èƒ½çš„é¢„æµ‹é—®é¢˜æ’åˆ—ç»„åˆçš„æœ‰é™é›†ï¼Œä¸ç•™ä¸€æ³•äº¤å‰éªŒè¯ä¸åŒï¼Œåœ¨ç•™ä¸€æ³•äº¤å‰éªŒè¯ä¸­ï¼Œæˆ‘ä»¬å°è¯•å¯¹ä¿ç•™ç”¨äºæµ‹è¯•çš„æ•°æ®çš„æ¯ä¸€ç§å¯èƒ½æƒ…å†µè¿›è¡Œæµ‹è¯•
- en: for K-fold cross validation we assign a single set of K equal size splits and
    we loop over the splits, withholding the \(k\) split for testing data and using
    the data outside the split for training
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºKæŠ˜äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ç»„Kä¸ªå¤§å°ç›¸ç­‰çš„åˆ†å‰²ï¼Œå¹¶å¾ªç¯éå†è¿™äº›åˆ†å‰²ï¼Œä¿ç•™ \(k\) ä¸ªåˆ†å‰²ä½œä¸ºæµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨åˆ†å‰²ä¹‹å¤–çš„æ•°æ®è¿›è¡Œè®­ç»ƒ
- en: the testing proportion is \(\frac{1}{K}\), e.g., for \(K=3\), 33.3% is withheld
    for testing, for \(K=4\), 25% is withheld for testing and for \(K=5\), 20% is
    withheld for testing
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ¯”ä¾‹æ˜¯ \(\frac{1}{K}\)ï¼Œä¾‹å¦‚ï¼Œå¯¹äº \(K=3\)ï¼Œ33.3% è¢«ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œå¯¹äº \(K=4\)ï¼Œ25% è¢«ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œå¯¹äº
    \(K=5\)ï¼Œ20% è¢«ä¿ç•™ç”¨äºæµ‹è¯•
- en: We call it K-fold cross validation, because each of the splits is known as a
    fold. Hereâ€™s the steps for K-fold cross validation,
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°ä¹‹ä¸ºKæŠ˜äº¤å‰éªŒè¯ï¼Œå› ä¸ºæ¯ä¸ªåˆ†å‰²éƒ½è¢«ç§°ä¸ºä¸€ä¸ªæŠ˜ã€‚ä»¥ä¸‹æ˜¯KæŠ˜äº¤å‰éªŒè¯çš„æ­¥éª¤ï¼Œ
- en: Select \(K\), integer number of folds
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹© \(K\)ï¼Œæ•´æ•°ä¸ªæŠ˜æ•°
- en: Split the data into \(K\) equal size folds
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ•°æ®åˆ†å‰²æˆ \(K\) ä¸ªå¤§å°ç›¸ç­‰çš„æŠ˜
- en: Loop over each \(k = 1,\ldots,K\) fold
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éå†æ¯ä¸ª \(k = 1,\ldots,K\) æŠ˜
- en: Assign the data outside the \(k\) fold as training data and inside the \(k\)
    fold as testing data
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† \(k\) æŠ˜ä»¥å¤–çš„æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå°† \(k\) æŠ˜ä»¥å†…çš„æ•°æ®ä½œä¸ºæµ‹è¯•æ•°æ®
- en: Train and test the prediction model and calculated the testing model metric
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•é¢„æµ‹æ¨¡å‹ï¼Œå¹¶è®¡ç®—æµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: Goto 3
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ°3
- en: Aggregate testing model metric over all K folds
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰KæŠ˜ä¸Šæ±‡æ€»æµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: As you can see above k-fold cross validation is exhaustive, since all data is
    tested, i.e., withheld as testing data, but the method is not exhaustive in that
    all possible \(\frac{n}{K}\) data subsets are not considered.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒkæŠ˜äº¤å‰éªŒè¯æ˜¯å…¨é¢çš„ï¼Œå› ä¸ºæ‰€æœ‰æ•°æ®éƒ½è¿›è¡Œäº†æµ‹è¯•ï¼Œå³ä½œä¸ºæµ‹è¯•æ•°æ®è¢«ä¿ç•™ï¼Œä½†è¿™ç§æ–¹æ³•ä¸æ˜¯å…¨é¢çš„ï¼Œå› ä¸ºå¹¶æ²¡æœ‰è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„ \(\frac{n}{K}\)
    ä¸ªæ•°æ®å­é›†ã€‚
- en: To calculated the combinatorial for exhaustive K folds we used the multinomial
    coefficient,
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—å…¨é¢KæŠ˜çš„æ’åˆ—ç»„åˆï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤šé¡¹å¼ç³»æ•°ï¼Œ
- en: \[ \frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!} \]
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!} \]
- en: For example, if there are \(n=100\) data and \(K=4\) folds, there are \(6.72
    \times 10^55\) possible combinations. I vote that we stick with regular K-fold
    cross validation.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ \(n=100\) ä¸ªæ•°æ®å’Œ \(K=4\) ä¸ªæŠ˜ï¼Œæœ‰ \(6.72 \times 10^55\) ç§å¯èƒ½çš„ç»„åˆã€‚æˆ‘å»ºè®®æˆ‘ä»¬åšæŒä½¿ç”¨å¸¸è§„çš„KæŠ˜äº¤å‰éªŒè¯ã€‚
- en: Letâ€™s visualize K-fold cross validation splits, for the case of \(K=4\).
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ– \(K\) æŠ˜äº¤å‰éªŒè¯çš„åˆ†å‰²ï¼Œä»¥ \(K=4\) ä¸ºä¾‹ã€‚
- en: '[PRE13]'
  id: totrans-638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png](../Images/dd257beaa821a963fc15b3318a7bd9f2.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/dd257beaa821a963fc15b3318a7bd9f2.png)'
- en: Leave-p-out Cross Validation (LpO-CV)
  id: totrans-640
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç•™å‡º \(p\) ä¸ªæ•°æ®äº¤å‰éªŒè¯ï¼ˆLpO-CVï¼‰
- en: This is the variant of K-fold cross validation that exhaustively samples the
    full combinatorial of withholding \(p\) testing data.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯KæŠ˜äº¤å‰éªŒè¯çš„ä¸€ç§å˜ä½“ï¼Œå®ƒå…¨é¢é‡‡æ ·äº†ä¿ç•™ \(p\) ä¸ªæµ‹è¯•æ•°æ®çš„å…¨éƒ¨æ’åˆ—ç»„åˆã€‚
- en: Select \(p\), integer number of testing data to withhold
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹© \(p\)ï¼Œæ•´æ•°ä¸ªè¦ä¿ç•™çš„æµ‹è¯•æ•°æ®
- en: For all possible \(p\) subsets of \(n\),
  id: totrans-643
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å¯èƒ½çš„ \(n\) çš„ \(p\) ä¸ªå­é›†ï¼Œ
- en: Assign the data outside the \(p\) as training data and inside the \(p\) as testing
    data
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† \(p\) ä»¥å¤–çš„æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå°† \(p\) ä»¥å†…çš„æ•°æ®ä½œä¸ºæµ‹è¯•æ•°æ®
- en: Train and test the prediction model and calculated the testing model metric
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæµ‹è¯•é¢„æµ‹æ¨¡å‹ï¼Œå¹¶è®¡ç®—æµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: Goto 2
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è½¬åˆ°2
- en: Aggregate testing model metric over the combinatorial
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨ç»„åˆä¸Šæ±‡æ€»æµ‹è¯•æ¨¡å‹æŒ‡æ ‡
- en: For this case the combinatorial of cases is, \(n\) choose \(p\),
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ç§æƒ…å†µï¼Œæ¡ˆä¾‹çš„æ’åˆ—ç»„åˆæ˜¯ï¼Œ\(n\) é€‰ \(p\)ï¼Œ
- en: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} \]
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \binom{n}{p} = \frac{n!}{p!(n - p)!} \]
- en: For \(n=100\) and \(p=20\), we have \(5.36 \times 10^{20}\) combinations to
    check!
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº \(n=100\) å’Œ \(p=20\)ï¼Œæˆ‘ä»¬æœ‰ \(5.36 \times 10^{20}\) ç§ç»„åˆéœ€è¦æ£€æŸ¥ï¼
- en: Limitations of Cross Validation
  id: totrans-651
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯çš„å±€é™æ€§
- en: Here are some additional issues with the model cross validation approach in
    general,
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸€äº›å…³äºæ¨¡å‹äº¤å‰éªŒè¯æ–¹æ³•çš„ä¸€èˆ¬é—®é¢˜ï¼Œ
- en: '**Peeking, Information Leakage** â€“ some information is transmitted from the
    withheld data into the model, some model decision(s) use all the data. Pipelines
    and wrappers help with this.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çª¥è§†ï¼Œä¿¡æ¯æ³„éœ²** â€“ ä¸€äº›ä¿¡æ¯ä»ä¿ç•™çš„æ•°æ®ä¸­ä¼ é€’åˆ°æ¨¡å‹ä¸­ï¼Œä¸€äº›æ¨¡å‹å†³ç­–ï¼ˆsï¼‰ä½¿ç”¨æ‰€æœ‰æ•°æ®ã€‚ç®¡é“å’ŒåŒ…è£…å™¨æœ‰åŠ©äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚'
- en: '**Black Swans / Stationarity** â€“ the model cannot be tested for data events
    not available in the data. This is also known as the â€˜No Free Lunch Theoremâ€™ in
    machine learning'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é»‘å¤©é¹…/å¹³ç¨³æ€§** â€“ æ¨¡å‹æ— æ³•å¯¹æ•°æ®ä¸­ä¸å¯ç”¨çš„äº‹ä»¶è¿›è¡Œæµ‹è¯•ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºæœºå™¨å­¦ä¹ ä¸­çš„â€œæ— å…è´¹åˆé¤å®šç†â€'
- en: Consider the words of Hume,
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸‹ä¼‘è°Ÿçš„è¯ï¼Œ
- en: â€œeven after the observation of the frequent or constant conjunction of objects,
    we have no reason to draw any inference concerning any object beyond those of
    which we have had experienceâ€ - Hume (1739â€“1740)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå³ä½¿è§‚å¯Ÿåˆ°äº†é¢‘ç¹æˆ–æ’å®šçš„å¯¹è±¡è”åˆï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰ç†ç”±å¯¹ä»»ä½•è¶…å‡ºæˆ‘ä»¬ç»éªŒèŒƒå›´çš„å¯¹è±¡åšå‡ºä»»ä½•æ¨è®ºâ€ - èµ«å°”ï¼ˆ1739-1740ï¼‰
- en: We cannot predict things that we have never seen in our data!
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ— æ³•é¢„æµ‹æˆ‘ä»¬åœ¨æ•°æ®ä¸­ä»æœªè§è¿‡çš„äº‹æƒ…ï¼
- en: hereâ€™s a quote from the famous Oreskes et al. (1994) paper on subsurface validation
    and verification,
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä»è‘—åçš„Oreskesç­‰äººï¼ˆ1994ï¼‰å…³äºåœ°ä¸‹éªŒè¯å’ŒéªŒè¯çš„è®ºæ–‡ä¸­å¼•ç”¨çš„ä¸€å¥è¯ï¼Œ
- en: â€œVerification and validation of numerical models of natural systems is impossible.
    This is because natural systems are never closed and because model results are
    always nonunique. Models can be confirmed by the demonstration of agreement between
    observation and prediction, but confirmation is inherently partial. Complete confirmation
    is logically precluded by the fallacy of affirming the consequent and by incomplete
    access to natural phenomena. Models can only be evaluated in relative terms, and
    their predictive value is always open to question. The primary value of models
    is heuristic.â€
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè‡ªç„¶ç³»ç»Ÿæ•°å€¼æ¨¡å‹çš„éªŒè¯å’ŒéªŒè¯æ˜¯ä¸å¯èƒ½çš„ã€‚è¿™æ˜¯å› ä¸ºè‡ªç„¶ç³»ç»Ÿæ°¸è¿œä¸ä¼šå°é—­ï¼Œå› ä¸ºæ¨¡å‹ç»“æœæ€»æ˜¯éå”¯ä¸€çš„ã€‚æ¨¡å‹å¯ä»¥é€šè¿‡è§‚å¯Ÿå’Œé¢„æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§æ¥è¯å®ï¼Œä½†è¯å®æœ¬è´¨ä¸Šæ˜¯ä¸å®Œå…¨çš„ã€‚ç”±äºè‚¯å®šåä»¶çš„è°¬è¯¯å’Œè‡ªç„¶ç°è±¡çš„ä¸å®Œå…¨è®¿é—®ï¼Œå®Œå…¨è¯å®åœ¨é€»è¾‘ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚æ¨¡å‹åªèƒ½ç›¸å¯¹è¯„ä¼°ï¼Œå®ƒä»¬çš„é¢„æµ‹ä»·å€¼æ€»æ˜¯å—åˆ°è´¨ç–‘ã€‚æ¨¡å‹çš„ä¸»è¦ä»·å€¼æ˜¯å¯å‘æ€§çš„ã€‚â€
- en: Oreskes et al. (1994)
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oreskesç­‰äººï¼ˆ1994ï¼‰
- en: all of this is summed up very well with,
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›éƒ½å¾ˆå¥½åœ°æ€»ç»“ä¸ºï¼Œ
- en: â€˜All models are wrong, but some are usefulâ€™ â€“ George Box
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œä½†æœ‰äº›æ˜¯æœ‰ç”¨çš„â€ â€“ ä¹”æ²»Â·åšå…‹æ–¯
- en: and a reminder of,
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠæé†’ï¼Œ
- en: '**Parsimony** â€“ since all models are wrong, an economical description of the
    system. Occamâ€™s Razor'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç®€çº¦æ€§** â€“ æ—¢ç„¶æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œå¯¹ç³»ç»Ÿçš„ç»æµæè¿°ã€‚å¥¥å¡å§†å‰ƒåˆ€'
- en: resulting in a pragmatic approach of,
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¼è‡´äº†ä¸€ç§å®ç”¨ä¸»ä¹‰çš„æ–¹æ³•ï¼Œ
- en: '**Worrying Selectively** â€“ since all models are wrong, figure out what is most
    importantly wrong.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©æ€§æ‹…å¿§** â€“ æ—¢ç„¶æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™è¯¯çš„ï¼Œæ‰¾å‡ºæœ€é‡è¦çš„é”™è¯¯æ˜¯ä»€ä¹ˆã€‚'
- en: finally, I add my own words,
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘åŠ ä¸Šè‡ªå·±çš„è¯ï¼Œ
- en: â€˜Be humble, the earth will surprise you!â€™ â€“ Michael Pyrcz
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä¿æŒè°¦é€Šï¼Œåœ°çƒä¼šç»™ä½ æƒŠå–œï¼â€ â€“ è¿ˆå…‹å°”Â·çš®å°”èŒ¨
- en: Comments
  id: totrans-669
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„è®º
- en: This was a basic description of machine learning concepts. Much more could be
    done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹æœºå™¨å­¦ä¹ æ¦‚å¿µçš„åˆæ­¥æè¿°ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šèµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„è§†é¢‘è®²åº§YouTubeé“¾æ¥ã€‚
- en: I hope this was helpful,
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-673
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-674
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆåœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­çš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[Cockrellå·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[Jacksonåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œåœ¨é‚£é‡Œä»–ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚'
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[Mathematical
    Geosciences](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚'
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿ˆå…‹å°”å·²ç»æ’°å†™äº†70å¤šç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[Geostatistical
    Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯[Applied
    Geostatistics in Python: a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[Applied
    Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚'
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å«100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHubè´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-681
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ä¸ªå†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢å—ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„Ÿå…´è¶£äºåˆä½œï¼Œæ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººåŒ…æ‹¬Fosteræ•™æˆã€Torres-Verdinæ•™æˆå’Œvan Oortæ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹æ¥å¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»åˆ°ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”èŒ¨ï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡Cockrellå·¥ç¨‹å­¦é™¢å’ŒJacksonåœ°çƒç§‘å­¦å­¦é™¢
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
