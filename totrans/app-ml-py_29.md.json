["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.svm import SVC                                   # support vector machine methods\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.metrics import confusion_matrix                  # for summarizing model performance\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,StratifiedShuffleSplit) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias\nbinary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef visualize_SVM(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,xlabel,ylabel,title,cat,label,cmap,plot_support): \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy,Z,cmap=cmap,vmin=z_min,vmax=z_max,levels = 50,alpha=0.6) # plot the predictions\n    for i in range(len(cat)):\n        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], \n                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors=\"black\")\n    plt.scatter(-9999,-9999,marker='s',c = cat[0],label=label[0],cmap=cmap,vmin=z_min,vmax=z_max) # custom legend\n    plt.scatter(-9999,-9999,marker='s',c = cat[1],label=label[1],cmap=cmap,vmin=z_min,vmax=z_max)\n    plt.scatter(-999,-999,s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\n    if plot_support:                                          # modified from Jake VanderPlas's Python Data Science Handbook \n        sv = model.support_vectors_                           # retrieve the support vectors\n        plt.scatter(sv[:, 0],sv[:, 1],s=3,linewidth=8,alpha = 0.6,facecolors='black',label='Support Vector');\n    plt.legend(loc='upper right'); plt.title(title)                    \n    plt.xlabel(xlabel); plt.ylabel(ylabel)\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()                       # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"12_sample_data.csv\") \n```", "```py\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv\")\n\nyname = 'Facies'; Xname = ['Porosity','AI']                   # specify the predictor features (x2) and response feature (x1)\nXmin = [0.1,1500.0]; Xmax = [0.3,6500.0]                      # set minimums and maximums for visualization \nymin = 0.0; ymax = 1.0\nXlabel = ['Porosity','Acoustic Impedance']; ylabel = 'Facies' # specify the feature labels for plotting\nXunit = ['Fraction',r'$\\frac{kg}{m^3} \\cdot \\frac{m}{s} \\cdot 10^3$']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\ny = pd.DataFrame(df[yname])                                   # extract selected features as X and y DataFrames\nX = df[Xname]\n\nysname = 's' + yname; Xsname = ['s' + element for element in Xname] # standardized predictor names\nXsmin = [-3.0,-3.0]; Xsmax = [3.0,3.0]                        # set minimums and maximums for standardized features\nXslabel = ['Standardized ' + element for element in Xlabel]   # standardized predictor names\nXsunit = ['S[' + element + ']' for element in Xunit]          # standardized predictor names\nXslabelunit = [Xslabel[0] + ' (' + Xsunit[0] + ')',Xslabel[1] + ' (' + Xsunit[1] + ')']\n\ntransform = StandardScaler();                                 # instantiate feature standardization method\nXs = transform.fit_transform(X)                               # standardize the data features to mean = 0, var = 1.0\nX[Xsname] = Xs                                                # add standardized features to the predictor feature DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train and test DataFrame with both X and y\ndf_test = pd.concat([X_test,y_test],axis=1) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame') # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(231)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(232)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(233)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplot(234)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xslabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[0]); add_grid()  \nplt.xlim([Xsmin[0],Xsmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(235)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xslabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[1]); add_grid()  \nplt.xlim([Xsmin[1],Xsmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(236)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(X_train[Xsname[0]],X_train[Xsname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(X_test[Xsname[0]],X_test[Xsname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xslabel[0] + ' vs ' +  Xslabel[1])\nplt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=1.6, wspace=0.3, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(131)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xsname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xsname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xslabelunit[0]); plt.xlim(Xsmin[0],Xsmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xslabel[0] + ' Train and Test CDFs')\n\nplt.subplot(132)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xsname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xsname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xslabelunit[1]); plt.xlim(Xsmin[1],Xsmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xslabel[1] + ' Train and Test CDFs')\n\nplt.subplot(133)                                              # categorical response feature grouped histogram\nplt.bar([-0.125],len(y_train[yname][y_train[yname]==0]),width=0.25,color=['darkorange'],edgecolor='black',label='Train')\nplt.bar([0.125],len(y_test[yname][y_test[yname]==0]),width=0.25,color=['red'],edgecolor='black',label='Test')\nplt.bar([0.875],len(y_train[yname][y_train[yname]==1]),width=0.25,color=['darkorange'],edgecolor='black')\nplt.bar([1.125],len(y_test[yname][y_test[yname]==1]),width=0.25,color=['red'],edgecolor='black')\nx_ticks = [0, 1]; x_labels = ['Shale', 'Sand']; plt.xticks(x_ticks,x_labels)\nplt.ylim([0.,250.0]); plt.xlim([-0.5,1.5]); add_grid(); plt.legend(loc='upper left')\nax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())\nplt.title(ylabel + ' Train and Test Categorical Response Frequencies'); plt.xlabel('Facies'); plt.ylabel('Frequency')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # plot train and test data in predictor feature space\nplt.scatter(X_train[Xsname[0]][y_train[yname]==1],X_train[Xsname[1]][y_train[yname]==1],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='Sand')\nplt.scatter(X_train[Xsname[0]][y_train[yname]==0],X_train[Xsname[1]][y_train[yname]==0],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Shale')\n\nplt.scatter(X_test[Xsname[0]][y_test[yname]==1],X_test[Xsname[1]][y_test[yname]==1],s=80,\n            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)\nplt.scatter(X_test[Xsname[0]][y_test[yname]==0],X_test[Xsname[1]][y_test[yname]==0],s=80,\n            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)\n\nplt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\nplt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Testing ' + ylabel + ' vs. ' + Xslabel[1] + ' and ' + Xlabel[0])\nplt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1]); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nsvm_linear = SVC() \n```", "```py\nsvm_linear.fit() \n```", "```py\nC1_list = [0.01,100]                                          # set hyperparameters\nSVM1_list = []\n\nfor C in C1_list:                                             # train the models\n    SVM1_list.append(SVC(kernel = 'linear',C = C, random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train \n```", "```py\nfor iC, C in enumerate(C1_list):                              # visualize the training data and model\n    plt.subplot(1,2,iC+1)\n    visualize_SVM(SVM1_list[iC],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],r'Training Data and Linear Support Vector Machine, $C$ = ' + str(C),[0,1],['Shale','Sand'],\n                binary_cmap,True)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nC2_list = [0.01,0.1,1,10]                                     # set hyperparameters\norder = np.full((len(C2_list)),3)       \nSVM2_list = []\n\nfor iC, C in enumerate(C2_list):                              # train the model and visualize the training data and model\n    SVM2_list.append(SVC(kernel = 'poly',degree=order[iC],C = C,random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train\n    plt.subplot(2,2,iC+1)\n    visualize_SVM(SVM2_list[iC-1],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order[iC]) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\nC3_list = [1e-1, 1, 1e2]                                      # set hyperparameters\ngamma1_list = [1e-1, 1, 1e1]\n\nindex= 1\nfor C in C3_list:\n    for gamma in gamma1_list:                                 # train the models, visualize the training data and models\n        svc = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xsname],y_train) # instantiate and train\n        plt.subplot(3,3,index)\n        visualize_SVM(svc,X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, $\\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\n        index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.4, top=2.4, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\norder = 3; C = 0.01                                           # set the hyperparameters\nsvc_test1 = SVC(kernel='poly',degree=order,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized\ngamma = 1.0; C = 1.0                                          # set the hyperparameters\nsvc_test2 = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized\n\nplt.subplot(121)                                              # visualize the training data and models\nvisualize_SVM(svc_test1,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,\n                Xlabelunit[0],Xlabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplot(122)\nvisualize_SVM(svc_test2,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,\n                Xlabelunit[0],Xlabelunit[1],r'RBF Support Vector Machine, $\\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\nC_range = np.logspace(-2, 7, 10)                              # set hyperparameter cases\ngamma_range = np.logspace(-6, 3, 10)\nparam_grid = dict(gamma=gamma_range, C=C_range)               # store hyperparameter cases in a dictionary\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=seed) # instantiate the cross validation method\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv,n_jobs=5).fit(X[Xsname],y) # brute force, full combinatorial search with cross validation \nscores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range)) # retrieve average accuracy and shape as a 2D array for plot \n```", "```py\nplt.subplot(111)                                              # plot results of hyperparameter tuning \nim = plt.imshow(scores,vmin=0.6,vmax=0.95,cmap=cmap,alpha=1.0)\nplt.xlabel(r'$\\gamma$ Hyperparameter')\nplt.ylabel(r'$C$ Hyperparameter')\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label('Average Classification Accuracy Over Splits', rotation=270, labelpad=20)\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title('SVC Hyperparameter Tuning, Cross Validation Accuracy');\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\ncases = ['Poor','OK','Good']                                  # selected hyperparameter cases for visualization\nC_list = [100,100,1e6]\ngamma_list = [100,10,0.01]\nmodel_cases = []\n\nfor icase, case in enumerate(cases):                          # visualize the training data and model\n    model_cases.append(SVC(kernel='rbf',C=C_list[icase],gamma=gamma_list[icase]).fit(X[Xsname],y)) # train on all the data\n    plt.subplot(1,3,icase+1)                                  # visualize model cases and all data\n    visualize_SVM(model_cases[icase],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n        Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, ' + str(cases[icase]) + ' Model',[0,1],['Shale','Sand'],binary_cmap,False)\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.svm import SVC                                   # support vector machine methods\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.metrics import confusion_matrix                  # for summarizing model performance\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,StratifiedShuffleSplit) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias\nbinary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef plot_CDF(data,color,alpha=1.0,lw=1,ls='solid',label='none'):\n    cumprob = (np.linspace(1,len(data),len(data)))/(len(data)+1)\n    plt.scatter(np.sort(data),cumprob,c=color,alpha=alpha,edgecolor='black',lw=lw,ls=ls,label=label,zorder=10)\n    plt.plot(np.sort(data),cumprob,c=color,alpha=alpha,lw=lw,ls=ls,zorder=8)\n\ndef visualize_SVM(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,xlabel,ylabel,title,cat,label,cmap,plot_support): \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy,Z,cmap=cmap,vmin=z_min,vmax=z_max,levels = 50,alpha=0.6) # plot the predictions\n    for i in range(len(cat)):\n        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], \n                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors=\"black\")\n    plt.scatter(-9999,-9999,marker='s',c = cat[0],label=label[0],cmap=cmap,vmin=z_min,vmax=z_max) # custom legend\n    plt.scatter(-9999,-9999,marker='s',c = cat[1],label=label[1],cmap=cmap,vmin=z_min,vmax=z_max)\n    plt.scatter(-999,-999,s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\n    if plot_support:                                          # modified from Jake VanderPlas's Python Data Science Handbook \n        sv = model.support_vectors_                           # retrieve the support vectors\n        plt.scatter(sv[:, 0],sv[:, 1],s=3,linewidth=8,alpha = 0.6,facecolors='black',label='Support Vector');\n    plt.legend(loc='upper right'); plt.title(title)                    \n    plt.xlabel(xlabel); plt.ylabel(ylabel)\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()                       # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"12_sample_data.csv\") \n```", "```py\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv\")\n\nyname = 'Facies'; Xname = ['Porosity','AI']                   # specify the predictor features (x2) and response feature (x1)\nXmin = [0.1,1500.0]; Xmax = [0.3,6500.0]                      # set minimums and maximums for visualization \nymin = 0.0; ymax = 1.0\nXlabel = ['Porosity','Acoustic Impedance']; ylabel = 'Facies' # specify the feature labels for plotting\nXunit = ['Fraction',r'$\\frac{kg}{m^3} \\cdot \\frac{m}{s} \\cdot 10^3$']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\ny = pd.DataFrame(df[yname])                                   # extract selected features as X and y DataFrames\nX = df[Xname]\n\nysname = 's' + yname; Xsname = ['s' + element for element in Xname] # standardized predictor names\nXsmin = [-3.0,-3.0]; Xsmax = [3.0,3.0]                        # set minimums and maximums for standardized features\nXslabel = ['Standardized ' + element for element in Xlabel]   # standardized predictor names\nXsunit = ['S[' + element + ']' for element in Xunit]          # standardized predictor names\nXslabelunit = [Xslabel[0] + ' (' + Xsunit[0] + ')',Xslabel[1] + ' (' + Xsunit[1] + ')']\n\ntransform = StandardScaler();                                 # instantiate feature standardization method\nXs = transform.fit_transform(X)                               # standardize the data features to mean = 0, var = 1.0\nX[Xsname] = Xs                                                # add standardized features to the predictor feature DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train and test DataFrame with both X and y\ndf_test = pd.concat([X_test,y_test],axis=1) \n```", "```py\nprint('       Training DataFrame          Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame          Testing DataFrame \n```", "```py\nprint('            Training DataFrame                      Testing DataFrame') # custom function for side-by-side summary statistics\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(231)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(232)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(233)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(X_train[Xname[0]],X_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(X_test[Xname[0]],X_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplot(234)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xsname[0]],weights=None,bins=np.linspace(Xsmin[0],Xsmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xslabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[0]); add_grid()  \nplt.xlim([Xsmin[0],Xsmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(235)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=X_train[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=X_test[Xsname[1]],weights=None,bins=np.linspace(Xsmin[1],Xsmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xslabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xslabel[1]); add_grid()  \nplt.xlim([Xsmin[1],Xsmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(236)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(X_train[Xsname[0]],X_train[Xsname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(X_test[Xsname[0]],X_test[Xsname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title(Xslabel[0] + ' vs ' +  Xslabel[1])\nplt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=1.6, wspace=0.3, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(131)                                              # predictor feature #1 CDF\nplot_CDF(X_train[Xsname[0]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xsname[0]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xslabelunit[0]); plt.xlim(Xsmin[0],Xsmax[0]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xslabel[0] + ' Train and Test CDFs')\n\nplt.subplot(132)                                              # predictor feature #2 CDF\nplot_CDF(X_train[Xsname[1]],'darkorange',alpha=0.6,lw=1,ls='solid',label='Train')\nplot_CDF(X_test[Xsname[1]],'red',alpha=0.6,lw=1,ls='solid',label='Test')\nplt.xlabel(Xslabelunit[1]); plt.xlim(Xsmin[1],Xsmax[1]); plt.ylim([0,1]); add_grid(); plt.legend(loc='lower right')\nplt.title(Xslabel[1] + ' Train and Test CDFs')\n\nplt.subplot(133)                                              # categorical response feature grouped histogram\nplt.bar([-0.125],len(y_train[yname][y_train[yname]==0]),width=0.25,color=['darkorange'],edgecolor='black',label='Train')\nplt.bar([0.125],len(y_test[yname][y_test[yname]==0]),width=0.25,color=['red'],edgecolor='black',label='Test')\nplt.bar([0.875],len(y_train[yname][y_train[yname]==1]),width=0.25,color=['darkorange'],edgecolor='black')\nplt.bar([1.125],len(y_test[yname][y_test[yname]==1]),width=0.25,color=['red'],edgecolor='black')\nx_ticks = [0, 1]; x_labels = ['Shale', 'Sand']; plt.xticks(x_ticks,x_labels)\nplt.ylim([0.,250.0]); plt.xlim([-0.5,1.5]); add_grid(); plt.legend(loc='upper left')\nax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())\nplt.title(ylabel + ' Train and Test Categorical Response Frequencies'); plt.xlabel('Facies'); plt.ylabel('Frequency')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=0.8, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # plot train and test data in predictor feature space\nplt.scatter(X_train[Xsname[0]][y_train[yname]==1],X_train[Xsname[1]][y_train[yname]==1],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='Sand')\nplt.scatter(X_train[Xsname[0]][y_train[yname]==0],X_train[Xsname[1]][y_train[yname]==0],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Shale')\n\nplt.scatter(X_test[Xsname[0]][y_test[yname]==1],X_test[Xsname[1]][y_test[yname]==1],s=80,\n            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)\nplt.scatter(X_test[Xsname[0]][y_test[yname]==0],X_test[Xsname[1]][y_test[yname]==0],s=80,\n            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)\n\nplt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\nplt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Testing ' + ylabel + ' vs. ' + Xslabel[1] + ' and ' + Xlabel[0])\nplt.xlabel(Xslabelunit[0]); plt.ylabel(Xslabelunit[1]); add_grid(); plt.xlim([Xsmin[0],Xsmax[0]]); plt.ylim([Xsmin[1],Xsmax[1]])\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nsvm_linear = SVC() \n```", "```py\nsvm_linear.fit() \n```", "```py\nC1_list = [0.01,100]                                          # set hyperparameters\nSVM1_list = []\n\nfor C in C1_list:                                             # train the models\n    SVM1_list.append(SVC(kernel = 'linear',C = C, random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train \n```", "```py\nfor iC, C in enumerate(C1_list):                              # visualize the training data and model\n    plt.subplot(1,2,iC+1)\n    visualize_SVM(SVM1_list[iC],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],r'Training Data and Linear Support Vector Machine, $C$ = ' + str(C),[0,1],['Shale','Sand'],\n                binary_cmap,True)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nC2_list = [0.01,0.1,1,10]                                     # set hyperparameters\norder = np.full((len(C2_list)),3)       \nSVM2_list = []\n\nfor iC, C in enumerate(C2_list):                              # train the model and visualize the training data and model\n    SVM2_list.append(SVC(kernel = 'poly',degree=order[iC],C = C,random_state = seed).fit(X_train[Xsname],y_train)) # instantiate and train\n    plt.subplot(2,2,iC+1)\n    visualize_SVM(SVM2_list[iC-1],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order[iC]) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\nC3_list = [1e-1, 1, 1e2]                                      # set hyperparameters\ngamma1_list = [1e-1, 1, 1e1]\n\nindex= 1\nfor C in C3_list:\n    for gamma in gamma1_list:                                 # train the models, visualize the training data and models\n        svc = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xsname],y_train) # instantiate and train\n        plt.subplot(3,3,index)\n        visualize_SVM(svc,X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n                Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, $\\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\n        index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.4, top=2.4, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\norder = 3; C = 0.01                                           # set the hyperparameters\nsvc_test1 = SVC(kernel='poly',degree=order,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized\ngamma = 1.0; C = 1.0                                          # set the hyperparameters\nsvc_test2 = SVC(kernel='rbf',gamma=gamma,C=C,random_state = seed).fit(X_train[Xname],y_train) # fit with original features, not standardized\n\nplt.subplot(121)                                              # visualize the training data and models\nvisualize_SVM(svc_test1,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,\n                Xlabelunit[0],Xlabelunit[1],'Polynomial Support Vector Machine, Order = ' + str(order) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplot(122)\nvisualize_SVM(svc_test2,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],0.0,1.0,\n                Xlabelunit[0],Xlabelunit[1],r'RBF Support Vector Machine, $\\gamma$ = ' + str(gamma) + r', $C$ = ' + str(C),\n                [0,1],['Shale','Sand'],binary_cmap,True)\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\nC_range = np.logspace(-2, 7, 10)                              # set hyperparameter cases\ngamma_range = np.logspace(-6, 3, 10)\nparam_grid = dict(gamma=gamma_range, C=C_range)               # store hyperparameter cases in a dictionary\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=seed) # instantiate the cross validation method\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv,n_jobs=5).fit(X[Xsname],y) # brute force, full combinatorial search with cross validation \nscores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range)) # retrieve average accuracy and shape as a 2D array for plot \n```", "```py\nplt.subplot(111)                                              # plot results of hyperparameter tuning \nim = plt.imshow(scores,vmin=0.6,vmax=0.95,cmap=cmap,alpha=1.0)\nplt.xlabel(r'$\\gamma$ Hyperparameter')\nplt.ylabel(r'$C$ Hyperparameter')\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label('Average Classification Accuracy Over Splits', rotation=270, labelpad=20)\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title('SVC Hyperparameter Tuning, Cross Validation Accuracy');\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.8, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\ncases = ['Poor','OK','Good']                                  # selected hyperparameter cases for visualization\nC_list = [100,100,1e6]\ngamma_list = [100,10,0.01]\nmodel_cases = []\n\nfor icase, case in enumerate(cases):                          # visualize the training data and model\n    model_cases.append(SVC(kernel='rbf',C=C_list[icase],gamma=gamma_list[icase]).fit(X[Xsname],y)) # train on all the data\n    plt.subplot(1,3,icase+1)                                  # visualize model cases and all data\n    visualize_SVM(model_cases[icase],X_train[Xsname[0]],Xsmin[0],Xsmax[0],X_train[Xsname[1]],Xsmin[1],Xsmax[1],y_train[yname],0.0,1.0,\n        Xslabelunit[0],Xslabelunit[1],r'RBF Support Vector Machine, ' + str(cases[icase]) + ' Model',[0,1],['Shale','Sand'],binary_cmap,False)\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.3, hspace=0.3); plt.show() \n```"]