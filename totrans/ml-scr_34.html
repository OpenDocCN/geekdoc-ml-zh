<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Probability</h1>
<blockquote>原文：<a href="https://dafriedman97.github.io/mlbook/content/appendix/probability.html">https://dafriedman97.github.io/mlbook/content/appendix/probability.html</a></blockquote>

<p>Many machine learning methods are rooted in probability theory. Probabilistic methods in this book include <a class="reference internal" href="../c1/concept.html"><span class="doc">linear regression</span></a>, <a class="reference internal" href="../c2/s1/bayesian.html"><span class="doc">Bayesian regression</span></a>, and <a class="reference internal" href="../c4/concept.html"><span class="doc">generative classifiers</span></a>. This section covers the probability theory needed to understand those methods.</p>
<div class="section" id="random-variables-and-distributions">
<h2>1. Random Variables and Distributions</h2>
<div class="section" id="random-variables">
<h3>Random Variables</h3>
<p>A <strong>random variable</strong> is a variable whose value is randomly determined. The set of possible values a random variable can take on is called the variable’s <strong>support</strong>. An example of a random variable is the value on a die roll. This variable’s support is <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Random variables will be represented with uppercase letters and values in their support with lowercase letters. For instance <span class="math notranslate nohighlight">\(X = x\)</span> implies that a random variable <span class="math notranslate nohighlight">\(X\)</span> happened to take on value <span class="math notranslate nohighlight">\(x\)</span>. Letting <span class="math notranslate nohighlight">\(X\)</span> be the value of a die roll, <span class="math notranslate nohighlight">\(X = 4\)</span> indicates that the die landed on 4.</p>
</div>
<div class="section" id="density-functions">
<h3>Density Functions</h3>
<p>The likelihood that a random variable takes on a given value is determined through its density function. For a discrete random variable (one that can take on a finite set of values), this density function is called the <strong>probability mass function</strong> <strong>(PMF)</strong>. The PMF of a random variable <span class="math notranslate nohighlight">\(X\)</span> gives the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal some value <span class="math notranslate nohighlight">\(x\)</span>. We write it as <span class="math notranslate nohighlight">\(f_X(x)\)</span> or just <span class="math notranslate nohighlight">\(f(x)\)</span>, and it is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = P(X = x).
\]</div>
<p>For a continuous random variable (one that can take on infinitely many values), the density function is called the <strong>probability density function (PDF)</strong>. The PDF <span class="math notranslate nohighlight">\(f_X(x)\)</span> of a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> does not give <span class="math notranslate nohighlight">\(P(X = x)\)</span> but it does determine the probability that <span class="math notranslate nohighlight">\(X\)</span> lands in a certain range. Specifically,</p>
<div class="math notranslate nohighlight">
\[
P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. 
\]</div>
<p>That is, integrating <span class="math notranslate nohighlight">\(f(x)\)</span> over a certain range gives the probability of <span class="math notranslate nohighlight">\(X\)</span> being in that range. While <span class="math notranslate nohighlight">\(f(x)\)</span> does not give the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal a certain value, it does indicate the relative likelihood that it will be <em>around</em> that value. E.g. if <span class="math notranslate nohighlight">\(f(a) &gt; f(b)\)</span>, we can say <span class="math notranslate nohighlight">\(X\)</span> is more likely to be in an arbitrarily small area around the value <span class="math notranslate nohighlight">\(a\)</span> than around the value <span class="math notranslate nohighlight">\(b\)</span>.</p>
</div>
<div class="section" id="distributions">
<h3>Distributions</h3>
<p>A random variable’s <strong>distribution</strong> is determined by its density function. Variables with the same density function are said to follow the same distributions. Certain families of distributions are very common in probability and machine learning. Two examples are given below.</p>
<p>The <strong>Bernoulli</strong> distribution is the most simple probability distribution and it describes the likelihood of the outcomes of a binary event. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable that equals 1 (representing “success”) with probability <span class="math notranslate nohighlight">\(p\)</span> and 0 (representing “failure”) with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Then, <span class="math notranslate nohighlight">\(X\)</span> is said to follow the Bernoulli distribution with probability parameter <span class="math notranslate nohighlight">\(p\)</span>, written <span class="math notranslate nohighlight">\(X \sim \text{Bern}(p)\)</span>, and its PMF is given by</p>
<div class="math notranslate nohighlight">
\[
f_X(x) = p^x(1-p)^{(1-x)}.
\]</div>
<p>We can check to see that for any valid value <span class="math notranslate nohighlight">\(x\)</span> in the support of <span class="math notranslate nohighlight">\(X\)</span>—i.e., 1 or 0—, <span class="math notranslate nohighlight">\(f(x)\)</span> gives <span class="math notranslate nohighlight">\(P(X = x)\)</span>.</p>
<p>The <strong>Normal</strong> distribution is extremely common and will be used throughout this book. A random variable <span class="math notranslate nohighlight">\(X\)</span> follows the Normal distribution with mean parameter <span class="math notranslate nohighlight">\(\mu \in \R\)</span> and variance parameter <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>, written <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, if its PDF is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\]</div>
<p>The shape of the Normal random variable’s density function gives this distribution the name “the bell curve”, as shown below. Values closest to <span class="math notranslate nohighlight">\(\mu\)</span> are most likely and the density is symmetric around <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><img alt="normal" src="../Images/d421cf0f88db6f8c4a0271d87de7ab98.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/normal.jpg"/></p>
</div>
<div class="section" id="independence">
<h3>Independence</h3>
<p>So far we’ve discussed the density of individual random variables. The picture can get much more complicated when we want to study the behavior of multiple random variables simultaneously. The assumption of independence simplifies things greatly. Let’s start by defining independence in the discrete case.</p>
<p>Two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if and only if</p>
<div class="math notranslate nohighlight">
\[
P(X = x, Y =y) = P(X = x)P(Y = y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This says that if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, the probability that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> simultaneously is just the product of the probabilities that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> individually.</p>
<p>To generalize this definition to continuous random variables, let’s first introduce <em>joint density function</em>. Quite simply, the joint density of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, written <span class="math notranslate nohighlight">\(f_{X, Y}(x, y)\)</span> gives the probability density of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> evaluated simultaneously at <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively. We can then say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if</p>
<div class="math notranslate nohighlight">
\[
f_{X, Y}(x, y) = f_X(x) f_Y(y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
</div>
</div>
<div class="section" id="maximum-likelihood-estimation">
<h2>2. Maximum Likelihood Estimation</h2>
<p>Maximum likelihood estimation is used to understand the parameters of a distribution that gave rise to observed data. In order to model a data generating process, we often assume it comes from some family of distributions, such as the Bernoulli or Normal distributions. These distributions are indexed by certain parameters (<span class="math notranslate nohighlight">\(p\)</span> for the Bernoulli and <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> for the Normal)—maximum likelihood estimation evaluates which parameters would be most consistent with the data we observed.</p>
<p>Specifically, maximum likelihood estimation finds the values of unknown parameters that maximize the probability of observing the data we did. Basic maximum likelihood estimation can be broken into three steps:</p>
<ol class="simple">
<li><p>Find the joint density of the observed data, also called the <em>likelihood</em></p></li>
<li><p>Take the log of the likelihood, giving the <em>log-likelihood</em>.</p></li>
<li><p>Find the value of the parameter that maximizes the log-likelihood (and therefore the likelihood as well) by setting its derivative equal to 0.</p></li>
</ol>
<p>Finding the value of the parameter to maximize the log-likelihood rather than the likelihood makes the math easier and gives us the same solution.</p>
<p>Let’s go through an example. Suppose we are interested in calculating the average weight of a Chihuahua. We assume the weight of any given Chihuahua is <em>independently</em> distributed Normally with <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> but an unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>. So, we gather 10 Chihuahuas and weigh them. Denote the <span class="math notranslate nohighlight">\(j^\text{th}\)</span> Chihuahua weight with <span class="math notranslate nohighlight">\(W_j \sim \mathcal{N}(\mu, 1)\)</span>.  For step 1, let’s calculate the probability density of our data (i.e., the 10 Chihuahua weights). Since the weights are assumed to be independent, the densities multiply. Letting <span class="math notranslate nohighlight">\(L(\mu)\)</span> be the likelihood of <span class="math notranslate nohighlight">\(\mu\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\mu) &amp;= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10}) \\
&amp;= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10})  \\
&amp;= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot 1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\
&amp;\propto \exp\left(-\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\
\end{align}
\end{split}\]</div>
<p>Note that we can work up to a constant of proportionality since the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes <span class="math notranslate nohighlight">\(L(\mu)\)</span> will also maximize anything proportional to <span class="math notranslate nohighlight">\(L(\mu)\)</span>. For step 2, take the log:</p>
<div class="math notranslate nohighlight">
\[
\log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is some constant. For step 3, take the derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j - \mu).
\end{align}
\]</div>
<p>Setting this equal to 0, we find that the (log) likelihood is maximized with</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. 
\]</div>
<p>We put a hat over <span class="math notranslate nohighlight">\(\mu\)</span> to indicate that it is our <em>estimate</em> of the true <span class="math notranslate nohighlight">\(\mu\)</span>. Note the sensible result—we estimate the true mean of the Chihuahua weight distribution to be the sample mean of our observed data.</p>
</div>
<div class="section" id="conditional-probability">
<h2>3. Conditional Probability</h2>
<p>Probabilistic machine learning methods typically consider the distribution of a target variable conditional on the value of one or more predictor variables. To understand these methods, let’s introduce some of the basic principles of conditional probability.</p>
<p>Consider two events, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. The <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is the probability that <span class="math notranslate nohighlight">\(A\)</span> occurs given <span class="math notranslate nohighlight">\(B\)</span> occurs, written <span class="math notranslate nohighlight">\(P(A|B)\)</span>. Closely related is the <strong>joint probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, or the probability that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur, written <span class="math notranslate nohighlight">\(P(A, B)\)</span>. We navigate between the conditional and joint probability with the following</p>
<div class="math notranslate nohighlight">
\[
P(A, B) = P(A|B)P(B).
\]</div>
<p>The above equation leads to an extremely important principle in conditional probability: Bayes’ rule. <strong>Bayes’ rule</strong> states that</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
\]</div>
<p>Both of the above expressions work for random variables as well as events. For any two discrete random variables, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P(X = x, Y = y) &amp;= P(X = x|Y = y)P(Y = y) \\
P(X = x|Y = y) &amp;= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}.
\end{align}
\end{split}\]</div>
<p>The same is true for continuous random variables, replacing the PMFs with PDFs.</p>
</div>
&#13;

<h2>1. Random Variables and Distributions</h2>
<div class="section" id="random-variables">
<h3>Random Variables</h3>
<p>A <strong>random variable</strong> is a variable whose value is randomly determined. The set of possible values a random variable can take on is called the variable’s <strong>support</strong>. An example of a random variable is the value on a die roll. This variable’s support is <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Random variables will be represented with uppercase letters and values in their support with lowercase letters. For instance <span class="math notranslate nohighlight">\(X = x\)</span> implies that a random variable <span class="math notranslate nohighlight">\(X\)</span> happened to take on value <span class="math notranslate nohighlight">\(x\)</span>. Letting <span class="math notranslate nohighlight">\(X\)</span> be the value of a die roll, <span class="math notranslate nohighlight">\(X = 4\)</span> indicates that the die landed on 4.</p>
</div>
<div class="section" id="density-functions">
<h3>Density Functions</h3>
<p>The likelihood that a random variable takes on a given value is determined through its density function. For a discrete random variable (one that can take on a finite set of values), this density function is called the <strong>probability mass function</strong> <strong>(PMF)</strong>. The PMF of a random variable <span class="math notranslate nohighlight">\(X\)</span> gives the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal some value <span class="math notranslate nohighlight">\(x\)</span>. We write it as <span class="math notranslate nohighlight">\(f_X(x)\)</span> or just <span class="math notranslate nohighlight">\(f(x)\)</span>, and it is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = P(X = x).
\]</div>
<p>For a continuous random variable (one that can take on infinitely many values), the density function is called the <strong>probability density function (PDF)</strong>. The PDF <span class="math notranslate nohighlight">\(f_X(x)\)</span> of a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> does not give <span class="math notranslate nohighlight">\(P(X = x)\)</span> but it does determine the probability that <span class="math notranslate nohighlight">\(X\)</span> lands in a certain range. Specifically,</p>
<div class="math notranslate nohighlight">
\[
P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. 
\]</div>
<p>That is, integrating <span class="math notranslate nohighlight">\(f(x)\)</span> over a certain range gives the probability of <span class="math notranslate nohighlight">\(X\)</span> being in that range. While <span class="math notranslate nohighlight">\(f(x)\)</span> does not give the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal a certain value, it does indicate the relative likelihood that it will be <em>around</em> that value. E.g. if <span class="math notranslate nohighlight">\(f(a) &gt; f(b)\)</span>, we can say <span class="math notranslate nohighlight">\(X\)</span> is more likely to be in an arbitrarily small area around the value <span class="math notranslate nohighlight">\(a\)</span> than around the value <span class="math notranslate nohighlight">\(b\)</span>.</p>
</div>
<div class="section" id="distributions">
<h3>Distributions</h3>
<p>A random variable’s <strong>distribution</strong> is determined by its density function. Variables with the same density function are said to follow the same distributions. Certain families of distributions are very common in probability and machine learning. Two examples are given below.</p>
<p>The <strong>Bernoulli</strong> distribution is the most simple probability distribution and it describes the likelihood of the outcomes of a binary event. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable that equals 1 (representing “success”) with probability <span class="math notranslate nohighlight">\(p\)</span> and 0 (representing “failure”) with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Then, <span class="math notranslate nohighlight">\(X\)</span> is said to follow the Bernoulli distribution with probability parameter <span class="math notranslate nohighlight">\(p\)</span>, written <span class="math notranslate nohighlight">\(X \sim \text{Bern}(p)\)</span>, and its PMF is given by</p>
<div class="math notranslate nohighlight">
\[
f_X(x) = p^x(1-p)^{(1-x)}.
\]</div>
<p>We can check to see that for any valid value <span class="math notranslate nohighlight">\(x\)</span> in the support of <span class="math notranslate nohighlight">\(X\)</span>—i.e., 1 or 0—, <span class="math notranslate nohighlight">\(f(x)\)</span> gives <span class="math notranslate nohighlight">\(P(X = x)\)</span>.</p>
<p>The <strong>Normal</strong> distribution is extremely common and will be used throughout this book. A random variable <span class="math notranslate nohighlight">\(X\)</span> follows the Normal distribution with mean parameter <span class="math notranslate nohighlight">\(\mu \in \R\)</span> and variance parameter <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>, written <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, if its PDF is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\]</div>
<p>The shape of the Normal random variable’s density function gives this distribution the name “the bell curve”, as shown below. Values closest to <span class="math notranslate nohighlight">\(\mu\)</span> are most likely and the density is symmetric around <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><img alt="normal" src="../Images/d421cf0f88db6f8c4a0271d87de7ab98.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/normal.jpg"/></p>
</div>
<div class="section" id="independence">
<h3>Independence</h3>
<p>So far we’ve discussed the density of individual random variables. The picture can get much more complicated when we want to study the behavior of multiple random variables simultaneously. The assumption of independence simplifies things greatly. Let’s start by defining independence in the discrete case.</p>
<p>Two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if and only if</p>
<div class="math notranslate nohighlight">
\[
P(X = x, Y =y) = P(X = x)P(Y = y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This says that if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, the probability that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> simultaneously is just the product of the probabilities that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> individually.</p>
<p>To generalize this definition to continuous random variables, let’s first introduce <em>joint density function</em>. Quite simply, the joint density of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, written <span class="math notranslate nohighlight">\(f_{X, Y}(x, y)\)</span> gives the probability density of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> evaluated simultaneously at <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively. We can then say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if</p>
<div class="math notranslate nohighlight">
\[
f_{X, Y}(x, y) = f_X(x) f_Y(y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
</div>
&#13;

<h3>Random Variables</h3>
<p>A <strong>random variable</strong> is a variable whose value is randomly determined. The set of possible values a random variable can take on is called the variable’s <strong>support</strong>. An example of a random variable is the value on a die roll. This variable’s support is <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Random variables will be represented with uppercase letters and values in their support with lowercase letters. For instance <span class="math notranslate nohighlight">\(X = x\)</span> implies that a random variable <span class="math notranslate nohighlight">\(X\)</span> happened to take on value <span class="math notranslate nohighlight">\(x\)</span>. Letting <span class="math notranslate nohighlight">\(X\)</span> be the value of a die roll, <span class="math notranslate nohighlight">\(X = 4\)</span> indicates that the die landed on 4.</p>
&#13;

<h3>Density Functions</h3>
<p>The likelihood that a random variable takes on a given value is determined through its density function. For a discrete random variable (one that can take on a finite set of values), this density function is called the <strong>probability mass function</strong> <strong>(PMF)</strong>. The PMF of a random variable <span class="math notranslate nohighlight">\(X\)</span> gives the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal some value <span class="math notranslate nohighlight">\(x\)</span>. We write it as <span class="math notranslate nohighlight">\(f_X(x)\)</span> or just <span class="math notranslate nohighlight">\(f(x)\)</span>, and it is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = P(X = x).
\]</div>
<p>For a continuous random variable (one that can take on infinitely many values), the density function is called the <strong>probability density function (PDF)</strong>. The PDF <span class="math notranslate nohighlight">\(f_X(x)\)</span> of a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> does not give <span class="math notranslate nohighlight">\(P(X = x)\)</span> but it does determine the probability that <span class="math notranslate nohighlight">\(X\)</span> lands in a certain range. Specifically,</p>
<div class="math notranslate nohighlight">
\[
P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. 
\]</div>
<p>That is, integrating <span class="math notranslate nohighlight">\(f(x)\)</span> over a certain range gives the probability of <span class="math notranslate nohighlight">\(X\)</span> being in that range. While <span class="math notranslate nohighlight">\(f(x)\)</span> does not give the probability that <span class="math notranslate nohighlight">\(X\)</span> will equal a certain value, it does indicate the relative likelihood that it will be <em>around</em> that value. E.g. if <span class="math notranslate nohighlight">\(f(a) &gt; f(b)\)</span>, we can say <span class="math notranslate nohighlight">\(X\)</span> is more likely to be in an arbitrarily small area around the value <span class="math notranslate nohighlight">\(a\)</span> than around the value <span class="math notranslate nohighlight">\(b\)</span>.</p>
&#13;

<h3>Distributions</h3>
<p>A random variable’s <strong>distribution</strong> is determined by its density function. Variables with the same density function are said to follow the same distributions. Certain families of distributions are very common in probability and machine learning. Two examples are given below.</p>
<p>The <strong>Bernoulli</strong> distribution is the most simple probability distribution and it describes the likelihood of the outcomes of a binary event. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable that equals 1 (representing “success”) with probability <span class="math notranslate nohighlight">\(p\)</span> and 0 (representing “failure”) with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Then, <span class="math notranslate nohighlight">\(X\)</span> is said to follow the Bernoulli distribution with probability parameter <span class="math notranslate nohighlight">\(p\)</span>, written <span class="math notranslate nohighlight">\(X \sim \text{Bern}(p)\)</span>, and its PMF is given by</p>
<div class="math notranslate nohighlight">
\[
f_X(x) = p^x(1-p)^{(1-x)}.
\]</div>
<p>We can check to see that for any valid value <span class="math notranslate nohighlight">\(x\)</span> in the support of <span class="math notranslate nohighlight">\(X\)</span>—i.e., 1 or 0—, <span class="math notranslate nohighlight">\(f(x)\)</span> gives <span class="math notranslate nohighlight">\(P(X = x)\)</span>.</p>
<p>The <strong>Normal</strong> distribution is extremely common and will be used throughout this book. A random variable <span class="math notranslate nohighlight">\(X\)</span> follows the Normal distribution with mean parameter <span class="math notranslate nohighlight">\(\mu \in \R\)</span> and variance parameter <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>, written <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, if its PDF is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.
\]</div>
<p>The shape of the Normal random variable’s density function gives this distribution the name “the bell curve”, as shown below. Values closest to <span class="math notranslate nohighlight">\(\mu\)</span> are most likely and the density is symmetric around <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p><img alt="normal" src="../Images/d421cf0f88db6f8c4a0271d87de7ab98.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/normal.jpg"/></p>
&#13;

<h3>Independence</h3>
<p>So far we’ve discussed the density of individual random variables. The picture can get much more complicated when we want to study the behavior of multiple random variables simultaneously. The assumption of independence simplifies things greatly. Let’s start by defining independence in the discrete case.</p>
<p>Two discrete random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if and only if</p>
<div class="math notranslate nohighlight">
\[
P(X = x, Y =y) = P(X = x)P(Y = y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This says that if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, the probability that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> simultaneously is just the product of the probabilities that <span class="math notranslate nohighlight">\(X = x\)</span> and <span class="math notranslate nohighlight">\(Y = y\)</span> individually.</p>
<p>To generalize this definition to continuous random variables, let’s first introduce <em>joint density function</em>. Quite simply, the joint density of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, written <span class="math notranslate nohighlight">\(f_{X, Y}(x, y)\)</span> gives the probability density of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> evaluated simultaneously at <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively. We can then say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if</p>
<div class="math notranslate nohighlight">
\[
f_{X, Y}(x, y) = f_X(x) f_Y(y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
&#13;

<h2>2. Maximum Likelihood Estimation</h2>
<p>Maximum likelihood estimation is used to understand the parameters of a distribution that gave rise to observed data. In order to model a data generating process, we often assume it comes from some family of distributions, such as the Bernoulli or Normal distributions. These distributions are indexed by certain parameters (<span class="math notranslate nohighlight">\(p\)</span> for the Bernoulli and <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> for the Normal)—maximum likelihood estimation evaluates which parameters would be most consistent with the data we observed.</p>
<p>Specifically, maximum likelihood estimation finds the values of unknown parameters that maximize the probability of observing the data we did. Basic maximum likelihood estimation can be broken into three steps:</p>
<ol class="simple">
<li><p>Find the joint density of the observed data, also called the <em>likelihood</em></p></li>
<li><p>Take the log of the likelihood, giving the <em>log-likelihood</em>.</p></li>
<li><p>Find the value of the parameter that maximizes the log-likelihood (and therefore the likelihood as well) by setting its derivative equal to 0.</p></li>
</ol>
<p>Finding the value of the parameter to maximize the log-likelihood rather than the likelihood makes the math easier and gives us the same solution.</p>
<p>Let’s go through an example. Suppose we are interested in calculating the average weight of a Chihuahua. We assume the weight of any given Chihuahua is <em>independently</em> distributed Normally with <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> but an unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>. So, we gather 10 Chihuahuas and weigh them. Denote the <span class="math notranslate nohighlight">\(j^\text{th}\)</span> Chihuahua weight with <span class="math notranslate nohighlight">\(W_j \sim \mathcal{N}(\mu, 1)\)</span>.  For step 1, let’s calculate the probability density of our data (i.e., the 10 Chihuahua weights). Since the weights are assumed to be independent, the densities multiply. Letting <span class="math notranslate nohighlight">\(L(\mu)\)</span> be the likelihood of <span class="math notranslate nohighlight">\(\mu\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
L(\mu) &amp;= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10}) \\
&amp;= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10})  \\
&amp;= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot 1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\
&amp;\propto \exp\left(-\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\
\end{align}
\end{split}\]</div>
<p>Note that we can work up to a constant of proportionality since the value of <span class="math notranslate nohighlight">\(\mu\)</span> that maximizes <span class="math notranslate nohighlight">\(L(\mu)\)</span> will also maximize anything proportional to <span class="math notranslate nohighlight">\(L(\mu)\)</span>. For step 2, take the log:</p>
<div class="math notranslate nohighlight">
\[
\log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is some constant. For step 3, take the derivative:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j - \mu).
\end{align}
\]</div>
<p>Setting this equal to 0, we find that the (log) likelihood is maximized with</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. 
\]</div>
<p>We put a hat over <span class="math notranslate nohighlight">\(\mu\)</span> to indicate that it is our <em>estimate</em> of the true <span class="math notranslate nohighlight">\(\mu\)</span>. Note the sensible result—we estimate the true mean of the Chihuahua weight distribution to be the sample mean of our observed data.</p>
&#13;

<h2>3. Conditional Probability</h2>
<p>Probabilistic machine learning methods typically consider the distribution of a target variable conditional on the value of one or more predictor variables. To understand these methods, let’s introduce some of the basic principles of conditional probability.</p>
<p>Consider two events, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. The <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is the probability that <span class="math notranslate nohighlight">\(A\)</span> occurs given <span class="math notranslate nohighlight">\(B\)</span> occurs, written <span class="math notranslate nohighlight">\(P(A|B)\)</span>. Closely related is the <strong>joint probability</strong> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, or the probability that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur, written <span class="math notranslate nohighlight">\(P(A, B)\)</span>. We navigate between the conditional and joint probability with the following</p>
<div class="math notranslate nohighlight">
\[
P(A, B) = P(A|B)P(B).
\]</div>
<p>The above equation leads to an extremely important principle in conditional probability: Bayes’ rule. <strong>Bayes’ rule</strong> states that</p>
<div class="math notranslate nohighlight">
\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
\]</div>
<p>Both of the above expressions work for random variables as well as events. For any two discrete random variables, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P(X = x, Y = y) &amp;= P(X = x|Y = y)P(Y = y) \\
P(X = x|Y = y) &amp;= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}.
\end{align}
\end{split}\]</div>
<p>The same is true for continuous random variables, replacing the PMFs with PDFs.</p>
    
</body>
</html>