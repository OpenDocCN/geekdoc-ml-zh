- en: Object Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测
- en: '![](../media/file848.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file848.jpg)'
- en: '*DALL·E prompt - A cover image for an ‘Object Detection’ chapter in a Raspberry
    Pi tutorial, designed in the same vintage 1950s electronics lab style as previous
    covers. The scene should prominently feature wheels and cubes, similar to those
    provided by the user, placed on a workbench in the foreground. A Raspberry Pi
    with a connected camera module should be capturing an image of these objects.
    Surround the scene with classic lab tools like soldering irons, resistors, and
    wires. The lab background should include vintage equipment like oscilloscopes
    and tube radios, maintaining the detailed and nostalgic feel of the era. No text
    or logos should be included.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 提示 - 一张用于树莓派教程中“目标检测”章节的封面图像，设计风格与之前的封面相同，采用1950年代电子实验室的风格。场景应突出显示轮子和立方体，类似于用户提供的那些，放置在前景的工作台上。一个连接了摄像头模块的树莓派应该正在捕捉这些对象的图像。周围环绕着经典的实验室工具，如烙铁、电阻和电线。实验室背景应包括示波器和管式收音机等复古设备，保持时代的详细和怀旧感。不应包含任何文本或标志。*'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'Building upon our exploration of image classification, we now turn our attention
    to a more advanced computer vision task: object detection. While image classification
    assigns a single label to an entire image, object detection goes further by identifying
    and locating multiple objects within a single image. This capability opens up
    many new applications and challenges, particularly in edge computing and IoT devices
    like the Raspberry Pi.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索图像分类的基础上，我们现在将注意力转向一个更高级的计算机视觉任务：目标检测。虽然图像分类将单个标签分配给整个图像，但目标检测通过在单个图像中识别和定位多个对象，更进一步。这种能力为许多新的应用和挑战打开了大门，尤其是在边缘计算和物联网设备（如树莓派）中。
- en: Object detection combines the tasks of classification and localization. It not
    only determines what objects are present in an image but also pinpoints their
    locations by, for example, drawing bounding boxes around them. This added complexity
    makes object detection a more powerful tool for understanding visual scenes, but
    it also requires more sophisticated models and training techniques.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测结合了分类和定位的任务。它不仅确定图像中存在哪些对象，而且通过例如围绕它们绘制边界框等方式，确定它们的位置。这种额外的复杂性使得目标检测成为理解视觉场景的更强大工具，但也需要更复杂的模型和训练技术。
- en: In edge AI, where we work with constrained computational resources, implementing
    efficient object detection models becomes crucial. The challenges we faced with
    image classification—balancing model size, inference speed, and accuracy—are amplified
    in object detection. However, the rewards are also more significant, as object
    detection enables more nuanced and detailed visual data analysis.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘人工智能领域，我们与受限的计算资源一起工作，实现高效的目标检测模型变得至关重要。我们在图像分类中面临的一些挑战——平衡模型大小、推理速度和准确性——在目标检测中得到了放大。然而，回报也更加显著，因为目标检测能够实现更细致和详细的可视数据分析。
- en: 'Some applications of object detection on edge devices include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘设备上目标检测的一些应用包括：
- en: Surveillance and security systems
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控和安全系统
- en: Autonomous vehicles and drones
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车和无人机
- en: Industrial quality control
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业质量控制
- en: Wildlife monitoring
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 野生动物监测
- en: Augmented reality applications
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增强现实应用
- en: 'As we put our hands into object detection, we’ll build upon the concepts and
    techniques we explored in image classification. We’ll examine popular object detection
    architectures designed for efficiency, such as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们着手目标检测时，我们将基于我们在图像分类中探索的概念和技术。我们将检查为效率而设计的流行目标检测架构，例如：
- en: Single Stage Detectors, such as MobileNet and EfficientDet,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单阶段检测器，如 MobileNet 和 EfficientDet，
- en: FOMO (Faster Objects, More Objects), and
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FOMO (更快的目标，更多的目标)，以及
- en: YOLO (You Only Look Once).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO (仅看一次)。
- en: To learn more about object detection models, follow the tutorial [A Gentle Introduction
    to Object Recognition With Deep Learning](https://machinelearningmastery.com/object-recognition-with-deep-learning/).
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要了解更多关于目标检测模型的信息，请遵循教程 [使用深度学习轻松入门目标识别](https://machinelearningmastery.com/object-recognition-with-deep-learning/)。
- en: We will explore those object detection models using
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用
- en: TensorFlow Lite Runtime (now changed to [LiteRT](https://ai.google.dev/edge/litert)),
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Lite 运行时（现在更名为 [LiteRT](https://ai.google.dev/edge/litert)），
- en: Edge Impulse Linux Python SDK and
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edge Impulse Linux Python SDK 和
- en: Ultralytics
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ultralytics
- en: '![](../media/file849.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file849.png)'
- en: Throughout this lab, we’ll cover the fundamentals of object detection and how
    it differs from image classification. We’ll also learn how to train, fine-tune,
    test, optimize, and deploy popular object detection architectures using a dataset
    created from scratch.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个实验室中，我们将涵盖目标检测的基础知识以及它与图像分类的区别。我们还将学习如何使用从头创建的数据集训练、微调、测试、优化和部署流行的目标检测架构。
- en: Object Detection Fundamentals
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测基础知识
- en: 'Object detection builds upon the foundations of image classification but extends
    its capabilities significantly. To understand object detection, it’s crucial first
    to recognize its key differences from image classification:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测建立在图像分类的基础上，但显著扩展了其功能。要理解目标检测，首先识别它与图像分类的关键区别至关重要：
- en: Image Classification vs. Object Detection
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像分类与目标检测的比较
- en: '**Image Classification**:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像分类**：'
- en: Assigns a single label to an entire image
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为整个图像分配一个标签
- en: 'Answers the question: “What is this image’s primary object or scene?”'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答问题：“这张图片的主要物体或场景是什么？”
- en: Outputs a single class prediction for the whole image
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为整个图像输出一个类别预测
- en: '**Object Detection**:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标检测**：'
- en: Identifies and locates multiple objects within an image
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像中识别和定位多个物体
- en: 'Answers the questions: “What objects are in this image, and where are they
    located?”'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答问题：“这张图片中有什么物体，它们在哪里？”
- en: Outputs multiple predictions, each consisting of a class label and a bounding
    box
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出多个预测，每个预测包括一个类别标签和一个边界框
- en: 'To visualize this difference, let’s consider an example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这种差异，让我们考虑一个例子：
- en: '![](../media/file850.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file850.jpg)'
- en: 'This diagram illustrates the critical difference: image classification provides
    a single label for the entire image, while object detection identifies multiple
    objects, their classes, and their locations within the image.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此图说明了关键的区别：图像分类为整个图像提供单个标签，而目标检测识别图像中的多个物体、它们的类别和位置。
- en: Key Components of Object Detection
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标检测的关键组件
- en: 'Object detection systems typically consist of two main components:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测系统通常由两个主要组件组成：
- en: 'Object Localization: This component identifies where objects are located in
    the image. It typically outputs bounding boxes, rectangular regions encompassing
    each detected object.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物体定位：此组件识别物体在图像中的位置。它通常输出边界框，即包含每个检测到的物体的矩形区域。
- en: 'Object Classification: This component determines the class or category of each
    detected object, similar to image classification but applied to each localized
    region.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物体分类：此组件确定每个检测到的物体的类别或类别，类似于图像分类，但应用于每个定位区域。
- en: Challenges in Object Detection
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标检测的挑战
- en: 'Object detection presents several challenges beyond those of image classification:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测除了图像分类的挑战之外，还提出了几个挑战：
- en: 'Multiple objects: An image may contain multiple objects of various classes,
    sizes, and positions.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个物体：一张图片可能包含多个不同类别、大小和位置的物体。
- en: 'Varying scales: Objects can appear at different sizes within the image.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变化的尺度：物体可以在图像中以不同的尺寸出现。
- en: 'Occlusion: Objects may be partially hidden or overlapping.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮挡：物体可能部分被隐藏或重叠。
- en: 'Background clutter: Distinguishing objects from complex backgrounds can be
    challenging.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背景杂乱：从复杂背景中区分物体可能具有挑战性。
- en: 'Real-time performance: Many applications require fast inference times, especially
    on edge devices.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时性能：许多应用需要快速的推理时间，尤其是在边缘设备上。
- en: Approaches to Object Detection
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标检测的方法
- en: 'There are two main approaches to object detection:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测主要有两种方法：
- en: 'Two-stage detectors: These first propose regions of interest and then classify
    each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双阶段检测器：这些首先提出感兴趣区域，然后对每个区域进行分类。例如包括R-CNN及其变体（Fast R-CNN、Faster R-CNN）。
- en: 'Single-stage detectors: These predict bounding boxes (or centroids) and class
    probabilities in one forward pass of the network. Examples include YOLO (You Only
    Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects,
    More Objects). These are often faster and more suitable for edge devices like
    Raspberry Pi.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单阶段检测器：这些在网络的单一前向传递中预测边界框（或质心）和类别概率。例如包括YOLO（You Only Look Once）、EfficientDet、SSD（Single
    Shot Detector）和FOMO（Faster Objects, More Objects）。这些通常更快，更适合像Raspberry Pi这样的边缘设备。
- en: Evaluation Metrics
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'Object detection uses different metrics compared to image classification:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像分类相比，目标检测使用不同的指标：
- en: '**Intersection over Union (IoU)**: Measures the overlap between predicted and
    ground truth bounding boxes.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交并比 (IoU)**: 衡量预测框和真实框的重叠程度。'
- en: '**Mean Average Precision (mAP)**: Combines precision and recall across all
    classes and IoU thresholds.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均精度 (mAP)**: 结合所有类别和 IoU 阈值的精度和召回率。'
- en: '**Frames Per Second (FPS)**: Measures detection speed, crucial for real-time
    applications on edge devices.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每秒帧数 (FPS)**: 衡量检测速度，对于边缘设备上的实时应用至关重要。'
- en: Pre-Trained Object Detection Models Overview
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练目标检测模型概述
- en: As we saw in the introduction, given an image or a video stream, an object detection
    model can identify which of a known set of objects might be present and provide
    information about their positions within the image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中看到的，给定一张图片或视频流，目标检测模型可以识别可能存在的已知对象集合中的哪些对象，并提供它们在图像中的位置信息。
- en: You can test some common models online by visiting [Object Detection - MediaPipe
    Studio](https://mediapipe-studio.webapps.google.com/studio/demo/object_detector)
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过访问 [Object Detection - MediaPipe Studio](https://mediapipe-studio.webapps.google.com/studio/demo/object_detector)
    在线测试一些常见的模型。
- en: On [Kaggle](https://www.kaggle.com/models?id=298,130,299), we can find the most
    common pre-trained tflite models to use with the Raspi, [ssd_mobilenet_v1,](https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v1/tfLite)
    and [EfficientDet](https://www.kaggle.com/models/tensorflow/efficientdet/tfLite).
    Those models were trained on the COCO (Common Objects in Context) dataset, with
    over 200,000 labeled images in 91 categories. Go, download the models, and upload
    them to the `./models` folder in the Raspi.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Kaggle](https://www.kaggle.com/models?id=298,130,299)，我们可以找到与 Raspi 一起使用的最常用的预训练
    tflite 模型，[ssd_mobilenet_v1,](https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v1/tfLite)
    和 [EfficientDet](https://www.kaggle.com/models/tensorflow/efficientdet/tfLite)。这些模型是在
    COCO (Common Objects in Context) 数据集上训练的，包含 91 个类别中超过 200,000 张标记的图片。去下载这些模型，并将它们上传到
    Raspi 的 `./models` 文件夹中。
- en: Alternatively[,](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models)
    you can find the models and the COCO labels on [GitHub](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models).
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 或者[,](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models)
    你可以在 [GitHub](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models)
    上找到模型和 COCO 标签。
- en: For the first part of this lab, we will focus on a pre-trained <semantics><mrow><mn>300</mn><mo>×</mo><mn>300</mn></mrow><annotation
    encoding="application/x-tex">300\times 300</annotation></semantics> SSD-Mobilenet
    V1 model and compare it with the <semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">320\times 320</annotation></semantics> EfficientDet-lite0,
    also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow
    Lite format (4.2 MB for the SSD Mobilenet and 4.6 MB for the EfficientDet).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验室的第一部分，我们将专注于一个预训练的 <semantics><mrow><mn>300</mn><mo>×</mo><mn>300</mn></mrow><annotation
    encoding="application/x-tex">300\times 300</annotation></semantics> SSD-Mobilenet
    V1 模型，并将其与 <semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">320\times 320</annotation></semantics> EfficientDet-lite0
    进行比较，后者也是使用 COCO 2017 数据集训练的。这两个模型都转换为 TensorFlow Lite 格式（SSD Mobilenet 为 4.2
    MB，EfficientDet 为 4.6 MB）。
- en: SSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once
    the V1 TFLite model is publicly available, we will use it for this overview.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SSD-Mobilenet V2 或 V3 建议用于迁移学习项目，但一旦 V1 TFLite 模型公开可用，我们将使用它进行此概述。
- en: '![](../media/file851.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file851.png)'
- en: Setting Up the TFLite Environment
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 TFLite 环境
- en: 'We should confirm the steps done on the last Hands-On Lab, Image Classification,
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该确认在上一节“动手实验室”中完成的步骤，如下所示：
- en: Updating the Raspberry Pi
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新 Raspberry Pi
- en: Installing Required Libraries
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装所需库
- en: Setting up a Virtual Environment (Optional but Recommended)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置虚拟环境（可选但推荐）
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Installing TensorFlow Lite Runtime
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 TensorFlow Lite 运行时
- en: Installing Additional Python Libraries (inside the environment)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在环境中安装额外的 Python 库
- en: 'Creating a Working Directory:'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建工作目录：
- en: 'Considering that we have created the `Documents/TFLITE` folder in the last
    Lab, let’s now create the specific folders for this object detection lab:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们在上一个实验室中创建了 `Documents/TFLITE` 文件夹，现在让我们为这个目标检测实验室创建特定的文件夹：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Inference and Post-Processing
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理和后处理
- en: 'Let’s start a new [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb)
    to follow all the steps to detect objects on an image:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一个新的 [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb)，以遵循检测图像上的所有步骤：
- en: 'Import the needed libraries:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Load the TFLite model and allocate tensors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 加载TFLite模型并分配张量：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Get input and output tensors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 获取输入和输出张量。
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Input details** will inform us how the model should be fed with an image.
    The shape of `(1, 300, 300, 3)` with a dtype of `uint8` tells us that a non-normalized
    (pixel value range from 0 to 255) image with dimensions <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>300</mn><mo>×</mo><mn>300</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(300\times
    300\times 3)</annotation></semantics> should be input one by one (Batch Dimension:
    1).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入细节**将告诉我们模型应该如何用图像进行喂养。形状为`(1, 300, 300, 3)`，数据类型为`uint8`告诉我们应该输入一个非归一化（像素值范围从0到255）的图像，其尺寸为<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>300</mn><mo>×</mo><mn>300</mn><mo>×</mo><mn>3</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(300\times
    300\times 3)</annotation></semantics>，逐个输入（批处理维度：1）。'
- en: The **output details** include not only the labels (“classes”) and probabilities
    (“scores”) but also the relative window position of the bounding boxes (“boxes”)
    about where the object is located on the image and the number of detected objects
    (“num_detections”). The output details also tell us that the model can detect
    a **maximum of 10 objects** in the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出细节**不仅包括标签（“类别”）和概率（“分数”），还包括边界框（“boxes”）相对于图像中对象位置的相对窗口位置以及检测到的对象数量（“num_detections”）。输出细节还告诉我们模型可以在图像中检测到**最多10个对象**。'
- en: '![](../media/file852.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file852.png)'
- en: So, for the above example, using the same cat image used with the *Image Classification
    Lab* looking for the output, we have a **76% probability** of having found an
    object with a **class ID of 16** on an area delimited by a **bounding box of [0.028011084,
    0.020121813, 0.9886069, 0.802299]**. Those four numbers are related to `ymin`,
    `xmin`, `ymax` and `xmax`, the box coordinates.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于上述示例，使用与*Image Classification Lab*相同的猫图像来寻找输出，我们有一个**76%的概率**在由**边界框[0.028011084,
    0.020121813, 0.9886069, 0.802299]**定义的区域中找到了一个**类别ID为16**的对象。这四个数字与`ymin`、`xmin`、`ymax`和`xmax`有关，即框坐标。
- en: 'Taking into consideration that **y** goes from the top `(ymin`) to the bottom
    (`ymax`) and **x** goes from left (`xmin`) to the right (`xmax`), we have, in
    fact, the coordinates of the top/left corner and the bottom/right one. With both
    edges and knowing the shape of the picture, it is possible to draw a rectangle
    around the object, as shown in the figure below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到**y**从顶部`(ymin)`到底部(`ymax`)，**x**从左(`xmin`)到右(`xmax`)，实际上我们有的是顶部/左上角的坐标和底部/右下角的坐标。有了两边和图片的形状，我们可以在对象周围画一个矩形，如图所示：
- en: '![](../media/file853.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file853.png)'
- en: Next, we should find what class ID equal to 16 means. Opening the file `coco_labels.txt`,
    as a list, each element has an associated index, and inspecting index 16, we get,
    as expected, `cat`. The probability is the value returning from the score.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该找出类别ID等于16代表什么。打开文件`coco_labels.txt`，作为一个列表，每个元素都有一个关联的索引，检查索引16，我们得到预期的`cat`。概率是分数返回的值。
- en: Let’s now upload some images with multiple objects on it for testing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们上传一些带有多个对象的图像进行测试。
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../media/file854.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file854.png)'
- en: 'Based on the input details, let’s pre-process the image, changing its shape
    and expanding its dimension:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入细节，让我们先对图像进行预处理，改变其形状并扩展其维度：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The new input_data shape is`(1, 300, 300, 3)` with a dtype of `uint8`, which
    is compatible with what the model expects.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`input_data`形状为`(1, 300, 300, 3)`，数据类型为`uint8`，这与模型期望的格式兼容。
- en: 'Using the input_data, let’s run the interpreter, measure the latency, and get
    the output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`input_data`，让我们运行解释器，测量延迟，并获取输出：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With a latency of around 800 ms, we can get 4 distinct outputs:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约800毫秒的延迟下，我们可以获得4个不同的输出：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'On a quick inspection, we can see that the model detected 2 objects with a
    score over 0.5:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查后，我们可以看到模型检测到了2个得分超过0.5的对象：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../media/file855.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file855.png)'
- en: 'And we can also visualize the results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化结果：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../media/file856.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file856.png)'
- en: EfficientDet
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EfficientDet
- en: 'EfficientDet is not technically an SSD (Single Shot Detector) model, but it
    shares some similarities and builds upon ideas from SSD and other object detection
    architectures:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientDet在技术上不是一个SSD（单次检测器）模型，但它与SSD和其他目标检测架构有一些相似之处，并在此基础上构建了想法：
- en: 'EfficientDet:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: EfficientDet：
- en: Developed by Google researchers in 2019
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由Google研究人员于2019年开发
- en: Uses EfficientNet as the backbone network
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用EfficientNet作为骨干网络
- en: Employs a novel bi-directional feature pyramid network (BiFPN)
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用新颖的双向特征金字塔网络（BiFPN）
- en: It uses compound scaling to scale the backbone network and the object detection
    components efficiently.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用复合缩放有效地缩放主干网络和目标检测组件。
- en: 'Similarities to SSD:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与SSD的相似之处：
- en: Both are single-stage detectors, meaning they perform object localization and
    classification in a single forward pass.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都是单阶段检测器，这意味着它们在单次前向传递中执行对象定位和分类。
- en: Both use multi-scale feature maps to detect objects at different scales.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都使用多尺度特征图来检测不同尺度的对象。
- en: 'Key differences:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关键区别：
- en: 'Backbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主干网络：SSD通常使用VGG或MobileNet，而EfficientDet使用EfficientNet。
- en: 'Feature fusion: SSD uses a simple feature pyramid, while EfficientDet uses
    the more advanced BiFPN.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征融合：SSD使用简单的特征金字塔，而EfficientDet使用更高级的BiFPN。
- en: 'Scaling method: EfficientDet introduces compound scaling for all components
    of the network'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放方法：EfficientDet为网络的所有组件引入了复合缩放
- en: 'Advantages of EfficientDet:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: EfficientDet的优势：
- en: Generally achieves better accuracy-efficiency trade-offs than SSD and many other
    object detection models.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常比SSD和许多其他目标检测模型实现更好的精度-效率权衡。
- en: More flexible scaling allows for a family of models with different size-performance
    trade-offs.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更灵活的缩放允许有一系列具有不同大小-性能权衡的模型。
- en: While EfficientDet is not an SSD model, it can be seen as an evolution of single-stage
    detection architectures, incorporating more advanced techniques to improve efficiency
    and accuracy. When using EfficientDet, we can expect similar output structures
    to SSD (e.g., bounding boxes and class scores).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然EfficientDet不是一个SSD模型，但它可以看作是单阶段检测架构的演变，它结合了更多高级技术以提高效率和准确性。当使用EfficientDet时，我们可以期待得到与SSD类似的输出结构（例如，边界框和类别分数）。
- en: On GitHub, you can find another [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb)
    exploring the EfficientDet model that we did with SSD MobileNet.
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在GitHub上，你可以找到另一个[notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb)，它探索了我们与SSD
    MobileNet一起使用的EfficientDet模型。
- en: Object Detection Project
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测项目
- en: Now, we will develop a complete Image Classification project from data collection
    to training and deployment. As we did with the Image Classification project, the
    trained and converted model will be used for inference.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从数据收集到训练和部署开发一个完整的图像分类项目。就像我们处理图像分类项目一样，训练和转换后的模型将被用于推理。
- en: 'We will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and
    YOLO.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的训练3个模型：SSD-MobileNet V2、FOMO和YOLO。
- en: The Goal
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标
- en: All Machine Learning projects need to start with a goal. Let’s assume we are
    in an industrial facility and must sort and count **wheels** and special **boxes**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习项目都需要从一个目标开始。假设我们在一个工业设施中，必须对**轮子**和特殊的**盒子**进行分类和计数。
- en: '![](../media/file857.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file857.jpg)'
- en: 'In other words, we should perform a multi-label classification, where each
    image can have three classes:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们应该执行一个多标签分类，其中每张图片可以属于三个类别：
- en: Background (no objects)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背景（没有对象）
- en: Box
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盒子
- en: Wheel
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮子
- en: Raw Data Collection
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始数据收集
- en: Once we have defined our Machine Learning project goal, the next and most crucial
    step is collecting the dataset. We can use a phone, the Raspi, or a mix to create
    the raw dataset (with no labels). Let’s use the simple web app on our Raspberry
    Pi to view the `QVGA (320 x 240)` captured images in a browser.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了我们的机器学习项目目标，下一步也是最重要的一步是收集数据集。我们可以使用手机、树莓派或者它们的组合来创建原始数据集（没有标签）。让我们使用树莓派上的简单Web应用在浏览器中查看捕获的`QVGA
    (320 x 240)`图像。
- en: 'From GitHub, get the Python script [get_img_data.py](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/python_scripts/get_img_data.py)
    and open it in the terminal:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub获取Python脚本[get_img_data.py](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/python_scripts/get_img_data.py)，并在终端中打开它：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Access the web interface:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Web界面：
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在树莓派本身（如果你有GUI）：打开Web浏览器并访问`http://localhost:5000`
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:5000/`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一网络上的另一台设备上：打开Web浏览器并访问`http://<raspberry_pi_ip>:5000`（将`<raspberry_pi_ip>`替换为你的树莓派的IP地址）。例如：`http://192.168.4.210:5000/`
- en: '![](../media/file858.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file858.png)'
- en: The Python script creates a web-based interface for capturing and organizing
    image datasets using a Raspberry Pi and its camera. It’s handy for machine learning
    projects that require labeled image data or not, as in our case here.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Python脚本创建了一个基于Web的界面，用于使用树莓派及其摄像头捕获和组织图像数据集。这对于需要标注图像数据或不需要的机器学习项目都很有用，就像我们在这里的情况一样。
- en: Access the web interface from a browser, enter a generic label for the images
    you want to capture, and press `Start Capture`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从浏览器访问Web界面，输入您想要捕获的图像的通用标签，然后按下`开始捕获`。
- en: '![](../media/file859.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file859.png)'
- en: Note that the images to be captured will have multiple labels that should be
    defined later.
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，要捕获的图像将具有多个标签，这些标签将在以后定义。
- en: Use the live preview to position the camera and click `Capture Image` to save
    images under the current label (in this case, `box-wheel`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实时预览定位摄像头，然后点击`捕获图像`以保存当前标签下的图像（在这种情况下，`box-wheel`）。
- en: '![](../media/file860.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file860.png)'
- en: 'When we have enough images, we can press `Stop Capture`. The captured images
    are saved on the folder dataset/box-wheel:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有足够的图像时，我们可以按下`停止捕获`。捕获的图像将保存在文件夹dataset/box-wheel中：
- en: '![](../media/file861.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file861.png)'
- en: Get around 60 images. Try to capture different angles, backgrounds, and light
    conditions. Filezilla can transfer the created raw dataset to your main computer.
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大约获取60张图像。尽量捕捉不同的角度、背景和光照条件。Filezilla可以将创建的原始数据集传输到您的计算机。
- en: Labeling Data
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注数据
- en: The next step in an Object Detect project is to create a labeled dataset. We
    should label the raw dataset images, creating bounding boxes around each picture’s
    objects (box and wheel). We can use labeling tools like [LabelImg,](https://pypi.org/project/labelImg/)
    [CVAT,](https://www.cvat.ai/) [Roboflow,](https://roboflow.com/annotate) or even
    the [Edge Impulse Studio.](https://edgeimpulse.com/) Once we have explored the
    Edge Impulse tool in other labs, let’s use Roboflow here.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在对象检测项目中下一步是创建一个标注的数据集。我们应该标注原始数据集图像，在每个图片的对象周围创建边界框（箱和轮）。我们可以使用标注工具如[LabelImg](https://pypi.org/project/labelImg/)、[CVAT](https://www.cvat.ai/)、[Roboflow](https://roboflow.com/annotate)或甚至[Edge
    Impulse Studio](https://edgeimpulse.com/)。一旦我们在其他实验室探索了Edge Impulse工具，让我们在这里使用Roboflow。
- en: We are using Roboflow (free version) here for two main reasons. 1) We can have
    auto-labeler, and 2) The annotated dataset is available in several formats and
    can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO
    train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset
    on Edge Impulse (Free account), it is not possible to use it for training on other
    platforms.
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在这里使用Roboflow（免费版本）有两个主要原因。1) 我们可以拥有自动标注器，2) 标注的数据集以多种格式提供，并且可以在Edge Impulse
    Studio（我们将用它来训练MobileNet V2和FOMO）和CoLab（YOLOv8训练）上使用，例如。在Edge Impulse（免费账户）上拥有标注的数据集时，无法在其他平台上用于训练。
- en: We should upload the raw dataset to [Roboflow.](https://roboflow.com/) Create
    a free account there and start a new project, for example, (“box-versus-wheel”).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该将原始数据集上传到[Roboflow](https://roboflow.com/)。在那里创建一个免费账户并启动一个新项目，例如（“box-versus-wheel”）。
- en: '![](../media/file862.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file862.png)'
- en: We will not enter in deep details about the Roboflow process once many tutorials
    are available.
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一旦许多教程可用，我们不会深入介绍Roboflow的过程。
- en: Annotate
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标注
- en: Once the project is created and the dataset is uploaded, you should make the
    annotations using the “Auto-Label” Tool. Note that you can also upload images
    with only a background, which should be saved w/o any annotations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建项目并上传数据集，您应该使用“自动标注”工具进行标注。请注意，您还可以上传只有背景的图像，这些图像应保存而不进行任何标注。
- en: '![](../media/file863.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file863.png)'
- en: Once all images are annotated, you should split them into training, validation,
    and testing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有图像都进行了标注，您应该将它们分成训练、验证和测试集。
- en: '![](../media/file864.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file864.png)'
- en: Data Pre-Processing
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据预处理
- en: The last step with the dataset is preprocessing to generate a final version
    for training. Let’s resize all images to <semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">320\times 320</annotation></semantics> and generate
    augmented versions of each image (augmentation) to create new training examples
    from which our model can learn.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的最后一步是预处理，以生成用于训练的最终版本。让我们将所有图像调整大小为<semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation
    encoding="application/x-tex">320\times 320</annotation></semantics>并生成每个图像的增强版本（增强），以从这些新训练示例中创建我们的模型可以学习的新训练示例。
- en: For augmentation, we will rotate the images (+/-15^o), crop, and vary the brightness
    and exposure.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于增强，我们将旋转图片（+/-15^o），裁剪，并调整亮度和曝光。
- en: '![](../media/file865.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file865.png)'
- en: At the end of the process, we will have 153 images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 处理结束后，我们将有153张图片。
- en: '![](../media/file866.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file866.png)'
- en: Now, you should export the annotated dataset in a format that Edge Impulse,
    Ultralytics, and other frameworks/tools understand, for example, `YOLOv8`. Let’s
    download a zipped version of the dataset to our desktop.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该导出Edge Impulse、Ultralytics和其他框架/工具可以理解的标注数据集格式，例如`YOLOv8`。让我们下载数据集的压缩版本到我们的桌面。
- en: '![](../media/file867.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file867.png)'
- en: Here, it is possible to review how the dataset was structured
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以查看数据集是如何构建的。
- en: '![](../media/file868.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file868.png)'
- en: There are 3 separate folders, one for each split (`train`/`test`/`valid`). For
    each of them, there are 2 subfolders, `images`, and `labels`. The pictures are
    stored as **image_id.jpg** and **images_id.txt**, where “image_id” is unique for
    every picture.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有3个独立的文件夹，每个文件夹对应一个分割（`train`/`test`/`valid`）。对于每个文件夹，都有2个子文件夹，`images`和`labels`。图片存储为**image_id.jpg**和**images_id.txt**，其中“image_id”对于每张图片都是唯一的。
- en: The labels file format will be `class_id` `bounding box coordinates`, where
    in our case, class_id will be `0` for `box` and `1` for `wheel`. The numerical
    id (o, 1, 2…) will follow the alphabetical order of the class name.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标签文件格式将是`class_id` `边界框坐标`，在我们的例子中，class_id将为`0`表示`box`，为`1`表示`wheel`。数字ID（o,
    1, 2…）将遵循类名的字母顺序。
- en: 'The `data.yaml` file has info about the dataset as the classes’ names (`names:
    [''box'', ''wheel'']`) following the YOLO format.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`data.yaml`文件包含有关数据集的信息，如类的名称（`names: [''box'', ''wheel'']`），遵循YOLO格式。'
- en: And that’s it! We are ready to start training using the Edge Impulse Studio
    (as we will do in the following step), Ultralytics (as we will when discussing
    YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset
    on the Image Classification lab).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们准备好开始使用Edge Impulse Studio（如我们在下一步中将要做的）进行训练，Ultralytics（如我们在讨论YOLO时将要做的），甚至可以在CoLab上从头开始训练（就像我们在图像分类实验室中对Cifar-10数据集所做的那样）。
- en: The pre-processed dataset can be found at the [Roboflow site](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset).
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预处理后的数据集可以在[Roboflow网站](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset)找到。
- en: Training an SSD MobileNet Model on Edge Impulse Studio
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Edge Impulse Studio上训练SSD MobileNet模型
- en: Go to [Edge Impulse Studio,](https://www.edgeimpulse.com/) enter your credentials
    at **Login** (or create an account), and start a new project.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前往[Edge Impulse Studio](https://www.edgeimpulse.com/)，在**登录**处输入您的凭据（或创建一个账户），并开始一个新项目。
- en: 'Here, you can clone the project developed for this hands-on lab: [Raspi - Object
    Detection](https://studio.edgeimpulse.com/public/515477/live).'
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这里，您可以克隆为这个动手实验室开发的工程：[Raspi - 目标检测](https://studio.edgeimpulse.com/public/515477/live)。
- en: On the Project `Dashboard` tab, go down and on **Project info,** and for Labeling
    method select `Bounding boxes (object detection)`
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目“仪表板”标签页，向下滚动到**项目信息**，并选择标注方法为“边界框（目标检测）”。
- en: Uploading the annotated data
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上传标注数据
- en: On Studio, go to the `Data acquisition` tab, and on the `UPLOAD DATA` section,
    upload from your computer the raw dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在Studio中，转到“数据采集”标签，然后在“上传数据”部分，从您的电脑上传原始数据集。
- en: We can use the option `Select a folder`, choosing, for example, the folder `train`
    in your computer, which contains two sub-folders, `images`, and `labels`. Select
    the `Image label format`, “YOLO TXT”, upload into the category `Training`, and
    press `Upload data`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`选择一个文件夹`选项，例如选择您电脑中的`train`文件夹，它包含两个子文件夹，`images`和`labels`。选择`图像标签格式`，“YOLO
    TXT”，将其上传到`训练`类别，并按`上传数据`。
- en: '![](../media/file869.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file869.png)'
- en: Repeat the process for the test data (upload both folders, test, and validation).
    At the end of the upload process, you should end with the annotated dataset of
    153 images split in the train/test (84%/16%).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为测试数据重复此过程（上传test和validation两个文件夹）。上传过程结束后，您应该得到一个包含153张图片的标注数据集，分为训练/测试（84%/16%）。
- en: Note that labels will be stored at the labels files `0` and `1` , which are
    equivalent to `box` and `wheel`.
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，标签将存储在标签文件`0`和`1`中，它们等同于`box`和`wheel`。
- en: '![](../media/file870.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file870.png)'
- en: The Impulse Design
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Impulse设计
- en: The first thing to define when we enter the `Create impulse` step is to describe
    the target device for deployment. A pop-up window will appear. We will select
    Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入“创建脉冲”步骤时，首先要定义的是描述部署的目标设备。将出现一个弹出窗口。我们将选择Raspberry 4，它是Raspi-Zero和Raspi-5之间的中间设备。
- en: This choice will not interfere with the training; it will only give us an idea
    about the latency of the model on that specific target.
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个选择不会干扰训练；它只会给我们一个关于模型在该特定目标上的延迟的印象。
- en: '![](../media/file871.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file871.png)'
- en: 'In this phase, you should define how to:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，您应该定义如何：
- en: '**Pre-processing** consists of resizing the individual images. In our case,
    the images were pre-processed on Roboflow, to `320x320` , so let’s keep it. The
    resize will not matter here because the images are already squared. If you upload
    a rectangular image, squash it (squared form, without cropping). Afterward, you
    could define if the images are converted from RGB to Grayscale or not.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**包括调整单个图像的大小。在我们的案例中，图像在Roboflow上进行了预处理，调整为`320x320`，所以让我们保持它。由于图像已经是方形的，所以调整大小在这里不会产生影响。如果您上传的是矩形图像，请将其压扁（方形形式，无需裁剪）。之后，您可以定义图像是否从RGB转换为灰度。'
- en: '**Design a Model,** in this case, “Object Detection.”'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设计一个模型**，在这种情况下，是“目标检测”。'
- en: '![](../media/file872.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file872.png)'
- en: Preprocessing all dataset
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理整个数据集
- en: In the section `Image`, select **Color depth** as `RGB`, and press `Save parameters`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在“图像”部分，选择**颜色深度**为`RGB`，然后按“保存参数”。
- en: '![](../media/file873.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file873.png)'
- en: 'The Studio moves automatically to the next section, `Generate features`, where
    all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273
    wheels.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Studio会自动移动到下一个部分，“生成特征”，其中所有样本都将进行预处理，结果生成480个对象：207个框和273个轮子。
- en: '![](../media/file874.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file874.png)'
- en: The feature explorer shows that all samples evidence a good separation after
    the feature generation.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 特征探索器显示，在特征生成后，所有样本都表现出良好的分离。
- en: Model Design, Training, and Test
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型设计、训练和测试
- en: For training, we should select a pre-trained model. Let’s use the **MobileNetV2
    SSD FPN-Lite (320x320 only)** . It is a pre-trained object detection model designed
    to locate up to 10 objects within an image, outputting a bounding box for each
    object detected. The model is around 3.7 MB in size. It supports an RGB input
    at <semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation encoding="application/x-tex">320\times
    320</annotation></semantics> px.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们应该选择一个预训练模型。让我们使用**MobileNetV2 SSD FPN-Lite (320x320 only**)。这是一个预训练的目标检测模型，旨在在图像中定位多达10个对象，并为每个检测到的对象输出一个边界框。该模型大小约为3.7
    MB。它支持<semantics><mrow><mn>320</mn><mo>×</mo><mn>320</mn></mrow><annotation encoding="application/x-tex">320\times
    320</annotation></semantics> px的RGB输入。
- en: 'Regarding the training hyper-parameters, the model will be trained with:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练超参数，模型将使用以下参数进行训练：
- en: 'Epochs: 25'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶段：25
- en: 'Batch size: 32'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小：32
- en: 'Learning Rate: 0.15.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.15。
- en: For validation during training, 20% of the dataset (*validation_dataset*) will
    be spared.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间进行验证时，将保留数据集的20% (*validation_dataset*)。
- en: '![](../media/file875.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file875.png)'
- en: As a result, the model ends with an overall precision score (based on COCO mAP)
    of 88.8%, higher than the result when using the test data (83.3%).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该模型的总体精度得分（基于COCO mAP）为88.8%，高于使用测试数据时的结果（83.3%）。
- en: Deploying the model
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'We have two ways to deploy our model:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种方式来部署我们的模型：
- en: '**TFLite model**, which lets deploy the trained model as `.tflite` for the
    Raspi to run it using Python.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TFLite模型**，它允许将训练好的模型部署为`.tflite`格式，以便Raspi使用Python运行。'
- en: '**Linux (AARCH64)**, a binary for Linux (AARCH64), implements the Edge Impulse
    Linux protocol, which lets us run our models on any Linux-based development board,
    with SDKs for Python, for example. See the documentation for more information
    and [setup instructions](https://docs.edgeimpulse.com/docs/edge-impulse-for-linux).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linux (AARCH64**)，一个适用于Linux (AARCH64)的二进制文件，实现了Edge Impulse Linux协议，使我们能够在任何基于Linux的开发板上运行我们的模型，例如，提供Python的SDK。有关更多信息，请参阅文档和[设置说明](https://docs.edgeimpulse.com/docs/edge-impulse-for-linux)。'
- en: 'Let’s deploy the **TFLite model**. On the `Dashboard` tab, go to Transfer learning
    model (int8 quantized) and click on the download icon:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署**TFLite模型**。在“仪表板”选项卡中，转到迁移学习模型（int8量化）并点击下载图标：
- en: '![](../media/file876.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file876.png)'
- en: Transfer the model from your computer to the Raspi folder`./models` and capture
    or get some images for inference and save them in the folder `./images`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型从您的计算机传输到Raspi文件夹`./models`，并捕获或获取一些用于推理的图像，并将它们保存在文件夹`./images`中。
- en: Inference and Post-Processing
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理和后处理
- en: The inference can be made as discussed in the *Pre-Trained Object Detection
    Models Overview*. Let’s start a new [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb)
    to follow all the steps to detect cubes and wheels on an image.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 推理可以按照*预训练目标检测模型概述*中讨论的方式进行。让我们开始一个新的[notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb)，以遵循所有步骤在图像上检测立方体和轮子。
- en: 'Import the needed libraries:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE12]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the model path and labels:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型路径和标签：
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Remember that the model will output the class ID as values (0 and 1), following
    an alphabetic order regarding the class names.
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，模型将输出类别ID作为值（0和1），按照类别名称的字母顺序。
- en: 'Load the model, allocate the tensors, and get the input and output tensor details:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型，分配张量，并获取输入和输出张量详情：
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One crucial difference to note is that the `dtype` of the input details of
    the model is now `int8`, which means that the input values go from –128 to +127,
    while each pixel of our raw image goes from 0 to 256\. This means that we should
    pre-process the image to match it. We can check here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个关键区别是，模型输入细节的`dtype`现在是`int8`，这意味着输入值从-128到+127，而我们的原始图像的每个像素值从0到256。这意味着我们应该预处理图像以匹配它。我们可以在以下位置进行检查：
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, let’s open the image and show it:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们打开图像并显示它：
- en: '[PRE17]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../media/file877.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file877.png)'
- en: 'And perform the pre-processing:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 并执行预处理：
- en: '[PRE18]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Checking the input data, we can verify that the input tensor is compatible
    with what is expected by the model:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 检查输入数据，我们可以验证输入张量与模型期望的兼容性：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, it is time to perform the inference. Let’s also calculate the latency
    of the model:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进行推理了。让我们也计算模型的延迟：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The model will take around 600ms to perform the inference in the Raspi-Zero,
    which is around 5 times longer than a Raspi-5.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在Raspi-Zero上执行推理需要大约600毫秒，这大约是Raspi-5的五倍。
- en: Now, we can get the output classes of objects detected, its bounding boxes coordinates,
    and probabilities.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获取检测到的对象的输出类别、边界框坐标和概率。
- en: '[PRE22]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../media/file878.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file878.png)'
- en: 'From the results, we can see that 4 objects were detected: two with class ID
    0 (`box`)and two with class ID 1 (`wheel`), what is correct!'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以看到检测到了4个对象：两个具有类别ID 0（`box`）和两个具有类别ID 1（`wheel`），这是正确的！
- en: Let’s visualize the result for a `threshold` of 0.5
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化`threshold`为0.5的结果
- en: '[PRE24]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../media/file879.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file879.png)'
- en: But what happens if we reduce the threshold to 0.3, for example?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们把阈值降低到0.3，比如呢？
- en: '![](../media/file880.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file880.png)'
- en: We start to see false positives and **multiple detections**, where the model
    detects the same object multiple times with different confidence levels and slightly
    different bounding boxes.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始看到假阳性和**多次检测**，其中模型以不同的置信度和略微不同的边界框多次检测到相同的对象。
- en: Commonly, sometimes, we need to adjust the threshold to smaller values to capture
    all objects, avoiding false negatives, which would lead to multiple detections.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有时我们需要将阈值调整为更小的值以捕获所有对象，避免假阴性，这会导致多次检测。
- en: To improve the detection results, we should implement **Non-Maximum Suppression
    (NMS**), which helps eliminate overlapping bounding boxes and keeps only the most
    confident detection.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高检测结果，我们应该实现**非极大值抑制（NMS**），这有助于消除重叠的边界框，并仅保留最自信的检测。
- en: For that, let’s create a general function named `non_max_suppression()`, with
    the role of refining object detection results by eliminating redundant and overlapping
    bounding boxes. It achieves this by iteratively selecting the detection with the
    highest confidence score and removing other significantly overlapping detections
    based on an Intersection over Union (IoU) threshold.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，让我们创建一个名为`non_max_suppression()`的通用函数，其作用是通过消除冗余和重叠的边界框来细化目标检测结果。它通过迭代选择置信度分数最高的检测，并根据交并比（IoU）阈值移除其他显著重叠的检测来实现这一点。
- en: '[PRE25]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'How it works:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的：
- en: 'Sorting: It starts by sorting all detections by their confidence scores, highest
    to lowest.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排序：首先，根据置信度分数对所有检测进行排序，从高到低。
- en: 'Selection: It selects the highest-scoring box and adds it to the final list
    of detections.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择：它选择得分最高的框并将其添加到最终的检测列表中。
- en: 'Comparison: This selected box is compared with all remaining lower-scoring
    boxes.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较：这个选定的框与所有剩余得分较低的框进行比较。
- en: 'Elimination: Any box that overlaps significantly (above the IoU threshold)
    with the selected box is eliminated.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消除：任何与选定框显著重叠（超过IoU阈值）的框将被消除。
- en: 'Iteration: This process repeats with the next highest-scoring box until all
    boxes are processed.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代：这个过程会重复进行，直到处理完所有框，使用下一个得分最高的框。
- en: 'Now, we can define a more precise visualization function that will take into
    consideration an IoU threshold, detecting only the objects that were selected
    by the `non_max_suppression` function:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个更精确的可视化函数，该函数将考虑IoU阈值，仅检测由`non_max_suppression`函数选定的对象：
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can create a function that will call the others, performing inference
    on any image:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个函数来调用其他函数，对任何图像进行推理：
- en: '[PRE27]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, running the code, having the same image again with a confidence threshold
    of 0.3, but with a small IoU:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行代码，使用相同的图像，但置信度阈值为0.3，并且有一个小的IoU：
- en: '[PRE28]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../media/file881.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file881.png)'
- en: Training a FOMO Model at Edge Impulse Studio
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Edge Impulse Studio中训练FOMO模型
- en: The inference with the SSD MobileNet model worked well, but the latency was
    significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero,
    which means around or less than 1 FPS (1 frame per second). One alternative to
    speed up the process is to use FOMO (Faster Objects, More Objects).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SSD MobileNet模型的推理效果良好，但延迟显著较高。在Raspi-Zero上的推理时间从0.5到1.3秒不等，这意味着大约或低于1 FPS（每秒1帧）。加快处理过程的一个替代方案是使用FOMO（更快的目标，更多目标）。
- en: This novel machine learning algorithm lets us count multiple objects and find
    their location in an image in real-time using up to <semantics><mrow><mn>30</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">30\times</annotation></semantics> less processing
    power and memory than MobileNet SSD or YOLO. The main reason this is possible
    is that while other models calculate the object’s size by drawing a square around
    it (bounding box), FOMO ignores the size of the image, providing only the information
    about where the object is located in the image through its centroid coordinates.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新颖的机器学习算法使我们能够在使用比MobileNet SSD或YOLO少至<semantics><mrow><mn>30</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">30\times</annotation></semantics>的处理能力和内存的情况下，实时地计数多个对象并找到图像中它们的位置。这之所以成为可能，主要原因是其他模型通过围绕对象绘制一个正方形（边界框）来计算对象的大小，而FOMO忽略了图像的大小，仅通过其质心坐标提供有关对象在图像中位置的信息。
- en: How FOMO works?
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FOMO是如何工作的？
- en: In a typical object detection pipeline, the first stage is extracting features
    from the input image. **FOMO leverages MobileNetV2 to perform this task**. MobileNetV2
    processes the input image to produce a feature map that captures essential characteristics,
    such as textures, shapes, and object edges, in a computationally efficient way.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的目标检测管道中，第一阶段是从输入图像中提取特征。**FOMO利用MobileNetV2来完成这项任务**。MobileNetV2处理输入图像以产生一个特征图，以计算有效的方式捕获诸如纹理、形状和对象边缘等基本特征。
- en: '![](../media/file882.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file882.png)'
- en: Once these features are extracted, FOMO’s simpler architecture, focused on center-point
    detection, interprets the feature map to determine where objects are located in
    the image. The output is a grid of cells, where each cell represents whether or
    not an object center is detected. The model outputs one or more confidence scores
    for each cell, indicating the likelihood of an object being present.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取了这些特征，FOMO的更简单架构，专注于中心点检测，将解释特征图以确定图像中对象的位置。输出是一个单元格网格，其中每个单元格表示是否检测到对象中心。该模型为每个单元格输出一个或多个置信度分数，表示对象存在的可能性。
- en: Let’s see how it works on an image.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在图像上的工作情况。
- en: FOMO divides the image into blocks of pixels using a factor of 8\. For the input
    of <semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn></mrow><annotation encoding="application/x-tex">96\times
    96</annotation></semantics>, the grid would be <semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation
    encoding="application/x-tex">12\times 12</annotation></semantics> <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>96</mn><mi>/</mi><mn>8</mn><mo>=</mo><mn>12</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96/8=12)</annotation></semantics>.
    For a <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation encoding="application/x-tex">160\times
    160</annotation></semantics>, the grid will be <semantics><mrow><mn>20</mn><mo>×</mo><mn>20</mn></mrow><annotation
    encoding="application/x-tex">20\times 20</annotation></semantics>, and so on.
    Next, FOMO will run a classifier through each pixel block to calculate the probability
    that there is a box or a wheel in each of them and, subsequently, determine the
    regions that have the highest probability of containing the object (If a pixel
    block has no objects, it will be classified as *background*). From the overlap
    of the final region, the FOMO provides the coordinates (related to the image dimensions)
    of the centroid of this region.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: FOMO使用8的因子将图像划分为像素块。对于输入的<semantics><mrow><mn>96</mn><mo>×</mo><mn>96</mn></mrow><annotation
    encoding="application/x-tex">96\times 96</annotation></semantics>，网格将是<semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation
    encoding="application/x-tex">12\times 12</annotation></semantics> <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>96</mn><mi>/</mi><mn>8</mn><mo>=</mo><mn>12</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96/8=12)</annotation></semantics>。对于<semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics>，网格将是<semantics><mrow><mn>20</mn><mo>×</mo><mn>20</mn></mrow><annotation
    encoding="application/x-tex">20\times 20</annotation></semantics>，依此类推。接下来，FOMO将对每个像素块运行一个分类器来计算其中是否存在一个框或轮子的概率，然后确定具有最高概率包含物体的区域（如果一个像素块没有物体，它将被分类为*背景*）。从最终区域的重叠部分，FOMO提供了该区域质心的坐标（与图像尺寸相关）。
- en: '![](../media/file883.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file883.png)'
- en: '**Trade-off Between Speed and Precision**:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**速度与精度之间的权衡**：'
- en: '**Grid Resolution**: FOMO uses a grid of fixed resolution, meaning each cell
    can detect if an object is present in that part of the image. While it doesn’t
    provide high localization accuracy, it makes a trade-off by being fast and computationally
    light, which is crucial for edge devices.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格分辨率**：FOMO使用固定分辨率的网格，这意味着每个单元格可以检测图像该部分是否存在物体。虽然它不提供高定位精度，但它通过快速和计算轻量来做出权衡，这对于边缘设备至关重要。'
- en: '**Multi-Object Detection**: Since each cell is independent, FOMO can detect
    multiple objects simultaneously in an image by identifying multiple centers.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多目标检测**：由于每个单元格是独立的，FOMO可以通过识别多个中心同时检测图像中的多个物体。'
- en: Impulse Design, new Training and Testing
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冲击设计，新的训练和测试
- en: Return to Edge Impulse Studio, and in the `Experiments` tab, create another
    impulse. Now, the input images should be <semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics> (this is the
    expected input size for MobilenetV2).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 返回Edge Impulse Studio，在“实验”选项卡中创建另一个冲击。现在，输入图像应该是<semantics><mrow><mn>160</mn><mo>×</mo><mn>160</mn></mrow><annotation
    encoding="application/x-tex">160\times 160</annotation></semantics>（这是MobilenetV2预期的输入大小）。
- en: '![](../media/file884.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file884.png)'
- en: On the `Image` tab, generate the features and go to the `Object detection` tab.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在“图像”选项卡上，生成特征并转到“对象检测”选项卡。
- en: We should select a pre-trained model for training. Let’s use the **FOMO (Faster
    Objects, More Objects) MobileNetV2 0.35.**
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择一个预训练的模型进行训练。让我们使用**FOMO (Faster Objects, More Objects) MobileNetV2 0.35**。
- en: '![](../media/file885.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file885.png)'
- en: 'Regarding the training hyper-parameters, the model will be trained with:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练超参数，模型将使用以下参数进行训练：
- en: 'Epochs: 30'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数：30
- en: 'Batch size: 32'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小：32
- en: 'Learning Rate: 0.001.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.001。
- en: For validation during training, 20% of the dataset (*validation_dataset*) will
    be spared. We will not apply Data Augmentation for the remaining 80% (*train_dataset*)
    because our dataset was already augmented during the labeling phase at Roboflow.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间进行验证时，将保留数据集的20% (*validation_dataset*)。对于剩余的80% (*train_dataset*)，我们不会应用数据增强，因为我们的数据集在Roboflow的标注阶段已经进行了增强。
- en: As a result, the model ends with an overall F1 score of 93.3% with an impressive
    latency of 8 ms (Raspi-4), around <semantics><mrow><mn>60</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">60\times</annotation></semantics> less than we got
    with the SSD MovileNetV2.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该模型最终的整体 F1 分数为 93.3%，具有令人印象深刻的延迟 8 毫秒（Raspi-4），大约比我们使用 SSD MovileNetV2 得到的延迟低
    <semantics><mrow><mn>60</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">60\times</annotation></semantics>。
- en: '![](../media/file886.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file886.png)'
- en: Note that FOMO automatically added a third label background to the two previously
    defined *boxes* (0) and *wheels* (1).
  id: totrans-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意 FOMO 自动为之前定义的两个标签 *boxes*（0）和 *wheels*（1）添加了第三个标签背景。
- en: 'On the `Model testing` tab, we can see that the accuracy was 94%. Here is one
    of the test sample results:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在“模型测试”选项卡中，我们可以看到准确率为 94%。以下是其中一个测试样本的结果：
- en: '![](../media/file887.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file887.png)'
- en: In object detection tasks, accuracy is generally not the primary [evaluation
    metric.](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/)
    Object detection involves classifying objects and providing bounding boxes around
    them, making it a more complex problem than simple classification. The issue is
    that we do not have the bounding box, only the centroids. In short, using accuracy
    as a metric could be misleading and may not provide a complete understanding of
    how well the model is performing.
  id: totrans-301
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在目标检测任务中，准确率通常不是主要的[评估指标](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/)。目标检测涉及对物体进行分类并在其周围提供边界框，这使得它比简单的分类更复杂。问题是我们没有边界框，只有质心。简而言之，使用准确率作为指标可能是误导性的，并且可能无法完全理解模型的表现。
- en: Deploying the model
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型
- en: As we did in the previous section, we can deploy the trained model as TFLite
    or Linux (AARCH64). Let’s do it now as **Linux (AARCH64)**, a binary that implements
    the [Edge Impulse Linux](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux)
    protocol.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，我们可以将训练好的模型部署为 TFLite 或 Linux (AARCH64)。现在让我们将其部署为 **Linux (AARCH64**)，这是一个实现
    [Edge Impulse Linux](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux)
    协议的二进制文件。
- en: Edge Impulse for Linux models is delivered in `.eim` format. This [executable](https://docs.edgeimpulse.com/docs/run-inference/linux-eim-executable)
    contains our “full impulse” created in Edge Impulse Studio. The impulse consists
    of the signal processing block(s) and any learning and anomaly block(s) we added
    and trained. It is compiled with optimizations for our processor or GPU (e.g.,
    NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix
    socket).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Impulse for Linux 模型以 `.eim` 格式提供。这个 [可执行文件](https://docs.edgeimpulse.com/docs/run-inference/linux-eim-executable)
    包含我们在 Edge Impulse Studio 中创建的“完整脉冲”。脉冲由信号处理块（s）和任何我们添加并训练的学习和异常块（s）组成。它是针对我们的处理器或
    GPU（例如，ARM 内核上的 NEON 指令）进行优化的，并带有简单的 IPC 层（通过 Unix 套接字）。
- en: At the `Deploy` tab, select the option `Linux (AARCH64)`, the `int8`model and
    press `Build`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在“部署”选项卡中，选择“Linux (AARCH64)”、“int8”模型并按“构建”。
- en: '![](../media/file888.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file888.png)'
- en: The model will be automatically downloaded to your computer.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将自动下载到您的计算机上。
- en: 'On our Raspi, let’s create a new working area:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Raspi 上，让我们创建一个新的工作区域：
- en: '[PRE29]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Rename the model for easy identification:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型重命名以便于识别：
- en: For example, `raspi-object-detection-linux-aarch64-FOMO-int8.eim` and transfer
    it to the new Raspi folder`./models` and capture or get some images for inference
    and save them in the folder `./images`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将 `raspi-object-detection-linux-aarch64-FOMO-int8.eim` 转移到新的 Raspi 文件夹 `./models`
    并捕获或获取一些图像进行推理，并将它们保存在文件夹 `./images` 中。
- en: Inference and Post-Processing
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理和后处理
- en: 'The inference will be made using the [Linux Python SDK](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux/linux-python-sdk).
    This library lets us run machine learning models and collect sensor data on [Linux](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux)
    machines using Python. The SDK is open source and hosted on GitHub: [edgeimpulse/linux-sdk-python](https://github.com/edgeimpulse/linux-sdk-python).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 推理将使用 [Linux Python SDK](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux/linux-python-sdk)
    进行。这个库允许我们使用 Python 在 [Linux](https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux)
    机器上运行机器学习模型并收集传感器数据。SDK 是开源的，托管在 GitHub 上：[edgeimpulse/linux-sdk-python](https://github.com/edgeimpulse/linux-sdk-python)。
- en: Let’s set up a Virtual Environment for working with the Linux Python SDK
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为使用 Linux Python SDK 设置一个虚拟环境
- en: '[PRE30]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And Install the all the libraries needed:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 安装所有需要的库：
- en: '[PRE31]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Permit our model to be executable.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 允许我们的模型可执行。
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Install the Jupiter Notebook on the new environment
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在新环境中安装 Jupiter Notebook
- en: '[PRE33]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Run a notebook locally (on the Raspi-4 or 5 with desktop)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行笔记本（在 Raspi-4 或 5 的桌面上）
- en: '[PRE34]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'or on the browser on your computer:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在你的电脑浏览器上：
- en: '[PRE35]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let’s start a new [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb)
    by following all the steps to detect cubes and wheels on an image using the FOMO
    model and the Edge Impulse Linux Python SDK.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照所有步骤开始一个新的 [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb)，使用
    FOMO 模型和 Edge Impulse Linux Python SDK 在图像上检测立方体和轮子。
- en: 'Import the needed libraries:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE36]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define the model path and labels:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型路径和标签：
- en: '[PRE37]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Remember that the model will output the class ID as values (0 and 1), following
    an alphabetic order regarding the class names.
  id: totrans-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 记住，模型将以值（0 和 1）的形式输出类别 ID，按照类别名称的字母顺序排列。
- en: 'Load and initialize the model:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 加载并初始化模型：
- en: '[PRE38]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `model_info` will contain critical information about our model. However,
    unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare
    the model for inference.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_info` 将包含关于我们模型的临界信息。然而，与 TFLite 解释器不同，EI Linux Python SDK 库现在将为推理准备模型。'
- en: 'So, let’s open the image and show it (Now, for compatibility, we will use OpenCV,
    the CV Library used internally by EI. OpenCV reads the image as BGR, so we will
    need to convert it to RGB :'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们打开图像并显示它（现在，为了兼容性，我们将使用 OpenCV，EI 内部使用的 CV 库。OpenCV 以 BGR 格式读取图像，因此我们需要将其转换为
    RGB：
- en: '[PRE39]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](../media/file889.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file889.png)'
- en: 'Now we will get the features and the preprocessed image (`cropped`) using the
    `runner`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `runner` 获取特征和预处理后的图像（`cropped`）：
- en: '[PRE40]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'And perform the inference. Let’s also calculate the latency of the model:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 并执行推理。让我们也计算模型的延迟：
- en: '[PRE41]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Let’s get the output classes of objects detected, their bounding boxes centroids,
    and probabilities.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取检测到的对象的输出类别、它们的边界框质心和概率。
- en: '[PRE42]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The results show that two objects were detected: one with class ID 0 (`box`)
    and one with class ID 1 (`wheel`), which is correct!'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示检测到两个对象：一个类别 ID 为 0（`box`）和一个类别 ID 为 1（`wheel`），这是正确的！
- en: Let’s visualize the result (The `threshold` is 0.5, the default value set during
    the model testing on the Edge Impulse Studio).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化结果（`阈值` 为 0.5，这是在 Edge Impulse Studio 上对模型进行测试时设置的默认值）。
- en: '[PRE44]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](../media/file890.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file890.png)'
- en: Exploring a YOLO Model using Ultralytics
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ultralytics 探索 YOLO 模型
- en: For this lab, we will explore YOLOv8\. [Ultralytics](https://ultralytics.com/)
    [YOLOv8](https://github.com/ultralytics/ultralytics) is a version of the acclaimed
    real-time object detection and image segmentation model, YOLO. YOLOv8 is built
    on cutting-edge advancements in deep learning and computer vision, offering unparalleled
    performance in terms of speed and accuracy. Its streamlined design makes it suitable
    for various applications and easily adaptable to different hardware platforms,
    from edge devices to cloud APIs.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将探索 YOLOv8\. [Ultralytics](https://ultralytics.com/) [YOLOv8](https://github.com/ultralytics/ultralytics)
    是一个著名的实时目标检测和图像分割模型 YOLO 的版本。YOLOv8 建立在深度学习和计算机视觉的尖端进步之上，在速度和准确性方面提供了无与伦比的性能。其简化的设计使其适用于各种应用，并且易于适应不同的硬件平台，从边缘设备到云
    API。
- en: Talking about the YOLO Model
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 讨论YOLO模型
- en: The YOLO (You Only Look Once) model is a highly efficient and widely used object
    detection algorithm known for its real-time processing capabilities. Unlike traditional
    object detection systems that repurpose classifiers or localizers to perform detection,
    YOLO frames the detection problem as a single regression task. This innovative
    approach enables YOLO to simultaneously predict multiple bounding boxes and their
    class probabilities from full images in one evaluation, significantly boosting
    its speed.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO（你只看一次）模型是一个高效且广泛使用的目标检测算法，以其实时处理能力而闻名。与传统的目标检测系统不同，这些系统将分类器或定位器重新用于执行检测，YOLO
    将检测问题作为一个单一的回归任务来处理。这种创新的方法使得 YOLO 能够在单个评估中同时预测多个边界框及其类别概率，从而显著提高了其速度。
- en: 'Key Features:'
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键特性：
- en: '**Single Network Architecture**:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单网络架构**:'
- en: YOLO employs a single neural network to process the entire image. This network
    divides the image into a grid and, for each grid cell, directly predicts bounding
    boxes and associated class probabilities. This end-to-end training improves speed
    and simplifies the model architecture.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO 使用单个神经网络处理整个图像。这个网络将图像划分为一个网格，并为每个网格单元直接预测边界框和相关类别概率。这种端到端训练提高了速度并简化了模型架构。
- en: '**Real-Time Processing**:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时处理**:'
- en: One of YOLO’s standout features is its ability to perform object detection in
    real-time. Depending on the version and hardware, YOLO can process images at high
    frames per second (FPS). This makes it ideal for applications requiring quick
    and accurate object detection, such as video surveillance, autonomous driving,
    and live sports analysis.
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO的一个突出特点是其实时进行目标检测的能力。根据版本和硬件，YOLO可以以每秒高帧数（FPS）处理图像。这使得它非常适合需要快速和准确目标检测的应用，如视频监控、自动驾驶和现场体育分析。
- en: '**Evolution of Versions**:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**版本演变**：'
- en: Over the years, YOLO has undergone significant improvements, from YOLOv1 to
    the latest YOLOv10\. Each iteration has introduced enhancements in accuracy, speed,
    and efficiency. YOLOv8, for instance, incorporates advancements in network architecture,
    improved training methodologies, and better support for various hardware, ensuring
    a more robust performance.
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几年来，YOLO经历了显著的改进，从YOLOv1到最新的YOLOv10。每一代迭代都在准确性、速度和效率方面引入了改进。例如，YOLOv8结合了网络架构的进步、改进的训练方法和更好的各种硬件支持，确保了更稳健的性能。
- en: Although YOLOv10 is the family’s newest member with an encouraging performance
    based on its paper, it was just released (May 2024) and is not fully integrated
    with the Ultralytics library. Conversely, the precision-recall curve analysis
    suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion
    of true positives while minimizing false positives more effectively (for more
    details, see this [article](https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/)).
    So, this lab is based on the YOLOv8n.
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管YOLOv10是该家族的最新成员，其论文基于令人鼓舞的性能，但它刚刚发布（2024年5月），并且尚未完全集成到Ultralytics库中。相反，精确度-召回率曲线分析表明，YOLOv8通常优于YOLOv9，在捕获更多真实正例的同时，更有效地最小化了误报（更多详情见此[文章](https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/))。因此，本实验室基于YOLOv8n。
- en: '![](../media/file891.jpg)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../media/file891.jpg)'
- en: '**Accuracy and Efficiency**:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准确性和效率**：'
- en: While early versions of YOLO traded off some accuracy for speed, recent versions
    have made substantial strides in balancing both. The newer models are faster and
    more accurate, detecting small objects (such as bees) and performing well on complex
    datasets.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然YOLO的早期版本在速度上做出了一些牺牲以换取准确性，但最近版本在平衡两者方面取得了重大进展。新模型速度更快、准确性更高，能够检测小物体（如蜜蜂）并在复杂数据集上表现良好。
- en: '**Wide Range of Applications**:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**广泛的应用范围**：'
- en: YOLO’s versatility has led to its adoption in numerous fields. It is used in
    traffic monitoring systems to detect and count vehicles, security applications
    to identify potential threats and agricultural technology to monitor crops and
    livestock. Its application extends to any domain requiring efficient and accurate
    object detection.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO的通用性使其在众多领域得到应用。它被用于交通监控系统检测和计数车辆，安全应用中识别潜在威胁，以及农业技术中监测作物和牲畜。其应用范围扩展到任何需要高效和准确目标检测的领域。
- en: '**Community and Development**:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**社区和开发**：'
- en: YOLO continues to evolve and is supported by a strong community of developers
    and researchers (being the YOLOv8 very strong). Open-source implementations and
    extensive documentation have made it accessible for customization and integration
    into various projects. Popular deep learning frameworks like Darknet, TensorFlow,
    and PyTorch support YOLO, further broadening its applicability.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO持续发展，并得到了强大的开发者和研究者社区的支持（YOLOv8尤其强大）。开源实现和广泛的文档使其易于定制和集成到各种项目中。流行的深度学习框架如Darknet、TensorFlow和PyTorch支持YOLO，进一步扩大了其适用性。
- en: '[Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics?tab=readme-ov-file)
    can not only [Detect](https://docs.ultralytics.com/tasks/detect) (our case here)
    but also [Segment](https://docs.ultralytics.com/tasks/segment) and [Pose](https://docs.ultralytics.com/tasks/pose)
    models pre-trained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco)
    dataset and YOLOv8 [Classify](https://docs.ultralytics.com/tasks/classify) models
    pre-trained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet)
    dataset. [Track](https://docs.ultralytics.com/modes/track) mode is available for
    all Detect, Segment, and Pose models.'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics?tab=readme-ov-file)不仅可以[检测](https://docs.ultralytics.com/tasks/detect)（我们这里的案例）还可以[分割](https://docs.ultralytics.com/tasks/segment)和[姿态](https://docs.ultralytics.com/tasks/pose)模型，这些模型在[COCO](https://docs.ultralytics.com/datasets/detect/coco)数据集上预训练，以及YOLOv8在[ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet)数据集上预训练的[分类](https://docs.ultralytics.com/tasks/classify)模型。[跟踪](https://docs.ultralytics.com/modes/track)模式适用于所有检测、分割和姿态模型。'
- en: '![](../media/file892.png)'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../media/file892.png)'
- en: Ultralytics YOLO supported tasks
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Ultralytics YOLO支持的任务
- en: Installation
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装
- en: 'On our Raspi, let’s deactivate the current environment to create a new working
    area:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Raspi上，让我们停用当前环境以创建一个新的工作区域：
- en: '[PRE45]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let’s set up a Virtual Environment for working with the Ultralytics YOLOv8
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为使用Ultralytics YOLOv8设置一个虚拟环境
- en: '[PRE46]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: And install the Ultralytics packages for local inference on the Raspi
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在Raspi上安装Ultralytics的本地推理包
- en: 'Update the packages list, install pip, and upgrade to the latest:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新包列表，安装pip，并升级到最新版本：
- en: '[PRE47]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Install the `ultralytics` pip package with optional dependencies:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用可选依赖项安装`ultralytics` pip包：
- en: '[PRE48]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Reboot the device:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启设备：
- en: '[PRE49]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Testing the YOLO
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试YOLO
- en: After the Raspi-Zero booting, let’s activate the `yolo` env, go to the working
    directory,
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: Raspi-Zero启动后，让我们激活`yolo`环境，进入工作目录，
- en: '[PRE50]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'and run inference on an image that will be downloaded from the Ultralytics
    website, using the YOLOV8n model (the smallest in the family) at the Terminal
    (CLI):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 并在终端（CLI）上运行从Ultralytics网站下载的图像的推理，使用YOLOV8n模型（家族中最小的模型）：
- en: '[PRE51]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The YOLO model family is pre-trained with the COCO dataset.
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: YOLO模型家族使用COCO数据集预训练。
- en: 'The inference result will appear in the terminal. In the image (bus.jpg), 4
    `persons`, 1 `bus,` and 1 `stop signal` were detected:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 推理结果将在终端中显示。在图像（bus.jpg）中，检测到4个`persons`，1个`bus`和1个`stop signal`：
- en: '![](../media/file893.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file893.png)'
- en: 'Also, we got a message that `Results saved to runs/detect/predict`. Inspecting
    that directory, we can see a new image saved (bus.jpg). Let’s download it from
    the Raspi-Zero to our desktop for inspection:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还收到一条消息，`Results saved to runs/detect/predict`。检查该目录，我们可以看到一个新图像已保存（bus.jpg）。让我们从Raspi-Zero下载到我们的桌面进行检查：
- en: '![](../media/file894.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file894.png)'
- en: So, the Ultralytics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero,
    an issue is the high latency for this inference, around 18 seconds, even with
    the most miniature model of the family (YOLOv8n).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Ultralytics YOLO已正确安装在我们的Raspi上。但是，在Raspi-Zero上，这个问题是推理的高延迟，大约18秒，即使是最小巧的模型（YOLOv8n）也是如此。
- en: Export Model to NCNN format
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将模型导出为NCNN格式
- en: Deploying computer vision models on edge devices with limited computational
    power, such as the Raspi-Zero, can cause latency issues. One alternative is to
    use a format optimized for optimal performance. This ensures that even devices
    with limited processing power can handle advanced computer vision tasks well.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算能力有限的边缘设备（如Raspi-Zero）上部署计算机视觉模型可能会导致延迟问题。一个替代方案是使用优化以获得最佳性能的格式。这确保了即使处理能力有限的设备也能很好地处理高级计算机视觉任务。
- en: Of all the model export formats supported by Ultralytics, the [NCNN](https://docs.ultralytics.com/integrations/ncnn)
    is a high-performance neural network inference computing framework optimized for
    mobile platforms. From the beginning of the design, NCNN was deeply considerate
    about deployment and use on mobile phones and did not have third-party dependencies.
    It is cross-platform and runs faster than all known open-source frameworks (such
    as TFLite).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ultralytics支持的所有模型导出格式中，[NCNN](https://docs.ultralytics.com/integrations/ncnn)是一个针对移动平台进行优化的高性能神经网络推理计算框架。从设计之初，NCNN就深入考虑了在手机上的部署和使用，并且没有第三方依赖。它是跨平台的，运行速度比所有已知的开源框架（如TFLite）都要快。
- en: NCNN delivers the best inference performance when working with Raspberry Pi
    devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: NCNN在处理Raspberry Pi设备时提供最佳推理性能。NCNN针对移动嵌入式平台（如ARM架构）进行了高度优化。
- en: 'So, let’s convert our model and rerun the inference:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们转换我们的模型并重新运行推理：
- en: 'Export a YOLOv8n PyTorch model to NCNN format, creating: ‘/yolov8n_ncnn_model’'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将YOLOv8n PyTorch模型导出为NCNN格式，创建：‘/yolov8n_ncnn_model’
- en: '[PRE52]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Run inference with the exported model (now the source could be the bus.jpg
    image that was downloaded from the website to the current directory on the last
    inference):'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导出的模型进行推理（现在源可以是上一次推理下载到当前目录的bus.jpg图像）：
- en: '[PRE53]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The first inference, when the model is loaded, usually has a high latency (around
    17s), but from the 2nd, it is possible to note that the inference goes down to
    around 2s.
  id: totrans-403
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当模型加载时，第一次推理通常具有很高的延迟（大约17秒），但从第二次开始，可以注意到推理时间下降到大约2秒。
- en: Exploring YOLO with Python
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python探索YOLO
- en: 'To start, let’s call the Python Interpreter so we can explore how the YOLO
    model works, line by line:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们调用Python解释器，这样我们可以逐行探索YOLO模型的工作原理：
- en: '[PRE54]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we should call the YOLO library from Ultralytics and load the model:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该从Ultralytics调用YOLO库并加载模型：
- en: '[PRE55]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, run inference over an image (let’s use again `bus.jpg`):'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对图像进行推理（让我们再次使用`bus.jpg`）：
- en: '[PRE56]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](../media/file895.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file895.png)'
- en: We can verify that the result is almost identical to the one we get running
    the inference at the terminal level (CLI), except that the bus stop was not detected
    with the reduced NCNN model. Note that the latency was reduced.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证，结果几乎与我们在终端级别（CLI）运行推理得到的结果相同，只是没有检测到减少的NCNN模型中的公交车站。注意，延迟已经减少。
- en: Let’s analyze the “result” content.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析“result”内容。
- en: 'For example, we can see `result[0].boxes.data`, showing us the main inference
    result, which is a tensor shape (4, 6). Each line is one of the objects detected,
    being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence,
    and the 6th, the class (in this case, `0: person` and `5: bus`):'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，我们可以看到`result[0].boxes.data`，显示主要推理结果，它是一个形状为（4，6）的张量。每一行是检测到的对象之一，前4列是边界框坐标，第5列是置信度，第6列是类别（在这种情况下，`0:
    person`和`5: bus`）：'
- en: '![](../media/file896.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file896.png)'
- en: 'We can access several inference results separately, as the inference time,
    and have it printed in a better format:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以分别访问几个推理结果，以及推理时间，并以更好的格式打印出来：
- en: '[PRE57]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Or we can have the total number of objects detected:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以得到检测到的对象总数：
- en: '[PRE58]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](../media/file897.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file897.png)'
- en: 'With Python, we can create a detailed output that meets our needs (See [Model
    Prediction with Ultralytics YOLO](https://docs.ultralytics.com/modes/predict/)
    for more details). Let’s run a Python script instead of manually entering it line
    by line in the interpreter, as shown below. Let’s use `nano` as our text editor.
    First, we should create an empty Python script named, for example, `yolov8_tests.py`:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python，我们可以创建一个详细输出，满足我们的需求（更多详情请参阅[使用Ultralytics YOLO进行模型预测](https://docs.ultralytics.com/modes/predict/)）。让我们运行一个Python脚本，而不是像下面那样逐行手动输入到解释器中。让我们使用`nano`作为我们的文本编辑器。首先，我们应该创建一个名为，例如，`yolov8_tests.py`的空Python脚本：
- en: '[PRE59]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Enter with the code lines:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以下代码行：
- en: '[PRE60]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](../media/file898.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file898.png)'
- en: 'And enter with the commands: `[CTRL+O]` + `[ENTER]` +`[CTRL+X]` to save the
    Python script.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用以下命令进行输入：`[CTRL+O]` + `[ENTER]` + `[CTRL+X]`来保存Python脚本。
- en: 'Run the script:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本：
- en: '[PRE61]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The result is the same as running the inference at the terminal level (CLI)
    and with the built-in Python interpreter.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与在终端级别（CLI）和内置Python解释器中运行推理的结果相同。
- en: Calling the YOLO library and loading the model for inference for the first time
    takes a long time, but the inferences after that will be much faster. For example,
    the first single inference can take several seconds, but after that, the inference
    time should be reduced to less than 1 second.
  id: totrans-430
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第一次调用YOLO库并加载模型进行推理需要很长时间，但之后的推理将会快得多。例如，第一次单次推理可能需要几秒钟，但之后，推理时间应该减少到不到1秒。
- en: Training YOLOv8 on a Customized Dataset
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在自定义数据集上训练YOLOv8
- en: Return to our “Box versus Wheel” dataset, labeled on [Roboflow](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset).
    On the `Download Dataset`, instead of `Download a zip to computer` option done
    for training on Edge Impulse Studio, we will opt for `Show download code`. This
    option will open a pop-up window with a code snippet that should be pasted into
    our training notebook.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的“箱体与轮子”数据集，该数据集在[Roboflow](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset)上标记。在“下载数据集”中，而不是为Edge
    Impulse Studio训练而执行的“下载到计算机的zip文件”选项，我们将选择“显示下载代码”。此选项将打开一个弹出窗口，其中包含一个代码片段，应将其粘贴到我们的训练笔记本中。
- en: '![](../media/file899.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file899.png)'
- en: 'For training, let’s adapt one of the public examples available from Ultralytics
    and run it on Google Colab. Below, you can find mine to be adapted in your project:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，让我们调整 Ultralytics 提供的公共示例之一，并在 Google Colab 上运行它。下面，你可以找到我的示例，你可以将其应用于你的项目：
- en: YOLOv8 Box versus Wheel Dataset Training [[Open In Colab]](https://colab.research.google.com/github/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb)
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv8 Box 与 Wheel 数据集训练 [[在 Colab 中打开]](https://colab.research.google.com/github/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb)
- en: 'Critical points on the Notebook:'
  id: totrans-436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 笔记本中的关键点：
- en: Run it with GPU (the NVidia T4 is free)
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 GPU 运行它（NVidia T4 是免费的）
- en: Install Ultralytics using PIP.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 PIP 安装 Ultralytics。
- en: '![](../media/file900.png)'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../media/file900.png)'
- en: 'Now, you can import the YOLO and upload your dataset to the CoLab, pasting
    the Download code that we get from Roboflow. Note that our dataset will be mounted
    under `/content/datasets/`:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以导入 YOLO 并将你的数据集上传到 CoLab，粘贴我们从 Roboflow 获得的下载代码。请注意，我们的数据集将挂载在 `/content/datasets/`：
- en: '![](../media/file901.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file901.png)'
- en: It is essential to verify and change the file `data.yaml` with the correct path
    for the images (copy the path on each `images` folder).
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证并更改文件 `data.yaml` 中的图像正确路径（在每个 `images` 文件夹中复制路径）是至关重要的。
- en: '[PRE62]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Define the main hyperparameters that you want to change from default, for example:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你想要从默认值更改的主要超参数，例如：
- en: '[PRE63]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Run the training (using CLI):'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 CLI 运行训练：
- en: '[PRE64]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![](../media/file902.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file902.png)'
- en: image-20240910111319804
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: image-20240910111319804
- en: 'The model took a few minutes to be trained and has an excellent result (mAP50
    of 0.995). At the end of the training, all results are saved in the folder listed,
    for example: `/runs/detect/train/`. There, you can find, for example, the confusion
    matrix.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练了几分钟，并取得了优异的结果（mAP50 为 0.995）。训练结束时，所有结果都保存在列出的文件夹中，例如：`/runs/detect/train/`。在那里，你可以找到，例如，混淆矩阵。
- en: '![](../media/file903.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file903.png)'
- en: Note that the trained model (`best.pt`) is saved in the folder `/runs/detect/train/weights/`.
    Now, you should validate the trained model with the `valid/images`.
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，训练好的模型 (`best.pt`) 保存在文件夹 `/runs/detect/train/weights/` 中。现在，你应该使用 `valid/images`
    验证训练好的模型。
- en: '[PRE65]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The results were similar to training.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与训练相似。
- en: Now, we should perform inference on the images left aside for testing
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们应该对留出的测试图像进行推理
- en: '[PRE66]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The inference results are saved in the folder `runs/detect/predict`. Let’s
    see some of them:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 推理结果保存在文件夹 `runs/detect/predict` 中。让我们看看其中的一些：
- en: '![](../media/file904.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file904.png)'
- en: It is advised to export the train, validation, and test results for a Drive
    at Google. To do so, we should mount the drive.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建议将训练、验证和测试结果导出到 Google Drive。为此，我们应该挂载驱动器。
- en: '[PRE67]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'and copy the content of `/runs` folder to a folder that you should create in
    your Drive, for example:'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并将 `/runs` 文件夹的内容复制到你在 Drive 中创建的文件夹中，例如：
- en: '[PRE68]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Inference with the trained model, using the Raspi
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用训练的模型，通过 Raspi 进行推理
- en: Download the trained model `/runs/detect/train/weights/best.pt` to your computer.
    Using the FileZilla FTP, let’s transfer the `best.pt` to the Raspi models folder
    (before the transfer, you may change the model name, for example, `box_wheel_320_yolo.pt`).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练好的模型 `/runs/detect/train/weights/best.pt` 下载到你的电脑。使用 FileZilla FTP，让我们将 `best.pt`
    转移到 Raspi 模型文件夹（在传输之前，你可能需要更改模型名称，例如，`box_wheel_320_yolo.pt`）。
- en: 'Using the FileZilla FTP, let’s transfer a few images from the test dataset
    to `.\YOLO\images`:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FileZilla FTP，让我们将测试数据集中的几张图像转移到 `.\YOLO\images`：
- en: 'Let’s return to the YOLO folder and use the Python Interpreter:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 YOLO 文件夹并使用 Python 解释器：
- en: '[PRE69]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As before, we will import the YOLO library and define our converted model to
    detect bees:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将导入 YOLO 库并定义我们的转换模型以检测蜜蜂：
- en: '[PRE70]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, let’s define an image and call the inference (we will save the image result
    this time to external verification):'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个图像并调用推理（这次我们将保存图像结果以进行外部验证）：
- en: '[PRE71]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Let’s repeat for several images. The inference result is saved on the variable
    `result,` and the processed image on `runs/detect/predict8`
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对多张图像重复操作。推理结果保存在变量 `result` 中，处理后的图像在 `runs/detect/predict8`
- en: '![](../media/file905.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file905.png)'
- en: 'Using FileZilla FTP, we can send the inference result to our Desktop for verification:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FileZilla FTP，我们可以将推理结果发送到我们的桌面进行验证：
- en: '![](../media/file906.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file906.png)'
- en: We can see that the inference result is excellent! The model was trained based
    on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency,
    around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency
    and convert the model to TFLite or NCNN.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到推理结果非常出色！该模型是基于YOLOv8家族的小型基础模型（YOLOv8n）训练的。问题是延迟，大约1秒（或Raspi-Zero上的1 FPS）。当然，我们可以减少这个延迟并将模型转换为TFLite或NCNN。
- en: Object Detection on a live stream
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在实时流中进行目标检测
- en: All the models explored in this lab can detect objects in real-time using a
    camera. The captured image should be the input for the trained and converted model.
    For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display
    the inference result.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验室中探索的所有模型都可以使用摄像头实时检测物体。捕获的图像应该是训练和转换后的模型的输入。对于带有桌面的Raspi-4或5，OpenCV可以捕获帧并显示推理结果。
- en: However, creating a live stream with a webcam to detect objects in real-time
    is also possible. For example, let’s start with the script developed for the Image
    Classification app and adapt it for a *Real-Time Object Detection Web Application
    Using TensorFlow Lite and Flask*.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用摄像头创建实时流以检测物体也是可能的。例如，让我们从为图像分类应用程序开发的脚本开始，并对其进行修改以适应*使用TensorFlow Lite和Flask的实时目标检测Web应用程序*。
- en: 'This app version will work for all TFLite models. Verify if the model is in
    its correct folder, for example:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序版本适用于所有TFLite模型。请验证模型是否位于其正确的文件夹中，例如：
- en: '[PRE72]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Download the Python script `object_detection_app.py` from [GitHub](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/python_scripts/object_detection_app.py).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 从[GitHub](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/python_scripts/object_detection_app.py)下载Python脚本`object_detection_app.py`。
- en: 'And on the terminal, run:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行：
- en: '[PRE73]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'And access the web interface:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 然后访问Web界面：
- en: 'On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to
    `http://localhost:5000`'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Raspberry Pi本身（如果您有GUI）：打开网页浏览器，访问`http://localhost:5000`
- en: 'From another device on the same network: Open a web browser and go to `http://<raspberry_pi_ip>:5000`
    (Replace `<raspberry_pi_ip>` with your Raspberry Pi’s IP address). For example:
    `http://192.168.4.210:` `5000/`'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从同一网络中的另一台设备：打开网页浏览器，访问`http://<raspberry_pi_ip>:5000`（将`<raspberry_pi_ip>`替换为您的Raspberry
    Pi的IP地址）。例如：`http://192.168.4.210:` `5000/`
- en: Here are some screenshots of the app running on an external desktop
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些应用程序在外部桌面运行时的截图
- en: '![](../media/file907.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file907.png)'
- en: 'Let’s see a technical description of the key modules used in the object detection
    application:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在目标检测应用程序中使用的关键模块的技术描述：
- en: '**TensorFlow Lite (tflite_runtime)**:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**TensorFlow Lite (tflite_runtime)**：'
- en: 'Purpose: Efficient inference of machine learning models on edge devices.'
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：在边缘设备上高效推理机器学习模型。
- en: 'Why: TFLite offers reduced model size and optimized performance compared to
    full TensorFlow, which is crucial for resource-constrained devices like Raspberry
    Pi. It supports hardware acceleration and quantization, further improving efficiency.'
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么：与完整的TensorFlow相比，TFLite提供了更小的模型大小和优化的性能，这对于资源受限的设备（如Raspberry Pi）至关重要。它支持硬件加速和量化，进一步提高了效率。
- en: 'Key functions: `Interpreter` for loading and running the model, `get_input_details(),`
    and `get_output_details()` for interfacing with the model.'
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键功能：用于加载和运行模型的`Interpreter`，以及用于与模型交互的`get_input_details()`和`get_output_details()`。
- en: '**Flask**:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Flask**：'
- en: 'Purpose: Lightweight web framework for creating the backend server.'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：轻量级Web框架，用于创建后端服务器。
- en: 'Why: Flask’s simplicity and flexibility make it ideal for rapidly developing
    and deploying web applications. It’s less resource-intensive than larger frameworks
    suitable for edge devices.'
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么：Flask的简单性和灵活性使其成为快速开发和部署Web应用程序的理想选择。它比适合边缘设备的较大框架资源消耗更少。
- en: 'Key components: route decorators for defining API endpoints, `Response` objects
    for streaming video, `render_template_string` for serving dynamic HTML.'
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键组件：用于定义API端点的路由装饰器，用于流式传输视频的`Response`对象，以及用于提供动态HTML的`render_template_string`。
- en: '**Picamera2**:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Picamera2**：'
- en: 'Purpose: Interface with the Raspberry Pi camera module.'
  id: totrans-500
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：与Raspberry Pi摄像头模块接口。
- en: 'Why: Picamera2 is the latest library for controlling Raspberry Pi cameras,
    offering improved performance and features over the original Picamera library.'
  id: totrans-501
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么：Picamera2是控制Raspberry Pi摄像头的最新库，在性能和功能上优于原始的Picamera库。
- en: 'Key functions: `create_preview_configuration()` for setting up the camera,
    `capture_file()` for capturing frames.'
  id: totrans-502
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键功能：`create_preview_configuration()`用于设置摄像头，`capture_file()`用于捕获帧。
- en: '**PIL (Python Imaging Library)**:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**PIL (Python Imaging Library)**：'
- en: 'Purpose: Image processing and manipulation.'
  id: totrans-504
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：图像处理和操作。
- en: 'Why: PIL provides a wide range of image processing capabilities. It’s used
    here to resize images, draw bounding boxes, and convert between image formats.'
  id: totrans-505
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：PIL 提供了广泛的电影处理能力。它在这里用于调整图像大小、绘制边界框和在不同图像格式之间转换。
- en: 'Key classes: `Image` for loading and manipulating images, `ImageDraw` for drawing
    shapes and text on images.'
  id: totrans-506
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键类：`Image` 用于加载和操作图像，`ImageDraw` 用于在图像上绘制形状和文本。
- en: '**NumPy**:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NumPy**：'
- en: 'Purpose: Efficient array operations and numerical computing.'
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：高效的数组操作和数值计算。
- en: 'Why: NumPy’s array operations are much faster than pure Python lists, which
    is crucial for efficiently processing image data and model inputs/outputs.'
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：NumPy 的数组操作比纯 Python 列表快得多，这对于高效处理图像数据和模型输入/输出至关重要。
- en: 'Key functions: `array()` for creating arrays, `expand_dims()` for adding dimensions
    to arrays.'
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键函数：`array()` 用于创建数组，`expand_dims()` 用于向数组添加维度。
- en: '**Threading**:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线程**：'
- en: 'Purpose: Concurrent execution of tasks.'
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：任务的并发执行。
- en: 'Why: Threading allows simultaneous frame capture, object detection, and web
    server operation, crucial for maintaining real-time performance.'
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：线程允许同时捕获帧、对象检测和 Web 服务器操作，这对于保持实时性能至关重要。
- en: 'Key components: `Thread` class creates separate execution threads, and Lock
    is used for thread synchronization.'
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键组件：`Thread` 类创建独立的执行线程，Lock 用于线程同步。
- en: '**io.BytesIO**:'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**io.BytesIO**：'
- en: 'Purpose: In-memory binary streams.'
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：内存中的二进制流。
- en: 'Why: Allows efficient handling of image data in memory without needing temporary
    files, improving speed and reducing I/O operations.'
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：允许在内存中高效处理图像数据，无需临时文件，提高速度并减少 I/O 操作。
- en: '**time**:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**时间**：'
- en: 'Purpose: Time-related functions.'
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：时间相关函数。
- en: 'Why: Used for adding delays (`time.sleep()`) to control frame rate and for
    performance measurements.'
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：用于添加延迟（`time.sleep()`）以控制帧率并进行性能测量。
- en: '**jQuery (client-side)**:'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**jQuery（客户端）**：'
- en: 'Purpose: Simplified DOM manipulation and AJAX requests.'
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的：简化 DOM 操作和 AJAX 请求。
- en: 'Why: It makes it easy to update the web interface dynamically and communicate
    with the server without page reloads.'
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原因：它使得动态更新网页界面并与服务器通信而不需要页面重新加载变得容易。
- en: 'Key functions: `.get()` and `.post()` for AJAX requests, DOM manipulation methods
    for updating the UI.'
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键函数：`.get()` 和 `.post()` 用于 AJAX 请求，DOM 操作方法用于更新用户界面。
- en: 'Regarding the main app system architecture:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 关于主应用程序系统架构：
- en: '**Main Thread**: Runs the Flask server, handling HTTP requests and serving
    the web interface.'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主线程**：运行 Flask 服务器，处理 HTTP 请求并提供服务端界面。'
- en: '**Camera Thread**: Continuously captures frames from the camera.'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相机线程**：持续从相机捕获帧。'
- en: '**Detection Thread**: Processes frames through the TFLite model for object
    detection.'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检测线程**：通过 TFLite 模型处理帧进行对象检测。'
- en: '**Frame Buffer**: Shared memory space (protected by locks) storing the latest
    frame and detection results.'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**帧缓冲区**：共享内存空间（由锁保护），存储最新帧和检测结果。'
- en: 'And the app data flow, we can describe in short:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 关于应用程序数据流，我们可以简要描述：
- en: Camera captures frame → Frame Buffer
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摄像头捕获帧 → 帧缓冲区
- en: Detection thread reads from Frame Buffer → Processes through TFLite model →
    Updates detection results in Frame Buffer
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测线程从帧缓冲区读取 → 通过 TFLite 模型处理 → 在帧缓冲区更新检测结果
- en: Flask routes access Frame Buffer to serve the latest frame and detection results
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Flask 路由访问帧缓冲区以提供最新帧和检测结果
- en: Web client receives updates via AJAX and updates UI
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络客户端通过 AJAX 接收更新并更新用户界面
- en: This architecture allows for efficient, real-time object detection while maintaining
    a responsive web interface running on a resource-constrained edge device like
    a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the
    system to process video frames in real-time, while Flask and jQuery provide a
    user-friendly way to interact with them.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构允许在保持资源受限的边缘设备（如 Raspberry Pi）上运行的响应式 Web 界面的同时，进行高效的实时对象检测。线程和高效的库如 TFLite
    和 PIL 使系统能够实时处理视频帧，而 Flask 和 jQuery 提供了一种用户友好的方式来与之交互。
- en: 'You can test the app with another pre-processed model, such as the EfficientDet,
    changing the app line:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用另一个预处理的模型测试应用程序，例如 EfficientDet，更改应用程序行：
- en: '[PRE74]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: If we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse
    Studio with the “Box versus Wheel” dataset, the code should also be adapted depending
    on the input details, as we have explored on its [notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb).
  id: totrans-538
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们想使用SSD-MobileNetV2模型的应用，该模型在Edge Impulse Studio上使用“盒子与轮子”数据集进行训练，代码也应根据输入细节进行适配，正如我们在其[notebook](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb)中探讨的那样。
- en: Summary
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This lab has explored the implementation of object detection on edge devices
    like the Raspberry Pi, demonstrating the power and potential of running advanced
    computer vision tasks on resource-constrained hardware. We’ve covered several
    vital aspects:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验室探讨了在Raspberry Pi等边缘设备上实现对象检测的实施，展示了在资源受限的硬件上运行高级计算机视觉任务的强大功能和潜力。我们涵盖了几个关键方面：
- en: '**Model Comparison**: We examined different object detection models, including
    SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs
    on edge devices.'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型比较**：我们研究了不同的对象检测模型，包括SSD-MobileNet、EfficientDet、FOMO和YOLO，比较了它们在边缘设备上的性能和权衡。'
- en: '**Training and Deployment**: Using a custom dataset of boxes and wheels (labeled
    on Roboflow), we walked through the process of training models using Edge Impulse
    Studio and Ultralytics and deploying them on Raspberry Pi.'
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练和部署**：使用在Roboflow上标记的盒子和小轮子（自定义数据集），我们介绍了使用Edge Impulse Studio和Ultralytics训练模型并在Raspberry
    Pi上部署它们的过程。'
- en: '**Optimization Techniques**: To improve inference speed on edge devices, we
    explored various optimization methods, such as model quantization (TFLite int8)
    and format conversion (e.g., to NCNN).'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化技术**：为了提高边缘设备上的推理速度，我们探索了各种优化方法，例如模型量化（TFLite int8）和格式转换（例如，转换为NCNN）。'
- en: '**Real-time Applications**: The lab exemplified a real-time object detection
    web application, demonstrating how these models can be integrated into practical,
    interactive systems.'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实时应用**：实验室展示了实时对象检测网络应用，展示了这些模型如何集成到实际、交互式系统中。'
- en: '**Performance Considerations**: Throughout the lab, we discussed the balance
    between model accuracy and inference speed, a critical consideration for edge
    AI applications.'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能考虑**：在整个实验室中，我们讨论了模型准确性与推理速度之间的平衡，这是边缘AI应用的关键考虑因素。'
- en: The ability to perform object detection on edge devices opens up numerous possibilities
    across various domains, from precision agriculture, industrial automation, and
    quality control to smart home applications and environmental monitoring. By processing
    data locally, these systems can offer reduced latency, improved privacy, and operation
    in environments with limited connectivity.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上执行对象检测的能力为各个领域打开了无数的可能性，从精准农业、工业自动化和质量控制到智能家居应用和环境监测。通过本地处理数据，这些系统可以提供降低延迟、提高隐私性以及在有限连接性的环境中运行的能力。
- en: 'Looking ahead, potential areas for further exploration include:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，进一步探索的潜在领域包括：
- en: Implementing multi-model pipelines for more complex tasks
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现多模型管道以处理更复杂的任务
- en: Exploring hardware acceleration options for Raspberry Pi
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Raspberry Pi的硬件加速选项
- en: Integrating object detection with other sensors for more comprehensive edge
    AI systems
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将对象检测与其他传感器集成以构建更全面的边缘AI系统
- en: Developing edge-to-cloud solutions that leverage both local processing and cloud
    resources
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发利用本地处理和云资源的边缘到云解决方案
- en: Object detection on edge devices can create intelligent, responsive systems
    that bring the power of AI directly into the physical world, opening up new frontiers
    in how we interact with and understand our environment.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上进行对象检测可以创建智能、响应迅速的系统，将AI的力量直接带入物理世界，开辟了我们与环境和理解环境的新领域。
- en: Resources
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Dataset (“Box versus Wheel”)](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset)'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据集（“盒子与轮子”)](https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset)'
- en: '[SSD-MobileNet Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb)'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Raspi上的SSD-MobileNet笔记本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb)'
- en: '[EfficientDet Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb)'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Raspi上的EfficientDet笔记本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb)'
- en: '[FOMO - EI Linux Notebook on a Raspi](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb)'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FOMO - EI Linux 笔记本在 Raspberry Pi 上](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb)'
- en: '[YOLOv8 Box versus Wheel Dataset Training on CoLab](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb)'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 CoLab 上进行 YOLOv8 Box 与 Wheel 数据集训练](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb)'
- en: '[Edge Impulse Project - SSD MobileNet and FOMO](https://studio.edgeimpulse.com/public/515477/live)'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse 项目 - SSD MobileNet 和 FOMO](https://studio.edgeimpulse.com/public/515477/live)'
- en: '[Python Scripts](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/python_scripts)'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 脚本](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/python_scripts)'
- en: '[Models](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models)'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型](https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models)'
