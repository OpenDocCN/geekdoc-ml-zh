<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch046.xhtml</title>
  <style>
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    /* Figure formatting */
    .quarto-layout-panel>figure>figcaption,
    .quarto-layout-panel>.panel-caption {
      margin-top: 10pt;
    }

    .quarto-layout-row {
      display: flex;
      align-items: flex-start;
    }

    .quarto-layout-valign-top {
      align-items: flex-start;
    }

    .quarto-layout-valign-bottom {
      align-items: flex-end;
    }

    .quarto-layout-valign-center {
      align-items: center;
    }

    .quarto-layout-cell {
      position: relative;
      margin-right: 20px;
    }

    .quarto-layout-cell:last-child {
      margin-right: 0;
    }

    .quarto-layout-cell figure,
    .quarto-layout-cell>p {
      margin: 0.2em;
    }

    .quarto-layout-cell .html-widget {
      width: 100% !important;
    }

    .quarto-layout-cell div figure p {
      margin: 0;
    }

    .quarto-layout-cell figure {
      display: inline-block;
      margin-inline-start: 0;
      margin-inline-end: 0;
    }

    .quarto-layout-cell table {
      display: inline-table;
    }

    .quarto-layout-cell-subref figcaption {
      font-style: italic;
      text-align: center;
    }

    .quarto-figure>figure {
      width: 100%;
    }

    .quarto-figure-left>figure>p {
      text-align: left;
    }

    .quarto-figure-center>figure>p {
      text-align: center;
    }

    .quarto-figure-right>figure>p {
      text-align: right;
    }

    figure>p:empty {
      display: none;
    }

    figure>p:first-child {
      margin-top: 0;
      margin-bottom: 0;
    }

    figure>figcaption {
      margin-top: 0.5em;
    }

    figcaption {
      font-size: 0.8em;
    }

    details {
      margin-bottom: 1em;
    }

    details[show] {
      margin-bottom: 0;
    }

    .quarto-unresolved-ref {
      font-weight: 600;
    }

    .quarto-cover-image {
      float: right;
      margin-left: 30px;
    }

    .cell-output-display {
      overflow-x: scroll;
    }

    .hidden {
      display: none;
    }
  </style>
</head>
<body epub:type="bodymatter">
<section id="image-classification-2" class="level1 unnumbered">
<h1 class="unnumbered">Image Classification</h1>
<p><strong>Using Seeed Studio Grove Vision AI Module V2 (Himax WiseEye2)</strong></p>
<p><img src="../media/file740.jpg" alt="" /></p>
<p>In this Lab, we will explore Image Classification using the Seeed Studio <a href="https://wiki.seeedstudio.com/grove_vision_ai_v2/"><em>Grove Vision AI Module V2</em></a>, a powerful yet compact device specifically designed for embedded machine learning applications. Based on the <strong>Himax WiseEye2</strong> chip, this module is designed to enable AI capabilities on edge devices, making it an ideal tool for Edge Machine Learning (ML) applications.</p>
<section id="sec-image-classification-introduction-59d5" class="level2 unnumbered">
<h2 class="unnumbered">Introduction</h2>
<p>So far, we have explored several computer vision models previously uploaded by Seeed Studio or used the SenseCraft AI Studio for Image Classification, without choosing a specific model. Let’s now develop our Image Classification project from scratch, where we will select our data and model.</p>
<p>Below, we can see the project’s main steps and where we will work with them:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file741.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<section id="sec-image-classification-project-goal-cc02" class="level3 unnumbered">
<h3 class="unnumbered">Project Goal</h3>
<p>The first step in any machine learning (ML) project is defining the goal. In this case, the goal is to detect and classify two specific objects present in a single image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named <em>Periquito</em>). Also, we will collect images of a background where those two objects are absent.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file742.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
</section>
<section id="sec-image-classification-data-collection-32ea" class="level3 unnumbered">
<h3 class="unnumbered">Data Collection</h3>
<p>With the Machine Learning project goal defined, dataset collection is the next and most crucial step. Suppose your project utilizes images that are publicly available on datasets, for example, to be used on a <strong>Person Detection</strong> project. In that case, you can download the <a href="https://edgeai.modelnova.ai/datasets/details/wake-vision">Wake Vision</a> dataset for use in the project.</p>
<p>But, in our case, we define a project where the images do not exist publicly, so we need to generate them. We can use a phone, computer camera, or other devices to capture the photos, offline or connected to the Edge Impulse Studio.</p>
<p>If you want to use the Grove Vision AI V2 to capture your dataset, you can use the SenseCraft AI Studio as we did in the previous Lab, or the <code>camera_web_server</code> sketch as we will describe later in the <strong>Postprocessing / Getting the Video Stream</strong> section of this Lab.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file743.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>In this Lab, we will use the SenseCraft AI Studio to collect the dataset.</p>
</section>
<section id="sec-image-classification-collecting-data-sensecraft-ai-studio-bbd4" class="level3 unnumbered">
<h3 class="unnumbered">Collecting Data with the SenseCraft AI Studio</h3>
<p>On SenseCraft AI Studio: Let’s open the tab <a href="https://sensecraft.seeed.cc/ai/training">Training</a>.</p>
<p>The default is to train a <code>Classification</code> model with a WebCam if it is available. Let’s select the <code>Grove Vision AI V2 instead</code>. Pressing the green button<code>[Connect]</code> <strong>(1),</strong> a Pop-Up window will appear. Select the corresponding Port <strong>(2)</strong> and press the blue button <code>[Connect]</code> <strong>(3)</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file744.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>The image streamed from the Grove Vision AI V2 will be displayed.</p>
<section id="sec-image-classification-image-collection-9c61" class="level4 unnumbered">
<h4 class="unnumbered">Image Collection</h4>
<p>Let’s create the classes, following, for example, an alphabetical order:</p>
<ul>
<li>Class1: background</li>
<li>Class 2: periquito</li>
<li>Class 3: robot</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file745.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Select one of the classes (note that a green line will be around the window) and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file746.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>After collecting the images, review them and, if necessary, delete any incorrect ones.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file747.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Collect around 50 images from each class. After you collect the three classes, open the menu on each of them and select <code>Export Data</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file748.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>In the Download area of the Computer, we will get three zip files, each one with its corresponding class name. Each Zip file contains a folder with the images.</p>
</section>
</section>
<section id="sec-image-classification-uploading-dataset-edge-impulse-studio-ebe4" class="level3 unnumbered">
<h3 class="unnumbered">Uploading the dataset to the Edge Impulse Studio</h3>
<p>We will use the Edge Impulse Studio to train our model. <a href="https://www.edgeimpulse.com/">Edge Impulse</a> is a leading development platform for machine learning on edge devices.</p>
<ul>
<li>Enter your account credentials (or create a free account) at Edge Impulse.</li>
<li>Next, create a new project:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file749.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>The dataset comprises approximately 50 images per label, with 40 for training and 10 for testing.</p>
</blockquote>
</section>
<section id="sec-image-classification-impulse-design-preprocessing-41ed" class="level3 unnumbered">
<h3 class="unnumbered">Impulse Design and Pre-Processing</h3>
<p><strong>Impulse Design</strong></p>
<p>An impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.</p>
<p>Classifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach” or “model” each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as “Transfer Learning” (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file750.jpg" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>So, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file751.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>For comparison, we will keep the image size as 96 x 96. However, keep in mind that with the Grove Vision AI Module V2 and its internal SRAM of 2.4 MB, larger images can be utilized (for example, 160 x 160).</p>
</blockquote>
<p>Also select the <code>Target</code> device (<code>Himax WiseEye2 (M55 400 MHz + U55)</code>) on the up-right corner.</p>
</section>
<section id="sec-image-classification-preprocessing-feature-generation-f356" class="level3 unnumbered">
<h3 class="unnumbered">Pre-processing (Feature generation)</h3>
<p>Besides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let’s select <code>[RGB]</code> in the <code>Image</code> section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing <code>[Save Parameters]</code> will open a new tab, <code>Generate Features</code>. Press the button <code>[Generate Features]</code>to generate the features.</p>
</section>
<section id="sec-image-classification-model-design-training-test-22dd" class="level3 unnumbered">
<h3 class="unnumbered">Model Design, Training, and Test</h3>
<p>In 2007, Google introduced <a href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNetV1</a>. In 2018, <a href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a>, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.</p>
<p>Although the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, <strong>α</strong> (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.</p>
<p>Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different <strong>α</strong> values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).</p>
<blockquote>
<p>For comparison, we will use the <strong>MobileNet V2 0.1</strong> as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 8 neurons with a 10% dropout rate for preventing overfitting.</p>
</blockquote>
<p>Another necessary technique to use with deep learning is <strong>data augmentation</strong>. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).</p>
<p>Set the Hyperparameters:</p>
<ul>
<li>Epochs: 20,</li>
<li>Bach Size: 32</li>
<li>Learning Rate: 0.0005</li>
<li>Validation size: 20%</li>
</ul>
<p>Training result:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file752.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>The model profile predicts <strong>146 KB of RAM and 187 KB of Flash</strong>, indicating no problem with the Grove AI Vision (V2), which has almost 2.5 MB of internal SRAM. Additionally, the Studio indicates a <strong>latency of around 4 ms.</strong></p>
<blockquote>
<p>Despite this, with a 100% accuracy on the Validation set when using the spare data for testing, we confirmed an Accuracy of 81%, using the Quantized (Int8) trained model. However, it is sufficient for our purposes in this lab.</p>
</blockquote>
</section>
<section id="sec-image-classification-model-deployment-cdd4" class="level3 unnumbered">
<h3 class="unnumbered">Model Deployment</h3>
<p>On the Deployment tab, we should select: <code>Seeed Grove Vision AI Module V2 (Himax WiseEye2)</code> and press <code>[Build]</code>. A ZIP file will be downloaded to our computer.</p>
<p>The Zip file contains the <code>model_vela.tflite</code>, which is a TensorFlow Lite (TFLite) model optimized for neural processing units (NPUs) using the Vela compiler, a tool developed by Arm to adapt TFLite models for Ethos-U NPUs.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file753.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>We can flash the model following the instructions in the <code>README.txt</code> or use the SenseCraft AI Studio. We will use the latter.</p>
</section>
<section id="sec-image-classification-deploy-model-sensecraft-ai-studio-e972" class="level3 unnumbered">
<h3 class="unnumbered">Deploy the model on the SenseCraft AI Studio</h3>
<p>On SenseCraft AI Studio, go to the <code>Vision Workspace</code> tab, and connect the device:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file754.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>You should see the last model that was uploaded to the device. Select the green button <code>[Upload Model]</code>. A pop-up window will ask for the <strong>model name</strong>, the <strong>model file,</strong> and to enter the class names (<strong>objects</strong>). We should use labels following alphabetical order: <code>0: background</code>, <code>1: periquito,</code> and <code>2: robot</code>, and then press <code>[Send]</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file755.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>After a few seconds, the model will be uploaded (“flashed”) to our device, and the camera image will appear in real-time on the <strong>Preview</strong> Sector. The Classification result will be displayed under the image preview. It is also possible to select the <code>Confidence Threshold</code> of your inference using the cursor on <strong>Settings</strong>.</p>
<p>On the <strong>Device Logger</strong>, we can view the Serial Monitor, where we can observe the latency, which is approximately 1 to 2 ms for pre-processing and 4 to 5 ms for inference, aligning with the estimates made in Edge Impulse Studio.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file756.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Here are other screenshots:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file757.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>The power consumption of this model is approximately 70 mA, equivalent to 0.4 W.</p>
</section>
<section id="sec-image-classification-image-classification-nonofficial-benchmark-b06a" class="level3 unnumbered">
<h3 class="unnumbered">Image Classification (non-official) Benchmark</h3>
<p>Several development boards can be used for embedded machine learning (tinyML), and the most common ones (so far) for Computer Vision applications (with low energy) are the <strong>ESP32 CAM,</strong> the <strong>Seeed XIAO ESP32S3 Sense</strong>, and the Arduino <strong>Nicla Vision</strong>.</p>
<p>Taking advantage of this opportunity, a similarly trained model, MobilenetV2 96x96, with an alpha of 0.1, was also deployed on the ESP-CAM, the XIAO, and a Raspberry Pi Zero W2. Here is the result:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file758.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>The Grove Vision AI V2 with an <strong>ARM Ethos-U55</strong> was approximately 14 times faster than devices with an ARM-M7, and more than 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi, with a much more powerful CPU, the U55 reduces latency by almost half. Additionally, the power consumption is lower than that of other devices (see the <a href="https://www.hackster.io/limengdu0117/2024-mcu-ai-vision-boards-performance-comparison-998505">full</a> article here for power consumption comparison).</p>
</blockquote>
</section>
<section id="sec-image-classification-postprocessing-9610" class="level3 unnumbered">
<h3 class="unnumbered">Postprocessing</h3>
<p>Now that we have the model uploaded to the board and working correctly, classifying our images, let’s connect a Master Device to export the inference result to it and see the result completely offline (disconnected from the PC and, for example, powered by a battery).</p>
<blockquote>
<p>Note that we can use any microcontroller as a Master Controller, such as the XIAO, Arduino, or Raspberry Pi.</p>
</blockquote>
<section id="sec-image-classification-getting-video-stream-7b2c" class="level4 unnumbered">
<h4 class="unnumbered">Getting the Video Stream</h4>
<p>The image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO (Master Controller) via IIC. For that, we will use the <strong>Arduino SSMA library</strong>. This library’s primary purpose is to process Grove Vision AI’s data stream, which does not involve model inference.</p>
<blockquote>
<p>The Grove Vision AI (V2) communicates (Inference result) with the XIAO via the IIC; the device’s IIC address is 0x62. Image information transfer is via the USB serial port.</p>
</blockquote>
<p><strong>Step 1:</strong> Download the <a href="https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA/">Arduino SSMA</a> library as a zip file from its GitHub:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file759.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p><strong>Step 2</strong>: Install it in the Arduino IDE (<code>sketch &gt; Include Library &gt; Add .Zip Library</code>).</p>
<p><strong>Step 3</strong>: Install the <strong>ArduinoJSON</strong> library.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file760.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p><strong>Step 4</strong>: Install the <strong>Eigen</strong> Library</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file761.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p><strong>Step 3</strong>: Now, connect the XIAO and Grove Vision AI (V2) via the socket (a row of pins) located at the back of the device.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file762.jpg" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p><strong>CAUTION</strong>: Please note the direction of the connection, Grove Vision AI’s Type-C connector should be in the same direction as XIAO’s Type-C connector.</p>
</blockquote>
<p><strong>Step 5</strong>: Connect the <strong>XIAO USB-C</strong> port to your computer</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file763.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p><strong>Step 6</strong>: In the Arduino IDE, select the Xiao board and the corresponding USB port.</p>
<p>Once we want to stream the video to a webpage, we will use the <strong>XIAO ESP32S3</strong>, which has wifi and enough memory to handle images. Select <code>XIAO_ESP32S3</code> and the appropriate USB Port:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file764.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>By default, the PSRAM is disabled. Open the <code>Tools</code> menu and on PSRAM: <code>"OPI PSRAM"</code>select <code>OPI PSRAM</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file765.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p><strong>Step 7</strong>: Open the example in Arduino IDE:</p>
<p><code>File</code> -&gt; <code>Examples</code> -&gt; <code>Seeed_Arduino_SSCMA</code> -&gt; <code>camera_web_server</code>.</p>
<p>And edit the <code>ssid</code> and <code>password</code> in the <code>camera_web_server.ino</code> sketch to match the Wi-Fi network.</p>
<p><strong>Step 8</strong>: Upload the sketch to the board and open the Serial Monitor. When connected to the Wi-Fi network, the board’s IP address will be displayed.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file766.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Open the address using a web browser. A Video App will be available. To see <strong>only</strong> the video stream from the Grove Vision AI V2, press <code>[Sample Only]</code> and <code>[Start Stream]</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file767.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>If you want to create an image dataset, you can use this app, saving frames of the video generated by the device. Pressing <code>[Save Frame]</code>, the image will be saved in the download area of our desktop.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file743.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Opening the App <strong>without</strong> selecting <code>[Sample Only]</code>, the inference result should appear on the video screen, but this does not happen for Image Classification. For Object Detection or Pose Estimation, the result is embedded with the video stream.</p>
<p>For example, if the model is a Person Detection using YoloV8:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file768.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
</section>
<section id="sec-image-classification-getting-inference-result-4f97" class="level4 unnumbered">
<h4 class="unnumbered">Getting the Inference Result</h4>
<ul>
<li><p>Go to <code>File</code> -&gt; <code>Examples</code> -&gt; <code>Seeed_Arduino_SSCMA</code> -&gt; <code>inference_class</code>.</p></li>
<li><p>Upload the sketch to the board, and open the Serial Monitor.</p></li>
<li><p>Pointing the camera at one of our objects, we can see the inference result on the Serial Terminal.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file769.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>The inference running on the Arduino IDE had an average consumption of 160 mA or 800 mW and a peak of 330 mA 1.65 W when transmitting the image to the App.</p>
</blockquote>
</section>
<section id="sec-image-classification-postprocessing-led-5e50" class="level4 unnumbered">
<h4 class="unnumbered">Postprocessing with LED</h4>
<p>The idea behind our postprocessing is that whenever a specific image is detected (for example, the Periquito - Label:1), the User LED is turned on. If the Robot or a background is detected, the LED will be off.</p>
<p>Copy the below code and past it to your IDE:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1"></a><span class="pp">#include </span><span class="im">&lt;Seeed_Arduino_SSCMA.h&gt;</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>SSCMA AI<span class="op">;</span></span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="dt">void</span> setup<span class="op">()</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="op">{</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>    AI<span class="op">.</span>begin<span class="op">();</span></span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>    Serial<span class="op">.</span>begin<span class="op">(</span><span class="dv">115200</span><span class="op">);</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="cf">while</span> <span class="op">(!</span>Serial<span class="op">);</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>    Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;Inferencing - Grove AI V2 / XIAO ESP32S3&quot;</span><span class="op">);</span></span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co">// Pins for the built-in LED</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    pinMode<span class="op">(</span>LED_BUILTIN<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="co">// Ensure the LED is OFF by default.</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>    <span class="co">// Note: The LED is ON when the pin is LOW, OFF when HIGH.</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>    digitalWrite<span class="op">(</span>LED_BUILTIN<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="op">}</span></span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="dt">void</span> loop<span class="op">()</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="op">{</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>    <span class="cf">if</span> <span class="op">(!</span>AI<span class="op">.</span>invoke<span class="op">()){</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>        Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;</span><span class="sc">\n</span><span class="st">Invoke Success&quot;</span><span class="op">);</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;Latency [ms]: prepocess=&quot;</span><span class="op">);</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>        Serial<span class="op">.</span>print<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>prepocess<span class="op">);</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, inference=&quot;</span><span class="op">);</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>        Serial<span class="op">.</span>print<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>inference<span class="op">);</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, postpocess=&quot;</span><span class="op">);</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>        Serial<span class="op">.</span>println<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>postprocess<span class="op">);</span></span>
<span id="cb1-29"><a href="#cb1-29"></a>        <span class="dt">int</span> pred_index <span class="op">=</span> AI<span class="op">.</span>classes<span class="op">()[</span><span class="dv">0</span><span class="op">].</span>target<span class="op">;</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;Result= Label: &quot;</span><span class="op">);</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>        Serial<span class="op">.</span>print<span class="op">(</span>pred_index<span class="op">);</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, score=&quot;</span><span class="op">);</span></span>
<span id="cb1-33"><a href="#cb1-33"></a>        Serial<span class="op">.</span>println<span class="op">(</span>AI<span class="op">.</span>classes<span class="op">()[</span><span class="dv">0</span><span class="op">].</span>score<span class="op">);</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>        turn_on_led<span class="op">(</span>pred_index<span class="op">);</span></span>
<span id="cb1-35"><a href="#cb1-35"></a>    <span class="op">}</span></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="op">}</span></span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="co">/**</span></span>
<span id="cb1-39"><a href="#cb1-39"></a><span class="co">* </span><span class="an">@brief</span><span class="co">      turn_off_led function - turn-off the User LED</span></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="co">*/</span></span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="dt">void</span> turn_off_led<span class="op">(){</span></span>
<span id="cb1-42"><a href="#cb1-42"></a>    digitalWrite<span class="op">(</span>LED_BUILTIN<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb1-43"><a href="#cb1-43"></a><span class="op">}</span></span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a><span class="co">/**</span></span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="co">* </span><span class="an">@brief</span><span class="co">      turn_on_led function used to turn on the User LED</span></span>
<span id="cb1-47"><a href="#cb1-47"></a><span class="co">* </span><span class="an">@param[in]</span><span class="co">  </span><span class="cv">pred_index</span></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="co">*             label 0: [0] ==&gt; ALL OFF</span></span>
<span id="cb1-49"><a href="#cb1-49"></a><span class="co">*             label 1: [1] ==&gt; LED ON</span></span>
<span id="cb1-50"><a href="#cb1-50"></a><span class="co">*             label 2: [2] ==&gt; ALL OFF</span></span>
<span id="cb1-51"><a href="#cb1-51"></a><span class="co">*             label 3: [3] ==&gt; ALL OFF</span></span>
<span id="cb1-52"><a href="#cb1-52"></a><span class="co">*/</span></span>
<span id="cb1-53"><a href="#cb1-53"></a><span class="dt">void</span> turn_on_led<span class="op">(</span><span class="dt">int</span> pred_index<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-54"><a href="#cb1-54"></a>    <span class="cf">switch</span> <span class="op">(</span>pred_index<span class="op">)</span></span>
<span id="cb1-55"><a href="#cb1-55"></a>    <span class="op">{</span></span>
<span id="cb1-56"><a href="#cb1-56"></a>        <span class="cf">case</span> <span class="dv">0</span><span class="op">:</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>            turn_off_led<span class="op">();</span></span>
<span id="cb1-58"><a href="#cb1-58"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb1-59"><a href="#cb1-59"></a>        <span class="cf">case</span> <span class="dv">1</span><span class="op">:</span></span>
<span id="cb1-60"><a href="#cb1-60"></a>            turn_off_led<span class="op">();</span></span>
<span id="cb1-61"><a href="#cb1-61"></a>            digitalWrite<span class="op">(</span>LED_BUILTIN<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb1-62"><a href="#cb1-62"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb1-63"><a href="#cb1-63"></a>        <span class="cf">case</span> <span class="dv">2</span><span class="op">:</span></span>
<span id="cb1-64"><a href="#cb1-64"></a>            turn_off_led<span class="op">();</span></span>
<span id="cb1-65"><a href="#cb1-65"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb1-66"><a href="#cb1-66"></a>        <span class="cf">case</span> <span class="dv">3</span><span class="op">:</span></span>
<span id="cb1-67"><a href="#cb1-67"></a>            turn_off_led<span class="op">();</span></span>
<span id="cb1-68"><a href="#cb1-68"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb1-69"><a href="#cb1-69"></a>    <span class="op">}</span></span>
<span id="cb1-70"><a href="#cb1-70"></a><span class="op">}</span></span></code></pre></div>
<p>This sketch uses the Seeed_Arduino_SSCMA.h library to interface with the Grove Vision AI Module V2. The AI module and the LED are initialized in the <code>setup()</code> function, and serial communication is started.</p>
<p>The <code>loop()</code> function repeatedly calls the <code>invoke()</code> method to perform inference using the built-in algorithms of the Grove Vision AI Module V2. Upon a successful inference, the sketch prints out performance metrics to the serial monitor, including preprocessing, inference, and postprocessing times.</p>
<p>The sketch processes and prints out detailed information about the results of the inference:</p>
<ul>
<li>(<code>AI.classes()[0]</code>) that identifies the class of image (<code>.target</code>) and its confidence score (<code>.score</code>).</li>
<li>The inference result (class) is stored in the integer variable <code>pred_index</code>, which will be used as an input to the function <code>turn_on_led()</code>. As a result, the LED will turn ON, depending on the classification result.</li>
</ul>
<p>Here is the result:</p>
<p>If the Periquito is detected (Label:1), the LED is ON:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file770.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>If the Robot is detected (Label:2) the LED is OFF (Same for Background (Label:0):</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file771.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Therefore, we can now power the Grove Vision AI V2 + Xiao ESP32S3 with an external battery, and the inference result will be displayed by the LED completely offline. The consumption is approximately 165 mA or 825 mW.</p>
<blockquote>
<p>It is also possible to send the result using Wifi, BLE, or other communication protocols available on the used Master Device.</p>
</blockquote>
</section>
</section>
<section id="sec-image-classification-optional-postprocessing-external-devices-6222" class="level3 unnumbered">
<h3 class="unnumbered">Optional: Post-processing on external devices</h3>
<p>Of course, one of the significant advantages of working with EdgeAI is that devices can run entirely disconnected from the cloud, allowing for seamless <strong>interactions with the real world</strong>. We did it in the last section, but using the internal Xiao LED. Now, we will connect external LEDs (which could be any actuator).</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file772.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>The LEDS should be connected to the XIAO ground via a 220-ohm resistor.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file773.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>The idea is to modify the previous sketch to handle the three external LEDs.</p>
<p><strong>GOAL</strong>: Whenever the image of a <strong>Periquito</strong> is detected, the LED <strong>Green</strong> will be ON; if it is a <strong>Robot</strong>, the LED <strong>Yellow</strong> will be ON; if it is a <strong>Background</strong>, the <strong>LED Red</strong> will be ON.</p>
<p>The image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO via IIC. For that, we will use the Arduino SSMA library again.</p>
<p>Here the sketch to be used:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource cpp number-lines"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1"></a><span class="pp">#include </span><span class="im">&lt;Seeed_Arduino_SSCMA.h&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>SSCMA AI<span class="op">;</span></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">// Define the LED pin according to the pin diagram</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">// The LEDS negative lead should be connected to the XIAO ground</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">// via a 220-ohm resistor.</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="dt">int</span> LEDR <span class="op">=</span> D1<span class="op">;</span> <span class="er"># XIAO ESP32S3 Pin 1</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="dt">int</span> LEDY <span class="op">=</span> D2<span class="op">;</span> <span class="er"># XIAO ESP32S3 Pin 2</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="dt">int</span> LEDG <span class="op">=</span> D3<span class="op">;</span> <span class="er"># XIAO ESP32S3 Pin 3</span></span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>  <span class="dt">void</span> setup<span class="op">()</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="op">{</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>    AI<span class="op">.</span>begin<span class="op">();</span></span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>    Serial<span class="op">.</span>begin<span class="op">(</span><span class="dv">115200</span><span class="op">);</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>    <span class="cf">while</span> <span class="op">(!</span>Serial<span class="op">);</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>    Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;Inferencing - Grove AI V2 / XIAO ESP32S3&quot;</span><span class="op">);</span></span>
<span id="cb2-18"><a href="#cb2-18"></a></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="co">// Initialize the external LEDs</span></span>
<span id="cb2-20"><a href="#cb2-20"></a>    pinMode<span class="op">(</span>LEDR<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>    pinMode<span class="op">(</span>LEDY<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb2-22"><a href="#cb2-22"></a>    pinMode<span class="op">(</span>LEDG<span class="op">,</span> OUTPUT<span class="op">);</span></span>
<span id="cb2-23"><a href="#cb2-23"></a>    <span class="co">// Ensure the LEDs are OFF by default.</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="co">// Note: The LEDs are ON when the pin is HIGH, OFF when LOW.</span></span>
<span id="cb2-25"><a href="#cb2-25"></a>    digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-26"><a href="#cb2-26"></a>    digitalWrite<span class="op">(</span>LEDY<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-27"><a href="#cb2-27"></a>    digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="op">}</span></span>
<span id="cb2-29"><a href="#cb2-29"></a></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="dt">void</span> loop<span class="op">()</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="op">{</span></span>
<span id="cb2-32"><a href="#cb2-32"></a>    <span class="cf">if</span> <span class="op">(!</span>AI<span class="op">.</span>invoke<span class="op">()){</span></span>
<span id="cb2-33"><a href="#cb2-33"></a>        Serial<span class="op">.</span>println<span class="op">(</span><span class="st">&quot;</span><span class="sc">\n</span><span class="st">Invoke Success&quot;</span><span class="op">);</span></span>
<span id="cb2-34"><a href="#cb2-34"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;Latency [ms]: prepocess=&quot;</span><span class="op">);</span></span>
<span id="cb2-35"><a href="#cb2-35"></a>        Serial<span class="op">.</span>print<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>prepocess<span class="op">);</span></span>
<span id="cb2-36"><a href="#cb2-36"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, inference=&quot;</span><span class="op">);</span></span>
<span id="cb2-37"><a href="#cb2-37"></a>        Serial<span class="op">.</span>print<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>inference<span class="op">);</span></span>
<span id="cb2-38"><a href="#cb2-38"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, postpocess=&quot;</span><span class="op">);</span></span>
<span id="cb2-39"><a href="#cb2-39"></a>        Serial<span class="op">.</span>println<span class="op">(</span>AI<span class="op">.</span>perf<span class="op">().</span>postprocess<span class="op">);</span></span>
<span id="cb2-40"><a href="#cb2-40"></a>        <span class="dt">int</span> pred_index <span class="op">=</span> AI<span class="op">.</span>classes<span class="op">()[</span><span class="dv">0</span><span class="op">].</span>target<span class="op">;</span></span>
<span id="cb2-41"><a href="#cb2-41"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;Result= Label: &quot;</span><span class="op">);</span></span>
<span id="cb2-42"><a href="#cb2-42"></a>        Serial<span class="op">.</span>print<span class="op">(</span>pred_index<span class="op">);</span></span>
<span id="cb2-43"><a href="#cb2-43"></a>        Serial<span class="op">.</span>print<span class="op">(</span><span class="st">&quot;, score=&quot;</span><span class="op">);</span></span>
<span id="cb2-44"><a href="#cb2-44"></a>        Serial<span class="op">.</span>println<span class="op">(</span>AI<span class="op">.</span>classes<span class="op">()[</span><span class="dv">0</span><span class="op">].</span>score<span class="op">);</span></span>
<span id="cb2-45"><a href="#cb2-45"></a>        turn_on_leds<span class="op">(</span>pred_index<span class="op">);</span></span>
<span id="cb2-46"><a href="#cb2-46"></a>    <span class="op">}</span></span>
<span id="cb2-47"><a href="#cb2-47"></a><span class="op">}</span></span>
<span id="cb2-48"><a href="#cb2-48"></a></span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="co">/**</span></span>
<span id="cb2-50"><a href="#cb2-50"></a><span class="co">* </span><span class="an">@brief</span><span class="co"> turn_off_leds function - turn-off all LEDs</span></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="co">*/</span></span>
<span id="cb2-52"><a href="#cb2-52"></a><span class="dt">void</span> turn_off_leds<span class="op">(){</span></span>
<span id="cb2-53"><a href="#cb2-53"></a>    digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-54"><a href="#cb2-54"></a>    digitalWrite<span class="op">(</span>LEDY<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-55"><a href="#cb2-55"></a>    digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> LOW<span class="op">);</span></span>
<span id="cb2-56"><a href="#cb2-56"></a><span class="op">}</span></span>
<span id="cb2-57"><a href="#cb2-57"></a></span>
<span id="cb2-58"><a href="#cb2-58"></a><span class="co">/**</span></span>
<span id="cb2-59"><a href="#cb2-59"></a><span class="co">* </span><span class="an">@brief</span><span class="co"> turn_on_leds function used to turn on a specific LED</span></span>
<span id="cb2-60"><a href="#cb2-60"></a><span class="co">* </span><span class="an">@param[in]</span><span class="co">  </span><span class="cv">pred_index</span></span>
<span id="cb2-61"><a href="#cb2-61"></a><span class="co">*             label 0: [0] ==&gt; Red ON</span></span>
<span id="cb2-62"><a href="#cb2-62"></a><span class="co">*             label 1: [1] ==&gt; Green ON</span></span>
<span id="cb2-63"><a href="#cb2-63"></a><span class="co">*             label 2: [2] ==&gt; Yellow ON</span></span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="co">*/</span></span>
<span id="cb2-65"><a href="#cb2-65"></a><span class="dt">void</span> turn_on_leds<span class="op">(</span><span class="dt">int</span> pred_index<span class="op">)</span> <span class="op">{</span></span>
<span id="cb2-66"><a href="#cb2-66"></a>    <span class="cf">switch</span> <span class="op">(</span>pred_index<span class="op">)</span></span>
<span id="cb2-67"><a href="#cb2-67"></a>    <span class="op">{</span></span>
<span id="cb2-68"><a href="#cb2-68"></a>        <span class="cf">case</span> <span class="dv">0</span><span class="op">:</span></span>
<span id="cb2-69"><a href="#cb2-69"></a>            turn_off_leds<span class="op">();</span></span>
<span id="cb2-70"><a href="#cb2-70"></a>            digitalWrite<span class="op">(</span>LEDR<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-71"><a href="#cb2-71"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb2-72"><a href="#cb2-72"></a>        <span class="cf">case</span> <span class="dv">1</span><span class="op">:</span></span>
<span id="cb2-73"><a href="#cb2-73"></a>            turn_off_leds<span class="op">();</span></span>
<span id="cb2-74"><a href="#cb2-74"></a>            digitalWrite<span class="op">(</span>LEDG<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-75"><a href="#cb2-75"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb2-76"><a href="#cb2-76"></a>        <span class="cf">case</span> <span class="dv">2</span><span class="op">:</span></span>
<span id="cb2-77"><a href="#cb2-77"></a>            turn_off_leds<span class="op">();</span></span>
<span id="cb2-78"><a href="#cb2-78"></a>            digitalWrite<span class="op">(</span>LEDY<span class="op">,</span> HIGH<span class="op">);</span></span>
<span id="cb2-79"><a href="#cb2-79"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb2-80"><a href="#cb2-80"></a>        <span class="cf">case</span> <span class="dv">3</span><span class="op">:</span></span>
<span id="cb2-81"><a href="#cb2-81"></a>            turn_off_leds<span class="op">();</span></span>
<span id="cb2-82"><a href="#cb2-82"></a>            <span class="cf">break</span><span class="op">;</span></span>
<span id="cb2-83"><a href="#cb2-83"></a>    <span class="op">}</span></span>
<span id="cb2-84"><a href="#cb2-84"></a><span class="op">}</span></span></code></pre></div>
<p>We should connect the Grove Vision AI V2 with the XIAO using its I2C Grove connector. For the XIAO, we will use an <a href="https://wiki.seeedstudio.com/Seeeduino-XIAO-Expansion-Board/">Expansion Board</a> for the facility (although it is possible to connect the I2C directly to the XIAO’s pins). We will power the boards using the USB-C connector, but a battery can also be used.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file774.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<p>Here is the result:</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="../media/file775.png" class="quarto-figure quarto-figure-center" style="width:80.0%" alt="" /></p>
</figure>
</div>
<blockquote>
<p>The power consumption reached a peak of 240 mA (Green LED), equivalent to 1.2 W. Driving the Yellow and Red LEDs consumes 14 mA, equivalent to 0.7 W. Sending information to the terminal via serial has no impact on power consumption.</p>
</blockquote>
</section>
</section>
<section id="sec-image-classification-summary-dba5" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<p>In this lab, we’ve explored the complete process of developing an image classification system using the Seeed Studio Grove Vision AI Module V2 powered by the Himax WiseEye2 chip. We’ve walked through every stage of the machine learning workflow, from defining our project goals to deploying a working model with real-world interactions.</p>
<p>The Grove Vision AI V2 has demonstrated impressive performance, with inference times of just 4-5ms, dramatically outperforming other common tinyML platforms. Our benchmark comparison showed it to be approximately 14 times faster than ARM-M7 devices and over 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi Zero W2, the Edge TPU architecture delivered nearly twice the speed while consuming less power.</p>
<p>Through this project, we’ve seen how transfer learning enables us to achieve good classification results with a relatively small dataset of custom images. The MobileNetV2 model with an alpha of 0.1 provided an excellent balance of accuracy and efficiency for our three-class problem, requiring only 146 KB of RAM and 187 KB of Flash memory, well within the capabilities of the Grove Vision AI Module V2’s 2.4 MB internal SRAM.</p>
<p>We also explored several deployment options, from viewing inference results through the SenseCraft AI Studio to creating a standalone system with visual feedback using LEDs. The ability to stream video to a web browser and process inference results locally demonstrates the versatility of edge AI systems for real-world applications.</p>
<p>The power consumption of our final system remained impressively low, ranging from approximately 70mA (0.4W) for basic inference to 240mA (1.2W) when driving external components. This efficiency makes the Grove Vision AI Module V2 an excellent choice for battery-powered applications where power consumption is critical.</p>
<p>This lab has demonstrated that sophisticated computer vision tasks can now be performed entirely at the edge, without reliance on cloud services or powerful computers. With tools like Edge Impulse Studio and SenseCraft AI Studio, the development process has become accessible even to those without extensive machine learning expertise.</p>
<p>As edge AI technology continues to evolve, we can expect even more powerful capabilities from compact, energy-efficient devices like the Grove Vision AI Module V2, opening up new possibilities for smart sensors, IoT applications, and embedded intelligence in everyday objects.</p>
</section>
<section id="sec-image-classification-resources-db82" class="level2 unnumbered">
<h2 class="unnumbered">Resources</h2>
<p><a href="https://sensecraft.seeed.cc/ai/training">Collecting Images with SenseCraft AI Studio</a>.</p>
<p><a href="https://studio.edgeimpulse.com/public/712491/live">Edge Impulse Studio Project</a></p>
<p><a href="https://sensecraft.seeed.cc/ai/device/local/36">SenseCraft AI Studio - Vision Workplace (Deploy Models)</a></p>
<p><a href="https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/Grove/Grove_Sensors/AI-powered/Grove-vision-ai-v2/Development/grove-vision-ai-v2-himax-sdk.md">Other Himax examples</a></p>
<p><a href="https://github.com/Mjrovai/Seeed-Grove-Vision-AI-V2/tree/main/Arduino_Sketches">Arduino Sketches</a></p>
</section>
</section>
</body>
</html>
