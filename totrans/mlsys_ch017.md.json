["```py\ndense = Dense(512)(input_tensor)\n```", "```py\noutput = matmul(input_weights) + bias\noutput = activation(output)\n```", "```py\nfor n in range(batch_size):\n    for m in range(output_size):\n        sum = bias[m]\n        for k in range(input_size):\n            sum += input[n, k] * weights[k, m]\n        output[n, m] = activation(sum)\n```", "```py\nlayer = nn.Linear(256, 512)  # 256 inputs to\n# 512 outputs\noutput = layer(input_tensor)  # Process a batch of inputs\n```", "```py\nZ = matmul(weights, input) + bias  # Each output needs all inputs\noutput = activation(Z)  # Transform each result\n```", "```py\nfor batch in range(32):            # Process 32 samples at once\n    for out_neuron in range(512):  # Compute each output neuron\n        sum = 0.0\n        for in_feature in range(256): # Each output needs\n                                      # all inputs\n            sum += input[batch, in_feature] *\n                         weights[out_neuron, in_feature]\n        output[batch, out_neuron] = activation(sum +\n                                    bias[out_neuron])\n```", "```py\nvsetvli t0, a0, e32   # Process 8 elements at once\nloop_batch:\n    loop_neuron:\n        vxor.vv v0, v0, v0    # Clear 8 accumulators\n        loop_feature:\n            vle32.v v1, (in_ptr)    # Load 8 inputs together\n            vle32.v v2, (wt_ptr)    # Load 8 weights together\n            vfmacc.vv v0, v1, v2    # 8 multiply-adds at once\n            add in_ptr, in_ptr, 32  # Move to next 8 inputs\n            add wt_ptr, wt_ptr, 32  # Move to next 8 weights\n            bnez feature_cnt, loop_feature\n```", "```py\nlayer = nn.Linear(256, 512)  # Layer transforms 256 inputs to\n# 512 outputs\noutput = layer(input_batch)  # Process a batch of 32 samples\n\n# Framework Internal: Core operations\nZ = matmul(weights, input)  # Matrix: transforms [256 x 32]\n# input to [512 x 32] output\nZ = Z + bias  # Vector: adds bias to each\n# output independently\noutput = relu(Z)  # Vector: applies activation to\n# each element independently\n```", "```py\nhidden = matmul(weights, inputs)\n# weights: [out_dim x in_dim], inputs: [in_dim x batch]\n# Result combines all inputs for each output\n\n# Attention Mechanisms - Multiple matrix operations\nQ = matmul(Wq, inputs)\n# Project inputs to query space [query_dim x batch]\nK = matmul(Wk, inputs)\n# Project inputs to key space[key_dim x batch]\nattention = matmul(Q, K.T)\n# Compare all queries with all keys [query_dim x key_dim]\n\n# Convolutions - Matrix multiply after reshaping\npatches = im2col(input)\n# Convert [H x W x C] image to matrix of patches\noutput = matmul(kernel, patches)\n# Apply kernels to all patches simultaneously\n```", "```py\nmload mr1, (weight_ptr)     # Load e.g., 16x16 block of\n                            # weight matrix\nmload mr2, (input_ptr)      # Load corresponding input block\nmatmul.mm mr3, mr1, mr2     # Multiply and accumulate entire\n                            # blocks at once\nmstore (output_ptr), mr3    # Store computed output block\n```", "```py\nlayer = nn.Sequential(\n    nn.Linear(256, 512), nn.ReLU(), nn.BatchNorm1d(512)\n)\noutput = layer(input_tensor)\n```", "```py\nZ = matmul(weights, input) + bias  # Linear transformation\nH = max(0, Z)  # ReLU activation\nmean = reduce_mean(H, axis=0)  # BatchNorm statistics\nvar = reduce_mean((H - mean) ** 2)  # Variance computation\noutput = gamma * (H - mean) / sqrt(var + eps) + beta\n# Normalization\n```", "```py\nfor batch in range(32):\n    for feature in range(512):\n       # ReLU: Requires branch prediction and potential\n       # pipeline stalls\n       z = matmul_output[batch, feature]\n       h = max(0.0, z)    # Conditional operation\n\n       # BatchNorm: Multiple passes over data\n       mean_sum[feature] += h    # First pass for mean\n       var_sum[feature] += h * h # Additional pass for variance\n\n       temp[batch, feature] = h  # Extra memory storage needed\n\n# Normalization requires complex arithmetic\nfor feature in range(512):\n    mean = mean_sum[feature] / batch_size\n    var = (var_sum[feature] / batch_size) - mean * mean\n\n    # Square root computation: Multiple iterations\n    scale = gamma[feature] / sqrt(var + eps)  # Iterative\n                                              # approximation\n    shift = beta[feature] - mean * scale\n\n    # Additional pass over data for final computation\n    for batch in range(32):\n        output[batch, feature] = temp[batch, feature] *\n                                 scale + shift\n```", "```py\nvld.v v1, (input_ptr)    # Load vector of values\nvrelu.v v2, v1           # Single-cycle ReLU on entire vector\nvsigm.v v3, v1           # Fixed-latency sigmoid computation\nvtanh.v v4, v1           # Direct hardware tanh implementation\nvrsqrt.v v5, v1          # Fast reciprocal square root\n```", "```py\nptrue p0.s              # Create predicate for vector length\nld1w z0.s, p0/z, [x0]   # Load vector of inputs\nfmul z1.s, z0.s, z0.s   # Multiply elements\nfadd z2.s, z1.s, z0.s   # Add elements\nst1w z2.s, p0, [x1]     # Store results\n```", "```py\n__global__ void matrix_multiply(float* C, float* A, float*\n                                B, int N) {\n    // Each thread processes one output element\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float sum = 0.0f;\n    for (int k = 0; k < N; k++) {\n        // Threads in a warp execute in parallel\n        sum += A[row * N + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n}\n```", "```py\n\nTensor Core Operation (NVIDIA A100):\nmma.sync.aligned.m16n16k16.f16.f16\n  {d0,d1,d2,d3},     // Destination registers\n  {a0,a1,a2,a3},     // Source matrix A\n  {b0,b1,b2,b3},     // Source matrix B\n  {c0,c1,c2,c3}      // Accumulator\n```", "```py\n## Matrix multiplication where:\n## weights: [512 x 256] - model parameters\n## input:   [256 x 32]  - batch of activations\n## Z:       [512 x 32]  - output activations\n\n## Computing each output element Z[i,j]:\nfor i in range(512):\n    for j in range(32):\n        for k in range(256):\n            Z[i, j] += weights[i, k] * input[k, j]\n```", "```py\n## Weight Stationary Matrix Multiplication\n## - Weights remain fixed in local memory\n## - Input activations stream through\n## - Partial sums accumulate for final output\n\nfor weight_block in weights:  # Load and keep weights stationary\n    load_to_local(weight_block)  # Fixed in local storage\n    for input_block in inputs:  # Stream inputs dynamically\n        for output_block in outputs:  # Compute results\n            output_block += compute(weight_block, input_block)\n            # Reuse weights across inputs\n```", "```py\n## - Partial sums remain in local memory\n## - Weights and input activations stream through dynamically\n## - Final outputs are written only once\n\nfor output_block in outputs:  # Keep partial sums stationary\n    accumulator = 0  # Initialize accumulation buffer\n    for weight_block, input_block in zip(weights, inputs):\n        accumulator += compute(weight_block, input_block)\n        # Accumulate partial sums\n    store_output(accumulator)  # Single write to memory\n```", "```py\n## - Input activations remain in local memory\n## - Weights stream through dynamically\n## - Partial sums accumulate and are written out\n\nfor input_block in inputs:  # Keep input activations stationary\n    load_to_local(input_block)  # Fixed in local storage\n    for weight_block in weights:  # Stream weights dynamically\n        for output_block in outputs:  # Compute results\n            output_block += compute(weight_block, input_block)\n            # Reuse inputs across weights\n```", "```py\nimport torch\n\n## Input tensor\nX = torch.randn(1024, 1024).cuda()\n\n## Step-by-step execution (naÃ¯ve approach)\nX1 = torch.relu(X)  # Intermediate tensor stored\n# in memory\nX2 = torch.batch_norm(X1)  # Another intermediate tensor stored\nY = 2.0 * X2 + 1.0  # Final result\n```", "```py\nfor i in range(N):\n    for j in range(N):\n        for k in range(N):\n            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching\n            # A[i, k] and B[k, j]\n```", "```py\nfor i in range(N):\n    for j in range(N):\n        for k in range(N):\n            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching\n            # A[i, k] and B[k, j]\n```", "```py\nTILE_SIZE = 32  # Choose a tile size based on\n# hardware constraints\n\nfor i in range(0, N, TILE_SIZE):\n    for j in range(0, N, TILE_SIZE):\n        for k in range(0, N, TILE_SIZE):\n            # Compute the submatrix\n            # C[i:i+TILE_SIZE, j:j+TILE_SIZE]\n            for ii in range(i, i + TILE_SIZE):\n                for jj in range(j, j + TILE_SIZE):\n                    for kk in range(k, k + TILE_SIZE):\n                        C[ii, jj] += A[ii, kk] * B[kk, jj]\n```", "```py\nTILE_SIZE = 32  # Tile size chosen based on available\n# fast memory\n\nfor i in range(0, N, TILE_SIZE):\n    for j in range(0, N, TILE_SIZE):\n        for k in range(0, N, TILE_SIZE):\n            # Process a submatrix (tile) at a time\n            for ii in range(i, i + TILE_SIZE):\n                for jj in range(j, j + TILE_SIZE):\n                    for kk in range(k, k + TILE_SIZE):\n                        C[ii, jj] += A[ii, kk] * B[kk, jj]\n```", "```py\nfor i in range(0, N, TILE_SIZE):\n    for j in range(0, N, TILE_SIZE):\n        for k in range(0, N, TILE_SIZE):\n            # Load tile into fast memory before computation\n            A_tile = A[i:i+TILE_SIZE, k:k+TILE_SIZE]\n            B_tile = B[k:k+TILE_SIZE, j:j+TILE_SIZE]\n\n            for ii in range(TILE_SIZE):\n                for jj in range(TILE_SIZE):\n                    for kk in range(TILE_SIZE):\n                        C[i+ii, j+jj] += A_tile[ii, kk] *\n                                         B_tile[kk, jj]\n```"]