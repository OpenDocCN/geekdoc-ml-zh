- en: K-means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_clustering.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_clustering.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **K-means Clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cluster Analysis](https://youtu.be/oFE10cLl0Fs?si=AwmYnrYggtYWGV2n)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Issues with k-Means Clustering](https://youtu.be/ysJw8M_J40I?si=EIlg2941QrfAt7zE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Density-based Clustering](https://www.youtube.com/watch?v=3GaLe8HaDMc&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael‚Äôs Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixing distinct populations to train prediction models often reduces model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the computer does not provide meaning nor description of the groups, that is
    our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e11056b222d3f45c1c5085264d61d033.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset divided into 5 groups.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, the above figure is misleading. If we calculate the boundaries as shown
    above, we would actually have a predictive classification model for the any new
    cases,
  prefs: []
  type: TYPE_NORMAL
- en: 'given normalized porosity of 0.7 and noramlized acoustic impedance of 0.18
    classify as group #5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering does not do this, it is an inferential, unsupervised machine learning
    method,
  prefs: []
  type: TYPE_NORMAL
- en: it is learning structures in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering assigns labels to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0a3e0f44dd8a52deff44dfb5728028a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset with the 5 groups labels assigned with clustering indicated
    by color.
  prefs: []
  type: TYPE_NORMAL
- en: Inferential Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no response features, \(y\), just predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëã_1,\ldots,ùëã_ùëö \]
  prefs: []
  type: TYPE_NORMAL
- en: Machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We focus on inference of the population, the natural system, instead of prediction
    of response features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The K-means clustering approach is primarily applied as an unsupervised machine
    learning method for clustering, group assignment to unlabeled data, where dissimilarity
    within clustered groups is minimized. The loss function that is minimized for
    K-means clustering, known as intertia, is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a schematic illustration of this loss function, summing the distances
    between group prototypes and the samples in the groups,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/565225902b6850e1dc7b0e7753431594.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic of the K-means clustering loss function, intertia. Note, for clarity
    only some of the distances from samples to prototypes are annotated and some of
    the data labels are included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for K-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution Heuristic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first define a solution heuristic,
  prefs: []
  type: TYPE_NORMAL
- en: '**Heuristic** - a shortcut the sacrifices accuracy for practicality, i.e.,
    the solution is usually good enough and has a reasonable run time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the K-means clustering problem of assigning one of \(k\) categorical labels
    to \(n\) sample data, the solution space includes,
  prefs: []
  type: TYPE_NORMAL
- en: \[ k^n \]
  prefs: []
  type: TYPE_NORMAL
- en: possible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means clustering solution heuristic includes these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign initial random prototype with labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/126504196c9d4714198240831b985280.png)'
  prefs: []
  type: TYPE_IMG
- en: \(k\) random prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Assign samples to the nearest prototype.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4cbb87fb2a0d2f760c85f99185e8a13c.png)'
  prefs: []
  type: TYPE_IMG
- en: Assign samples to the \(k\) nearest prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Update prototype based on centroids of samples belonging to this prototype.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/2a4b59e292816e768aa90758a4e6e622.png)'
  prefs: []
  type: TYPE_IMG
- en: Update \(k\) prototypes to centroids of assigned data.
  prefs: []
  type: TYPE_NORMAL
- en: Iterate (return to step 2) until no sample assignments change (prototypes stop
    moving).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e2996c8a1c128b6988b22181f0bcad16.png)'
  prefs: []
  type: TYPE_IMG
- en: Assignment and update iterations until heuristic solution convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I start with a ‚Äòby-hand‚Äô approach to calculate the K-means clustering group
    assignments.
  prefs: []
  type: TYPE_NORMAL
- en: this allows us to be able to watch the method in action, as opposed to just
    getting a result. I think this is more instructive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: afterwards, I show the function from scikit-learn to complete the calculation
    in one line of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have also developed a set of [interactive k-means clustering dashboards](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_kMeans_Clustering.ipynb)
    to explore the k-mean clustering heuristic, including this one that shows each
    solution for multiple random prototypes and the cumulative distribution function
    of intertia to explore the consistency in the solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f8aec16054f230eb82b570adfb69ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: Interactive Python dashboard to explore the performance of the k-means clustering
    solution heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: Note, for the workflow below I have modified code from the tutorial provided
    by Ben Keen as functions to take care of the steps (assign training data to the
    nearest prototype, update the prototype to the centroid of the assigned data).
  prefs: []
  type: TYPE_NORMAL
- en: The original tutorial is available at [here](http://benalexkeen.com/k-means-clustering-in-python).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My modifications tailor this workflow for my specific example, and to include
    the normalized and original data. Appreciation to Ben!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following functions perform the steps required by K-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: assign the training data to the nearest prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the prototype to the centroid of the assigned training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don‚Äôt be concerned if you don‚Äôt understand the code, we have used some advanced
    approaches for the benefit of concise code.
  prefs: []
  type: TYPE_NORMAL
- en: I also added a convenience function to add major and minor gridlines to improve
    plot interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äò12_sample_data.csv‚Äô.
    It is a comma delimited file with:'
  prefs: []
  type: TYPE_NORMAL
- en: X and Y coordinates (\(m\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies 0 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (fraction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (\(mD\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (\(\frac{kg}{m^3} \cdot \frac{m}{s} \cdot 10^3\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòdf‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‚Äòread_csv‚Äô with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: but read csv has required input parameters. The essential one is the name of
    the file. For our circumstance all the other default parameters are fine. If you
    want to see all the possible parameters for this function, just go to the docs
    [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: The docs are always helpful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is often a lot of flexibility for Python functions, possible through using
    various inputs parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data. So
    we have to specify the name / variable representing that new object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs run this command to load the data and then this command to extract a random
    subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We do this to reduce the number of data for ease of visualization (hard to see
    if too many points on our plots).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X | 144.0 | 3910.668909 | 2563.309885 | 6.161100 | 1978.194998 | 3500.392517
    | 5502.096026 | 9703.495538 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 144.0 | 5093.498955 | 2861.683625 | 491.789431 | 2503.509193 | 5393.325918
    | 7596.505416 | 9897.863401 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 144.0 | 0.618056 | 0.487559 | 0.000000 | 0.000000 | 1.000000 | 1.000000
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 144.0 | 0.191045 | 0.031262 | 0.133681 | 0.165889 | 0.185460 |
    0.220655 | 0.261091 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 144.0 | 568.892892 | 1265.582175 | 0.035608 | 9.382939 | 64.000905
    | 425.323240 | 7452.343369 |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 144.0 | 3749.924448 | 821.100292 | 1746.387548 | 3131.159498 | 3686.800017
    | 4292.981181 | 5725.525232 |'
  prefs: []
  type: TYPE_TB
- en: Normalize the Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two features are quite incompatible. They have dramatically different:'
  prefs: []
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We should normalize each feature to range between 0 to 1\. The equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_i^{\prime} = \frac{ \left( x_i - min(x) \right)}{\left( max(x) - min(x)
    \right)} \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: This is a distribution, shift, stretch or squeeze without any distribution shape
    change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we can use these normalized values for calculating distance in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: to remove the influence of magnitude and range on our similarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Of course there is a normalize function in scikit-learn, but we did this ‚Äòby-hand‚Äô
    this first time to ensure the operation is perfectly clear.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X | 144.0 | 3910.668909 | 2563.309885 | 6.161100 | 1978.194998 | 3500.392517
    | 5502.096026 | 9703.495538 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 144.0 | 5093.498955 | 2861.683625 | 491.789431 | 2503.509193 | 5393.325918
    | 7596.505416 | 9897.863401 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 144.0 | 0.618056 | 0.487559 | 0.000000 | 0.000000 | 1.000000 | 1.000000
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 144.0 | 0.191045 | 0.031262 | 0.133681 | 0.165889 | 0.185460 |
    0.220655 | 0.261091 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 144.0 | 568.892892 | 1265.582175 | 0.035608 | 9.382939 | 64.000905
    | 425.323240 | 7452.343369 |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 144.0 | 3749.924448 | 821.100292 | 1746.387548 | 3131.159498 | 3686.800017
    | 4292.981181 | 5725.525232 |'
  prefs: []
  type: TYPE_TB
- en: '| Norm_Porosity | 144.0 | 0.450230 | 0.245367 | 0.000000 | 0.252789 | 0.406397
    | 0.682631 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Norm_AI | 144.0 | 0.503510 | 0.206351 | 0.000000 | 0.348008 | 0.487646 |
    0.639986 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: Extract Features of Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs slice out the porosity and acoustic impedance features and then look
    at the resulting DataFrame to ensure that we loaded and reformatted as expected.
  prefs: []
  type: TYPE_NORMAL
- en: I often separate only the feature of interest to simplify my workflows and to
    reduce the probability of blunders, such as accidentally referring to a feature
    not being used in the current workflow!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beware, this is a shallow copy; therefore, any changes to the df_subset DataFrame
    will be reflected in the original df DataFrame. The slice is actually a reference
    to the original DataFrame in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Porosity | AI | Norm_Porosity | Norm_AI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.252772 | 2862.446918 | 0.934709 | 0.280478 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.181580 | 2919.237330 | 0.375944 | 0.294750 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.230303 | 2999.248935 | 0.758358 | 0.314858 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.163732 | 3823.747676 | 0.235860 | 0.522063 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.197078 | 4609.845251 | 0.497583 | 0.719618 |'
  prefs: []
  type: TYPE_TB
- en: Infer Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the summary statistics we can assign a reasonable minimum and maximum for
    each feature.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this for plotting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also set the random number seed to ensure that the program does the
    same thing every time it is run.
  prefs: []
  type: TYPE_NORMAL
- en: Change the seed number for a different result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will set the number of prototypes / clusters, *K*
  prefs: []
  type: TYPE_NORMAL
- en: We define a dictionary with the color code for each cluster, \(k = 1,\ldots,K\).
    Given 7 codes currently, there will be an error if \(K\) is set larger than 7\.
    Add more color codes to the dictionary to allow for more categories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use K-means clustering provide facies based on
    acoustic impedance and porosity predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: This allows use to group rock with similar petrophysical and geophysical properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start by looking at the scatterplot of our training data features, porosity
    and acoustic impedance.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the data in original units and normalized units through this
    entire exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a6ff75dc69e4d9e155cc3824fad6b2ec17861b5e64d9ba32a73f07e4723f8a00.png](../Images/2e2467ebd154caca8e4e52b7300f7694.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating k-Means Clustering By-hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the steps to calculate k-means clusters for our unlabelled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize k Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we will assign k prototypes in the feature space randomly.
  prefs: []
  type: TYPE_NORMAL
- en: for K prototypes assign a random porosity and acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: don‚Äôt worry, these prototypes won‚Äôt make much sense initially, but they will
    improve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will do this and then visualize the prototypes as red, green, blue etc. dots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/89aa79fe89eb7252de4eaaa96655d8cab72c87945bda9896b49d5fb0e1ff3d02.png](../Images/02b4099e179d93c3172e40e919bad5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Assignment of Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All training data are assigned to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: recall we have a function to do this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: we work with the normalized features and visualize normalized and original features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/54cb330a74898631faa71c40d5e5871042e52db556b1804a0b581917e8135eb9.png](../Images/1c02ec498a0c88f40f35392dc8f96d75.png)'
  prefs: []
  type: TYPE_IMG
- en: Update the Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we reassign the prototypes to the centroids of the training data belonging
    to each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d407ac38fa40f3583c66b8c936ef6fa4d3520c2125e0c75a4033f08f6c812f5f.png](../Images/1d1f241b430124c478d75f34f681377c.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the Assignment of the Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once again we assign the training data to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Note the prototypes were updated in the previous step so the assignments may
    change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f940da261c3075cbde5da1809f680248e34d82d4c1e57dda341ca76265cc6242.png](../Images/6d0949fbf0d11568ea7c8a5e9d800441.png)'
  prefs: []
  type: TYPE_IMG
- en: Iterate Until Convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we iterate over the previous set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: assign the training data to the nearest prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the prototypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do this until there is no further chance in the category assigned to each
    of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ba1c9c046617e225fb4e6fcedc7cd0ccd6b4d1e017a1b4199b12b40d97b845f3.png](../Images/4aae6daa9fe09b698417a91fee1405bc.png)'
  prefs: []
  type: TYPE_IMG
- en: k-Means Clustering with the scikit-learn Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs repeat with the scikit-learn function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e78be07d00c4ccada7b5914d3f073c3fe4155b63472a216aea3ceef74e3769d4.png](../Images/c1d55c5d935fcc425dba10a6471708c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Optimum Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For K-means clustering, the K number of clusters is an input.
  prefs: []
  type: TYPE_NORMAL
- en: in some cases this choice is easy or constrained by the context of the problem,
    for example, we know that there are 3 distinct facies in our subsurface dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this can be seen as a strength for K-means clustering as we can use our expert
    knowledge to constrain the number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or this may be seen as a weekness for K-means clustering as the method is unable
    to determine the number of clusters, as with methods such as DBSCAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the number of clusters, K, is not know there are multiple methods to quantitatively
    determine an optimum K value, including,
  prefs: []
  type: TYPE_NORMAL
- en: '**Elbow Method** - perform k-means clustering for a range of K values and the
    within-cluster sum of squares (WCSS) or inertia (the sum of squared distances
    from each point to its assigned cluster centroid) against the number of clusters
    (K) and look for the ‚Äúelbow‚Äù point where the rate of decrease in WCSS slows down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silhouette Score** - the silhouette score measures how similar an object
    is to its own cluster compared to other clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elbow Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Elbow Method** is a heuristic used to determine the optimal number of
    clusters, \(K\), in a clustering algorithm, such as **K-Means**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to run the clustering algorithm for a range of cluster values \(K\),
    calculate a performance metric, and plot it. The **elbow** point on the plot represents
    the optimal number of clusters, where adding more clusters doesn‚Äôt significantly
    improve the model, that is, further reduce the loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method is based on the K-means loss function, inertia, also known more decriptively
    as Within-Cluster Sum of Squares (WCSS), once again this is,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method proceeds with the following steps,
  prefs: []
  type: TYPE_NORMAL
- en: '**Fit K-Means for different values of \(K\)**, select a range of possible values
    for (K) and for each value of (K), fit the K-Means algorithm and calculate the
    inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Plot the Intertia vs. \(K\)**, create a plot where the x-axis represents
    the number of clusters (\(K\)) and the y-axis represents the inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Look for the ‚Äúelbow‚Äù**, The **elbow** point on the plot is where the decrease
    in WCSS starts to slow down, indicating that increasing the number of clusters
    beyond this point provides diminishing returns in terms of reducing inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages of the elbow method include,
  prefs: []
  type: TYPE_NORMAL
- en: For K-means clustering the exact loss function is used for consistency, i.e.,
    the fit is minimizes inertia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of sihouette score include,
  prefs: []
  type: TYPE_NORMAL
- en: The inertia method, can only be applied to K means clustering, but an elbow
    approach with k-distance plots is applied with density-based clustering, for example,
    DBSCAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not provide clear results for complex datasets with overlapping or irregularly
    shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9b4490f494f9ed350eba4adf028c03f3f0e3c8bd4f03d40ce20e2694e80ad8e0.png](../Images/a2556b3ae48b8ab3050eea9979bef29a.png)'
  prefs: []
  type: TYPE_IMG
- en: in this case there is a continuous change in gradient and not a clear elbow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silhouette Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Description: The Silhouette Score measures how similar an object is to its
    own cluster compared to other clusters. How it works: Compute the silhouette score
    for each K value. The silhouette score ranges from -1 (poor fit) to +1 (good fit),
    where higher values indicate better clustering. Interpretation: The optimal K
    corresponds to the highest average silhouette score. Pros: Provides both cohesion
    and separation metrics. Cons: Can be computationally expensive for large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The silhouette calculation proceeds as, for a given data point (i), the silhouette
    score is calculated using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **average distance within the same cluster**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ a(i) = \frac{1}{|C_i| - 1} \sum_{j \in C_i, j \neq i} d(i, j) \]
  prefs: []
  type: TYPE_NORMAL
- en: where, \(C_i\) is the cluster containing point \(i\), \(d(i, j)\) is the Euclidean
    distance between points \(i\) and \(j\), and \(C_i|\) is the number of points
    in the cluster \(C_i\).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **average distance to the nearest cluster**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ b(i) = \min_{C_k \neq C_i} \left( \frac{1}{|C_k|} \sum_{j \in C_k} d(i, j)
    \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(C_k\) is any other cluster, different from the cluster \(C_i\) that
    contains point \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **silhouette score for point \(i\)**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(s(i)\) is the silhouette score for point \(i\), \(a(i)\) is the average
    distance to points in the same cluster, \(b(i)\) is the average distance to points
    in the nearest neighboring cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now over all points, calculate the **overall silhouette score**, the average
    silhouette score of all points,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ S = \frac{1}{n} \sum_{i=1}^{n} s(i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n\) is the number of data points, \(s(i)\) is the silhouette score of
    point \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: We interpret overall silhouette score values as,
  prefs: []
  type: TYPE_NORMAL
- en: '**+1**, the points on average are well clustered and far from neighboring clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0**, the points on average are on or very close to the boundary between two
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-1**, the points on average are likely assigned to the wrong cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we repeat the overall silhouette score calculation on a range of \(K\) values
    and select the maximium silhouette score case as the optimum \(K\).
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of the silhouette method include,
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette score provides an understanding of both the compactness and separation
    of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be applied to any clustering algorithm, not just K-Means, i.e., the input
    is the data and cluster assignments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of sihouette score include,
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette method is computationally expensive, especially for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not provide clear results for complex datasets with overlapping or irregularly
    shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d832b7eb61524e55f01f2c0e1d3511d436008f09a451ec121046dce075d36982.png](../Images/308dcd3974b7408e9fac2639f930aba5.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering without Normalization / Standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the critical assumptions of clustering is that the variability is the
    same over each feature.
  prefs: []
  type: TYPE_NORMAL
- en: an exception would be if the features have the same units and the variability
    differences are meaningful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs take this dataset and draw it to scale (to show what a distance metric
    would see in original units.
  prefs: []
  type: TYPE_NORMAL
- en: we rotate the plot and provide an approximate visualization with porosity 1
    unit equal to permeability 1 unit on the plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: effectively the dataset looks 1D to the clustering algorithm, difference in
    porosity becomes meaningless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs what our plot looks like with equal aspect ratio between the 2 features.
  prefs: []
  type: TYPE_NORMAL
- en: the plot becomes a line and the data does not show up properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4f6ebe27d62c2cec31298b9304e5761354ba457676c91a47dc292391053a85f5.png](../Images/80633e750a5a3dd85aea3a46d7d66927.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let‚Äôs repeat the previous k-means clustering, but with the original features
    (not normalized).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e4fcddae460e9a2e4a28cbf468199a9b389038debb6e44b45ec723241a313db4.png](../Images/352441df30fb014b45cfc675aa3a7bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: The clusters are only in acoustic impedance since the differences in porosity
    on not significant due to the much larger range for the acoustic impedance feature.
  prefs: []
  type: TYPE_NORMAL
- en: clusters are orthogonal to acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice on a New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ok, time to get to work. Let‚Äôs load up a dataset and perform cluster analysis
    with,
  prefs: []
  type: TYPE_NORMAL
- en: compact code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: basic visaulizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can select any of these datasets or modify the code and add your own to
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: Perform Cluster Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now calculate the silhouette plot to calculate a good number of categories and
    perform K-means clustering with that ‚ÄòK‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0a1ffbd9465edbd37a21e4a5c76af7b2e863639a218bb6dd8f81c3c7fc0824db.png](../Images/f985a9d2dd02fe7bafba136823cb96de.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs visualize the clusters with a matrix scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e994277777d8dfb7eb5faf49d97b1186211bfda69185ac313704f3d2f7d2b8ae.png](../Images/bebbfd16a3687097674b408fed86ec68.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we can choose to write out the DataFrame with the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of cluster analysis. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mixing distinct populations to train prediction models often reduces model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the computer does not provide meaning nor description of the groups, that is
    our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e11056b222d3f45c1c5085264d61d033.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset divided into 5 groups.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, the above figure is misleading. If we calculate the boundaries as shown
    above, we would actually have a predictive classification model for the any new
    cases,
  prefs: []
  type: TYPE_NORMAL
- en: 'given normalized porosity of 0.7 and noramlized acoustic impedance of 0.18
    classify as group #5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering does not do this, it is an inferential, unsupervised machine learning
    method,
  prefs: []
  type: TYPE_NORMAL
- en: it is learning structures in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering assigns labels to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0a3e0f44dd8a52deff44dfb5728028a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset with the 5 groups labels assigned with clustering indicated
    by color.
  prefs: []
  type: TYPE_NORMAL
- en: Inferential Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no response features, \(y\), just predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëã_1,\ldots,ùëã_ùëö \]
  prefs: []
  type: TYPE_NORMAL
- en: Machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We focus on inference of the population, the natural system, instead of prediction
    of response features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The K-means clustering approach is primarily applied as an unsupervised machine
    learning method for clustering, group assignment to unlabeled data, where dissimilarity
    within clustered groups is minimized. The loss function that is minimized for
    K-means clustering, known as intertia, is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a schematic illustration of this loss function, summing the distances
    between group prototypes and the samples in the groups,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/565225902b6850e1dc7b0e7753431594.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic of the K-means clustering loss function, intertia. Note, for clarity
    only some of the distances from samples to prototypes are annotated and some of
    the data labels are included.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for K-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solution Heuristic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first define a solution heuristic,
  prefs: []
  type: TYPE_NORMAL
- en: '**Heuristic** - a shortcut the sacrifices accuracy for practicality, i.e.,
    the solution is usually good enough and has a reasonable run time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the K-means clustering problem of assigning one of \(k\) categorical labels
    to \(n\) sample data, the solution space includes,
  prefs: []
  type: TYPE_NORMAL
- en: \[ k^n \]
  prefs: []
  type: TYPE_NORMAL
- en: possible solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means clustering solution heuristic includes these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign initial random prototype with labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/126504196c9d4714198240831b985280.png)'
  prefs: []
  type: TYPE_IMG
- en: \(k\) random prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Assign samples to the nearest prototype.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4cbb87fb2a0d2f760c85f99185e8a13c.png)'
  prefs: []
  type: TYPE_IMG
- en: Assign samples to the \(k\) nearest prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Update prototype based on centroids of samples belonging to this prototype.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/2a4b59e292816e768aa90758a4e6e622.png)'
  prefs: []
  type: TYPE_IMG
- en: Update \(k\) prototypes to centroids of assigned data.
  prefs: []
  type: TYPE_NORMAL
- en: Iterate (return to step 2) until no sample assignments change (prototypes stop
    moving).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e2996c8a1c128b6988b22181f0bcad16.png)'
  prefs: []
  type: TYPE_IMG
- en: Assignment and update iterations until heuristic solution convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I start with a ‚Äòby-hand‚Äô approach to calculate the K-means clustering group
    assignments.
  prefs: []
  type: TYPE_NORMAL
- en: this allows us to be able to watch the method in action, as opposed to just
    getting a result. I think this is more instructive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: afterwards, I show the function from scikit-learn to complete the calculation
    in one line of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have also developed a set of [interactive k-means clustering dashboards](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_kMeans_Clustering.ipynb)
    to explore the k-mean clustering heuristic, including this one that shows each
    solution for multiple random prototypes and the cumulative distribution function
    of intertia to explore the consistency in the solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f8aec16054f230eb82b570adfb69ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: Interactive Python dashboard to explore the performance of the k-means clustering
    solution heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: Note, for the workflow below I have modified code from the tutorial provided
    by Ben Keen as functions to take care of the steps (assign training data to the
    nearest prototype, update the prototype to the centroid of the assigned data).
  prefs: []
  type: TYPE_NORMAL
- en: The original tutorial is available at [here](http://benalexkeen.com/k-means-clustering-in-python).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My modifications tailor this workflow for my specific example, and to include
    the normalized and original data. Appreciation to Ben!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following functions perform the steps required by K-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: assign the training data to the nearest prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the prototype to the centroid of the assigned training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don‚Äôt be concerned if you don‚Äôt understand the code, we have used some advanced
    approaches for the benefit of concise code.
  prefs: []
  type: TYPE_NORMAL
- en: I also added a convenience function to add major and minor gridlines to improve
    plot interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äò12_sample_data.csv‚Äô.
    It is a comma delimited file with:'
  prefs: []
  type: TYPE_NORMAL
- en: X and Y coordinates (\(m\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies 0 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (fraction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (\(mD\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (\(\frac{kg}{m^3} \cdot \frac{m}{s} \cdot 10^3\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòdf‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‚Äòread_csv‚Äô with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: but read csv has required input parameters. The essential one is the name of
    the file. For our circumstance all the other default parameters are fine. If you
    want to see all the possible parameters for this function, just go to the docs
    [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: The docs are always helpful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is often a lot of flexibility for Python functions, possible through using
    various inputs parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data. So
    we have to specify the name / variable representing that new object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs run this command to load the data and then this command to extract a random
    subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We do this to reduce the number of data for ease of visualization (hard to see
    if too many points on our plots).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X | 144.0 | 3910.668909 | 2563.309885 | 6.161100 | 1978.194998 | 3500.392517
    | 5502.096026 | 9703.495538 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 144.0 | 5093.498955 | 2861.683625 | 491.789431 | 2503.509193 | 5393.325918
    | 7596.505416 | 9897.863401 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 144.0 | 0.618056 | 0.487559 | 0.000000 | 0.000000 | 1.000000 | 1.000000
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 144.0 | 0.191045 | 0.031262 | 0.133681 | 0.165889 | 0.185460 |
    0.220655 | 0.261091 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 144.0 | 568.892892 | 1265.582175 | 0.035608 | 9.382939 | 64.000905
    | 425.323240 | 7452.343369 |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 144.0 | 3749.924448 | 821.100292 | 1746.387548 | 3131.159498 | 3686.800017
    | 4292.981181 | 5725.525232 |'
  prefs: []
  type: TYPE_TB
- en: Normalize the Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two features are quite incompatible. They have dramatically different:'
  prefs: []
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We should normalize each feature to range between 0 to 1\. The equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_i^{\prime} = \frac{ \left( x_i - min(x) \right)}{\left( max(x) - min(x)
    \right)} \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: This is a distribution, shift, stretch or squeeze without any distribution shape
    change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we can use these normalized values for calculating distance in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: to remove the influence of magnitude and range on our similarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Of course there is a normalize function in scikit-learn, but we did this ‚Äòby-hand‚Äô
    this first time to ensure the operation is perfectly clear.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X | 144.0 | 3910.668909 | 2563.309885 | 6.161100 | 1978.194998 | 3500.392517
    | 5502.096026 | 9703.495538 |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 144.0 | 5093.498955 | 2861.683625 | 491.789431 | 2503.509193 | 5393.325918
    | 7596.505416 | 9897.863401 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 144.0 | 0.618056 | 0.487559 | 0.000000 | 0.000000 | 1.000000 | 1.000000
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Porosity | 144.0 | 0.191045 | 0.031262 | 0.133681 | 0.165889 | 0.185460 |
    0.220655 | 0.261091 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 144.0 | 568.892892 | 1265.582175 | 0.035608 | 9.382939 | 64.000905
    | 425.323240 | 7452.343369 |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 144.0 | 3749.924448 | 821.100292 | 1746.387548 | 3131.159498 | 3686.800017
    | 4292.981181 | 5725.525232 |'
  prefs: []
  type: TYPE_TB
- en: '| Norm_Porosity | 144.0 | 0.450230 | 0.245367 | 0.000000 | 0.252789 | 0.406397
    | 0.682631 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| Norm_AI | 144.0 | 0.503510 | 0.206351 | 0.000000 | 0.348008 | 0.487646 |
    0.639986 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: Extract Features of Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs slice out the porosity and acoustic impedance features and then look
    at the resulting DataFrame to ensure that we loaded and reformatted as expected.
  prefs: []
  type: TYPE_NORMAL
- en: I often separate only the feature of interest to simplify my workflows and to
    reduce the probability of blunders, such as accidentally referring to a feature
    not being used in the current workflow!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beware, this is a shallow copy; therefore, any changes to the df_subset DataFrame
    will be reflected in the original df DataFrame. The slice is actually a reference
    to the original DataFrame in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Porosity | AI | Norm_Porosity | Norm_AI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.252772 | 2862.446918 | 0.934709 | 0.280478 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.181580 | 2919.237330 | 0.375944 | 0.294750 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.230303 | 2999.248935 | 0.758358 | 0.314858 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.163732 | 3823.747676 | 0.235860 | 0.522063 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.197078 | 4609.845251 | 0.497583 | 0.719618 |'
  prefs: []
  type: TYPE_TB
- en: Infer Model Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the summary statistics we can assign a reasonable minimum and maximum for
    each feature.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this for plotting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also set the random number seed to ensure that the program does the
    same thing every time it is run.
  prefs: []
  type: TYPE_NORMAL
- en: Change the seed number for a different result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will set the number of prototypes / clusters, *K*
  prefs: []
  type: TYPE_NORMAL
- en: We define a dictionary with the color code for each cluster, \(k = 1,\ldots,K\).
    Given 7 codes currently, there will be an error if \(K\) is set larger than 7\.
    Add more color codes to the dictionary to allow for more categories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use K-means clustering provide facies based on
    acoustic impedance and porosity predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: This allows use to group rock with similar petrophysical and geophysical properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start by looking at the scatterplot of our training data features, porosity
    and acoustic impedance.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the data in original units and normalized units through this
    entire exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a6ff75dc69e4d9e155cc3824fad6b2ec17861b5e64d9ba32a73f07e4723f8a00.png](../Images/2e2467ebd154caca8e4e52b7300f7694.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating k-Means Clustering By-hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the steps to calculate k-means clusters for our unlabelled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize k Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we will assign k prototypes in the feature space randomly.
  prefs: []
  type: TYPE_NORMAL
- en: for K prototypes assign a random porosity and acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: don‚Äôt worry, these prototypes won‚Äôt make much sense initially, but they will
    improve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will do this and then visualize the prototypes as red, green, blue etc. dots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/89aa79fe89eb7252de4eaaa96655d8cab72c87945bda9896b49d5fb0e1ff3d02.png](../Images/02b4099e179d93c3172e40e919bad5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Assignment of Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All training data are assigned to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: recall we have a function to do this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: we work with the normalized features and visualize normalized and original features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/54cb330a74898631faa71c40d5e5871042e52db556b1804a0b581917e8135eb9.png](../Images/1c02ec498a0c88f40f35392dc8f96d75.png)'
  prefs: []
  type: TYPE_IMG
- en: Update the Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we reassign the prototypes to the centroids of the training data belonging
    to each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d407ac38fa40f3583c66b8c936ef6fa4d3520c2125e0c75a4033f08f6c812f5f.png](../Images/1d1f241b430124c478d75f34f681377c.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the Assignment of the Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once again we assign the training data to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Note the prototypes were updated in the previous step so the assignments may
    change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f940da261c3075cbde5da1809f680248e34d82d4c1e57dda341ca76265cc6242.png](../Images/6d0949fbf0d11568ea7c8a5e9d800441.png)'
  prefs: []
  type: TYPE_IMG
- en: Iterate Until Convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we iterate over the previous set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: assign the training data to the nearest prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the prototypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do this until there is no further chance in the category assigned to each
    of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ba1c9c046617e225fb4e6fcedc7cd0ccd6b4d1e017a1b4199b12b40d97b845f3.png](../Images/4aae6daa9fe09b698417a91fee1405bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Initialize k Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First we will assign k prototypes in the feature space randomly.
  prefs: []
  type: TYPE_NORMAL
- en: for K prototypes assign a random porosity and acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: don‚Äôt worry, these prototypes won‚Äôt make much sense initially, but they will
    improve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will do this and then visualize the prototypes as red, green, blue etc. dots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/89aa79fe89eb7252de4eaaa96655d8cab72c87945bda9896b49d5fb0e1ff3d02.png](../Images/02b4099e179d93c3172e40e919bad5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Assignment of Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All training data are assigned to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: recall we have a function to do this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: we work with the normalized features and visualize normalized and original features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/54cb330a74898631faa71c40d5e5871042e52db556b1804a0b581917e8135eb9.png](../Images/1c02ec498a0c88f40f35392dc8f96d75.png)'
  prefs: []
  type: TYPE_IMG
- en: Update the Prototypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we reassign the prototypes to the centroids of the training data belonging
    to each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d407ac38fa40f3583c66b8c936ef6fa4d3520c2125e0c75a4033f08f6c812f5f.png](../Images/1d1f241b430124c478d75f34f681377c.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the Assignment of the Training Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once again we assign the training data to the nearest prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Note the prototypes were updated in the previous step so the assignments may
    change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f940da261c3075cbde5da1809f680248e34d82d4c1e57dda341ca76265cc6242.png](../Images/6d0949fbf0d11568ea7c8a5e9d800441.png)'
  prefs: []
  type: TYPE_IMG
- en: Iterate Until Convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we iterate over the previous set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: assign the training data to the nearest prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the prototypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do this until there is no further chance in the category assigned to each
    of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ba1c9c046617e225fb4e6fcedc7cd0ccd6b4d1e017a1b4199b12b40d97b845f3.png](../Images/4aae6daa9fe09b698417a91fee1405bc.png)'
  prefs: []
  type: TYPE_IMG
- en: k-Means Clustering with the scikit-learn Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs repeat with the scikit-learn function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e78be07d00c4ccada7b5914d3f073c3fe4155b63472a216aea3ceef74e3769d4.png](../Images/c1d55c5d935fcc425dba10a6471708c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Optimum Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For K-means clustering, the K number of clusters is an input.
  prefs: []
  type: TYPE_NORMAL
- en: in some cases this choice is easy or constrained by the context of the problem,
    for example, we know that there are 3 distinct facies in our subsurface dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this can be seen as a strength for K-means clustering as we can use our expert
    knowledge to constrain the number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or this may be seen as a weekness for K-means clustering as the method is unable
    to determine the number of clusters, as with methods such as DBSCAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the number of clusters, K, is not know there are multiple methods to quantitatively
    determine an optimum K value, including,
  prefs: []
  type: TYPE_NORMAL
- en: '**Elbow Method** - perform k-means clustering for a range of K values and the
    within-cluster sum of squares (WCSS) or inertia (the sum of squared distances
    from each point to its assigned cluster centroid) against the number of clusters
    (K) and look for the ‚Äúelbow‚Äù point where the rate of decrease in WCSS slows down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silhouette Score** - the silhouette score measures how similar an object
    is to its own cluster compared to other clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elbow Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Elbow Method** is a heuristic used to determine the optimal number of
    clusters, \(K\), in a clustering algorithm, such as **K-Means**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to run the clustering algorithm for a range of cluster values \(K\),
    calculate a performance metric, and plot it. The **elbow** point on the plot represents
    the optimal number of clusters, where adding more clusters doesn‚Äôt significantly
    improve the model, that is, further reduce the loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method is based on the K-means loss function, inertia, also known more decriptively
    as Within-Cluster Sum of Squares (WCSS), once again this is,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method proceeds with the following steps,
  prefs: []
  type: TYPE_NORMAL
- en: '**Fit K-Means for different values of \(K\)**, select a range of possible values
    for (K) and for each value of (K), fit the K-Means algorithm and calculate the
    inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Plot the Intertia vs. \(K\)**, create a plot where the x-axis represents
    the number of clusters (\(K\)) and the y-axis represents the inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Look for the ‚Äúelbow‚Äù**, The **elbow** point on the plot is where the decrease
    in WCSS starts to slow down, indicating that increasing the number of clusters
    beyond this point provides diminishing returns in terms of reducing inertia.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages of the elbow method include,
  prefs: []
  type: TYPE_NORMAL
- en: For K-means clustering the exact loss function is used for consistency, i.e.,
    the fit is minimizes inertia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of sihouette score include,
  prefs: []
  type: TYPE_NORMAL
- en: The inertia method, can only be applied to K means clustering, but an elbow
    approach with k-distance plots is applied with density-based clustering, for example,
    DBSCAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not provide clear results for complex datasets with overlapping or irregularly
    shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9b4490f494f9ed350eba4adf028c03f3f0e3c8bd4f03d40ce20e2694e80ad8e0.png](../Images/a2556b3ae48b8ab3050eea9979bef29a.png)'
  prefs: []
  type: TYPE_IMG
- en: in this case there is a continuous change in gradient and not a clear elbow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silhouette Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Description: The Silhouette Score measures how similar an object is to its
    own cluster compared to other clusters. How it works: Compute the silhouette score
    for each K value. The silhouette score ranges from -1 (poor fit) to +1 (good fit),
    where higher values indicate better clustering. Interpretation: The optimal K
    corresponds to the highest average silhouette score. Pros: Provides both cohesion
    and separation metrics. Cons: Can be computationally expensive for large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The silhouette calculation proceeds as, for a given data point (i), the silhouette
    score is calculated using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **average distance within the same cluster**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ a(i) = \frac{1}{|C_i| - 1} \sum_{j \in C_i, j \neq i} d(i, j) \]
  prefs: []
  type: TYPE_NORMAL
- en: where, \(C_i\) is the cluster containing point \(i\), \(d(i, j)\) is the Euclidean
    distance between points \(i\) and \(j\), and \(C_i|\) is the number of points
    in the cluster \(C_i\).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **average distance to the nearest cluster**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ b(i) = \min_{C_k \neq C_i} \left( \frac{1}{|C_k|} \sum_{j \in C_k} d(i, j)
    \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(C_k\) is any other cluster, different from the cluster \(C_i\) that
    contains point \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the **silhouette score for point \(i\)**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(s(i)\) is the silhouette score for point \(i\), \(a(i)\) is the average
    distance to points in the same cluster, \(b(i)\) is the average distance to points
    in the nearest neighboring cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now over all points, calculate the **overall silhouette score**, the average
    silhouette score of all points,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ S = \frac{1}{n} \sum_{i=1}^{n} s(i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n\) is the number of data points, \(s(i)\) is the silhouette score of
    point \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: We interpret overall silhouette score values as,
  prefs: []
  type: TYPE_NORMAL
- en: '**+1**, the points on average are well clustered and far from neighboring clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0**, the points on average are on or very close to the boundary between two
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-1**, the points on average are likely assigned to the wrong cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we repeat the overall silhouette score calculation on a range of \(K\) values
    and select the maximium silhouette score case as the optimum \(K\).
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of the silhouette method include,
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette score provides an understanding of both the compactness and separation
    of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be applied to any clustering algorithm, not just K-Means, i.e., the input
    is the data and cluster assignments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of sihouette score include,
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette method is computationally expensive, especially for large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not provide clear results for complex datasets with overlapping or irregularly
    shaped clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d832b7eb61524e55f01f2c0e1d3511d436008f09a451ec121046dce075d36982.png](../Images/308dcd3974b7408e9fac2639f930aba5.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering without Normalization / Standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the critical assumptions of clustering is that the variability is the
    same over each feature.
  prefs: []
  type: TYPE_NORMAL
- en: an exception would be if the features have the same units and the variability
    differences are meaningful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs take this dataset and draw it to scale (to show what a distance metric
    would see in original units.
  prefs: []
  type: TYPE_NORMAL
- en: we rotate the plot and provide an approximate visualization with porosity 1
    unit equal to permeability 1 unit on the plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: effectively the dataset looks 1D to the clustering algorithm, difference in
    porosity becomes meaningless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs what our plot looks like with equal aspect ratio between the 2 features.
  prefs: []
  type: TYPE_NORMAL
- en: the plot becomes a line and the data does not show up properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4f6ebe27d62c2cec31298b9304e5761354ba457676c91a47dc292391053a85f5.png](../Images/80633e750a5a3dd85aea3a46d7d66927.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let‚Äôs repeat the previous k-means clustering, but with the original features
    (not normalized).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e4fcddae460e9a2e4a28cbf468199a9b389038debb6e44b45ec723241a313db4.png](../Images/352441df30fb014b45cfc675aa3a7bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: The clusters are only in acoustic impedance since the differences in porosity
    on not significant due to the much larger range for the acoustic impedance feature.
  prefs: []
  type: TYPE_NORMAL
- en: clusters are orthogonal to acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice on a New Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ok, time to get to work. Let‚Äôs load up a dataset and perform cluster analysis
    with,
  prefs: []
  type: TYPE_NORMAL
- en: compact code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: basic visaulizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: save the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can select any of these datasets or modify the code and add your own to
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: Perform Cluster Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now calculate the silhouette plot to calculate a good number of categories and
    perform K-means clustering with that ‚ÄòK‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0a1ffbd9465edbd37a21e4a5c76af7b2e863639a218bb6dd8f81c3c7fc0824db.png](../Images/f985a9d2dd02fe7bafba136823cb96de.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs visualize the clusters with a matrix scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e994277777d8dfb7eb5faf49d97b1186211bfda69185ac313704f3d2f7d2b8ae.png](../Images/bebbfd16a3687097674b408fed86ec68.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we can choose to write out the DataFrame with the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Dataset 0, Unconventional Multivariate v4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, dataset [unconv_MV.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v4.csv).
    This dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: well average porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: log transform of permeability (to linearize the relationships with other variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: brittleness ratio (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total organic carbon (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vitrinite reflectance (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: initial production 90 day average (MCFPD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 1, Twelve, 12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 2D spatial dataset [12_sample_data.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/12_sample_data.csv).
    This dataset has variables from 480 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: X (m), Y (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: facies (0 - shale, 1 - sand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: acoustic impedance (kg/m^3 x m/s x 10^6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset 2, Reservoir 21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, 3D spatial dataset [res21_wells.csv](https://github.com/GeostatsGuy/GeoDataSets/blob/master/res21_wells.csv).
    This dataset has variables from 73 vertical wells over a 10,000m x 10,000m x 50
    m reservoir unit:'
  prefs: []
  type: TYPE_NORMAL
- en: well (ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X (m), Y (m), Depth (m) location coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porosity (%) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permeability (mD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acoustic Impedance (kg/m2s*10^6) after units conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facies (categorical) - ordinal with ordering from Shale, Sandy Shale, Shaley
    Sand, to Sandstone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density (g/cm^3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressible velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Youngs modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear velocity (m/s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shear modulus (GPa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the tabular data with the pandas ‚Äòread_csv‚Äô function into a DataFrame
    we called ‚Äòmy_data‚Äô and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: we also populate lists with data ranges and labels for ease of plotting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load and format the data,
  prefs: []
  type: TYPE_NORMAL
- en: drop the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reformate the features as needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, I like to store the metadata in lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | AI | Brittle | TOC | VR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.325294 | 0.204805 | 0.453731 | 0.960076 | 0.569620 | 0.711340 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.342941 | 0.274600 | 0.579104 | 0.480038 | 0.455696 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.439412 | 0.167048 | 0.814925 | 0.842894 | 0.455696 | 0.922680 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.654118 | 0.643021 | 0.402985 | 0.393378 | 0.535865 | 0.489691 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.645294 | 0.393593 | 0.567164 | 0.000000 | 0.717300 | 0.500000 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.469412 | 0.421053 | 0.420896 | 0.581278 | 0.476793 | 0.381443 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.408235 | 0.282609 | 0.492537 | 0.719035 | 0.417722 | 0.474227 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.295882 | 0.217391 | 0.588060 | 0.573103 | 0.371308 | 0.515464 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.351176 | 0.181922 | 0.343284 | 0.747105 | 0.481013 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.394118 | 0.321510 | 0.725373 | 0.752964 | 0.561181 | 0.886598 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.499412 | 0.372998 | 0.280597 | 0.683608 | 0.535865 | 0.432990 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.567059 | 0.591533 | 0.301493 | 0.519962 | 0.725738 | 0.479381 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.604118 | 0.490847 | 0.453731 | 0.759095 | 0.573840 | 0.541237 |'
  prefs: []
  type: TYPE_TB
- en: Perform Cluster Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now calculate the silhouette plot to calculate a good number of categories and
    perform K-means clustering with that ‚ÄòK‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0a1ffbd9465edbd37a21e4a5c76af7b2e863639a218bb6dd8f81c3c7fc0824db.png](../Images/f985a9d2dd02fe7bafba136823cb96de.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let‚Äôs visualize the clusters with a matrix scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e994277777d8dfb7eb5faf49d97b1186211bfda69185ac313704f3d2f7d2b8ae.png](../Images/bebbfd16a3687097674b408fed86ec68.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the Clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we can choose to write out the DataFrame with the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of cluster analysis. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
