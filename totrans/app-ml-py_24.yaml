- en: Polynomial Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_polynomial_regression.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_polynomial_regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **Polynomial Regression**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression](https://youtu.be/0fzbyhWiP84?si=uRdmHOTzdnUvDPA9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Polynomial Regression](https://youtu.be/z19Hs2HfO88?si=etUIb3LegiTigEio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Numerical Optimization](https://youtu.be/4nYz5j0sAQs?si=n_553YQdh5grTquV)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael‚Äôs Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By moving from linear regression to polynomial regression we,
  prefs: []
  type: TYPE_NORMAL
- en: add prediction flexibility by modeling non-linearity in our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build on the feature engineering concept of feature expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: while benefiting from the analytical solutions for training model parameters
    like linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We accomplish all of this with basis expansion,
  prefs: []
  type: TYPE_NORMAL
- en: we transform and expand the features \(\rightarrow\) introduce basis expansion!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can increase our predictive model complexity and flexibility \(\rightarrow\)
    nonlinear basis!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can improve the model robustness by removing multicollinearity \(\rightarrow\)
    orthogonal basis!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start with linear regression and then build to polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/806bf5f702f9bb5a63e30d6e1f7969d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada2fcc2740c48478e79404563c91061.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/835541b16e1038a4606f7d97b628c4f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictor Feature / Basis Expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can improve model flexibility and complexity by applying basis expansion
    with basis functions applied to our predictor features. The fundamental idea is
    to utilize a suite of basis functions, \(h_1, h_2, \ldots, h_k\), that provide
    new predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x_i) = (h_1(x_i),h_1(x_i),\ldots,h_k(x_i)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we from one feature \(X\) to an expanded basis of \(k\) features, \(X_1,
    X_2,\ldots, X_k\).
  prefs: []
  type: TYPE_NORMAL
- en: if we had \(m\) features in our data table, we now have \(k \times m\) features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3cf75cc4ca509f9dd86ecfb64061b7cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Basis expansion of predictor $m$ features with $k$ basis functions to $m \times
    k$ expanded features.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It can be shown that polynomial regression is just linear regression applied
    to a polynomial expansion of the predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{j} \rightarrow X_{j}, X_{j}^2, X_{j}^3, \ldots X_{j}^k \]
  prefs: []
  type: TYPE_NORMAL
- en: where we have \(j = 1, \ldots, m\) original features.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a expanded set of predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{j,k}(X_j) = X_j^k \]
  prefs: []
  type: TYPE_NORMAL
- en: were we have \(j = 1, \ldots, m\) original features and \(k = 1, \ldots, K\)
    polynomial orders.
  prefs: []
  type: TYPE_NORMAL
- en: We can now state our model as a linear regression of the transformed features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = f(x) = \sum_{j=1}^{m} \sum_{k = 1}^{K} \beta_{j,k} h_{j,m}(X_j) + \beta_0
    \]
  prefs: []
  type: TYPE_NORMAL
- en: after the \(h_l, l=1,\ldots,k\) transforms, over the \(j=1,\ldots,m\) predictor
    features we have the same linear equation and the ability to utilize the previously
    discussed analytical solution, see the chapter on linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We are assuming linearity after application of our basis transforms.
  prefs: []
  type: TYPE_NORMAL
- en: now the model coefficients, \(\beta_{l,i}\), relate to a transformed version
    of the initial predictor feature, \(h_l(X_i)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we lose the ability to interpret the coefficients, e.g., what is \(\phi^4\)
    where \(\phi\) is porosity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, with a single predictor feature, \(m = 1\), and up to a \(4^{th}\)
    order the model is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1}X_1 + \beta_{1,1}X_1^2 + \beta_{1,3}X_1^3 + \beta_{1,4}X_1^4
    + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where the model parameter notation is \(\beta_{m,k}\), were \(m\) is the feature
    and \(k\) is the order. To clarify here is the case for \(m = 2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1}X_1 + \beta_{1,2}X_1^2 + \beta_{1,3}X_1^3 + \beta_{1,4}X_1^4
    + \beta_{2,1}X_2 + \beta_{2,2}X_2^2 + \beta_{2,3}X_2^3 + \beta_{2,4}X_2^4 + \beta_0
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So our predictive modeling workflow is:'
  prefs: []
  type: TYPE_NORMAL
- en: apply polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform linear regression on the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage and Disadvantages of Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advantages of polynomial regression vs. linear regression, include,
  prefs: []
  type: TYPE_NORMAL
- en: improved flexibility to fit nonlinear phenomenon, with linear analysis and an
    analytical solution to train the model parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: Generally, significantly higher model variance! May have unstable interpolation
    and especially extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: sensitivity to outliers, especially with \(‚Ñé_ùëò \left(ùë•_{ùëñ,ùëó}\right)=ùë•_{ùëñ,ùëó}^ùëò\)
    where \(ùëò\) is large
  prefs: []
  type: TYPE_NORMAL
- en: we lose model parameter interpretability, \(ùõΩ_{ùëó,ùëò}\) is related to \(‚Ñé_ùëò \left(ùëã_j
    \right)\).
  prefs: []
  type: TYPE_NORMAL
- en: Adding Elementary Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative interpretation of polynomial regression is the construction of
    a regression model by adding elementary functions, i.e., the basis functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs work with a single predictor feature and \(K\) basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{l=1}^{k} \beta_{1,k} h_k (X_j) \]
  prefs: []
  type: TYPE_NORMAL
- en: For our simple, single predictor feature, \(X\), polynomial problem this is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,K} X^K + \beta_{1,K-1} X^{K-1} + \dots + \beta_{1,2} X^2 + \beta_{1,1}
    X + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs work with a 4th order polynomial expansion, \(K=4\), of standardized depth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea64332d4805861caa74b4d26e6bd3f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial basis for up to \(K=4\).
  prefs: []
  type: TYPE_NORMAL
- en: 'To build our function we are moving, scaling and adding these elementary functions.
    Let‚Äôs review how we perform moving and scaling of an elementary function with
    the example of the \(k=2\) basis function, i.e., a parabola, \(h_2: ùë¶=ùë•^2\). Consider
    the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: shifting on the X-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: shifting on the Y-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flipping on the X-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: changing the slope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each, I show a visualization of the change and then the impact on the polynomial
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting the function on the X-axis,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87df4ff1a6183394b90b31dfe989e9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Shifting $2^{nd}$ order elementary function on the X-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = (x - \Delta_x)^2 = x^2 - 2\Delta_x x + \Delta_x^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Shifting the function on the Y-axis,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87df4ff1a6183394b90b31dfe989e9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Shifting $2^{nd}$ order elementary function on the Y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = x^2 - \Delta_y \]
  prefs: []
  type: TYPE_NORMAL
- en: 'flipping the function on the X-axis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2e93ae27cb57ce4b016c4823c8e50642.png)'
  prefs: []
  type: TYPE_IMG
- en: Flipping the $2^{nd}$ order elmentary function on the X-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \pm \beta_2 x^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'changing the slope:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/63aa39b205aca7c3c08dd272484377e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the slope of the $2^{nd}$ order elmentary function.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \downarrow \beta_2 x^2, \text{wider / shallower} \]\[ y = \uparrow \beta_2
    x^2, \text{narrower / deeper} \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs make some observations from above,
  prefs: []
  type: TYPE_NORMAL
- en: shifting on the Y-axis only requires modification of the contant term of the
    model parameters in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: shifting on the X-axis requires modification of the lower order model parameters
    in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flipping on the X-axis requires change in sign of the current order model parameter
    in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increasing the slope requires increasing the current order model parameter in
    the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are important assumption with our polynomial regression model, extended
    from the assumptions of linear regression above,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor features basis expansions are error free, not random
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of basis features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial** - relationships between ùëã and Y is polynomial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Multicollinearity** - none of the basis feature expansions are linearly
    redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the polynomial basis expansion above, are the colinearities between
    our basis. To check, I calculated the correlation matrix for the basis expansion
    used in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08d2443894d5916687f1cf4785734bec.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation matrix from a polynomial basis expansion with $K=4$.
  prefs: []
  type: TYPE_NORMAL
- en: There is strong collinearity between the \(K=1\) and \(K=3\) bases and the \(k=2\)
    and \(k=4\) bases.
  prefs: []
  type: TYPE_NORMAL
- en: recall, collinearity and multicolinearity may increase model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To remove this collinearity we can apply Hermite polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hermite Polynomials**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is a family of orthogonal polynomials on the real number line.
  prefs: []
  type: TYPE_NORMAL
- en: '| Order | Hermite Polynomial \(H_e(x)\) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0th Order | \(H_{e_0}(x) = 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 1st Order | \(H_{e_1}(x) = x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 2nd Order | \(H_{e_2}(x) = x^2 - 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 3rd Order | \(H_{e_3}(x) = x^3 - 3x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 4th Order | \(H_{e_4}(x) = x^4 - 6x^2 + 3\) |'
  prefs: []
  type: TYPE_TB
- en: These polynomials are orthogonal with respect to a weighting function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùë§(ùë•)=ùëí^{‚àí\frac{ùë•^2}{2}} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the standard Gaussian probability density function without the scaler,
    \(\frac{1}{\sqrt{2\pi}}\). The definition of orthogonality is stated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The Hermite polynomials are orthogonal over the interval \([‚àí\infty,\infty]\)
    for the standard normal probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: By applying hermite polynomials instead of regular polynomials for polynomial
    basis expandion in polynomial regression were remove the multicolinearity between
    the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: recall, independence of the predictor features is an assumption of the linear
    system applied in polynomial regression with the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a convenience function to add gridlines to our plots and to plot
    correlation matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided bivariate, spatial dataset [Density_Por_data.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv)
    available in my GeoDataSet repo. It is a comma delimited file with:'
  prefs: []
  type: TYPE_NORMAL
- en: depth (m)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian transformed porosity (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the train and test DataFrame is useful check before we build our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Depth | Nporosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.25 | -1.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.50 | -2.08 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.75 | -1.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.00 | -1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.25 | -0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.50 | -0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.75 | 0.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2.00 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.25 | -0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 2.50 | -0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 2.75 | -1.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 3.00 | -1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 3.25 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The describe command provides count, mean, standard deviation, percentiles,
    minimum, maximum in a nice data table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I like to specify the percentiles, otherwise P25, P50 and P75 quartiles are
    the default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 10% | 50% | 90% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Depth | 40.0 | 5.12500 | 2.922613 | 0.25 | 1.225 | 5.125 | 9.025 | 10.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| Nporosity | 40.0 | 0.02225 | 0.992111 | -2.08 | -1.271 | 0.140 | 1.220 |
    2.35 |'
  prefs: []
  type: TYPE_TB
- en: Here we extract the Depth and Gaussian transformed porosity, Nporosity, from
    the DataFrame into separate 1D arrays called ‚Äòdepth‚Äô and ‚ÄòNPor‚Äô for readable code.
  prefs: []
  type: TYPE_NORMAL
- en: warning, this is a shallow copy, if we change these 1D arrays, the change will
    be reflected back in the original DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first calculate the linear regression model with the LinearRegression
    class from scikit-learn. The steps include,
  prefs: []
  type: TYPE_NORMAL
- en: '**instantiate** - the linear regression object, note there are no hyperparameters
    to specify.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**fit** - train the instantiated linear regression object with the training
    data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**predict** - with the trained linear regression object'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here‚Äôs the instantiation and fit steps for our linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: note, we add the reshape to our predictor feature because scikit-learn assumes
    more than one predictor feature and expects a 2D array. We reshape our 1D ndarray
    to a 2D array with only 1 column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we train the model we plot it with the data for visual model checking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/64b4519fff29b4b1c8eef0c0d94e3ceba809f3543abba1333ea33b4f4120ac4a.png](../Images/ba77774bef128a461422095cb22a2827.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison to a Nonparametric Predictive Machine Learning Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs run a couple nonparametric predictive machine learning models to contrast
    with the linear and polynomial parametric models. First we train a quick decision
    tree model and then a random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: we gain significant flexibility to fit any patterns from the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requires more inference as nonparametric is actually parameter rich!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details, see the chapter on decision trees and random forest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b5e3bfa9d8d83005e43dd6add7fb70f36813fa375d987065f62bbdf04957cddb.png](../Images/8e31233c62876ecb3c64296751df5ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and here is a random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d5ecd12edcb40da8fada767537df53155d9f68f2ab79546bb129a6d93f2cc28e.png](../Images/704a35303eabbbf03215f2c0a311653d.png)'
  prefs: []
  type: TYPE_IMG
- en: Note, no effort was made to tune the hyperparameters for these models. I just
    wanted to demonstrate the great flexibility of a nonparametric model to learn
    the shape of the system from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we return to our parametric polynomial model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs first transform our data to be standard normal, Gaussian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do this to improve the model fit (handle outliers) and to comply with theory
    for the Hermite polynomials that will be introduced shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian Anamorphosis \ Gaussian Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs transform the features to standard normal,
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mean of 0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: standard deviation of 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The porosity feature was ‚Äòtransformed‚Äô to Gaussian previously, but there is
    an opportunity to clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: compare the original and transformed below
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: note, I use my GeostatsPy Gaussian transform ported from the original GSLIB
    (Deutsch and Journel, 1997) because the scikit-learn Gaussian transform creates
    truncation spikes / outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs make some good cumulative distribution function plots to check the original
    and transformed variables.
  prefs: []
  type: TYPE_NORMAL
- en: the results look very good
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are doing this because we will need a Gaussian distribution for the predictor
    feature for orthogonality. More later!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/502106ad20a71cc9f6a412707dabe69539c7d2e42d6c72fa8b141c5695c13588.png](../Images/7b0e4b346e5f29d5e18e8d52b82145f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model with Standardized Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs repeat the linear regression model, now with the standardized features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/1966b7337c4a5b38596f989a8211aa0c1e8cfbab292369ed714bb5b7ebefb550.png](../Images/4c865fd8f2805646d61c8babce9fdbbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, not a good fit. Let‚Äôs use a more complex, flexible predictive machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will do polynomial regression by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: create the polynomial basis expansion of the original predictor feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform linear regression on the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial Basis Expansion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs start with calculating the polynomial basis expansion for the 1 predictor
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Values | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |'
  prefs: []
  type: TYPE_TB
- en: Now let‚Äôs check the correlation between the polynomial basis expansion of the
    original predictor features data.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a high degree of correlation between predictor features increases
    model variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2f3f18b5d2988d034a420125ea0efceca61840db5af413c92d054ca206d52af6.png](../Images/9b9eee94daf8d4510c17a21728efa520.png)'
  prefs: []
  type: TYPE_IMG
- en: We have high correlations between order 1 and 3 and order 2 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs check this with matrix scatter plot of the polynomial basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the Polynomial Expansion Features‚Äô Pairwise Relationship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9f46b45339b77aeca8a91c3df0f6006093ccd5e8e509c9feb5231794e4797834.png](../Images/3ea96d491efa1ce020f1f121c4e5fc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize the polynomial expansion over the Gaussian transformed depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/918e255b4a95601966627b5b6f5cd42f0edf824785db2cd3669e31d5f4519ed5.png](../Images/3f62f7e21b741e2cd86be2687d0f5fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also check the arithmetic average of each polynomial basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs fit the linear regression model to the polynomial basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: note the model is quite flexible to fit this complicated / nonlinear data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8e8a2889184278abe92b133ef03f98039f95b3558223864483cc2bfb0461e2d1.png](../Images/8544dd539b8a7b35cc6999a54d0fecb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression with Hermite Basis Expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use Hermite polynomials to reduce the correlation between the basis predictor
    features.
  prefs: []
  type: TYPE_NORMAL
- en: We transform the predictor feature, depth, to standard normal since the Hermite
    polynomial expansion approach independence over the range of negative infinity
    to positive infinity under the assumption of standard normal probability density
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|  | value | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -2.026808 | 3.107951 | -2.245605 | -4.772444 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -1.780464 | 2.170053 | -0.302774 | -5.971082 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -1.534121 | 1.353526 | 0.991769 | -5.582071 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -1.356312 | 0.839582 | 1.573889 | -4.653429 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -1.213340 | 0.472193 | 1.853749 | -3.665806 |'
  prefs: []
  type: TYPE_TB
- en: 'Note: I have omitted orders that had a higher degree of correlation for our
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs check the correlation between the Hermite predictor features. There is
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/767654dc38789e23879b6040b9c52283c5c21eb95f0671e3d0470ab4b3b8c71f.png](../Images/f1b9e3f1eac053a1460fbb16b9d3ab5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The pairwise linear correlation is quite low compared to the polynomial basis.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs visualize the bivariate relationships between our Hermite basis orders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/695a6474c0e6f88800c5be51a4e1437b374986904af57e9dfff07a1184f65977.png](../Images/9fd622d54d360b533c8ba76f2bd6a2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: We can check the arithmetic averages of all the Hermite basis expansions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs visualize Hermite polynomials over the range of the standardized depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f32be19da954ac4bb5bdc7d502a565b9c8e6c3c007728bd33276d37d1eaf6259.png](../Images/4014172673585ba0353f9f413f88bd94.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let‚Äôs fit our Hermite basis regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5b4926cdbd311bf1b5374a042995789082def9fd00376dc30620f948c00f669d.png](../Images/f42cc4ffefcd978188c7348b3d653b8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we have less correlation between the expanded basis features we can check
    out the model coefficients and interpret the unique importance of each order.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal Polynomials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs try the orthogonal polynomial basis expansion reimplemented in Python
    by Dave Moore from the poly() function in R.
  prefs: []
  type: TYPE_NORMAL
- en: the functions below for fit and predict are directly from Dave‚Äôs [blog](http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: note during the fit to the training data the norm2 and alpha model parameters
    are calcluated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these parameters must be passed to each subsequent predict to ensure the results
    are consistent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs give it a try and perform orthogonal polynomial expansion of our standard
    normal transformed depth
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | value | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -0.330385 | 0.440404 | -0.460160 | 0.420374 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -0.290335 | 0.313201 | -0.207862 | 0.021278 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -0.250285 | 0.202153 | -0.029761 | -0.172968 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -0.221377 | 0.132038 | 0.058235 | -0.220834 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -0.198133 | 0.081765 | 0.107183 | -0.219084 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs check the correlation between the orthogonal polynomial predictor features.
    I‚Äôm impressed! The between basis feature order correlations are all zero!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/51874452bf180e62d5f0824897a2b248a5e40bd802ff4ba4fefefda0c75d9c65.png](../Images/5c444109d15f6a2e24d2be84d8941629.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize the bivariate relationships between our orthogonal polynomial
    basis orders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ad56938294bca2fddf0cb90bd5afb4aa9c16bdec043fae497f76527072c2698b.png](../Images/5a2f9488237446e128cc99a668c78ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize orthogonal polynomial basis orders over the range of the standardized
    depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/fb2ab3f98848b762f738beb5133cf2b9963dbab254ba2882a5a5265e218ba8ce.png](../Images/74a68b06994a2941d62a1c4da558780c.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally let‚Äôs fit our orthogonal polynomial basis expansion regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b9adcfb2f480cc56becaad7a7b7d097564da38381dbf860303502572e5eab039.png](../Images/2a189618f9055efc6488a7eb46c5c41d.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial Regression in scikit-learn with Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The need to first perform basis expansion and then train the resulting (after
    basis transformations) linear model may seem a bit complicated.
  prefs: []
  type: TYPE_NORMAL
- en: one solution is to use the Pipeline object from scikit-learn. Here are some
    highlights on Pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning workflows can be complicated, with various steps:'
  prefs: []
  type: TYPE_NORMAL
- en: data preparation, feature engineering transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model parameter fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: modeling method selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: searching over a large combinatorial of hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training and testing model runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipelines are a scikit-learn class that allows for the encapsulation of a sequence
    of data preparation and modeling steps
  prefs: []
  type: TYPE_NORMAL
- en: then we can treat the pipeline as an object in our much condensed workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline class allows us to:'
  prefs: []
  type: TYPE_NORMAL
- en: improve code readability and to keep everything straight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoid common workflow problems like data leakage, testing data informing model
    parameter training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: abstract common machine learning modeling and focus on building the best model
    possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamental philosophy is to treat machine learning as a combinatorial search
    to find the best model (AutoML)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4c8e565b643fa879eb74f2d3c49419386b5073f8c2dce53cd9dd9142465f16ee.png](../Images/e0ba7ea47dd6042f5976cb230740cb93.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of polynomial regression. Much more could be done
    and discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By moving from linear regression to polynomial regression we,
  prefs: []
  type: TYPE_NORMAL
- en: add prediction flexibility by modeling non-linearity in our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build on the feature engineering concept of feature expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: while benefiting from the analytical solutions for training model parameters
    like linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We accomplish all of this with basis expansion,
  prefs: []
  type: TYPE_NORMAL
- en: we transform and expand the features \(\rightarrow\) introduce basis expansion!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can increase our predictive model complexity and flexibility \(\rightarrow\)
    nonlinear basis!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can improve the model robustness by removing multicollinearity \(\rightarrow\)
    orthogonal basis!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs start with linear regression and then build to polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/806bf5f702f9bb5a63e30d6e1f7969d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada2fcc2740c48478e79404563c91061.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/835541b16e1038a4606f7d97b628c4f9.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictor Feature / Basis Expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can improve model flexibility and complexity by applying basis expansion
    with basis functions applied to our predictor features. The fundamental idea is
    to utilize a suite of basis functions, \(h_1, h_2, \ldots, h_k\), that provide
    new predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x_i) = (h_1(x_i),h_1(x_i),\ldots,h_k(x_i)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we from one feature \(X\) to an expanded basis of \(k\) features, \(X_1,
    X_2,\ldots, X_k\).
  prefs: []
  type: TYPE_NORMAL
- en: if we had \(m\) features in our data table, we now have \(k \times m\) features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3cf75cc4ca509f9dd86ecfb64061b7cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Basis expansion of predictor $m$ features with $k$ basis functions to $m \times
    k$ expanded features.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It can be shown that polynomial regression is just linear regression applied
    to a polynomial expansion of the predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{j} \rightarrow X_{j}, X_{j}^2, X_{j}^3, \ldots X_{j}^k \]
  prefs: []
  type: TYPE_NORMAL
- en: where we have \(j = 1, \ldots, m\) original features.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a expanded set of predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{j,k}(X_j) = X_j^k \]
  prefs: []
  type: TYPE_NORMAL
- en: were we have \(j = 1, \ldots, m\) original features and \(k = 1, \ldots, K\)
    polynomial orders.
  prefs: []
  type: TYPE_NORMAL
- en: We can now state our model as a linear regression of the transformed features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = f(x) = \sum_{j=1}^{m} \sum_{k = 1}^{K} \beta_{j,k} h_{j,m}(X_j) + \beta_0
    \]
  prefs: []
  type: TYPE_NORMAL
- en: after the \(h_l, l=1,\ldots,k\) transforms, over the \(j=1,\ldots,m\) predictor
    features we have the same linear equation and the ability to utilize the previously
    discussed analytical solution, see the chapter on linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We are assuming linearity after application of our basis transforms.
  prefs: []
  type: TYPE_NORMAL
- en: now the model coefficients, \(\beta_{l,i}\), relate to a transformed version
    of the initial predictor feature, \(h_l(X_i)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we lose the ability to interpret the coefficients, e.g., what is \(\phi^4\)
    where \(\phi\) is porosity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, with a single predictor feature, \(m = 1\), and up to a \(4^{th}\)
    order the model is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1}X_1 + \beta_{1,1}X_1^2 + \beta_{1,3}X_1^3 + \beta_{1,4}X_1^4
    + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where the model parameter notation is \(\beta_{m,k}\), were \(m\) is the feature
    and \(k\) is the order. To clarify here is the case for \(m = 2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1}X_1 + \beta_{1,2}X_1^2 + \beta_{1,3}X_1^3 + \beta_{1,4}X_1^4
    + \beta_{2,1}X_2 + \beta_{2,2}X_2^2 + \beta_{2,3}X_2^3 + \beta_{2,4}X_2^4 + \beta_0
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So our predictive modeling workflow is:'
  prefs: []
  type: TYPE_NORMAL
- en: apply polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform linear regression on the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage and Disadvantages of Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advantages of polynomial regression vs. linear regression, include,
  prefs: []
  type: TYPE_NORMAL
- en: improved flexibility to fit nonlinear phenomenon, with linear analysis and an
    analytical solution to train the model parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: Generally, significantly higher model variance! May have unstable interpolation
    and especially extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: sensitivity to outliers, especially with \(‚Ñé_ùëò \left(ùë•_{ùëñ,ùëó}\right)=ùë•_{ùëñ,ùëó}^ùëò\)
    where \(ùëò\) is large
  prefs: []
  type: TYPE_NORMAL
- en: we lose model parameter interpretability, \(ùõΩ_{ùëó,ùëò}\) is related to \(‚Ñé_ùëò \left(ùëã_j
    \right)\).
  prefs: []
  type: TYPE_NORMAL
- en: Adding Elementary Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative interpretation of polynomial regression is the construction of
    a regression model by adding elementary functions, i.e., the basis functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs work with a single predictor feature and \(K\) basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{l=1}^{k} \beta_{1,k} h_k (X_j) \]
  prefs: []
  type: TYPE_NORMAL
- en: For our simple, single predictor feature, \(X\), polynomial problem this is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,K} X^K + \beta_{1,K-1} X^{K-1} + \dots + \beta_{1,2} X^2 + \beta_{1,1}
    X + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs work with a 4th order polynomial expansion, \(K=4\), of standardized depth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea64332d4805861caa74b4d26e6bd3f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial basis for up to \(K=4\).
  prefs: []
  type: TYPE_NORMAL
- en: 'To build our function we are moving, scaling and adding these elementary functions.
    Let‚Äôs review how we perform moving and scaling of an elementary function with
    the example of the \(k=2\) basis function, i.e., a parabola, \(h_2: ùë¶=ùë•^2\). Consider
    the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: shifting on the X-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: shifting on the Y-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flipping on the X-axis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: changing the slope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each, I show a visualization of the change and then the impact on the polynomial
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting the function on the X-axis,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87df4ff1a6183394b90b31dfe989e9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Shifting $2^{nd}$ order elementary function on the X-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = (x - \Delta_x)^2 = x^2 - 2\Delta_x x + \Delta_x^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Shifting the function on the Y-axis,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/87df4ff1a6183394b90b31dfe989e9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Shifting $2^{nd}$ order elementary function on the Y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = x^2 - \Delta_y \]
  prefs: []
  type: TYPE_NORMAL
- en: 'flipping the function on the X-axis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2e93ae27cb57ce4b016c4823c8e50642.png)'
  prefs: []
  type: TYPE_IMG
- en: Flipping the $2^{nd}$ order elmentary function on the X-axis.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \pm \beta_2 x^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'changing the slope:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/63aa39b205aca7c3c08dd272484377e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the slope of the $2^{nd}$ order elmentary function.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \downarrow \beta_2 x^2, \text{wider / shallower} \]\[ y = \uparrow \beta_2
    x^2, \text{narrower / deeper} \]
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs make some observations from above,
  prefs: []
  type: TYPE_NORMAL
- en: shifting on the Y-axis only requires modification of the contant term of the
    model parameters in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: shifting on the X-axis requires modification of the lower order model parameters
    in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flipping on the X-axis requires change in sign of the current order model parameter
    in the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increasing the slope requires increasing the current order model parameter in
    the polynomial equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are important assumption with our polynomial regression model, extended
    from the assumptions of linear regression above,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor features basis expansions are error free, not random
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of basis features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial** - relationships between ùëã and Y is polynomial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Multicollinearity** - none of the basis feature expansions are linearly
    redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the polynomial basis expansion above, are the colinearities between
    our basis. To check, I calculated the correlation matrix for the basis expansion
    used in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08d2443894d5916687f1cf4785734bec.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation matrix from a polynomial basis expansion with $K=4$.
  prefs: []
  type: TYPE_NORMAL
- en: There is strong collinearity between the \(K=1\) and \(K=3\) bases and the \(k=2\)
    and \(k=4\) bases.
  prefs: []
  type: TYPE_NORMAL
- en: recall, collinearity and multicolinearity may increase model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To remove this collinearity we can apply Hermite polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hermite Polynomials**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is a family of orthogonal polynomials on the real number line.
  prefs: []
  type: TYPE_NORMAL
- en: '| Order | Hermite Polynomial \(H_e(x)\) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0th Order | \(H_{e_0}(x) = 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 1st Order | \(H_{e_1}(x) = x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 2nd Order | \(H_{e_2}(x) = x^2 - 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 3rd Order | \(H_{e_3}(x) = x^3 - 3x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 4th Order | \(H_{e_4}(x) = x^4 - 6x^2 + 3\) |'
  prefs: []
  type: TYPE_TB
- en: These polynomials are orthogonal with respect to a weighting function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùë§(ùë•)=ùëí^{‚àí\frac{ùë•^2}{2}} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the standard Gaussian probability density function without the scaler,
    \(\frac{1}{\sqrt{2\pi}}\). The definition of orthogonality is stated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The Hermite polynomials are orthogonal over the interval \([‚àí\infty,\infty]\)
    for the standard normal probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: By applying hermite polynomials instead of regular polynomials for polynomial
    basis expandion in polynomial regression were remove the multicolinearity between
    the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: recall, independence of the predictor features is an assumption of the linear
    system applied in polynomial regression with the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a convenience function to add gridlines to our plots and to plot
    correlation matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided bivariate, spatial dataset [Density_Por_data.csv](https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv)
    available in my GeoDataSet repo. It is a comma delimited file with:'
  prefs: []
  type: TYPE_NORMAL
- en: depth (m)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian transformed porosity (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the train and test DataFrame is useful check before we build our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Depth | Nporosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.25 | -1.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.50 | -2.08 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.75 | -1.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.00 | -1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.25 | -0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1.50 | -0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1.75 | 0.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2.00 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2.25 | -0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 2.50 | -0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 2.75 | -1.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 3.00 | -1.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 3.25 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The describe command provides count, mean, standard deviation, percentiles,
    minimum, maximum in a nice data table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I like to specify the percentiles, otherwise P25, P50 and P75 quartiles are
    the default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 10% | 50% | 90% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Depth | 40.0 | 5.12500 | 2.922613 | 0.25 | 1.225 | 5.125 | 9.025 | 10.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| Nporosity | 40.0 | 0.02225 | 0.992111 | -2.08 | -1.271 | 0.140 | 1.220 |
    2.35 |'
  prefs: []
  type: TYPE_TB
- en: Here we extract the Depth and Gaussian transformed porosity, Nporosity, from
    the DataFrame into separate 1D arrays called ‚Äòdepth‚Äô and ‚ÄòNPor‚Äô for readable code.
  prefs: []
  type: TYPE_NORMAL
- en: warning, this is a shallow copy, if we change these 1D arrays, the change will
    be reflected back in the original DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first calculate the linear regression model with the LinearRegression
    class from scikit-learn. The steps include,
  prefs: []
  type: TYPE_NORMAL
- en: '**instantiate** - the linear regression object, note there are no hyperparameters
    to specify.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**fit** - train the instantiated linear regression object with the training
    data'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**predict** - with the trained linear regression object'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here‚Äôs the instantiation and fit steps for our linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: note, we add the reshape to our predictor feature because scikit-learn assumes
    more than one predictor feature and expects a 2D array. We reshape our 1D ndarray
    to a 2D array with only 1 column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After we train the model we plot it with the data for visual model checking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/64b4519fff29b4b1c8eef0c0d94e3ceba809f3543abba1333ea33b4f4120ac4a.png](../Images/ba77774bef128a461422095cb22a2827.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison to a Nonparametric Predictive Machine Learning Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs run a couple nonparametric predictive machine learning models to contrast
    with the linear and polynomial parametric models. First we train a quick decision
    tree model and then a random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: we gain significant flexibility to fit any patterns from the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requires more inference as nonparametric is actually parameter rich!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details, see the chapter on decision trees and random forest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b5e3bfa9d8d83005e43dd6add7fb70f36813fa375d987065f62bbdf04957cddb.png](../Images/8e31233c62876ecb3c64296751df5ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and here is a random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d5ecd12edcb40da8fada767537df53155d9f68f2ab79546bb129a6d93f2cc28e.png](../Images/704a35303eabbbf03215f2c0a311653d.png)'
  prefs: []
  type: TYPE_IMG
- en: Note, no effort was made to tune the hyperparameters for these models. I just
    wanted to demonstrate the great flexibility of a nonparametric model to learn
    the shape of the system from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we return to our parametric polynomial model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs first transform our data to be standard normal, Gaussian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do this to improve the model fit (handle outliers) and to comply with theory
    for the Hermite polynomials that will be introduced shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian Anamorphosis \ Gaussian Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs transform the features to standard normal,
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mean of 0.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: standard deviation of 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The porosity feature was ‚Äòtransformed‚Äô to Gaussian previously, but there is
    an opportunity to clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: compare the original and transformed below
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: note, I use my GeostatsPy Gaussian transform ported from the original GSLIB
    (Deutsch and Journel, 1997) because the scikit-learn Gaussian transform creates
    truncation spikes / outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs make some good cumulative distribution function plots to check the original
    and transformed variables.
  prefs: []
  type: TYPE_NORMAL
- en: the results look very good
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are doing this because we will need a Gaussian distribution for the predictor
    feature for orthogonality. More later!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/502106ad20a71cc9f6a412707dabe69539c7d2e42d6c72fa8b141c5695c13588.png](../Images/7b0e4b346e5f29d5e18e8d52b82145f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model with Standardized Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs repeat the linear regression model, now with the standardized features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/1966b7337c4a5b38596f989a8211aa0c1e8cfbab292369ed714bb5b7ebefb550.png](../Images/4c865fd8f2805646d61c8babce9fdbbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, not a good fit. Let‚Äôs use a more complex, flexible predictive machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will do polynomial regression by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: create the polynomial basis expansion of the original predictor feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform linear regression on the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial Basis Expansion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs start with calculating the polynomial basis expansion for the 1 predictor
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Values | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |'
  prefs: []
  type: TYPE_TB
- en: Now let‚Äôs check the correlation between the polynomial basis expansion of the
    original predictor features data.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a high degree of correlation between predictor features increases
    model variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2f3f18b5d2988d034a420125ea0efceca61840db5af413c92d054ca206d52af6.png](../Images/9b9eee94daf8d4510c17a21728efa520.png)'
  prefs: []
  type: TYPE_IMG
- en: We have high correlations between order 1 and 3 and order 2 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs check this with matrix scatter plot of the polynomial basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial Basis Expansion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs start with calculating the polynomial basis expansion for the 1 predictor
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Values | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -2.026808 | 4.107951 | -8.326029 | 16.875264 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -1.780464 | 3.170053 | -5.644167 | 10.049238 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -1.534121 | 2.353526 | -3.610592 | 5.539084 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -1.356312 | 1.839582 | -2.495046 | 3.384060 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -1.213340 | 1.472193 | -1.786270 | 2.167352 |'
  prefs: []
  type: TYPE_TB
- en: Now let‚Äôs check the correlation between the polynomial basis expansion of the
    original predictor features data.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a high degree of correlation between predictor features increases
    model variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2f3f18b5d2988d034a420125ea0efceca61840db5af413c92d054ca206d52af6.png](../Images/9b9eee94daf8d4510c17a21728efa520.png)'
  prefs: []
  type: TYPE_IMG
- en: We have high correlations between order 1 and 3 and order 2 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs check this with matrix scatter plot of the polynomial basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the Polynomial Expansion Features‚Äô Pairwise Relationship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9f46b45339b77aeca8a91c3df0f6006093ccd5e8e509c9feb5231794e4797834.png](../Images/3ea96d491efa1ce020f1f121c4e5fc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize the polynomial expansion over the Gaussian transformed depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/918e255b4a95601966627b5b6f5cd42f0edf824785db2cd3669e31d5f4519ed5.png](../Images/3f62f7e21b741e2cd86be2687d0f5fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also check the arithmetic average of each polynomial basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs fit the linear regression model to the polynomial basis expansion.
  prefs: []
  type: TYPE_NORMAL
- en: note the model is quite flexible to fit this complicated / nonlinear data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8e8a2889184278abe92b133ef03f98039f95b3558223864483cc2bfb0461e2d1.png](../Images/8544dd539b8a7b35cc6999a54d0fecb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression with Hermite Basis Expansion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use Hermite polynomials to reduce the correlation between the basis predictor
    features.
  prefs: []
  type: TYPE_NORMAL
- en: We transform the predictor feature, depth, to standard normal since the Hermite
    polynomial expansion approach independence over the range of negative infinity
    to positive infinity under the assumption of standard normal probability density
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '|  | value | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -2.026808 | 3.107951 | -2.245605 | -4.772444 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -1.780464 | 2.170053 | -0.302774 | -5.971082 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -1.534121 | 1.353526 | 0.991769 | -5.582071 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -1.356312 | 0.839582 | 1.573889 | -4.653429 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -1.213340 | 0.472193 | 1.853749 | -3.665806 |'
  prefs: []
  type: TYPE_TB
- en: 'Note: I have omitted orders that had a higher degree of correlation for our
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs check the correlation between the Hermite predictor features. There is
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/767654dc38789e23879b6040b9c52283c5c21eb95f0671e3d0470ab4b3b8c71f.png](../Images/f1b9e3f1eac053a1460fbb16b9d3ab5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The pairwise linear correlation is quite low compared to the polynomial basis.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs visualize the bivariate relationships between our Hermite basis orders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/695a6474c0e6f88800c5be51a4e1437b374986904af57e9dfff07a1184f65977.png](../Images/9fd622d54d360b533c8ba76f2bd6a2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: We can check the arithmetic averages of all the Hermite basis expansions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs visualize Hermite polynomials over the range of the standardized depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/f32be19da954ac4bb5bdc7d502a565b9c8e6c3c007728bd33276d37d1eaf6259.png](../Images/4014172673585ba0353f9f413f88bd94.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let‚Äôs fit our Hermite basis regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5b4926cdbd311bf1b5374a042995789082def9fd00376dc30620f948c00f669d.png](../Images/f42cc4ffefcd978188c7348b3d653b8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we have less correlation between the expanded basis features we can check
    out the model coefficients and interpret the unique importance of each order.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal Polynomials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs try the orthogonal polynomial basis expansion reimplemented in Python
    by Dave Moore from the poly() function in R.
  prefs: []
  type: TYPE_NORMAL
- en: the functions below for fit and predict are directly from Dave‚Äôs [blog](http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: note during the fit to the training data the norm2 and alpha model parameters
    are calcluated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these parameters must be passed to each subsequent predict to ensure the results
    are consistent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs give it a try and perform orthogonal polynomial expansion of our standard
    normal transformed depth
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '|  | value | 1st | 2nd | 3rd | 4th |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -2.026808 | -0.330385 | 0.440404 | -0.460160 | 0.420374 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | -1.780464 | -0.290335 | 0.313201 | -0.207862 | 0.021278 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | -1.534121 | -0.250285 | 0.202153 | -0.029761 | -0.172968 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -1.356312 | -0.221377 | 0.132038 | 0.058235 | -0.220834 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | -1.213340 | -0.198133 | 0.081765 | 0.107183 | -0.219084 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs check the correlation between the orthogonal polynomial predictor features.
    I‚Äôm impressed! The between basis feature order correlations are all zero!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/51874452bf180e62d5f0824897a2b248a5e40bd802ff4ba4fefefda0c75d9c65.png](../Images/5c444109d15f6a2e24d2be84d8941629.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize the bivariate relationships between our orthogonal polynomial
    basis orders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ad56938294bca2fddf0cb90bd5afb4aa9c16bdec043fae497f76527072c2698b.png](../Images/5a2f9488237446e128cc99a668c78ad7.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs visualize orthogonal polynomial basis orders over the range of the standardized
    depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/fb2ab3f98848b762f738beb5133cf2b9963dbab254ba2882a5a5265e218ba8ce.png](../Images/74a68b06994a2941d62a1c4da558780c.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally let‚Äôs fit our orthogonal polynomial basis expansion regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b9adcfb2f480cc56becaad7a7b7d097564da38381dbf860303502572e5eab039.png](../Images/2a189618f9055efc6488a7eb46c5c41d.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial Regression in scikit-learn with Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The need to first perform basis expansion and then train the resulting (after
    basis transformations) linear model may seem a bit complicated.
  prefs: []
  type: TYPE_NORMAL
- en: one solution is to use the Pipeline object from scikit-learn. Here are some
    highlights on Pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning workflows can be complicated, with various steps:'
  prefs: []
  type: TYPE_NORMAL
- en: data preparation, feature engineering transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model parameter fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: modeling method selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: searching over a large combinatorial of hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training and testing model runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipelines are a scikit-learn class that allows for the encapsulation of a sequence
    of data preparation and modeling steps
  prefs: []
  type: TYPE_NORMAL
- en: then we can treat the pipeline as an object in our much condensed workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pipeline class allows us to:'
  prefs: []
  type: TYPE_NORMAL
- en: improve code readability and to keep everything straight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoid common workflow problems like data leakage, testing data informing model
    parameter training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: abstract common machine learning modeling and focus on building the best model
    possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamental philosophy is to treat machine learning as a combinatorial search
    to find the best model (AutoML)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4c8e565b643fa879eb74f2d3c49419386b5073f8c2dce53cd9dd9142465f16ee.png](../Images/e0ba7ea47dd6042f5976cb230740cb93.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of polynomial regression. Much more could be done
    and discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
