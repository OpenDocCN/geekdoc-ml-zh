- en: AI Acceleration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 加速
- en: '*DALL·E 3 Prompt: Create an intricate and colorful representation of a System
    on Chip (SoC) design in a rectangular format. Showcase a variety of specialized
    machine learning accelerators and chiplets, all integrated into the processor.
    Provide a detailed view inside the chip, highlighting the rapid movement of electrons.
    Each accelerator and chiplet should be designed to interact with neural network
    neurons, layers, and activations, emphasizing their processing speed. Depict the
    neural networks as a network of interconnected nodes, with vibrant data streams
    flowing between the accelerator pieces, showcasing the enhanced computation speed.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：创建一个复杂且色彩丰富的矩形格式芯片级联（SoC）设计图。展示各种专用机器学习加速器和芯片片，所有这些都被集成到处理器中。提供芯片内部的详细视图，突出电子的快速运动。每个加速器和芯片片都应设计为与神经网络神经元、层和激活相互作用，强调其处理速度。将神经网络描绘为相互连接的节点网络，展示加速器部件之间充满活力的数据流，展示增强的计算速度。*'
- en: '![](../media/file179.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file179.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*What makes specialized hardware acceleration not just beneficial but essential
    for practical machine learning deployment, and why does this represent a fundamental
    shift in how we approach computational system design?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*是什么使得专用硬件加速不仅有益，而且对于实际机器学习部署来说是必不可少的，这为什么代表了我们在计算系统设计方法上的根本转变？*'
- en: Practical machine learning systems depend entirely on hardware acceleration.
    Without specialized processors, computational demands remain economically and
    physically infeasible. General-purpose CPUs achieve only 100 GFLOPS[1](#fn1) for
    neural network operations ([Sze et al. 2017a](ch058.xhtml#ref-sze2017efficient)),
    while modern training workloads require trillions of operations per second, creating
    a performance gap that traditional scaling cannot bridge. Hardware acceleration
    transforms computationally impossible tasks into practical deployments, enabling
    entirely new application categories. Engineers working with modern AI systems
    must understand acceleration principles to harness 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    that make real-time inference, large-scale training, and edge deployment economically
    viable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的机器学习系统完全依赖于硬件加速。没有专用处理器，计算需求在经济和物理上都是不可行的。通用 CPU 在神经网络操作中只能达到 100 GFLOPS[1](#fn1)
    ([Sze 等人 2017a](ch058.xhtml#ref-sze2017efficient))，而现代训练工作负载需要每秒数万亿次的操作，这造成了传统扩展无法弥合的性能差距。硬件加速将计算上不可能的任务转化为实际部署，使全新的应用类别成为可能。与现代
    AI 系统合作的工程师必须了解加速原理，以利用 100-1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    的性能提升，使实时推理、大规模训练和边缘部署在经济上可行。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Trace the evolution of hardware acceleration from floating-point coprocessors
    to modern AI accelerators and explain the architectural principles driving this
    progression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪硬件加速从浮点协处理器到现代 AI 加速器的演变过程，并解释推动这一进程的架构原则
- en: Classify AI compute primitives (vector operations, matrix multiplication, systolic
    arrays) and analyze their implementation in contemporary accelerators
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 AI 计算原语（向量操作、矩阵乘法、阵列波前传播）进行分类，并分析它们在当代加速器中的实现
- en: Evaluate memory hierarchy designs for AI accelerators and predict their impact
    on performance bottlenecks using bandwidth and energy consumption metrics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 AI 加速器的内存层次结构设计，并使用带宽和能耗指标预测其对性能瓶颈的影响
- en: Design mapping strategies for neural network layers onto specialized hardware
    architectures, considering dataflow patterns and resource utilization trade-offs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计将神经网络层映射到专用硬件架构的映射策略，考虑数据流模式和资源利用权衡
- en: Apply compiler optimization techniques (graph optimization, kernel fusion, memory
    planning) to transform high-level ML models into efficient hardware execution
    plans
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用编译器优化技术（图优化、内核融合、内存规划）将高级 ML 模型转换为高效的硬件执行计划
- en: Compare multi-chip scaling approaches (chiplets, multi-GPU, distributed systems)
    and assess their suitability for different AI workload characteristics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较多芯片扩展方法（芯片片、多 GPU、分布式系统）并评估它们对不同 AI 工作负载特性的适用性
- en: Critique common misconceptions about hardware acceleration and identify potential
    pitfalls in accelerator selection and deployment strategies
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批判关于硬件加速的常见误解，并识别加速器选择和部署策略中的潜在陷阱
- en: AI Hardware Acceleration Fundamentals
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 硬件加速基础
- en: Modern machine learning systems challenge the architectural assumptions underlying
    general-purpose processors. While software optimization techniques examined in
    the preceding chapter provide systematic approaches to algorithmic efficiency
    through precision reduction, structural pruning, and execution refinements, they
    operate within the constraints of existing computational substrates. Conventional
    CPUs achieve utilization rates of merely 5-10% when executing typical machine
    learning workloads ([Gholami et al. 2024](ch058.xhtml#ref-gholami2024ai)), due
    to architectural misalignments between sequential processing models and the highly
    parallel, data-intensive nature of neural network computations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习系统对通用处理器的底层架构假设提出了挑战。虽然前一章中探讨的软件优化技术通过精度降低、结构剪枝和执行优化等手段提供了系统化的算法效率方法，但它们在现有计算基础结构的约束下运行。传统的CPU在执行典型的机器学习工作负载时，利用率仅为5-10%
    ([Gholami等人2024](ch058.xhtml#ref-gholami2024ai))，这是由于顺序处理模型与神经网络计算高度并行、数据密集的本质之间存在架构不匹配。
- en: This performance gap has driven a shift toward domain-specific hardware acceleration
    within computer architecture. Hardware acceleration complements software optimization,
    addressing efficiency limitations through architectural redesign rather than algorithmic
    modification. The co-evolution of machine learning algorithms and specialized
    computing architectures has enabled the transition from computationally prohibitive
    research conducted on high-performance computing systems to ubiquitous deployment
    across diverse computing environments, from hyperscale data centers to resource-constrained
    edge devices.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种性能差距推动了计算机架构向特定领域硬件加速的转变。硬件加速补充了软件优化，通过架构重设计而不是算法修改来解决效率限制。机器学习算法和专用计算架构的协同进化，使得从高性能计算系统上进行的计算成本高昂的研究过渡到在多样化的计算环境中无处不在的部署成为可能，从超大规模数据中心到资源受限的边缘设备。
- en: Hardware acceleration for machine learning systems sits at the intersection
    of computer systems engineering, computer architecture, and applied machine learning.
    For practitioners developing production systems, architectural selection decisions
    regarding accelerator technologies encompassing graphics processing units, tensor
    processing units, and neuromorphic processors directly determine system-level
    performance characteristics, energy efficiency profiles, and implementation complexity.
    Deployed systems in domains such as natural language processing, computer vision,
    and autonomous systems demonstrate performance improvements spanning two to three
    orders of magnitude relative to general-purpose implementations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的硬件加速位于计算机系统工程、计算机架构和应用机器学习的交汇点。对于开发生产系统的从业者来说，关于包括图形处理单元、张量处理单元和神经形态处理器在内的加速器技术的架构选择决策，直接决定了系统级性能特征、能效配置文件和实现复杂性。在自然语言处理、计算机视觉和自主系统等领域的部署系统，与通用实现相比，性能提高了两个到三个数量级。
- en: This chapter examines hardware acceleration principles and methodologies for
    machine learning systems. The analysis begins with the historical evolution of
    domain-specific computing architectures, showing how design patterns from floating-point
    coprocessors to graphics processing units inform contemporary AI acceleration
    strategies. We then address the computational primitives that characterize machine
    learning workloads, including matrix multiplication, vector operations, and nonlinear
    activation functions, and analyze the architectural mechanisms through which specialized
    hardware optimizes these operations via innovations such as systolic array architectures
    and tensor processing cores.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了机器学习系统的硬件加速原理和方法。分析从特定领域计算架构的历史演变开始，展示了从浮点协处理器到图形处理单元的设计模式如何指导当代人工智能加速策略。然后我们讨论了表征机器学习工作负载的计算原语，包括矩阵乘法、向量操作和非线性激活函数，并分析了通过诸如脉动阵列架构和张量处理核心等创新来优化这些操作的专用硬件的架构机制。
- en: Memory hierarchy design plays a critical role in acceleration effectiveness,
    given that data movement energy costs typically exceed computational energy by
    more than two orders of magnitude. This analysis covers memory architecture design
    principles, from on-chip SRAM buffer optimization to high-bandwidth memory interfaces,
    and examines approaches to minimizing energy-intensive data movement patterns.
    We also address compiler optimization and runtime system support, which determine
    the extent to which theoretical hardware capabilities translate into measurable
    system performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据移动能耗通常超过计算能耗两个数量级的情况下，内存层次结构设计在加速有效性中起着至关重要的作用。本分析涵盖了内存架构设计原则，从片上SRAM缓冲区优化到高带宽内存接口，并探讨了最小化能耗密集型数据移动模式的方法。我们还讨论了编译器优化和运行时系统支持，这些因素决定了理论硬件能力转化为可测量的系统性能的程度。
- en: The chapter concludes with scaling methodologies for systems requiring computational
    capacity beyond single-chip implementations. Multi-chip architectures, ranging
    from chiplet-based integration to distributed warehouse-scale systems, introduce
    trade-offs between computational parallelism and inter-chip communication overhead.
    Through detailed analysis of contemporary systems including NVIDIA GPU architectures,
    Google Tensor Processing Units, and emerging neuromorphic computing platforms,
    we establish the theoretical foundations and practical considerations necessary
    for effective deployment of AI acceleration across diverse system contexts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以需要超出单芯片实现计算能力的系统扩展方法结束。从芯片级集成到分布式仓库规模系统，多芯片架构在计算并行性和芯片间通信开销之间引入了权衡。通过对包括NVIDIA
    GPU架构、谷歌Tensor Processing Units和新兴神经形态计算平台在内的当代系统的详细分析，我们确立了在多样化的系统环境中有效部署AI加速的理论基础和实践考虑。
- en: Evolution of Hardware Specialization
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件专用化的演变
- en: 'Computing architectures follow a recurring pattern: as computational workloads
    grow in complexity, general-purpose processors become increasingly inefficient,
    prompting the development of specialized hardware accelerators. The need for higher
    computational efficiency, reduced energy consumption, and optimized execution
    of domain-specific workloads drives this transition. Machine learning acceleration
    represents the latest stage in this ongoing evolution, following a trajectory
    observed in prior domains such as floating-point arithmetic, graphics processing,
    and digital signal processing.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算架构遵循一个反复出现的模式：随着计算工作负载的复杂性增加，通用处理器变得越来越低效，这促使专用硬件加速器的开发。对更高计算效率、降低能耗和优化特定领域工作负载执行的需求推动了这一转变。机器学习加速代表了这一持续演变的最新阶段，这一阶段遵循了先前领域（如浮点运算、图形处理和数字信号处理）观察到的轨迹。
- en: This evolutionary progression provides context for understanding how modern
    ML accelerators including GPUs with tensor cores (specialized units that accelerate
    matrix operations), Google’s TPUs[2](#fn2), and Apple’s Neural Engine emerged
    from established architectural principles. These technologies enable widely deployed
    applications such as real-time language translation, image recognition, and personalized
    recommendations. The architectural strategies enabling such capabilities derive
    from decades of hardware specialization research and development.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种进化过程为理解现代ML加速器（包括具有张量核心（加速矩阵运算的专用单元）的GPU、谷歌的TPU[2](#fn2)和苹果的Neural Engine）如何从既定的架构原则中产生提供了背景。这些技术使得广泛部署的应用成为可能，如实时语言翻译、图像识别和个性化推荐。使这些能力成为可能的架构策略源于数十年的硬件专用化研究和开发。
- en: Hardware specialization forms the foundation of this transition, enhancing performance
    and efficiency by optimizing frequently executed computational patterns through
    dedicated circuit implementations. While this approach yields significant gains,
    it introduces trade-offs in flexibility, silicon area utilization, and programming
    complexity. As computing demands continue to evolve, specialized accelerators
    must balance these factors to deliver sustained improvements in efficiency and
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件专用化构成了这一转变的基础，通过优化频繁执行的计算模式并通过专用电路实现来提高性能和效率。虽然这种方法带来了显著的收益，但它也在灵活性、硅面积利用率和编程复杂性方面引入了权衡。随着计算需求不断演变，专用加速器必须平衡这些因素，以实现效率和性能的持续改进。
- en: The evolution of hardware specialization provides perspective for understanding
    modern machine learning accelerators. Many principles that shaped the development
    of early floating-point and graphics accelerators now inform the design of AI-specific
    hardware. Examining these past trends offers a framework for analyzing contemporary
    approaches to AI acceleration and anticipating future developments in specialized
    computing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件专业化的发展为理解现代机器学习加速器提供了视角。塑造早期浮点图形加速器发展的许多原则现在正影响着AI专用硬件的设计。审视这些历史趋势为分析当代AI加速方法提供了一个框架，并预测专用计算的未来发展。
- en: Specialized Computing
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专用计算
- en: The transition toward specialized computing architectures stems from the limitations
    of general-purpose processors. Early computing systems relied on central processing
    units (CPUs) to execute all computational tasks sequentially, following a one-size-fits-all
    approach. As computing workloads diversified and grew in complexity, certain operations,
    especially floating-point arithmetic, emerged as performance bottlenecks that
    could not be efficiently handled by CPUs alone. These inefficiencies prompted
    the development of specialized hardware architectures designed to accelerate specific
    computational patterns ([Flynn 1966](ch058.xhtml#ref-flynn1966very)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 向专用计算架构的转变源于通用处理器的局限性。早期的计算系统依赖于中央处理单元(CPU)按顺序执行所有计算任务，采用一种一刀切的方法。随着计算工作负载的多样化和复杂性增长，某些操作，尤其是浮点运算，成为性能瓶颈，这些瓶颈无法仅通过CPU有效处理。这些低效率促使开发出专门设计的硬件架构，以加速特定的计算模式([Flynn
    1966](ch058.xhtml#ref-flynn1966very))。
- en: 'One of the earliest examples of hardware specialization was the Intel 8087
    mathematics coprocessor[3](#fn3), introduced in 1980\. This floating-point unit
    (FPU) was designed to offload arithmetic-intensive computations from the main
    CPU, dramatically improving performance for scientific and engineering applications.
    The 8087 demonstrated unprecedented efficiency, achieving performance gains of
    up to 100× for floating-point operations compared to software-based implementations
    on general-purpose processors ([Fisher 1981](ch058.xhtml#ref-fisher_8087_1981)).
    This milestone established a principle in computer architecture: carefully designed
    hardware specialization could provide order-of-magnitude improvements for well-defined,
    computationally intensive tasks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件专业化的早期例子之一是1980年推出的英特尔8087数学协处理器[3](#fn3)。这个浮点单元(FPU)被设计用来从主CPU卸载密集型算术计算，显著提高了科学和工程应用的性能。8087展示了前所未有的效率，与通用处理器上基于软件的实现相比，浮点操作的性能提升了高达100倍([Fisher
    1981](ch058.xhtml#ref-fisher_8087_1981))。这一里程碑在计算机架构中确立了一个原则：精心设计的硬件专业化可以为定义明确、计算密集型任务提供数量级的改进。
- en: 'The success of floating-point coprocessors[4](#fn4) led to their eventual integration
    into mainstream processors. The Intel 486DX, released in 1989, incorporated an
    on-chip floating-point unit, eliminating the requirement for an external coprocessor.
    This integration improved processing efficiency and established a recurring pattern
    in computer architecture: successful specialized functions become standard features
    in subsequent generations of general-purpose processors ([David A. Patterson and
    Hennessy 2021c](ch058.xhtml#ref-patterson2021computer)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点协处理器的成功[4](#fn4)最终导致了它们被集成到主流处理器中。1989年发布的英特尔486DX集成了片上浮点单元，消除了对外部协处理器的需求。这种集成提高了处理效率，并在计算机架构中确立了一个反复出现的模式：成功的专用功能成为后续一代通用处理器的标准特性([David
    A. Patterson and Hennessy 2021c](ch058.xhtml#ref-patterson2021computer))。
- en: 'Early floating-point acceleration established principles that continue to influence
    modern hardware specialization:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 早期浮点加速确立的原则继续影响着现代硬件专业化：
- en: Identification of computational bottlenecks through workload analysis
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过工作负载分析识别计算瓶颈
- en: Development of specialized circuits for frequent operations
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频繁操作专用电路的开发
- en: Creation of efficient hardware-software interfaces
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高效的软硬件接口的创建
- en: Progressive integration of proven specialized functions
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认的专用功能的渐进式集成
- en: This progression from domain-specific specialization to general-purpose integration
    has shaped modern computing architectures. As computational workloads expanded
    beyond arithmetic operations, these core principles were applied to new domains,
    such as graphics processing, digital signal processing, and ultimately, machine
    learning acceleration. Each domain introduced specialized architectures tailored
    to their unique computational requirements, establishing hardware specialization
    as an approach for advancing computing performance and efficiency in increasingly
    complex workloads.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从特定领域专业化到通用集成的这一进展塑造了现代计算架构。随着计算工作负载超越了算术运算，这些核心原则被应用于新的领域，如图形处理、数字信号处理，最终是机器学习加速。每个领域都引入了针对其独特计算需求的专用架构，确立了硬件专业化作为提高计算性能和效率在日益复杂的工作负载中的方法。
- en: The evolution of specialized computing hardware follows a consistent trajectory,
    wherein architectural innovations are introduced to address emerging computational
    bottlenecks and are subsequently incorporated into mainstream computing platforms.
    As illustrated in [Figure 11.1](ch017.xhtml#fig-timeline), each computing era
    produced accelerators that addressed the dominant workload characteristics of
    the period. These developments have advanced architectural efficiency and shaped
    the foundation upon which contemporary machine learning systems operate. The computational
    capabilities required for tasks such as real-time language translation, personalized
    recommendations, and on-device inference depend on foundational principles and
    architectural innovations established in earlier domains, including floating-point
    computation, graphics processing, and digital signal processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 专用计算硬件的演变遵循一致的轨迹，其中引入了架构创新来解决新兴的计算瓶颈，随后被纳入主流计算平台。如图11.1所示，每个计算时代都产生了针对该时期主导工作负载特性的加速器。这些发展提高了架构效率，并塑造了当代机器学习系统运行的基础。如实时语言翻译、个性化推荐和设备端推理等任务所需的计算能力，依赖于在早期领域（包括浮点计算、图形处理和数字信号处理）中建立的基石性原则和架构创新。
- en: '![](../media/file180.svg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file180.svg)'
- en: 'Figure 11.1: **Hardware Specialization Trajectory**: Computing architectures
    progressively incorporate specialized accelerators to address emerging performance
    bottlenecks and workload demands, mirroring a historical pattern from floating-point
    units to graphics processors and, ultimately, machine learning accelerators. This
    evolution reflects a strategy for improving computational efficiency by tailoring
    hardware to specific task characteristics and advancing increasingly complex applications.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：**硬件专业化轨迹**：计算架构逐步融入专用加速器，以应对新兴的性能瓶颈和工作负载需求，这与从浮点单元到图形处理器，最终到机器学习加速器的历史模式相呼应。这种演变反映了一种通过针对特定任务特性定制硬件并推进日益复杂的应用来提高计算效率的策略。
- en: Parallel Computing and Graphics Processing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行计算和图形处理
- en: The principles established through floating-point acceleration provided a blueprint
    for addressing emerging computational challenges. As computing applications diversified,
    new computational patterns emerged that exceeded the capabilities of general-purpose
    processors. This expansion of specialized computing manifested across multiple
    domains, each contributing unique insights to hardware acceleration strategies.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过浮点加速建立的原则为解决新兴的计算挑战提供了蓝图。随着计算应用的多样化，出现了新的计算模式，这些模式超出了通用处理器的功能。这种专用计算的发展体现在多个领域，每个领域都为硬件加速策略贡献了独特的见解。
- en: Graphics processing emerged as a primary driver of hardware specialization in
    the 1990s. Early graphics accelerators focused on specific operations like bitmap
    transfers and polygon filling. The introduction of programmable graphics pipelines
    with NVIDIA’s GeForce 256 in 1999 represented a significant advancement in specialized
    computing. Graphics Processing Units (GPUs) demonstrated how parallel processing
    architectures could efficiently handle data-parallel workloads, achieving 50-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> speedups in 3D rendering
    tasks like texture mapping and vertex transformation. By 2004, high-end GPUs could
    process over 100 million polygons per second ([Owens et al. 2008](ch058.xhtml#ref-owens2008gpu)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理在20世纪90年代成为硬件专业化的主要驱动因素。早期的图形加速器专注于特定的操作，如位图传输和多边形填充。1999年，随着NVIDIA的GeForce
    256引入可编程图形管道，代表了专用计算的重大进步。图形处理单元（GPU）展示了并行处理架构如何高效地处理数据并行工作负载，在3D渲染任务如纹理映射和顶点变换中实现了50-100倍的速度提升。到2004年，高端GPU每秒可以处理超过1亿个多边形([Owens
    et al. 2008](ch058.xhtml#ref-owens2008gpu))。
- en: Concurrently, Digital Signal Processing (DSP) processors established parallel
    data path architectures with specialized multiply-accumulate units and circular
    buffers optimized for filtering and transform operations. Texas Instruments’ TMS32010
    (1983) demonstrated how domain-specific instruction sets could dramatically improve
    performance for signal processing applications ([Lyons 2011](ch058.xhtml#ref-lyons2011understanding)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，数字信号处理（DSP）处理器建立了具有专用乘加单元和优化过滤和变换操作的循环缓冲区的并行数据路径架构。德州仪器的TMS32010（1983年）展示了特定领域的指令集如何显著提高信号处理应用的性能([Lyons
    2011](ch058.xhtml#ref-lyons2011understanding))。
- en: Network processing introduced additional patterns of specialization. Network
    processors developed unique architectures to handle packet processing at line
    rate, incorporating multiple processing cores, specialized packet manipulation
    units, and sophisticated memory management systems. Intel’s IXP2800 network processor
    demonstrated how multiple levels of hardware specialization could be combined
    to address complex processing requirements.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 网络处理引入了额外的专业化模式。网络处理器开发了独特的架构来以线路速率处理数据包，包括多个处理核心、专门的包操作单元和复杂的内存管理系统。英特尔IXP2800网络处理器展示了如何将多个级别的硬件专业化结合起来以满足复杂的处理需求。
- en: 'These diverse domains of specialization exhibit several common characteristics:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些多样化的专业化领域表现出几个共同的特征：
- en: Identification of domain-specific computational patterns
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别特定领域的计算模式
- en: Development of specialized processing elements and memory hierarchies
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发专门的处理器和内存层次结构
- en: Creation of domain-specific programming models
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建特定领域的编程模型
- en: Progressive evolution toward more flexible architectures
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向更灵活架构的渐进式进化
- en: This period of expanding specialization demonstrated that hardware acceleration
    strategies could address diverse computational requirements across multiple domains.
    The GPU’s success in parallelizing 3D graphics pipelines enabled its subsequent
    adoption for training deep neural networks, exemplified by AlexNet[5](#fn5) in
    2012, which executed on consumer-grade NVIDIA GPUs. DSP innovations in low-power
    signal processing facilitated real-time inference on edge devices, including voice
    assistants and wearables. These domains informed ML hardware designs and established
    that accelerators could be deployed across both cloud and embedded contexts, principles
    that continue to influence contemporary AI ecosystem development.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一扩展专业化的时期表明，硬件加速策略可以满足多个领域多样化的计算需求。GPU在并行化3D图形管道方面的成功使其随后被用于训练深度神经网络，例如2012年的AlexNet[5](#fn5)，它在消费级NVIDIA
    GPU上运行。低功耗信号处理中的DSP创新促进了边缘设备上的实时推理，包括语音助手和可穿戴设备。这些领域为ML硬件设计提供了信息，并确立了加速器可以部署在云和嵌入式环境中的原则，这些原则继续影响着当代AI生态系统的发展。
- en: Emergence of Domain-Specific Architectures
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特定领域架构的出现
- en: 'The emergence of domain-specific architectures (DSA)[6](#fn6) marks a shift
    in computer system design, driven by two factors: the breakdown of traditional
    scaling laws and the increasing computational demands of specialized workloads.
    The slowdown of Moore’s Law[7](#fn7), which previously ensured predictable enhancements
    in transistor density every 18 to 24 months, and the end of Dennard scaling[8](#fn8),
    which permitted frequency increases without corresponding power increases, created
    a performance and efficiency bottleneck in general-purpose computing. As John
    Hennessy and David Patterson noted in their 2017 Turing Lecture ([Hennessy and
    Patterson 2019](ch058.xhtml#ref-HennessyPatterson2017Turing)), these limitations
    signaled the onset of a new era in computer architecture, one centered on domain-specific
    solutions that optimize hardware for specialized workloads.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域架构（DSA）[6](#fn6)的出现标志着计算机系统设计的转变，由两个因素驱动：传统缩放定律的崩溃和专用工作负载计算需求的增加。摩尔定律[7](#fn7)的放缓，该定律以前确保了每18到24个月晶体管密度的可预测增强，以及Dennard缩放[8](#fn8)的结束，它允许频率增加而不伴随功率增加，在通用计算中创造了性能和效率瓶颈。正如约翰·亨尼斯和戴夫·帕特森在2017年的图灵讲座([亨尼斯和帕特森
    2019](ch058.xhtml#ref-HennessyPatterson2017Turing))中指出的，这些限制预示着计算机架构新时代的到来，这一时代以特定领域解决方案为中心，这些解决方案优化硬件以适应专用工作负载。
- en: Historically, improvements in processor performance depended on semiconductor
    process scaling and increasing clock speeds. However, as power density limitations
    restricted further frequency scaling, and as transistor miniaturization encountered
    increasing physical and economic constraints, architects explored alternative
    approaches to sustain computational growth. This resulted in a shift toward domain-specific
    architectures, which dedicate silicon resources to optimize computation for specific
    application domains, trading flexibility for efficiency.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，处理器性能的提升依赖于半导体工艺的缩小和时钟速度的提高。然而，随着功率密度限制进一步限制了频率提升，并且随着晶体管小型化遇到越来越多的物理和经济约束，架构师探索了替代方法以维持计算增长。这导致了向特定领域架构的转变，这些架构将硅资源专门用于优化特定应用领域的计算，以效率换取灵活性。
- en: 'Domain-specific architectures achieve superior performance and energy efficiency
    through several key principles:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域架构通过以下几个关键原则实现卓越的性能和能效：
- en: '**Customized data paths**: Design processing paths specifically optimized for
    target application patterns, enabling direct hardware execution of common operations.
    For example, matrix multiplication units in AI accelerators implement systolic
    arrays—grid-like networks of processing elements that rhythmically compute and
    pass data through neighboring units—tailored for neural network computations.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定制数据路径**：设计专门针对目标应用模式优化的处理路径，使常见操作能够直接在硬件上执行。例如，人工智能加速器中的矩阵乘法单元实现了脉动阵列——由处理单元组成的网格状网络，这些单元以节奏计算并通过相邻单元传递数据，专门用于神经网络计算。'
- en: '**Specialized memory hierarchies**: Optimize memory systems around domain-specific
    access patterns and data reuse characteristics. This includes custom cache configurations,
    prefetching logic, and memory controllers tuned for expected workloads.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**专用内存层次结构**：针对特定领域的访问模式和数据重用特性优化内存系统。这包括定制的缓存配置、预取逻辑和针对预期工作负载优化的内存控制器。'
- en: '**Reduced instruction overhead**: Implement domain-specific instruction sets
    that minimize decode and dispatch complexity by encoding common operation sequences
    into single instructions. This improves both performance and energy efficiency.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**减少指令开销**：实现特定领域的指令集，通过将常见操作序列编码为单个指令来最小化解码和调度复杂性。这提高了性能和能效。'
- en: '**Direct hardware implementation**: Create dedicated circuit blocks that natively
    execute frequently used operations without software intervention. This eliminates
    instruction processing overhead and maximizes throughput.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**直接硬件实现**：创建专用电路块，这些电路块能够原生执行常用操作，无需软件干预。这消除了指令处理开销，并最大化了吞吐量。'
- en: These principles achieve compelling demonstration in modern smartphones. Modern
    smartphones can decode 4K video at 60 frames per second while consuming only a
    few watts of power, despite video processing requiring billions of operations
    per second. This efficiency is achieved through dedicated hardware video codecs
    that implement industry standards such as H.264/AVC (introduced in 2003) and H.265/HEVC
    (finalized in 2013) ([Sullivan et al. 2012](ch058.xhtml#ref-sullivan2012overview)).
    These specialized circuits provide 100–1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    improvements in both performance and power efficiency compared to software-based
    decoding on general-purpose processors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则在现代智能手机中得到了令人信服的展示。现代智能手机可以在仅消耗几瓦功率的情况下，以每秒60帧的速度解码4K视频，尽管视频处理每秒需要数十亿次操作。这种效率是通过实现行业标准的专用硬件视频编解码器实现的，如H.264/AVC（2003年推出）和H.265/HEVC（2013年完成）([Sullivan
    et al. 2012](ch058.xhtml#ref-sullivan2012overview))。这些专用电路与基于通用处理器的软件解码相比，在性能和功耗效率方面提供了100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的改进。
- en: The trend toward specialization continues to accelerate, with new architectures
    emerging for an expanding range of domains. Genomics processing benefits from
    custom accelerators that optimize sequence alignment and variant calling, reducing
    the time required for DNA analysis ([Shang, Wang, and Liu 2018](ch058.xhtml#ref-Shang2018GenomicsAccel)).
    Similarly, blockchain computation has produced application-specific integrated
    circuits (ASICs)[9](#fn9) optimized for cryptographic hashing, substantially increasing
    the efficiency of mining operations ([Bedford Taylor 2017](ch058.xhtml#ref-Taylor2017ASICMining)).
    These examples demonstrate that domain-specific architecture represents a fundamental
    transformation in computing systems, offering tailored solutions that address
    the growing complexity and diversity of modern computational workloads.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 专业化趋势持续加速，新的架构不断涌现，以适应不断扩大的领域范围。基因组处理得益于定制加速器，这些加速器优化了序列对齐和变异调用，从而减少了DNA分析所需的时间（[Shang,
    Wang, and Liu 2018](ch058.xhtml#ref-Shang2018GenomicsAccel)）。同样，区块链计算产生了针对加密散列优化的专用集成电路（ASICs）[9](#fn9)，显著提高了挖矿操作的效率（[Bedford
    Taylor 2017](ch058.xhtml#ref-Taylor2017ASICMining)）。这些例子表明，特定领域的架构代表了计算系统中的根本性变革，提供了针对现代计算工作负载日益复杂性和多样性的定制解决方案。
- en: Machine Learning Hardware Specialization
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习硬件专用化
- en: 'Machine learning constitutes a computational domain with unique characteristics
    that have driven the development of specialized hardware architectures. Unlike
    traditional computing workloads that exhibit irregular memory access patterns
    and diverse instruction streams, neural networks are characterized by predictable
    patterns: dense matrix multiplications, regular data flow, and tolerance for reduced
    precision. These characteristics enable specialized hardware optimizations that
    would be ineffective for general-purpose computing but provide substantial speedups
    for ML workloads.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习构成了一个具有独特特征的计算领域，这些特征推动了专用硬件架构的发展。与表现出不规则内存访问模式和多样化指令流的传统计算工作负载不同，神经网络的特点是具有可预测的模式：密集的矩阵乘法、规律的数据流以及对降低精度的容忍。这些特性使得专用硬件优化成为可能，这对于通用计算是无效的，但为机器学习工作负载提供了显著的加速。
- en: '***Machine Learning Accelerators*** are specialized computing hardware optimized
    for the *computational patterns* of neural networks, achieving superior *performance
    per watt* through *parallel processing*, *specialized memory hierarchies*, and
    *reduced-precision arithmetic*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习加速器**是针对神经网络**计算模式**优化的专用计算硬件，通过**并行处理**、**专用内存层次结构**和**降低精度算术**实现了卓越的**性能每瓦**。'
- en: 'Machine learning computational requirements reveal limitations in traditional
    processors. CPUs achieve only 5-10% utilization on neural network workloads, delivering
    approximately 100 GFLOPS[10](#fn10) while consuming hundreds of watts. This inefficiency
    results from architectural mismatches: CPUs optimize for single-thread performance
    and irregular memory access, while neural networks require massive parallelism
    and predictable data streams. The memory bandwidth[11](#fn11) constraint becomes
    particularly severe: a single neural network layer may require accessing gigabytes
    of parameters, overwhelming CPU cache hierarchies[12](#fn12) designed for kilobyte-scale
    working sets.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的计算需求揭示了传统处理器的局限性。CPU在神经网络工作负载上仅实现5-10%的利用率，提供大约100 GFLOPS[10](#fn10)，同时消耗数百瓦的功率。这种低效源于架构不匹配：CPU优化单线程性能和不规则内存访问，而神经网络需要大规模并行和可预测的数据流。内存带宽[11](#fn11)限制变得尤为严重：单个神经网络层可能需要访问数GB的参数，这超出了为千字节规模的工作集设计的CPU缓存层次[12](#fn12)。
- en: The energy economics of data movement influence accelerator design. Accessing
    data from DRAM requires approximately 640 picojoules while performing a multiply-accumulate
    operation consumes only 3.7 pJ, approximately a 173× penalty (specific values
    vary by technology node and design) that establishes minimizing data movement
    as the primary optimization target. This disparity explains the progression from
    repurposed graphics processors to purpose-built neural network accelerators. GPUs
    achieve 15,000+ GFLOPS through massive parallelism but encounter efficiency challenges
    from their graphics heritage. TPUs and other custom accelerators achieve utilization
    above 85% by implementing systolic arrays and other architectures that maximize
    data reuse while minimizing movement.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据移动的能量经济学影响了加速器的设计。从DRAM访问数据需要大约640皮焦耳，而执行乘累加操作仅消耗3.7皮焦耳，大约是173倍的惩罚（具体数值因技术节点和设计而异），这确立了最小化数据移动作为主要优化目标。这种差异解释了从改用的图形处理器到专门设计的神经网络加速器的演变。GPU通过大规模并行处理实现15,000+
    GFLOPS，但遭遇了来自其图形传统的效率挑战。TPU和其他定制加速器通过实现收缩阵列和其他最大化数据重用同时最小化移动的架构，实现了超过85%的利用率。
- en: 'Training and inference present distinct computational profiles that influence
    accelerator design. Training requires high-precision arithmetic (FP32 or FP16)
    for gradient computation and weight updates, bidirectional data flow for backpropagation[13](#fn13),
    and large memory capacity for storing activations. Inference can exploit reduced
    precision (INT8 or INT4), requires only forward computation, and prioritizes latency
    over throughput[14](#fn14). These differences drive specialized architectures:
    training accelerators maximize FLOPS and memory bandwidth, while inference accelerators
    optimize for energy efficiency and deterministic latency.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和推理呈现不同的计算特征，这影响了加速器的设计。训练需要高精度算术（FP32或FP16）进行梯度计算和权重更新，双向数据流进行反向传播[13](#fn13)，以及大内存容量来存储激活。推理可以利用降低的精度（INT8或INT4），只需要正向计算，并优先考虑延迟[14](#fn14)而不是吞吐量。这些差异推动了专用架构：训练加速器最大化FLOPS和内存带宽，而推理加速器优化能效和确定性的延迟。
- en: Deployment context shapes architectural choices. Datacenter accelerators accept
    700-watt power budgets to maximize throughput for training massive models. Edge
    devices must deliver real-time inference within milliwatt constraints, driving
    architectures that eliminate every unnecessary data movement. Mobile processors
    balance performance with battery life, while automotive systems prioritize deterministic
    response times for safety-critical applications. This diversity has produced a
    rich ecosystem of specialized accelerators, each optimized for specific deployment
    scenarios and computational requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 部署环境塑造了架构选择。数据中心加速器接受700瓦的功率预算，以最大化训练大型模型的吞吐量。边缘设备必须在毫瓦级约束内提供实时推理，这推动了消除所有不必要的数据移动的架构。移动处理器在性能和电池寿命之间取得平衡，而汽车系统优先考虑安全关键应用的确定性响应时间。这种多样性产生了一个丰富的专用加速器生态系统，每个都针对特定的部署场景和计算需求进行了优化。
- en: In data centers, training accelerators such as NVIDIA H100 and Google TPUv4
    reduce model development from weeks to days through massive parallelism and high-bandwidth
    memory systems. These systems prioritize raw computational throughput, accepting
    700-watt power consumption to achieve petaflop-scale performance. The economics
    support this trade-off—reducing training time from months to days can reduce millions
    in operational costs and accelerate time-to-market for AI applications.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中心，如NVIDIA H100和Google TPUv4这样的训练加速器通过大规模并行和高带宽内存系统将模型开发时间从数周缩短至数天。这些系统优先考虑原始的计算吞吐量，接受700瓦的功耗以实现每秒万亿次性能。这种经济性支持这种权衡——将训练时间从数月缩短至数天可以减少数百万的运营成本并加快AI应用的上市时间。
- en: At the opposite extreme, edge deployment requires different optimization strategies.
    Processing-in-memory architectures eliminate data movement by integrating compute
    directly with memory. Dynamic voltage scaling reduces power by 50-90% during low-intensity
    operations. Neuromorphic designs process only changing inputs, achieving 1000×
    power reduction for temporal workloads. These techniques enable sophisticated
    AI models to operate continuously on battery power, supporting applications from
    smartphone photography to autonomous sensors that function for years without external
    power.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在相反的极端情况下，边缘部署需要不同的优化策略。内存中处理架构通过直接将计算与内存集成来消除数据移动。动态电压调整在低强度操作期间可将功耗降低50-90%。神经形态设计仅处理变化的输入，对于时序工作负载实现1000倍的功耗降低。这些技术使得复杂的AI模型能够在电池供电的情况下持续运行，支持从智能手机摄影到无需外部电源即可运行数年的自主传感器等应用。
- en: The success of application-specific accelerators demonstrates that no single
    architecture can efficiently address all ML workloads. The 156 billion edge devices
    projected by 2030 will require architectures optimized for energy efficiency and
    real-time guarantees, while cloud-scale training will continue advancing the boundaries
    of computational throughput. This diversity drives continued innovation in specialized
    architectures, each optimized for its specific deployment context and computational
    requirements.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 专用加速器的成功表明，没有单一的架构能够高效地解决所有ML工作负载。预计到2030年将有1560亿个边缘设备，这将需要针对能源效率和实时保证进行优化的架构，而云规模训练将继续推进计算吞吐量的边界。这种多样性推动了专用架构的持续创新，每个架构都针对其特定的部署环境和计算需求进行了优化。
- en: 'The evolution of specialized hardware architectures illustrates a principle
    in computing systems: as computational patterns emerge and mature, hardware specialization
    follows to achieve optimal performance and energy efficiency. This progression
    appears clearly in machine learning acceleration, where domain-specific architectures
    have evolved to meet the increasing computational demands of machine learning
    models. Unlike general-purpose processors, which prioritize flexibility, specialized
    accelerators optimize execution for well-defined workloads, balancing performance,
    energy efficiency, and integration with software frameworks.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 专用硬件架构的演变说明了计算机系统中的一个原则：随着计算模式的出现和成熟，硬件专业化随之而来，以实现最佳性能和能源效率。这种进步在机器学习加速中表现得尤为明显，特定领域的架构已经发展起来，以满足机器学习模型日益增长的计算需求。与优先考虑灵活性的通用处理器不同，专用加速器针对明确的工作负载进行优化，平衡性能、能源效率和与软件框架的集成。
- en: '[Table 11.1](ch017.xhtml#tbl-hw-evolution) summarizes key milestones in the
    evolution of hardware specialization, showing how each era produced architectures
    tailored to the prevailing computational demands. While these accelerators initially
    emerged to optimize domain-specific workloads, including floating-point operations,
    graphics rendering, and media processing, they also introduced architectural strategies
    that persist in contemporary systems. The specialization principles outlined in
    earlier generations now underpin the design of modern AI accelerators. Understanding
    this historical trajectory provides context for analyzing how hardware specialization
    continues to enable scalable, efficient execution of machine learning workloads
    across diverse deployment environments.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.1](ch017.xhtml#tbl-hw-evolution)总结了硬件专业化演变的关键里程碑，展示了每个时代如何产生针对当时计算需求的定制架构。虽然这些加速器最初出现是为了优化特定领域的负载，包括浮点运算、图形渲染和媒体处理，但它们也引入了在当代系统中持续存在的架构策略。早期代际中概述的专业化原则现在支撑着现代AI加速器的设计。理解这一历史轨迹为分析硬件专业化如何继续在多样化的部署环境中实现可扩展、高效的机器学习工作负载提供了背景。'
- en: 'Table 11.1: **Hardware Specialization Trends**: Successive computing eras progressively
    integrate specialized hardware to accelerate prevalent workloads, moving from
    general-purpose CPUs to domain-specific architectures and ultimately to customizable
    AI accelerators. This evolution reflects a fundamental principle: tailoring hardware
    to computational patterns improves performance and energy efficiency, driving
    innovation in machine learning systems.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1：**硬件专业化趋势**：连续的计算时代逐渐整合专用硬件以加速流行的负载，从通用CPU发展到特定领域的架构，最终到可定制的AI加速器。这一演变反映了一个基本原理：根据计算模式定制硬件可以提高性能和能源效率，推动机器学习系统中的创新。
- en: '| **Era** | **Computational Pattern** | **Architecture Examples** | **Characteristics**
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **时代** | **计算模式** | **架构示例** | **特点** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **1980s** | Floating-Point & Signal Processing | FPU, DSP |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **1980年代** | 浮点运算与信号处理 | FPU, DSP |'
- en: Single-purpose engines
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单用途引擎
- en: Focused instruction sets
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 焦点指令集
- en: Coprocessor interfaces
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协处理器接口
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **1990s** | 3D Graphics & Multimedia | GPU, SIMD Units |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **1990年代** | 3D图形与多媒体 | GPU, SIMD单元 |'
- en: Many identical compute units
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多相同的计算单元
- en: Regular data patterns
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规数据模式
- en: Wide memory interfaces
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽内存接口
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **2000s** | Real-time Media Coding | Media Codecs, Network Processors |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **2000年代** | 实时媒体编码 | 媒体编解码器、网络处理器 |'
- en: Fixed-function pipelines
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定功能流水线
- en: High throughput processing
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高吞吐量处理
- en: Power-performance optimization
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功耗性能优化
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **2010s** | Deep Learning Tensor Operations | TPU, GPU Tensor Cores |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **2010年代** | 深度学习张量运算 | TPU、GPU张量核心 |'
- en: Matrix multiplication units
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法单元
- en: Massive parallelism
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巨大并行性
- en: Memory bandwidth optimization
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存带宽优化
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **2020s** | Application-Specific Acceleration | ML Engines, Smart NICs, Domain
    Accelerators |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **2020年代** | 应用特定加速 | 机器学习引擎、智能网络接口卡、领域加速器 |'
- en: Workload-specific datapaths
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对特定工作负载的数据路径
- en: Customized memory hierarchies
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定制内存层次结构
- en: Application-optimized designs
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用优化设计
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'This historical progression reveals a recurring pattern: each wave of hardware
    specialization responded to a computational bottleneck, whether graphics rendering,
    media encoding, or neural network inference. What distinguishes the 2020s is not
    just specialization, but its pervasiveness: AI accelerators now underpin everything
    from product recommendations on YouTube to object detection in autonomous vehicles.
    Unlike earlier accelerators, today’s AI hardware must integrate tightly with dynamic
    software frameworks and scale across cloud-to-edge deployments. The table illustrates
    not just the past but also the trajectory toward increasingly tailored, high-impact
    computing platforms.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这一历史进步揭示了一个反复出现的模式：每一波硬件专业化都响应了计算瓶颈，无论是图形渲染、媒体编码还是神经网络推理。2020年代的特点不仅在于专业化，还在于其普遍性：AI加速器现在支撑着从YouTube上的产品推荐到自动驾驶汽车中的目标检测等一切事物。与早期的加速器不同，今天的AI硬件必须紧密集成动态软件框架，并扩展到云到边缘的部署。表格不仅展示了过去，还展示了向越来越定制、高影响力的计算平台发展的轨迹。
- en: For AI acceleration, this transition has introduced challenges that extend well
    beyond hardware design. Machine learning accelerators must integrate seamlessly
    into ML workflows by aligning with optimizations at multiple levels of the computing
    stack. They must operate effectively with widely adopted frameworks such as TensorFlow,
    PyTorch, and JAX, ensuring that deployment is smooth and consistent across varied
    hardware platforms. Compiler and runtime support become necessary; advanced optimization
    techniques, such as graph-level transformations, kernel fusion, and memory scheduling,
    are critical for using the full potential of these specialized accelerators.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工智能加速来说，这一转变带来了挑战，这些挑战远远超出了硬件设计的范畴。机器学习加速器必须通过在计算堆栈的多个级别上进行优化来无缝集成到机器学习工作流程中。它们必须与广泛采用的框架如TensorFlow、PyTorch和JAX有效协同工作，确保在各种硬件平台上部署的平滑和一致性。编译器和运行时支持变得必要；高级优化技术，如图级别转换、内核融合和内存调度，对于充分利用这些专用加速器的全部潜力至关重要。
- en: Scalability drives additional complexity as AI accelerators deploy across diverse
    environments from high-throughput data centers to resource-constrained edge and
    mobile devices, requiring tailored performance tuning and energy efficiency strategies.
    Integration into heterogeneous computing[15](#fn15) environments demands interoperability
    that enables specialized units to coordinate effectively with conventional CPUs
    and GPUs in distributed systems.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性推动了额外的复杂性，因为人工智能加速器部署在从高吞吐量数据中心到资源受限的边缘和移动设备等多样化的环境中，需要定制化的性能调整和能源效率策略。集成到异构计算环境中[15](#fn15)需要互操作性，这使专用单元能够与分布式系统中的传统CPU和GPU有效协调工作。
- en: AI accelerators represent a system-level transformation that requires tight
    hardware-software coupling. This transformation manifests in three specific computational
    patterns, compute primitives, that drive accelerator design decisions. Understanding
    these primitives determines the architectural features that enable 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    through coordinated hardware specialization and software optimization strategies
    examined in subsequent sections.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速器代表了一种系统级的转变，它需要紧密的软硬件耦合。这种转变体现在三个具体的计算模式上，即计算原语，这些模式驱动了加速器的设计决策。理解这些原语决定了能够通过协调的硬件专业化和软件优化策略实现100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>性能提升的体系结构特征，这些策略将在后续章节中探讨。
- en: 'The evolution from floating-point coprocessors to AI accelerators reveals a
    consistent pattern: computational bottlenecks drive specialized hardware development.
    Where the Intel 8087 addressed floating-point operations that consumed 80% of
    scientific computing time, modern AI workloads present an even more extreme case.
    Matrix multiplications and convolutions constitute over 95% of neural network
    computation. This concentration of computational demand creates unprecedented
    opportunities for specialization, explaining why AI accelerators achieve 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    over general-purpose processors.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从浮点协处理器到人工智能加速器的演变揭示了一个一致的规律：计算瓶颈推动专用硬件的发展。英特尔8087处理器解决了占科学计算时间80%的浮点运算问题，而现代人工智能工作负载则是一个更为极端的案例。矩阵乘法和卷积构成了神经网络计算超过95%的部分。这种计算需求的集中创造了前所未有的专业化机会，这也解释了为什么人工智能加速器在通用处理器上实现了100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的性能提升。
- en: 'The specialization principles established through decades of hardware evolution
    identifying dominant operations, creating dedicated datapaths, and optimizing
    memory access patterns now guide AI accelerator design. However, neural networks
    introduce unique characteristics that demand new architectural approaches: massive
    parallelism in matrix operations, predictable data access patterns enabling prefetching,
    and tolerance for reduced precision that allows aggressive optimization. Understanding
    these computational patterns, which we term AI compute primitives, helps comprehend
    how modern accelerators transform the theoretical efficiency gains from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    into practical performance improvements. These hardware-software optimizations
    become critical in deployment scenarios ranging from [Chapter 2](ch008.xhtml#sec-ml-systems)
    edge devices to cloud-scale inference systems.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数十年的硬件进化所确立的专业化原则，识别主导操作、创建专用数据路径和优化内存访问模式，现在指导着AI加速器的设计。然而，神经网络引入了独特的特性，要求新的架构方法：矩阵操作中的大规模并行性、可预测的数据访问模式以实现预取，以及对降低精度的容忍度，这允许进行激进的优化。理解这些计算模式，我们称之为AI计算原语，有助于理解现代加速器如何将[第10章](ch016.xhtml#sec-model-optimizations)中的理论效率提升转化为实际性能改进。这些软硬件优化在从[第2章](ch008.xhtml#sec-ml-systems)边缘设备到云规模推理系统等部署场景中变得至关重要。
- en: Before examining these computational primitives in detail, we need to understand
    the architectural organization that enables their efficient execution. Modern
    AI accelerators achieve their dramatic performance improvements through a carefully
    orchestrated hierarchy of specialized components operating in concert. The architecture
    comprises three subsystems, each addressing distinct aspects of the computational
    challenge.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细检查这些计算原语之前，我们需要了解能够实现其高效执行的架构组织。现代AI加速器通过精心编排的、协同工作的专用组件的层次结构实现了显著的性能提升。该架构包括三个子系统，每个子系统都针对计算挑战的不同方面。
- en: 'The processing substrate consists of an array of processing elements, each
    containing dedicated computational units optimized for specific operations: tensor
    cores execute matrix multiplication, vector units perform element-wise operations,
    and special function units compute activation functions. These processing elements
    are organized in a grid topology that enables massive parallelism, with dozens
    to hundreds of units operating simultaneously on different portions of the computation,
    exploiting the data-level parallelism inherent in neural network workloads.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 处理基板由一系列处理单元组成，每个单元都包含针对特定操作优化的专用计算单元：张量核心执行矩阵乘法，向量单元执行逐元素操作，而特殊功能单元计算激活函数。这些处理单元以网格拓扑组织，实现了大规模并行处理，数十到数百个单元可以同时处理计算的不同部分，利用神经网络工作负载中固有的数据级并行性。
- en: 'The memory hierarchy forms an equally critical architectural component. High-bandwidth
    memory provides the aggregate throughput required to sustain these numerous processing
    elements, while a multi-level cache hierarchy from shared L2 caches down to per-element
    L1 caches and scratchpads minimizes the energy cost of data movement. This hierarchical
    organization embodies a design principle: in AI accelerators, data movement typically
    consumes more energy than computation itself, necessitating architectural strategies
    that prioritize data reuse by maintaining frequently accessed values, including
    weights and partial results, in proximity to compute units.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构是一个同样关键的架构组件。高带宽内存提供了维持这些众多处理单元所需的总体吞吐量，而从共享L2缓存到每个元素的L1缓存和临时存储的多级缓存层次结构最小化了数据移动的能量成本。这种层次化组织体现了一个设计原则：在AI加速器中，数据移动通常比计算本身消耗更多的能量，需要优先考虑数据重用，通过在计算单元附近保持频繁访问的值（包括权重和部分结果）来实现。
- en: 'The host interface establishes connectivity between the specialized accelerator
    and the broader computing system, enabling coordination between general-purpose
    CPUs that manage program control flow and the accelerator that executes computationally
    intensive neural network operations. This architectural partitioning reflects
    specialization at the system level: CPUs address control flow, conditional logic,
    and system coordination, while accelerators focus on the regular, massively parallel
    arithmetic operations that dominate neural network execution.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 主机接口在专用加速器和更广泛的计算系统之间建立连接，使管理程序控制流的通用CPU和执行计算密集型神经网络操作的加速器之间的协调成为可能。这种架构分区反映了系统级别的专业化：CPU处理控制流、条件逻辑和系统协调，而加速器专注于主导神经网络执行的规律、大规模并行算术操作。
- en: '[Figure 11.2](ch017.xhtml#fig-accelerator-anatomy) illustrates this architectural
    organization, showing how specialized compute units, hierarchical memory subsystems,
    and host connectivity integrate to form a system optimized for AI workloads.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.2](ch017.xhtml#fig-accelerator-anatomy)展示了这种架构组织，展示了专用计算单元、分层内存子系统和主机连接如何集成，形成一个针对人工智能工作负载优化的系统。'
- en: '![](../media/file181.svg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file181.svg)'
- en: 'Figure 11.2: **Anatomy of a Modern AI Accelerator**: AI accelerators integrate
    specialized processing elements containing tensor cores, vector units, and special
    function units, supported by a hierarchical memory system from high-bandwidth
    memory down to local caches. This architecture maximizes data reuse and parallel
    execution while minimizing energy-intensive data movement, forming the foundation
    for 100-1000× performance improvements over general-purpose processors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：**现代人工智能加速器的解剖结构**：人工智能加速器集成了包含张量核心、向量单元和特殊功能单元的专用处理元素，由从高带宽内存到本地缓存的分层内存系统支持。这种架构最大化了数据重用和并行执行，同时最小化了能耗密集型数据移动，为相对于通用处理器的100-1000倍性能提升奠定了基础。
- en: AI Compute Primitives
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能计算原语
- en: Understanding how hardware evolved toward AI-specific designs requires examining
    the computational patterns that drove this specialization. The transition from
    general-purpose CPUs achieving 100 GFLOPS to specialized accelerators delivering
    100,000+ GFLOPS reflects architectural optimization for specific computational
    patterns that dominate machine learning workloads. These patterns, which we term
    compute primitives, appear repeatedly across all neural network architectures
    regardless of application domain or model size.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 了解硬件如何向针对人工智能特定设计演变，需要检查推动这种专业化的计算模式。从实现100 GFLOPS的通用CPU到提供100,000+ GFLOPS的专用加速器过渡，反映了针对主导机器学习工作负载的特定计算模式的架构优化。这些模式，我们称之为计算原语，无论应用领域或模型大小如何，都在所有神经网络架构中反复出现。
- en: 'Modern neural networks are built upon a small number of core computational
    patterns. Regardless of the layer type—whether fully connected, convolutional,
    or attention-based layers—the underlying operation typically involves multiplying
    input values by learned weights and accumulating the results. This repeated multiply-accumulate
    process dominates neural network execution and defines the arithmetic foundation
    of AI workloads. The regularity and frequency of these operations have led to
    the development of AI compute primitives: hardware-level abstractions optimized
    to execute these core computations with high efficiency.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现代神经网络建立在少数几个核心计算模式之上。无论层类型是全连接、卷积还是基于注意力的层，底层操作通常涉及将输入值乘以学习到的权重并累积结果。这种重复的乘加过程主导了神经网络执行，并定义了人工智能工作负载的算术基础。这些操作的规律性和频率导致了人工智能计算原语的发展：硬件级别的抽象，旨在以高效率执行这些核心计算。
- en: Neural networks exhibit highly structured, data-parallel computations that enable
    architectural specialization. Building on the parallelization principles established
    in [Section 11.2.2](ch017.xhtml#sec-ai-acceleration-parallel-computing-graphics-processing-66b1),
    these patterns emphasize predictable data reuse and fixed operation sequences.
    AI compute primitives distill these patterns into reusable architectural units
    that support high-throughput and energy-efficient execution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络表现出高度结构化和数据并行的计算，这使架构专业化成为可能。在[第11.2.2节](ch017.xhtml#sec-ai-acceleration-parallel-computing-graphics-processing-66b1)中建立的并行化原则的基础上，这些模式强调了可预测的数据重用和固定的操作序列。人工智能计算原语将这些模式提炼成可重用的架构单元，支持高吞吐量和节能的执行。
- en: This decomposition is illustrated in [Listing 11.1](ch017.xhtml#lst-dense_layer_def),
    which defines a dense layer at the framework level.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解在[列表 11.1](ch017.xhtml#lst-dense_layer_def)中展示，它定义了框架级别的密集层。
- en: 'Listing 11.1: **Dense Layer Definition**: Defines a dense layer using a high-level
    API, illustrating how neural networks implement parallel transformations across
    input tensors.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 11.1: **密集层定义**: 使用高级 API 定义密集层，展示了神经网络如何实现输入张量上的并行转换。'
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This high-level call expands into mathematical operations is shown in [Listing 11.2](ch017.xhtml#lst-dense_expansion).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高级调用扩展为数学操作，如[列表 11.2](ch017.xhtml#lst-dense_expansion)所示。
- en: 'Listing 11.2: **Layer Computation**: Neural networks compute each layer’s output
    via weighted input summation followed by an activation function transformation.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 11.2: **层计算**: 神经网络通过加权输入求和后应用激活函数转换来计算每一层的输出。'
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At the processor level, the computation reduces to nested loops that multiply
    inputs and weights, sum the results, and apply a nonlinear function, as shown
    in [Listing 11.3](ch017.xhtml#lst-loop_level_dense).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理器级别，计算简化为嵌套循环，这些循环乘以输入和权重，求和结果，并应用非线性函数，如[列表 11.3](ch017.xhtml#lst-loop_level_dense)所示。
- en: 'Listing 11.3: **Nested Loops**: Computes output values through sequential matrix
    multiplications and bias additions, followed by activation function application
    to produce final outputs.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 11.3: **嵌套循环**: 通过顺序矩阵乘法和偏置添加计算输出值，随后应用激活函数以产生最终输出。'
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This transformation reveals four computational characteristics: data-level
    parallelism enabling simultaneous execution, structured matrix operations defining
    computational workloads, predictable data movement patterns driving memory optimization,
    and frequent nonlinear transformations motivating specialized function units.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换揭示了四个计算特性：数据级并行性允许同时执行，结构化矩阵操作定义计算工作负载，可预测的数据移动模式驱动内存优化，以及频繁的非线性变换推动专用功能单元。
- en: The design of AI compute primitives follows three architectural criteria. First,
    the primitive must be used frequently enough to justify dedicated hardware resources.
    Second, its specialized implementation must offer substantial performance or energy
    efficiency gains relative to general-purpose alternatives. Third, the primitive
    must remain stable across generations of neural network architectures to ensure
    long-term applicability. These considerations shape the inclusion of primitives
    such as vector operations, matrix operations, and special function units in modern
    ML accelerators. Together, they serve as the architectural foundation for efficient
    and scalable neural network execution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能计算原语的设计遵循三个架构标准。首先，原语必须频繁使用，以证明专用硬件资源的合理性。其次，其专用实现必须相对于通用替代方案提供实质性的性能或能效提升。第三，原语必须跨代神经网络架构保持稳定，以确保长期适用性。这些考虑因素决定了向量操作、矩阵操作和特殊功能单元等原语在现代机器学习加速器中的包含。它们共同构成了高效且可扩展神经网络执行的架构基础。
- en: Vector Operations
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量操作
- en: Vector operations provide the first level of hardware acceleration by processing
    multiple data elements simultaneously. This parallelism exists at multiple scales,
    from individual neurons to entire layers, making vector processing essential for
    efficient neural network execution. Framework-level code translates to hardware
    instructions, revealing the critical role of vector processing in neural accelerators.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 向量操作通过同时处理多个数据元素提供第一层硬件加速。这种并行性存在于多个尺度上，从单个神经元到整个层，使得向量处理对于高效神经网络执行至关重要。框架级代码转换为硬件指令，揭示了向量处理在神经网络加速器中的关键作用。
- en: High-Level Framework Operations
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高级框架操作
- en: Machine learning frameworks hide hardware complexity through high-level abstractions.
    These abstractions decompose into progressively lower-level operations, revealing
    opportunities for hardware acceleration. One such abstraction is shown in [Listing 11.4](ch017.xhtml#lst-linear_layer_highlevel),
    which illustrates the execution flow of a linear layer.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习框架通过高级抽象隐藏硬件复杂性。这些抽象分解为越来越低级的操作，揭示了硬件加速的机会。其中一个这样的抽象在[列表 11.4](ch017.xhtml#lst-linear_layer_highlevel)中展示，它说明了线性层的执行流程。
- en: 'Listing 11.4: **Linear Layer**: Neural networks transform input data into a
    higher-dimensional space using linear mappings to enable complex feature extraction.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 11.4: **线性层**: 神经网络通过线性映射将输入数据转换到更高维的空间，以实现复杂特征提取。'
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This abstraction represents a fully connected layer that transforms input features
    through learned weights. To understand how hardware acceleration opportunities
    emerge, [Listing 11.5](ch017.xhtml#lst-linear_math_internal) shows how the framework
    translates this high-level expression into mathematical operations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象表示一个全连接层，通过学习到的权重将输入特征进行转换。为了理解硬件加速机会的出现，[列表 11.5](ch017.xhtml#lst-linear_math_internal)
    展示了框架如何将这一高级表达式转换为数学运算。
- en: 'Listing 11.5: **Fully Connected Layer**: Each output is computed as a weighted
    sum of all inputs plus a bias, followed by an activation function transformation.
    Linear transformations enable complex model architectures in neural networks.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5：**全连接层**：每个输出被计算为所有输入的加权总和加上一个偏置，然后通过激活函数转换。线性变换使神经网络中的复杂模型架构成为可能。
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These mathematical operations further decompose into explicit computational
    steps during processor execution. [Listing 11.6](ch017.xhtml#lst-loop_linear_layer)
    illustrates the nested loops that implement these multiply-accumulate operations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数学运算在处理器执行过程中进一步分解为明确的计算步骤。[列表 11.6](ch017.xhtml#lst-loop_linear_layer) 展示了实现这些乘加操作的嵌套循环。
- en: 'Listing 11.6: **Linear Layer Computation**: Each output neuron is computed
    by summing weighted inputs from all features, followed by an activation function
    application. Understanding this process helps in grasping the fundamental building
    blocks of neural networks.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.6：**线性层计算**：每个输出神经元通过将所有特征的加权输入求和，然后应用激活函数来计算。理解这一过程有助于掌握神经网络的基本构建块。
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sequential Scalar Execution
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 顺序标量执行
- en: Traditional scalar processors execute these operations sequentially, processing
    individual values one at a time. For the linear layer example above with a batch
    of 32 samples, computing the outputs requires over 4 million multiply-accumulate
    operations. Each operation involves loading an input value and a weight value,
    multiplying them, and accumulating the result. This sequential approach becomes
    highly inefficient when processing the massive number of identical operations
    required by neural networks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 传统标量处理器按顺序执行这些操作，一次处理一个单独的值。对于上面提到的包含 32 个样本的线性层示例，计算输出需要超过 400 万次乘加运算。每次操作都涉及加载一个输入值和一个权重值，将它们相乘，并累加结果。当处理神经网络所需的巨大数量的相同操作时，这种顺序方法变得非常低效。
- en: Recognizing this inefficiency, modern processors leverage vector processing
    to transform execution patterns fundamentally.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到这种低效性，现代处理器利用向量处理来从根本上改变执行模式。
- en: Parallel Vector Execution
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行向量执行
- en: Vector processing units achieve this transformation by operating on multiple
    data elements simultaneously. [Listing 11.7](ch017.xhtml#lst-riscv_vector_mac)
    demonstrates this approach using RISC-V[16](#fn16) assembly code that showcases
    modern vector processing capabilities.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 向量处理单元通过同时操作多个数据元素来实现这种转换。[列表 11.7](ch017.xhtml#lst-riscv_vector_mac) 使用 RISC-V[16](#fn16)
    汇编代码展示了现代向量处理能力。
- en: 'Listing 11.7: **Vectorized Multiply-Accumulate Loop**: This loop showcases
    how RISC-V vector instructions enable efficient batch processing by performing
    8 multiply-add operations simultaneously, reducing computational latency in neural
    network training. *Source: RISC-V Architecture Manual*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.7：**向量化的乘加循环**：这个循环展示了 RISC-V 向量指令如何通过同时执行 8 次乘加操作来高效地进行批量处理，从而降低神经网络训练中的计算延迟。*来源：RISC-V
    架构手册*
- en: '[PRE6]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This vector implementation processes eight data elements in parallel, reducing
    both computation time and energy consumption. Vector load instructions transfer
    eight values simultaneously, maximizing memory bandwidth utilization. The vector
    multiply-accumulate instruction processes eight pairs of values in parallel, dramatically
    reducing the total instruction count from over 4 million to approximately 500,000.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向量实现并行处理八个数据元素，减少了计算时间和能耗。向量加载指令同时传输八个值，最大化内存带宽利用率。向量乘加指令并行处理八个值对，将总指令数从超过
    400 万减少到大约 50 万。
- en: To clarify how vector instructions map to common deep learning patterns, [Table 11.2](ch017.xhtml#tbl-vector)
    introduces key vector operations and their typical applications in neural network
    computation. These operations, such as reduction, gather, scatter, and masked
    operations, are frequently encountered in layers like pooling, embedding lookups,
    and attention mechanisms. This terminology is necessary for interpreting how low-level
    vector hardware accelerates high-level machine learning workloads.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明向量指令如何映射到常见的深度学习模式，[表11.2](ch017.xhtml#tbl-vector)介绍了关键向量操作及其在神经网络计算中的典型应用。这些操作，如归约、收集、分散和掩码操作，在池化、嵌入查找和注意力机制等层中经常遇到。这些术语对于解释低级向量硬件如何加速高级机器学习工作负载是必要的。
- en: 'Table 11.2: **Vector Operations**: Neural network layers frequently utilize
    core vector operations such as reduction, gather, and scatter to accelerate computation
    and efficiently process data in parallel; these operations clarify how low-level
    hardware optimizations map to high-level machine learning algorithms. These operations
    enable efficient implementation of common layers like pooling, embedding lookups,
    and attention mechanisms within deep learning models.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.2：**向量操作**：神经网络层经常使用核心向量操作，如归约、收集和分散，以加速计算并有效地并行处理数据；这些操作阐明了低级硬件优化如何映射到高级机器学习算法。这些操作使得在深度学习模型中高效实现常见的层，如池化、嵌入查找和注意力机制成为可能。
- en: '| **Vector Operation** | **Description** | **Neural Network Application** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **向量操作** | **描述** | **神经网络应用** |'
- en: '| --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Reduction** | Combines elements across a vector (e.g., sum, max) | Pooling
    layers, attention score computation |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **归约** | 将向量中的元素组合在一起（例如，求和、最大值） | 池化层、注意力分数计算 |'
- en: '| **Gather** | Loads multiple non-consecutive memory elements | Embedding lookups,
    sparse operations |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **收集** | 加载多个非连续内存元素 | 嵌入查找，稀疏操作 |'
- en: '| **Scatter** | Writes to multiple non-consecutive memory locations | Gradient
    updates for embeddings |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **分散** | 写入多个非连续内存位置 | 嵌入的梯度更新 |'
- en: '| **Masked operations** | Selectively operates on vector elements | Attention
    masks, padding handling |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| **掩码操作** | 选择性地对向量元素进行操作 | 注意力掩码，填充处理 |'
- en: '| **Vector-scalar broadcast** | Applies scalar to all vector elements | Bias
    addition, scaling operations |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **向量-标量广播** | 将标量应用于所有向量元素 | 偏置添加，缩放操作 |'
- en: Vector processing efficiency gains extend beyond instruction count reduction.
    Memory bandwidth utilization improves as vector loads transfer multiple values
    per operation. Energy efficiency increases because control logic is shared across
    multiple operations. These improvements compound across the deep layers of modern
    neural networks, where billions of operations execute for each forward pass.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 向量处理效率的提升不仅限于指令计数减少。由于向量加载在每次操作中传输多个值，内存带宽利用率得到提高。由于控制逻辑在多个操作中共享，能效也得到提升。这些改进在现代神经网络的深层中累积，每个前向传递都会执行数十亿个操作。
- en: Vector Processing History
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量处理历史
- en: The principles underlying vector operations have long been central to high-performance
    computing. In the 1970s and 1980s, vector processors emerged as an architectural
    solution for scientific computing, weather modeling, and physics simulations,
    where large arrays of data required efficient parallel processing. Early systems
    such as the Cray-1[17](#fn17), one of the first commercially successful supercomputers,
    introduced dedicated vector units to perform arithmetic operations on entire data
    vectors in a single instruction. These vector units dramatically improved computational
    throughput compared to traditional scalar execution ([Jordan 1982](ch058.xhtml#ref-jordan1982guide)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 向量操作背后的原理长期以来一直是高性能计算的核心。在20世纪70年代和80年代，向量处理器作为科学计算、天气预报和物理模拟的架构解决方案出现，在这些领域，大量数据需要高效的并行处理。早期的系统，如Cray-1[17](#fn17)，作为首批商业上成功的超级计算机之一，引入了专门的向量单元，可以在单条指令中执行整个数据向量的算术运算。这些向量单元与传统的标量执行相比，显著提高了计算吞吐量([Jordan
    1982](ch058.xhtml#ref-jordan1982guide))。
- en: These concepts have reemerged in machine learning, where neural networks exhibit
    structure well suited to vectorized execution. The same operations, such as vector
    addition, multiplication, and reduction, that once accelerated numerical simulations
    now drive the execution of machine learning workloads. While the scale and specialization
    of modern AI accelerators differ from their historical predecessors, the underlying
    architectural principles remain the same. The resurgence of vector processing
    in neural network acceleration highlights its utility for achieving high computational
    efficiency.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在机器学习中重新出现，其中神经网络表现出适合向量执行的架构。曾经加速数值模拟的相同操作，如向量加法、乘法和归约，现在驱动着机器学习工作负载的执行。虽然现代AI加速器的规模和专业性与其历史前身不同，但底层架构原则保持不变。向量处理在神经网络加速中的复兴突出了其在实现高计算效率方面的效用。
- en: 'Vector operations establish the foundation for neural network acceleration
    by enabling efficient parallel processing of independent data elements. While
    vector operations excel at element-wise transformations like activation functions,
    neural networks also require structured computations that combine multiple input
    features to produce output features, transformations that naturally express themselves
    as matrix operations. This need for coordinated computation across multiple dimensions
    simultaneously leads to the next architectural primitive: matrix operations.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 向量运算通过允许独立数据元素的并行处理，为神经网络加速奠定了基础。虽然向量运算在元素级变换，如激活函数方面表现出色，但神经网络还需要结构化计算，将多个输入特征组合以产生输出特征，这些变换自然地表现为矩阵运算。这种在多个维度上同时进行协调计算的需求导致了下一个架构原语：矩阵运算。
- en: Matrix Operations
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: Matrix operations form the computational workhorse of neural networks, transforming
    high-dimensional data through structured patterns of weights, activations, and
    gradients ([Goodfellow, Courville, and Bengio 2013](ch058.xhtml#ref-Goodfellow-et-al-2016)).
    While vector operations process elements independently, matrix operations orchestrate
    computations across multiple dimensions simultaneously. These operations reveal
    patterns that drive hardware acceleration strategies.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算构成了神经网络的计算主力，通过权重、激活和梯度的结构化模式转换高维数据([Goodfellow, Courville, and Bengio 2013](ch058.xhtml#ref-Goodfellow-et-al-2016))。虽然向量运算独立处理元素，但矩阵运算同时协调多个维度的计算。这些操作揭示了驱动硬件加速策略的模式。
- en: Matrix Operations in Neural Networks
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络中的矩阵运算
- en: Neural network computations decompose into hierarchical matrix operations. As
    shown in [Listing 11.8](ch017.xhtml#lst-linear_matrix_hierarchy), a linear layer
    demonstrates this hierarchy by transforming input features into output neurons
    over a batch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络计算分解为分层矩阵运算。如图[列表11.8](ch017.xhtml#lst-linear_matrix_hierarchy)所示，一个线性层通过将输入特征转换为批次中的输出神经元来展示这种层次结构。
- en: 'Listing 11.8: **Matrix Operations**: Neural networks perform transformations
    using matrix multiplications and biases to achieve output predictions. Training
    requires careful management of input batches and activation functions to optimize
    model performance.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.8：**矩阵运算**：神经网络通过矩阵乘法和偏置来实现输出预测的转换。训练需要仔细管理输入批次和激活函数以优化模型性能。
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This computation demonstrates the scale of matrix operations in neural networks.
    Each output neuron (512 total) must process all input features (256 total) for
    every sample in the batch (32 samples). The weight matrix alone contains <semantics><mrow><mn>256</mn><mo>×</mo><mn>512</mn><mo>=</mo><mn>131</mn><mo>,</mo><mn>072</mn></mrow><annotation
    encoding="application/x-tex">256 \times 512 = 131,072</annotation></semantics>
    parameters that define these transformations, illustrating why efficient matrix
    multiplication becomes crucial for performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算展示了神经网络中矩阵运算的规模。每个输出神经元（总共512个）必须处理批次中每个样本的所有输入特征（总共256个）。仅权重矩阵就包含<semantics><mrow><mn>256</mn><mo>×</mo><mn>512</mn><mo>=</mo><mn>131</mn><mo>,</mo><mn>072</mn></mrow><annotation
    encoding="application/x-tex">256 \times 512 = 131,072</annotation></semantics>个参数，这些参数定义了这些转换，说明了为什么高效的矩阵乘法对于性能至关重要。
- en: Neural networks employ matrix operations across diverse architectural patterns
    beyond simple linear layers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在超越简单线性层的各种架构模式中采用矩阵运算。
- en: Types of Matrix Computations in Neural Networks
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络中的矩阵计算类型
- en: Matrix operations appear consistently across modern neural architectures, as
    illustrated in [Listing 11.9](ch017.xhtml#lst-matrix_patterns). Convolution operations
    are transformed into matrix multiplications through the im2col technique[18](#fn18),
    enabling efficient execution on hardware optimized for matrix operations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算在现代神经网络架构中始终如一，如[列表11.9](ch017.xhtml#lst-matrix_patterns)所示。通过im2col技术[18](#fn18)将卷积操作转换为矩阵乘法，从而在针对矩阵操作优化的硬件上实现高效执行。
- en: 'Listing 11.9: **Linear Layers**: Layer transformations combine input features
    to produce hidden representations. Matrix operations in neural networks enable
    efficient feature extraction and transformation, forming the backbone of many
    machine learning architectures.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.9：**线性层**：层变换将输入特征组合起来产生隐藏表示。神经网络中的矩阵运算能够实现高效的特征提取和转换，构成了许多机器学习架构的骨架。
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This pervasive pattern of matrix multiplication has direct implications for
    hardware design. The need for efficient matrix operations drives the development
    of specialized hardware architectures that can handle these computations at scale.
    The following sections explore how modern AI accelerators implement matrix operations,
    focusing on their architectural features and performance optimizations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种普遍的矩阵乘法模式对硬件设计有直接影响。对高效矩阵操作的需求推动了专门硬件架构的发展，这些架构能够大规模处理这些计算。以下章节将探讨现代AI加速器如何实现矩阵运算，重点关注其架构特性和性能优化。
- en: Matrix Operations Hardware Acceleration
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵运算硬件加速
- en: The computational demands of matrix operations have driven specialized hardware
    optimizations. Modern processors implement dedicated matrix units that extend
    beyond vector processing capabilities. An example of such matrix acceleration
    is shown in [Listing 11.10](ch017.xhtml#lst-matrix_unit).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算的计算需求推动了专门的硬件优化。现代处理器实现了超越向量处理能力的专用矩阵单元。这种矩阵加速的例子在[列表11.10](ch017.xhtml#lst-matrix_unit)中展示。
- en: 'Listing 11.10: **Matrix Unit Operation**: Enables efficient block-wise matrix
    multiplication and accumulation in hardware-accelerated systems, showcasing how
    specialized units streamline computational tasks essential for AI/ML operations.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.10：**矩阵单元操作**：在硬件加速系统中启用高效的块状矩阵乘法和累加，展示了专用单元如何简化对AI/ML操作至关重要的计算任务。
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This matrix processing unit can handle <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics> blocks of the
    linear layer computation described earlier, processing 256 multiply-accumulate
    operations simultaneously compared to the 8 operations possible with vector processing.
    These matrix operations complement vectorized computation by enabling structured
    many-to-many transformations. The interplay between matrix and vector operations
    shapes the efficiency of neural network execution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵处理单元可以处理之前描述的线性层计算的<semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics>块，与使用向量处理可能的8个操作相比，可以同时处理256个乘加操作。这些矩阵操作通过实现结构化的多对多转换，补充了向量化的计算。矩阵和向量操作之间的相互作用决定了神经网络执行效率。
- en: Matrix operations provide computational capabilities for neural networks through
    coordinated parallel processing across multiple dimensions (see [Table 11.3](ch017.xhtml#tbl-matrix)).
    While they enable transformations such as attention mechanisms and convolutions,
    their performance depends on efficient data handling. Conversely, vector operations
    are optimized for one-to-one transformations like activation functions and layer
    normalization. The distinction between these operations highlights the importance
    of dataflow patterns in neural accelerator design, examined next ([Hwu 2011](ch058.xhtml#ref-Hwu2011GPU)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算通过在多个维度上协调并行处理，为神经网络提供计算能力（参见[表11.3](ch017.xhtml#tbl-matrix)）。虽然它们能够实现诸如注意力机制和卷积等转换，但它们的性能取决于高效的数据处理。相反，向量运算针对一对一转换进行了优化，如激活函数和层归一化。这些操作的区分突出了数据流模式在神经网络加速器设计中的重要性，下文将对此进行探讨([Hwu
    2011](ch058.xhtml#ref-Hwu2011GPU))。
- en: 'Table 11.3: **Operation Characteristics**: Matrix operations excel at many-to-many
    transformations common in neural network layers, while vector operations efficiently
    handle one-to-one transformations like activation functions and normalization.
    Understanding these distinctions guides the selection of appropriate computational
    primitives for different machine learning tasks and impacts system performance.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.3：**操作特性**：矩阵运算擅长神经网络层中常见的多对多转换，而向量运算则高效地处理激活函数和归一化等一对一转换。理解这些区别有助于为不同的机器学习任务选择适当的计算原语，并影响系统性能。
- en: '| **Operation Type** | **Best For** | **Examples** | **Key Characteristic**
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **操作类型** | **最佳用途** | **示例** | **关键特性** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | *   Layer transformations |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  |  | *   层转换 |  |'
- en: '| **Matrix Operations** | Many-to-many transforms | *   Attention computation*   Convolutions*   Activation
    functions | Each output depends on multiple inputs |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **矩阵运算** | 多对多转换 | *   注意力计算*   卷积*   激活函数 | 每个输出依赖于多个输入 |'
- en: '| **Vector Operations** | One-to-one transforms | *   Layer normalization*   Element-wise
    gradients | Each output depends only on corresponding input |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **向量运算** | 一对一转换 | *   层归一化*   元素级梯度 | 每个输出只依赖于相应的输入 |'
- en: Historical Foundations of Matrix Computation
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵计算的历史基础
- en: Matrix operations have long served as a cornerstone of computational mathematics,
    with applications extending from numerical simulations to graphics processing
    ([Golub and Loan 1996](ch058.xhtml#ref-Golub1996Matrix)). The structured nature
    of matrix multiplications and transformations made them natural targets for acceleration
    in early computing architectures. In the 1980s and 1990s, specialized digital
    signal processors (DSPs) and graphics processing units (GPUs) optimized for matrix
    computations played a critical role in accelerating workloads such as image processing,
    scientific computing, and 3D rendering ([Owens et al. 2008](ch058.xhtml#ref-owens2008gpu)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算长期以来一直是计算数学的基石，其应用范围从数值模拟扩展到图形处理（[Golub 和 Loan 1996](ch058.xhtml#ref-Golub1996Matrix)）。矩阵乘法和变换的结构性质使它们成为早期计算架构加速的自然目标。在20世纪80年代和90年代，针对矩阵计算优化的专用数字信号处理器（DSPs）和图形处理单元（GPUs）在加速图像处理、科学计算和3D渲染等工作负载中发挥了关键作用（[Owens
    等人 2008](ch058.xhtml#ref-owens2008gpu)）。
- en: The widespread adoption of machine learning has reinforced the importance of
    efficient matrix computation. Neural networks, fundamentally built on matrix multiplications
    and tensor operations, have driven the development of dedicated hardware architectures
    that extend beyond traditional vector processing. Modern tensor processing units
    (TPUs) and AI accelerators implement matrix multiplication at scale, reflecting
    the same architectural principles that once underpinned early scientific computing
    and graphics workloads. The resurgence of matrix-centric architectures highlights
    the deep connection between classical numerical computing and contemporary AI
    acceleration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的广泛应用强化了高效矩阵计算的重要性。神经网络，其根本建立在矩阵乘法和张量运算之上，推动了专用硬件架构的发展，这些架构超越了传统的向量处理。现代的张量处理单元（TPUs）和AI加速器实现了大规模的矩阵乘法，反映了曾经支撑早期科学计算和图形工作负载的相同架构原则。矩阵中心架构的复兴突显了经典数值计算与当代AI加速之间的深层联系。
- en: While matrix operations provide the computational backbone for neural networks,
    they represent only part of the acceleration challenge. Neural networks also depend
    critically on non-linear transformations that cannot be efficiently expressed
    through linear algebra alone.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然矩阵运算为神经网络提供了计算骨干，但它们只代表了加速挑战的一部分。神经网络还严重依赖于无法仅通过线性代数有效表达的非线性变换。
- en: Special Function Units
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特殊功能单元
- en: While vector and matrix operations efficiently handle the linear transformations
    in neural networks, non-linear functions present unique computational challenges
    that require dedicated hardware solutions. Special Function Units (SFUs) provide
    hardware acceleration for these essential computations, completing the set of
    fundamental processing primitives needed for efficient neural network execution.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量矩阵运算能够高效地处理神经网络中的线性变换，但非线性函数提出了独特的计算挑战，需要专门的硬件解决方案。特殊功能单元（SFUs）为这些基本计算提供硬件加速，完成了高效神经网络执行所需的基本处理原语集。
- en: Non-Linear Functions
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非线性函数
- en: Non-linear functions play a fundamental role in machine learning by enabling
    neural networks to model complex relationships ([Goodfellow, Courville, and Bengio
    2013](ch058.xhtml#ref-Goodfellow-et-al-2016)). [Listing 11.11](ch017.xhtml#lst-nonlinear_layer)
    illustrates a typical neural network layer sequence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性函数在机器学习中扮演着基础角色，通过使神经网络能够模拟复杂关系([Goodfellow, Courville, and Bengio 2013](ch058.xhtml#ref-Goodfellow-et-al-2016))。[列表11.11](ch017.xhtml#lst-nonlinear_layer)展示了典型的神经网络层序列。
- en: 'Listing 11.11: **Non-Linear Transformations**: Neural networks process input
    data through a sequence of linear transformations followed by non-linear activations
    to capture complex patterns. This layer sequence enhances model expressiveness
    and learning capabilities.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.11：**非线性变换**：神经网络通过一系列线性变换后跟非线性激活来处理输入数据，以捕捉复杂模式。这个层序列增强了模型的表达能力和学习能力。
- en: '[PRE10]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This sequence introduces multiple non-linear transformations that extend beyond
    simple matrix operations. [Listing 11.12](ch017.xhtml#lst-nonlinear_math) demonstrates
    how the framework decomposes these operations into their mathematical components.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列引入了多个非线性变换，这些变换超越了简单的矩阵运算。[列表11.12](ch017.xhtml#lst-nonlinear_math)展示了框架如何将这些操作分解为其数学组成部分。
- en: 'Listing 11.12: **Non-linear Transformations**: Neural networks apply linear
    and non-linear operations to transform input data into meaningful features for
    learning. Machine learning models leverage these transformations to capture complex
    patterns in data efficiently.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.12：**非线性变换**：神经网络应用线性和非线性操作将输入数据转换为学习有意义特征。机器学习模型利用这些变换来有效地捕捉数据中的复杂模式。
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Hardware Implementation of Non-Linear Functions
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非线性函数的硬件实现
- en: 'The computational complexity of these operations becomes apparent when examining
    their implementation on traditional processors. These seemingly simple mathematical
    operations translate into complex sequences of instructions. Consider the computation
    of batch normalization: calculating the square root requires multiple iterations
    of numerical approximation, while exponential functions in operations like softmax
    need series expansion or lookup tables ([Ioffe and Szegedy 2015b](ch058.xhtml#ref-Ioffe2015)).
    Even a simple ReLU activation introduces branching logic that can disrupt instruction
    pipelining (see [Listing 11.13](ch017.xhtml#lst-traditional_overhead) for an example).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当在传统处理器上检查这些操作的实现时，它们的计算复杂性变得明显。这些看似简单的数学运算转化为复杂的指令序列。考虑批归一化的计算：计算平方根需要多次数值逼近迭代，而在softmax等操作中的指数函数需要级数展开或查找表([Ioffe
    and Szegedy 2015b](ch058.xhtml#ref-Ioffe2015))。即使是简单的ReLU激活也会引入分支逻辑，这可能会破坏指令流水线（参见[列表11.13](ch017.xhtml#lst-traditional_overhead)中的示例）。
- en: 'Listing 11.13: **ReLU and BatchNorm Operations**: Neural networks process input
    data through conditional operations that can disrupt instruction pipelining and
    multiple passes required for normalization, highlighting efficiency challenges
    in traditional implementations. Source: IEEE Spectrum'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.13：**ReLU和BatchNorm操作**：神经网络通过可能导致指令流水线中断的条件操作处理输入数据，以及为归一化所需的多次遍历，突出了传统实现中的效率挑战。来源：IEEE
    Spectrum
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These operations introduce several key inefficiencies:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作引入了几个关键的低效因素：
- en: Multiple passes over data, increasing memory bandwidth requirements
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多次遍历数据，增加内存带宽需求
- en: Complex arithmetic requiring many instruction cycles
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要许多指令周期的复杂算术
- en: Conditional operations that can cause pipeline stalls
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件操作可能导致流水线停滞
- en: Additional memory storage for intermediate results
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为中间结果提供额外的内存存储
- en: Poor utilization of vector processing units
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量处理单元利用率低
- en: 'More specifically, each operation introduces distinct challenges. Batch normalization
    requires multiple passes through data: one for mean computation, another for variance,
    and a final pass for output transformation. Each pass loads and stores data through
    the memory hierarchy. Operations that appear simple in mathematical notation often
    expand into many instructions. The square root computation typically requires
    10-20 iterations of numerical methods like Newton-Raphson approximation for suitable
    precision ([Goldberg 1991](ch058.xhtml#ref-Goldberg1991)). Conditional operations
    like ReLU’s max function require branch instructions that can stall the processor’s
    pipeline. The implementation needs temporary storage for intermediate values,
    increasing memory usage and bandwidth consumption. While vector units excel at
    regular computations, functions like exponentials and square roots often require
    scalar operations that cannot fully utilize vector processing capabilities.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，每个操作都引入了独特的挑战。批量归一化需要多次遍历数据：一次用于计算均值，另一次用于计算方差，最后一次用于输出转换。每次遍历都通过内存层次结构加载和存储数据。在数学符号中看似简单的操作通常扩展为许多指令。平方根计算通常需要10-20次牛顿-拉夫森近似等数值方法的迭代，以达到适当的精度（[Goldberg
    1991](ch058.xhtml#ref-Goldberg1991)）。像ReLU的最大函数这样的条件操作需要可能导致处理器流水线停滞的分支指令。实现需要临时存储中间值，这增加了内存使用和带宽消耗。虽然向量单元擅长常规计算，但像指数和平方根这样的函数通常需要不能充分利用向量处理能力的标量操作。
- en: Hardware Acceleration
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件加速
- en: SFUs address these inefficiencies through dedicated hardware implementation.
    Modern ML accelerators include specialized circuits that transform these complex
    operations into single-cycle or fixed-latency computations. The accelerator can
    load a vector of values and apply non-linear functions directly, eliminating the
    need for multiple passes and complex instruction sequences as shown in [Listing 11.14](ch017.xhtml#lst-sfu_vector_ops).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: SFUs通过专用硬件实现来解决这些低效问题。现代机器学习加速器包括将复杂操作转换为单周期或固定延迟计算的专用电路。加速器可以加载值向量并直接应用非线性函数，消除了多次遍历和复杂指令序列的需要，如[列表11.14](ch017.xhtml#lst-sfu_vector_ops)所示。
- en: 'Listing 11.14: **Hardware Acceleration**: Single-cycle non-linear operations
    enable efficient vector processing in ML accelerators, showcasing how specialized
    hardware reduces computational latency.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.14：**硬件加速**：单周期非线性操作使机器学习加速器中的向量处理效率更高，展示了专用硬件如何减少计算延迟。
- en: '[PRE13]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each SFU implements a specific function through specialized circuitry. For instance,
    a ReLU unit performs the comparison and selection in dedicated logic, eliminating
    branching overhead. Square root operations use hardware implementations of algorithms
    like Newton-Raphson with fixed iteration counts, providing guaranteed latency.
    Exponential and logarithmic functions often combine small lookup tables with hardware
    interpolation circuits ([Costa et al. 2019](ch058.xhtml#ref-Lauterbach2019)).
    Using these custom instructions, the SFU implementation eliminates multiple passes
    over data, removes complex arithmetic sequences, and maintains high computational
    efficiency. [Table 11.4](ch017.xhtml#tbl-sfu) shows the various hardware implementations
    and their typical latencies.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SFU通过专用电路实现特定的功能。例如，ReLU单元在专用逻辑中执行比较和选择，消除了分支开销。平方根操作使用具有固定迭代次数的算法（如牛顿-拉夫森算法）的硬件实现，提供保证的延迟。指数和对数函数通常结合小的查找表和硬件插值电路（[Costa等人2019](ch058.xhtml#ref-Lauterbach2019)）。使用这些自定义指令，SFU实现消除了对数据的多次遍历，移除了复杂的算术序列，并保持了高计算效率。[表11.4](ch017.xhtml#tbl-sfu)显示了各种硬件实现及其典型延迟。
- en: 'Table 11.4: **Special Function Units**: Dedicated hardware implementations
    of common mathematical functions—like relu, sigmoid, and reciprocal square root—accelerate
    machine learning computations by eliminating software overhead and enabling parallel
    processing of vector data. Typical latencies of 1–2 cycles per function demonstrate
    the performance gains achieved through specialized circuitry instead of general-purpose
    arithmetic.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.4：**特殊功能单元**：通过消除软件开销并允许并行处理向量数据，专用硬件实现常见的数学函数（如relu、sigmoid和倒数平方根）加速了机器学习计算。每个函数的典型延迟为1-2周期，这表明通过专用电路而不是通用算术所实现的性能提升。
- en: '| **Function Unit** | **Operation** | **Implementation Strategy** | **Typical
    Latency** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **功能单元** | **操作** | **实现策略** | **典型延迟** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Activation Unit** | ReLU, sigmoid, tanh | Piece-wise approximation circuits
    | 1-2 cycles |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **激活单元** | ReLU, sigmoid, tanh | 分段近似电路 | 1-2周期 |'
- en: '| **Statistics Unit** | Mean, variance | Parallel reduction trees | log(N)
    cycles |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **统计单元** | Mean, variance | 并行归约树 | log(N)周期 |'
- en: '| **Exponential Unit** | exp, log | Table lookup + hardware interpolation |
    2-4 cycles |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **指数单元** | exp, log | 表查找 + 硬件插值 | 2-4周期 |'
- en: '| **Root/Power Unit** | sqrt, rsqrt | Fixed-iteration Newton-Raphson | 4-8
    cycles |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **根/幂单元** | sqrt, rsqrt | 固定迭代牛顿-拉夫森 | 4-8周期 |'
- en: SFUs History
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SFU历史
- en: The need for efficient non-linear function evaluation has shaped computer architecture
    for decades. Early processors incorporated hardware support for complex mathematical
    functions, such as logarithms and trigonometric operations, to accelerate workloads
    in scientific computing and signal processing ([Smith 1997](ch058.xhtml#ref-Smith1997)).
    In the 1970s and 1980s, floating-point co-processors were introduced to handle
    complex mathematical operations separately from the main CPU ([Palmer 1980](ch058.xhtml#ref-palmer_8087_1981)).
    In the 1990s, instruction set extensions such as Intel’s SSE and ARM’s NEON provided
    dedicated hardware for vectorized mathematical transformations, improving efficiency
    for multimedia and signal processing applications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的非线性函数评估需求几十年来一直影响着计算机架构。早期的处理器集成了对复杂数学函数的硬件支持，例如对数和三角运算，以加速科学计算和信号处理工作负载（[Smith
    1997](ch058.xhtml#ref-Smith1997)）。在20世纪70年代和80年代，浮点协处理器被引入以独立于主CPU处理复杂数学运算（[Palmer
    1980](ch058.xhtml#ref-palmer_8087_1981)）。在20世纪90年代，指令集扩展如Intel的SSE和ARM的NEON提供了针对向量化的数学变换的专用硬件，提高了多媒体和信号处理应用的效率。
- en: Machine learning workloads have reintroduced a strong demand for specialized
    functional units, as activation functions, normalization layers, and exponential
    transformations are fundamental to neural network computations. Rather than relying
    on iterative software approximations, modern AI accelerators implement fast, fixed-latency
    SFUs for these operations, mirroring historical trends in scientific computing.
    The reemergence of dedicated special function units underscores the ongoing cycle
    in hardware evolution, where domain-specific requirements drive the reinvention
    of classical architectural concepts in new computational paradigms.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载重新引入了对专用功能单元的强烈需求，因为激活函数、归一化层和指数变换是神经网络计算的基础。现代人工智能加速器不再依赖于迭代的软件近似，而是为这些操作实现了快速、固定延迟的SFU，这与科学计算的历史趋势相呼应。专用特殊功能单元的重新出现强调了硬件演变的持续周期，其中特定领域的需求推动了在新的计算范式中对经典架构概念的重新发明。
- en: The combination of vector, matrix, and special function units provides the computational
    foundation for modern AI accelerators. However, the effective utilization of these
    processing primitives depends critically on data movement and access patterns.
    This leads us to examine the architectures, hierarchies, and strategies that enable
    efficient data flow in neural network execution.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 向量、矩阵和特殊功能单元的组合为现代人工智能加速器提供了计算基础。然而，这些处理原语的有效利用严重依赖于数据移动和访问模式。这使我们转向研究神经网络执行中实现高效数据流的架构、层次结构和策略。
- en: Compute Units and Execution Models
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算单元和执行模型
- en: The vector operations, matrix operations, and special function units examined
    previously represent the fundamental computational primitives in AI accelerators.
    Modern AI processors package these primitives into distinct execution units, such
    as SIMD units, tensor cores, and processing elements, which define how computations
    are structured and exposed to users. Understanding this organization reveals both
    the theoretical capabilities and practical performance characteristics that developers
    can leverage in contemporary AI accelerators.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 向量运算、矩阵运算和之前考察的特殊功能单元代表了人工智能加速器中的基本计算原语。现代人工智能处理器将这些原语打包成不同的执行单元，例如SIMD单元、张量核心和处理元素，这些单元定义了计算的结构以及如何向用户暴露。理解这种组织结构揭示了开发者可以利用的当代人工智能加速器的理论能力和实际性能特征。
- en: Mapping Primitives to Execution Units
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将原语映射到执行单元
- en: 'The progression from computational primitives to execution units follows a
    structured hierarchy that reflects the increasing complexity and specialization
    of AI accelerators:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算原语到执行单元的演变遵循一个结构化的层次结构，反映了人工智能加速器日益复杂化和专业化的趋势：
- en: Vector operations → SIMD/SIMT units that enable parallel processing of independent
    data elements
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量运算 → 允许独立数据元素并行处理的SIMD/SIMT单元
- en: Matrix operations → Tensor cores and systolic arrays that provide structured
    matrix multiplication
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵运算 → 提供结构化矩阵乘法的张量核心和收缩阵列
- en: Special functions → Dedicated hardware units integrated within processing elements
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特殊功能 → 集成在处理元素内的专用硬件单元
- en: Each execution unit combines these computational primitives with specialized
    memory and control mechanisms, optimizing both performance and energy efficiency.
    This structured packaging allows hardware vendors to expose standardized programming
    interfaces while implementing diverse underlying architectures tailored to specific
    workload requirements. The choice of execution unit significantly influences overall
    system efficiency, affecting data locality, compute density, and workload adaptability.
    Subsequent sections examine how these execution units operate within AI accelerators
    to maximize performance across different machine learning tasks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 每个执行单元将这组计算原语与专用内存和控制机制相结合，优化性能和能效。这种结构化封装允许硬件供应商在实现针对特定工作负载需求定制的多样化底层架构的同时，公开标准化的编程接口。执行单元的选择显著影响整体系统效率，影响数据局部性、计算密度和工作负载适应性。后续章节将探讨这些执行单元如何在人工智能加速器中运行，以最大化不同机器学习任务中的性能。
- en: Evolution from SIMD to SIMT Architectures
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从SIMD到SIMT架构的演变
- en: Single Instruction Multiple Data (SIMD)[19](#fn19) execution applies identical
    operations to multiple data elements in parallel, minimizing instruction overhead
    while maximizing data throughput. This execution model is widely used to accelerate
    workloads with regular, independent data parallelism, such as neural network computations.
    The ARM Scalable Vector Extension (SVE) provides a representative example of how
    modern architectures implement SIMD operations efficiently, as illustrated in
    [Listing 11.15](ch017.xhtml#lst-arm_sve_vector).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 单指令多数据（SIMD)[19](#fn19)执行并行地对多个数据元素应用相同的操作，最小化指令开销同时最大化数据吞吐量。这种执行模型广泛应用于具有规则、独立数据并行性的工作负载加速，如神经网络计算。ARM可伸缩向量扩展（SVE）提供了一个现代架构如何高效实现SIMD操作的代表性示例，如[列表11.15](ch017.xhtml#lst-arm_sve_vector)所示。
- en: 'Listing 11.15: **Vector Operation**: Vector multiplication and addition operations
    enable efficient parallel processing in machine learning models. *Source: ARM
    Documentation*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.15：**向量运算**：向量乘法和加法运算在机器学习模型中实现高效的并行处理。*来源：ARM文档*
- en: '[PRE14]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Processor architectures continue to expand SIMD capabilities to accommodate
    increasing computational demands. Intel’s Advanced Matrix Extensions (AMX) ([I.
    Corporation 2021](ch058.xhtml#ref-intel2021amx)) and ARM’s SVE2 architecture ([Stephens
    et al. 2017](ch058.xhtml#ref-stephens2017arm)) provide flexible SIMD execution,
    enabling software to scale across different hardware implementations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器架构继续扩展SIMD能力，以适应不断增长的计算需求。Intel的高级矩阵扩展（AMX）([I. 公司 2021](ch058.xhtml#ref-intel2021amx))和ARM的SVE2架构([Stephens等人
    2017](ch058.xhtml#ref-stephens2017arm))提供了灵活的SIMD执行，使软件能够跨不同的硬件实现进行扩展。
- en: To address these limitations, SIMT extends SIMD principles by enabling parallel
    execution across multiple independent threads, each maintaining its own program
    counter and architectural state ([E. Lindholm et al. 2008](ch058.xhtml#ref-lindholm2008nvidia)).
    This model maps naturally to matrix computations, where each thread processes
    different portions of a workload while still benefiting from shared instruction
    execution. In NVIDIA’s GPU architectures, each Streaming Multiprocessor (SM)[20](#fn20)
    coordinates thousands of threads executing in parallel, allowing for efficient
    scaling of neural network computations, as demonstrated in [Listing 11.16](ch017.xhtml#lst-cuda_simt).
    Threads are organized into warps[21](#fn21), which are the fundamental execution
    units that enable SIMT efficiency.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，SIMT通过允许多个独立线程的并行执行扩展了SIMD原则，每个线程保持自己的程序计数器和架构状态（[E. Lindholm等人 2008](ch058.xhtml#ref-lindholm2008nvidia)）。这种模型自然地映射到矩阵计算，其中每个线程处理工作负载的不同部分，同时仍然受益于共享指令执行。在NVIDIA的GPU架构中，每个流多处理器（SM)[20](#fn20)协调成千上万的并行执行的线程，允许高效地扩展神经网络计算，如[列表11.16](ch017.xhtml#lst-cuda_simt)所示。线程被组织成warp[21](#fn21)，它们是使SIMT效率化的基本执行单元。
- en: 'Listing 11.16: **SIMT Execution**: Each thread processes a unique output element
    in parallel, demonstrating how SIMT enables efficient matrix multiplication on
    GPUs.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.16：**SIMT执行**：每个线程并行处理一个独特的输出元素，展示了SIMT如何在GPU上实现高效的矩阵乘法。
- en: '[PRE15]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: SIMT execution allows neural network computations to scale efficiently across
    thousands of threads while maintaining flexibility for divergent execution paths.
    Similar execution models appear in AMD’s RDNA and Intel’s Xe architectures, reinforcing
    SIMT as a fundamental mechanism for AI acceleration.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: SIMT执行允许神经网络计算在数千个线程上高效扩展，同时保持对发散执行路径的灵活性。类似的执行模型出现在AMD的RDNA和Intel的Xe架构中，强化了SIMT作为AI加速的基本机制。
- en: Tensor Cores
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量核心
- en: 'While SIMD and SIMT units provide efficient execution of vector operations,
    neural networks rely heavily on matrix computations that require specialized execution
    units for structured multi-dimensional processing. The energy economics of matrix
    operations drive this specialization: traditional scalar processing requires multiple
    DRAM accesses per operation, consuming 640 pJ per fetch, while tensor cores amortize
    this energy cost across entire matrix blocks. Tensor processing units extend SIMD
    and SIMT principles by enabling efficient matrix operations through dedicated
    hardware blocks that execute matrix multiplications and accumulations on entire
    matrix blocks in a single operation. Tensor cores transform the energy profile
    from 173× memory-bound inefficiency to compute-optimized execution where the 3.7 pJ
    multiply-accumulate operation dominates the energy budget rather than data movement.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SIMD和SIMT单元提供了高效的向量操作执行，但神经网络严重依赖于需要专门执行单元进行结构化多维处理的矩阵计算。矩阵操作的能量经济学推动了这种专业化：传统的标量处理需要每个操作多次DRAM访问，每次访问消耗640
    pJ，而张量核心将这种能量成本分摊到整个矩阵块上。张量处理单元通过启用通过专用硬件块执行矩阵乘法和累加的矩阵操作，扩展了SIMD和SIMT原则，这些硬件块在单个操作中在整个矩阵块上执行。张量核心将能量配置文件从173×内存受限的低效转变为计算优化的执行，其中3.7
    pJ的乘累加操作主导了能量预算，而不是数据移动。
- en: Tensor cores[22](#fn22), implemented in architectures such as NVIDIA’s Ampere
    GPUs, provide an example of this approach. They expose matrix computation capabilities
    through specialized instructions, such as the tensor core operation shown in [Listing 11.17](ch017.xhtml#lst-tensor_core_op)
    on the NVIDIA A100 GPU.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 张量核心[22](#fn22)，如NVIDIA的Ampere GPU架构中所实现，是这种方法的例子。它们通过专用指令，如NVIDIA A100 GPU上[列表
    11.17](ch017.xhtml#lst-tensor_core_op)中所示的张量核心操作，公开了矩阵计算能力。
- en: 'Listing 11.17: **Tensor Core Operation**: Matrix multiplications are performed
    in parallel across entire matrix blocks, optimizing computational efficiency for
    neural network training.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.17：**张量核心操作**：矩阵乘法在整个矩阵块上并行执行，优化了神经网络训练的计算效率。
- en: '[PRE16]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A single tensor core instruction processes an entire matrix block while maintaining
    intermediate results in local registers, significantly improving computational
    efficiency compared to implementations based on scalar or vector operations. This
    structured approach enables hardware to achieve high throughput while reducing
    the burden of explicit loop unrolling and data management at the software level.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 单个张量核心指令处理整个矩阵块，同时在局部寄存器中保持中间结果，与基于标量或向量操作的实施相比，显著提高了计算效率。这种结构化方法使硬件能够实现高吞吐量，同时减少软件层面上的显式循环展开和数据管理的负担。
- en: Tensor processing unit architectures differ based on design priorities. NVIDIA’s
    Ampere architecture incorporates tensor cores optimized for general-purpose deep
    learning acceleration. Google’s TPUv4 utilizes large-scale matrix units arranged
    in systolic arrays to maximize sustained training throughput. Apple’s M1 neural
    engine[23](#fn23) integrates smaller matrix processors optimized for mobile inference
    workloads, while Intel’s Sapphire Rapids architecture introduces AMX tiles designed
    for high-performance datacenter applications.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 张量处理单元架构根据设计优先级而有所不同。NVIDIA的Ampere架构集成了针对通用深度学习加速优化的张量核心。Google的TPUv4利用大规模矩阵单元，以阵列形式排列，以最大化持续的训练吞吐量。Apple的M1神经网络引擎[23](#fn23)集成了针对移动推理工作负载优化的较小矩阵处理器，而Intel的Sapphire
    Rapids架构引入了专为高性能数据中心应用设计的AMX瓦片。
- en: The increasing specialization of AI hardware has driven significant performance
    improvements in deep learning workloads. [Figure 11.3](ch017.xhtml#fig-ai-performance)
    illustrates the trajectory of AI accelerator performance in NVIDIA GPUs, highlighting
    the transition from general-purpose floating-point execution units to highly optimized
    tensor processing cores.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: AI硬件的日益专业化推动了深度学习工作负载的显著性能提升。[图11.3](ch017.xhtml#fig-ai-performance)展示了NVIDIA
    GPU中AI加速器性能的轨迹，突出了从通用浮点执行单元到高度优化的张量处理核心的过渡。
- en: '![](../media/file182.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file182.png)'
- en: 'Figure 11.3: **GPU Performance Scaling**: NVIDIA GPUs experienced a <semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics> increase in integer
    8-bit TOPS (tera operations per second) over a decade, driven by architectural
    innovations transitioning from floating-point to tensor core acceleration. This
    trend reflects the growing specialization of hardware for deep learning workloads
    and the increasing demand for efficient inference capabilities.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：**GPU性能提升**：NVIDIA GPU在十年间整数8位TOPS（每秒兆次运算）上实现了<semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics>的增长，这一增长是由从浮点运算向张量核心加速的架构创新驱动的。这一趋势反映了硬件在深度学习工作负载中的日益专业化以及对于高效推理能力的不断增长需求。
- en: Processing Elements
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理单元
- en: The highest level of execution unit organization integrates multiple tensor
    cores with local memory into processing elements (PEs). A processing element serves
    as a fundamental building block in many AI accelerators, combining different computational
    units to efficiently execute neural network operations. Each PE typically includes
    vector units for element-wise operations, tensor cores for matrix computation,
    special function units for non-linear transformations, and dedicated memory resources
    to optimize data locality and minimize data movement overhead.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最高级别的执行单元组织将多个张量核心与本地内存集成到处理单元（PEs）中。处理单元是许多AI加速器的基本构建块，通过结合不同的计算单元来高效执行神经网络操作。每个PE通常包括用于逐元素操作的向量单元、用于矩阵计算的张量核心、用于非线性变换的特殊功能单元以及专门的内存资源，以优化数据局部性和最小化数据移动开销。
- en: Processing elements play an essential role in AI hardware by balancing computational
    density with memory access efficiency. Their design varies across different architectures
    to support diverse workloads and scalability requirements. Graphcore’s Intelligence
    Processing Unit (IPU) distributes computation across 1,472 tiles, each containing
    independent processing elements optimized for fine-grained parallelism ([Graphcore
    2020](ch058.xhtml#ref-Graphcore2020)). Cerebras extends this approach in the CS-2
    system, integrating 850,000 processing elements across a wafer-scale device to
    accelerate sparse computations. Tesla’s D1 processor arranges processing elements
    with substantial local memory, optimizing throughput and latency for real-time
    autonomous vehicle workloads ([Quinnell 2024](ch058.xhtml#ref-Tesla2021)).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 处理单元在AI硬件中发挥着至关重要的作用，通过平衡计算密度与内存访问效率。它们的设计在不同架构中有所不同，以支持多样化的工作负载和可扩展性要求。Graphcore的智能处理单元（IPU）将计算分布在1,472个瓦片上，每个瓦片包含独立优化的处理单元，以支持细粒度并行性（[Graphcore
    2020](ch058.xhtml#ref-Graphcore2020)）。Cerebras在CS-2系统中扩展了这种方法，通过晶圆级设备集成了850,000个处理单元，以加速稀疏计算。Tesla的D1处理器通过大量本地内存排列处理单元，优化了实时自动驾驶工作负载的吞吐量和延迟（[Quinnell
    2024](ch058.xhtml#ref-Tesla2021)）。
- en: Processing elements provide the structural foundation for large-scale AI acceleration.
    Their efficiency depends not only on computational capability but also on interconnect
    strategies and memory hierarchy design. The next sections explore how these architectural
    choices impact performance across different AI workloads.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 处理单元为大规模AI加速提供了结构基础。它们的效率不仅取决于计算能力，还取决于互连策略和内存层次结构设计。接下来的几节将探讨这些架构选择如何影响不同AI工作负载的性能。
- en: Tensor processing units have enabled substantial efficiency gains in AI workloads
    by using hardware-accelerated matrix computation. Their role continues to evolve
    as architectures incorporate support for advanced execution techniques, including
    structured sparsity and workload-specific optimizations. The effectiveness of
    these units, however, depends not only on their computational capabilities but
    also on how they interact with memory hierarchies and data movement mechanisms,
    which are examined in subsequent sections.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 张量处理单元通过使用硬件加速的矩阵计算，在人工智能工作负载中实现了显著的效率提升。随着架构中纳入对高级执行技术的支持，包括结构化稀疏性和特定工作负载优化，它们的作用也在不断演变。然而，这些单元的有效性不仅取决于它们的计算能力，还取决于它们与内存层次结构和数据移动机制如何交互，这些内容将在后续章节中探讨。
- en: Systolic Arrays
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 系统阵列
- en: While tensor cores package matrix operations into structured computational units,
    systolic arrays provide an alternative approach optimized for continuous data
    flow and operand reuse. The fundamental motivation for systolic architectures
    stems from the energy efficiency constraints discussed earlier—minimizing the
    impact of memory access penalties through architectural design. A systolic array
    arranges processing elements in a grid pattern, where data flows rhythmically
    between neighboring units in a synchronized manner, enabling each operand to participate
    in multiple computations as it propagates through the array. This structured movement
    minimizes external memory accesses by maximizing local data reuse—a single weight
    value can contribute to dozens of operations as it moves through the processing
    elements, fundamentally transforming the energy profile from memory-bound to compute-efficient
    execution.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当张量核心将矩阵运算打包成结构化计算单元时，系统阵列提供了一种针对连续数据流和操作数重用优化的替代方法。系统架构的基本动机源于之前讨论的能量效率约束——通过架构设计最小化内存访问惩罚的影响。系统阵列以网格模式排列处理单元，其中数据以同步方式在相邻单元之间有节奏地流动，使得每个操作数在通过阵列传播时能够参与多个计算。这种结构化移动通过最大化本地数据重用最小化了外部内存访问——单个权重值可以在通过处理单元时对数十个操作做出贡献，从而从根本上将能量配置文件从内存受限转变为计算高效执行。
- en: The concept of systolic arrays was first introduced by Kung and Leiserson[24](#fn24),
    who formalized their use in parallel computing architectures for efficient matrix
    operations ([Kung 1982](ch058.xhtml#ref-Kung1982)). Unlike general-purpose execution
    units, systolic arrays exploit spatial and temporal locality by reusing operands
    as they propagate through the grid. Google’s TPU exemplifies this architectural
    approach. In the TPUv4, a <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics> systolic array
    of multiply-accumulate units processes matrix operations by streaming data through
    the array in a pipelined manner, as shown in [Figure 11.4](ch017.xhtml#fig-systolic-array).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 系统阵列的概念最初由Kung和Leiserson[24](#fn24)提出，他们在并行计算架构中正式化了其应用，以实现高效的矩阵运算([Kung 1982](ch058.xhtml#ref-Kung1982))。与通用执行单元不同，系统阵列通过在网格中传播时重复使用操作数来利用空间和时间局部性。谷歌的TPU就是这种架构方法的例证。在TPUv4中，一个<semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics>的系统阵列乘累加单元通过以流水线方式通过阵列流数据来处理矩阵运算，如图[图11.4](ch017.xhtml#fig-systolic-array)所示。
- en: 'The systolic array architecture achieves computational efficiency through synchronized
    data movement across a structured grid of processing elements. Systolic arrays
    organize computation around four fundamental components:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 系统阵列架构通过在结构化处理单元网格上同步数据移动来实现计算效率。系统阵列围绕以下四个基本组件组织计算：
- en: '**Control Unit**: Coordinates timing and data distribution across the array,
    maintaining synchronized operation throughout the computational grid'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**控制单元**：协调阵列中的时序和数据分布，在整个计算网格中保持同步操作'
- en: 'Data Streams: Input matrices propagate through coordinated pathways—matrix
    A elements traverse horizontally while matrix B elements flow vertically through
    the processing grid'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据流：输入矩阵通过协调的路径传播——矩阵A的元素水平穿越，而矩阵B的元素通过处理网格垂直流动
- en: '**Processing Element Grid**: Individual processing elements execute multiply-accumulate
    operations on streaming data, generating partial results that accumulate toward
    the final computation'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理单元网格**：单个处理单元对流数据执行乘累加运算，生成部分结果，这些结果累积到最终计算中'
- en: '**Output Collection**: Results aggregate at designated output boundaries where
    accumulated partial sums form complete matrix elements'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出收集**：结果在指定的输出边界处汇总，其中累积的局部和形成完整的矩阵元素'
- en: The synchronized data flow ensures that matrix element A[i,k] encounters corresponding
    B[k,j] elements at precise temporal intervals, executing the multiply-accumulate
    operations required for matrix multiplication C[i,j] = Σ A[i,k] × B[k,j]. This
    systematic reuse of operands across multiple processing elements substantially
    reduces memory bandwidth requirements by eliminating redundant data fetches from
    external memory subsystems.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 同步数据流确保矩阵元素A[i,k]在精确的时间间隔内遇到相应的B[k,j]元素，执行矩阵乘法所需的乘累加操作C[i,j] = Σ A[i,k] × B[k,j]。这种在多个处理元素之间系统地重用操作数，通过消除从外部内存子系统中的冗余数据检索，显著降低了内存带宽需求。
- en: Consider the multiplication of 2×2 matrices A and B within a systolic array.
    During the first computational cycle, element A[0,0]=2 propagates horizontally
    while B[0,0]=1 moves vertically, converging at processing element PE(0,0) to execute
    the multiplication 2×1=2\. In the subsequent cycle, the same A[0,0]=2 advances
    to PE(0,1) where it encounters B[0,1]=3, computing 2×3=6\. Concurrently, A[0,1]=4
    enters PE(0,0) to engage with the next B matrix element. This coordinated data
    movement enables systematic operand reuse across multiple computational operations,
    eliminating redundant memory accesses and exemplifying the fundamental efficiency
    principle underlying systolic array architectures.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在心脏阵列内乘以2×2矩阵A和B。在第一个计算周期内，元素A[0,0]=2水平传播，而B[0,0]=1垂直移动，在处理元素PE(0,0)处汇聚以执行乘法2×1=2。在下一个周期，相同的A[0,0]=2移动到PE(0,1)，在那里它遇到B[0,1]=3，计算2×3=6。同时，A[0,1]=4进入PE(0,0)以与下一个B矩阵元素交互。这种协调的数据移动使得在多个计算操作中系统地重用操作数，消除冗余内存访问，并体现了心脏阵列架构的基本效率原则。
- en: '![](../media/file183.svg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file183.svg)'
- en: 'Figure 11.4: **Systolic Array Dataflow**: Processing elements within the array
    execute matrix operations by streaming data in a pipelined manner, maximizing
    operand reuse and minimizing memory access compared to traditional memory-compute
    architectures. This spatial and temporal locality enables efficient parallel computation,
    as exemplified by the multiply-accumulate units in Google’s tpuv4.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：**心脏阵列数据流**：数组内的处理元素通过流水线方式流式传输数据执行矩阵操作，与传统的内存-计算架构相比，最大程度地重用操作数并最小化内存访问。这种空间和时间局部性使得高效并行计算成为可能，如谷歌tpuv4中的乘累加单元所示。
- en: 'Each processing element in the array performs a multiply-accumulate operation
    in every cycle:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 数组中的每个处理元素在每个周期内执行乘累加操作：
- en: Receives an input activation from above
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从上方接收输入激活
- en: Receives a weight value from the left
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧接收权重值
- en: Multiplies these values and adds to its running sum
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些值相乘并加到其运行总和上
- en: Passes the input activation downward and the weight value rightward to neighboring
    elements
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入激活向下传递，将权重值向右传递到相邻元素
- en: This structured computation model minimizes data movement between global memory
    and processing elements, improving both efficiency and scalability. As systolic
    arrays operate in a streaming fashion, they are particularly effective for high-throughput
    workloads such as deep learning training and inference.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构化计算模型最小化了全局内存和处理元素之间的数据移动，提高了效率和可扩展性。由于心脏阵列以流式操作，因此它们特别适用于高吞吐量工作负载，如深度学习训练和推理。
- en: While the diagram in [Figure 11.4](ch017.xhtml#fig-systolic-array) illustrates
    one common systolic array implementation, systolic architectures vary significantly
    across different accelerator designs. Training-focused architectures like Google’s
    TPU employ large arrays optimized for high computational throughput, while inference-oriented
    designs found in edge devices prioritize energy efficiency with smaller configurations.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[图11.4](ch017.xhtml#fig-systolic-array)中的图示说明了常见的心脏阵列实现方式，但心脏架构在不同加速器设计中差异很大。以训练为重点的架构，如谷歌的TPU，采用大阵列，优化了高计算吞吐量，而边缘设备中发现的推理设计则优先考虑能源效率，采用较小的配置。
- en: 'The fundamental principle remains consistent: data flows systematically through
    processing elements, with inputs moving horizontally and vertically to compute
    partial sums in a synchronized fashion. However, as detailed in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    practical effectiveness is ultimately constrained by memory bandwidth bottlenecks.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 基本原则保持一致：数据系统地通过处理元素流动，输入在水平和垂直方向上移动以同步计算部分和。然而，如[第11.4.1节](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9)中详细说明的，实际的有效性最终受到内存带宽瓶颈的限制。
- en: A 128×128 systolic array capable of 16,384 operations per cycle requires continuous
    data feed to maintain utilization—each cycle demands fresh input activations and
    weight parameters that must traverse from off-chip memory through on-chip buffers
    to the array edges. The TPU’s 1,200 GB/s on-chip bandwidth enables high utilization,
    but even this substantial bandwidth becomes limiting when processing large transformer
    models where memory requirements exceed on-chip capacity.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够每周期执行16,384次操作的128×128收缩阵列需要持续的数据输入来维持利用率——每个周期都需要新鲜输入激活和权重参数，这些参数必须从片外内存通过片上缓冲区传输到阵列边缘。TPU的1,200
    GB/s片上带宽使得利用率很高，但当处理内存需求超过片上容量的大型变压器模型时，即使这个巨大的带宽也会变得有限。
- en: Recall from [Chapter 10](ch016.xhtml#sec-model-optimizations) that quantization
    reduces model memory footprint by converting FP32 weights to INT8 representations—this
    optimization directly addresses the memory bandwidth constraints identified here.
    Converting 32-bit floating-point weights to 8-bit integers reduces memory traffic
    by 4×, transforming bandwidth-bound operations into compute-bound workloads where
    systolic arrays can achieve higher utilization. Similarly, structured pruning
    removes entire rows or columns of weight matrices, reducing both the data volume
    that must traverse memory hierarchies and the computation required. These algorithmic
    optimizations prove valuable precisely because they target the memory bottleneck
    that limits accelerator performance in practice.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 回想[第10章](ch016.xhtml#sec-model-optimizations)，量化通过将FP32权重转换为INT8表示来减少模型内存占用——这种优化直接针对此处确定的内存带宽限制。将32位浮点权重转换为8位整数可以将内存流量减少4倍，将带宽受限的操作转换为计算受限的工作负载，其中收缩阵列可以实现更高的利用率。同样，结构化剪枝移除权重矩阵的整个行或列，减少了必须穿越内存层次结构的数据量以及所需的计算。这些算法优化之所以有价值，正是因为它们针对限制实际加速器性能的内存瓶颈。
- en: Numerics in AI Acceleration
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工智能加速中的数值计算
- en: The efficiency of AI accelerators is not determined by computational power alone
    but also by the precision of numerical representations. The choice of numerical
    format shapes the balance between accuracy, throughput, and energy consumption,
    influencing how different execution units, such as SIMD and SIMT units, tensor
    cores, and systolic arrays, are designed and deployed.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速器的效率不仅取决于计算能力，还取决于数值表示的精度。数值格式的选择决定了精度、吞吐量和能耗之间的平衡，影响着不同的执行单元，如SIMD和SIMT单元、张量核心和收缩阵列的设计和部署。
- en: Precision Trade-offs
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 精度权衡
- en: Numerical precision represents a critical design parameter in modern AI accelerators.
    While higher precision formats provide mathematical stability and accuracy, they
    come with substantial costs in terms of power consumption, memory bandwidth, and
    computational throughput. Finding the optimal precision point has become a central
    challenge in AI hardware architecture.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 数值精度是现代人工智能加速器中的一个关键设计参数。虽然更高的精度格式提供了数学上的稳定性和准确性，但它们在功耗、内存带宽和计算吞吐量方面带来了巨大的成本。找到最佳精度点已成为人工智能硬件架构中的一个核心挑战。
- en: Early deep learning models primarily relied on single-precision floating point
    (FP32) for both training and inference. While FP32 offers sufficient dynamic range
    and precision for stable learning, it imposes high computational and memory costs,
    limiting efficiency, especially as model sizes increase. Over time, hardware architectures
    evolved to support lower precision formats such as half-precision floating point
    (FP16) and bfloat16 (BF16), which reduce memory usage and increase computational
    throughput while maintaining sufficient accuracy for deep learning tasks. More
    recently, integer formats (INT8, INT4) have gained prominence in inference workloads,
    where small numerical representations significantly improve energy efficiency
    without compromising model accuracy beyond acceptable limits.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 早期深度学习模型主要依赖于单精度浮点数（FP32）进行训练和推理。虽然FP32提供了足够的动态范围和精度以支持稳定的学习，但它带来了高计算和内存成本，限制了效率，尤其是在模型规模增加时。随着时间的推移，硬件架构发展以支持更低精度的格式，如半精度浮点数（FP16）和bfloat16（BF16），这些格式可以减少内存使用并提高计算吞吐量，同时保持深度学习任务所需的足够精度。最近，整数格式（INT8、INT4）在推理工作负载中越来越受欢迎，其中小的数值表示可以显著提高能效，同时不会超过可接受的模型精度极限。
- en: The transition from high-precision to lower-precision formats is deeply integrated
    into hardware execution models. As detailed in [Section 11.3.4.2](ch017.xhtml#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd),
    SIMD and SIMT units provide flexible support for multiple precisions. Tensor cores
    ([Section 11.3.4.3](ch017.xhtml#sec-ai-acceleration-tensor-cores-771f)) accelerate
    computation using reduced-precision arithmetic, while systolic arrays ([Section 11.3.4.5](ch017.xhtml#sec-ai-acceleration-systolic-arrays-6fa8))
    optimize performance by minimizing memory bandwidth constraints through low-precision
    formats that maximize operand reuse.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 从高精度到低精度格式的过渡已经深深集成到硬件执行模型中。正如[第11.3.4.2节](ch017.xhtml#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd)中详细所述，SIMD和SIMT单元为多种精度提供了灵活的支持。张量核心([第11.3.4.3节](ch017.xhtml#sec-ai-acceleration-tensor-cores-771f))通过使用降低精度的算术来加速计算，而脉动阵列([第11.3.4.5节](ch017.xhtml#sec-ai-acceleration-systolic-arrays-6fa8))通过最小化内存带宽约束来优化性能，通过使用最大化操作数重用的低精度格式。
- en: Despite the advantages of reduced precision, deep learning models cannot always
    rely solely on low-bit representations. To address this challenge, modern AI accelerators
    implement mixed-precision computing, where different numerical formats are used
    at different stages of execution. These precision choices have important implications
    for model fairness and reliability. For example, matrix multiplications may be
    performed in FP16 or BF16, while accumulations are maintained in FP32 to prevent
    precision loss. Similarly, inference engines leverage INT8 arithmetic while preserving
    key activations in higher precision when necessary.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管降低精度具有优势，但深度学习模型并不总是可以完全依赖低比特表示。为了应对这一挑战，现代AI加速器实现了混合精度计算，在执行的不同阶段使用不同的数值格式。这些精度选择对模型的公平性和可靠性有重要影响。例如，矩阵乘法可能在FP16或BF16中执行，而累加则保持在FP32以防止精度损失。同样，推理引擎在必要时利用INT8算术，同时保留关键激活在更高精度中。
- en: Mixed-Precision Computing
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混合精度计算
- en: Modern AI accelerators increasingly support mixed-precision execution, allowing
    different numerical formats to be used at various stages of computation. Training
    workloads often leverage FP16 or BF16 for matrix multiplications, while maintaining
    FP32 accumulations to preserve precision. Inference workloads, by contrast, optimize
    for INT8 or even INT4, achieving high efficiency while retaining acceptable accuracy.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI加速器越来越多地支持混合精度执行，允许在计算的各个阶段使用不同的数值格式。训练工作负载通常利用FP16或BF16进行矩阵乘法，同时保持FP32的累加以保持精度。相比之下，推理工作负载优化为INT8甚至INT4，在保持可接受精度的同时实现高效率。
- en: This shift toward precision diversity is evident in the evolution of AI hardware.
    Early architectures such as NVIDIA Volta provided limited support for lower precision
    beyond FP16, whereas later architectures, including Turing and Ampere, expanded
    the range of supported formats. Ampere GPUs introduced TF32 as a hybrid between
    FP32 and FP16, alongside broader support for BF16, INT8, and INT4\. [Table 11.5](ch017.xhtml#tbl-nvidia-numerics)
    illustrates this trend.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向精度多样性的转变在人工智能硬件的演变中表现得非常明显。早期架构，如 NVIDIA Volta，对低于 FP16 的低精度支持有限，而后续架构，包括
    Turing 和 Ampere，扩展了支持的格式范围。Ampere GPU 引入了 TF32 作为 FP32 和 FP16 之间的混合格式，同时更广泛地支持
    BF16、INT8 和 INT4。 [表11.5](ch017.xhtml#tbl-nvidia-numerics) 展示了这一趋势。
- en: 'Table 11.5: **Precision Support Evolution**: GPU architectures progressively
    expanded support for lower-precision data types, enabling performance gains and
    efficiency improvements in AI workloads. Early architectures primarily utilized
    FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate
    both training and inference tasks.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.5：**精度支持演变**：GPU 架构逐步扩展了对低精度数据类型的支持，使人工智能工作负载在性能提升和效率改进方面受益。早期架构主要使用 FP32，而后续几代架构则纳入了
    FP16、BF16、INT8 和 INT4，以加速训练和推理任务。
- en: '| **Architecture** | **Year** | **Supported Tensor Core Precisions** | **Supported
    CUDA Core Precisions** |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **年份** | **支持的 Tensor Core 精度** | **支持的 CUDA Core 精度** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Volta** | 2017 | FP16 | FP64, FP32, FP16 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **Volta** | 2017 | FP16 | FP64, FP32, FP16 |'
- en: '| **Turing** | 2018 | FP16, INT8 | FP64, FP32, FP16, INT8 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **Turing** | 2018 | FP16, INT8 | FP64, FP32, FP16, INT8 |'
- en: '| **Ampere** | 2020 | FP64, TF32, bfloat16, FP16, INT8, INT4 | FP64, FP32,
    FP16, bfloat16, INT8 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **Ampere** | 2020 | FP64, TF32, bfloat16, FP16, INT8, INT4 | FP64, FP32,
    FP16, bfloat16, INT8 |'
- en: '[Table 11.5](ch017.xhtml#tbl-nvidia-numerics) highlights how newer architectures
    incorporate a growing diversity of numerical formats, reflecting the need for
    greater flexibility across different AI workloads. This trend suggests that future
    AI accelerators will continue expanding support for adaptive precision, optimizing
    both computational efficiency and model accuracy.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.5](ch017.xhtml#tbl-nvidia-numerics) 突出了如何将更多样化的数值格式纳入到新的架构中，反映了在不同人工智能工作负载中需要更大的灵活性。这一趋势表明，未来的人工智能加速器将继续扩展对自适应精度的支持，优化计算效率和模型精度。'
- en: The precision format used in hardware design has far-reaching implications.
    By adopting lower-precision formats, the data transferred between execution units
    and memory is reduced, leading to decreased memory bandwidth requirements and
    storage. Tensor cores and systolic arrays can process more lower-precision elements
    in parallel, thereby increasing the effective throughput in terms of FLOPs. Energy
    efficiency is also improved, as integer-based computations (e.g., INT8) require
    lower power compared to floating-point arithmetic—a clear advantage for inference
    workloads.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件设计中使用的精度格式具有深远的影响。通过采用低精度格式，执行单元和内存之间的数据传输减少，导致内存带宽要求和存储需求降低。Tensor 核心和收缩阵列可以并行处理更多低精度元素，从而在
    FLOPs 方面提高有效吞吐量。由于基于整数的计算（例如，INT8）与浮点运算相比需要更低的功耗，因此能效也得到了提高——这对于推理工作负载来说是一个明显的优势。
- en: As AI models continue to scale in size, accelerator architectures are evolving
    to support more efficient numerical formats. Future designs are expected to incorporate
    adaptive precision techniques, dynamically adjusting computation precision based
    on workload characteristics. This evolution promises further optimization of deep
    learning performance while striking an optimal balance between accuracy and energy
    efficiency.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能模型规模的持续扩大，加速器架构正在演变以支持更高效的数值格式。未来的设计预计将采用自适应精度技术，根据工作负载特性动态调整计算精度。这种演变有望进一步优化深度学习性能，同时在精度和能效之间达到最佳平衡。
- en: Architectural Integration
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建筑集成
- en: The organization of computational primitives into execution units determines
    the efficiency of AI accelerators. While SIMD, tensor cores, and systolic arrays
    serve as fundamental building blocks, their integration into full-chip architectures
    varies significantly across different AI processors. The choice of execution units,
    their numerical precision support, and their connectivity impact how effectively
    hardware can scale for deep learning workloads.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 计算原语组织到执行单元中的方式决定了AI加速器的效率。虽然SIMD、tensor核心和波前阵列是基本构建块，但它们在不同AI处理器中集成到全芯片架构中的方式差异很大。执行单元的选择、它们支持的数值精度以及它们的连接性影响硬件如何有效地扩展以适应深度学习工作负载。
- en: Modern AI processors exhibit a range of design trade-offs based on their intended
    applications. Some architectures, such as NVIDIA’s A100, integrate large numbers
    of tensor cores optimized for FP16-based training, while Google’s TPUv4 prioritizes
    high-throughput BF16 matrix multiplications. Inference-focused processors, such
    as Intel’s Sapphire Rapids, incorporate INT8-optimized tensor cores to maximize
    efficiency. The Apple M1, designed for mobile workloads, employs smaller processing
    elements optimized for low-power FP16 execution. These design choices reflect
    the growing flexibility in numerical precision and execution unit organization,
    as discussed in the previous section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI处理器根据其预期应用表现出各种设计权衡。一些架构，如NVIDIA的A100，集成了大量针对FP16训练优化的tensor核心，而Google的TPUv4优先考虑高吞吐量的BF16矩阵乘法。以推理为重点的处理器，如Intel的Sapphire
    Rapids，集成了INT8优化的tensor核心以最大化效率。专为移动工作负载设计的Apple M1采用较小的处理单元，这些处理单元针对低功耗FP16执行进行了优化。这些设计选择反映了数值精度和执行单元组织方面的日益灵活性，正如前文所述。
- en: '[Table 11.6](ch017.xhtml#tbl-execution-units) summarizes the execution unit
    configurations across contemporary AI processors.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.6](ch017.xhtml#tbl-execution-units)总结了当代AI处理器中的执行单元配置。'
- en: 'Table 11.6: **AI Processor Configurations**: Modern AI processors prioritize
    different execution unit characteristics to optimize performance for specific
    workloads; NVIDIA A100 leverages wide SIMD and tensor cores for training, Google
    TPUv4 emphasizes high-throughput BF16 matrix multiplication, and Intel Sapphire
    Rapids focuses on INT8-optimized inference, while apple M1 prioritizes low-power
    FP16 execution on smaller processing elements. These variations in SIMD width,
    tensor core size, and processing element count reflect the growing diversity in
    AI hardware architectures and their targeted applications.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.6：**AI处理器配置**：现代AI处理器优先考虑不同的执行单元特性，以优化特定工作负载的性能；NVIDIA A100利用宽SIMD和tensor核心进行训练，Google
    TPUv4强调高吞吐量的BF16矩阵乘法，而Intel Sapphire Rapids专注于INT8优化的推理，而apple M1优先考虑低功耗FP16在较小的处理单元上的执行。这些在SIMD宽度、tensor核心大小和处理单元数量上的变化反映了AI硬件架构及其目标应用的日益多样化。
- en: '| **Processor** | **SIMD Width** | **Tensor Core Size** | **Processing Elements**
    | **Primary Workloads** |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **处理器** | **SIMD宽度** | **Tensor核心大小** | **处理单元** | **主要工作负载** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **NVIDIA A100** | 1024-bit | <semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">4\times4\times4</annotation></semantics> FP16 | 108
    SMs | Training, HPC |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| **NVIDIA A100** | 1024位 | <semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">4\times4\times4</annotation></semantics> FP16 | 108个SM
    | 训练、HPC |'
- en: '| **Google TPUv4** | 128-wide | <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics> BF16 | 2 cores/chip
    | Training |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **Google TPUv4** | 128位宽 | <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics> BF16 | 每芯片2核心
    | 训练 |'
- en: '| **Intel Sapphire** | 512-bit AVX | <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times32</annotation></semantics> INT8/BF16 | 56
    cores | Inference |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **Intel Sapphire** | 512位AVX | <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times32</annotation></semantics> INT8/BF16 | 56核心
    | 推理 |'
- en: '| **Apple M1** | 128-bit NEON | <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics> FP16 | 8 NPU
    cores | Mobile inference |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **Apple M1** | 128位NEON | <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics> FP16 | 8个NPU核心
    | 移动推理 |'
- en: '[Table 11.6](ch017.xhtml#tbl-execution-units) highlights how execution unit
    configurations vary across architectures to optimize for different deep learning
    workloads. Training accelerators prioritize high-throughput floating-point tensor
    operations, whereas inference processors focus on low-precision integer execution
    for efficiency. Meanwhile, mobile accelerators balance precision and power efficiency
    to meet real-time constraints.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.6](ch017.xhtml#tbl-execution-units)突出了执行单元配置在不同架构中的差异，以优化不同的深度学习工作负载。训练加速器优先考虑高吞吐量的浮点张量运算，而推理处理器则专注于低精度整数执行以提高效率。同时，移动加速器在精度和功耗效率之间取得平衡，以满足实时约束。'
- en: Cost-Performance Analysis
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本效益分析
- en: While architectural specifications define computational potential, practical
    deployment decisions require understanding cost-performance trade-offs across
    different accelerator options. However, raw computational metrics alone provide
    an incomplete picture—the fundamental constraint in modern AI acceleration is
    not compute capacity but data movement efficiency.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然架构规范定义了计算潜力，但实际的部署决策需要理解不同加速器选项之间的成本效益权衡。然而，仅凭原始的计算指标并不能提供一个完整的画面——现代AI加速的基本约束不是计算能力，而是数据移动效率。
- en: The energy differential established earlier—where memory access costs dominate
    computation—drives the entire specialized hardware revolution. This disparity
    explains why GPUs with high memory bandwidth achieve 40-60% utilization, while
    TPUs with systolic arrays achieve 85% utilization by minimizing data movement.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 之前建立的能量差异——其中内存访问成本主导计算——推动了整个专用硬件革命。这种差异解释了为什么具有高内存带宽的GPU实现了40-60%的利用率，而具有阵列的TPU通过最小化数据移动实现了85%的利用率。
- en: '[Table 11.7](ch017.xhtml#tbl-accelerator-economics) provides concrete cost-performance
    data for representative accelerators, but the economic analysis must account for
    utilization efficiency and energy consumption patterns that determine real-world
    performance.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.7](ch017.xhtml#tbl-accelerator-economics)提供了代表性加速器的具体成本效益数据，但经济分析必须考虑利用率效率和能源消耗模式，这些因素决定了实际性能。'
- en: 'Table 11.7: **Accelerator Cost-Performance Comparison**: Hardware costs must
    be evaluated against computational capabilities to determine optimal deployment
    strategies. While newer accelerators like H100 offer better price-performance
    ratios, total cost of ownership includes power consumption, cooling requirements,
    and infrastructure costs that significantly impact operational economics. *TPU
    pricing estimated from cloud rates.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.7：**加速器成本效益比较**：硬件成本必须与计算能力相权衡，以确定最佳部署策略。虽然像H100这样的新型加速器提供了更好的性价比，但总拥有成本包括功耗、冷却需求和基础设施成本，这些因素会显著影响运营经济性。*TPU定价基于云服务费率。
- en: '| **Accelerator** | **List Price (USD)** | **Peak FLOPS (FP16)** | **Memory
    Bandwidth** | **Price/Performance** |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| **加速器** | **零售价（美元）** | **峰值FLOPS（FP16）** | **内存带宽** | **价格/性能** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **NVIDIA V100** | ~$9,000 (2017-19) | 125 TFLOPS | 900 GB/s | $72/TFLOP |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| **NVIDIA V100** | ~$9,000 (2017-19) | 125 TFLOPS | 900 GB/s | $72/TFLOP |'
- en: '| **NVIDIA A100** | $15,000 | 312 TFLOPS (FP16) | 1,935 GB/s | $48/TFLOP |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| **NVIDIA A100** | $15,000 | 312 TFLOPS (FP16) | 1,935 GB/s | $48/TFLOP |'
- en: '| **NVIDIA H100** | $25,000-30,000 | 756 TFLOPS (TF32) | 3,350 GB/s | $33/TFLOP
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| **NVIDIA H100** | $25,000-30,000 | 756 TFLOPS (TF32) | 3,350 GB/s | $33/TFLOP
    |'
- en: '| **Google TPUv4** | ~$8,000* | 275 TFLOPS (BF16) | 1,200 GB/s | $29/TFLOP
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **Google TPUv4** | ~$8,000* | 275 TFLOPS (BF16) | 1,200 GB/s | $29/TFLOP
    |'
- en: '| **Intel H100** | $12,000 | 200 TFLOPS (INT8) | 800 GB/s | $60/TFLOP |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **Intel H100** | $12,000 | 200 TFLOPS (INT8) | 800 GB/s | $60/TFLOP |'
- en: A startup training large language models faces the choice between 8 V100s ($72K)
    providing 1,000 TFLOPS or 4 A100s ($60K) delivering 1,248 TFLOPS. However, performance
    analysis reveals the true performance story—transformer training with its arithmetic
    intensity of 0.5-2 FLOPS/byte makes both configurations memory-bandwidth bound
    rather than compute-bound. The A100’s 1,935 GB/s bandwidth delivers 2.15× higher
    sustainable performance than V100’s 900 GB/s, making the effective performance
    gain 115% rather than the 25% suggested by peak FLOPS. When combined with 17%
    lower hardware cost and 30% better energy efficiency (400 W vs 300 W per effective
    TFLOP), the A100 configuration provides compelling economic advantages that compound
    over multi-year deployments.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一家训练大型语言模型的初创公司面临着在 8 个 V100（$72K）提供 1,000 TFLOPS 或 4 个 A100（$60K）提供 1,248 TFLOPS
    之间的选择。然而，性能分析揭示了真正的性能故事——具有 0.5-2 FLOPS/byte 的算术强度的变压器训练使得两种配置都受内存带宽限制，而不是计算限制。A100
    的 1,935 GB/s 带宽比 V100 的 900 GB/s 提供了 2.15 倍的可持续性能，使得有效性能提升达到 115%，而不是峰值 FLOPS
    所暗示的 25%。当与 17% 低的硬件成本和 30% 的更好的能效（每有效 TFLOP 400 W 对比 300 W）相结合时，A100 配置在多年部署中提供了令人信服的经济优势。
- en: These cost dynamics explain the rapid adoption of newer accelerators despite
    higher unit prices. The H100’s $33/TFLOP represents a 54% improvement over V100’s
    $72/TFLOP, but more importantly, its 3,350 GB/s bandwidth enables 3.7× higher
    memory throughput per dollar—the metric that determines real-world transformer
    performance. Cloud deployment further complicates the analysis, as providers typically
    charge $2-4/hour for high-end accelerators, making the break-even point between
    purchase and rental highly dependent on utilization patterns and energy costs
    that can account for 60-70% of total operational expenses over a three-year lifecycle.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成本动态解释了为何尽管单价更高，但新加速器的采用速度仍然很快。H100 的 $33/TFLOP 比 V100 的 $72/TFLOP 提高了 54%，但更重要的是，其
    3,350 GB/s 的带宽使得每美元的内存吞吐量提高了 3.7 倍——这是决定现实世界变压器性能的指标。云部署进一步复杂化了分析，因为提供商通常对高端加速器收取
    $2-4/小时 的费用，这使得购买和租赁的盈亏平衡点高度依赖于利用模式和能源成本，这些成本在三年生命周期内可能占运营总成本的 60-70%。
- en: Framework selection significantly impacts these economic decisions—detailed
    hardware-framework optimization strategies are covered in [Chapter 7](ch013.xhtml#sec-ai-frameworks),
    while performance evaluation methodologies are discussed in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 框架选择对经济决策的影响很大——详细的硬件-框架优化策略在[第7章](ch013.xhtml#sec-ai-frameworks)中介绍，而性能评估方法在第[12章](ch018.xhtml#sec-benchmarking-ai)中讨论。
- en: While execution units define the compute potential of an accelerator, their
    effectiveness is fundamentally constrained by data movement and memory hierarchy.
    Achieving high utilization of compute resources requires efficient memory systems
    that minimize data transfer overhead and optimize locality. Understanding these
    constraints reveals why memory architecture becomes as critical as computational
    design in AI acceleration.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然执行单元定义了加速器的计算潜力，但它们的有效性从根本上受到数据移动和内存层次结构的限制。实现计算资源的充分利用需要高效的内存系统，以最小化数据传输开销并优化局部性。了解这些限制揭示了为什么内存架构在
    AI 加速中变得与计算设计一样关键。
- en: AI Memory Systems
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 存储系统
- en: 'The execution units examined in previous sections—SIMD units, tensor cores,
    and systolic arrays—provide impressive computational throughput: modern accelerators
    achieve 100 to 1000 TFLOPS for neural network operations. Yet these theoretical
    capabilities remain unrealized in practice when memory subsystems cannot supply
    data at sufficient rates. This fundamental constraint, termed the AI memory wall,
    represents the dominant bottleneck in real-world accelerator performance.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中检查的执行单元——SIMD 单元、张量核心和收缩阵列——提供了令人印象深刻的计算吞吐量：现代加速器在神经网络操作中实现了 100 到 1000
    TFLOPS。然而，当内存子系统无法以足够的速率提供数据时，这些理论能力在实践中仍然无法实现。这种基本约束，称为 AI 内存墙，代表了现实世界加速器性能中的主要瓶颈。
- en: Unlike conventional workloads, ML models require frequent access to large volumes
    of parameters, activations, and intermediate results, leading to substantial memory
    bandwidth demands—a challenge that intersects with the data management strategies
    covered in [Chapter 6](ch012.xhtml#sec-data-engineering). Modern AI hardware addresses
    these challenges through advanced memory hierarchies, efficient data movement
    techniques, and compression strategies that promote efficient execution and improved
    AI acceleration.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统工作负载不同，机器学习模型需要频繁访问大量参数、激活和中间结果，导致对内存带宽的大量需求——这是一个与第6章（ch012.xhtml#sec-data-engineering）中介绍的数据管理策略相交的挑战。现代人工智能硬件通过高级内存层次结构、高效的数据移动技术和压缩策略来应对这些挑战，以促进高效的执行和改进的人工智能加速。
- en: This section examines memory system design through four interconnected perspectives.
    First, we quantify the growing disparity between computational throughput and
    memory bandwidth, revealing why the AI memory wall represents the dominant performance
    constraint in modern accelerators. Second, we explore how memory hierarchies balance
    competing demands for speed, capacity, and energy efficiency through carefully
    structured tiers from on-chip SRAM to off-chip DRAM. Third, we analyze communication
    patterns between host systems and accelerators, exposing transfer bottlenecks
    that limit end-to-end performance. Finally, we examine how different neural network
    architectures—multilayer perceptrons, convolutional networks, and transformers—create
    distinct memory pressure patterns that inform hardware design decisions and optimization
    strategies.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过四个相互关联的视角来考察内存系统设计。首先，我们量化了计算吞吐量和内存带宽之间不断扩大的差异，揭示了为什么人工智能内存墙代表了现代加速器中占主导地位的性能约束。其次，我们探讨了内存层次结构如何通过精心构建的层级（从片上SRAM到片外DRAM）来平衡速度、容量和能效之间的竞争需求。第三，我们分析了主机系统和加速器之间的通信模式，揭示了限制端到端性能的传输瓶颈。最后，我们考察了不同的神经网络架构——多层感知器、卷积网络和变换器——如何创建不同的内存压力模式，这些模式为硬件设计决策和优化策略提供了信息。
- en: Understanding the AI Memory Wall
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解人工智能内存墙
- en: The AI memory wall represents the fundamental bottleneck constraining modern
    accelerator performance—the growing disparity between computational throughput
    and memory bandwidth that prevents accelerators from achieving their theoretical
    capabilities. While compute units can execute millions of operations per second
    through specialized primitives like vector operations and matrix multiplications,
    they depend entirely on memory systems to supply the continuous stream of weights,
    activations, and intermediate results these operations require.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能内存墙代表了限制现代加速器性能的基本瓶颈——计算吞吐量和内存带宽之间不断扩大的差异，这阻止了加速器达到其理论能力。虽然计算单元可以通过向量操作和矩阵乘法等专用原语每秒执行数百万次操作，但它们完全依赖于内存系统来提供这些操作所需的连续流权重、激活和中间结果。
- en: Quantifying the Compute-Memory Performance Gap
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化计算-内存性能差距
- en: 'The severity of this constraint becomes apparent when examining scaling trends.
    Over the past 20 years, peak computational capabilities have scaled at 3.0× every
    two years, while DRAM bandwidth has grown at only 1.6× during the same period
    ([Gholami et al. 2024](ch058.xhtml#ref-gholami2024ai)). This divergence creates
    an exponentially widening gap where accelerators possess massive computational
    power but cannot access data quickly enough to utilize it. Modern hardware exemplifies
    this imbalance: an NVIDIA H100 delivers 989 TFLOPS but only 3.35 TB/s memory bandwidth
    ([Choquette 2023a](ch058.xhtml#ref-nvidia2022h100)), requiring 295 operations
    per byte to achieve full utilization—far exceeding the 1-10 operations per byte
    typical in neural networks.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察扩展趋势时，这种约束的严重性变得显而易见。在过去20年中，峰值计算能力每两年增长3.0倍，而在此期间DRAM带宽仅增长1.6倍（[Gholami等人2024](ch058.xhtml#ref-gholami2024ai)）。这种差异导致了一个指数级扩大的差距，其中加速器拥有巨大的计算能力，但无法快速访问数据以充分利用它。现代硬件体现了这种不平衡：NVIDIA
    H100提供989 TFLOPS的计算能力，但只有3.35 TB/s的内存带宽（[Choquette 2023a](ch058.xhtml#ref-nvidia2022h100)），需要每字节295次操作才能实现完全利用——远超过神经网络中典型的每字节1-10次操作。
- en: The memory wall manifests through three critical constraints. First, the energy
    disparity—accessing DRAM consumes 640 pJ compared to 3.7 pJ for computation ([Horowitz
    2014](ch058.xhtml#ref-Horowitz2014)), creating a 173× energy penalty that often
    limits performance due to power budgets rather than computational capacity. Second,
    the bandwidth limitation—even TB/s memory systems cannot feed thousands of parallel
    compute units continuously, forcing 50-70% idle time in typical workloads. Third,
    the latency hierarchy—off-chip memory access requires hundreds of cycles, creating
    pipeline stalls that cascade through parallel execution units.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 存储墙通过三个关键约束体现出来。首先，能量差异——访问DRAM消耗640 pJ，而计算仅需3.7 pJ（[Horowitz 2014](ch058.xhtml#ref-Horowitz2014)），这造成了173倍的能量惩罚，通常由于功耗预算而非计算能力而限制了性能。其次，带宽限制——即使是TB/s的内存系统也无法持续不断地为数千个并行计算单元提供数据，在典型的工作负载中导致50-70%的空闲时间。第三，延迟层次——片外内存访问需要数百个周期，这会在并行执行单元中产生流水线停滞。
- en: As illustrated in [Figure 11.5](ch017.xhtml#fig-compute-memory-imbalance), this
    “AI Memory Wall” continues to widen, making memory bandwidth rather than compute
    the primary constraint in AI acceleration.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图11.5](ch017.xhtml#fig-compute-memory-imbalance)所示，这种“AI存储墙”持续扩大，使得内存带宽而不是计算成为AI加速的主要约束。
- en: '![](../media/file184.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file184.png)'
- en: 'Figure 11.5: **AI Memory Wall**: The growing disparity between compute performance
    and memory bandwidth emphasizes the increasing challenge in sustaining peak computational
    efficiency due to memory constraints. Over the past 20 years, while computational
    capabilities have advanced rapidly, memory bandwidth has not kept pace, leading
    to potential bottlenecks in data-intensive applications.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：**AI存储墙**：计算性能与内存带宽之间不断扩大的差距强调了由于内存限制而维持峰值计算效率的挑战日益增加。在过去20年里，尽管计算能力迅速发展，但内存带宽并没有跟上，导致数据密集型应用中可能出现瓶颈。
- en: Beyond performance limitations, memory access imposes a significant energy cost.
    Fetching data from off-chip DRAM consumes far more energy than performing arithmetic
    operations ([Horowitz 2014](ch058.xhtml#ref-Horowitz2014)). This inefficiency
    is particularly evident in machine learning models, where large parameter sizes,
    frequent memory accesses, and non-uniform data movement patterns exacerbate memory
    bottlenecks. The energy differential drives architectural decisions—Google’s TPU
    achieves 30-83<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    better energy efficiency than contemporary GPUs by minimizing data movement through
    systolic arrays and large on-chip memory. These design choices demonstrate that
    energy constraints, not computational limits, often determine practical deployment
    feasibility.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能限制之外，内存访问还带来了显著的能量成本。从片外DRAM中获取数据比执行算术运算消耗的能量要多得多（[Horowitz 2014](ch058.xhtml#ref-Horowitz2014)）。这种低效性在机器学习模型中尤为明显，其中大参数尺寸、频繁的内存访问和非均匀的数据移动模式加剧了内存瓶颈。能量差异驱动了架构决策——谷歌的TPU通过最小化通过收缩阵列和大型片上内存的数据移动，实现了比当代GPU高30-83<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的能量效率。这些设计选择表明，能量约束而非计算限制通常决定了实际部署的可行性。
- en: Memory Access Patterns in ML Workloads
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机器学习工作负载中的内存访问模式
- en: Machine learning workloads place substantial demands on memory systems due to
    the large volume of data involved in computation. Unlike traditional compute-bound
    applications, where performance is often dictated by the speed of arithmetic operations,
    ML workloads are characterized by high data movement requirements. The efficiency
    of an accelerator is not solely determined by its computational throughput but
    also by its ability to continuously supply data to processing units without introducing
    stalls or delays.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算中涉及的大量数据，机器学习工作负载对内存系统提出了巨大的需求。与性能通常由算术运算速度决定的传统计算密集型应用不同，ML工作负载的特点是高数据移动需求。加速器的效率不仅取决于其计算吞吐量，还取决于其连续向处理单元提供数据的能力，而不引入停滞或延迟。
- en: 'A neural network processes multiple types of data throughout its execution,
    each with distinct memory access patterns:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在其执行过程中处理多种类型的数据，每种数据都有独特的内存访问模式：
- en: '**Model parameters (weights and biases)**: Machine learning models, particularly
    those used in large-scale applications such as natural language processing and
    computer vision, often contain millions to billions of parameters. Storing and
    accessing these weights efficiently is essential for maintaining throughput.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数（权重和偏置）**：机器学习模型，尤其是在自然语言处理和计算机视觉等大规模应用中使用的模型，通常包含数百万到数十亿个参数。高效地存储和访问这些权重对于保持吞吐量至关重要。'
- en: '**Intermediate activations**: During both training and inference, each layer
    produces intermediate results that must be temporarily stored and retrieved for
    subsequent operations. These activations can contribute significantly to memory
    overhead, particularly in deep architectures.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中间激活值**：在训练和推理过程中，每一层都会产生中间结果，这些结果必须临时存储和检索以供后续操作使用。这些激活值可以显著增加内存开销，尤其是在深度架构中。'
- en: '**Gradients (during training)**: Backpropagation requires storing and accessing
    gradients for every parameter, further increasing the volume of data movement
    between compute units and memory.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度（训练期间）**：反向传播需要存储和访问每个参数的梯度，这进一步增加了计算单元和内存之间数据移动的量。'
- en: As models increase in size and complexity, improvements in memory capacity and
    bandwidth become essential. Although specialized compute units accelerate operations
    like matrix multiplications, their overall performance depends on the continuous,
    efficient delivery of data to the processing elements. In large-scale applications,
    such as natural language processing and computer vision, models often incorporate
    millions to billions of parameters ([T. B. Brown, Mann, Ryder, Subbiah, Kaplan,
    Dhariwal, Neelakantan, Shyam, Sastry, et al. 2020](ch058.xhtml#ref-Brown2020)).
    Consequently, achieving high performance necessitates minimizing delays and stalls
    caused by inefficient data movement between memory and compute units ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021); [Xingyu 2019](ch058.xhtml#ref-Huang2019)).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模和复杂性的增加，内存容量和带宽的改进变得至关重要。尽管专门的计算单元可以加速矩阵乘法等操作，但它们的整体性能取决于数据持续、高效地传递到处理单元。在自然语言处理和计算机视觉等大规模应用中，模型通常包含数百万到数十亿个参数（[T.
    B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry,
    等人，2020](ch058.xhtml#ref-Brown2020)）。因此，实现高性能需要最小化由内存和计算单元之间不高效的数据移动引起的延迟和停滞（[D.
    Narayanan 等人，2021a](ch058.xhtml#ref-Narayanan2021)；[Xingyu，2019](ch058.xhtml#ref-Huang2019)）。
- en: One way to quantify this challenge is by comparing the data transfer time with
    the time required for computations. Specifically, we define the memory transfer
    time as <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">mem</mtext></msub><mo>=</mo><mfrac><msub><mi>M</mi><mtext
    mathvariant="normal">total</mtext></msub><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub></mfrac><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},</annotation></semantics>
    where <semantics><msub><mi>M</mi><mtext mathvariant="normal">total</mtext></msub><annotation
    encoding="application/x-tex">M_{\text{total}}</annotation></semantics> is the
    total data volume and <semantics><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{mem}}</annotation></semantics> is the available
    memory bandwidth. In contrast, the compute time is given by <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">compute</mtext></msub><mo>=</mo><mfrac><mtext mathvariant="normal">FLOPs</mtext><msub><mi>P</mi><mtext
    mathvariant="normal">peak</mtext></msub></mfrac><mo>,</mo></mrow> <annotation
    encoding="application/x-tex">T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},</annotation></semantics>
    with the number of floating-point operations (FLOPs) divided by the peak hardware
    throughput, <semantics><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub><annotation
    encoding="application/x-tex">P_{\text{peak}}</annotation></semantics>. When <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">mem</mtext></msub><mo>></mo><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub></mrow><annotation
    encoding="application/x-tex">T_{\text{mem}} > T_{\text{compute}}</annotation></semantics>,
    the system becomes memory-bound, meaning that the processing elements spend more
    time waiting for data than performing computations. This imbalance demonstrates
    the need for memory-optimized architectures and efficient data movement strategies
    to sustain high performance.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 量化这一挑战的一种方法是将数据传输时间与计算所需时间进行比较。具体来说，我们定义内存传输时间为 <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">mem</mtext></msub><mo>=</mo><mfrac><msub><mi>M</mi><mtext
    mathvariant="normal">total</mtext></msub><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub></mfrac><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},</annotation></semantics>
    其中 <semantics><msub><mi>M</mi><mtext mathvariant="normal">total</mtext></msub><annotation
    encoding="application/x-tex">M_{\text{total}}</annotation></semantics> 是总数据量，而
    <semantics><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{mem}}</annotation></semantics> 是可用的内存带宽。相比之下，计算时间由
    <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub><mo>=</mo><mfrac><mtext
    mathvariant="normal">FLOPs</mtext><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub></mfrac><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},</annotation></semantics>
    给出，其中浮点运算次数（FLOPs）除以峰值硬件吞吐量，<semantics><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub><annotation
    encoding="application/x-tex">P_{\text{peak}}</annotation></semantics>。当 <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">mem</mtext></msub><mo>></mo><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub></mrow><annotation
    encoding="application/x-tex">T_{\text{mem}} > T_{\text{compute}}</annotation></semantics>
    时，系统变为内存受限，这意味着处理元素花费更多的时间等待数据而不是执行计算。这种不平衡表明了需要内存优化的架构和高效的数据移动策略来维持高性能。
- en: '[Figure 11.6](ch017.xhtml#fig-memory-wall) demonstrates the emerging challenge
    between model growth and hardware memory capabilities, illustrating the “AI Memory
    Wall.” The figure tracks AI model sizes (red dots) and hardware memory bandwidth
    (blue dots) over time on a log scale. Model parameters have grown exponentially,
    from AlexNet’s ~62.3M parameters in 2012 to Gemini 1’s trillion-scale parameters
    in 2023, as shown by the steeper red trend line. In contrast, hardware memory
    bandwidth, represented by successive generations of NVIDIA GPUs (~100-200 GB/s)
    and Google TPUs (~2-3 TB/s), has increased more gradually (blue trend line). The
    expanding shaded region between these trends corresponds to the “AI Memory Wall,”
    which will be an architectural challenge where model scaling outpaces available
    memory bandwidth. This growing disparity necessitates increasingly sophisticated
    memory management and model optimization techniques to maintain computational
    efficiency.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.6](ch017.xhtml#fig-memory-wall) 展示了模型增长与硬件内存能力之间的新兴挑战，阐述了“AI内存墙”。该图以对数尺度跟踪了AI模型大小（红色点）和硬件内存带宽（蓝色点）随时间的变化。模型参数呈指数增长，从2012年AlexNet的约6230万个参数到2023年Gemini
    1的万亿级参数，如更陡峭的红色趋势线所示。相比之下，硬件内存带宽，由NVIDIA GPU的连续几代（约100-200 GB/s）和Google TPUs（约2-3
    TB/s）代表，增长更为缓慢（蓝色趋势线）。这些趋势线之间的扩展阴影区域对应于“AI内存墙”，这将是一个架构挑战，其中模型扩展速度超过可用的内存带宽。这种不断扩大的差异需要越来越复杂的内存管理和模型优化技术，以保持计算效率。'
- en: Irregular Memory Access
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不规则内存访问
- en: Unlike traditional computing workloads, where memory access follows well-structured
    and predictable patterns, machine learning models often exhibit irregular memory
    access behaviors that make efficient data retrieval a challenge. These irregularities
    arise due to the nature of ML computations, where memory access patterns are influenced
    by factors such as batch size, layer type, and sparsity. As a result, standard
    caching mechanisms and memory hierarchies often struggle to optimize performance,
    leading to increased memory latency and inefficient bandwidth utilization.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 与遵循结构化和可预测模式的传统计算工作负载不同，机器学习模型通常表现出不规则内存访问行为，这使得高效的数据检索成为挑战。这些不规则性是由于机器学习计算的本质，其中内存访问模式受批量大小、层类型和稀疏性等因素的影响。因此，标准的缓存机制和内存层次结构往往难以优化性能，导致内存延迟增加和带宽利用率低下。
- en: '![](../media/file185.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file185.png)'
- en: 'Figure 11.6: **AI Memory Wall**: The figure emphasizes the growing disparity
    between model sizes and hardware memory bandwidths, illustrating the challenge
    in sustaining performance as models become more complex.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：**AI内存墙**：该图强调了模型大小与硬件内存带宽之间的不断扩大的差异，说明了随着模型变得更加复杂，维持性能的挑战。
- en: To better understand how ML workloads differ from traditional computing workloads,
    it is useful to compare their respective memory access patterns ([Table 11.8](ch017.xhtml#tbl-traditional-vs-ml-mem)).
    Traditional workloads, such as scientific computing, general-purpose CPU applications,
    and database processing, typically exhibit well-defined memory access characteristics
    that benefit from standard caching and prefetching techniques. ML workloads, on
    the other hand, introduce highly dynamic access patterns that challenge conventional
    memory optimization strategies.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解机器学习工作负载与传统计算工作负载的不同，比较它们各自的内存访问模式（[表11.8](ch017.xhtml#tbl-traditional-vs-ml-mem)）是有用的。传统的任务，如科学计算、通用CPU应用程序和数据库处理，通常表现出定义良好的内存访问特征，这些特征受益于标准的缓存和预取技术。另一方面，机器学习工作负载引入了高度动态的访问模式，这对传统的内存优化策略构成了挑战。
- en: One key source of irregularity in ML workloads stems from batch size and execution
    order. The way input data is processed in batches directly affects memory reuse,
    creating a complex optimization challenge. Small batch sizes decrease the likelihood
    of reusing cached activations and weights, resulting in frequent memory fetches
    from slower, off-chip memory. Larger batch sizes can improve reuse and amortize
    memory access costs, but simultaneously place higher demands on available memory
    bandwidth, potentially creating congestion at different memory hierarchy levels.
    This delicate balance requires careful consideration of model architecture and
    available hardware resources.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载中不规则性的一个关键来源是批量大小和执行顺序。输入数据在批量中的处理方式直接影响内存重用，从而产生复杂的优化挑战。小批量大小降低了重用缓存激活和权重的可能性，导致频繁地从较慢的片外内存中获取内存。较大的批量大小可以提高重用并分摊内存访问成本，但同时也对可用的内存带宽提出了更高的要求，可能在不同的内存层次结构级别上造成拥塞。这种微妙的平衡需要仔细考虑模型架构和可用的硬件资源。
- en: 'Table 11.8: **Memory Access Characteristics**: Traditional workloads exhibit
    predictable, sequential memory access, benefiting from standard caching, while
    machine learning workloads introduce irregular and dynamic patterns due to sparsity
    and data dependencies that challenge conventional memory optimization techniques.
    Understanding these differences is crucial for designing memory systems that efficiently
    support the unique demands of modern AI applications.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.8：**内存访问特性**：传统工作负载表现出可预测的、顺序的内存访问，得益于标准缓存，而机器学习工作负载由于稀疏性和数据依赖性引入了不规则和动态的模式，这挑战了传统的内存优化技术。理解这些差异对于设计能够高效支持现代人工智能应用独特需求的内存系统至关重要。
- en: '| **Feature** | **Traditional Computing Workloads** | **Machine Learning Workloads**
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **传统计算工作负载** | **机器学习工作负载** |'
- en: '| --- | --- | --- |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Memory Access Pattern** | Regular and predictable (e.g., sequential reads,
    structured patterns) | Irregular and dynamic (e.g., sparsity, attention mechanisms)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| **内存访问模式** | 规则且可预测的（例如，顺序读取，结构化模式） | 不规则且动态的（例如，稀疏性，注意力机制） |'
- en: '| **Cache Locality** | High temporal and spatial locality | Often low locality,
    especially in large models |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| **缓存局部性** | 高时间局部性和空间局部性 | 通常局部性较低，尤其是在大型模型中 |'
- en: '| **Data Reuse** | Structured loops with frequent data reuse | Sparse and dynamic
    reuse depending on layer type |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| **数据重用** | 具有频繁数据重用的结构化循环 | 依赖于层类型的稀疏和动态重用 |'
- en: '| **Data Dependencies** | Well-defined dependencies allow efficient prefetching
    | Variable dependencies based on network structure |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| **数据依赖性** | 明确的依赖性允许高效的预取 | 基于网络结构的可变依赖性 |'
- en: '| **Workload Example** | Scientific computing (e.g., matrix factorizations,
    physics simulations) | Neural networks (e.g., CNNs, Transformers, sparse models)
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载示例** | 科学计算（例如，矩阵分解，物理模拟） | 神经网络（例如，CNN，Transformer，稀疏模型） |'
- en: '| **Memory Bottleneck** | DRAM latency, cache misses | Off-chip bandwidth constraints,
    memory fragmentation |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **内存瓶颈** | DRAM延迟，缓存未命中 | 片外带宽限制，内存碎片化 |'
- en: '| **Impact on Energy Consumption** | Moderate, driven by FLOP-heavy execution
    | High, dominated by data movement costs |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **对能耗的影响** | 中等，由密集的浮点运算执行驱动 | 高，主要由数据移动成本主导 |'
- en: Different neural network layers interact with memory in distinct ways beyond
    batch size considerations. Convolutional layers benefit from spatial locality,
    as neighboring pixels in an image are processed together, enabling efficient caching
    of small weight kernels. Conversely, fully connected layers require frequent access
    to large weight matrices, often leading to more randomized memory access patterns
    that poorly align with standard caching policies. Transformers introduce additional
    complexity, as attention mechanisms demand accessing large key-value pairs stored
    across varied memory locations. The dynamic nature of sequence length and attention
    span renders traditional prefetching strategies ineffective, resulting in unpredictable
    memory latencies.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的神经网络层在考虑批量大小之外，以不同的方式与内存交互。卷积层得益于空间局部性，因为图像中的相邻像素一起处理，从而能够高效地缓存小的权重内核。相反，全连接层需要频繁访问大型权重矩阵，这通常会导致与标准缓存策略不匹配的更多随机内存访问模式。Transformer
    引入了额外的复杂性，因为注意力机制需要访问存储在各个内存位置的大型键值对。序列长度和注意力范围的动态性质使得传统的预取策略无效，导致不可预测的内存延迟。
- en: Another significant factor contributing to irregular memory access is sparsity[25](#fn25)
    in neural networks. Many modern ML models employ techniques such as weight pruning,
    activation sparsity, and structured sparsity to reduce computational overhead.
    However, these optimizations often lead to non-uniform memory access, as sparse
    representations necessitate fetching scattered elements rather than sequential
    blocks, making hardware caching less effective. Models that incorporate dynamic
    computation paths, such as Mixture of Experts and Adaptive Computation Time, introduce
    highly non-deterministic memory access patterns, where the active neurons or model
    components can vary with each inference step. This variability challenges efficient
    prefetching and caching strategies.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个导致不规则内存访问的显著因素是神经网络中的稀疏性[25](#fn25)。许多现代机器学习模型采用权重剪枝、激活稀疏性和结构化稀疏性等技术来减少计算开销。然而，这些优化通常导致非均匀的内存访问，因为稀疏表示需要检索分散的元素而不是顺序块，这使得硬件缓存效率降低。包含动态计算路径的模型，如专家混合和自适应计算时间，引入了高度非确定性的内存访问模式，其中活动的神经元或模型组件可以随着每个推理步骤而变化。这种可变性对有效的预取和缓存策略构成了挑战。
- en: These irregularities have significant consequences. ML workloads often experience
    reduced cache efficiency, as activations and weights may not be accessed in predictable
    sequences. This leads to increased reliance on off-chip memory traffic, which
    slows down execution and consumes more energy. Irregular access patterns contribute
    to memory fragmentation, where the way data is allocated and retrieved results
    in inefficient utilization of available memory resources. The combined effect
    is that ML accelerators frequently encounter memory bottlenecks that limit their
    ability to fully utilize available compute power.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不规则性具有重大影响。机器学习工作负载通常经历缓存效率降低，因为激活和权重可能不会以可预测的顺序访问。这导致对片外内存流量的依赖性增加，从而减慢执行速度并消耗更多能量。不规则访问模式导致内存碎片化，其中数据分配和检索的方式导致可用内存资源的低效利用。综合影响是，机器学习加速器经常遇到内存瓶颈，限制了它们充分利用可用计算能力的能力。
- en: Memory Hierarchy
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存层次结构
- en: To address the memory challenges in ML acceleration, hardware designers implement
    sophisticated memory hierarchies that balance speed, capacity, and energy efficiency.
    Understanding this hierarchy is essential before examining how different ML architectures
    utilize memory resources. Unlike general-purpose computing, where memory access
    patterns are often unpredictable, ML workloads exhibit structured reuse patterns
    that can be optimized through careful organization of data across multiple memory
    levels.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决机器学习加速中的内存挑战，硬件设计师实施了复杂的内存层次结构，以平衡速度、容量和能源效率。在检查不同机器学习架构如何利用内存资源之前，理解这一层次结构是至关重要的。与通用计算不同，其中内存访问模式通常是不可预测的，机器学习工作负载表现出结构化的重用模式，这些模式可以通过在多个内存级别上仔细组织数据来优化。
- en: At the highest level, large-capacity but slow storage devices provide long-term
    model storage. At the lowest level, high-speed registers and caches ensure that
    compute units can access operands with minimal latency. Between these extremes,
    intermediate memory levels, such as scratchpad memory, high-bandwidth memory,
    and off-chip DRAM, offer trade-offs between performance and capacity.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高级别，大容量但速度较慢的存储设备提供长期模型存储。在最低级别，高速寄存器和缓存确保计算单元可以以最小的延迟访问操作数。在这两个极端之间，中间内存级别，如暂存内存、高带宽内存和片外DRAM，在性能和容量之间提供权衡。
- en: '[Table 11.9](ch017.xhtml#tbl-memory-heirarchy) summarizes the key characteristics
    of different memory levels in modern AI accelerators. Each level in the hierarchy
    has distinct latency, bandwidth, and capacity properties, which directly influence
    how neural network data, such as weights, activations, and intermediate results,
    should be allocated.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.9](ch017.xhtml#tbl-memory-heirarchy)总结了现代人工智能加速器中不同内存级别的关键特性。层次结构中的每一级都具有独特的延迟、带宽和容量属性，这些属性直接影响神经网络数据，如权重、激活和中间结果，应该如何分配。'
- en: 'Table 11.9: **Memory Hierarchy Trade-Offs**: AI accelerators leverage a multi-level
    memory hierarchy to balance performance and capacity, optimizing data access for
    computationally intensive machine learning tasks. Each level provides distinct
    latency, bandwidth, and capacity characteristics that dictate how neural network
    components—weights, activations, and intermediate results—should be strategically
    allocated to minimize bottlenecks and maximize throughput.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.9：**内存层次结构权衡**：AI加速器利用多级内存层次结构来平衡性能和容量，优化计算密集型机器学习任务的数据访问。每一层提供独特的延迟、带宽和容量特性，决定了神经网络组件（权重、激活和中间结果）应该如何战略性地分配，以最小化瓶颈并最大化吞吐量。
- en: '| **Memory Level** | **Approx. Latency** | **Bandwidth** | **Capacity** | **Example
    Use in Deep Learning** |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| **Memory Level** | **Approx. Latency** | **Bandwidth** | **Capacity** | **Example
    Use in Deep Learning** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Registers** | ~1 cycle | Highest | Few values | Storing operands for immediate
    computation |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **Registers** | ~1 cycle | Highest | Few values | 存储即时计算的操作数 |'
- en: '| **L1/L2 Cache (SRAM)** | ~1-10 ns | High | KBs-MBs | Caching frequently accessed
    activations and small weight blocks |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| **L1/L2 Cache (SRAM)** | ~1-10 ns | High | KBs-MBs | 缓存频繁访问的激活和小型权重块 |'
- en: '| **Scratchpad Memory** | ~5-20 ns | High | MBs | Software-managed storage
    for intermediate computations |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **Scratchpad Memory** | ~5-20 ns | High | MBs | 软件管理的中间计算存储 |'
- en: '| **High-Bandwidth Memory (HBM)** | ~100 ns | Very High | GBs | Storing large
    model parameters and activations for high-speed access |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **High-Bandwidth Memory (HBM)** | ~100 ns | Very High | GBs | 存储大型模型参数和激活，以实现高速访问
    |'
- en: '| **Off-Chip DRAM (DDR, GDDR, LPDDR)** | ~50-150 ns | Moderate | GBs-TBs |
    Storing entire model weights that do not fit on-chip |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| **Off-Chip DRAM (DDR, GDDR, LPDDR)** | ~50-150 ns | Moderate | GBs-TBs |
    存储整个模型权重，这些权重无法存储在芯片上 |'
- en: '| **Flash Storage (SSD/NVMe)** | ~100 µs - 1 ms | Low | TBs | Storing pre-trained
    models and checkpoints for later loading |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| **Flash Storage (SSD/NVMe)** | ~100 µs - 1 ms | Low | TBs | 存储预训练模型和检查点以供后续加载
    |'
- en: On-Chip Memory
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: On-Chip Memory
- en: Each level of the memory hierarchy serves a distinct role in AI acceleration,
    with different trade-offs in speed, capacity, and accessibility. Registers, located
    within compute cores, provide the fastest access but can only store a few operands
    at a time. These are best utilized for immediate computations, where the operands
    needed for an operation can be loaded and consumed within a few cycles. However,
    because register storage is so limited, frequent memory accesses are required
    to fetch new operands and store intermediate results.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构的每一层在AI加速中都扮演着独特的角色，在速度、容量和可访问性方面有不同的权衡。位于计算核心内的寄存器提供最快的访问，但一次只能存储几个操作数。这些寄存器最适合用于即时计算，其中操作数可以在几个周期内加载和消耗。然而，由于寄存器存储非常有限，需要频繁的内存访问来获取新的操作数并存储中间结果。
- en: To reduce the need for constant data movement between registers and external
    memory, small but fast caches serve as an intermediary buffer. These caches store
    recently accessed activations, weights, and intermediate values, ensuring that
    frequently used data remains available with minimal delay. However, the size of
    caches is limited, making them insufficient for storing full feature maps or large
    weight tensors in machine learning models. As a result, only the most frequently
    used portions of a model’s parameters or activations can reside here at any given
    time.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少寄存器和外部内存之间不断的数据移动需求，小型但快速的缓存作为中介缓冲区。这些缓存存储最近访问的激活、权重和中间值，确保频繁使用的数据以最小的延迟保持可用。然而，缓存的容量有限，使得它们不足以存储完整的特征图或大型权重张量在机器学习模型中。因此，在任何给定时间，模型参数或激活的最常用部分只能存储在这里。
- en: 'For larger working datasets, many AI accelerators include scratchpad memory,
    which offers more storage than caches but with a crucial difference: it allows
    explicit software control over what data is stored and when it is evicted. Unlike
    caches, which rely on hardware-based eviction policies, scratchpad memory enables
    machine learning workloads to retain key values such as activations and filter
    weights for multiple layers of computation. This capability is particularly useful
    in models like convolutional neural networks, where the same input feature maps
    and filter weights are reused across multiple operations. By keeping this data
    in scratchpad memory rather than reloading it from external memory, accelerators
    can significantly reduce unnecessary memory transfers and improve overall efficiency
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的工作数据集，许多AI加速器包括临时存储器，它提供的存储空间比缓存更多，但有一个关键的区别：它允许显式软件控制存储的数据以及何时将其移除。与依赖于基于硬件的淘汰策略的缓存不同，临时存储器使机器学习工作负载能够保留关键值，如激活和多个计算层的滤波器权重。这种能力在卷积神经网络等模型中特别有用，在这些模型中，相同的输入特征图和滤波器权重在多个操作中被重复使用。通过将数据保留在临时存储器中而不是从外部内存重新加载，加速器可以显著减少不必要的内存传输并提高整体效率
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016))。
- en: Off-Chip Memory
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 芯片外存储器
- en: Beyond on-chip memory, high-bandwidth memory provides rapid access to larger
    model parameters and activations that do not fit within caches or scratchpad buffers.
    HBM achieves its high performance by stacking multiple memory dies and using wide
    memory interfaces, allowing it to transfer large amounts of data with minimal
    latency compared to traditional DRAM. Because of its high bandwidth and lower
    latency, HBM is often used to store entire layers of machine learning models that
    must be accessed quickly during execution. However, its cost and power consumption
    limit its use primarily to high-performance AI accelerators, making it less common
    in power-constrained environments such as edge devices.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 除了片上内存之外，高带宽内存提供了对不适合缓存或临时缓冲区的大模型参数和激活的快速访问。HBM通过堆叠多个内存芯片和采用宽内存接口来实现其高性能，与传统的DRAM相比，它以最小的延迟传输大量数据。由于其高带宽和低延迟，HBM通常用于存储在执行期间必须快速访问的整个机器学习模型层。然而，其成本和功耗限制了其在高性能AI加速器中的使用，使其在边缘设备等功耗受限的环境中不太常见。
- en: When a machine learning model exceeds the capacity of on-chip memory and HBM,
    it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While DRAM offers
    significantly greater storage capacity, its access latency is higher, meaning
    that frequent retrievals from DRAM can introduce execution bottlenecks. To make
    effective use of DRAM, models must be structured so that only the necessary portions
    of weights and activations are retrieved at any given time, minimizing the impact
    of long memory fetch times.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习模型超出片上内存和HBM的容量时，它必须依赖于芯片外DRAM，如DDR、GDDR或LPDDR。虽然DRAM提供了显著更大的存储容量，但其访问延迟更高，这意味着频繁从DRAM中检索数据可能会引入执行瓶颈。为了有效地使用DRAM，模型必须构建成在任何给定时间只检索必要的权重和激活部分，以最小化长时间内存检索时间的影响。
- en: At the highest level of the hierarchy, flash storage and solid-state drives
    (SSDs) store large pre-trained models, datasets, and checkpointed weights. These
    storage devices offer large capacities but are too slow for real-time execution,
    requiring models to be loaded into faster memory tiers before computation begins.
    For instance, in training scenarios, checkpointed models stored in SSDs must be
    loaded into DRAM or HBM before resuming computation, as direct execution from
    SSDs would be too slow to maintain efficient accelerator utilization ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021)).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次结构的最高级别，闪存存储和固态硬盘（SSD）存储大型预训练模型、数据集和检查点权重。这些存储设备提供了大容量，但速度太慢，无法进行实时执行，需要在计算开始之前将模型加载到更快的内存层级。例如，在训练场景中，存储在SSD中的检查点模型必须在恢复计算之前加载到DRAM或HBM中，因为直接从SSD执行会太慢，无法维持高效的加速器利用率
    ([D. Narayanan et al. 2021a](ch058.xhtml#ref-Narayanan2021))。
- en: The memory hierarchy balances competing objectives of speed, capacity, and energy
    efficiency. However, moving data through multiple memory levels introduces bottlenecks
    that limit accelerator performance. Data transfers between memory levels incur
    latency costs, particularly for off-chip accesses. Limited bandwidth restricts
    data flow between memory tiers. Memory capacity constraints force constant data
    movement as models exceed local storage. These constraints make memory bandwidth
    the fundamental determinant of real-world accelerator performance.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 内存层次结构平衡了速度、容量和能源效率之间的竞争目标。然而，通过多个内存级别移动数据引入了瓶颈，限制了加速器的性能。内存级别之间的数据传输产生延迟成本，尤其是对于片外访问。有限的带宽限制了内存层级之间的数据流动。内存容量限制迫使模型超过本地存储时不断移动数据。这些限制使内存带宽成为现实世界加速器性能的基本决定因素。
- en: Memory Bandwidth and Architectural Trade-offs
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存带宽和架构权衡
- en: Building on the memory wall analysis established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    this section quantifies how specific bandwidth characteristics impact system performance
    across different deployment scenarios.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11.4.1节](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9)中建立的内存墙分析的基础上，本节量化了特定带宽特性如何影响不同部署场景下的系统性能。
- en: 'Modern accelerators exhibit distinct bandwidth-capacity trade-offs: NVIDIA
    H100 GPUs provide 3.35 TB/s HBM3 bandwidth with 80 GB capacity, optimizing for
    flexibility across diverse workloads. Google’s TPUv4 delivers 1.2 TB/s bandwidth
    with 128 MB on-chip memory, prioritizing energy efficiency for tensor operations.
    This 3:1 bandwidth advantage enables H100 to handle memory-intensive models like
    large language models, while TPU’s lower bandwidth suffices for compute-intensive
    inference due to superior data reuse.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现代加速器表现出不同的带宽-容量权衡：NVIDIA H100 GPU提供3.35 TB/s HBM3带宽和80 GB容量，优化了跨不同工作负载的灵活性。谷歌的TPUv4提供1.2
    TB/s带宽和128 MB片上内存，优先考虑张量操作的能量效率。这个3:1的带宽优势使H100能够处理内存密集型模型，如大型语言模型，而TPU的较低带宽足以处理计算密集型推理，因为数据重用性更高。
- en: 'Different neural network operations achieve varying bandwidth utilization:
    transformer attention mechanisms achieve only 20-40% of peak memory bandwidth
    due to irregular access patterns, convolutional layers achieve 60-85% through
    predictable spatial access patterns, and fully connected layers approach 90% when
    batch sizes exceed 128.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的神经网络操作实现不同的带宽利用率：由于访问模式不规则，transformer注意力机制仅达到峰值内存带宽的20-40%，卷积层通过可预测的空间访问模式达到60-85%，当批量大小超过128时，全连接层接近90%。
- en: 'As established earlier, on-chip memory access consumes 5-10 pJ per access,
    while external DRAM requires 640 pJ per access—a 65-125<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> energy penalty. AI
    accelerators minimize DRAM access through three key strategies: weight stationarity
    (keeping model parameters in on-chip memory), input stationarity (buffering input
    activations locally), and output stationarity (accumulating partial sums on-chip).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，片上内存访问消耗5-10 pJ每访问，而外部DRAM每访问需要640 pJ——这是一项65-125<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的能量惩罚。AI加速器通过三种关键策略最小化DRAM访问：权重稳定性（保持模型参数在片上内存中）、输入稳定性（局部缓冲输入激活）和输出稳定性（在片上累积部分和）。
- en: 'Memory bandwidth scaling follows different trajectories across accelerator
    designs:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 内存带宽扩展在加速器设计中遵循不同的轨迹：
- en: '**GPU scaling**: Bandwidth increases linearly with memory channels, from 900
    GB/s (A100) to 3,350 GB/s (H100), enabling larger model support'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU扩展**：带宽随着内存通道的增加而线性增长，从900 GB/s（A100）到3,350 GB/s（H100），从而支持更大的模型'
- en: '**TPU scaling**: Bandwidth optimization through systolic array design achieves
    900 GB/s with 35% lower power than GPU alternatives'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TPU扩展**：通过阵列设计优化带宽，实现900 GB/s，比GPU替代方案低35%的功耗'
- en: '**Mobile accelerator scaling**: Apple’s M3 Neural Engine achieves 400 GB/s
    unified memory bandwidth while consuming <5 W through aggressive voltage scaling'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动加速器扩展**：苹果的M3神经网络引擎通过积极的电压扩展实现400 GB/s的统一内存带宽，同时消耗<5 W'
- en: HBM memory costs $8-15 per GB compared to $0.05 per GB for DDR5, creating 160-300<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> cost differences.
    High-bandwidth accelerators require 40-80 GB HBM for competitive performance,
    adding $320-1,200 to manufacturing costs. Edge accelerators sacrifice bandwidth
    (50-200 GB/s) to achieve sub-$100 cost targets while maintaining sufficient performance
    for inference workloads.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 与DDR5每GB 0.05美元相比，HBM内存的成本为每GB 8-15美元，造成160-300<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的成本差异。高性能加速器需要40-80GB
    HBM以实现有竞争力的性能，这增加了320-1,200美元的制造成本。边缘加速器牺牲带宽（50-200 GB/s）以实现低于100美元的成本目标，同时保持足够的性能以满足推理工作负载。
- en: 'These bandwidth characteristics directly influence deployment decisions: cloud
    training prioritizes raw bandwidth for maximum model capacity, edge inference
    optimizes bandwidth efficiency for energy constraints, and mobile deployment balances
    bandwidth with cost limitations. While these hardware-specific optimizations are
    fundamental, the integrated system-level efficiency approaches that combine hardware
    acceleration with software optimization techniques are comprehensively covered
    in [Chapter 9](ch015.xhtml#sec-efficient-ai). The deployment of these optimizations
    across different system contexts—from mobile devices in [Chapter 2](ch008.xhtml#sec-ml-systems)
    to production workflows in [Chapter 13](ch019.xhtml#sec-ml-operations)—determines
    their real-world impact.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这些带宽特性直接影响部署决策：云训练优先考虑原始带宽以实现最大模型容量，边缘推理优化带宽效率以适应能源限制，而移动部署在带宽和成本限制之间取得平衡。虽然这些硬件特定的优化是基本的，但第[9章](ch015.xhtml#sec-efficient-ai)中全面介绍了结合硬件加速和软件优化技术的集成系统级效率方法。这些优化在不同系统环境中的部署——从第[2章](ch008.xhtml#sec-ml-systems)中的移动设备到第[13章](ch019.xhtml#sec-ml-operations)中的生产工作流程——决定了它们的实际影响。
- en: Host-Accelerator Communication
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主机-加速器通信
- en: Machine learning accelerators, such as GPUs and TPUs, achieve high computational
    throughput through parallel execution. However, their efficiency is fundamentally
    constrained by data movement between the host (CPU) and accelerator memory. Unlike
    general-purpose workloads that operate entirely within a CPU’s memory subsystem,
    AI workloads require frequent data transfers between CPU main memory and the accelerator,
    introducing latency, consuming bandwidth, and affecting overall performance.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习加速器，如GPU和TPU，通过并行执行实现高计算吞吐量。然而，它们的效率从根本上受到主机（CPU）和加速器内存之间数据传输的限制。与完全在CPU内存子系统中运行的一般工作负载不同，AI工作负载需要在CPU主内存和加速器之间频繁传输数据，这引入了延迟，消耗了带宽，并影响了整体性能。
- en: Host-accelerator data movement follows a structured sequence, as illustrated
    in [Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement). Before computation
    begins, data is copied from CPU memory to the accelerator’s memory. The CPU then
    issues execution instructions, and the accelerator processes the data in parallel.
    Once computation completes, the results are stored in accelerator memory and transferred
    back to the CPU. Each step introduces potential inefficiencies that must be managed
    to optimize performance.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 主机-加速器数据传输遵循一个结构化的顺序，如图 [图11.7](ch017.xhtml#fig-host-accelerator-data-movement)
    所示。在计算开始之前，数据从CPU内存复制到加速器的内存。然后CPU发出执行指令，加速器并行处理数据。一旦计算完成，结果存储在加速器内存中，并传输回CPU。每一步都可能引入潜在的低效，必须管理这些低效以优化性能。
- en: '![](../media/file186.svg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file186.svg)'
- en: 'Figure 11.7: **Host-Accelerator Data Transfer**: AI workloads require frequent
    data movement between CPU memory and accelerators; this figure details the sequential
    steps of copying input data, executing computation, and transferring results,
    each introducing potential performance bottlenecks. Understanding this data transfer
    sequence is crucial for optimizing AI system performance and minimizing latency.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：**主机-加速器数据传输**：AI工作负载需要在CPU内存和加速器之间频繁移动数据；此图详细说明了复制输入数据、执行计算和传输结果的顺序步骤，每一步都可能引入潜在的性能瓶颈。理解这个数据传输序列对于优化AI系统性能和最小化延迟至关重要。
- en: The key challenges in host-accelerator data movement include latency, bandwidth
    constraints, and synchronization overheads. Optimizing data transfers through
    efficient memory management and interconnect technologies is essential for maximizing
    accelerator utilization.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 主机-加速器数据传输中的关键挑战包括延迟、带宽限制和同步开销。通过高效的内存管理和互连技术优化数据传输对于最大化加速器利用率至关重要。
- en: Data Transfer Patterns
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据传输模式
- en: The efficiency of ML accelerators depends not only on their computational power
    but also on the continuous supply of data. Even high-performance GPUs and TPUs
    remain underutilized if data transfers are inefficient. Host and accelerator memory
    exist as separate domains, requiring explicit transfers over interconnects such
    as PCIe, NVLink, or proprietary links. Ineffective data movement can cause execution
    stalls, making transfer optimization critical.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ML 加速器的效率不仅取决于其计算能力，还取决于数据的持续供应。即使高性能的 GPU 和 TPU，如果数据传输效率低下，也会被低效利用。主机和加速器内存作为独立的域存在，需要通过
    PCIe、NVLink 或专有链路等互连进行显式传输。无效的数据移动可能导致执行停滞，因此传输优化至关重要。
- en: '[Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement) illustrates this
    structured sequence. In step (1), data is copied from CPU memory to accelerator
    memory, as GPUs cannot directly access host memory at high speeds. A direct memory
    access (DMA)[26](#fn26) engine typically handles this transfer without consuming
    CPU cycles. In step (2), the CPU issues execution commands via APIs like CUDA,
    ROCm, or OpenCL. Step (3) involves parallel execution on the accelerator, where
    stalls can occur if data is not available when needed. Finally, in step (4), computed
    results are copied back to CPU memory for further processing.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.7](ch017.xhtml#fig-host-accelerator-data-movement) 展示了这种结构化序列。在步骤（1）中，数据从
    CPU 内存复制到加速器内存，因为 GPU 无法以高速直接访问主机内存。通常，直接内存访问（DMA）[26](#fn26) 引擎处理这种传输而不会消耗 CPU
    周期。在步骤（2）中，CPU 通过 CUDA、ROCm 或 OpenCL 等API发出执行命令。步骤（3）涉及加速器上的并行执行，如果需要数据时数据不可用，可能会发生停滞。最后，在步骤（4）中，计算结果被复制回
    CPU 内存以进行进一步处理。'
- en: Latency and bandwidth limitations significantly impact AI workloads. PCIe, with
    a peak bandwidth of 32 GB/s (PCIe 4.0), is much slower than an accelerator’s high-bandwidth
    memory, which can exceed 1 TB/s. Large data transfers exacerbate bottlenecks,
    particularly in deep learning tasks. Additionally, synchronization overheads arise
    when computation must wait for data transfers to complete. Efficient scheduling
    and overlapping transfers with execution are essential to mitigate these inefficiencies.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟和带宽限制对 AI 工作负载有显著影响。PCIe 的峰值带宽为 32 GB/s（PCIe 4.0），比加速器的高带宽内存慢得多，后者可以超过 1 TB/s。大数据传输加剧了瓶颈，尤其是在深度学习任务中。此外，当计算必须等待数据传输完成时，还会出现同步开销。有效的调度和与执行重叠传输是减轻这些低效的关键。
- en: Data Transfer Mechanisms
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据传输机制
- en: The movement of data between the host (CPU) and the accelerator (GPU, TPU, or
    other AI hardware) depends on the interconnect technology that links the two processing
    units. The choice of interconnect determines the bandwidth available for transfers,
    the latency of communication, and the overall efficiency of host-accelerator execution.
    The most commonly used transfer mechanisms include PCIe (Peripheral Component
    Interconnect Express), NVLink, Direct Memory Access, and Unified Memory Architectures.
    Each of these plays a crucial role in optimizing the four-step data movement process
    illustrated in [Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 主机（CPU）和加速器（GPU、TPU 或其他 AI 硬件）之间的数据移动取决于连接两个处理单元的互连技术。互连的选择决定了传输可用带宽、通信延迟以及主机-加速器执行的整体效率。最常用的传输机制包括
    PCIe（外围组件互连扩展）、NVLink、直接内存访问和统一内存架构。这些机制中的每一个都在优化图 11.7 中所示的四个步骤数据移动过程中发挥着关键作用。
- en: PCIe Interface
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PCIe 接口
- en: Most accelerators communicate with the CPU via PCIe, the industry-standard interconnect
    for data movement. PCIe 4.0 provides up to 32 GB/s bandwidth, while PCIe 5.0 doubles
    this to 64 GB/s. However, this is still significantly lower than HBM bandwidth
    within accelerators, making PCIe a bottleneck for large AI workloads.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数加速器通过 PCIe 与 CPU 通信，PCIe 是数据移动的行业标准互连。PCIe 4.0 提供高达 32 GB/s 的带宽，而 PCIe 5.0
    将此翻倍至 64 GB/s。然而，这仍然远低于加速器内部的 HBM 带宽，使得 PCIe 成为大型 AI 工作负载的瓶颈。
- en: PCIe also introduces latency overheads due to its packet-based communication
    and memory-mapped I/O model. Frequent small transfers are inefficient, so batching
    data movement reduces overhead. Computation commands, issued over PCIe, further
    contribute to latency, requiring careful optimization of execution scheduling.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: PCIe 由于其基于数据包的通信和内存映射 I/O 模型，也引入了延迟开销。频繁的小数据传输效率低下，因此批量数据移动可以减少开销。通过 PCIe 发出的计算命令进一步增加了延迟，需要仔细优化执行调度。
- en: NVLink Interface
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: NVLink 接口
- en: To address the bandwidth limitations of PCIe, NVIDIA developed NVLink, a proprietary
    high-speed interconnect that provides significantly higher bandwidth between GPUs
    and, in some configurations, between the CPU and GPU. Unlike PCIe, which operates
    as a shared bus, NVLink enables direct point-to-point communication between connected
    devices, reducing contention and improving efficiency for AI workloads.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决PCIe的带宽限制，NVIDIA开发了NVLink，这是一种专有高速互连，在GPU之间以及在某些配置中在CPU和GPU之间提供了显著更高的带宽。与作为共享总线的PCIe不同，NVLink允许连接设备之间的直接点对点通信，减少争用并提高AI工作负载的效率。
- en: For host-accelerator transfers, NVLink can be used in step (1) to transfer input
    data from main memory to GPU memory at speeds far exceeding PCIe, with bandwidths
    reaching up to 600 GB/s in NVLink 4.0\. This significantly reduces the data movement
    bottleneck, allowing accelerators to access input data with lower latency. In
    multi-GPU configurations, NVLink also accelerates peer-to-peer transfers, allowing
    accelerators to exchange data without routing through main memory, thereby optimizing
    step (3) of the computation process.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主机-加速器传输，NVLink可以在步骤（1）中使用，以远超PCIe的速度将输入数据从主内存传输到GPU内存，NVLink 4.0的带宽可达600
    GB/s。这显著减少了数据移动瓶颈，允许加速器以更低的延迟访问输入数据。在多GPU配置中，NVLink还加速了端到端传输，允许加速器交换数据而无需通过主内存路由，从而优化计算过程的步骤（3）。
- en: Although NVLink offers substantial performance benefits, it is not universally
    available. Unlike PCIe, which is an industry standard across all accelerators,
    NVLink is specific to NVIDIA hardware, limiting its applicability to systems designed
    with NVLink-enabled GPUs.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NVLink提供了显著的性能优势，但它并非普遍可用。与适用于所有加速器的行业标准PCIe不同，NVLink仅适用于NVIDIA硬件，限制了其在配备NVLink启用GPU的系统中的应用。
- en: DMA for Data Transfers
  id: totrans-421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据传输的DMA
- en: In conventional memory transfers, the CPU issues load/store instructions, consuming
    processing cycles. DMA offloads this task, enabling asynchronous data movement
    without CPU intervention.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的内存传输中，CPU发出加载/存储指令，消耗处理周期。DMA卸载这项任务，允许在不干预CPU的情况下异步移动数据。
- en: During data transfers, the CPU initiates a DMA request, allowing data to be
    copied to accelerator memory in the background. Similarly, result transfers back
    to main memory occur without blocking execution. This enables overlapping computation
    with data movement, reducing idle time and improving accelerator utilization.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据传输过程中，CPU发起DMA请求，允许数据在后台复制到加速器内存。同样，结果传输回主内存不会阻塞执行。这使计算与数据移动重叠，减少空闲时间，提高加速器利用率。
- en: DMA is essential for enabling asynchronous data movement, which allows transfers
    to overlap with computation. Instead of waiting for transfers to complete before
    execution begins, AI workloads can stream data into the accelerator while earlier
    computations are still in progress, reducing idle time and improving accelerator
    utilization.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: DMA对于启用异步数据移动至关重要，这允许传输与计算重叠。AI工作负载不需要在传输完成之前开始执行，可以在早期计算仍在进行时将数据流式传输到加速器，减少空闲时间并提高加速器利用率。
- en: Unified Memory
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统一内存
- en: While PCIe, NVLink, and DMA optimize explicit memory transfers, some AI workloads
    require a more flexible memory model that eliminates the need for manual data
    copying. Unified Memory provides an abstraction that allows both the host and
    accelerator to access a single, shared memory space, automatically handling data
    movement when needed.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCIe、NVLink和DMA优化了显式内存传输，但某些AI工作负载需要更灵活的内存模型，以消除手动数据复制的需求。统一内存提供了一个抽象层，允许主机和加速器访问单个共享内存空间，在需要时自动处理数据移动。
- en: With Unified Memory, data does not need to be explicitly copied between CPU
    and GPU memory before execution. Instead, when a computation requires a memory
    region that is currently located in host memory, the system automatically migrates
    it to the accelerator, handling step (1) transparently. Similarly, when computed
    results are accessed by the CPU, step (4) occurs automatically, eliminating the
    need for manual memory management.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 使用统一内存，在执行前不需要在CPU和GPU内存之间显式复制数据。相反，当计算需要位于主机内存中的内存区域时，系统会自动将其迁移到加速器，透明地处理步骤（1）。同样，当CPU访问计算结果时，步骤（4）会自动发生，消除了手动内存管理的需求。
- en: Although Unified Memory simplifies programming, it introduces performance trade-offs.
    Since memory migrations occur on demand, they can lead to unpredictable latencies,
    particularly if large datasets need to be transferred frequently. Additionally,
    since Unified Memory is implemented through page migration techniques, small memory
    accesses can trigger excessive data movement, further reducing efficiency.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管统一内存简化了编程，但它引入了性能权衡。由于内存迁移是在需要时发生的，这可能导致不可预测的延迟，尤其是当需要频繁传输大数据集时。此外，由于统一内存是通过页面迁移技术实现的，小的内存访问可能会触发过多的数据移动，进一步降低效率。
- en: For AI workloads that require fine-grained memory control, explicit data transfers
    using PCIe, NVLink, and DMA often provide better performance. However, for applications
    where ease of development is more important than absolute speed, Unified Memory
    offers a convenient alternative.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要精细内存控制的人工智能工作负载，使用PCIe、NVLink和DMA进行显式数据传输通常提供更好的性能。然而，对于开发简便性比绝对速度更重要的应用程序，统一内存提供了一个方便的替代方案。
- en: Data Transfer Overheads
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据传输开销
- en: Host-accelerator data movement introduces overheads that impact AI workload
    execution. Unlike on-chip memory accesses, which occur at nanosecond latencies,
    host-accelerator transfers traverse system interconnects, adding latency, bandwidth
    constraints, and synchronization delays.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 主机-加速器数据移动引入了影响人工智能工作负载执行的开销。与在芯片内存访问相比，主机-加速器传输穿越系统互连，增加了延迟、带宽限制和同步延迟。
- en: Interconnect latency affects transfer speed, with PCIe, the standard host-accelerator
    link, incurring significant overhead due to packet-based transactions and memory-mapped
    I/O. This makes frequent small transfers inefficient. Faster alternatives like
    NVLink reduce latency and improve bandwidth but are limited to specific hardware
    ecosystems.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 互连延迟影响传输速度，PCIe，作为标准的主机-加速器链路，由于基于分组的交易和内存映射I/O而承担了显著的开销。这使得频繁的小传输变得低效。更快的替代品如NVLink减少了延迟并提高了带宽，但仅限于特定的硬件生态系统。
- en: Synchronization delays further contribute to inefficiencies. Synchronous transfers
    block execution until data movement completes, ensuring data consistency but introducing
    idle time. Asynchronous transfers allow computation and data movement to overlap,
    reducing stalls but requiring careful coordination to avoid execution mismatches.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 同步延迟进一步加剧了低效。同步传输会阻塞执行，直到数据移动完成，确保数据一致性，但引入了空闲时间。异步传输允许计算和数据移动重叠，减少停滞，但需要仔细协调以避免执行不匹配。
- en: These factors, including interconnect latency, bandwidth limitations, and synchronization
    overheads, determine AI workload efficiency. While optimization techniques mitigate
    these limitations, understanding these fundamental transfer mechanics is essential
    for improving performance.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素，包括互连延迟、带宽限制和同步开销，决定了人工智能工作负载的效率。虽然优化技术减轻了这些限制，但了解这些基本的传输机制对于提高性能至关重要。
- en: Model Memory Pressure
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型内存压力
- en: Machine learning models impose varying memory access patterns that significantly
    influence accelerator performance. The way data is transferred between the host
    and accelerator, how frequently memory is accessed, and the efficiency of caching
    mechanisms all determine overall execution efficiency. While multilayer perceptrons
    (MLPs), convolutional neural networks (CNNs), and transformer networks each require
    large parameter sets, their distinct memory demands necessitate tailored optimization
    strategies for accelerators. Understanding these differences provides insight
    into why different hardware architectures exhibit varying levels of efficiency
    across workloads.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型强加不同的内存访问模式，这显著影响了加速器的性能。数据在主机和加速器之间传输的方式、内存访问的频率以及缓存机制的效率都决定了整体执行效率。虽然多层感知器（MLPs）、卷积神经网络（CNNs）和变换器网络各自需要大量的参数集，但它们不同的内存需求需要为加速器制定定制化的优化策略。了解这些差异有助于理解为什么不同的硬件架构在工作负载中表现出不同的效率水平。
- en: Multilayer Perceptrons
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层感知器
- en: MLPs, also referred to as fully connected networks, are among the simplest neural
    architectures. Each layer consists of a dense matrix multiplication, requiring
    every neuron to interact with all neurons in the preceding layer. This results
    in high memory bandwidth demands, particularly for weights, as every input activation
    contributes to a large set of computations.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs，也称为全连接网络，是神经网络架构中最简单的之一。每一层都由密集矩阵乘法组成，要求每个神经元与前一层的所有神经元进行交互。这导致了高内存带宽需求，尤其是对于权重，因为每个输入激活都会对大量计算有贡献。
- en: From a memory perspective, MLPs rely on large, dense weight matrices that frequently
    exceed on-chip memory capacity, necessitating off-chip memory accesses. Since
    accelerators cannot directly access host memory at high speed, data transfers
    must be explicitly managed via interconnects such as PCIe or NVLink. These transfers
    introduce latency and consume bandwidth, affecting execution efficiency.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 从内存角度来看，MLPs依赖于大型的密集权重矩阵，这些矩阵经常超过片上内存容量，需要片外内存访问。由于加速器不能直接以高速访问主机内存，数据传输必须通过PCIe或NVLink等互连显式管理。这些传输引入了延迟并消耗了带宽，影响了执行效率。
- en: Despite their bandwidth-heavy nature, MLPs exhibit regular and predictable memory
    access patterns, making them amenable to optimizations such as prefetching and
    streaming memory accesses. Dedicated AI accelerators mitigate transfer overhead
    by staging weight matrices in fast SRAM caches and overlapping data movement with
    computation through direct memory access engines, reducing execution stalls. These
    optimizations allow accelerators to sustain high throughput even when handling
    large parameter sets ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MLPs具有带宽密集的特性，但它们表现出规律和可预测的内存访问模式，这使得它们适合进行预取和流式内存访问等优化。专用AI加速器通过在快速SRAM缓存中分阶段存储权重矩阵，并通过直接内存访问引擎重叠数据移动与计算来减轻传输开销，减少执行停滞。这些优化使得加速器即使在处理大型参数集时也能保持高吞吐量
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016))。
- en: Convolutional Neural Networks
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Convolutional Neural Networks
- en: Convolutional Neural Networks (CNNs) are widely used in image processing and
    computer vision tasks. Unlike MLPs, which require dense matrix multiplications,
    CNNs process input feature maps using small filter kernels that slide across the
    image. This localized computation structure results in high spatial data reuse,
    where the same input pixels contribute to multiple convolutions.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）在图像处理和计算机视觉任务中得到了广泛应用。与需要密集矩阵乘法的MLPs不同，CNNs使用小的滤波核在图像上滑动来处理输入特征图。这种局部计算结构导致了高空间数据重用，即相同的输入像素对多个卷积有贡献。
- en: CNN accelerators benefit from on-chip memory optimizations, as convolution filters
    exhibit extensive reuse, allowing weights to be stored in fast local SRAM instead
    of frequently accessing off-chip memory. However, activation maps require careful
    management due to their size. Since accessing main memory over interconnects like
    PCIe introduces latency and bandwidth bottlenecks, CNN accelerators employ tiling
    techniques to divide feature maps into smaller regions that fit within on-chip
    buffers. This minimizes costly external memory transfers, improving overall efficiency
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: CNN加速器受益于片上内存优化，因为卷积滤波器具有广泛的复用性，允许权重存储在快速的片上SRAM中，而不是频繁访问片外内存。然而，由于激活图的大小，它们需要仔细管理。由于通过PCIe等互连访问主内存会引入延迟和带宽瓶颈，CNN加速器采用分块技术将特征图划分为更小的区域，这些区域适合片上缓冲区。这最小化了昂贵的片外内存传输，提高了整体效率
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016))。
- en: While CNN workloads are more memory-efficient than MLPs, managing intermediate
    activations remains a challenge. Accelerators use hierarchical caching strategies
    and DMA engines to optimize memory movement, ensuring that computations are not
    stalled by inefficient host-accelerator data transfers. These memory optimizations
    help CNN accelerators maintain high throughput by reducing reliance on off-chip
    memory bandwidth ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CNN工作负载比MLPs更节省内存，但管理中间激活仍然是一个挑战。加速器使用分层缓存策略和DMA引擎来优化内存移动，确保计算不会被低效的主机-加速器数据传输所阻塞。这些内存优化有助于CNN加速器通过减少对片外内存带宽的依赖来保持高吞吐量
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016))。
- en: Transformer Networks
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Transformer Networks
- en: Transformers have become the dominant architecture for natural language processing
    and are increasingly used in other domains such as vision and speech recognition.
    Unlike CNNs, which rely on local computations, transformers perform global attention
    mechanisms, where each token in an input sequence can interact with all other
    tokens. This leads to irregular and bandwidth-intensive memory access patterns,
    as large key-value matrices must be fetched and updated frequently.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器已成为自然语言处理的主导架构，并在视觉和语音识别等其他领域得到越来越广泛的应用。与依赖于局部计算的CNN不同，变压器执行全局注意力机制，其中输入序列中的每个标记都可以与其他所有标记交互。这导致不规则的、带宽密集型的内存访问模式，因为必须频繁地检索和更新大型键值矩阵。
- en: These models are particularly challenging for accelerators due to their massive
    parameter sizes, which often exceed on-chip memory capacity. As a result, frequent
    memory transfers between host and accelerator introduce substantial latency overheads,
    particularly when relying on interconnects such as PCIe. Unified Memory architectures
    can mitigate some of these issues by dynamically handling data movement, but they
    introduce additional latency due to unpredictable on-demand memory migrations.
    Because transformers are memory-bound rather than compute-bound, accelerators
    optimized for them rely on high-bandwidth memory, tensor tiling, and memory partitioning
    to sustain performance ([T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
    Neelakantan, Shyam, Sastry, et al. 2020](ch058.xhtml#ref-Brown2020)).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型由于其庞大的参数大小，通常超过片上内存容量，对加速器来说尤其具有挑战性。因此，主机和加速器之间频繁的内存传输引入了大量的延迟开销，尤其是在依赖于PCIe等互连的情况下。统一内存架构可以通过动态处理数据移动来缓解这些问题，但它们由于不可预测的按需内存迁移而引入了额外的延迟。由于变压器是内存密集型而非计算密集型，针对它们的优化加速器依赖于高带宽内存、张量分块和内存分区以维持性能（[T.
    B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry,
    等人 2020](ch058.xhtml#ref-Brown2020)）。
- en: Additionally, attention caching mechanisms and specialized tensor layouts reduce
    redundant memory fetches, improving execution efficiency. Given the bandwidth
    limitations of traditional interconnects, NVLink-enabled architectures offer significant
    advantages for large-scale transformer training, as they provide higher throughput
    and lower latency compared to PCIe. DMA-based asynchronous memory transfers enable
    overlapping computation with data movement, reducing execution stalls ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021)).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意缓存机制和专门的张量布局减少了冗余的内存访问，提高了执行效率。鉴于传统互连的带宽限制，NVLink启用的架构在大型变压器训练方面提供了显著优势，因为它们比PCIe提供了更高的吞吐量和更低的延迟。基于DMA的异步内存传输允许在数据移动的同时重叠计算，减少执行停滞（[D.
    Narayanan 等人 2021a](ch058.xhtml#ref-Narayanan2021)）。
- en: ML Accelerators Implications
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML加速器的影响
- en: The diverse memory requirements of MLPs, CNNs, and Transformers highlight the
    need to tailor memory architectures to specific workloads. [Table 11.10](ch017.xhtml#tbl-model-mem-compare)
    compares the memory access patterns across these different models.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: MLP、CNN和Transformer多样化的内存需求突显了根据特定工作负载定制内存架构的必要性。[表11.10](ch017.xhtml#tbl-model-mem-compare)比较了这些不同模型之间的内存访问模式。
- en: 'Table 11.10: **ML Model Memory Access**: Different machine learning models
    exhibit distinct memory access patterns and bottlenecks due to variations in weight
    size, activation reuse, and data sparsity; these characteristics significantly
    impact hardware accelerator design and performance optimization. Transformers
    demand high bandwidth and capacity due to their massive, sparsely accessed weights,
    while cnns benefit from spatial locality and high activation reuse, reducing memory
    pressure.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.10：**机器学习模型内存访问**：不同的机器学习模型由于权重大小、激活重用和数据稀疏度的变化表现出不同的内存访问模式和瓶颈；这些特性对硬件加速器设计和性能优化有重大影响。由于其巨大且稀疏访问的权重，变压器需要高带宽和容量，而CNN则受益于空间局部性和高激活重用，从而降低了内存压力。
- en: '| **Model Type** | **Weight Size** | **Activation Reuse** | **Memory Access
    Pattern** | **Primary Bottleneck** |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **权重大小** | **激活重用** | **内存访问模式** | **主要瓶颈** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **MLP (Dense)** | Large, dense | Low | Regular, sequential (streamed) | Bandwidth
    (off-chip) |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| **MLP (密集)** | 大型，密集 | 低 | 规则的，顺序的（流式） | 带宽（片外） |'
- en: '| **CNN** | Small, reused | High | Spatial locality | Feature map movement
    |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| **CNN** | 小型，可重用 | 高 | 空间局部性 | 特征图移动 |'
- en: '| **Transformer** | Massive, sparse | Low | Irregular, high-bandwidth | Memory
    capacity + Interconnect |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| **Transformer** | 大型、稀疏 | 低 | 不规则、高带宽 | 内存容量 + 互连'
- en: Each model type presents unique challenges that directly impact accelerator
    design. MLPs benefit from fast streaming access to dense weight matrices, making
    memory bandwidth a critical factor in performance, especially when transferring
    large weights from host memory to accelerator memory. CNNs, with their high activation
    reuse and structured memory access patterns, can leverage on-chip caching and
    tiling strategies to minimize off-chip memory transfers. Transformers, however,
    impose significant demands on both bandwidth and capacity, as attention mechanisms
    require frequent access to large key-value matrices, leading to high interconnect
    traffic and increased memory pressure.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 每种模型类型都面临着独特的挑战，这些挑战直接影响加速器设计。MLPs受益于快速流式访问密集权重矩阵，这使得内存带宽成为性能的关键因素，尤其是在将大型权重从主机内存传输到加速器内存时。CNNs由于其高激活重用和结构化内存访问模式，可以利用片上缓存和分块策略来最小化片外内存传输。然而，Transformers对带宽和容量都有显著需求，因为注意力机制需要频繁访问大型键值矩阵，导致高互连流量和增加的内存压力。
- en: To address these challenges, modern AI accelerators incorporate multi-tier memory
    hierarchies that balance speed, capacity, and energy efficiency. On-chip SRAM
    caches and scratchpad memories store frequently accessed data, while high-bandwidth
    external memory provides scalability for large models. Efficient interconnects,
    such as NVLink, help alleviate host-accelerator transfer bottlenecks, particularly
    in transformer workloads where memory movement constraints can dominate execution
    time.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，现代人工智能加速器采用多层内存层次结构，以平衡速度、容量和能源效率。片上SRAM缓存和暂存内存存储频繁访问的数据，而高带宽的外部内存为大型模型提供可扩展性。高效的互连，如NVLink，有助于缓解主机-加速器传输瓶颈，尤其是在Transformer工作负载中，内存移动限制可能会主导执行时间。
- en: 'As ML workloads continue to grow in complexity, memory efficiency becomes as
    critical as raw compute power. The analysis reveals how memory systems dominate
    accelerator performance: the 173× energy penalty for DRAM access creates a fundamental
    bottleneck, carefully structured memory hierarchies can improve effective bandwidth
    by 10-100×, and different neural network architectures create distinct memory
    pressure patterns. These constraints—from bandwidth limitations to communication
    overheads—determine whether theoretical computational capabilities translate into
    real-world performance. Having established how memory systems constrain accelerator
    effectiveness, we now examine how mapping strategies systematically address these
    limitations.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习工作负载的复杂性持续增长，内存效率变得与原始计算能力一样关键。分析揭示了内存系统如何主导加速器的性能：DRAM访问的173倍能量惩罚造成了一个基本瓶颈，精心设计的内存层次结构可以提高有效带宽10-100倍，不同的神经网络架构产生不同的内存压力模式。这些限制——从带宽限制到通信开销——决定了理论计算能力是否转化为实际性能。在确定了内存系统如何限制加速器有效性之后，我们现在考察映射策略如何系统地解决这些限制。
- en: Hardware Mapping Fundamentals for Neural Networks
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络硬件映射基础
- en: The memory system challenges examined in the previous section—bandwidth limitations,
    hierarchical access costs, and model-specific pressure patterns—directly determine
    how effectively neural networks execute on accelerators. A systolic array with
    1,200 GB/s on-chip bandwidth and sophisticated memory hierarchies delivers no
    performance benefit if computations are mapped without considering these memory
    access patterns. As established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    the extreme energy penalty for memory access means that mapping strategies must
    prioritize data reuse and locality above all other considerations. This reality
    drives the need for systematic mapping approaches that coordinate computation
    placement, memory allocation, and data movement to exploit hardware capabilities
    while respecting memory constraints.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中考察的内存系统挑战——带宽限制、分层访问成本和模型特定的压力模式——直接决定了神经网络在加速器上的执行效率。如果一个具有1,200 GB/s片上带宽和复杂内存层次结构的阵列在映射计算时没有考虑这些内存访问模式，那么它将无法带来性能上的好处。如[第11.4.1节](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9)所确立的，内存访问的极端能量惩罚意味着映射策略必须优先考虑数据重用和局部性，而忽略其他所有考虑因素。这一现实推动了系统化映射方法的需求，这些方法协调计算放置、内存分配和数据移动，以利用硬件能力同时尊重内存限制。
- en: Efficient execution of machine learning models on specialized AI acceleration
    hardware requires a structured approach to computation, ensuring that available
    resources are fully utilized while minimizing performance bottlenecks. These mapping
    considerations become particularly critical in distributed training scenarios,
    as explored in [Chapter 8](ch014.xhtml#sec-ai-training). Unlike general-purpose
    processors, which rely on dynamic task scheduling, AI accelerators operate under
    a structured execution model that maximizes throughput by carefully assigning
    computations to processing elements. This process, known as mapping, dictates
    how computations are distributed across hardware resources, influencing execution
    speed, memory access patterns, and overall efficiency.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在专用人工智能加速硬件上高效执行机器学习模型需要一种结构化的计算方法，确保充分利用可用资源，同时最小化性能瓶颈。这些映射考虑在分布式训练场景中尤为重要，如第8章[（ch014.xhtml#sec-ai-training）]所述。与依赖于动态任务调度的通用处理器不同，人工智能加速器在结构化执行模型下运行，通过仔细分配计算到处理元素来最大化吞吐量。这个过程，称为映射，决定了计算如何在硬件资源之间分布，影响执行速度、内存访问模式和整体效率。
- en: '***Mapping in AI Acceleration*** is the process of assigning ML *computations*
    to *hardware units* through *spatial allocation*, *temporal scheduling*, and *memory-aware
    placement* to maximize execution efficiency and resource utilization.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能加速中的映射**是通过空间分配、时间调度和内存感知放置将机器学习*计算*分配给*硬件单元*的过程，以最大化执行效率和资源利用率。'
- en: Mapping machine learning models onto AI accelerators presents several challenges
    due to hardware constraints and the diversity of model architectures. Given the
    hierarchical memory system of modern accelerators, mapping strategies must carefully
    manage when and where data is accessed to minimize latency and power overhead
    while ensuring that compute units remain actively engaged. Poor mapping decisions
    can lead to underutilized compute resources, excessive data movement, and increased
    execution time, ultimately reducing overall efficiency.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件限制和模型架构的多样性，将机器学习模型映射到人工智能加速器面临几个挑战。考虑到现代加速器的层次化内存系统，映射策略必须仔细管理何时以及在哪里访问数据，以最小化延迟和功耗，同时确保计算单元保持活跃。不良的映射决策可能导致计算资源利用率低下、数据移动过多和执行时间增加，最终降低整体效率。
- en: 'To understand the complexity of this challenge, consider an analogy: mapping
    a neural network to an accelerator is like planning a massive, factory-wide assembly
    process. You have thousands of workers (processing elements) and a complex set
    of tasks (computations). You must decide which worker does which task (computation
    placement), where to store the parts they need (memory allocation), and the exact
    sequence of operations to minimize time spent walking around (dataflow). A small
    change in the plan can lead to massive differences in factory output. Just as
    a poorly organized factory might have workers idle while others are overwhelmed,
    or materials stored too far from where they’re needed, a poorly mapped neural
    network can leave processing elements underutilized while creating memory bottlenecks
    that stall the entire system.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一挑战的复杂性，可以考虑一个类比：将神经网络映射到加速器就像规划一个大规模、全厂范围的组装过程。你拥有数千名工人（处理元素）和一套复杂的任务（计算）。你必须决定哪个工人执行哪个任务（计算放置），在哪里存储他们需要的部件（内存分配），以及精确的操作顺序以最小化行走时间（数据流）。计划中的微小变化可能导致工厂产出出现巨大差异。就像一个组织不善的工厂可能会让一些工人闲置，而其他人则不堪重负，或者材料存储得太远，难以到达所需位置一样，一个映射不当的神经网络可能会让处理元素利用率低下，同时造成内存瓶颈，阻碍整个系统的运行。
- en: Mapping encompasses three interrelated aspects that form the foundation of effective
    AI accelerator design.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 映射包括三个相互关联的方面，这些方面构成了有效人工智能加速器设计的基石。
- en: '**Computation Placement**: Systematically assigns operations (e.g., matrix
    multiplications, convolutions) to processing elements to maximize parallelism
    and reduce idle time.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算放置**：系统性地将操作（例如，矩阵乘法、卷积）分配给处理元素，以最大化并行性和减少空闲时间。'
- en: '**Memory Allocation**: Carefully determines where model parameters, activations,
    and intermediate results reside within the memory hierarchy to optimize access
    efficiency.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存分配**：仔细确定模型参数、激活和中间结果在内存层次结构中的位置，以优化访问效率。'
- en: '**Dataflow and Execution Scheduling**: Structures the movement of data between
    compute units to reduce bandwidth bottlenecks and ensure smooth, continuous execution.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流和执行调度**：结构化计算单元之间的数据流动，以减少带宽瓶颈并确保平稳、连续的执行。'
- en: Effective mapping strategies minimize off-chip memory accesses, maximize compute
    utilization, and efficiently manage data movement across different levels of the
    memory hierarchy.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的映射策略最小化芯片外内存访问，最大化计算利用率，并有效地管理不同内存层次之间的数据移动。
- en: '**The Role of the Compiler**'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '**编译器的作用**'
- en: Developers rarely perform this complex mapping manually. Instead, a specialized
    **compiler** (like NVIDIA’s NVCC or Google’s XLA) takes the high-level model from
    the framework and automatically explores the mapping search space to find an optimal
    execution plan for the target hardware. The compiler is the crucial software layer
    that translates the model’s computational graph into an efficient hardware-specific
    dataflow, balancing the three interrelated aspects of computation placement, memory
    allocation, and execution scheduling described above. This compiler support is
    examined in detail in [Section 11.7](ch017.xhtml#sec-ai-acceleration-compiler-support-172e).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者很少手动执行这种复杂的映射。相反，一个专门的**编译器**（如NVIDIA的NVCC或Google的XLA）从框架中提取高级模型，并自动探索映射搜索空间，以找到针对目标硬件的最佳执行计划。编译器是至关重要的软件层，它将模型的计算图转换为高效的硬件特定数据流，平衡上述计算放置、内存分配和执行调度三个相互关联的方面。这种编译器支持在[第11.7节](ch017.xhtml#sec-ai-acceleration-compiler-support-172e)中进行了详细探讨。
- en: The following sections explore the key mapping choices that influence execution
    efficiency and lay the groundwork for optimization strategies that refine these
    decisions.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分探讨了影响执行效率的关键映射选择，并为优化策略奠定了基础，这些策略可以细化这些决策。
- en: Computation Placement
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算放置
- en: Modern AI accelerators are designed to execute machine learning models with
    massive parallelism, using thousands to millions of processing elements to perform
    computations simultaneously. However, simply having many compute units is not
    enough. How computations are assigned to these units determines overall efficiency.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能加速器旨在以大规模并行执行机器学习模型，使用数千到数百万个处理元素同时进行计算。然而，仅仅拥有许多计算单元是不够的。如何将这些计算分配给这些单元决定了整体效率。
- en: Without careful placement, some processing elements may sit idle while others
    are overloaded, leading to wasted resources, increased memory traffic, and reduced
    performance. Computation placement is the process of strategically mapping operations
    onto available hardware resources to sustain high throughput, minimize stalls,
    and optimize execution efficiency.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 没有仔细的放置，一些处理元素可能会闲置，而其他处理元素可能会过载，导致资源浪费、内存流量增加和性能降低。计算放置是将操作战略性地映射到可用硬件资源的过程，以维持高吞吐量、最小化停滞并优化执行效率。
- en: Computation Placement Definition
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算放置定义
- en: AI accelerators contain thousands to millions of processing elements, making
    computation placement a large-scale problem. Modern GPUs, such as the NVIDIA H100,
    feature over 16,000 streaming processors and more than 500 specialized tensor
    cores, each designed to accelerate matrix operations ([Choquette 2023a](ch058.xhtml#ref-nvidia2022h100)).
    TPUs utilize systolic arrays composed of thousands of interconnected multiply-accumulate
    (MAC) units, while wafer-scale processors like Cerebras’ CS-2 push parallelism
    even further, integrating over 850,000 cores on a single chip ([Systems 2021b](ch058.xhtml#ref-Cerebras2021)).
    In these architectures, even minor inefficiencies in computation placement can
    lead to significant performance losses, as idle cores or excessive memory movement
    compound across the system.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速器包含数千到数百万个处理元素，使得计算放置成为一个大规模问题。现代GPU，如NVIDIA H100，拥有超过16,000个流处理器和超过500个专门的张量核心，每个核心都旨在加速矩阵运算（[Choquette
    2023a](ch058.xhtml#ref-nvidia2022h100)）。TPU使用由数千个相互连接的乘加（MAC）单元组成的收缩阵列，而像Cerebras的CS-2这样的晶圆级处理器将并行性进一步推向极致，在一个芯片上集成了超过850,000个核心（[Systems
    2021b](ch058.xhtml#ref-Cerebras2021)）。在这些架构中，计算放置的微小低效可能导致显著的性能损失，因为空闲核心或过度的内存移动在整个系统中累积。
- en: Computation placement ensures that all processing elements contribute effectively
    to execution. This means that workloads must be distributed in a way that avoids
    imbalanced execution, where some processing elements sit idle while others remain
    overloaded. Similarly, placement must minimize unnecessary data movement, as excessive
    memory transfers introduce latency and power overheads that degrade system performance.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 计算放置确保所有处理单元都能有效地参与执行。这意味着工作负载必须以避免不平衡执行的方式分配，即某些处理单元空闲，而其他处理单元过载。同样，放置必须最小化不必要的数据移动，因为过度的内存传输会引入延迟和功耗，从而降低系统性能。
- en: Neural network computations vary significantly based on the model architecture,
    influencing how placement strategies are applied. For example, in a CNN, placement
    focuses on dividing image regions across processing elements to maximize parallelism.
    A <semantics><mrow><mn>256</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">256\times256</annotation></semantics>
    image processed through thousands of GPU cores might be broken into small tiles,
    each mapped to a different processing unit to execute convolutional operations
    simultaneously. In contrast, a transformer-based model requires placement strategies
    that accommodate self-attention mechanisms, where each token in a sequence interacts
    with all others, leading to irregular and memory-intensive computation patterns.
    Meanwhile, Graph Neural Networks (GNNs) introduce additional complexity, as computations
    depend on sparse and dynamic graph structures that require adaptive workload distribution
    ([Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)).
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络计算因模型架构的不同而有很大差异，这影响着放置策略的应用。例如，在卷积神经网络（CNN）中，放置策略侧重于将图像区域分配到处理单元中，以最大化并行性。一个通过数千个GPU核心处理的<semantics><mrow><mn>256</mn><mo>×</mo><mn>256</mn></mrow><annotation
    encoding="application/x-tex">256\times256</annotation></semantics>图像可能会被分割成小块，每块映射到不同的处理单元以同时执行卷积操作。相比之下，基于转换器的模型需要适应自注意力机制的放置策略，其中序列中的每个标记都与所有其他标记交互，导致不规则和内存密集的计算模式。同时，图神经网络（GNNs）引入了额外的复杂性，因为计算依赖于稀疏和动态的图结构，这需要自适应的工作负载分配（[Zheng等人2020](ch058.xhtml#ref-Zheng2020)）。
- en: Because computation placement directly impacts resource utilization, execution
    speed, and power efficiency, it is one of the most critical factors in AI acceleration.
    A well-placed computation can reduce latency by orders of magnitude, while a poorly
    placed one can render thousands of processing units underutilized. The next section
    explores why efficient computation placement is essential and the consequences
    of suboptimal mapping strategies.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算放置直接影响资源利用率、执行速度和功耗效率，它是人工智能加速中最关键的因素之一。一个放置得当的计算可以降低延迟几个数量级，而一个放置不当的计算可能导致数千个处理单元利用率低下。下一节将探讨为什么高效的计算放置至关重要以及次优映射策略的后果。
- en: Computation Placement Importance
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算放置的重要性
- en: While computation placement is a hardware-driven process, its importance is
    fundamentally shaped by the structure of neural network workloads. Different types
    of machine learning models exhibit distinct computation patterns, which directly
    influence how efficiently they can be mapped onto accelerators. Without careful
    placement, workloads can become unbalanced, memory access patterns can become
    inefficient, and the overall performance of the system can degrade significantly.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算放置是一个硬件驱动的过程，但其重要性从根本上是由神经网络工作负载的结构所决定的。不同类型的机器学习模型表现出不同的计算模式，这直接影响它们如何有效地映射到加速器上。如果没有仔细的放置，工作负载可能会变得不平衡，内存访问模式可能变得低效，系统的整体性能可能会显著下降。
- en: For models with structured computation patterns, such as CNNs, computation placement
    is relatively straightforward. CNNs process images using filters that are applied
    to small, localized regions, meaning their computations can be evenly distributed
    across processing elements. Because these operations are highly parallelizable,
    CNNs benefit from spatial partitioning, where the input is divided into tiles
    that are processed independently. This structured execution makes CNNs well-suited
    for accelerators that favor regular dataflows, minimizing the complexity of placement
    decisions.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有结构化计算模式的模型，如CNN，计算放置相对简单。CNN使用过滤器处理图像，这些过滤器应用于小而局部的区域，这意味着它们的计算可以在处理单元之间均匀分布。由于这些操作高度可并行化，CNN从空间分区中受益，输入被分割成独立处理的瓦片。这种结构化执行使CNN非常适合那些偏好常规数据流的加速器，从而最小化放置决策的复杂性。
- en: However, for models with irregular computation patterns, such as transformers
    and GNNs, computation placement becomes significantly more challenging. Transformers,
    which rely on self-attention mechanisms, require each token in a sequence to interact
    with all others, resulting in non-uniform computation demands. Unlike CNNs, where
    each processing element performs a similar amount of work, transformers introduce
    workload imbalance, where certain operations, including the computation of attention
    scores, require far more computation than others. Without careful placement, this
    imbalance can lead to stalls, where some processing elements remain idle while
    others struggle to keep up.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于具有不规则计算模式的模型，例如变换器和GNNs，计算放置变得显著更具挑战性。依赖于自注意力机制的变换器需要序列中的每个标记与所有其他标记进行交互，从而导致非均匀的计算需求。与每个处理元素执行相似工作量计算的CNNs不同，变换器引入了工作负载不平衡，其中某些操作，包括注意力分数的计算，需要比其他操作更多的计算。如果没有仔细放置，这种不平衡可能导致停滞，其中一些处理元素保持空闲，而其他处理元素则难以跟上。
- en: The challenge is even greater in graph neural networks (GNNs), where computation
    depends on sparse and dynamically changing graph structures. Unlike CNNs, which
    operate on dense and regularly structured data, GNNs must process nodes and edges
    with highly variable degrees of connectivity. Some regions of a graph may require
    significantly more computation than others, making workload balancing across processing
    elements difficult ([Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)). If computations
    are not placed strategically, some compute units will sit idle while others remain
    overloaded, leading to underutilization and inefficiencies in execution.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在图神经网络（GNNs）中，挑战更大，因为计算依赖于稀疏和动态变化的图结构。与在密集和规则结构数据上操作的CNNs不同，GNNs必须以高度可变连接度处理节点和边。图的一些区域可能需要比其他区域显著更多的计算，这使得在处理元素之间平衡工作负载变得困难（[Zheng等人2020](ch058.xhtml#ref-Zheng2020)）。如果没有战略性地放置计算，一些计算单元将保持空闲，而其他计算单元则可能过载，导致利用率低下和执行效率低下。
- en: Poor computation placement adversely affects AI execution by creating workload
    imbalance, inducing excessive data movement, and causing execution stalls and
    bottlenecks. An uneven distribution of computations can lead to idle processing
    elements, preventing full hardware utilization and diminishing throughput. Inefficient
    execution assignment increases memory traffic by necessitating frequent data transfers
    between memory hierarchies, introducing latency and raising power consumption.
    Finally, such misallocation can cause operations to wait on data dependencies,
    resulting in pipeline inefficiencies that ultimately lower overall system performance.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 不良的计算放置通过创建工作负载不平衡、引起过多的数据移动以及导致执行停滞和瓶颈，对AI执行产生不利影响。计算的不均匀分布可能导致处理元素空闲，阻止硬件的充分利用并降低吞吐量。低效的执行分配通过需要在内存层次结构之间频繁传输数据，引入延迟并提高功耗，增加了内存流量。最后，这种错误分配可能导致操作在数据依赖上等待，从而导致流水线效率低下，最终降低整体系统性能。
- en: Computation placement ensures that models execute efficiently given their unique
    computational structure. A well-placed workload reduces execution time, memory
    overhead, and power consumption, while a poorly placed one can lead to stalled
    execution pipelines and inefficient resource utilization. The next section explores
    the key considerations that must be addressed to ensure that computation placement
    is both efficient and adaptable to different model architectures.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 计算放置确保模型在给定的独特计算结构下高效执行。放置得当的工作负载可以减少执行时间、内存开销和功耗，而放置不当的工作负载可能导致执行流水线停滞和资源利用效率低下。下一节将探讨必须解决的关键考虑因素，以确保计算放置既高效又能够适应不同的模型架构。
- en: Effective Computation Placement
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有效的计算放置
- en: Computation placement is a balancing act between hardware constraints and workload
    characteristics. To achieve high efficiency, placement strategies must account
    for parallelism, memory access, and workload variability while ensuring that processing
    elements remain fully utilized. Poor placement leads to imbalanced execution,
    increased data movement, and performance degradation, making it essential to consider
    key factors when designing placement strategies.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 计算放置是在硬件约束和工作负载特征之间的一种平衡行为。为了实现高效率，放置策略必须考虑并行性、内存访问和工作负载可变性，同时确保处理元素得到充分利用。不良的放置会导致执行不平衡、数据移动增加和性能下降，因此在设计放置策略时考虑关键因素至关重要。
- en: As summarized in [Table 11.11](ch017.xhtml#tbl-placement-challenges), computation
    placement faces several critical challenges that impact execution efficiency.
    Effective mapping strategies must address these challenges by balancing workload
    distribution, minimizing data movement, and optimizing communication across processing
    elements.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表11.11](ch017.xhtml#tbl-placement-challenges)总结所示，计算放置面临几个影响执行效率的关键挑战。有效的映射策略必须通过平衡工作负载分配、最小化数据移动和优化处理元素之间的通信来解决这些挑战。
- en: 'Table 11.11: **Computation Placement Challenges**: Effective neural network
    deployment requires strategic allocation of computations to processing elements,
    balancing workload distribution, data movement costs, and hardware constraints
    to maximize execution efficiency and avoid performance bottlenecks. Understanding
    these challenges guides the design of mapping strategies that optimize resource
    utilization and minimize communication overhead.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.11：**计算放置挑战**：有效的神经网络部署需要战略性地将计算分配给处理元素，平衡工作负载分配、数据移动成本和硬件约束，以最大化执行效率并避免性能瓶颈。理解这些挑战指导了映射策略的设计，这些策略优化资源利用并最小化通信开销。
- en: '| **Challenge** | **Impact on Execution** | **Key Considerations for Placement**
    |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | **对执行的影响** | **放置的关键考虑因素** |'
- en: '| --- | --- | --- |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Workload Imbalance** | Some processing elements finish early while others
    remain overloaded, leading to idle compute resources. | Distribute operations
    evenly to prevent stalls and ensure full utilization of PEs. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载不平衡** | 一些处理元素提前完成，而其他处理元素仍然超载，导致计算资源闲置。 | 均匀分配操作以防止停滞并确保处理元素充分利用。
    |'
- en: '| **Irregular Computation Patterns** | Models like transformers and GNNs introduce
    non-uniform computation demands, making static placement difficult. | Use adaptive
    placement strategies that adjust execution based on workload characteristics.
    |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| **不规则的计算模式** | 类似于变换器和GNNs的模型引入了非均匀的计算需求，使得静态放置困难。 | 使用基于工作负载特征的自适应放置策略来调整执行。
    |'
- en: '| **Excessive Data Movement** | Frequent memory transfers introduce latency
    and increase power consumption. | Keep frequently used data close to the compute
    units and minimize off-chip memory accesses. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| **过多的数据移动** | 频繁的内存传输引入延迟并增加功耗。 | 将常用数据保持在计算单元附近并最小化片外内存访问。 |'
- en: '| **Limited Interconnect Bandwidth** | Poorly placed operations can create
    congestion, slowing data movement between PEs. | Optimize spatial and temporal
    placement to reduce communication overhead. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| **有限的互连带宽** | 放置不当的操作可能造成拥塞，减慢处理元素之间的数据移动。 | 优化空间和时间放置以减少通信开销。 |'
- en: '| **Model-Specific Execution Needs** | CNNs, transformers, and GNNs require
    different execution patterns, making a single placement strategy ineffective.
    | Tailor placement strategies to match the computational structure of each model
    type. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| **模型特定的执行需求** | CNNs、变换器和GNNs需要不同的执行模式，使得单一的放置策略无效。 | 针对每种模型类型的计算结构定制放置策略。
    |'
- en: 'Each of these challenges highlights a core trade-off in computation placement:
    maximizing parallelism while minimizing memory overhead. For CNNs, placement strategies
    prioritize structured tiling to maintain efficient data reuse. For transformers,
    placement must ensure balanced execution across attention layers. For GNNs, placement
    must dynamically adjust to sparse computation patterns.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些挑战都突显了计算放置的核心权衡：在最大化并行性的同时最小化内存开销。对于卷积神经网络（CNNs），放置策略优先考虑结构化分块以维持高效的数据重用。对于变换器（transformers），放置必须确保注意力层之间的执行平衡。对于图神经网络（GNNs），放置必须动态调整以适应稀疏计算模式。
- en: Beyond model-specific needs, effective computation placement must also be scalable.
    As models grow in size and complexity, placement strategies must adapt dynamically
    rather than relying on static execution patterns. Future AI accelerators increasingly
    integrate runtime-aware scheduling mechanisms, where placement is optimized based
    on real-time workload behavior rather than predetermined execution plans.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型特定的需求之外，有效的计算放置还必须是可扩展的。随着模型的大小和复杂性的增长，放置策略必须动态适应，而不是依赖于静态的执行模式。未来的AI加速器越来越多地集成运行时感知的调度机制，其中放置是基于实时工作负载行为而不是预定的执行计划进行优化的。
- en: Effective computation placement requires balancing hardware capabilities with
    model characteristics. The next section explores how computation placement interacts
    with memory allocation and data movement, ensuring that AI accelerators operate
    at peak efficiency.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的计算放置需要平衡硬件能力和模型特性。下一节将探讨计算放置如何与内存分配和数据移动相互作用，确保人工智能加速器以峰值效率运行。
- en: Memory Allocation
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存分配
- en: Efficient memory allocation is essential for high-performance AI acceleration.
    As AI models grow in complexity, accelerators must manage vast amounts of data
    movement—loading model parameters, storing intermediate activations, and handling
    gradient computations. The way this data is allocated across the memory hierarchy
    directly affects execution efficiency, power consumption, and overall system throughput.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能人工智能加速需要有效的内存分配。随着人工智能模型复杂性的增加，加速器必须管理大量的数据移动——加载模型参数、存储中间激活和处理梯度计算。这些数据在内存层次结构中的分配方式直接影响执行效率、功耗和整体系统吞吐量。
- en: Memory Allocation Definition
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存分配定义
- en: While computation placement determines where operations are executed, memory
    allocation defines where data is stored and how it is accessed throughout execution.
    All AI accelerators rely on hierarchical memory systems, ranging from on-chip
    caches and scratchpads to HBM and DRAM. Poor memory allocation can lead to excessive
    off-chip memory accesses, increasing bandwidth contention and slowing down execution.
    Since AI accelerators operate at teraflop and petaflop scales, inefficient memory
    access patterns can result in substantial performance bottlenecks.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算放置决定了操作在哪里执行，但内存分配定义了数据在哪里存储以及在整个执行过程中如何访问。所有人工智能加速器都依赖于从片上缓存和擦除板到HBM和DRAM的分层内存系统。不良的内存分配可能导致过多的片外内存访问，增加带宽竞争并减慢执行速度。由于人工智能加速器在太浮点和百万亿浮点运算的规模上运行，低效的内存访问模式可能导致显著的性能瓶颈。
- en: The primary goal of memory allocation is to minimize latency and reduce power
    consumption by keeping frequently accessed data as close as possible to the processing
    elements. Different hardware architectures implement memory hierarchies tailored
    for AI workloads. GPUs rely on a mix of global memory, shared memory, and registers,
    requiring careful tiling strategies to optimize locality ([X. Qi, Kantarci, and
    Liu 2017](ch058.xhtml#ref-nvidia2020ampere)). TPUs use on-chip SRAM scratchpads,
    where activations and weights must be efficiently preloaded to sustain systolic
    array execution ([Norman P. Jouppi et al. 2017c](ch058.xhtml#ref-jouppi_tpu_2017)).
    Wafer-scale processors, with their hundreds of thousands of cores, demand sophisticated
    memory partitioning strategies to avoid excessive interconnect traffic ([Systems
    2021b](ch058.xhtml#ref-Cerebras2021)). In all cases, the effectiveness of memory
    allocation determines the overall throughput, power efficiency, and scalability
    of AI execution.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分配的主要目标是通过尽可能将频繁访问的数据保留在处理元素附近，以最小化延迟并减少功耗。不同的硬件架构实现了针对人工智能工作负载定制的内存层次结构。GPU依赖于全局内存、共享内存和寄存器的组合，需要仔细的平铺策略来优化局部性（[X.
    Qi, Kantarci, 和 Liu 2017](ch058.xhtml#ref-nvidia2020ampere)）。TPU使用片上SRAM擦除板，其中激活和权重必须被有效地预加载以维持阵列执行（[Norman
    P. Jouppi 等人 2017c](ch058.xhtml#ref-jouppi_tpu_2017)）。晶圆级处理器，拥有数十万个核心，需要复杂的内存分区策略以避免过多的互连流量（[Systems
    2021b](ch058.xhtml#ref-Cerebras2021)）。在所有情况下，内存分配的有效性决定了人工智能执行的总体吞吐量、功耗和可扩展性。
- en: Memory allocation directly impacts AI acceleration efficiency through data storage
    and access patterns. Unlike general-purpose computing, where memory management
    is abstracted by caches and dynamic allocation, AI accelerators require explicit
    data placement strategies to sustain high throughput and avoid unnecessary stalls.
    This is particularly evident in systolic arrays ([Figure 11.4](ch017.xhtml#fig-systolic-array)),
    where the rhythmic data flow between processing elements depends on precisely
    timed memory access patterns. In TPU’s systolic arrays, for instance, weights
    must be preloaded into on-chip scratchpads and streamed through the array in perfect
    synchronization with input activations to maintain the pipelined computation flow.
    When memory is not allocated efficiently, AI workloads suffer from latency overhead,
    excessive power consumption, and bottlenecks that limit computational performance.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分配通过数据存储和访问模式直接影响AI加速器的效率。与通用计算不同，在通用计算中，内存管理通过缓存和动态分配抽象化，AI加速器需要显式的数据放置策略来维持高吞吐量并避免不必要的停滞。这在节拍阵列（[图11.4](ch017.xhtml#fig-systolic-array)）中尤为明显，其中处理元素之间的节奏性数据流依赖于精确的时间内存访问模式。例如，在TPU的节拍阵列中，权重必须预先加载到片上暂存器中，并与输入激活同步流经阵列，以维持流水线计算流程。当内存分配效率不高时，AI工作负载将遭受延迟开销、过度的功耗和限制计算性能的瓶颈。
- en: Memory Challenges for Different Workloads
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同工作负载的内存挑战
- en: 'Neural network architectures have varying memory demands, which influence the
    importance of proper allocation. CNNs rely on structured and localized data access
    patterns, meaning that inefficient memory allocation can lead to redundant data
    loads and cache inefficiencies ([Y.-H. Chen et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
    In contrast, transformer models require frequent access to large model parameters
    and intermediate activations, making them highly sensitive to memory bandwidth
    constraints. GNNs introduce even greater challenges, as their irregular and sparse
    data structures result in unpredictable memory access patterns that can lead to
    inefficient use of memory resources. Poor memory allocation has three major consequences
    for AI execution:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构具有不同的内存需求，这影响了适当分配的重要性。CNN依赖于结构化和局部化的数据访问模式，这意味着不合理的内存分配可能导致冗余数据加载和缓存效率低下（[Y.-H.
    Chen等人2016](ch058.xhtml#ref-chen2016eyeriss)）。相比之下，变换器模型需要频繁访问大型模型参数和中间激活，这使得它们对内存带宽限制非常敏感。GNN引入了更大的挑战，因为它们的非规则和稀疏数据结构导致不可预测的内存访问模式，可能导致内存资源使用效率低下。不合理的内存分配对AI执行有三个主要后果：
- en: '**Increased Memory Latency**: When frequently accessed data is not stored in
    the right location, accelerators must retrieve it from higher-latency memory,
    slowing down execution.'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增加内存延迟**：当频繁访问的数据没有存储在正确的位置时，加速器必须从更高延迟的内存中检索它，从而减慢执行速度。'
- en: '**Higher Power Consumption**: Off-chip memory accesses consume significantly
    more energy than on-chip storage, leading to inefficiencies at scale.'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更高的功耗**：片外内存访问比片上存储消耗的能量显著更多，导致在规模上的低效。'
- en: '**Reduced Computational Throughput**: If data is not available when needed,
    processing elements remain idle, reducing the overall performance of the system.'
  id: totrans-513
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**降低计算吞吐量**：如果数据在需要时不可用，处理元素将保持空闲，从而降低系统的整体性能。'
- en: As AI models continue to grow in size and complexity, the importance of scalable
    and efficient memory allocation increases. Memory limitations can dictate how
    large of a model can be deployed on a given accelerator, affecting feasibility
    and performance.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI模型在规模和复杂性上的持续增长，可扩展和高效的内存分配的重要性也在增加。内存限制可以决定在给定的加速器上可以部署多大的模型，影响可行性和性能。
- en: 'Table 11.12: **Memory Allocation Challenges**: Efficient memory management
    in AI accelerators balances data access speed with hardware constraints, mitigating
    performance bottlenecks caused by latency, bandwidth limitations, and irregular
    data patterns. Addressing these challenges is critical for deploying complex models,
    such as transformers and graphs, which have variable and demanding memory requirements.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.12：**内存分配挑战**：在AI加速器中，高效的内存管理需要在数据访问速度与硬件限制之间取得平衡，以减轻由延迟、带宽限制和不规则数据模式引起的性能瓶颈。解决这些挑战对于部署复杂模型至关重要，例如变换器和图模型，它们具有可变和苛刻的内存需求。
- en: '| **Challenge** | **Impact on Execution** | **Key Considerations for Allocation**
    |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | **对执行的影响** | **分配的关键考虑因素** |'
- en: '| --- | --- | --- |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **High Memory Latency** | Slow data access delays execution and reduces throughput.
    | Prioritize placing frequently accessed data in faster memory locations. |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| **高内存延迟** | 慢速数据访问延迟执行并降低吞吐量。 | 优先将频繁访问的数据放置在更快的内存位置。 |'
- en: '| **Limited On-Chip Storage** | Small local memory constrains the amount of
    data available near compute units. | Allocate storage efficiently to maximize
    data availability without exceeding hardware limits. |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| **片上存储有限** | 小型本地内存限制了计算单元附近可用的数据量。 | 高效分配存储以最大化数据可用性，同时不超过硬件限制。 |'
- en: '| **High Off-Chip Bandwidth Demand** | Frequent access to external memory increases
    delays and power consumption. | Reduce unnecessary memory transfers by carefully
    managing when and how data is moved. |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| **高片外带宽需求** | 频繁访问外部内存会增加延迟和功耗。 | 通过仔细管理何时以及如何移动数据来减少不必要的内存传输。 |'
- en: '| **Irregular Memory Access Patterns** | Some models require accessing data
    unpredictably, leading to inefficient memory usage. | Organize memory layout to
    align with access patterns and minimize unnecessary data movement. |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| **不规则内存访问模式** | 一些模型需要不可预测地访问数据，导致内存使用效率低下。 | 组织内存布局以与访问模式对齐并最小化不必要的数据移动。
    |'
- en: '| **Model-Specific Memory Needs** | Different models require different allocation
    strategies to optimize performance. | Tailor allocation decisions based on the
    structure and execution characteristics of the workload. |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| **特定模型内存需求** | 不同的模型需要不同的分配策略来优化性能。 | 根据工作负载的结构和执行特性定制分配决策。 |'
- en: As summarized in [Table 11.12](ch017.xhtml#tbl-memory-allocation), memory allocation
    in AI accelerators must address several key challenges that influence execution
    efficiency. Effective allocation strategies mitigate high latency, bandwidth limitations,
    and irregular access patterns by carefully managing data placement and movement.
    Ensuring that frequently accessed data is stored in faster memory locations while
    minimizing unnecessary transfers is essential for maintaining performance and
    energy efficiency.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表11.12](ch017.xhtml#tbl-memory-allocation)所述，AI加速器中的内存分配必须解决影响执行效率的几个关键挑战。有效的分配策略通过精心管理数据放置和移动来减轻高延迟、带宽限制和不规则访问模式。确保频繁访问的数据存储在更快的内存位置，同时最小化不必要的传输，对于保持性能和能源效率至关重要。
- en: Each of these challenges requires careful memory management to balance execution
    efficiency with hardware constraints. While structured models may benefit from
    well-defined memory layouts that facilitate predictable access, others, like transformer-based
    and graph-based models, require more adaptive allocation strategies to handle
    variable and complex memory demands. Beyond workload-specific considerations,
    memory allocation must also be scalable. As model sizes continue to grow, accelerators
    must dynamically manage memory resources rather than relying on static allocation
    schemes. Ensuring that frequently used data is accessible when needed without
    overwhelming memory capacity is essential for maintaining high efficiency.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些挑战都需要仔细的内存管理来平衡执行效率与硬件限制。虽然结构化模型可能从便于预测访问的明确定义的内存布局中受益，但其他模型，如基于转换器和基于图模型，需要更适应的分配策略来处理可变和复杂的内存需求。除了特定于工作负载的考虑之外，内存分配还必须是可扩展的。随着模型尺寸的持续增长，加速器必须动态管理内存资源，而不是依赖于静态的分配方案。确保在需要时可以访问常用数据，同时不超出内存容量，对于保持高效率至关重要。
- en: Combinatorial Complexity
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组合复杂性
- en: The efficient execution of machine learning models on AI accelerators requires
    careful consideration of placement and allocation. Placement involves spatial
    assignment of computations and data, while allocation covers temporal distribution
    of resources. These decisions are interdependent, and each introduces trade-offs
    that impact performance, energy efficiency, and scalability. [Table 11.13](ch017.xhtml#tbl-combinatorial-complexity)
    outlines the fundamental trade-offs between computation placement and resource
    allocation in AI accelerators. Placement decisions influence parallelism, memory
    access patterns, and communication overhead, while allocation strategies determine
    how resources are distributed over time to balance execution efficiency. The interplay
    between these factors shapes overall performance, requiring a careful balance
    to avoid bottlenecks such as excessive synchronization, memory congestion, or
    underutilized compute resources. Optimizing these trade-offs is essential for
    ensuring that AI accelerators operate at peak efficiency.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI加速器上高效执行机器学习模型需要仔细考虑放置和分配。放置涉及计算和数据的空间分配，而分配则涵盖资源的时序分布。这些决策相互依赖，每个决策都引入了影响性能、能效和可扩展性的权衡。[表11.13](ch017.xhtml#tbl-combinatorial-complexity)概述了AI加速器中计算放置和资源分配之间的基本权衡。放置决策影响并行性、内存访问模式和通信开销，而分配策略决定资源如何随时间分布以平衡执行效率。这些因素之间的相互作用决定了整体性能，需要仔细平衡以避免如过度同步、内存拥塞或计算资源未充分利用等瓶颈。优化这些权衡对于确保AI加速器以峰值效率运行至关重要。
- en: Each of these dimensions requires balancing trade-offs between placement and
    allocation. For instance, spatially distributing computations across multiple
    processing elements can increase throughput; however, if data allocation is not
    optimized, memory bandwidth limitations may introduce bottlenecks. Likewise, allocating
    resources for fine-grained computations may enhance flexibility but, without appropriate
    placement strategies, may lead to excessive synchronization overhead.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些维度都需要在放置和分配之间平衡权衡。例如，将计算在多个处理单元之间空间分布可以增加吞吐量；然而，如果数据分配没有优化，内存带宽限制可能会引入瓶颈。同样，为细粒度计算分配资源可能增强灵活性，但没有适当的放置策略，可能会导致过度的同步开销。
- en: 'Table 11.13: **Placement-Allocation Trade-Offs**: AI accelerator performance
    depends on strategically mapping computations to hardware and allocating resources
    over time, balancing parallelism, memory access, and execution efficiency to avoid
    bottlenecks. Careful consideration of these interdependent factors is essential
    for maximizing throughput and minimizing energy consumption in machine learning
    systems.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.13：**放置-分配权衡**：AI加速器的性能取决于战略性地将计算映射到硬件并在时间上分配资源，平衡并行性、内存访问和执行效率以避免瓶颈。仔细考虑这些相互依赖的因素对于在机器学习系统中最大化吞吐量和最小化能耗至关重要。
- en: '| **Dimension** | **Placement Considerations** | **Allocation Considerations**
    |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| **维度** | **放置考虑因素** | **分配考虑因素** |'
- en: '| --- | --- | --- |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Computational Granularity** | Fine-grained placement enables greater parallelism
    but increases synchronization overhead. | Coarse-grained allocation reduces synchronization
    overhead but may limit flexibility. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| **计算粒度** | 细粒度放置能够实现更高的并行性，但会增加同步开销。 | 粗粒度分配减少同步开销，但可能限制灵活性。 |'
- en: '| **Spatial vs. Temporal Mapping** | Spatial placement enhances parallel execution
    but can lead to resource contention and memory congestion. | Temporal allocation
    balances resource sharing but may reduce overall throughput. |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| **空间映射与时间映射** | 空间放置增强并行执行，但可能导致资源竞争和内存拥塞。 | 时间分配平衡资源共享，但可能降低整体吞吐量。 |'
- en: '| **Memory and Data Locality** | Placing data closer to compute units minimizes
    latency but may reduce overall memory availability. | Allocating data across multiple
    memory levels increases capacity but introduces higher access costs. |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| **内存和数据局部性** | 将数据放置在计算单元附近可以最小化延迟，但可能减少整体内存可用性。 | 在多个内存级别分配数据可以增加容量，但引入更高的访问成本。
    |'
- en: '| **Communication and Synchronization** | Co-locating compute units reduces
    communication latency but may introduce contention. | Allocating synchronization
    mechanisms mitigates stalls but can introduce additional overhead. |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| **通信和同步** | 将计算单元本地化可以减少通信延迟，但可能引入竞争。 | 分配同步机制可以缓解停滞，但可能引入额外的开销。 |'
- en: '| **Dataflow and Execution Ordering** | Static placement simplifies execution
    but limits adaptability to workload variations. | Dynamic allocation improves
    adaptability but adds scheduling complexity. |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| **数据流和执行顺序** | 静态放置简化了执行但限制了适应工作负载变化的能力。 | 动态分配提高了适应性但增加了调度复杂性。 |'
- en: Because AI accelerator architectures impose constraints on both where computations
    execute and how resources are assigned over time, selecting an effective mapping
    strategy necessitates a coordinated approach to placement and allocation. Understanding
    how these trade-offs influence execution efficiency is essential for optimizing
    performance on AI accelerators.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AI加速器架构对计算执行的位置和随时间分配的资源都施加了限制，因此选择有效的映射策略需要一种协调的放置和分配方法。理解这些权衡如何影响执行效率对于优化AI加速器上的性能至关重要。
- en: Exploring the Configuration Space
  id: totrans-537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索配置空间
- en: The efficiency of AI accelerators is determined not only by their computational
    capabilities but also by how neural network computations are mapped to hardware
    resources. Mapping defines how computations are assigned to processing elements,
    how data is placed and moved through the memory hierarchy, and how execution is
    scheduled. The choices made in this process significantly impact performance,
    influencing compute utilization, memory bandwidth efficiency, and energy consumption.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: AI加速器的效率不仅取决于其计算能力，还取决于神经网络计算如何映射到硬件资源。映射定义了计算如何分配给处理元素，数据如何在内存层次结构中放置和移动，以及执行如何调度。在这个过程中做出的选择对性能有重大影响，影响着计算利用率、内存带宽效率和能耗。
- en: Mapping machine learning models to hardware presents a large and complex design
    space. Unlike traditional computational workloads, model execution involves multiple
    interacting factors, including computation, data movement, parallelism, and scheduling,
    each introducing constraints and trade-offs. The hierarchical memory structure
    of accelerators, as discussed in the Memory Systems section, further complicates
    this process by imposing limits on bandwidth, latency, and data reuse. As a result,
    effective mapping strategies must carefully balance competing objectives to maximize
    efficiency.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习模型映射到硬件上呈现了一个庞大且复杂的设计空间。与传统的计算工作负载不同，模型执行涉及多个相互作用的因素，包括计算、数据移动、并行性和调度，每个因素都引入了约束和权衡。正如内存系统部分所讨论的，加速器的层次化内存结构通过限制带宽、延迟和数据重用进一步复杂化了这个过程。因此，有效的映射策略必须仔细平衡相互竞争的目标，以最大化效率。
- en: 'At the heart of this design space lie three interconnected aspects: data placement,
    computation scheduling, and data movement timing. Data placement refers to the
    allocation of data across various memory hierarchies, such as on-chip buffers,
    caches, and off-chip DRAM, and its effective management is critical because it
    influences both latency and energy consumption. Inefficient placement often results
    in frequent, costly memory accesses, whereas strategic placement ensures that
    data used regularly remains in fast-access storage. Computation scheduling governs
    the order in which operations execute, impacting compute efficiency and memory
    access patterns; for instance, some execution orders may optimize parallelism
    while introducing synchronization overheads, and others may improve data locality
    at the expense of throughput. Meanwhile, timing in data movement is equally essential,
    as transferring data between memory levels incurs significant latency and energy
    costs. Efficient mapping strategies thus focus on minimizing unnecessary transfers
    by reusing data and overlapping communication with computation to enhance overall
    performance.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计空间的中心有三个相互关联的方面：数据放置、计算调度和数据移动时机。数据放置指的是数据在各种内存层次结构中的分配，例如片上缓冲区、缓存和片外DRAM，其有效管理至关重要，因为它影响延迟和能耗。不合理的放置往往导致频繁且昂贵的内存访问，而战略性的放置确保经常使用的数据保持在快速访问的存储中。计算调度控制操作的执行顺序，影响计算效率和内存访问模式；例如，某些执行顺序可能优化并行性同时引入同步开销，而其他执行顺序可能以吞吐量为代价提高数据局部性。同时，数据移动的时机同样重要，因为在不同内存级别之间传输数据会产生显著的延迟和能耗。因此，有效的映射策略侧重于通过重用数据和将通信与计算重叠来最小化不必要的传输，从而提高整体性能。
- en: These factors define a vast combinatorial design space, where small variations
    in mapping decisions can lead to large differences in performance and energy efficiency.
    A poor mapping strategy can result in underutilized compute resources, excessive
    data movement, or imbalanced workloads, creating bottlenecks that degrade overall
    efficiency. Conversely, a well-designed mapping maximizes both throughput and
    resource utilization, making efficient use of available hardware.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素定义了一个庞大的组合设计空间，其中映射决策的微小变化可能导致性能和能效的大幅差异。一个糟糕的映射策略可能导致计算资源利用率低、数据移动过多或不平衡的工作负载，从而形成瓶颈，降低整体效率。相反，一个精心设计的映射最大化了吞吐量和资源利用率，充分利用了可用的硬件。
- en: Because of the interconnected nature of mapping decisions, there is no single
    optimal solution—different workloads and hardware architectures demand different
    approaches. The next sections examine the structure of this design space and how
    different mapping choices shape the execution of machine learning workloads.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 由于映射决策的相互关联性，没有单一的解决方案——不同的工作负载和硬件架构需要不同的方法。接下来的几节将探讨这个设计空间的结构以及不同的映射选择如何塑造机器学习工作负载的执行。
- en: Mapping machine learning computations onto specialized hardware requires balancing
    multiple constraints, including compute efficiency, memory bandwidth, and execution
    scheduling. The challenge arises from the vast number of possible ways to assign
    computations to processing elements, order execution, and manage data movement.
    Each decision contributes to a high-dimensional search space, where even minor
    variations in mapping choices can significantly impact performance.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习计算映射到专用硬件需要平衡多个约束，包括计算效率、内存带宽和执行调度。挑战来自于将计算分配给处理元素、排序执行和管理数据移动的无数可能方式。每个决策都贡献于一个高维搜索空间，其中映射选择的微小变化都可能对性能产生重大影响。
- en: 'Unlike traditional workloads with predictable execution patterns, machine learning
    models introduce diverse computational structures that require flexible mappings
    adapted to data reuse, parallelization opportunities, and memory constraints.
    The search space grows combinatorially, making exhaustive search infeasible. To
    understand this complexity, three sources emerge of variation:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 与具有可预测执行模式的传统工作负载不同，机器学习模型引入了多种计算结构，这些结构需要灵活的映射以适应数据重用、并行化机会和内存限制。搜索空间呈组合式增长，使得穷举搜索变得不可行。为了理解这种复杂性，出现了三个来源的变体：
- en: Ordering Computation and Execution
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算和执行的排序
- en: Machine learning workloads are often structured as nested loops, iterating over
    various dimensions of computation. For instance, a matrix multiplication kernel
    may loop over batch size (<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>),
    input features (<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>),
    and output features (<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>).
    The order in which these loops execute has a profound effect on data locality,
    reuse patterns, and computational efficiency.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载通常以嵌套循环的形式组织，遍历计算的各个维度。例如，一个矩阵乘法内核可能会遍历批量大小（<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>）、输入特征（<semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics>）和输出特征（<semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics>）。这些循环执行的顺序对数据局部性、重用模式和计算效率有深远的影响。
- en: 'The number of ways to arrange <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    loops follows a factorial growth pattern: <semantics><mrow><mi>𝒪</mi><mo>=</mo><mi>d</mi><mi>!</mi></mrow>
    <annotation encoding="application/x-tex">\mathcal{O} = d!</annotation></semantics>
    which scales rapidly. A typical convolutional layer may involve up to seven loop
    dimensions, leading to: <semantics><mrow><mn>7</mn><mi>!</mi><mo>=</mo><mn>5</mn><mo>,</mo><mn>040</mn>
    <mrow><mtext mathvariant="normal">possible execution orders.</mtext></mrow></mrow>
    <annotation encoding="application/x-tex">7! = 5,040 \text{ possible execution
    orders.}</annotation></semantics>'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 安排 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    循环的方式遵循阶乘增长模式：<semantics><mrow><mi>𝒪</mi><mo>=</mo><mi>d</mi><mi>!</mi></mrow>
    <annotation encoding="application/x-tex">\mathcal{O} = d!</annotation></semantics>，这会迅速扩展。一个典型的卷积层可能涉及多达七个循环维度，导致：<semantics><mrow><mn>7</mn><mi>!</mi><mo>=</mo><mn>5</mn><mo>,</mo><mn>040</mn>
    <mrow><mtext mathvariant="normal">可能的执行顺序。</mtext></mrow></mrow> <annotation encoding="application/x-tex">7!
    = 5,040 \text{ possible execution orders.}</annotation></semantics>
- en: 'When considering multiple memory levels, the search space expands as: <semantics><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup>
    <annotation encoding="application/x-tex">(d!)^l</annotation></semantics> where
    <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    is the number of memory hierarchy levels. This rapid expansion highlights why
    execution order optimization is crucial—poor loop ordering can lead to excessive
    memory traffic, while an optimized order improves cache utilization ([Sze et al.
    2017a](ch058.xhtml#ref-sze2017efficient)).'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑多个内存级别时，搜索空间会按如下方式扩展：<semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>!</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup> <annotation encoding="application/x-tex">(d!)^l</annotation></semantics>
    其中 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    是内存层次结构的级别数量。这种快速扩展突出了为什么执行顺序优化至关重要——不良的循环排序可能导致过度的内存流量，而优化的顺序可以提高缓存利用率 ([Sze
    et al. 2017a](ch058.xhtml#ref-sze2017efficient))。
- en: Parallelization Across Processing Elements
  id: totrans-549
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理元素间的并行化
- en: Modern AI accelerators leverage thousands of processing elements to maximize
    parallelism, but determining which computations should be parallelized is non-trivial.
    Excessive parallelization can introduce synchronization overheads and increased
    bandwidth demands, while insufficient parallelization leads to underutilized hardware.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能加速器利用数千个处理元素以最大化并行性，但确定哪些计算应该并行化并非易事。过度的并行化可能会引入同步开销和增加的带宽需求，而不足的并行化会导致硬件利用率低下。
- en: 'The number of ways to distribute computations among parallel units follows
    the binomial coefficient: <semantics><mrow><mi>𝒫</mi><mo>=</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\mathcal{P}
    = \frac{d!}{(d-k)!}</annotation></semantics> where <semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics> is the number of loops,
    and <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is the number selected for parallel execution. For a six-loop computation where
    three loops are chosen for parallel execution, the number of valid configurations
    is: <semantics><mrow><mfrac><mrow><mn>6</mn><mi>!</mi></mrow><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>6</mn><mo>−</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo>=</mo><mn>120</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">\frac{6!}{(6-3)!} = 120.</annotation></semantics>'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行单元之间分配计算的方法遵循二项式系数：<semantics><mrow><mi>𝒫</mi><mo>=</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mrow></mfrac></mrow> <annotation
    encoding="application/x-tex">\mathcal{P} = \frac{d!}{(d-k)!}</annotation></semantics>
    其中 <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    是循环的数量，而 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    是用于并行执行的选择数量。对于一个选择三个循环进行并行执行的六循环计算，有效的配置数量为：<semantics><mrow><mfrac><mrow><mn>6</mn><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>6</mn><mo>−</mo><mn>3</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mrow><mo>=</mo><mn>120</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">\frac{6!}{(6-3)!} = 120.</annotation></semantics>
- en: Even for a single layer, there can be hundreds of valid parallelization strategies,
    each affecting data synchronization, memory contention, and overall compute efficiency.
    Expanding this across multiple layers and model architectures further magnifies
    the complexity.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于单层来说，也可能有数百种有效的并行化策略，每种策略都会影响数据同步、内存竞争和整体计算效率。将这种策略扩展到多层和模型架构中，将进一步放大复杂性。
- en: Memory Placement and Data Movement
  id: totrans-553
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存放置和数据移动
- en: The hierarchical memory structure of AI accelerators introduces additional constraints,
    as data must be efficiently placed across registers, caches, shared memory, and
    off-chip DRAM. Data placement impacts latency, bandwidth consumption, and energy
    efficiency—frequent access to slow memory creates bottlenecks, while optimized
    placement reduces costly memory transfers.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速器的分层内存结构引入了额外的约束，因为数据必须在寄存器、缓存、共享内存和片外DRAM之间高效地放置。数据放置影响延迟、带宽消耗和能源效率——频繁访问慢速内存会形成瓶颈，而优化的放置可以减少昂贵的内存传输。
- en: 'The number of ways to allocate data across memory levels follows an exponential
    growth function: <semantics><mrow><mi>ℳ</mi><mo>=</mo><msup><mi>n</mi><mrow><mi>d</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{M} = n^{d \times l}</annotation></semantics>
    where:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存级别之间分配数据的方法遵循指数增长函数：<semantics><mrow><mi>ℳ</mi><mo>=</mo><msup><mi>n</mi><mrow><mi>d</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{M} = n^{d \times l}</annotation></semantics>
    其中：
- en: <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    = number of placement choices per level,
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    = 每层的放置选择数量，
- en: <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    = number of computational dimensions,
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    = 计算维度数量，
- en: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    = number of memory hierarchy levels.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    = 内存层次级别数量。
- en: 'For a model with:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个模型，每层有：
- en: <semantics><mrow><mi>d</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d
    = 5</annotation></semantics> computational dimensions,
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>d</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d
    = 5</annotation></semantics> 计算维度，
- en: <semantics><mrow><mi>l</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">l
    = 3</annotation></semantics> memory levels,
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>l</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">l
    = 3</annotation></semantics> 内存级别，
- en: <semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n
    = 4</annotation></semantics> possible placement choices per level,
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n
    = 4</annotation></semantics> 种可能的放置选择，
- en: 'the number of possible memory allocations is: <semantics><mrow><msup><mn>4</mn><mrow><mn>5</mn><mo>×</mo><mn>3</mn></mrow></msup><mo>=</mo><msup><mn>4</mn><mn>15</mn></msup><mo>=</mo><mn>1</mn><mo>,</mo><mn>073</mn><mo>,</mo><mn>741</mn><mo>,</mo><mn>824</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">4^{5 \times 3} = 4^{15} = 1,073,741,824.</annotation></semantics>'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的内存分配数量为：<semantics><mrow><msup><mn>4</mn><mrow><mn>5</mn><mo>×</mo><mn>3</mn></mrow></msup><mo>=</mo><msup><mn>4</mn><mn>15</mn></msup><mo>=</mo><mn>1</mn><mo>,</mo><mn>073</mn><mo>,</mo><mn>741</mn><mo>,</mo><mn>824</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">4^{5 \times 3} = 4^{15} = 1,073,741,824.</annotation></semantics>
- en: This highlights how even a single layer may have over a billion possible memory
    configurations, making manual optimization impractical.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了即使是单层也可能有超过十亿种可能的内存配置，使得手动优化变得不切实际。
- en: Mapping Search Space
  id: totrans-565
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 映射搜索空间
- en: 'By combining the complexity from computation ordering, parallelization, and
    memory placement, the total mapping search space can be approximated as: <semantics><mrow><mi>𝒮</mi><mo>=</mo><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mi>d</mi></msup><mo>×</mo><mi>d</mi><mi>!</mi><mo>×</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{S} = \left( n^d \times d! \times
    \frac{d!}{(d-k)!} \right)^l</annotation></semantics> where:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合计算顺序、并行化和内存放置的复杂性，总的映射搜索空间可以近似为：<semantics><mrow><mi>𝒮</mi><mo>=</mo><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mi>d</mi></msup><mo>×</mo><mi>d</mi><mi>!</mi><mo>×</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{S} = \left( n^d \times d! \times
    \frac{d!}{(d-k)!} \right)^l</annotation></semantics> 其中：
- en: <semantics><msup><mi>n</mi><mi>d</mi></msup><annotation encoding="application/x-tex">n^d</annotation></semantics>
    represents memory placement choices,
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msup><mi>n</mi><mi>d</mi></msup><annotation encoding="application/x-tex">n^d</annotation></semantics>
    表示内存放置选择，
- en: <semantics><mrow><mi>d</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">d!</annotation></semantics>
    accounts for computation ordering choices,
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>d</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">d!</annotation></semantics>
    考虑了计算顺序选择，
- en: <semantics><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{d!}{(d-k)!}</annotation></semantics> captures
    parallelization possibilities,
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{d!}{(d-k)!}</annotation></semantics> 揭示了并行化的可能性，
- en: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    is the number of memory hierarchy levels.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    是内存层次结构的层数。
- en: This equation illustrates the exponential growth of the search space, making
    brute-force search infeasible for all but the simplest cases.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程说明了搜索空间的指数增长，使得除了最简单的情况外，穷举搜索变得不可行。
- en: Dataflow Optimization Strategies
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流优化策略
- en: Mapping strategies establish *where* computations execute and *where* data resides
    within an accelerator’s architecture, but they do not specify *how* data flows
    through processing elements during execution. A systolic array might process a
    matrix multiplication with weights stored in local memory, but the order in which
    weights, inputs, and outputs move through the array fundamentally determines memory
    bandwidth consumption and energy efficiency. These dataflow patterns—termed optimization
    strategies—represent the critical implementation dimension that translates abstract
    mapping decisions into concrete execution plans.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 映射策略确定了计算在加速器架构中的执行位置和数据存储位置，但它们并没有指定数据在执行过程中如何通过处理单元流动。一个脉动阵列可能使用存储在本地内存中的权重来处理矩阵乘法，但权重、输入和输出在阵列中移动的顺序从根本上决定了内存带宽消耗和能源效率。这些数据流模式——称为优化策略——代表了将抽象映射决策转化为具体执行计划的临界实现维度。
- en: The choice among weight-stationary, input-stationary, and output-stationary
    approaches directly impacts whether an accelerator operates in the compute-bound
    or memory-bound region. Understanding these trade-offs is essential because compilers
    ([Section 11.7](ch017.xhtml#sec-ai-acceleration-compiler-support-172e)) and runtime
    systems ([Section 11.8](ch017.xhtml#sec-ai-acceleration-runtime-support-f94f))
    must select appropriate dataflow patterns based on computational characteristics
    and memory hierarchy capabilities analyzed in [Section 11.4.2](ch017.xhtml#sec-ai-acceleration-memory-hierarchy-1839).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重固定、输入固定和输出固定方法之间的选择直接影响到加速器是在计算受限区域还是内存受限区域运行。理解这些权衡是至关重要的，因为编译器（[第11.7节](ch017.xhtml#sec-ai-acceleration-compiler-support-172e)）和运行时系统（[第11.8节](ch017.xhtml#sec-ai-acceleration-runtime-support-f94f)）必须根据第11.4.2节（[第11.4.2节](ch017.xhtml#sec-ai-acceleration-memory-hierarchy-1839)）中分析的计算特性和内存层次结构能力来选择适当的数据流模式。
- en: Efficiently mapping machine learning computations onto hardware is a complex
    challenge due to the vast number of possible configurations. As models grow in
    complexity, the number of potential mappings increases exponentially. Even for
    a single layer, there are thousands of ways to order computation loops, hundreds
    of parallelization strategies, and an exponentially growing number of memory placement
    choices. This combinatorial explosion makes exhaustive search impractical.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习计算高效映射到硬件上是一个复杂的挑战，因为可能的配置数量庞大。随着模型复杂性的增加，潜在映射的数量呈指数增长。即使是单层，也有数千种计算循环的排序方式，数百种并行化策略，以及指数增长的内存放置选择。这种组合爆炸使得穷举搜索变得不切实际。
- en: To overcome this challenge, AI accelerators rely on structured mapping strategies
    that systematically balance computational efficiency, data locality, and parallel
    execution. Rather than evaluating every possible configuration, these approaches
    use a combination of heuristic, analytical, and machine learning-based techniques
    to find high-performance mappings efficiently.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，AI加速器依赖于结构化的映射策略，这些策略系统地平衡计算效率、数据局部性和并行执行。这些方法不是评估每一个可能的配置，而是结合启发式、分析和基于机器学习的技巧，以高效地找到高性能的映射。
- en: The key to effective mapping lies in understanding and applying a set of core
    techniques that optimize data movement, memory access, and computation. These
    building blocks of mapping strategies provide a structured foundation for efficient
    execution, explored in the next section.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 有效映射的关键在于理解和应用一系列核心技术，这些技术优化数据移动、内存访问和计算。这些映射策略的构建块为高效执行提供了一个结构化的基础，这在下一节中将会探讨。
- en: Building Blocks of Mapping Strategies
  id: totrans-578
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射策略的构建块
- en: To navigate the complexity of mapping decisions, a set of foundational techniques
    is leveraged that optimizes execution across data movement, memory access, and
    computation efficiency. These techniques provide the necessary structure for mapping
    strategies that maximize hardware performance while minimizing bottlenecks.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 为了导航映射决策的复杂性，利用一系列基础技术，这些技术优化了数据移动、内存访问和计算效率的执行。这些技术为映射策略提供了必要的结构，以最大化硬件性能同时最小化瓶颈。
- en: Key techniques include data movement strategies, which determine where data
    is staged during computation in order to reduce redundant transfers, such as in
    weight stationary, output stationary, and input stationary approaches. Memory-aware
    tensor layouts also play an important role by influencing memory access patterns
    and cache efficiency through the organization of data in formats such as row-major
    or channel-major.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 关键技术包括数据移动策略，这些策略确定在计算过程中数据在哪里被暂存，以减少冗余传输，例如在权重静止、输出静止和输入静止方法中。通过组织数据为行主序或通道主序等格式，内存感知张量布局也在通过影响内存访问模式和缓存效率方面发挥着重要作用。
- en: Other strategies involve kernel fusion, a method that minimizes redundant memory
    writes by combining multiple operations into a single computational step. Tiling
    is employed as a technique that partitions large computations into smaller, memory-friendly
    blocks to improve cache efficiency and reduce memory bandwidth requirements. Finally,
    balancing computation and communication is essential for managing the trade-offs
    between parallel execution and memory access to achieve high throughput.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 其他策略包括内核融合，这是一种将多个操作组合成单个计算步骤的方法，以最小化冗余内存写入。分块技术被用作一种将大型计算分割成更小、内存友好的块的技术，以提高缓存效率并减少内存带宽需求。最后，平衡计算和通信对于在并行执行和内存访问之间管理权衡以实现高吞吐量至关重要。
- en: Each of these building blocks plays a crucial role in structuring high-performance
    execution, forming the basis for both heuristic and model-driven optimization
    techniques. The next section explores how these strategies are adapted to different
    types of AI models.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 这些构建块中的每一个都在构建高性能执行结构中发挥着关键作用，为启发式和模型驱动优化技术奠定了基础。下一节将探讨这些策略如何适应不同类型的 AI 模型。
- en: Data Movement Patterns
  id: totrans-583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据移动模式
- en: While computational mapping determines where and when operations occur, its
    success depends heavily on how efficiently data is accessed and transferred across
    the memory hierarchy. Unlike traditional computing workloads, which often exhibit
    structured and predictable memory access patterns, machine learning workloads
    present irregular access behaviors due to frequent retrieval of weights, activations,
    and intermediate values.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算映射确定操作发生的位置和时间，但其成功在很大程度上取决于数据如何在内存层次结构中高效地访问和传输。与通常表现出结构化和可预测内存访问模式的传统计算工作负载不同，机器学习工作负载由于频繁检索权重、激活和中间值而表现出不规则的访问行为。
- en: Even when computational units are mapped efficiently, poor data movement strategies
    can severely degrade performance, leading to frequent memory stalls and underutilized
    hardware resources. If data cannot be supplied to processing elements at the required
    rate, computational units remain idle, increasing latency, memory traffic, and
    energy consumption ([Y.-H. Chen et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 即使计算单元被高效地映射，不良的数据移动策略也可能严重降低性能，导致频繁的内存停滞和硬件资源利用率低下。如果数据不能以所需的速率供应给处理单元，计算单元将保持空闲状态，增加延迟、内存流量和能耗
    ([Y.-H. Chen 等人 2016](ch058.xhtml#ref-chen2016eyeriss))。
- en: To illustrate the impact of data movement inefficiencies, consider a typical
    matrix multiplication operation shown in [Listing 11.18](ch017.xhtml#lst-matmul_data_movement),
    which forms the backbone of many machine learning models.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明数据移动效率低下的影响，可以考虑典型的矩阵乘法操作，如[列表 11.18](ch017.xhtml#lst-matmul_data_movement)所示，这是许多机器学习模型的基础。
- en: 'Listing 11.18: **Matrix Multiplication**: Data movement bottlenecks can lead
    to underutilized hardware resources, illustrating the importance of efficient
    data flow in optimizing machine learning model performance. Via This operation'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.18：**矩阵乘法**：数据移动瓶颈可能导致硬件资源利用率低下，说明了在优化机器学习模型性能中高效数据流的重要性。通过此操作
- en: '[PRE17]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This computation reveals several critical dataflow challenges. The first challenge
    is the number of memory accesses required. For each output <semantics><mrow><mi>Z</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,
    j]</annotation></semantics>, the computation must fetch an entire row of weights
    from the weight matrix and a full column of activations from the input matrix.
    Since the weight matrix contains 512 rows and the input matrix contains 32 columns,
    this results in repeated memory accesses that place a significant burden on memory
    bandwidth.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算揭示了几个关键的数据流挑战。第一个挑战是需要访问的内存次数。对于每个输出 <semantics><mrow><mi>Z</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,
    j]</annotation></semantics>，计算必须从权重矩阵中获取一整行的权重，并从输入矩阵中获取一整列的激活值。由于权重矩阵包含512行，输入矩阵包含32列，这导致了重复的内存访问，给内存带宽带来了重大负担。
- en: The second challenge comes from weight reuse. The same weights are applied to
    multiple inputs, meaning that an ideal mapping strategy should maximize weight
    locality to avoid redundant memory fetches. Without proper reuse, the accelerator
    would waste bandwidth loading the same weights multiple times ([Tianqi et al.
    2018](ch058.xhtml#ref-chen2018tvm)).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个挑战来自权重重用。相同的权重应用于多个输入，这意味着理想的映射策略应该最大化权重的局部性，以避免冗余的内存读取。如果没有适当的重用，加速器将浪费带宽多次加载相同的权重([Tianqi
    et al. 2018](ch058.xhtml#ref-chen2018tvm))。
- en: The third challenge involves the accumulation of intermediate results. Since
    each element in <semantics><mrow><mi>Z</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,j]</annotation></semantics>
    requires contributions from 256 different weight-input pairs, partial sums must
    be stored and retrieved before the final value is computed. If these intermediate
    values are stored inefficiently, the system will require frequent memory accesses,
    further increasing bandwidth demands.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个挑战涉及中间结果的累积。由于<semantics><mrow><mi>Z</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,j]</annotation></semantics>中的每个元素都需要来自256个不同的权重-输入对的贡献，必须在计算最终值之前存储和检索部分和。如果这些中间值存储效率低下，系统将需要频繁的内存访问，进一步增加带宽需求。
- en: A natural way to mitigate these challenges is to leverage SIMD and SIMT execution
    models, which allow multiple values to be fetched in parallel. However, even with
    these optimizations, data movement remains a bottleneck. The issue is not just
    how quickly data is retrieved but how often it must be moved and where it is placed
    within the memory hierarchy ([Han et al. 2016](ch058.xhtml#ref-han2016eie)).
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这些挑战的一种自然方式是利用SIMD和SIMT执行模型，这些模型允许并行获取多个值。然而，即使有这些优化，数据移动仍然是一个瓶颈。问题不仅在于数据检索的速度，还在于数据必须移动的频率以及它在内存层次结构中的位置([Han
    et al. 2016](ch058.xhtml#ref-han2016eie))。
- en: 'Given that data movement is 100-1000× more expensive than computation, the
    single most important goal of an accelerator is to minimize memory access. Dataflow
    strategies are the architectural patterns designed to achieve this by maximizing
    data reuse. The question is: which data is most valuable to keep local? This directly
    addresses the AI Memory Wall challenge examined in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    where the extreme energy penalty for memory access dominates system performance.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据移动比计算贵100-1000倍，加速器的最重要的目标就是最小化内存访问。数据流策略是设计用来通过最大化数据重用来实现这一目标的架构模式。问题是：哪种数据最值得保留在本地？这直接针对了[第11.4.1节](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9)中考察的AI内存墙挑战，其中内存访问的极端能量惩罚主导了系统性能。
- en: 'To address these constraints, accelerators implement dataflow strategies that
    determine which data remains fixed in memory and which data is streamed dynamically.
    These strategies represent different answers to the fundamental question of data
    locality: weight-stationary keeps model parameters local, input-stationary maintains
    activation data, and output-stationary preserves intermediate results. Each approach
    trades off different memory access patterns to maximize data reuse and minimize
    the energy-intensive transfers that constitute the primary bottleneck in AI acceleration.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，加速器实现数据流策略，以确定哪些数据保持在内存中，哪些数据动态流过。这些策略代表了对于数据局部性基本问题的不同回答：权重静态保持模型参数局部化，输入静态保持激活数据，输出静态保持中间结果。每种方法都权衡不同的内存访问模式，以最大化数据重用并最小化构成AI加速主要瓶颈的能量密集型传输。
- en: Weight Stationary
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重静态
- en: The Weight Stationary strategy keeps weights fixed in local memory, while input
    activations and partial sums are streamed through the system. Weight stationary
    approaches prove particularly beneficial in CNNs and matrix multiplications, where
    the same set of weights is applied across multiple inputs. By ensuring weights
    remain stationary, this method reduces redundant memory fetches, which helps alleviate
    bandwidth bottlenecks and improves energy efficiency.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 权重静态策略将权重固定在本地内存中，同时输入激活和部分和通过系统流过。在CNN和矩阵乘法中，权重静态方法特别有益，因为相同的权重集应用于多个输入。通过确保权重保持静态，这种方法减少了冗余的内存访问，有助于缓解带宽瓶颈并提高能源效率。
- en: A key advantage of the weight stationary approach is that it maximizes weight
    reuse, reducing the frequency of memory accesses to external storage. Since weight
    parameters are often shared across multiple computations, keeping them in local
    memory eliminates unnecessary data movement, lowering the overall energy cost
    of computation. This makes it particularly effective for architectures where weights
    represent the dominant memory overhead, such as systolic arrays and custom accelerators
    designed for machine learning.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 权重静态方法的一个关键优势是它最大化了权重重用，减少了对外部存储的内存访问频率。由于权重参数通常在多个计算中共享，将它们保持在本地内存中消除了不必要的数据移动，降低了计算的整体能源成本。这使得它在权重代表主导内存开销的架构中特别有效，例如用于机器学习的阵列和定制加速器。
- en: A simplified Weight Stationary implementation for matrix multiplication is illustrated
    in [Listing 11.19](ch017.xhtml#lst-weight_stationary).
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的矩阵乘法权重静态实现示于[列表11.19](ch017.xhtml#lst-weight_stationary)。
- en: 'Listing 11.19: **Weight Stationary Matrix Multiplication**: Weight stationary
    matrix multiplication keeps weights fixed in local memory while input activations
    stream through, demonstrating how it maximizes weight reuse to reduce energy costs.'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.19：**权重静态矩阵乘法**：权重静态矩阵乘法将权重固定在本地内存中，同时输入激活流过，展示了如何最大化权重重用以降低能耗。
- en: '[PRE18]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In weight stationary execution, weights are loaded once into local memory and
    remain fixed throughout the computation, while inputs are streamed dynamically,
    thereby reducing redundant memory accesses. At the same time, partial sums are
    accumulated in an efficient manner that minimizes unnecessary data movement, ensuring
    that the system maintains high throughput and energy efficiency.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重静态执行中，权重一次性加载到本地内存中，并在整个计算过程中保持固定，而输入动态流过，从而减少了冗余的内存访问。同时，部分和在一种有效的方式中累积，以最小化不必要的数据移动，确保系统保持高吞吐量和能源效率。
- en: By keeping weights fixed in local storage, memory bandwidth requirements are
    significantly reduced, as weights do not need to be reloaded for each new computation.
    Instead, the system efficiently reuses the stored weights across multiple input
    activations, allowing for high throughput execution. This makes weight stationary
    dataflow highly effective for workloads with heavy weight reuse patterns, such
    as CNNs and matrix multiplications.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在本地存储中保持权重固定，显著降低了内存带宽需求，因为对于每次新的计算，不需要重新加载权重。相反，系统高效地重复使用存储的权重，跨越多个输入激活，从而实现高吞吐量执行。这使得权重静态数据流对于具有大量权重重用模式的负载，如CNN和矩阵乘法，非常有效。
- en: However, while this strategy reduces weight-related memory traffic, it introduces
    trade-offs in input and output movement. Since inputs must be streamed dynamically
    while weights remain fixed, the efficiency of this approach depends on how well
    input activations can be delivered to the computational units without causing
    stalls. Additionally, partial sums, which represent intermediate results, must
    be carefully accumulated to avoid excessive memory traffic. The total performance
    gain depends on the size of available on-chip memory, as storing larger weight
    matrices locally can become a constraint in models with millions or billions of
    parameters.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管这种策略减少了与权重相关的内存流量，但它也在输入和输出移动方面引入了权衡。由于输入必须在权重保持固定的情况下动态流过，因此这种方法的有效性取决于输入激活能否有效地传递到计算单元而不会造成停滞。此外，代表中间结果的局部和必须谨慎累积以避免过度的内存流量。总性能提升取决于可用的片上内存大小，因为对于具有数百万或数十亿参数的模型，在本地存储较大的权重矩阵可能会成为限制因素。
- en: The weight stationary strategy is well-suited for workloads where weights exhibit
    high reuse and memory bandwidth is a limiting factor. It is commonly employed
    in CNNs, systolic arrays, and matrix multiplication kernels, where structured
    weight reuse leads to significant performance improvements. However, for models
    where input or output reuse is more critical, alternative dataflow strategies,
    such as output stationary or input stationary, may provide better trade-offs.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 权重站式策略非常适合于权重具有高重用率和内存带宽是限制因素的工作负载。它通常用于卷积神经网络（CNN）、收缩阵列和矩阵乘法内核，其中结构化权重重用导致性能显著提升。然而，对于输入或输出重用更为关键的模式，可能需要采用其他数据流策略，如输出站式或输入站式，以提供更好的权衡。
- en: Output Stationary
  id: totrans-605
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输出站式
- en: The Output Stationary strategy keeps partial sums fixed in local memory, while
    weights and input activations stream through the system. This approach is particularly
    effective for fully connected layers, systolic arrays, and other operations where
    an output element accumulates contributions from multiple weight-input pairs.
    By keeping partial sums stationary, this method reduces redundant memory writes,
    minimizing bandwidth consumption and improving energy efficiency ([Y.-H. Chen
    et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 输出站式策略将部分和在局部内存中保持固定，而权重和输入激活则通过系统流动。这种方法对于全连接层、收缩阵列和其他输出元素累积来自多个权重-输入对贡献的操作特别有效。通过保持部分和固定，这种方法减少了冗余的内存写入，最小化了带宽消耗并提高了能源效率
    ([Y.-H. Chen 等人 2016](ch058.xhtml#ref-chen2016eyeriss))。
- en: A key advantage of the output stationary approach is that it optimizes accumulation
    efficiency, ensuring that each output element is computed as efficiently as possible
    before being written to memory. Unlike Weight Stationary, which prioritizes weight
    reuse, Output Stationary execution is designed to minimize memory bandwidth overhead
    caused by frequent writes of intermediate results. This makes it well-suited for
    workloads where accumulation dominates the computational pattern, such as fully
    connected layers and matrix multiplications in transformer-based models.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 输出站式方法的一个关键优势是它优化了累积效率，确保在写入内存之前，每个输出元素都以尽可能高效的方式计算。与优先考虑权重重用的权重站式不同，输出站式执行旨在最小化由频繁写入中间结果引起的内存带宽开销。这使得它非常适合于累积占主导地位的计算模式的工作负载，如全连接层和基于转换器的模型中的矩阵乘法。
- en: '[Listing 11.20](ch017.xhtml#lst-output_stationary) shows a simplified Output
    Stationary implementation for matrix multiplication.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11.20](ch017.xhtml#lst-output_stationary) 展示了矩阵乘法的简化输出站式实现。'
- en: 'Listing 11.20: **Output Stationary Execution**: Accumulates partial sums locally
    to reduce memory writes and enhance efficiency during matrix multiplication, making
    it ideal for transformer-based models.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.20：**输出站式执行**：在矩阵乘法过程中，局部累积部分和以减少内存写入并提高效率，这使得它非常适合基于转换器的模型。
- en: '[PRE19]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This implementation follows the core principles of output stationary execution:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现遵循输出站式执行的核心原则：
- en: Partial sums are kept in local memory throughout the computation.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个计算过程中，部分和保持在局部内存中。
- en: Weights and inputs are streamed dynamically, ensuring that intermediate results
    remain locally accessible.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和输入动态流过，确保中间结果保持局部可访问。
- en: Final outputs are written back to memory only once, reducing unnecessary memory
    traffic.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终输出只写入内存一次，减少了不必要的内存流量。
- en: By accumulating partial sums locally, this approach eliminates excessive memory
    writes, improving overall system efficiency. In architectures such as systolic
    arrays, where computation progresses through a grid of processing elements, keeping
    partial sums stationary aligns naturally with structured accumulation workflows,
    reducing synchronization overhead.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在本地累积部分和，这种方法消除了过多的内存写入，提高了整体系统效率。在如收缩阵列等架构中，计算通过一个处理元素网格进行，保持部分和固定自然地与结构化累积工作流程相一致，减少了同步开销。
- en: However, while Output Stationary reduces memory write traffic, it introduces
    trade-offs in weight and input movement. Since weights and activations must be
    streamed dynamically, the efficiency of this approach depends on how well data
    can be fed into the system without causing stalls. Additionally, parallel implementations
    must carefully synchronize updates to partial sums, especially in architectures
    where multiple processing elements contribute to the same output.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管输出固定减少了内存写入流量，但它引入了权重和输入移动的权衡。由于权重和激活必须动态流过，这种方法的有效性取决于数据如何被有效地输入系统而不会造成停滞。此外，并行实现必须仔细同步部分和的更新，特别是在多个处理元素贡献相同输出的架构中。
- en: The Output Stationary strategy is most effective for workloads where accumulation
    is the dominant operation and minimizing intermediate memory writes is critical.
    It is commonly employed in fully connected layers, attention mechanisms, and systolic
    arrays, where structured accumulation leads to significant performance improvements.
    However, for models where input reuse is more critical, alternative dataflow strategies,
    such as Input Stationary, may provide better trade-offs.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 输出固定策略对于以累积为主要操作且最小化中间内存写入至关重要的工作负载最为有效。它通常用于全连接层、注意力机制和收缩阵列，其中结构化累积导致显著的性能提升。然而，对于输入重用更为关键的模型，如输入固定等替代数据流策略，可能提供更好的权衡。
- en: Input Stationary
  id: totrans-618
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入固定
- en: The Input Stationary strategy keeps input activations fixed in local memory,
    while weights and partial sums stream through the system. This approach is particularly
    effective for batch processing, transformer models, and sequence-based architectures,
    where input activations are reused across multiple computations. By ensuring that
    activations remain in local memory, this method reduces redundant input fetches,
    improving data locality and minimizing memory traffic.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 输入固定策略将输入激活固定在本地内存中，而权重和部分和则通过系统流过。这种方法特别适用于批量处理、转换器模型和基于序列的架构，在这些架构中，输入激活在多个计算中被重用。通过确保激活保持在本地内存中，这种方法减少了冗余的输入检索，提高了数据局部性并最小化了内存流量。
- en: A key advantage of the Input Stationary approach is that it maximizes input
    reuse, reducing the frequency of memory accesses for activations. Since many models,
    especially those in NLP and recommendation systems, process the same input data
    across multiple computations, keeping inputs stationary eliminates unnecessary
    memory transfers, thereby lowering energy consumption. This strategy is particularly
    useful when dealing with large batch sizes, where a single batch of input activations
    contributes to multiple weight transformations.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 输入固定方法的一个关键优势是它最大化了输入重用，减少了激活的内存访问频率。由于许多模型，尤其是自然语言处理和推荐系统中的模型，在多个计算中处理相同的输入数据，保持输入固定消除了不必要的内存传输，从而降低了能耗。这种策略在处理大型批量数据时特别有用，其中单个输入激活批次对多个权重转换做出贡献。
- en: A simplified Input Stationary implementation for matrix multiplication is illustrated
    in [Listing 11.21](ch017.xhtml#lst-input_stationary).
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11.21](ch017.xhtml#lst-input_stationary) 展示了一个简化的矩阵乘法输入固定实现。'
- en: 'Listing 11.21: **Input Stationary**: This approach keeps input activations
    stationary while dynamically streaming weights to maximize memory reuse and reduce
    energy consumption.'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.21：**输入固定**：这种方法在动态流过权重的同时保持输入激活固定，以最大化内存重用并减少能耗。
- en: '[PRE20]'
  id: totrans-623
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This implementation follows the core principles of input stationary execution:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现遵循输入固定执行的核心原则：
- en: Input activations are loaded into local memory and remain fixed during computation.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入激活被加载到本地内存中，并在计算过程中保持固定。
- en: '**Weights are streamed dynamically**, ensuring efficient application across
    multiple inputs.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重被动态流过**，确保了在多个输入上的高效应用。'
- en: '**Partial sums are accumulated and written out**, optimizing memory bandwidth
    usage.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分和被累积并写入**，优化了内存带宽使用。'
- en: By keeping input activations stationary, this strategy minimizes redundant memory
    accesses to input data, significantly reducing external memory bandwidth requirements.
    This is particularly beneficial in transformer architectures, where each token
    in an input sequence is used across multiple attention heads and layers. Additionally,
    in batch processing scenarios, keeping input activations in local memory improves
    data locality, making it well-suited for fully connected layers and matrix multiplications.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持输入激活固定，这种策略最小化了输入数据的冗余内存访问，显著降低了外部内存带宽需求。这在变压器架构中尤其有益，其中输入序列中的每个标记在多个注意力头和层中使用。此外，在批量处理场景中，保持输入激活在本地内存中可以提高数据局部性，使其非常适合全连接层和矩阵乘法。
- en: However, while Input Stationary reduces memory traffic for activations, it introduces
    trade-offs in weight and output movement. Since weights must be streamed dynamically
    while inputs remain fixed, the efficiency of this approach depends on how well
    weights can be delivered to the computational units without causing stalls. Additionally,
    partial sums must be accumulated efficiently before being written back to memory,
    which may require additional buffering mechanisms.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管输入固定可以减少激活的内存流量，但它引入了权重和输出移动的权衡。由于权重必须在输入保持固定的情况下动态流式传输，因此这种方法的有效性取决于权重能否有效地传递到计算单元而不会造成停滞。此外，在将部分和写回内存之前，必须有效地累积，这可能需要额外的缓冲机制。
- en: The Input Stationary strategy is most effective for workloads where input activations
    exhibit high reuse, and memory bandwidth for inputs is a critical constraint.
    It is commonly employed in transformers, recurrent networks, and batch processing
    workloads, where structured input reuse leads to significant performance improvements.
    However, for models where output accumulation is more critical, alternative dataflow
    strategies, such as Output Stationary, may provide better trade-offs.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 输入固定策略对于输入激活具有高重用率的工作负载最为有效，并且输入的内存带宽是一个关键约束。它通常用于变压器、循环网络和批量处理工作负载，其中结构化输入重用导致显著的性能提升。然而，对于输出累积更为重要的模型，可能需要采用其他数据流策略，如输出固定，以提供更好的权衡。
- en: Memory-Efficient Tensor Layouts
  id: totrans-631
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存高效的张量布局
- en: Efficient execution of machine learning workloads depends not only on how data
    moves (dataflow strategies) but also on how data is stored and accessed in memory.
    Tensor layouts, which refers to the arrangement of multidimensional data in memory,
    can significantly impact memory access efficiency, cache performance, and computational
    throughput. Poorly chosen layouts can lead to excessive memory stalls, inefficient
    cache usage, and increased data movement costs.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载的有效执行不仅取决于数据如何移动（数据流策略），还取决于数据如何在内存中存储和访问。张量布局，指的是多维数据在内存中的排列，可以显著影响内存访问效率、缓存性能和计算吞吐量。选择不当的布局可能导致过多的内存停滞、不高效的缓存使用和增加的数据移动成本。
- en: In AI accelerators, tensor layout optimization is particularly important because
    data is frequently accessed in patterns dictated by the underlying hardware architecture.
    Choosing the right layout ensures that memory accesses align with hardware-friendly
    access patterns, minimizing overhead from costly memory transactions ([C. NVIDIA
    2025](ch058.xhtml#ref-nvidia2021cudnn)).
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能加速器中，张量布局优化尤为重要，因为数据通常按照底层硬件架构指定的模式频繁访问。选择正确的布局确保内存访问与硬件友好的访问模式对齐，从而最小化昂贵的内存事务开销
    ([C. NVIDIA 2025](ch058.xhtml#ref-nvidia2021cudnn))。
- en: While developers can sometimes manually specify tensor layouts, the choice is
    often determined automatically by machine learning frameworks (e.g., TensorFlow,
    PyTorch, JAX), compilers, or AI accelerator runtimes. Low-level optimization tools
    such as cuDNN (for NVIDIA GPUs), XLA (for TPUs), and MLIR (for custom accelerators)
    may rearrange tensor layouts dynamically to optimize performance ([X. He 2023a](ch058.xhtml#ref-xla2020)).
    In high-level frameworks, layout transformations are typically applied transparently,
    but developers working with custom kernels or low-level libraries (e.g., CUDA,
    Metal, or OpenCL) may have direct control over tensor format selection.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然开发者有时可以手动指定张量布局，但选择通常由机器学习框架（例如，TensorFlow、PyTorch、JAX）、编译器或AI加速器运行时自动确定。低级优化工具，如用于NVIDIA
    GPU的cuDNN、用于TPU的XLA和用于自定义加速器的MLIR，可能会动态地重新排列张量布局以优化性能([X. He 2023a](ch058.xhtml#ref-xla2020))。在高层框架中，布局转换通常透明地应用，但与自定义内核或低级库（例如，CUDA、Metal或OpenCL）一起工作的开发者可能对张量格式选择有直接控制权。
- en: For example, in PyTorch, users can manually modify layouts using tensor.permute()
    or tensor.contiguous() to ensure efficient memory access ([Paszke et al. 2019](ch058.xhtml#ref-paszke2019pytorch)).
    In TensorFlow, layout optimizations are often applied internally by the XLA compiler,
    choosing between NHWC (row-major) and NCHW (channel-major) based on the target
    hardware ([Brain 2022](ch058.xhtml#ref-tensorflow2022)). Hardware-aware machine
    learning libraries, such as cuDNN for GPUs or OneDNN for CPUs, enforce specific
    memory layouts to maximize cache locality and SIMD efficiency. Ultimately, while
    developers may have some control over tensor layout selection, most layout decisions
    are driven by the compiler and runtime system, ensuring that tensors are stored
    in memory in a way that best suits the underlying hardware.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在PyTorch中，用户可以使用tensor.permute()或tensor.contiguous()手动修改布局，以确保高效的内存访问([Paszke等人
    2019](ch058.xhtml#ref-paszke2019pytorch))。在TensorFlow中，布局优化通常由XLA编译器内部应用，根据目标硬件在NHWC（行主序）和NCHW（通道主序）之间进行选择([Brain
    2022](ch058.xhtml#ref-tensorflow2022))。硬件感知的机器学习库，如用于GPU的cuDNN或用于CPU的OneDNN，强制执行特定的内存布局以最大化缓存局部性和SIMD效率。最终，尽管开发者可能对张量布局选择有一些控制权，但大多数布局决策是由编译器和运行时系统驱动的，确保张量以最适合底层硬件的方式存储在内存中。
- en: Row-Major Layout
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 行主序布局
- en: Row-major layout refers to the way multi-dimensional tensors are stored in memory,
    where elements are arranged row by row, ensuring that all values in a given row
    are placed contiguously before moving to the next row. This storage format is
    widely used in general-purpose CPUs and some machine learning frameworks because
    it aligns naturally with sequential memory access patterns, making it more cache-efficient
    for certain types of operations ([Intel 2021](ch058.xhtml#ref-oneDNN2021)).
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 行主序布局指的是多维张量在内存中的存储方式，其中元素按行排列，确保给定行的所有值在移动到下一行之前连续放置。这种存储格式在通用CPU和一些机器学习框架中广泛使用，因为它与顺序内存访问模式自然对齐，使得某些类型的操作更加缓存高效([Intel
    2021](ch058.xhtml#ref-oneDNN2021))。
- en: 'To understand how row-major layout works, consider a single RGB image represented
    as a tensor of shape (Height, Width, Channels). If the image has a size of <semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> pixels with 3
    channels (RGB), the corresponding tensor is structured as (3, 3, 3). The values
    are stored in memory as follows: <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0),
    I(0,1,1), \\ I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots \end{gather*}</annotation></semantics>'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '要理解行主序布局是如何工作的，可以考虑一个表示为形状为（高度，宽度，通道）的张量的单个RGB图像。如果图像的大小为<semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>像素，并且有3个通道（RGB），则相应的张量结构为（3，3，3）。值在内存中的存储方式如下：<semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0),
    I(0,1,1), \\ I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots \end{gather*}</annotation></semantics>'
- en: Each row is stored contiguously, meaning all pixel values in the first row are
    placed sequentially in memory before moving on to the second row. This ordering
    is advantageous because CPUs and cache hierarchies are optimized for sequential
    memory access. When data is accessed in a row-wise fashion, such as when applying
    element-wise operations like activation functions or basic arithmetic transformations,
    memory fetches are efficient, and cache utilization is maximized ([Sodani 2015](ch058.xhtml#ref-sodani2017knl)).
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行都是连续存储的，这意味着第一行中的所有像素值在内存中按顺序放置，然后再移动到第二行。这种排序是有利的，因为CPU和缓存层次结构是针对顺序内存访问进行优化的。当以行方式访问数据时，例如在应用逐元素操作（如激活函数或基本算术变换）时，内存读取是高效的，并且缓存利用率最大化
    ([Sodani 2015](ch058.xhtml#ref-sodani2017knl))。
- en: The efficiency of row-major storage becomes particularly evident in CPU-based
    machine learning workloads, where operations such as batch normalization, matrix
    multiplications, and element-wise arithmetic frequently process rows of data sequentially.
    Since modern CPUs employ cache prefetching mechanisms, a row-major layout allows
    the next required data values to be preloaded into cache ahead of execution, reducing
    memory latency and improving overall computational throughput.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 行主存储的效率在基于CPU的机器学习工作负载中表现得尤为明显，在这些工作负载中，批归一化、矩阵乘法和逐元素算术等操作通常按顺序处理数据行。由于现代CPU采用缓存预取机制，行主布局允许在执行之前将下一个所需的数据值预加载到缓存中，从而降低内存延迟并提高整体计算吞吐量。
- en: However, row-major layout can introduce inefficiencies when performing operations
    that require accessing data across channels rather than across rows. Consider
    a convolutional layer that applies a filter across multiple channels of an input
    image. Since channel values are interleaved in row-major storage, the convolution
    operation must jump across memory locations to fetch all the necessary channel
    values for a given pixel. These strided memory accesses can be costly on hardware
    architectures that rely on vectorized execution and coalesced memory access, such
    as GPUs and TPUs.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当执行需要跨通道而不是跨行访问数据的操作时，行主布局可能会引入低效。考虑一个卷积层，它在输入图像的多个通道上应用过滤器。由于通道值在行主存储中是交错存储的，卷积操作必须跳转多个内存位置来获取给定像素的所有必要的通道值。这些跨步内存访问在依赖于矢量化执行和内存归约的硬件架构上可能代价高昂，例如GPU和TPU。
- en: Despite these limitations, row-major layout remains a dominant storage format
    in CPU-based machine learning frameworks. TensorFlow, for instance, defaults to
    the NHWC (row-major) format on CPUs, ensuring that cache locality is optimized
    for sequential processing. However, when targeting GPUs, frameworks often rearrange
    data dynamically to take advantage of more efficient memory layouts, such as channel-major
    storage, which aligns better with parallelized computation.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，行主布局仍然是基于CPU的机器学习框架中的主导存储格式。例如，TensorFlow在CPU上默认使用NHWC（行主）格式，确保了缓存局部性优化以适应顺序处理。然而，当针对GPU时，框架通常会动态重新排列数据，以利用更有效的内存布局，如通道主存储，这更好地与并行计算相匹配。
- en: Channel-Major Layout
  id: totrans-643
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通道主布局
- en: In contrast to row-major layout, channel-major layout arranges data in memory
    such that all values for a given channel are stored together before moving to
    the next channel. This format is particularly beneficial for GPUs, TPUs, and other
    AI accelerators, where vectorized operations and memory coalescing significantly
    impact computational efficiency.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 与行主布局相反，通道主布局在内存中排列数据，使得给定通道的所有值都存储在一起，然后再移动到下一个通道。这种格式对于GPU、TPU和其他AI加速器特别有益，在这些设备上，矢量化操作和内存归约对计算效率有显著影响。
- en: 'To understand how channel-major layout works, consider the same RGB image tensor
    of size (Height, Width, Channels) = (3, 3, 3). Instead of storing pixel values
    row by row, the data is structured channel-first in memory as follows: <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><mi>I</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0),
    I(1,1,0), I(2,1,0), \ldots, \\ I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2),
    I(1,0,2), I(2,0,2), \ldots \end{gather*}</annotation></semantics>'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '要理解通道主布局是如何工作的，可以考虑一个大小为 (高度, 宽度, 通道) = (3, 3, 3) 的相同的 RGB 图像张量。不同于按行存储像素值，数据在内存中按通道优先的结构组织如下：<semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><mi>I</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0),
    I(1,1,0), I(2,1,0), \ldots, \\ I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2),
    I(1,0,2), I(2,0,2), \ldots \end{gather*}</annotation></semantics>'
- en: In this format, all red channel values for the entire image are stored first,
    followed by all green values, and then all blue values. This ordering allows hardware
    accelerators to efficiently load and process data across channels in parallel,
    which is crucial for convolution operations and SIMD (Single Instruction, Multiple
    Data) execution models ([Chetlur et al. 2014](ch058.xhtml#ref-chetlur2014cudnn)).
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种格式中，整个图像的所有红色通道值首先存储，然后是所有绿色值，最后是所有蓝色值。这种排序允许硬件加速器有效地并行加载和处理通道中的数据，这对于卷积操作和
    SIMD（单指令，多数据）执行模型至关重要([Chetlur 等人 2014](ch058.xhtml#ref-chetlur2014cudnn))。
- en: The advantage of channel-major layout becomes clear when performing convolutions
    in machine learning models. Convolutional layers process images by applying a
    shared set of filters across all channels. When the data is stored in a channel-major
    format, a convolution kernel can load an entire channel efficiently, reducing
    the number of scattered memory fetches. This reduces memory latency, improves
    throughput, and enhances data locality for matrix multiplications, which are fundamental
    to machine learning workloads.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 当在机器学习模型中执行卷积时，通道主序布局的优势变得明显。卷积层通过在所有通道上应用共享的过滤器集来处理图像。当数据以通道主序格式存储时，卷积核可以有效地加载整个通道，减少分散的内存读取次数。这减少了内存延迟，提高了吞吐量，并增强了矩阵乘法的数据局部性，这对于机器学习工作负载至关重要。
- en: Because GPUs and TPUs rely on memory coalescing[27](#fn27), a technique in which
    consecutive threads fetch contiguous memory addresses, channel-major layout aligns
    naturally with the way these processors execute parallel computations. For example,
    in NVIDIA GPUs, each thread in a warp (a group of threads executed simultaneously)
    processes different elements of the same channel, ensuring that memory accesses
    are efficient and reducing the likelihood of strided memory accesses, which can
    degrade performance.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPU 和 TPU 依赖于内存归约[27](#fn27)，这是一种连续线程获取连续内存地址的技术，通道主序布局自然地与这些处理器执行并行计算的方式相吻合。例如，在
    NVIDIA GPU 中，每个 warp（同时执行的线程组）中的每个线程处理同一通道的不同元素，确保内存访问高效，并减少步进内存访问的可能性，这可能会降低性能。
- en: Despite its advantages in machine learning accelerators, channel-major layout
    can introduce inefficiencies when running on general-purpose CPUs. Since CPUs
    optimize for sequential memory access, storing all values for a single channel
    before moving to the next disrupts cache locality for row-wise operations. This
    is why many machine learning frameworks (e.g., TensorFlow, PyTorch) default to
    row-major (NHWC) on CPUs and channel-major (NCHW) on GPUs—optimizing for the strengths
    of each hardware type.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在机器学习加速器中具有优势，但通道主序布局在通用 CPU 上运行时可能会引入低效。由于 CPU 优化的是顺序内存访问，因此在移动到下一个通道之前存储单个通道的所有值会破坏按行操作的缓存局部性。这就是为什么许多机器学习框架（例如
    TensorFlow、PyTorch）在 CPU 上默认使用行主序（NHWC），在 GPU 上使用通道主序（NCHW）——优化每种硬件类型的优势。
- en: Modern AI frameworks and compilers often transform tensor layouts dynamically
    depending on the execution environment. For instance, TensorFlow and PyTorch automatically
    switch between NHWC[28](#fn28) and NCHW based on whether a model is running on
    a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most efficient
    execution path.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能框架和编译器通常会根据执行环境动态地转换张量布局。例如，TensorFlow 和 PyTorch 会根据模型是在 CPU、GPU 还是 TPU
    上运行自动在 NHWC[28](#fn28) 和 NCHW 之间切换，确保内存布局与最有效的执行路径相匹配。
- en: Comparing Row-Major and Channel-Major Layouts
  id: totrans-651
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 比较行主序和通道主序布局
- en: Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct purposes
    in machine learning workloads, with their efficiency largely determined by the
    hardware architecture, memory access patterns, and computational requirements.
    The choice of layout directly influences cache utilization, memory bandwidth efficiency,
    and processing throughput. [Table 11.14](ch017.xhtml#tbl-major) summarizes the
    differences between row-major (NHWC) and channel-major (NCHW) layouts in terms
    of performance trade-offs and hardware compatibility.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 行主序（NHWC）和通道主序（NCHW）布局在机器学习工作负载中具有不同的用途，其效率很大程度上取决于硬件架构、内存访问模式和计算需求。布局的选择直接影响缓存利用率、内存带宽效率和处理吞吐量。[表
    11.14](ch017.xhtml#tbl-major) 总结了行主序（NHWC）和通道主序（NCHW）布局在性能权衡和硬件兼容性方面的差异。
- en: 'Table 11.14: **Data Layout Strategies**: Row-major (NHWC) and channel-major
    (NCHW) layouts optimize memory access patterns for different hardware architectures;
    NHWC suits cpus and element-wise operations, while NCHW accelerates GPU and TPU-based
    convolution operations. Choosing the appropriate layout significantly impacts
    performance by maximizing cache utilization and memory bandwidth efficiency.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.14：**数据布局策略**：行主序（NHWC）和通道主序（NCHW）布局优化了不同硬件架构的内存访问模式；NHWC适用于CPU和逐元素操作，而NCHW加速了基于GPU和TPU的卷积操作。选择合适的布局通过最大化缓存利用率和内存带宽效率，对性能产生重大影响。
- en: '| **Feature** | **Row-Major (NHWC)** | **Channel-Major (NCHW)** |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **行主序（NHWC）** | **通道主序（NCHW）** |'
- en: '| --- | --- | --- |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Memory Storage Order** | Pixels are stored row-by-row, channel interleaved
    | All values for a given channel are stored together first |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| **内存存储顺序** | 像素按行存储，通道交错 | 给定通道的所有值首先存储在一起 |'
- en: '| **Best for** | CPUs, element-wise operations | GPUs, TPUs, convolution operations
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| **最佳适用** | CPU，逐元素操作 | GPU，TPU，卷积操作 |'
- en: '| **Cache Efficiency** | High cache locality for sequential row access | Optimized
    for memory coalescing across channels |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| **缓存效率** | 高缓存局部性，适用于顺序行访问 | 优化跨通道的内存合并 |'
- en: '| **Convolution Performance** | Requires strided memory accesses (inefficient
    on GPUs) | Efficient for GPU convolution kernels |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| **卷积性能** | 需要步进内存访问（在GPU上效率低下） | 适用于GPU卷积内核 |'
- en: '| **Memory Fetching** | Good for operations that process rows sequentially
    | Optimized for SIMD execution across channels |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| **内存获取** | 适用于按顺序处理行的操作 | 优化跨通道的SIMD执行 |'
- en: '| **Default in Frameworks** | Default on CPUs (e.g., TensorFlow NHWC) | Default
    on GPUs (e.g., cuDNN prefers NCHW) |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| **框架中的默认值** | CPU上的默认值（例如，TensorFlow NHWC） | GPU上的默认值（例如，cuDNN偏好NCHW） |'
- en: The decision to use row-major (NHWC) or channel-major (NCHW) layouts is not
    always made manually by developers. Instead, machine learning frameworks and AI
    compilers often determine the optimal layout dynamically based on the target hardware
    and operation type. CPUs tend to favor NHWC due to cache-friendly sequential memory
    access, while GPUs perform better with NCHW, which reduces memory fetch overhead
    for machine learning computations.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 使用行主序（NHWC）或通道主序（NCHW）布局的决定并不总是由开发者手动做出。相反，机器学习框架和AI编译器通常根据目标硬件和操作类型动态确定最佳布局。由于CPU倾向于缓存友好的顺序内存访问，因此它们更偏好NHWC，而GPU在NCHW上表现更好，这减少了机器学习计算中的内存获取开销。
- en: In practice, modern AI compilers such as TensorFlow’s XLA and PyTorch’s TorchScript
    perform automatic layout transformations, converting tensors between NHWC and
    NCHW as needed to optimize performance across different processing units. This
    ensures that machine learning models achieve the highest possible throughput without
    requiring developers to manually specify tensor layouts.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，现代AI编译器，如TensorFlow的XLA和PyTorch的TorchScript，执行自动布局转换，根据需要将张量在NHWC和NCHW之间转换，以优化不同处理单元的性能。这确保了机器学习模型在不要求开发者手动指定张量布局的情况下，达到最高的吞吐量。
- en: Kernel Fusion
  id: totrans-664
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内核融合
- en: One of the most impactful optimization techniques in AI acceleration involves
    reducing the overhead of intermediate data movement between operations. This section
    examines how kernel fusion transforms multiple separate computations into unified
    operations, dramatically improving memory efficiency and execution performance.
    We first analyze the memory bottlenecks created by intermediate writes, then explore
    how fusion techniques eliminate these inefficiencies.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: AI加速中最有影响力的优化技术之一是减少操作之间中间数据移动的开销。本节探讨了如何通过内核融合将多个独立的计算转换为统一操作，从而显著提高内存效率和执行性能。我们首先分析由中间写入造成的内存瓶颈，然后探讨融合技术如何消除这些低效性。
- en: Intermediate Memory Write
  id: totrans-666
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中间内存写入
- en: Optimizing memory access is a fundamental challenge in AI acceleration. While
    AI models rely on high-throughput computation, their performance is often constrained
    by memory bandwidth and intermediate memory writes rather than pure arithmetic
    operations. Every time an operation produces an intermediate result that must
    be written to memory and later read back, execution stalls occur due to data movement
    overhead.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 优化内存访问是AI加速中的一个基本挑战。虽然AI模型依赖于高吞吐量计算，但它们的性能通常受限于内存带宽和中间内存写入，而不是纯算术操作。每当一个操作产生必须写入内存并在稍后读回的中间结果时，由于数据移动开销，执行就会停滞。
- en: Building on software optimization techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    and memory bandwidth constraints established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    kernel fusion represents the critical bridge between software optimization and
    hardware acceleration. Many AI workloads introduce unnecessary intermediate memory
    writes, leading to increased memory bandwidth consumption and reduced execution
    efficiency ([Ye et al. 2025](ch058.xhtml#ref-nvidia2017gpu)).
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 [第 10 章](ch016.xhtml#sec-model-optimizations) 中的软件优化技术以及 [第 11.4.1 节](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9)
    中建立的内存带宽约束，内核融合代表了软件优化和硬件加速之间的关键桥梁。许多 AI 工作负载引入了不必要的中间内存写入，导致内存带宽消耗增加和执行效率降低 ([Ye
    等人 2025](ch058.xhtml#ref-nvidia2017gpu))。
- en: '[Listing 11.22](ch017.xhtml#lst-naive_execution) illustrates a naïve execution
    model in which each operation is treated as a separate kernel, meaning that each
    intermediate result is written to memory and then read back for the next operation.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11.22](ch017.xhtml#lst-naive_execution) 展示了一个简单的执行模型，其中每个操作都被视为一个独立的内核，这意味着每个中间结果都会写入内存，然后在下一个操作中读取回来。'
- en: 'Listing 11.22: **Naïve Execution**: Each step writes intermediate results to
    memory before processing the next, leading to increased bandwidth usage and reduced
    efficiency. *Source: NVIDIA GPU Technology Conference 2017*[nvidia2017gpu]'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.22：**简单执行**：每个步骤在处理下一个操作之前将中间结果写入内存，导致带宽使用增加和效率降低。*来源：NVIDIA GPU 技术大会
    2017*[nvidia2017gpu]
- en: '[PRE21]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Each operation produces an intermediate tensor that must be written to memory
    and retrieved for the next operation. On large tensors, this overhead of moving
    data can outweigh the computational cost of the operations ([Shazeer et al. 2018](ch058.xhtml#ref-shazeer2018mesh)).
    [Table 11.15](ch017.xhtml#tbl-memory-footprint) illustrates the memory overhead
    in a naïve execution model. While only the final result <semantics><mi>Y</mi><annotation
    encoding="application/x-tex">Y</annotation></semantics> is needed, storing multiple
    intermediate tensors creates unnecessary memory traffic and inefficient memory
    usage. This data movement bottleneck significantly impacts performance, making
    memory optimization crucial for AI accelerators.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作都会生成一个中间张量，必须将其写入内存并在下一个操作中检索。在大型张量上，这种数据移动的开销可能会超过操作的计算成本 ([Shazeer 等人
    2018](ch058.xhtml#ref-shazeer2018mesh))。[表 11.15](ch017.xhtml#tbl-memory-footprint)
    展示了简单执行模型中的内存开销。虽然只需要最终的输出 <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>，但存储多个中间张量会创建不必要的内存流量和低效的内存使用。这种数据移动瓶颈会显著影响性能，使得内存优化对于
    AI 加速器至关重要。
- en: 'Table 11.15: **Intermediate Tensor Storage**: Naïve execution models require
    substantial memory to store intermediate tensors generated by each operation;
    for a 1024x1024 tensor, this table shows that storing these intermediate results—even
    if only the final output is needed—quadruples the total memory footprint from
    4 MB to 16 MB. Minimizing this intermediate data storage is crucial for improving
    memory efficiency and accelerating AI computations.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.15：**中间张量存储**：简单的执行模型需要大量内存来存储每个操作生成的中间张量；对于 1024x1024 的张量，此表显示即使只需要最终的输出，存储这些中间结果也会将总内存占用从
    4 MB 增加到 16 MB。最小化这种中间数据存储对于提高内存效率和加速 AI 计算至关重要。
- en: '| **Tensor** | **Size (MB) for 1024 <semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    1024 Tensor** |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| **张量** | **1024 <semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    1024 张量的大小 (MB)** |'
- en: '| --- | --- |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **X** | 4 MB |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| **X** | 4 MB |'
- en: '| **X’** | 4 MB |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| **X’** | 4 MB |'
- en: '| **X’’** | 4 MB |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| **X’’** | 4 MB |'
- en: '| **Y** | 4 MB |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| **Y** | 4 MB |'
- en: '| **Total Memory** | 16 MB |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| **总内存** | 16 MB |'
- en: Even though only the final result <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>
    is needed, three additional intermediate tensors consume extra memory without
    contributing to final output storage. This excessive memory usage limits scalability
    and wastes memory bandwidth, particularly in AI accelerators where minimizing
    data movement is critical.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管只需要最终的输出 <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>，但三个额外的中间张量消耗了额外的内存，而没有对最终的输出存储做出贡献。这种过度的内存使用限制了可扩展性，浪费了内存带宽，尤其是在最小化数据移动至关重要的
    AI 加速器中。
- en: Kernel Fusion for Memory Efficiency
  id: totrans-682
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存效率内核融合
- en: Kernel fusion is a key optimization technique that aims to minimize intermediate
    memory writes, reducing the memory footprint and bandwidth consumption of machine
    learning workloads ([Zhihao Jia, Zaharia, and Aiken 2018](ch058.xhtml#ref-jia2018beyond)).
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合是一种关键的优化技术，旨在最小化中间内存写入，减少机器学习工作负载的内存占用和带宽消耗 ([Zhihao Jia, Zaharia, and Aiken
    2018](ch058.xhtml#ref-jia2018beyond))。
- en: Kernel fusion involves merging multiple computation steps into a single, optimized
    operation, eliminating the need for storing and reloading intermediate tensors.
    Instead of executing each layer or element-wise operation separately, in which
    each step writes its output to memory before the next step begins, fusion enables
    direct data propagation between operations, keeping computations within high-speed
    registers or local memory.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合涉及将多个计算步骤合并成一个单一、优化的操作，消除了存储和重新加载中间张量的需求。不是单独执行每一层或逐元素操作，其中每个步骤在下一个步骤开始之前将输出写入内存，融合允许操作之间直接数据传递，保持计算在高速寄存器或局部内存中。
- en: 'A common machine learning sequence might involve applying a nonlinear activation
    function (e.g., ReLU), followed by batch normalization, and then scaling the values
    for input to the next layer. In a naïve implementation, each of these steps generates
    an intermediate tensor, which is written to memory, read back, and then modified
    again: <semantics><mrow><mi>X</mi><mi>′</mi><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>X</mi><mi>″</mi><mo>=</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mi>′</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mi>X</mi><mi>″</mi><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">X'' = \text{ReLU}(X) X'''' = \text{BatchNorm}(X'')
    Y = \alpha \cdot X'''' + \beta</annotation></semantics>'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的机器学习序列可能包括应用非线性激活函数（例如，ReLU），然后是批量归一化，最后是对下一层输入的值进行缩放。在原始实现中，这些步骤中的每一个都会生成一个中间张量，并将其写入内存，然后读取回来，并再次修改：<semantics><mrow><mi>X</mi><mi>′</mi><mo>=</mo><mtext
    mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>X</mi><mi>″</mi><mo>=</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mi>′</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mi><mi>X</mi><mi>″</mi><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">X' = \text{ReLU}(X) X'' = \text{BatchNorm}(X')
    Y = \alpha \cdot X'' + \beta</annotation></semantics>
- en: 'With kernel fusion, these operations are combined into a single computation
    step, allowing the entire transformation to occur without generating unnecessary
    intermediate tensors: <semantics><mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big)
    + \beta</annotation></semantics>'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 通过内核融合，这些操作被合并成一个单一的计算步骤，使得整个转换过程无需生成不必要的中间张量：<semantics><mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big)
    + \beta</annotation></semantics>
- en: '[Table 11.16](ch017.xhtml#tbl-fusion-benefits) highlights the impact of operation
    fusion on memory efficiency. By keeping intermediate results in registers or local
    memory rather than writing them to main memory, fusion significantly reduces memory
    traffic. This optimization is especially beneficial on highly parallel architectures
    like GPUs and TPUs, where minimizing memory accesses translates directly into
    improved execution throughput. Compared to the naïve execution model, fused execution
    eliminates the need for storing intermediate tensors, dramatically lowering the
    total memory footprint and improving overall efficiency.'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.16](ch017.xhtml#tbl-fusion-benefits)突出了操作融合对内存效率的影响。通过将中间结果保存在寄存器或局部内存中，而不是写入主内存，融合显著减少了内存流量。这种优化在高度并行的架构如GPU和TPU上特别有益，因为最小化内存访问直接转化为提高了执行吞吐量。与原始执行模型相比，融合执行消除了存储中间张量的需求，大幅降低了总内存占用并提高了整体效率。'
- en: 'Table 11.16: **Operation Fusion Benefits**: Fused execution reduces memory
    usage by eliminating the need to store intermediate tensors, directly improving
    efficiency on memory-bound hardware like gpus and tpus. This table quantifies
    the memory savings, showing a reduction from 16 MB in naïve execution to 4 MB
    with fused operations.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.16：**操作融合优势**：融合执行通过消除存储中间张量的需要，直接提高了在内存受限硬件（如GPU和TPU）上的效率。此表量化了内存节省，显示从原始执行的16
    MB减少到融合操作的4 MB。
- en: '| **Execution Model** | **Intermediate Tensors Stored** | **Total Memory Usage
    (MB)** |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| **执行模型** | **存储的中间张量** | **总内存使用（MB）** |'
- en: '| --- | --- | --- |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Naïve Execution** | X’, X’’ | 16 MB |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| **原始执行** | X’，X’’ | 16 MB |'
- en: '| **Fused Execution** | None | 4 MB |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| **融合执行** | 无 | 4 MB |'
- en: Kernel fusion reduces total memory consumption from 16 MB to 4 MB, eliminating
    redundant memory writes while improving execution efficiency.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合将总内存消耗从16 MB减少到4 MB，消除了冗余的内存写入，同时提高了执行效率。
- en: Performance Benefits and Constraints
  id: totrans-694
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 性能优势和限制
- en: Kernel fusion brings several key advantages that enhance memory efficiency and
    computation throughput. By reducing memory accesses, fused kernels ensure that
    intermediate values stay within registers instead of being repeatedly written
    to and read from memory. This significantly lowers memory traffic, which is one
    of the primary bottlenecks in machine learning workloads. GPUs and TPUs, in particular,
    benefit from kernel fusion because high-bandwidth memory is a scarce resource,
    and reducing memory transactions leads to better utilization of compute units
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere)).
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合带来了几个关键优势，这些优势增强了内存效率和计算吞吐量。通过减少内存访问，融合内核确保中间值保持在寄存器中，而不是反复写入和读取内存。这显著降低了内存流量，这是机器学习工作负载中的主要瓶颈之一。特别是，GPU和TPU从内核融合中受益，因为高带宽内存是一种稀缺资源，减少内存事务可以提高计算单元的利用率
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere))。
- en: However, not all operations can be fused. Element-wise operations, such as ReLU,
    batch normalization, and simple arithmetic transformations, are ideal candidates
    for fusion since their computations depend only on single elements from the input
    tensor. In contrast, operations with complex data dependencies, such as matrix
    multiplications and convolutions, involve global data movement, making direct
    fusion impractical. These operations require values from multiple input elements
    to compute a single output, which prevents them from being executed as a single
    fused kernel.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有操作都可以融合。元素级操作，如ReLU、批量归一化和简单的算术变换，是融合的理想候选者，因为它们的计算只依赖于输入张量的单个元素。相比之下，具有复杂数据依赖性的操作，如矩阵乘法和卷积，涉及全局数据移动，使得直接融合不切实际。这些操作需要从多个输入元素中获取值来计算单个输出，这阻止了它们作为一个单一的融合内核执行。
- en: Another major consideration is register pressure. Fusing multiple operations
    means all temporary values must be kept in registers rather than memory. While
    this eliminates redundant memory writes, it also increases register demand. If
    a fused kernel exceeds the available registers per thread, the system must spill
    excess values into shared memory, introducing additional latency and potentially
    negating the benefits of fusion. On GPUs, where thread occupancy (the number of
    threads that can run in parallel) is limited by available registers, excessive
    fusion can reduce parallelism, leading to diminishing returns.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是寄存器压力。融合多个操作意味着所有临时值都必须保存在寄存器中，而不是内存中。虽然这消除了冗余的内存写入，但也增加了寄存器需求。如果一个融合内核超过了每个线程可用的寄存器数量，系统必须将多余的值溢出到共享内存中，这引入了额外的延迟，并可能抵消融合的好处。在GPU上，由于线程占用（可以并行运行的线程数）受可用寄存器的限制，过度的融合会降低并行性，导致收益递减。
- en: Different AI accelerators and compilers handle fusion in distinct ways. NVIDIA
    GPUs, for example, favor warp-level parallelism, where element-wise fusion is
    straightforward. TPUs, on the other hand, prioritize systolic array execution,
    which is optimized for matrix-matrix operations rather than element-wise fusion
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere)). AI compilers
    such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and MLIR automatically
    detect fusion opportunities and apply heuristics to balance memory savings and
    execution efficiency ([X. He 2023b](ch058.xhtml#ref-xla2021)).
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的AI加速器和编译器以不同的方式处理融合。例如，NVIDIA GPU倾向于warp级别的并行处理，其中元素级融合简单直接。另一方面，TPU优先考虑脉动阵列执行，这针对矩阵-矩阵操作进行了优化，而不是元素级融合
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere))。AI编译器如XLA
    (TensorFlow)、TorchScript (PyTorch)、TensorRT (NVIDIA) 和 MLIR自动检测融合机会，并应用启发式方法以平衡内存节省和执行效率
    ([X. He 2023b](ch058.xhtml#ref-xla2021))。
- en: Despite its advantages, fusion is not always beneficial. Some AI frameworks
    allow developers to disable fusion selectively, especially when debugging performance
    issues or making frequent model modifications. The decision to fuse operations
    must consider trade-offs between memory efficiency, register usage, and hardware
    execution constraints to ensure that fusion leads to tangible performance improvements.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管融合具有优势，但并不总是有益。一些AI框架允许开发者有选择地禁用融合，尤其是在调试性能问题或频繁修改模型时。决定是否融合操作必须考虑内存效率、寄存器使用和硬件执行约束之间的权衡，以确保融合能带来实际性能提升。
- en: Memory-Efficient Tiling Strategies
  id: totrans-700
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存高效的分块策略
- en: While modern AI accelerators offer high computational throughput, their performance
    is often limited by memory bandwidth rather than raw processing power. If data
    cannot be supplied to processing units fast enough, execution stalls occur, leading
    to wasted cycles and inefficient hardware utilization.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代人工智能加速器提供了高计算吞吐量，但它们的性能通常受限于内存带宽而不是原始处理能力。如果数据不能足够快地供应给处理单元，就会发生执行停滞，导致周期浪费和硬件利用率低效。
- en: Tiling is a technique used to mitigate this issue by restructuring computations
    into smaller, memory-friendly subproblems. Instead of processing entire matrices
    or tensors at once, which leads to excessive memory traffic, tiling partitions
    computations into smaller blocks (tiles) that fit within fast local memory (e.g.,
    caches, shared memory, or registers) ([Lam, Rothberg, and Wolf 1991](ch058.xhtml#ref-lam1991cache)).
    By doing so, tiling increases data reuse, minimizes memory fetches, and improves
    overall computational efficiency.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 分块是一种通过将计算重新结构化为更小、内存友好的子问题来减轻这一问题的技术。不是一次性处理整个矩阵或张量，这会导致过度的内存流量，分块将计算分割成更小的块（分块），这些块适合于快速局部内存（例如，缓存、共享内存或寄存器）([Lam,
    Rothberg, and Wolf 1991](ch058.xhtml#ref-lam1991cache))。通过这样做，分块增加了数据重用，最小化了内存检索，并提高了整体计算效率。
- en: A classic example of inefficient memory access is matrix multiplication, which
    is widely used in AI models. Without tiling, the naïve approach results in repeated
    memory accesses for the same data, leading to unnecessary bandwidth consumption
    ([Listing 11.23](ch017.xhtml#lst-naive_matmul)).
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 不高效的内存访问的经典例子是矩阵乘法，这在AI模型中得到了广泛应用。没有分块，原始方法会导致对相同数据的重复内存访问，导致不必要的带宽消耗 ([Listing 11.23](ch017.xhtml#lst-naive_matmul))。
- en: 'Listing 11.23: Naïve matrix multiplication without tiling'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.23：没有分块的原生矩阵乘法
- en: '[PRE22]'
  id: totrans-705
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Each iteration requires loading elements from matrices <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> and <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> multiple times from memory,
    causing excessive data movement. As the size of the matrices increases, the memory
    bottleneck worsens, limiting performance.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代都需要从内存中多次加载矩阵 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    的元素，造成数据移动过多。随着矩阵大小的增加，内存瓶颈加剧，限制了性能。
- en: Tiling addresses this problem by ensuring that smaller portions of matrices
    are loaded into fast memory, reused efficiently, and only written back to main
    memory when necessary. This technique is especially crucial in AI accelerators,
    where memory accesses dominate execution time. By breaking up large matrices into
    smaller tiles, as illustrated in [Figure 11.8](ch017.xhtml#fig-tiling-diagram),
    computation can be performed more efficiently on hardware by maximizing data reuse
    in fast memory. In the following sections, the fundamental principles emerge of
    tiling, its different strategies, and the key trade-offs involved in selecting
    an effective tiling approach.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 分块通过确保将矩阵的小部分加载到快速内存中，高效重用，并在必要时才写回主内存来解决此问题。这种技术在人工智能加速器中尤为重要，因为内存访问主导了执行时间。通过将大矩阵分解成小块，如图
    11.8 所示，可以在硬件上通过最大化快速内存中的数据重用来更有效地执行计算。在接下来的章节中，将阐述分块的基本原理、其不同的策略以及选择有效分块方法所涉及的关键权衡。
- en: '![](../media/file187.svg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file187.svg)'
- en: 'Figure 11.8: **Matrix Tiling**: Partitioning large matrices into smaller tiles
    optimizes data reuse and reduces memory access overhead during computation. This
    technique improves performance on AI accelerators by enabling efficient loading
    and processing of data in fast memory, minimizing transfers from slower main memory.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：**矩阵分块**：将大矩阵划分为小块优化了数据重用，并在计算过程中减少了内存访问开销。这项技术通过允许在快速内存中高效加载和处理数据，最小化从较慢的主内存中的传输，从而提高了人工智能加速器的性能。
- en: Tiling Fundamentals
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分块基础
- en: 'Tiling is based on a simple but powerful principle: instead of operating on
    an entire data structure at once, computations are divided into smaller tiles
    that fit within the available fast memory. By structuring execution around these
    tiles, data reuse is maximized, reducing redundant memory accesses and improving
    overall efficiency.'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 分块（Tiling）基于一个简单但强大的原则：不是一次性对整个数据结构进行操作，而是将计算划分为适合可用快速内存的小块。通过围绕这些小块来结构化执行，最大化数据重用，减少冗余内存访问，从而提高整体效率。
- en: 'Consider matrix multiplication, a key operation in machine learning workloads.
    The operation computes the output matrix <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>
    from two input matrices <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>:
    <semantics><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi></mrow> <annotation
    encoding="application/x-tex">C = A \times B</annotation></semantics> where each
    element <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">C[i,j]</annotation></semantics>
    is computed as: <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="true"
    form="postfix">]</mo></mrow><mo>×</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">C[i,j]
    = \sum_{k} A[i,k] \times B[k,j]</annotation></semantics>'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑矩阵乘法，这是机器学习工作负载中的关键操作。该操作通过两个输入矩阵 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    计算输出矩阵 <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>：<semantics><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi></mrow>
    <annotation encoding="application/x-tex">C = A \times B</annotation></semantics>
    其中每个元素 <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">C[i,j]</annotation></semantics>
    的计算如下：<semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="true"
    form="postfix">]</mo></mrow><mo>×</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">C[i,j]
    = \sum_{k} A[i,k] \times B[k,j]</annotation></semantics>
- en: A naïve implementation follows this formula directly ([Listing 11.24](ch017.xhtml#lst-naive_matmul_repeat)).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的实现直接遵循这个公式（[列表 11.24](ch017.xhtml#lst-naive_matmul_repeat)）。
- en: 'Listing 11.24: **Naïve Matrix Multiplication**: This code directly implements
    matrix multiplication using nested loops, showing how each element in the output
    matrix is computed as a sum of products from corresponding elements in the input
    matrices.'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.24：**朴素矩阵乘法**：此代码直接使用嵌套循环实现矩阵乘法，展示了输出矩阵中的每个元素是如何作为输入矩阵中对应元素乘积之和来计算的。
- en: '[PRE23]'
  id: totrans-715
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: At first glance, this approach seems correct—it computes the desired result
    and follows the mathematical definition. However, the issue lies in how memory
    is accessed. Every time the innermost loop runs, it fetches an element from matrix
    <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and matrix <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    from memory, performs a multiplication, and updates an element in matrix <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics>. Because matrices are
    large, the processor frequently reloads the same values from memory, even though
    they were just used in previous computations.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这种方法似乎是正确的——它计算了所需的结果，并遵循了数学定义。然而，问题在于内存的访问方式。每当最内层循环运行时，它都会从矩阵 <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> 和矩阵 <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> 中从内存中获取一个元素，执行乘法，并在矩阵
    <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>
    中更新一个元素。由于矩阵很大，处理器频繁地从内存中重新加载相同的值，即使这些值在之前的计算中已经被使用过。
- en: This unnecessary data movement is expensive. Fetching values from main memory
    (DRAM) is hundreds of times slower than accessing values stored in on-chip cache
    or registers. If the same values must be reloaded multiple times instead of being
    stored in fast memory, execution slows down significantly.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不必要的数据移动代价高昂。从主内存（DRAM）中获取值比访问片上缓存或寄存器中存储的值慢数百倍。如果必须多次重新加载相同的值而不是将其存储在快速内存中，执行速度会显著减慢。
- en: Performance Benefits of Tiling
  id: totrans-718
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分块的性能优势
- en: Instead of computing one element at a time and constantly moving data in and
    out of slow memory, tiling processes submatrices (tiles) at a time, keeping frequently
    used values in fast memory. The idea is to divide the matrices into smaller blocks
    that fit within the processor’s cache or shared memory, ensuring that once a block
    is loaded, it is reused multiple times before moving to the next one.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 与每次计算一个元素并不断在慢速内存中移动数据不同，分块处理子矩阵（分块）一次，将常用值保持在快速内存中。想法是将矩阵划分为适合处理器缓存或共享内存的小块，确保一旦一个块被加载，它就会被多次重用，然后再移动到下一个块。
- en: '[Listing 11.25](ch017.xhtml#lst-tiled_matmul) illustrates a tiled version of
    matrix multiplication, which improves memory locality by processing blocks of
    data.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 11.25](ch017.xhtml#lst-tiled_matmul) 展示了矩阵乘法的分块版本，通过处理数据块来提高内存局部性。'
- en: 'Listing 11.25: **Tiled Matrix Multiplication**: This approach divides matrices
    into smaller blocks to optimize memory usage by reusing data within processor
    cache, thereby improving computational efficiency.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.25：**分块矩阵乘法**：这种方法将矩阵划分为更小的块，通过在处理器缓存中重用数据来优化内存使用，从而提高计算效率。
- en: '[PRE24]'
  id: totrans-722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This restructuring significantly improves performance for three main reasons:'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重构显著提高了性能，主要原因有三个：
- en: '**Better Memory Reuse**: Instead of fetching elements from <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> and <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> repeatedly from slow memory,
    this approach loads a small tile of data into fast memory, performs multiple computations
    using it, and only then moves on to the next tile. This minimizes redundant memory
    accesses.'
  id: totrans-724
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更好的内存重用**：而不是反复从 <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    和 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    中从慢速内存中获取元素，这种方法将一小块数据加载到快速内存中，使用它进行多次计算，然后才移动到下一个分块。这最小化了冗余的内存访问。'
- en: '**Reduced Memory Bandwidth Usage**: Since each tile is used multiple times
    before being evicted, memory traffic is reduced. Instead of repeatedly accessing
    DRAM, most required data is available in L1/L2 cache or shared memory, leading
    to faster execution.'
  id: totrans-725
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**减少内存带宽使用**：由于每个分块在使用后被多次使用，内存流量减少。与反复访问 DRAM 相比，大多数所需数据都可在 L1/L2 缓存或共享内存中找到，从而加快执行速度。'
- en: '**Increased Compute Efficiency**: Processors spend less time waiting for data
    and more time performing useful computations. In architectures like GPUs and TPUs,
    where thousands of parallel processing units operate simultaneously, tiling ensures
    that data is read and processed in a structured manner, avoiding unnecessary stalls.'
  id: totrans-726
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提高计算效率**：处理器花费更少的时间等待数据，更多的时间进行有用的计算。在像GPU和TPU这样的架构中，数千个并行处理单元同时运行，划分确保数据以结构化的方式读取和处理，避免不必要的停滞。'
- en: This technique is particularly effective in AI accelerators, where machine learning
    workloads consist of large matrix multiplications and tensor transformations.
    Without tiling, these workloads quickly become memory-bound, meaning performance
    is constrained by how fast data can be retrieved rather than by the raw computational
    power of the processor.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在AI加速器中特别有效，其中机器学习工作负载包括大规模矩阵乘法和张量变换。没有划分，这些工作负载很快就会成为内存限制的，这意味着性能受限于数据检索的速度，而不是处理器的原始计算能力。
- en: Tiling Methods
  id: totrans-728
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 划分方法
- en: While the general principle of tiling remains the same, which involves partitioning
    large computations into smaller subproblems to improve memory reuse, there are
    different ways to apply tiling based on the structure of the computation and hardware
    constraints. The two primary tiling strategies are spatial tiling and temporal
    tiling. These strategies optimize different aspects of computation and memory
    access, and in practice, they are often combined to achieve the best performance.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然划分的一般原则保持不变，即通过将大型计算划分为更小的子问题来提高内存重用，但根据计算结构和硬件约束，有不同方式应用划分。两种主要的划分策略是空间划分和时间划分。这些策略优化计算和内存访问的不同方面，在实践中，它们通常结合使用以达到最佳性能。
- en: Spatial Tiling
  id: totrans-730
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 空间划分
- en: Spatial tiling focuses on partitioning data structures into smaller blocks that
    fit within the fast memory of the processor. This approach ensures that each tile
    is fully processed before moving to the next, reducing redundant memory accesses.
    Spatial tiling is widely used in operations such as matrix multiplication, convolutions,
    and attention mechanisms in transformer models.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 空间划分关注将数据结构划分为更小的块，这些块适合处理器的高速内存。这种方法确保在移动到下一个块之前，每个块都得到完全处理，从而减少了冗余的内存访问。空间划分在矩阵乘法、卷积和变换器模型中的注意力机制等操作中得到了广泛应用。
- en: Spatial tiling is illustrated in [Listing 11.26](ch017.xhtml#lst-tiled_spatial),
    where the computation proceeds over blocks of the input matrices.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 空间划分在[列表11.26](ch017.xhtml#lst-tiled_spatial)中得到了说明，其中计算过程是按照输入矩阵的块进行的。
- en: 'Listing 11.26: **Spatial Tiling**: Reduces redundant memory accesses by processing
    matrix tiles sequentially.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.26：**空间划分**：通过顺序处理矩阵块来减少冗余内存访问。
- en: '[PRE25]'
  id: totrans-734
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this implementation, each tile of <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    is loaded into cache or shared memory before processing, ensuring that the same
    data does not need to be fetched repeatedly from slower memory. The tile is fully
    used before moving to the next block, minimizing redundant memory accesses. Since
    data is accessed in a structured, localized way, cache efficiency improves significantly.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，在处理之前，<semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>和<semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics>的每个块都加载到缓存或共享内存中，确保相同的数据不需要从较慢的内存中反复获取。在移动到下一个块之前，块被完全使用，从而最小化了冗余的内存访问。由于数据以结构化和局部化的方式访问，缓存效率显著提高。
- en: Spatial tiling is particularly beneficial when dealing with large tensors that
    do not fit entirely in fast memory. By breaking them into smaller tiles, computations
    remain localized, avoiding excessive data movement between memory levels. This
    technique is widely used in AI accelerators where machine learning workloads involve
    large-scale tensor operations that require careful memory management to achieve
    high performance.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理不适合完全放入快速内存中的大型张量时，空间划分特别有益。通过将它们划分为更小的块，计算保持局部化，避免了在内存级别之间进行过多的数据移动。这项技术在涉及大规模张量操作且需要精心管理内存以实现高性能的机器学习工作负载的AI加速器中得到了广泛应用。
- en: Temporal Tiling
  id: totrans-737
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 时间划分
- en: While spatial tiling optimizes how data is partitioned, temporal tiling focuses
    on reorganizing the computation itself to improve data reuse over time. Many machine
    learning workloads involve operations where the same data is accessed repeatedly
    across multiple iterations. Without temporal tiling, this often results in redundant
    memory fetches, leading to inefficiencies. Temporal tiling, also known as loop
    blocking, restructures the computation to ensure that frequently used data stays
    in fast memory for as long as possible before moving on to the next computation.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 当空间分割优化数据分区时，时间分割专注于重新组织计算本身，以改善随时间的数据重用。许多机器学习工作负载涉及在多个迭代中重复访问相同数据的操作。没有时间分割，这通常会导致冗余的内存访问，导致效率低下。时间分割，也称为循环阻塞，重构计算以确保频繁使用的数据尽可能长时间地保留在快速内存中，然后再进行下一个计算。
- en: A classic example where temporal tiling is beneficial is convolutional operations,
    where the same set of weights is applied to multiple input regions. Without loop
    blocking, these weights might be loaded from memory multiple times for each computation.
    With temporal tiling, the computation is reordered so that the weights remain
    in fast memory across multiple inputs, reducing unnecessary memory fetches and
    improving overall efficiency.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分割有益的一个经典例子是卷积操作，其中相同的权重集应用于多个输入区域。没有循环阻塞，这些权重可能需要在每次计算时从内存中多次加载。使用时间分割，计算被重新排序，使得权重在多个输入之间保持快速内存中，减少了不必要的内存访问，并提高了整体效率。
- en: '[Listing 11.27](ch017.xhtml#lst-loop_blocking) illustrates a simplified example
    of loop blocking in matrix multiplication.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表11.27](ch017.xhtml#lst-loop_blocking)展示了矩阵乘法中循环阻塞的简化示例。'
- en: 'Listing 11.27: **Temporal Tiling**: Reduces redundant memory accesses by caching
    weights in fast memory across multiple matrix multiplications.'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.27：**时间分割**：通过在多个矩阵乘法中缓存权重到快速内存中，减少了冗余内存访问。
- en: '[PRE26]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Temporal tiling improves performance by ensuring that the data loaded into fast
    memory is used multiple times before being evicted. In this implementation, small
    tiles of matrices <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    are explicitly loaded into temporary storage before performing computations, reducing
    memory fetch overhead. This restructuring allows the computation to process an
    entire tile before moving to the next, thereby reducing the number of times data
    must be loaded from slower memory.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分割技术通过确保在数据被移除之前多次使用加载到快速内存中的数据，从而提高了性能。在本实现中，在执行计算之前，矩阵的小块<semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics>和<semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics>被明确加载到临时存储中，从而减少了内存访问开销。这种重构允许计算在移动到下一个块之前处理整个块，从而减少了从较慢内存加载数据的次数。
- en: This technique is particularly useful in workloads where certain values are
    used repeatedly, such as convolutions, recurrent neural networks (RNNs), and self-attention
    mechanisms in transformers. By applying loop blocking, AI accelerators can significantly
    reduce memory stalls and improve execution throughput.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在某些值被重复使用的负载中特别有用，例如卷积、循环神经网络（RNN）和变换器中的自注意力机制。通过应用循环阻塞，AI加速器可以显著减少内存停滞并提高执行吞吐量。
- en: Tiling Challenges and Trade-offs
  id: totrans-745
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分割的挑战和权衡
- en: While tiling significantly improves performance by optimizing memory reuse and
    reducing redundant memory accesses, it introduces several challenges and trade-offs.
    Selecting the right tile size is a critical decision, as it directly affects computational
    efficiency and memory bandwidth usage. If the tile size is too small, the benefits
    of tiling diminish, as memory fetches still dominate execution time. On the other
    hand, if the tile size is too large, it may exceed the available fast memory,
    causing cache thrashing and performance degradation.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分割通过优化内存重用和减少冗余内存访问显著提高了性能，但它引入了几个挑战和权衡。选择合适的块大小是一个关键决策，因为它直接影响计算效率和内存带宽使用。如果块大小太小，分割的好处会减少，因为内存访问仍然主导执行时间。另一方面，如果块大小太大，它可能超过可用的快速内存，导致缓存冲突和性能下降。
- en: Load balancing is another key concern. In architectures such as GPUs and TPUs,
    computations are executed in parallel across thousands of processing units. If
    tiles are not evenly distributed, some units may remain idle while others are
    overloaded, leading to suboptimal utilization of computational resources. Effective
    tile scheduling ensures that parallel execution remains balanced and efficient.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是另一个关键问题。在如GPU和TPU这样的架构中，计算在数千个处理单元上并行执行。如果分割不均匀分布，一些单元可能保持空闲，而其他单元可能过载，导致计算资源利用不充分。有效的分割调度确保并行执行保持平衡和高效。
- en: Data movement overhead is also an important consideration. Although tiling reduces
    the number of slow memory accesses, transferring tiles between different levels
    of memory still incurs a cost. This is especially relevant in hierarchical memory
    systems, where accessing data from cache is much faster than accessing it from
    DRAM. Efficient memory prefetching and scheduling strategies are required to minimize
    latency and ensure that data is available when needed.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 数据移动开销也是一个重要的考虑因素。尽管分割减少了慢速内存访问的次数，但在不同内存级别之间传输分割仍然会产生成本。这在层次化内存系统中尤为重要，从缓存中访问数据比从DRAM中访问数据要快得多。需要有效的内存预取和调度策略来最小化延迟并确保数据在需要时可用。
- en: Beyond spatial and temporal tiling, hybrid approaches combine elements of both
    strategies to achieve optimal performance. Hybrid tiling adapts to workload-specific
    constraints by dynamically adjusting tile sizes or reordering computations based
    on real-time execution conditions. For example, some AI accelerators use spatial
    tiling for matrix multiplications while employing temporal tiling for weight reuse
    in convolutional layers.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是空间和时间分割，混合方法结合了两种策略的元素以实现最佳性能。混合分割通过动态调整分割大小或根据实时执行条件重新排序计算来适应特定工作负载的约束。例如，一些AI加速器使用空间分割进行矩阵乘法，同时在卷积层中采用时间分割以重用权重。
- en: Other methods exist for optimizing memory usage and computational efficiency
    beyond tiling. Techniques such as register blocking, double buffering, and hierarchical
    tiling extend the basic tiling principles to further optimize execution. AI compilers
    and runtime systems, such as TensorFlow XLA, TVM, and MLIR, automatically select
    tiling strategies based on hardware constraints, enabling fine-tuned performance
    optimization without manual intervention.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分割之外，还有其他方法可以优化内存使用和计算效率。例如，寄存器分割、双缓冲和分层分割将基本的分割原则扩展到进一步优化执行。如TensorFlow XLA、TVM和MLIR等AI编译器和运行时系统根据硬件约束自动选择分割策略，从而实现无需人工干预的精细性能优化。
- en: '[Table 11.17](ch017.xhtml#tbl-tiling-strategies) provides a comparative overview
    of spatial, temporal, and hybrid tiling approaches, highlighting their respective
    benefits and trade-offs.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.17](ch017.xhtml#tbl-tiling-strategies)提供了空间分割、时间分割和混合分割方法的比较概述，突出了各自的优缺点。'
- en: 'Table 11.17: **Tiling Strategies**: Spatial, temporal, and hybrid tiling optimize
    memory access patterns for improved performance; spatial tiling maximizes data
    reuse within fast memory, temporal tiling exploits loop structure for reduced
    accesses, and hybrid tiling combines both approaches to balance computational
    efficiency and memory bandwidth. These techniques are crucial for AI compilers
    and runtime systems to automatically optimize model execution on diverse hardware.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.17：**分割策略**：空间分割、时间分割和混合分割优化内存访问模式以提升性能；空间分割最大化快速内存中的数据重用，时间分割利用循环结构以减少访问，混合分割结合两种方法以平衡计算效率和内存带宽。这些技术对于AI编译器和运行时系统自动优化不同硬件上的模型执行至关重要。
- en: '| **Aspect** | **Spatial Tiling (Data Tiling)** | **Temporal Tiling (Loop Blocking)**
    | **Hybrid Tiling** |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **空间分割（数据分割）** | **时间分割（循环分割）** | **混合分割** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Primary Goal** | Reduce memory accesses by keeping data in fast memory
    longer | Increase data reuse across loop iterations | Adapt dynamically to workload
    constraints |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| **主要目标** | 通过将数据保留在快速内存中更长时间来减少内存访问 | 在循环迭代中增加数据重用 | 动态适应工作负载约束 |'
- en: '| **Optimization Focus** | Partitioning data structures into smaller, memory-friendly
    blocks | Reordering computations to maximize reuse before eviction | Balancing
    spatial and temporal reuse strategies |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| **优化重点** | 将数据结构分割成更小、内存友好的块 | 重新排序计算以最大化重用，在淘汰之前 | 平衡空间和时间重用策略 |'
- en: '| **Memory Usage** | Improves cache locality and reduces DRAM access | Keeps
    frequently used data in fast memory for multiple iterations | Minimizes data movement
    while ensuring high reuse |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| **内存使用** | 提高缓存局部性并减少DRAM访问 | 将频繁使用的数据保留在快速内存中，以便多次迭代 | 最小化数据移动，同时确保高重复使用
    |'
- en: '| **Common Use Cases** | Matrix multiplications, CNNs, self-attention in transformers
    | Convolutions, recurrent neural networks (RNNs), iterative computations | AI
    accelerators with hierarchical memory, mixed workloads |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| **常见用例** | 矩阵乘法、CNN、变换器中的自注意力 | 卷积、循环神经网络（RNN）、迭代计算 | 具有分层内存的AI加速器、混合工作负载
    |'
- en: '| **Performance Gains** | Reduced memory bandwidth requirements, better cache
    utilization | Lower memory fetch latency, improved data locality | Maximized efficiency
    across multiple hardware types |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| **性能提升** | 降低内存带宽需求，更好的缓存利用率 | 降低内存获取延迟，提高数据局部性 | 在多种硬件类型上最大化效率 |'
- en: '| **Challenges** | Requires careful tile size selection, inefficient for workloads
    with minimal spatial reuse | Can increase register pressure, requires loop restructuring
    | Complexity in tuning tile size and execution order dynamically |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | 需要仔细选择分块大小，对于空间重复使用最少的工作负载效率低下 | 可能会增加寄存器压力，需要循环重构 | 动态调整分块大小和执行顺序的复杂性
    |'
- en: '| **Best When** | Data is large and needs to be partitioned for efficient processing
    | The same data is accessed multiple times across iterations | Both data partitioning
    and iteration-based reuse are important |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| **最佳使用时机** | 数据量大且需要分区以进行高效处理 | 相同数据在迭代中多次访问 | 数据分区和基于迭代的重复使用都很重要 |'
- en: As machine learning models continue to grow in size and complexity, tiling remains
    a critical tool for improving hardware efficiency, ensuring that AI accelerators
    operate at their full potential. While manual tiling strategies can provide substantial
    benefits, modern compilers and hardware-aware optimization techniques further
    enhance performance by automatically selecting the most effective tiling strategies
    for a given workload.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型在规模和复杂性上的持续增长，分块仍然是一个提高硬件效率的关键工具，确保AI加速器发挥其全部潜力。虽然手动分块策略可以提供大量好处，但现代编译器和硬件感知优化技术通过自动选择针对给定工作负载最有效的分块策略来进一步提高性能。
- en: Applying Mapping Strategies to Neural Networks
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将映射策略应用于神经网络
- en: While these foundational mapping techniques apply broadly, their effectiveness
    varies based on the computational structure, data access patterns, and parallelization
    opportunities of different neural network architectures. Each architecture imposes
    distinct constraints on data movement, memory hierarchy, and computation scheduling,
    requiring tailored mapping strategies to optimize performance.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些基础映射技术具有广泛的适用性，但它们的有效性取决于不同神经网络架构的计算结构、数据访问模式和并行化机会。每种架构都对数据移动、内存层次结构和计算调度施加了独特的约束，需要定制映射策略以优化性能。
- en: A structured approach to mapping is essential to address the combinatorial explosion
    of choices that arise when assigning computations to AI accelerators. Rather than
    treating each model as a separate optimization problem, we recognize that the
    same fundamental principles apply across different architectures—only their priority
    shifts based on workload characteristics. The goal is to systematically select
    and apply mapping strategies that maximize efficiency for different types of machine
    learning models.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 在将计算分配给AI加速器时，采用结构化的映射方法对于解决由此产生的组合爆炸式选择至关重要。我们不是将每个模型视为一个独立的优化问题，而是认识到，相同的根本原则适用于不同的架构——只是它们的优先级根据工作负载特性而变化。目标是系统地选择并应用映射策略，以最大化不同类型机器学习模型的效率。
- en: These principles apply to three representative AI workloads, each characterized
    by distinct computational demands. CNNs benefit from spatial data reuse, making
    weight-stationary execution and the application of tiling techniques especially
    effective. In contrast, Transformers are inherently memory-bound and rely on strategies
    such as efficient KV-cache management, fused attention mechanisms, and highly
    parallel execution to mitigate memory traffic. MLPs, which involve substantial
    matrix multiplication operations, demand the use of structured tiling, optimized
    weight layouts, and memory-aware execution to enhance overall performance.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则适用于三个代表性的AI工作负载，每个工作负载都有独特的计算需求。CNNs受益于空间数据重用，使得权重静止执行和应用瓦片技术特别有效。相比之下，Transformers本质上是内存受限的，依赖于诸如高效的KV缓存管理、融合的注意力机制和高度并行执行等策略来减轻内存流量。MLPs涉及大量的矩阵乘法操作，需要使用结构化瓦片、优化的权重布局和内存感知执行来提高整体性能。
- en: Despite their differences, each of these models follows a common set of mapping
    principles, with variations in how optimizations are prioritized. The following
    table provides a structured mapping between different optimization strategies
    and their suitability for CNNs, Transformers, and MLPs. This table serves as a
    roadmap for selecting appropriate mapping strategies for different machine learning
    workloads.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们之间存在差异，但每个模型都遵循一套共同的映射原则，优化优先级的差异。以下表格提供了不同优化策略及其对CNNs、Transformers和MLPs适用性的结构化映射。此表作为选择不同机器学习工作负载的适当映射策略的路线图。
- en: '| **Optimization Technique** | **CNNs** | **Transformers** | **MLPs** | **Rationale**
    |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| **优化技术** | **CNNs** | **Transformers** | **MLPs** | **理由** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Dataflow Strategy** | Weight Stationary | Activation Stationary | Weight
    Stationary | CNNs reuse filters across spatial locations; Transformers reuse activations
    (KV-cache); MLPs reuse weights across batches. |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| **数据流策略** | 权重静止 | 激活静止 | 权重静止 | CNNs在空间位置间重用过滤器；Transformers重用激活（KV缓存）；MLPs在批次间重用权重。|'
- en: '| **Memory-Aware Tensor Layouts** | NCHW (Channel-Major) | NHWC (Row-Major)
    | NHWC | CNNs favor channel-major for convolution efficiency; Transformers and
    MLPs prioritize row-major for fast memory access. |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| **内存感知张量布局** | NCHW (通道优先) | NHWC (行优先) | NHWC | CNNs偏好通道优先以提高卷积效率；Transformers和MLPs优先考虑行优先以实现快速内存访问。|'
- en: '| **Kernel Fusion** | Convolution + Activation | Fused Attention | GEMM Fusion
    | CNNs optimize convolution+activation fusion; Transformers fuse attention mechanisms;
    MLPs benefit from fused matrix multiplications. |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| **内核融合** | 卷积 + 激活 | 融合注意力 | GEMM融合 | CNNs优化卷积+激活融合；Transformers融合注意力机制；MLPs从融合的矩阵乘法中受益。|'
- en: '| **Tiling for Memory Efficiency** | Spatial Tiling | Temporal Tiling | Blocked
    Tiling | CNNs tile along spatial dimensions; Transformers use loop blocking to
    improve sequence memory efficiency; MLPs use blocked tiling for large matrix multiplications.
    |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| **内存效率的瓦片化** | 空间瓦片 | 时间瓦片 | 分块瓦片 | CNNs沿空间维度瓦片化；Transformers使用循环分块以提高序列内存效率；MLPs使用分块瓦片进行大型矩阵乘法。|'
- en: This table highlights that each machine learning model benefits from a different
    combination of optimization techniques, reinforcing the importance of tailoring
    execution strategies to the computational and memory characteristics of the workload.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 此表强调，每个机器学习模型都受益于不同组合的优化技术，这强调了根据工作负载的计算和内存特性定制执行策略的重要性。
- en: In the following sections, we explore how these optimizations apply to each
    network type, explaining how CNNs, Transformers, and MLPs leverage specific mapping
    strategies to improve execution efficiency and hardware utilization.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下各节中，我们将探讨这些优化如何应用于每种网络类型，解释CNNs、Transformers和MLPs如何利用特定的映射策略来提高执行效率和硬件利用率。
- en: Convolutional Neural Networks
  id: totrans-776
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: CNNs are characterized by their structured spatial computations, where small
    filters (or kernels) are repeatedly applied across an input feature map. This
    structured weight reuse makes weight stationary execution the most effective strategy
    for CNNs. Keeping filter weights in fast memory while streaming activations ensures
    that weights do not need to be repeatedly fetched from slower external memory,
    significantly reducing memory bandwidth demands. Since each weight is applied
    to multiple spatial locations, weight stationary execution maximizes arithmetic
    intensity and minimizes redundant memory transfers.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs以其结构化的空间计算为特征，其中小型滤波器（或核）在输入特征图上反复应用。这种结构化的权重重用使得权重固定执行成为CNNs中最有效的策略。在流激活的同时保持滤波器权重在快速内存中，确保权重不需要从较慢的外部内存中反复获取，从而显著降低内存带宽需求。由于每个权重应用于多个空间位置，权重固定执行最大化了算术强度并最小化了冗余内存传输。
- en: Memory-aware tensor layouts also play a critical role in CNN execution. Convolution
    operations benefit from a channel-major memory format, often represented as NCHW
    (batch, channels, height, width). This layout aligns with the access patterns
    of convolutions, enabling efficient memory coalescing on accelerators such as
    GPUs and TPUs. By storing data in a format that optimizes cache locality, accelerators
    can fetch contiguous memory blocks efficiently, reducing latency and improving
    throughput.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆感知的张量布局在CNN执行中也发挥着关键作用。卷积操作受益于通道主内存格式，通常表示为NCHW（批次、通道、高度、宽度）。这种布局与卷积的访问模式相一致，使得在GPU和TPU等加速器上实现高效的内存归约。通过以优化缓存局部性的格式存储数据，加速器可以有效地获取连续的内存块，减少延迟并提高吞吐量。
- en: Kernel fusion is another important optimization for CNNs. In a typical machine
    learning pipeline, convolution operations are often followed by activation functions
    such as ReLU and batch normalization. Instead of treating these operations as
    separate computational steps, fusing them into a single kernel reduces intermediate
    memory writes and improves execution efficiency. This optimization minimizes memory
    bandwidth pressure by keeping intermediate values in registers rather than writing
    them to memory and fetching them back in subsequent steps.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 核融合是CNNs的另一个重要优化。在典型的机器学习管道中，卷积操作通常随后跟随着激活函数，如ReLU和批量归一化。将这些操作视为单独的计算步骤而不是融合成一个单一核，可以减少中间内存写入并提高执行效率。这种优化通过将中间值保持在寄存器中而不是写入内存并在后续步骤中重新获取，从而最小化内存带宽压力。
- en: Given the size of input images and feature maps, tiling is necessary to ensure
    that computations fit within fast memory hierarchies. Spatial tiling, where input
    feature maps are processed in smaller subregions, allows for efficient utilization
    of on-chip memory while avoiding excessive off-chip memory transfers. This technique
    ensures that input activations, weights, and intermediate outputs remain within
    high-speed caches or shared memory as long as possible, reducing memory stalls
    and improving overall performance.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输入图像和特征图的大小，分块是必要的，以确保计算适合快速内存层次结构。空间分块，即输入特征图在较小的子区域中处理，允许高效地利用片上内存，同时避免过多的片外内存传输。这项技术确保输入激活、权重和中间输出尽可能长时间地保持在高速缓存或共享内存中，减少内存停滞并提高整体性能。
- en: Together, these optimizations ensure that CNNs make efficient use of available
    compute resources by maximizing weight reuse, optimizing memory access patterns,
    reducing redundant memory writes, and structuring computation to fit within fast
    memory constraints.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化共同确保了卷积神经网络（CNNs）通过最大化权重重用、优化内存访问模式、减少冗余内存写入以及结构化计算以适应快速内存约束，从而高效地利用可用的计算资源。
- en: Transformer Architectures
  id: totrans-782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Unlike CNNs, which rely on structured spatial computations, Transformers process
    variable-length sequences and rely heavily on attention mechanisms. The primary
    computational bottleneck in Transformers is memory bandwidth, as attention mechanisms
    require frequent access to stored key-value pairs across multiple query vectors.
    Given this access pattern, activation stationary execution is the most effective
    strategy. By keeping key-value activations in fast memory and streaming query
    vectors dynamically, activation reuse is maximized while minimizing redundant
    memory fetches. This approach is critical in reducing bandwidth overhead, especially
    in long-sequence tasks such as natural language processing.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于结构化空间计算的 CNNs 不同，Transformers 处理可变长度的序列，并且高度依赖于注意力机制。Transformers 的主要计算瓶颈是内存带宽，因为注意力机制需要频繁访问多个查询向量中的存储键值对。鉴于这种访问模式，激活站态执行是最有效的策略。通过将键值激活保留在快速内存中，并动态地流式传输查询向量，最大化了激活的重用，同时最小化了冗余的内存读取。这种方法对于减少带宽开销至关重要，尤其是在自然语言处理等长序列任务中。
- en: Memory layout optimization is equally important for Transformers. Unlike CNNs,
    which benefit from channel-major layouts, Transformers require efficient access
    to sequences of activations, making a row-major format (NHWC) the preferred choice.
    This layout ensures that activations are accessed contiguously in memory, reducing
    cache misses and improving memory coalescing for matrix multiplications.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 内存布局优化对于 Transformers 同样重要。与受益于通道主布局的 CNNs 不同，Transformers 需要有效地访问激活序列，使得行主格式（NHWC）成为首选。这种布局确保激活在内存中连续访问，减少了缓存未命中并提高了矩阵乘法中的内存归约。
- en: Kernel fusion plays a key role in optimizing Transformer execution. In self-attention,
    multiple computational steps, such as query-key dot products, softmax normalization,
    and weighted summation, can be fused into a single operation. Fused attention
    kernels eliminate intermediate memory writes by computing attention scores and
    performing weighted summations within a single execution step. This optimization
    significantly reduces memory traffic, particularly for large batch sizes and long
    sequences.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合在优化 Transformer 执行中扮演着关键角色。在自注意力中，多个计算步骤，如查询-键点积、softmax 归一化和加权求和，可以融合为一个单一操作。融合的注意力内核通过在单个执行步骤中计算注意力分数和执行加权求和来消除中间内存写入。这种优化显著减少了内存流量，尤其是在大批量大小和长序列中。
- en: Due to the nature of sequence processing, tiling must be adapted to improve
    memory efficiency. Instead of spatial tiling, which is effective for CNNs, Transformers
    benefit from temporal tiling, where computations are structured to process sequence
    blocks efficiently. This method ensures that activations are loaded into fast
    memory in manageable chunks, reducing excessive memory transfers. Temporal tiling
    is particularly beneficial for long-sequence models, where the memory footprint
    of key-value activations grows significantly. By tiling sequences into smaller
    segments, memory locality is improved, enabling efficient cache utilization and
    reducing bandwidth pressure.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列处理的特性，必须对分块进行适配以提高内存效率。与对 CNNs 有效的空间分块不同，Transformers 从时间分块中受益，其中计算被结构化以有效地处理序列块。这种方法确保激活以可管理的块加载到快速内存中，减少了过度的内存传输。时间分块对于长序列模型特别有益，因为键值激活的内存占用显著增加。通过将序列分块为更小的段，提高了内存局部性，使得高效的缓存利用成为可能，并减少了带宽压力。
- en: These optimizations collectively address the primary bottlenecks in Transformer
    models by prioritizing activation reuse, structuring memory layouts for efficient
    batched computations, fusing attention operations to reduce intermediate memory
    writes, and employing tiling techniques suited to sequence-based processing.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化通过优先考虑激活重用、结构化内存布局以进行高效的批量计算、融合注意力操作以减少中间内存写入，以及采用适合基于序列处理的分块技术，共同解决了 Transformer
    模型的瓶颈。
- en: Multi-Layer Perceptrons
  id: totrans-788
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层感知器
- en: MLPs primarily consist of fully connected layers, where large matrices of weights
    and activations are multiplied to produce output representations. Given this structure,
    weight stationary execution is the most effective strategy for MLPs. Similar to
    CNNs, MLPs benefit from keeping weights in local memory while streaming activations
    dynamically, as this ensures that weight matrices, which are typically reused
    across multiple activations in a batch, do not need to be frequently reloaded.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs主要由全连接层组成，其中权重和激活的大型矩阵相乘以产生输出表示。鉴于这种结构，权重静态执行是MLPs最有效的策略。类似于CNNs，MLPs从保持权重在局部内存中同时动态流激活中受益，因为这确保了权重矩阵，通常在批处理中的多个激活之间重复使用，不需要频繁重新加载。
- en: The preferred memory layout for MLPs aligns with that of Transformers, as matrix
    multiplications are more efficient when using a row-major (NHWC) format. Since
    activation matrices are processed in batches, this layout ensures that input activations
    are accessed efficiently without introducing memory fragmentation. By aligning
    tensor storage with compute-friendly memory access patterns, cache utilization
    is improved, reducing memory stalls.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs（多层感知器）的首选内存布局与Transformers的布局一致，因为在使用行主序（NHWC）格式时矩阵乘法更高效。由于激活矩阵是分批处理的，这种布局确保了输入激活的访问效率，同时不会引入内存碎片。通过将张量存储与计算友好的内存访问模式对齐，提高了缓存利用率，减少了内存停滞。
- en: Kernel fusion in MLPs is primarily applied to General Matrix Multiplication
    (GEMM)[29](#fn29) operations. Since dense layers are often followed by activation
    functions and bias additions, fusing these operations into a single computation
    step reduces memory traffic. GEMM fusion ensures that activations, weights, and
    biases are processed within a single optimized kernel, avoiding unnecessary memory
    writes and reloads.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs中的内核融合主要应用于通用矩阵乘法（GEMM）操作[29](#fn29)。由于密集层通常随后跟随着激活函数和偏置添加，将这些操作融合到单个计算步骤中可以减少内存流量。GEMM融合确保激活、权重和偏置在单个优化的内核中处理，避免了不必要的内存写入和重新加载。
- en: To further improve memory efficiency, MLPs rely on blocked tiling strategies,
    where large matrix multiplications are divided into smaller sub-blocks that fit
    within the accelerator’s shared memory. This method ensures that frequently accessed
    portions of matrices remain in fast memory throughout computation, reducing external
    memory accesses. By structuring computations in a way that balances memory utilization
    with efficient parallel execution, blocked tiling minimizes bandwidth limitations
    and maximizes throughput.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高内存效率，MLPs依赖于分块填充策略，其中大型矩阵乘法被分成适合加速器共享内存的小型子块。这种方法确保了矩阵频繁访问的部分在整个计算过程中保持在快速内存中，减少了外部内存访问。通过以平衡内存利用与高效并行执行的方式结构化计算，分块填充最小化了带宽限制并最大化了吞吐量。
- en: These optimizations ensure that MLPs achieve high computational efficiency by
    structuring execution around weight reuse, optimizing memory layouts for dense
    matrix operations, reducing redundant memory writes through kernel fusion, and
    employing blocked tiling strategies to maximize on-chip memory utilization.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优化确保了MLPs通过围绕权重重用结构化执行、优化密集矩阵操作的内存布局、通过内核融合减少冗余内存写入以及采用分块填充策略以最大化片上内存利用率，实现了高计算效率。
- en: Hybrid Mapping Strategies
  id: totrans-794
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合映射策略
- en: While general mapping strategies provide a structured framework for optimizing
    machine learning models, real-world architectures often involve diverse computational
    requirements that cannot be effectively addressed with a single, fixed approach.
    Hybrid mapping strategies allow AI accelerators to dynamically apply different
    optimizations to specific layers or components within a model, ensuring that each
    computation is executed with maximum efficiency.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通用的映射策略为优化机器学习模型提供了一个结构化的框架，但现实世界的架构往往涉及多样化的计算需求，这些需求无法通过单一、固定的方法有效解决。混合映射策略允许AI加速器动态地将不同的优化应用于模型中的特定层或组件，确保每个计算都以最大效率执行。
- en: Machine learning models typically consist of multiple layer types, each exhibiting
    distinct memory access patterns, data reuse characteristics, and parallelization
    opportunities. By tailoring mapping strategies to these specific properties, hybrid
    approaches achieve higher computational efficiency, improved memory bandwidth
    utilization, and reduced data movement overhead compared to a uniform mapping
    approach ([Sze et al. 2017b](ch058.xhtml#ref-sze2020efficient)).
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常由多种层类型组成，每种类型都表现出独特的内存访问模式、数据重用特性和并行化机会。通过针对这些特定属性定制映射策略，混合方法比统一映射方法实现了更高的计算效率、改进的内存带宽利用率和减少的数据移动开销（参见[Sze等人2017b](ch058.xhtml#ref-sze2020efficient)）。
- en: Layer-Specific Mapping
  id: totrans-797
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层特定映射
- en: Hybrid mapping strategies are particularly beneficial in models that combine
    spatially localized computations, such as convolutions, with fully connected operations,
    such as dense layers or attention mechanisms. These operations possess distinct
    characteristics that require different mapping strategies for optimal performance.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 混合映射策略在结合空间局部计算（如卷积）和全连接操作（如密集层或注意力机制）的模型中特别有益。这些操作具有不同的特性，需要不同的映射策略以实现最佳性能。
- en: In convolutional neural networks, hybrid strategies are frequently employed
    to optimize performance. Specifically, weight stationary execution is applied
    to convolutional layers, ensuring that filters remain in local memory while activations
    are streamed dynamically. For fully connected layers, output stationary execution
    is utilized to minimize redundant memory writes during matrix multiplications.
    Additionally, kernel fusion is integrated to combine activation functions, batch
    normalization, and element wise operations into a single computational step, thereby
    reducing intermediate memory traffic. Collectively, these approaches enhance computational
    efficiency and memory utilization, contributing to the overall performance of
    the network.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络中，混合策略经常被采用以优化性能。具体来说，权重静态执行应用于卷积层，确保滤波器保持在局部内存中，而激活动态流过。对于全连接层，输出静态执行被用来最小化矩阵乘法过程中的冗余内存写入。此外，核融合被集成以将激活函数、批量归一化和逐元素操作合并为单个计算步骤，从而减少中间内存流量。总体而言，这些方法提高了计算效率和内存利用率，从而有助于网络的总体性能。
- en: Transformers employ several strategies to enhance performance by optimizing
    memory usage and computational efficiency. Specifically, they use activation stationary
    mapping in self-attention layers to maximize the reuse of stored key-value pairs,
    thereby reducing memory fetches. In feedforward layers, weight stationary mapping
    is applied to ensure that large weight matrices are efficiently reused across
    computations. Additionally, these models incorporate fused attention kernels that
    integrate softmax and weighted summation into a single computation step, significantly
    enhancing execution speed ([Jacobs et al. 2002](ch058.xhtml#ref-dao2022flashattention)).
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器通过优化内存使用和计算效率来采用多种策略以提高性能。具体来说，它们在自注意力层中使用激活静态映射以最大化存储的关键值对的复用，从而减少内存访问。在前馈层中，应用权重静态映射以确保大型权重矩阵在计算中高效复用。此外，这些模型还集成了融合的注意力内核，将softmax和加权求和集成到单个计算步骤中，显著提高了执行速度（参见[Jacobs等人2002](ch058.xhtml#ref-dao2022flashattention)）。
- en: For multilayer perceptrons, hybrid mapping strategies are employed to optimize
    performance through a combination of techniques that enhance both memory efficiency
    and computational throughput. Specifically, weight stationary execution is utilized
    to maximize the reuse of weights across activations, ensuring that these frequently
    accessed parameters remain readily available and reduce redundant memory accesses.
    In addition, blocked tiling strategies are implemented for large matrix multiplications,
    which significantly improve cache locality by partitioning the computation into
    manageable sub-blocks that fit within fast memory. Complementing these approaches,
    general matrix multiplication fusion is applied, effectively reducing memory stalls
    by merging consecutive matrix multiplication operations with subsequent functional
    transformations. Collectively, these optimizations illustrate how tailored mapping
    strategies can systematically balance memory constraints with computational demands
    in multilayer perceptron architectures.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多层感知器，混合映射策略通过结合提高内存效率和计算吞吐量的技术来优化性能。具体来说，使用权重静态执行来最大化激活之间的权重重用，确保这些频繁访问的参数始终可用，并减少冗余内存访问。此外，对于大型矩阵乘法，实施分块填充策略，通过将计算划分为适合快速内存的管理子块来显著提高缓存局部性。补充这些方法，应用通用矩阵乘法融合，通过合并连续的矩阵乘法操作与后续的功能转换来有效减少内存停滞。总体而言，这些优化展示了如何通过定制映射策略在多层感知器架构中系统地平衡内存约束和计算需求。
- en: Hybrid mapping strategies are widely employed in vision transformers, which
    seamlessly integrate convolutional and self-attention operations. In these models,
    the patch embedding layer performs a convolution-like operation that benefits
    from weight stationary mapping ([Dosovitskiy et al. 2020](ch058.xhtml#ref-Dosovitskiy2020ViT)).
    The self-attention layers, on the other hand, require activation stationary execution
    to efficiently reuse the key-value cache across multiple queries. Additionally,
    the MLP component leverages general matrix multiplication fusion and blocked tiling
    to execute dense matrix multiplications efficiently. This layer-specific optimization
    framework effectively balances memory locality with computational efficiency,
    rendering vision transformers particularly well-suited for AI accelerators.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 混合映射策略在视觉Transformer中得到了广泛应用，这些模型无缝集成了卷积和自注意力操作。在这些模型中，补丁嵌入层执行类似于卷积的操作，并受益于权重静态映射（[Dosovitskiy等人2020](ch058.xhtml#ref-Dosovitskiy2020ViT)）。另一方面，自注意力层需要激活静态执行来有效地在多个查询之间重用键值缓存。此外，MLP组件利用通用矩阵乘法融合和分块填充来有效地执行密集矩阵乘法。这一针对特定层的优化框架有效地平衡了内存局部性和计算效率，使得视觉Transformer特别适合人工智能加速器。
- en: Hardware Implementations of Hybrid Strategies
  id: totrans-803
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合策略的硬件实现
- en: Several modern AI accelerators incorporate hybrid mapping strategies to optimize
    execution by tailoring layer-specific techniques to the unique computational requirements
    of diverse neural network architectures. For example, Google TPUs employ weight
    stationary mapping for convolutional layers and activation stationary mapping
    for attention layers within transformer models, ensuring that the most critical
    data remains in fast memory. Likewise, NVIDIA GPUs leverage fused kernels alongside
    hybrid memory layouts, which enable the application of different mapping strategies
    within the same model to maximize performance. In addition, Graphcore IPUs dynamically
    select execution strategies on a per-layer basis to optimize memory access, thereby
    enhancing overall computational efficiency.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 几种现代人工智能加速器采用了混合映射策略，通过针对不同神经网络架构的独特计算需求定制层特定技术来优化执行。例如，谷歌TPU在Transformer模型中对卷积层使用权重静态映射，对注意力层使用激活静态映射，确保最关键的数据保留在快速内存中。同样，NVIDIA
    GPU利用融合内核和混合内存布局，在同一个模型内应用不同的映射策略，以最大化性能。此外，Graphcore IPU根据每层动态选择执行策略，以优化内存访问，从而提高整体计算效率。
- en: These real-world implementations illustrate how hybrid mapping strategies bridge
    the gap between different types of machine learning computations, ensuring that
    each layer executes with maximum efficiency. However, hardware support is essential
    for these techniques to be practical. Accelerators must provide architectural
    features such as programmable memory hierarchies, efficient interconnects, and
    specialized execution pipelines to fully exploit hybrid mapping.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现实世界的实现说明了混合映射策略如何弥合不同类型机器学习计算之间的差距，确保每一层都以最大效率执行。然而，硬件支持对于这些技术的实用性至关重要。加速器必须提供可编程内存层次结构、高效的互连和专门的执行流水线等架构特性，以充分利用混合映射。
- en: Hybrid mapping provides a flexible and efficient approach to deep learning execution,
    enabling AI accelerators to adapt to the diverse computational requirements of
    modern architectures. By selecting the optimal mapping technique for each layer,
    hybrid strategies help reduce memory bandwidth constraints, improve data locality,
    and maximize parallelism.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 混合映射为深度学习执行提供了一种灵活且高效的方法，使AI加速器能够适应现代架构的多样化计算需求。通过为每一层选择最优的映射技术，混合策略有助于减少内存带宽限制，提高数据局部性，并最大化并行性。
- en: While hybrid mapping strategies offer an effective way to optimize computations
    at a layer-specific level, they remain static design-time optimizations. In real-world
    AI workloads, execution conditions can change dynamically due to varying input
    sizes, memory contention, or hardware resource availability. Machine learning
    compilers and runtime systems extend these mapping techniques by introducing dynamic
    scheduling, memory optimizations, and automatic tuning mechanisms. These systems
    ensure that hybrid strategies are not just predefined execution choices, but rather
    adaptive mechanisms that allow deep learning workloads to operate efficiently
    across different accelerators and deployment environments. In the next section,
    we explore how machine learning compilers and runtime stacks enable these adaptive
    optimizations through just-in-time scheduling, memory-aware execution, and workload
    balancing strategies.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然混合映射策略在层特定级别优化计算提供了一个有效的方法，但它们仍然是静态的设计时优化。在现实世界的AI工作负载中，由于输入大小、内存竞争或硬件资源可用性的变化，执行条件可以动态变化。机器学习编译器和运行时系统通过引入动态调度、内存优化和自动调整机制来扩展这些映射技术。这些系统确保混合策略不仅是一系列预定义的执行选择，而且是一种适应性机制，允许深度学习工作负载在不同的加速器和部署环境中高效运行。在下一节中，我们将探讨机器学习编译器和运行时堆栈如何通过即时调度、内存感知执行和工作负载平衡策略实现这些适应性优化。
- en: Compiler Support
  id: totrans-808
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译器支持
- en: The performance of machine learning acceleration depends not only on hardware
    capabilities but also on how efficiently models are translated into executable
    operations. These optimization techniques, including kernel fusion, tiling, memory
    scheduling, and data movement strategies, are essential for maximizing efficiency.
    However, these optimizations must be systematically applied before execution to
    ensure they align with hardware constraints and computational requirements.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习加速的性能不仅取决于硬件能力，还取决于模型如何有效地转换为可执行操作。包括内核融合、分块、内存调度和数据移动策略在内的这些优化技术对于最大化效率至关重要。然而，这些优化必须在执行前系统地应用，以确保它们与硬件约束和计算需求相一致。
- en: This process exemplifies the hardware-software co-design principle established
    in [Section 11.1](ch017.xhtml#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096),
    where machine learning compilers bridge high-level model representations with
    low-level hardware execution. The compiler optimizes models by restructuring computations,
    selecting efficient execution kernels, and maximizing hardware utilization ([0001
    et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018)). Unlike traditional compilers
    designed for general-purpose computing, ML workloads require specialized approaches
    for tensor computations and parallel execution.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程体现了在[第11.1节](ch017.xhtml#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096)中建立的硬件-软件协同设计原则，其中机器学习编译器将高级模型表示与低级硬件执行相连接。编译器通过重构计算、选择高效的执行内核和最大化硬件利用率来优化模型([0001
    et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018))。与为通用计算设计的传统编译器不同，机器学习工作负载需要针对张量计算和并行执行采用专门的方案。
- en: Compiler Design Differences for ML Workloads
  id: totrans-811
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习工作负载的编译器设计差异
- en: Machine learning workloads introduce unique challenges that traditional compilers
    were not designed to handle. Unlike conventional software execution, which primarily
    involves sequential or multi-threaded program flow, machine learning models are
    expressed as computation graphs that describe large-scale tensor operations. These
    graphs require specialized optimizations that traditional compilers cannot efficiently
    apply ([Cui, Li, and Xie 2019](ch058.xhtml#ref-cui_mlcompilers_2019)).
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载引入了传统编译器未设计来处理的独特挑战。与主要涉及顺序或多线程程序流的传统软件执行不同，机器学习模型以计算图的形式表达，描述了大规模的张量操作。这些图需要专门的优化，而传统编译器无法有效地应用这些优化（[Cui,
    Li, and Xie 2019](ch058.xhtml#ref-cui_mlcompilers_2019)）。
- en: '[Table 11.18](ch017.xhtml#tbl-ml-vs-traditional-compilers) outlines the fundamental
    differences between traditional compilers and those designed for machine learning
    workloads. While traditional compilers optimize linear program execution through
    techniques like instruction scheduling and register allocation, ML compilers focus
    on optimizing computation graphs for efficient tensor operations. This distinction
    is critical, as ML compilers must incorporate domain-specific transformations
    such as kernel fusion, memory-aware scheduling, and hardware-accelerated execution
    plans to achieve high performance on specialized accelerators like GPUs and TPUs.'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.18](ch017.xhtml#tbl-ml-vs-traditional-compilers)概述了传统编译器和为机器学习工作负载设计的编译器之间的基本差异。虽然传统编译器通过指令调度和寄存器分配等技术优化线性程序执行，但机器学习编译器则专注于优化计算图以实现高效的张量操作。这种区别至关重要，因为机器学习编译器必须结合领域特定的转换，如内核融合、内存感知调度和硬件加速执行计划，以在如GPU和TPU等专用加速器上实现高性能。'
- en: This comparison highlights why machine learning models require a different compilation
    approach. Instead of optimizing instruction-level execution, machine learning
    compilers must transform entire computation graphs, apply tensor-aware memory
    optimizations, and schedule operations across thousands of parallel processing
    elements. These requirements make traditional compiler techniques insufficient
    for modern deep learning workloads.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 这种比较突出了为什么机器学习模型需要不同的编译方法。机器学习编译器不是优化指令级执行，而是必须转换整个计算图，应用张量感知的内存优化，并在数千个并行处理元素之间调度操作。这些需求使得传统编译技术不足以应对现代深度学习工作负载。
- en: 'Table 11.18: **Compiler Optimization Priorities**: Traditional and machine
    learning compilers diverge in their optimization targets; traditional compilers
    prioritize efficient execution of sequential code, while ML compilers focus on
    optimizing tensor operations within computation graphs for specialized hardware.
    This table clarifies how ML compilers incorporate domain-specific transformations—like
    kernel fusion and memory-aware scheduling—to achieve high performance on accelerators,
    unlike the instruction scheduling and register allocation techniques used in conventional
    software compilation.'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.18：**编译器优化优先级**：传统编译器和机器学习编译器在优化目标上存在差异；传统编译器优先考虑顺序代码的高效执行，而机器学习编译器则专注于优化计算图中的张量操作以适应专用硬件。此表阐明了机器学习编译器如何结合领域特定的转换——如内核融合和内存感知调度——以在加速器上实现高性能，这与传统软件编译中使用的指令调度和寄存器分配技术不同。
- en: '| **Aspect** | **Traditional Compiler** | **Machine Learning Compiler** |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **传统编译器** | **机器学习编译器** |'
- en: '| --- | --- | --- |'
  id: totrans-817
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Input Representation** | Linear program code (C, Python) | Computational
    graph (ML models) |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| **输入表示** | 线性程序代码（C、Python） | 计算图（机器学习模型） |'
- en: '| **Execution Model** | Sequential or multi-threaded execution | Massively
    parallel tensor-based execution |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '| **执行模型** | 顺序或多线程执行 | 基于张量的大规模并行执行 |'
- en: '| **Optimization Priorities** | Instruction scheduling, loop unrolling, register
    allocation | Graph transformations, kernel fusion, memory-aware execution |'
  id: totrans-820
  prefs: []
  type: TYPE_TB
  zh: '| **优化优先级** | 指令调度、循环展开、寄存器分配 | 图转换、内核融合、内存感知执行 |'
- en: '| **Memory Management** | Stack and heap memory allocation | Tensor layout
    transformations, tiling, memory-aware scheduling |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
  zh: '| **内存管理** | 栈和堆内存分配 | 张量布局转换、分块、内存感知调度 |'
- en: '| **Target Hardware** | CPUs (general-purpose execution) | GPUs, TPUs, and
    custom accelerators |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
  zh: '| **目标硬件** | CPU（通用执行） | GPU、TPU和定制加速器 |'
- en: '| **Compilation Output** | CPU-specific machine code | Hardware-specific execution
    plan (kernels, memory scheduling) |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
  zh: '| **编译输出** | CPU特定的机器代码 | 硬件特定的执行计划（内核、内存调度） |'
- en: ML Compilation Pipeline
  id: totrans-824
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习编译流程
- en: Machine learning models, as defined in modern frameworks, are initially represented
    in a high-level computation graph that describes operations on tensors. However,
    these representations are not directly executable on hardware accelerators such
    as GPUs, TPUs, and custom AI chips. To achieve efficient execution, models must
    go through a compilation process that transforms them into optimized execution
    plans suited for the target hardware ([Brain 2020](ch058.xhtml#ref-tensorflow_xla_2020)).
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代框架中定义的机器学习模型最初以高级计算图的形式表示，该图描述了对张量的操作。然而，这些表示不能直接在GPU、TPU和定制AI芯片等硬件加速器上执行。为了实现高效执行，模型必须经过一个编译过程，将其转换为针对目标硬件优化的执行计划（[Brain
    2020](ch058.xhtml#ref-tensorflow_xla_2020)）。
- en: 'The machine learning compilation workflow consists of several key stages, each
    responsible for applying specific optimizations that ensure minimal memory overhead,
    maximum parallel execution, and optimal compute utilization. These stages include:'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习编译工作流程包括几个关键阶段，每个阶段负责应用特定的优化，以确保最小内存开销、最大并行执行和最佳计算利用率。这些阶段包括：
- en: '**Graph Optimization**: The computation graph is restructured to eliminate
    inefficiencies.'
  id: totrans-827
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图优化**：计算图被重构以消除低效部分。'
- en: '**Kernel Selection**: Each operation is mapped to an optimized hardware-specific
    implementation.'
  id: totrans-828
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内核选择**：每个操作映射到一个优化的特定于硬件的实现。'
- en: '**Memory Planning**: Tensor layouts and memory access patterns are optimized
    to reduce bandwidth consumption.'
  id: totrans-829
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内存规划**：张量布局和内存访问模式被优化以减少带宽消耗。'
- en: '**Computation Scheduling**: Workloads are distributed across parallel processing
    elements to maximize hardware utilization.'
  id: totrans-830
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算调度**：工作负载被分配到并行处理元素，以最大化硬件利用率。'
- en: '**Code Generation**: The optimized execution plan is translated into machine-specific
    instructions for execution.'
  id: totrans-831
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码生成**：优化的执行计划被转换为特定于机器的执行指令。'
- en: At each stage, the compiler applies theoretical optimizations discussed earlier,
    including kernel fusion, tiling, data movement strategies, and computation placement,
    ensuring that these optimizations are systematically incorporated into the final
    execution plan.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，编译器应用之前讨论过的理论优化，包括内核融合、分块、数据移动策略和计算放置，确保这些优化系统地纳入最终的执行计划。
- en: By understanding this workflow, we can see how machine learning acceleration
    is realized not just through hardware improvements but also through compiler-driven
    software optimizations.
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这个工作流程，我们可以看到机器学习加速不仅通过硬件改进，还通过编译器驱动的软件优化来实现。
- en: Graph Optimization
  id: totrans-834
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图优化
- en: AI accelerators provide specialized hardware to speed up computation, but raw
    model representations are not inherently optimized for execution on these accelerators.
    Machine learning frameworks define models using high-level computation graphs,
    where nodes represent operations (such as convolutions, matrix multiplications,
    and activations), and edges define data dependencies. However, if executed as
    defined, these graphs often contain redundant operations, inefficient memory access
    patterns, and suboptimal execution sequences that can prevent the hardware from
    operating at peak efficiency.
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: AI加速器提供专门的硬件以加快计算速度，但原始模型表示并非天生就针对这些加速器的执行进行了优化。机器学习框架使用高级计算图定义模型，其中节点表示操作（如卷积、矩阵乘法和激活），边定义数据依赖关系。然而，如果按照定义执行，这些图通常包含冗余操作、低效的内存访问模式和不理想的执行序列，这可能会阻止硬件以峰值效率运行。
- en: For example, in a Transformer model, the self-attention mechanism involves repeated
    accesses to the same key-value pairs across multiple attention heads. If compiled
    naïvely, the model may reload the same data multiple times, leading to excessive
    memory traffic ([Shoeybi et al. 2019a](ch058.xhtml#ref-shoeybi_megatron_2020)).
    Similarly, in a CNN, applying batch normalization and activation functions as
    separate operations after each convolution leads to unnecessary intermediate memory
    writes, increasing memory bandwidth usage. These inefficiencies are addressed
    during graph optimization, where the compiler restructures the computation graph
    to eliminate unnecessary operations and improve memory locality ([0001 et al.
    2018a](ch058.xhtml#ref-chen_tvmlang_2018)).
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在Transformer模型中，自注意力机制涉及在多个注意力头之间重复访问相同的键值对。如果简单编译，模型可能会多次重新加载相同的数据，导致过度的内存流量（[Shoeybi等人2019a](ch058.xhtml#ref-shoeybi_megatron_2020)）。同样，在CNN中，在每个卷积之后将批量归一化和激活函数作为单独的操作应用会导致不必要的中间内存写入，增加内存带宽使用。这些低效性在图优化阶段得到解决，编译器重构计算图以消除不必要的操作并提高内存局部性（[0001等人2018a](ch058.xhtml#ref-chen_tvmlang_2018)）。
- en: The graph optimization phase of compilation is responsible for transforming
    this high-level computation graph into an optimized execution plan before it is
    mapped to hardware. Rather than requiring manual optimization, the compiler systematically
    applies transformations that improve data movement, reduce redundant computations,
    and restructure operations for efficient parallel execution ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)).
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 编译的图优化阶段负责在将其映射到硬件之前将这个高级计算图转换为优化的执行计划。编译器不需要手动优化，而是系统地应用改进数据移动、减少冗余计算和重构操作以实现高效并行执行的变化（[NVIDIA
    2021](ch058.xhtml#ref-nvidia_tensorRT_2021)）。
- en: At this stage, the compiler is still working at a hardware-agnostic level, focusing
    on high-level restructuring that improves efficiency before more hardware-specific
    optimizations are applied later.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，编译器仍然在硬件无关的层面上工作，专注于在后续应用更具体的硬件优化之前，提高效率的高级重构。
- en: Computation Graph Optimization
  id: totrans-839
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算图优化
- en: Graph optimization transforms the computation graph through a series of structured
    techniques designed to enhance execution efficiency. One key technique is kernel
    fusion, which merges consecutive operations to eliminate unnecessary memory writes
    and reduce the number of kernel launches. This approach is particularly effective
    in convolutional neural networks, where fusing convolution, batch normalization,
    and activation functions notably accelerates processing. Another important technique
    is computation reordering, which adjusts the execution order of operations to
    improve data locality and maximize parallel execution. For instance, in Transformer
    models, such reordering enables the reuse of cached key-value pairs rather than
    reloading them repeatedly from memory, thereby reducing latency.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 图优化通过一系列旨在提高执行效率的结构化技术来转换计算图。其中一个关键技术是内核融合，它将连续的操作合并以消除不必要的内存写入并减少内核启动次数。这种方法在卷积神经网络中特别有效，其中融合卷积、批量归一化和激活函数可以显著加速处理。另一个重要技术是计算重排，它调整操作的执行顺序以改善数据局部性和最大化并行执行。例如，在Transformer模型中，这种重排使得可以重复使用缓存的键值对，而不是反复从内存中重新加载，从而降低延迟。
- en: Additionally, redundant computation elimination plays an important role. By
    identifying and removing duplicate or unnecessary operations, this method is especially
    beneficial in models with residual connections where common subexpressions might
    otherwise be redundantly computed. Memory-aware dataflow adjustments enhance overall
    performance by refining tensor layouts and optimizing memory movement. For example,
    tiling matrix multiplications to meet the structural requirements of systolic
    arrays in TPUs ensures that hardware resources are utilized optimally. This combined
    approach not only reduces unnecessary processing but also aligns data storage
    and movement with the accelerator’s strengths, leading to efficient execution
    across diverse AI workloads. Together, these techniques prepare the model for
    acceleration by minimizing overhead and ensuring an optimal balance between computational
    and memory resources.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，冗余计算消除也发挥着重要作用。通过识别和删除重复或不必要的操作，这种方法在具有残差连接的模型中特别有益，在这些模型中，常见的子表达式可能会被冗余计算。内存感知的数据流调整通过优化张量布局和内存移动来提高整体性能。例如，将矩阵乘法分块以满足TPU中收缩阵列的结构要求，确保硬件资源得到最优利用。这种结合的方法不仅减少了不必要的处理，而且使数据存储和移动与加速器的优势保持一致，从而在多样化的AI工作负载中实现高效的执行。这些技术共同通过最小化开销并确保计算资源和内存资源之间的最佳平衡，为模型的加速做准备。
- en: Implementation in AI Compilers
  id: totrans-842
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在AI编译器中的实现
- en: Modern AI compilers perform graph optimization through the use of automated
    pattern recognition and structured rewrite rules, systematically transforming
    computation graphs to maximize efficiency without manual intervention. For example,
    Google’s XLA (Accelerated Linear Algebra) in TensorFlow applies graph-level transformations
    such as fusion and layout optimizations that streamline execution on TPUs and
    GPUs. Similarly, TVM (Tensor Virtual Machine) not only refines tensor layouts
    and adjusts computational structures but also tunes execution strategies across
    diverse hardware backends, which is particularly beneficial for deploying models
    on embedded Tiny ML devices with strict memory constraints.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI编译器通过使用自动模式识别和结构化重写规则来执行图优化，系统地转换计算图以最大化效率，而无需人工干预。例如，TensorFlow中的Google
    XLA（加速线性代数）在TPU和GPU上应用图级转换，如融合和布局优化，以简化执行。同样，TVM（张量虚拟机）不仅优化张量布局和调整计算结构，还针对不同的硬件后端调整执行策略，这对于在具有严格内存约束的嵌入式Tiny
    ML设备上部署模型特别有益。
- en: NVIDIA’s TensorRT, another specialized deep learning compiler, focuses on minimizing
    kernel launch overhead by fusing operations and optimizing execution scheduling
    on GPUs, thereby improving utilization and reducing inference latency in large-scale
    convolutional neural network applications. Additionally, MLIR (Multi-Level Intermediate
    Representation) facilitates flexible graph optimization across various AI accelerators
    by enabling multi-stage transformations that improve execution order and memory
    access patterns, thus easing the transition of models from CPU-based implementations
    to accelerator-optimized versions. These compilers preserve the mathematical integrity
    of the models while rewriting the computation graph to ensure that the subsequent
    hardware-specific optimizations can be effectively applied.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的TensorRT，另一个专门的深度学习编译器，通过融合操作和优化GPU上的执行调度来最小化内核启动开销，从而提高大规模卷积神经网络应用中的利用率并减少推理延迟。此外，MLIR（多级中间表示）通过启用多阶段转换来促进跨各种AI加速器的灵活图优化，这些转换可以改善执行顺序和内存访问模式，从而简化模型从基于CPU的实现到加速器优化版本的过渡。这些编译器在重写计算图以确保后续的硬件特定优化可以有效地应用的同时，保留了模型的数学完整性。
- en: Graph Optimization Importance
  id: totrans-845
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图优化的重要性
- en: Graph optimization enables AI accelerators to operate at peak efficiency. Without
    this phase, even the most optimized hardware would be underutilized, as models
    would be executed in a way that introduces unnecessary memory stalls, redundant
    computations, and inefficient data movement. By systematically restructuring computation
    graphs, the compiler arranges operations for efficient execution that mitigates
    bottlenecks before mapping to hardware, minimizes memory movement to keep tensors
    in high-speed memory, and optimizes parallel execution to reduce unnecessary serialization
    while enhancing hardware utilization. For instance, without proper graph optimization,
    a large Transformer model running on an edge device may experience excessive memory
    stalls due to suboptimal data access patterns; however, through effective graph
    restructuring, the model can operate with significantly reduced memory bandwidth
    consumption and latency, thus enabling real-time inference on devices with constrained
    resources.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 图优化使AI加速器能够以峰值效率运行。如果没有这个阶段，即使是最优化的硬件也会被低效利用，因为模型将以引入不必要的内存停滞、冗余计算和低效数据移动的方式执行。通过系统地重构计算图，编译器安排操作以实现高效的执行，在映射到硬件之前缓解瓶颈，最小化内存移动以保持张量在高速内存中，并优化并行执行以减少不必要的序列化并提高硬件利用率。例如，如果没有适当的图优化，运行在边缘设备上的大型Transformer模型可能会因为次优的数据访问模式而经历过度的内存停滞；然而，通过有效的图重构，模型可以以显著降低的内存带宽消耗和延迟运行，从而在资源受限的设备上实现实时推理。
- en: With the computation graph now fully optimized, the next step in compilation
    is kernel selection, where the compiler determines which hardware-specific implementation
    should be used for each operation. This ensures that the structured execution
    plan is translated into optimized low-level instructions for the target accelerator.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图现在已完全优化，编译的下一步是内核选择，编译器确定每个操作应使用哪个硬件特定的实现。这确保了结构化执行计划被转换为针对目标加速器的优化低级指令。
- en: Kernel Selection
  id: totrans-848
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核选择
- en: At this stage, the compiler translates the abstract operations in the computation
    graph into optimized low-level functions, ensuring that execution is performed
    as efficiently as possible given the constraints of the target accelerator. A
    kernel is a specialized implementation of a computational operation designed to
    run efficiently on a particular hardware architecture. Most accelerators, including
    GPUs, TPUs, and custom AI chips, provide multiple kernel implementations for the
    same operation, each optimized for different execution scenarios. Choosing the
    right kernel for each operation is essential for maximizing computational throughput,
    minimizing memory stalls, and ensuring that the accelerator’s specialized processing
    elements are fully utilized ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)).
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，编译器将计算图中的抽象操作转换为优化后的低级函数，确保在目标加速器的约束下执行尽可能高效。内核是针对特定硬件架构高效运行的计算操作的专用实现。大多数加速器，包括GPU、TPU和定制AI芯片，为同一操作提供多个内核实现，每个实现针对不同的执行场景进行了优化。为每个操作选择正确的内核对于最大化计算吞吐量、最小化内存停滞并确保加速器的专用处理元素得到充分利用至关重要
    ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021))。
- en: Kernel selection builds upon the graph optimization phase, ensuring that the
    structured execution plan is mapped to the most efficient implementation available.
    While graph optimization eliminates inefficiencies at the model level, kernel
    selection ensures that each individual operation is executed using the most efficient
    hardware-specific routine. The effectiveness of this process directly impacts
    the model’s overall performance, as poor kernel choices can nullify the benefits
    of prior optimizations by introducing unnecessary computation overhead or memory
    bottlenecks ([0001 et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018)).
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 内核选择建立在图优化阶段之上，确保结构化执行计划映射到最有效的实现。虽然图优化消除了模型层面的低效，但内核选择确保每个单独的操作都使用最有效的硬件特定例程执行。此过程的有效性直接影响模型的总体性能，因为较差的内核选择可能会通过引入不必要的计算开销或内存瓶颈来抵消先前优化的好处
    ([0001 et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018))。
- en: In a Transformer model, the matrix multiplications that dominate self-attention
    computations can be executed using different strategies depending on the available
    hardware. On a CPU, a general-purpose matrix multiplication routine is typically
    employed, exploiting vectorized execution to improve efficiency. In contrast,
    on a GPU, the compiler may select an implementation that leverages tensor cores
    to accelerate matrix multiplications using mixed-precision arithmetic. When the
    model is deployed on a TPU, the operation can be mapped onto a systolic array,
    ensuring that data flows through the accelerator in a manner that maximizes reuse
    and minimizes off-chip memory accesses. Additionally, for inference workloads,
    an integer arithmetic kernel may be preferable, as it facilitates computations
    in INT8 instead of floating-point precision, thereby reducing power consumption
    without significantly compromising accuracy.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 模型中，主导自注意力计算的矩阵乘法可以根据可用的硬件采用不同的策略执行。在 CPU 上，通常使用通用矩阵乘法例程，利用向量化执行来提高效率。相比之下，在
    GPU 上，编译器可能会选择一种利用张量核心来加速矩阵乘法并使用混合精度算术的实施方案。当模型部署在 TPU 上时，操作可以映射到一个脉动阵列，确保数据以最大化重用和最小化片外内存访问的方式通过加速器。此外，对于推理工作负载，整数算术内核可能更可取，因为它便于在
    INT8 而不是浮点精度下进行计算，从而在不显著降低精度的同时减少功耗。
- en: In many cases, compilers do not generate custom kernels from scratch but instead
    select from vendor-optimized kernel libraries that provide highly tuned implementations
    for different architectures. For instance, cuDNN and cuBLAS offer optimized kernels
    for deep learning on NVIDIA GPUs, while oneDNN provides optimized execution for
    Intel architectures. Similarly, ACL (Arm Compute Library) is optimized for Arm-based
    devices, and Eigen and BLIS provide efficient CPU-based implementations of deep
    learning operations. These libraries allow the compiler to choose pre-optimized,
    high-performance kernels rather than having to reinvent execution strategies for
    each hardware platform.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，编译器不是从头开始生成自定义内核，而是从供应商优化的内核库中选择，这些库为不同的架构提供了高度优化的实现。例如，cuDNN 和 cuBLAS
    为 NVIDIA GPU 上的深度学习提供了优化内核，而 oneDNN 为 Intel 架构提供了优化执行。同样，ACL（Arm Compute Library）针对基于
    Arm 的设备进行了优化，Eigen 和 BLIS 为深度学习操作提供了高效的基于 CPU 的实现。这些库允许编译器选择预优化的、高性能的内核，而不是为每个硬件平台重新发明执行策略。
- en: Implementation in AI Compilers
  id: totrans-853
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 AI 编译器中的实现
- en: AI compilers use heuristics, profiling, and cost models to determine the best
    kernel for each operation. These strategies ensure that each computation is executed
    in a way that maximizes throughput and minimizes memory bottlenecks.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: AI 编译器使用启发式方法、配置文件分析和成本模型来确定每个操作的最佳内核。这些策略确保每个计算都以最大化吞吐量和最小化内存瓶颈的方式进行执行。
- en: In rule-based selection, the compiler applies predefined heuristics based on
    the known capabilities of the hardware. For instance, XLA, the compiler used in
    TensorFlow, automatically selects tensor core-optimized kernels for NVIDIA GPUs
    when mixed-precision execution is enabled. These predefined rules allow the compiler
    to make fast, reliable decisions about which kernel to use without requiring extensive
    analysis.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于规则的选取中，编译器根据硬件的已知能力应用预定义的启发式方法。例如，TensorFlow 中使用的编译器 XLA，当启用混合精度执行时，会自动选择针对
    NVIDIA GPU 的张量核心优化内核。这些预定义的规则允许编译器快速、可靠地做出关于使用哪个内核的决定，而无需进行广泛的分析。
- en: Profile-guided selection takes a more dynamic approach, benchmarking different
    kernel options and choosing the one that performs best for a given workload. TVM,
    an open-source AI compiler, uses AutoTVM to empirically evaluate kernel performance,
    tuning execution strategies based on real-world execution times. By testing different
    kernels before deployment, profile-guided selection helps ensure that operations
    are assigned to the most efficient implementation under actual execution conditions.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 基于配置文件的选择采用了一种更动态的方法，基准测试不同的内核选项，并选择对给定工作负载表现最佳的选项。开源 AI 编译器 TVM 使用 AutoTVM
    来经验性地评估内核性能，根据实际的执行时间调整执行策略。通过在部署前测试不同的内核，基于配置文件的选择有助于确保操作在实际执行条件下被分配给最有效的实现。
- en: Another approach, cost model-based selection, relies on performance predictions
    to estimate execution time and memory consumption for various kernels before choosing
    the most efficient one. MLIR, a compiler infrastructure designed for machine learning
    workloads, applies this technique to determine the most effective tiling and memory
    access strategies ([Lattner et al. 2020](ch058.xhtml#ref-mlir_framework_2021)).
    By modeling how different kernels interact with the accelerator’s compute units
    and memory hierarchy, the compiler can select the kernel that minimizes execution
    cost while maximizing performance.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于成本模型的选择方法，依赖于性能预测来估计各种内核的执行时间和内存消耗，在选择了最有效的内核之后。MLIR是一种为机器学习工作负载设计的编译器基础设施，它应用了这种技术来确定最有效的瓦片和内存访问策略（[Lattner等人2020](ch058.xhtml#ref-mlir_framework_2021)）。通过模拟不同内核如何与加速器的计算单元和内存层次结构交互，编译器可以选择在最大化性能的同时最小化执行成本的内核。
- en: Many AI compilers also incorporate precision-aware kernel selection, where the
    selected kernel is optimized for specific numerical formats such as FP32, FP16,
    BF16, or INT8\. Training workloads often prioritize higher precision (FP32, BF16)
    to maintain model accuracy, whereas inference workloads favor lower precision
    (FP16, INT8) to increase speed and reduce power consumption. For example, an NVIDIA
    GPU running inference with TensorRT can dynamically select FP16 or INT8 kernels
    based on a model’s accuracy constraints. This trade-off between precision and
    performance is a key aspect of kernel selection, especially when deploying models
    in resource-constrained environments.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人工智能编译器还集成了精度感知的内核选择，所选内核针对特定的数值格式进行优化，例如FP32、FP16、BF16或INT8。训练工作负载通常优先考虑更高的精度（FP32、BF16），以保持模型精度，而推理工作负载则更倾向于较低的精度（FP16、INT8），以提高速度并减少功耗。例如，运行TensorRT进行推理的NVIDIA
    GPU可以根据模型的精度约束动态选择FP16或INT8内核。精度与性能之间的权衡是内核选择的关键方面，尤其是在资源受限的环境中部署模型时。
- en: Some compilers go beyond static kernel selection and implement adaptive kernel
    tuning, where execution strategies are adjusted at runtime based on the system’s
    workload and available resources. AutoTVM in TVM measures kernel performance across
    different workloads and dynamically refines execution strategies. TensorRT applies
    real-time optimizations based on batch size, memory constraints, and GPU load,
    adjusting kernel selection dynamically. Google’s TPU compiler takes a similar
    approach, optimizing kernel selection based on cloud resource availability and
    execution environment constraints.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编译器不仅超越了静态内核选择，还实现了自适应内核调整，即在运行时根据系统的负载和可用资源调整执行策略。TVM中的AutoTVM通过在不同工作负载上测量内核性能，动态地优化执行策略。TensorRT根据批大小、内存约束和GPU负载进行实时优化，动态调整内核选择。谷歌的TPU编译器采取了类似的方法，根据云资源可用性和执行环境约束优化内核选择。
- en: Kernel Selection Importance
  id: totrans-860
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内核选择的重要性
- en: The efficiency of AI acceleration depends not only on how computations are structured
    but also on how they are executed. Even the best-designed computation graph will
    fail to achieve peak performance if the selected kernels do not fully utilize
    the hardware’s capabilities.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速的效率不仅取决于计算的构建方式，还取决于它们的执行方式。即使是最精心设计的计算图，如果所选内核没有充分利用硬件的能力，也无法达到峰值性能。
- en: Proper kernel selection allows models to execute using the most efficient algorithms
    available for the given hardware, ensuring that memory is accessed in a way that
    avoids unnecessary stalls and that specialized acceleration features, such as
    tensor cores or systolic arrays, are leveraged wherever possible. Selecting an
    inappropriate kernel can lead to underutilized compute resources, excessive memory
    transfers, and increased power consumption, all of which limit the performance
    of AI accelerators.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的内核选择允许模型使用针对给定硬件的最有效算法执行，确保内存访问方式避免不必要的停滞，并在可能的情况下利用专门的加速功能，例如张量核心或脉动阵列。选择不适当的内核可能导致计算资源利用率低下、内存传输过多以及功耗增加，所有这些都会限制人工智能加速器的性能。
- en: For instance, if a Transformer model running on a GPU is assigned a non-tensor-core
    kernel for its matrix multiplications, it may execute at only a fraction of the
    possible performance. Conversely, if a model designed for FP32 execution is forced
    to run on an INT8-optimized kernel, it may experience significant numerical instability,
    degrading accuracy. These choices illustrate why kernel selection is as much about
    maintaining numerical correctness as it is about optimizing performance.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个在GPU上运行的Transformer模型被分配了一个非张量核心内核进行矩阵乘法，它可能只能达到可能性能的一小部分。相反，如果一个为FP32执行设计的模型被迫在一个INT8优化的内核上运行，它可能会经历显著的数值不稳定性，降低精度。这些选择说明了为什么内核选择与保持数值正确性一样，关乎优化性能。
- en: With kernel selection complete, the next stage in compilation involves execution
    scheduling and memory management, where the compiler determines how kernels are
    launched and how data is transferred between different levels of the memory hierarchy.
    These final steps in the compilation pipeline ensure that computations run with
    maximum parallelism while minimizing the overhead of data movement. As kernel
    selection determines what to execute, execution scheduling and memory management
    dictate when and how those kernels are executed, ensuring that AI accelerators
    operate at peak efficiency.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 内核选择完成后，编译的下一阶段涉及执行调度和内存管理，其中编译器确定如何启动内核以及如何在内存层次结构的不同级别之间传输数据。这些编译管道的最后一步确保计算以最大并行性运行，同时最小化数据移动的开销。内核选择决定了要执行的内容，而执行调度和内存管理则决定了这些内核何时以及如何执行，确保人工智能加速器以最高效率运行。
- en: Memory Planning
  id: totrans-865
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存规划
- en: The memory planning phase ensures that data is allocated and accessed in a way
    that minimizes memory bandwidth consumption, reduces latency, and maximizes cache
    efficiency ([Y. Zhang, Li, and Ouyang 2020](ch058.xhtml#ref-zhang2020optimizing)).
    Even with the most optimized execution plan, a model can still suffer from severe
    performance degradation if memory is not managed efficiently.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 内存规划阶段确保数据以最小化内存带宽消耗、降低延迟和最大化缓存效率的方式分配和访问([Y. Zhang, Li, 和 Ouyang 2020](ch058.xhtml#ref-zhang2020optimizing))。即使是最优化的执行计划，如果内存管理效率不高，模型仍然可能遭受严重的性能下降。
- en: Machine learning workloads are often memory-intensive. They require frequent
    movement of large tensors between different levels of the memory hierarchy. The
    compiler must determine how tensors are stored, how they are accessed, and how
    intermediate results are handled to ensure that memory does not become a bottleneck.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载通常是内存密集型的。它们需要在内存层次结构的多个级别之间频繁移动大型张量。编译器必须确定张量的存储方式、访问方式以及中间结果的处理方式，以确保内存不会成为瓶颈。
- en: The memory planning phase focuses on optimizing tensor layouts, memory access
    patterns, and buffer reuse to prevent unnecessary stalls and memory contention
    during execution. In this phase, tensors are arranged in a memory-efficient format
    that aligns with hardware access patterns, thereby minimizing the need for format
    conversions. Additionally, memory accesses are structured to reduce cache misses
    and stalls, which in turn lowers overall bandwidth consumption. Buffer reuse is
    also a critical aspect, as it reduces redundant memory allocations by intelligently
    managing intermediate results. Together, these strategies ensure that data is
    efficiently placed and accessed, thereby enhancing both computational performance
    and energy efficiency in AI workloads.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 内存规划阶段专注于优化张量布局、内存访问模式和缓冲区重用，以防止执行过程中的不必要的停滞和内存竞争。在这个阶段，张量以内存高效的格式排列，与硬件访问模式相一致，从而最小化格式转换的需要。此外，内存访问被结构化以减少缓存未命中和停滞，这反过来又降低了整体带宽消耗。缓冲区重用也是一个关键方面，因为它通过智能管理中间结果来减少冗余内存分配。这些策略共同确保数据被有效地放置和访问，从而在人工智能工作负载中提高计算性能和能源效率。
- en: Implementation in AI Compilers
  id: totrans-869
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在人工智能编译器中的实现
- en: Memory planning is a complex problem because AI models must balance memory availability,
    reuse, and access efficiency while operating across multiple levels of the memory
    hierarchy. AI compilers use several key strategies to manage memory effectively
    and prevent unnecessary data movement.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 内存规划是一个复杂的问题，因为人工智能模型必须在跨越内存层次结构的多个级别上平衡内存可用性、重用和访问效率。人工智能编译器使用几种关键策略来有效地管理内存并防止不必要的数据移动。
- en: The first step in memory planning is tensor layout optimization, where the compiler
    determines how tensors should be arranged in memory to maximize locality and prevent
    unnecessary data format conversions. Different hardware accelerators have different
    preferred storage layouts—for instance, NVIDIA GPUs often use row-major storage
    (NHWC format), while TPUs favor channel-major layouts (NCHW format) to optimize
    memory coalescing ([Martín Abadi et al. 2016](ch058.xhtml#ref-abadi2016tensorflow)).
    The compiler automatically transforms tensor layouts based on the expected access
    patterns of the target hardware, ensuring that memory accesses are aligned for
    maximum efficiency.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 内存规划的第一步是张量布局优化，编译器确定张量在内存中的排列方式，以最大化局部性并防止不必要的格式转换。不同的硬件加速器有不同的首选存储布局——例如，NVIDIA
    GPU通常使用行主序存储（NHWC格式），而TPU则偏好通道主序布局（NCHW格式）以优化内存归约（[Martín Abadi等人 2016](ch058.xhtml#ref-abadi2016tensorflow)）。编译器根据目标硬件预期的访问模式自动转换张量布局，确保内存访问对齐以实现最大效率。
- en: Beyond layout optimization, memory planning also includes buffer allocation
    and reuse, where the compiler minimizes memory footprint by reusing intermediate
    storage whenever possible. Deep learning workloads generate many temporary tensors,
    such as activations and gradients, which can quickly overwhelm on-chip memory
    if not carefully managed. Instead of allocating new memory for each tensor, the
    compiler analyzes the computation graph to identify opportunities for buffer reuse,
    ensuring that intermediate values are stored and overwritten efficiently ([G.
    A. Jones 2018](ch058.xhtml#ref-moreau2018relay)).
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 除了布局优化之外，内存规划还包括缓冲区分配和重用，编译器通过尽可能重用中间存储来最小化内存占用。深度学习工作负载生成许多临时张量，如激活和梯度，如果不加管理，这些张量可能会迅速耗尽片上内存。而不是为每个张量分配新的内存，编译器分析计算图以识别缓冲区重用的机会，确保中间值被高效地存储和覆盖（[G.
    A. Jones 2018](ch058.xhtml#ref-moreau2018relay)）。
- en: Another critical aspect of memory planning is minimizing data movement between
    different levels of the memory hierarchy. AI accelerators typically have a mix
    of high-speed on-chip memory (such as caches or shared SRAM) and larger, but slower,
    external DRAM. If tensor data is repeatedly moved between these memory levels,
    the model may become memory-bound, reducing computational efficiency. To prevent
    this, compilers use tiling strategies that break large computations into smaller,
    memory-friendly chunks, allowing execution to fit within fast, local memory and
    reducing the need for costly off-chip memory accesses.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 内存规划的另一个关键方面是尽量减少不同内存层次之间的数据移动。人工智能加速器通常包含高速片上内存（如缓存或共享SRAM）和更大但较慢的外部DRAM。如果张量数据在这些内存层次之间反复移动，模型可能会变得内存受限，降低计算效率。为了防止这种情况，编译器使用分块策略，将大型计算分解成更小、内存友好的块，使执行能够适应快速、局部内存，并减少对昂贵的片外内存访问的需求。
- en: Memory Planning Importance
  id: totrans-874
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存规划的重要性
- en: Without proper memory planning, even the most optimized computation graph and
    kernel selection will fail to deliver high performance. Excessive memory transfers,
    inefficient memory layouts, and redundant memory allocations can all lead to bottlenecks
    that prevent AI accelerators from reaching their peak throughput.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 没有适当的内存规划，即使是最优化的计算图和内核选择也无法实现高性能。过度的内存传输、低效的内存布局和冗余的内存分配都可能造成瓶颈，阻碍人工智能加速器达到其峰值吞吐量。
- en: For instance, a CNN running on a GPU may achieve high computational efficiency
    in theory, but if its convolutional feature maps are stored in an incompatible
    format, for example, if it uses a row-major layout that necessitates conversion
    to a channel-friendly format such as NCHW or a variant like NHCW, constant tensor
    format conversions can introduce significant overhead. Similarly, a Transformer
    model deployed on an edge device may struggle to meet real-time inference requirements
    if memory is not carefully planned, leading to frequent off-chip memory accesses
    that increase latency and power consumption.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个GPU上运行的卷积神经网络（CNN）在理论上可能实现高计算效率，但如果其卷积特征图以不兼容的格式存储，例如，如果它使用需要转换为NCHW或类似格式的行主序布局，那么频繁的张量格式转换会引入显著的开销。同样，如果内存规划不当，部署在边缘设备上的Transformer模型可能难以满足实时推理要求，导致频繁的片外内存访问，增加延迟和功耗。
- en: Through careful management of tensor placement, optimizing memory access patterns,
    and reducing unnecessary data movement, memory planning guarantees efficient operation
    of AI accelerators, leading to tangible performance improvements in real-world
    applications.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细管理张量放置、优化内存访问模式和减少不必要的数据移动，内存规划保证了 AI 加速器的有效运行，从而在实际应用中带来了可观的性能提升。
- en: Computation Scheduling
  id: totrans-878
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算调度
- en: With graph optimization completed, kernels selected, and memory planning finalized,
    the next step in the compilation pipeline is computation scheduling. This phase
    determines when and where each computation should be executed, ensuring that workloads
    are efficiently distributed across available processing elements while avoiding
    unnecessary stalls and resource contention ([Rajbhandari et al. 2020a](ch058.xhtml#ref-Rajbhandari2020);
    [Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)).
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成图优化、选择内核和最终确定内存规划之后，编译流程中的下一步是计算调度。这一阶段决定每个计算何时何地执行，确保工作负载在可用的处理元素之间高效分布，同时避免不必要的停滞和资源竞争
    ([Rajbhandari 等人 2020a](ch058.xhtml#ref-Rajbhandari2020); [Zheng 等人 2020](ch058.xhtml#ref-Zheng2020))。
- en: AI accelerators achieve high performance through massive parallelism, but without
    an effective scheduling strategy, computational units may sit idle, memory bandwidth
    may be underutilized, and execution efficiency may degrade. Computation scheduling
    is responsible for ensuring that all processing elements remain active, execution
    dependencies are managed correctly, and workloads are distributed optimally ([Ziheng
    Jia et al. 2019](ch058.xhtml#ref-Jia2019)).
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: AI 加速器通过大规模并行化实现高性能，但没有有效的调度策略，计算单元可能会闲置，内存带宽可能未被充分利用，执行效率可能会下降。计算调度负责确保所有处理元素保持活跃，执行依赖关系得到正确管理，并且工作负载得到优化分配
    ([Ziheng Jia 等人 2019](ch058.xhtml#ref-Jia2019))。
- en: In the scheduling phase, parallel execution, synchronization, and resource allocation
    are managed systematically. Task partitioning decomposes extensive computations
    into smaller, manageable tasks that can be distributed efficiently among multiple
    compute cores. Execution order optimization then determines the most effective
    sequence for launching these operations, maximizing hardware performance while
    reducing execution stalls. Additionally, resource allocation and synchronization
    are orchestrated to ensure that compute cores, memory bandwidth, and shared caches
    are utilized effectively, avoiding contention. Through these coordinated strategies,
    computation scheduling achieves optimal hardware utilization, minimizes memory
    access delays, and supports a streamlined and efficient execution process.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度阶段，并行执行、同步和资源分配得到系统化管理。任务分区将大量计算分解为更小、更易于管理的任务，这些任务可以高效地分配到多个计算核心。执行顺序优化随后确定启动这些操作的最有效顺序，最大化硬件性能同时减少执行停滞。此外，资源分配和同步得到协调，以确保计算核心、内存带宽和共享缓存得到有效利用，避免竞争。通过这些协调策略，计算调度实现了最优的硬件利用率，最小化了内存访问延迟，并支持了流畅高效的执行过程。
- en: Implementation in AI Compilers
  id: totrans-882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AI 编译器中的实现
- en: Computation scheduling is highly dependent on the underlying hardware architecture,
    as different AI accelerators have unique execution models that must be considered
    when determining how workloads are scheduled. AI compilers implement several key
    strategies to optimize scheduling for efficient execution.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 计算调度高度依赖于底层硬件架构，因为不同的 AI 加速器具有独特的执行模型，在确定工作负载如何调度时必须考虑这些模型。AI 编译器实施了几种关键策略来优化调度以实现高效执行。
- en: One of the most fundamental aspects of scheduling is task partitioning, where
    the compiler divides large computational graphs into smaller, manageable units
    that can be executed in parallel. On GPUs, this typically means mapping matrix
    multiplications and convolutions to thousands of CUDA cores, while on TPUs, tasks
    are partitioned to fit within systolic arrays that operate on structured data
    flows ([Norrie et al. 2021](ch058.xhtml#ref-norrie2021design)). In CPUs, partitioning
    is often focused on breaking computations into vectorized chunks that align with
    SIMD execution. The goal is to map workloads to available processing units efficiently,
    ensuring that each core remains active throughout execution.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 调度中最基本的方面之一是任务划分，编译器将大的计算图划分为更小、更易于管理的单元，以便并行执行。在GPU上，这通常意味着将矩阵乘法和卷积映射到数千个CUDA核心，而在TPU上，任务被划分以适应操作结构化数据流的脉冲阵列（[Norrie等人2021](ch058.xhtml#ref-norrie2021design)）。在CPU上，划分通常侧重于将计算分解为与SIMD执行对齐的矢量化块。目标是高效地将工作负载映射到可用的处理单元，确保每个核心在整个执行过程中保持活跃。
- en: Scheduling involves optimizing execution order to minimize dependencies and
    maximize throughput beyond task partitioning. Many AI models include operations
    that can be computed independently (e.g., different batches in a batch processing
    pipeline) alongside operations that have strict dependencies (e.g., recurrent
    layers in an RNN). AI compilers analyze these dependencies and attempt to rearrange
    execution where possible, reducing idle time and improving parallel efficiency.
    For example, in Transformer models, scheduling may prioritize preloading attention
    matrices into memory while earlier layers are still executing, ensuring that data
    is ready when needed ([Shoeybi et al. 2019b](ch058.xhtml#ref-Shoeybi2019)).
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 调度涉及优化执行顺序以最小化依赖关系并最大化吞吐量，而不仅仅是任务划分。许多AI模型包括可以独立计算的运算（例如，批处理管道中的不同批次）以及具有严格依赖关系的运算（例如，RNN中的循环层）。AI编译器分析这些依赖关系，并尝试尽可能重新排列执行，以减少闲置时间并提高并行效率。例如，在Transformer模型中，调度可能优先将注意力矩阵预加载到内存中，同时早期层仍在执行，确保数据在需要时已准备好（[Shoeybi等人2019b](ch058.xhtml#ref-Shoeybi2019)）。
- en: Another crucial aspect of computation scheduling is resource allocation and
    synchronization, where the compiler determines how compute cores share memory
    and coordinate execution. Modern AI accelerators often support overlapping computation
    and data transfers, meaning that while one task executes, the next task can begin
    fetching its required data. Compilers take advantage of this by scheduling tasks
    in a way that hides memory latency, ensuring that execution remains compute-bound
    rather than memory-bound ([0001 et al. 2018b](ch058.xhtml#ref-Chen2018)). TensorRT
    and XLA, for example, employ streaming execution strategies where multiple kernels
    are launched in parallel, and synchronization is carefully managed to prevent
    execution stalls ([Google 2025](ch058.xhtml#ref-GoogleXLA)).
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 计算调度的另一个关键方面是资源分配和同步，编译器确定计算核心如何共享内存并协调执行。现代AI加速器通常支持重叠计算和数据传输，这意味着当一个任务执行时，下一个任务可以开始获取其所需的数据。编译器通过以这种方式调度任务来利用这一点，隐藏内存延迟，确保执行保持计算密集型而不是内存密集型（[0001等人2018b](ch058.xhtml#ref-Chen2018)）。例如，TensorRT和XLA采用流式执行策略，其中并行启动多个内核，并仔细管理同步以防止执行停滞（[Google
    2025](ch058.xhtml#ref-GoogleXLA)）。
- en: Computation Scheduling Importance
  id: totrans-887
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算调度的重要性
- en: Without effective scheduling, even the most optimized model can suffer from
    underutilized compute resources, memory bottlenecks, and execution inefficiencies.
    Poor scheduling decisions can lead to idle processing elements, forcing expensive
    compute cores to wait for data or synchronization events before continuing execution.
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 没有有效的调度，即使是经过优化的模型也可能因为计算资源的利用率不足、内存瓶颈和执行效率低下而受到影响。不良的调度决策可能导致处理元素闲置，迫使昂贵的计算核心在继续执行之前等待数据或同步事件。
- en: For instance, a CNN running on a GPU may have highly optimized kernels and efficient
    memory layouts, but if its execution is not scheduled correctly, compute units
    may remain idle between kernel launches, reducing throughput. Similarly, a Transformer
    model deployed on a TPU may perform matrix multiplications efficiently but could
    experience performance degradation if attention layers are not scheduled to overlap
    efficiently with memory transfers.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在GPU上运行的卷积神经网络（CNN）可能具有高度优化的内核和高效的内存布局，但如果其执行调度不当，计算单元在内核启动之间可能会闲置，从而降低吞吐量。同样，部署在TPU上的Transformer模型可能能够高效地执行矩阵乘法，但如果注意力层没有有效地与内存传输重叠调度，可能会经历性能下降。
- en: Effective computation scheduling occupies a central role in the orchestration
    of parallel workloads, ensuring that processing elements are utilized to their
    fullest capacity while preventing idle cores—a critical aspect for maximizing
    overall throughput. By strategically overlapping computation with data movement,
    the scheduling mechanism effectively conceals memory latency, thereby preventing
    operational stalls during data retrieval. By resolving execution dependencies
    with precision, it minimizes waiting periods and enhances the concurrent progression
    of computation and data transfer. This systematic integration of scheduling and
    data handling serves to not only elevate performance but also exemplify the rigorous
    engineering principles that underpin modern accelerator design.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的计算调度在并行工作负载的编排中占据核心地位，确保处理元素被充分利用，同时防止闲置核心——这是最大化整体吞吐量的关键方面。通过策略性地重叠计算与数据移动，调度机制有效地隐藏了内存延迟，从而防止在数据检索期间出现操作停滞。通过精确解决执行依赖关系，它最小化了等待时间，并增强了计算和数据传输的并发进展。这种调度和数据处理的系统整合不仅提高了性能，而且展示了支撑现代加速器设计的严谨工程原则。
- en: Code Generation
  id: totrans-891
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成
- en: Unlike the previous phases, which required AI-specific optimizations, code generation
    follows many of the same principles as traditional compilers. This process includes
    instruction selection, register allocation, and final optimization passes, ensuring
    that execution makes full use of hardware-specific features such as vectorized
    execution, memory prefetching, and instruction reordering.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前需要特定AI优化优化的阶段不同，代码生成遵循了许多与传统编译器相同的原理。这个过程包括指令选择、寄存器分配和最终的优化遍历，确保执行充分利用了硬件特定的特性，如向量执行、内存预取和指令重排。
- en: For CPUs and GPUs, AI compilers typically generate machine code or optimized
    assembly instructions, while for TPUs, FPGAs[30](#fn30), and other accelerators,
    the output may be optimized bytecode or execution graphs that are interpreted
    by the hardware’s runtime system.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CPU和GPU，AI编译器通常生成机器代码或优化的汇编指令，而对于TPU、FPGA和其他加速器，输出可能是优化后的字节码或由硬件的运行时系统解释的执行图。
- en: 'At this point, the compilation pipeline is complete: the original high-level
    model representation has been transformed into an optimized, executable format
    tailored for efficient execution on the target hardware. The combination of graph
    transformations, kernel selection, memory-aware execution, and parallel scheduling
    ensures that AI accelerators run workloads with maximum efficiency, minimal memory
    overhead, and optimal computational throughput.'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，编译管道已经完成：原始的高级模型表示已经被转换成针对目标硬件高效执行的优化、可执行格式。图变换、内核选择、内存感知执行和并行调度的组合确保AI加速器以最大效率、最小内存开销和最佳计算吞吐量运行工作负载。
- en: Compilation-Runtime Support
  id: totrans-895
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译-运行时支持
- en: The compiler plays a fundamental role in AI acceleration, transforming high-level
    machine learning models into optimized execution plans tailored to the constraints
    of specialized hardware. Throughout this section, we have seen how graph optimization
    restructures computation, kernel selection maps operations to hardware-efficient
    implementations, memory planning optimizes data placement, and computation scheduling
    ensures efficient parallel execution. Each of these phases is crucial in enabling
    AI models to fully leverage modern accelerators, ensuring high throughput, minimal
    memory overhead, and efficient execution pipelines.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器在 AI 加速中扮演着基本角色，将高级机器学习模型转换为针对专用硬件约束优化的执行计划。在本节中，我们已经看到图优化如何重构计算，内核选择如何将操作映射到硬件高效的实现，内存规划如何优化数据放置，以及计算调度如何确保高效的并行执行。这些阶段中的每一个对于使
    AI 模型充分利用现代加速器至关重要，确保高吞吐量、最小内存开销和高效的执行管道。
- en: However, compilation alone is not enough to guarantee efficient execution in
    real-world AI workloads. While compilers statically optimize computation based
    on known model structures and hardware capabilities, AI execution environments
    are often dynamic and unpredictable. Batch sizes fluctuate, hardware resources
    may be shared across multiple workloads, and accelerators must adapt to real-time
    performance constraints. In these cases, a static execution plan is insufficient,
    and runtime management becomes critical in ensuring that models execute optimally
    under real-world conditions.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅靠编译本身并不能保证在现实世界的 AI 工作负载中实现高效的执行。虽然编译器根据已知的模型结构和硬件能力静态优化计算，但 AI 执行环境通常是动态和不可预测的。批量大小波动，硬件资源可能被多个工作负载共享，加速器必须适应实时性能约束。在这些情况下，静态执行计划是不够的，运行时管理变得至关重要，以确保模型在现实世界条件下以最佳方式执行。
- en: This transition from static compilation to adaptive execution is where AI runtimes
    come into play. Runtimes provide dynamic memory allocation, real-time kernel selection,
    workload scheduling, and multi-chip coordination, allowing AI models to adapt
    to varying execution conditions while maintaining efficiency. In the next section,
    we explore how AI runtimes extend the capabilities of compilers, enabling models
    to run effectively in diverse and scalable deployment scenarios.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从静态编译到自适应执行的转变正是 AI 运行时发挥作用的地方。运行时提供动态内存分配、实时内核选择、工作负载调度和多芯片协调，使 AI 模型能够适应变化的执行条件，同时保持效率。在下一节中，我们将探讨
    AI 运行时如何扩展编译器的功能，使模型能够在各种可扩展的部署场景中有效运行。
- en: Runtime Support
  id: totrans-899
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行时支持
- en: While compilers optimize AI models before execution, real-world deployment introduces
    dynamic and unpredictable conditions that static compilation alone cannot fully
    address ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)). AI workloads operate
    in varied execution environments, where factors such as fluctuating batch sizes,
    shared hardware resources, memory contention, and latency constraints necessitate
    real-time adaptation. Precompiled execution plans, optimized for a fixed set of
    assumptions, may become suboptimal when actual runtime conditions change.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然编译器在执行前优化 AI 模型，但实际部署引入了动态和不可预测的条件，这些条件仅靠静态编译无法完全解决 ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021))。AI
    工作负载在多种执行环境中运行，其中诸如批量大小波动、共享硬件资源、内存竞争和延迟约束等因素需要实时适应。针对固定假设集预先编译的执行计划，当实际运行时条件发生变化时，可能变得次优。
- en: To bridge this gap, AI runtimes provide a dynamic layer of execution management,
    extending the optimizations performed at compile time with real-time decision-making.
    Unlike traditional compiled programs that execute a fixed sequence of instructions,
    AI workloads require adaptive control over memory allocation, kernel execution,
    and resource scheduling. AI runtimes continuously monitor execution conditions
    and make on-the-fly adjustments to ensure that machine learning models fully utilize
    available hardware while maintaining efficiency and performance guarantees.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥合这一差距，AI 运行时提供了一层动态的执行管理，通过实时决策扩展了编译时执行的优化。与执行固定指令序列的传统编译程序不同，AI 工作负载需要适应内存分配、内核执行和资源调度的控制。AI
    运行时持续监控执行条件，并实时调整以确保机器学习模型充分利用可用硬件，同时保持效率和性能保证。
- en: 'At a high level, AI runtimes manage three critical aspects of execution:'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，AI 运行时管理执行过程中的三个关键方面：
- en: '**Kernel Execution Management**: AI runtimes dynamically select and dispatch
    computation kernels based on the current system state, ensuring that workloads
    are executed with minimal latency.'
  id: totrans-903
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内核执行管理**：AI运行时根据当前系统状态动态选择和调度计算内核，确保工作负载以最小延迟执行。'
- en: '**Memory Adaptation and Allocation**: Since AI workloads frequently process
    large tensors with varying memory footprints, runtimes adjust memory allocation
    dynamically to prevent bottlenecks and excessive data movement ([Y. Huang et al.
    2019](ch058.xhtml#ref-deepmind_gpipe_2019)).'
  id: totrans-904
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内存适应和分配**：由于AI工作负载经常处理具有不同内存占用的大张量，运行时动态调整内存分配以防止瓶颈和过多的数据移动（[Y. Huang等人，2019](ch058.xhtml#ref-deepmind_gpipe_2019)）。'
- en: '**Execution Scaling**: AI runtimes handle workload distribution across multiple
    accelerators, supporting large-scale execution in multi-chip, multi-node, or cloud
    environments ([Mirhoseini et al. 2017](ch058.xhtml#ref-mirhoseini_device_placement_2017)).'
  id: totrans-905
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行扩展**：AI运行时处理跨多个加速器的作业分配，支持在多芯片、多节点或云环境中进行大规模执行（[Mirhoseini等人，2017](ch058.xhtml#ref-mirhoseini_device_placement_2017)）。'
- en: By dynamically handling these execution aspects, AI runtimes complement compiler-based
    optimizations, ensuring that models continue to perform efficiently under varying
    runtime conditions. The next section explores how AI runtimes differ from traditional
    software runtimes, highlighting why machine learning workloads require fundamentally
    different execution strategies compared to conventional CPU-based programs.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态处理这些执行方面，AI运行时补充了基于编译器的优化，确保模型在变化的运行时条件下继续高效运行。下一节将探讨AI运行时与传统软件运行时的不同之处，突出为什么机器学习工作负载需要与基于传统CPU的程序相比，采用根本不同的执行策略。
- en: Runtime Architecture Differences for ML Systems
  id: totrans-907
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习系统运行时架构差异
- en: Traditional software runtimes are designed for managing general-purpose program
    execution, primarily handling sequential and multi-threaded workloads on CPUs.
    These runtimes allocate memory, schedule tasks, and optimize execution at the
    level of individual function calls and instructions. In contrast, AI runtimes
    are specialized for machine learning workloads, which require massively parallel
    computation, large-scale tensor operations, and dynamic memory management.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 传统软件运行时旨在管理通用程序执行，主要处理CPU上的顺序和多线程工作负载。这些运行时在单个函数调用和指令级别分配内存、调度任务和优化执行。相比之下，AI运行时专门针对机器学习工作负载，这些工作负载需要大规模并行计算、大规模张量操作和动态内存管理。
- en: '[Table 11.19](ch017.xhtml#tbl-runtime-comparison) highlights the fundamental
    differences between traditional and AI runtimes. One of the key distinctions lies
    in execution flow. Traditional software runtimes operate on a predictable, structured
    execution model where function calls and CPU threads follow a predefined control
    path. AI runtimes, however, execute computational graphs, requiring complex scheduling
    decisions that account for dependencies between tensor operations, parallel kernel
    execution, and efficient memory access.'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.19](ch017.xhtml#tbl-runtime-comparison)突出了传统运行时和AI运行时的基本差异。其中一个关键区别在于执行流程。传统软件运行时在可预测、结构化的执行模型上运行，其中函数调用和CPU线程遵循预定义的控制路径。然而，AI运行时执行计算图，需要考虑张量操作之间的依赖关系、并行内核执行和高效内存访问的复杂调度决策。'
- en: 'Table 11.19: **Runtime Execution Models**: Traditional and AI runtimes diverge
    in their execution approaches; traditional runtimes prioritize sequential or multi-threaded
    instruction processing, while AI runtimes leverage massively parallel tensor operations
    for accelerated computation on machine learning workloads. This distinction necessitates
    specialized AI runtime architectures designed for efficient parallelization and
    memory management of large-scale tensor data.'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.19：**运行时执行模型**：传统运行时和AI运行时在执行方法上存在差异；传统运行时优先考虑顺序或多线程指令处理，而AI运行时利用大规模并行张量操作来加速机器学习工作负载的计算。这种区别需要专门设计的AI运行时架构，以实现大规模张量数据的有效并行化和内存管理。
- en: '| **Aspect** | **Traditional Runtime** | **AI Runtime** |'
  id: totrans-911
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **传统运行时** | **AI运行时** |'
- en: '| --- | --- | --- |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Execution Model** | Sequential or multi-threaded execution | Massively
    parallel tensor execution |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
  zh: '| **执行模型** | 顺序或多线程执行 | 大规模并行张量执行 |'
- en: '| **Task Scheduling** | CPU thread management | Kernel dispatch across accelerators
    |'
  id: totrans-914
  prefs: []
  type: TYPE_TB
  zh: '| **任务调度** | CPU线程管理 | 核心在加速器之间的调度 |'
- en: '| **Memory Management** | Static allocation (stack/heap) | Dynamic tensor allocation,
    buffer reuse |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '| **内存管理** | 静态分配（栈/堆） | 动态张量分配，缓冲区重用 |'
- en: '| **Optimization Priorities** | Low-latency instruction execution | Minimizing
    memory stalls, maximizing parallel execution |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '| **优化优先级** | 低延迟指令执行 | 最小化内存停滞，最大化并行执行 |'
- en: '| **Adaptability** | Mostly static execution plan | Adapts to batch size and
    hardware availability |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| **适应性** | 主要静态执行计划 | 适应批量大小和硬件可用性 |'
- en: '| **Target Hardware** | CPUs (general-purpose execution) | GPUs, TPUs, and
    custom accelerators |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
  zh: '| **目标硬件** | CPUs（通用执行） | GPUs、TPUs 和定制加速器 |'
- en: Memory management is another major differentiator. Traditional software runtimes
    handle small, frequent memory allocations, optimizing for cache efficiency and
    low-latency access. AI runtimes, in contrast, must dynamically allocate, reuse,
    and optimize large tensors, ensuring that memory access patterns align with accelerator-friendly
    execution. Poor memory management in AI workloads can lead to performance bottlenecks,
    particularly due to excessive off-chip memory transfers and inefficient cache
    usage.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理是另一个主要区别。传统的软件运行时处理小而频繁的内存分配，优化缓存效率和低延迟访问。相比之下，AI 运行时必须动态分配、重用和优化大型张量，确保内存访问模式与加速器友好的执行相匹配。AI
    工作负载中的内存管理不当可能导致性能瓶颈，尤其是由于过度的片外内存传输和低效的缓存使用。
- en: AI runtimes are inherently designed for adaptability. While traditional runtimes
    often follow a mostly static execution plan, AI workloads typically operate in
    highly variable execution environments, such as cloud-based accelerators or multi-tenant
    hardware. As a result, AI runtimes must continuously adjust batch sizes, reallocate
    compute resources, and manage real-time scheduling decisions to maintain high
    throughput and minimize execution delays.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: AI 运行时天生具有适应性。虽然传统的运行时通常遵循一个主要静态的执行计划，但 AI 工作负载通常在高度可变的执行环境中运行，例如基于云的加速器或多租户硬件。因此，AI
    运行时必须持续调整批量大小、重新分配计算资源，并管理实时调度决策，以保持高吞吐量和最小化执行延迟。
- en: These distinctions demonstrate why AI runtimes require fundamentally different
    execution strategies compared to traditional software runtimes. Rather than simply
    managing CPU processes, AI runtimes must oversee large-scale tensor execution,
    multi-device coordination, and real-time workload adaptation to ensure that machine
    learning models can run efficiently under diverse and ever-changing deployment
    conditions.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 这些区别说明了为什么与传统的软件运行时相比，AI 运行时需要根本不同的执行策略。AI 运行时不仅要管理 CPU 进程，还必须监督大规模张量执行、多设备协调和实时工作负载适应，以确保机器学习模型能够在各种不断变化的部署条件下高效运行。
- en: Dynamic Kernel Execution
  id: totrans-922
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态内核执行
- en: Dynamic kernel execution is the process of mapping machine learning models to
    hardware and optimizing runtime execution. While static compilation provides a
    solid foundation, efficient execution of machine learning workloads requires real-time
    adaptation to fluctuating conditions such as available memory, data sizes, and
    computational loads. The runtime functions as an intermediary that continuously
    adjusts execution strategies to match both the constraints of the underlying hardware
    and the characteristics of the workload.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 动态内核执行是将机器学习模型映射到硬件并优化运行时执行的过程。虽然静态编译提供了一个坚实的基础，但高效执行机器学习工作负载需要实时适应波动条件，如可用内存、数据大小和计算负载。运行时充当一个中介，持续调整执行策略以匹配底层硬件的约束和工作负载的特性。
- en: When mapping a machine learning model to hardware, individual computational
    operations, including matrix multiplications, convolutions, and activation functions,
    must be assigned to the most appropriate processing units. This mapping is not
    fixed; it must be modified during runtime in response to changes in input data,
    memory availability, and overall system load. Dynamic kernel execution allows
    the runtime to make real-time decisions regarding kernel selection, execution
    order, and memory management, ensuring that workloads remain efficient despite
    these changing conditions.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 当将机器学习模型映射到硬件时，必须将单个计算操作（包括矩阵乘法、卷积和激活函数）分配给最合适的处理单元。这种映射不是固定的；它必须在运行时根据输入数据、内存可用性和整体系统负载的变化进行修改。动态内核执行允许运行时实时做出关于内核选择、执行顺序和内存管理的决策，确保即使在这些变化条件下，工作负载也能保持高效。
- en: For example, consider an AI accelerator executing a deep neural network (DNN)
    for image classification. If an incoming batch of high-resolution images requires
    significantly more memory than expected, a statically planned execution may cause
    cache thrashing or excessive off-chip memory accesses. Instead, a dynamic runtime
    can adjust tiling strategies on the fly, breaking down tensor operations into
    smaller tiles that fit within the high-speed on-chip memory. This prevents memory
    stalls and ensures optimal utilization of caches.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个执行图像分类的深度神经网络（DNN）的AI加速器。如果传入的高分辨率图像批次所需的内存比预期多得多，静态计划的执行可能会导致缓存冲突或过度的片外内存访问。相反，动态运行时可以即时调整分块策略，将张量操作分解成适合高速片上内存的小块。这可以防止内存停滞并确保缓存的最佳利用率。
- en: Similarly, when running a transformer-based NLP model, the sequence length of
    input text may vary between inference requests. A static execution plan optimized
    for a fixed sequence length may lead to underutilization of compute resources
    when processing shorter sequences or excessive memory pressure with longer sequences.
    Dynamic kernel execution can mitigate this by selecting different kernel implementations
    based on the actual sequence length, dynamically adjusting memory allocations
    and execution strategies to maintain efficiency.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当运行基于transformer的自然语言处理（NLP）模型时，输入文本的序列长度可能在推理请求之间变化。针对固定序列长度优化的静态执行计划在处理较短的序列时可能会导致计算资源利用率不足，而在处理较长的序列时可能会产生过度的内存压力。动态内核执行可以通过根据实际序列长度选择不同的内核实现来缓解这种情况，动态调整内存分配和执行策略以保持效率。
- en: Overlapping computation with memory movement is a vital strategy to mitigate
    performance bottlenecks. AI workloads often encounter delays due to memory-bound
    issues, where data movement between memory hierarchies limits computation speed.
    To combat this, AI runtimes implement techniques like asynchronous execution and
    double buffering, ensuring that computations proceed without waiting for memory
    transfers to complete. In a large-scale model, for instance, image data can be
    prefetched while computations are performed on the previous batch, thus maintaining
    a steady flow of data and avoiding pipeline stalls.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 计算与内存移动重叠是一种缓解性能瓶颈的重要策略。AI工作负载经常遇到由于内存绑定问题而导致的延迟，其中数据在内存层次结构之间的移动限制了计算速度。为了应对这种情况，AI运行时实施异步执行和双缓冲等技术，确保计算在没有等待内存传输完成的情况下进行。例如，在一个大规模模型中，可以在对前一个批次进行计算的同时预取图像数据，从而保持数据流的稳定并避免流水线停滞。
- en: Another practical example is the execution of convolutional layers in a CNN
    on a GPU. If multiple convolution kernels need to be scheduled, a static scheduling
    approach may lead to inefficient resource utilization due to variation in layer
    sizes and compute requirements. By dynamically scheduling kernel execution, AI
    runtimes can prioritize smaller kernels when compute units are partially occupied,
    improving hardware utilization. For instance, in NVIDIA’s TensorRT runtime, fusion
    of small kernels into larger execution units is done dynamically to avoid launch
    overhead, optimizing latency-sensitive inference tasks.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实际例子是在GPU上执行卷积层。如果需要调度多个卷积内核，静态调度方法可能会因为层大小和计算需求的变化而导致资源利用率低下。通过动态调度内核执行，AI运行时可以在计算单元部分占用时优先选择较小的内核，从而提高硬件利用率。例如，在NVIDIA的TensorRT运行时，将小内核融合到较大的执行单元中是动态进行的，以避免启动开销，优化对延迟敏感的推理任务。
- en: Dynamic kernel execution plays an essential role in ensuring that machine learning
    models are executed efficiently. By dynamically adjusting execution strategies
    in response to real-time system conditions, AI runtimes optimize both training
    and inference performance across various hardware platforms.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 动态内核执行在确保机器学习模型高效执行中起着至关重要的作用。通过根据实时系统条件动态调整执行策略，AI运行时优化了跨各种硬件平台的训练和推理性能。
- en: Runtime Kernel Selection
  id: totrans-930
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行时内核选择
- en: While compilers may perform an initial selection of kernels based on static
    analysis of the machine learning model and hardware target, AI runtimes often
    need to override these decisions during execution. Real-time factors, such as
    available memory, hardware utilization, and workload priorities, may differ significantly
    from the assumptions made during compilation. By dynamically selecting and switching
    kernels at runtime, AI runtimes can adapt to these changing conditions, ensuring
    that models continue to perform efficiently.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然编译器可能会根据机器学习模型和硬件目标的静态分析进行内核的初始选择，但AI运行时通常需要在执行过程中覆盖这些决策。实时因素，如可用内存、硬件利用率和工作负载优先级，可能与编译期间做出的假设有很大不同。通过在运行时动态选择和切换内核，AI运行时可以适应这些变化条件，确保模型继续高效运行。
- en: For instance, consider transformer-based language models, where a significant
    portion of execution time is spent on matrix multiplications. The AI runtime must
    determine the most efficient way to execute these operations based on the current
    system state. If the model is running on a GPU with specialized Tensor Cores,
    the runtime may switch from a standard FP32 kernel to an FP16 kernel to take advantage
    of hardware acceleration ([Shoeybi et al. 2019a](ch058.xhtml#ref-shoeybi_megatron_2020)).
    Conversely, if the lower precision of FP16 causes unacceptable numerical instability,
    the runtime can opt for mixed-precision execution, selectively using FP32 where
    higher precision is necessary.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑基于transformer的语言模型，其中执行时间的大部分都花在了矩阵乘法上。AI运行时必须根据当前系统状态确定执行这些操作的最有效方式。如果模型运行在具有专用Tensor
    Cores的GPU上，运行时可能会从标准FP32内核切换到FP16内核，以利用硬件加速（[Shoeybi等人2019a](ch058.xhtml#ref-shoeybi_megatron_2020)）。相反，如果FP16的较低精度导致不可接受的数值不稳定性，运行时可以选择混合精度执行，在需要更高精度的位置选择性地使用FP32。
- en: Memory constraints also influence kernel selection. When memory bandwidth is
    limited, the runtime may adjust its execution strategy, reordering operations
    or changing the tiling strategy to fit computations into the available cache rather
    than relying on slower main memory. For example, a large matrix multiplication
    may be broken into smaller chunks, ensuring that the computation fits into the
    on-chip memory of the GPU, reducing overall latency.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 内存限制也会影响内核选择。当内存带宽有限时，运行时可能会调整其执行策略，重新排序操作或更改分块策略，以便将计算放入可用的缓存中，而不是依赖于较慢的主内存。例如，一个大的矩阵乘法可能被分解成更小的块，确保计算适合GPU的片上内存，从而降低整体延迟。
- en: Additionally, batch size can influence kernel selection. For workloads that
    handle a mix of small and large batches, the AI runtime may choose a latency-optimized
    kernel for small batches and a throughput-optimized kernel for large-scale batch
    processing. This adjustment ensures that the model continues to operate efficiently
    across different execution scenarios, without the need for manual tuning.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，批处理大小也会影响内核选择。对于处理小批量和大批量混合的工作负载，AI运行时可能会为小批量选择延迟优化的内核，为大规模批处理选择吞吐量优化的内核。这种调整确保模型在不同执行场景中继续高效运行，无需手动调整。
- en: Kernel Scheduling and Utilization
  id: totrans-935
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核调度和利用率
- en: Once the AI runtime selects an appropriate kernel, the next step is scheduling
    it in a way that maximizes parallelism and resource utilization. Unlike traditional
    task schedulers, which are designed to manage CPU threads, AI runtimes must coordinate
    a much larger number of tasks across parallel execution units such as GPU cores,
    tensor processing units, or custom AI accelerators ([Norman P. Jouppi et al. 2017a](ch058.xhtml#ref-google_tpu_2017)).
    Effective scheduling ensures that these computational resources are kept fully
    engaged, preventing bottlenecks and maximizing throughput.
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦AI运行时选择了合适的内核，下一步就是以最大化并行性和资源利用率为目标进行调度。与设计用于管理CPU线程的传统任务调度器不同，AI运行时必须在并行执行单元（如GPU核心、张量处理单元或定制AI加速器）之间协调大量任务（[Norman
    P. Jouppi等人2017a](ch058.xhtml#ref-google_tpu_2017)）。有效的调度确保这些计算资源保持完全投入，防止瓶颈并最大化吞吐量。
- en: For example, in image recognition models that use convolutional layers, operations
    can be distributed across multiple processing units, enabling different filters
    to run concurrently. This parallelization ensures that the available hardware
    is fully utilized, speeding up execution. Similarly, batch normalization and activation
    functions must be scheduled efficiently to avoid unnecessary delays. If these
    operations are not interleaved with other computations, they may block the pipeline
    and reduce overall throughput.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在采用卷积层的图像识别模型中，操作可以分布在多个处理单元上，使得不同的过滤器可以同时运行。这种并行化确保了可用硬件得到充分利用，从而加快了执行速度。同样，批量归一化和激活函数必须高效地调度，以避免不必要的延迟。如果这些操作没有与其他计算交织进行，它们可能会阻塞流水线并降低整体吞吐量。
- en: Efficient kernel scheduling can also be influenced by real-time memory management
    . AI runtimes ensure that intermediate data, such as feature maps in deep neural
    networks, are preloaded into cache before they are needed. This proactive management
    helps prevent delays caused by waiting for data to be loaded from slower memory
    tiers, ensuring continuous execution.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的内核调度也可能受到实时内存管理的影响。人工智能运行时确保在需要之前，将中间数据，如深度神经网络中的特征图，预先加载到缓存中。这种主动管理有助于防止由于从较慢的内存层级加载数据而导致的延迟，确保连续执行。
- en: These techniques enable AI runtimes to ensure optimal resource utilization and
    efficient parallel computation, which are essential for the high-performance execution
    of machine learning models, particularly in environments that require scaling
    across multiple hardware accelerators.
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术使人工智能运行时能够确保最佳资源利用和高效的并行计算，这对于机器学习模型的高性能执行至关重要，尤其是在需要跨多个硬件加速器进行扩展的环境中。
- en: The compiler and runtime systems examined thus far optimize execution within
    single accelerators—managing computation mapping, memory hierarchies, and kernel
    scheduling. While these single-chip optimizations achieve impressive performance
    gains, modern AI workloads increasingly exceed what any individual chip can deliver.
    Training GPT-3 would require running a single H100 continuously for 10 years,
    consuming 314 sextillion floating-point operations. Real-time inference serving
    for global applications demands throughput beyond any single accelerator’s capacity.
    These computational requirements, rooted in the scaling laws from [Chapter 9](ch015.xhtml#sec-efficient-ai),
    necessitate a fundamental shift from single-chip optimization to distributed acceleration
    strategies.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所检查的编译器和运行时系统优化了单个加速器内的执行——管理计算映射、内存层次结构和内核调度。虽然这些单芯片优化实现了令人印象深刻的性能提升，但现代人工智能工作负载越来越多地超出了任何单个芯片的能力。训练
    GPT-3 需要连续运行单个 H100 10 年，消耗 314 十亿浮点运算。为全球应用提供实时推理服务需要超越任何单个加速器的吞吐量。这些计算需求源于第
    9 章中的扩展定律（[第 9 章](ch015.xhtml#sec-efficient-ai)），需要从单芯片优化到分布式加速策略的根本转变。
- en: Multi-Chip AI Acceleration
  id: totrans-941
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多芯片人工智能加速
- en: The transition from single-chip to multi-chip architectures represents more
    than simple replication—it requires rethinking how computations distribute across
    processors, how data flows between chips, and how systems maintain coherence at
    scale. Where single-chip optimization focuses on maximizing utilization within
    fixed resources, multi-chip systems must balance computational distribution against
    communication overhead, memory coherence costs, and synchronization complexity.
    These challenges fundamentally transform the optimization landscape, requiring
    new abstractions and techniques beyond those developed for individual accelerators.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 从单芯片到多芯片架构的转变不仅仅是简单的复制——它需要重新思考计算如何在处理器之间分布，数据如何在芯片之间流动，以及系统如何在规模上保持一致性。单芯片优化侧重于在固定资源内最大化利用，而多芯片系统必须在计算分布与通信开销、内存一致性成本和同步复杂性之间取得平衡。这些挑战从根本上改变了优化格局，需要超出为单个加速器开发的抽象和技术之外的新抽象和技术。
- en: Modern AI workloads increasingly demand computational resources that exceed
    the capabilities of single-chip accelerators. This section examines how AI systems
    scale from individual processors to multi-chip architectures, analyzing the motivation
    behind different scaling approaches and their impact on system design. These scaling
    considerations are fundamental to the distributed training strategies covered
    in [Chapter 8](ch014.xhtml#sec-ai-training) and the operational challenges discussed
    in [Chapter 13](ch019.xhtml#sec-ml-operations). The security implications of distributed
    acceleration, particularly around model protection and data privacy, are examined
    in [Chapter 15](ch021.xhtml#sec-security-privacy). By understanding this progression,
    we can better appreciate how each component of the AI hardware stack, ranging
    from compute units to memory systems, must adapt to support large-scale machine
    learning workloads.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能工作负载越来越需要超出单片加速器能力的计算资源。本节探讨了人工智能系统如何从单个处理器扩展到多芯片架构，分析了不同扩展方法的动机及其对系统设计的影响。这些扩展考虑因素是[第8章](ch014.xhtml#sec-ai-training)中涵盖的分布式训练策略和[第13章](ch019.xhtml#sec-ml-operations)中讨论的操作挑战的基础。在[第15章](ch021.xhtml#sec-security-privacy)中，探讨了分布式加速的安全影响，特别是在模型保护和数据隐私方面。通过理解这一进展，我们可以更好地欣赏人工智能硬件堆栈的每个组件，从计算单元到内存系统，如何适应以支持大规模机器学习工作负载。
- en: The scaling of AI systems follows a natural progression, starting with integration
    within a single package through chiplet architectures, extending to multi-GPU
    configurations within a server, expanding to distributed accelerator pods, and
    culminating in wafer-scale integration. Each approach presents unique trade-offs
    between computational density, communication overhead, and system complexity.
    For instance, chiplet architectures maintain high-speed interconnects within a
    package, while distributed systems sacrifice communication latency for massive
    parallelism.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统的扩展遵循自然进展，从单个封装内的集成开始，通过芯片模块架构，扩展到服务器内的多GPU配置，扩展到分布式加速器集群，最终达到晶圆级集成。每种方法都在计算密度、通信开销和系统复杂性之间提供了独特的权衡。例如，芯片模块架构在封装内保持高速互连，而分布式系统为了巨大的并行性而牺牲了通信延迟。
- en: Understanding these scaling strategies is essential for several reasons. First,
    it provides insight into how different hardware architectures address the growing
    computational demands of AI workloads. Second, it reveals the fundamental challenges
    that arise when extending beyond single-chip execution, such as managing inter-chip
    communication and coordinating distributed computation. Finally, it establishes
    the foundation for subsequent discussions on how mapping strategies, compilation
    techniques, and runtime systems evolve to support efficient execution at scale.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些扩展策略对于多个原因至关重要。首先，它提供了洞察不同硬件架构如何应对人工智能工作负载不断增长的计算需求。其次，它揭示了在扩展到单芯片执行之外时出现的根本挑战，例如管理芯片间通信和协调分布式计算。最后，它为后续讨论映射策略、编译技术和运行时系统如何演变以支持大规模高效执行奠定了基础。
- en: The progression begins with chiplet architectures, which represent the most
    tightly integrated form of multi-chip scaling.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 进展始于芯片模块架构，这代表了多芯片扩展最紧密集成的形式。
- en: Chiplet-Based Architectures
  id: totrans-947
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于芯片模块的架构
- en: Chiplet[31](#fn31) architectures achieve this scaling by partitioning large
    designs into smaller, modular dies that are interconnected within a single package,
    as illustrated in [Figure 11.9](ch017.xhtml#fig-AMD_chiplet_based).
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片模块[31](#fn31)架构通过将大型设计划分为更小、模块化的晶圆，并在单个封装内互连来实现扩展，如图11.9[图](ch017.xhtml#fig-AMD_chiplet_based)所示。
- en: '![](../media/file188.png)'
  id: totrans-949
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file188.png)'
- en: 'Figure 11.9: **Chiplet Interconnect**: Modern AI accelerators partition large
    designs into smaller chiplets and connect them via high-bandwidth interconnects,
    enabling scalability beyond monolithic die limitations and improving manufacturing
    yields. HBM stacks provide fast access to data, crucial for the memory-intensive
    workloads common in machine learning.'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：**芯片模块互连**：现代人工智能加速器将大型设计划分为更小的芯片模块，并通过高带宽互连连接它们，从而超越单片晶圆的限制，提高制造良率。HBM堆栈提供对数据的快速访问，这对于机器学习中常见的内存密集型工作负载至关重要。
- en: Modern AI accelerators, such as AMD’s Instinct MI300, take this approach by
    integrating multiple compute chiplets alongside memory chiplets, linked by high-speed
    die-to-die interconnects. This modular design allows manufacturers to bypass the
    manufacturing limits of monolithic chips while still achieving high-density compute.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI加速器，如AMD的Instinct MI300，通过将多个计算芯片和内存芯片集成在一起，并通过高速晶圆到晶圆互连连接，采用这种方法。这种模块化设计允许制造商绕过单片芯片的制造限制，同时仍然实现高密度计算。
- en: However, even within a single package, scaling is not without challenges. Inter-chiplet
    communication latency, memory coherence[32](#fn32), and thermal management become
    critical factors as more chiplets are integrated. Unlike traditional multi-chip
    systems, chiplet-based designs must carefully balance latency-sensitive workloads
    across multiple dies without introducing excessive bottlenecks.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在单个封装内，扩展也并非没有挑战。随着更多芯片的集成，芯片间通信延迟、内存一致性[32](#fn32)和热管理成为关键因素。与传统多芯片系统不同，基于芯片的设计必须仔细平衡多个晶圆上的延迟敏感型工作负载，同时避免引入过多的瓶颈。
- en: Multi-GPU Systems
  id: totrans-953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多GPU系统
- en: Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs
    working together. In multi-GPU systems, each accelerator has its own dedicated
    memory and compute resources, but they must efficiently share data and synchronize
    execution.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于芯片的设计之外，AI工作负载通常需要多个离散GPU协同工作。在多GPU系统中，每个加速器都有自己的专用内存和计算资源，但它们必须有效地共享数据和同步执行。
- en: A common example is NVIDIA DGX systems, which integrate multiple GPUs connected
    via NVLink or PCIe. This architecture enables workloads to be split across GPUs,
    typically using data parallelism (where each GPU processes a different batch of
    data) or model parallelism (where different GPUs handle different parts of a neural
    network) ([Ben-Nun and Hoefler 2019](ch058.xhtml#ref-Ben-Nun2019data)). These
    parallelization strategies are explored in depth in [Chapter 8](ch014.xhtml#sec-ai-training).
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子是NVIDIA DGX系统，它通过NVLink或PCIe连接多个GPU。这种架构使得工作负载可以在GPU之间分割，通常使用数据并行（每个GPU处理不同的数据批次）或模型并行（不同的GPU处理神经网络的不同部分）([Ben-Nun和Hoefler
    2019](ch058.xhtml#ref-Ben-Nun2019data))。这些并行化策略在[第8章](ch014.xhtml#sec-ai-training)中进行了深入探讨。
- en: As illustrated in [Figure 11.10](ch017.xhtml#fig-multi-gpu), NVSwitch interconnects
    enable high-speed communication between GPUs, reducing bottlenecks in distributed
    training. However, scaling up the number of GPUs introduces fundamental distributed
    coordination challenges that become the dominant performance constraint. The arithmetic
    intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient
    synchronization across GPUs, where AllReduce operations must aggregate 175 billion
    parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth,
    but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs
    simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds
    available capacity. The coordination complexity compounds exponentially—while
    two GPUs require a single communication channel, eight GPUs need 28 interconnect
    paths, and fault tolerance requirements mandate redundant communication patterns.
    Memory consistency protocols further complicate coordination as different GPUs
    may observe weight updates at different times, requiring sophisticated synchronization
    primitives that can add 10-50μs latency per training step—seemingly small delays
    that aggregate to hours of training time across million-iteration runs.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图11.10](ch017.xhtml#fig-multi-gpu)所示，NVSwitch互连使得GPU之间能够实现高速通信，从而减少了分布式训练中的瓶颈。然而，增加GPU数量会引入基本的分布式协调挑战，这些挑战成为主导性能约束的因素。变换器训练的算术强度（0.5-2
    FLOPS/byte）迫使GPU之间频繁同步梯度，在GPT-3规模模型中，AllReduce操作必须聚合1750亿个参数。NVSwitch提供600 GB/s的双向带宽，但当8个H100
    GPU同时交换梯度时，即使这个大量的互连也变成了带宽限制，产生了4.8 TB/s的总需求，超过了可用容量。协调复杂性呈指数级增长——当两个GPU只需要一个通信通道时，八个GPU需要28个互连路径，而容错要求则规定冗余的通信模式。内存一致性协议进一步复杂化了协调，因为不同的GPU可能在不同的时间观察到权重更新，需要复杂的同步原语，这可能会在每个训练步骤中增加10-50μs的延迟——看似微小的延迟，但在百万次迭代运行中会累积成数小时的训练时间。
- en: '![](../media/file189.svg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
  zh: '![图](../media/file189.svg)'
- en: 'Figure 11.10: **Multi-GPU Scaling**: NVSwitch interconnects enable high-bandwidth,
    low-latency communication between GPUs, overcoming PCIe bottlenecks for distributed
    training of large models. Scaling GPU count introduces challenges in maintaining
    memory consistency and efficiently scheduling workloads across interconnected
    devices.'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：**多GPU扩展**：NVSwitch互连使得GPU之间能够实现高带宽、低延迟的通信，克服了PCIe瓶颈，从而支持大型模型的分布式训练。增加GPU数量会引入保持内存一致性和在互连设备间高效调度工作负载的挑战。
- en: Communication Overhead and Amdahl’s Law Analysis
  id: totrans-959
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信开销和Amdahl定律分析
- en: The fundamental limitation of distributed AI training stems from Amdahl’s Law,
    which quantifies how communication overhead constrains parallel speedup regardless
    of available compute power. For distributed neural network training, communication
    overhead during gradient synchronization creates a sequential bottleneck that
    limits scalability even with infinite parallelism.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式AI训练的基本限制源于Amdahl定律，该定律量化了通信开销如何限制并行速度提升，无论可用的计算能力如何。对于分布式神经网络训练，梯度同步期间的通信开销创建了一个顺序瓶颈，即使在无限并行的情况下也限制了可扩展性。
- en: 'The maximum speedup achievable with distributed training is bound by Amdahl’s
    Law: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}</annotation></semantics>
    where <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    is the fraction of work that can be parallelized and <semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics> is the number of processors.
    However, for AI training, communication overhead introduces additional sequential
    time: <semantics><mrow><msub><mtext mathvariant="normal">Speedup</mtext><mtext
    mathvariant="normal">AI</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac><mo>+</mo><mfrac><mi>C</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Speedup}_{\text{AI}} = \frac{1}{(1-P)
    + \frac{P}{N} + \frac{C}{N}}</annotation></semantics> where <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> represents the communication
    overhead fraction.'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练所能达到的最大加速受Amdahl定律的限制：<semantics><mrow><mtext mathvariant="normal">加速</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{加速} = \frac{1}{(1-P) + \frac{P}{N}}</annotation></semantics>
    其中 <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    是可以并行化的工作比例，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    是处理器的数量。然而，对于AI训练，通信开销引入了额外的顺序时间：<semantics><mrow><msub><mtext mathvariant="normal">加速</mtext><mtext
    mathvariant="normal">AI</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac><mo>+</mo><mfrac><mi>C</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{加速}_{\text{AI}} = \frac{1}{(1-P)
    + \frac{P}{N} + \frac{C}{N}}</annotation></semantics> 其中 <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> 代表通信开销比例。
- en: 'Consider training a 175 B parameter model with 1000 H100 GPUs as a concrete
    example:'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 以训练一个175 B参数模型并使用1000个H100 GPU作为具体例子：
- en: '**Computation time per iteration**: 100 ms of forward/backward passes'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每次迭代的计算时间**：前向/反向传递100 ms'
- en: '**Communication time**: AllReduce of 175 B parameters (700 GB in FP32) across
    1000 GPUs'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信时间**：在1000个GPU上对175 B参数（FP32中的700 GB）进行AllReduce操作'
- en: '**Available bandwidth**: 600 GB/s per NVSwitch link'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用带宽**：每个NVSwitch链路600 GB/s'
- en: '**Communication overhead**: <semantics><mrow><mfrac><mrow><mn>700</mn><mtext
    mathvariant="normal">GB</mtext></mrow><mrow><mn>600</mn><mtext mathvariant="normal">GB/s</mtext></mrow></mfrac><mo>×</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1000</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>11.6</mn><mtext
    mathvariant="normal">ms</mtext></mrow><annotation encoding="application/x-tex">\frac{700\text{GB}}{600\text{GB/s}}
    \times \log_2(1000) \approx 11.6\text{ms}</annotation></semantics>'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信开销**：<semantics><mrow><mfrac><mrow><mn>700</mn><mtext mathvariant="normal">GB</mtext></mrow><mrow><mn>600</mn><mtext
    mathvariant="normal">GB/s</mtext></mrow></mfrac><mo>×</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1000</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>11.6</mn><mtext
    mathvariant="normal">ms</mtext></mrow><annotation encoding="application/x-tex">\frac{700\text{GB}}{600\text{GB/s}}
    \times \log_2(1000) \approx 11.6\text{ms}</annotation></semantics>'
- en: 'Even if only 5% of training requires communication (P = 0.95), the maximum
    speedup is: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mn>0.05</mn><mo>+</mo><mfrac><mn>0.95</mn><mn>1000</mn></mfrac><mo>+</mo><mfrac><mn>0.116</mn><mn>100</mn></mfrac></mrow></mfrac><mo>≈</mo><mn>8.3</mn><mtext
    mathvariant="normal">x</mtext></mrow> <annotation encoding="application/x-tex">\text{Speedup}
    = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{x}</annotation></semantics>'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有5%的训练需要通信（P = 0.95），最大加速比是：<semantics><mrow><mtext mathvariant="normal">加速比</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mn>0.05</mn><mo>+</mo><mfrac><mn>0.95</mn><mn>1000</mn></mfrac><mo>+</mo><mfrac><mn>0.116</mn><mn>100</mn></mfrac></mrow></mfrac><mo>≈</mo><mn>8.3</mn><mtext
    mathvariant="normal">倍</mtext></mrow> <annotation encoding="application/x-tex">\text{加速比}
    = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{倍}</annotation></semantics>
- en: This demonstrates why adding more GPUs beyond ~100 provides diminishing returns
    for large model training.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了为什么在100个以上添加更多GPU对于大型模型训练来说，收益是递减的。
- en: 'Communication requirements scale superlinearly with model size and linearly
    with the number of parameters. Modern transformer models require gradient synchronization
    across all parameters during each training step:'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 通信需求随着模型大小呈超线性增长，与参数数量呈线性增长。现代转换器模型在每个训练步骤中需要同步所有参数的梯度：
- en: '**GPT-3 (175 B parameters)**: 700 GB gradient exchange per step'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3（175 B参数）**：每步700 GB的梯度交换'
- en: '**GPT-4 (estimated 1.8 T parameters)**: ~7 TB gradient exchange per step'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-4（估计1.8 T参数）**：每步约7 TB的梯度交换'
- en: '**Future 10 T parameter models**: ~40 TB gradient exchange per step'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未来10 T参数模型**：每步约40 TB的梯度交换'
- en: Even with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization
    for 10 T parameter models would require 22+ seconds per training step, making
    distributed training impractical without algorithmic innovations like gradient
    compression or asynchronous updates.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 即使拥有像 NVLink 4.0（1.8 TB/s）这样的高级互连，对于10 T参数模型的梯度同步也需要每步训练22+秒，这使得没有像梯度压缩或异步更新这样的算法创新，分布式训练变得不切实际。
- en: Multi-GPU systems face additional bottlenecks from memory bandwidth competition.
    When 8 H100 GPUs simultaneously access HBM during gradient computation, the effective
    memory bandwidth per GPU drops from 3.35 TB/s to approximately 2.1 TB/s due to
    memory controller contention and NUMA effects. This 37% reduction in memory performance
    compounds communication overhead, further limiting scalability.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 多GPU系统面临来自内存带宽竞争的额外瓶颈。当8个H100 GPU在梯度计算期间同时访问HBM时，由于内存控制器竞争和NUMA效应，每个GPU的有效内存带宽从3.35 TB/s下降到大约2.1 TB/s。这种37%的内存性能下降加剧了通信开销，进一步限制了可扩展性。
- en: 'Understanding Amdahl’s Law guides optimization strategies:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Amdahl定律指导优化策略：
- en: '**Gradient Compression**: Reduce communication volume by 10-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> through sparsification
    and quantization'
  id: totrans-976
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**梯度压缩**：通过稀疏化和量化将通信量减少10-100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>'
- en: '**Pipeline Parallelism**: Overlap communication with computation to hide gradient
    synchronization latency'
  id: totrans-977
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**流水线并行性**：通过重叠通信和计算来隐藏梯度同步延迟'
- en: '**Model Parallelism**: Partition models across devices to reduce gradient synchronization
    requirements'
  id: totrans-978
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型并行性**：将模型分区到设备上以减少梯度同步需求'
- en: '**Asynchronous Updates**: Relax consistency requirements to eliminate synchronization
    barriers'
  id: totrans-979
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步更新**：放宽一致性要求以消除同步障碍'
- en: These techniques modify the effective value of <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> and <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> in Amdahl’s equation,
    enabling better scaling behavior at the cost of algorithmic complexity.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术修改了Amdahl方程中<semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>和<semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics>的有效值，以算法复杂性的代价实现了更好的扩展行为。
- en: TPU Pods
  id: totrans-981
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPU Pods
- en: As models and datasets continue to expand, training and inference workloads
    must extend beyond single-server configurations. This scaling requirement has
    led to the development of sophisticated distributed systems where multiple accelerators
    communicate across networks. Google’s TPU Pods represent a pioneering approach
    to this challenge, interconnecting hundreds of TPUs to function as a unified system
    ([Norman P. Jouppi et al. 2020](ch058.xhtml#ref-Jouppi2020tpuv4)).
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型和数据集的不断扩展，训练和推理工作负载必须超越单服务器配置。这种扩展需求导致了复杂分布式系统的发展，其中多个加速器通过网络进行通信。谷歌的TPU
    Pods代表了应对这一挑战的先驱方法，将数百个TPU互联以作为一个统一系统([Norman P. Jouppi等人 2020](ch058.xhtml#ref-Jouppi2020tpuv4))。
- en: The architectural design of TPU Pods differs fundamentally from traditional
    multi-GPU systems. While multi-GPU configurations typically rely on NVLink or
    PCIe connections within a single machine, TPU Pods employ high-bandwidth optical
    links to interconnect accelerators at data center scale. This design implements
    a 2D torus interconnect topology, enabling efficient data exchange between accelerators
    while minimizing communication bottlenecks as workloads scale across nodes.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: TPU Pods的架构设计与传统多GPU系统有根本性的不同。虽然多GPU配置通常依赖于单台机器内的NVLink或PCIe连接，但TPU Pods使用高带宽的光纤链路在数据中心规模上互联加速器。这种设计实现了2D环面互连拓扑，使得加速器之间能够高效交换数据，同时最小化通信瓶颈，当工作负载跨节点扩展时。
- en: The effectiveness of this architecture is demonstrated in its performance scaling
    capabilities. As illustrated in [Figure 11.11](ch017.xhtml#fig-tpu-pod-perf),
    TPU Pod performance exhibits near-linear scaling when running ResNet-50, from
    quarter-pod to full-pod configurations. The system achieves a remarkable 33.0<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> speedup when scaled
    to 1024 chips compared to a 16-TPU baseline. This scaling efficiency is particularly
    noteworthy in larger configurations, where performance continues to scale strongly
    even as the system expands from 128 to 1024 chips.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的有效性体现在其性能扩展能力上。如图 [图11.11](ch017.xhtml#fig-tpu-pod-perf) 所示，当运行ResNet-50时，TPU
    Pod的性能在从四分之一Pod配置到全Pod配置的过程中表现出近乎线性的扩展。当扩展到1024个芯片时，与16-TPU基线相比，系统实现了显著的33.0<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>速度提升。这种扩展效率在较大配置中尤其值得注意，即使系统从128个芯片扩展到1024个芯片，性能仍然能够持续强劲地扩展。
- en: '![](../media/file190.png)'
  id: totrans-985
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file190.png)'
- en: 'Figure 11.11: **Scaling Efficiency of TPU Pods**: Increasing the number of
    TPU chips within a pod maintains near-linear performance gains on ResNet-50, achieving
    a 33.0<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    speedup from 16 to 1024 chips. This efficient scaling provides the effectiveness
    of the 2D torus interconnect and high-bandwidth optical links in minimizing communication
    bottlenecks as workloads expand across multiple accelerators.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：**TPU Pods的扩展效率**：在Pod内增加TPU芯片的数量，在ResNet-50上保持近乎线性的性能提升，从16个芯片到1024个芯片实现了33.0<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的速度提升。这种高效的扩展提供了2D环面互连和高带宽光纤链路在最小化通信瓶颈方面的有效性，当工作负载在多个加速器之间扩展时。
- en: However, distributing AI workloads across an entire data center introduces distributed
    coordination challenges that fundamentally differ from single-node systems. The
    2D torus interconnect, while providing high bisection bandwidth, creates communication
    bottlenecks when training large transformer models that require AllReduce operations
    across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through
    the torus network, with worst-case communication requiring 32 hops between distant
    TPUs, creating latency penalties that compound with model size.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在整个数据中心内部分布AI工作负载引入了分布式协调挑战，这些挑战与单节点系统根本不同。2D环面互连虽然提供了高分割带宽，但在训练需要跨所有1,024个TPU进行AllReduce操作的大型变压器模型时，会创建通信瓶颈。每个参数梯度必须通过环面网络进行多次跳跃，最坏情况下，需要32次跳跃才能在遥远的TPU之间进行通信，这会随着模型大小的增加而累积延迟惩罚。
- en: The distributed memory architecture exacerbates coordination complexity—unlike
    multi-GPU systems with shared host memory, each TPU node maintains independent
    memory spaces, forcing explicit data marshaling and synchronization protocols.
    Network partition tolerance becomes critical as optical link failures can split
    the pod into disconnected islands, requiring sophisticated consensus algorithms
    to maintain training consistency.
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式内存架构加剧了协调复杂性——与具有共享主机内存的多GPU系统不同，每个TPU节点维护独立的内存空间，迫使显式数据打包和同步协议。网络分区容错变得至关重要，因为光链路故障可以将Pod分割成孤立的岛屿，需要复杂的共识算法来保持训练一致性。
- en: 'The energy cost of coordination also scales dramatically: moving data across
    the pod’s optical interconnect consumes 1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    more energy than on-chip communication within individual TPUs, transforming distributed
    training into a careful balance between computation parallelism and communication
    efficiency where AllReduce bandwidth, not compute capacity, determines overall
    training throughput.'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 协调的能量成本也急剧增加：在Pod的光互连中移动数据比在单个TPU内部的片上通信消耗1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>更多的能量，将分布式训练转化为计算并行性和通信效率之间的谨慎平衡，其中AllReduce带宽，而不是计算能力，决定了整体训练吞吐量。
- en: Wafer-Scale AI
  id: totrans-990
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 晶圆级AI
- en: At the frontier of AI scaling, wafer-scale[33](#fn33) integration represents
    a paradigm shift—abandoning traditional multi-chip architectures in favor of a
    single, massive AI processor. Rather than partitioning computation across discrete
    chips, this approach treats an entire silicon wafer as a unified compute fabric,
    eliminating the inefficiencies of inter-chip communication.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能扩展的前沿，晶圆级[33](#fn33)集成代表了范式转变——放弃传统的多芯片架构，转而采用单一、庞大的AI处理器。这种方法不是将计算分配到离散的芯片上，而是将整个硅晶圆视为统一的计算布线，消除了芯片间通信的低效性。
- en: As shown in [Figure 11.12](ch017.xhtml#fig-processor-trends), Cerebras’ Wafer-Scale
    Engine (WSE) processors break away from the historical transistor scaling trends
    of CPUs, GPUs, and TPUs. While these architectures have steadily increased transistor
    counts along an exponential trajectory, WSE introduces an entirely new scaling
    paradigm, integrating trillions of transistors onto a single wafer—far surpassing
    even the most advanced GPUs and TPUs. With WSE-3, this trajectory continues, pushing
    wafer-scale AI to unprecedented levels ([Systems 2021a](ch058.xhtml#ref-Cerebras2021wse2)).
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图11.12](ch017.xhtml#fig-processor-trends)所示，Cerebras的晶圆级引擎(WSE)处理器打破了CPU、GPU和TPU历史上晶体管缩放的趋势。虽然这些架构一直在指数轨迹上稳步增加晶体管数量，但WSE引入了一种全新的缩放范式，将数万亿个晶体管集成到单个晶圆上——远远超过了最先进的GPU和TPU。随着WSE-3的推出，这一轨迹继续推进，将晶圆级AI推向前所未有的水平([Systems
    2021a](ch058.xhtml#ref-Cerebras2021wse2))。
- en: '![](../media/file191.png)'
  id: totrans-993
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file191.png)'
- en: 'Figure 11.12: **Wafer-Scale Integration**: Wafer-scale AI processors integrate
    trillions of transistors onto a single wafer, offering ultra-fast on-die communication
    to surpass traditional multi-chip architectures and achieve unprecedented performance
    levels.'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：**晶圆级集成**：晶圆级AI处理器将数万亿个晶体管集成到单个晶圆上，提供超快的片上通信，以超越传统的多芯片架构并实现前所未有的性能水平。
- en: The fundamental advantage of wafer-scale AI is its ultra-fast, on-die communication.
    Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries
    between separate devices, wafer-scale AI enables near-instantaneous data transfer
    across its vast compute array. This architecture drastically reduces communication
    latency, unlocking performance levels that are unachievable with conventional
    multi-chip systems.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 晶圆级AI的基本优势是其超快的芯片内通信。与芯片、GPU或TPU Pod不同，在这些设备中数据必须穿越不同设备之间的物理边界，晶圆级AI能够在其庞大的计算阵列中实现近乎瞬间的数据传输。这种架构极大地降低了通信延迟，解锁了传统多芯片系统无法达到的性能水平。
- en: However, achieving this level of integration introduces formidable engineering
    challenges. Thermal dissipation, fault tolerance, and manufacturing yield become
    major constraints when fabricating a processor of this scale. These sustainability
    challenges, including energy consumption and resource utilization, are examined
    in [Chapter 18](ch024.xhtml#sec-sustainable-ai). Unlike distributed TPU systems,
    which mitigate failures by dynamically re-routing workloads, wafer-scale AI must
    incorporate built-in redundancy mechanisms to tolerate localized defects in the
    silicon. Successfully addressing these challenges is essential to realizing the
    full potential of wafer-scale computing as the next frontier in AI acceleration.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实现这种程度的集成引入了巨大的工程挑战。当制造这种规模的处理器时，热量散失、容错性和制造良率成为主要约束。这些可持续性挑战，包括能源消耗和资源利用，在[第18章](ch024.xhtml#sec-sustainable-ai)中进行了探讨。与通过动态重新路由工作负载来减轻故障的分布式TPU系统不同，晶圆级AI必须内置冗余机制以容忍硅片上的局部缺陷。成功解决这些挑战对于实现晶圆级计算作为AI加速下一个前沿的潜力至关重要。
- en: AI Systems Scaling Trajectory
  id: totrans-997
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI系统扩展轨迹
- en: '[Table 11.20](ch017.xhtml#tbl-scaling-trajectory) illustrates the progressive
    scaling of AI acceleration, from single-chip processors to increasingly complex
    architectures such as chiplet-based designs, multi-GPU systems, TPU Pods, and
    wafer-scale AI. Each step in this evolution introduces new challenges related
    to data movement, memory access, interconnect efficiency, and workload distribution.
    While chiplets enable modular scaling within a package, they introduce latency
    and memory coherence issues. Multi-GPU systems rely on high-speed interconnects
    like NVLink but face synchronization and communication bottlenecks. TPU Pods push
    scalability further by distributing workloads across clusters, yet they must contend
    with interconnect congestion and workload partitioning. At the extreme end, wafer-scale
    AI integrates an entire wafer into a single computational unit, presenting unique
    challenges in thermal management and fault tolerance.'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.20](ch017.xhtml#tbl-scaling-trajectory)展示了AI加速的渐进式扩展，从单芯片处理器到越来越复杂的架构，如基于Chiplet的设计、多GPU系统、TPU
    Pod和晶圆级AI。这一演变过程中的每一步都引入了与数据移动、内存访问、互连效率和负载分配相关的新挑战。虽然Chiplet允许封装内的模块化扩展，但它们引入了延迟和内存一致性问题。多GPU系统依赖于高速互连如NVLink，但面临同步和通信瓶颈。TPU
    Pod通过在集群间分配工作负载进一步推动可扩展性，但它们必须应对互连拥塞和工作负载分区。在极端情况下，晶圆级AI将整个晶圆集成到一个单一的计算单元中，在热管理和容错性方面提出了独特的挑战。'
- en: 'Table 11.20: **AI Acceleration Trends**: Scaling AI systems provides increasing
    challenges in data movement and memory access, driving architectural innovations
    from chiplets to wafer-scale integration. Each approach introduces unique trade-offs
    between modularity, latency, and complexity, demanding careful consideration of
    interconnect efficiency and workload distribution.'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.20：**AI加速趋势**：扩展AI系统在数据移动和内存访问方面提供了越来越多的挑战，推动了从Chiplet到晶圆级集成的架构创新。每种方法都引入了模块化、延迟和复杂性之间的独特权衡，需要仔细考虑互连效率和负载分配。
- en: '| **Scaling Approach** | **Key Feature** | **Challenges** |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| **扩展方法** | **关键特性** | **挑战** |'
- en: '| --- | --- | --- |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Chiplets** | Modular scaling within a package | Inter-chiplet latency,
    memory coherence |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
  zh: '| **Chiplet** | 封装内的模块化扩展 | 芯片间延迟，内存一致性 |'
- en: '| **Multi-GPU** | External GPU interconnects (NVLink) | Synchronization overhead,
    communication bottlenecks |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '| **多GPU** | 外部GPU互连（NVLink） | 同步开销，通信瓶颈 |'
- en: '| **TPU Pods** | Distributed accelerator clusters | Interconnect congestion,
    workload partitioning |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '| **TPU Pod** | 分布式加速器集群 | 互连拥塞，工作负载分区 |'
- en: '| **Wafer-Scale AI** | Entire wafer as a single processor | Thermal dissipation,
    fault tolerance |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '| **晶圆级AI** | 整个晶圆作为一个单一处理器 | 热量散失，容错性 |'
- en: Computation and Memory Scaling Changes
  id: totrans-1006
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算和内存扩展变化
- en: As AI systems scale from single-chip accelerators to multi-chip architectures,
    the fundamental challenges in computation and memory evolve. In a single accelerator,
    execution is primarily optimized for locality—ensuring that computations are mapped
    efficiently to available processing elements while minimizing memory access latency.
    However, as AI systems extend beyond a single chip, the scope of these optimizations
    expands significantly. Computation must now be distributed across multiple accelerators,
    and memory access patterns become constrained by interconnect bandwidth and communication
    overhead.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI系统从单芯片加速器扩展到多芯片架构，计算和内存的基本挑战也在演变。在单个加速器中，执行主要优化局部性——确保计算被高效地映射到可用的处理元素，同时最小化内存访问延迟。然而，当AI系统扩展到单个芯片之外时，这些优化的范围显著扩大。计算现在必须分布到多个加速器，内存访问模式受到互连带宽和通信开销的限制。
- en: Multi-chip Execution Mapping
  id: totrans-1008
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多芯片执行映射
- en: In single-chip AI accelerators, computation placement is concerned with mapping
    workloads to PEs, vector units, and tensor cores. Mapping strategies aim to maximize
    data locality, ensuring that computations access nearby memory to reduce costly
    data movement.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 在单芯片AI加速器中，计算放置涉及将工作负载映射到PE（处理元素）、向量单元和张量核心。映射策略旨在最大化数据局部性，确保计算访问附近的内存以减少昂贵的内存移动。
- en: As AI systems scale to multi-chip execution, computation placement must consider
    several critical factors. Workloads need to be partitioned across multiple accelerators,
    which requires explicit coordination of execution order and dependencies. This
    division is essential due to the inherent latency associated with cross-chip communication,
    which contrasts sharply with single-chip systems that benefit from shared on-chip
    memory. Accordingly, computation scheduling must be interconnect-aware to manage
    these delays effectively. Additionally, achieving load balancing across accelerators
    is vital; an uneven distribution of tasks can result in some accelerators remaining
    underutilized while others operate at full capacity, ultimately hindering overall
    system performance.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI系统扩展到多芯片执行，计算放置必须考虑几个关键因素。工作负载需要在多个加速器之间分区，这需要显式协调执行顺序和依赖关系。这种划分对于与跨芯片通信相关的固有延迟至关重要，这与受益于共享片上内存的单芯片系统形成鲜明对比。因此，计算调度必须考虑互连以有效管理这些延迟。此外，在加速器之间实现负载平衡至关重要；任务的不均匀分布可能导致某些加速器利用率不足，而其他加速器则处于满负荷运行，最终阻碍整体系统性能。
- en: For example, in multi-GPU training, computation mapping must ensure that each
    GPU has a balanced portion of the workload while minimizing expensive cross-GPU
    communication. Similarly, in TPU Pods, mapping strategies must align with the
    torus interconnect topology, ensuring that computation is placed to minimize long-distance
    data transfers.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在多GPU训练中，计算映射必须确保每个GPU都有平衡的工作负载部分，同时最小化昂贵的跨GPU通信。同样，在TPU Pods中，映射策略必须与环状互连拓扑相一致，确保计算放置以最小化长距离数据传输。
- en: Thus, while computation placement in single-chip systems is a local optimization
    problem, in multi-chip architectures, it becomes a global optimization challenge
    where execution efficiency depends on minimizing inter-chip communication and
    balancing workload distribution.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然单芯片系统中的计算放置是一个局部优化问题，但在多芯片架构中，它变成了一个全局优化挑战，其中执行效率取决于最小化芯片间通信和平衡工作负载分配。
- en: Distributed Access Memory Allocation
  id: totrans-1013
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分布式访问内存分配
- en: Memory allocation strategies in single-chip AI accelerators are designed to
    minimize off-chip memory accesses by using on-chip caches, SRAM, and HBM. Techniques
    such as tiling, data reuse, and kernel fusion ensure that computations make efficient
    use of fast local memory.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 单芯片AI加速器中的内存分配策略旨在通过使用片上缓存、SRAM和HBM来最小化片外内存访问。如分块、数据重用和内核融合等技术确保计算有效地使用快速本地内存。
- en: In multi-chip AI systems, each accelerator manages its own local memory, which
    necessitates the explicit allocation of model parameters, activations, and intermediate
    data across the devices. Unlike single-chip execution where data is fetched once
    and reused, multi-chip setups require deliberate strategies to minimize redundant
    data transfers, as data must be communicated between accelerators. Additionally,
    when overlapping data is processed by multiple accelerators, the synchronization
    of shared data can introduce significant overhead that must be carefully managed
    to ensure efficient execution.
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 在多芯片AI系统中，每个加速器管理自己的本地内存，这需要在设备之间显式分配模型参数、激活和中间数据。与单芯片执行中数据只获取一次并重复使用不同，多芯片设置需要采取明确策略以最小化冗余数据传输，因为数据必须在加速器之间进行通信。此外，当多个加速器处理重叠数据时，共享数据的同步可能会引入显著的开销，这必须被仔细管理以确保高效执行。
- en: For instance, in multi-GPU deep learning, gradient synchronization across GPUs
    is a memory-intensive operation that must be optimized to avoid network congestion
    ([Shallue et al. 2019](ch058.xhtml#ref-Shallue2019measuring)). In wafer-scale
    AI, memory allocation must account for fault tolerance and redundancy mechanisms,
    ensuring that defective regions of the wafer do not disrupt execution.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在多GPU深度学习中，跨GPU的梯度同步是一个内存密集型操作，必须优化以避免网络拥塞（[Shallue等人2019](ch058.xhtml#ref-Shallue2019measuring)）。在晶圆级AI中，内存分配必须考虑容错和冗余机制，确保晶圆上的缺陷区域不会干扰执行。
- en: Thus, while memory allocation in single-chip accelerators focuses on local cache
    efficiency, in multi-chip architectures, it must be explicitly coordinated across
    accelerators to balance memory bandwidth, minimize redundant transfers, and reduce
    synchronization overhead.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然单芯片加速器中的内存分配侧重于本地缓存效率，但在多芯片架构中，必须在加速器之间显式协调以平衡内存带宽、最小化冗余传输并减少同步开销。
- en: Data Movement Constraints
  id: totrans-1018
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据移动限制
- en: In single-chip AI accelerators, data movement optimization is largely focused
    on minimizing on-chip memory access latency. Techniques such as weight stationarity,
    input stationarity, and tiling ensure that frequently used data remains close
    to the execution units, reducing off-chip memory traffic.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 在单芯片AI加速器中，数据移动优化主要集中在最小化芯片内内存访问延迟。诸如权重稳定性、输入稳定性和瓦片化等技术确保常用数据保持靠近执行单元，从而减少芯片外内存流量。
- en: In multi-chip architectures, data movement transcends being merely an intra-chip
    issue and becomes a significant system-wide bottleneck. Scaling introduces several
    critical challenges, foremost among them being inter-chip bandwidth constraints;
    communication links such as PCIe, NVLink, and TPU interconnects operate at speeds
    that are considerably slower than those of on-chip memory accesses. Additionally,
    when accelerators share model parameters or intermediate computations, the resulting
    data synchronization overhead, which encompass latency and contention, can markedly
    impede execution. Finally, optimizing collective communication is essential for
    workloads that require frequent data exchanges, such as gradient updates in deep
    learning training, where minimizing synchronization penalties is imperative for
    achieving efficient system performance.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 在多芯片架构中，数据移动已超越仅仅是芯片内部问题，并成为系统级的一个重大瓶颈。扩展引入了几个关键挑战，其中最重要的是芯片间带宽限制；如PCIe、NVLink和TPU互连等通信链路的速度远低于芯片内内存访问速度。此外，当加速器共享模型参数或中间计算时，由此产生的数据同步开销（包括延迟和竞争），可能会显著阻碍执行。最后，对于需要频繁数据交换的工作负载，如深度学习训练中的梯度更新，优化集体通信对于实现高效系统性能至关重要。
- en: For example, in TPU Pods, systolic execution models ensure that data moves in
    structured patterns, reducing unnecessary off-chip transfers. In multi-GPU inference,
    techniques like asynchronous data fetching and overlapping computation with communication
    help mitigate inter-chip latency.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在TPU Pods中，收缩执行模型确保数据以结构化模式移动，减少不必要的芯片外传输。在多GPU推理中，异步数据获取和计算与通信重叠等技术有助于减轻芯片间延迟。
- en: Thus, while data movement optimization in single-chip systems focuses on cache
    locality and tiling, in multi-chip architectures, the primary challenge is reducing
    inter-chip communication overhead to maximize efficiency.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然单芯片系统中的数据移动优化侧重于缓存局部性和瓦片化，但在多芯片架构中，主要挑战是减少芯片间通信开销以最大化效率。
- en: Compilers and Runtimes Adaptation
  id: totrans-1023
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编译器和运行时适应性
- en: As AI acceleration extends beyond a single chip, compilers and runtimes must
    adapt to manage computation placement, memory organization, and execution scheduling
    across multiple accelerators. The fundamental principles of locality, parallelism,
    and efficient scheduling remain essential, but their implementation requires new
    strategies for distributed execution.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI加速扩展到单个芯片之外，编译器和运行时必须适应以管理多个加速器之间的计算放置、内存组织和执行调度。局部性、并行性和高效调度的基本原理仍然是基本的，但它们的实现需要分布式执行的新策略。
- en: One of the primary challenges in scaling AI execution is computation placement.
    In a single-chip accelerator, workloads are mapped to processing elements, vector
    units, and tensor cores with an emphasis on minimizing on-chip data movement and
    maximizing parallel execution. However, in a multi-chip system, computation must
    be partitioned hierarchically, where workloads are distributed not just across
    cores within a chip, but also across multiple accelerators. Compilers handle this
    by implementing interconnect-aware scheduling, optimizing workload placement to
    minimize costly inter-chip communication.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展AI执行规模的主要挑战中，计算放置是一个关键问题。在单芯片加速器中，工作负载被映射到处理元素、向量单元和张量核心，重点在于最小化芯片内部数据移动并最大化并行执行。然而，在多芯片系统中，计算必须进行分层分区，工作负载不仅分布在芯片内部的各个核心之间，还分布在多个加速器之间。编译器通过实现互联感知调度，优化工作负载放置以最小化昂贵的芯片间通信来处理这个问题。
- en: Similarly, memory management evolves as scaling extends beyond a single accelerator.
    In a single-chip system, local caching, HBM reuse, and efficient tiling strategies
    ensure that frequently accessed data remains close to computation units. However,
    in a multi-chip system, each accelerator has its own independent memory, requiring
    explicit memory partitioning and coordination. Compilers optimize memory layouts
    for distributed execution, while runtimes introduce data prefetching and caching
    mechanisms to reduce inter-chip memory access overhead.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，随着扩展超出单个加速器，内存管理也随着扩展而发展。在单芯片系统中，本地缓存、HBM重用和高效的分块策略确保频繁访问的数据保持靠近计算单元。然而，在多芯片系统中，每个加速器都有自己的独立内存，需要显式的内存分区和协调。编译器优化分布式执行的内存布局，而运行时引入数据预取和缓存机制以减少芯片间内存访问开销。
- en: Beyond computation and memory, data movement becomes a major bottleneck at scale.
    In a single-chip accelerator, efficient on-chip caching and minimized DRAM accesses
    ensure that data is reused efficiently. However, in a multi-chip system, communication-aware
    execution becomes critical, requiring compilers to generate execution plans that
    overlap computation with data transfers. Runtimes handle inter-chip synchronization,
    ensuring that workloads are not stalled by waiting for data to arrive from remote
    accelerators.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算和内存之外，在扩展规模时，数据移动成为了一个主要的瓶颈。在单芯片加速器中，高效的芯片内部缓存和最小化的DRAM访问确保数据被高效地重用。然而，在多芯片系统中，通信感知执行变得至关重要，需要编译器生成执行计划，使计算与数据传输重叠。运行时处理芯片间同步，确保工作负载不会被等待从远程加速器到达的数据所阻塞。
- en: Finally, execution scheduling must be extended for global coordination. In single-chip
    AI execution, scheduling is primarily concerned with parallelism and maximizing
    compute occupancy within the accelerator. However, in a multi-chip system, scheduling
    must balance workload distribution across accelerators while taking interconnect
    bandwidth and synchronization latency into account. Runtimes manage this complexity
    by implementing adaptive scheduling strategies that dynamically adjust execution
    plans based on system state and network congestion.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，执行调度必须扩展以实现全局协调。在单芯片AI执行中，调度主要关注并行性和最大化加速器内的计算占用率。然而，在多芯片系统中，调度必须在考虑互联带宽和同步延迟的同时平衡加速器之间的工作负载分布。运行时通过实现自适应调度策略来管理这种复杂性，这些策略根据系统状态和网络拥塞动态调整执行计划。
- en: '[Table 11.21](ch017.xhtml#tbl-scaling-adaptations) summarizes these key adaptations,
    highlighting how compilers and runtimes extend their capabilities to efficiently
    support multi-chip AI execution.'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.21](ch017.xhtml#tbl-scaling-adaptations) 总结了这些关键适应措施，突出了编译器和运行时如何扩展其功能以有效地支持多芯片AI执行。'
- en: Thus, while the fundamentals of AI acceleration remain intact, compilers and
    runtimes must extend their functionality to operate efficiently across distributed
    systems. The next section will explore how mapping strategies evolve to further
    optimize multi-chip AI execution.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管AI加速的基本原理保持不变，但编译器和运行时必须扩展其功能以在分布式系统中高效运行。下一节将探讨映射策略如何演变以进一步优化多芯片AI执行。
- en: 'Table 11.21: **Multi-Chip Adaptations**: Efficient AI execution on multiple
    accelerators requires coordinated adjustments to computation placement, memory
    management, and scheduling to balance workload distribution and minimize communication
    overhead. Compilers and runtimes extend their capabilities to dynamically adapt
    to system state and network congestion, enabling scalable and performant multi-chip
    AI systems.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.21：**多芯片适应**：在多个加速器上高效执行AI需要协调调整计算放置、内存管理和调度，以平衡工作负载分布并最小化通信开销。编译器和运行时扩展其功能以动态适应系统状态和网络拥塞，从而实现可扩展和性能良好的多芯片AI系统。
- en: '| **Aspect** | **Single-Chip AI Accelerator** | **Multi-Chip AI System & How
    Compilers/Runtimes Adapt** |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **单芯片AI加速器** | **多芯片AI系统及编译器/运行时如何适应** |'
- en: '| --- | --- | --- |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Computation Placement** | Local PEs, tensor cores, vector units | Hierarchical
    mapping, interconnect-aware scheduling |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| **计算放置** | 本地PE、张量核心、向量单元 | 分层映射、互联感知调度 |'
- en: '| **Memory Management** | Caching, HBM reuse, local tiling | Distributed allocation,
    prefetching, caching |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| **内存管理** | 缓存、HBM重用、本地分块 | 分布式分配、预取、缓存 |'
- en: '| **Data Movement** | On-chip reuse, minimal DRAM access | Communication-aware
    execution, overlap transfers |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| **数据移动** | 芯片内重用，最小DRAM访问 | 通信感知执行，重叠传输 |'
- en: '| **Execution Scheduling** | Parallelism, compute occupancy | Global scheduling,
    interconnect-aware balancing |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| **执行调度** | 并行性，计算占用率 | 全局调度，互联感知平衡 |'
- en: Execution Models Adaptation
  id: totrans-1038
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行模型适应
- en: As AI accelerators scale beyond a single chip, execution models must evolve
    to account for the complexities introduced by distributed computation, memory
    partitioning, and inter-chip communication. In single-chip accelerators, execution
    is optimized for local processing elements, with scheduling strategies that balance
    parallelism, locality, and data reuse. However, in multi-chip AI systems, execution
    must now be coordinated across multiple accelerators, introducing new challenges
    in workload scheduling, memory coherence, and interconnect-aware execution.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 当AI加速器扩展到单芯片之外时，执行模型必须演变以应对分布式计算、内存分区和芯片间通信引入的复杂性。在单芯片加速器中，执行针对本地处理元素进行优化，采用平衡并行性、局部性和数据重用的调度策略。然而，在多芯片AI系统中，现在必须在多个加速器之间协调执行，这引入了新的工作负载调度、内存一致性和互联感知执行挑战。
- en: This section explores how execution models change as AI acceleration scales,
    focusing on scheduling, memory coordination, and runtime management in multi-chip
    systems.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了随着AI加速的扩展，执行模型如何变化，重点关注多芯片系统中的调度、内存协调和运行时管理。
- en: Cross-Accelerator Scheduling
  id: totrans-1041
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨加速器调度
- en: In single-chip AI accelerators, execution scheduling is primarily aimed at optimizing
    parallelism within the processor. This involves ensuring that workloads are effectively
    mapped to tensor cores, vector units, and special function units by employing
    techniques designed to enhance data locality and resource utilization. For instance,
    static scheduling uses a predetermined execution order that is carefully optimized
    for locality and reuse, while dynamic scheduling adapts in real time to variations
    in workload demands. Additionally, pipeline execution divides computations into
    stages, thereby maximizing hardware utilization by maintaining a continuous flow
    of operations.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 在单芯片AI加速器中，执行调度主要针对优化处理器内部的并行性。这涉及到通过采用旨在增强数据局部性和资源利用率的技巧，确保工作负载有效地映射到张量核心、向量单元和特殊功能单元。例如，静态调度使用预先确定的执行顺序，该顺序经过精心优化以适应局部性和重用，而动态调度则实时适应工作负载需求的变化。此外，流水线执行将计算划分为阶段，通过保持操作的连续流动来最大化硬件利用率。
- en: In contrast, scheduling in multi-chip architectures must address the additional
    challenges posed by inter-chip dependencies. Workload partitioning in such systems
    involves distributing tasks across various accelerators such that each receives
    an optimal share of the workload, all while minimizing the overhead caused by
    excessive communication. Interconnect-aware scheduling is essential to align execution
    timing with the constraints of inter-chip bandwidth, thus preventing performance
    stalls. Latency hiding techniques also play a critical role, as they enable the
    overlapping of computation with communication, effectively reducing waiting times.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多芯片架构中的调度必须解决芯片间依赖带来的额外挑战。在这样的系统中，工作负载分区涉及将任务分配到各种加速器，以便每个加速器都获得最佳的工作负载份额，同时最大限度地减少由过多通信造成的开销。感知互连的调度对于将执行时间与芯片间带宽约束对齐至关重要，从而防止性能停滞。延迟隐藏技术也发挥着关键作用，因为它们使计算与通信重叠，有效地减少了等待时间。
- en: For example, in multi-GPU inference scenarios, execution scheduling is implemented
    in a way that allows data to be prefetched concurrently with computation, thereby
    mitigating memory stalls. Similarly, TPU Pods leverage the systolic array model
    to tightly couple execution scheduling with data flow, ensuring that each TPU
    core receives its required data precisely when needed. Therefore, while single-chip
    execution scheduling is focused largely on maximizing internal parallelism, multi-chip
    systems require a more holistic approach that explicitly manages communication
    overhead and synchronizes workload distribution across accelerators.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在多GPU推理场景中，执行调度以允许数据与计算同时预取，从而减轻内存停滞。同样，TPU Pods利用 systolic array 模型将执行调度与数据流紧密耦合，确保每个TPU核心在需要时精确地接收到所需数据。因此，虽然单芯片执行调度主要关注最大化内部并行性，但多芯片系统需要更全面的方法，该方法明确管理通信开销并同步加速器之间的工作负载分配。
- en: Cross-Accelerator Coordination
  id: totrans-1045
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨加速器协调
- en: In single-chip AI accelerators, memory coordination is managed through sophisticated
    local caching strategies that keep frequently used data in close proximity to
    the execution units. Techniques such as tiling, kernel fusion, and data reuse
    are employed to reduce the dependency on slower memory hierarchies, thereby enhancing
    performance and reducing latency.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 在单芯片AI加速器中，内存协调通过复杂的本地缓存策略来管理，这些策略将频繁使用的数据保留在执行单元附近。采用如分块、内核融合和数据重用等技术来减少对较慢的内存层次结构的依赖，从而提高性能并降低延迟。
- en: In contrast, multi-chip architectures present a distributed memory coordination
    challenge that necessitates more deliberate management. Each accelerator in such
    a system possesses its own independent memory, which must be organized through
    explicit memory partitioning to minimize cross-chip data accesses. Additionally,
    ensuring consistency and synchronization of shared data across accelerators is
    essential to maintain computational correctness. Efficient communication mechanisms
    must also be implemented to schedule data transfers in a way that limits overhead
    associated with synchronization delays.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多芯片架构提出了分布式内存协调挑战，这需要更谨慎的管理。在这样的系统中，每个加速器都拥有自己的独立内存，必须通过显式的内存分区来组织，以最小化跨芯片数据访问。此外，确保加速器之间共享数据的一致性和同步对于保持计算正确性至关重要。还必须实施有效的通信机制，以便以限制与同步延迟相关的开销的方式进行数据传输。
- en: For instance, in distributed deep learning training, model parameters must be
    synchronized across multiple GPUs using methods such as all-reduce, where gradients
    are aggregated across accelerators while reducing communication latency. In wafer-scale
    AI, memory coordination must further address fault-tolerant execution, ensuring
    that defective areas do not compromise overall system performance. Consequently,
    while memory coordination in single-chip systems is primarily concerned with cache
    optimization, multi-chip architectures require management of distributed memory
    access, synchronization, and communication to achieve efficient execution.
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在分布式深度学习训练中，模型参数必须使用全归约等方法在多个GPU之间同步，这些方法可以在加速器之间聚合梯度同时减少通信延迟。在晶圆级AI中，内存协调必须进一步解决容错执行问题，确保损坏区域不会影响整体系统性能。因此，虽然单芯片系统中的内存协调主要关注缓存优化，但多芯片架构需要管理分布式内存访问、同步和通信以实现高效执行。
- en: Cross-Accelerator Execution Management
  id: totrans-1049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨加速器执行管理
- en: Execution in single-chip AI accelerators is managed by AI runtimes that handle
    workload scheduling, memory allocation, and hardware execution. These runtimes
    optimize execution at the kernel level, ensuring that computations are executed
    efficiently within the available resources.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 单芯片人工智能加速器中的执行由处理工作负载调度、内存分配和硬件执行的AI运行时管理。这些运行时在内核级别优化执行，确保计算在可用资源内高效执行。
- en: In multi-chip AI systems, runtimes must incorporate a strategy for distributed
    execution orchestration. This approach ensures that both computation and memory
    access are seamlessly coordinated across multiple accelerators, enabling efficient
    utilization of hardware resources and minimizing bottlenecks associated with data
    transfers.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 在多芯片人工智能系统中，运行时必须包含分布式执行编排的策略。这种方法确保计算和内存访问在多个加速器之间无缝协调，从而实现硬件资源的有效利用并最小化与数据传输相关的瓶颈。
- en: These systems require robust mechanisms for cross-chip workload synchronization.
    Careful management of dependencies and timely coordination between accelerators
    are essential to prevent stalls in execution that may arise from delays in inter-chip
    communication. Such synchronization is critical for maintaining the flow of computation,
    particularly in environments where latency can significantly impact overall performance.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统需要强大的跨芯片工作负载同步机制。仔细管理依赖关系和加速器之间的及时协调对于防止由于芯片间通信延迟而可能出现的执行停滞至关重要。这种同步对于保持计算流程至关重要，尤其是在延迟可能显著影响整体性能的环境中。
- en: Finally, adaptive execution models play a pivotal role in contemporary multi-chip
    architectures. These models dynamically adjust execution plans based on current
    hardware availability and communication constraints, ensuring that the system
    can respond to changing conditions and optimize performance in real time. Together,
    these strategies provide a resilient framework for managing the complexities of
    distributed AI execution.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，自适应执行模型在当代多芯片架构中扮演着关键角色。这些模型根据当前硬件可用性和通信约束动态调整执行计划，确保系统能够响应变化条件并在实时中优化性能。这些策略共同提供了一个有弹性的框架，用于管理分布式人工智能执行中的复杂性。
- en: For example, in Google’s TPU Pods, the TPU runtime is responsible for scheduling
    computations across multiple TPU cores, ensuring that workloads are executed in
    a way that minimizes communication bottlenecks. In multi-GPU frameworks like PyTorch
    and TensorFlow, runtime execution must synchronize operations across GPUs, ensuring
    that data is transferred efficiently while maintaining execution order.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在谷歌的TPU Pods中，TPU运行时负责在多个TPU核心之间调度计算，确保工作负载以最小化通信瓶颈的方式执行。在PyTorch和TensorFlow等多GPU框架中，运行时执行必须同步GPU之间的操作，确保数据高效传输的同时保持执行顺序。
- en: Thus, while single-chip runtimes focus on optimizing execution within a single
    processor, multi-chip runtimes must handle system-wide execution, balancing computation,
    memory, and interconnect performance.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然单芯片运行时专注于优化单个处理器内的执行，但多芯片运行时必须处理整个系统的执行，平衡计算、内存和互连性能。
- en: Computation Placement Adaptation
  id: totrans-1056
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算放置自适应
- en: As AI systems expand beyond single-chip execution, computation placement must
    adapt to account for inter-chip workload distribution and interconnect efficiency.
    In single-chip accelerators, compilers optimize placement by mapping workloads
    to tensor cores, vector units, and PEs, ensuring maximum parallelism while minimizing
    on-chip data movement. However, in multi-chip systems, placement strategies must
    address interconnect bandwidth constraints, synchronization latency, and hierarchical
    workload partitioning across multiple accelerators.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能系统扩展到单芯片执行之外，计算放置必须适应考虑芯片间工作负载分布和互连效率。在单芯片加速器中，编译器通过将工作负载映射到张量核心、向量单元和PE来优化放置，确保最大并行性同时最小化芯片内数据移动。然而，在多芯片系统中，放置策略必须解决互连带宽约束、同步延迟以及多个加速器之间的分层工作负载分区问题。
- en: '[Table 11.22](ch017.xhtml#tbl-computation-placement) highlights these adaptations.
    To reduce expensive cross-chip communication, compilers now implement interconnect-aware
    workload partitioning, strategically assigning computations to accelerators based
    on communication cost. For instance, in multi-GPU training, compilers optimize
    placement to minimize NVLink or PCIe traffic, whereas TPU Pods leverage the torus
    interconnect topology to enhance data exchanges.'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11.22](ch017.xhtml#tbl-computation-placement)突出了这些适应措施。为了减少昂贵的跨芯片通信，编译器现在实现感知互联的工作负载分区，根据通信成本战略性地将计算分配给加速器。例如，在多GPU训练中，编译器优化放置以最小化NVLink或PCIe流量，而TPU
    Pods利用环状互联拓扑结构来增强数据交换。'
- en: 'Table 11.22: **Computation Placement Strategies**: Multi-chip AI systems necessitate
    hierarchical workload mapping to minimize communication overhead; compilers adapt
    single-chip optimization techniques by considering interconnect bandwidth and
    latency when assigning computations to accelerators. This table contrasts computation
    placement in single-chip systems—local to processing elements—with multi-chip
    systems, where placement strategies prioritize efficient data exchange across
    accelerators.'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.22：**计算放置策略**：多芯片AI系统需要分层工作负载映射以最小化通信开销；编译器通过考虑互联带宽和延迟，在分配计算到加速器时采用单芯片优化技术。此表对比了单芯片系统中的计算放置（本地到处理单元）与多芯片系统，其中放置策略优先考虑加速器之间高效的数据交换。
- en: '| **Aspect** | **Single-Chip AI Accelerator** | **Multi-Chip AI System & How
    Compilers/Runtimes Adapt** |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **单芯片AI加速器** | **多芯片AI系统及编译器/运行时的适应方式** |'
- en: '| --- | --- | --- |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Computation Placement** | Local PEs, tensor cores, vector units | Hierarchical
    mapping, interconnect-aware scheduling |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '| **计算放置** | 本地PE、张量核心、向量单元 | 分层映射，感知互联的调度 |'
- en: '| **Workload Distribution** | Optimized within a single chip | Partitioning
    across accelerators, minimizing inter-chip communication |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
  zh: '| **工作负载分布** | 单芯片内优化 | 横跨加速器分区，最小化芯片间通信 |'
- en: '| **Synchronization** | Managed within local execution units | Runtimes dynamically
    balance workloads, adjust execution plans |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
  zh: '| **同步** | 在本地执行单元内管理 | 运行时动态平衡工作负载，调整执行计划 |'
- en: Runtimes complement this by dynamically managing execution workloads, adjusting
    placement in real-time to balance loads across accelerators. Unlike static compilation,
    which assumes a fixed hardware topology, AI runtimes continuously monitor system
    conditions and migrate tasks as needed to prevent bottlenecks. This ensures efficient
    execution even in environments with fluctuating workload demands or varying hardware
    availability.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时通过动态管理执行工作负载，实时调整放置策略以平衡加速器之间的负载来补充这一点。与假设固定硬件拓扑的静态编译不同，AI运行时持续监控系统条件，并根据需要迁移任务以防止瓶颈。这确保了即使在工作负载需求波动或硬件可用性变化的环境中也能高效执行。
- en: Thus, computation placement at scale builds upon local execution optimizations
    while introducing new challenges in inter-chip coordination, communication-aware
    execution, and dynamic load balancing—challenges that extend to how memory hierarchies
    must adapt to support efficient execution across multi-chip architectures.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大规模计算放置建立在本地执行优化之上，同时引入了芯片间协调、通信感知执行和动态负载平衡的新挑战——这些挑战扩展到内存层次结构如何适应以支持多芯片架构上的高效执行。
- en: Navigating Multi-Chip AI Complexities
  id: totrans-1067
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索多芯片AI的复杂性
- en: The evolution of AI hardware, from single-chip accelerators to multi-chip systems
    and wafer-scale integration, highlights the increasing complexity of efficiently
    executing large-scale machine learning workloads. Scaling AI systems introduces
    new challenges in computation placement, memory management, and data movement.
    While the fundamental principles of AI acceleration remain consistent, their implementation
    must adapt to the constraints of distributed execution, interconnect bandwidth
    limitations, and synchronization overhead.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 从单芯片加速器到多芯片系统再到晶圆级集成，AI硬件的演变突显了高效执行大规模机器学习工作负载的复杂性不断增加。扩展AI系统引入了计算放置、内存管理和数据移动的新挑战。虽然AI加速的基本原理保持一致，但它们的实现必须适应分布式执行、互联带宽限制和同步开销的限制。
- en: Multi-chip AI architectures represent a significant step forward in addressing
    the computational demands of modern machine learning models. By distributing workloads
    across multiple accelerators, these systems offer increased performance, memory
    capacity, and scalability. However, realizing these benefits requires careful
    consideration of how computations are mapped to hardware, how memory is partitioned
    and accessed, and how execution is scheduled across a distributed system.
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 多芯片人工智能架构在解决现代机器学习模型的计算需求方面迈出了重要一步。通过在多个加速器之间分配工作负载，这些系统提供了更高的性能、更大的内存容量和可扩展性。然而，实现这些好处需要仔细考虑如何将计算映射到硬件，如何划分和访问内存，以及如何在分布式系统中调度执行。
- en: While we an overview of the key concepts and challenges in multi-chip AI acceleration
    as they extend beyond a single system, there is still much more to explore. As
    AI models continue to grow in size and complexity, new architectural innovations,
    mapping strategies, and runtime optimizations will be needed to sustain efficient
    execution. These emerging trends and future directions continue to evolve rapidly
    in the field. The ongoing development of AI hardware and software reflects a broader
    trend in computing, where specialization and domain-specific architectures are
    becoming increasingly important for addressing the unique demands of emerging
    workloads.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经概述了多芯片人工智能加速在超越单一系统时的关键概念和挑战，但仍有许多内容值得进一步探索。随着人工智能模型在规模和复杂性上的持续增长，需要新的架构创新、映射策略和运行时优化来维持高效的执行。这些新兴趋势和未来方向在该领域正迅速演变。人工智能硬件和软件的持续发展反映了计算领域的一个更广泛趋势，即针对新兴工作负载的独特需求，专业化和特定领域架构正变得越来越重要。
- en: Understanding the principles and trade-offs involved in multi-chip AI acceleration
    enables machine learning engineers and system designers to make informed decisions
    about how to best deploy and optimize their models. Whether training large language
    models on TPU pods or deploying computer vision applications on multi-GPU systems,
    the ability to efficiently map computations to hardware will continue to be a
    critical factor in realizing the full potential of AI.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 理解多芯片人工智能加速所涉及的原则和权衡，使机器学习工程师和系统设计师能够就如何最佳部署和优化他们的模型做出明智的决定。无论是在大规模语言模型TPU集群上训练还是将计算机视觉应用部署在多GPU系统上，将计算高效映射到硬件的能力将继续是实现人工智能全部潜力的关键因素。
- en: Heterogeneous SoC AI Acceleration
  id: totrans-1072
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异构SoC人工智能加速
- en: The multi-chip architectures examined in previous sections focused primarily
    on maximizing computational throughput for data center workloads, where power
    budgets extend to kilowatts and cooling infrastructure supports rack-scale deployments.
    However, the hardware acceleration principles established—specialized compute
    units, memory hierarchy optimization, and workload mapping strategies—must adapt
    dramatically when deploying AI systems in mobile and edge environments. A smartphone
    operates within a 2 to 5 watt power budget, autonomous vehicles require deterministic
    real-time guarantees, and IoT sensors must function for years on battery power.
    These constraints necessitate heterogeneous System-on-Chip (SoC) architectures
    that coordinate multiple specialized processors within a single chip while meeting
    stringent power, thermal, and latency requirements fundamentally different from
    data center deployments.
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中考察的多芯片架构主要关注最大化数据中心工作负载的计算吞吐量，其中电源预算扩展到千瓦级别，冷却基础设施支持机架级部署。然而，在移动和边缘环境中部署人工智能系统时，建立的硬件加速原则——专用计算单元、内存层次优化和工作负载映射策略——必须做出巨大调整。智能手机在2到5瓦特的电源预算内运行，自动驾驶汽车需要确定性的实时保证，而物联网传感器必须在电池供电下工作数年。这些限制需要异构系统级芯片（SoC）架构，在单个芯片内协调多个专用处理器，同时满足与数据中心部署根本不同的严格电源、热和延迟要求。
- en: The mobile AI revolution has fundamentally transformed how we think about AI
    acceleration, moving beyond homogeneous data center architectures to heterogeneous
    System-on-Chip (SoC) designs that coordinate multiple specialized processors.
    Modern smartphones, automotive systems, and IoT devices integrate CPU cores, GPU
    shaders, digital signal processors (DSPs), and dedicated neural processing units
    (NPUs) within a single chip, requiring sophisticated orchestration to achieve
    optimal performance under strict power and thermal constraints.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 移动人工智能革命从根本上改变了我们对人工智能加速的看法，从同构数据中心架构转向异构片上系统（SoC）设计，这种设计协调多个专用处理器。现代智能手机、汽车系统和物联网设备在单个芯片内集成了CPU核心、GPU着色器、数字信号处理器（DSP）和专用神经网络处理器（NPU），需要复杂的编排才能在严格的功率和热约束下实现最佳性能。
- en: Mobile SoC Architecture Evolution
  id: totrans-1075
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移动SoC架构演变
- en: Qualcomm’s Snapdragon AI Engine exemplifies heterogeneous computing for mobile
    AI, coordinating Kryo CPU cores, Adreno GPU, Hexagon DSP, and dedicated NPU[34](#fn34)
    across a shared memory hierarchy. The Snapdragon 8 Gen 3 achieves 73 TOPS through
    intelligent workload distribution—computer vision kernels execute on the GPU’s
    parallel shaders, audio processing leverages the DSP’s specialized arithmetic
    units, while transformer attention mechanisms utilize the NPU’s optimized matrix
    engines. This coordination requires millisecond-precision scheduling to meet real-time
    constraints while managing thermal throttling and battery life optimization.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 高通Snapdragon AI Engine是移动人工智能异构计算的典范，它在一个共享的内存层次结构中协调Kryo CPU核心、Adreno GPU、Hexagon
    DSP和专用NPU[34](#fn34)。Snapdragon 8 Gen 3通过智能的工作负载分配实现了73 TOPS，计算机视觉内核在GPU的并行着色器上执行，音频处理利用DSP的专用算术单元，而变压器注意力机制利用NPU优化的矩阵引擎。这种协调需要毫秒级的调度精度以满足实时约束，同时管理热管理限制和电池寿命优化。
- en: While Qualcomm’s approach emphasizes diverse processor specialization, Apple’s
    vertically integrated strategy demonstrates how tight hardware-software co-design
    enables even more sophisticated heterogeneous execution. The M2 chip’s 16-core
    Neural Engine (15.8 TOPS) coordinates with the 10-core GPU and 8-core CPU through
    a unified memory architecture that eliminates data copying overhead. The Neural
    Engine’s specialized matrix multiplication units handle transformer layers, while
    the GPU’s Metal Performance Shaders accelerate convolutional operations, and the
    CPU manages control flow and dynamic layer selection. This fine-grained coordination
    enables real-time language translation and on-device image generation while maintaining
    millisecond response times.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然高通的方法强调处理器多样化，但苹果的垂直整合策略展示了紧密的软硬件协同设计如何实现更复杂的异构执行。M2芯片的16核心神经网络引擎（15.8 TOPS）通过统一的内存架构与10核心GPU和8核心CPU协调，消除了数据复制的开销。神经网络引擎的专用矩阵乘法单元处理变压器层，GPU的Metal性能着色器加速卷积操作，而CPU管理控制流和动态层选择。这种精细的协调实现了实时语言翻译和设备上图像生成，同时保持毫秒级响应时间。
- en: Beyond these vertically integrated solutions from Qualcomm and Apple, ARM’s
    IP licensing model offers a fundamentally different approach that enables SoC
    designers to customize processor combinations based on target applications. The
    Mali-G78 GPU’s 24 cores can be paired with Ethos-N78 NPU for balanced general-purpose
    and AI acceleration, while the Cortex-M55 microcontroller integrates Ethos-U55
    microNPU for ultra-low-power edge applications. This modular flexibility allows
    automotive SoCs to emphasize deterministic real-time processing while smartphone
    SoCs optimize for interactive performance and battery efficiency.
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 除了高通和苹果的垂直整合解决方案之外，ARM的知识产权许可模式提供了一种根本不同的方法，使SoC设计者能够根据目标应用定制处理器组合。Mali-G78
    GPU的24个核心可以与Ethos-N78 NPU配对，以实现平衡的通用和人工智能加速，而Cortex-M55微控制器则集成了Ethos-U55微NPU，用于超低功耗的边缘应用。这种模块化灵活性允许汽车SoC强调确定性实时处理，而智能手机SoC则优化交互性能和电池效率。
- en: Strategies for Dynamic Workload Distribution
  id: totrans-1079
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态工作负载分配策略
- en: With multiple specialized processors available on heterogeneous SoCs, the critical
    challenge becomes intelligently distributing neural network operations across
    these resources to maximize performance while respecting power and latency constraints.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 在异构SoC上可用的多个专用处理器中，关键挑战成为智能地在这些资源之间分配神经网络操作，以最大化性能同时尊重功率和延迟约束。
- en: Modern neural networks require intelligent partitioning across heterogeneous
    processors based on operation characteristics and current system state. Convolutional
    layers with regular data access patterns typically execute efficiently on GPU
    shader cores, while fully connected layers with irregular sparsity patterns may
    perform better on general-purpose CPU cores with large caches. Attention mechanisms
    in transformers benefit from NPU matrix engines when sequences are long, but may
    execute more efficiently on CPU when sequence lengths are small due to the NPU
    setup overhead.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 现代神经网络需要根据操作特性和当前系统状态在异构处理器之间进行智能分区。具有常规数据访问模式的卷积层通常在GPU着色器核心上高效执行，而具有不规则稀疏模式的完全连接层可能在具有大缓存的通用CPU核心上表现更好。在序列较长时，变压器中的注意力机制受益于NPU矩阵引擎，但当序列长度较小时，由于NPU设置开销，它们可能在CPU上执行得更高效。
- en: 'Beyond static operation-to-processor mapping, heterogeneous SoCs implement
    dynamic processor selection based on multiple constraints:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于静态的操作到处理器的映射，异构SoC根据多个约束条件实现动态处理器选择：
- en: '**Power Budget**: During battery operation, the system may route computations
    to lower-power DSP cores rather than high-performance GPU cores'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功率预算**：在电池操作期间，系统可能会将计算路由到低功耗DSP核心，而不是高性能GPU核心'
- en: '**Thermal State**: When approaching thermal limits, workloads shift from power-hungry
    NPU to more efficient CPU execution'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热状态**：当接近热限制时，工作负载会从功耗较高的NPU转移到更高效的CPU执行'
- en: '**Latency Requirements**: Safety-critical automotive applications prioritize
    deterministic CPU execution over potentially faster but variable NPU processing'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟要求**：对安全至关重要的汽车应用优先考虑确定性的CPU执行，而不是可能更快但可变的NPU处理'
- en: '**Concurrent Workload Interference**: Multiple AI applications may require
    load balancing across available processors to maintain Quality of Service'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发工作负载干扰**：多个AI应用可能需要在可用的处理器之间进行负载平衡，以保持服务质量'
- en: Compounding the processor selection challenge, shared memory architectures require
    sophisticated arbitration when multiple processors access LPDDR simultaneously.
    The Snapdragon 8 Gen 3’s memory controller implements priority-based scheduling
    where camera processing receives higher priority than background AI tasks, ensuring
    real-time video processing while background neural networks adapt their execution
    patterns to available memory bandwidth. This arbitration becomes critical during
    memory-intensive operations like large language model inference, where parameter
    streaming from DRAM must be carefully coordinated across processors.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理器选择挑战的基础上，共享内存架构在多个处理器同时访问LPDDR时需要复杂的仲裁。Snapdragon 8 Gen 3的内存控制器实现了基于优先级的调度，其中相机处理获得比后台AI任务更高的优先级，确保实时视频处理，同时后台神经网络调整它们的执行模式以适应可用的内存带宽。这种仲裁在内存密集型操作（如大型语言模型推理）期间变得至关重要，在这些操作中，从DRAM流出的参数必须在处理器之间仔细协调。
- en: Power and Thermal Management
  id: totrans-1088
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功率和热管理
- en: Mobile AI workloads must maintain high performance while operating within strict
    power budgets and thermal envelopes—constraints that require sophisticated coordination
    across heterogeneous processors.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 移动AI工作负载必须在严格的功率预算和热 envelopes 内运行，同时保持高性能——这些约束需要在异构处理器之间进行复杂的协调。
- en: Heterogeneous SoCs implement coordinated DVFS across multiple processors to
    optimize the power-performance envelope. When one processor increases frequency
    to meet latency demands, the system may reduce voltage on other processors to
    maintain total power budget. This coordination becomes complex in AI workloads
    where computational phases may shift rapidly between processors—the system must
    predict upcoming workload transitions to preemptively adjust operating points
    while avoiding voltage/frequency oscillations that degrade efficiency.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 异构SoC在多个处理器之间实现协调的动态电压频率调整（DVFS），以优化功率-性能包络。当一个处理器增加频率以满足延迟需求时，系统可能会降低其他处理器的电压以维持总功率预算。在AI工作负载中，计算阶段可能在处理器之间快速切换，这种协调变得复杂——系统必须预测即将到来的工作负载转换，以便预先调整工作点，同时避免电压/频率振荡，这些振荡会降低效率。
- en: When DVFS alone cannot maintain the power envelope, mobile SoCs implement thermal
    throttling through intelligent task migration rather than simple frequency reduction.
    When the NPU approaches thermal limits during intensive neural network processing,
    the runtime system can migrate layers to the GPU or CPU while maintaining computational
    throughput. This approach preserves performance during thermal events, though
    it requires sophisticated workload characterization to predict execution time
    and power consumption across different processors.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 当仅DVFS无法维持功率包络时，移动SoC通过智能任务迁移而不是简单的频率降低来实现热管理。当NPU在密集的神经网络处理期间接近热极限时，运行时系统可以将层迁移到GPU或CPU，同时保持计算吞吐量。这种方法在热事件期间保持性能，尽管它需要复杂的任务特征化来预测不同处理器的执行时间和功耗。
- en: Beyond real-time power and thermal management, mobile AI systems must also adapt
    their computational strategies based on battery state and charging status. During
    low battery conditions, the system may switch from high-accuracy models to efficient
    approximations, migrate workloads from power-hungry NPU to energy-efficient DSP,
    or reduce inference frequency while maintaining application responsiveness. Conversely,
    during charging, the system can enable higher-performance models and increase
    processing frequency to deliver enhanced user experiences.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 除了实时电源和热管理之外，移动AI系统还必须根据电池状态和充电状态调整其计算策略。在低电量条件下，系统可能从高精度模型切换到高效近似，将工作负载从能耗高的NPU迁移到节能的DSP，或降低推理频率同时保持应用程序响应性。相反，在充电期间，系统可以启用更高性能的模型并增加处理频率，以提供增强的用户体验。
- en: Automotive Heterogeneous AI Systems
  id: totrans-1093
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汽车异构AI系统
- en: Automotive applications introduce unique heterogeneous computing challenges
    that combine mobile-style power efficiency with hard real-time guarantees and
    functional safety requirements—a combination that demands fundamentally different
    architectural approaches.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车应用引入了独特的异构计算挑战，这些挑战结合了移动风格的电源效率、硬实时保证和功能安全要求——这种组合需要根本不同的架构方法。
- en: Automotive SoCs must guarantee deterministic inference latency for safety-critical
    functions while supporting advanced driver assistance systems (ADAS). The Snapdragon
    Ride platform coordinates multiple AI accelerators across safety domains—redundant
    processing elements ensure functional safety compliance while high-performance
    accelerators handle perception, planning, and control algorithms. This architecture
    requires temporal isolation between safety-critical and convenience functions,
    implemented through hardware partitioning and time-triggered scheduling.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车SoC必须保证安全关键功能的确定性推理延迟，同时支持高级驾驶辅助系统（ADAS）。Snapdragon Ride平台协调跨安全域的多个AI加速器——冗余处理元素确保功能安全合规性，而高性能加速器处理感知、规划和控制算法。这种架构需要在安全关键功能和便利功能之间实现时间隔离，通过硬件分区和时间触发的调度来实现。
- en: These safety requirements become even more complex when considering that modern
    vehicles integrate multiple AI-enabled SoCs for different domains—vision processing
    SoCs handle camera-based perception, radar processing SoCs manage RF sensor data,
    while central compute platforms coordinate high-level decision making. These distributed
    systems must maintain temporal coherence across sensor modalities with microsecond-precision
    timing, requiring specialized inter-SoC communication protocols and distributed
    synchronization mechanisms.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑到现代车辆集成多个AI启用SoC以处理不同领域时，这些安全要求变得更加复杂——视觉处理SoC处理基于摄像头的感知，雷达处理SoC管理射频传感器数据，而中央计算平台协调高级决策。这些分布式系统必须在传感器模态之间保持时间一致性，具有微秒级精度的定时，需要专门的跨SoC通信协议和分布式同步机制。
- en: Extending beyond the vehicle’s internal sensors, vehicle-to-everything (V2X)
    communication adds another layer of heterogeneous processing where AI algorithms
    must coordinate local sensor processing with information received from other vehicles
    and infrastructure. This requires ultra-low latency processing chains where 5G
    modems, AI accelerators, and control systems operate within millisecond deadlines
    while maintaining functional safety requirements.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆到一切（V2X）通信超越了车辆内部传感器的范围，它增加了一层异构处理层，其中AI算法必须协调本地传感器处理与从其他车辆和基础设施接收到的信息。这需要超低延迟的处理链，其中5G调制解调器、AI加速器和控制系统在毫秒级截止时间内运行，同时保持功能安全要求。
- en: Software Stack Challenges
  id: totrans-1098
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软件堆栈挑战
- en: The architectural sophistication of heterogeneous SoCs creates substantial software
    development challenges that span programming models, memory management, and runtime
    optimization.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 异构SoC的架构复杂性为软件开发带来了重大挑战，这些挑战跨越编程模型、内存管理和运行时优化。
- en: Programming heterogeneous SoCs requires frameworks that abstract processor differences
    while exposing performance-critical optimization opportunities. OpenCL and Vulkan
    provide cross-processor execution, but achieving optimal performance requires
    processor-specific optimizations that complicate portable development. Modern
    ML frameworks like TensorFlow Lite and PyTorch Mobile implement automatic processor
    selection, but developers still need to understand heterogeneous execution patterns
    to achieve optimal results.
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 编程异构SoC需要框架来抽象处理器差异，同时暴露性能关键优化机会。OpenCL和Vulkan提供跨处理器执行，但要实现最佳性能，需要针对特定处理器的优化，这会复杂化可移植开发。现代ML框架如TensorFlow
    Lite和PyTorch Mobile实现了自动处理器选择，但开发者仍然需要理解异构执行模式以实现最佳结果。
- en: Complicating the programming challenge further, heterogeneous SoCs with shared
    memory architectures require sophisticated memory management that considers processor-specific
    caching behaviors, memory access patterns, and coherency requirements. CPU caches
    may interfere with GPU memory access patterns, while NPU direct memory access
    (DMA) operations must be synchronized with CPU cache operations to maintain data
    consistency.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步复杂化编程挑战，具有共享内存架构的异构SoC需要考虑处理器特定缓存行为、内存访问模式和一致性要求的复杂内存管理。CPU缓存可能会干扰GPU内存访问模式，而NPU直接内存访问（DMA）操作必须与CPU缓存操作同步，以保持数据一致性。
- en: To address the complexity of manual optimization across these dimensions, advanced
    heterogeneous SoCs implement machine learning-based runtime optimization that
    learns from execution patterns to improve processor selection, thermal management,
    and power optimization. These systems collect telemetry on workload characteristics,
    processor utilization, and power consumption to build models that predict optimal
    execution strategies for new workloads.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些维度上手动优化的复杂性，高级异构SoC实现了基于机器学习的运行时优化，它从执行模式中学习以改进处理器选择、热管理和电源优化。这些系统收集关于工作负载特性、处理器利用率和功耗的遥测数据，以构建预测新工作负载最佳执行策略的模型。
- en: This heterogeneous approach to AI acceleration represents the future of computing,
    where no single processor architecture can optimally handle the diverse computational
    patterns in modern AI applications. Understanding these coordination challenges
    is essential for developing efficient mobile AI systems that deliver high performance
    while meeting the strict power, thermal, and real-time constraints of edge deployment
    scenarios.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种异构AI加速方法代表了计算的未来，其中没有单个处理器架构可以最优地处理现代AI应用中的各种计算模式。理解这些协调挑战对于开发高效移动AI系统至关重要，这些系统能够提供高性能，同时满足边缘部署场景严格的电源、热和实时约束。
- en: However, the complexity of these heterogeneous systems creates numerous opportunities
    for misconception and suboptimal design decisions. The following fallacies and
    pitfalls highlight common misunderstandings that can undermine acceleration strategies.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些异构系统的复杂性为误解和次优设计决策创造了众多机会。以下谬误和陷阱突出了可能破坏加速策略的常见误解。
- en: Fallacies and Pitfalls
  id: totrans-1105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Hardware acceleration involves complex interactions between specialized architectures,
    software stacks, and workload characteristics that create significant opportunities
    for misunderstanding optimal deployment strategies. The impressive performance
    numbers often associated with AI accelerators can mask important constraints and
    trade-offs that determine real-world effectiveness across different deployment
    scenarios.
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加速涉及专用架构、软件堆栈和工作负载特性之间的复杂交互，这为误解最佳部署策略创造了重大机会。与AI加速器相关联的令人印象深刻的性能数字往往掩盖了决定不同部署场景中实际效果的重要约束和权衡。
- en: '**Fallacy:** *More specialized hardware always provides better performance
    than general-purpose alternatives.*'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *更专业的硬件总是比通用替代品提供更好的性能。*'
- en: This belief assumes that specialized accelerators automatically outperform general-purpose
    processors for all AI workloads. Specialized hardware achieves peak performance
    only when workloads match the architectural assumptions and optimization targets.
    Models with irregular memory access patterns, small batch sizes, or dynamic computation
    graphs may perform better on flexible general-purpose processors than on specialized
    accelerators designed for dense, regular computations. The overhead of data movement,
    format conversion, and synchronization can eliminate the benefits of specialized
    computation. Effective hardware selection requires matching workload characteristics
    to architectural strengths rather than assuming specialization always wins.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设专用加速器自动优于通用处理器处理所有AI工作负载。专用硬件仅在工作负载与架构假设和优化目标匹配时才能达到峰值性能。具有不规则内存访问模式、小批量大小或动态计算图的模型，在灵活的通用处理器上可能比为密集、规则计算设计的专用加速器表现更好。数据移动、格式转换和同步的开销可能会消除专用计算的好处。有效的硬件选择需要将工作负载特征与架构优势相匹配，而不是假设专业化总是获胜。
- en: '**Pitfall:** *Ignoring memory bandwidth limitations when selecting acceleration
    strategies.*'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *在选择加速策略时忽略内存带宽限制。*'
- en: Many practitioners focus on computational throughput metrics without considering
    memory bandwidth constraints that often limit real-world performance. AI accelerators
    with impressive computational capabilities can be severely bottlenecked by insufficient
    memory bandwidth, leading to poor hardware utilization. The ratio between computation
    intensity and memory access requirements determines whether an accelerator can
    achieve its theoretical performance. This oversight leads to expensive hardware
    deployments that fail to deliver expected performance improvements because the
    workload is memory-bound rather than compute-bound.
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者关注计算吞吐量指标，而没有考虑到内存带宽约束，这些约束通常限制了实际性能。具有令人印象深刻的计算能力的AI加速器可能会因为内存带宽不足而严重瓶颈，导致硬件利用率低下。计算强度与内存访问需求之间的比率决定了加速器能否达到其理论性能。这种疏忽导致昂贵的硬件部署未能实现预期的性能提升，因为工作负载是内存受限而不是计算受限。
- en: '**Fallacy:** *Hardware acceleration benefits scale linearly with additional
    accelerators.*'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *硬件加速的好处与额外加速器的数量成线性增长。*'
- en: This misconception drives teams to expect proportional performance gains when
    adding more accelerators to their systems. Multi-accelerator setups introduce
    communication overhead, synchronization costs, and load balancing challenges that
    can severely limit scaling efficiency. Small models may not provide enough parallel
    work to utilize multiple accelerators effectively, while large models may be limited
    by communication bandwidth between devices. Distributed training and inference
    face additional challenges from gradient aggregation, model partitioning, and
    coordination overhead that create non-linear scaling relationships.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解驱使团队期望在系统中添加更多加速器时能够获得成比例的性能提升。多加速器配置引入了通信开销、同步成本和负载平衡挑战，这些都可能严重限制扩展效率。小型模型可能无法提供足够的并行工作来有效利用多个加速器，而大型模型可能受到设备间通信带宽的限制。分布式训练和推理面临梯度聚合、模型分区和协调开销等额外挑战，这些挑战创造了非线性扩展关系。
- en: '**Pitfall:** *Vendor-specific optimizations without considering long-term portability
    and flexibility.*'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *不考虑长期可移植性和灵活性而进行的供应商特定优化。*'
- en: Organizations often optimize exclusively for specific hardware vendors to achieve
    maximum performance without considering the implications for system flexibility
    and future migration. Deep integration with vendor-specific libraries, custom
    kernels, and proprietary optimization tools creates lock-in that complicates hardware
    upgrades, vendor changes, or multi-vendor deployments. While vendor-specific optimizations
    can provide significant performance benefits, they should be balanced against
    the need for system portability and the ability to adapt to evolving hardware
    landscapes. Maintaining some level of hardware abstraction preserves strategic
    flexibility while still capturing most performance benefits.
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: 组织通常仅针对特定的硬件供应商进行优化，以实现最大性能，而不考虑系统灵活性和未来迁移的影响。与特定供应商库、定制内核和专有优化工具的深度集成，创造了锁定效应，使得硬件升级、供应商更换或多供应商部署变得复杂。虽然特定供应商的优化可以提供显著的性能优势，但它们应该与系统可移植性和适应不断变化的硬件景观的能力相平衡。保持一定程度的硬件抽象可以保持战略灵活性，同时仍然捕捉到大多数性能优势。
- en: Summary
  id: totrans-1115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Hardware acceleration has emerged as the critical enabler that transforms machine
    learning from academic curiosity to practical reality, fundamentally reshaping
    how we design both computational systems and the algorithms that run on them.
    The evolution from general-purpose processors to specialized AI accelerators represents
    more than just incremental improvement—it reflects a paradigm shift toward domain-specific
    computing where hardware and software are co-designed to optimize specific computational
    patterns. The journey from CPUs through GPUs to specialized TPUs, NPUs, and wafer-scale
    systems demonstrates how understanding workload characteristics drives architectural
    innovation, creating opportunities for orders-of-magnitude performance improvements
    through targeted specialization.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加速已成为将机器学习从学术好奇心转变为实际现实的关键推动者，从根本上改变了我们设计计算系统和在其上运行的算法的方式。从通用处理器到专用人工智能加速器的演变，不仅仅是渐进式的改进——它反映了一种范式转变，即向特定领域计算转变，在这种计算中，硬件和软件是协同设计的，以优化特定的计算模式。从CPU到GPU，再到专门的TPU、NPU和晶圆级系统的发展历程，展示了理解工作负载特征如何推动架构创新，通过针对性的专业化，创造了数量级性能提升的机会。
- en: The technical challenges of AI acceleration span multiple layers of the computing
    stack, from low-level memory hierarchy optimization to high-level compiler transformations
    and runtime orchestration. Memory bandwidth limitations create fundamental bottlenecks
    that require sophisticated techniques like data tiling, kernel fusion, and hierarchy-aware
    scheduling to overcome. Mapping neural network computations to hardware involves
    complex trade-offs between different dataflow patterns, memory allocation strategies,
    and execution scheduling approaches that must balance computational efficiency
    with resource utilization.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能加速的技术挑战跨越了计算堆栈的多个层次，从低级内存层次结构优化到高级编译器转换和运行时编排。内存带宽限制创造了基本瓶颈，需要像数据分块、内核融合和层次感知调度这样的复杂技术来克服。将神经网络计算映射到硬件涉及在不同数据流模式、内存分配策略和执行调度方法之间进行复杂的权衡，这些方法必须在计算效率与资源利用之间取得平衡。
- en: Building on these foundational concepts, the emergence of multi-chip and distributed
    acceleration systems introduces additional complexities around communication overhead,
    memory coherence, and workload partitioning that require careful system-level
    optimization.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基础概念的基础上，多芯片和分布式加速系统的出现引入了通信开销、内存一致性和工作负载分区等方面的额外复杂性，这些都需要仔细的系统级优化。
- en: '**Key Takeaways**'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Specialized AI accelerators achieve performance gains through domain-specific
    architectures optimized for tensor operations and dataflow patterns
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用人工智能加速器通过针对张量操作和数据流模式优化的特定领域架构实现性能提升。
- en: Memory hierarchy management is often the primary bottleneck in AI acceleration,
    requiring sophisticated data movement optimization strategies
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存层次结构管理通常是人工智能加速的主要瓶颈，需要复杂的数据移动优化策略。
- en: Hardware-software co-design enables order-of-magnitude improvements by aligning
    algorithm characteristics with architectural capabilities
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件-软件协同设计通过将算法特征与架构能力对齐，实现了数量级的改进。
- en: Multi-chip scaling introduces distributed computing challenges that require
    new approaches to communication, synchronization, and resource management
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多芯片扩展引入了分布式计算挑战，需要新的通信、同步和资源管理方法。
- en: The principles of hardware acceleration established here provide the foundation
    for understanding how benchmarking methodologies evaluate accelerator performance
    and how deployment strategies must account for hardware constraints and capabilities.
    As AI models continue growing in complexity and computational requirements, the
    ability to effectively leverage specialized hardware becomes increasingly critical
    for practical system deployment, influencing everything from energy efficiency
    and cost optimization to the feasibility of real-time inference and large-scale
    training across diverse application domains.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里确立的硬件加速原则为理解基准测试方法如何评估加速器性能以及部署策略如何考虑硬件约束和能力提供了基础。随着人工智能模型在复杂性和计算需求上的持续增长，有效地利用专用硬件的能力对于实际系统部署变得越来越关键，它影响着从能效和成本优化到跨不同应用领域实时推理和大规模训练的可行性等各个方面。
- en: '* * *'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
