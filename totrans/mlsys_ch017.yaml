- en: AI Acceleration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*DALL·E 3 Prompt: Create an intricate and colorful representation of a System
    on Chip (SoC) design in a rectangular format. Showcase a variety of specialized
    machine learning accelerators and chiplets, all integrated into the processor.
    Provide a detailed view inside the chip, highlighting the rapid movement of electrons.
    Each accelerator and chiplet should be designed to interact with neural network
    neurons, layers, and activations, emphasizing their processing speed. Depict the
    neural networks as a network of interconnected nodes, with vibrant data streams
    flowing between the accelerator pieces, showcasing the enhanced computation speed.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file179.png)'
  prefs: []
  type: TYPE_IMG
- en: Purpose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*What makes specialized hardware acceleration not just beneficial but essential
    for practical machine learning deployment, and why does this represent a fundamental
    shift in how we approach computational system design?*'
  prefs: []
  type: TYPE_NORMAL
- en: Practical machine learning systems depend entirely on hardware acceleration.
    Without specialized processors, computational demands remain economically and
    physically infeasible. General-purpose CPUs achieve only 100 GFLOPS[1](#fn1) for
    neural network operations ([Sze et al. 2017a](ch058.xhtml#ref-sze2017efficient)),
    while modern training workloads require trillions of operations per second, creating
    a performance gap that traditional scaling cannot bridge. Hardware acceleration
    transforms computationally impossible tasks into practical deployments, enabling
    entirely new application categories. Engineers working with modern AI systems
    must understand acceleration principles to harness 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    that make real-time inference, large-scale training, and edge deployment economically
    viable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Objectives**'
  prefs: []
  type: TYPE_NORMAL
- en: Trace the evolution of hardware acceleration from floating-point coprocessors
    to modern AI accelerators and explain the architectural principles driving this
    progression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classify AI compute primitives (vector operations, matrix multiplication, systolic
    arrays) and analyze their implementation in contemporary accelerators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate memory hierarchy designs for AI accelerators and predict their impact
    on performance bottlenecks using bandwidth and energy consumption metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design mapping strategies for neural network layers onto specialized hardware
    architectures, considering dataflow patterns and resource utilization trade-offs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply compiler optimization techniques (graph optimization, kernel fusion, memory
    planning) to transform high-level ML models into efficient hardware execution
    plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare multi-chip scaling approaches (chiplets, multi-GPU, distributed systems)
    and assess their suitability for different AI workload characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critique common misconceptions about hardware acceleration and identify potential
    pitfalls in accelerator selection and deployment strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI Hardware Acceleration Fundamentals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern machine learning systems challenge the architectural assumptions underlying
    general-purpose processors. While software optimization techniques examined in
    the preceding chapter provide systematic approaches to algorithmic efficiency
    through precision reduction, structural pruning, and execution refinements, they
    operate within the constraints of existing computational substrates. Conventional
    CPUs achieve utilization rates of merely 5-10% when executing typical machine
    learning workloads ([Gholami et al. 2024](ch058.xhtml#ref-gholami2024ai)), due
    to architectural misalignments between sequential processing models and the highly
    parallel, data-intensive nature of neural network computations.
  prefs: []
  type: TYPE_NORMAL
- en: This performance gap has driven a shift toward domain-specific hardware acceleration
    within computer architecture. Hardware acceleration complements software optimization,
    addressing efficiency limitations through architectural redesign rather than algorithmic
    modification. The co-evolution of machine learning algorithms and specialized
    computing architectures has enabled the transition from computationally prohibitive
    research conducted on high-performance computing systems to ubiquitous deployment
    across diverse computing environments, from hyperscale data centers to resource-constrained
    edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware acceleration for machine learning systems sits at the intersection
    of computer systems engineering, computer architecture, and applied machine learning.
    For practitioners developing production systems, architectural selection decisions
    regarding accelerator technologies encompassing graphics processing units, tensor
    processing units, and neuromorphic processors directly determine system-level
    performance characteristics, energy efficiency profiles, and implementation complexity.
    Deployed systems in domains such as natural language processing, computer vision,
    and autonomous systems demonstrate performance improvements spanning two to three
    orders of magnitude relative to general-purpose implementations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter examines hardware acceleration principles and methodologies for
    machine learning systems. The analysis begins with the historical evolution of
    domain-specific computing architectures, showing how design patterns from floating-point
    coprocessors to graphics processing units inform contemporary AI acceleration
    strategies. We then address the computational primitives that characterize machine
    learning workloads, including matrix multiplication, vector operations, and nonlinear
    activation functions, and analyze the architectural mechanisms through which specialized
    hardware optimizes these operations via innovations such as systolic array architectures
    and tensor processing cores.
  prefs: []
  type: TYPE_NORMAL
- en: Memory hierarchy design plays a critical role in acceleration effectiveness,
    given that data movement energy costs typically exceed computational energy by
    more than two orders of magnitude. This analysis covers memory architecture design
    principles, from on-chip SRAM buffer optimization to high-bandwidth memory interfaces,
    and examines approaches to minimizing energy-intensive data movement patterns.
    We also address compiler optimization and runtime system support, which determine
    the extent to which theoretical hardware capabilities translate into measurable
    system performance.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter concludes with scaling methodologies for systems requiring computational
    capacity beyond single-chip implementations. Multi-chip architectures, ranging
    from chiplet-based integration to distributed warehouse-scale systems, introduce
    trade-offs between computational parallelism and inter-chip communication overhead.
    Through detailed analysis of contemporary systems including NVIDIA GPU architectures,
    Google Tensor Processing Units, and emerging neuromorphic computing platforms,
    we establish the theoretical foundations and practical considerations necessary
    for effective deployment of AI acceleration across diverse system contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of Hardware Specialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Computing architectures follow a recurring pattern: as computational workloads
    grow in complexity, general-purpose processors become increasingly inefficient,
    prompting the development of specialized hardware accelerators. The need for higher
    computational efficiency, reduced energy consumption, and optimized execution
    of domain-specific workloads drives this transition. Machine learning acceleration
    represents the latest stage in this ongoing evolution, following a trajectory
    observed in prior domains such as floating-point arithmetic, graphics processing,
    and digital signal processing.'
  prefs: []
  type: TYPE_NORMAL
- en: This evolutionary progression provides context for understanding how modern
    ML accelerators including GPUs with tensor cores (specialized units that accelerate
    matrix operations), Google’s TPUs[2](#fn2), and Apple’s Neural Engine emerged
    from established architectural principles. These technologies enable widely deployed
    applications such as real-time language translation, image recognition, and personalized
    recommendations. The architectural strategies enabling such capabilities derive
    from decades of hardware specialization research and development.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware specialization forms the foundation of this transition, enhancing performance
    and efficiency by optimizing frequently executed computational patterns through
    dedicated circuit implementations. While this approach yields significant gains,
    it introduces trade-offs in flexibility, silicon area utilization, and programming
    complexity. As computing demands continue to evolve, specialized accelerators
    must balance these factors to deliver sustained improvements in efficiency and
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of hardware specialization provides perspective for understanding
    modern machine learning accelerators. Many principles that shaped the development
    of early floating-point and graphics accelerators now inform the design of AI-specific
    hardware. Examining these past trends offers a framework for analyzing contemporary
    approaches to AI acceleration and anticipating future developments in specialized
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized Computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transition toward specialized computing architectures stems from the limitations
    of general-purpose processors. Early computing systems relied on central processing
    units (CPUs) to execute all computational tasks sequentially, following a one-size-fits-all
    approach. As computing workloads diversified and grew in complexity, certain operations,
    especially floating-point arithmetic, emerged as performance bottlenecks that
    could not be efficiently handled by CPUs alone. These inefficiencies prompted
    the development of specialized hardware architectures designed to accelerate specific
    computational patterns ([Flynn 1966](ch058.xhtml#ref-flynn1966very)).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the earliest examples of hardware specialization was the Intel 8087
    mathematics coprocessor[3](#fn3), introduced in 1980\. This floating-point unit
    (FPU) was designed to offload arithmetic-intensive computations from the main
    CPU, dramatically improving performance for scientific and engineering applications.
    The 8087 demonstrated unprecedented efficiency, achieving performance gains of
    up to 100× for floating-point operations compared to software-based implementations
    on general-purpose processors ([Fisher 1981](ch058.xhtml#ref-fisher_8087_1981)).
    This milestone established a principle in computer architecture: carefully designed
    hardware specialization could provide order-of-magnitude improvements for well-defined,
    computationally intensive tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The success of floating-point coprocessors[4](#fn4) led to their eventual integration
    into mainstream processors. The Intel 486DX, released in 1989, incorporated an
    on-chip floating-point unit, eliminating the requirement for an external coprocessor.
    This integration improved processing efficiency and established a recurring pattern
    in computer architecture: successful specialized functions become standard features
    in subsequent generations of general-purpose processors ([David A. Patterson and
    Hennessy 2021c](ch058.xhtml#ref-patterson2021computer)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Early floating-point acceleration established principles that continue to influence
    modern hardware specialization:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of computational bottlenecks through workload analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of specialized circuits for frequent operations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creation of efficient hardware-software interfaces
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Progressive integration of proven specialized functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This progression from domain-specific specialization to general-purpose integration
    has shaped modern computing architectures. As computational workloads expanded
    beyond arithmetic operations, these core principles were applied to new domains,
    such as graphics processing, digital signal processing, and ultimately, machine
    learning acceleration. Each domain introduced specialized architectures tailored
    to their unique computational requirements, establishing hardware specialization
    as an approach for advancing computing performance and efficiency in increasingly
    complex workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of specialized computing hardware follows a consistent trajectory,
    wherein architectural innovations are introduced to address emerging computational
    bottlenecks and are subsequently incorporated into mainstream computing platforms.
    As illustrated in [Figure 11.1](ch017.xhtml#fig-timeline), each computing era
    produced accelerators that addressed the dominant workload characteristics of
    the period. These developments have advanced architectural efficiency and shaped
    the foundation upon which contemporary machine learning systems operate. The computational
    capabilities required for tasks such as real-time language translation, personalized
    recommendations, and on-device inference depend on foundational principles and
    architectural innovations established in earlier domains, including floating-point
    computation, graphics processing, and digital signal processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file180.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: **Hardware Specialization Trajectory**: Computing architectures
    progressively incorporate specialized accelerators to address emerging performance
    bottlenecks and workload demands, mirroring a historical pattern from floating-point
    units to graphics processors and, ultimately, machine learning accelerators. This
    evolution reflects a strategy for improving computational efficiency by tailoring
    hardware to specific task characteristics and advancing increasingly complex applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Computing and Graphics Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principles established through floating-point acceleration provided a blueprint
    for addressing emerging computational challenges. As computing applications diversified,
    new computational patterns emerged that exceeded the capabilities of general-purpose
    processors. This expansion of specialized computing manifested across multiple
    domains, each contributing unique insights to hardware acceleration strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics processing emerged as a primary driver of hardware specialization in
    the 1990s. Early graphics accelerators focused on specific operations like bitmap
    transfers and polygon filling. The introduction of programmable graphics pipelines
    with NVIDIA’s GeForce 256 in 1999 represented a significant advancement in specialized
    computing. Graphics Processing Units (GPUs) demonstrated how parallel processing
    architectures could efficiently handle data-parallel workloads, achieving 50-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> speedups in 3D rendering
    tasks like texture mapping and vertex transformation. By 2004, high-end GPUs could
    process over 100 million polygons per second ([Owens et al. 2008](ch058.xhtml#ref-owens2008gpu)).
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently, Digital Signal Processing (DSP) processors established parallel
    data path architectures with specialized multiply-accumulate units and circular
    buffers optimized for filtering and transform operations. Texas Instruments’ TMS32010
    (1983) demonstrated how domain-specific instruction sets could dramatically improve
    performance for signal processing applications ([Lyons 2011](ch058.xhtml#ref-lyons2011understanding)).
  prefs: []
  type: TYPE_NORMAL
- en: Network processing introduced additional patterns of specialization. Network
    processors developed unique architectures to handle packet processing at line
    rate, incorporating multiple processing cores, specialized packet manipulation
    units, and sophisticated memory management systems. Intel’s IXP2800 network processor
    demonstrated how multiple levels of hardware specialization could be combined
    to address complex processing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'These diverse domains of specialization exhibit several common characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of domain-specific computational patterns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of specialized processing elements and memory hierarchies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creation of domain-specific programming models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Progressive evolution toward more flexible architectures
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This period of expanding specialization demonstrated that hardware acceleration
    strategies could address diverse computational requirements across multiple domains.
    The GPU’s success in parallelizing 3D graphics pipelines enabled its subsequent
    adoption for training deep neural networks, exemplified by AlexNet[5](#fn5) in
    2012, which executed on consumer-grade NVIDIA GPUs. DSP innovations in low-power
    signal processing facilitated real-time inference on edge devices, including voice
    assistants and wearables. These domains informed ML hardware designs and established
    that accelerators could be deployed across both cloud and embedded contexts, principles
    that continue to influence contemporary AI ecosystem development.
  prefs: []
  type: TYPE_NORMAL
- en: Emergence of Domain-Specific Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The emergence of domain-specific architectures (DSA)[6](#fn6) marks a shift
    in computer system design, driven by two factors: the breakdown of traditional
    scaling laws and the increasing computational demands of specialized workloads.
    The slowdown of Moore’s Law[7](#fn7), which previously ensured predictable enhancements
    in transistor density every 18 to 24 months, and the end of Dennard scaling[8](#fn8),
    which permitted frequency increases without corresponding power increases, created
    a performance and efficiency bottleneck in general-purpose computing. As John
    Hennessy and David Patterson noted in their 2017 Turing Lecture ([Hennessy and
    Patterson 2019](ch058.xhtml#ref-HennessyPatterson2017Turing)), these limitations
    signaled the onset of a new era in computer architecture, one centered on domain-specific
    solutions that optimize hardware for specialized workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, improvements in processor performance depended on semiconductor
    process scaling and increasing clock speeds. However, as power density limitations
    restricted further frequency scaling, and as transistor miniaturization encountered
    increasing physical and economic constraints, architects explored alternative
    approaches to sustain computational growth. This resulted in a shift toward domain-specific
    architectures, which dedicate silicon resources to optimize computation for specific
    application domains, trading flexibility for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Domain-specific architectures achieve superior performance and energy efficiency
    through several key principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customized data paths**: Design processing paths specifically optimized for
    target application patterns, enabling direct hardware execution of common operations.
    For example, matrix multiplication units in AI accelerators implement systolic
    arrays—grid-like networks of processing elements that rhythmically compute and
    pass data through neighboring units—tailored for neural network computations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Specialized memory hierarchies**: Optimize memory systems around domain-specific
    access patterns and data reuse characteristics. This includes custom cache configurations,
    prefetching logic, and memory controllers tuned for expected workloads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reduced instruction overhead**: Implement domain-specific instruction sets
    that minimize decode and dispatch complexity by encoding common operation sequences
    into single instructions. This improves both performance and energy efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Direct hardware implementation**: Create dedicated circuit blocks that natively
    execute frequently used operations without software intervention. This eliminates
    instruction processing overhead and maximizes throughput.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These principles achieve compelling demonstration in modern smartphones. Modern
    smartphones can decode 4K video at 60 frames per second while consuming only a
    few watts of power, despite video processing requiring billions of operations
    per second. This efficiency is achieved through dedicated hardware video codecs
    that implement industry standards such as H.264/AVC (introduced in 2003) and H.265/HEVC
    (finalized in 2013) ([Sullivan et al. 2012](ch058.xhtml#ref-sullivan2012overview)).
    These specialized circuits provide 100–1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    improvements in both performance and power efficiency compared to software-based
    decoding on general-purpose processors.
  prefs: []
  type: TYPE_NORMAL
- en: The trend toward specialization continues to accelerate, with new architectures
    emerging for an expanding range of domains. Genomics processing benefits from
    custom accelerators that optimize sequence alignment and variant calling, reducing
    the time required for DNA analysis ([Shang, Wang, and Liu 2018](ch058.xhtml#ref-Shang2018GenomicsAccel)).
    Similarly, blockchain computation has produced application-specific integrated
    circuits (ASICs)[9](#fn9) optimized for cryptographic hashing, substantially increasing
    the efficiency of mining operations ([Bedford Taylor 2017](ch058.xhtml#ref-Taylor2017ASICMining)).
    These examples demonstrate that domain-specific architecture represents a fundamental
    transformation in computing systems, offering tailored solutions that address
    the growing complexity and diversity of modern computational workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Hardware Specialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Machine learning constitutes a computational domain with unique characteristics
    that have driven the development of specialized hardware architectures. Unlike
    traditional computing workloads that exhibit irregular memory access patterns
    and diverse instruction streams, neural networks are characterized by predictable
    patterns: dense matrix multiplications, regular data flow, and tolerance for reduced
    precision. These characteristics enable specialized hardware optimizations that
    would be ineffective for general-purpose computing but provide substantial speedups
    for ML workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Machine Learning Accelerators*** are specialized computing hardware optimized
    for the *computational patterns* of neural networks, achieving superior *performance
    per watt* through *parallel processing*, *specialized memory hierarchies*, and
    *reduced-precision arithmetic*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning computational requirements reveal limitations in traditional
    processors. CPUs achieve only 5-10% utilization on neural network workloads, delivering
    approximately 100 GFLOPS[10](#fn10) while consuming hundreds of watts. This inefficiency
    results from architectural mismatches: CPUs optimize for single-thread performance
    and irregular memory access, while neural networks require massive parallelism
    and predictable data streams. The memory bandwidth[11](#fn11) constraint becomes
    particularly severe: a single neural network layer may require accessing gigabytes
    of parameters, overwhelming CPU cache hierarchies[12](#fn12) designed for kilobyte-scale
    working sets.'
  prefs: []
  type: TYPE_NORMAL
- en: The energy economics of data movement influence accelerator design. Accessing
    data from DRAM requires approximately 640 picojoules while performing a multiply-accumulate
    operation consumes only 3.7 pJ, approximately a 173× penalty (specific values
    vary by technology node and design) that establishes minimizing data movement
    as the primary optimization target. This disparity explains the progression from
    repurposed graphics processors to purpose-built neural network accelerators. GPUs
    achieve 15,000+ GFLOPS through massive parallelism but encounter efficiency challenges
    from their graphics heritage. TPUs and other custom accelerators achieve utilization
    above 85% by implementing systolic arrays and other architectures that maximize
    data reuse while minimizing movement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training and inference present distinct computational profiles that influence
    accelerator design. Training requires high-precision arithmetic (FP32 or FP16)
    for gradient computation and weight updates, bidirectional data flow for backpropagation[13](#fn13),
    and large memory capacity for storing activations. Inference can exploit reduced
    precision (INT8 or INT4), requires only forward computation, and prioritizes latency
    over throughput[14](#fn14). These differences drive specialized architectures:
    training accelerators maximize FLOPS and memory bandwidth, while inference accelerators
    optimize for energy efficiency and deterministic latency.'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment context shapes architectural choices. Datacenter accelerators accept
    700-watt power budgets to maximize throughput for training massive models. Edge
    devices must deliver real-time inference within milliwatt constraints, driving
    architectures that eliminate every unnecessary data movement. Mobile processors
    balance performance with battery life, while automotive systems prioritize deterministic
    response times for safety-critical applications. This diversity has produced a
    rich ecosystem of specialized accelerators, each optimized for specific deployment
    scenarios and computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In data centers, training accelerators such as NVIDIA H100 and Google TPUv4
    reduce model development from weeks to days through massive parallelism and high-bandwidth
    memory systems. These systems prioritize raw computational throughput, accepting
    700-watt power consumption to achieve petaflop-scale performance. The economics
    support this trade-off—reducing training time from months to days can reduce millions
    in operational costs and accelerate time-to-market for AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: At the opposite extreme, edge deployment requires different optimization strategies.
    Processing-in-memory architectures eliminate data movement by integrating compute
    directly with memory. Dynamic voltage scaling reduces power by 50-90% during low-intensity
    operations. Neuromorphic designs process only changing inputs, achieving 1000×
    power reduction for temporal workloads. These techniques enable sophisticated
    AI models to operate continuously on battery power, supporting applications from
    smartphone photography to autonomous sensors that function for years without external
    power.
  prefs: []
  type: TYPE_NORMAL
- en: The success of application-specific accelerators demonstrates that no single
    architecture can efficiently address all ML workloads. The 156 billion edge devices
    projected by 2030 will require architectures optimized for energy efficiency and
    real-time guarantees, while cloud-scale training will continue advancing the boundaries
    of computational throughput. This diversity drives continued innovation in specialized
    architectures, each optimized for its specific deployment context and computational
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of specialized hardware architectures illustrates a principle
    in computing systems: as computational patterns emerge and mature, hardware specialization
    follows to achieve optimal performance and energy efficiency. This progression
    appears clearly in machine learning acceleration, where domain-specific architectures
    have evolved to meet the increasing computational demands of machine learning
    models. Unlike general-purpose processors, which prioritize flexibility, specialized
    accelerators optimize execution for well-defined workloads, balancing performance,
    energy efficiency, and integration with software frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.1](ch017.xhtml#tbl-hw-evolution) summarizes key milestones in the
    evolution of hardware specialization, showing how each era produced architectures
    tailored to the prevailing computational demands. While these accelerators initially
    emerged to optimize domain-specific workloads, including floating-point operations,
    graphics rendering, and media processing, they also introduced architectural strategies
    that persist in contemporary systems. The specialization principles outlined in
    earlier generations now underpin the design of modern AI accelerators. Understanding
    this historical trajectory provides context for analyzing how hardware specialization
    continues to enable scalable, efficient execution of machine learning workloads
    across diverse deployment environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.1: **Hardware Specialization Trends**: Successive computing eras progressively
    integrate specialized hardware to accelerate prevalent workloads, moving from
    general-purpose CPUs to domain-specific architectures and ultimately to customizable
    AI accelerators. This evolution reflects a fundamental principle: tailoring hardware
    to computational patterns improves performance and energy efficiency, driving
    innovation in machine learning systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Era** | **Computational Pattern** | **Architecture Examples** | **Characteristics**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1980s** | Floating-Point & Signal Processing | FPU, DSP |'
  prefs: []
  type: TYPE_TB
- en: Single-purpose engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focused instruction sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coprocessor interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **1990s** | 3D Graphics & Multimedia | GPU, SIMD Units |'
  prefs: []
  type: TYPE_TB
- en: Many identical compute units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular data patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide memory interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **2000s** | Real-time Media Coding | Media Codecs, Network Processors |'
  prefs: []
  type: TYPE_TB
- en: Fixed-function pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High throughput processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power-performance optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **2010s** | Deep Learning Tensor Operations | TPU, GPU Tensor Cores |'
  prefs: []
  type: TYPE_TB
- en: Matrix multiplication units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massive parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory bandwidth optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **2020s** | Application-Specific Acceleration | ML Engines, Smart NICs, Domain
    Accelerators |'
  prefs: []
  type: TYPE_TB
- en: Workload-specific datapaths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customized memory hierarchies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application-optimized designs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'This historical progression reveals a recurring pattern: each wave of hardware
    specialization responded to a computational bottleneck, whether graphics rendering,
    media encoding, or neural network inference. What distinguishes the 2020s is not
    just specialization, but its pervasiveness: AI accelerators now underpin everything
    from product recommendations on YouTube to object detection in autonomous vehicles.
    Unlike earlier accelerators, today’s AI hardware must integrate tightly with dynamic
    software frameworks and scale across cloud-to-edge deployments. The table illustrates
    not just the past but also the trajectory toward increasingly tailored, high-impact
    computing platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: For AI acceleration, this transition has introduced challenges that extend well
    beyond hardware design. Machine learning accelerators must integrate seamlessly
    into ML workflows by aligning with optimizations at multiple levels of the computing
    stack. They must operate effectively with widely adopted frameworks such as TensorFlow,
    PyTorch, and JAX, ensuring that deployment is smooth and consistent across varied
    hardware platforms. Compiler and runtime support become necessary; advanced optimization
    techniques, such as graph-level transformations, kernel fusion, and memory scheduling,
    are critical for using the full potential of these specialized accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability drives additional complexity as AI accelerators deploy across diverse
    environments from high-throughput data centers to resource-constrained edge and
    mobile devices, requiring tailored performance tuning and energy efficiency strategies.
    Integration into heterogeneous computing[15](#fn15) environments demands interoperability
    that enables specialized units to coordinate effectively with conventional CPUs
    and GPUs in distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: AI accelerators represent a system-level transformation that requires tight
    hardware-software coupling. This transformation manifests in three specific computational
    patterns, compute primitives, that drive accelerator design decisions. Understanding
    these primitives determines the architectural features that enable 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    through coordinated hardware specialization and software optimization strategies
    examined in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution from floating-point coprocessors to AI accelerators reveals a
    consistent pattern: computational bottlenecks drive specialized hardware development.
    Where the Intel 8087 addressed floating-point operations that consumed 80% of
    scientific computing time, modern AI workloads present an even more extreme case.
    Matrix multiplications and convolutions constitute over 95% of neural network
    computation. This concentration of computational demand creates unprecedented
    opportunities for specialization, explaining why AI accelerators achieve 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> performance improvements
    over general-purpose processors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The specialization principles established through decades of hardware evolution
    identifying dominant operations, creating dedicated datapaths, and optimizing
    memory access patterns now guide AI accelerator design. However, neural networks
    introduce unique characteristics that demand new architectural approaches: massive
    parallelism in matrix operations, predictable data access patterns enabling prefetching,
    and tolerance for reduced precision that allows aggressive optimization. Understanding
    these computational patterns, which we term AI compute primitives, helps comprehend
    how modern accelerators transform the theoretical efficiency gains from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    into practical performance improvements. These hardware-software optimizations
    become critical in deployment scenarios ranging from [Chapter 2](ch008.xhtml#sec-ml-systems)
    edge devices to cloud-scale inference systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Before examining these computational primitives in detail, we need to understand
    the architectural organization that enables their efficient execution. Modern
    AI accelerators achieve their dramatic performance improvements through a carefully
    orchestrated hierarchy of specialized components operating in concert. The architecture
    comprises three subsystems, each addressing distinct aspects of the computational
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing substrate consists of an array of processing elements, each
    containing dedicated computational units optimized for specific operations: tensor
    cores execute matrix multiplication, vector units perform element-wise operations,
    and special function units compute activation functions. These processing elements
    are organized in a grid topology that enables massive parallelism, with dozens
    to hundreds of units operating simultaneously on different portions of the computation,
    exploiting the data-level parallelism inherent in neural network workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory hierarchy forms an equally critical architectural component. High-bandwidth
    memory provides the aggregate throughput required to sustain these numerous processing
    elements, while a multi-level cache hierarchy from shared L2 caches down to per-element
    L1 caches and scratchpads minimizes the energy cost of data movement. This hierarchical
    organization embodies a design principle: in AI accelerators, data movement typically
    consumes more energy than computation itself, necessitating architectural strategies
    that prioritize data reuse by maintaining frequently accessed values, including
    weights and partial results, in proximity to compute units.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The host interface establishes connectivity between the specialized accelerator
    and the broader computing system, enabling coordination between general-purpose
    CPUs that manage program control flow and the accelerator that executes computationally
    intensive neural network operations. This architectural partitioning reflects
    specialization at the system level: CPUs address control flow, conditional logic,
    and system coordination, while accelerators focus on the regular, massively parallel
    arithmetic operations that dominate neural network execution.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11.2](ch017.xhtml#fig-accelerator-anatomy) illustrates this architectural
    organization, showing how specialized compute units, hierarchical memory subsystems,
    and host connectivity integrate to form a system optimized for AI workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file181.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: **Anatomy of a Modern AI Accelerator**: AI accelerators integrate
    specialized processing elements containing tensor cores, vector units, and special
    function units, supported by a hierarchical memory system from high-bandwidth
    memory down to local caches. This architecture maximizes data reuse and parallel
    execution while minimizing energy-intensive data movement, forming the foundation
    for 100-1000× performance improvements over general-purpose processors.'
  prefs: []
  type: TYPE_NORMAL
- en: AI Compute Primitives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding how hardware evolved toward AI-specific designs requires examining
    the computational patterns that drove this specialization. The transition from
    general-purpose CPUs achieving 100 GFLOPS to specialized accelerators delivering
    100,000+ GFLOPS reflects architectural optimization for specific computational
    patterns that dominate machine learning workloads. These patterns, which we term
    compute primitives, appear repeatedly across all neural network architectures
    regardless of application domain or model size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern neural networks are built upon a small number of core computational
    patterns. Regardless of the layer type—whether fully connected, convolutional,
    or attention-based layers—the underlying operation typically involves multiplying
    input values by learned weights and accumulating the results. This repeated multiply-accumulate
    process dominates neural network execution and defines the arithmetic foundation
    of AI workloads. The regularity and frequency of these operations have led to
    the development of AI compute primitives: hardware-level abstractions optimized
    to execute these core computations with high efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks exhibit highly structured, data-parallel computations that enable
    architectural specialization. Building on the parallelization principles established
    in [Section 11.2.2](ch017.xhtml#sec-ai-acceleration-parallel-computing-graphics-processing-66b1),
    these patterns emphasize predictable data reuse and fixed operation sequences.
    AI compute primitives distill these patterns into reusable architectural units
    that support high-throughput and energy-efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: This decomposition is illustrated in [Listing 11.1](ch017.xhtml#lst-dense_layer_def),
    which defines a dense layer at the framework level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.1: **Dense Layer Definition**: Defines a dense layer using a high-level
    API, illustrating how neural networks implement parallel transformations across
    input tensors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This high-level call expands into mathematical operations is shown in [Listing 11.2](ch017.xhtml#lst-dense_expansion).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.2: **Layer Computation**: Neural networks compute each layer’s output
    via weighted input summation followed by an activation function transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: At the processor level, the computation reduces to nested loops that multiply
    inputs and weights, sum the results, and apply a nonlinear function, as shown
    in [Listing 11.3](ch017.xhtml#lst-loop_level_dense).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.3: **Nested Loops**: Computes output values through sequential matrix
    multiplications and bias additions, followed by activation function application
    to produce final outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This transformation reveals four computational characteristics: data-level
    parallelism enabling simultaneous execution, structured matrix operations defining
    computational workloads, predictable data movement patterns driving memory optimization,
    and frequent nonlinear transformations motivating specialized function units.'
  prefs: []
  type: TYPE_NORMAL
- en: The design of AI compute primitives follows three architectural criteria. First,
    the primitive must be used frequently enough to justify dedicated hardware resources.
    Second, its specialized implementation must offer substantial performance or energy
    efficiency gains relative to general-purpose alternatives. Third, the primitive
    must remain stable across generations of neural network architectures to ensure
    long-term applicability. These considerations shape the inclusion of primitives
    such as vector operations, matrix operations, and special function units in modern
    ML accelerators. Together, they serve as the architectural foundation for efficient
    and scalable neural network execution.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector operations provide the first level of hardware acceleration by processing
    multiple data elements simultaneously. This parallelism exists at multiple scales,
    from individual neurons to entire layers, making vector processing essential for
    efficient neural network execution. Framework-level code translates to hardware
    instructions, revealing the critical role of vector processing in neural accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Framework Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine learning frameworks hide hardware complexity through high-level abstractions.
    These abstractions decompose into progressively lower-level operations, revealing
    opportunities for hardware acceleration. One such abstraction is shown in [Listing 11.4](ch017.xhtml#lst-linear_layer_highlevel),
    which illustrates the execution flow of a linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.4: **Linear Layer**: Neural networks transform input data into a
    higher-dimensional space using linear mappings to enable complex feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This abstraction represents a fully connected layer that transforms input features
    through learned weights. To understand how hardware acceleration opportunities
    emerge, [Listing 11.5](ch017.xhtml#lst-linear_math_internal) shows how the framework
    translates this high-level expression into mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.5: **Fully Connected Layer**: Each output is computed as a weighted
    sum of all inputs plus a bias, followed by an activation function transformation.
    Linear transformations enable complex model architectures in neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These mathematical operations further decompose into explicit computational
    steps during processor execution. [Listing 11.6](ch017.xhtml#lst-loop_linear_layer)
    illustrates the nested loops that implement these multiply-accumulate operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.6: **Linear Layer Computation**: Each output neuron is computed
    by summing weighted inputs from all features, followed by an activation function
    application. Understanding this process helps in grasping the fundamental building
    blocks of neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Sequential Scalar Execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional scalar processors execute these operations sequentially, processing
    individual values one at a time. For the linear layer example above with a batch
    of 32 samples, computing the outputs requires over 4 million multiply-accumulate
    operations. Each operation involves loading an input value and a weight value,
    multiplying them, and accumulating the result. This sequential approach becomes
    highly inefficient when processing the massive number of identical operations
    required by neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing this inefficiency, modern processors leverage vector processing
    to transform execution patterns fundamentally.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Vector Execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector processing units achieve this transformation by operating on multiple
    data elements simultaneously. [Listing 11.7](ch017.xhtml#lst-riscv_vector_mac)
    demonstrates this approach using RISC-V[16](#fn16) assembly code that showcases
    modern vector processing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.7: **Vectorized Multiply-Accumulate Loop**: This loop showcases
    how RISC-V vector instructions enable efficient batch processing by performing
    8 multiply-add operations simultaneously, reducing computational latency in neural
    network training. *Source: RISC-V Architecture Manual*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This vector implementation processes eight data elements in parallel, reducing
    both computation time and energy consumption. Vector load instructions transfer
    eight values simultaneously, maximizing memory bandwidth utilization. The vector
    multiply-accumulate instruction processes eight pairs of values in parallel, dramatically
    reducing the total instruction count from over 4 million to approximately 500,000.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify how vector instructions map to common deep learning patterns, [Table 11.2](ch017.xhtml#tbl-vector)
    introduces key vector operations and their typical applications in neural network
    computation. These operations, such as reduction, gather, scatter, and masked
    operations, are frequently encountered in layers like pooling, embedding lookups,
    and attention mechanisms. This terminology is necessary for interpreting how low-level
    vector hardware accelerates high-level machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.2: **Vector Operations**: Neural network layers frequently utilize
    core vector operations such as reduction, gather, and scatter to accelerate computation
    and efficiently process data in parallel; these operations clarify how low-level
    hardware optimizations map to high-level machine learning algorithms. These operations
    enable efficient implementation of common layers like pooling, embedding lookups,
    and attention mechanisms within deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Vector Operation** | **Description** | **Neural Network Application** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Reduction** | Combines elements across a vector (e.g., sum, max) | Pooling
    layers, attention score computation |'
  prefs: []
  type: TYPE_TB
- en: '| **Gather** | Loads multiple non-consecutive memory elements | Embedding lookups,
    sparse operations |'
  prefs: []
  type: TYPE_TB
- en: '| **Scatter** | Writes to multiple non-consecutive memory locations | Gradient
    updates for embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| **Masked operations** | Selectively operates on vector elements | Attention
    masks, padding handling |'
  prefs: []
  type: TYPE_TB
- en: '| **Vector-scalar broadcast** | Applies scalar to all vector elements | Bias
    addition, scaling operations |'
  prefs: []
  type: TYPE_TB
- en: Vector processing efficiency gains extend beyond instruction count reduction.
    Memory bandwidth utilization improves as vector loads transfer multiple values
    per operation. Energy efficiency increases because control logic is shared across
    multiple operations. These improvements compound across the deep layers of modern
    neural networks, where billions of operations execute for each forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Processing History
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The principles underlying vector operations have long been central to high-performance
    computing. In the 1970s and 1980s, vector processors emerged as an architectural
    solution for scientific computing, weather modeling, and physics simulations,
    where large arrays of data required efficient parallel processing. Early systems
    such as the Cray-1[17](#fn17), one of the first commercially successful supercomputers,
    introduced dedicated vector units to perform arithmetic operations on entire data
    vectors in a single instruction. These vector units dramatically improved computational
    throughput compared to traditional scalar execution ([Jordan 1982](ch058.xhtml#ref-jordan1982guide)).
  prefs: []
  type: TYPE_NORMAL
- en: These concepts have reemerged in machine learning, where neural networks exhibit
    structure well suited to vectorized execution. The same operations, such as vector
    addition, multiplication, and reduction, that once accelerated numerical simulations
    now drive the execution of machine learning workloads. While the scale and specialization
    of modern AI accelerators differ from their historical predecessors, the underlying
    architectural principles remain the same. The resurgence of vector processing
    in neural network acceleration highlights its utility for achieving high computational
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector operations establish the foundation for neural network acceleration
    by enabling efficient parallel processing of independent data elements. While
    vector operations excel at element-wise transformations like activation functions,
    neural networks also require structured computations that combine multiple input
    features to produce output features, transformations that naturally express themselves
    as matrix operations. This need for coordinated computation across multiple dimensions
    simultaneously leads to the next architectural primitive: matrix operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matrix operations form the computational workhorse of neural networks, transforming
    high-dimensional data through structured patterns of weights, activations, and
    gradients ([Goodfellow, Courville, and Bengio 2013](ch058.xhtml#ref-Goodfellow-et-al-2016)).
    While vector operations process elements independently, matrix operations orchestrate
    computations across multiple dimensions simultaneously. These operations reveal
    patterns that drive hardware acceleration strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Operations in Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural network computations decompose into hierarchical matrix operations. As
    shown in [Listing 11.8](ch017.xhtml#lst-linear_matrix_hierarchy), a linear layer
    demonstrates this hierarchy by transforming input features into output neurons
    over a batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.8: **Matrix Operations**: Neural networks perform transformations
    using matrix multiplications and biases to achieve output predictions. Training
    requires careful management of input batches and activation functions to optimize
    model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This computation demonstrates the scale of matrix operations in neural networks.
    Each output neuron (512 total) must process all input features (256 total) for
    every sample in the batch (32 samples). The weight matrix alone contains <semantics><mrow><mn>256</mn><mo>×</mo><mn>512</mn><mo>=</mo><mn>131</mn><mo>,</mo><mn>072</mn></mrow><annotation
    encoding="application/x-tex">256 \times 512 = 131,072</annotation></semantics>
    parameters that define these transformations, illustrating why efficient matrix
    multiplication becomes crucial for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks employ matrix operations across diverse architectural patterns
    beyond simple linear layers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Matrix Computations in Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix operations appear consistently across modern neural architectures, as
    illustrated in [Listing 11.9](ch017.xhtml#lst-matrix_patterns). Convolution operations
    are transformed into matrix multiplications through the im2col technique[18](#fn18),
    enabling efficient execution on hardware optimized for matrix operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.9: **Linear Layers**: Layer transformations combine input features
    to produce hidden representations. Matrix operations in neural networks enable
    efficient feature extraction and transformation, forming the backbone of many
    machine learning architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This pervasive pattern of matrix multiplication has direct implications for
    hardware design. The need for efficient matrix operations drives the development
    of specialized hardware architectures that can handle these computations at scale.
    The following sections explore how modern AI accelerators implement matrix operations,
    focusing on their architectural features and performance optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Operations Hardware Acceleration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The computational demands of matrix operations have driven specialized hardware
    optimizations. Modern processors implement dedicated matrix units that extend
    beyond vector processing capabilities. An example of such matrix acceleration
    is shown in [Listing 11.10](ch017.xhtml#lst-matrix_unit).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.10: **Matrix Unit Operation**: Enables efficient block-wise matrix
    multiplication and accumulation in hardware-accelerated systems, showcasing how
    specialized units streamline computational tasks essential for AI/ML operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This matrix processing unit can handle <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics> blocks of the
    linear layer computation described earlier, processing 256 multiply-accumulate
    operations simultaneously compared to the 8 operations possible with vector processing.
    These matrix operations complement vectorized computation by enabling structured
    many-to-many transformations. The interplay between matrix and vector operations
    shapes the efficiency of neural network execution.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix operations provide computational capabilities for neural networks through
    coordinated parallel processing across multiple dimensions (see [Table 11.3](ch017.xhtml#tbl-matrix)).
    While they enable transformations such as attention mechanisms and convolutions,
    their performance depends on efficient data handling. Conversely, vector operations
    are optimized for one-to-one transformations like activation functions and layer
    normalization. The distinction between these operations highlights the importance
    of dataflow patterns in neural accelerator design, examined next ([Hwu 2011](ch058.xhtml#ref-Hwu2011GPU)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.3: **Operation Characteristics**: Matrix operations excel at many-to-many
    transformations common in neural network layers, while vector operations efficiently
    handle one-to-one transformations like activation functions and normalization.
    Understanding these distinctions guides the selection of appropriate computational
    primitives for different machine learning tasks and impacts system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operation Type** | **Best For** | **Examples** | **Key Characteristic**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | *   Layer transformations |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Matrix Operations** | Many-to-many transforms | *   Attention computation*   Convolutions*   Activation
    functions | Each output depends on multiple inputs |'
  prefs: []
  type: TYPE_TB
- en: '| **Vector Operations** | One-to-one transforms | *   Layer normalization*   Element-wise
    gradients | Each output depends only on corresponding input |'
  prefs: []
  type: TYPE_TB
- en: Historical Foundations of Matrix Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix operations have long served as a cornerstone of computational mathematics,
    with applications extending from numerical simulations to graphics processing
    ([Golub and Loan 1996](ch058.xhtml#ref-Golub1996Matrix)). The structured nature
    of matrix multiplications and transformations made them natural targets for acceleration
    in early computing architectures. In the 1980s and 1990s, specialized digital
    signal processors (DSPs) and graphics processing units (GPUs) optimized for matrix
    computations played a critical role in accelerating workloads such as image processing,
    scientific computing, and 3D rendering ([Owens et al. 2008](ch058.xhtml#ref-owens2008gpu)).
  prefs: []
  type: TYPE_NORMAL
- en: The widespread adoption of machine learning has reinforced the importance of
    efficient matrix computation. Neural networks, fundamentally built on matrix multiplications
    and tensor operations, have driven the development of dedicated hardware architectures
    that extend beyond traditional vector processing. Modern tensor processing units
    (TPUs) and AI accelerators implement matrix multiplication at scale, reflecting
    the same architectural principles that once underpinned early scientific computing
    and graphics workloads. The resurgence of matrix-centric architectures highlights
    the deep connection between classical numerical computing and contemporary AI
    acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: While matrix operations provide the computational backbone for neural networks,
    they represent only part of the acceleration challenge. Neural networks also depend
    critically on non-linear transformations that cannot be efficiently expressed
    through linear algebra alone.
  prefs: []
  type: TYPE_NORMAL
- en: Special Function Units
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While vector and matrix operations efficiently handle the linear transformations
    in neural networks, non-linear functions present unique computational challenges
    that require dedicated hardware solutions. Special Function Units (SFUs) provide
    hardware acceleration for these essential computations, completing the set of
    fundamental processing primitives needed for efficient neural network execution.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Linear Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Non-linear functions play a fundamental role in machine learning by enabling
    neural networks to model complex relationships ([Goodfellow, Courville, and Bengio
    2013](ch058.xhtml#ref-Goodfellow-et-al-2016)). [Listing 11.11](ch017.xhtml#lst-nonlinear_layer)
    illustrates a typical neural network layer sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.11: **Non-Linear Transformations**: Neural networks process input
    data through a sequence of linear transformations followed by non-linear activations
    to capture complex patterns. This layer sequence enhances model expressiveness
    and learning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This sequence introduces multiple non-linear transformations that extend beyond
    simple matrix operations. [Listing 11.12](ch017.xhtml#lst-nonlinear_math) demonstrates
    how the framework decomposes these operations into their mathematical components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.12: **Non-linear Transformations**: Neural networks apply linear
    and non-linear operations to transform input data into meaningful features for
    learning. Machine learning models leverage these transformations to capture complex
    patterns in data efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Hardware Implementation of Non-Linear Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The computational complexity of these operations becomes apparent when examining
    their implementation on traditional processors. These seemingly simple mathematical
    operations translate into complex sequences of instructions. Consider the computation
    of batch normalization: calculating the square root requires multiple iterations
    of numerical approximation, while exponential functions in operations like softmax
    need series expansion or lookup tables ([Ioffe and Szegedy 2015b](ch058.xhtml#ref-Ioffe2015)).
    Even a simple ReLU activation introduces branching logic that can disrupt instruction
    pipelining (see [Listing 11.13](ch017.xhtml#lst-traditional_overhead) for an example).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.13: **ReLU and BatchNorm Operations**: Neural networks process input
    data through conditional operations that can disrupt instruction pipelining and
    multiple passes required for normalization, highlighting efficiency challenges
    in traditional implementations. Source: IEEE Spectrum'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'These operations introduce several key inefficiencies:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple passes over data, increasing memory bandwidth requirements
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complex arithmetic requiring many instruction cycles
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conditional operations that can cause pipeline stalls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additional memory storage for intermediate results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Poor utilization of vector processing units
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'More specifically, each operation introduces distinct challenges. Batch normalization
    requires multiple passes through data: one for mean computation, another for variance,
    and a final pass for output transformation. Each pass loads and stores data through
    the memory hierarchy. Operations that appear simple in mathematical notation often
    expand into many instructions. The square root computation typically requires
    10-20 iterations of numerical methods like Newton-Raphson approximation for suitable
    precision ([Goldberg 1991](ch058.xhtml#ref-Goldberg1991)). Conditional operations
    like ReLU’s max function require branch instructions that can stall the processor’s
    pipeline. The implementation needs temporary storage for intermediate values,
    increasing memory usage and bandwidth consumption. While vector units excel at
    regular computations, functions like exponentials and square roots often require
    scalar operations that cannot fully utilize vector processing capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Acceleration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SFUs address these inefficiencies through dedicated hardware implementation.
    Modern ML accelerators include specialized circuits that transform these complex
    operations into single-cycle or fixed-latency computations. The accelerator can
    load a vector of values and apply non-linear functions directly, eliminating the
    need for multiple passes and complex instruction sequences as shown in [Listing 11.14](ch017.xhtml#lst-sfu_vector_ops).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.14: **Hardware Acceleration**: Single-cycle non-linear operations
    enable efficient vector processing in ML accelerators, showcasing how specialized
    hardware reduces computational latency.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Each SFU implements a specific function through specialized circuitry. For instance,
    a ReLU unit performs the comparison and selection in dedicated logic, eliminating
    branching overhead. Square root operations use hardware implementations of algorithms
    like Newton-Raphson with fixed iteration counts, providing guaranteed latency.
    Exponential and logarithmic functions often combine small lookup tables with hardware
    interpolation circuits ([Costa et al. 2019](ch058.xhtml#ref-Lauterbach2019)).
    Using these custom instructions, the SFU implementation eliminates multiple passes
    over data, removes complex arithmetic sequences, and maintains high computational
    efficiency. [Table 11.4](ch017.xhtml#tbl-sfu) shows the various hardware implementations
    and their typical latencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.4: **Special Function Units**: Dedicated hardware implementations
    of common mathematical functions—like relu, sigmoid, and reciprocal square root—accelerate
    machine learning computations by eliminating software overhead and enabling parallel
    processing of vector data. Typical latencies of 1–2 cycles per function demonstrate
    the performance gains achieved through specialized circuitry instead of general-purpose
    arithmetic.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function Unit** | **Operation** | **Implementation Strategy** | **Typical
    Latency** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Activation Unit** | ReLU, sigmoid, tanh | Piece-wise approximation circuits
    | 1-2 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| **Statistics Unit** | Mean, variance | Parallel reduction trees | log(N)
    cycles |'
  prefs: []
  type: TYPE_TB
- en: '| **Exponential Unit** | exp, log | Table lookup + hardware interpolation |
    2-4 cycles |'
  prefs: []
  type: TYPE_TB
- en: '| **Root/Power Unit** | sqrt, rsqrt | Fixed-iteration Newton-Raphson | 4-8
    cycles |'
  prefs: []
  type: TYPE_TB
- en: SFUs History
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The need for efficient non-linear function evaluation has shaped computer architecture
    for decades. Early processors incorporated hardware support for complex mathematical
    functions, such as logarithms and trigonometric operations, to accelerate workloads
    in scientific computing and signal processing ([Smith 1997](ch058.xhtml#ref-Smith1997)).
    In the 1970s and 1980s, floating-point co-processors were introduced to handle
    complex mathematical operations separately from the main CPU ([Palmer 1980](ch058.xhtml#ref-palmer_8087_1981)).
    In the 1990s, instruction set extensions such as Intel’s SSE and ARM’s NEON provided
    dedicated hardware for vectorized mathematical transformations, improving efficiency
    for multimedia and signal processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning workloads have reintroduced a strong demand for specialized
    functional units, as activation functions, normalization layers, and exponential
    transformations are fundamental to neural network computations. Rather than relying
    on iterative software approximations, modern AI accelerators implement fast, fixed-latency
    SFUs for these operations, mirroring historical trends in scientific computing.
    The reemergence of dedicated special function units underscores the ongoing cycle
    in hardware evolution, where domain-specific requirements drive the reinvention
    of classical architectural concepts in new computational paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: The combination of vector, matrix, and special function units provides the computational
    foundation for modern AI accelerators. However, the effective utilization of these
    processing primitives depends critically on data movement and access patterns.
    This leads us to examine the architectures, hierarchies, and strategies that enable
    efficient data flow in neural network execution.
  prefs: []
  type: TYPE_NORMAL
- en: Compute Units and Execution Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The vector operations, matrix operations, and special function units examined
    previously represent the fundamental computational primitives in AI accelerators.
    Modern AI processors package these primitives into distinct execution units, such
    as SIMD units, tensor cores, and processing elements, which define how computations
    are structured and exposed to users. Understanding this organization reveals both
    the theoretical capabilities and practical performance characteristics that developers
    can leverage in contemporary AI accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Primitives to Execution Units
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The progression from computational primitives to execution units follows a
    structured hierarchy that reflects the increasing complexity and specialization
    of AI accelerators:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector operations → SIMD/SIMT units that enable parallel processing of independent
    data elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix operations → Tensor cores and systolic arrays that provide structured
    matrix multiplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special functions → Dedicated hardware units integrated within processing elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each execution unit combines these computational primitives with specialized
    memory and control mechanisms, optimizing both performance and energy efficiency.
    This structured packaging allows hardware vendors to expose standardized programming
    interfaces while implementing diverse underlying architectures tailored to specific
    workload requirements. The choice of execution unit significantly influences overall
    system efficiency, affecting data locality, compute density, and workload adaptability.
    Subsequent sections examine how these execution units operate within AI accelerators
    to maximize performance across different machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution from SIMD to SIMT Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single Instruction Multiple Data (SIMD)[19](#fn19) execution applies identical
    operations to multiple data elements in parallel, minimizing instruction overhead
    while maximizing data throughput. This execution model is widely used to accelerate
    workloads with regular, independent data parallelism, such as neural network computations.
    The ARM Scalable Vector Extension (SVE) provides a representative example of how
    modern architectures implement SIMD operations efficiently, as illustrated in
    [Listing 11.15](ch017.xhtml#lst-arm_sve_vector).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.15: **Vector Operation**: Vector multiplication and addition operations
    enable efficient parallel processing in machine learning models. *Source: ARM
    Documentation*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Processor architectures continue to expand SIMD capabilities to accommodate
    increasing computational demands. Intel’s Advanced Matrix Extensions (AMX) ([I.
    Corporation 2021](ch058.xhtml#ref-intel2021amx)) and ARM’s SVE2 architecture ([Stephens
    et al. 2017](ch058.xhtml#ref-stephens2017arm)) provide flexible SIMD execution,
    enabling software to scale across different hardware implementations.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, SIMT extends SIMD principles by enabling parallel
    execution across multiple independent threads, each maintaining its own program
    counter and architectural state ([E. Lindholm et al. 2008](ch058.xhtml#ref-lindholm2008nvidia)).
    This model maps naturally to matrix computations, where each thread processes
    different portions of a workload while still benefiting from shared instruction
    execution. In NVIDIA’s GPU architectures, each Streaming Multiprocessor (SM)[20](#fn20)
    coordinates thousands of threads executing in parallel, allowing for efficient
    scaling of neural network computations, as demonstrated in [Listing 11.16](ch017.xhtml#lst-cuda_simt).
    Threads are organized into warps[21](#fn21), which are the fundamental execution
    units that enable SIMT efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.16: **SIMT Execution**: Each thread processes a unique output element
    in parallel, demonstrating how SIMT enables efficient matrix multiplication on
    GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: SIMT execution allows neural network computations to scale efficiently across
    thousands of threads while maintaining flexibility for divergent execution paths.
    Similar execution models appear in AMD’s RDNA and Intel’s Xe architectures, reinforcing
    SIMT as a fundamental mechanism for AI acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Cores
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While SIMD and SIMT units provide efficient execution of vector operations,
    neural networks rely heavily on matrix computations that require specialized execution
    units for structured multi-dimensional processing. The energy economics of matrix
    operations drive this specialization: traditional scalar processing requires multiple
    DRAM accesses per operation, consuming 640 pJ per fetch, while tensor cores amortize
    this energy cost across entire matrix blocks. Tensor processing units extend SIMD
    and SIMT principles by enabling efficient matrix operations through dedicated
    hardware blocks that execute matrix multiplications and accumulations on entire
    matrix blocks in a single operation. Tensor cores transform the energy profile
    from 173× memory-bound inefficiency to compute-optimized execution where the 3.7 pJ
    multiply-accumulate operation dominates the energy budget rather than data movement.'
  prefs: []
  type: TYPE_NORMAL
- en: Tensor cores[22](#fn22), implemented in architectures such as NVIDIA’s Ampere
    GPUs, provide an example of this approach. They expose matrix computation capabilities
    through specialized instructions, such as the tensor core operation shown in [Listing 11.17](ch017.xhtml#lst-tensor_core_op)
    on the NVIDIA A100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.17: **Tensor Core Operation**: Matrix multiplications are performed
    in parallel across entire matrix blocks, optimizing computational efficiency for
    neural network training.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A single tensor core instruction processes an entire matrix block while maintaining
    intermediate results in local registers, significantly improving computational
    efficiency compared to implementations based on scalar or vector operations. This
    structured approach enables hardware to achieve high throughput while reducing
    the burden of explicit loop unrolling and data management at the software level.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor processing unit architectures differ based on design priorities. NVIDIA’s
    Ampere architecture incorporates tensor cores optimized for general-purpose deep
    learning acceleration. Google’s TPUv4 utilizes large-scale matrix units arranged
    in systolic arrays to maximize sustained training throughput. Apple’s M1 neural
    engine[23](#fn23) integrates smaller matrix processors optimized for mobile inference
    workloads, while Intel’s Sapphire Rapids architecture introduces AMX tiles designed
    for high-performance datacenter applications.
  prefs: []
  type: TYPE_NORMAL
- en: The increasing specialization of AI hardware has driven significant performance
    improvements in deep learning workloads. [Figure 11.3](ch017.xhtml#fig-ai-performance)
    illustrates the trajectory of AI accelerator performance in NVIDIA GPUs, highlighting
    the transition from general-purpose floating-point execution units to highly optimized
    tensor processing cores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: **GPU Performance Scaling**: NVIDIA GPUs experienced a <semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics> increase in integer
    8-bit TOPS (tera operations per second) over a decade, driven by architectural
    innovations transitioning from floating-point to tensor core acceleration. This
    trend reflects the growing specialization of hardware for deep learning workloads
    and the increasing demand for efficient inference capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Processing Elements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The highest level of execution unit organization integrates multiple tensor
    cores with local memory into processing elements (PEs). A processing element serves
    as a fundamental building block in many AI accelerators, combining different computational
    units to efficiently execute neural network operations. Each PE typically includes
    vector units for element-wise operations, tensor cores for matrix computation,
    special function units for non-linear transformations, and dedicated memory resources
    to optimize data locality and minimize data movement overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Processing elements play an essential role in AI hardware by balancing computational
    density with memory access efficiency. Their design varies across different architectures
    to support diverse workloads and scalability requirements. Graphcore’s Intelligence
    Processing Unit (IPU) distributes computation across 1,472 tiles, each containing
    independent processing elements optimized for fine-grained parallelism ([Graphcore
    2020](ch058.xhtml#ref-Graphcore2020)). Cerebras extends this approach in the CS-2
    system, integrating 850,000 processing elements across a wafer-scale device to
    accelerate sparse computations. Tesla’s D1 processor arranges processing elements
    with substantial local memory, optimizing throughput and latency for real-time
    autonomous vehicle workloads ([Quinnell 2024](ch058.xhtml#ref-Tesla2021)).
  prefs: []
  type: TYPE_NORMAL
- en: Processing elements provide the structural foundation for large-scale AI acceleration.
    Their efficiency depends not only on computational capability but also on interconnect
    strategies and memory hierarchy design. The next sections explore how these architectural
    choices impact performance across different AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor processing units have enabled substantial efficiency gains in AI workloads
    by using hardware-accelerated matrix computation. Their role continues to evolve
    as architectures incorporate support for advanced execution techniques, including
    structured sparsity and workload-specific optimizations. The effectiveness of
    these units, however, depends not only on their computational capabilities but
    also on how they interact with memory hierarchies and data movement mechanisms,
    which are examined in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Systolic Arrays
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While tensor cores package matrix operations into structured computational units,
    systolic arrays provide an alternative approach optimized for continuous data
    flow and operand reuse. The fundamental motivation for systolic architectures
    stems from the energy efficiency constraints discussed earlier—minimizing the
    impact of memory access penalties through architectural design. A systolic array
    arranges processing elements in a grid pattern, where data flows rhythmically
    between neighboring units in a synchronized manner, enabling each operand to participate
    in multiple computations as it propagates through the array. This structured movement
    minimizes external memory accesses by maximizing local data reuse—a single weight
    value can contribute to dozens of operations as it moves through the processing
    elements, fundamentally transforming the energy profile from memory-bound to compute-efficient
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of systolic arrays was first introduced by Kung and Leiserson[24](#fn24),
    who formalized their use in parallel computing architectures for efficient matrix
    operations ([Kung 1982](ch058.xhtml#ref-Kung1982)). Unlike general-purpose execution
    units, systolic arrays exploit spatial and temporal locality by reusing operands
    as they propagate through the grid. Google’s TPU exemplifies this architectural
    approach. In the TPUv4, a <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics> systolic array
    of multiply-accumulate units processes matrix operations by streaming data through
    the array in a pipelined manner, as shown in [Figure 11.4](ch017.xhtml#fig-systolic-array).
  prefs: []
  type: TYPE_NORMAL
- en: 'The systolic array architecture achieves computational efficiency through synchronized
    data movement across a structured grid of processing elements. Systolic arrays
    organize computation around four fundamental components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Control Unit**: Coordinates timing and data distribution across the array,
    maintaining synchronized operation throughout the computational grid'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Streams: Input matrices propagate through coordinated pathways—matrix
    A elements traverse horizontally while matrix B elements flow vertically through
    the processing grid'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Processing Element Grid**: Individual processing elements execute multiply-accumulate
    operations on streaming data, generating partial results that accumulate toward
    the final computation'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output Collection**: Results aggregate at designated output boundaries where
    accumulated partial sums form complete matrix elements'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The synchronized data flow ensures that matrix element A[i,k] encounters corresponding
    B[k,j] elements at precise temporal intervals, executing the multiply-accumulate
    operations required for matrix multiplication C[i,j] = Σ A[i,k] × B[k,j]. This
    systematic reuse of operands across multiple processing elements substantially
    reduces memory bandwidth requirements by eliminating redundant data fetches from
    external memory subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the multiplication of 2×2 matrices A and B within a systolic array.
    During the first computational cycle, element A[0,0]=2 propagates horizontally
    while B[0,0]=1 moves vertically, converging at processing element PE(0,0) to execute
    the multiplication 2×1=2\. In the subsequent cycle, the same A[0,0]=2 advances
    to PE(0,1) where it encounters B[0,1]=3, computing 2×3=6\. Concurrently, A[0,1]=4
    enters PE(0,0) to engage with the next B matrix element. This coordinated data
    movement enables systematic operand reuse across multiple computational operations,
    eliminating redundant memory accesses and exemplifying the fundamental efficiency
    principle underlying systolic array architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file183.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: **Systolic Array Dataflow**: Processing elements within the array
    execute matrix operations by streaming data in a pipelined manner, maximizing
    operand reuse and minimizing memory access compared to traditional memory-compute
    architectures. This spatial and temporal locality enables efficient parallel computation,
    as exemplified by the multiply-accumulate units in Google’s tpuv4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each processing element in the array performs a multiply-accumulate operation
    in every cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: Receives an input activation from above
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receives a weight value from the left
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiplies these values and adds to its running sum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Passes the input activation downward and the weight value rightward to neighboring
    elements
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This structured computation model minimizes data movement between global memory
    and processing elements, improving both efficiency and scalability. As systolic
    arrays operate in a streaming fashion, they are particularly effective for high-throughput
    workloads such as deep learning training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: While the diagram in [Figure 11.4](ch017.xhtml#fig-systolic-array) illustrates
    one common systolic array implementation, systolic architectures vary significantly
    across different accelerator designs. Training-focused architectures like Google’s
    TPU employ large arrays optimized for high computational throughput, while inference-oriented
    designs found in edge devices prioritize energy efficiency with smaller configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental principle remains consistent: data flows systematically through
    processing elements, with inputs moving horizontally and vertically to compute
    partial sums in a synchronized fashion. However, as detailed in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    practical effectiveness is ultimately constrained by memory bandwidth bottlenecks.'
  prefs: []
  type: TYPE_NORMAL
- en: A 128×128 systolic array capable of 16,384 operations per cycle requires continuous
    data feed to maintain utilization—each cycle demands fresh input activations and
    weight parameters that must traverse from off-chip memory through on-chip buffers
    to the array edges. The TPU’s 1,200 GB/s on-chip bandwidth enables high utilization,
    but even this substantial bandwidth becomes limiting when processing large transformer
    models where memory requirements exceed on-chip capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Chapter 10](ch016.xhtml#sec-model-optimizations) that quantization
    reduces model memory footprint by converting FP32 weights to INT8 representations—this
    optimization directly addresses the memory bandwidth constraints identified here.
    Converting 32-bit floating-point weights to 8-bit integers reduces memory traffic
    by 4×, transforming bandwidth-bound operations into compute-bound workloads where
    systolic arrays can achieve higher utilization. Similarly, structured pruning
    removes entire rows or columns of weight matrices, reducing both the data volume
    that must traverse memory hierarchies and the computation required. These algorithmic
    optimizations prove valuable precisely because they target the memory bottleneck
    that limits accelerator performance in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Numerics in AI Acceleration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The efficiency of AI accelerators is not determined by computational power alone
    but also by the precision of numerical representations. The choice of numerical
    format shapes the balance between accuracy, throughput, and energy consumption,
    influencing how different execution units, such as SIMD and SIMT units, tensor
    cores, and systolic arrays, are designed and deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Precision Trade-offs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Numerical precision represents a critical design parameter in modern AI accelerators.
    While higher precision formats provide mathematical stability and accuracy, they
    come with substantial costs in terms of power consumption, memory bandwidth, and
    computational throughput. Finding the optimal precision point has become a central
    challenge in AI hardware architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Early deep learning models primarily relied on single-precision floating point
    (FP32) for both training and inference. While FP32 offers sufficient dynamic range
    and precision for stable learning, it imposes high computational and memory costs,
    limiting efficiency, especially as model sizes increase. Over time, hardware architectures
    evolved to support lower precision formats such as half-precision floating point
    (FP16) and bfloat16 (BF16), which reduce memory usage and increase computational
    throughput while maintaining sufficient accuracy for deep learning tasks. More
    recently, integer formats (INT8, INT4) have gained prominence in inference workloads,
    where small numerical representations significantly improve energy efficiency
    without compromising model accuracy beyond acceptable limits.
  prefs: []
  type: TYPE_NORMAL
- en: The transition from high-precision to lower-precision formats is deeply integrated
    into hardware execution models. As detailed in [Section 11.3.4.2](ch017.xhtml#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd),
    SIMD and SIMT units provide flexible support for multiple precisions. Tensor cores
    ([Section 11.3.4.3](ch017.xhtml#sec-ai-acceleration-tensor-cores-771f)) accelerate
    computation using reduced-precision arithmetic, while systolic arrays ([Section 11.3.4.5](ch017.xhtml#sec-ai-acceleration-systolic-arrays-6fa8))
    optimize performance by minimizing memory bandwidth constraints through low-precision
    formats that maximize operand reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the advantages of reduced precision, deep learning models cannot always
    rely solely on low-bit representations. To address this challenge, modern AI accelerators
    implement mixed-precision computing, where different numerical formats are used
    at different stages of execution. These precision choices have important implications
    for model fairness and reliability. For example, matrix multiplications may be
    performed in FP16 or BF16, while accumulations are maintained in FP32 to prevent
    precision loss. Similarly, inference engines leverage INT8 arithmetic while preserving
    key activations in higher precision when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed-Precision Computing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Modern AI accelerators increasingly support mixed-precision execution, allowing
    different numerical formats to be used at various stages of computation. Training
    workloads often leverage FP16 or BF16 for matrix multiplications, while maintaining
    FP32 accumulations to preserve precision. Inference workloads, by contrast, optimize
    for INT8 or even INT4, achieving high efficiency while retaining acceptable accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This shift toward precision diversity is evident in the evolution of AI hardware.
    Early architectures such as NVIDIA Volta provided limited support for lower precision
    beyond FP16, whereas later architectures, including Turing and Ampere, expanded
    the range of supported formats. Ampere GPUs introduced TF32 as a hybrid between
    FP32 and FP16, alongside broader support for BF16, INT8, and INT4\. [Table 11.5](ch017.xhtml#tbl-nvidia-numerics)
    illustrates this trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.5: **Precision Support Evolution**: GPU architectures progressively
    expanded support for lower-precision data types, enabling performance gains and
    efficiency improvements in AI workloads. Early architectures primarily utilized
    FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate
    both training and inference tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture** | **Year** | **Supported Tensor Core Precisions** | **Supported
    CUDA Core Precisions** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Volta** | 2017 | FP16 | FP64, FP32, FP16 |'
  prefs: []
  type: TYPE_TB
- en: '| **Turing** | 2018 | FP16, INT8 | FP64, FP32, FP16, INT8 |'
  prefs: []
  type: TYPE_TB
- en: '| **Ampere** | 2020 | FP64, TF32, bfloat16, FP16, INT8, INT4 | FP64, FP32,
    FP16, bfloat16, INT8 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 11.5](ch017.xhtml#tbl-nvidia-numerics) highlights how newer architectures
    incorporate a growing diversity of numerical formats, reflecting the need for
    greater flexibility across different AI workloads. This trend suggests that future
    AI accelerators will continue expanding support for adaptive precision, optimizing
    both computational efficiency and model accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: The precision format used in hardware design has far-reaching implications.
    By adopting lower-precision formats, the data transferred between execution units
    and memory is reduced, leading to decreased memory bandwidth requirements and
    storage. Tensor cores and systolic arrays can process more lower-precision elements
    in parallel, thereby increasing the effective throughput in terms of FLOPs. Energy
    efficiency is also improved, as integer-based computations (e.g., INT8) require
    lower power compared to floating-point arithmetic—a clear advantage for inference
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: As AI models continue to scale in size, accelerator architectures are evolving
    to support more efficient numerical formats. Future designs are expected to incorporate
    adaptive precision techniques, dynamically adjusting computation precision based
    on workload characteristics. This evolution promises further optimization of deep
    learning performance while striking an optimal balance between accuracy and energy
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural Integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The organization of computational primitives into execution units determines
    the efficiency of AI accelerators. While SIMD, tensor cores, and systolic arrays
    serve as fundamental building blocks, their integration into full-chip architectures
    varies significantly across different AI processors. The choice of execution units,
    their numerical precision support, and their connectivity impact how effectively
    hardware can scale for deep learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Modern AI processors exhibit a range of design trade-offs based on their intended
    applications. Some architectures, such as NVIDIA’s A100, integrate large numbers
    of tensor cores optimized for FP16-based training, while Google’s TPUv4 prioritizes
    high-throughput BF16 matrix multiplications. Inference-focused processors, such
    as Intel’s Sapphire Rapids, incorporate INT8-optimized tensor cores to maximize
    efficiency. The Apple M1, designed for mobile workloads, employs smaller processing
    elements optimized for low-power FP16 execution. These design choices reflect
    the growing flexibility in numerical precision and execution unit organization,
    as discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.6](ch017.xhtml#tbl-execution-units) summarizes the execution unit
    configurations across contemporary AI processors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.6: **AI Processor Configurations**: Modern AI processors prioritize
    different execution unit characteristics to optimize performance for specific
    workloads; NVIDIA A100 leverages wide SIMD and tensor cores for training, Google
    TPUv4 emphasizes high-throughput BF16 matrix multiplication, and Intel Sapphire
    Rapids focuses on INT8-optimized inference, while apple M1 prioritizes low-power
    FP16 execution on smaller processing elements. These variations in SIMD width,
    tensor core size, and processing element count reflect the growing diversity in
    AI hardware architectures and their targeted applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Processor** | **SIMD Width** | **Tensor Core Size** | **Processing Elements**
    | **Primary Workloads** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **NVIDIA A100** | 1024-bit | <semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">4\times4\times4</annotation></semantics> FP16 | 108
    SMs | Training, HPC |'
  prefs: []
  type: TYPE_TB
- en: '| **Google TPUv4** | 128-wide | <semantics><mrow><mn>128</mn><mo>×</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times128</annotation></semantics> BF16 | 2 cores/chip
    | Training |'
  prefs: []
  type: TYPE_TB
- en: '| **Intel Sapphire** | 512-bit AVX | <semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">32\times32</annotation></semantics> INT8/BF16 | 56
    cores | Inference |'
  prefs: []
  type: TYPE_TB
- en: '| **Apple M1** | 128-bit NEON | <semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation
    encoding="application/x-tex">16\times16</annotation></semantics> FP16 | 8 NPU
    cores | Mobile inference |'
  prefs: []
  type: TYPE_TB
- en: '[Table 11.6](ch017.xhtml#tbl-execution-units) highlights how execution unit
    configurations vary across architectures to optimize for different deep learning
    workloads. Training accelerators prioritize high-throughput floating-point tensor
    operations, whereas inference processors focus on low-precision integer execution
    for efficiency. Meanwhile, mobile accelerators balance precision and power efficiency
    to meet real-time constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: Cost-Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While architectural specifications define computational potential, practical
    deployment decisions require understanding cost-performance trade-offs across
    different accelerator options. However, raw computational metrics alone provide
    an incomplete picture—the fundamental constraint in modern AI acceleration is
    not compute capacity but data movement efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The energy differential established earlier—where memory access costs dominate
    computation—drives the entire specialized hardware revolution. This disparity
    explains why GPUs with high memory bandwidth achieve 40-60% utilization, while
    TPUs with systolic arrays achieve 85% utilization by minimizing data movement.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.7](ch017.xhtml#tbl-accelerator-economics) provides concrete cost-performance
    data for representative accelerators, but the economic analysis must account for
    utilization efficiency and energy consumption patterns that determine real-world
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.7: **Accelerator Cost-Performance Comparison**: Hardware costs must
    be evaluated against computational capabilities to determine optimal deployment
    strategies. While newer accelerators like H100 offer better price-performance
    ratios, total cost of ownership includes power consumption, cooling requirements,
    and infrastructure costs that significantly impact operational economics. *TPU
    pricing estimated from cloud rates.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Accelerator** | **List Price (USD)** | **Peak FLOPS (FP16)** | **Memory
    Bandwidth** | **Price/Performance** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **NVIDIA V100** | ~$9,000 (2017-19) | 125 TFLOPS | 900 GB/s | $72/TFLOP |'
  prefs: []
  type: TYPE_TB
- en: '| **NVIDIA A100** | $15,000 | 312 TFLOPS (FP16) | 1,935 GB/s | $48/TFLOP |'
  prefs: []
  type: TYPE_TB
- en: '| **NVIDIA H100** | $25,000-30,000 | 756 TFLOPS (TF32) | 3,350 GB/s | $33/TFLOP
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Google TPUv4** | ~$8,000* | 275 TFLOPS (BF16) | 1,200 GB/s | $29/TFLOP
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Intel H100** | $12,000 | 200 TFLOPS (INT8) | 800 GB/s | $60/TFLOP |'
  prefs: []
  type: TYPE_TB
- en: A startup training large language models faces the choice between 8 V100s ($72K)
    providing 1,000 TFLOPS or 4 A100s ($60K) delivering 1,248 TFLOPS. However, performance
    analysis reveals the true performance story—transformer training with its arithmetic
    intensity of 0.5-2 FLOPS/byte makes both configurations memory-bandwidth bound
    rather than compute-bound. The A100’s 1,935 GB/s bandwidth delivers 2.15× higher
    sustainable performance than V100’s 900 GB/s, making the effective performance
    gain 115% rather than the 25% suggested by peak FLOPS. When combined with 17%
    lower hardware cost and 30% better energy efficiency (400 W vs 300 W per effective
    TFLOP), the A100 configuration provides compelling economic advantages that compound
    over multi-year deployments.
  prefs: []
  type: TYPE_NORMAL
- en: These cost dynamics explain the rapid adoption of newer accelerators despite
    higher unit prices. The H100’s $33/TFLOP represents a 54% improvement over V100’s
    $72/TFLOP, but more importantly, its 3,350 GB/s bandwidth enables 3.7× higher
    memory throughput per dollar—the metric that determines real-world transformer
    performance. Cloud deployment further complicates the analysis, as providers typically
    charge $2-4/hour for high-end accelerators, making the break-even point between
    purchase and rental highly dependent on utilization patterns and energy costs
    that can account for 60-70% of total operational expenses over a three-year lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Framework selection significantly impacts these economic decisions—detailed
    hardware-framework optimization strategies are covered in [Chapter 7](ch013.xhtml#sec-ai-frameworks),
    while performance evaluation methodologies are discussed in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  prefs: []
  type: TYPE_NORMAL
- en: While execution units define the compute potential of an accelerator, their
    effectiveness is fundamentally constrained by data movement and memory hierarchy.
    Achieving high utilization of compute resources requires efficient memory systems
    that minimize data transfer overhead and optimize locality. Understanding these
    constraints reveals why memory architecture becomes as critical as computational
    design in AI acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: AI Memory Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The execution units examined in previous sections—SIMD units, tensor cores,
    and systolic arrays—provide impressive computational throughput: modern accelerators
    achieve 100 to 1000 TFLOPS for neural network operations. Yet these theoretical
    capabilities remain unrealized in practice when memory subsystems cannot supply
    data at sufficient rates. This fundamental constraint, termed the AI memory wall,
    represents the dominant bottleneck in real-world accelerator performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike conventional workloads, ML models require frequent access to large volumes
    of parameters, activations, and intermediate results, leading to substantial memory
    bandwidth demands—a challenge that intersects with the data management strategies
    covered in [Chapter 6](ch012.xhtml#sec-data-engineering). Modern AI hardware addresses
    these challenges through advanced memory hierarchies, efficient data movement
    techniques, and compression strategies that promote efficient execution and improved
    AI acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: This section examines memory system design through four interconnected perspectives.
    First, we quantify the growing disparity between computational throughput and
    memory bandwidth, revealing why the AI memory wall represents the dominant performance
    constraint in modern accelerators. Second, we explore how memory hierarchies balance
    competing demands for speed, capacity, and energy efficiency through carefully
    structured tiers from on-chip SRAM to off-chip DRAM. Third, we analyze communication
    patterns between host systems and accelerators, exposing transfer bottlenecks
    that limit end-to-end performance. Finally, we examine how different neural network
    architectures—multilayer perceptrons, convolutional networks, and transformers—create
    distinct memory pressure patterns that inform hardware design decisions and optimization
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the AI Memory Wall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AI memory wall represents the fundamental bottleneck constraining modern
    accelerator performance—the growing disparity between computational throughput
    and memory bandwidth that prevents accelerators from achieving their theoretical
    capabilities. While compute units can execute millions of operations per second
    through specialized primitives like vector operations and matrix multiplications,
    they depend entirely on memory systems to supply the continuous stream of weights,
    activations, and intermediate results these operations require.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying the Compute-Memory Performance Gap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The severity of this constraint becomes apparent when examining scaling trends.
    Over the past 20 years, peak computational capabilities have scaled at 3.0× every
    two years, while DRAM bandwidth has grown at only 1.6× during the same period
    ([Gholami et al. 2024](ch058.xhtml#ref-gholami2024ai)). This divergence creates
    an exponentially widening gap where accelerators possess massive computational
    power but cannot access data quickly enough to utilize it. Modern hardware exemplifies
    this imbalance: an NVIDIA H100 delivers 989 TFLOPS but only 3.35 TB/s memory bandwidth
    ([Choquette 2023a](ch058.xhtml#ref-nvidia2022h100)), requiring 295 operations
    per byte to achieve full utilization—far exceeding the 1-10 operations per byte
    typical in neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The memory wall manifests through three critical constraints. First, the energy
    disparity—accessing DRAM consumes 640 pJ compared to 3.7 pJ for computation ([Horowitz
    2014](ch058.xhtml#ref-Horowitz2014)), creating a 173× energy penalty that often
    limits performance due to power budgets rather than computational capacity. Second,
    the bandwidth limitation—even TB/s memory systems cannot feed thousands of parallel
    compute units continuously, forcing 50-70% idle time in typical workloads. Third,
    the latency hierarchy—off-chip memory access requires hundreds of cycles, creating
    pipeline stalls that cascade through parallel execution units.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 11.5](ch017.xhtml#fig-compute-memory-imbalance), this
    “AI Memory Wall” continues to widen, making memory bandwidth rather than compute
    the primary constraint in AI acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: **AI Memory Wall**: The growing disparity between compute performance
    and memory bandwidth emphasizes the increasing challenge in sustaining peak computational
    efficiency due to memory constraints. Over the past 20 years, while computational
    capabilities have advanced rapidly, memory bandwidth has not kept pace, leading
    to potential bottlenecks in data-intensive applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond performance limitations, memory access imposes a significant energy cost.
    Fetching data from off-chip DRAM consumes far more energy than performing arithmetic
    operations ([Horowitz 2014](ch058.xhtml#ref-Horowitz2014)). This inefficiency
    is particularly evident in machine learning models, where large parameter sizes,
    frequent memory accesses, and non-uniform data movement patterns exacerbate memory
    bottlenecks. The energy differential drives architectural decisions—Google’s TPU
    achieves 30-83<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    better energy efficiency than contemporary GPUs by minimizing data movement through
    systolic arrays and large on-chip memory. These design choices demonstrate that
    energy constraints, not computational limits, often determine practical deployment
    feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Access Patterns in ML Workloads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine learning workloads place substantial demands on memory systems due to
    the large volume of data involved in computation. Unlike traditional compute-bound
    applications, where performance is often dictated by the speed of arithmetic operations,
    ML workloads are characterized by high data movement requirements. The efficiency
    of an accelerator is not solely determined by its computational throughput but
    also by its ability to continuously supply data to processing units without introducing
    stalls or delays.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network processes multiple types of data throughout its execution,
    each with distinct memory access patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model parameters (weights and biases)**: Machine learning models, particularly
    those used in large-scale applications such as natural language processing and
    computer vision, often contain millions to billions of parameters. Storing and
    accessing these weights efficiently is essential for maintaining throughput.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermediate activations**: During both training and inference, each layer
    produces intermediate results that must be temporarily stored and retrieved for
    subsequent operations. These activations can contribute significantly to memory
    overhead, particularly in deep architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradients (during training)**: Backpropagation requires storing and accessing
    gradients for every parameter, further increasing the volume of data movement
    between compute units and memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As models increase in size and complexity, improvements in memory capacity and
    bandwidth become essential. Although specialized compute units accelerate operations
    like matrix multiplications, their overall performance depends on the continuous,
    efficient delivery of data to the processing elements. In large-scale applications,
    such as natural language processing and computer vision, models often incorporate
    millions to billions of parameters ([T. B. Brown, Mann, Ryder, Subbiah, Kaplan,
    Dhariwal, Neelakantan, Shyam, Sastry, et al. 2020](ch058.xhtml#ref-Brown2020)).
    Consequently, achieving high performance necessitates minimizing delays and stalls
    caused by inefficient data movement between memory and compute units ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021); [Xingyu 2019](ch058.xhtml#ref-Huang2019)).
  prefs: []
  type: TYPE_NORMAL
- en: One way to quantify this challenge is by comparing the data transfer time with
    the time required for computations. Specifically, we define the memory transfer
    time as <semantics><mrow><msub><mi>T</mi><mtext mathvariant="normal">mem</mtext></msub><mo>=</mo><mfrac><msub><mi>M</mi><mtext
    mathvariant="normal">total</mtext></msub><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub></mfrac><mo>,</mo></mrow>
    <annotation encoding="application/x-tex">T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},</annotation></semantics>
    where <semantics><msub><mi>M</mi><mtext mathvariant="normal">total</mtext></msub><annotation
    encoding="application/x-tex">M_{\text{total}}</annotation></semantics> is the
    total data volume and <semantics><msub><mi>B</mi><mtext mathvariant="normal">mem</mtext></msub><annotation
    encoding="application/x-tex">B_{\text{mem}}</annotation></semantics> is the available
    memory bandwidth. In contrast, the compute time is given by <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">compute</mtext></msub><mo>=</mo><mfrac><mtext mathvariant="normal">FLOPs</mtext><msub><mi>P</mi><mtext
    mathvariant="normal">peak</mtext></msub></mfrac><mo>,</mo></mrow> <annotation
    encoding="application/x-tex">T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},</annotation></semantics>
    with the number of floating-point operations (FLOPs) divided by the peak hardware
    throughput, <semantics><msub><mi>P</mi><mtext mathvariant="normal">peak</mtext></msub><annotation
    encoding="application/x-tex">P_{\text{peak}}</annotation></semantics>. When <semantics><mrow><msub><mi>T</mi><mtext
    mathvariant="normal">mem</mtext></msub><mo>></mo><msub><mi>T</mi><mtext mathvariant="normal">compute</mtext></msub></mrow><annotation
    encoding="application/x-tex">T_{\text{mem}} > T_{\text{compute}}</annotation></semantics>,
    the system becomes memory-bound, meaning that the processing elements spend more
    time waiting for data than performing computations. This imbalance demonstrates
    the need for memory-optimized architectures and efficient data movement strategies
    to sustain high performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11.6](ch017.xhtml#fig-memory-wall) demonstrates the emerging challenge
    between model growth and hardware memory capabilities, illustrating the “AI Memory
    Wall.” The figure tracks AI model sizes (red dots) and hardware memory bandwidth
    (blue dots) over time on a log scale. Model parameters have grown exponentially,
    from AlexNet’s ~62.3M parameters in 2012 to Gemini 1’s trillion-scale parameters
    in 2023, as shown by the steeper red trend line. In contrast, hardware memory
    bandwidth, represented by successive generations of NVIDIA GPUs (~100-200 GB/s)
    and Google TPUs (~2-3 TB/s), has increased more gradually (blue trend line). The
    expanding shaded region between these trends corresponds to the “AI Memory Wall,”
    which will be an architectural challenge where model scaling outpaces available
    memory bandwidth. This growing disparity necessitates increasingly sophisticated
    memory management and model optimization techniques to maintain computational
    efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Irregular Memory Access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike traditional computing workloads, where memory access follows well-structured
    and predictable patterns, machine learning models often exhibit irregular memory
    access behaviors that make efficient data retrieval a challenge. These irregularities
    arise due to the nature of ML computations, where memory access patterns are influenced
    by factors such as batch size, layer type, and sparsity. As a result, standard
    caching mechanisms and memory hierarchies often struggle to optimize performance,
    leading to increased memory latency and inefficient bandwidth utilization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file185.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: **AI Memory Wall**: The figure emphasizes the growing disparity
    between model sizes and hardware memory bandwidths, illustrating the challenge
    in sustaining performance as models become more complex.'
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how ML workloads differ from traditional computing workloads,
    it is useful to compare their respective memory access patterns ([Table 11.8](ch017.xhtml#tbl-traditional-vs-ml-mem)).
    Traditional workloads, such as scientific computing, general-purpose CPU applications,
    and database processing, typically exhibit well-defined memory access characteristics
    that benefit from standard caching and prefetching techniques. ML workloads, on
    the other hand, introduce highly dynamic access patterns that challenge conventional
    memory optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: One key source of irregularity in ML workloads stems from batch size and execution
    order. The way input data is processed in batches directly affects memory reuse,
    creating a complex optimization challenge. Small batch sizes decrease the likelihood
    of reusing cached activations and weights, resulting in frequent memory fetches
    from slower, off-chip memory. Larger batch sizes can improve reuse and amortize
    memory access costs, but simultaneously place higher demands on available memory
    bandwidth, potentially creating congestion at different memory hierarchy levels.
    This delicate balance requires careful consideration of model architecture and
    available hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.8: **Memory Access Characteristics**: Traditional workloads exhibit
    predictable, sequential memory access, benefiting from standard caching, while
    machine learning workloads introduce irregular and dynamic patterns due to sparsity
    and data dependencies that challenge conventional memory optimization techniques.
    Understanding these differences is crucial for designing memory systems that efficiently
    support the unique demands of modern AI applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Traditional Computing Workloads** | **Machine Learning Workloads**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Access Pattern** | Regular and predictable (e.g., sequential reads,
    structured patterns) | Irregular and dynamic (e.g., sparsity, attention mechanisms)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Cache Locality** | High temporal and spatial locality | Often low locality,
    especially in large models |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Reuse** | Structured loops with frequent data reuse | Sparse and dynamic
    reuse depending on layer type |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Dependencies** | Well-defined dependencies allow efficient prefetching
    | Variable dependencies based on network structure |'
  prefs: []
  type: TYPE_TB
- en: '| **Workload Example** | Scientific computing (e.g., matrix factorizations,
    physics simulations) | Neural networks (e.g., CNNs, Transformers, sparse models)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Bottleneck** | DRAM latency, cache misses | Off-chip bandwidth constraints,
    memory fragmentation |'
  prefs: []
  type: TYPE_TB
- en: '| **Impact on Energy Consumption** | Moderate, driven by FLOP-heavy execution
    | High, dominated by data movement costs |'
  prefs: []
  type: TYPE_TB
- en: Different neural network layers interact with memory in distinct ways beyond
    batch size considerations. Convolutional layers benefit from spatial locality,
    as neighboring pixels in an image are processed together, enabling efficient caching
    of small weight kernels. Conversely, fully connected layers require frequent access
    to large weight matrices, often leading to more randomized memory access patterns
    that poorly align with standard caching policies. Transformers introduce additional
    complexity, as attention mechanisms demand accessing large key-value pairs stored
    across varied memory locations. The dynamic nature of sequence length and attention
    span renders traditional prefetching strategies ineffective, resulting in unpredictable
    memory latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant factor contributing to irregular memory access is sparsity[25](#fn25)
    in neural networks. Many modern ML models employ techniques such as weight pruning,
    activation sparsity, and structured sparsity to reduce computational overhead.
    However, these optimizations often lead to non-uniform memory access, as sparse
    representations necessitate fetching scattered elements rather than sequential
    blocks, making hardware caching less effective. Models that incorporate dynamic
    computation paths, such as Mixture of Experts and Adaptive Computation Time, introduce
    highly non-deterministic memory access patterns, where the active neurons or model
    components can vary with each inference step. This variability challenges efficient
    prefetching and caching strategies.
  prefs: []
  type: TYPE_NORMAL
- en: These irregularities have significant consequences. ML workloads often experience
    reduced cache efficiency, as activations and weights may not be accessed in predictable
    sequences. This leads to increased reliance on off-chip memory traffic, which
    slows down execution and consumes more energy. Irregular access patterns contribute
    to memory fragmentation, where the way data is allocated and retrieved results
    in inefficient utilization of available memory resources. The combined effect
    is that ML accelerators frequently encounter memory bottlenecks that limit their
    ability to fully utilize available compute power.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address the memory challenges in ML acceleration, hardware designers implement
    sophisticated memory hierarchies that balance speed, capacity, and energy efficiency.
    Understanding this hierarchy is essential before examining how different ML architectures
    utilize memory resources. Unlike general-purpose computing, where memory access
    patterns are often unpredictable, ML workloads exhibit structured reuse patterns
    that can be optimized through careful organization of data across multiple memory
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level, large-capacity but slow storage devices provide long-term
    model storage. At the lowest level, high-speed registers and caches ensure that
    compute units can access operands with minimal latency. Between these extremes,
    intermediate memory levels, such as scratchpad memory, high-bandwidth memory,
    and off-chip DRAM, offer trade-offs between performance and capacity.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.9](ch017.xhtml#tbl-memory-heirarchy) summarizes the key characteristics
    of different memory levels in modern AI accelerators. Each level in the hierarchy
    has distinct latency, bandwidth, and capacity properties, which directly influence
    how neural network data, such as weights, activations, and intermediate results,
    should be allocated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.9: **Memory Hierarchy Trade-Offs**: AI accelerators leverage a multi-level
    memory hierarchy to balance performance and capacity, optimizing data access for
    computationally intensive machine learning tasks. Each level provides distinct
    latency, bandwidth, and capacity characteristics that dictate how neural network
    components—weights, activations, and intermediate results—should be strategically
    allocated to minimize bottlenecks and maximize throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Memory Level** | **Approx. Latency** | **Bandwidth** | **Capacity** | **Example
    Use in Deep Learning** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Registers** | ~1 cycle | Highest | Few values | Storing operands for immediate
    computation |'
  prefs: []
  type: TYPE_TB
- en: '| **L1/L2 Cache (SRAM)** | ~1-10 ns | High | KBs-MBs | Caching frequently accessed
    activations and small weight blocks |'
  prefs: []
  type: TYPE_TB
- en: '| **Scratchpad Memory** | ~5-20 ns | High | MBs | Software-managed storage
    for intermediate computations |'
  prefs: []
  type: TYPE_TB
- en: '| **High-Bandwidth Memory (HBM)** | ~100 ns | Very High | GBs | Storing large
    model parameters and activations for high-speed access |'
  prefs: []
  type: TYPE_TB
- en: '| **Off-Chip DRAM (DDR, GDDR, LPDDR)** | ~50-150 ns | Moderate | GBs-TBs |
    Storing entire model weights that do not fit on-chip |'
  prefs: []
  type: TYPE_TB
- en: '| **Flash Storage (SSD/NVMe)** | ~100 µs - 1 ms | Low | TBs | Storing pre-trained
    models and checkpoints for later loading |'
  prefs: []
  type: TYPE_TB
- en: On-Chip Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each level of the memory hierarchy serves a distinct role in AI acceleration,
    with different trade-offs in speed, capacity, and accessibility. Registers, located
    within compute cores, provide the fastest access but can only store a few operands
    at a time. These are best utilized for immediate computations, where the operands
    needed for an operation can be loaded and consumed within a few cycles. However,
    because register storage is so limited, frequent memory accesses are required
    to fetch new operands and store intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the need for constant data movement between registers and external
    memory, small but fast caches serve as an intermediary buffer. These caches store
    recently accessed activations, weights, and intermediate values, ensuring that
    frequently used data remains available with minimal delay. However, the size of
    caches is limited, making them insufficient for storing full feature maps or large
    weight tensors in machine learning models. As a result, only the most frequently
    used portions of a model’s parameters or activations can reside here at any given
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For larger working datasets, many AI accelerators include scratchpad memory,
    which offers more storage than caches but with a crucial difference: it allows
    explicit software control over what data is stored and when it is evicted. Unlike
    caches, which rely on hardware-based eviction policies, scratchpad memory enables
    machine learning workloads to retain key values such as activations and filter
    weights for multiple layers of computation. This capability is particularly useful
    in models like convolutional neural networks, where the same input feature maps
    and filter weights are reused across multiple operations. By keeping this data
    in scratchpad memory rather than reloading it from external memory, accelerators
    can significantly reduce unnecessary memory transfers and improve overall efficiency
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).'
  prefs: []
  type: TYPE_NORMAL
- en: Off-Chip Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beyond on-chip memory, high-bandwidth memory provides rapid access to larger
    model parameters and activations that do not fit within caches or scratchpad buffers.
    HBM achieves its high performance by stacking multiple memory dies and using wide
    memory interfaces, allowing it to transfer large amounts of data with minimal
    latency compared to traditional DRAM. Because of its high bandwidth and lower
    latency, HBM is often used to store entire layers of machine learning models that
    must be accessed quickly during execution. However, its cost and power consumption
    limit its use primarily to high-performance AI accelerators, making it less common
    in power-constrained environments such as edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: When a machine learning model exceeds the capacity of on-chip memory and HBM,
    it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While DRAM offers
    significantly greater storage capacity, its access latency is higher, meaning
    that frequent retrievals from DRAM can introduce execution bottlenecks. To make
    effective use of DRAM, models must be structured so that only the necessary portions
    of weights and activations are retrieved at any given time, minimizing the impact
    of long memory fetch times.
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level of the hierarchy, flash storage and solid-state drives
    (SSDs) store large pre-trained models, datasets, and checkpointed weights. These
    storage devices offer large capacities but are too slow for real-time execution,
    requiring models to be loaded into faster memory tiers before computation begins.
    For instance, in training scenarios, checkpointed models stored in SSDs must be
    loaded into DRAM or HBM before resuming computation, as direct execution from
    SSDs would be too slow to maintain efficient accelerator utilization ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021)).
  prefs: []
  type: TYPE_NORMAL
- en: The memory hierarchy balances competing objectives of speed, capacity, and energy
    efficiency. However, moving data through multiple memory levels introduces bottlenecks
    that limit accelerator performance. Data transfers between memory levels incur
    latency costs, particularly for off-chip accesses. Limited bandwidth restricts
    data flow between memory tiers. Memory capacity constraints force constant data
    movement as models exceed local storage. These constraints make memory bandwidth
    the fundamental determinant of real-world accelerator performance.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Bandwidth and Architectural Trade-offs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on the memory wall analysis established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    this section quantifies how specific bandwidth characteristics impact system performance
    across different deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern accelerators exhibit distinct bandwidth-capacity trade-offs: NVIDIA
    H100 GPUs provide 3.35 TB/s HBM3 bandwidth with 80 GB capacity, optimizing for
    flexibility across diverse workloads. Google’s TPUv4 delivers 1.2 TB/s bandwidth
    with 128 MB on-chip memory, prioritizing energy efficiency for tensor operations.
    This 3:1 bandwidth advantage enables H100 to handle memory-intensive models like
    large language models, while TPU’s lower bandwidth suffices for compute-intensive
    inference due to superior data reuse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different neural network operations achieve varying bandwidth utilization:
    transformer attention mechanisms achieve only 20-40% of peak memory bandwidth
    due to irregular access patterns, convolutional layers achieve 60-85% through
    predictable spatial access patterns, and fully connected layers approach 90% when
    batch sizes exceed 128.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As established earlier, on-chip memory access consumes 5-10 pJ per access,
    while external DRAM requires 640 pJ per access—a 65-125<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> energy penalty. AI
    accelerators minimize DRAM access through three key strategies: weight stationarity
    (keeping model parameters in on-chip memory), input stationarity (buffering input
    activations locally), and output stationarity (accumulating partial sums on-chip).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory bandwidth scaling follows different trajectories across accelerator
    designs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU scaling**: Bandwidth increases linearly with memory channels, from 900
    GB/s (A100) to 3,350 GB/s (H100), enabling larger model support'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPU scaling**: Bandwidth optimization through systolic array design achieves
    900 GB/s with 35% lower power than GPU alternatives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mobile accelerator scaling**: Apple’s M3 Neural Engine achieves 400 GB/s
    unified memory bandwidth while consuming <5 W through aggressive voltage scaling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBM memory costs $8-15 per GB compared to $0.05 per GB for DDR5, creating 160-300<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> cost differences.
    High-bandwidth accelerators require 40-80 GB HBM for competitive performance,
    adding $320-1,200 to manufacturing costs. Edge accelerators sacrifice bandwidth
    (50-200 GB/s) to achieve sub-$100 cost targets while maintaining sufficient performance
    for inference workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'These bandwidth characteristics directly influence deployment decisions: cloud
    training prioritizes raw bandwidth for maximum model capacity, edge inference
    optimizes bandwidth efficiency for energy constraints, and mobile deployment balances
    bandwidth with cost limitations. While these hardware-specific optimizations are
    fundamental, the integrated system-level efficiency approaches that combine hardware
    acceleration with software optimization techniques are comprehensively covered
    in [Chapter 9](ch015.xhtml#sec-efficient-ai). The deployment of these optimizations
    across different system contexts—from mobile devices in [Chapter 2](ch008.xhtml#sec-ml-systems)
    to production workflows in [Chapter 13](ch019.xhtml#sec-ml-operations)—determines
    their real-world impact.'
  prefs: []
  type: TYPE_NORMAL
- en: Host-Accelerator Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning accelerators, such as GPUs and TPUs, achieve high computational
    throughput through parallel execution. However, their efficiency is fundamentally
    constrained by data movement between the host (CPU) and accelerator memory. Unlike
    general-purpose workloads that operate entirely within a CPU’s memory subsystem,
    AI workloads require frequent data transfers between CPU main memory and the accelerator,
    introducing latency, consuming bandwidth, and affecting overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Host-accelerator data movement follows a structured sequence, as illustrated
    in [Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement). Before computation
    begins, data is copied from CPU memory to the accelerator’s memory. The CPU then
    issues execution instructions, and the accelerator processes the data in parallel.
    Once computation completes, the results are stored in accelerator memory and transferred
    back to the CPU. Each step introduces potential inefficiencies that must be managed
    to optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file186.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: **Host-Accelerator Data Transfer**: AI workloads require frequent
    data movement between CPU memory and accelerators; this figure details the sequential
    steps of copying input data, executing computation, and transferring results,
    each introducing potential performance bottlenecks. Understanding this data transfer
    sequence is crucial for optimizing AI system performance and minimizing latency.'
  prefs: []
  type: TYPE_NORMAL
- en: The key challenges in host-accelerator data movement include latency, bandwidth
    constraints, and synchronization overheads. Optimizing data transfers through
    efficient memory management and interconnect technologies is essential for maximizing
    accelerator utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transfer Patterns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The efficiency of ML accelerators depends not only on their computational power
    but also on the continuous supply of data. Even high-performance GPUs and TPUs
    remain underutilized if data transfers are inefficient. Host and accelerator memory
    exist as separate domains, requiring explicit transfers over interconnects such
    as PCIe, NVLink, or proprietary links. Ineffective data movement can cause execution
    stalls, making transfer optimization critical.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement) illustrates this
    structured sequence. In step (1), data is copied from CPU memory to accelerator
    memory, as GPUs cannot directly access host memory at high speeds. A direct memory
    access (DMA)[26](#fn26) engine typically handles this transfer without consuming
    CPU cycles. In step (2), the CPU issues execution commands via APIs like CUDA,
    ROCm, or OpenCL. Step (3) involves parallel execution on the accelerator, where
    stalls can occur if data is not available when needed. Finally, in step (4), computed
    results are copied back to CPU memory for further processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Latency and bandwidth limitations significantly impact AI workloads. PCIe, with
    a peak bandwidth of 32 GB/s (PCIe 4.0), is much slower than an accelerator’s high-bandwidth
    memory, which can exceed 1 TB/s. Large data transfers exacerbate bottlenecks,
    particularly in deep learning tasks. Additionally, synchronization overheads arise
    when computation must wait for data transfers to complete. Efficient scheduling
    and overlapping transfers with execution are essential to mitigate these inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transfer Mechanisms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The movement of data between the host (CPU) and the accelerator (GPU, TPU, or
    other AI hardware) depends on the interconnect technology that links the two processing
    units. The choice of interconnect determines the bandwidth available for transfers,
    the latency of communication, and the overall efficiency of host-accelerator execution.
    The most commonly used transfer mechanisms include PCIe (Peripheral Component
    Interconnect Express), NVLink, Direct Memory Access, and Unified Memory Architectures.
    Each of these plays a crucial role in optimizing the four-step data movement process
    illustrated in [Figure 11.7](ch017.xhtml#fig-host-accelerator-data-movement).
  prefs: []
  type: TYPE_NORMAL
- en: PCIe Interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most accelerators communicate with the CPU via PCIe, the industry-standard interconnect
    for data movement. PCIe 4.0 provides up to 32 GB/s bandwidth, while PCIe 5.0 doubles
    this to 64 GB/s. However, this is still significantly lower than HBM bandwidth
    within accelerators, making PCIe a bottleneck for large AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: PCIe also introduces latency overheads due to its packet-based communication
    and memory-mapped I/O model. Frequent small transfers are inefficient, so batching
    data movement reduces overhead. Computation commands, issued over PCIe, further
    contribute to latency, requiring careful optimization of execution scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: NVLink Interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To address the bandwidth limitations of PCIe, NVIDIA developed NVLink, a proprietary
    high-speed interconnect that provides significantly higher bandwidth between GPUs
    and, in some configurations, between the CPU and GPU. Unlike PCIe, which operates
    as a shared bus, NVLink enables direct point-to-point communication between connected
    devices, reducing contention and improving efficiency for AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: For host-accelerator transfers, NVLink can be used in step (1) to transfer input
    data from main memory to GPU memory at speeds far exceeding PCIe, with bandwidths
    reaching up to 600 GB/s in NVLink 4.0\. This significantly reduces the data movement
    bottleneck, allowing accelerators to access input data with lower latency. In
    multi-GPU configurations, NVLink also accelerates peer-to-peer transfers, allowing
    accelerators to exchange data without routing through main memory, thereby optimizing
    step (3) of the computation process.
  prefs: []
  type: TYPE_NORMAL
- en: Although NVLink offers substantial performance benefits, it is not universally
    available. Unlike PCIe, which is an industry standard across all accelerators,
    NVLink is specific to NVIDIA hardware, limiting its applicability to systems designed
    with NVLink-enabled GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: DMA for Data Transfers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In conventional memory transfers, the CPU issues load/store instructions, consuming
    processing cycles. DMA offloads this task, enabling asynchronous data movement
    without CPU intervention.
  prefs: []
  type: TYPE_NORMAL
- en: During data transfers, the CPU initiates a DMA request, allowing data to be
    copied to accelerator memory in the background. Similarly, result transfers back
    to main memory occur without blocking execution. This enables overlapping computation
    with data movement, reducing idle time and improving accelerator utilization.
  prefs: []
  type: TYPE_NORMAL
- en: DMA is essential for enabling asynchronous data movement, which allows transfers
    to overlap with computation. Instead of waiting for transfers to complete before
    execution begins, AI workloads can stream data into the accelerator while earlier
    computations are still in progress, reducing idle time and improving accelerator
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Unified Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While PCIe, NVLink, and DMA optimize explicit memory transfers, some AI workloads
    require a more flexible memory model that eliminates the need for manual data
    copying. Unified Memory provides an abstraction that allows both the host and
    accelerator to access a single, shared memory space, automatically handling data
    movement when needed.
  prefs: []
  type: TYPE_NORMAL
- en: With Unified Memory, data does not need to be explicitly copied between CPU
    and GPU memory before execution. Instead, when a computation requires a memory
    region that is currently located in host memory, the system automatically migrates
    it to the accelerator, handling step (1) transparently. Similarly, when computed
    results are accessed by the CPU, step (4) occurs automatically, eliminating the
    need for manual memory management.
  prefs: []
  type: TYPE_NORMAL
- en: Although Unified Memory simplifies programming, it introduces performance trade-offs.
    Since memory migrations occur on demand, they can lead to unpredictable latencies,
    particularly if large datasets need to be transferred frequently. Additionally,
    since Unified Memory is implemented through page migration techniques, small memory
    accesses can trigger excessive data movement, further reducing efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: For AI workloads that require fine-grained memory control, explicit data transfers
    using PCIe, NVLink, and DMA often provide better performance. However, for applications
    where ease of development is more important than absolute speed, Unified Memory
    offers a convenient alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transfer Overheads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Host-accelerator data movement introduces overheads that impact AI workload
    execution. Unlike on-chip memory accesses, which occur at nanosecond latencies,
    host-accelerator transfers traverse system interconnects, adding latency, bandwidth
    constraints, and synchronization delays.
  prefs: []
  type: TYPE_NORMAL
- en: Interconnect latency affects transfer speed, with PCIe, the standard host-accelerator
    link, incurring significant overhead due to packet-based transactions and memory-mapped
    I/O. This makes frequent small transfers inefficient. Faster alternatives like
    NVLink reduce latency and improve bandwidth but are limited to specific hardware
    ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization delays further contribute to inefficiencies. Synchronous transfers
    block execution until data movement completes, ensuring data consistency but introducing
    idle time. Asynchronous transfers allow computation and data movement to overlap,
    reducing stalls but requiring careful coordination to avoid execution mismatches.
  prefs: []
  type: TYPE_NORMAL
- en: These factors, including interconnect latency, bandwidth limitations, and synchronization
    overheads, determine AI workload efficiency. While optimization techniques mitigate
    these limitations, understanding these fundamental transfer mechanics is essential
    for improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model Memory Pressure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models impose varying memory access patterns that significantly
    influence accelerator performance. The way data is transferred between the host
    and accelerator, how frequently memory is accessed, and the efficiency of caching
    mechanisms all determine overall execution efficiency. While multilayer perceptrons
    (MLPs), convolutional neural networks (CNNs), and transformer networks each require
    large parameter sets, their distinct memory demands necessitate tailored optimization
    strategies for accelerators. Understanding these differences provides insight
    into why different hardware architectures exhibit varying levels of efficiency
    across workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptrons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MLPs, also referred to as fully connected networks, are among the simplest neural
    architectures. Each layer consists of a dense matrix multiplication, requiring
    every neuron to interact with all neurons in the preceding layer. This results
    in high memory bandwidth demands, particularly for weights, as every input activation
    contributes to a large set of computations.
  prefs: []
  type: TYPE_NORMAL
- en: From a memory perspective, MLPs rely on large, dense weight matrices that frequently
    exceed on-chip memory capacity, necessitating off-chip memory accesses. Since
    accelerators cannot directly access host memory at high speed, data transfers
    must be explicitly managed via interconnects such as PCIe or NVLink. These transfers
    introduce latency and consume bandwidth, affecting execution efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their bandwidth-heavy nature, MLPs exhibit regular and predictable memory
    access patterns, making them amenable to optimizations such as prefetching and
    streaming memory accesses. Dedicated AI accelerators mitigate transfer overhead
    by staging weight matrices in fast SRAM caches and overlapping data movement with
    computation through direct memory access engines, reducing execution stalls. These
    optimizations allow accelerators to sustain high throughput even when handling
    large parameter sets ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) are widely used in image processing and
    computer vision tasks. Unlike MLPs, which require dense matrix multiplications,
    CNNs process input feature maps using small filter kernels that slide across the
    image. This localized computation structure results in high spatial data reuse,
    where the same input pixels contribute to multiple convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: CNN accelerators benefit from on-chip memory optimizations, as convolution filters
    exhibit extensive reuse, allowing weights to be stored in fast local SRAM instead
    of frequently accessing off-chip memory. However, activation maps require careful
    management due to their size. Since accessing main memory over interconnects like
    PCIe introduces latency and bandwidth bottlenecks, CNN accelerators employ tiling
    techniques to divide feature maps into smaller regions that fit within on-chip
    buffers. This minimizes costly external memory transfers, improving overall efficiency
    ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  prefs: []
  type: TYPE_NORMAL
- en: While CNN workloads are more memory-efficient than MLPs, managing intermediate
    activations remains a challenge. Accelerators use hierarchical caching strategies
    and DMA engines to optimize memory movement, ensuring that computations are not
    stalled by inefficient host-accelerator data transfers. These memory optimizations
    help CNN accelerators maintain high throughput by reducing reliance on off-chip
    memory bandwidth ([Y.-H. Chen, Emer, and Sze 2017](ch058.xhtml#ref-Chen2016)).
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformers have become the dominant architecture for natural language processing
    and are increasingly used in other domains such as vision and speech recognition.
    Unlike CNNs, which rely on local computations, transformers perform global attention
    mechanisms, where each token in an input sequence can interact with all other
    tokens. This leads to irregular and bandwidth-intensive memory access patterns,
    as large key-value matrices must be fetched and updated frequently.
  prefs: []
  type: TYPE_NORMAL
- en: These models are particularly challenging for accelerators due to their massive
    parameter sizes, which often exceed on-chip memory capacity. As a result, frequent
    memory transfers between host and accelerator introduce substantial latency overheads,
    particularly when relying on interconnects such as PCIe. Unified Memory architectures
    can mitigate some of these issues by dynamically handling data movement, but they
    introduce additional latency due to unpredictable on-demand memory migrations.
    Because transformers are memory-bound rather than compute-bound, accelerators
    optimized for them rely on high-bandwidth memory, tensor tiling, and memory partitioning
    to sustain performance ([T. B. Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
    Neelakantan, Shyam, Sastry, et al. 2020](ch058.xhtml#ref-Brown2020)).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, attention caching mechanisms and specialized tensor layouts reduce
    redundant memory fetches, improving execution efficiency. Given the bandwidth
    limitations of traditional interconnects, NVLink-enabled architectures offer significant
    advantages for large-scale transformer training, as they provide higher throughput
    and lower latency compared to PCIe. DMA-based asynchronous memory transfers enable
    overlapping computation with data movement, reducing execution stalls ([D. Narayanan
    et al. 2021a](ch058.xhtml#ref-Narayanan2021)).
  prefs: []
  type: TYPE_NORMAL
- en: ML Accelerators Implications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The diverse memory requirements of MLPs, CNNs, and Transformers highlight the
    need to tailor memory architectures to specific workloads. [Table 11.10](ch017.xhtml#tbl-model-mem-compare)
    compares the memory access patterns across these different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.10: **ML Model Memory Access**: Different machine learning models
    exhibit distinct memory access patterns and bottlenecks due to variations in weight
    size, activation reuse, and data sparsity; these characteristics significantly
    impact hardware accelerator design and performance optimization. Transformers
    demand high bandwidth and capacity due to their massive, sparsely accessed weights,
    while cnns benefit from spatial locality and high activation reuse, reducing memory
    pressure.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model Type** | **Weight Size** | **Activation Reuse** | **Memory Access
    Pattern** | **Primary Bottleneck** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **MLP (Dense)** | Large, dense | Low | Regular, sequential (streamed) | Bandwidth
    (off-chip) |'
  prefs: []
  type: TYPE_TB
- en: '| **CNN** | Small, reused | High | Spatial locality | Feature map movement
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Transformer** | Massive, sparse | Low | Irregular, high-bandwidth | Memory
    capacity + Interconnect |'
  prefs: []
  type: TYPE_TB
- en: Each model type presents unique challenges that directly impact accelerator
    design. MLPs benefit from fast streaming access to dense weight matrices, making
    memory bandwidth a critical factor in performance, especially when transferring
    large weights from host memory to accelerator memory. CNNs, with their high activation
    reuse and structured memory access patterns, can leverage on-chip caching and
    tiling strategies to minimize off-chip memory transfers. Transformers, however,
    impose significant demands on both bandwidth and capacity, as attention mechanisms
    require frequent access to large key-value matrices, leading to high interconnect
    traffic and increased memory pressure.
  prefs: []
  type: TYPE_NORMAL
- en: To address these challenges, modern AI accelerators incorporate multi-tier memory
    hierarchies that balance speed, capacity, and energy efficiency. On-chip SRAM
    caches and scratchpad memories store frequently accessed data, while high-bandwidth
    external memory provides scalability for large models. Efficient interconnects,
    such as NVLink, help alleviate host-accelerator transfer bottlenecks, particularly
    in transformer workloads where memory movement constraints can dominate execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As ML workloads continue to grow in complexity, memory efficiency becomes as
    critical as raw compute power. The analysis reveals how memory systems dominate
    accelerator performance: the 173× energy penalty for DRAM access creates a fundamental
    bottleneck, carefully structured memory hierarchies can improve effective bandwidth
    by 10-100×, and different neural network architectures create distinct memory
    pressure patterns. These constraints—from bandwidth limitations to communication
    overheads—determine whether theoretical computational capabilities translate into
    real-world performance. Having established how memory systems constrain accelerator
    effectiveness, we now examine how mapping strategies systematically address these
    limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Mapping Fundamentals for Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The memory system challenges examined in the previous section—bandwidth limitations,
    hierarchical access costs, and model-specific pressure patterns—directly determine
    how effectively neural networks execute on accelerators. A systolic array with
    1,200 GB/s on-chip bandwidth and sophisticated memory hierarchies delivers no
    performance benefit if computations are mapped without considering these memory
    access patterns. As established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    the extreme energy penalty for memory access means that mapping strategies must
    prioritize data reuse and locality above all other considerations. This reality
    drives the need for systematic mapping approaches that coordinate computation
    placement, memory allocation, and data movement to exploit hardware capabilities
    while respecting memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient execution of machine learning models on specialized AI acceleration
    hardware requires a structured approach to computation, ensuring that available
    resources are fully utilized while minimizing performance bottlenecks. These mapping
    considerations become particularly critical in distributed training scenarios,
    as explored in [Chapter 8](ch014.xhtml#sec-ai-training). Unlike general-purpose
    processors, which rely on dynamic task scheduling, AI accelerators operate under
    a structured execution model that maximizes throughput by carefully assigning
    computations to processing elements. This process, known as mapping, dictates
    how computations are distributed across hardware resources, influencing execution
    speed, memory access patterns, and overall efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '***Mapping in AI Acceleration*** is the process of assigning ML *computations*
    to *hardware units* through *spatial allocation*, *temporal scheduling*, and *memory-aware
    placement* to maximize execution efficiency and resource utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping machine learning models onto AI accelerators presents several challenges
    due to hardware constraints and the diversity of model architectures. Given the
    hierarchical memory system of modern accelerators, mapping strategies must carefully
    manage when and where data is accessed to minimize latency and power overhead
    while ensuring that compute units remain actively engaged. Poor mapping decisions
    can lead to underutilized compute resources, excessive data movement, and increased
    execution time, ultimately reducing overall efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the complexity of this challenge, consider an analogy: mapping
    a neural network to an accelerator is like planning a massive, factory-wide assembly
    process. You have thousands of workers (processing elements) and a complex set
    of tasks (computations). You must decide which worker does which task (computation
    placement), where to store the parts they need (memory allocation), and the exact
    sequence of operations to minimize time spent walking around (dataflow). A small
    change in the plan can lead to massive differences in factory output. Just as
    a poorly organized factory might have workers idle while others are overwhelmed,
    or materials stored too far from where they’re needed, a poorly mapped neural
    network can leave processing elements underutilized while creating memory bottlenecks
    that stall the entire system.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping encompasses three interrelated aspects that form the foundation of effective
    AI accelerator design.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computation Placement**: Systematically assigns operations (e.g., matrix
    multiplications, convolutions) to processing elements to maximize parallelism
    and reduce idle time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Allocation**: Carefully determines where model parameters, activations,
    and intermediate results reside within the memory hierarchy to optimize access
    efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataflow and Execution Scheduling**: Structures the movement of data between
    compute units to reduce bandwidth bottlenecks and ensure smooth, continuous execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective mapping strategies minimize off-chip memory accesses, maximize compute
    utilization, and efficiently manage data movement across different levels of the
    memory hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Role of the Compiler**'
  prefs: []
  type: TYPE_NORMAL
- en: Developers rarely perform this complex mapping manually. Instead, a specialized
    **compiler** (like NVIDIA’s NVCC or Google’s XLA) takes the high-level model from
    the framework and automatically explores the mapping search space to find an optimal
    execution plan for the target hardware. The compiler is the crucial software layer
    that translates the model’s computational graph into an efficient hardware-specific
    dataflow, balancing the three interrelated aspects of computation placement, memory
    allocation, and execution scheduling described above. This compiler support is
    examined in detail in [Section 11.7](ch017.xhtml#sec-ai-acceleration-compiler-support-172e).
  prefs: []
  type: TYPE_NORMAL
- en: The following sections explore the key mapping choices that influence execution
    efficiency and lay the groundwork for optimization strategies that refine these
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Placement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modern AI accelerators are designed to execute machine learning models with
    massive parallelism, using thousands to millions of processing elements to perform
    computations simultaneously. However, simply having many compute units is not
    enough. How computations are assigned to these units determines overall efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Without careful placement, some processing elements may sit idle while others
    are overloaded, leading to wasted resources, increased memory traffic, and reduced
    performance. Computation placement is the process of strategically mapping operations
    onto available hardware resources to sustain high throughput, minimize stalls,
    and optimize execution efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Placement Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AI accelerators contain thousands to millions of processing elements, making
    computation placement a large-scale problem. Modern GPUs, such as the NVIDIA H100,
    feature over 16,000 streaming processors and more than 500 specialized tensor
    cores, each designed to accelerate matrix operations ([Choquette 2023a](ch058.xhtml#ref-nvidia2022h100)).
    TPUs utilize systolic arrays composed of thousands of interconnected multiply-accumulate
    (MAC) units, while wafer-scale processors like Cerebras’ CS-2 push parallelism
    even further, integrating over 850,000 cores on a single chip ([Systems 2021b](ch058.xhtml#ref-Cerebras2021)).
    In these architectures, even minor inefficiencies in computation placement can
    lead to significant performance losses, as idle cores or excessive memory movement
    compound across the system.
  prefs: []
  type: TYPE_NORMAL
- en: Computation placement ensures that all processing elements contribute effectively
    to execution. This means that workloads must be distributed in a way that avoids
    imbalanced execution, where some processing elements sit idle while others remain
    overloaded. Similarly, placement must minimize unnecessary data movement, as excessive
    memory transfers introduce latency and power overheads that degrade system performance.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network computations vary significantly based on the model architecture,
    influencing how placement strategies are applied. For example, in a CNN, placement
    focuses on dividing image regions across processing elements to maximize parallelism.
    A <semantics><mrow><mn>256</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">256\times256</annotation></semantics>
    image processed through thousands of GPU cores might be broken into small tiles,
    each mapped to a different processing unit to execute convolutional operations
    simultaneously. In contrast, a transformer-based model requires placement strategies
    that accommodate self-attention mechanisms, where each token in a sequence interacts
    with all others, leading to irregular and memory-intensive computation patterns.
    Meanwhile, Graph Neural Networks (GNNs) introduce additional complexity, as computations
    depend on sparse and dynamic graph structures that require adaptive workload distribution
    ([Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)).
  prefs: []
  type: TYPE_NORMAL
- en: Because computation placement directly impacts resource utilization, execution
    speed, and power efficiency, it is one of the most critical factors in AI acceleration.
    A well-placed computation can reduce latency by orders of magnitude, while a poorly
    placed one can render thousands of processing units underutilized. The next section
    explores why efficient computation placement is essential and the consequences
    of suboptimal mapping strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Placement Importance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While computation placement is a hardware-driven process, its importance is
    fundamentally shaped by the structure of neural network workloads. Different types
    of machine learning models exhibit distinct computation patterns, which directly
    influence how efficiently they can be mapped onto accelerators. Without careful
    placement, workloads can become unbalanced, memory access patterns can become
    inefficient, and the overall performance of the system can degrade significantly.
  prefs: []
  type: TYPE_NORMAL
- en: For models with structured computation patterns, such as CNNs, computation placement
    is relatively straightforward. CNNs process images using filters that are applied
    to small, localized regions, meaning their computations can be evenly distributed
    across processing elements. Because these operations are highly parallelizable,
    CNNs benefit from spatial partitioning, where the input is divided into tiles
    that are processed independently. This structured execution makes CNNs well-suited
    for accelerators that favor regular dataflows, minimizing the complexity of placement
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: However, for models with irregular computation patterns, such as transformers
    and GNNs, computation placement becomes significantly more challenging. Transformers,
    which rely on self-attention mechanisms, require each token in a sequence to interact
    with all others, resulting in non-uniform computation demands. Unlike CNNs, where
    each processing element performs a similar amount of work, transformers introduce
    workload imbalance, where certain operations, including the computation of attention
    scores, require far more computation than others. Without careful placement, this
    imbalance can lead to stalls, where some processing elements remain idle while
    others struggle to keep up.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is even greater in graph neural networks (GNNs), where computation
    depends on sparse and dynamically changing graph structures. Unlike CNNs, which
    operate on dense and regularly structured data, GNNs must process nodes and edges
    with highly variable degrees of connectivity. Some regions of a graph may require
    significantly more computation than others, making workload balancing across processing
    elements difficult ([Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)). If computations
    are not placed strategically, some compute units will sit idle while others remain
    overloaded, leading to underutilization and inefficiencies in execution.
  prefs: []
  type: TYPE_NORMAL
- en: Poor computation placement adversely affects AI execution by creating workload
    imbalance, inducing excessive data movement, and causing execution stalls and
    bottlenecks. An uneven distribution of computations can lead to idle processing
    elements, preventing full hardware utilization and diminishing throughput. Inefficient
    execution assignment increases memory traffic by necessitating frequent data transfers
    between memory hierarchies, introducing latency and raising power consumption.
    Finally, such misallocation can cause operations to wait on data dependencies,
    resulting in pipeline inefficiencies that ultimately lower overall system performance.
  prefs: []
  type: TYPE_NORMAL
- en: Computation placement ensures that models execute efficiently given their unique
    computational structure. A well-placed workload reduces execution time, memory
    overhead, and power consumption, while a poorly placed one can lead to stalled
    execution pipelines and inefficient resource utilization. The next section explores
    the key considerations that must be addressed to ensure that computation placement
    is both efficient and adaptable to different model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Effective Computation Placement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computation placement is a balancing act between hardware constraints and workload
    characteristics. To achieve high efficiency, placement strategies must account
    for parallelism, memory access, and workload variability while ensuring that processing
    elements remain fully utilized. Poor placement leads to imbalanced execution,
    increased data movement, and performance degradation, making it essential to consider
    key factors when designing placement strategies.
  prefs: []
  type: TYPE_NORMAL
- en: As summarized in [Table 11.11](ch017.xhtml#tbl-placement-challenges), computation
    placement faces several critical challenges that impact execution efficiency.
    Effective mapping strategies must address these challenges by balancing workload
    distribution, minimizing data movement, and optimizing communication across processing
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.11: **Computation Placement Challenges**: Effective neural network
    deployment requires strategic allocation of computations to processing elements,
    balancing workload distribution, data movement costs, and hardware constraints
    to maximize execution efficiency and avoid performance bottlenecks. Understanding
    these challenges guides the design of mapping strategies that optimize resource
    utilization and minimize communication overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Challenge** | **Impact on Execution** | **Key Considerations for Placement**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Workload Imbalance** | Some processing elements finish early while others
    remain overloaded, leading to idle compute resources. | Distribute operations
    evenly to prevent stalls and ensure full utilization of PEs. |'
  prefs: []
  type: TYPE_TB
- en: '| **Irregular Computation Patterns** | Models like transformers and GNNs introduce
    non-uniform computation demands, making static placement difficult. | Use adaptive
    placement strategies that adjust execution based on workload characteristics.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Excessive Data Movement** | Frequent memory transfers introduce latency
    and increase power consumption. | Keep frequently used data close to the compute
    units and minimize off-chip memory accesses. |'
  prefs: []
  type: TYPE_TB
- en: '| **Limited Interconnect Bandwidth** | Poorly placed operations can create
    congestion, slowing data movement between PEs. | Optimize spatial and temporal
    placement to reduce communication overhead. |'
  prefs: []
  type: TYPE_TB
- en: '| **Model-Specific Execution Needs** | CNNs, transformers, and GNNs require
    different execution patterns, making a single placement strategy ineffective.
    | Tailor placement strategies to match the computational structure of each model
    type. |'
  prefs: []
  type: TYPE_TB
- en: 'Each of these challenges highlights a core trade-off in computation placement:
    maximizing parallelism while minimizing memory overhead. For CNNs, placement strategies
    prioritize structured tiling to maintain efficient data reuse. For transformers,
    placement must ensure balanced execution across attention layers. For GNNs, placement
    must dynamically adjust to sparse computation patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond model-specific needs, effective computation placement must also be scalable.
    As models grow in size and complexity, placement strategies must adapt dynamically
    rather than relying on static execution patterns. Future AI accelerators increasingly
    integrate runtime-aware scheduling mechanisms, where placement is optimized based
    on real-time workload behavior rather than predetermined execution plans.
  prefs: []
  type: TYPE_NORMAL
- en: Effective computation placement requires balancing hardware capabilities with
    model characteristics. The next section explores how computation placement interacts
    with memory allocation and data movement, ensuring that AI accelerators operate
    at peak efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Allocation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efficient memory allocation is essential for high-performance AI acceleration.
    As AI models grow in complexity, accelerators must manage vast amounts of data
    movement—loading model parameters, storing intermediate activations, and handling
    gradient computations. The way this data is allocated across the memory hierarchy
    directly affects execution efficiency, power consumption, and overall system throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Allocation Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While computation placement determines where operations are executed, memory
    allocation defines where data is stored and how it is accessed throughout execution.
    All AI accelerators rely on hierarchical memory systems, ranging from on-chip
    caches and scratchpads to HBM and DRAM. Poor memory allocation can lead to excessive
    off-chip memory accesses, increasing bandwidth contention and slowing down execution.
    Since AI accelerators operate at teraflop and petaflop scales, inefficient memory
    access patterns can result in substantial performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of memory allocation is to minimize latency and reduce power
    consumption by keeping frequently accessed data as close as possible to the processing
    elements. Different hardware architectures implement memory hierarchies tailored
    for AI workloads. GPUs rely on a mix of global memory, shared memory, and registers,
    requiring careful tiling strategies to optimize locality ([X. Qi, Kantarci, and
    Liu 2017](ch058.xhtml#ref-nvidia2020ampere)). TPUs use on-chip SRAM scratchpads,
    where activations and weights must be efficiently preloaded to sustain systolic
    array execution ([Norman P. Jouppi et al. 2017c](ch058.xhtml#ref-jouppi_tpu_2017)).
    Wafer-scale processors, with their hundreds of thousands of cores, demand sophisticated
    memory partitioning strategies to avoid excessive interconnect traffic ([Systems
    2021b](ch058.xhtml#ref-Cerebras2021)). In all cases, the effectiveness of memory
    allocation determines the overall throughput, power efficiency, and scalability
    of AI execution.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocation directly impacts AI acceleration efficiency through data storage
    and access patterns. Unlike general-purpose computing, where memory management
    is abstracted by caches and dynamic allocation, AI accelerators require explicit
    data placement strategies to sustain high throughput and avoid unnecessary stalls.
    This is particularly evident in systolic arrays ([Figure 11.4](ch017.xhtml#fig-systolic-array)),
    where the rhythmic data flow between processing elements depends on precisely
    timed memory access patterns. In TPU’s systolic arrays, for instance, weights
    must be preloaded into on-chip scratchpads and streamed through the array in perfect
    synchronization with input activations to maintain the pipelined computation flow.
    When memory is not allocated efficiently, AI workloads suffer from latency overhead,
    excessive power consumption, and bottlenecks that limit computational performance.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Challenges for Different Workloads
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neural network architectures have varying memory demands, which influence the
    importance of proper allocation. CNNs rely on structured and localized data access
    patterns, meaning that inefficient memory allocation can lead to redundant data
    loads and cache inefficiencies ([Y.-H. Chen et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
    In contrast, transformer models require frequent access to large model parameters
    and intermediate activations, making them highly sensitive to memory bandwidth
    constraints. GNNs introduce even greater challenges, as their irregular and sparse
    data structures result in unpredictable memory access patterns that can lead to
    inefficient use of memory resources. Poor memory allocation has three major consequences
    for AI execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased Memory Latency**: When frequently accessed data is not stored in
    the right location, accelerators must retrieve it from higher-latency memory,
    slowing down execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Higher Power Consumption**: Off-chip memory accesses consume significantly
    more energy than on-chip storage, leading to inefficiencies at scale.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reduced Computational Throughput**: If data is not available when needed,
    processing elements remain idle, reducing the overall performance of the system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As AI models continue to grow in size and complexity, the importance of scalable
    and efficient memory allocation increases. Memory limitations can dictate how
    large of a model can be deployed on a given accelerator, affecting feasibility
    and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.12: **Memory Allocation Challenges**: Efficient memory management
    in AI accelerators balances data access speed with hardware constraints, mitigating
    performance bottlenecks caused by latency, bandwidth limitations, and irregular
    data patterns. Addressing these challenges is critical for deploying complex models,
    such as transformers and graphs, which have variable and demanding memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Challenge** | **Impact on Execution** | **Key Considerations for Allocation**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **High Memory Latency** | Slow data access delays execution and reduces throughput.
    | Prioritize placing frequently accessed data in faster memory locations. |'
  prefs: []
  type: TYPE_TB
- en: '| **Limited On-Chip Storage** | Small local memory constrains the amount of
    data available near compute units. | Allocate storage efficiently to maximize
    data availability without exceeding hardware limits. |'
  prefs: []
  type: TYPE_TB
- en: '| **High Off-Chip Bandwidth Demand** | Frequent access to external memory increases
    delays and power consumption. | Reduce unnecessary memory transfers by carefully
    managing when and how data is moved. |'
  prefs: []
  type: TYPE_TB
- en: '| **Irregular Memory Access Patterns** | Some models require accessing data
    unpredictably, leading to inefficient memory usage. | Organize memory layout to
    align with access patterns and minimize unnecessary data movement. |'
  prefs: []
  type: TYPE_TB
- en: '| **Model-Specific Memory Needs** | Different models require different allocation
    strategies to optimize performance. | Tailor allocation decisions based on the
    structure and execution characteristics of the workload. |'
  prefs: []
  type: TYPE_TB
- en: As summarized in [Table 11.12](ch017.xhtml#tbl-memory-allocation), memory allocation
    in AI accelerators must address several key challenges that influence execution
    efficiency. Effective allocation strategies mitigate high latency, bandwidth limitations,
    and irregular access patterns by carefully managing data placement and movement.
    Ensuring that frequently accessed data is stored in faster memory locations while
    minimizing unnecessary transfers is essential for maintaining performance and
    energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these challenges requires careful memory management to balance execution
    efficiency with hardware constraints. While structured models may benefit from
    well-defined memory layouts that facilitate predictable access, others, like transformer-based
    and graph-based models, require more adaptive allocation strategies to handle
    variable and complex memory demands. Beyond workload-specific considerations,
    memory allocation must also be scalable. As model sizes continue to grow, accelerators
    must dynamically manage memory resources rather than relying on static allocation
    schemes. Ensuring that frequently used data is accessible when needed without
    overwhelming memory capacity is essential for maintaining high efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Combinatorial Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The efficient execution of machine learning models on AI accelerators requires
    careful consideration of placement and allocation. Placement involves spatial
    assignment of computations and data, while allocation covers temporal distribution
    of resources. These decisions are interdependent, and each introduces trade-offs
    that impact performance, energy efficiency, and scalability. [Table 11.13](ch017.xhtml#tbl-combinatorial-complexity)
    outlines the fundamental trade-offs between computation placement and resource
    allocation in AI accelerators. Placement decisions influence parallelism, memory
    access patterns, and communication overhead, while allocation strategies determine
    how resources are distributed over time to balance execution efficiency. The interplay
    between these factors shapes overall performance, requiring a careful balance
    to avoid bottlenecks such as excessive synchronization, memory congestion, or
    underutilized compute resources. Optimizing these trade-offs is essential for
    ensuring that AI accelerators operate at peak efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these dimensions requires balancing trade-offs between placement and
    allocation. For instance, spatially distributing computations across multiple
    processing elements can increase throughput; however, if data allocation is not
    optimized, memory bandwidth limitations may introduce bottlenecks. Likewise, allocating
    resources for fine-grained computations may enhance flexibility but, without appropriate
    placement strategies, may lead to excessive synchronization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.13: **Placement-Allocation Trade-Offs**: AI accelerator performance
    depends on strategically mapping computations to hardware and allocating resources
    over time, balancing parallelism, memory access, and execution efficiency to avoid
    bottlenecks. Careful consideration of these interdependent factors is essential
    for maximizing throughput and minimizing energy consumption in machine learning
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dimension** | **Placement Considerations** | **Allocation Considerations**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Computational Granularity** | Fine-grained placement enables greater parallelism
    but increases synchronization overhead. | Coarse-grained allocation reduces synchronization
    overhead but may limit flexibility. |'
  prefs: []
  type: TYPE_TB
- en: '| **Spatial vs. Temporal Mapping** | Spatial placement enhances parallel execution
    but can lead to resource contention and memory congestion. | Temporal allocation
    balances resource sharing but may reduce overall throughput. |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory and Data Locality** | Placing data closer to compute units minimizes
    latency but may reduce overall memory availability. | Allocating data across multiple
    memory levels increases capacity but introduces higher access costs. |'
  prefs: []
  type: TYPE_TB
- en: '| **Communication and Synchronization** | Co-locating compute units reduces
    communication latency but may introduce contention. | Allocating synchronization
    mechanisms mitigates stalls but can introduce additional overhead. |'
  prefs: []
  type: TYPE_TB
- en: '| **Dataflow and Execution Ordering** | Static placement simplifies execution
    but limits adaptability to workload variations. | Dynamic allocation improves
    adaptability but adds scheduling complexity. |'
  prefs: []
  type: TYPE_TB
- en: Because AI accelerator architectures impose constraints on both where computations
    execute and how resources are assigned over time, selecting an effective mapping
    strategy necessitates a coordinated approach to placement and allocation. Understanding
    how these trade-offs influence execution efficiency is essential for optimizing
    performance on AI accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Configuration Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The efficiency of AI accelerators is determined not only by their computational
    capabilities but also by how neural network computations are mapped to hardware
    resources. Mapping defines how computations are assigned to processing elements,
    how data is placed and moved through the memory hierarchy, and how execution is
    scheduled. The choices made in this process significantly impact performance,
    influencing compute utilization, memory bandwidth efficiency, and energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping machine learning models to hardware presents a large and complex design
    space. Unlike traditional computational workloads, model execution involves multiple
    interacting factors, including computation, data movement, parallelism, and scheduling,
    each introducing constraints and trade-offs. The hierarchical memory structure
    of accelerators, as discussed in the Memory Systems section, further complicates
    this process by imposing limits on bandwidth, latency, and data reuse. As a result,
    effective mapping strategies must carefully balance competing objectives to maximize
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the heart of this design space lie three interconnected aspects: data placement,
    computation scheduling, and data movement timing. Data placement refers to the
    allocation of data across various memory hierarchies, such as on-chip buffers,
    caches, and off-chip DRAM, and its effective management is critical because it
    influences both latency and energy consumption. Inefficient placement often results
    in frequent, costly memory accesses, whereas strategic placement ensures that
    data used regularly remains in fast-access storage. Computation scheduling governs
    the order in which operations execute, impacting compute efficiency and memory
    access patterns; for instance, some execution orders may optimize parallelism
    while introducing synchronization overheads, and others may improve data locality
    at the expense of throughput. Meanwhile, timing in data movement is equally essential,
    as transferring data between memory levels incurs significant latency and energy
    costs. Efficient mapping strategies thus focus on minimizing unnecessary transfers
    by reusing data and overlapping communication with computation to enhance overall
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: These factors define a vast combinatorial design space, where small variations
    in mapping decisions can lead to large differences in performance and energy efficiency.
    A poor mapping strategy can result in underutilized compute resources, excessive
    data movement, or imbalanced workloads, creating bottlenecks that degrade overall
    efficiency. Conversely, a well-designed mapping maximizes both throughput and
    resource utilization, making efficient use of available hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the interconnected nature of mapping decisions, there is no single
    optimal solution—different workloads and hardware architectures demand different
    approaches. The next sections examine the structure of this design space and how
    different mapping choices shape the execution of machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping machine learning computations onto specialized hardware requires balancing
    multiple constraints, including compute efficiency, memory bandwidth, and execution
    scheduling. The challenge arises from the vast number of possible ways to assign
    computations to processing elements, order execution, and manage data movement.
    Each decision contributes to a high-dimensional search space, where even minor
    variations in mapping choices can significantly impact performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike traditional workloads with predictable execution patterns, machine learning
    models introduce diverse computational structures that require flexible mappings
    adapted to data reuse, parallelization opportunities, and memory constraints.
    The search space grows combinatorially, making exhaustive search infeasible. To
    understand this complexity, three sources emerge of variation:'
  prefs: []
  type: TYPE_NORMAL
- en: Ordering Computation and Execution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine learning workloads are often structured as nested loops, iterating over
    various dimensions of computation. For instance, a matrix multiplication kernel
    may loop over batch size (<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>),
    input features (<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>),
    and output features (<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>).
    The order in which these loops execute has a profound effect on data locality,
    reuse patterns, and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of ways to arrange <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    loops follows a factorial growth pattern: <semantics><mrow><mi>𝒪</mi><mo>=</mo><mi>d</mi><mi>!</mi></mrow>
    <annotation encoding="application/x-tex">\mathcal{O} = d!</annotation></semantics>
    which scales rapidly. A typical convolutional layer may involve up to seven loop
    dimensions, leading to: <semantics><mrow><mn>7</mn><mi>!</mi><mo>=</mo><mn>5</mn><mo>,</mo><mn>040</mn>
    <mrow><mtext mathvariant="normal">possible execution orders.</mtext></mrow></mrow>
    <annotation encoding="application/x-tex">7! = 5,040 \text{ possible execution
    orders.}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering multiple memory levels, the search space expands as: <semantics><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mi>!</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup>
    <annotation encoding="application/x-tex">(d!)^l</annotation></semantics> where
    <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    is the number of memory hierarchy levels. This rapid expansion highlights why
    execution order optimization is crucial—poor loop ordering can lead to excessive
    memory traffic, while an optimized order improves cache utilization ([Sze et al.
    2017a](ch058.xhtml#ref-sze2017efficient)).'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization Across Processing Elements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern AI accelerators leverage thousands of processing elements to maximize
    parallelism, but determining which computations should be parallelized is non-trivial.
    Excessive parallelization can introduce synchronization overheads and increased
    bandwidth demands, while insufficient parallelization leads to underutilized hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of ways to distribute computations among parallel units follows
    the binomial coefficient: <semantics><mrow><mi>𝒫</mi><mo>=</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\mathcal{P}
    = \frac{d!}{(d-k)!}</annotation></semantics> where <semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics> is the number of loops,
    and <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    is the number selected for parallel execution. For a six-loop computation where
    three loops are chosen for parallel execution, the number of valid configurations
    is: <semantics><mrow><mfrac><mrow><mn>6</mn><mi>!</mi></mrow><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>6</mn><mo>−</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo>=</mo><mn>120</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">\frac{6!}{(6-3)!} = 120.</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: Even for a single layer, there can be hundreds of valid parallelization strategies,
    each affecting data synchronization, memory contention, and overall compute efficiency.
    Expanding this across multiple layers and model architectures further magnifies
    the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Placement and Data Movement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The hierarchical memory structure of AI accelerators introduces additional constraints,
    as data must be efficiently placed across registers, caches, shared memory, and
    off-chip DRAM. Data placement impacts latency, bandwidth consumption, and energy
    efficiency—frequent access to slow memory creates bottlenecks, while optimized
    placement reduces costly memory transfers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of ways to allocate data across memory levels follows an exponential
    growth function: <semantics><mrow><mi>ℳ</mi><mo>=</mo><msup><mi>n</mi><mrow><mi>d</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{M} = n^{d \times l}</annotation></semantics>
    where:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    = number of placement choices per level,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    = number of computational dimensions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    = number of memory hierarchy levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a model with:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>d</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">d
    = 5</annotation></semantics> computational dimensions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>l</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">l
    = 3</annotation></semantics> memory levels,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n
    = 4</annotation></semantics> possible placement choices per level,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the number of possible memory allocations is: <semantics><mrow><msup><mn>4</mn><mrow><mn>5</mn><mo>×</mo><mn>3</mn></mrow></msup><mo>=</mo><msup><mn>4</mn><mn>15</mn></msup><mo>=</mo><mn>1</mn><mo>,</mo><mn>073</mn><mo>,</mo><mn>741</mn><mo>,</mo><mn>824</mn><mi>.</mi></mrow>
    <annotation encoding="application/x-tex">4^{5 \times 3} = 4^{15} = 1,073,741,824.</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: This highlights how even a single layer may have over a billion possible memory
    configurations, making manual optimization impractical.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Search Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By combining the complexity from computation ordering, parallelization, and
    memory placement, the total mapping search space can be approximated as: <semantics><mrow><mi>𝒮</mi><mo>=</mo><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mi>d</mi></msup><mo>×</mo><mi>d</mi><mi>!</mi><mo>×</mo><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>l</mi></msup></mrow>
    <annotation encoding="application/x-tex">\mathcal{S} = \left( n^d \times d! \times
    \frac{d!}{(d-k)!} \right)^l</annotation></semantics> where:'
  prefs: []
  type: TYPE_NORMAL
- en: <semantics><msup><mi>n</mi><mi>d</mi></msup><annotation encoding="application/x-tex">n^d</annotation></semantics>
    represents memory placement choices,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mrow><mi>d</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">d!</annotation></semantics>
    accounts for computation ordering choices,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mfrac><mrow><mi>d</mi><mi>!</mi></mrow><mrow><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>d</mi><mo>−</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><annotation
    encoding="application/x-tex">\frac{d!}{(d-k)!}</annotation></semantics> captures
    parallelization possibilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    is the number of memory hierarchy levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This equation illustrates the exponential growth of the search space, making
    brute-force search infeasible for all but the simplest cases.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow Optimization Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mapping strategies establish *where* computations execute and *where* data resides
    within an accelerator’s architecture, but they do not specify *how* data flows
    through processing elements during execution. A systolic array might process a
    matrix multiplication with weights stored in local memory, but the order in which
    weights, inputs, and outputs move through the array fundamentally determines memory
    bandwidth consumption and energy efficiency. These dataflow patterns—termed optimization
    strategies—represent the critical implementation dimension that translates abstract
    mapping decisions into concrete execution plans.
  prefs: []
  type: TYPE_NORMAL
- en: The choice among weight-stationary, input-stationary, and output-stationary
    approaches directly impacts whether an accelerator operates in the compute-bound
    or memory-bound region. Understanding these trade-offs is essential because compilers
    ([Section 11.7](ch017.xhtml#sec-ai-acceleration-compiler-support-172e)) and runtime
    systems ([Section 11.8](ch017.xhtml#sec-ai-acceleration-runtime-support-f94f))
    must select appropriate dataflow patterns based on computational characteristics
    and memory hierarchy capabilities analyzed in [Section 11.4.2](ch017.xhtml#sec-ai-acceleration-memory-hierarchy-1839).
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently mapping machine learning computations onto hardware is a complex
    challenge due to the vast number of possible configurations. As models grow in
    complexity, the number of potential mappings increases exponentially. Even for
    a single layer, there are thousands of ways to order computation loops, hundreds
    of parallelization strategies, and an exponentially growing number of memory placement
    choices. This combinatorial explosion makes exhaustive search impractical.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, AI accelerators rely on structured mapping strategies
    that systematically balance computational efficiency, data locality, and parallel
    execution. Rather than evaluating every possible configuration, these approaches
    use a combination of heuristic, analytical, and machine learning-based techniques
    to find high-performance mappings efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The key to effective mapping lies in understanding and applying a set of core
    techniques that optimize data movement, memory access, and computation. These
    building blocks of mapping strategies provide a structured foundation for efficient
    execution, explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building Blocks of Mapping Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To navigate the complexity of mapping decisions, a set of foundational techniques
    is leveraged that optimizes execution across data movement, memory access, and
    computation efficiency. These techniques provide the necessary structure for mapping
    strategies that maximize hardware performance while minimizing bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Key techniques include data movement strategies, which determine where data
    is staged during computation in order to reduce redundant transfers, such as in
    weight stationary, output stationary, and input stationary approaches. Memory-aware
    tensor layouts also play an important role by influencing memory access patterns
    and cache efficiency through the organization of data in formats such as row-major
    or channel-major.
  prefs: []
  type: TYPE_NORMAL
- en: Other strategies involve kernel fusion, a method that minimizes redundant memory
    writes by combining multiple operations into a single computational step. Tiling
    is employed as a technique that partitions large computations into smaller, memory-friendly
    blocks to improve cache efficiency and reduce memory bandwidth requirements. Finally,
    balancing computation and communication is essential for managing the trade-offs
    between parallel execution and memory access to achieve high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these building blocks plays a crucial role in structuring high-performance
    execution, forming the basis for both heuristic and model-driven optimization
    techniques. The next section explores how these strategies are adapted to different
    types of AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Data Movement Patterns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While computational mapping determines where and when operations occur, its
    success depends heavily on how efficiently data is accessed and transferred across
    the memory hierarchy. Unlike traditional computing workloads, which often exhibit
    structured and predictable memory access patterns, machine learning workloads
    present irregular access behaviors due to frequent retrieval of weights, activations,
    and intermediate values.
  prefs: []
  type: TYPE_NORMAL
- en: Even when computational units are mapped efficiently, poor data movement strategies
    can severely degrade performance, leading to frequent memory stalls and underutilized
    hardware resources. If data cannot be supplied to processing elements at the required
    rate, computational units remain idle, increasing latency, memory traffic, and
    energy consumption ([Y.-H. Chen et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the impact of data movement inefficiencies, consider a typical
    matrix multiplication operation shown in [Listing 11.18](ch017.xhtml#lst-matmul_data_movement),
    which forms the backbone of many machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.18: **Matrix Multiplication**: Data movement bottlenecks can lead
    to underutilized hardware resources, illustrating the importance of efficient
    data flow in optimizing machine learning model performance. Via This operation'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This computation reveals several critical dataflow challenges. The first challenge
    is the number of memory accesses required. For each output <semantics><mrow><mi>Z</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true"
    form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,
    j]</annotation></semantics>, the computation must fetch an entire row of weights
    from the weight matrix and a full column of activations from the input matrix.
    Since the weight matrix contains 512 rows and the input matrix contains 32 columns,
    this results in repeated memory accesses that place a significant burden on memory
    bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge comes from weight reuse. The same weights are applied to
    multiple inputs, meaning that an ideal mapping strategy should maximize weight
    locality to avoid redundant memory fetches. Without proper reuse, the accelerator
    would waste bandwidth loading the same weights multiple times ([Tianqi et al.
    2018](ch058.xhtml#ref-chen2018tvm)).
  prefs: []
  type: TYPE_NORMAL
- en: The third challenge involves the accumulation of intermediate results. Since
    each element in <semantics><mrow><mi>Z</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Z[i,j]</annotation></semantics>
    requires contributions from 256 different weight-input pairs, partial sums must
    be stored and retrieved before the final value is computed. If these intermediate
    values are stored inefficiently, the system will require frequent memory accesses,
    further increasing bandwidth demands.
  prefs: []
  type: TYPE_NORMAL
- en: A natural way to mitigate these challenges is to leverage SIMD and SIMT execution
    models, which allow multiple values to be fetched in parallel. However, even with
    these optimizations, data movement remains a bottleneck. The issue is not just
    how quickly data is retrieved but how often it must be moved and where it is placed
    within the memory hierarchy ([Han et al. 2016](ch058.xhtml#ref-han2016eie)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that data movement is 100-1000× more expensive than computation, the
    single most important goal of an accelerator is to minimize memory access. Dataflow
    strategies are the architectural patterns designed to achieve this by maximizing
    data reuse. The question is: which data is most valuable to keep local? This directly
    addresses the AI Memory Wall challenge examined in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    where the extreme energy penalty for memory access dominates system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these constraints, accelerators implement dataflow strategies that
    determine which data remains fixed in memory and which data is streamed dynamically.
    These strategies represent different answers to the fundamental question of data
    locality: weight-stationary keeps model parameters local, input-stationary maintains
    activation data, and output-stationary preserves intermediate results. Each approach
    trades off different memory access patterns to maximize data reuse and minimize
    the energy-intensive transfers that constitute the primary bottleneck in AI acceleration.'
  prefs: []
  type: TYPE_NORMAL
- en: Weight Stationary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Weight Stationary strategy keeps weights fixed in local memory, while input
    activations and partial sums are streamed through the system. Weight stationary
    approaches prove particularly beneficial in CNNs and matrix multiplications, where
    the same set of weights is applied across multiple inputs. By ensuring weights
    remain stationary, this method reduces redundant memory fetches, which helps alleviate
    bandwidth bottlenecks and improves energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of the weight stationary approach is that it maximizes weight
    reuse, reducing the frequency of memory accesses to external storage. Since weight
    parameters are often shared across multiple computations, keeping them in local
    memory eliminates unnecessary data movement, lowering the overall energy cost
    of computation. This makes it particularly effective for architectures where weights
    represent the dominant memory overhead, such as systolic arrays and custom accelerators
    designed for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: A simplified Weight Stationary implementation for matrix multiplication is illustrated
    in [Listing 11.19](ch017.xhtml#lst-weight_stationary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.19: **Weight Stationary Matrix Multiplication**: Weight stationary
    matrix multiplication keeps weights fixed in local memory while input activations
    stream through, demonstrating how it maximizes weight reuse to reduce energy costs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In weight stationary execution, weights are loaded once into local memory and
    remain fixed throughout the computation, while inputs are streamed dynamically,
    thereby reducing redundant memory accesses. At the same time, partial sums are
    accumulated in an efficient manner that minimizes unnecessary data movement, ensuring
    that the system maintains high throughput and energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: By keeping weights fixed in local storage, memory bandwidth requirements are
    significantly reduced, as weights do not need to be reloaded for each new computation.
    Instead, the system efficiently reuses the stored weights across multiple input
    activations, allowing for high throughput execution. This makes weight stationary
    dataflow highly effective for workloads with heavy weight reuse patterns, such
    as CNNs and matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: However, while this strategy reduces weight-related memory traffic, it introduces
    trade-offs in input and output movement. Since inputs must be streamed dynamically
    while weights remain fixed, the efficiency of this approach depends on how well
    input activations can be delivered to the computational units without causing
    stalls. Additionally, partial sums, which represent intermediate results, must
    be carefully accumulated to avoid excessive memory traffic. The total performance
    gain depends on the size of available on-chip memory, as storing larger weight
    matrices locally can become a constraint in models with millions or billions of
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The weight stationary strategy is well-suited for workloads where weights exhibit
    high reuse and memory bandwidth is a limiting factor. It is commonly employed
    in CNNs, systolic arrays, and matrix multiplication kernels, where structured
    weight reuse leads to significant performance improvements. However, for models
    where input or output reuse is more critical, alternative dataflow strategies,
    such as output stationary or input stationary, may provide better trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Output Stationary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Output Stationary strategy keeps partial sums fixed in local memory, while
    weights and input activations stream through the system. This approach is particularly
    effective for fully connected layers, systolic arrays, and other operations where
    an output element accumulates contributions from multiple weight-input pairs.
    By keeping partial sums stationary, this method reduces redundant memory writes,
    minimizing bandwidth consumption and improving energy efficiency ([Y.-H. Chen
    et al. 2016](ch058.xhtml#ref-chen2016eyeriss)).
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of the output stationary approach is that it optimizes accumulation
    efficiency, ensuring that each output element is computed as efficiently as possible
    before being written to memory. Unlike Weight Stationary, which prioritizes weight
    reuse, Output Stationary execution is designed to minimize memory bandwidth overhead
    caused by frequent writes of intermediate results. This makes it well-suited for
    workloads where accumulation dominates the computational pattern, such as fully
    connected layers and matrix multiplications in transformer-based models.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.20](ch017.xhtml#lst-output_stationary) shows a simplified Output
    Stationary implementation for matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.20: **Output Stationary Execution**: Accumulates partial sums locally
    to reduce memory writes and enhance efficiency during matrix multiplication, making
    it ideal for transformer-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation follows the core principles of output stationary execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Partial sums are kept in local memory throughout the computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights and inputs are streamed dynamically, ensuring that intermediate results
    remain locally accessible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final outputs are written back to memory only once, reducing unnecessary memory
    traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By accumulating partial sums locally, this approach eliminates excessive memory
    writes, improving overall system efficiency. In architectures such as systolic
    arrays, where computation progresses through a grid of processing elements, keeping
    partial sums stationary aligns naturally with structured accumulation workflows,
    reducing synchronization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: However, while Output Stationary reduces memory write traffic, it introduces
    trade-offs in weight and input movement. Since weights and activations must be
    streamed dynamically, the efficiency of this approach depends on how well data
    can be fed into the system without causing stalls. Additionally, parallel implementations
    must carefully synchronize updates to partial sums, especially in architectures
    where multiple processing elements contribute to the same output.
  prefs: []
  type: TYPE_NORMAL
- en: The Output Stationary strategy is most effective for workloads where accumulation
    is the dominant operation and minimizing intermediate memory writes is critical.
    It is commonly employed in fully connected layers, attention mechanisms, and systolic
    arrays, where structured accumulation leads to significant performance improvements.
    However, for models where input reuse is more critical, alternative dataflow strategies,
    such as Input Stationary, may provide better trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Input Stationary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Input Stationary strategy keeps input activations fixed in local memory,
    while weights and partial sums stream through the system. This approach is particularly
    effective for batch processing, transformer models, and sequence-based architectures,
    where input activations are reused across multiple computations. By ensuring that
    activations remain in local memory, this method reduces redundant input fetches,
    improving data locality and minimizing memory traffic.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of the Input Stationary approach is that it maximizes input
    reuse, reducing the frequency of memory accesses for activations. Since many models,
    especially those in NLP and recommendation systems, process the same input data
    across multiple computations, keeping inputs stationary eliminates unnecessary
    memory transfers, thereby lowering energy consumption. This strategy is particularly
    useful when dealing with large batch sizes, where a single batch of input activations
    contributes to multiple weight transformations.
  prefs: []
  type: TYPE_NORMAL
- en: A simplified Input Stationary implementation for matrix multiplication is illustrated
    in [Listing 11.21](ch017.xhtml#lst-input_stationary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.21: **Input Stationary**: This approach keeps input activations
    stationary while dynamically streaming weights to maximize memory reuse and reduce
    energy consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation follows the core principles of input stationary execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Input activations are loaded into local memory and remain fixed during computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights are streamed dynamically**, ensuring efficient application across
    multiple inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial sums are accumulated and written out**, optimizing memory bandwidth
    usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By keeping input activations stationary, this strategy minimizes redundant memory
    accesses to input data, significantly reducing external memory bandwidth requirements.
    This is particularly beneficial in transformer architectures, where each token
    in an input sequence is used across multiple attention heads and layers. Additionally,
    in batch processing scenarios, keeping input activations in local memory improves
    data locality, making it well-suited for fully connected layers and matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: However, while Input Stationary reduces memory traffic for activations, it introduces
    trade-offs in weight and output movement. Since weights must be streamed dynamically
    while inputs remain fixed, the efficiency of this approach depends on how well
    weights can be delivered to the computational units without causing stalls. Additionally,
    partial sums must be accumulated efficiently before being written back to memory,
    which may require additional buffering mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: The Input Stationary strategy is most effective for workloads where input activations
    exhibit high reuse, and memory bandwidth for inputs is a critical constraint.
    It is commonly employed in transformers, recurrent networks, and batch processing
    workloads, where structured input reuse leads to significant performance improvements.
    However, for models where output accumulation is more critical, alternative dataflow
    strategies, such as Output Stationary, may provide better trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-Efficient Tensor Layouts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Efficient execution of machine learning workloads depends not only on how data
    moves (dataflow strategies) but also on how data is stored and accessed in memory.
    Tensor layouts, which refers to the arrangement of multidimensional data in memory,
    can significantly impact memory access efficiency, cache performance, and computational
    throughput. Poorly chosen layouts can lead to excessive memory stalls, inefficient
    cache usage, and increased data movement costs.
  prefs: []
  type: TYPE_NORMAL
- en: In AI accelerators, tensor layout optimization is particularly important because
    data is frequently accessed in patterns dictated by the underlying hardware architecture.
    Choosing the right layout ensures that memory accesses align with hardware-friendly
    access patterns, minimizing overhead from costly memory transactions ([C. NVIDIA
    2025](ch058.xhtml#ref-nvidia2021cudnn)).
  prefs: []
  type: TYPE_NORMAL
- en: While developers can sometimes manually specify tensor layouts, the choice is
    often determined automatically by machine learning frameworks (e.g., TensorFlow,
    PyTorch, JAX), compilers, or AI accelerator runtimes. Low-level optimization tools
    such as cuDNN (for NVIDIA GPUs), XLA (for TPUs), and MLIR (for custom accelerators)
    may rearrange tensor layouts dynamically to optimize performance ([X. He 2023a](ch058.xhtml#ref-xla2020)).
    In high-level frameworks, layout transformations are typically applied transparently,
    but developers working with custom kernels or low-level libraries (e.g., CUDA,
    Metal, or OpenCL) may have direct control over tensor format selection.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in PyTorch, users can manually modify layouts using tensor.permute()
    or tensor.contiguous() to ensure efficient memory access ([Paszke et al. 2019](ch058.xhtml#ref-paszke2019pytorch)).
    In TensorFlow, layout optimizations are often applied internally by the XLA compiler,
    choosing between NHWC (row-major) and NCHW (channel-major) based on the target
    hardware ([Brain 2022](ch058.xhtml#ref-tensorflow2022)). Hardware-aware machine
    learning libraries, such as cuDNN for GPUs or OneDNN for CPUs, enforce specific
    memory layouts to maximize cache locality and SIMD efficiency. Ultimately, while
    developers may have some control over tensor layout selection, most layout decisions
    are driven by the compiler and runtime system, ensuring that tensors are stored
    in memory in a way that best suits the underlying hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Row-Major Layout
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Row-major layout refers to the way multi-dimensional tensors are stored in memory,
    where elements are arranged row by row, ensuring that all values in a given row
    are placed contiguously before moving to the next row. This storage format is
    widely used in general-purpose CPUs and some machine learning frameworks because
    it aligns naturally with sequential memory access patterns, making it more cache-efficient
    for certain types of operations ([Intel 2021](ch058.xhtml#ref-oneDNN2021)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how row-major layout works, consider a single RGB image represented
    as a tensor of shape (Height, Width, Channels). If the image has a size of <semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> pixels with 3
    channels (RGB), the corresponding tensor is structured as (3, 3, 3). The values
    are stored in memory as follows: <semantics><mtable><mtr><mtd columnalign="center"
    style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0),
    I(0,1,1), \\ I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots \end{gather*}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: Each row is stored contiguously, meaning all pixel values in the first row are
    placed sequentially in memory before moving on to the second row. This ordering
    is advantageous because CPUs and cache hierarchies are optimized for sequential
    memory access. When data is accessed in a row-wise fashion, such as when applying
    element-wise operations like activation functions or basic arithmetic transformations,
    memory fetches are efficient, and cache utilization is maximized ([Sodani 2015](ch058.xhtml#ref-sodani2017knl)).
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency of row-major storage becomes particularly evident in CPU-based
    machine learning workloads, where operations such as batch normalization, matrix
    multiplications, and element-wise arithmetic frequently process rows of data sequentially.
    Since modern CPUs employ cache prefetching mechanisms, a row-major layout allows
    the next required data values to be preloaded into cache ahead of execution, reducing
    memory latency and improving overall computational throughput.
  prefs: []
  type: TYPE_NORMAL
- en: However, row-major layout can introduce inefficiencies when performing operations
    that require accessing data across channels rather than across rows. Consider
    a convolutional layer that applies a filter across multiple channels of an input
    image. Since channel values are interleaved in row-major storage, the convolution
    operation must jump across memory locations to fetch all the necessary channel
    values for a given pixel. These strided memory accesses can be costly on hardware
    architectures that rely on vectorized execution and coalesced memory access, such
    as GPUs and TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, row-major layout remains a dominant storage format
    in CPU-based machine learning frameworks. TensorFlow, for instance, defaults to
    the NHWC (row-major) format on CPUs, ensuring that cache locality is optimized
    for sequential processing. However, when targeting GPUs, frameworks often rearrange
    data dynamically to take advantage of more efficient memory layouts, such as channel-major
    storage, which aligns better with parallelized computation.
  prefs: []
  type: TYPE_NORMAL
- en: Channel-Major Layout
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In contrast to row-major layout, channel-major layout arranges data in memory
    such that all values for a given channel are stored together before moving to
    the next channel. This format is particularly beneficial for GPUs, TPUs, and other
    AI accelerators, where vectorized operations and memory coalescing significantly
    impact computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how channel-major layout works, consider the same RGB image tensor
    of size (Height, Width, Channels) = (3, 3, 3). Instead of storing pixel values
    row by row, the data is structured channel-first in memory as follows: <semantics><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi><mo>,</mo><mi>I</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>,</mo><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>…</mi></mtd></mtr></mtable><annotation
    encoding="application/x-tex">\begin{gather*} I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0),
    I(1,1,0), I(2,1,0), \ldots, \\ I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2),
    I(1,0,2), I(2,0,2), \ldots \end{gather*}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: In this format, all red channel values for the entire image are stored first,
    followed by all green values, and then all blue values. This ordering allows hardware
    accelerators to efficiently load and process data across channels in parallel,
    which is crucial for convolution operations and SIMD (Single Instruction, Multiple
    Data) execution models ([Chetlur et al. 2014](ch058.xhtml#ref-chetlur2014cudnn)).
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of channel-major layout becomes clear when performing convolutions
    in machine learning models. Convolutional layers process images by applying a
    shared set of filters across all channels. When the data is stored in a channel-major
    format, a convolution kernel can load an entire channel efficiently, reducing
    the number of scattered memory fetches. This reduces memory latency, improves
    throughput, and enhances data locality for matrix multiplications, which are fundamental
    to machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Because GPUs and TPUs rely on memory coalescing[27](#fn27), a technique in which
    consecutive threads fetch contiguous memory addresses, channel-major layout aligns
    naturally with the way these processors execute parallel computations. For example,
    in NVIDIA GPUs, each thread in a warp (a group of threads executed simultaneously)
    processes different elements of the same channel, ensuring that memory accesses
    are efficient and reducing the likelihood of strided memory accesses, which can
    degrade performance.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its advantages in machine learning accelerators, channel-major layout
    can introduce inefficiencies when running on general-purpose CPUs. Since CPUs
    optimize for sequential memory access, storing all values for a single channel
    before moving to the next disrupts cache locality for row-wise operations. This
    is why many machine learning frameworks (e.g., TensorFlow, PyTorch) default to
    row-major (NHWC) on CPUs and channel-major (NCHW) on GPUs—optimizing for the strengths
    of each hardware type.
  prefs: []
  type: TYPE_NORMAL
- en: Modern AI frameworks and compilers often transform tensor layouts dynamically
    depending on the execution environment. For instance, TensorFlow and PyTorch automatically
    switch between NHWC[28](#fn28) and NCHW based on whether a model is running on
    a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most efficient
    execution path.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Row-Major and Channel-Major Layouts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct purposes
    in machine learning workloads, with their efficiency largely determined by the
    hardware architecture, memory access patterns, and computational requirements.
    The choice of layout directly influences cache utilization, memory bandwidth efficiency,
    and processing throughput. [Table 11.14](ch017.xhtml#tbl-major) summarizes the
    differences between row-major (NHWC) and channel-major (NCHW) layouts in terms
    of performance trade-offs and hardware compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.14: **Data Layout Strategies**: Row-major (NHWC) and channel-major
    (NCHW) layouts optimize memory access patterns for different hardware architectures;
    NHWC suits cpus and element-wise operations, while NCHW accelerates GPU and TPU-based
    convolution operations. Choosing the appropriate layout significantly impacts
    performance by maximizing cache utilization and memory bandwidth efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Row-Major (NHWC)** | **Channel-Major (NCHW)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Storage Order** | Pixels are stored row-by-row, channel interleaved
    | All values for a given channel are stored together first |'
  prefs: []
  type: TYPE_TB
- en: '| **Best for** | CPUs, element-wise operations | GPUs, TPUs, convolution operations
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Cache Efficiency** | High cache locality for sequential row access | Optimized
    for memory coalescing across channels |'
  prefs: []
  type: TYPE_TB
- en: '| **Convolution Performance** | Requires strided memory accesses (inefficient
    on GPUs) | Efficient for GPU convolution kernels |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Fetching** | Good for operations that process rows sequentially
    | Optimized for SIMD execution across channels |'
  prefs: []
  type: TYPE_TB
- en: '| **Default in Frameworks** | Default on CPUs (e.g., TensorFlow NHWC) | Default
    on GPUs (e.g., cuDNN prefers NCHW) |'
  prefs: []
  type: TYPE_TB
- en: The decision to use row-major (NHWC) or channel-major (NCHW) layouts is not
    always made manually by developers. Instead, machine learning frameworks and AI
    compilers often determine the optimal layout dynamically based on the target hardware
    and operation type. CPUs tend to favor NHWC due to cache-friendly sequential memory
    access, while GPUs perform better with NCHW, which reduces memory fetch overhead
    for machine learning computations.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, modern AI compilers such as TensorFlow’s XLA and PyTorch’s TorchScript
    perform automatic layout transformations, converting tensors between NHWC and
    NCHW as needed to optimize performance across different processing units. This
    ensures that machine learning models achieve the highest possible throughput without
    requiring developers to manually specify tensor layouts.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most impactful optimization techniques in AI acceleration involves
    reducing the overhead of intermediate data movement between operations. This section
    examines how kernel fusion transforms multiple separate computations into unified
    operations, dramatically improving memory efficiency and execution performance.
    We first analyze the memory bottlenecks created by intermediate writes, then explore
    how fusion techniques eliminate these inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Memory Write
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Optimizing memory access is a fundamental challenge in AI acceleration. While
    AI models rely on high-throughput computation, their performance is often constrained
    by memory bandwidth and intermediate memory writes rather than pure arithmetic
    operations. Every time an operation produces an intermediate result that must
    be written to memory and later read back, execution stalls occur due to data movement
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Building on software optimization techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    and memory bandwidth constraints established in [Section 11.4.1](ch017.xhtml#sec-ai-acceleration-understanding-ai-memory-wall-3ea9),
    kernel fusion represents the critical bridge between software optimization and
    hardware acceleration. Many AI workloads introduce unnecessary intermediate memory
    writes, leading to increased memory bandwidth consumption and reduced execution
    efficiency ([Ye et al. 2025](ch058.xhtml#ref-nvidia2017gpu)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.22](ch017.xhtml#lst-naive_execution) illustrates a naïve execution
    model in which each operation is treated as a separate kernel, meaning that each
    intermediate result is written to memory and then read back for the next operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.22: **Naïve Execution**: Each step writes intermediate results to
    memory before processing the next, leading to increased bandwidth usage and reduced
    efficiency. *Source: NVIDIA GPU Technology Conference 2017*[nvidia2017gpu]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Each operation produces an intermediate tensor that must be written to memory
    and retrieved for the next operation. On large tensors, this overhead of moving
    data can outweigh the computational cost of the operations ([Shazeer et al. 2018](ch058.xhtml#ref-shazeer2018mesh)).
    [Table 11.15](ch017.xhtml#tbl-memory-footprint) illustrates the memory overhead
    in a naïve execution model. While only the final result <semantics><mi>Y</mi><annotation
    encoding="application/x-tex">Y</annotation></semantics> is needed, storing multiple
    intermediate tensors creates unnecessary memory traffic and inefficient memory
    usage. This data movement bottleneck significantly impacts performance, making
    memory optimization crucial for AI accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.15: **Intermediate Tensor Storage**: Naïve execution models require
    substantial memory to store intermediate tensors generated by each operation;
    for a 1024x1024 tensor, this table shows that storing these intermediate results—even
    if only the final output is needed—quadruples the total memory footprint from
    4 MB to 16 MB. Minimizing this intermediate data storage is crucial for improving
    memory efficiency and accelerating AI computations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tensor** | **Size (MB) for 1024 <semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    1024 Tensor** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **X** | 4 MB |'
  prefs: []
  type: TYPE_TB
- en: '| **X’** | 4 MB |'
  prefs: []
  type: TYPE_TB
- en: '| **X’’** | 4 MB |'
  prefs: []
  type: TYPE_TB
- en: '| **Y** | 4 MB |'
  prefs: []
  type: TYPE_TB
- en: '| **Total Memory** | 16 MB |'
  prefs: []
  type: TYPE_TB
- en: Even though only the final result <semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics>
    is needed, three additional intermediate tensors consume extra memory without
    contributing to final output storage. This excessive memory usage limits scalability
    and wastes memory bandwidth, particularly in AI accelerators where minimizing
    data movement is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Fusion for Memory Efficiency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kernel fusion is a key optimization technique that aims to minimize intermediate
    memory writes, reducing the memory footprint and bandwidth consumption of machine
    learning workloads ([Zhihao Jia, Zaharia, and Aiken 2018](ch058.xhtml#ref-jia2018beyond)).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel fusion involves merging multiple computation steps into a single, optimized
    operation, eliminating the need for storing and reloading intermediate tensors.
    Instead of executing each layer or element-wise operation separately, in which
    each step writes its output to memory before the next step begins, fusion enables
    direct data propagation between operations, keeping computations within high-speed
    registers or local memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common machine learning sequence might involve applying a nonlinear activation
    function (e.g., ReLU), followed by batch normalization, and then scaling the values
    for input to the next layer. In a naïve implementation, each of these steps generates
    an intermediate tensor, which is written to memory, read back, and then modified
    again: <semantics><mrow><mi>X</mi><mi>′</mi><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>X</mi><mi>″</mi><mo>=</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mi>′</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mi>X</mi><mi>″</mi><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">X'' = \text{ReLU}(X) X'''' = \text{BatchNorm}(X'')
    Y = \alpha \cdot X'''' + \beta</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: 'With kernel fusion, these operations are combined into a single computation
    step, allowing the entire transformation to occur without generating unnecessary
    intermediate tensors: <semantics><mrow><mi>Y</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mtext
    mathvariant="normal">BatchNorm</mtext><mo minsize="1.2" maxsize="1.2" stretchy="false"
    form="prefix">(</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mo>+</mo><mi>β</mi></mrow>
    <annotation encoding="application/x-tex">Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big)
    + \beta</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.16](ch017.xhtml#tbl-fusion-benefits) highlights the impact of operation
    fusion on memory efficiency. By keeping intermediate results in registers or local
    memory rather than writing them to main memory, fusion significantly reduces memory
    traffic. This optimization is especially beneficial on highly parallel architectures
    like GPUs and TPUs, where minimizing memory accesses translates directly into
    improved execution throughput. Compared to the naïve execution model, fused execution
    eliminates the need for storing intermediate tensors, dramatically lowering the
    total memory footprint and improving overall efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.16: **Operation Fusion Benefits**: Fused execution reduces memory
    usage by eliminating the need to store intermediate tensors, directly improving
    efficiency on memory-bound hardware like gpus and tpus. This table quantifies
    the memory savings, showing a reduction from 16 MB in naïve execution to 4 MB
    with fused operations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Execution Model** | **Intermediate Tensors Stored** | **Total Memory Usage
    (MB)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Naïve Execution** | X’, X’’ | 16 MB |'
  prefs: []
  type: TYPE_TB
- en: '| **Fused Execution** | None | 4 MB |'
  prefs: []
  type: TYPE_TB
- en: Kernel fusion reduces total memory consumption from 16 MB to 4 MB, eliminating
    redundant memory writes while improving execution efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Benefits and Constraints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kernel fusion brings several key advantages that enhance memory efficiency and
    computation throughput. By reducing memory accesses, fused kernels ensure that
    intermediate values stay within registers instead of being repeatedly written
    to and read from memory. This significantly lowers memory traffic, which is one
    of the primary bottlenecks in machine learning workloads. GPUs and TPUs, in particular,
    benefit from kernel fusion because high-bandwidth memory is a scarce resource,
    and reducing memory transactions leads to better utilization of compute units
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere)).
  prefs: []
  type: TYPE_NORMAL
- en: However, not all operations can be fused. Element-wise operations, such as ReLU,
    batch normalization, and simple arithmetic transformations, are ideal candidates
    for fusion since their computations depend only on single elements from the input
    tensor. In contrast, operations with complex data dependencies, such as matrix
    multiplications and convolutions, involve global data movement, making direct
    fusion impractical. These operations require values from multiple input elements
    to compute a single output, which prevents them from being executed as a single
    fused kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Another major consideration is register pressure. Fusing multiple operations
    means all temporary values must be kept in registers rather than memory. While
    this eliminates redundant memory writes, it also increases register demand. If
    a fused kernel exceeds the available registers per thread, the system must spill
    excess values into shared memory, introducing additional latency and potentially
    negating the benefits of fusion. On GPUs, where thread occupancy (the number of
    threads that can run in parallel) is limited by available registers, excessive
    fusion can reduce parallelism, leading to diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: Different AI accelerators and compilers handle fusion in distinct ways. NVIDIA
    GPUs, for example, favor warp-level parallelism, where element-wise fusion is
    straightforward. TPUs, on the other hand, prioritize systolic array execution,
    which is optimized for matrix-matrix operations rather than element-wise fusion
    ([X. Qi, Kantarci, and Liu 2017](ch058.xhtml#ref-nvidia2020ampere)). AI compilers
    such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and MLIR automatically
    detect fusion opportunities and apply heuristics to balance memory savings and
    execution efficiency ([X. He 2023b](ch058.xhtml#ref-xla2021)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite its advantages, fusion is not always beneficial. Some AI frameworks
    allow developers to disable fusion selectively, especially when debugging performance
    issues or making frequent model modifications. The decision to fuse operations
    must consider trade-offs between memory efficiency, register usage, and hardware
    execution constraints to ensure that fusion leads to tangible performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-Efficient Tiling Strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While modern AI accelerators offer high computational throughput, their performance
    is often limited by memory bandwidth rather than raw processing power. If data
    cannot be supplied to processing units fast enough, execution stalls occur, leading
    to wasted cycles and inefficient hardware utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Tiling is a technique used to mitigate this issue by restructuring computations
    into smaller, memory-friendly subproblems. Instead of processing entire matrices
    or tensors at once, which leads to excessive memory traffic, tiling partitions
    computations into smaller blocks (tiles) that fit within fast local memory (e.g.,
    caches, shared memory, or registers) ([Lam, Rothberg, and Wolf 1991](ch058.xhtml#ref-lam1991cache)).
    By doing so, tiling increases data reuse, minimizes memory fetches, and improves
    overall computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: A classic example of inefficient memory access is matrix multiplication, which
    is widely used in AI models. Without tiling, the naïve approach results in repeated
    memory accesses for the same data, leading to unnecessary bandwidth consumption
    ([Listing 11.23](ch017.xhtml#lst-naive_matmul)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.23: Naïve matrix multiplication without tiling'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Each iteration requires loading elements from matrices <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> and <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> multiple times from memory,
    causing excessive data movement. As the size of the matrices increases, the memory
    bottleneck worsens, limiting performance.
  prefs: []
  type: TYPE_NORMAL
- en: Tiling addresses this problem by ensuring that smaller portions of matrices
    are loaded into fast memory, reused efficiently, and only written back to main
    memory when necessary. This technique is especially crucial in AI accelerators,
    where memory accesses dominate execution time. By breaking up large matrices into
    smaller tiles, as illustrated in [Figure 11.8](ch017.xhtml#fig-tiling-diagram),
    computation can be performed more efficiently on hardware by maximizing data reuse
    in fast memory. In the following sections, the fundamental principles emerge of
    tiling, its different strategies, and the key trade-offs involved in selecting
    an effective tiling approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file187.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: **Matrix Tiling**: Partitioning large matrices into smaller tiles
    optimizes data reuse and reduces memory access overhead during computation. This
    technique improves performance on AI accelerators by enabling efficient loading
    and processing of data in fast memory, minimizing transfers from slower main memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Tiling Fundamentals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Tiling is based on a simple but powerful principle: instead of operating on
    an entire data structure at once, computations are divided into smaller tiles
    that fit within the available fast memory. By structuring execution around these
    tiles, data reuse is maximized, reducing redundant memory accesses and improving
    overall efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider matrix multiplication, a key operation in machine learning workloads.
    The operation computes the output matrix <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>
    from two input matrices <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>:
    <semantics><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi></mrow> <annotation
    encoding="application/x-tex">C = A \times B</annotation></semantics> where each
    element <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">C[i,j]</annotation></semantics>
    is computed as: <semantics><mrow><mi>C</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo stretchy="true"
    form="postfix">]</mo></mrow><mo>×</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>k</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow> <annotation encoding="application/x-tex">C[i,j]
    = \sum_{k} A[i,k] \times B[k,j]</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: A naïve implementation follows this formula directly ([Listing 11.24](ch017.xhtml#lst-naive_matmul_repeat)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.24: **Naïve Matrix Multiplication**: This code directly implements
    matrix multiplication using nested loops, showing how each element in the output
    matrix is computed as a sum of products from corresponding elements in the input
    matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: At first glance, this approach seems correct—it computes the desired result
    and follows the mathematical definition. However, the issue lies in how memory
    is accessed. Every time the innermost loop runs, it fetches an element from matrix
    <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and matrix <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    from memory, performs a multiplication, and updates an element in matrix <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics>. Because matrices are
    large, the processor frequently reloads the same values from memory, even though
    they were just used in previous computations.
  prefs: []
  type: TYPE_NORMAL
- en: This unnecessary data movement is expensive. Fetching values from main memory
    (DRAM) is hundreds of times slower than accessing values stored in on-chip cache
    or registers. If the same values must be reloaded multiple times instead of being
    stored in fast memory, execution slows down significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Benefits of Tiling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Instead of computing one element at a time and constantly moving data in and
    out of slow memory, tiling processes submatrices (tiles) at a time, keeping frequently
    used values in fast memory. The idea is to divide the matrices into smaller blocks
    that fit within the processor’s cache or shared memory, ensuring that once a block
    is loaded, it is reused multiple times before moving to the next one.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.25](ch017.xhtml#lst-tiled_matmul) illustrates a tiled version of
    matrix multiplication, which improves memory locality by processing blocks of
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.25: **Tiled Matrix Multiplication**: This approach divides matrices
    into smaller blocks to optimize memory usage by reusing data within processor
    cache, thereby improving computational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This restructuring significantly improves performance for three main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Better Memory Reuse**: Instead of fetching elements from <semantics><mi>A</mi><annotation
    encoding="application/x-tex">A</annotation></semantics> and <semantics><mi>B</mi><annotation
    encoding="application/x-tex">B</annotation></semantics> repeatedly from slow memory,
    this approach loads a small tile of data into fast memory, performs multiple computations
    using it, and only then moves on to the next tile. This minimizes redundant memory
    accesses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reduced Memory Bandwidth Usage**: Since each tile is used multiple times
    before being evicted, memory traffic is reduced. Instead of repeatedly accessing
    DRAM, most required data is available in L1/L2 cache or shared memory, leading
    to faster execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Increased Compute Efficiency**: Processors spend less time waiting for data
    and more time performing useful computations. In architectures like GPUs and TPUs,
    where thousands of parallel processing units operate simultaneously, tiling ensures
    that data is read and processed in a structured manner, avoiding unnecessary stalls.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This technique is particularly effective in AI accelerators, where machine learning
    workloads consist of large matrix multiplications and tensor transformations.
    Without tiling, these workloads quickly become memory-bound, meaning performance
    is constrained by how fast data can be retrieved rather than by the raw computational
    power of the processor.
  prefs: []
  type: TYPE_NORMAL
- en: Tiling Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While the general principle of tiling remains the same, which involves partitioning
    large computations into smaller subproblems to improve memory reuse, there are
    different ways to apply tiling based on the structure of the computation and hardware
    constraints. The two primary tiling strategies are spatial tiling and temporal
    tiling. These strategies optimize different aspects of computation and memory
    access, and in practice, they are often combined to achieve the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial Tiling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Spatial tiling focuses on partitioning data structures into smaller blocks that
    fit within the fast memory of the processor. This approach ensures that each tile
    is fully processed before moving to the next, reducing redundant memory accesses.
    Spatial tiling is widely used in operations such as matrix multiplication, convolutions,
    and attention mechanisms in transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial tiling is illustrated in [Listing 11.26](ch017.xhtml#lst-tiled_spatial),
    where the computation proceeds over blocks of the input matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.26: **Spatial Tiling**: Reduces redundant memory accesses by processing
    matrix tiles sequentially.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this implementation, each tile of <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    is loaded into cache or shared memory before processing, ensuring that the same
    data does not need to be fetched repeatedly from slower memory. The tile is fully
    used before moving to the next block, minimizing redundant memory accesses. Since
    data is accessed in a structured, localized way, cache efficiency improves significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial tiling is particularly beneficial when dealing with large tensors that
    do not fit entirely in fast memory. By breaking them into smaller tiles, computations
    remain localized, avoiding excessive data movement between memory levels. This
    technique is widely used in AI accelerators where machine learning workloads involve
    large-scale tensor operations that require careful memory management to achieve
    high performance.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Tiling
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While spatial tiling optimizes how data is partitioned, temporal tiling focuses
    on reorganizing the computation itself to improve data reuse over time. Many machine
    learning workloads involve operations where the same data is accessed repeatedly
    across multiple iterations. Without temporal tiling, this often results in redundant
    memory fetches, leading to inefficiencies. Temporal tiling, also known as loop
    blocking, restructures the computation to ensure that frequently used data stays
    in fast memory for as long as possible before moving on to the next computation.
  prefs: []
  type: TYPE_NORMAL
- en: A classic example where temporal tiling is beneficial is convolutional operations,
    where the same set of weights is applied to multiple input regions. Without loop
    blocking, these weights might be loaded from memory multiple times for each computation.
    With temporal tiling, the computation is reordered so that the weights remain
    in fast memory across multiple inputs, reducing unnecessary memory fetches and
    improving overall efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.27](ch017.xhtml#lst-loop_blocking) illustrates a simplified example
    of loop blocking in matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 11.27: **Temporal Tiling**: Reduces redundant memory accesses by caching
    weights in fast memory across multiple matrix multiplications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Temporal tiling improves performance by ensuring that the data loaded into fast
    memory is used multiple times before being evicted. In this implementation, small
    tiles of matrices <semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics>
    and <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>
    are explicitly loaded into temporary storage before performing computations, reducing
    memory fetch overhead. This restructuring allows the computation to process an
    entire tile before moving to the next, thereby reducing the number of times data
    must be loaded from slower memory.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is particularly useful in workloads where certain values are
    used repeatedly, such as convolutions, recurrent neural networks (RNNs), and self-attention
    mechanisms in transformers. By applying loop blocking, AI accelerators can significantly
    reduce memory stalls and improve execution throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Tiling Challenges and Trade-offs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While tiling significantly improves performance by optimizing memory reuse and
    reducing redundant memory accesses, it introduces several challenges and trade-offs.
    Selecting the right tile size is a critical decision, as it directly affects computational
    efficiency and memory bandwidth usage. If the tile size is too small, the benefits
    of tiling diminish, as memory fetches still dominate execution time. On the other
    hand, if the tile size is too large, it may exceed the available fast memory,
    causing cache thrashing and performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing is another key concern. In architectures such as GPUs and TPUs,
    computations are executed in parallel across thousands of processing units. If
    tiles are not evenly distributed, some units may remain idle while others are
    overloaded, leading to suboptimal utilization of computational resources. Effective
    tile scheduling ensures that parallel execution remains balanced and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Data movement overhead is also an important consideration. Although tiling reduces
    the number of slow memory accesses, transferring tiles between different levels
    of memory still incurs a cost. This is especially relevant in hierarchical memory
    systems, where accessing data from cache is much faster than accessing it from
    DRAM. Efficient memory prefetching and scheduling strategies are required to minimize
    latency and ensure that data is available when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond spatial and temporal tiling, hybrid approaches combine elements of both
    strategies to achieve optimal performance. Hybrid tiling adapts to workload-specific
    constraints by dynamically adjusting tile sizes or reordering computations based
    on real-time execution conditions. For example, some AI accelerators use spatial
    tiling for matrix multiplications while employing temporal tiling for weight reuse
    in convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Other methods exist for optimizing memory usage and computational efficiency
    beyond tiling. Techniques such as register blocking, double buffering, and hierarchical
    tiling extend the basic tiling principles to further optimize execution. AI compilers
    and runtime systems, such as TensorFlow XLA, TVM, and MLIR, automatically select
    tiling strategies based on hardware constraints, enabling fine-tuned performance
    optimization without manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.17](ch017.xhtml#tbl-tiling-strategies) provides a comparative overview
    of spatial, temporal, and hybrid tiling approaches, highlighting their respective
    benefits and trade-offs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.17: **Tiling Strategies**: Spatial, temporal, and hybrid tiling optimize
    memory access patterns for improved performance; spatial tiling maximizes data
    reuse within fast memory, temporal tiling exploits loop structure for reduced
    accesses, and hybrid tiling combines both approaches to balance computational
    efficiency and memory bandwidth. These techniques are crucial for AI compilers
    and runtime systems to automatically optimize model execution on diverse hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Spatial Tiling (Data Tiling)** | **Temporal Tiling (Loop Blocking)**
    | **Hybrid Tiling** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Primary Goal** | Reduce memory accesses by keeping data in fast memory
    longer | Increase data reuse across loop iterations | Adapt dynamically to workload
    constraints |'
  prefs: []
  type: TYPE_TB
- en: '| **Optimization Focus** | Partitioning data structures into smaller, memory-friendly
    blocks | Reordering computations to maximize reuse before eviction | Balancing
    spatial and temporal reuse strategies |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Usage** | Improves cache locality and reduces DRAM access | Keeps
    frequently used data in fast memory for multiple iterations | Minimizes data movement
    while ensuring high reuse |'
  prefs: []
  type: TYPE_TB
- en: '| **Common Use Cases** | Matrix multiplications, CNNs, self-attention in transformers
    | Convolutions, recurrent neural networks (RNNs), iterative computations | AI
    accelerators with hierarchical memory, mixed workloads |'
  prefs: []
  type: TYPE_TB
- en: '| **Performance Gains** | Reduced memory bandwidth requirements, better cache
    utilization | Lower memory fetch latency, improved data locality | Maximized efficiency
    across multiple hardware types |'
  prefs: []
  type: TYPE_TB
- en: '| **Challenges** | Requires careful tile size selection, inefficient for workloads
    with minimal spatial reuse | Can increase register pressure, requires loop restructuring
    | Complexity in tuning tile size and execution order dynamically |'
  prefs: []
  type: TYPE_TB
- en: '| **Best When** | Data is large and needs to be partitioned for efficient processing
    | The same data is accessed multiple times across iterations | Both data partitioning
    and iteration-based reuse are important |'
  prefs: []
  type: TYPE_TB
- en: As machine learning models continue to grow in size and complexity, tiling remains
    a critical tool for improving hardware efficiency, ensuring that AI accelerators
    operate at their full potential. While manual tiling strategies can provide substantial
    benefits, modern compilers and hardware-aware optimization techniques further
    enhance performance by automatically selecting the most effective tiling strategies
    for a given workload.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Mapping Strategies to Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While these foundational mapping techniques apply broadly, their effectiveness
    varies based on the computational structure, data access patterns, and parallelization
    opportunities of different neural network architectures. Each architecture imposes
    distinct constraints on data movement, memory hierarchy, and computation scheduling,
    requiring tailored mapping strategies to optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: A structured approach to mapping is essential to address the combinatorial explosion
    of choices that arise when assigning computations to AI accelerators. Rather than
    treating each model as a separate optimization problem, we recognize that the
    same fundamental principles apply across different architectures—only their priority
    shifts based on workload characteristics. The goal is to systematically select
    and apply mapping strategies that maximize efficiency for different types of machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: These principles apply to three representative AI workloads, each characterized
    by distinct computational demands. CNNs benefit from spatial data reuse, making
    weight-stationary execution and the application of tiling techniques especially
    effective. In contrast, Transformers are inherently memory-bound and rely on strategies
    such as efficient KV-cache management, fused attention mechanisms, and highly
    parallel execution to mitigate memory traffic. MLPs, which involve substantial
    matrix multiplication operations, demand the use of structured tiling, optimized
    weight layouts, and memory-aware execution to enhance overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their differences, each of these models follows a common set of mapping
    principles, with variations in how optimizations are prioritized. The following
    table provides a structured mapping between different optimization strategies
    and their suitability for CNNs, Transformers, and MLPs. This table serves as a
    roadmap for selecting appropriate mapping strategies for different machine learning
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Optimization Technique** | **CNNs** | **Transformers** | **MLPs** | **Rationale**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Dataflow Strategy** | Weight Stationary | Activation Stationary | Weight
    Stationary | CNNs reuse filters across spatial locations; Transformers reuse activations
    (KV-cache); MLPs reuse weights across batches. |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory-Aware Tensor Layouts** | NCHW (Channel-Major) | NHWC (Row-Major)
    | NHWC | CNNs favor channel-major for convolution efficiency; Transformers and
    MLPs prioritize row-major for fast memory access. |'
  prefs: []
  type: TYPE_TB
- en: '| **Kernel Fusion** | Convolution + Activation | Fused Attention | GEMM Fusion
    | CNNs optimize convolution+activation fusion; Transformers fuse attention mechanisms;
    MLPs benefit from fused matrix multiplications. |'
  prefs: []
  type: TYPE_TB
- en: '| **Tiling for Memory Efficiency** | Spatial Tiling | Temporal Tiling | Blocked
    Tiling | CNNs tile along spatial dimensions; Transformers use loop blocking to
    improve sequence memory efficiency; MLPs use blocked tiling for large matrix multiplications.
    |'
  prefs: []
  type: TYPE_TB
- en: This table highlights that each machine learning model benefits from a different
    combination of optimization techniques, reinforcing the importance of tailoring
    execution strategies to the computational and memory characteristics of the workload.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we explore how these optimizations apply to each
    network type, explaining how CNNs, Transformers, and MLPs leverage specific mapping
    strategies to improve execution efficiency and hardware utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNNs are characterized by their structured spatial computations, where small
    filters (or kernels) are repeatedly applied across an input feature map. This
    structured weight reuse makes weight stationary execution the most effective strategy
    for CNNs. Keeping filter weights in fast memory while streaming activations ensures
    that weights do not need to be repeatedly fetched from slower external memory,
    significantly reducing memory bandwidth demands. Since each weight is applied
    to multiple spatial locations, weight stationary execution maximizes arithmetic
    intensity and minimizes redundant memory transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-aware tensor layouts also play a critical role in CNN execution. Convolution
    operations benefit from a channel-major memory format, often represented as NCHW
    (batch, channels, height, width). This layout aligns with the access patterns
    of convolutions, enabling efficient memory coalescing on accelerators such as
    GPUs and TPUs. By storing data in a format that optimizes cache locality, accelerators
    can fetch contiguous memory blocks efficiently, reducing latency and improving
    throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel fusion is another important optimization for CNNs. In a typical machine
    learning pipeline, convolution operations are often followed by activation functions
    such as ReLU and batch normalization. Instead of treating these operations as
    separate computational steps, fusing them into a single kernel reduces intermediate
    memory writes and improves execution efficiency. This optimization minimizes memory
    bandwidth pressure by keeping intermediate values in registers rather than writing
    them to memory and fetching them back in subsequent steps.
  prefs: []
  type: TYPE_NORMAL
- en: Given the size of input images and feature maps, tiling is necessary to ensure
    that computations fit within fast memory hierarchies. Spatial tiling, where input
    feature maps are processed in smaller subregions, allows for efficient utilization
    of on-chip memory while avoiding excessive off-chip memory transfers. This technique
    ensures that input activations, weights, and intermediate outputs remain within
    high-speed caches or shared memory as long as possible, reducing memory stalls
    and improving overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these optimizations ensure that CNNs make efficient use of available
    compute resources by maximizing weight reuse, optimizing memory access patterns,
    reducing redundant memory writes, and structuring computation to fit within fast
    memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike CNNs, which rely on structured spatial computations, Transformers process
    variable-length sequences and rely heavily on attention mechanisms. The primary
    computational bottleneck in Transformers is memory bandwidth, as attention mechanisms
    require frequent access to stored key-value pairs across multiple query vectors.
    Given this access pattern, activation stationary execution is the most effective
    strategy. By keeping key-value activations in fast memory and streaming query
    vectors dynamically, activation reuse is maximized while minimizing redundant
    memory fetches. This approach is critical in reducing bandwidth overhead, especially
    in long-sequence tasks such as natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Memory layout optimization is equally important for Transformers. Unlike CNNs,
    which benefit from channel-major layouts, Transformers require efficient access
    to sequences of activations, making a row-major format (NHWC) the preferred choice.
    This layout ensures that activations are accessed contiguously in memory, reducing
    cache misses and improving memory coalescing for matrix multiplications.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel fusion plays a key role in optimizing Transformer execution. In self-attention,
    multiple computational steps, such as query-key dot products, softmax normalization,
    and weighted summation, can be fused into a single operation. Fused attention
    kernels eliminate intermediate memory writes by computing attention scores and
    performing weighted summations within a single execution step. This optimization
    significantly reduces memory traffic, particularly for large batch sizes and long
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the nature of sequence processing, tiling must be adapted to improve
    memory efficiency. Instead of spatial tiling, which is effective for CNNs, Transformers
    benefit from temporal tiling, where computations are structured to process sequence
    blocks efficiently. This method ensures that activations are loaded into fast
    memory in manageable chunks, reducing excessive memory transfers. Temporal tiling
    is particularly beneficial for long-sequence models, where the memory footprint
    of key-value activations grows significantly. By tiling sequences into smaller
    segments, memory locality is improved, enabling efficient cache utilization and
    reducing bandwidth pressure.
  prefs: []
  type: TYPE_NORMAL
- en: These optimizations collectively address the primary bottlenecks in Transformer
    models by prioritizing activation reuse, structuring memory layouts for efficient
    batched computations, fusing attention operations to reduce intermediate memory
    writes, and employing tiling techniques suited to sequence-based processing.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Layer Perceptrons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MLPs primarily consist of fully connected layers, where large matrices of weights
    and activations are multiplied to produce output representations. Given this structure,
    weight stationary execution is the most effective strategy for MLPs. Similar to
    CNNs, MLPs benefit from keeping weights in local memory while streaming activations
    dynamically, as this ensures that weight matrices, which are typically reused
    across multiple activations in a batch, do not need to be frequently reloaded.
  prefs: []
  type: TYPE_NORMAL
- en: The preferred memory layout for MLPs aligns with that of Transformers, as matrix
    multiplications are more efficient when using a row-major (NHWC) format. Since
    activation matrices are processed in batches, this layout ensures that input activations
    are accessed efficiently without introducing memory fragmentation. By aligning
    tensor storage with compute-friendly memory access patterns, cache utilization
    is improved, reducing memory stalls.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel fusion in MLPs is primarily applied to General Matrix Multiplication
    (GEMM)[29](#fn29) operations. Since dense layers are often followed by activation
    functions and bias additions, fusing these operations into a single computation
    step reduces memory traffic. GEMM fusion ensures that activations, weights, and
    biases are processed within a single optimized kernel, avoiding unnecessary memory
    writes and reloads.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve memory efficiency, MLPs rely on blocked tiling strategies,
    where large matrix multiplications are divided into smaller sub-blocks that fit
    within the accelerator’s shared memory. This method ensures that frequently accessed
    portions of matrices remain in fast memory throughout computation, reducing external
    memory accesses. By structuring computations in a way that balances memory utilization
    with efficient parallel execution, blocked tiling minimizes bandwidth limitations
    and maximizes throughput.
  prefs: []
  type: TYPE_NORMAL
- en: These optimizations ensure that MLPs achieve high computational efficiency by
    structuring execution around weight reuse, optimizing memory layouts for dense
    matrix operations, reducing redundant memory writes through kernel fusion, and
    employing blocked tiling strategies to maximize on-chip memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid Mapping Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While general mapping strategies provide a structured framework for optimizing
    machine learning models, real-world architectures often involve diverse computational
    requirements that cannot be effectively addressed with a single, fixed approach.
    Hybrid mapping strategies allow AI accelerators to dynamically apply different
    optimizations to specific layers or components within a model, ensuring that each
    computation is executed with maximum efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models typically consist of multiple layer types, each exhibiting
    distinct memory access patterns, data reuse characteristics, and parallelization
    opportunities. By tailoring mapping strategies to these specific properties, hybrid
    approaches achieve higher computational efficiency, improved memory bandwidth
    utilization, and reduced data movement overhead compared to a uniform mapping
    approach ([Sze et al. 2017b](ch058.xhtml#ref-sze2020efficient)).
  prefs: []
  type: TYPE_NORMAL
- en: Layer-Specific Mapping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hybrid mapping strategies are particularly beneficial in models that combine
    spatially localized computations, such as convolutions, with fully connected operations,
    such as dense layers or attention mechanisms. These operations possess distinct
    characteristics that require different mapping strategies for optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: In convolutional neural networks, hybrid strategies are frequently employed
    to optimize performance. Specifically, weight stationary execution is applied
    to convolutional layers, ensuring that filters remain in local memory while activations
    are streamed dynamically. For fully connected layers, output stationary execution
    is utilized to minimize redundant memory writes during matrix multiplications.
    Additionally, kernel fusion is integrated to combine activation functions, batch
    normalization, and element wise operations into a single computational step, thereby
    reducing intermediate memory traffic. Collectively, these approaches enhance computational
    efficiency and memory utilization, contributing to the overall performance of
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers employ several strategies to enhance performance by optimizing
    memory usage and computational efficiency. Specifically, they use activation stationary
    mapping in self-attention layers to maximize the reuse of stored key-value pairs,
    thereby reducing memory fetches. In feedforward layers, weight stationary mapping
    is applied to ensure that large weight matrices are efficiently reused across
    computations. Additionally, these models incorporate fused attention kernels that
    integrate softmax and weighted summation into a single computation step, significantly
    enhancing execution speed ([Jacobs et al. 2002](ch058.xhtml#ref-dao2022flashattention)).
  prefs: []
  type: TYPE_NORMAL
- en: For multilayer perceptrons, hybrid mapping strategies are employed to optimize
    performance through a combination of techniques that enhance both memory efficiency
    and computational throughput. Specifically, weight stationary execution is utilized
    to maximize the reuse of weights across activations, ensuring that these frequently
    accessed parameters remain readily available and reduce redundant memory accesses.
    In addition, blocked tiling strategies are implemented for large matrix multiplications,
    which significantly improve cache locality by partitioning the computation into
    manageable sub-blocks that fit within fast memory. Complementing these approaches,
    general matrix multiplication fusion is applied, effectively reducing memory stalls
    by merging consecutive matrix multiplication operations with subsequent functional
    transformations. Collectively, these optimizations illustrate how tailored mapping
    strategies can systematically balance memory constraints with computational demands
    in multilayer perceptron architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid mapping strategies are widely employed in vision transformers, which
    seamlessly integrate convolutional and self-attention operations. In these models,
    the patch embedding layer performs a convolution-like operation that benefits
    from weight stationary mapping ([Dosovitskiy et al. 2020](ch058.xhtml#ref-Dosovitskiy2020ViT)).
    The self-attention layers, on the other hand, require activation stationary execution
    to efficiently reuse the key-value cache across multiple queries. Additionally,
    the MLP component leverages general matrix multiplication fusion and blocked tiling
    to execute dense matrix multiplications efficiently. This layer-specific optimization
    framework effectively balances memory locality with computational efficiency,
    rendering vision transformers particularly well-suited for AI accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware Implementations of Hybrid Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several modern AI accelerators incorporate hybrid mapping strategies to optimize
    execution by tailoring layer-specific techniques to the unique computational requirements
    of diverse neural network architectures. For example, Google TPUs employ weight
    stationary mapping for convolutional layers and activation stationary mapping
    for attention layers within transformer models, ensuring that the most critical
    data remains in fast memory. Likewise, NVIDIA GPUs leverage fused kernels alongside
    hybrid memory layouts, which enable the application of different mapping strategies
    within the same model to maximize performance. In addition, Graphcore IPUs dynamically
    select execution strategies on a per-layer basis to optimize memory access, thereby
    enhancing overall computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: These real-world implementations illustrate how hybrid mapping strategies bridge
    the gap between different types of machine learning computations, ensuring that
    each layer executes with maximum efficiency. However, hardware support is essential
    for these techniques to be practical. Accelerators must provide architectural
    features such as programmable memory hierarchies, efficient interconnects, and
    specialized execution pipelines to fully exploit hybrid mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid mapping provides a flexible and efficient approach to deep learning execution,
    enabling AI accelerators to adapt to the diverse computational requirements of
    modern architectures. By selecting the optimal mapping technique for each layer,
    hybrid strategies help reduce memory bandwidth constraints, improve data locality,
    and maximize parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: While hybrid mapping strategies offer an effective way to optimize computations
    at a layer-specific level, they remain static design-time optimizations. In real-world
    AI workloads, execution conditions can change dynamically due to varying input
    sizes, memory contention, or hardware resource availability. Machine learning
    compilers and runtime systems extend these mapping techniques by introducing dynamic
    scheduling, memory optimizations, and automatic tuning mechanisms. These systems
    ensure that hybrid strategies are not just predefined execution choices, but rather
    adaptive mechanisms that allow deep learning workloads to operate efficiently
    across different accelerators and deployment environments. In the next section,
    we explore how machine learning compilers and runtime stacks enable these adaptive
    optimizations through just-in-time scheduling, memory-aware execution, and workload
    balancing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Compiler Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of machine learning acceleration depends not only on hardware
    capabilities but also on how efficiently models are translated into executable
    operations. These optimization techniques, including kernel fusion, tiling, memory
    scheduling, and data movement strategies, are essential for maximizing efficiency.
    However, these optimizations must be systematically applied before execution to
    ensure they align with hardware constraints and computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: This process exemplifies the hardware-software co-design principle established
    in [Section 11.1](ch017.xhtml#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096),
    where machine learning compilers bridge high-level model representations with
    low-level hardware execution. The compiler optimizes models by restructuring computations,
    selecting efficient execution kernels, and maximizing hardware utilization ([0001
    et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018)). Unlike traditional compilers
    designed for general-purpose computing, ML workloads require specialized approaches
    for tensor computations and parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Compiler Design Differences for ML Workloads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning workloads introduce unique challenges that traditional compilers
    were not designed to handle. Unlike conventional software execution, which primarily
    involves sequential or multi-threaded program flow, machine learning models are
    expressed as computation graphs that describe large-scale tensor operations. These
    graphs require specialized optimizations that traditional compilers cannot efficiently
    apply ([Cui, Li, and Xie 2019](ch058.xhtml#ref-cui_mlcompilers_2019)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.18](ch017.xhtml#tbl-ml-vs-traditional-compilers) outlines the fundamental
    differences between traditional compilers and those designed for machine learning
    workloads. While traditional compilers optimize linear program execution through
    techniques like instruction scheduling and register allocation, ML compilers focus
    on optimizing computation graphs for efficient tensor operations. This distinction
    is critical, as ML compilers must incorporate domain-specific transformations
    such as kernel fusion, memory-aware scheduling, and hardware-accelerated execution
    plans to achieve high performance on specialized accelerators like GPUs and TPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: This comparison highlights why machine learning models require a different compilation
    approach. Instead of optimizing instruction-level execution, machine learning
    compilers must transform entire computation graphs, apply tensor-aware memory
    optimizations, and schedule operations across thousands of parallel processing
    elements. These requirements make traditional compiler techniques insufficient
    for modern deep learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.18: **Compiler Optimization Priorities**: Traditional and machine
    learning compilers diverge in their optimization targets; traditional compilers
    prioritize efficient execution of sequential code, while ML compilers focus on
    optimizing tensor operations within computation graphs for specialized hardware.
    This table clarifies how ML compilers incorporate domain-specific transformations—like
    kernel fusion and memory-aware scheduling—to achieve high performance on accelerators,
    unlike the instruction scheduling and register allocation techniques used in conventional
    software compilation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Traditional Compiler** | **Machine Learning Compiler** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Input Representation** | Linear program code (C, Python) | Computational
    graph (ML models) |'
  prefs: []
  type: TYPE_TB
- en: '| **Execution Model** | Sequential or multi-threaded execution | Massively
    parallel tensor-based execution |'
  prefs: []
  type: TYPE_TB
- en: '| **Optimization Priorities** | Instruction scheduling, loop unrolling, register
    allocation | Graph transformations, kernel fusion, memory-aware execution |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Management** | Stack and heap memory allocation | Tensor layout
    transformations, tiling, memory-aware scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| **Target Hardware** | CPUs (general-purpose execution) | GPUs, TPUs, and
    custom accelerators |'
  prefs: []
  type: TYPE_TB
- en: '| **Compilation Output** | CPU-specific machine code | Hardware-specific execution
    plan (kernels, memory scheduling) |'
  prefs: []
  type: TYPE_TB
- en: ML Compilation Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models, as defined in modern frameworks, are initially represented
    in a high-level computation graph that describes operations on tensors. However,
    these representations are not directly executable on hardware accelerators such
    as GPUs, TPUs, and custom AI chips. To achieve efficient execution, models must
    go through a compilation process that transforms them into optimized execution
    plans suited for the target hardware ([Brain 2020](ch058.xhtml#ref-tensorflow_xla_2020)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The machine learning compilation workflow consists of several key stages, each
    responsible for applying specific optimizations that ensure minimal memory overhead,
    maximum parallel execution, and optimal compute utilization. These stages include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph Optimization**: The computation graph is restructured to eliminate
    inefficiencies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kernel Selection**: Each operation is mapped to an optimized hardware-specific
    implementation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory Planning**: Tensor layouts and memory access patterns are optimized
    to reduce bandwidth consumption.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computation Scheduling**: Workloads are distributed across parallel processing
    elements to maximize hardware utilization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Code Generation**: The optimized execution plan is translated into machine-specific
    instructions for execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each stage, the compiler applies theoretical optimizations discussed earlier,
    including kernel fusion, tiling, data movement strategies, and computation placement,
    ensuring that these optimizations are systematically incorporated into the final
    execution plan.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding this workflow, we can see how machine learning acceleration
    is realized not just through hardware improvements but also through compiler-driven
    software optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI accelerators provide specialized hardware to speed up computation, but raw
    model representations are not inherently optimized for execution on these accelerators.
    Machine learning frameworks define models using high-level computation graphs,
    where nodes represent operations (such as convolutions, matrix multiplications,
    and activations), and edges define data dependencies. However, if executed as
    defined, these graphs often contain redundant operations, inefficient memory access
    patterns, and suboptimal execution sequences that can prevent the hardware from
    operating at peak efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a Transformer model, the self-attention mechanism involves repeated
    accesses to the same key-value pairs across multiple attention heads. If compiled
    naïvely, the model may reload the same data multiple times, leading to excessive
    memory traffic ([Shoeybi et al. 2019a](ch058.xhtml#ref-shoeybi_megatron_2020)).
    Similarly, in a CNN, applying batch normalization and activation functions as
    separate operations after each convolution leads to unnecessary intermediate memory
    writes, increasing memory bandwidth usage. These inefficiencies are addressed
    during graph optimization, where the compiler restructures the computation graph
    to eliminate unnecessary operations and improve memory locality ([0001 et al.
    2018a](ch058.xhtml#ref-chen_tvmlang_2018)).
  prefs: []
  type: TYPE_NORMAL
- en: The graph optimization phase of compilation is responsible for transforming
    this high-level computation graph into an optimized execution plan before it is
    mapped to hardware. Rather than requiring manual optimization, the compiler systematically
    applies transformations that improve data movement, reduce redundant computations,
    and restructure operations for efficient parallel execution ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)).
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, the compiler is still working at a hardware-agnostic level, focusing
    on high-level restructuring that improves efficiency before more hardware-specific
    optimizations are applied later.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Graph Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph optimization transforms the computation graph through a series of structured
    techniques designed to enhance execution efficiency. One key technique is kernel
    fusion, which merges consecutive operations to eliminate unnecessary memory writes
    and reduce the number of kernel launches. This approach is particularly effective
    in convolutional neural networks, where fusing convolution, batch normalization,
    and activation functions notably accelerates processing. Another important technique
    is computation reordering, which adjusts the execution order of operations to
    improve data locality and maximize parallel execution. For instance, in Transformer
    models, such reordering enables the reuse of cached key-value pairs rather than
    reloading them repeatedly from memory, thereby reducing latency.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, redundant computation elimination plays an important role. By
    identifying and removing duplicate or unnecessary operations, this method is especially
    beneficial in models with residual connections where common subexpressions might
    otherwise be redundantly computed. Memory-aware dataflow adjustments enhance overall
    performance by refining tensor layouts and optimizing memory movement. For example,
    tiling matrix multiplications to meet the structural requirements of systolic
    arrays in TPUs ensures that hardware resources are utilized optimally. This combined
    approach not only reduces unnecessary processing but also aligns data storage
    and movement with the accelerator’s strengths, leading to efficient execution
    across diverse AI workloads. Together, these techniques prepare the model for
    acceleration by minimizing overhead and ensuring an optimal balance between computational
    and memory resources.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in AI Compilers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern AI compilers perform graph optimization through the use of automated
    pattern recognition and structured rewrite rules, systematically transforming
    computation graphs to maximize efficiency without manual intervention. For example,
    Google’s XLA (Accelerated Linear Algebra) in TensorFlow applies graph-level transformations
    such as fusion and layout optimizations that streamline execution on TPUs and
    GPUs. Similarly, TVM (Tensor Virtual Machine) not only refines tensor layouts
    and adjusts computational structures but also tunes execution strategies across
    diverse hardware backends, which is particularly beneficial for deploying models
    on embedded Tiny ML devices with strict memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA’s TensorRT, another specialized deep learning compiler, focuses on minimizing
    kernel launch overhead by fusing operations and optimizing execution scheduling
    on GPUs, thereby improving utilization and reducing inference latency in large-scale
    convolutional neural network applications. Additionally, MLIR (Multi-Level Intermediate
    Representation) facilitates flexible graph optimization across various AI accelerators
    by enabling multi-stage transformations that improve execution order and memory
    access patterns, thus easing the transition of models from CPU-based implementations
    to accelerator-optimized versions. These compilers preserve the mathematical integrity
    of the models while rewriting the computation graph to ensure that the subsequent
    hardware-specific optimizations can be effectively applied.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Optimization Importance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph optimization enables AI accelerators to operate at peak efficiency. Without
    this phase, even the most optimized hardware would be underutilized, as models
    would be executed in a way that introduces unnecessary memory stalls, redundant
    computations, and inefficient data movement. By systematically restructuring computation
    graphs, the compiler arranges operations for efficient execution that mitigates
    bottlenecks before mapping to hardware, minimizes memory movement to keep tensors
    in high-speed memory, and optimizes parallel execution to reduce unnecessary serialization
    while enhancing hardware utilization. For instance, without proper graph optimization,
    a large Transformer model running on an edge device may experience excessive memory
    stalls due to suboptimal data access patterns; however, through effective graph
    restructuring, the model can operate with significantly reduced memory bandwidth
    consumption and latency, thus enabling real-time inference on devices with constrained
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: With the computation graph now fully optimized, the next step in compilation
    is kernel selection, where the compiler determines which hardware-specific implementation
    should be used for each operation. This ensures that the structured execution
    plan is translated into optimized low-level instructions for the target accelerator.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this stage, the compiler translates the abstract operations in the computation
    graph into optimized low-level functions, ensuring that execution is performed
    as efficiently as possible given the constraints of the target accelerator. A
    kernel is a specialized implementation of a computational operation designed to
    run efficiently on a particular hardware architecture. Most accelerators, including
    GPUs, TPUs, and custom AI chips, provide multiple kernel implementations for the
    same operation, each optimized for different execution scenarios. Choosing the
    right kernel for each operation is essential for maximizing computational throughput,
    minimizing memory stalls, and ensuring that the accelerator’s specialized processing
    elements are fully utilized ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel selection builds upon the graph optimization phase, ensuring that the
    structured execution plan is mapped to the most efficient implementation available.
    While graph optimization eliminates inefficiencies at the model level, kernel
    selection ensures that each individual operation is executed using the most efficient
    hardware-specific routine. The effectiveness of this process directly impacts
    the model’s overall performance, as poor kernel choices can nullify the benefits
    of prior optimizations by introducing unnecessary computation overhead or memory
    bottlenecks ([0001 et al. 2018a](ch058.xhtml#ref-chen_tvmlang_2018)).
  prefs: []
  type: TYPE_NORMAL
- en: In a Transformer model, the matrix multiplications that dominate self-attention
    computations can be executed using different strategies depending on the available
    hardware. On a CPU, a general-purpose matrix multiplication routine is typically
    employed, exploiting vectorized execution to improve efficiency. In contrast,
    on a GPU, the compiler may select an implementation that leverages tensor cores
    to accelerate matrix multiplications using mixed-precision arithmetic. When the
    model is deployed on a TPU, the operation can be mapped onto a systolic array,
    ensuring that data flows through the accelerator in a manner that maximizes reuse
    and minimizes off-chip memory accesses. Additionally, for inference workloads,
    an integer arithmetic kernel may be preferable, as it facilitates computations
    in INT8 instead of floating-point precision, thereby reducing power consumption
    without significantly compromising accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, compilers do not generate custom kernels from scratch but instead
    select from vendor-optimized kernel libraries that provide highly tuned implementations
    for different architectures. For instance, cuDNN and cuBLAS offer optimized kernels
    for deep learning on NVIDIA GPUs, while oneDNN provides optimized execution for
    Intel architectures. Similarly, ACL (Arm Compute Library) is optimized for Arm-based
    devices, and Eigen and BLIS provide efficient CPU-based implementations of deep
    learning operations. These libraries allow the compiler to choose pre-optimized,
    high-performance kernels rather than having to reinvent execution strategies for
    each hardware platform.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in AI Compilers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AI compilers use heuristics, profiling, and cost models to determine the best
    kernel for each operation. These strategies ensure that each computation is executed
    in a way that maximizes throughput and minimizes memory bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: In rule-based selection, the compiler applies predefined heuristics based on
    the known capabilities of the hardware. For instance, XLA, the compiler used in
    TensorFlow, automatically selects tensor core-optimized kernels for NVIDIA GPUs
    when mixed-precision execution is enabled. These predefined rules allow the compiler
    to make fast, reliable decisions about which kernel to use without requiring extensive
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Profile-guided selection takes a more dynamic approach, benchmarking different
    kernel options and choosing the one that performs best for a given workload. TVM,
    an open-source AI compiler, uses AutoTVM to empirically evaluate kernel performance,
    tuning execution strategies based on real-world execution times. By testing different
    kernels before deployment, profile-guided selection helps ensure that operations
    are assigned to the most efficient implementation under actual execution conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach, cost model-based selection, relies on performance predictions
    to estimate execution time and memory consumption for various kernels before choosing
    the most efficient one. MLIR, a compiler infrastructure designed for machine learning
    workloads, applies this technique to determine the most effective tiling and memory
    access strategies ([Lattner et al. 2020](ch058.xhtml#ref-mlir_framework_2021)).
    By modeling how different kernels interact with the accelerator’s compute units
    and memory hierarchy, the compiler can select the kernel that minimizes execution
    cost while maximizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Many AI compilers also incorporate precision-aware kernel selection, where the
    selected kernel is optimized for specific numerical formats such as FP32, FP16,
    BF16, or INT8\. Training workloads often prioritize higher precision (FP32, BF16)
    to maintain model accuracy, whereas inference workloads favor lower precision
    (FP16, INT8) to increase speed and reduce power consumption. For example, an NVIDIA
    GPU running inference with TensorRT can dynamically select FP16 or INT8 kernels
    based on a model’s accuracy constraints. This trade-off between precision and
    performance is a key aspect of kernel selection, especially when deploying models
    in resource-constrained environments.
  prefs: []
  type: TYPE_NORMAL
- en: Some compilers go beyond static kernel selection and implement adaptive kernel
    tuning, where execution strategies are adjusted at runtime based on the system’s
    workload and available resources. AutoTVM in TVM measures kernel performance across
    different workloads and dynamically refines execution strategies. TensorRT applies
    real-time optimizations based on batch size, memory constraints, and GPU load,
    adjusting kernel selection dynamically. Google’s TPU compiler takes a similar
    approach, optimizing kernel selection based on cloud resource availability and
    execution environment constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Selection Importance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The efficiency of AI acceleration depends not only on how computations are structured
    but also on how they are executed. Even the best-designed computation graph will
    fail to achieve peak performance if the selected kernels do not fully utilize
    the hardware’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Proper kernel selection allows models to execute using the most efficient algorithms
    available for the given hardware, ensuring that memory is accessed in a way that
    avoids unnecessary stalls and that specialized acceleration features, such as
    tensor cores or systolic arrays, are leveraged wherever possible. Selecting an
    inappropriate kernel can lead to underutilized compute resources, excessive memory
    transfers, and increased power consumption, all of which limit the performance
    of AI accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if a Transformer model running on a GPU is assigned a non-tensor-core
    kernel for its matrix multiplications, it may execute at only a fraction of the
    possible performance. Conversely, if a model designed for FP32 execution is forced
    to run on an INT8-optimized kernel, it may experience significant numerical instability,
    degrading accuracy. These choices illustrate why kernel selection is as much about
    maintaining numerical correctness as it is about optimizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: With kernel selection complete, the next stage in compilation involves execution
    scheduling and memory management, where the compiler determines how kernels are
    launched and how data is transferred between different levels of the memory hierarchy.
    These final steps in the compilation pipeline ensure that computations run with
    maximum parallelism while minimizing the overhead of data movement. As kernel
    selection determines what to execute, execution scheduling and memory management
    dictate when and how those kernels are executed, ensuring that AI accelerators
    operate at peak efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The memory planning phase ensures that data is allocated and accessed in a way
    that minimizes memory bandwidth consumption, reduces latency, and maximizes cache
    efficiency ([Y. Zhang, Li, and Ouyang 2020](ch058.xhtml#ref-zhang2020optimizing)).
    Even with the most optimized execution plan, a model can still suffer from severe
    performance degradation if memory is not managed efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning workloads are often memory-intensive. They require frequent
    movement of large tensors between different levels of the memory hierarchy. The
    compiler must determine how tensors are stored, how they are accessed, and how
    intermediate results are handled to ensure that memory does not become a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: The memory planning phase focuses on optimizing tensor layouts, memory access
    patterns, and buffer reuse to prevent unnecessary stalls and memory contention
    during execution. In this phase, tensors are arranged in a memory-efficient format
    that aligns with hardware access patterns, thereby minimizing the need for format
    conversions. Additionally, memory accesses are structured to reduce cache misses
    and stalls, which in turn lowers overall bandwidth consumption. Buffer reuse is
    also a critical aspect, as it reduces redundant memory allocations by intelligently
    managing intermediate results. Together, these strategies ensure that data is
    efficiently placed and accessed, thereby enhancing both computational performance
    and energy efficiency in AI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in AI Compilers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory planning is a complex problem because AI models must balance memory availability,
    reuse, and access efficiency while operating across multiple levels of the memory
    hierarchy. AI compilers use several key strategies to manage memory effectively
    and prevent unnecessary data movement.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in memory planning is tensor layout optimization, where the compiler
    determines how tensors should be arranged in memory to maximize locality and prevent
    unnecessary data format conversions. Different hardware accelerators have different
    preferred storage layouts—for instance, NVIDIA GPUs often use row-major storage
    (NHWC format), while TPUs favor channel-major layouts (NCHW format) to optimize
    memory coalescing ([Martín Abadi et al. 2016](ch058.xhtml#ref-abadi2016tensorflow)).
    The compiler automatically transforms tensor layouts based on the expected access
    patterns of the target hardware, ensuring that memory accesses are aligned for
    maximum efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond layout optimization, memory planning also includes buffer allocation
    and reuse, where the compiler minimizes memory footprint by reusing intermediate
    storage whenever possible. Deep learning workloads generate many temporary tensors,
    such as activations and gradients, which can quickly overwhelm on-chip memory
    if not carefully managed. Instead of allocating new memory for each tensor, the
    compiler analyzes the computation graph to identify opportunities for buffer reuse,
    ensuring that intermediate values are stored and overwritten efficiently ([G.
    A. Jones 2018](ch058.xhtml#ref-moreau2018relay)).
  prefs: []
  type: TYPE_NORMAL
- en: Another critical aspect of memory planning is minimizing data movement between
    different levels of the memory hierarchy. AI accelerators typically have a mix
    of high-speed on-chip memory (such as caches or shared SRAM) and larger, but slower,
    external DRAM. If tensor data is repeatedly moved between these memory levels,
    the model may become memory-bound, reducing computational efficiency. To prevent
    this, compilers use tiling strategies that break large computations into smaller,
    memory-friendly chunks, allowing execution to fit within fast, local memory and
    reducing the need for costly off-chip memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Planning Importance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without proper memory planning, even the most optimized computation graph and
    kernel selection will fail to deliver high performance. Excessive memory transfers,
    inefficient memory layouts, and redundant memory allocations can all lead to bottlenecks
    that prevent AI accelerators from reaching their peak throughput.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a CNN running on a GPU may achieve high computational efficiency
    in theory, but if its convolutional feature maps are stored in an incompatible
    format, for example, if it uses a row-major layout that necessitates conversion
    to a channel-friendly format such as NCHW or a variant like NHCW, constant tensor
    format conversions can introduce significant overhead. Similarly, a Transformer
    model deployed on an edge device may struggle to meet real-time inference requirements
    if memory is not carefully planned, leading to frequent off-chip memory accesses
    that increase latency and power consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Through careful management of tensor placement, optimizing memory access patterns,
    and reducing unnecessary data movement, memory planning guarantees efficient operation
    of AI accelerators, leading to tangible performance improvements in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With graph optimization completed, kernels selected, and memory planning finalized,
    the next step in the compilation pipeline is computation scheduling. This phase
    determines when and where each computation should be executed, ensuring that workloads
    are efficiently distributed across available processing elements while avoiding
    unnecessary stalls and resource contention ([Rajbhandari et al. 2020a](ch058.xhtml#ref-Rajbhandari2020);
    [Zheng et al. 2020](ch058.xhtml#ref-Zheng2020)).
  prefs: []
  type: TYPE_NORMAL
- en: AI accelerators achieve high performance through massive parallelism, but without
    an effective scheduling strategy, computational units may sit idle, memory bandwidth
    may be underutilized, and execution efficiency may degrade. Computation scheduling
    is responsible for ensuring that all processing elements remain active, execution
    dependencies are managed correctly, and workloads are distributed optimally ([Ziheng
    Jia et al. 2019](ch058.xhtml#ref-Jia2019)).
  prefs: []
  type: TYPE_NORMAL
- en: In the scheduling phase, parallel execution, synchronization, and resource allocation
    are managed systematically. Task partitioning decomposes extensive computations
    into smaller, manageable tasks that can be distributed efficiently among multiple
    compute cores. Execution order optimization then determines the most effective
    sequence for launching these operations, maximizing hardware performance while
    reducing execution stalls. Additionally, resource allocation and synchronization
    are orchestrated to ensure that compute cores, memory bandwidth, and shared caches
    are utilized effectively, avoiding contention. Through these coordinated strategies,
    computation scheduling achieves optimal hardware utilization, minimizes memory
    access delays, and supports a streamlined and efficient execution process.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in AI Compilers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computation scheduling is highly dependent on the underlying hardware architecture,
    as different AI accelerators have unique execution models that must be considered
    when determining how workloads are scheduled. AI compilers implement several key
    strategies to optimize scheduling for efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most fundamental aspects of scheduling is task partitioning, where
    the compiler divides large computational graphs into smaller, manageable units
    that can be executed in parallel. On GPUs, this typically means mapping matrix
    multiplications and convolutions to thousands of CUDA cores, while on TPUs, tasks
    are partitioned to fit within systolic arrays that operate on structured data
    flows ([Norrie et al. 2021](ch058.xhtml#ref-norrie2021design)). In CPUs, partitioning
    is often focused on breaking computations into vectorized chunks that align with
    SIMD execution. The goal is to map workloads to available processing units efficiently,
    ensuring that each core remains active throughout execution.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling involves optimizing execution order to minimize dependencies and
    maximize throughput beyond task partitioning. Many AI models include operations
    that can be computed independently (e.g., different batches in a batch processing
    pipeline) alongside operations that have strict dependencies (e.g., recurrent
    layers in an RNN). AI compilers analyze these dependencies and attempt to rearrange
    execution where possible, reducing idle time and improving parallel efficiency.
    For example, in Transformer models, scheduling may prioritize preloading attention
    matrices into memory while earlier layers are still executing, ensuring that data
    is ready when needed ([Shoeybi et al. 2019b](ch058.xhtml#ref-Shoeybi2019)).
  prefs: []
  type: TYPE_NORMAL
- en: Another crucial aspect of computation scheduling is resource allocation and
    synchronization, where the compiler determines how compute cores share memory
    and coordinate execution. Modern AI accelerators often support overlapping computation
    and data transfers, meaning that while one task executes, the next task can begin
    fetching its required data. Compilers take advantage of this by scheduling tasks
    in a way that hides memory latency, ensuring that execution remains compute-bound
    rather than memory-bound ([0001 et al. 2018b](ch058.xhtml#ref-Chen2018)). TensorRT
    and XLA, for example, employ streaming execution strategies where multiple kernels
    are launched in parallel, and synchronization is carefully managed to prevent
    execution stalls ([Google 2025](ch058.xhtml#ref-GoogleXLA)).
  prefs: []
  type: TYPE_NORMAL
- en: Computation Scheduling Importance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without effective scheduling, even the most optimized model can suffer from
    underutilized compute resources, memory bottlenecks, and execution inefficiencies.
    Poor scheduling decisions can lead to idle processing elements, forcing expensive
    compute cores to wait for data or synchronization events before continuing execution.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a CNN running on a GPU may have highly optimized kernels and efficient
    memory layouts, but if its execution is not scheduled correctly, compute units
    may remain idle between kernel launches, reducing throughput. Similarly, a Transformer
    model deployed on a TPU may perform matrix multiplications efficiently but could
    experience performance degradation if attention layers are not scheduled to overlap
    efficiently with memory transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Effective computation scheduling occupies a central role in the orchestration
    of parallel workloads, ensuring that processing elements are utilized to their
    fullest capacity while preventing idle cores—a critical aspect for maximizing
    overall throughput. By strategically overlapping computation with data movement,
    the scheduling mechanism effectively conceals memory latency, thereby preventing
    operational stalls during data retrieval. By resolving execution dependencies
    with precision, it minimizes waiting periods and enhances the concurrent progression
    of computation and data transfer. This systematic integration of scheduling and
    data handling serves to not only elevate performance but also exemplify the rigorous
    engineering principles that underpin modern accelerator design.
  prefs: []
  type: TYPE_NORMAL
- en: Code Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike the previous phases, which required AI-specific optimizations, code generation
    follows many of the same principles as traditional compilers. This process includes
    instruction selection, register allocation, and final optimization passes, ensuring
    that execution makes full use of hardware-specific features such as vectorized
    execution, memory prefetching, and instruction reordering.
  prefs: []
  type: TYPE_NORMAL
- en: For CPUs and GPUs, AI compilers typically generate machine code or optimized
    assembly instructions, while for TPUs, FPGAs[30](#fn30), and other accelerators,
    the output may be optimized bytecode or execution graphs that are interpreted
    by the hardware’s runtime system.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the compilation pipeline is complete: the original high-level
    model representation has been transformed into an optimized, executable format
    tailored for efficient execution on the target hardware. The combination of graph
    transformations, kernel selection, memory-aware execution, and parallel scheduling
    ensures that AI accelerators run workloads with maximum efficiency, minimal memory
    overhead, and optimal computational throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: Compilation-Runtime Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The compiler plays a fundamental role in AI acceleration, transforming high-level
    machine learning models into optimized execution plans tailored to the constraints
    of specialized hardware. Throughout this section, we have seen how graph optimization
    restructures computation, kernel selection maps operations to hardware-efficient
    implementations, memory planning optimizes data placement, and computation scheduling
    ensures efficient parallel execution. Each of these phases is crucial in enabling
    AI models to fully leverage modern accelerators, ensuring high throughput, minimal
    memory overhead, and efficient execution pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: However, compilation alone is not enough to guarantee efficient execution in
    real-world AI workloads. While compilers statically optimize computation based
    on known model structures and hardware capabilities, AI execution environments
    are often dynamic and unpredictable. Batch sizes fluctuate, hardware resources
    may be shared across multiple workloads, and accelerators must adapt to real-time
    performance constraints. In these cases, a static execution plan is insufficient,
    and runtime management becomes critical in ensuring that models execute optimally
    under real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: This transition from static compilation to adaptive execution is where AI runtimes
    come into play. Runtimes provide dynamic memory allocation, real-time kernel selection,
    workload scheduling, and multi-chip coordination, allowing AI models to adapt
    to varying execution conditions while maintaining efficiency. In the next section,
    we explore how AI runtimes extend the capabilities of compilers, enabling models
    to run effectively in diverse and scalable deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While compilers optimize AI models before execution, real-world deployment introduces
    dynamic and unpredictable conditions that static compilation alone cannot fully
    address ([NVIDIA 2021](ch058.xhtml#ref-nvidia_tensorRT_2021)). AI workloads operate
    in varied execution environments, where factors such as fluctuating batch sizes,
    shared hardware resources, memory contention, and latency constraints necessitate
    real-time adaptation. Precompiled execution plans, optimized for a fixed set of
    assumptions, may become suboptimal when actual runtime conditions change.
  prefs: []
  type: TYPE_NORMAL
- en: To bridge this gap, AI runtimes provide a dynamic layer of execution management,
    extending the optimizations performed at compile time with real-time decision-making.
    Unlike traditional compiled programs that execute a fixed sequence of instructions,
    AI workloads require adaptive control over memory allocation, kernel execution,
    and resource scheduling. AI runtimes continuously monitor execution conditions
    and make on-the-fly adjustments to ensure that machine learning models fully utilize
    available hardware while maintaining efficiency and performance guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, AI runtimes manage three critical aspects of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel Execution Management**: AI runtimes dynamically select and dispatch
    computation kernels based on the current system state, ensuring that workloads
    are executed with minimal latency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory Adaptation and Allocation**: Since AI workloads frequently process
    large tensors with varying memory footprints, runtimes adjust memory allocation
    dynamically to prevent bottlenecks and excessive data movement ([Y. Huang et al.
    2019](ch058.xhtml#ref-deepmind_gpipe_2019)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execution Scaling**: AI runtimes handle workload distribution across multiple
    accelerators, supporting large-scale execution in multi-chip, multi-node, or cloud
    environments ([Mirhoseini et al. 2017](ch058.xhtml#ref-mirhoseini_device_placement_2017)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By dynamically handling these execution aspects, AI runtimes complement compiler-based
    optimizations, ensuring that models continue to perform efficiently under varying
    runtime conditions. The next section explores how AI runtimes differ from traditional
    software runtimes, highlighting why machine learning workloads require fundamentally
    different execution strategies compared to conventional CPU-based programs.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Architecture Differences for ML Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional software runtimes are designed for managing general-purpose program
    execution, primarily handling sequential and multi-threaded workloads on CPUs.
    These runtimes allocate memory, schedule tasks, and optimize execution at the
    level of individual function calls and instructions. In contrast, AI runtimes
    are specialized for machine learning workloads, which require massively parallel
    computation, large-scale tensor operations, and dynamic memory management.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.19](ch017.xhtml#tbl-runtime-comparison) highlights the fundamental
    differences between traditional and AI runtimes. One of the key distinctions lies
    in execution flow. Traditional software runtimes operate on a predictable, structured
    execution model where function calls and CPU threads follow a predefined control
    path. AI runtimes, however, execute computational graphs, requiring complex scheduling
    decisions that account for dependencies between tensor operations, parallel kernel
    execution, and efficient memory access.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.19: **Runtime Execution Models**: Traditional and AI runtimes diverge
    in their execution approaches; traditional runtimes prioritize sequential or multi-threaded
    instruction processing, while AI runtimes leverage massively parallel tensor operations
    for accelerated computation on machine learning workloads. This distinction necessitates
    specialized AI runtime architectures designed for efficient parallelization and
    memory management of large-scale tensor data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Traditional Runtime** | **AI Runtime** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Execution Model** | Sequential or multi-threaded execution | Massively
    parallel tensor execution |'
  prefs: []
  type: TYPE_TB
- en: '| **Task Scheduling** | CPU thread management | Kernel dispatch across accelerators
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Management** | Static allocation (stack/heap) | Dynamic tensor allocation,
    buffer reuse |'
  prefs: []
  type: TYPE_TB
- en: '| **Optimization Priorities** | Low-latency instruction execution | Minimizing
    memory stalls, maximizing parallel execution |'
  prefs: []
  type: TYPE_TB
- en: '| **Adaptability** | Mostly static execution plan | Adapts to batch size and
    hardware availability |'
  prefs: []
  type: TYPE_TB
- en: '| **Target Hardware** | CPUs (general-purpose execution) | GPUs, TPUs, and
    custom accelerators |'
  prefs: []
  type: TYPE_TB
- en: Memory management is another major differentiator. Traditional software runtimes
    handle small, frequent memory allocations, optimizing for cache efficiency and
    low-latency access. AI runtimes, in contrast, must dynamically allocate, reuse,
    and optimize large tensors, ensuring that memory access patterns align with accelerator-friendly
    execution. Poor memory management in AI workloads can lead to performance bottlenecks,
    particularly due to excessive off-chip memory transfers and inefficient cache
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: AI runtimes are inherently designed for adaptability. While traditional runtimes
    often follow a mostly static execution plan, AI workloads typically operate in
    highly variable execution environments, such as cloud-based accelerators or multi-tenant
    hardware. As a result, AI runtimes must continuously adjust batch sizes, reallocate
    compute resources, and manage real-time scheduling decisions to maintain high
    throughput and minimize execution delays.
  prefs: []
  type: TYPE_NORMAL
- en: These distinctions demonstrate why AI runtimes require fundamentally different
    execution strategies compared to traditional software runtimes. Rather than simply
    managing CPU processes, AI runtimes must oversee large-scale tensor execution,
    multi-device coordination, and real-time workload adaptation to ensure that machine
    learning models can run efficiently under diverse and ever-changing deployment
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Kernel Execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dynamic kernel execution is the process of mapping machine learning models to
    hardware and optimizing runtime execution. While static compilation provides a
    solid foundation, efficient execution of machine learning workloads requires real-time
    adaptation to fluctuating conditions such as available memory, data sizes, and
    computational loads. The runtime functions as an intermediary that continuously
    adjusts execution strategies to match both the constraints of the underlying hardware
    and the characteristics of the workload.
  prefs: []
  type: TYPE_NORMAL
- en: When mapping a machine learning model to hardware, individual computational
    operations, including matrix multiplications, convolutions, and activation functions,
    must be assigned to the most appropriate processing units. This mapping is not
    fixed; it must be modified during runtime in response to changes in input data,
    memory availability, and overall system load. Dynamic kernel execution allows
    the runtime to make real-time decisions regarding kernel selection, execution
    order, and memory management, ensuring that workloads remain efficient despite
    these changing conditions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an AI accelerator executing a deep neural network (DNN)
    for image classification. If an incoming batch of high-resolution images requires
    significantly more memory than expected, a statically planned execution may cause
    cache thrashing or excessive off-chip memory accesses. Instead, a dynamic runtime
    can adjust tiling strategies on the fly, breaking down tensor operations into
    smaller tiles that fit within the high-speed on-chip memory. This prevents memory
    stalls and ensures optimal utilization of caches.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when running a transformer-based NLP model, the sequence length of
    input text may vary between inference requests. A static execution plan optimized
    for a fixed sequence length may lead to underutilization of compute resources
    when processing shorter sequences or excessive memory pressure with longer sequences.
    Dynamic kernel execution can mitigate this by selecting different kernel implementations
    based on the actual sequence length, dynamically adjusting memory allocations
    and execution strategies to maintain efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Overlapping computation with memory movement is a vital strategy to mitigate
    performance bottlenecks. AI workloads often encounter delays due to memory-bound
    issues, where data movement between memory hierarchies limits computation speed.
    To combat this, AI runtimes implement techniques like asynchronous execution and
    double buffering, ensuring that computations proceed without waiting for memory
    transfers to complete. In a large-scale model, for instance, image data can be
    prefetched while computations are performed on the previous batch, thus maintaining
    a steady flow of data and avoiding pipeline stalls.
  prefs: []
  type: TYPE_NORMAL
- en: Another practical example is the execution of convolutional layers in a CNN
    on a GPU. If multiple convolution kernels need to be scheduled, a static scheduling
    approach may lead to inefficient resource utilization due to variation in layer
    sizes and compute requirements. By dynamically scheduling kernel execution, AI
    runtimes can prioritize smaller kernels when compute units are partially occupied,
    improving hardware utilization. For instance, in NVIDIA’s TensorRT runtime, fusion
    of small kernels into larger execution units is done dynamically to avoid launch
    overhead, optimizing latency-sensitive inference tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic kernel execution plays an essential role in ensuring that machine learning
    models are executed efficiently. By dynamically adjusting execution strategies
    in response to real-time system conditions, AI runtimes optimize both training
    and inference performance across various hardware platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Runtime Kernel Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While compilers may perform an initial selection of kernels based on static
    analysis of the machine learning model and hardware target, AI runtimes often
    need to override these decisions during execution. Real-time factors, such as
    available memory, hardware utilization, and workload priorities, may differ significantly
    from the assumptions made during compilation. By dynamically selecting and switching
    kernels at runtime, AI runtimes can adapt to these changing conditions, ensuring
    that models continue to perform efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider transformer-based language models, where a significant
    portion of execution time is spent on matrix multiplications. The AI runtime must
    determine the most efficient way to execute these operations based on the current
    system state. If the model is running on a GPU with specialized Tensor Cores,
    the runtime may switch from a standard FP32 kernel to an FP16 kernel to take advantage
    of hardware acceleration ([Shoeybi et al. 2019a](ch058.xhtml#ref-shoeybi_megatron_2020)).
    Conversely, if the lower precision of FP16 causes unacceptable numerical instability,
    the runtime can opt for mixed-precision execution, selectively using FP32 where
    higher precision is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Memory constraints also influence kernel selection. When memory bandwidth is
    limited, the runtime may adjust its execution strategy, reordering operations
    or changing the tiling strategy to fit computations into the available cache rather
    than relying on slower main memory. For example, a large matrix multiplication
    may be broken into smaller chunks, ensuring that the computation fits into the
    on-chip memory of the GPU, reducing overall latency.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, batch size can influence kernel selection. For workloads that
    handle a mix of small and large batches, the AI runtime may choose a latency-optimized
    kernel for small batches and a throughput-optimized kernel for large-scale batch
    processing. This adjustment ensures that the model continues to operate efficiently
    across different execution scenarios, without the need for manual tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Scheduling and Utilization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the AI runtime selects an appropriate kernel, the next step is scheduling
    it in a way that maximizes parallelism and resource utilization. Unlike traditional
    task schedulers, which are designed to manage CPU threads, AI runtimes must coordinate
    a much larger number of tasks across parallel execution units such as GPU cores,
    tensor processing units, or custom AI accelerators ([Norman P. Jouppi et al. 2017a](ch058.xhtml#ref-google_tpu_2017)).
    Effective scheduling ensures that these computational resources are kept fully
    engaged, preventing bottlenecks and maximizing throughput.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in image recognition models that use convolutional layers, operations
    can be distributed across multiple processing units, enabling different filters
    to run concurrently. This parallelization ensures that the available hardware
    is fully utilized, speeding up execution. Similarly, batch normalization and activation
    functions must be scheduled efficiently to avoid unnecessary delays. If these
    operations are not interleaved with other computations, they may block the pipeline
    and reduce overall throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient kernel scheduling can also be influenced by real-time memory management
    . AI runtimes ensure that intermediate data, such as feature maps in deep neural
    networks, are preloaded into cache before they are needed. This proactive management
    helps prevent delays caused by waiting for data to be loaded from slower memory
    tiers, ensuring continuous execution.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques enable AI runtimes to ensure optimal resource utilization and
    efficient parallel computation, which are essential for the high-performance execution
    of machine learning models, particularly in environments that require scaling
    across multiple hardware accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler and runtime systems examined thus far optimize execution within
    single accelerators—managing computation mapping, memory hierarchies, and kernel
    scheduling. While these single-chip optimizations achieve impressive performance
    gains, modern AI workloads increasingly exceed what any individual chip can deliver.
    Training GPT-3 would require running a single H100 continuously for 10 years,
    consuming 314 sextillion floating-point operations. Real-time inference serving
    for global applications demands throughput beyond any single accelerator’s capacity.
    These computational requirements, rooted in the scaling laws from [Chapter 9](ch015.xhtml#sec-efficient-ai),
    necessitate a fundamental shift from single-chip optimization to distributed acceleration
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Chip AI Acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transition from single-chip to multi-chip architectures represents more
    than simple replication—it requires rethinking how computations distribute across
    processors, how data flows between chips, and how systems maintain coherence at
    scale. Where single-chip optimization focuses on maximizing utilization within
    fixed resources, multi-chip systems must balance computational distribution against
    communication overhead, memory coherence costs, and synchronization complexity.
    These challenges fundamentally transform the optimization landscape, requiring
    new abstractions and techniques beyond those developed for individual accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Modern AI workloads increasingly demand computational resources that exceed
    the capabilities of single-chip accelerators. This section examines how AI systems
    scale from individual processors to multi-chip architectures, analyzing the motivation
    behind different scaling approaches and their impact on system design. These scaling
    considerations are fundamental to the distributed training strategies covered
    in [Chapter 8](ch014.xhtml#sec-ai-training) and the operational challenges discussed
    in [Chapter 13](ch019.xhtml#sec-ml-operations). The security implications of distributed
    acceleration, particularly around model protection and data privacy, are examined
    in [Chapter 15](ch021.xhtml#sec-security-privacy). By understanding this progression,
    we can better appreciate how each component of the AI hardware stack, ranging
    from compute units to memory systems, must adapt to support large-scale machine
    learning workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The scaling of AI systems follows a natural progression, starting with integration
    within a single package through chiplet architectures, extending to multi-GPU
    configurations within a server, expanding to distributed accelerator pods, and
    culminating in wafer-scale integration. Each approach presents unique trade-offs
    between computational density, communication overhead, and system complexity.
    For instance, chiplet architectures maintain high-speed interconnects within a
    package, while distributed systems sacrifice communication latency for massive
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these scaling strategies is essential for several reasons. First,
    it provides insight into how different hardware architectures address the growing
    computational demands of AI workloads. Second, it reveals the fundamental challenges
    that arise when extending beyond single-chip execution, such as managing inter-chip
    communication and coordinating distributed computation. Finally, it establishes
    the foundation for subsequent discussions on how mapping strategies, compilation
    techniques, and runtime systems evolve to support efficient execution at scale.
  prefs: []
  type: TYPE_NORMAL
- en: The progression begins with chiplet architectures, which represent the most
    tightly integrated form of multi-chip scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Chiplet-Based Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chiplet[31](#fn31) architectures achieve this scaling by partitioning large
    designs into smaller, modular dies that are interconnected within a single package,
    as illustrated in [Figure 11.9](ch017.xhtml#fig-AMD_chiplet_based).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: **Chiplet Interconnect**: Modern AI accelerators partition large
    designs into smaller chiplets and connect them via high-bandwidth interconnects,
    enabling scalability beyond monolithic die limitations and improving manufacturing
    yields. HBM stacks provide fast access to data, crucial for the memory-intensive
    workloads common in machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern AI accelerators, such as AMD’s Instinct MI300, take this approach by
    integrating multiple compute chiplets alongside memory chiplets, linked by high-speed
    die-to-die interconnects. This modular design allows manufacturers to bypass the
    manufacturing limits of monolithic chips while still achieving high-density compute.
  prefs: []
  type: TYPE_NORMAL
- en: However, even within a single package, scaling is not without challenges. Inter-chiplet
    communication latency, memory coherence[32](#fn32), and thermal management become
    critical factors as more chiplets are integrated. Unlike traditional multi-chip
    systems, chiplet-based designs must carefully balance latency-sensitive workloads
    across multiple dies without introducing excessive bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs
    working together. In multi-GPU systems, each accelerator has its own dedicated
    memory and compute resources, but they must efficiently share data and synchronize
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: A common example is NVIDIA DGX systems, which integrate multiple GPUs connected
    via NVLink or PCIe. This architecture enables workloads to be split across GPUs,
    typically using data parallelism (where each GPU processes a different batch of
    data) or model parallelism (where different GPUs handle different parts of a neural
    network) ([Ben-Nun and Hoefler 2019](ch058.xhtml#ref-Ben-Nun2019data)). These
    parallelization strategies are explored in depth in [Chapter 8](ch014.xhtml#sec-ai-training).
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in [Figure 11.10](ch017.xhtml#fig-multi-gpu), NVSwitch interconnects
    enable high-speed communication between GPUs, reducing bottlenecks in distributed
    training. However, scaling up the number of GPUs introduces fundamental distributed
    coordination challenges that become the dominant performance constraint. The arithmetic
    intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient
    synchronization across GPUs, where AllReduce operations must aggregate 175 billion
    parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth,
    but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs
    simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds
    available capacity. The coordination complexity compounds exponentially—while
    two GPUs require a single communication channel, eight GPUs need 28 interconnect
    paths, and fault tolerance requirements mandate redundant communication patterns.
    Memory consistency protocols further complicate coordination as different GPUs
    may observe weight updates at different times, requiring sophisticated synchronization
    primitives that can add 10-50μs latency per training step—seemingly small delays
    that aggregate to hours of training time across million-iteration runs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file189.svg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: **Multi-GPU Scaling**: NVSwitch interconnects enable high-bandwidth,
    low-latency communication between GPUs, overcoming PCIe bottlenecks for distributed
    training of large models. Scaling GPU count introduces challenges in maintaining
    memory consistency and efficiently scheduling workloads across interconnected
    devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Communication Overhead and Amdahl’s Law Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The fundamental limitation of distributed AI training stems from Amdahl’s Law,
    which quantifies how communication overhead constrains parallel speedup regardless
    of available compute power. For distributed neural network training, communication
    overhead during gradient synchronization creates a sequential bottleneck that
    limits scalability even with infinite parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum speedup achievable with distributed training is bound by Amdahl’s
    Law: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}</annotation></semantics>
    where <semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    is the fraction of work that can be parallelized and <semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics> is the number of processors.
    However, for AI training, communication overhead introduces additional sequential
    time: <semantics><mrow><msub><mtext mathvariant="normal">Speedup</mtext><mtext
    mathvariant="normal">AI</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac><mo>+</mo><mfrac><mi>C</mi><mi>N</mi></mfrac></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\text{Speedup}_{\text{AI}} = \frac{1}{(1-P)
    + \frac{P}{N} + \frac{C}{N}}</annotation></semantics> where <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> represents the communication
    overhead fraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider training a 175 B parameter model with 1000 H100 GPUs as a concrete
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computation time per iteration**: 100 ms of forward/backward passes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication time**: AllReduce of 175 B parameters (700 GB in FP32) across
    1000 GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Available bandwidth**: 600 GB/s per NVSwitch link'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communication overhead**: <semantics><mrow><mfrac><mrow><mn>700</mn><mtext
    mathvariant="normal">GB</mtext></mrow><mrow><mn>600</mn><mtext mathvariant="normal">GB/s</mtext></mrow></mfrac><mo>×</mo><msub><mo>log</mo><mn>2</mn></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1000</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>11.6</mn><mtext
    mathvariant="normal">ms</mtext></mrow><annotation encoding="application/x-tex">\frac{700\text{GB}}{600\text{GB/s}}
    \times \log_2(1000) \approx 11.6\text{ms}</annotation></semantics>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if only 5% of training requires communication (P = 0.95), the maximum
    speedup is: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mn>0.05</mn><mo>+</mo><mfrac><mn>0.95</mn><mn>1000</mn></mfrac><mo>+</mo><mfrac><mn>0.116</mn><mn>100</mn></mfrac></mrow></mfrac><mo>≈</mo><mn>8.3</mn><mtext
    mathvariant="normal">x</mtext></mrow> <annotation encoding="application/x-tex">\text{Speedup}
    = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{x}</annotation></semantics>'
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates why adding more GPUs beyond ~100 provides diminishing returns
    for large model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Communication requirements scale superlinearly with model size and linearly
    with the number of parameters. Modern transformer models require gradient synchronization
    across all parameters during each training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-3 (175 B parameters)**: 700 GB gradient exchange per step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4 (estimated 1.8 T parameters)**: ~7 TB gradient exchange per step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Future 10 T parameter models**: ~40 TB gradient exchange per step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even with advanced interconnects like NVLink 4.0 (1.8 TB/s), gradient synchronization
    for 10 T parameter models would require 22+ seconds per training step, making
    distributed training impractical without algorithmic innovations like gradient
    compression or asynchronous updates.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU systems face additional bottlenecks from memory bandwidth competition.
    When 8 H100 GPUs simultaneously access HBM during gradient computation, the effective
    memory bandwidth per GPU drops from 3.35 TB/s to approximately 2.1 TB/s due to
    memory controller contention and NUMA effects. This 37% reduction in memory performance
    compounds communication overhead, further limiting scalability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding Amdahl’s Law guides optimization strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Compression**: Reduce communication volume by 10-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> through sparsification
    and quantization'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pipeline Parallelism**: Overlap communication with computation to hide gradient
    synchronization latency'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Parallelism**: Partition models across devices to reduce gradient synchronization
    requirements'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Asynchronous Updates**: Relax consistency requirements to eliminate synchronization
    barriers'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These techniques modify the effective value of <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> and <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> in Amdahl’s equation,
    enabling better scaling behavior at the cost of algorithmic complexity.
  prefs: []
  type: TYPE_NORMAL
- en: TPU Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As models and datasets continue to expand, training and inference workloads
    must extend beyond single-server configurations. This scaling requirement has
    led to the development of sophisticated distributed systems where multiple accelerators
    communicate across networks. Google’s TPU Pods represent a pioneering approach
    to this challenge, interconnecting hundreds of TPUs to function as a unified system
    ([Norman P. Jouppi et al. 2020](ch058.xhtml#ref-Jouppi2020tpuv4)).
  prefs: []
  type: TYPE_NORMAL
- en: The architectural design of TPU Pods differs fundamentally from traditional
    multi-GPU systems. While multi-GPU configurations typically rely on NVLink or
    PCIe connections within a single machine, TPU Pods employ high-bandwidth optical
    links to interconnect accelerators at data center scale. This design implements
    a 2D torus interconnect topology, enabling efficient data exchange between accelerators
    while minimizing communication bottlenecks as workloads scale across nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of this architecture is demonstrated in its performance scaling
    capabilities. As illustrated in [Figure 11.11](ch017.xhtml#fig-tpu-pod-perf),
    TPU Pod performance exhibits near-linear scaling when running ResNet-50, from
    quarter-pod to full-pod configurations. The system achieves a remarkable 33.0<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> speedup when scaled
    to 1024 chips compared to a 16-TPU baseline. This scaling efficiency is particularly
    noteworthy in larger configurations, where performance continues to scale strongly
    even as the system expands from 128 to 1024 chips.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file190.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: **Scaling Efficiency of TPU Pods**: Increasing the number of
    TPU chips within a pod maintains near-linear performance gains on ResNet-50, achieving
    a 33.0<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    speedup from 16 to 1024 chips. This efficient scaling provides the effectiveness
    of the 2D torus interconnect and high-bandwidth optical links in minimizing communication
    bottlenecks as workloads expand across multiple accelerators.'
  prefs: []
  type: TYPE_NORMAL
- en: However, distributing AI workloads across an entire data center introduces distributed
    coordination challenges that fundamentally differ from single-node systems. The
    2D torus interconnect, while providing high bisection bandwidth, creates communication
    bottlenecks when training large transformer models that require AllReduce operations
    across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through
    the torus network, with worst-case communication requiring 32 hops between distant
    TPUs, creating latency penalties that compound with model size.
  prefs: []
  type: TYPE_NORMAL
- en: The distributed memory architecture exacerbates coordination complexity—unlike
    multi-GPU systems with shared host memory, each TPU node maintains independent
    memory spaces, forcing explicit data marshaling and synchronization protocols.
    Network partition tolerance becomes critical as optical link failures can split
    the pod into disconnected islands, requiring sophisticated consensus algorithms
    to maintain training consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The energy cost of coordination also scales dramatically: moving data across
    the pod’s optical interconnect consumes 1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    more energy than on-chip communication within individual TPUs, transforming distributed
    training into a careful balance between computation parallelism and communication
    efficiency where AllReduce bandwidth, not compute capacity, determines overall
    training throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: Wafer-Scale AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the frontier of AI scaling, wafer-scale[33](#fn33) integration represents
    a paradigm shift—abandoning traditional multi-chip architectures in favor of a
    single, massive AI processor. Rather than partitioning computation across discrete
    chips, this approach treats an entire silicon wafer as a unified compute fabric,
    eliminating the inefficiencies of inter-chip communication.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 11.12](ch017.xhtml#fig-processor-trends), Cerebras’ Wafer-Scale
    Engine (WSE) processors break away from the historical transistor scaling trends
    of CPUs, GPUs, and TPUs. While these architectures have steadily increased transistor
    counts along an exponential trajectory, WSE introduces an entirely new scaling
    paradigm, integrating trillions of transistors onto a single wafer—far surpassing
    even the most advanced GPUs and TPUs. With WSE-3, this trajectory continues, pushing
    wafer-scale AI to unprecedented levels ([Systems 2021a](ch058.xhtml#ref-Cerebras2021wse2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file191.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: **Wafer-Scale Integration**: Wafer-scale AI processors integrate
    trillions of transistors onto a single wafer, offering ultra-fast on-die communication
    to surpass traditional multi-chip architectures and achieve unprecedented performance
    levels.'
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental advantage of wafer-scale AI is its ultra-fast, on-die communication.
    Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries
    between separate devices, wafer-scale AI enables near-instantaneous data transfer
    across its vast compute array. This architecture drastically reduces communication
    latency, unlocking performance levels that are unachievable with conventional
    multi-chip systems.
  prefs: []
  type: TYPE_NORMAL
- en: However, achieving this level of integration introduces formidable engineering
    challenges. Thermal dissipation, fault tolerance, and manufacturing yield become
    major constraints when fabricating a processor of this scale. These sustainability
    challenges, including energy consumption and resource utilization, are examined
    in [Chapter 18](ch024.xhtml#sec-sustainable-ai). Unlike distributed TPU systems,
    which mitigate failures by dynamically re-routing workloads, wafer-scale AI must
    incorporate built-in redundancy mechanisms to tolerate localized defects in the
    silicon. Successfully addressing these challenges is essential to realizing the
    full potential of wafer-scale computing as the next frontier in AI acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: AI Systems Scaling Trajectory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 11.20](ch017.xhtml#tbl-scaling-trajectory) illustrates the progressive
    scaling of AI acceleration, from single-chip processors to increasingly complex
    architectures such as chiplet-based designs, multi-GPU systems, TPU Pods, and
    wafer-scale AI. Each step in this evolution introduces new challenges related
    to data movement, memory access, interconnect efficiency, and workload distribution.
    While chiplets enable modular scaling within a package, they introduce latency
    and memory coherence issues. Multi-GPU systems rely on high-speed interconnects
    like NVLink but face synchronization and communication bottlenecks. TPU Pods push
    scalability further by distributing workloads across clusters, yet they must contend
    with interconnect congestion and workload partitioning. At the extreme end, wafer-scale
    AI integrates an entire wafer into a single computational unit, presenting unique
    challenges in thermal management and fault tolerance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.20: **AI Acceleration Trends**: Scaling AI systems provides increasing
    challenges in data movement and memory access, driving architectural innovations
    from chiplets to wafer-scale integration. Each approach introduces unique trade-offs
    between modularity, latency, and complexity, demanding careful consideration of
    interconnect efficiency and workload distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scaling Approach** | **Key Feature** | **Challenges** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Chiplets** | Modular scaling within a package | Inter-chiplet latency,
    memory coherence |'
  prefs: []
  type: TYPE_TB
- en: '| **Multi-GPU** | External GPU interconnects (NVLink) | Synchronization overhead,
    communication bottlenecks |'
  prefs: []
  type: TYPE_TB
- en: '| **TPU Pods** | Distributed accelerator clusters | Interconnect congestion,
    workload partitioning |'
  prefs: []
  type: TYPE_TB
- en: '| **Wafer-Scale AI** | Entire wafer as a single processor | Thermal dissipation,
    fault tolerance |'
  prefs: []
  type: TYPE_TB
- en: Computation and Memory Scaling Changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As AI systems scale from single-chip accelerators to multi-chip architectures,
    the fundamental challenges in computation and memory evolve. In a single accelerator,
    execution is primarily optimized for locality—ensuring that computations are mapped
    efficiently to available processing elements while minimizing memory access latency.
    However, as AI systems extend beyond a single chip, the scope of these optimizations
    expands significantly. Computation must now be distributed across multiple accelerators,
    and memory access patterns become constrained by interconnect bandwidth and communication
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-chip Execution Mapping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In single-chip AI accelerators, computation placement is concerned with mapping
    workloads to PEs, vector units, and tensor cores. Mapping strategies aim to maximize
    data locality, ensuring that computations access nearby memory to reduce costly
    data movement.
  prefs: []
  type: TYPE_NORMAL
- en: As AI systems scale to multi-chip execution, computation placement must consider
    several critical factors. Workloads need to be partitioned across multiple accelerators,
    which requires explicit coordination of execution order and dependencies. This
    division is essential due to the inherent latency associated with cross-chip communication,
    which contrasts sharply with single-chip systems that benefit from shared on-chip
    memory. Accordingly, computation scheduling must be interconnect-aware to manage
    these delays effectively. Additionally, achieving load balancing across accelerators
    is vital; an uneven distribution of tasks can result in some accelerators remaining
    underutilized while others operate at full capacity, ultimately hindering overall
    system performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in multi-GPU training, computation mapping must ensure that each
    GPU has a balanced portion of the workload while minimizing expensive cross-GPU
    communication. Similarly, in TPU Pods, mapping strategies must align with the
    torus interconnect topology, ensuring that computation is placed to minimize long-distance
    data transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while computation placement in single-chip systems is a local optimization
    problem, in multi-chip architectures, it becomes a global optimization challenge
    where execution efficiency depends on minimizing inter-chip communication and
    balancing workload distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Access Memory Allocation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory allocation strategies in single-chip AI accelerators are designed to
    minimize off-chip memory accesses by using on-chip caches, SRAM, and HBM. Techniques
    such as tiling, data reuse, and kernel fusion ensure that computations make efficient
    use of fast local memory.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-chip AI systems, each accelerator manages its own local memory, which
    necessitates the explicit allocation of model parameters, activations, and intermediate
    data across the devices. Unlike single-chip execution where data is fetched once
    and reused, multi-chip setups require deliberate strategies to minimize redundant
    data transfers, as data must be communicated between accelerators. Additionally,
    when overlapping data is processed by multiple accelerators, the synchronization
    of shared data can introduce significant overhead that must be carefully managed
    to ensure efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in multi-GPU deep learning, gradient synchronization across GPUs
    is a memory-intensive operation that must be optimized to avoid network congestion
    ([Shallue et al. 2019](ch058.xhtml#ref-Shallue2019measuring)). In wafer-scale
    AI, memory allocation must account for fault tolerance and redundancy mechanisms,
    ensuring that defective regions of the wafer do not disrupt execution.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while memory allocation in single-chip accelerators focuses on local cache
    efficiency, in multi-chip architectures, it must be explicitly coordinated across
    accelerators to balance memory bandwidth, minimize redundant transfers, and reduce
    synchronization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Data Movement Constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In single-chip AI accelerators, data movement optimization is largely focused
    on minimizing on-chip memory access latency. Techniques such as weight stationarity,
    input stationarity, and tiling ensure that frequently used data remains close
    to the execution units, reducing off-chip memory traffic.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-chip architectures, data movement transcends being merely an intra-chip
    issue and becomes a significant system-wide bottleneck. Scaling introduces several
    critical challenges, foremost among them being inter-chip bandwidth constraints;
    communication links such as PCIe, NVLink, and TPU interconnects operate at speeds
    that are considerably slower than those of on-chip memory accesses. Additionally,
    when accelerators share model parameters or intermediate computations, the resulting
    data synchronization overhead, which encompass latency and contention, can markedly
    impede execution. Finally, optimizing collective communication is essential for
    workloads that require frequent data exchanges, such as gradient updates in deep
    learning training, where minimizing synchronization penalties is imperative for
    achieving efficient system performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in TPU Pods, systolic execution models ensure that data moves in
    structured patterns, reducing unnecessary off-chip transfers. In multi-GPU inference,
    techniques like asynchronous data fetching and overlapping computation with communication
    help mitigate inter-chip latency.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while data movement optimization in single-chip systems focuses on cache
    locality and tiling, in multi-chip architectures, the primary challenge is reducing
    inter-chip communication overhead to maximize efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Compilers and Runtimes Adaptation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As AI acceleration extends beyond a single chip, compilers and runtimes must
    adapt to manage computation placement, memory organization, and execution scheduling
    across multiple accelerators. The fundamental principles of locality, parallelism,
    and efficient scheduling remain essential, but their implementation requires new
    strategies for distributed execution.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary challenges in scaling AI execution is computation placement.
    In a single-chip accelerator, workloads are mapped to processing elements, vector
    units, and tensor cores with an emphasis on minimizing on-chip data movement and
    maximizing parallel execution. However, in a multi-chip system, computation must
    be partitioned hierarchically, where workloads are distributed not just across
    cores within a chip, but also across multiple accelerators. Compilers handle this
    by implementing interconnect-aware scheduling, optimizing workload placement to
    minimize costly inter-chip communication.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, memory management evolves as scaling extends beyond a single accelerator.
    In a single-chip system, local caching, HBM reuse, and efficient tiling strategies
    ensure that frequently accessed data remains close to computation units. However,
    in a multi-chip system, each accelerator has its own independent memory, requiring
    explicit memory partitioning and coordination. Compilers optimize memory layouts
    for distributed execution, while runtimes introduce data prefetching and caching
    mechanisms to reduce inter-chip memory access overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond computation and memory, data movement becomes a major bottleneck at scale.
    In a single-chip accelerator, efficient on-chip caching and minimized DRAM accesses
    ensure that data is reused efficiently. However, in a multi-chip system, communication-aware
    execution becomes critical, requiring compilers to generate execution plans that
    overlap computation with data transfers. Runtimes handle inter-chip synchronization,
    ensuring that workloads are not stalled by waiting for data to arrive from remote
    accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, execution scheduling must be extended for global coordination. In single-chip
    AI execution, scheduling is primarily concerned with parallelism and maximizing
    compute occupancy within the accelerator. However, in a multi-chip system, scheduling
    must balance workload distribution across accelerators while taking interconnect
    bandwidth and synchronization latency into account. Runtimes manage this complexity
    by implementing adaptive scheduling strategies that dynamically adjust execution
    plans based on system state and network congestion.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.21](ch017.xhtml#tbl-scaling-adaptations) summarizes these key adaptations,
    highlighting how compilers and runtimes extend their capabilities to efficiently
    support multi-chip AI execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while the fundamentals of AI acceleration remain intact, compilers and
    runtimes must extend their functionality to operate efficiently across distributed
    systems. The next section will explore how mapping strategies evolve to further
    optimize multi-chip AI execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.21: **Multi-Chip Adaptations**: Efficient AI execution on multiple
    accelerators requires coordinated adjustments to computation placement, memory
    management, and scheduling to balance workload distribution and minimize communication
    overhead. Compilers and runtimes extend their capabilities to dynamically adapt
    to system state and network congestion, enabling scalable and performant multi-chip
    AI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Single-Chip AI Accelerator** | **Multi-Chip AI System & How
    Compilers/Runtimes Adapt** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Computation Placement** | Local PEs, tensor cores, vector units | Hierarchical
    mapping, interconnect-aware scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| **Memory Management** | Caching, HBM reuse, local tiling | Distributed allocation,
    prefetching, caching |'
  prefs: []
  type: TYPE_TB
- en: '| **Data Movement** | On-chip reuse, minimal DRAM access | Communication-aware
    execution, overlap transfers |'
  prefs: []
  type: TYPE_TB
- en: '| **Execution Scheduling** | Parallelism, compute occupancy | Global scheduling,
    interconnect-aware balancing |'
  prefs: []
  type: TYPE_TB
- en: Execution Models Adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As AI accelerators scale beyond a single chip, execution models must evolve
    to account for the complexities introduced by distributed computation, memory
    partitioning, and inter-chip communication. In single-chip accelerators, execution
    is optimized for local processing elements, with scheduling strategies that balance
    parallelism, locality, and data reuse. However, in multi-chip AI systems, execution
    must now be coordinated across multiple accelerators, introducing new challenges
    in workload scheduling, memory coherence, and interconnect-aware execution.
  prefs: []
  type: TYPE_NORMAL
- en: This section explores how execution models change as AI acceleration scales,
    focusing on scheduling, memory coordination, and runtime management in multi-chip
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Accelerator Scheduling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In single-chip AI accelerators, execution scheduling is primarily aimed at optimizing
    parallelism within the processor. This involves ensuring that workloads are effectively
    mapped to tensor cores, vector units, and special function units by employing
    techniques designed to enhance data locality and resource utilization. For instance,
    static scheduling uses a predetermined execution order that is carefully optimized
    for locality and reuse, while dynamic scheduling adapts in real time to variations
    in workload demands. Additionally, pipeline execution divides computations into
    stages, thereby maximizing hardware utilization by maintaining a continuous flow
    of operations.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, scheduling in multi-chip architectures must address the additional
    challenges posed by inter-chip dependencies. Workload partitioning in such systems
    involves distributing tasks across various accelerators such that each receives
    an optimal share of the workload, all while minimizing the overhead caused by
    excessive communication. Interconnect-aware scheduling is essential to align execution
    timing with the constraints of inter-chip bandwidth, thus preventing performance
    stalls. Latency hiding techniques also play a critical role, as they enable the
    overlapping of computation with communication, effectively reducing waiting times.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in multi-GPU inference scenarios, execution scheduling is implemented
    in a way that allows data to be prefetched concurrently with computation, thereby
    mitigating memory stalls. Similarly, TPU Pods leverage the systolic array model
    to tightly couple execution scheduling with data flow, ensuring that each TPU
    core receives its required data precisely when needed. Therefore, while single-chip
    execution scheduling is focused largely on maximizing internal parallelism, multi-chip
    systems require a more holistic approach that explicitly manages communication
    overhead and synchronizes workload distribution across accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Accelerator Coordination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In single-chip AI accelerators, memory coordination is managed through sophisticated
    local caching strategies that keep frequently used data in close proximity to
    the execution units. Techniques such as tiling, kernel fusion, and data reuse
    are employed to reduce the dependency on slower memory hierarchies, thereby enhancing
    performance and reducing latency.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, multi-chip architectures present a distributed memory coordination
    challenge that necessitates more deliberate management. Each accelerator in such
    a system possesses its own independent memory, which must be organized through
    explicit memory partitioning to minimize cross-chip data accesses. Additionally,
    ensuring consistency and synchronization of shared data across accelerators is
    essential to maintain computational correctness. Efficient communication mechanisms
    must also be implemented to schedule data transfers in a way that limits overhead
    associated with synchronization delays.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in distributed deep learning training, model parameters must be
    synchronized across multiple GPUs using methods such as all-reduce, where gradients
    are aggregated across accelerators while reducing communication latency. In wafer-scale
    AI, memory coordination must further address fault-tolerant execution, ensuring
    that defective areas do not compromise overall system performance. Consequently,
    while memory coordination in single-chip systems is primarily concerned with cache
    optimization, multi-chip architectures require management of distributed memory
    access, synchronization, and communication to achieve efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Accelerator Execution Management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Execution in single-chip AI accelerators is managed by AI runtimes that handle
    workload scheduling, memory allocation, and hardware execution. These runtimes
    optimize execution at the kernel level, ensuring that computations are executed
    efficiently within the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-chip AI systems, runtimes must incorporate a strategy for distributed
    execution orchestration. This approach ensures that both computation and memory
    access are seamlessly coordinated across multiple accelerators, enabling efficient
    utilization of hardware resources and minimizing bottlenecks associated with data
    transfers.
  prefs: []
  type: TYPE_NORMAL
- en: These systems require robust mechanisms for cross-chip workload synchronization.
    Careful management of dependencies and timely coordination between accelerators
    are essential to prevent stalls in execution that may arise from delays in inter-chip
    communication. Such synchronization is critical for maintaining the flow of computation,
    particularly in environments where latency can significantly impact overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, adaptive execution models play a pivotal role in contemporary multi-chip
    architectures. These models dynamically adjust execution plans based on current
    hardware availability and communication constraints, ensuring that the system
    can respond to changing conditions and optimize performance in real time. Together,
    these strategies provide a resilient framework for managing the complexities of
    distributed AI execution.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in Google’s TPU Pods, the TPU runtime is responsible for scheduling
    computations across multiple TPU cores, ensuring that workloads are executed in
    a way that minimizes communication bottlenecks. In multi-GPU frameworks like PyTorch
    and TensorFlow, runtime execution must synchronize operations across GPUs, ensuring
    that data is transferred efficiently while maintaining execution order.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, while single-chip runtimes focus on optimizing execution within a single
    processor, multi-chip runtimes must handle system-wide execution, balancing computation,
    memory, and interconnect performance.
  prefs: []
  type: TYPE_NORMAL
- en: Computation Placement Adaptation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As AI systems expand beyond single-chip execution, computation placement must
    adapt to account for inter-chip workload distribution and interconnect efficiency.
    In single-chip accelerators, compilers optimize placement by mapping workloads
    to tensor cores, vector units, and PEs, ensuring maximum parallelism while minimizing
    on-chip data movement. However, in multi-chip systems, placement strategies must
    address interconnect bandwidth constraints, synchronization latency, and hierarchical
    workload partitioning across multiple accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.22](ch017.xhtml#tbl-computation-placement) highlights these adaptations.
    To reduce expensive cross-chip communication, compilers now implement interconnect-aware
    workload partitioning, strategically assigning computations to accelerators based
    on communication cost. For instance, in multi-GPU training, compilers optimize
    placement to minimize NVLink or PCIe traffic, whereas TPU Pods leverage the torus
    interconnect topology to enhance data exchanges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11.22: **Computation Placement Strategies**: Multi-chip AI systems necessitate
    hierarchical workload mapping to minimize communication overhead; compilers adapt
    single-chip optimization techniques by considering interconnect bandwidth and
    latency when assigning computations to accelerators. This table contrasts computation
    placement in single-chip systems—local to processing elements—with multi-chip
    systems, where placement strategies prioritize efficient data exchange across
    accelerators.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Single-Chip AI Accelerator** | **Multi-Chip AI System & How
    Compilers/Runtimes Adapt** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Computation Placement** | Local PEs, tensor cores, vector units | Hierarchical
    mapping, interconnect-aware scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| **Workload Distribution** | Optimized within a single chip | Partitioning
    across accelerators, minimizing inter-chip communication |'
  prefs: []
  type: TYPE_TB
- en: '| **Synchronization** | Managed within local execution units | Runtimes dynamically
    balance workloads, adjust execution plans |'
  prefs: []
  type: TYPE_TB
- en: Runtimes complement this by dynamically managing execution workloads, adjusting
    placement in real-time to balance loads across accelerators. Unlike static compilation,
    which assumes a fixed hardware topology, AI runtimes continuously monitor system
    conditions and migrate tasks as needed to prevent bottlenecks. This ensures efficient
    execution even in environments with fluctuating workload demands or varying hardware
    availability.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, computation placement at scale builds upon local execution optimizations
    while introducing new challenges in inter-chip coordination, communication-aware
    execution, and dynamic load balancing—challenges that extend to how memory hierarchies
    must adapt to support efficient execution across multi-chip architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating Multi-Chip AI Complexities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evolution of AI hardware, from single-chip accelerators to multi-chip systems
    and wafer-scale integration, highlights the increasing complexity of efficiently
    executing large-scale machine learning workloads. Scaling AI systems introduces
    new challenges in computation placement, memory management, and data movement.
    While the fundamental principles of AI acceleration remain consistent, their implementation
    must adapt to the constraints of distributed execution, interconnect bandwidth
    limitations, and synchronization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-chip AI architectures represent a significant step forward in addressing
    the computational demands of modern machine learning models. By distributing workloads
    across multiple accelerators, these systems offer increased performance, memory
    capacity, and scalability. However, realizing these benefits requires careful
    consideration of how computations are mapped to hardware, how memory is partitioned
    and accessed, and how execution is scheduled across a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: While we an overview of the key concepts and challenges in multi-chip AI acceleration
    as they extend beyond a single system, there is still much more to explore. As
    AI models continue to grow in size and complexity, new architectural innovations,
    mapping strategies, and runtime optimizations will be needed to sustain efficient
    execution. These emerging trends and future directions continue to evolve rapidly
    in the field. The ongoing development of AI hardware and software reflects a broader
    trend in computing, where specialization and domain-specific architectures are
    becoming increasingly important for addressing the unique demands of emerging
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the principles and trade-offs involved in multi-chip AI acceleration
    enables machine learning engineers and system designers to make informed decisions
    about how to best deploy and optimize their models. Whether training large language
    models on TPU pods or deploying computer vision applications on multi-GPU systems,
    the ability to efficiently map computations to hardware will continue to be a
    critical factor in realizing the full potential of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneous SoC AI Acceleration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The multi-chip architectures examined in previous sections focused primarily
    on maximizing computational throughput for data center workloads, where power
    budgets extend to kilowatts and cooling infrastructure supports rack-scale deployments.
    However, the hardware acceleration principles established—specialized compute
    units, memory hierarchy optimization, and workload mapping strategies—must adapt
    dramatically when deploying AI systems in mobile and edge environments. A smartphone
    operates within a 2 to 5 watt power budget, autonomous vehicles require deterministic
    real-time guarantees, and IoT sensors must function for years on battery power.
    These constraints necessitate heterogeneous System-on-Chip (SoC) architectures
    that coordinate multiple specialized processors within a single chip while meeting
    stringent power, thermal, and latency requirements fundamentally different from
    data center deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The mobile AI revolution has fundamentally transformed how we think about AI
    acceleration, moving beyond homogeneous data center architectures to heterogeneous
    System-on-Chip (SoC) designs that coordinate multiple specialized processors.
    Modern smartphones, automotive systems, and IoT devices integrate CPU cores, GPU
    shaders, digital signal processors (DSPs), and dedicated neural processing units
    (NPUs) within a single chip, requiring sophisticated orchestration to achieve
    optimal performance under strict power and thermal constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile SoC Architecture Evolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Qualcomm’s Snapdragon AI Engine exemplifies heterogeneous computing for mobile
    AI, coordinating Kryo CPU cores, Adreno GPU, Hexagon DSP, and dedicated NPU[34](#fn34)
    across a shared memory hierarchy. The Snapdragon 8 Gen 3 achieves 73 TOPS through
    intelligent workload distribution—computer vision kernels execute on the GPU’s
    parallel shaders, audio processing leverages the DSP’s specialized arithmetic
    units, while transformer attention mechanisms utilize the NPU’s optimized matrix
    engines. This coordination requires millisecond-precision scheduling to meet real-time
    constraints while managing thermal throttling and battery life optimization.
  prefs: []
  type: TYPE_NORMAL
- en: While Qualcomm’s approach emphasizes diverse processor specialization, Apple’s
    vertically integrated strategy demonstrates how tight hardware-software co-design
    enables even more sophisticated heterogeneous execution. The M2 chip’s 16-core
    Neural Engine (15.8 TOPS) coordinates with the 10-core GPU and 8-core CPU through
    a unified memory architecture that eliminates data copying overhead. The Neural
    Engine’s specialized matrix multiplication units handle transformer layers, while
    the GPU’s Metal Performance Shaders accelerate convolutional operations, and the
    CPU manages control flow and dynamic layer selection. This fine-grained coordination
    enables real-time language translation and on-device image generation while maintaining
    millisecond response times.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these vertically integrated solutions from Qualcomm and Apple, ARM’s
    IP licensing model offers a fundamentally different approach that enables SoC
    designers to customize processor combinations based on target applications. The
    Mali-G78 GPU’s 24 cores can be paired with Ethos-N78 NPU for balanced general-purpose
    and AI acceleration, while the Cortex-M55 microcontroller integrates Ethos-U55
    microNPU for ultra-low-power edge applications. This modular flexibility allows
    automotive SoCs to emphasize deterministic real-time processing while smartphone
    SoCs optimize for interactive performance and battery efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for Dynamic Workload Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With multiple specialized processors available on heterogeneous SoCs, the critical
    challenge becomes intelligently distributing neural network operations across
    these resources to maximize performance while respecting power and latency constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Modern neural networks require intelligent partitioning across heterogeneous
    processors based on operation characteristics and current system state. Convolutional
    layers with regular data access patterns typically execute efficiently on GPU
    shader cores, while fully connected layers with irregular sparsity patterns may
    perform better on general-purpose CPU cores with large caches. Attention mechanisms
    in transformers benefit from NPU matrix engines when sequences are long, but may
    execute more efficiently on CPU when sequence lengths are small due to the NPU
    setup overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond static operation-to-processor mapping, heterogeneous SoCs implement
    dynamic processor selection based on multiple constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Power Budget**: During battery operation, the system may route computations
    to lower-power DSP cores rather than high-performance GPU cores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thermal State**: When approaching thermal limits, workloads shift from power-hungry
    NPU to more efficient CPU execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency Requirements**: Safety-critical automotive applications prioritize
    deterministic CPU execution over potentially faster but variable NPU processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent Workload Interference**: Multiple AI applications may require
    load balancing across available processors to maintain Quality of Service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compounding the processor selection challenge, shared memory architectures require
    sophisticated arbitration when multiple processors access LPDDR simultaneously.
    The Snapdragon 8 Gen 3’s memory controller implements priority-based scheduling
    where camera processing receives higher priority than background AI tasks, ensuring
    real-time video processing while background neural networks adapt their execution
    patterns to available memory bandwidth. This arbitration becomes critical during
    memory-intensive operations like large language model inference, where parameter
    streaming from DRAM must be carefully coordinated across processors.
  prefs: []
  type: TYPE_NORMAL
- en: Power and Thermal Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mobile AI workloads must maintain high performance while operating within strict
    power budgets and thermal envelopes—constraints that require sophisticated coordination
    across heterogeneous processors.
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneous SoCs implement coordinated DVFS across multiple processors to
    optimize the power-performance envelope. When one processor increases frequency
    to meet latency demands, the system may reduce voltage on other processors to
    maintain total power budget. This coordination becomes complex in AI workloads
    where computational phases may shift rapidly between processors—the system must
    predict upcoming workload transitions to preemptively adjust operating points
    while avoiding voltage/frequency oscillations that degrade efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: When DVFS alone cannot maintain the power envelope, mobile SoCs implement thermal
    throttling through intelligent task migration rather than simple frequency reduction.
    When the NPU approaches thermal limits during intensive neural network processing,
    the runtime system can migrate layers to the GPU or CPU while maintaining computational
    throughput. This approach preserves performance during thermal events, though
    it requires sophisticated workload characterization to predict execution time
    and power consumption across different processors.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond real-time power and thermal management, mobile AI systems must also adapt
    their computational strategies based on battery state and charging status. During
    low battery conditions, the system may switch from high-accuracy models to efficient
    approximations, migrate workloads from power-hungry NPU to energy-efficient DSP,
    or reduce inference frequency while maintaining application responsiveness. Conversely,
    during charging, the system can enable higher-performance models and increase
    processing frequency to deliver enhanced user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Automotive Heterogeneous AI Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automotive applications introduce unique heterogeneous computing challenges
    that combine mobile-style power efficiency with hard real-time guarantees and
    functional safety requirements—a combination that demands fundamentally different
    architectural approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Automotive SoCs must guarantee deterministic inference latency for safety-critical
    functions while supporting advanced driver assistance systems (ADAS). The Snapdragon
    Ride platform coordinates multiple AI accelerators across safety domains—redundant
    processing elements ensure functional safety compliance while high-performance
    accelerators handle perception, planning, and control algorithms. This architecture
    requires temporal isolation between safety-critical and convenience functions,
    implemented through hardware partitioning and time-triggered scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: These safety requirements become even more complex when considering that modern
    vehicles integrate multiple AI-enabled SoCs for different domains—vision processing
    SoCs handle camera-based perception, radar processing SoCs manage RF sensor data,
    while central compute platforms coordinate high-level decision making. These distributed
    systems must maintain temporal coherence across sensor modalities with microsecond-precision
    timing, requiring specialized inter-SoC communication protocols and distributed
    synchronization mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Extending beyond the vehicle’s internal sensors, vehicle-to-everything (V2X)
    communication adds another layer of heterogeneous processing where AI algorithms
    must coordinate local sensor processing with information received from other vehicles
    and infrastructure. This requires ultra-low latency processing chains where 5G
    modems, AI accelerators, and control systems operate within millisecond deadlines
    while maintaining functional safety requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Software Stack Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The architectural sophistication of heterogeneous SoCs creates substantial software
    development challenges that span programming models, memory management, and runtime
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Programming heterogeneous SoCs requires frameworks that abstract processor differences
    while exposing performance-critical optimization opportunities. OpenCL and Vulkan
    provide cross-processor execution, but achieving optimal performance requires
    processor-specific optimizations that complicate portable development. Modern
    ML frameworks like TensorFlow Lite and PyTorch Mobile implement automatic processor
    selection, but developers still need to understand heterogeneous execution patterns
    to achieve optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Complicating the programming challenge further, heterogeneous SoCs with shared
    memory architectures require sophisticated memory management that considers processor-specific
    caching behaviors, memory access patterns, and coherency requirements. CPU caches
    may interfere with GPU memory access patterns, while NPU direct memory access
    (DMA) operations must be synchronized with CPU cache operations to maintain data
    consistency.
  prefs: []
  type: TYPE_NORMAL
- en: To address the complexity of manual optimization across these dimensions, advanced
    heterogeneous SoCs implement machine learning-based runtime optimization that
    learns from execution patterns to improve processor selection, thermal management,
    and power optimization. These systems collect telemetry on workload characteristics,
    processor utilization, and power consumption to build models that predict optimal
    execution strategies for new workloads.
  prefs: []
  type: TYPE_NORMAL
- en: This heterogeneous approach to AI acceleration represents the future of computing,
    where no single processor architecture can optimally handle the diverse computational
    patterns in modern AI applications. Understanding these coordination challenges
    is essential for developing efficient mobile AI systems that deliver high performance
    while meeting the strict power, thermal, and real-time constraints of edge deployment
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: However, the complexity of these heterogeneous systems creates numerous opportunities
    for misconception and suboptimal design decisions. The following fallacies and
    pitfalls highlight common misunderstandings that can undermine acceleration strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Fallacies and Pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hardware acceleration involves complex interactions between specialized architectures,
    software stacks, and workload characteristics that create significant opportunities
    for misunderstanding optimal deployment strategies. The impressive performance
    numbers often associated with AI accelerators can mask important constraints and
    trade-offs that determine real-world effectiveness across different deployment
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fallacy:** *More specialized hardware always provides better performance
    than general-purpose alternatives.*'
  prefs: []
  type: TYPE_NORMAL
- en: This belief assumes that specialized accelerators automatically outperform general-purpose
    processors for all AI workloads. Specialized hardware achieves peak performance
    only when workloads match the architectural assumptions and optimization targets.
    Models with irregular memory access patterns, small batch sizes, or dynamic computation
    graphs may perform better on flexible general-purpose processors than on specialized
    accelerators designed for dense, regular computations. The overhead of data movement,
    format conversion, and synchronization can eliminate the benefits of specialized
    computation. Effective hardware selection requires matching workload characteristics
    to architectural strengths rather than assuming specialization always wins.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall:** *Ignoring memory bandwidth limitations when selecting acceleration
    strategies.*'
  prefs: []
  type: TYPE_NORMAL
- en: Many practitioners focus on computational throughput metrics without considering
    memory bandwidth constraints that often limit real-world performance. AI accelerators
    with impressive computational capabilities can be severely bottlenecked by insufficient
    memory bandwidth, leading to poor hardware utilization. The ratio between computation
    intensity and memory access requirements determines whether an accelerator can
    achieve its theoretical performance. This oversight leads to expensive hardware
    deployments that fail to deliver expected performance improvements because the
    workload is memory-bound rather than compute-bound.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fallacy:** *Hardware acceleration benefits scale linearly with additional
    accelerators.*'
  prefs: []
  type: TYPE_NORMAL
- en: This misconception drives teams to expect proportional performance gains when
    adding more accelerators to their systems. Multi-accelerator setups introduce
    communication overhead, synchronization costs, and load balancing challenges that
    can severely limit scaling efficiency. Small models may not provide enough parallel
    work to utilize multiple accelerators effectively, while large models may be limited
    by communication bandwidth between devices. Distributed training and inference
    face additional challenges from gradient aggregation, model partitioning, and
    coordination overhead that create non-linear scaling relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitfall:** *Vendor-specific optimizations without considering long-term portability
    and flexibility.*'
  prefs: []
  type: TYPE_NORMAL
- en: Organizations often optimize exclusively for specific hardware vendors to achieve
    maximum performance without considering the implications for system flexibility
    and future migration. Deep integration with vendor-specific libraries, custom
    kernels, and proprietary optimization tools creates lock-in that complicates hardware
    upgrades, vendor changes, or multi-vendor deployments. While vendor-specific optimizations
    can provide significant performance benefits, they should be balanced against
    the need for system portability and the ability to adapt to evolving hardware
    landscapes. Maintaining some level of hardware abstraction preserves strategic
    flexibility while still capturing most performance benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hardware acceleration has emerged as the critical enabler that transforms machine
    learning from academic curiosity to practical reality, fundamentally reshaping
    how we design both computational systems and the algorithms that run on them.
    The evolution from general-purpose processors to specialized AI accelerators represents
    more than just incremental improvement—it reflects a paradigm shift toward domain-specific
    computing where hardware and software are co-designed to optimize specific computational
    patterns. The journey from CPUs through GPUs to specialized TPUs, NPUs, and wafer-scale
    systems demonstrates how understanding workload characteristics drives architectural
    innovation, creating opportunities for orders-of-magnitude performance improvements
    through targeted specialization.
  prefs: []
  type: TYPE_NORMAL
- en: The technical challenges of AI acceleration span multiple layers of the computing
    stack, from low-level memory hierarchy optimization to high-level compiler transformations
    and runtime orchestration. Memory bandwidth limitations create fundamental bottlenecks
    that require sophisticated techniques like data tiling, kernel fusion, and hierarchy-aware
    scheduling to overcome. Mapping neural network computations to hardware involves
    complex trade-offs between different dataflow patterns, memory allocation strategies,
    and execution scheduling approaches that must balance computational efficiency
    with resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Building on these foundational concepts, the emergence of multi-chip and distributed
    acceleration systems introduces additional complexities around communication overhead,
    memory coherence, and workload partitioning that require careful system-level
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Takeaways**'
  prefs: []
  type: TYPE_NORMAL
- en: Specialized AI accelerators achieve performance gains through domain-specific
    architectures optimized for tensor operations and dataflow patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory hierarchy management is often the primary bottleneck in AI acceleration,
    requiring sophisticated data movement optimization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware-software co-design enables order-of-magnitude improvements by aligning
    algorithm characteristics with architectural capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-chip scaling introduces distributed computing challenges that require
    new approaches to communication, synchronization, and resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The principles of hardware acceleration established here provide the foundation
    for understanding how benchmarking methodologies evaluate accelerator performance
    and how deployment strategies must account for hardware constraints and capabilities.
    As AI models continue growing in complexity and computational requirements, the
    ability to effectively leverage specialized hardware becomes increasingly critical
    for practical system deployment, influencing everything from energy efficiency
    and cost optimization to the feasibility of real-time inference and large-scale
    training across diverse application domains.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
