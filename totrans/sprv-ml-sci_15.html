<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>9  Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>9  Interpretability</h1>
<blockquote>原文：<a href="https://ml-science-book.com/interpretability.html">https://ml-science-book.com/interpretability.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-two.html">Integrating Machine Learning Into Science</a></li><li class="breadcrumb-item"><a href="./interpretability.html"><span class="chapter-number">9</span>  <span class="chapter-title">Interpretability</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>What’s your biggest concern about using machine learning for science? A survey <span class="citation" data-cites="vannoordenAIScienceWhat2023"><a href="references.html#ref-vannoordenAIScienceWhat2023" role="doc-biblioref">[1]</a></span> asked 1600 scientists this question. “Leads to more reliance on pattern recognition without understanding” was the top concern, which is well-founded: Supervised machine learning is foremost about prediction, not understanding (see <a href="supervised-ml.html" class="quarto-xref"><span>Chapter 2</span></a>).</p>
<p>To solve that problem, interpretable machine learning was invented. Interpretable machine learning (or explainable AI<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) offers a wide range of solutions to tackle the lack of understanding. Interpretability, in the widest sense, is about making the model understandable to humans <span class="citation" data-cites="miller2019explanation"><a href="references.html#ref-miller2019explanation" role="doc-biblioref">[4]</a></span>. There are plenty of tools for this task, ranging from using decision rules to applying game theory (Shapley values) and analyzing neurons in a neural network. But before we talk solutions let’s talk about the problem first. Interpretability isn’t a goal in itself. Interpretability is a tool that helps you achieve your actual goals. Goals that a prediction-focused approach alone can’t satisfy.</p>
<div class="raven-box">
<p>The integration of domain knowledge eliminated many childhood diseases of machine learning. The Ravens began building larger and larger models to study increasingly complex phenomena. Krarah approached Rattle with a question: “The models must have learned so many interesting relationships, is there any way to extract this knowledge?” Rattle nodded and winked at Krarah. Recently, Raven Krähstof wrote a book about interpretable machine learning.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/048d7003a2c6fc2ab939dda01b89b6d8.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%" data-original-src="https://ml-science-book.com/images/raven-interpretability.jpg"/></p>
</figure>
</div>
</div>
<section id="goals-of-interpretation" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="goals-of-interpretation"><span class="header-section-number">9.1</span> Goals of interpretation</h2>
<p>Imagine you model the yield of almond orchards. The prediction model predicts almond yield in a given year based on precipitation, fertilizer use, prior yield, and so on. You are satisfied with the predictive performance, but you have this nagging feeling of incompleteness that no benchmark can fill. And when you find a task can’t be solved by performance alone, you might need interpretability <span class="citation" data-cites="doshi-velezRigorousScienceInterpretable2017"><a href="references.html#ref-doshi-velezRigorousScienceInterpretable2017" role="doc-biblioref">[5]</a></span>. For example, you might be interested in the effect of fertilizer on almond yield.</p>
<p>We roughly distinguish three interpretability goals (inspired by <span class="citation" data-cites="adadiPeekingBlackBoxSurvey2018"><a href="references.html#ref-adadiPeekingBlackBoxSurvey2018" role="doc-biblioref">[6]</a></span>):</p>
<ul>
<li><strong>Discover</strong>: The model may have learned something interesting about the studied phenomenon. In science, we can further distinguish between confirmatory (aka inference) and exploratory types of discovery. In the case of almond yield, the goal might be to study the effect of fertilizer.</li>
<li><strong>Improve</strong>: Interpretation of a model can help debug and improve the model. This can lead to better performance and higher robustness (see also <a href="robustness.html" class="quarto-xref"><span>Chapter 11</span></a>). Studying feature importance you might find out that the prior yield variable is suspiciously important and detect that a coding error introduced a data leakage.</li>
<li><strong>Justify</strong>: Interpretation helps to justify a prediction, or also the model itself. Justification ranges from justifying a prediction towards an end-user touching upon ethics and fairness, to more formal model audits, but also building trust through, e.g. showing coherence with physics. For example, you are trying to convince an agency to adapt your model, but first, you have to convince them that your model aligns with common agricultural knowledge.</li>
</ul>
<p>The goals interact with each other in the life cycle of a machine learning project. For example, to discover knowledge, your model should show good performance. However, building a performative model is an iterative process in which interpretability can immensely help by monitoring the importance of features.</p>
<p>But how do you achieve interpretability for complex machine learning models? To oversimplify, the field of interpretable machine learning knows two paths: interpretability-by-design and post-hoc interpretability. Interpretability-by-design is about only using interpretable, aka simple models. Post-hoc interpretation is the attempt to interpret potentially complex models after they were trained.</p>
</section>
<section id="interpretability-by-design" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="interpretability-by-design"><span class="header-section-number">9.2</span> Interpretability-by-design</h2>
<p>Interpretability by design is the status quo in many research fields. For example, quantitative medical research often relies on classical statistical models, with logistic regression being a commonly used model. These statistical models are considered inherently interpretable because they often relate the features to the outcome through a linearly weighted sum in one way or another. Statistical models are also dominant in the social sciences and many other fields. The use of a linear regression model to predict almond yield is not unheard of <span class="citation" data-cites="bassoSeasonalCropYield2019"><a href="references.html#ref-bassoSeasonalCropYield2019" role="doc-biblioref">[7]</a></span>. However, models with very different motivations can also be interpretable by design, such as differential equations and physics-based simulations in fields such as physics, meteorology, and ecology.</p>
<p>If you value interpretability, you might decide that you only use machine learning algorithms that produce interpretable models. You would still approach the modeling task in a typical performance-first manner, but restrict our solutions to be only models that you deem interpretable. It would still be about optimization, but you strongly limit the hypothesis space, which is the pool of models that are deemed okay to be learned.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/50cc5471aa90e1f79c678f1094e11e40.png" class="img-fluid figure-img" data-original-src="https://ml-science-book.com/images/white-box.jpg"/></p>
<figcaption>Some models are (supposedly) interpretable by design.</figcaption>
</figure>
</div>
<p>Interpretable machine learning models may include:</p>
<ul>
<li>Anything with linearity. Essentially the entire pantheon of frequentist models. Linear regression, logistic regression, frameworks such as generalized linear models (GLMs) and generalized additive models (GAMs), …</li>
<li>Decision trees. Structures that present the prediction in the form of typical but not necessarily binary trees that are split by features.</li>
<li>Decision rule lists</li>
<li>Combinations of linear models and decision rules such as RuleFit <span class="citation" data-cites="friedman2008predictive"><a href="references.html#ref-friedman2008predictive" role="doc-biblioref">[8]</a></span> and model-based trees <span class="citation" data-cites="zeileis2008modelbased"><a href="references.html#ref-zeileis2008modelbased" role="doc-biblioref">[9]</a></span></li>
<li>Case-based reasoning</li>
</ul>
<p>Researchers continue to invent new machine-learning approaches that produce models that are interpretable by design <span class="citation" data-cites="rudin2022interpretable"><a href="references.html#ref-rudin2022interpretable" role="doc-biblioref">[10]</a></span>, but there’s also controversy as to what even constitutes an interpretable model <span class="citation" data-cites="liptonMythosModelInterpretability2017"><a href="references.html#ref-liptonMythosModelInterpretability2017" role="doc-biblioref">[11]</a></span>. Arguably, not even a linear regression model is interpretable if you have many features. Others argue that whether a particular model can be seen as interpretable depends on the audience and the context. But whatever your definition of interpretation: By restricting the functional form of the models, it becomes functionally better controllable. Even if you can argue that linear regression isn’t as easy to interpret, then it is still true that the model contains no interactions (at least on the input space of the model). And models that are interpretable by design, or at least structurally simpler, can help with goals such as discovery, improvement, and justification.</p>
<ul>
<li><strong>Discover:</strong> A logistic regression model allows insights about the features that increase the probability of, e.g., diabetes.</li>
<li><strong>Improve:</strong> Regression coefficients with a wrong direction may signal that a feature was wrongly coded.</li>
<li><strong>Justify:</strong> A domain expert can manually check decision rules.</li>
</ul>
<p>Interpretability by design has a cost: Restricting the hypothesis may exclude many good models and you might end up with a model that is worse in performance. The winners in machine learning competitions are usually complex gradient-boosted trees (e.g. LightGBM, catboost, xgboost) for tabular data and neural networks (CNN, transformer) for image and text data. It is not the models that are interpretable by design that are cashing in the price money.</p>
<p>Besides lower performance, interpretability-by-design has a conceptual problem when it comes to discovery (inference). To extend the interpretation of the model to the real world, you have to establish a connection between your model and the world. In classical statistical modeling, for example, you make lots of assumptions about what the data-generating process might be. What’s the distribution of the target given the features? What’s the process of missing values? What correlation structures do you have to account for? Other approaches such as physics-based simulations also have a link between the model and the world: it is assumed that certain parts of the simulation represent parts of the world. For interpretability by design, there is no such link. For example, there is no justification for saying that almond yield is produced by decision tree-like processes.</p>
<p>Another problem is performance: What do you do when you find models that have better predictive performance than your interpretable one? A model that represents the world well should also be able to predict it well. You would have to argue why your interpretable model better represents the world, even though it predicts it less well. You might have just been oversimplifying the world for your convenience. This weakens any claims to link the model to reality. An even worse conceptual problem is the Rashomon effect <span class="citation" data-cites="breiman2001random"><a href="references.html#ref-breiman2001random" role="doc-biblioref">[12]</a></span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Origin of the name Rashomon
</div>
</div>
<div class="callout-body-container callout-body">
<p>Rashomon is a 1950 Japanese film. It tells the story of a murdered samurai from four different perspectives: a bandit, the samurai’s wife, the samurai’s spirit communicating through a psychic, and a commoner observing the event. While each perspective is coherent in itself, it is incompatible with the other three stories. This so-called Rashomon effect has become an established concept in law, philosophy, and, as you can see, statistics and machine learning.</p>
</div>
</div>
<p>The Rashomon effect describes the phenomenon that multiple models may have roughly the same performance, but are structurally different. A head-scratcher for interpretation: If optimization leads to a set of equally performant models with different internal structures, then you can’t pick one over the other based purely based on an argument of performance. An example: Decision trees are inherently interpretable (at least if short) but also inherently unstable – small changes in the data may lead to very different trees even if the performance might not suffer too much. If you use a decision tree for discovery, then the Rashomon effect makes it difficult to argue that this is the exact tree that you should even be interpreting.</p>
<p>The verdict: Inherently interpretable models are much easier to justify than their more complex counterparts. However, using them for insights (inference) has conceptual problems. Fortunately, there’s also the class of post-hoc interpretation methods, for which we have a theory of how their interpretation may be extended to the modeled phenomenon.</p>
</section>
<section id="model-specific-post-hoc-interpretability" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="model-specific-post-hoc-interpretability"><span class="header-section-number">9.3</span> Model-specific post-hoc interpretability</h2>
<p>Instead of opposing model complexity, you can also embrace it and try to extract insights from the complex models. These interpretability methods are called post-hoc, which translates to “after the event”, the event being model training. Post-hoc methods can be either model-specific or model-agnostic: Model-specific interpretation methods work with the structure of the model, whereas model-agnostic methods treat the model as a black box and only work with the input-output data pairs. Model-specific methods are tied to a specific model type and require that you inspect the model, for example:</p>
<ul>
<li>Gini importance leverages the splitting criterion in decision trees to assign importance values to features</li>
<li>Transformers, a popular neural network architecture, has an attention layer that decides which part of the (processed) input to attend to for the prediction. Attention visualization is a model-specific post-hoc approach to interpret transformers.</li>
<li>Activation maximization approaches assign semantic meaning to individual neurons and layers: What concept maximally activates each neuron?</li>
<li>Gradient-based explanation methods like Grad-CAM or layer-wise relevance propagation (LRP) make use of neural network gradients to highlight the inputs that affected the predictions strongly</li>
</ul>
<p>Model-specific methods occupy an odd spot in the interpretability space. The underlying models are usually more complex than the interpretability-by-design ones and the interpretations can only address subsets of the model. For most interpretation goals, model-specific post-hoc interpretation is worse than interpretability-by-design.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/028921c2556c1f668687c1b8e95046a4.png" class="img-fluid figure-img" data-original-src="https://ml-science-book.com/images/specific-black-box.jpg"/></p>
<figcaption>Model-specific post-hoc interpretation analyze parts of the model.</figcaption>
</figure>
</div>
<p>When it comes to inference, model-specific methods share the same conceptual problem with interpretability by design: If you want to interpret the model in place of the real world, you need a theory that connects your model with the world and allows a transfer of interpretation. Model-specific approaches, as the name says, rely on the specific model they are building, and such a link between model and reality would require making assumptions about why this model’s structure is representative of the world, which are probably invalid <span class="citation" data-cites="freiesleben2023artificial"><a href="references.html#ref-freiesleben2023artificial" role="doc-biblioref">[13]</a></span>. However, they still may be useful in pointing out associations for forming causal hypotheses (see <a href="causality.html" class="quarto-xref"><span>Chapter 10</span></a>).</p>
<p>The line is blurred between interpretability by design and model-specific post-hoc interpretation. You might see a linear regression model as inherently interpretable because the coefficients are directly interpretable as feature effects. But what if you log-transform the target? Then you also have to transform the coefficients for interpretation. You can argue that this is still interpretable by design, but you can add many modifications that make the model stray further from interpretability heaven. And for interpretation, you rely more and more on post-hoc computations, like transforming the coefficients, visualizing splines, etc.</p>
<p>Now that we are done talking down on model-specific interpretation, let’s talk about our favorite methods: model-agnostic interpretation techniques. And yes, we make no secret out of this, but we are quite the fans of model-agnostic interpretation.</p>
</section>
<section id="model-agnostic-post-hoc-interpretation-methods" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="model-agnostic-post-hoc-interpretation-methods"><span class="header-section-number">9.4</span> Model-agnostic post-hoc interpretation methods</h2>
<p>You are tasked to write a manual for the world’s weirdest vending machine. Nobody knows how it works. You tried to open it but decided against inspection for fear of breaking the machine. But you have an idea of how you could write that manual. You start pressing some of the buttons and levers until, finally, the machine drops an item: a pack of chips with the flavor “Anchovies”. Fortunately, they are past the due date. You feel no remorse throwing them away. The other good news is that you made the first step toward writing that manual. The secret: Just try things out systematically and find out what happens.</p>
<p>The fictive almond yield model is just like the vending machine. By intervening in the input features, the output (aka the prediction) changes and you can gather information about the model behavior. The recipe is to sample data, intervene in the data, predict, and aggregate the results <span class="citation" data-cites="scholbeck2020sampling"><a href="references.html#ref-scholbeck2020sampling" role="doc-biblioref">[14]</a></span>. And that’s why model-agnostic interpretation methods are post-hoc – they don’t require you to access the model internals or change the model training process. Model-agnostic interpretation methods have many advantages:</p>
<ul>
<li>You can use the same model-agnostic interpretation method for different models and compare the results.</li>
<li>You are free to include pre- and post-processing steps in the interpretation. For example, when the model uses principal components as inputs (dimensionality reduction), you can still produce the interpretations based on the original features.</li>
<li>You can apply model-agnostic methods also to inherently interpretable models like using feature importance on decision trees.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/e7ce6a0164b54623c5bd54d861783007.png" class="img-fluid figure-img" data-original-src="https://ml-science-book.com/images/agnostic-black-box.jpg"/></p>
<figcaption>Model-agnostic interpretation ignores the inner workings of the model and studies input-output pairs.</figcaption>
</figure>
</div>
<p>One of the simplest model-agnostic to explain is permutation feature importance: Imagine the almond yield researcher wants to know which features were most important for a prediction. First, they measure the performance of the model. Then they take one of the features, say the amount of fertilizer used, and shuffle it, which destroys the relationship between the fertilizer feature and the actual yield outcome. If the model relies on the fertilizer feature, the predictions will change for this manipulated dataset. For these predictions, the researchers again measure the performance. Usually, shuffling makes the performance worse. The larger the drop in performance, the more important the feature. <a href="#fig-importance" class="quarto-xref">Figure <span>9.1</span></a> shows an example of permutation feature importance from the almond yield paper <span class="citation" data-cites="zhang2019california"><a href="references.html#ref-zhang2019california" role="doc-biblioref">[15]</a></span>.</p>
<div id="fig-importance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/1eefdfa0a0157e653f494476b366218a.png" class="img-fluid figure-img" data-original-src="https://ml-science-book.com/images/almond-yield-pfis.jpg"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.1: Feature importances for almond yield. Figure by Zhang, Jin, Chen, and Brown (2019) <span class="citation" data-cites="zhang2019california"><a href="references.html#ref-zhang2019california" role="doc-biblioref">[15]</a></span>, CC-BY (https://creativecommons.org/licenses/by/4.0).
</figcaption>
</figure>
</div>
<p>PFI is one of many model-agnostic explanation methods. This chapter won’t introduce all of them, because that’s already its own book called <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> <span class="citation" data-cites="molnar2022"><a href="references.html#ref-molnar2022" role="doc-biblioref">[16]</a></span> and you can read it for free! But still, we’ll provide a brief overview of the interpretability landscape of model-agnostic methods. The biggest differentiator is local versus global methods:</p>
<ul>
<li>Local methods explain individual predictions</li>
<li>Global methods interpret the average model behavior.</li>
</ul>
<section id="local-explaining-individual-predictions" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="local-explaining-individual-predictions"><span class="header-section-number">9.4.1</span> Local: Explaining individual predictions</h3>
<p>The almond researchers might want to explain the yield prediction for a particular field and year. Why did the model make this particular prediction? Explaining individual predictions is one of the holy grails of interpretability. Tons of methods are available. Explanations of predictions attribute, in one way or another, the prediction to the individual features.</p>
<p>Here are some examples of local model-agnostic interpretation methods:</p>
<ul>
<li>Local surrogate models (LIME) <span class="citation" data-cites="ribeiroWhyShouldTrust2016"><a href="references.html#ref-ribeiroWhyShouldTrust2016" role="doc-biblioref">[17]</a></span> explain predictions by approximating the complex model locally with a neighborhood-based interpretable model.</li>
<li>Scoped rules (anchors) <span class="citation" data-cites="ribeiroAnchorsHighPrecisionModelAgnostic2018"><a href="references.html#ref-ribeiroAnchorsHighPrecisionModelAgnostic2018" role="doc-biblioref">[18]</a></span> describe which feature values “anchor” a prediction, meaning that within this range, the prediction can’t be changed beyond a chosen threshold.</li>
<li>Counterfactual explanations <span class="citation" data-cites="wachterCounterfactualExplanationsOpening2017"><a href="references.html#ref-wachterCounterfactualExplanationsOpening2017" role="doc-biblioref">[19]</a></span> explain a prediction by examining which features to change to achieve a desired counterfactual prediction.</li>
<li>Shapley values <span class="citation" data-cites="strumbeljExplainingPredictionModels2014"><a href="references.html#ref-strumbeljExplainingPredictionModels2014" role="doc-biblioref">[20]</a></span> and SHAP <span class="citation" data-cites="lundbergUnifiedApproachInterpreting2017"><a href="references.html#ref-lundbergUnifiedApproachInterpreting2017" role="doc-biblioref">[21]</a></span> are attribution methods that assign the prediction to individual features based on game theory.</li>
<li>Individual conditional expectation curves <span class="citation" data-cites="goldsteinPeekingBlackBox2015"><a href="references.html#ref-goldsteinPeekingBlackBox2015" role="doc-biblioref">[22]</a></span> describe how changing individual features changes the prediction.</li>
</ul>
</section>
<section id="global-interpreting-average-model-behavior" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="global-interpreting-average-model-behavior"><span class="header-section-number">9.4.2</span> Global: Interpreting average model behavior</h3>
<p>While local explanations are about data points, global explanations are about datasets: they describe how the model behaves, on average, for a given dataset, and, by extension, the distribution that the dataset represents.</p>
<p>We can further separate global interpretability:</p>
<ul>
<li><strong>Feature importance methods</strong> <span class="citation" data-cites="fisher2019all"><a href="references.html#ref-fisher2019all" role="doc-biblioref">[23]</a></span> rank features by how much they influence the predictions. Examples:
<ul>
<li>Permutation feature importance ranks features by how much permuting the feature destroys the information.</li>
<li>SAGE <span class="citation" data-cites="covert2020understanding"><a href="references.html#ref-covert2020understanding" role="doc-biblioref">[24]</a></span> ranks features based on predictive performance when the model is retrained.</li>
<li>SHAP importance <span class="citation" data-cites="lundbergUnifiedApproachInterpreting2017"><a href="references.html#ref-lundbergUnifiedApproachInterpreting2017" role="doc-biblioref">[21]</a></span> ranks features based on the average absolute SHAP values.</li>
</ul></li>
<li><strong>Feature effect methods</strong> describe how features influence the prediction. We can further divide feature effect methods into main and interaction effects: Main feature effects describe how an isolated feature changes the prediction, as shown in <a href="#fig-effect-almond" class="quarto-xref">Figure <span>9.2</span></a>. Interaction effects describe how features interact with each other to influence the predictions. Examples:
<ul>
<li>Partial dependence plots <span class="citation" data-cites="friedman2001greedy"><a href="references.html#ref-friedman2001greedy" role="doc-biblioref">[25]</a></span></li>
<li>Accumulated local effect plots <span class="citation" data-cites="apley2020visualizingeffects"><a href="references.html#ref-apley2020visualizingeffects" role="doc-biblioref">[26]</a></span></li>
<li>SHAP dependence plots <span class="citation" data-cites="lundbergUnifiedApproachInterpreting2017"><a href="references.html#ref-lundbergUnifiedApproachInterpreting2017" role="doc-biblioref">[21]</a></span></li>
</ul></li>
</ul>
<div id="fig-effect-almond" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-effect-almond-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/db76844776ea949f0f07480f9133fbb5.png" class="img-fluid figure-img" data-original-src="https://ml-science-book.com/images/almond-yield-pdps.jpg"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-effect-almond-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.2: Feature effect using PDPs for “age of orchard” on almond yield. Figure by Zhang, Jin, Chen, and Brown (2019) <span class="citation" data-cites="zhang2019california"><a href="references.html#ref-zhang2019california" role="doc-biblioref">[15]</a></span>, CC-BY (https://creativecommons.org/licenses/by/4.0).
</figcaption>
</figure>
</div>
<p>We see global interpretation methods as central, especially for discovery.</p>
</section>
<section id="interpretation-for-scientific-discovery" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="interpretation-for-scientific-discovery"><span class="header-section-number">9.4.3</span> Interpretation for scientific discovery</h3>
<p>How useful is model-agnostic interpretation for scientific insights? Let’s start with two observations:</p>
<ul>
<li>Interpretation is first and foremost about the model.</li>
<li>If you want to extend the model interpretation to the world, you need a theoretical link.</li>
</ul>
<p>Let’s say you model the almond yield with a linear regression model. The coefficients of the linear equation tell you how the features linearly affect the yield. Without further assumption, this interpretation concerns only the model. In classical statistical modeling, a statistician might make the assumption that the conditional distribution of yield given the features is Gaussian, that the errors are homoscedastic, that the data are representative, and so on, and only then carefully make a statement about the real world. Such results in turn may have real-world consequences, such as deciding the fertilizer usage.</p>
<p>In machine learning, you don’t make these assumptions about the data-generating process but let predictive performance guide modeling decisions. But can we establish a theoretical link between the model interpretation and the world maybe differently? We’ve discussed that establishing this link isn’t easy when it comes to model-specific interpretation:</p>
<ul>
<li>You would have to justify whether the specific model structures represent the phenomenon you study. That’s unreasonable for most model classes: What reason do you have to believe that phenomena in the world are structured like a decision rule list or a transformer model?</li>
<li>The Rashomon effect presents you with an unresolvable conflict: If different models have similar predictive performance, how can you justify that the structure of one model represents the world, but the others don’t?</li>
</ul>
<p>But there’s a way to link model and reality via model-agnostic interpretation. With model-agnostic interpretation, you don’t have to link model components to variables of reality. Instead, you interpret the model’s <strong>behavior</strong> and link these interpretations to the phenomenon <span class="citation" data-cites="freiesleben2022scientific"><a href="references.html#ref-freiesleben2022scientific" role="doc-biblioref">[27]</a></span>. This, of course, also doesn’t come for free.</p>
<ul>
<li>You have to assume there is a true function <span class="math inline">\(f\)</span> that describes the relation between the underlying features and the prediction target (see <a href="uncertainty.html#sec-Bayes" class="quarto-xref"><span>Section 12.5.2</span></a> for theoretical background on this true function).</li>
<li>The model needs to be a good representation of function <span class="math inline">\(f\)</span> or at least you should be able to quantify uncertainties.</li>
</ul>
<p>Let’s make it more concrete: The almond researcher has visualized a partial dependence plot that shows how fertilizer use influences the predicted yield. Now they want to extend the interpretation of this effect curve to the real world. First, they assume that there is some true <span class="math inline">\(f\)</span> that their model <span class="math inline">\(\hat{f}\)</span> tries to approximate, a standard assumption in statistical learning theory <span class="citation" data-cites="vapnik1999overview"><a href="references.html#ref-vapnik1999overview" role="doc-biblioref">[28]</a></span>.</p>
<p>The partial dependence plot is defined as:</p>
<p><span class="math display">\[PDP(x) = \mathbb{E}[\hat{f}(x_j, X_{-j})]\]</span></p>
<p>and estimated with:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n \hat{f}(x_j, x_{-j}^{(i)})\]</span></p>
<p>The index indicates the feature of interest, and <span class="math inline">\(-j\)</span> indicates all the other features. You can’t estimate the true PDP because <span class="math inline">\(f\)</span> is unknown, but you can define a theoretical partial dependence plot on the true function by replacing the model <span class="math inline">\(\hat{f}\)</span> for the real <span class="math inline">\(f\)</span>: <span class="math inline">\(PDP_{true}(x) = \mathbb{E}[f(x_j, X_{-j})]\)</span>.</p>
<p>That allows you, at least in theory and simulation, to compare true PDP with estimated PDP, as visualized in <a href="#fig-true-pdp" class="quarto-xref">Figure <span>9.3</span></a>.</p>
<div id="fig-true-pdp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-true-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/d814b17a1155ea3a13ab9694daa94ac0.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" data-original-src="https://ml-science-book.com/images/inference-motivation3.jpg"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-true-pdp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.3: Assumption: there is a “true” PDP of the data-generating process.
</figcaption>
</figure>
</div>
<p>That’s what we have done in <span class="citation" data-cites="molnar2023relating"><a href="references.html#ref-molnar2023relating" role="doc-biblioref">[29]</a></span> and discussed more philosophically in <span class="citation" data-cites="freiesleben2022scientific"><a href="references.html#ref-freiesleben2022scientific" role="doc-biblioref">[27]</a></span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> We’ll give an abbreviated sketch of our ideas here. The error between the true and the model PDP consists of 3 parts:</p>
<ul>
<li>The <em>model bias</em> describes the bias of your learning algorithm.</li>
<li>The <em>model variance</em> describes the algorithm’s variance over datasets from the same distribution.</li>
<li>The <em>estimation error</em> describes the variance that arises from estimating the PDP empirically.</li>
</ul>
<p>Ideally, you can either reduce or remove each source of uncertainty or at least quantify them. The model bias is the toughest part: You can’t easily quantify it because it would require you to know the true function <span class="math inline">\(f\)</span>. A way to reduce the bias is to train and tune the models well and then assume that the model’s bias is negligible, which is a strong assumption to make, of course. Especially considering that many machine learning algorithms rely on regularization which may introduce a bias to reduce variance. An example of model bias: If you use a linear model to model non-linear data, then the PDP will be biased since it can only model a linear relation.</p>
<p>The model variance is a source of uncertainty that stems from the model itself being a random variable since the model is a function of the training data, which is merely a sample of the distribution. If you would sample a different dataset from the same distribution then the trained model might come out slightly different. If you retrain the model multiple times with different data (but from the same distribution), you get an idea of the model’s variance. This makes model variance at least quantifiable.</p>
<p>The third uncertainty source is the estimation error: The PDP, like other interpretation methods, is estimated with data, so it is subject to variance. The estimation error is the simplest to quantify since the PDP at a given position <span class="math inline">\(x\)</span> is an average for which you know the variance.</p>
<p>In our paper, we showed that permutation feature importance also has these 3 sources of uncertainty when comparing model PFI with the “true” PFI. We have conceptually generalized this approach to arbitrary interpretation methods in <span class="citation" data-cites="freiesleben2022scientific"><a href="references.html#ref-freiesleben2022scientific" role="doc-biblioref">[27]</a></span>. While we haven’t tested the approach for all interpretation methods in practice, our intuition says it should be similar to PDP and PFI. This line of research is rather new and we have to see where it leads. To justify using machine learning + post-hoc interpretation as an inference about the real world this alone might be too thin. But then again, many researchers already use machine learning for insights, so having <strong>some</strong> theoretical justification is great to have.</p>
<p>There’s one challenge to interpretability that gives us bad dreams, specifically for the goal of insights: correlated features.</p>
</section>
</section>
<section id="correlation-may-destroy-interpretability" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="correlation-may-destroy-interpretability"><span class="header-section-number">9.5</span> Correlation may destroy interpretability</h2>
<p>When features correlate with each other, model-agnostic interpretation may run into issues. To understand why, let’s revisit how most model-agnostic methods work: Sample data, intervene in the data, get model predictions, and aggregate the results. During the intervention steps, new data points are created, often treating features as independent.</p>
<p>For example, to compute the permutation importance of the feature “absolute humidity” for the almond yield model, the feature gets shuffled (permuted) independently of the other features such as temperature. This can result in unrealistic data points of high humidity but low temperatures. These unrealistic new data points are fed into the model and the predictions are used to interpret the model. This may produce misleading interpretations:</p>
<ul>
<li>The model is probed with data points from regions of the feature space where the model doesn’t work well (because it was never trained with data from this region) or even makes extreme predictions.</li>
<li>Unrealistic data points shouldn’t be used for the interpretation, as they aren’t representative of the studied reality.</li>
<li>Features don’t independently change in the real world</li>
</ul>
<p>Given these issues, why do many still use so-called “marginal” methods for interpretation that treat features as independent? Two reasons: 1) It is technically much easier to, for example, shuffle a feature independently compared to shuffling the feature in a way that preserves the correlation. 2) In an ideal world, you want a disentangled interpretation where you can study each feature in isolation.</p>
<p>In practice, you can avoid or at least reduce the problem of correlated features with these approaches:</p>
<ul>
<li>Study correlations between features. If they are low, you can proceed with the usual marginal version of the interpretation methods.</li>
<li>Interpret feature groups instead of individual features. For most interpretation methods you can also compute the importance or effect for an entire group of features. For example, you can compute the combined PFI for humidity and precipitation by shuffling them together.</li>
<li>Use conditional versions of the interpretation methods. For example, conditional feature importance <span class="citation" data-cites="watson2021testing"><a href="references.html#ref-watson2021testing" role="doc-biblioref">[30]</a></span>, conditional SHAP <span class="citation" data-cites="aas2021explaining"><a href="references.html#ref-aas2021explaining" role="doc-biblioref">[31]</a></span>, M-Plot and ALE <span class="citation" data-cites="apley2020visualizingeffects"><a href="references.html#ref-apley2020visualizingeffects" role="doc-biblioref">[26]</a></span> and subgroup-wise PDP and PFI <span class="citation" data-cites="molnar2023modelagnostic"><a href="references.html#ref-molnar2023modelagnostic" role="doc-biblioref">[32]</a></span>, Leave-one-covariate out (LOCO) <span class="citation" data-cites="lei2018distributionfree"><a href="references.html#ref-lei2018distributionfree" role="doc-biblioref">[33]</a></span>.</li>
</ul>
<p>The third option, conditional interpretation, is more than a technical fix.</p>
<p>Let’s say you want to shuffle the humidity feature to get its importance for almond yield. It is correlated with temperature, so you have to ensure that new data points respect the correlation structure. Instead of shuffling humidity independently, you sample it based on the values of temperature. The first data point has a high absolute humidity, you draw the temperature feature based on this high humidity. This leads to the “permuted” humidity feature to respect the correlation with precipitation, while breaking the relation with the yield target.</p>
<p>Conditional intervention changes the interpretation. For example, for permutation feature importance, the interpretation changes to: Given that the model has access to temperature, how important is it to know the humidity in addition? The more complex the dependence structures between features are, the more complex the (conditional) interpretation becomes. To use conditional interpretation, you must understand how the interpretation changes compared to the marginal versions. But on the other side, we’d argue that conditional interpretation is the way to go, especially for the goal of insights. Not only because it fixes the extrapolation problem, but also because for insights you are interested in how the features are related to the target regardless of the model.</p>
</section>
<section id="interpretability-is-just-one-part" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="interpretability-is-just-one-part"><span class="header-section-number">9.6</span> Interpretability is just one part</h2>
<p>Model interpretability alone might not solve your problems. Rather, it is strongly interlinked with the other topics covered in this book.</p>
<ul>
<li>Interpretability for scientific insights is tightly linked to generalization (see <a href="generalization.html" class="quarto-xref"><span>Chapter 7</span></a>). If you aren’t aware of how the data were collected and which population the dataset represents, it is unclear what to do with the conclusions from model interpretation.</li>
<li>When you justify the model for your boss, your colleague, or your peers, it is by showing the model coheres with domain knowledge (see <a href="domain.html" class="quarto-xref"><span>Chapter 8</span></a>).</li>
<li>Understanding causality (see <a href="causality.html" class="quarto-xref"><span>Chapter 10</span></a>) is crucial for interpretation. For example, a feature effect may be rather misleading if confounders are missing from the model.</li>
</ul>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-vannoordenAIScienceWhat2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R. Van Noorden and J. M. Perkel, <span>“<span>AI</span> and science: What 1,600 researchers think,”</span> <em>Nature</em>, vol. 621, no. 7980, pp. 672–675, Sep. 2023, doi: <a href="https://doi.org/10.1038/d41586-023-02980-0">10.1038/d41586-023-02980-0</a>.</div>
</div>
<div id="ref-flora2022comparing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">M. Flora, C. Potvin, A. McGovern, and S. Handler, <span>“Comparing <span>Explanation Methods</span> for <span>Traditional Machine Learning Models Part</span> 1: <span>An Overview</span> of <span>Current Methods</span> and <span>Quantifying Their Disagreement</span>.”</span> <span>arXiv</span>, Nov. 2022. doi: <a href="https://doi.org/10.48550/arXiv.2211.08943">10.48550/arXiv.2211.08943</a>.</div>
</div>
<div id="ref-roscherExplainableMachineLearning2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">R. Roscher, B. Bohn, M. F. Duarte, and J. Garcke, <span>“Explainable <span>Machine Learning</span> for <span>Scientific Insights</span> and <span>Discoveries</span>,”</span> <em>IEEE Access</em>, vol. 8, pp. 42200–42216, 2020, doi: <a href="https://doi.org/10.1109/ACCESS.2020.2976199">10.1109/ACCESS.2020.2976199</a>.</div>
</div>
<div id="ref-miller2019explanation" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">T. Miller, <span>“Explanation in artificial intelligence: <span>Insights</span> from the social sciences,”</span> <em>Artificial Intelligence</em>, vol. 267, pp. 1–38, Feb. 2019, doi: <a href="https://doi.org/10.1016/j.artint.2018.07.007">10.1016/j.artint.2018.07.007</a>.</div>
</div>
<div id="ref-doshi-velezRigorousScienceInterpretable2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">F. Doshi-Velez and B. Kim, <span>“Towards <span>A Rigorous Science</span> of <span>Interpretable Machine Learning</span>.”</span> <span>arXiv</span>, Mar. 2017. Accessed: Dec. 01, 2023. [Online]. Available: <a href="http://arxiv.org/abs/1702.08608">http://arxiv.org/abs/1702.08608</a></div>
</div>
<div id="ref-adadiPeekingBlackBoxSurvey2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">A. Adadi and M. Berrada, <span>“Peeking <span>Inside</span> the <span>Black-Box</span>: <span>A Survey</span> on <span>Explainable Artificial Intelligence</span> (<span>XAI</span>),”</span> <em>IEEE Access</em>, vol. 6, pp. 52138–52160, 2018, doi: <a href="https://doi.org/10.1109/ACCESS.2018.2870052">10.1109/ACCESS.2018.2870052</a>.</div>
</div>
<div id="ref-bassoSeasonalCropYield2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">B. Basso and L. Liu, <span>“Seasonal crop yield forecast: <span>Methods</span>, applications, and accuracies,”</span> in <em>Advances in <span>Agronomy</span></em>, vol. 154, <span>Elsevier</span>, 2019, pp. 201–255. doi: <a href="https://doi.org/10.1016/bs.agron.2018.11.002">10.1016/bs.agron.2018.11.002</a>.</div>
</div>
<div id="ref-friedman2008predictive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">J. H. Friedman and B. E. Popescu, <span>“Predictive <span>Learning</span> via <span>Rule Ensembles</span>,”</span> <em>The Annals of Applied Statistics</em>, vol. 2, no. 3, pp. 916–954, 2008, doi: <a href="https://doi.org/10.1214/07-AOAS148">10.1214/07-AOAS148</a>.</div>
</div>
<div id="ref-zeileis2008modelbased" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">A. Zeileis, T. Hothorn, and K. Hornik, <span>“Model-<span>Based Recursive Partitioning</span>,”</span> <em>Journal of Computational and Graphical Statistics</em>, vol. 17, no. 2, pp. 492–514, Jun. 2008, doi: <a href="https://doi.org/10.1198/106186008X319331">10.1198/106186008X319331</a>.</div>
</div>
<div id="ref-rudin2022interpretable" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and C. Zhong, <span>“Interpretable machine learning: <span>Fundamental</span> principles and 10 grand challenges,”</span> <em>Statistics Surveys</em>, vol. 16, no. none, pp. 1–85, Jan. 2022, doi: <a href="https://doi.org/10.1214/21-SS133">10.1214/21-SS133</a>.</div>
</div>
<div id="ref-liptonMythosModelInterpretability2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">Z. C. Lipton, <span>“The <span>Mythos</span> of <span>Model Interpretability</span>.”</span> <span>arXiv</span>, Mar. 2017. doi: <a href="https://doi.org/10.48550/arXiv.1606.03490">10.48550/arXiv.1606.03490</a>.</div>
</div>
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">L. Breiman, <span>“Random <span>Forests</span>,”</span> <em>Machine Learning</em>, vol. 45, no. 1, pp. 5–32, Oct. 2001, doi: <a href="https://doi.org/10.1023/A:1010933404324">10.1023/A:1010933404324</a>.</div>
</div>
<div id="ref-freiesleben2023artificial" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">T. Freiesleben, <span>“Artificial neural nets and the representation of human concepts,”</span> <em>arXiv preprint arXiv:2312.05337</em>, 2023, doi: <a href="https://doi.org/10.48550/arXiv.2312.05337">10.48550/arXiv.2312.05337</a>.</div>
</div>
<div id="ref-scholbeck2020sampling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">C. A. Scholbeck, C. Molnar, C. Heumann, B. Bischl, and G. Casalicchio, <span>“Sampling, <span>Intervention</span>, <span>Prediction</span>, <span>Aggregation</span>: <span>A</span> <span>Generalized</span> <span>Framework</span> for <span>Model</span>-<span>Agnostic</span> <span>Interpretations</span>,”</span> in <em>Machine <span>Learning</span> and <span>Knowledge</span> <span>Discovery</span> in <span>Databases</span></em>, P. Cellier and K. Driessens, Eds., in Communications in <span>Computer</span> and <span>Information</span> <span>Science</span>. Cham: Springer International Publishing, 2020, pp. 205–216. doi: <a href="https://doi.org/10.1007/978-3-030-43823-4_18">10.1007/978-3-030-43823-4_18</a>.</div>
</div>
<div id="ref-zhang2019california" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">Z. Zhang, Y. Jin, B. Chen, and P. Brown, <span>“California almond yield prediction at the orchard level with a machine learning approach,”</span> <em>Frontiers in plant science</em>, vol. 10, p. 809, 2019, doi: <a href="https://doi.org/10.3389/fpls.2019.00809/full">10.3389/fpls.2019.00809/full</a>.</div>
</div>
<div id="ref-molnar2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">C. Molnar, <em>Interpretable machine learning: A guide for making black box models explainable</em>, 2nd ed. 2022. Available: <a href="https://christophm.github.io/interpretable-ml-book">https://christophm.github.io/interpretable-ml-book</a></div>
</div>
<div id="ref-ribeiroWhyShouldTrust2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">M. T. Ribeiro, S. Singh, and C. Guestrin, <span>“"<span>Why Should I Trust You</span>?": <span>Explaining</span> the <span>Predictions</span> of <span>Any Classifier</span>,”</span> in <em>Proceedings of the 22nd <span>ACM SIGKDD International Conference</span> on <span>Knowledge Discovery</span> and <span>Data Mining</span></em>, in <span>KDD</span> ’16. <span>New York, NY, USA</span>: <span>Association for Computing Machinery</span>, Aug. 2016, pp. 1135–1144. doi: <a href="https://doi.org/10.1145/2939672.2939778">10.1145/2939672.2939778</a>.</div>
</div>
<div id="ref-ribeiroAnchorsHighPrecisionModelAgnostic2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">M. T. Ribeiro, S. Singh, and C. Guestrin, <span>“Anchors: <span>High-Precision Model-Agnostic Explanations</span>,”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 32, no. 1, Apr. 2018, doi: <a href="https://doi.org/10.1609/aaai.v32i1.11491">10.1609/aaai.v32i1.11491</a>.</div>
</div>
<div id="ref-wachterCounterfactualExplanationsOpening2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">S. Wachter, B. Mittelstadt, and C. Russell, <span>“Counterfactual <span>Explanations Without Opening</span> the <span>Black Box</span>: <span>Automated Decisions</span> and the <span>GDPR</span>,”</span> <em>SSRN Electronic Journal</em>, 2017, doi: <a href="https://doi.org/10.2139/ssrn.3063289">10.2139/ssrn.3063289</a>.</div>
</div>
<div id="ref-strumbeljExplainingPredictionModels2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">E. Štrumbelj and I. Kononenko, <span>“Explaining prediction models and individual predictions with feature contributions,”</span> <em>Knowledge and Information Systems</em>, vol. 41, no. 3, pp. 647–665, Dec. 2014, doi: <a href="https://doi.org/10.1007/s10115-013-0679-x">10.1007/s10115-013-0679-x</a>.</div>
</div>
<div id="ref-lundbergUnifiedApproachInterpreting2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">S. M. Lundberg and S.-I. Lee, <span>“A unified approach to interpreting model predictions,”</span> in <em>Proceedings of the 31st <span>International Conference</span> on <span>Neural Information Processing Systems</span></em>, in <span>NIPS</span>’17. <span>Red Hook, NY, USA</span>: <span>Curran Associates Inc.</span>, Dec. 2017, pp. 4768–4777. doi: <a href="https://doi.org/10.5555/3295222.3295230">10.5555/3295222.3295230</a>.</div>
</div>
<div id="ref-goldsteinPeekingBlackBox2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, <span>“Peeking <span>Inside</span> the <span>Black Box</span>: <span>Visualizing Statistical Learning With Plots</span> of <span>Individual Conditional Expectation</span>,”</span> <em>Journal of Computational and Graphical Statistics</em>, vol. 24, no. 1, pp. 44–65, Jan. 2015, doi: <a href="https://doi.org/10.1080/10618600.2014.907095">10.1080/10618600.2014.907095</a>.</div>
</div>
<div id="ref-fisher2019all" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">A. Fisher, C. Rudin, and F. Dominici, <span>“All <span>Models</span> are <span>Wrong</span>, but <span>Many</span> are <span>Useful</span>: <span>Learning</span> a <span>Variable</span>’s <span>Importance</span> by <span>Studying</span> an <span>Entire</span> <span>Class</span> of <span>Prediction</span> <span>Models</span> <span>Simultaneously</span>,”</span> <em>Journal of machine learning research : JMLR</em>, vol. 20, p. 177, 2019, Accessed: Jan. 16, 2024. [Online]. Available: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323609/</a></div>
</div>
<div id="ref-covert2020understanding" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">I. C. Covert, S. Lundberg, and S.-I. Lee, <span>“Understanding global feature contributions with additive importance measures,”</span> in <em>Proceedings of the 34th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em>, in <span>NIPS</span>’20. <span>Red Hook, NY, USA</span>: <span>Curran Associates Inc.</span>, Dec. 2020, pp. 17212–17223.</div>
</div>
<div id="ref-friedman2001greedy" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">J. H. Friedman, <span>“Greedy function approximation: <span>A</span> gradient boosting machine.”</span> <em>The Annals of Statistics</em>, vol. 29, no. 5, pp. 1189–1232, Oct. 2001, doi: <a href="https://doi.org/10.1214/aos/1013203451">10.1214/aos/1013203451</a>.</div>
</div>
<div id="ref-apley2020visualizingeffects" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">D. W. Apley and J. Zhu, <span>“Visualizing the <span>Effects</span> of <span>Predictor Variables</span> in <span>Black Box Supervised Learning Models</span>,”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, vol. 82, no. 4, pp. 1059–1086, Sep. 2020, doi: <a href="https://doi.org/10.1111/rssb.12377">10.1111/rssb.12377</a>.</div>
</div>
<div id="ref-freiesleben2022scientific" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">T. Freiesleben, G. König, C. Molnar, and Á. Tejero-Cantero, <span>“Scientific inference with interpretable machine learning: Analyzing models to learn about real-world phenomena,”</span> <em>Minds and Machines</em>, vol. 34, no. 3, p. 32, 2024, doi: <a href="https://doi.org/10.1007/s11023-024-09691-z">10.1007/s11023-024-09691-z</a>.</div>
</div>
<div id="ref-vapnik1999overview" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">V. N. Vapnik, <span>“An overview of statistical learning theory,”</span> <em>IEEE transactions on neural networks</em>, vol. 10, no. 5, pp. 988–999, 1999, doi: <a href="https://doi.org/10.1109/72.788640">10.1109/72.788640</a>.</div>
</div>
<div id="ref-molnar2023relating" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">C. Molnar <em>et al.</em>, <span>“Relating the <span>Partial Dependence Plot</span> and <span>Permutation Feature Importance</span> to the <span>Data Generating Process</span>,”</span> in <em>Explainable <span>Artificial Intelligence</span></em>, L. Longo, Ed., in Communications in <span>Computer</span> and <span>Information Science</span>. <span>Cham</span>: <span>Springer Nature Switzerland</span>, 2023, pp. 456–479. doi: <a href="https://doi.org/10.1007/978-3-031-44064-9_24">10.1007/978-3-031-44064-9_24</a>.</div>
</div>
<div id="ref-watson2021testing" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">D. S. Watson and M. N. Wright, <span>“Testing conditional independence in supervised learning algorithms,”</span> <em>Machine Learning</em>, vol. 110, no. 8, pp. 2107–2129, Aug. 2021, doi: <a href="https://doi.org/10.1007/s10994-021-06030-6">10.1007/s10994-021-06030-6</a>.</div>
</div>
<div id="ref-aas2021explaining" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">K. Aas, M. Jullum, and A. Løland, <span>“Explaining individual predictions when features are dependent: <span>More</span> accurate approximations to <span>Shapley</span> values,”</span> <em>Artificial Intelligence</em>, vol. 298, p. 103502, Sep. 2021, doi: <a href="https://doi.org/10.1016/j.artint.2021.103502">10.1016/j.artint.2021.103502</a>.</div>
</div>
<div id="ref-molnar2023modelagnostic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">C. Molnar, G. König, B. Bischl, and G. Casalicchio, <span>“Model-agnostic <span>Feature Importance</span> and <span>Effects</span> with <span>Dependent Features</span> – <span>A Conditional Subgroup Approach</span>,”</span> <em>Data Mining and Knowledge Discovery</em>, Jan. 2023, doi: <a href="https://doi.org/10.1007/s10618-022-00901-9">10.1007/s10618-022-00901-9</a>.</div>
</div>
<div id="ref-lei2018distributionfree" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman, <span>“Distribution-<span>Free Predictive Inference</span> for <span>Regression</span>,”</span> <em>Journal of the American Statistical Association</em>, vol. 113, no. 523, pp. 1094–1111, Jul. 2018, doi: <a href="https://doi.org/10.1080/01621459.2017.1307116">10.1080/01621459.2017.1307116</a>.</div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>What do interpretability and explainability exactly mean? Even researchers in this field can’t decide on a definition <span class="citation" data-cites="flora2022comparing"><a href="references.html#ref-flora2022comparing" role="doc-biblioref">[2]</a></span>. From an application-oriented perspective, it is most useful to treat these terms interchangeably. Under these keywords, you find approaches that allow you to extract information from the model about how it makes predictions. We use the definition from <span class="citation" data-cites="roscherExplainableMachineLearning2020"><a href="references.html#ref-roscherExplainableMachineLearning2020" role="doc-biblioref">[3]</a></span>: Interpretability is about mapping an abstract concept from the models to an understandable form. Explainability is a stronger term that requires interpretability and additional context.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Our work on extending interpretation for insights is what motivated us to write the book you are reading.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>