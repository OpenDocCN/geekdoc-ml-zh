<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Implementation</h1>
<blockquote>原文：<a href="https://dafriedman97.github.io/mlbook/content/c3/code.html">https://dafriedman97.github.io/mlbook/content/c3/code.html</a></blockquote>

<p>This section will demonstrate how to fit the discriminative classifiers discussed in this chapter with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. Note that other libraries are frequently used—e.g. <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> for logistic regresssion and <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> for the perceptron.</p>
<p>For binary tasks, we’ll be using the <a class="reference internal" href="../appendix/data.html"><span class="doc">breast cancer</span></a> dataset and for multiclass tasks, we’ll be using the <a class="reference internal" href="../appendix/data.html"><span class="doc">wine</span></a> dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># import data</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_cancer</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">y_cancer</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="s1">'target'</span><span class="p">]</span>
<span class="n">wine</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_wine</span><span class="p">()</span>
<span class="n">X_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">'data'</span><span class="p">]</span>
<span class="n">y_wine</span> <span class="o">=</span> <span class="n">wine</span><span class="p">[</span><span class="s1">'target'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="logistic-regression">
<h2>Logistic Regression</h2>
<div class="section" id="binary-logistic-regression">
<h3>Binary Logistic Regression</h3>
<p>A standard <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> implementation of binary logistic regression is shown below. Note the two arguments set when instantiating the model: <code class="docutils literal notranslate"><span class="pre">C</span></code> is a regularization term where a higher <code class="docutils literal notranslate"><span class="pre">C</span></code> indicates <em>less</em> penalty on the magnitude of the coefficients and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> determines the maximum number of iterations the solver will use. We set <code class="docutils literal notranslate"><span class="pre">C</span></code> to be arbitrarily high such that there is effectively no regulariation and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> to be 1,000, which is enough for this model to converge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">binary_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">)</span>
<span class="n">binary_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s logistic regression model can return two forms of predictions: the predicted classes or the predicted probabilities. The <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method predicts an observation for each class while <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code> gives the probability for all classes included in the training set (in this case, just 0 and 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">binary_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 0.984182776801406
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="multiclass-logistic-regression">
<h3>Multiclass Logistic Regression</h3>
<p>Multiclass logistic regression can be fit in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> as below. In fact, no arguments need to be changed in order to fit a multiclass model versus a binary one. However, the implementation below adds one new argument. Setting <code class="docutils literal notranslate"><span class="pre">multiclass</span></code> equal to ‘multinomial’ tells the model explicitly to follow the algorithm introduced in the <a class="reference internal" href="s2/logistic_regression.html"><span class="doc">concept section</span></a>. This will be done by default for non-binary problems unless the <code class="docutils literal notranslate"><span class="pre">solver</span></code> is set to ‘liblinear’. In that case, it will fit a “<a class="reference external" href="https://www.google.com/search?q=one+versus+rest+classifier&amp;oq=one+versus+rest+classifier&amp;aqs=chrome..69i57j0l4j69i64.3569j0j1&amp;sourceid=chrome&amp;ie=UTF-8">one-versus-rest</a>” model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">multiclass_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span> <span class="o">=</span> <span class="s1">'multinomial'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span>
<span class="n">multiclass_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=10000,
                   multi_class='multinomial', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p>Again, we can see the predicted classes and predicted probabilities for each class, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">multiclass_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 1.0
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="the-perceptron-algorithm">
<h2>The Perceptron Algorithm</h2>
<p>The perceptron algorithm is implemented below. This algorithm is rarely used in practice but serves as an important part of neural networks, the topic of Chapter 7.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">()</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="fisher-s-linear-discriminant">
<h2>Fisher’s Linear Discriminant</h2>
<p>Finally, we fit Fisher’s Linear Discriminant with the <code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code> class from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. This class can also be viewed as a generative model, which is discussed in the next chapter, but the implementation below reduces to the discriminative classifier derived in the concept section. Specifying <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">=</span> <span class="pre">1</span></code> tells the model to reduce the data to one dimension. This is the equivalent of generating the</p>
<div class="math notranslate nohighlight">
\[
f(\bx_n) = \bbeta^\top \bx_n
\]</div>
<p>transformations that we saw in the concept section. We can then see if the two classes are separated by checking that either 1) <span class="math notranslate nohighlight">\(f(\bx_n) &lt; f(\bx_m)\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> in class 0 and <span class="math notranslate nohighlight">\(m\)</span> in class 1 or 2) <span class="math notranslate nohighlight">\(f(\bx_n) &gt; f(\bx_m)\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> in class 0 and <span class="math notranslate nohighlight">\(m\)</span> in class 1. Equivalently, we can see that the two classes are not separated in the histogram below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">);</span>

<span class="n">f0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">y_cancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">y_cancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Separated:'</span><span class="p">,</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">f0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">max</span><span class="p">(</span><span class="n">f1</span><span class="p">))</span> <span class="o">|</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">f0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="n">f1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Separated: False
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
             <span class="n">color</span> <span class="o">=</span> <span class="s1">'cornflowerblue'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Class 0'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
             <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkblue'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Class 1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$f\hspace{.25}(x_n)$"</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">"Histogram of $f\hspace{.25}(x_n)$ by Class"</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/code_20_0.png" src="../Images/19e8f02c59c9b29f4ab3056d2d3c0dec.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/code_20_0.png"/>
</div>
</div>
</div>
&#13;

<h2>Logistic Regression</h2>
<div class="section" id="binary-logistic-regression">
<h3>Binary Logistic Regression</h3>
<p>A standard <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> implementation of binary logistic regression is shown below. Note the two arguments set when instantiating the model: <code class="docutils literal notranslate"><span class="pre">C</span></code> is a regularization term where a higher <code class="docutils literal notranslate"><span class="pre">C</span></code> indicates <em>less</em> penalty on the magnitude of the coefficients and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> determines the maximum number of iterations the solver will use. We set <code class="docutils literal notranslate"><span class="pre">C</span></code> to be arbitrarily high such that there is effectively no regulariation and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> to be 1,000, which is enough for this model to converge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">binary_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">)</span>
<span class="n">binary_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s logistic regression model can return two forms of predictions: the predicted classes or the predicted probabilities. The <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method predicts an observation for each class while <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code> gives the probability for all classes included in the training set (in this case, just 0 and 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">binary_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 0.984182776801406
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="multiclass-logistic-regression">
<h3>Multiclass Logistic Regression</h3>
<p>Multiclass logistic regression can be fit in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> as below. In fact, no arguments need to be changed in order to fit a multiclass model versus a binary one. However, the implementation below adds one new argument. Setting <code class="docutils literal notranslate"><span class="pre">multiclass</span></code> equal to ‘multinomial’ tells the model explicitly to follow the algorithm introduced in the <a class="reference internal" href="s2/logistic_regression.html"><span class="doc">concept section</span></a>. This will be done by default for non-binary problems unless the <code class="docutils literal notranslate"><span class="pre">solver</span></code> is set to ‘liblinear’. In that case, it will fit a “<a class="reference external" href="https://www.google.com/search?q=one+versus+rest+classifier&amp;oq=one+versus+rest+classifier&amp;aqs=chrome..69i57j0l4j69i64.3569j0j1&amp;sourceid=chrome&amp;ie=UTF-8">one-versus-rest</a>” model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">multiclass_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span> <span class="o">=</span> <span class="s1">'multinomial'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span>
<span class="n">multiclass_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=10000,
                   multi_class='multinomial', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p>Again, we can see the predicted classes and predicted probabilities for each class, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">multiclass_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 1.0
</pre></div>
</div>
</div>
</div>
</div>
&#13;

<h3>Binary Logistic Regression</h3>
<p>A standard <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> implementation of binary logistic regression is shown below. Note the two arguments set when instantiating the model: <code class="docutils literal notranslate"><span class="pre">C</span></code> is a regularization term where a higher <code class="docutils literal notranslate"><span class="pre">C</span></code> indicates <em>less</em> penalty on the magnitude of the coefficients and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> determines the maximum number of iterations the solver will use. We set <code class="docutils literal notranslate"><span class="pre">C</span></code> to be arbitrarily high such that there is effectively no regulariation and <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> to be 1,000, which is enough for this model to converge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">binary_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mf">1e5</span><span class="p">)</span>
<span class="n">binary_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s logistic regression model can return two forms of predictions: the predicted classes or the predicted probabilities. The <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method predicts an observation for each class while <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code> gives the probability for all classes included in the training set (in this case, just 0 and 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">binary_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">binary_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 0.984182776801406
</pre></div>
</div>
</div>
</div>
&#13;

<h3>Multiclass Logistic Regression</h3>
<p>Multiclass logistic regression can be fit in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> as below. In fact, no arguments need to be changed in order to fit a multiclass model versus a binary one. However, the implementation below adds one new argument. Setting <code class="docutils literal notranslate"><span class="pre">multiclass</span></code> equal to ‘multinomial’ tells the model explicitly to follow the algorithm introduced in the <a class="reference internal" href="s2/logistic_regression.html"><span class="doc">concept section</span></a>. This will be done by default for non-binary problems unless the <code class="docutils literal notranslate"><span class="pre">solver</span></code> is set to ‘liblinear’. In that case, it will fit a “<a class="reference external" href="https://www.google.com/search?q=one+versus+rest+classifier&amp;oq=one+versus+rest+classifier&amp;aqs=chrome..69i57j0l4j69i64.3569j0j1&amp;sourceid=chrome&amp;ie=UTF-8">one-versus-rest</a>” model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">multiclass_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span> <span class="o">=</span> <span class="s1">'multinomial'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">)</span>
<span class="n">multiclass_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>LogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=10000,
                   multi_class='multinomial', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</pre></div>
</div>
</div>
</div>
<p>Again, we can see the predicted classes and predicted probabilities for each class, as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="n">p_hats</span> <span class="o">=</span> <span class="n">multiclass_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_wine</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Training accuracy: </span><span class="si">{</span><span class="n">multiclass_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Training accuracy: 1.0
</pre></div>
</div>
</div>
</div>
&#13;

<h2>The Perceptron Algorithm</h2>
<p>The perceptron algorithm is implemented below. This algorithm is rarely used in practice but serves as an important part of neural networks, the topic of Chapter 7.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">()</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Fisher’s Linear Discriminant</h2>
<p>Finally, we fit Fisher’s Linear Discriminant with the <code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code> class from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. This class can also be viewed as a generative model, which is discussed in the next chapter, but the implementation below reduces to the discriminative classifier derived in the concept section. Specifying <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">=</span> <span class="pre">1</span></code> tells the model to reduce the data to one dimension. This is the equivalent of generating the</p>
<div class="math notranslate nohighlight">
\[
f(\bx_n) = \bbeta^\top \bx_n
\]</div>
<p>transformations that we saw in the concept section. We can then see if the two classes are separated by checking that either 1) <span class="math notranslate nohighlight">\(f(\bx_n) &lt; f(\bx_m)\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> in class 0 and <span class="math notranslate nohighlight">\(m\)</span> in class 1 or 2) <span class="math notranslate nohighlight">\(f(\bx_n) &gt; f(\bx_m)\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> in class 0 and <span class="math notranslate nohighlight">\(m\)</span> in class 1. Equivalently, we can see that the two classes are not separated in the histogram below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">);</span>

<span class="n">f0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">y_cancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">y_cancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Separated:'</span><span class="p">,</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">f0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">max</span><span class="p">(</span><span class="n">f1</span><span class="p">))</span> <span class="o">|</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">f0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="n">f1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Separated: False
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
             <span class="n">color</span> <span class="o">=</span> <span class="s1">'cornflowerblue'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Class 0'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
             <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkblue'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Class 1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$f\hspace{.25}(x_n)$"</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">"Histogram of $f\hspace{.25}(x_n)$ by Class"</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/code_20_0.png" src="../Images/19e8f02c59c9b29f4ab3056d2d3c0dec.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/code_20_0.png"/>
</div>
</div>
    
</body>
</html>