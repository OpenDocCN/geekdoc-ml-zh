- en: Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Using Seeed Studio Grove Vision AI Module V2 (Himax WiseEye2)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file740.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this Lab, we will explore Image Classification using the Seeed Studio [*Grove
    Vision AI Module V2*](https://wiki.seeedstudio.com/grove_vision_ai_v2/), a powerful
    yet compact device specifically designed for embedded machine learning applications.
    Based on the **Himax WiseEye2** chip, this module is designed to enable AI capabilities
    on edge devices, making it an ideal tool for Edge Machine Learning (ML) applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have explored several computer vision models previously uploaded
    by Seeed Studio or used the SenseCraft AI Studio for Image Classification, without
    choosing a specific model. Let’s now develop our Image Classification project
    from scratch, where we will select our data and model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we can see the project’s main steps and where we will work with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file741.png)'
  prefs: []
  type: TYPE_IMG
- en: Project Goal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in any machine learning (ML) project is defining the goal. In
    this case, the goal is to detect and classify two specific objects present in
    a single image. For this project, we will use two small toys: a robot and a small
    Brazilian parrot (named *Periquito*). Also, we will collect images of a background
    where those two objects are absent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file742.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the Machine Learning project goal defined, dataset collection is the next
    and most crucial step. Suppose your project utilizes images that are publicly
    available on datasets, for example, to be used on a **Person Detection** project.
    In that case, you can download the [Wake Vision](https://edgeai.modelnova.ai/datasets/details/wake-vision)
    dataset for use in the project.
  prefs: []
  type: TYPE_NORMAL
- en: But, in our case, we define a project where the images do not exist publicly,
    so we need to generate them. We can use a phone, computer camera, or other devices
    to capture the photos, offline or connected to the Edge Impulse Studio.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use the Grove Vision AI V2 to capture your dataset, you can use
    the SenseCraft AI Studio as we did in the previous Lab, or the `camera_web_server`
    sketch as we will describe later in the **Postprocessing / Getting the Video Stream**
    section of this Lab.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file743.png)'
  prefs: []
  type: TYPE_IMG
- en: In this Lab, we will use the SenseCraft AI Studio to collect the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting Data with the SenseCraft AI Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On SenseCraft AI Studio: Let’s open the tab [Training](https://sensecraft.seeed.cc/ai/training).'
  prefs: []
  type: TYPE_NORMAL
- en: The default is to train a `Classification` model with a WebCam if it is available.
    Let’s select the `Grove Vision AI V2 instead`. Pressing the green button`[Connect]`
    **(1),** a Pop-Up window will appear. Select the corresponding Port **(2)** and
    press the blue button `[Connect]` **(3)**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file744.png)'
  prefs: []
  type: TYPE_IMG
- en: The image streamed from the Grove Vision AI V2 will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Image Collection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s create the classes, following, for example, an alphabetical order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Class1: background'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class 2: periquito'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class 3: robot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../media/file745.png)'
  prefs: []
  type: TYPE_IMG
- en: Select one of the classes (note that a green line will be around the window)
    and keep pressing the green button under the preview area. The collected images
    will appear on the Image Samples Screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file746.png)'
  prefs: []
  type: TYPE_IMG
- en: After collecting the images, review them and, if necessary, delete any incorrect
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file747.png)'
  prefs: []
  type: TYPE_IMG
- en: Collect around 50 images from each class. After you collect the three classes,
    open the menu on each of them and select `Export Data`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file748.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Download area of the Computer, we will get three zip files, each one
    with its corresponding class name. Each Zip file contains a folder with the images.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the dataset to the Edge Impulse Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the Edge Impulse Studio to train our model. [Edge Impulse](https://www.edgeimpulse.com/)
    is a leading development platform for machine learning on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Enter your account credentials (or create a free account) at Edge Impulse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, create a new project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../media/file749.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset comprises approximately 50 images per label, with 40 for training
    and 10 for testing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Impulse Design and Pre-Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Impulse Design**'
  prefs: []
  type: TYPE_NORMAL
- en: An impulse takes raw data (in this case, images), extracts features (resizes
    pictures), and then uses a learning block to classify new data.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images is the most common application of deep learning, but a substantial
    amount of data is required to accomplish this task. We have around 50 images for
    each category. Is this number enough? Not at all! We will need thousands of images
    to “teach” or “model” each class, allowing us to differentiate them. However,
    we can resolve this issue by retraining a previously trained model using thousands
    of images. We refer to this technique as “Transfer Learning” (TL). With TL, we
    can fine-tune a pre-trained image classification model on our data, achieving
    good performance even with relatively small image datasets, as in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file750.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, starting from the raw images, we will resize them (96x96) pixels and feed
    them to our Transfer Learning block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file751.png)'
  prefs: []
  type: TYPE_IMG
- en: For comparison, we will keep the image size as 96 x 96\. However, keep in mind
    that with the Grove Vision AI Module V2 and its internal SRAM of 2.4 MB, larger
    images can be utilized (for example, 160 x 160).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also select the `Target` device (`Himax WiseEye2 (M55 400 MHz + U55)`) on the
    up-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing (Feature generation)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides resizing the images, we can convert them to grayscale or retain their
    original RGB color depth. Let’s select `[RGB]` in the `Image` section. Doing that,
    each data sample will have a dimension of 27,648 features (96x96x3). Pressing
    `[Save Parameters]` will open a new tab, `Generate Features`. Press the button
    `[Generate Features]`to generate the features.
  prefs: []
  type: TYPE_NORMAL
- en: Model Design, Training, and Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html).
    In 2018, [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381),
    was launched, and, in 2019, the V3\. The Mobilinet is a family of general-purpose
    computer vision neural networks explicitly designed for mobile devices to support
    classification, detection, and other applications. MobileNets are small, low-latency,
    low-power models parameterized to meet the resource constraints of various use
    cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the base MobileNet architecture is already compact and has low latency,
    a specific use case or application may often require the model to be even smaller
    and faster. MobileNets introduce a straightforward parameter, **α** (alpha), called
    the width multiplier to construct these smaller, less computationally expensive
    models. The role of the width multiplier α is to thin a network uniformly at each
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96
    and 160x160 images), with several different **α** values (from 0.05 to 1.0). For
    example, you will get the highest accuracy with V2, 160x160 images, and α=1.0\.
    Of course, there is a trade-off. The higher the accuracy, the more memory (around
    1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.
    The smaller footprint will be obtained at another extreme with MobileNet V1 and
    α=0.10 (around 53.2K RAM and 101K ROM).
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, we will use the **MobileNet V2 0.1** as our base model (but
    a model with a greater alpha can be used here). The final layer of our model,
    preceding the output layer, will have 8 neurons with a 10% dropout rate for preventing
    overfitting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another necessary technique to use with deep learning is **data augmentation**.
    Data augmentation is a method that can help improve the accuracy of machine learning
    models by creating additional artificial data. A data augmentation system makes
    small, random changes to your training data during the training process (such
    as flipping, cropping, or rotating the images).
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the Hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Epochs: 20,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bach Size: 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning Rate: 0.0005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation size: 20%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file752.png)'
  prefs: []
  type: TYPE_IMG
- en: The model profile predicts **146 KB of RAM and 187 KB of Flash**, indicating
    no problem with the Grove AI Vision (V2), which has almost 2.5 MB of internal
    SRAM. Additionally, the Studio indicates a **latency of around 4 ms.**
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, with a 100% accuracy on the Validation set when using the spare
    data for testing, we confirmed an Accuracy of 81%, using the Quantized (Int8)
    trained model. However, it is sufficient for our purposes in this lab.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On the Deployment tab, we should select: `Seeed Grove Vision AI Module V2 (Himax
    WiseEye2)` and press `[Build]`. A ZIP file will be downloaded to our computer.'
  prefs: []
  type: TYPE_NORMAL
- en: The Zip file contains the `model_vela.tflite`, which is a TensorFlow Lite (TFLite)
    model optimized for neural processing units (NPUs) using the Vela compiler, a
    tool developed by Arm to adapt TFLite models for Ethos-U NPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file753.png)'
  prefs: []
  type: TYPE_IMG
- en: We can flash the model following the instructions in the `README.txt` or use
    the SenseCraft AI Studio. We will use the latter.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the model on the SenseCraft AI Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On SenseCraft AI Studio, go to the `Vision Workspace` tab, and connect the
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file754.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should see the last model that was uploaded to the device. Select the green
    button `[Upload Model]`. A pop-up window will ask for the **model name**, the
    **model file,** and to enter the class names (**objects**). We should use labels
    following alphabetical order: `0: background`, `1: periquito,` and `2: robot`,
    and then press `[Send]`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file755.png)'
  prefs: []
  type: TYPE_IMG
- en: After a few seconds, the model will be uploaded (“flashed”) to our device, and
    the camera image will appear in real-time on the **Preview** Sector. The Classification
    result will be displayed under the image preview. It is also possible to select
    the `Confidence Threshold` of your inference using the cursor on **Settings**.
  prefs: []
  type: TYPE_NORMAL
- en: On the **Device Logger**, we can view the Serial Monitor, where we can observe
    the latency, which is approximately 1 to 2 ms for pre-processing and 4 to 5 ms
    for inference, aligning with the estimates made in Edge Impulse Studio.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file756.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are other screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file757.png)'
  prefs: []
  type: TYPE_IMG
- en: The power consumption of this model is approximately 70 mA, equivalent to 0.4
    W.
  prefs: []
  type: TYPE_NORMAL
- en: Image Classification (non-official) Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several development boards can be used for embedded machine learning (tinyML),
    and the most common ones (so far) for Computer Vision applications (with low energy)
    are the **ESP32 CAM,** the **Seeed XIAO ESP32S3 Sense**, and the Arduino **Nicla
    Vision**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking advantage of this opportunity, a similarly trained model, MobilenetV2
    96x96, with an alpha of 0.1, was also deployed on the ESP-CAM, the XIAO, and a
    Raspberry Pi Zero W2\. Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file758.png)'
  prefs: []
  type: TYPE_IMG
- en: The Grove Vision AI V2 with an **ARM Ethos-U55** was approximately 14 times
    faster than devices with an ARM-M7, and more than 100 times faster than an Xtensa
    LX6 (ESP-CAM). Even when compared to a Raspberry Pi, with a much more powerful
    CPU, the U55 reduces latency by almost half. Additionally, the power consumption
    is lower than that of other devices (see the [full](https://www.hackster.io/limengdu0117/2024-mcu-ai-vision-boards-performance-comparison-998505)
    article here for power consumption comparison).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Postprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the model uploaded to the board and working correctly, classifying
    our images, let’s connect a Master Device to export the inference result to it
    and see the result completely offline (disconnected from the PC and, for example,
    powered by a battery).
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can use any microcontroller as a Master Controller, such as the
    XIAO, Arduino, or Raspberry Pi.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Getting the Video Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The image processing and model inference are processed locally in Grove Vision
    AI (V2), and we want the result to be output to the XIAO (Master Controller) via
    IIC. For that, we will use the **Arduino SSMA library**. This library’s primary
    purpose is to process Grove Vision AI’s data stream, which does not involve model
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: The Grove Vision AI (V2) communicates (Inference result) with the XIAO via the
    IIC; the device’s IIC address is 0x62\. Image information transfer is via the
    USB serial port.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Step 1:** Download the [Arduino SSMA](https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA/)
    library as a zip file from its GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file759.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: Install it in the Arduino IDE (`sketch > Include Library > Add
    .Zip Library`).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: Install the **ArduinoJSON** library.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file760.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 4**: Install the **Eigen** Library'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file761.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 3**: Now, connect the XIAO and Grove Vision AI (V2) via the socket (a
    row of pins) located at the back of the device.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file762.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**CAUTION**: Please note the direction of the connection, Grove Vision AI’s
    Type-C connector should be in the same direction as XIAO’s Type-C connector.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Step 5**: Connect the **XIAO USB-C** port to your computer'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file763.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 6**: In the Arduino IDE, select the Xiao board and the corresponding
    USB port.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we want to stream the video to a webpage, we will use the **XIAO ESP32S3**,
    which has wifi and enough memory to handle images. Select `XIAO_ESP32S3` and the
    appropriate USB Port:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file764.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, the PSRAM is disabled. Open the `Tools` menu and on PSRAM: `"OPI
    PSRAM"`select `OPI PSRAM`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file765.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 7**: Open the example in Arduino IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '`File` -> `Examples` -> `Seeed_Arduino_SSCMA` -> `camera_web_server`.'
  prefs: []
  type: TYPE_NORMAL
- en: And edit the `ssid` and `password` in the `camera_web_server.ino` sketch to
    match the Wi-Fi network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8**: Upload the sketch to the board and open the Serial Monitor. When
    connected to the Wi-Fi network, the board’s IP address will be displayed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file766.png)'
  prefs: []
  type: TYPE_IMG
- en: Open the address using a web browser. A Video App will be available. To see
    **only** the video stream from the Grove Vision AI V2, press `[Sample Only]` and
    `[Start Stream]`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file767.png)'
  prefs: []
  type: TYPE_IMG
- en: If you want to create an image dataset, you can use this app, saving frames
    of the video generated by the device. Pressing `[Save Frame]`, the image will
    be saved in the download area of our desktop.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file743.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening the App **without** selecting `[Sample Only]`, the inference result
    should appear on the video screen, but this does not happen for Image Classification.
    For Object Detection or Pose Estimation, the result is embedded with the video
    stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the model is a Person Detection using YoloV8:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file768.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting the Inference Result
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Go to `File` -> `Examples` -> `Seeed_Arduino_SSCMA` -> `inference_class`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the sketch to the board, and open the Serial Monitor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pointing the camera at one of our objects, we can see the inference result on
    the Serial Terminal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../media/file769.png)'
  prefs: []
  type: TYPE_IMG
- en: The inference running on the Arduino IDE had an average consumption of 160 mA
    or 800 mW and a peak of 330 mA 1.65 W when transmitting the image to the App.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Postprocessing with LED
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea behind our postprocessing is that whenever a specific image is detected
    (for example, the Periquito - Label:1), the User LED is turned on. If the Robot
    or a background is detected, the LED will be off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the below code and past it to your IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This sketch uses the Seeed_Arduino_SSCMA.h library to interface with the Grove
    Vision AI Module V2\. The AI module and the LED are initialized in the `setup()`
    function, and serial communication is started.
  prefs: []
  type: TYPE_NORMAL
- en: The `loop()` function repeatedly calls the `invoke()` method to perform inference
    using the built-in algorithms of the Grove Vision AI Module V2\. Upon a successful
    inference, the sketch prints out performance metrics to the serial monitor, including
    preprocessing, inference, and postprocessing times.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sketch processes and prints out detailed information about the results
    of the inference:'
  prefs: []
  type: TYPE_NORMAL
- en: (`AI.classes()[0]`) that identifies the class of image (`.target`) and its confidence
    score (`.score`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inference result (class) is stored in the integer variable `pred_index`,
    which will be used as an input to the function `turn_on_led()`. As a result, the
    LED will turn ON, depending on the classification result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the Periquito is detected (Label:1), the LED is ON:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file770.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the Robot is detected (Label:2) the LED is OFF (Same for Background (Label:0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file771.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, we can now power the Grove Vision AI V2 + Xiao ESP32S3 with an external
    battery, and the inference result will be displayed by the LED completely offline.
    The consumption is approximately 165 mA or 825 mW.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to send the result using Wifi, BLE, or other communication
    protocols available on the used Master Device.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Optional: Post-processing on external devices'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of course, one of the significant advantages of working with EdgeAI is that
    devices can run entirely disconnected from the cloud, allowing for seamless **interactions
    with the real world**. We did it in the last section, but using the internal Xiao
    LED. Now, we will connect external LEDs (which could be any actuator).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file772.png)'
  prefs: []
  type: TYPE_IMG
- en: The LEDS should be connected to the XIAO ground via a 220-ohm resistor.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../media/file773.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea is to modify the previous sketch to handle the three external LEDs.
  prefs: []
  type: TYPE_NORMAL
- en: '**GOAL**: Whenever the image of a **Periquito** is detected, the LED **Green**
    will be ON; if it is a **Robot**, the LED **Yellow** will be ON; if it is a **Background**,
    the **LED Red** will be ON.'
  prefs: []
  type: TYPE_NORMAL
- en: The image processing and model inference are processed locally in Grove Vision
    AI (V2), and we want the result to be output to the XIAO via IIC. For that, we
    will use the Arduino SSMA library again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the sketch to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We should connect the Grove Vision AI V2 with the XIAO using its I2C Grove connector.
    For the XIAO, we will use an [Expansion Board](https://wiki.seeedstudio.com/Seeeduino-XIAO-Expansion-Board/)
    for the facility (although it is possible to connect the I2C directly to the XIAO’s
    pins). We will power the boards using the USB-C connector, but a battery can also
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file774.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../media/file775.png)'
  prefs: []
  type: TYPE_IMG
- en: The power consumption reached a peak of 240 mA (Green LED), equivalent to 1.2
    W. Driving the Yellow and Red LEDs consumes 14 mA, equivalent to 0.7 W. Sending
    information to the terminal via serial has no impact on power consumption.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lab, we’ve explored the complete process of developing an image classification
    system using the Seeed Studio Grove Vision AI Module V2 powered by the Himax WiseEye2
    chip. We’ve walked through every stage of the machine learning workflow, from
    defining our project goals to deploying a working model with real-world interactions.
  prefs: []
  type: TYPE_NORMAL
- en: The Grove Vision AI V2 has demonstrated impressive performance, with inference
    times of just 4-5ms, dramatically outperforming other common tinyML platforms.
    Our benchmark comparison showed it to be approximately 14 times faster than ARM-M7
    devices and over 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared
    to a Raspberry Pi Zero W2, the Edge TPU architecture delivered nearly twice the
    speed while consuming less power.
  prefs: []
  type: TYPE_NORMAL
- en: Through this project, we’ve seen how transfer learning enables us to achieve
    good classification results with a relatively small dataset of custom images.
    The MobileNetV2 model with an alpha of 0.1 provided an excellent balance of accuracy
    and efficiency for our three-class problem, requiring only 146 KB of RAM and 187
    KB of Flash memory, well within the capabilities of the Grove Vision AI Module
    V2’s 2.4 MB internal SRAM.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored several deployment options, from viewing inference results
    through the SenseCraft AI Studio to creating a standalone system with visual feedback
    using LEDs. The ability to stream video to a web browser and process inference
    results locally demonstrates the versatility of edge AI systems for real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The power consumption of our final system remained impressively low, ranging
    from approximately 70mA (0.4W) for basic inference to 240mA (1.2W) when driving
    external components. This efficiency makes the Grove Vision AI Module V2 an excellent
    choice for battery-powered applications where power consumption is critical.
  prefs: []
  type: TYPE_NORMAL
- en: This lab has demonstrated that sophisticated computer vision tasks can now be
    performed entirely at the edge, without reliance on cloud services or powerful
    computers. With tools like Edge Impulse Studio and SenseCraft AI Studio, the development
    process has become accessible even to those without extensive machine learning
    expertise.
  prefs: []
  type: TYPE_NORMAL
- en: As edge AI technology continues to evolve, we can expect even more powerful
    capabilities from compact, energy-efficient devices like the Grove Vision AI Module
    V2, opening up new possibilities for smart sensors, IoT applications, and embedded
    intelligence in everyday objects.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Collecting Images with SenseCraft AI Studio](https://sensecraft.seeed.cc/ai/training).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edge Impulse Studio Project](https://studio.edgeimpulse.com/public/712491/live)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SenseCraft AI Studio - Vision Workplace (Deploy Models)](https://sensecraft.seeed.cc/ai/device/local/36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Other Himax examples](https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/Grove/Grove_Sensors/AI-powered/Grove-vision-ai-v2/Development/grove-vision-ai-v2-himax-sdk.md)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Arduino Sketches](https://github.com/Mjrovai/Seeed-Grove-Vision-AI-V2/tree/main/Arduino_Sketches)'
  prefs: []
  type: TYPE_NORMAL
