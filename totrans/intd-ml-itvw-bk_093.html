<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.1.2 Questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.1.2 Questions</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html">https://huyenchip.com/ml-interviews-book/contents/8.1.2-questions.html</a></blockquote>
                                
                                
<ol>
<li>[E] What are the basic assumptions to be made for linear regression?</li>
<li>[E] What happens if we don’t apply feature scaling to logistic regression?</li>
<li>[E] What are the algorithms you’d use when developing the prototype of a fraud detection model?</li>
<li>Feature selection.<ol>
<li>[E] Why do we use feature selection?</li>
<li>[M] What are some of the algorithms for feature selection? Pros and cons of each.</li>
</ol>
</li>
<li><p>k-means clustering.</p>
<ol>
<li>[E] How would you choose the value of k?</li>
<li>[E] If the labels are known, how would you evaluate the performance of your k-means clustering algorithm?</li>
<li>[M] How would you do it if the labels aren’t known?</li>
<li><p>[H] Given the following dataset, can you predict how K-means clustering works on it? Explain.</p>
<center>
<img src="../Images/8ca03d741d2431482b90658ce75bae5b.png" width="45%" alt="k-means clustering" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image28.png"/>
</center></li>
</ol>
</li>
<li>k-nearest neighbor classification.<ol>
<li>[E] How would you choose the value of k?</li>
<li>[E] What happens when you increase or decrease the value of k?</li>
<li>[M] How does the value of k impact the bias and variance?</li>
</ol>
</li>
<li><p>k-means and GMM are both powerful clustering algorithms.</p>
<ol>
<li>[M] Compare the two.</li>
<li><p>[M] When would you choose one over another?</p>
<p><strong>Hint</strong>: Here’s an example of how K-means and GMM algorithms perform on the artificial mouse dataset.</p>
<center>
<img src="../Images/defccceebaa2c20384ab102a0cb3cfbc.png" width="95%" alt="k-means clustering vs. gaussian mixture model" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image29.png"/><br/>
Image from <a href="https://www.mghassany.com/MLcourse/gaussian-mixture-models-em.html" target="_blank">Mohamad Ghassany’s course on Machine Learning</a>
</center></li>
</ol>
</li>
<li>Bagging and boosting are two popular ensembling methods. Random forest is a bagging example while XGBoost is a boosting example.<ol>
<li>[M] What are some of the fundamental differences between bagging and boosting algorithms?</li>
<li>[M] How are they used in deep learning?</li>
</ol>
</li>
<li>Given this directed graph.
 <center>
   <img src="../Images/7a269af62de8405ddc2ba8f041e3d690.png" width="30%" alt="Adjacency matrix" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image30.png"/><br/>
 </center><ol>
<li>[E] Construct its adjacency matrix.</li>
<li>[E] How would this matrix change if the graph is now undirected?</li>
<li>[M] What can you say about the adjacency matrices of two isomorphic graphs?</li>
</ol>
</li>
<li>Imagine we build a user-item collaborative filtering system to recommend to each user items similar to the items they’ve bought before.<ol>
<li>[M] You can build either a user-item matrix or an item-item matrix. What are the pros and cons of each approach?</li>
<li>[E] How would you handle a new user who hasn’t made any purchases in the past?</li>
</ol>
</li>
<li>[E] Is feature scaling necessary for kernel methods?</li>
<li><p>Naive Bayes classifier.</p>
<ol>
<li>[E] How is Naive Bayes classifier naive?</li>
<li><p>[M] Let’s try to construct a Naive Bayes classifier to classify whether a tweet has a positive or negative sentiment. We have four training samples:</p>
<table>
<tr>
 <td>
<strong>Tweet</strong>
 </td>
 <td><strong>Label</strong>
 </td>
</tr>
<tr>
 <td>This makes me so upset
 </td>
 <td>Negative
 </td>
</tr>
<tr>
 <td>This puppy makes me happy
 </td>
 <td>Positive
 </td>
</tr>
<tr>
 <td>Look at this happy hamster
 </td>
 <td>Positive
 </td>
</tr>
<tr>
 <td>No hamsters allowed in my house
 </td>
 <td>Negative
 </td>
</tr>
</table>

</li>
</ol>
<p>According to your classifier, what's sentiment of the sentence <code>The hamster is upset with the puppy</code>?</p>
</li>
<li><p>Two popular algorithms for winning Kaggle solutions are Light GBM and XGBoost. They are both gradient boosting algorithms.</p>
<ol>
<li>[E] What is gradient boosting?</li>
<li>[M] What problems is gradient boosting good for?</li>
</ol>
</li>
<li><p>SVM.</p>
<ol>
<li>[E] What’s linear separation? Why is it desirable when we use SVM?</li>
<li><p>[M] How well would vanilla SVM work on this dataset?</p>
<center>
 <img src="../Images/28ebf3793aeb63ffb66fbd2ea3b9166b.png" width="30%" alt="Adjacency matrix" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image31.png"/><br/>
</center>
</li>
<li><p>[M] How well would vanilla SVM work on this dataset?</p>
<center>
 <img src="../Images/7e618f095e074e364a89f262b97d5855.png" width="30%" alt="Adjacency matrix" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image32.png"/><br/>
</center>
</li>
<li><p>[M] How well would vanilla SVM work on this dataset?</p>
<center>
 <img src="../Images/22476012f7e6c58e01c258ca4feff488.png" width="27%" alt="Adjacency matrix" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image33.png"/><br/>
</center>

</li>
</ol>
</li>
</ol>
<p><em>This book was created by <a href="https://huyenchip.com" target="_blank">Chip Huyen</a> with the help of wonderful friends. For feedback, errata, and suggestions, the author can be reached <a href="https://huyenchip.com/communication/" target="_blank">here</a>. Copyright ©2021 Chip Huyen.</em></p>

                                
                                    
</body>
</html>