- en: ML Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统
- en: '*DALL·E 3 Prompt: Illustration in a rectangular format depicting the merger
    of embedded systems with Embedded AI. The left half of the image portrays traditional
    embedded systems, including microcontrollers and processors, detailed and precise.
    The right half showcases the world of artificial intelligence, with abstract representations
    of machine learning models, neurons, and data flow. The two halves are distinctly
    separated, emphasizing the individual significance of embedded tech and AI, but
    they come together in harmony at the center.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：以矩形格式展示嵌入式系统与嵌入式人工智能融合的插图。图像的左侧展示了传统的嵌入式系统，包括微控制器和处理器，细节丰富且精确。右侧展示了人工智能的世界，以抽象的形式展示了机器学习模型、神经元和数据流。两部分明显分开，强调嵌入式技术和人工智能的个体重要性，但在中心和谐地结合在一起。*'
- en: '![](../media/file18.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file18.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*How do the environments where machine learning operates shape the nature of
    these systems, and what drives their widespread deployment across computing platforms?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习运行的环境如何塑造这些系统的本质，是什么推动了它们在计算平台上的广泛应用？*'
- en: Machine learning systems must adapt to radically different computational environments,
    each imposing distinct constraints and opportunities. Cloud deployments leverage
    massive computational resources but face network latency, while mobile devices
    offer user proximity but operate under severe power limitations. Embedded systems
    minimize latency through local processing but constrain model complexity, and
    tiny devices enable widespread sensing while restricting memory to kilobytes.
    These deployment contexts fundamentally determine system architecture, algorithmic
    choices, and performance trade-offs. Understanding environment-specific requirements
    establishes the foundation for engineering decisions in machine learning systems.
    This knowledge enables engineers to select appropriate deployment paradigms and
    design architectures that balance performance, efficiency, and practicality across
    computing platforms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统必须适应截然不同的计算环境，每个环境都施加独特的约束和机遇。云部署利用庞大的计算资源，但面临网络延迟，而移动设备提供用户接近性，但受到严重的电源限制。嵌入式系统通过本地处理最小化延迟，但限制了模型复杂性，而小型设备使广泛感知成为可能，但将内存限制在千字节。这些部署环境从根本上决定了系统架构、算法选择和性能权衡。了解特定环境的要求为机器学习系统中的工程决策奠定了基础。这种知识使工程师能够选择适当的部署范式并设计在计算平台之间平衡性能、效率和实用性的架构。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Explain the physical constraints (speed of light, power wall, memory wall) that
    necessitate diverse ML deployment paradigms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释物理约束（光速、功率墙、内存墙）如何导致多样化的机器学习部署范式
- en: Distinguish Cloud, Edge, Mobile, and TinyML paradigms by resource profiles and
    optimal use cases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过资源配置和最佳用例区分云、边缘、移动和TinyML范式
- en: Analyze resource trade-offs (computational power, latency, privacy, energy efficiency)
    to determine appropriate deployment strategies for specific applications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析资源权衡（计算能力、延迟、隐私、能源效率），以确定特定应用的适当部署策略
- en: Apply the systematic deployment decision framework to evaluate privacy, latency,
    computational, and cost requirements for ML applications
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用系统部署决策框架，评估机器学习应用的隐私、延迟、计算和成本需求
- en: Design hybrid ML architectures integrating multiple deployment paradigms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计集成多个部署范式的混合机器学习架构
- en: Evaluate real-world ML systems to identify which deployment paradigms are being
    used and assess their effectiveness
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估现实世界的机器学习系统，以确定正在使用哪些部署范式并评估其有效性
- en: Critique common deployment fallacies and misconceptions to avoid poor architectural
    decisions in ML systems design
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批判常见的部署谬误和误解，以避免在机器学习系统设计中做出不良的架构决策
- en: Synthesize universal design principles to create ML systems that effectively
    balance performance, efficiency, and practicality across deployment contexts
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 综合通用设计原则，创建在部署环境中有效平衡性能、效率和实用性的机器学习系统
- en: Deployment Paradigm Framework
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署范式框架
- en: 'The preceding introduction established machine learning systems as comprising
    three fundamental components: data, algorithms, and computing infrastructure.
    While this triadic framework provides a theoretical foundation, the transition
    from conceptual understanding to practical implementation introduces a critical
    dimension that fundamentally governs system design: the deployment environment.
    This chapter analyzes how computational context shapes architectural decisions
    in machine learning systems, establishing the theoretical basis for deployment-driven
    design principles.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的介绍将机器学习系统确立为包含三个基本组成部分：数据、算法和计算基础设施。虽然这个三重框架提供了一个理论基础，但从概念理解到实际应用的过渡引入了一个关键维度，这一维度从根本上决定了系统设计：部署环境。本章分析了计算环境如何塑造机器学习系统中的架构决策，为以部署为导向的设计原则建立了理论基础。
- en: 'Contemporary machine learning applications demonstrate remarkable architectural
    diversity driven by deployment constraints. Consider the domain of computer vision[1](#fn1):
    a convolutional neural network trained for image classification manifests as distinctly
    different systems when deployed across environments. In cloud-based medical imaging,
    the system exploits virtually unlimited computational resources to implement ensemble
    methods[2](#fn2) and sophisticated preprocessing pipelines. When deployed on mobile
    devices for real-time object detection, the same fundamental algorithm undergoes
    architectural transformation to satisfy stringent latency requirements while preserving
    acceptable accuracy. Factory automation applications further constrain the design
    space, prioritizing power efficiency and deterministic response times over model
    complexity. These variations represent distinctly different architectural solutions
    to the same computational problem, shaped by environmental constraints rather
    than algorithmic considerations.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当代机器学习应用展示了由部署限制驱动的显著架构多样性。以计算机视觉领域[1](#fn1)为例：用于图像分类的卷积神经网络在部署到不同环境时表现为截然不同的系统。在基于云的医疗影像中，系统利用几乎无限的计算资源来实现集成方法[2](#fn2)和复杂的预处理流程。当部署在移动设备上进行实时目标检测时，相同的根本算法经过架构转换以满足严格的延迟要求，同时保持可接受的准确性。在工厂自动化应用中，设计空间进一步受到限制，优先考虑能源效率和确定性的响应时间，而不是模型复杂性。这些变化代表了针对同一计算问题的不同架构解决方案，这些解决方案由环境限制而不是算法考虑所塑造。
- en: 'This chapter presents a systematic taxonomy of machine learning deployment
    paradigms, analyzing four primary categories that span the computational spectrum
    from cloud data centers to microcontroller-based embedded systems. Each paradigm
    emerges from distinct operational requirements: computational resource availability,
    power consumption constraints, latency specifications, privacy requirements, and
    network connectivity assumptions. The theoretical framework developed here provides
    the analytical foundation for making informed architectural decisions in production
    machine learning systems.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提出了机器学习部署范例的系统分类法，分析了涵盖从云数据中心到基于微控制器的嵌入式系统的计算谱系的四个主要类别。每个范例都源于不同的操作要求：计算资源可用性、功耗限制、延迟规格、隐私要求以及网络连接假设。这里开发的框架为在生产机器学习系统中做出明智的架构决策提供了分析基础。
- en: 'Modern deployment strategies transcend traditional dichotomies between centralized
    and distributed processing. Contemporary applications increasingly implement hybrid
    architectures that strategically allocate computational tasks across multiple
    paradigms to optimize system-wide performance. Voice recognition systems exemplify
    this architectural sophistication: wake-word detection operates on ultra-low-power
    embedded processors to enable continuous monitoring, speech-to-text conversion
    utilizes mobile processors to maintain privacy and minimize latency, while semantic
    understanding leverages cloud infrastructure for complex natural language processing.
    This multi-paradigm approach reflects the engineering reality that optimal machine
    learning systems require architectural heterogeneity.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现代部署策略超越了传统集中式和分布式处理的二分法。当代应用越来越多地实施混合架构，战略性地在多个范式之间分配计算任务，以优化系统级性能。语音识别系统是这种架构复杂性的例证：唤醒词检测在超低功耗嵌入式处理器上运行，以实现持续监控，语音到文本转换使用移动处理器以保持隐私并最小化延迟，而语义理解则利用云基础设施进行复杂自然语言处理。这种多范式方法反映了工程现实，即最佳的机器学习系统需要架构异质性。
- en: The deployment paradigm space exhibits clear dimensional structure. Cloud machine
    learning maximizes computational capabilities while accepting network-induced
    latency constraints. Edge computing positions inference computation proximate
    to data sources when latency requirements preclude cloud-based processing. Mobile
    machine learning extends computational capabilities to personal devices where
    user proximity and offline operation represent critical requirements. Tiny machine
    learning enables distributed intelligence on severely resource-constrained devices
    where energy efficiency supersedes computational sophistication.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 部署范式空间展现出清晰的维度结构。云机器学习在接受网络引入的延迟约束的同时，最大化计算能力。边缘计算在延迟要求不允许基于云的处理时，将推理计算定位在数据源附近。移动机器学习将计算能力扩展到个人设备，其中用户接近性和离线操作是关键要求。微型机器学习使资源受限的设备能够实现分布式智能，其中能效优于计算复杂性。
- en: Through comprehensive analysis of these deployment paradigms, this chapter develops
    the systems engineering perspective necessary for designing machine learning architectures
    that effectively balance algorithmic capabilities with operational constraints.
    This systems-oriented approach provides essential methodological foundations for
    translating theoretical machine learning advances into production systems that
    demonstrate reliable performance at scale. The analysis culminates with paradigm
    integration strategies for hybrid architectures and identification of core design
    principles that govern all machine learning deployment contexts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对这些部署范式的全面分析，本章发展了设计有效平衡算法能力和操作约束的机器学习架构所需的系统工程视角。这种以系统为导向的方法为将理论机器学习进步转化为在生产系统中展示可靠性能的规模化系统提供了基本方法论基础。分析以混合架构的范式集成策略和识别所有机器学习部署环境中起主导作用的核心设计原则告终。
- en: '[Figure 2.1](ch008.xhtml#fig-cloud-edge-TinyML-comparison) illustrates how
    computational resources, latency requirements, and deployment constraints create
    this deployment spectrum. While [Chapter 7](ch013.xhtml#sec-ai-frameworks) explores
    the software tools that enable ML across these paradigms, and [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    examines the specialized hardware that powers them, this chapter focuses on the
    fundamental deployment trade-offs that govern system architecture decisions. The
    subsequent analysis addresses each paradigm systematically, building toward an
    understanding of how they integrate into modern ML systems.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.1](ch008.xhtml#fig-cloud-edge-TinyML-comparison)展示了计算资源、延迟要求和部署约束如何创建这种部署频谱。虽然[第7章](ch013.xhtml#sec-ai-frameworks)探讨了使机器学习跨越这些范式成为可能的软件工具，而[第11章](ch017.xhtml#sec-ai-acceleration)则考察了推动它们的专用硬件，但本章重点讨论了支配系统架构决策的基本部署权衡。后续分析系统地处理每个范式，旨在理解它们如何整合到现代机器学习系统中。'
- en: The Deployment Spectrum
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署频谱
- en: The deployment spectrum from cloud to embedded systems exists not by choice,
    but by necessity imposed by physical laws that govern computing systems. These
    immutable constraints create hard boundaries that no engineering advancement can
    overcome, forcing the evolution of specialized deployment paradigms optimized
    for different operational contexts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从云到嵌入式系统的部署范围并非出于选择，而是由物理定律强加于计算系统的必要性所决定的。这些不可改变的约束创造了无法被工程进步克服的硬边界，迫使专门部署范例的演变，这些范例针对不同的操作环境进行了优化。
- en: The **speed of light** establishes absolute minimum latencies that constrain
    real-time applications. Light traveling through optical fiber covers approximately
    200,000 kilometers per second, creating a theoretical minimum 40ms round-trip
    time between California and Virginia. Internet routing, DNS resolution, and processing
    overhead typically add another 60-460ms, resulting in total latencies of 100-500ms
    for cloud services. This physics-imposed delay makes cloud deployment impossible
    for safety-critical applications requiring sub-10ms response times, such as autonomous
    vehicle emergency braking or industrial robotics precision control.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**光速**确立了绝对的最小延迟，这限制了实时应用。光在光纤中传播的速度约为每秒200,000公里，在加利福尼亚和弗吉尼亚之间理论上最小的往返时间为40毫秒。互联网路由、DNS解析和处理开销通常会增加另外60-460毫秒，导致云服务的总延迟为100-500毫秒。这种由物理定律造成的延迟使得对于需要低于10毫秒响应时间的安全关键应用，如自动驾驶车辆的紧急制动或工业机器人的精确控制，云部署变得不可能。'
- en: The **power wall**, resulting from the breakdown of Dennard scaling around 2005,
    transformed computing economics. Transistor shrinking no longer reduces power
    density, meaning chips cannot be made arbitrarily fast without proportional increases
    in power consumption and heat generation. This constraint forces trade-offs between
    computational performance and energy efficiency, directly driving the need for
    specialized low-power architectures in mobile and embedded systems. Data centers
    now dedicate 30-40% of their power budget to cooling, while mobile devices must
    implement thermal throttling to prevent component damage.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**功耗墙**，由于2005年左右Dennard缩放效应的崩溃而形成，改变了计算的经济性。晶体管缩小不再降低功率密度，这意味着芯片不能在功率消耗和热量生成成比例增加的情况下任意加速。这种限制迫使在计算性能和能源效率之间进行权衡，直接推动了在移动和嵌入式系统中对专用低功耗架构的需求。数据中心现在将30-40%的电力预算用于冷却，而移动设备必须实施热管理以防止组件损坏。'
- en: The **memory wall** represents the growing gap between processor speed and memory
    bandwidth. While computational capacity scales linearly through additional processing
    units, memory bandwidth scales approximately as the square root of chip area due
    to physical routing constraints. This creates an increasingly severe bottleneck
    where processors become data-starved, spending more time waiting for memory transfers
    than performing calculations. Large machine learning models exacerbate this problem,
    requiring parameter datasets that exceed available memory bandwidth by orders
    of magnitude.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**内存墙**代表了处理器速度和内存带宽之间不断扩大的差距。虽然计算能力通过增加处理单元线性扩展，但由于物理布线限制，内存带宽大约与芯片面积的平方根成正比。这导致处理器成为数据饥渴，花费更多时间等待内存传输而不是进行计算，从而形成了越来越严重的瓶颈。大型机器学习模型加剧了这个问题，需要的数据集参数量远远超过可用的内存带宽。'
- en: '**Economics of scale** create significant cost-per-unit differences that justify
    different deployment approaches. A cloud server costing $50,000 can support thousands
    of users through virtualization, achieving per-user costs under $50\. However,
    applications requiring guaranteed response times or private data processing cannot
    share resources, eliminating this economic advantage. Meanwhile, embedded processors
    costing $5-50 enable deployment at billions of endpoints where individual cloud
    connections would be economically infeasible.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**规模经济**产生了显著的单位成本差异，这为不同的部署方法提供了合理性。一台成本为50,000美元的云服务器可以通过虚拟化支持成千上万的用户，实现每个用户的成本低于50美元。然而，需要保证响应时间或私人数据处理的应用程序不能共享资源，消除了这种经济优势。与此同时，成本在5-50美元之间的嵌入式处理器使得在数十亿个端点部署成为可能，而单个云连接在经济上是不切实际的。'
- en: These physical constraints are not temporary engineering challenges but permanent
    limitations that shape the computational landscape. Understanding these boundaries
    explains why the deployment spectrum exists and provides the theoretical foundation
    for making informed architectural decisions in machine learning systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些物理约束不是暂时的工程挑战，而是塑造计算景观的永久性限制。理解这些边界解释了为什么存在部署范围，并为在机器学习系统中做出明智的架构决策提供了理论基础。
- en: '![](../media/file19.svg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file19.svg)'
- en: 'Figure 2.1: **Distributed Intelligence Spectrum**: Machine learning system
    design involves trade-offs between computational resources, latency, and connectivity,
    resulting in a spectrum of deployment options ranging from centralized cloud infrastructure
    to resource-constrained edge and TinyML devices. This figure maps these options,
    highlighting how each approach balances processing location with device capability
    and network dependence. Source: ([ABI Research 2024](ch058.xhtml#ref-abiresearch2024tinyml)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：**分布式智能频谱**：机器学习系统设计涉及计算资源、延迟和连接性的权衡，从而产生从集中式云基础设施到资源受限的边缘和微型机器学习设备的部署选项频谱。此图映射了这些选项，突出显示每种方法如何平衡处理位置与设备能力和网络依赖性。来源：([ABI
    Research 2024](ch058.xhtml#ref-abiresearch2024tinyml))。
- en: Deployment Paradigm Foundations
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署范式基础
- en: The deployment spectrum illustrated in [Figure 2.1](ch008.xhtml#fig-cloud-edge-TinyML-comparison)
    exists not through design preference, but from necessity driven by immutable physical
    and hardware constraints. Understanding these limitations reveals why ML systems
    cannot adopt uniform approaches and must instead span the complete deployment
    spectrum from cloud to embedded devices.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.1](ch008.xhtml#fig-cloud-edge-TinyML-comparison) 中所示的部署范围并非出于设计偏好，而是由不可变的物理和硬件约束驱动的必要性。理解这些限制揭示了为什么机器学习系统不能采用统一的方法，而必须跨越从云到嵌入式设备的完整部署范围。'
- en: '[Chapter 1](ch007.xhtml#sec-introduction) established the three foundational
    components of ML systems (data, algorithms, and infrastructure) as a unified framework
    that these deployment paradigms now optimize differently based on physical constraints.
    Cloud ML prioritizes algorithmic complexity through abundant infrastructure, while
    Mobile ML emphasizes data locality with constrained infrastructure, and Tiny ML
    maximizes algorithmic efficiency under extreme infrastructure limitations.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](ch007.xhtml#sec-introduction) 建立了机器学习系统（数据、算法和基础设施）的三个基础组件作为一个统一的框架，这些部署范式现在基于物理约束不同地优化。云机器学习通过丰富的基础设施优先考虑算法复杂性，而移动机器学习强调数据局部性，在受限的基础设施下，而微型机器学习在极端基础设施限制下最大化算法效率。'
- en: The most critical bottleneck in modern computing stems from memory bandwidth
    scaling differently than computational capacity. While compute power scales linearly
    through additional processing units, memory bandwidth scales approximately as
    the square root of chip area due to physical routing constraints. This creates
    a progressively worsening bottleneck where processors become data-starved. In
    practice, this manifests as ML models spending more time awaiting memory transfers
    than performing calculations, particularly problematic for large models[3](#fn3)
    that require more data than can be efficiently transferred.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现代计算中最关键的瓶颈源于内存带宽扩展速度与计算能力不同。虽然计算能力通过额外的处理单元线性扩展，但由于物理路由约束，内存带宽大约以芯片面积的平方根扩展。这创造了一个逐渐恶化的瓶颈，处理器变得数据匮乏。在实践中，这表现为机器学习模型花费更多时间等待内存传输而不是进行计算，这对需要比能高效传输的数据更多的[3](#fn3)
    大型模型尤其成问题。
- en: Compounding these memory challenges, the breakdown of Dennard scaling[4](#fn4)
    transformed computing constraints around 2005, when transistor shrinking stopped
    reducing power density. Power dissipation per unit area now remains constant or
    increases with each technology generation, creating hard limits on computational
    density. For mobile devices, this translates to thermal throttling that reduces
    performance when sustained computation generates excessive heat. Data centers
    face similar constraints at scale, requiring extensive cooling infrastructure
    that can consume 30-40% of total power budget. These power density limits directly
    drive the need for specialized low-power architectures in mobile and embedded
    contexts, and explain why edge deployment becomes necessary when power budgets
    are constrained.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些内存挑战的基础上，Dennard缩放[4](#fn4)的崩溃在2005年左右改变了计算约束，当时晶体管缩小停止了降低功率密度的过程。现在，每单位面积的功耗保持不变或随着每一代技术的进步而增加，为计算密度设定了硬性限制。对于移动设备来说，这意味着当持续计算产生过多热量时，性能会降低，即热管理。数据中心在规模上也面临类似的约束，需要大量的冷却基础设施，这可能消耗总电力预算的30-40%。这些功率密度限制直接推动了在移动和嵌入式环境中对专用低功耗架构的需求，并解释了为什么在电力预算受限时边缘部署变得必要。
- en: 'Beyond power considerations, physical limits impose minimum latencies that
    no engineering optimization can overcome. The speed of light establishes an inherent
    80ms round-trip time between California and Virginia, while internet routing,
    DNS resolution, and processing overhead typically contribute another 20-420ms.
    This 100-500ms total latency renders real-time applications infeasible with pure
    cloud deployment. Network bandwidth faces physical constraints: fiber optic cables
    have theoretical limits, and wireless communication remains bounded by spectrum
    availability and signal propagation physics. These communication constraints create
    hard boundaries that necessitate local processing for latency-sensitive applications
    and drive edge deployment decisions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了功率考虑之外，物理限制设定了无法通过工程优化克服的最小延迟。光速在加利福尼亚和弗吉尼亚之间建立了固有的80ms往返时间，而互联网路由、DNS解析和处理开销通常又贡献了额外的20-420ms。这100-500ms的总延迟使得纯云部署中的实时应用变得不可行。网络带宽面临物理限制：光纤电缆有理论上的限制，而无线通信仍然受限于频谱可用性和信号传播物理。这些通信约束创造了硬边界，需要为延迟敏感型应用进行本地处理，并推动边缘部署决策。
- en: 'Heat dissipation emerges as an additional limiting factor as computational
    density increases. Mobile devices must throttle performance to prevent component
    damage and maintain user comfort, while data centers require extensive cooling
    systems that limit placement options and increase operational costs. Thermal constraints
    create cascading effects: elevated temperatures reduce semiconductor reliability,
    increase error rates, and accelerate component aging. These thermal realities
    necessitate trade-offs between computational performance and sustainable operation,
    driving specialized cooling solutions in cloud environments and ultra-low-power
    designs in embedded systems.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算密度的增加，散热成为另一个限制因素。移动设备必须降低性能以防止组件损坏并保持用户舒适，而数据中心需要广泛的冷却系统，这限制了放置选项并增加了运营成本。热约束会产生级联效应：高温降低半导体可靠性，增加错误率，并加速组件老化。这些热现实需要在计算性能和可持续运行之间进行权衡，推动云环境中的专用冷却解决方案和嵌入式系统中的超低功耗设计。
- en: These fundamental constraints drove the evolution of the four distinct deployment
    paradigms outlined in this overview ([Section 2.2](ch008.xhtml#sec-ml-systems-deployment-spectrum-38d0)).
    Understanding these core constraints proves essential for selecting appropriate
    deployment paradigms and establishing realistic performance expectations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本约束推动了本概述中概述的四种不同部署范例的演变（[第2.2节](ch008.xhtml#sec-ml-systems-deployment-spectrum-38d0)）。理解这些核心约束对于选择适当的部署范例和建立现实性能预期至关重要。
- en: These theoretical constraints manifest in concrete hardware differences across
    the deployment spectrum. To understand the practical implications of these physical
    limitations, [Table 2.1](ch008.xhtml#tbl-representative-systems) provides representative
    hardware platforms for each category. These examples demonstrate the range of
    computational resources, power requirements, and cost considerations[5](#fn5)
    across the ML systems spectrum, illustrating the practical implications of each
    deployment approach.[6](#fn6)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些理论限制在部署谱系中表现为具体的硬件差异。为了理解这些物理限制的实际影响，[表2.1](ch008.xhtml#tbl-representative-systems)提供了每个类别的代表性硬件平台。这些示例展示了ML系统谱系中的计算资源、功耗和成本考虑的范围[5](#fn5)，说明了每种部署方法的实际影响。[6](#fn6)
- en: These quantitative thresholds reflect essential relationships between computational
    requirements, energy consumption, and deployment feasibility. These scaling relationships
    determine when distributed cloud deployment becomes advantageous relative to edge
    or mobile alternatives. Understanding these quantitative trade-offs enables informed
    deployment decisions across the spectrum of ML systems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定量阈值反映了计算需求、能耗和部署可行性之间的基本关系。这些缩放关系决定了分布式云部署相对于边缘或移动替代方案何时具有优势。理解这些定量权衡，能够使从业者在对ML系统谱系进行部署决策时做出明智的选择。
- en: '[Figure 2.2](ch008.xhtml#fig-vMLsizes) illustrates the differences between
    Cloud ML, Edge ML, Mobile ML, and Tiny ML in terms of hardware specifications,
    latency characteristics, connectivity requirements, power consumption, and model
    complexity constraints. As systems transition from Cloud to Edge to Tiny ML, available
    resources decrease dramatically, presenting significant challenges for machine
    learning model deployment. This resource disparity becomes particularly evident
    when deploying ML models on microcontrollers, the primary hardware platform for
    Tiny ML. These devices possess severely constrained memory and storage capacities
    that prove insufficient for conventional complex ML models.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.2](ch008.xhtml#fig-vMLsizes)展示了云ML、边缘ML、移动ML和Tiny ML在硬件规格、延迟特性、连接要求、功耗和模型复杂度约束方面的差异。随着系统从云过渡到边缘再到Tiny
    ML，可用资源急剧减少，这对机器学习模型的部署提出了重大挑战。这种资源差异在将ML模型部署到微控制器（Tiny ML的主要硬件平台）时尤为明显。这些设备具有严重受限的内存和存储容量，对于传统的复杂ML模型来说是不够的。'
- en: 'Table 2.1: **Hardware Spectrum**: Machine learning system design necessitates
    trade-offs between computational resources, power consumption, and cost, as exemplified
    by the diverse hardware platforms suitable for cloud, edge, mobile, and TinyML
    deployments. This table quantifies those trade-offs, revealing how device capabilities,
    from specialized ML accelerators in cloud data centers to low-power microcontrollers
    in embedded systems, shape the types of models and tasks each platform can effectively
    support. The quantitative thresholds provide specific decision criteria to help
    practitioners determine the most appropriate deployment paradigm for their applications.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：**硬件谱系**：机器学习系统设计需要在计算资源、功耗和成本之间进行权衡，正如适用于云、边缘、移动和TinyML部署的多样化硬件平台所例证。此表量化了这些权衡，揭示了从云数据中心中的专用ML加速器到嵌入式系统中的低功耗微控制器，设备能力如何塑造每个平台可以有效地支持的模式和任务。这些定量阈值提供了具体的决策标准，以帮助从业者确定他们应用的最合适的部署范式。
- en: '| **Category** | **Example Device** | **Processor** | **Memory** | **Storage**
    | **Power** | **Price Range** | **Example Models/Tasks** | **Quantitative Thresholds**
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **示例设备** | **处理器** | **内存** | **存储** | **功耗** | **价格范围** | **示例模型/任务**
    | **定量阈值** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Cloud ML** | Google TPU v4 Pod | 4,096x TPU v4 chips (1.1 exaflops peak)
    | 131 TB HBM2 | Cloud-scale (PB-scale) | ~3 MW | Cloud service (rental only) |
    Large language models, massive-scale training | >1000 TFLOPS compute, real-time
    video processing, >100GB/s memory bandwidth, PUE 1.1-1.3, 100-500ms latency |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **云ML** | Google TPU v4 Pod | 4,096x TPU v4芯片（峰值1.1 exaflops） | 131 TB HBM2
    | 云规模（PB规模） | ~3 MW | 云服务（仅限租赁） | 大型语言模型、大规模训练 | >1000 TFLOPS计算、实时视频处理、>100GB/s内存带宽、PUE
    1.1-1.3，100-500ms延迟 |'
- en: '| **Edge ML** | NVIDIA DGX Spark | GB10 Grace Blackwell Superchip (20-core
    Arm, 1 PFLOPS AI) | 128 GB LPDDR5x | 4 TB NVMe | ~200 W | ~$5,000 | Model fine-tuning,
    on-premise inference, prototype development | ~1 PFLOPS AI compute, >270 GB/s
    memory bandwidth, desktop deployment, local processing |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **Edge ML** | NVIDIA DGX Spark | GB10 Grace Blackwell Superchip (20-core
    Arm, 1 PFLOPS AI) | 128 GB LPDDR5x | 4 TB NVMe | ~200 W | ~$5,000 | 模型微调，本地推理，原型开发
    | ~1 PFLOPS AI计算，>270 GB/s 内存带宽，桌面部署，本地处理 |'
- en: '| **Mobile ML** | iPhone 15 Pro | A17 Pro (6-core CPU, 6-core GPU) | 8 GB RAM
    | 128 GB-1 TB | 3-5 W | $999+ | Face ID, computational photography, voice recognition
    | 1-10 TOPS compute, <2W sustained power, <50ms UI response |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **Mobile ML** | iPhone 15 Pro | A17 Pro (6-core CPU, 6-core GPU) | 8 GB RAM
    | 128 GB-1 TB | 3-5 W | $999+ | 面容识别，计算摄影，语音识别 | 1-10 TOPS 计算，<2W 持续功率，<50ms UI响应
    |'
- en: '| **Tiny ML** | ESP32-CAM | Dual-core @ 240MHz | 520 KB RAM | 4 MB Flash |
    0.05-0.25 W | $10 | Image classification, motion detection | <1 TOPS compute,
    <1mW power, microsecond response times |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **Tiny ML** | ESP32-CAM | 双核 @ 240MHz | 520 KB RAM | 4 MB Flash | 0.05-0.25
    W | $10 | 图像分类，运动检测 | <1 TOPS 计算，<1mW 功耗，微秒级响应时间 |'
- en: '![](../media/file20.svg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file20.svg)'
- en: 'Figure 2.2: **Device Memory Constraints**: AI model deployment spans a wide
    range of devices with drastically different memory capacities, from cloud servers
    with 16 GB to microcontroller-based systems with only 320 kb. This progression
    necessitates specialized optimization techniques and efficient architectures to
    enable on-device intelligence with limited resources. Source: ([Ji Lin, Zhu, et
    al. 2023](ch058.xhtml#ref-lin2023tiny)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：**设备内存限制**：AI模型部署跨越了具有截然不同内存容量的广泛设备，从16 GB的云服务器到只有320 kb的基于微控制器的系统。这种进步需要专门的优化技术和高效的架构，以在有限的资源下实现设备上的智能。来源：([Ji
    Lin, Zhu, et al. 2023](ch058.xhtml#ref-lin2023tiny))。
- en: 'Cloud ML: Maximizing Computational Power'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云ML：最大化计算能力
- en: Having established the constraints and evolutionary progression that shape ML
    deployment paradigms, this analysis addresses each paradigm systematically, beginning
    with Cloud ML, the foundation from which other paradigms emerged. This approach
    maximizes computational resources while accepting latency constraints, providing
    the optimal choice when computational power matters more than response time. Cloud
    deployments prove ideal for complex training tasks and inference workloads that
    can tolerate network delays.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了塑造ML部署范例的约束和进化进程之后，本分析系统地处理每个范例，从云ML开始，这是其他范例出现的基石。这种方法在接受延迟约束的同时最大化计算资源，当计算能力比响应时间更重要时提供最佳选择。云部署对于可以容忍网络延迟的复杂训练任务和推理工作负载来说证明是理想的。
- en: 'Cloud Machine Learning leverages the scalability and power of centralized infrastructures[7](#fn7)
    to handle computationally intensive tasks: large-scale data processing, collaborative
    model development, and advanced analytics. Cloud data centers utilize distributed
    architectures and specialized resources to train complex models and support diverse
    applications, from recommendation systems to natural language processing[8](#fn8).
    The subsequent analysis addresses the deployment characteristics that make cloud
    ML systems effective for large-scale applications.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 云机器学习利用集中基础设施的扩展性和能力[7](#fn7) 来处理计算密集型任务：大规模数据处理、协作模型开发以及高级分析。云数据中心利用分布式架构和专用资源来训练复杂模型并支持多样化的应用，从推荐系统到自然语言处理[8](#fn8)。后续分析将讨论使云ML系统适用于大规模应用的部署特性。
- en: '***Cloud Machine Learning (Cloud ML)*** is the deployment of machine learning
    models on *centralized data center infrastructure*, offering *massive computational
    capacity* and *scalability* for training and serving complex models at the cost
    of *network latency* and *connectivity dependence*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***云机器学习（Cloud ML）*** 是在 *集中数据中心基础设施* 上部署机器学习模型，以 *巨大的计算能力* 和 *可扩展性* 来降低训练和部署复杂模型的成本，但代价是
    *网络延迟* 和 *连接依赖性*。'
- en: '[Figure 2.3](ch008.xhtml#fig-cloud-ml) provides an overview of Cloud ML’s capabilities,
    which we will discuss in greater detail throughout this section.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.3](ch008.xhtml#fig-cloud-ml) 提供了云ML功能的概述，我们将在本节中详细讨论。'
- en: '![](../media/file21.svg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file21.svg)'
- en: 'Figure 2.3: **Cloud ML Capabilities**: Cloud machine learning systems address
    challenges related to scale, complexity, and resource management through centralized
    computing infrastructure and specialized hardware. This figure outlines key considerations
    for deploying models in the cloud, including the need for reliable infrastructure
    and efficient resource allocation to handle large datasets and complex computations.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：**云机器学习能力**：云机器学习系统通过集中式计算基础设施和专用硬件解决与规模、复杂性和资源管理相关的挑战。此图概述了在云中部署模型的关键考虑因素，包括需要可靠的基础设施和高效的资源分配，以处理大型数据集和复杂的计算。
- en: Cloud Infrastructure and Scale
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云基础设施和规模
- en: 'To understand cloud ML’s position in the deployment spectrum, we must first
    consider its defining characteristics. Cloud ML’s primary distinguishing feature
    is its centralized infrastructure operating at unprecedented scale. [Figure 2.4](ch008.xhtml#fig-cloudml-example)
    illustrates this concept with an example from Google’s Cloud TPU[9](#fn9) data
    center. As detailed in [Table 2.1](ch008.xhtml#tbl-representative-systems), cloud
    systems like Google’s TPU v4 Pod represent a 100-1000x computational advantage
    over mobile devices, with >1000 TFLOPS compute power and megawatt-scale power
    consumption. Cloud service providers offer virtual platforms with >100GB/s memory
    bandwidth housed in globally distributed data centers[10](#fn10). These centralized
    facilities enable computational workloads impossible on resource-constrained devices.
    However, this centralization introduces critical trade-offs: network round-trip
    latency of 100-500ms eliminates real-time applications, while operational costs
    scale linearly with usage.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解云机器学习在部署谱系中的位置，我们首先必须考虑其定义特征。云机器学习的主要区别特征是其集中式基础设施，在前所未有的规模上运行。[图2.4](ch008.xhtml#fig-cloudml-example)通过谷歌云TPU[9](#fn9)数据中心的一个例子来说明这一概念。正如[表2.1](ch008.xhtml#tbl-representative-systems)中详细说明的那样，像谷歌的TPU
    v4 Pod这样的云系统，与移动设备相比，具有100-1000倍的计算优势，拥有>1000 TFLOPS的计算能力和兆瓦级功耗。云服务提供商在全球分布的数据中心中提供具有>100GB/s内存带宽的虚拟平台[10](#fn10)。这些集中式设施使得在资源受限的设备上无法完成的计算工作成为可能。然而，这种集中化引入了关键的权衡：100-500ms的网络往返延迟消除了实时应用，而运营成本与使用量成线性增长。
- en: '![](../media/file22.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file22.jpg)'
- en: 'Figure 2.4: **Cloud Data Center Scale**: Large-scale machine learning systems
    require centralized infrastructure with massive computational resources and storage
    capacity. Google’s cloud TPU data center provides this need, housing specialized
    AI accelerator hardware to efficiently manage the demands of training and deploying
    complex models. Source: ([DeepMind 2024](ch058.xhtml#ref-google2024gemini)).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：**云数据中心规模**：大规模机器学习系统需要具有巨大计算资源和存储容量的集中式基础设施。谷歌的云TPU数据中心提供了这一需求，拥有专门的人工智能加速器硬件，以高效地管理训练和部署复杂模型的需求。来源：([DeepMind
    2024](ch058.xhtml#ref-google2024gemini)))。
- en: Cloud ML excels in processing massive data volumes through parallelized architectures.
    Through techniques detailed in [Chapter 10](ch016.xhtml#sec-model-optimizations),
    distributed training across hundreds of GPUs enables processing that would require
    months on single devices, while [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    covers the memory bandwidth analysis underlying this performance. This enables
    training on datasets requiring hundreds of terabytes of storage and petaflops
    of computation, resources impossible on constrained devices.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 云机器学习在处理大量数据方面表现出色，通过并行化架构。通过[第10章](ch016.xhtml#sec-model-optimizations)中详细说明的技术，跨数百个GPU的分布式训练使得在单个设备上需要数月的处理可以在短时间内完成，而[第11章](ch017.xhtml#sec-ai-acceleration)涵盖了支撑这一性能的内存带宽分析。这使得在需要数百PB存储和PFLOPS计算的数据库上进行训练成为可能，这些资源在受限设备上是不可能实现的。
- en: The centralized infrastructure creates exceptional deployment flexibility through
    cloud APIs[11](#fn11), making trained models accessible worldwide across mobile,
    web, and IoT platforms. Seamless collaboration enables multiple teams to access
    projects simultaneously with integrated version control. Pay-as-you-go pricing
    models[12](#fn12) eliminate upfront capital expenditure while resources scale
    elastically with demand.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 集中式基础设施通过云API[11](#fn11)创造了卓越的部署灵活性，使得训练好的模型可以通过移动、Web和物联网平台在全球范围内访问。无缝协作使得多个团队可以同时访问项目，并使用集成版本控制。按需付费的定价模式[12](#fn12)消除了前期资本支出，同时资源可以根据需求弹性扩展。
- en: A common misconception assumes that Cloud ML’s vast computational resources
    make it universally superior to alternative deployment approaches. Cloud infrastructure
    offers exceptional computational power and storage, yet this advantage doesn’t
    automatically translate to optimal solutions for all applications. Cloud deployment
    introduces significant trade-offs including network latency (often 100-500ms round
    trip), privacy concerns when transmitting sensitive data, ongoing operational
    costs that scale with usage, and complete dependence on network connectivity.
    Edge and embedded deployments excel in scenarios requiring real-time response
    (autonomous vehicles need sub-10ms decision making), strict data privacy (medical
    devices processing patient data), predictable costs (one-time hardware investment
    versus recurring cloud fees), or operation in disconnected environments (industrial
    equipment in remote locations). The optimal deployment paradigm depends on specific
    application requirements rather than raw computational capability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误解是认为云机器学习庞大的计算资源使其在所有替代部署方法中普遍优于其他方法。云基础设施提供了卓越的计算能力和存储，但这种优势并不自动转化为所有应用的理想解决方案。云部署引入了显著的权衡，包括网络延迟（通常是100-500毫秒的往返延迟）、传输敏感数据时的隐私问题、随着使用量增加的持续运营成本，以及对网络连接的完全依赖。边缘和嵌入式部署在需要实时响应的场景中表现卓越（自动驾驶汽车需要低于10毫秒的决策制定），在需要严格数据隐私（处理患者数据的医疗设备）、可预测的成本（一次性硬件投资与持续云费用）、或在断开连接的环境中运行（偏远地区的工业设备）方面表现出色。最佳的部署范式取决于具体的应用需求，而不是原始的计算能力。
- en: Cloud ML Trade-offs and Constraints
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云机器学习权衡与限制
- en: Cloud ML’s substantial advantages carry inherent trade-offs that shape deployment
    decisions. Latency represents the most significant physical constraint. Network
    round-trip delays typically range from 100-500ms, making cloud processing unsuitable
    for real-time applications requiring sub-10ms responses, such as autonomous vehicles
    and industrial control systems. Beyond basic timing constraints, unpredictable
    response times complicate performance monitoring and debugging across geographically
    distributed infrastructure.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 云机器学习（Cloud ML）的显著优势伴随着固有的权衡，这些权衡影响着部署决策。延迟是最大的物理限制。网络往返延迟通常在100-500毫秒之间，这使得云处理不适合需要低于10毫秒响应时间的实时应用，例如自动驾驶汽车和工业控制系统。除了基本的定时限制外，不可预测的响应时间使得在地理上分布的基础设施中进行性能监控和调试变得复杂。
- en: Privacy and security present significant challenges when adopting cloud deployment.
    Transmitting sensitive data to remote data centers creates potential vulnerabilities
    and complicates regulatory compliance. Organizations handling data subject to
    regulations like GDPR[13](#fn13) or HIPAA[14](#fn14) must implement comprehensive
    security measures including encryption, strict access controls, and continuous
    monitoring to meet stringent data handling requirements.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用云部署时，隐私和安全问题带来了重大挑战。将敏感数据传输到远程数据中心可能产生潜在的安全漏洞并使合规性复杂化。处理受GDPR[13](#fn13)或HIPAA[14](#fn14)等法规约束的数据的组织必须实施包括加密、严格的访问控制和持续监控在内的全面安全措施，以满足严格的数据处理要求。
- en: 'Cost management introduces operational complexity as expenses scale with usage.
    Consider a production system serving 1 million daily inferences at $0.001 each:
    annual costs reach $365,000, compared to $100,000 for equivalent edge hardware
    purchased once. The break-even point occurs around 100,000-1,000,000 requests,
    directly influencing deployment strategy. Unpredictable usage spikes further complicate
    budgeting, requiring sophisticated monitoring and cost governance frameworks.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 成本管理随着支出的增加而引入了运营复杂性。考虑一个每天处理100万次推理的生产系统，每次推理的费用为0.001美元：年费用达到365,000美元，而一次性购买等效边缘硬件的费用为100,000美元。盈亏平衡点出现在大约100,000-1,000,000次请求时，这直接影响部署策略。不可预测的使用高峰进一步复杂化了预算编制，需要复杂的监控和成本治理框架。
- en: Network dependency creates another critical constraint. Any connectivity disruption
    directly impacts system availability, proving particularly problematic where network
    access is limited or unreliable. Vendor lock-in further complicates the landscape,
    as dependencies on specific tools and APIs create portability and interoperability
    challenges when transitioning between providers. Organizations must carefully
    balance these constraints against cloud benefits based on application requirements
    and risk tolerance, with resilience strategies detailed in [Chapter 16](ch022.xhtml#sec-robust-ai).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 网络依赖性创造了另一个关键约束。任何连接中断都会直接影响系统可用性，在网络访问受限或不稳定的地方尤其成问题。供应商锁定进一步复杂化了这一局面，因为对特定工具和API的依赖在切换提供商时创造了可移植性和互操作性挑战。组织必须根据应用需求和风险承受能力，仔细权衡这些约束与云的好处，并在[第16章](ch022.xhtml#sec-robust-ai)中详细阐述弹性策略。
- en: Large-Scale Training and Inference
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模训练和推理
- en: Cloud ML’s computational advantages manifest most visibly in consumer-facing
    applications requiring massive scale. Virtual assistants like Siri and Alexa exemplify
    cloud ML’s ability to handle computationally intensive natural language processing,
    leveraging extensive computational resources to process vast numbers of concurrent
    interactions while continuously improving through exposure to diverse linguistic
    patterns and use cases.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 云机器学习的计算优势在面向消费者的应用中表现得最为明显，这些应用需要巨大的规模。Siri和Alexa等虚拟助手展示了云机器学习处理计算密集型自然语言处理的能力，利用广泛的计算资源处理大量并发交互，并通过接触不同的语言模式和用例不断改进。
- en: Recommendation engines deployed by Netflix and Amazon demonstrate another compelling
    application of cloud resources. These systems process massive datasets using collaborative
    filtering[15](#fn15) and other machine learning techniques to uncover patterns
    in user preferences and behavior. Cloud computational resources enable continuous
    updates and refinements as user data grows, with Netflix processing over 100 billion
    data points daily to deliver personalized content suggestions that directly enhance
    user engagement.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix和Amazon部署的推荐引擎展示了云资源另一个令人信服的应用。这些系统使用协同过滤[15](#fn15)和其他机器学习技术处理大量数据集，以揭示用户偏好和行为中的模式。随着用户数据的增长，云计算资源能够实现持续更新和优化，Netflix每天处理超过1000亿个数据点，以提供个性化的内容推荐，这些推荐直接增强了用户参与度。
- en: Financial institutions have revolutionized fraud detection through cloud ML
    capabilities. By analyzing vast amounts of transactional data in real-time, ML
    algorithms trained on historical fraud patterns can detect anomalies and suspicious
    behavior across millions of accounts, enabling proactive fraud prevention that
    minimizes financial losses.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 金融机构通过云机器学习能力实现了欺诈检测的革命。通过实时分析大量交易数据，基于历史欺诈模式训练的机器学习算法可以检测数百万账户中的异常和可疑行为，从而实现主动的欺诈预防，最大限度地减少财务损失。
- en: These applications demonstrate how cloud ML’s computational advantages translate
    into transformative capabilities for large-scale, complex processing tasks. Beyond
    these flagship applications, cloud ML permeates everyday online experiences through
    personalized advertisements on social media, predictive text in email services,
    product recommendations in e-commerce, enhanced search results, and security anomaly
    detection systems that continuously monitor for cyber threats at scale.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用展示了云机器学习的计算优势如何转化为大规模、复杂处理任务的变革性能力。除了这些旗舰应用之外，云机器学习通过社交媒体上的个性化广告、电子邮件服务中的预测文本、电子商务中的产品推荐、增强的搜索结果以及连续监控大规模网络威胁的安全异常检测系统，渗透到日常在线体验中。
- en: 'Edge ML: Reducing Latency and Privacy Risk'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Edge ML：降低延迟和隐私风险
- en: Cloud ML’s computational advantages come with inherent trade-offs that limit
    its applicability for many real-world scenarios. The 100-500ms latency and privacy
    concerns that we examined create fundamental barriers for applications requiring
    immediate response or local data processing. Edge ML emerged as a direct response
    to these specific limitations, moving computation closer to data sources and trading
    unlimited computational resources for sub-100ms latency and local data sovereignty.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 云机器学习的计算优势伴随着固有的权衡，这限制了其在许多现实场景中的应用。我们考察的100-500毫秒延迟和隐私问题为需要即时响应或本地数据处理的应用创造了基本障碍。Edge
    ML作为对这些特定限制的直接回应出现，将计算更靠近数据源，以100毫秒以下的延迟和本地数据主权为代价，换取了无限的计算资源。
- en: This paradigm shift becomes essential for applications where cloud’s 100-500ms
    round-trip delays prove unacceptable. Autonomous systems requiring split-second
    decisions and industrial IoT[16](#fn16) applications demanding real-time response
    cannot tolerate network delays. Similarly, applications subject to strict data
    privacy regulations must process information locally rather than transmitting
    it to remote data centers. Edge devices (gateways and IoT hubs[17](#fn17)) occupy
    a middle ground in the deployment spectrum, maintaining acceptable performance
    while operating under intermediate resource constraints.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式转变对于云的100-500毫秒往返延迟被认为不可接受的应用至关重要。需要瞬间决策的自主系统和要求实时响应的工业物联网[16](#fn16)应用不能容忍网络延迟。同样，受到严格数据隐私法规约束的应用必须在本地处理信息，而不是将其传输到远程数据中心。边缘设备（网关和物联网中心[17](#fn17)）在部署谱中占据中间位置，在中间资源约束下保持可接受的性能。
- en: '***Edge Machine Learning (Edge ML)*** is the deployment of machine learning
    models on *localized infrastructure* at the network edge, enabling *low-latency
    processing* and *data privacy* through local computation on stationary devices
    like gateways and industrial controllers.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '***边缘机器学习（Edge ML）***是在网络边缘的本地基础设施上部署机器学习模型，通过在网关和工业控制器等静止设备上本地计算，实现*低延迟处理*和*数据隐私*。'
- en: '[Figure 2.5](ch008.xhtml#fig-edge-ml) provides an overview of Edge ML’s key
    dimensions, which this analysis addresses in detail.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.5](ch008.xhtml#fig-edge-ml)提供了Edge ML关键维度的概述，本分析将详细阐述这些维度。'
- en: '![](../media/file23.svg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file23.svg)'
- en: 'Figure 2.5: **Edge ML Dimensions**: This figure outlines key considerations
    for edge machine learning, contrasting challenges with benefits and providing
    representative examples and characteristics. Understanding these dimensions enables
    designing and deploying effective AI solutions on resource-constrained devices.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：**边缘机器学习维度**：此图概述了边缘机器学习的关键考虑因素，对比了挑战与好处，并提供了代表性的示例和特征。理解这些维度有助于在资源受限的设备上设计和部署有效的AI解决方案。
- en: Distributed Processing Architecture
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式处理架构
- en: 'Edge ML’s diversity spans wearables, industrial sensors, and smart home appliances,
    devices that process data locally[18](#fn18) without depending on central servers
    ([Figure 2.6](ch008.xhtml#fig-edgeml-example)). Edge devices occupy the middle
    ground between cloud systems and mobile devices in computational resources, power
    consumption, and cost. Memory bandwidth at 25-100 GB/s enables models requiring
    100MB-1GB parameters, using optimization techniques ([Chapter 10](ch016.xhtml#sec-model-optimizations))
    to achieve 2-4x speedup compared to cloud models. Local processing eliminates
    network round-trip latency, enabling <100ms response times while generating substantial
    bandwidth savings: processing 1000 camera feeds locally avoids 1Gbps uplink costs
    and reduces cloud expenses by $10,000-100,000 annually.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘机器学习（Edge ML）的多样性涵盖了可穿戴设备、工业传感器和智能家居设备，这些设备在本地[18](#fn18)处理数据而不依赖于中央服务器（[图2.6](ch008.xhtml#fig-edgeml-example)）。边缘设备在计算资源、功耗和成本方面介于云系统和移动设备之间。25-100
    GB/s的内存带宽使得需要100MB-1GB参数的模型能够通过优化技术（[第10章](ch016.xhtml#sec-model-optimizations)）实现与云模型相比2-4倍的加速。本地处理消除了网络往返延迟，实现了<100毫秒的响应时间，同时产生了大量的带宽节省：本地处理1000个摄像头流避免了1Gbps的上行成本，并每年减少10,000-100,000美元的云费用。
- en: Edge ML Benefits and Deployment Challenges
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘机器学习（Edge ML）的好处和部署挑战
- en: 'Edge ML provides quantifiable benefits that address key cloud limitations.
    Latency reduction from 100-500ms in cloud deployments to 1-50ms at the edge enables
    safety-critical applications[19](#fn19) requiring real-time response. Bandwidth
    savings prove equally substantial: a retail store with 50 cameras streaming video
    can reduce bandwidth requirements from 100 Mbps (costing $1,000-2,000 monthly)
    to less than 1 Mbps by processing locally and transmitting only metadata, a 99%
    reduction. Privacy improves through local processing, eliminating transmission
    risks and simplifying regulatory compliance. Operational resilience ensures systems
    continue functioning during network outages, proving critical for manufacturing,
    healthcare, and building management applications.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘机器学习提供了可量化的好处，解决了关键云限制。从云部署中的100-500ms延迟降低到边缘的1-50ms延迟，使得需要实时响应的安全关键应用[19](#fn19)成为可能。带宽节省同样显著：一家拥有50个摄像头的零售店通过本地处理和仅传输元数据，可以将带宽需求从100
    Mbps（每月成本1,000-2,000美元）降低到不到1 Mbps，减少了99%。通过本地处理，隐私得到改善，消除了传输风险并简化了合规性。运营弹性确保系统在网络中断期间继续运行，这对于制造、医疗保健和建筑管理应用至关重要。
- en: 'These benefits carry corresponding limitations. Limited computational resources[20](#fn20)
    significantly constrain model complexity: edge servers typically provide 10-100x
    less processing power than cloud infrastructure, limiting deployable models to
    millions rather than billions of parameters. Managing distributed networks introduces
    complexity that scales nonlinearly with deployment size. Coordinating version
    control and updates across thousands of devices requires sophisticated orchestration
    systems[21](#fn21). Security challenges intensify with physical accessibility—edge
    devices deployed in retail stores or public infrastructure face tampering risks
    requiring hardware-based protection mechanisms. Hardware heterogeneity further
    complicates deployment, as diverse platforms with varying capabilities demand
    different optimization strategies. Initial deployment costs of $500-2,000 per
    edge server create substantial capital requirements. Deploying 1,000 locations
    requires $500,000-2,000,000 upfront investment, though these costs are offset
    by long-term operational savings.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处伴随着相应的限制。有限的计算资源[20](#fn20)显著限制了模型复杂性：边缘服务器通常提供的处理能力比云基础设施低10-100倍，将可部署的模型限制在数百万参数而不是数十亿参数。管理分布式网络引入的复杂性随着部署规模的非线性增长。在数千台设备上协调版本控制和更新需要复杂的编排系统[21](#fn21)。随着物理可访问性的增加，安全挑战加剧——在零售店或公共基础设施中部署的边缘设备面临篡改风险，需要基于硬件的保护机制。硬件异构性进一步复杂化了部署，因为具有不同能力的各种平台需要不同的优化策略。边缘服务器的初始部署成本为500-2,000美元，创造了大量的资本需求。部署1,000个地点需要50万至200万美元的前期投资，尽管这些成本可以通过长期运营节省来抵消。
- en: '![](../media/file24.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file24.jpg)'
- en: 'Figure 2.6: **Edge Device Deployment**: Diverse IoT devices, from wearables
    to home appliances, enable decentralized machine learning by performing inference
    locally, reducing reliance on cloud connectivity and improving response times.
    Source: Edge Impulse.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：**边缘设备部署**：从可穿戴设备到家用电器，各种物联网设备通过本地执行推理，实现了去中心化机器学习，减少了对于云连接的依赖，并提高了响应时间。来源：Edge
    Impulse。
- en: Real-Time Industrial and IoT Systems
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时工业和物联网系统
- en: Industries deploy Edge ML widely where low latency, data privacy, and operational
    resilience justify the additional complexity of distributed processing. Autonomous
    vehicles represent perhaps the most demanding application, where safety-critical
    decisions must occur within milliseconds based on sensor data that cannot be transmitted
    to remote servers. Systems like Tesla’s Full Self-Driving process inputs from
    eight cameras at 36 frames per second through custom edge hardware, making driving
    decisions with latencies under 10ms, a response time physically impossible with
    cloud processing due to network delays.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 行业在低延迟、数据隐私和运营弹性可以证明分布式处理额外复杂性的情况下广泛部署边缘机器学习。自动驾驶汽车可能是最具有挑战性的应用，其中基于无法传输到远程服务器的传感器数据，必须在毫秒内做出安全关键的决定。像特斯拉的全自动驾驶系统这样的系统通过定制的边缘硬件，以每秒36帧的速度处理来自八个摄像头的输入，在10ms以下的延迟下做出驾驶决策，这是由于网络延迟，使用云处理无法实现的物理上不可能的响应时间。
- en: 'Smart retail environments demonstrate edge ML’s practical advantages for privacy-sensitive,
    bandwidth-intensive applications. Amazon Go stores process video from hundreds
    of cameras through local edge servers, tracking customer movements and item selections
    to enable checkout-free shopping. This edge-based approach addresses both technical
    and privacy concerns: transmitting high-resolution video from hundreds of cameras
    would require over 200 Mbps sustained bandwidth, while local processing ensures
    customer video never leaves the premises, addressing privacy concerns and regulatory
    requirements.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 智能零售环境展示了边缘机器学习在隐私敏感、带宽密集型应用中的实际优势。亚马逊Go商店通过本地边缘服务器处理来自数百个摄像头的视频，跟踪顾客的移动和商品选择，以实现无结账购物。这种基于边缘的方法解决了技术和隐私方面的担忧：从数百个摄像头传输高分辨率视频需要超过200
    Mbps的持续带宽，而本地处理确保顾客的视频永远不会离开场所，从而解决隐私问题和监管要求。
- en: The Industrial IoT[22](#fn22) leverages edge ML for applications where millisecond-level
    responsiveness directly impacts production efficiency and worker safety. Manufacturing
    facilities deploy edge ML systems for real-time quality control, with vision systems
    inspecting welds at speeds exceeding 60 parts per minute, and predictive maintenance[23](#fn23)
    applications that monitor over 10,000 industrial assets per facility. This approach
    has demonstrated 25-35% reductions in unplanned downtime across various manufacturing
    sectors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 工业物联网[22](#fn22)利用边缘机器学习，在毫秒级响应直接影响生产效率和工人安全的应用中。制造设施部署边缘机器学习系统进行实时质量控制，视觉系统以每分钟超过60个零件的速度检查焊接，以及监控每个设施超过10,000个工业资产的预测性维护[23](#fn23)应用。这种方法在各种制造行业证明了25-35%的不计划停机时间减少。
- en: Smart buildings utilize edge ML to optimize energy consumption while maintaining
    operational continuity during network outages. Commercial buildings equipped with
    edge-based building management systems process data from 5,000-10,000 sensors
    monitoring temperature, occupancy, air quality, and energy usage, with edge processing
    reducing cloud transmission requirements by 95% while enabling sub-second response
    times. Healthcare applications similarly leverage edge ML for patient monitoring
    and surgical assistance, maintaining HIPAA compliance through local processing
    while achieving sub-100ms latency for real-time surgical guidance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 智能建筑利用边缘机器学习优化能源消耗，同时在网络中断期间保持运营连续性。配备基于边缘的建筑管理系统的商业建筑处理来自5,000-10,000个传感器的数据，监测温度、占用率、空气质量和能源使用，边缘处理将云传输需求减少了95%，同时实现了亚秒级响应时间。医疗保健应用同样利用边缘机器学习进行患者监测和手术辅助，通过本地处理保持HIPAA合规性，同时实现实时手术指导的100ms以下延迟。
- en: 'Mobile ML: Personal and Offline Intelligence'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移动机器学习：个人和离线智能
- en: 'While Edge ML addressed the latency and privacy limitations of cloud deployment,
    it introduced new constraints: the need for dedicated edge infrastructure, ongoing
    network connectivity, and substantial upfront hardware investments. The proliferation
    of billions of personal computing devices (smartphones, tablets, and wearables)
    created an opportunity to extend ML capabilities even further by bringing intelligence
    directly to users’ hands. Mobile ML represents this next step in the distribution
    of intelligence, prioritizing user proximity, offline capability, and personalized
    experiences while operating under the strict power and thermal constraints inherent
    to battery-powered devices.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然边缘机器学习解决了云部署的延迟和隐私限制，但它引入了新的限制：需要专用边缘基础设施、持续的网络安全连接和大量的前期硬件投资。数十亿个人计算设备（智能手机、平板电脑和可穿戴设备）的普及创造了将智能直接带到用户手中的机会，从而进一步扩展机器学习能力。移动机器学习代表了智能分布的下一步，优先考虑用户接近性、离线能力和个性化体验，同时在电池供电设备的严格功率和热限制下运行。
- en: Mobile ML integrates machine learning directly into portable devices like smartphones
    and tablets, providing users with real-time, personalized capabilities. This paradigm
    excels when user privacy, offline operation, and immediate responsiveness matter
    more than computational sophistication. Mobile ML supports applications such as
    voice recognition[24](#fn24), computational photography[25](#fn25), and health
    monitoring while maintaining data privacy through on-device computation. These
    battery-powered devices must balance performance with power efficiency and thermal
    management, making them ideal for frequent, short-duration AI tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习将机器学习直接集成到智能手机和平板电脑等便携式设备中，为用户提供实时、个性化的功能。当用户隐私、离线操作和即时响应比计算复杂性更重要时，这种范式表现卓越。移动机器学习支持语音识别[24](#fn24)、计算摄影[25](#fn25)和健康监测等应用，同时通过设备端计算保持数据隐私。这些电池供电设备必须在性能和功耗、热管理之间取得平衡，使其成为频繁、短时AI任务的理想选择。
- en: '***Mobile Machine Learning (Mobile ML)*** is the deployment of machine learning
    models directly on *portable, battery-powered devices*, enabling *personalization*,
    *privacy*, and *offline operation* within severe energy and resource constraints.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '***移动机器学习（Mobile ML）*** 是将机器学习模型直接部署在*便携式、电池供电设备*上，在严格的能源和资源限制下实现*个性化*、*隐私*和*离线操作*。'
- en: This section analyzes Mobile ML across four key dimensions, revealing how this
    paradigm balances capability with constraints. [Figure 2.7](ch008.xhtml#fig-mobile-ml)
    provides an overview of Mobile ML’s capabilities.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从四个关键维度分析移动机器学习，揭示这一范式如何平衡能力和限制。[图2.7](ch008.xhtml#fig-mobile-ml)提供了移动机器学习能力概述。
- en: '![](../media/file25.svg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file25.svg)'
- en: 'Figure 2.7: **Mobile ML Capabilities**: Mobile machine learning systems balance
    performance with resource constraints through on-device processing, specialized
    hardware acceleration, and optimized frameworks. This figure outlines key considerations
    for deploying ML models on mobile devices, including the trade-offs between computational
    efficiency, battery life, and model performance.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：**移动机器学习能力**：移动机器学习系统通过设备端处理、专用硬件加速和优化框架，在性能和资源限制之间取得平衡。此图概述了在移动设备上部署机器学习模型的关键考虑因素，包括计算效率、电池寿命和模型性能之间的权衡。
- en: Battery and Thermal Constraints
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电池和热限制
- en: 'Mobile devices exemplify intermediate constraints: 8GB RAM, 128GB-1TB storage,
    1-10 TOPS AI compute through Neural Processing Units[26](#fn26) consuming 3-5W
    power. System-on-Chip architectures[27](#fn27) integrate computation and memory
    to minimize energy costs. Memory bandwidth of 25-50 GB/s limits models to 10-100MB
    parameters, requiring aggressive optimization ([Chapter 10](ch016.xhtml#sec-model-optimizations)).
    Battery constraints (18-22Wh capacity) make energy optimization critical: 1W continuous
    ML processing reduces device lifetime from 24 to 18 hours. Specialized frameworks
    (TensorFlow Lite[28](#fn28), Core ML[29](#fn29)) provide hardware-optimized inference
    enabling <50ms UI response times.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备体现了中间限制：8GB RAM、128GB-1TB存储、1-10 TOPS通过神经网络单元[26](#fn26)进行AI计算，消耗3-5W功率。片上系统架构[27](#fn27)将计算和内存集成以最小化能源成本。25-50
    GB/s的内存带宽限制了模型参数为10-100MB，需要积极的优化([第10章](ch016.xhtml#sec-model-optimizations))。电池限制（18-22Wh容量）使能源优化变得至关重要：1W连续的机器学习处理将设备寿命从24小时减少到18小时。专用框架（TensorFlow
    Lite[28](#fn28)、Core ML[29](#fn29)）提供硬件优化的推理，使UI响应时间低于50ms。
- en: Mobile ML Benefits and Resource Constraints
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移动机器学习优势和资源限制
- en: 'Mobile ML excels at delivering responsive, privacy-preserving user experiences.
    Real-time processing achieves sub-10ms latency, enabling imperceptible response:
    face detection operates at 60fps with under 5ms latency, while voice wake-word
    detection responds within 2-3ms. Privacy guarantees emerge from complete data
    sovereignty through on-device processing. Face ID processes biometric data entirely
    within a hardware-isolated Secure Enclave[30](#fn30), keyboard prediction trains
    locally on user data, and health monitoring maintains HIPAA compliance without
    complex infrastructure requirements. Offline functionality eliminates network
    dependency: Google Maps analyzes millions of road segments locally for navigation,
    translation[31](#fn31) supports 40+ language pairs using 35-45MB models that achieve
    90% of cloud accuracy, and music identification matches against on-device databases.
    Personalization reaches unprecedented depth by leveraging behavioral data accumulated
    over months: iOS predicts which app users will open next with 70-80% accuracy,
    notification management optimizes delivery timing based on individual patterns,
    and camera systems continuously adapt to user preferences through implicit feedback.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习在提供响应迅速、保护隐私的用户体验方面表现出色。实时处理实现了低于10毫秒的延迟，实现了几乎不可察觉的响应：人脸检测以60fps的速度运行，延迟低于5毫秒，而语音唤醒词检测在2-3毫秒内响应。通过设备上的处理实现完全的数据主权，确保了隐私保障：面部识别在硬件隔离的安全区域[30](#fn30)内处理生物识别数据，键盘预测在用户数据上本地训练，健康监测无需复杂的基础设施即可保持HIPAA合规性。离线功能消除了对网络的依赖：Google
    Maps在本地分析数百万个路段以进行导航，翻译[31](#fn31)支持40多个语言对，使用35-45MB的模型实现90%的云端准确性，音乐识别与设备数据库进行匹配。通过利用数月积累的行为数据，个性化达到了前所未有的深度：iOS以70-80%的准确性预测用户将打开哪个应用，通知管理根据个人模式优化交付时间，相机系统通过隐式反馈持续适应用户偏好。
- en: 'These benefits require accepting significant resource constraints. Flagship
    phones allocate only 100MB-1GB to individual ML applications, representing just
    0.5-5% of total memory, forcing models to remain under 100-500MB compared to cloud’s
    ability to deploy 350GB+ models. Battery life[32](#fn32) presents visible user
    impact: processing 100 inferences per hour at 0.1 joules each consumes 0.36% of
    battery daily, compounding with baseline drain; video processing at 30fps can
    reduce battery life from 24 hours to 6-8 hours. Thermal throttling unpredictably
    limits sustained performance, with the A17 Pro chip achieving 35 TOPS peak performance
    but sustaining only 10-15 TOPS during extended operation, requiring adaptive performance
    strategies. Development complexity multiplies across platforms, demanding separate
    implementations for Core ML and TensorFlow Lite, while device heterogeneity—particularly
    Android’s span from $100 budget phones to $1,500 flagships—requires multiple model
    variants. Deployment friction adds further challenges: app store approval processes
    taking 1-7 days prevent rapid bug fixes that cloud deployments can deploy instantly.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处需要接受重大的资源限制。旗舰手机只为单个机器学习应用分配了100MB-1GB的内存，仅占总内存的0.5-5%，迫使模型保持低于100-500MB，而云服务可以部署350GB+的模型。电池寿命[32](#fn32]对用户有明显的负面影响：每小时处理100次推理，每次消耗0.1焦耳，每天消耗电池的0.36%，与基准消耗相加；以30fps的速度处理视频可以将电池寿命从24小时减少到6-8小时。热管理的不确定性限制了持续的性能，A17
    Pro芯片在峰值性能达到35 TOPS，但在长时间运行期间只能维持10-15 TOPS，需要自适应性能策略。开发复杂性在各个平台之间成倍增加，需要为Core
    ML和TensorFlow Lite实现单独的实现，而设备异质性——尤其是从100美元预算手机到1500美元旗舰手机的跨度——需要多个模型变体。部署摩擦增加了进一步的挑战：应用商店的审批流程需要1-7天，这阻止了云部署可以即时部署的快速错误修复。
- en: Personal Assistant and Media Processing
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 个人助理与媒体处理
- en: 'Mobile ML has achieved transformative success across diverse applications that
    showcase the unique advantages of on-device processing for billions of users worldwide.
    Computational photography represents perhaps the most visible success, transforming
    smartphone cameras into sophisticated imaging systems. Modern flagships process
    every photo through multiple ML pipelines operating in real-time: portrait mode[33](#fn33)
    uses depth estimation and segmentation networks to achieve DSLR-quality bokeh
    effects, night mode captures and aligns 9-15 frames with ML-based denoising that
    reduces noise by 10-20dB, and systems like Google Pixel process 10-15 distinct
    ML models per photo for HDR merging, super-resolution, and scene optimization.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器学习在多样化的应用中取得了变革性的成功，展示了设备上处理对全球数十亿用户的独特优势。计算摄影可能是最明显的成功案例，将智能手机摄像头转变为复杂的成像系统。现代旗舰手机通过多个实时运行的机器学习管道处理每一张照片：人像模式[33](#fn33)使用深度估计和分割网络实现类似单反相机的散景效果，夜间模式通过基于机器学习的降噪技术捕捉并对齐9-15帧，降低噪声10-20dB，而像Google
    Pixel这样的系统则对每张照片处理10-15个不同的机器学习模型，用于HDR合并、超分辨率和场景优化。
- en: Voice-driven interactions demonstrate mobile ML’s transformation of human-device
    communication. These systems combine ultra-low-power wake-word detection consuming
    less than 1mW with on-device speech recognition achieving under 10ms latency for
    simple commands. Keyboard prediction has evolved to context-aware neural models
    achieving 60-70% phrase prediction accuracy, reducing typing effort by 30-40%.
    Real-time camera translation processes over 100 languages at 15-30fps entirely
    on-device, enabling instant visual translation without internet connectivity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语音的交互展示了移动机器学习如何改变人机通信。这些系统结合了超低功耗的唤醒词检测，消耗不到1mW，以及设备上的语音识别，简单命令的延迟低于10ms。键盘预测已发展到上下文感知的神经网络模型，实现了60-70%的短语预测准确性，减少了30-40%的打字工作量。实时相机翻译在15-30fps的速度下处理超过100种语言，完全在设备上完成，无需互联网连接即可实现即时视觉翻译。
- en: 'Health monitoring through wearables like Apple Watch extracts sophisticated
    insights from sensor data while maintaining complete privacy. These systems achieve
    over 95% accuracy in activity detection and include FDA-cleared atrial fibrillation
    detection with 98%+ sensitivity, processing extraordinarily sensitive health data
    entirely on-device to maintain HIPAA compliance. Accessibility features demonstrate
    transformative social impact through continuous local processing: Live Text detects
    and recognizes text from camera feeds, Sound Recognition alerts deaf users to
    environmental cues through haptic feedback, and VoiceOver generates natural language
    descriptions of visual content.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Apple Watch等可穿戴设备进行健康监测，从传感器数据中提取复杂的见解，同时保持完全的隐私。这些系统在活动检测中达到超过95%的准确性，包括FDA批准的心房颤动检测，灵敏度超过98%，完全在设备上处理极其敏感的健康数据，以保持HIPAA合规性。无障碍功能通过持续本地处理展示了变革性的社会影响：实时文本识别从摄像头流中检测和识别文本，声音识别通过触觉反馈提醒聋人用户环境提示，语音覆盖生成视觉内容的自然语言描述。
- en: Augmented reality frameworks leverage mobile ML for real-time environment understanding
    at 60fps. ARCore and ARKit track device position with centimeter-level accuracy
    while simultaneously mapping 3D surroundings, enabling hand tracking that extracts
    21-joint 3D poses and face analysis of 50+ landmark meshes for real-time effects.
    These applications demand consistent sub-16ms frame times, making only on-device
    processing viable for delivering the seamless experiences users expect.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 增强现实框架利用移动机器学习在60fps的速度下实现实时环境理解。ARCore和ARKit以厘米级的精度追踪设备位置，同时绘制3D周围环境，实现手部追踪，提取21个关节的3D姿态和50多个地标网格的面部分析，以实现实时效果。这些应用需要保持16ms以下的帧时间，使得仅在设备上处理成为提供用户期望的无缝体验的唯一可行方案。
- en: Despite mobile ML’s demonstrated capabilities, a common pitfall involves attempting
    to deploy desktop-trained models directly to mobile or edge devices without architecture
    modifications. Models developed on powerful workstations often fail dramatically
    when deployed to resource-constrained devices. A ResNet-50 model requiring 4GB
    memory for inference (including activations and batch processing) and 4 billion
    FLOPs per inference cannot run on a device with 512MB of RAM and a 1 GFLOP/s processor.
    Beyond simple resource violations, desktop-optimized models may use operations
    unsupported by mobile hardware (specialized mathematical operations), assume floating-point
    precision unavailable on embedded systems, or require batch processing incompatible
    with single-sample inference. Successful deployment demands architecture-aware
    design from the beginning, including specialized architectural techniques for
    mobile devices ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets)),
    integer-only operations for microcontrollers, and optimization strategies that
    maintain accuracy while reducing computation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管移动机器学习已经展示了其能力，但一个常见的陷阱是试图直接将桌面训练的模型部署到移动或边缘设备上，而不进行架构修改。在强大的工作站上开发的模型在部署到资源受限的设备上时往往表现糟糕。一个需要4GB内存进行推理（包括激活和批量处理）和每推理4亿次浮点运算（FLOPs）的ResNet-50模型无法在具有512MB
    RAM和1 GFLOP/s处理器的设备上运行。除了简单的资源违规之外，桌面优化的模型可能使用移动硬件不支持的操作（专门的数学操作），假设嵌入式系统中不可用的浮点精度，或者需要与单样本推理不兼容的批量处理。成功的部署需要从一开始就进行架构感知设计，包括为移动设备专门设计的架构技术（[A.
    G. Howard等人2017](ch058.xhtml#ref-howard2017mobilenets)），仅使用整数的微控制器操作，以及保持准确性的同时减少计算量的优化策略。
- en: 'Tiny ML: Ubiquitous Sensing at Scale'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微型机器学习：大规模无处不在的感知
- en: The progression from Cloud to Edge to Mobile ML demonstrates the increasing
    distribution of intelligence across computing platforms, yet each step still requires
    significant resources. Even mobile devices, with their sophisticated processors
    and gigabytes of memory, represent a relatively privileged position in the global
    computing landscape, demanding watts of power and hundreds of dollars in hardware
    investment. For truly ubiquitous intelligence (sensors in every surface, monitor
    on every machine, intelligence in every object), these resource requirements remain
    prohibitive. Tiny ML completes the deployment spectrum by pushing intelligence
    to its absolute limits, using devices costing less than $10 and consuming less
    than 1 milliwatt of power. This paradigm makes ubiquitous sensing not just technically
    feasible but economically practical at massive scales.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从云到边缘再到移动机器学习的进步展示了智能在计算平台上的分布越来越广泛，然而每一步仍然需要大量的资源。即使是拥有复杂处理器和数GB内存的移动设备，在全球计算格局中也处于相对有利的地位，需要消耗瓦特的电力和数百美元的硬件投资。对于真正无处不在的智能（每个表面都有传感器，每台机器都有监控，每个物体都有智能），这些资源需求仍然具有阻碍性。微型机器学习通过将智能推向绝对极限，使用成本低于10美元且功耗低于1毫瓦的设备，从而完成了部署光谱。这种范式使得无处不在的感知不仅在技术上可行，而且在大规模上经济上也是实用的。
- en: Where mobile ML still requires sophisticated hardware with gigabytes of memory
    and multi-core processors, Tiny Machine Learning operates on microcontrollers
    with kilobytes of RAM and single-digit dollar price points. This extreme constraint
    forces a significant shift in how we approach machine learning deployment, prioritizing
    ultra-low power consumption and minimal cost over computational sophistication.
    The result enables entirely new categories of applications impossible at any other
    scale.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动机器学习仍然需要具有数GB内存和多核处理器的复杂硬件时，微型机器学习在具有千字节RAM和单价仅为数美元的微控制器上运行。这种极端的限制迫使我们在机器学习部署方法上发生重大转变，优先考虑超低功耗和最小成本，而不是计算复杂性。结果是使得在其它任何规模下都不可行的全新类别应用成为可能。
- en: Tiny ML brings intelligence to the smallest devices, from microcontrollers[34](#fn34)
    to embedded sensors, enabling real-time computation in severely resource-constrained
    environments. This paradigm excels in applications requiring ubiquitous sensing,
    autonomous operation, and extreme energy efficiency. Tiny ML systems power applications
    such as predictive maintenance, environmental monitoring, and simple gesture recognition
    while optimized for energy efficiency[35](#fn35), often running for months or
    years on limited power sources such as coin-cell batteries[36](#fn36). These systems
    deliver actionable insights in remote or disconnected environments where power,
    connectivity, and maintenance access are impractical.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 小型机器学习（Tiny ML）将智能带到最小的设备中，从微控制器[34](#fn34)到嵌入式传感器，使在资源严重受限的环境中实现实时计算成为可能。这种范式在需要无处不在的感知、自主操作和极端能源效率的应用中表现出色。Tiny
    ML系统为预测性维护、环境监测和简单的手势识别等应用提供动力，同时优化能源效率[35](#fn35)，通常在有限的电源，如纽扣电池[36](#fn36)上运行数月或数年。这些系统在电力、连接性和维护访问不切实际的环境中提供可操作的见解。
- en: '***Tiny Machine Learning (Tiny ML)*** is the deployment of machine learning
    models on *microcontrollers* and *ultra-constrained devices*, enabling *autonomous
    decision-making* with milliwatt-scale power consumption for applications requiring
    years of battery life.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '***微型机器学习（Tiny ML）***是在*微控制器*和*超受限设备*上部署机器学习模型，使应用需要数年电池寿命时，能够以毫瓦级功耗实现*自主决策*。'
- en: This section analyzes Tiny ML through four critical dimensions that define its
    unique position in the ML deployment spectrum. [Figure 2.8](ch008.xhtml#fig-tiny-ml)
    encapsulates the key aspects of Tiny ML discussed in this section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过四个关键维度分析Tiny ML，这些维度定义了它在机器学习部署谱中的独特位置。[图2.8](ch008.xhtml#fig-tiny-ml)概括了本节讨论的Tiny
    ML的关键方面。
- en: '![](../media/file26.svg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file26.svg)'
- en: 'Figure 2.8: **TinyML System Characteristics**: Constrained devices necessitate
    a focus on efficiency, driving trade-offs between model complexity, accuracy,
    and energy consumption, while enabling localized intelligence and real-time responsiveness
    in embedded applications. This figure outlines key aspects of TinyML, including
    the challenges of resource limitations, example applications, and the benefits
    of on-device machine learning.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：**TinyML系统特性**：受限设备需要关注效率，在模型复杂度、准确性和能耗之间进行权衡，同时在嵌入式应用中实现本地智能和实时响应。此图概述了TinyML的关键方面，包括资源限制的挑战、示例应用和设备上机器学习的益处。
- en: Extreme Resource Constraints
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 极端资源限制
- en: 'TinyML operates at hardware extremes: Arduino Nano 33 BLE Sense (256KB RAM,
    1MB Flash, 0.02-0.04W, $35) and ESP32-CAM (520KB RAM, 4MB Flash, 0.05-0.25W, $10)
    represent 30,000-50,000x memory reduction versus cloud systems and 160,000x power
    reduction ([Figure 2.9](ch008.xhtml#fig-TinyML-example)). These constraints enable
    months or years of autonomous operation[37](#fn37) but demand specialized algorithms
    delivering acceptable performance at <1 TOPS compute with microsecond response
    times. Devices range from palm-sized to 5x5mm chips[38](#fn38), enabling ubiquitous
    sensing in previously impossible contexts.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML在硬件极端条件下运行：Arduino Nano 33 BLE Sense（256KB RAM，1MB闪存，0.02-0.04W，$35）和ESP32-CAM（520KB
    RAM，4MB闪存，0.05-0.25W，$10）与云系统相比，内存减少了30,000-50,000倍，与[图2.9](ch008.xhtml#fig-TinyML-example)相比，功耗降低了160,000倍。这些限制使设备能够实现数月或数年的自主运行[37](#fn37)，但需要专门的算法，在<1
    TOPS的计算能力和微秒级响应时间内提供可接受的性能。设备大小从手掌大小到5x5mm的芯片[38](#fn38)不等，使在以前不可能的情境中实现无处不在的感知成为可能。
- en: '![](../media/file27.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file27.png)'
- en: 'Figure 2.9: **TinyML System Scale**: These device kits exemplify the extreme
    miniaturization achievable with TinyML, enabling deployment of machine learning
    on resource-constrained devices with limited power and memory. such compact systems
    broaden the applicability of ML to previously inaccessible edge applications,
    including wearable sensors and embedded IoT devices. Source: ([Warden 2018](ch058.xhtml#ref-warden2018speech))'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：**TinyML系统规模**：这些设备套件展示了TinyML能够实现的极端小型化，使机器学习能够在有限的电源和内存的受限设备上部署。这些紧凑的系统扩大了ML的应用范围，包括以前无法访问的边缘应用，如可穿戴传感器和嵌入式物联网设备。来源：([Warden
    2018](ch058.xhtml#ref-warden2018speech))
- en: TinyML Advantages and Operational Trade-offs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TinyML优势和操作权衡
- en: 'TinyML’s extreme resource constraints enable unique advantages impossible at
    other scales. Microsecond-level latency eliminates all transmission overhead,
    achieving 10-100μs response times that enable applications requiring sub-millisecond
    decisions: industrial vibration monitoring processes 10kHz sampling at under 50μs
    latency, audio wake-word detection analyzes 16kHz audio streams under 100μs, and
    precision manufacturing systems inspect over 1000 parts per minute. Economic advantages
    prove transformative for massive-scale deployments: complete ESP32-CAM systems
    cost $8-12, enabling 1000-sensor deployments for $10,000 versus $500,000-1,000,000
    for cellular alternatives. Agricultural monitoring can instrument buildings for
    $5,000 versus $50,000+ for camera-based systems, while city-scale networks of
    100,000 sensors become economically viable at $1-2 million versus $50-100 million
    for edge alternatives. Energy efficiency enables 1-10 year operation on coin-cell
    batteries consuming just 1-10mW, supporting applications like wildlife tracking
    for years without recapture, structural health monitoring embedded in concrete
    during construction, and agricultural sensors deployed where power infrastructure
    doesn’t exist. Energy harvesting from solar, vibration, or thermal sources can
    even enable perpetual operation. Privacy surpasses all other paradigms through
    physical data confinement—data never leaves the sensor, providing mathematical
    guarantees impossible in networked systems regardless of encryption strength.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TinyML的极端资源限制使其在其它规模下无法实现的独特优势成为可能。微秒级延迟消除了所有传输开销，实现了10-100μs的响应时间，这使得需要亚毫秒级决策的应用成为可能：工业振动监测过程在低于50μs的延迟下进行10kHz采样，音频唤醒词检测在低于100μs下分析16kHz音频流，精密制造系统每分钟检查超过1000个部件。经济优势证明了大规模部署的变革性：完整的ESP32-CAM系统成本为8-12美元，使得1000个传感器的部署成本为1万美元，而基于蜂窝网络的替代方案成本为50万至100万美元。农业监测可以通过5000美元的设备进行，而基于摄像头的系统则需要超过5万美元，而城市规模的10万个传感器的网络在100万至200万美元的成本下变得经济可行，而边缘替代方案的成本为5000万至1亿美元。能源效率使得设备能够在仅消耗1-10mW的纽扣电池上运行1-10年，支持像野生动物追踪这样的应用数年无需重新捕获，在建设期间嵌入混凝土中的结构健康监测，以及在电力基础设施不存在的地区部署的农业传感器。甚至可以从太阳能、振动或热源中收集能量，从而实现永续运行。通过物理数据封装，隐私超越了所有其他范式——数据永远不会离开传感器，提供了在加密强度无论多强的情况下，网络化系统中不可能实现的数学保证。
- en: 'These capabilities require substantial trade-offs. Computational constraints
    impose severe limits: microcontrollers provide 256KB-2MB RAM versus smartphones’
    12-24GB (a 5,000-50,000x difference), forcing models to remain under 100-500KB
    with 10,000-100,000 parameters compared to mobile’s 1-10 million parameters. Development
    complexity requires expertise spanning neural network optimization, hardware-level
    memory management, embedded toolchains, and specialized debugging using oscilloscopes
    and JTAG debuggers across diverse microcontroller architectures. Model accuracy
    suffers from extreme compression: TinyML models typically achieve 70-85% of cloud
    model accuracy versus mobile’s 90-95%, limiting suitability for applications requiring
    high precision. Deployment inflexibility constrains adaptation, as devices typically
    run single fixed models requiring power-intensive firmware flashing for updates
    that risk bricking devices. With operational lifetimes spanning years, initial
    deployment decisions become critical. Ecosystem fragmentation[39](#fn39) across
    microcontroller vendors and ML frameworks creates substantial development overhead
    and platform lock-in challenges.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力需要巨大的权衡。计算限制施加了严格的限制：微控制器提供256KB-2MB的RAM，而智能手机提供12-24GB（相差5,000-50,000倍），迫使模型保持小于100-500KB，参数在10,000-100,000之间，与移动设备的1-1000万个参数相比。开发复杂性需要跨越神经网络优化、硬件级内存管理、嵌入式工具链以及使用示波器和JTAG调试器在多种微控制器架构上进行专业调试的专家知识。模型精度因极端压缩而受损：TinyML模型通常达到云模型精度的70-85%，而移动设备为90-95%，限制了其在需要高精度应用中的适用性。部署的不灵活性限制了适应性，因为设备通常运行单个固定的模型，需要耗电的固件更新，这些更新可能会损坏设备。设备的使用寿命跨越数年，初始部署决策变得至关重要。微控制器供应商和机器学习框架之间的生态系统碎片化[39](#fn39)创造了巨大的开发开销和平台锁定挑战。
- en: Environmental and Health Monitoring
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境与健康监测
- en: Tiny ML succeeds remarkably across domains where its unique advantages—ultra-low
    power, minimal cost, and complete data privacy—enable applications impossible
    with other paradigms. Industrial predictive maintenance demonstrates TinyML’s
    ability to transform traditional infrastructure through distributed intelligence.
    Manufacturing facilities deploy thousands of vibration sensors operating continuously
    for 5-10 years on coin-cell batteries while consuming less than 2mW average power.
    These sensors cost $15-50 compared to traditional wired sensors at $500-2,000
    per point, reducing deployment costs from $5-20 million to $150,000-500,000 for
    10,000 monitoring points. Local anomaly detection provides 7-14 day advance warning
    of equipment failures, enabling companies to achieve 25-45% reductions in unplanned
    downtime.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Tiny ML在各个领域都取得了显著的成果，其独特的优势——超低功耗、最小成本和完全的数据隐私——使得其他范式无法实现的应用成为可能。工业预测性维护展示了TinyML通过分布式智能改造传统基础设施的能力。制造设施部署了成千上万的振动传感器，这些传感器在5-10年内连续运行，使用纽扣电池，平均功耗低于2mW。这些传感器的成本为15-50美元，而传统的有线传感器每点的成本为500-2,000美元，将部署成本从10,000个监测点的5-20百万美元降低到150,000-500,000美元。本地异常检测提供了7-14天的设备故障预警，使公司能够将非计划停机时间减少25-45%。
- en: Wake-word detection represents TinyML’s most visible consumer application, with
    billions of devices employing always-listening capabilities at under 1mW continuous
    power consumption. These systems process 16kHz audio through neural networks containing
    5,000-20,000 parameters compressed to 10-50KB, detecting wake phrases with over
    95% accuracy. Amazon Echo devices use dedicated TinyML chips like the AML05 that
    consume less than 10mW for detection, only activating the main processor when
    wake words trigger—reducing average power consumption by 10-20x[40](#fn40).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 唤醒词检测代表了TinyML最明显的消费级应用，数十亿设备在低于1mW的持续功耗下使用始终倾听的功能。这些系统通过包含5,000-20,000个参数的神经网络处理16kHz音频，这些参数被压缩到10-50KB，以超过95%的准确率检测唤醒词。亚马逊Echo设备使用专门的TinyML芯片，如AML05，检测功耗低于10mW，仅在唤醒词触发时激活主处理器——将平均功耗降低了10-20倍[40](#fn40)。
- en: Precision agriculture leverages TinyML’s economic advantages where traditional
    solutions prove cost-prohibitive. Monitoring 100 hectares requires approximately
    1,000 monitoring points, which TinyML enables for $15,000-30,000 compared to $100,000-200,000+
    for cellular-connected alternatives. These sensors operate 3-5 years on batteries
    while analyzing temporal patterns locally, transmitting only actionable insights
    rather than raw data streams.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 精准农业利用了TinyML的经济优势，在传统解决方案成本过高的情况下。监测100公顷大约需要1,000个监测点，而TinyML只需15,000-30,000美元，相比之下，使用蜂窝连接的替代方案需要100,000-200,000美元以上。这些传感器在电池上运行3-5年，同时本地分析时间模式，仅传输可操作的见解而不是原始数据流。
- en: Wildlife conservation demonstrates TinyML’s transformative potential for remote
    environmental monitoring. Researchers deploy solar-powered audio sensors consuming
    100-500mW that process continuous audio streams for species identification. By
    performing local analysis, these systems reduce satellite transmission requirements
    from 4.3GB per day to 400KB of detection summaries, a 10,000x reduction that makes
    large-scale deployments of 100-1,000 sensors economically feasible. Medical wearables
    achieve FDA-cleared cardiac monitoring with 95-98% sensitivity while processing
    250-500 ECG samples per second at under 5mW power consumption. This efficiency
    enables week-long continuous monitoring versus hours for smartphone-based alternatives,
    while reducing diagnostic costs from $2,000-5,000 for traditional in-lab studies
    to under $100 for at-home testing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 野生动物保护展示了TinyML在远程环境监测方面的变革潜力。研究人员部署了太阳能供电的音频传感器，功耗为100-500mW，这些传感器处理连续的音频流以进行物种识别。通过本地分析，这些系统将卫星传输需求从每天4.3GB减少到检测摘要的400KB，减少了10,000倍，使得100-1,000个传感器的规模化部署在经济上可行。医疗可穿戴设备在5mW以下功耗下实现了FDA批准的心脏监测，灵敏度达到95-98%，每秒处理250-500个ECG样本。这种效率使得连续监测时间从基于智能手机的替代方案的数小时延长到一周，同时将诊断成本从传统实验室研究的2,000-5,000美元降低到家庭测试的100美元以下。
- en: 'Hybrid Architectures: Combining Paradigms'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合架构：结合范式
- en: Our examination of individual deployment paradigms—from cloud’s massive computational
    power to tiny ML’s ultra-efficient sensing—reveals a spectrum of engineering trade-offs,
    each with distinct advantages and limitations. Cloud ML maximizes algorithmic
    sophistication but introduces latency and privacy constraints. Edge ML reduces
    latency but requires dedicated infrastructure and constrains computational resources.
    Mobile ML prioritizes user experience but operates within strict battery and thermal
    limitations. Tiny ML achieves ubiquity through extreme efficiency but severely
    constrains model complexity. Each paradigm occupies a distinct niche, optimized
    for specific constraints and use cases.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对个别部署范式的考察——从云的强大计算能力到微型ML的超高效传感——揭示了一系列工程权衡，每个都有其独特的优势和局限性。云ML最大化算法的复杂性，但引入了延迟和隐私限制。边缘ML减少了延迟，但需要专用基础设施并限制了计算资源。移动ML优先考虑用户体验，但运行在严格的电池和热限制内。微型ML通过极端效率实现普遍性，但严重限制了模型复杂性。每个范式都占据一个独特的细分市场，针对特定的约束和使用案例进行了优化。
- en: Yet in practice, production systems rarely confine themselves to a single paradigm,
    as the limitations of each approach create opportunities for complementary integration.
    A voice assistant that uses tiny ML for wake-word detection, mobile ML for local
    speech recognition, edge ML for contextual processing, and cloud ML for complex
    natural language understanding demonstrates a more powerful approach. Hybrid Machine
    Learning formalizes this integration strategy, creating unified systems that leverage
    each paradigm’s complementary strengths while mitigating individual limitations.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，生产系统很少局限于单一范式，因为每种方法的局限性都为互补集成创造了机会。一个使用微型ML进行唤醒词检测、移动ML进行本地语音识别、边缘ML进行上下文处理和云ML进行复杂自然语言理解的语音助手，展示了一种更强大的方法。混合机器学习将这种集成策略形式化，创建了利用每个范式互补优势的同时减轻个别局限性的统一系统。
- en: '***Hybrid Machine Learning (Hybrid ML)*** is the integration of *multiple deployment
    paradigms* into unified systems, strategically distributing workloads across *computational
    tiers* to achieve *scalability*, *privacy*, and *performance* impossible with
    single-paradigm approaches.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***混合机器学习（Hybrid ML）***是将多种部署范式集成到统一系统中，战略性地在计算层之间分配工作负载，以实现单范式方法无法实现的**可扩展性**、**隐私**和**性能**。'
- en: Multi-Tier Integration Patterns
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层集成模式
- en: Hybrid ML design patterns provide reusable architectural solutions for integrating
    paradigms effectively. Each pattern represents a strategic approach to distributing
    ML workloads across computational tiers, optimized for specific trade-offs in
    latency, privacy, resource efficiency, and scalability.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 混合机器学习（Hybrid ML）设计模式提供了可重用的架构解决方案，以有效地集成范式。每个模式代表了一种战略方法，用于在计算层之间分配机器学习工作负载，针对特定的延迟、隐私、资源效率和可扩展性权衡进行了优化。
- en: This analysis identifies five essential patterns that address common integration
    challenges in hybrid ML systems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该分析确定了五个基本模式，这些模式解决了混合机器学习系统中常见的集成挑战。
- en: Train-Serve Split
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练-服务分离
- en: One of the most common hybrid patterns is the train-serve split, where model
    training occurs in the cloud but inference happens on edge, mobile, or tiny devices.
    This pattern takes advantage of the cloud’s vast computational resources for the
    training phase while benefiting from the low latency and privacy advantages of
    on-device inference[41](#fn41). For example, smart home devices often use models
    trained on large datasets in the cloud but run inference locally to ensure quick
    response times and protect user privacy. In practice, this might involve training
    models on powerful cloud systems like TPU Pods with exaflop-scale compute and
    hundreds of terabytes of memory, before deploying optimized versions to edge servers
    or embedded edge devices for efficient inference. Similarly, mobile vision models
    for computational photography are typically trained on powerful cloud infrastructure
    but deployed to run efficiently on phone hardware.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的混合模式之一是训练-服务分离，其中模型训练在云端进行，但推理发生在边缘、移动或小型设备上。这种模式利用云端的强大计算资源进行训练阶段，同时受益于设备端推理的低延迟和隐私优势[41](#fn41)。例如，智能家居设备通常使用在云端基于大型数据集训练的模型，但本地运行推理以确保快速响应时间和保护用户隐私。在实践中，这可能涉及在具有百亿级计算能力和数百TB内存的强大云系统（如TPU
    Pods）上训练模型，然后再将优化版本部署到边缘服务器或嵌入式边缘设备上进行高效的推理。同样，用于计算摄影的移动视觉模型通常在强大的云基础设施上训练，但部署到手机硬件上以实现高效运行。
- en: Hierarchical Processing
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**分层处理**'
- en: Hierarchical processing creates a multi-tier system where data and intelligence
    flow between different levels of the ML stack. This pattern effectively combines
    the capabilities of Cloud ML systems (like the large-scale training infrastructure
    discussed in previous sections) with multiple Edge ML systems (like edge servers
    and embedded devices from our edge deployment examples) to balance central processing
    power with local responsiveness. In industrial IoT applications, tiny sensors
    might perform basic anomaly detection, edge devices aggregate and analyze data
    from multiple sensors, and cloud systems handle complex analytics and model updates.
    For instance, we might see ESP32-CAM devices (from our Tiny ML examples) performing
    basic image classification at the sensor level with their minimal 520 KB RAM,
    feeding data up to edge servers or embedded systems for more sophisticated analysis,
    and ultimately connecting to cloud infrastructure for complex analytics and model
    updates.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**分层处理**创建了一个多级系统，其中数据和智能在机器学习堆栈的不同层级之间流动。这种模式有效地结合了云机器学习系统的能力（如前几节讨论的大规模训练基础设施）和多个边缘机器学习系统（如边缘服务器和我们的边缘部署示例中的嵌入式设备），以平衡中央处理能力和本地响应性。在工业物联网应用中，微型传感器可能执行基本的异常检测，边缘设备汇总并分析来自多个传感器的数据，而云系统处理复杂的分析和模型更新。例如，我们可能会看到ESP32-CAM设备（来自我们的小型机器学习示例）在传感器级别使用最小的520
    KB RAM执行基本的图像分类，将数据传输到边缘服务器或嵌入式系统进行更复杂分析，并最终连接到云基础设施进行复杂分析和模型更新。'
- en: This hierarchy allows each tier to handle tasks appropriate to its capabilities.
    Tiny ML devices handle immediate, simple decisions; edge devices manage local
    coordination; and cloud systems tackle complex analytics and learning tasks. Smart
    city installations often use this pattern, with street-level sensors feeding data
    to neighborhood-level edge processors, which in turn connect to city-wide cloud
    analytics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种层次结构允许每个层级处理适合其能力的任务。小型机器学习设备处理即时、简单的决策；边缘设备管理本地协调；云系统处理复杂的分析和学习任务。智能城市安装通常使用这种模式，街级传感器将数据传输到社区级的边缘处理器，后者再连接到城市级的云分析。
- en: Progressive Deployment
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**渐进式部署**'
- en: Progressive deployment creates tiered intelligence architectures by adapting
    models across computational tiers through systematic compression. A model might
    start as a large cloud version, then be progressively optimized for edge servers,
    mobile devices, and finally tiny sensors using techniques detailed in [Chapter 10](ch016.xhtml#sec-model-optimizations).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**渐进式部署**通过系统性地压缩模型，通过计算层级的适应性创建分层智能架构。一个模型可能从一个大型云版本开始，然后通过第10章中详细描述的技术逐步优化边缘服务器、移动设备和最终的小型传感器。'
- en: 'Amazon Alexa exemplifies this pattern: wake-word detection uses <1KB models
    on TinyML devices consuming <1mW, edge processing handles simple commands with
    1-10MB models at 1-10W, while complex natural language understanding requires
    GB+ models in cloud infrastructure. This tiered approach reduces cloud inference
    costs by 95% while maintaining user experience.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊Alexa是这种模式的例证：唤醒词检测在TinyML设备上使用<1KB模型，消耗<1mW，边缘处理使用1-10MB模型在1-10W下处理简单命令，而复杂的自然语言理解则需要云基础设施中的GB+模型。这种分层方法将云推理成本降低了95%，同时保持了用户体验。
- en: 'However, progressive deployment introduces operational complexity: model versioning
    across tiers, ensuring consistency between generations, managing failure cascades
    during connectivity loss, and coordinating updates across millions of devices.
    Production teams must maintain specialized expertise spanning TinyML optimization,
    edge orchestration, and cloud scaling.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，渐进式部署引入了操作复杂性：层间的模型版本管理、确保各代之间的一致性、在连接丢失期间管理故障级联以及协调数百万设备之间的更新。生产团队必须维护涵盖小型机器学习优化、边缘编排和云扩展的专门专业知识。
- en: Federated Learning
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**联邦学习**'
- en: Federated learning[42](#fn42) enables learning from distributed data while maintaining
    privacy. Google’s production system processes 6 billion mobile keyboards, training
    improved models while keeping typed text local. Each training round involves 100-10,000
    devices contributing model updates, requiring orchestration to manage device availability,
    network conditions, and computational heterogeneity.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习**[42](#fn42)允许从分布式数据中学习同时保持隐私。谷歌的生产系统处理60亿个移动键盘，在保持输入文本本地化的同时训练改进的模型。每一轮训练涉及100-10,000个设备贡献模型更新，需要编排来管理设备可用性、网络条件和计算异构性。'
- en: 'Production deployments face significant operational challenges: device dropout
    rates of 50-90% during training rounds, network bandwidth constraints limiting
    update frequency, and differential privacy mechanisms preventing information leakage.
    Aggregation servers must handle intermittent connectivity, varying device capabilities,
    and ensure convergence despite non-IID data distributions. This requires specialized
    monitoring infrastructure to track distributed training progress and debug issues
    without accessing raw data.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 生产部署面临重大的运营挑战：训练轮次中的设备掉线率高达50-90%，网络带宽限制更新频率，以及差分隐私机制防止信息泄露。聚合服务器必须处理间歇性连接、不同的设备能力，并确保在非独立同分布数据分布的情况下实现收敛。这需要专门的监控基础设施来跟踪分布式训练进度并调试问题，而无需访问原始数据。
- en: Collaborative Learning
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 协作学习
- en: Collaborative learning enables peer-to-peer learning between devices at the
    same tier, often complementing hierarchical structures.[43](#fn43) Autonomous
    vehicle fleets, for example, might share learning about road conditions or traffic
    patterns directly between vehicles while also communicating with cloud infrastructure.
    This horizontal collaboration allows systems to share time-sensitive information
    and learn from each other’s experiences without always routing through central
    servers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 协作学习使得同一层级的设备之间能够进行点对点学习，通常补充了层次结构。[43](#fn43) 例如，自动驾驶车队可以在车辆之间直接共享关于道路状况或交通模式的学习，同时与云基础设施进行通信。这种横向协作使得系统可以共享时间敏感信息，并从彼此的经验中学习，而无需始终通过中央服务器路由。
- en: Production System Case Studies
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产系统案例研究
- en: Real-world implementations integrate multiple design patterns into cohesive
    solutions rather than applying them in isolation. Production ML systems form interconnected
    networks where each paradigm plays a specific role while communicating with others,
    following integration patterns that leverage the strengths and address the limitations
    established in our four-paradigm framework ([Section 2.2](ch008.xhtml#sec-ml-systems-deployment-spectrum-38d0)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的实现将多个设计模式整合成统一的解决方案，而不是孤立地应用它们。生产机器学习系统形成相互连接的网络，其中每个范式都扮演特定的角色，并与他人通信，遵循利用我们的四范式框架中确立的强项并解决局限性的集成模式（[第2.2节](ch008.xhtml#sec-ml-systems-deployment-spectrum-38d0)）。
- en: '[Figure 2.10](ch008.xhtml#fig-hybrid) illustrates these key interactions through
    specific connection types: “Deploy” paths show how models flow from cloud training
    to various devices, “Data” and “Results” show information flow from sensors through
    processing stages, “Analyze” shows how processed information reaches cloud analytics,
    and “Sync” demonstrates device coordination. Notice how data generally flows upward
    from sensors through processing layers to cloud analytics, while model deployments
    flow downward from cloud training to various inference points. The interactions
    aren’t strictly hierarchical. Mobile devices might communicate directly with both
    cloud services and tiny sensors, while edge systems can assist mobile devices
    with complex processing tasks.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.10](ch008.xhtml#fig-hybrid) 通过具体的连接类型说明了这些关键交互：“部署”路径显示了模型如何从云训练流向各种设备，“数据”和“结果”显示了信息如何从传感器通过处理阶段流动，“分析”显示了处理后的信息如何达到云分析，“同步”展示了设备协调。注意数据通常从传感器通过处理层向上流向云分析，而模型部署则从云训练向下流向各种推理点。这些交互并非严格分层。移动设备可以直接与云服务和微型传感器通信，而边缘系统可以帮助移动设备处理复杂的处理任务。'
- en: '![](../media/file28.svg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file28.svg)'
- en: 'Figure 2.10: **Hybrid System Interactions**: Data flows upward from sensors
    through processing layers to cloud analytics for insights, while trained models
    deploy downward from the cloud to enable inference at the edge, mobile, and Tiny
    ML devices. These connection types (deploy, data/results, analyze, and sync) establish
    a distributed architecture where each paradigm contributes unique capabilities
    to the overall machine learning system.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：**混合系统交互**：数据从传感器通过处理层向上流向云分析以获取洞察，而训练好的模型从云向下部署，以在边缘、移动和Tiny ML设备上实现推理。这些连接类型（部署、数据/结果、分析和同步）建立了一个分布式架构，其中每个范式都为整体机器学习系统贡献独特的功能。
- en: 'Production systems demonstrate these integration patterns across diverse applications
    where no single paradigm could deliver the required functionality. Industrial
    defect detection exemplifies model deployment patterns: cloud infrastructure trains
    vision models on datasets from multiple facilities, then distributes optimized
    versions to edge servers managing factory operations, tablets for quality inspectors,
    and embedded cameras on manufacturing equipment. This demonstrates how a single
    ML solution flows from centralized training to inference points at multiple computational
    scales.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 生产系统在多种应用中展示了这些集成模式，在这些应用中，没有任何单一范式能够提供所需的功能。工业缺陷检测是模型部署模式的例证：云基础设施在多个设施的数据集上训练视觉模型，然后将优化的版本分发到管理工厂运营的边缘服务器、质量检查员使用的平板电脑以及制造设备上的嵌入式摄像头。这展示了单个机器学习解决方案如何从集中式训练流向多个计算规模上的推理点。
- en: 'Agricultural monitoring illustrates hierarchical data flow: soil sensors perform
    local anomaly detection, transmit results to edge processors that aggregate data
    from dozens of sensors, which then route insights to cloud infrastructure for
    farm-wide analytics while simultaneously updating farmers’ mobile applications.
    Information traverses upward through processing layers, with each tier adding
    analytical sophistication appropriate to its computational resources.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 农业监测说明了分层数据流：土壤传感器执行本地异常检测，将结果传输到边缘处理器，这些处理器从数十个传感器汇总数据，然后将洞察传递到云基础设施进行农场范围的统计分析，同时更新农民的移动应用程序。信息通过处理层向上传递，每一层都添加了适合其计算资源的分析复杂性。
- en: 'Fitness trackers exemplify gateway patterns between Tiny ML and mobile devices:
    wearables continuously monitor activity using algorithms optimized for microcontroller
    execution, sync processed data to smartphones that combine metrics from multiple
    sources, then transmit periodic updates to cloud infrastructure for long-term
    analysis. This enables tiny devices to participate in large-scale systems despite
    lacking direct network connectivity.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 健身追踪器是Tiny ML和移动设备之间的网关模式的例证：可穿戴设备使用针对微控制器执行优化的算法持续监控活动，将处理后的数据同步到结合来自多个来源的指标的智能手机，然后定期将更新传输到云基础设施进行长期分析。这使得小型设备能够在缺乏直接网络连接的情况下参与大规模系统。
- en: These integration patterns reveal how deployment paradigms complement each other
    through orchestrated data flows, model deployments, and cross-tier assistance.
    Industrial systems compose capabilities from Cloud, Edge, Mobile, and Tiny ML
    into distributed architectures that optimize for latency, privacy, cost, and operational
    requirements simultaneously. The interactions between paradigms often determine
    system success more than individual component capabilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集成模式揭示了部署范式如何通过协调数据流、模型部署和跨层协助相互补充。工业系统将云、边缘、移动和Tiny ML的能力组合成分布式架构，同时优化延迟、隐私、成本和运营需求。范式之间的交互往往比单个组件的能力更能决定系统成功。
- en: Shared Principles Across Deployment Paradigms
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署范式中的共享原则
- en: 'Despite their diversity, all ML deployment paradigms share core principles
    that enable systematic understanding and effective hybrid combinations. [Figure 2.11](ch008.xhtml#fig-ml-systems-convergence)
    illustrates how implementations spanning cloud to tiny devices converge on core
    system challenges: managing data pipelines, balancing resource constraints, and
    implementing reliable architectures. This convergence explains why techniques
    transfer effectively between paradigms and hybrid approaches work successfully
    in practice.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们多样化，但所有机器学习部署范式都共享核心原则，这些原则使得系统性的理解和有效的混合组合成为可能。[图2.11](ch008.xhtml#fig-ml-systems-convergence)说明了从云到小型设备的实现如何汇聚到核心系统挑战：管理数据管道、平衡资源约束和实现可靠的架构。这种汇聚解释了为什么技术能够在范式之间有效转移，并且混合方法在实践中能够成功工作。
- en: '![](../media/file29.svg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file29.svg)'
- en: 'Figure 2.11: **Convergence of ML Systems**: Diverse machine learning deployments
    (cloud, edge, mobile, and tiny) share foundational principles in data pipelines,
    resource management, and system architecture, enabling hybrid solutions and systematic
    design approaches. Understanding these shared principles allows practitioners
    to adapt techniques across different paradigms and build cohesive, efficient ML
    workflows despite varying constraints and optimization goals.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：**机器学习系统收敛**：不同的机器学习部署（云、边缘、移动和微型）在数据管道、资源管理和系统架构方面共享基础原则，使得混合解决方案和系统化设计方法成为可能。理解这些共享原则允许从业者跨不同范式调整技术，并在不同的约束和优化目标下构建一致、高效的机器学习工作流程。
- en: '[Figure 2.11](ch008.xhtml#fig-ml-systems-convergence) reveals three distinct
    layers of abstraction that unify ML system design across deployment contexts.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.11](ch008.xhtml#fig-ml-systems-convergence)揭示了三个不同的抽象层，这些层统一了不同部署环境下的机器学习系统设计。'
- en: The top layer represents ML system implementations—the four deployment paradigms
    examined throughout this chapter. Cloud ML operates in data centers with training
    at scale, Edge ML performs local processing focused on inference, Mobile ML runs
    on personal devices for user applications, and TinyML executes on embedded systems
    under severe resource constraints. Despite their apparent differences, these implementations
    share deeper commonalities that emerge in the underlying layers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层代表机器学习系统的实现——本章中检验的四个部署范式。云机器学习在数据中心进行大规模训练，边缘机器学习执行本地处理，专注于推理，移动机器学习在个人设备上运行，用于用户应用，而TinyML在资源受限的嵌入式系统上执行。尽管它们表面上存在差异，但这些实现共享更深层次的共同点，这些共同点在底层中显现出来。
- en: The middle layer identifies core system principles that unite all paradigms.
    Data pipeline management ([Chapter 6](ch012.xhtml#sec-data-engineering)) governs
    information flow from collection through deployment, maintaining consistent patterns
    whether processing petabytes in cloud data centers or kilobytes on microcontrollers.
    Resource management creates universal challenges in balancing competing demands
    for computation, memory, energy, and network capacity across all scales. System
    architecture principles guide the integration of models, hardware, and software
    components regardless of deployment context. These foundational principles remain
    remarkably consistent even as implementations vary by orders of magnitude in available
    resources.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层识别出将所有范式统一起来的核心系统原则。数据管道管理（[第6章](ch012.xhtml#sec-data-engineering)）控制着从收集到部署的信息流，无论是在云端数据中心处理PB级数据还是在微控制器上处理KB级数据，都保持一致的模式。资源管理在所有规模上创造着平衡计算、内存、能源和网络容量之间竞争需求的普遍挑战。系统架构原则指导着无论部署环境如何，模型、硬件和软件组件的集成。即使实施方式在可用资源上存在数量级的差异，这些基础原则仍然非常一致。
- en: 'The bottom layer shows how system considerations manifest these principles
    across practical dimensions. Optimization and efficiency strategies ([Chapter 10](ch016.xhtml#sec-model-optimizations))
    take different forms at each scale: cloud GPU cluster training, edge model compression,
    mobile thermal management, and TinyML numerical precision, yet all pursue maximizing
    performance within available resources. Operational aspects ([Chapter 13](ch019.xhtml#sec-ml-operations))
    address deployment, monitoring, and updates with paradigm-specific approaches
    that tackle fundamentally similar challenges. Trustworthy AI ([Chapter 17](ch023.xhtml#sec-responsible-ai),
    [Chapter 16](ch022.xhtml#sec-robust-ai)) requirements for security, privacy, and
    reliability apply universally, though implementation techniques necessarily adapt
    to each deployment context.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 底层展示了系统考虑如何在实际维度上体现这些原则。优化和效率策略（[第10章](ch016.xhtml#sec-model-optimizations)）在各个规模上采取不同的形式：云GPU集群训练、边缘模型压缩、移动热管理以及TinyML数值精度，但所有这些都在追求在可用资源内最大化性能。操作方面（[第13章](ch019.xhtml#sec-ml-operations)）通过针对范式特定方法的部署、监控和更新来应对根本性相似挑战。可信赖的人工智能（[第17章](ch023.xhtml#sec-responsible-ai)，[第16章](ch022.xhtml#sec-robust-ai)）对安全、隐私和可靠性的要求普遍适用，尽管实现技术必然要适应每个部署环境。
- en: This three-layer structure explains why techniques transfer effectively between
    scales. Cloud-trained models deploy successfully to edge devices because training
    and inference optimize similar objectives under different constraints. Mobile
    optimization insights inform cloud efficiency strategies because both manage the
    same fundamental resource trade-offs. TinyML innovations drive cross-paradigm
    advances precisely because extreme constraints force solutions to core problems
    that exist at all scales. Hybrid approaches work effectively (train-serve splits,
    hierarchical processing, federated learning) because underlying principles align
    across paradigms, enabling seamless integration despite vast differences in available
    resources.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这种三层结构解释了为什么技术能够在不同规模之间有效转移。云训练的模型成功部署到边缘设备，因为训练和推理在不同的约束下优化了相似的目标。移动优化见解为云效率策略提供了信息，因为两者都管理着相同的根本资源权衡。TinyML的创新推动了跨范例的进步，正是因为极端的限制迫使解决方案解决所有规模都存在的核心问题。混合方法（训练-服务拆分、分层处理、联邦学习）有效，因为底层原则在范例之间是一致的，即使在可用资源存在巨大差异的情况下也能实现无缝集成。
- en: Comparative Analysis and Selection Framework
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较分析和选择框架
- en: Building from this understanding of shared principles, systematic comparison
    across deployment paradigms reveals the precise trade-offs that should drive deployment
    decisions and highlights scenarios where each paradigm excels, providing practitioners
    with analytical frameworks for making informed architectural choices.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对共享原则的理解，对部署范例的系统比较揭示了应驱动部署决策的精确权衡，并突出了每个范例表现优异的场景，为从业者提供了解决方案分析框架，以便做出明智的架构选择。
- en: The relationship between computational resources and deployment location forms
    one of the most important comparisons across ML systems. As we move from cloud
    deployments to tiny devices, we observe a dramatic reduction in available computing
    power, storage, and energy consumption. Cloud ML systems, with their data center
    infrastructure, can leverage virtually unlimited resources, processing data at
    the scale of petabytes and training models with billions of parameters. Edge ML
    systems, while more constrained, still offer significant computational capability
    through specialized hardware like edge GPUs and neural processing units. Mobile
    ML represents a middle ground, balancing computational power with energy efficiency
    on devices like smartphones and tablets. At the far end of the spectrum, TinyML
    operates under severe resource constraints, often limited to kilobytes of memory
    and milliwatts of power consumption.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源与部署位置之间的关系构成了机器学习系统中最重要的比较之一。随着我们从云部署转向微型设备，我们观察到可用的计算能力、存储和能耗发生了显著下降。云机器学习系统凭借其数据中心基础设施，几乎可以无限制地利用资源，以PB级规模处理数据，并使用具有数十亿参数的模型进行训练。边缘机器学习系统虽然受到更多限制，但通过边缘GPU和神经处理单元等专用硬件，仍然提供了显著的计算能力。移动机器学习代表了中间地带，在智能手机和平板电脑等设备上平衡计算能力和能源效率。在光谱的另一端，TinyML在严格的资源限制下运行，通常仅限于千字节级的内存和毫瓦级的能耗。
- en: 'Table 2.2: **Deployment Locations**: Machine learning systems vary in where
    computation occurs, from centralized cloud servers to local edge devices and ultra-low-power
    TinyML chips, each impacting latency, bandwidth, and energy consumption. This
    table categorizes these deployments by their processing location and associated
    characteristics, enabling informed decisions about system architecture and resource
    allocation.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2：**部署位置**：机器学习系统的计算位置各不相同，从集中的云服务器到本地边缘设备，再到超低功耗的TinyML芯片，每个都影响着延迟、带宽和能耗。本表按处理位置和相关特征对这些部署进行分类，有助于做出关于系统架构和资源分配的明智决策。
- en: '| **Aspect** | **Cloud ML** | **Edge ML** | **Mobile ML** | **Tiny ML** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **方面** | **云机器学习** | **边缘机器学习** | **移动机器学习** | **TinyML** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Performance** |  |  |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **性能** |  |  |  |  |'
- en: '| **Processing Location** | Centralized cloud servers (Data Centers) | Local
    edge devices (gateways, servers) | Smartphones and tablets | Ultra-low-power microcontrollers
    and embedded systems |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **处理位置** | 集中云服务器（数据中心） | 本地边缘设备（网关、服务器） | 智能手机和平板电脑 | 超低功耗微控制器和嵌入式系统 |'
- en: '| **Latency** | High (100 ms-1000 ms+) | Moderate (10-100 ms) | Low-Moderate
    (5-50 ms) | Very Low (1-10 ms) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 高（100 ms-1000 ms+） | 中等（10-100 ms） | 低-中等（5-50 ms） | 非常低（1-10 ms）
    |'
- en: '| **Compute Power** | Very High (Multiple GPUs/TPUs) | High (Edge GPUs) | Moderate
    (Mobile NPUs/GPUs) | Very Low (MCU/tiny processors) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **计算能力** | 非常高（多个GPU/TPU） | 高（Edge GPU） | 中等（移动NPUs/GPU） | 非常低（MCU/小型处理器）
    |'
- en: '| **Storage Capacity** | Unlimited (petabytes+) | Large (terabytes) | Moderate
    (gigabytes) | Very Limited (kilobytes-megabytes) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **存储容量** | 无限（PB+） | 大（TB） | 中等（GB） | 非常有限（KB-MB） |'
- en: '| **Energy Consumption** | Very High (kW-MW range) | High (100 s W) | Moderate
    (1-10 W) | Very Low (mW range) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **能源消耗** | 非常高（kW-MW范围） | 高（100 s W） | 中等（1-10 W） | 非常低（mW范围） |'
- en: '| **Scalability** | Excellent (virtually unlimited) | Good (limited by edge
    hardware) | Moderate (per-device scaling) | Limited (fixed hardware) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 极佳（几乎无限） | 良好（受边缘硬件限制） | 中等（按设备扩展） | 有限（固定硬件） |'
- en: '| **Operational** |  |  |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **操作** |  |  |  |  |'
- en: '| **Data Privacy** | Basic-Moderate (Data leaves device) | High (Data stays
    in local network) | High (Data stays on phone) | Very High (Data never leaves
    sensor) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **数据隐私** | 基本到中等（数据离开设备） | 高（数据留在本地网络） | 高（数据留在手机上） | 非常高（数据从不离开传感器） |'
- en: '| **Connectivity Required** | Constant high-bandwidth | Intermittent | Optional
    | None |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **所需连接** | 持续高带宽 | 断续 | 可选 | 无需 |'
- en: '| **Offline Capability** | None | Good | Excellent | Complete |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **离线能力** | 无 | 良好 | 极佳 | 完全 |'
- en: '| **Real-time Processing** | Dependent on network | Good | Very Good | Excellent
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **实时处理** | 依赖于网络 | 良好 | 非常良好 | 极佳 |'
- en: '| **Deployment** |  |  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **部署** |  |  |  |  |'
- en: '| **Cost** | High ($1000s+/month) | Moderate ($100s-1000s) | Low ($0-10s) |
    Very Low ($1-10s) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **成本** | 高（每月1000美元以上） | 中等（100-1000美元） | 低（0-10美元） | 非常低（1-10美元） |'
- en: '| **Hardware Requirements** | Cloud infrastructure | Edge servers/gateways
    | Modern smartphones | MCUs/embedded systems |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **硬件要求** | 云基础设施 | Edge服务器/网关 | 现代智能手机 | MCU/嵌入式系统 |'
- en: '| **Development Complexity** | High (cloud expertise needed) | Moderate-High
    (edge+networking) | Moderate (mobile SDKs) | High (embedded expertise) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **开发复杂性** | 高（需要云专业知识） | 中等到高（边缘+网络） | 中等（移动SDK） | 高（嵌入式专业知识） |'
- en: '| **Deployment Speed** | Fast | Moderate | Fast | Slow |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **部署速度** | 快速 | 中等 | 快速 | 慢速 |'
- en: '[Table 2.2](ch008.xhtml#tbl-big_vs_tiny) quantifies these paradigm differences
    across performance, operational, and deployment dimensions, revealing clear gradients
    in latency (cloud: 100-1000ms → edge: 10-100ms → mobile: 5-50ms → tiny: 1-10ms)
    and privacy guarantees (strongest with TinyML’s complete local processing).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2.2](ch008.xhtml#tbl-big_vs_tiny) 在性能、操作和部署维度上量化了这些范式差异，揭示了延迟（云：100-1000ms
    → 边缘：10-100ms → 移动：5-50ms → 微型：1-10ms）和隐私保证（TinyML的完全本地处理最强）的明显梯度。'
- en: '[Figure 2.12](ch008.xhtml#fig-op_char) visualizes performance and operational
    characteristics through radar plots. Plot a) contrasts compute power and scalability
    (Cloud ML’s strengths) against latency and energy efficiency (TinyML’s advantages),
    with Edge and Mobile ML occupying intermediate positions.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.12](ch008.xhtml#fig-op_char) 通过雷达图展示了性能和操作特性。图a)对比了计算能力和可扩展性（Cloud ML的优势）与延迟和能源效率（TinyML的优势），Edge和Mobile
    ML位于中间位置。'
- en: '![](../media/file30.svg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file30.svg)'
- en: 'Figure 2.12: **ML System Trade-Offs**: Radar plots quantify performance and
    operational characteristics across cloud, edge, mobile, and Tiny ML paradigms,
    revealing inherent trade-offs between compute power, latency, energy consumption,
    and scalability. These visualizations enable informed selection of the most suitable
    deployment approach based on application-specific constraints and priorities.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '图2.12: **ML系统权衡**: 雷达图量化了云、边缘、移动和Tiny ML范式在性能和操作特性方面的差异，揭示了计算能力、延迟、能源消耗和可扩展性之间的固有权衡。这些可视化有助于根据特定应用的限制和优先级，选择最合适的部署方法。'
- en: Plot b) emphasizes operational dimensions where TinyML excels (privacy, connectivity
    independence, offline capability) versus Cloud ML’s dependency on centralized
    infrastructure and constant connectivity.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图b)强调了TinyML在操作维度上的优势（隐私、连接独立性、离线能力）与Cloud ML对集中式基础设施和持续连接的依赖。
- en: 'Development complexity varies inversely with hardware capability: Cloud and
    TinyML require deep expertise (cloud infrastructure and embedded systems respectively),
    while Mobile and Edge leverage more accessible SDKs and tooling. Cost structures
    show similar inversion: Cloud incurs ongoing operational expenses ($1000s+/month),
    Edge requires moderate upfront investment ($100s-1000s), Mobile leverages existing
    devices ($0-10s), and TinyML minimizes hardware costs ($1-10s) while demanding
    higher development investment.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 开发复杂性随着硬件能力的增加而减少：云和TinyML需要深厚的专业知识（分别对应云基础设施和嵌入式系统），而移动和边缘则利用更易获取的SDK和工具。成本结构也显示出类似的反转：云产生持续的操作费用（每月1000美元以上），边缘需要适度的前期投资（100-1000美元），移动利用现有设备（0-10美元），而TinyML最小化硬件成本（1-10美元）的同时，要求更高的开发投资。
- en: Understanding these trade-offs proves crucial for selecting appropriate deployment
    strategies that align application requirements with paradigm capabilities.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些权衡对于选择适当的部署策略至关重要，这些策略将应用需求与范式能力相一致。
- en: 'A critical pitfall in deployment selection involves choosing paradigms based
    solely on model accuracy metrics without considering system-level constraints.
    Teams often select deployment strategies by comparing model accuracy in isolation,
    overlooking critical system requirements that determine real-world viability.
    A cloud-deployed model achieving 99% accuracy becomes useless for autonomous emergency
    braking if network latency exceeds reaction time requirements. Similarly, a sophisticated
    edge model that drains a mobile device’s battery in minutes fails despite superior
    accuracy. Successful deployment requires evaluating multiple dimensions simultaneously:
    latency requirements, power budgets, network reliability, data privacy regulations,
    and total cost of ownership. Establish these constraints before model development
    to avoid expensive architectural pivots late in the project.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 部署选择中的一个关键陷阱是仅根据模型准确度指标选择范式，而不考虑系统级约束。团队通常通过单独比较模型准确度来选择部署策略，忽略了决定实际可行性的关键系统要求。如果一个云部署的模型达到99%的准确度，但网络延迟超过了反应时间要求，那么它对自动驾驶紧急制动就毫无用处。同样，一个复杂的边缘模型可能在几分钟内耗尽移动设备的电池，尽管其准确度更高。成功的部署需要同时评估多个维度：延迟要求、电源预算、网络可靠性、数据隐私法规和总拥有成本。在模型开发之前建立这些约束，以避免在项目后期进行昂贵的架构调整。
- en: Decision Framework for Deployment Selection
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署选择决策框架
- en: 'Selecting the appropriate deployment paradigm requires systematic evaluation
    of application constraints rather than organizational biases or technology trends.
    [Figure 2.13](ch008.xhtml#fig-mlsys-playbook-flowchart) provides a hierarchical
    decision framework that filters options through critical requirements: privacy
    (can data leave the device?), latency (sub-10ms response needed?), computational
    demands (heavy processing required?), and cost constraints (budget limitations?).
    This structured approach ensures deployment decisions emerge from application
    requirements, grounded in the physical constraints ([Section 2.2.1](ch008.xhtml#sec-ml-systems-deployment-paradigm-foundations-0c17))
    and quantitative comparisons ([Section 2.9](ch008.xhtml#sec-ml-systems-comparative-analysis-selection-framework-832e))
    established earlier.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的部署范式需要系统地评估应用约束，而不是组织偏见或技术趋势。[图2.13](ch008.xhtml#fig-mlsys-playbook-flowchart)提供了一个分层决策框架，通过关键要求过滤选项：隐私（数据能否离开设备？）、延迟（需要低于10毫秒的响应？）、计算需求（需要大量处理？）和成本约束（预算限制？）。这种结构化方法确保部署决策源于应用需求，基于之前建立的物理约束（[第2.2.1节](ch008.xhtml#sec-ml-systems-deployment-paradigm-foundations-0c17)）和定量比较（[第2.9节](ch008.xhtml#sec-ml-systems-comparative-analysis-selection-framework-832e)）。
- en: '![](../media/file31.svg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file31.svg)'
- en: 'Figure 2.13: **Deployment Decision Logic**: This flowchart guides selection
    of an appropriate machine learning deployment paradigm by systematically evaluating
    privacy requirements and processing constraints, ultimately balancing performance,
    cost, and data security. Navigating the decision tree helps practitioners determine
    whether cloud, edge, mobile, or tiny machine learning best suits a given application.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：**部署决策逻辑**：此流程图通过系统地评估隐私要求和处理约束，引导选择合适的机器学习部署范式，最终在性能、成本和数据安全之间取得平衡。通过导航决策树，帮助实践者确定云、边缘、移动或微型机器学习最适合特定应用。
- en: 'The framework evaluates four critical decision layers sequentially. Privacy
    constraints form the first filter, determining whether data can be transmitted
    externally. Applications handling sensitive data under GDPR, HIPAA, or proprietary
    restrictions mandate local processing, immediately eliminating cloud-only deployments.
    Latency requirements establish the second constraint through response time budgets:
    applications requiring sub-10ms response times cannot use cloud processing, as
    physics-imposed network delays alone exceed this threshold. Computational demands
    form the third evaluation layer, assessing whether applications require high-performance
    infrastructure that only cloud or edge systems provide, or whether they can operate
    within the resource constraints of mobile or tiny devices. Cost considerations
    complete the framework by balancing capital expenditure, operational expenses,
    and energy efficiency across expected deployment lifetimes.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架按顺序评估四个关键决策层。隐私限制形成第一个过滤器，确定数据是否可以外部传输。在GDPR、HIPAA或专有权限下处理敏感数据的应用程序要求本地处理，立即排除了仅限云的部署。延迟要求通过响应时间预算建立第二个限制：需要低于10毫秒响应时间的应用程序不能使用云处理，因为物理施加的网络延迟本身就超过了这个阈值。计算需求形成第三个评估层，评估应用程序是否需要只有云或边缘系统提供的高性能基础设施，或者它们是否可以在移动或微型设备的资源限制内运行。成本考虑通过在整个预期部署寿命内平衡资本支出、运营费用和能源效率来完成框架。
- en: 'Technical constraints alone prove insufficient for deployment decisions. Organizational
    factors critically shape success by determining whether teams possess the capabilities
    to implement and maintain chosen paradigms. Team expertise must align with paradigm
    requirements: Cloud ML demands distributed systems knowledge, Edge ML requires
    device management capabilities, Mobile ML needs platform-specific optimization
    skills, and TinyML requires embedded systems expertise. Organizations lacking
    appropriate skills face extended development timelines and ongoing maintenance
    challenges that undermine technical advantages. Monitoring and maintenance capabilities
    similarly determine viability at scale: edge deployments require distributed device
    orchestration, while TinyML demands specialized firmware management that many
    organizations lack. Cost structures further complicate decisions through their
    temporal patterns: Cloud incurs recurring operational expenses favorable for unpredictable
    workloads, Edge requires substantial upfront investment offset by lower ongoing
    costs, Mobile leverages user-provided devices to minimize infrastructure expenses,
    and TinyML minimizes hardware and connectivity costs while demanding significant
    development investment.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 技术限制本身并不能证明部署决策的充分性。组织因素通过决定团队是否具备实施和维护所选范例的能力，对成功起着至关重要的作用。团队的专业技能必须与范例要求相一致：云机器学习需要分布式系统知识，边缘机器学习需要设备管理能力，移动机器学习需要特定平台的优化技能，而微型机器学习需要嵌入式系统专业知识。缺乏适当技能的组织将面临延长的发展周期和持续的维护挑战，从而削弱了技术优势。监控和维护能力同样通过其时间模式决定大规模的可行性：边缘部署需要分布式设备编排，而微型机器学习需要许多组织缺乏的专业固件管理。成本结构通过其时间模式进一步复杂化决策：云产生持续的经营费用，有利于不可预测的工作负载，边缘需要大量的前期投资，但通过较低的持续成本得到补偿，移动利用用户提供的设备以最小化基础设施费用，而微型机器学习最小化硬件和连接成本，同时要求显著的开发投资。
- en: Successful deployment emerges from balancing technical optimization against
    organizational capability. Paradigm selection represents systems engineering challenges
    that extend well beyond pure technical requirements, encompassing team skills,
    operational capacity, and economic constraints. These decisions remain constrained
    by fundamental scaling laws explored in [Section 9.3](ch015.xhtml#sec-efficient-ai-ai-scaling-laws-a043),
    with operational aspects detailed in [Chapter 13](ch019.xhtml#sec-ml-operations)
    and benchmarking approaches covered in [Chapter 12](ch018.xhtml#sec-benchmarking-ai).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的部署来自于平衡技术优化与组织能力。范例选择代表了系统工程挑战，这些挑战远远超出了纯粹的技术要求，包括团队技能、运营能力和经济限制。这些决策仍然受到在[第9.3节](ch015.xhtml#sec-efficient-ai-ai-scaling-laws-a043)中探讨的基本扩展定律的限制，运营方面在[第13章](ch019.xhtml#sec-ml-operations)中详细说明，而基准测试方法在[第12章](ch018.xhtml#sec-benchmarking-ai)中介绍。
- en: Fallacies and Pitfalls
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Understanding deployment paradigms requires recognizing common misconceptions
    that can lead to poor architectural decisions. These fallacies often stem from
    oversimplified thinking about the core trade-offs governing ML systems design.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 理解部署范式需要识别可能导致不良架构决策的常见误解。这些谬误通常源于对指导机器学习系统设计的核心权衡的过度简化思考。
- en: '**Fallacy: “One Paradigm Fits All”** - The most pervasive misconception assumes
    that one deployment approach can solve all ML problems. Teams often standardize
    on cloud, edge, or mobile solutions without considering application-specific constraints.
    This fallacy ignores the physics-imposed boundaries discussed in [Section 2.2.1](ch008.xhtml#sec-ml-systems-deployment-paradigm-foundations-0c17).
    Real-time robotics cannot tolerate cloud latency, while complex language models
    exceed tiny device capabilities. Effective systems often require hybrid architectures
    that leverage multiple paradigms strategically.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：“一种范式适用于所有”** - 最普遍的误解是认为一种部署方法可以解决所有机器学习问题。团队往往在云、边缘或移动解决方案上标准化，而不考虑特定应用的约束。这种谬误忽略了在[第2.2.1节](ch008.xhtml#sec-ml-systems-deployment-paradigm-foundations-0c17)中讨论的物理限制。实时机器人无法容忍云延迟，而复杂的语言模型超出了小型设备的处理能力。有效的系统通常需要混合架构，战略性地利用多个范式。'
- en: '**Fallacy: “Edge Computing Always Reduces Latency”** - Many practitioners assume
    edge deployment automatically improves response times. However, edge systems introduce
    processing delays, load balancing overhead, and potential network hops that can
    exceed direct cloud connections. A poorly designed edge deployment with insufficient
    local compute power may exhibit worse latency than optimized cloud services. Edge
    benefits emerge only when local processing time plus reduced network distance
    outweighs the infrastructure complexity costs.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：“边缘计算总是减少延迟”** - 许多从业者认为边缘部署会自动提高响应时间。然而，边缘系统引入了处理延迟、负载平衡开销和可能的网络跳跃，这些可能超过直接云连接。设计不良、本地计算能力不足的边缘部署可能表现出比优化云服务更差的延迟。只有当本地处理时间加上减少的网络距离超过基础设施复杂性成本时，边缘优势才会显现。'
- en: '**Fallacy: “Mobile Devices Can Handle Any Workload with Optimization”** - This
    misconception underestimates the fundamental constraints imposed by battery life
    and thermal management. Teams often assume that model compression techniques can
    arbitrarily reduce resource requirements while maintaining performance. However,
    mobile devices face hard physical limits: battery capacity scales with volume
    while computational demand scales with model complexity. Some applications require
    computational resources that no amount of optimization can fit within mobile power
    budgets.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：“移动设备可以通过优化处理任何工作负载”** - 这种误解低估了电池寿命和热管理带来的基本限制。团队常常假设模型压缩技术可以任意减少资源需求，同时保持性能。然而，移动设备面临硬性物理限制：电池容量与体积成正比，而计算需求与模型复杂性成正比。某些应用需要的计算资源，任何优化都无法在移动电源预算内适应。'
- en: '**Fallacy: “Tiny ML is Just Smaller Mobile ML”** - This fallacy misunderstands
    the qualitative differences between resource-constrained paradigms. Tiny ML operates
    under constraints so severe that different algorithmic approaches become necessary.
    The microcontroller environments impose memory limitations measured in kilobytes,
    not megabytes, requiring specialized techniques like quantization beyond what
    mobile optimization employs. Applications suitable for tiny ML represent a fundamentally
    different problem class, not simply scaled-down versions of mobile applications.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：“小型机器学习只是更小的移动机器学习”** - 这种谬误误解了资源受限范式之间的定性差异。小型机器学习在如此严格的限制下运行，以至于需要不同的算法方法。微控制器环境施加的内存限制是以千字节计，而不是兆字节，需要使用量化等专门技术，这超出了移动优化的范畴。适用于小型机器学习的应用代表了一个根本不同的问题类别，而不仅仅是移动应用的缩小版。'
- en: '**Fallacy: “Cost Optimization Equals Resource Minimization”** - Teams frequently
    assume that minimizing computational resources automatically reduces costs. This
    perspective ignores operational complexity, development time, and infrastructure
    overhead. Cloud deployments may consume more compute resources while providing
    lower total cost of ownership through reduced maintenance, automatic scaling,
    and shared infrastructure. The optimal cost solution often involves accepting
    higher per-unit resource consumption in exchange for simplified operations and
    faster development cycles.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：“成本优化等于资源最小化”** - 团队经常假设最小化计算资源会自动降低成本。这种观点忽略了运营复杂性、开发时间和基础设施开销。云部署可能会消耗更多的计算资源，但通过降低维护成本、自动扩展和共享基础设施，提供更低的总体拥有成本。最佳成本解决方案通常涉及接受更高的单位资源消耗，以换取简化操作和更快的开发周期。'
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter analyzed the diverse landscape of machine learning systems, revealing
    how deployment context directly shapes every aspect of system design. From cloud
    environments with vast computational resources to tiny devices operating under
    extreme constraints, each paradigm presents unique opportunities and challenges
    that directly influence architectural decisions, algorithmic choices, and performance
    trade-offs. The spectrum from cloud to edge to mobile to tiny ML represents more
    than just different scales of computation; it reflects a significant evolution
    in how we distribute intelligence across computing infrastructure.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分析了机器学习系统的多样化景观，揭示了部署环境如何直接影响系统设计的各个方面。从拥有大量计算资源的云环境到在极端限制下运行的微型设备，每个范式都提供了独特的机遇和挑战，这些机遇和挑战直接影响架构决策、算法选择和性能权衡。从云到边缘到移动再到微型机器学习的谱系，不仅仅代表了计算规模的不同，它反映了我们在计算基础设施上分布智能的重大演变。
- en: 'The evolution from centralized cloud systems to distributed edge and mobile
    deployments shows how resource constraints drive innovation rather than simply
    limiting capabilities. Each paradigm emerged to address specific limitations of
    its predecessors: Cloud ML leverages centralized power for complex processing
    but must navigate latency and privacy concerns. Edge ML brings computation closer
    to data sources, reducing latency while introducing intermediate resource constraints.
    Mobile ML extends these capabilities to personal devices, balancing user experience
    with battery life and thermal management. Tiny ML pushes the boundaries of what’s
    possible with minimal resources, enabling ubiquitous sensing and intelligence
    in previously impossible deployment contexts. This evolution showcases how thoughtful
    system design can transform limitations into opportunities for specialized optimization.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从集中式云系统到分布式边缘和移动部署的演变展示了资源限制如何推动创新，而不仅仅是限制能力。每个范式都是为了解决其前辈的具体限制而出现的：云机器学习利用集中式力量进行复杂处理，但必须应对延迟和隐私问题。边缘机器学习将计算更靠近数据源，减少延迟，同时引入中间资源限制。移动机器学习将这些能力扩展到个人设备，平衡用户体验与电池寿命和热管理。微型机器学习将资源推向极限，使无处不在的感知和智能成为以前不可能部署环境中的可能。这一演变展示了深思熟虑的系统设计如何将限制转化为专门优化的机会。
- en: '**Key Takeaways**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Deployment context drives architectural decisions more than algorithmic preferences
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署环境比算法偏好更多地驱动架构决策
- en: Resource constraints create opportunities for innovation, not just limitations
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源限制创造了创新的机会，而不仅仅是限制
- en: Hybrid approaches are emerging as the future of ML system design
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合方法正在成为机器学习系统设计的未来
- en: Privacy and latency considerations increasingly favor distributed intelligence
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私和延迟考虑因素越来越有利于分布式智能
- en: 'These paradigms reflect an ongoing shift toward systems that are finely tuned
    to specific operational requirements, moving beyond one-size-fits-all approaches
    toward context-aware system design. As these deployment models mature, hybrid
    architectures emerge that combine their strengths: cloud-based training paired
    with edge inference, federated learning across mobile devices, and hierarchical
    processing that optimizes across the entire spectrum. This evolution demonstrates
    how deployment contexts will continue driving innovation in system architecture,
    training methodologies, and optimization techniques, creating more sophisticated
    and context-aware ML systems.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些范例反映了向针对特定操作需求精细调优的系统转变的趋势，超越了“一刀切”的方法，转向了情境感知的系统设计。随着这些部署模型成熟，混合架构应运而生，结合了它们的优点：基于云的训练与边缘推理相结合，跨移动设备的联邦学习，以及优化整个频谱的分层处理。这一演变展示了部署情境将继续推动系统架构、训练方法和优化技术的创新，创造更加复杂和情境感知的机器学习系统。
- en: Yet deployment context represents only one dimension of system design. The algorithms
    executing within these environments equally influence resource requirements, computational
    patterns, and optimization strategies. A neural network requiring gigabytes of
    memory and billions of floating-point operations demands fundamentally different
    deployment approaches than a decision tree requiring kilobytes and integer comparisons.
    The next chapter ([Chapter 3](ch009.xhtml#sec-dl-primer)) examines the mathematical
    foundations of neural networks, revealing why certain deployment paradigms suit
    specific algorithms and how algorithmic choices propagate through the entire system
    stack.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，部署情境只是系统设计的一个维度。在这些环境中执行的计算算法同样影响资源需求、计算模式和优化策略。一个需要数GB内存和数十亿浮点运算的神经网络，与一个只需要数KB和整数比较的决策树相比，需要根本不同的部署方法。下一章（[第3章](ch009.xhtml#sec-dl-primer)）将探讨神经网络的数学基础，揭示为什么某些部署范例适合特定的算法，以及算法选择如何在整个系统堆栈中传播。
- en: '* * *'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
