<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.3 Training neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.3 Training neural networks</h1>
<blockquote>åŽŸæ–‡ï¼š<a href="https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html">https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html</a></blockquote>
                                
                                
<blockquote>
<p>ðŸŒ³ <strong>Tip</strong> ðŸŒ³<br/>
For more tips on training neural networks, check out:</p>
<ul>
<li><a href="http://karpathy.github.io/2019/04/25/recipe/" target="_blank">A Recipe for Training Neural Networks</a> (Karpathy 2019)</li>
<li><a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/" target="_blank">NLP's Clever Hans Moment has Arrived</a> (Heinzerling 2019): an excellent writeup on trying to understand what exactly your neural network learns, and techniques to ensure that your model works correctly with textual data.</li>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank">An overview of gradient descent optimization algorithms</a> (Ruder 2016)</li>
</ul>
</blockquote>
<ol>
<li>[E] When building a neural network, should you overfit or underfit it first?</li>
<li>[E] Write the vanilla gradient update.</li>
<li>Neural network in simple Numpy.<ol>
<li>[E] Write in plain NumPy the forward and backward pass for a two-layer feed-forward neural network with a ReLU layer in between.</li>
<li>[M] Implement vanilla dropout for the forward and backward pass in NumPy.</li>
</ol>
</li>
<li>Activation functions.<ol>
<li>[E] Draw the graphs for sigmoid, tanh, ReLU, and leaky ReLU.</li>
<li>[E] Pros and cons of each activation function.</li>
<li>[E] Is ReLU differentiable? What to do when itâ€™s not differentiable?</li>
<li>[M] Derive derivatives for sigmoid function <script type="math/tex; ">\sigma(x)</script> when <script type="math/tex; ">x</script> is a vector.</li>
</ol>
</li>
<li>[E] Whatâ€™s the motivation for skip connection in neural works?</li>
<li>Vanishing and exploding gradients.<ol>
<li>[E] How do we know that gradients are exploding? How do we prevent it?</li>
<li>[E] Why are RNNs especially susceptible to vanishing and exploding gradients?</li>
</ol>
</li>
<li>[M] Weight normalization separates a weight vectorâ€™s norm from its gradient. How would it help with training?</li>
<li>[M] When training a large neural network, say a language model with a billion parameters, you evaluate your model on a validation set at the end of every epoch. You realize that your validation loss is often lower than your train loss. What might be happening?</li>
<li>[E] What criteria would you use for early stopping?</li>
<li>[E] Gradient descent vs SGD vs mini-batch SGD.</li>
<li>[H] Itâ€™s a common practice to train deep learning models using epochs: we sample batches from data <strong>without</strong> replacement. Why would we use epochs instead of just sampling data <strong>with</strong> replacement?</li>
<li>[M] Your modelâ€™ weights fluctuate a lot during training. How does that affect your modelâ€™s performance? What to do about it?</li>
<li>Learning rate.<ol>
<li>[E] Draw a graph number of training epochs vs training error for when the learning rate is:<ol>
<li>too high</li>
<li>too low</li>
<li>acceptable.</li>
</ol>
</li>
<li>[E] Whatâ€™s learning rate warmup? Why do we need it?</li>
</ol>
</li>
<li>[E] Compare batch norm and layer norm.</li>
<li>[M] Why is squared L2 norm sometimes preferred to L2 norm for regularizing neural networks?</li>
<li>[E] Some models use weight decay: after each gradient update, the weights are multiplied by a factor slightly less than 1. What is this useful for?</li>
<li>Itâ€™s a common practice for the learning rate to be reduced throughout the training.<ol>
<li>[E] Whatâ€™s the motivation?</li>
<li>[M] What might be the exceptions?</li>
</ol>
</li>
<li>Batch size.<ol>
<li>[E] What happens to your model training when you decrease the batch size to 1?</li>
<li>[E] What happens when you use the entire training data in a batch?</li>
<li>[M] How should we adjust the learning rate as we increase or decrease the batch size?</li>
</ol>
</li>
<li>[M] Why is Adagrad sometimes favored in problems with sparse gradients?</li>
<li>Adam vs. SGD.<ol>
<li>[M] What can you say about the ability to converge and generalize of Adam vs. SGD?</li>
<li>[M] What else can you say about the difference between these two optimizers? </li>
</ol>
</li>
<li>[M] With model parallelism, you might update your model weights using the gradients from each machine asynchronously or synchronously. What are the pros and cons of asynchronous SGD vs. synchronous SGD?</li>
<li>[M] Why shouldnâ€™t we have two consecutive linear layers in a neural network?</li>
<li>[M] Can a neural network with only RELU (non-linearity) act as a linear classifier?</li>
<li>[M] Design the smallest neural network that can function as an XOR gate.</li>
<li>[E] Why donâ€™t we just initialize all weights in a neural network to zero?</li>
<li>Stochasticity.<ol>
<li>[M] What are some sources of randomness in a neural network?</li>
<li>[M] Sometimes stochasticity is desirable when training neural networks. Why is that?</li>
</ol>
</li>
<li>Dead neuron.<ol>
<li>[E] Whatâ€™s a dead neuron?</li>
<li>[E] How do we detect them in our neural network?</li>
<li>[M] How to prevent them?</li>
</ol>
</li>
<li>Pruning.<ol>
<li>[M] Pruning is a popular technique where certain weights of a neural network are set to 0. Why is it desirable?</li>
<li>[M] How do you choose what to prune from a neural network?</li>
</ol>
</li>
<li>[H] Under what conditions would it be possible to recover training data from the weight checkpoints?</li>
<li>[H] Why do we try to reduce the size of a big trained model through techniques such as knowledge distillation instead of just training a small model from the beginning?</li>
</ol>
<hr/>
<p><em>This book was created by <a href="https://huyenchip.com" target="_blank">Chip Huyen</a> with the help of wonderful friends. For feedback, errata, and suggestions, the author can be reached <a href="https://huyenchip.com/communication/" target="_blank">here</a>. Copyright Â©2021 Chip Huyen.</em></p>

                                
                                    
</body>
</html>