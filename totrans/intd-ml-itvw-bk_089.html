<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>7.3 Objective functions, metrics, and evaluation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7.3 Objective functions, metrics, and evaluation</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html">https://huyenchip.com/ml-interviews-book/contents/7.3-objective-functions,-metrics,-and-evaluation.html</a></blockquote>
                                
                                
<ol>
<li>Convergence.<ol>
<li>[E] When we say an algorithm converges, what does convergence mean?</li>
<li>[E] How do we know when a model has converged?</li>
</ol>
</li>
<li>[E] Draw the loss curves for overfitting and underfitting.</li>
<li>Bias-variance trade-off<ol>
<li>[E] What’s the bias-variance trade-off?</li>
<li>[M] How’s this tradeoff related to overfitting and underfitting?</li>
<li>[M] How do you know that your model is high variance, low bias? What would you do in this case?</li>
<li>[M] How do you know that your model is low variance, high bias? What would you do in this case?</li>
</ol>
</li>
<li>Cross-validation.<ol>
<li>[E] Explain different methods for cross-validation.</li>
<li>[M] Why don’t we see more cross-validation in deep learning? </li>
</ol>
</li>
<li><p>Train, valid, test splits.</p>
<ol>
<li>[E] What’s wrong with training and testing a model on the same data?</li>
<li>[E] Why do we need a validation set on top of a train set and a test set?</li>
<li>[M] Your model’s loss curves on the train, valid, and test sets look like this. What might have been the cause of this? What would you do?</li>
</ol>
<center>
 <img src="../Images/d5852d1412ffaf6b2f4afe31c459b3f2.png" width="60%" alt="Problematic loss curves" title="image_tooltip" data-original-src="https://huyenchip.com/ml-interviews-book/contents/images/image25.png"/>
</center>
</li>
<li><p>[E] Your team is building a system to aid doctors in predicting whether a patient has cancer or not from their X-ray scan. Your colleague announces that the problem is solved now that they’ve built a system that can predict with 99.99% accuracy. How would you respond to that claim?</p>
</li>
<li>F1 score.<ol>
<li>[E] What’s the benefit of F1 over the accuracy?</li>
<li>[M] Can we still use F1 for a problem with more than two classes. How?</li>
</ol>
</li>
<li><p>Given a binary classifier that outputs the following confusion matrix.</p>
<table>
 <tr>
  <td>
  </td>
  <td>
Predicted True
  </td>
  <td>Predicted False
  </td>
 </tr>
 <tr>
  <td>Actual True
  </td>
  <td>30
  </td>
  <td>20
  </td>
 </tr>
 <tr>
  <td>Actual False
  </td>
  <td>5
  </td>
  <td>40
  </td>
 </tr>
</table>

<ol>
<li>[E] Calculate the model’s precision, recall, and F1.</li>
<li>[M] What can we do to improve the model’s performance?</li>
</ol>
</li>
<li>Consider a classification where 99% of data belongs to class A and 1% of data belongs to class B.<ol>
<li>[M] If your model predicts A 100% of the time, what would the F1 score be? <strong>Hint</strong>: The F1 score when A is mapped to 0 and B to 1 is different from the F1 score when A is mapped to 1 and B to 0.</li>
<li>[M] If we have a model that predicts A and B at a random (uniformly), what would the expected F1 be?</li>
</ol>
</li>
<li>[M] For logistic regression, why is log loss recommended over MSE (mean squared error)?</li>
<li>[M] When should we use RMSE (Root Mean Squared Error) over MAE (Mean Absolute Error) and vice versa?</li>
<li>[M] Show that the negative log-likelihood and cross-entropy are the same for binary classification tasks.</li>
<li>[M] For classification tasks with more than two labels (e.g. MNIST with 10 labels), why is cross-entropy a better loss function than MSE?</li>
<li>[E] Consider a language with an alphabet of 27 characters. What would be the maximal entropy of this language?</li>
<li>[E] A lot of machine learning models aim to approximate probability distributions. Let’s say P is the distribution of the data and Q is the distribution learned by our model. How do measure how close Q is to P?</li>
<li>MPE (Most Probable Explanation) vs. MAP (Maximum A Posteriori)<ol>
<li>[E] How do MPE and MAP differ?</li>
<li>[H] Give an example of when they would produce different results.</li>
</ol>
</li>
<li><p>[E] Suppose you want to build a model to predict the price of a stock in the next 8 hours and that the predicted price should never be off more than 10% from the actual price. Which metric would you use?</p>
<p> <strong>Hint</strong>: check out MAPE.</p>
</li>
</ol>
<hr/>
<blockquote>
<p>In case you need a refresh on information entropy, here's an explanation without any math.</p>
<p>Your parents are finally letting you adopt a pet! They spend the entire weekend taking you to various pet shelters to find a pet.</p>
<p>The first shelter has only dogs. Your mom covers your eyes when your dad picks out an animal for you. You don't need to open your eyes to know that this animal is a dog. It isn't hard to guess.</p>
<p>The second shelter has both dogs and cats. Again your mom covers your eyes and your dad picks out an email. This time, you have to think harder to guess which animal is that. You make a guess that it's a dog, and your dad says no. So you guess it's a cat and you're right. It takes you two guesses to know for sure what animal it is.</p>
<p>The next shelter is the biggest one of them all. They have so many different kinds of animals: dogs, cats, hamsters, fish, parrots, cute little pigs, bunnies, ferrets, hedgehogs, chickens, even the exotic bearded dragons! There must be close to a hundred different types of pets. Now it's really hard for you to guess which one your dad brings you. It takes you a dozen guesses to guess the right animal.</p>
<p>Entropy is a measure of the "spread out" in diversity. The more spread out the diversity, the header it is to guess an item correctly. The first shelter has very low entropy. The second shelter has a little bit higher entropy. The third shelter has the highest entropy.</p>
</blockquote>

                                
                                    
</body>
</html>