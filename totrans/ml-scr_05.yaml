- en: Concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c1/concept.html](https://dafriedman97.github.io/mlbook/content/c1/concept.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\btheta}{\boldsymbol{\theta}}
    \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}} \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}}
    \newcommand{\dadb}[2]{\frac{\partial #1}{\partial #2}} \newcommand{\by}{\mathbf{y}}
    \newcommand{\bX}{\mathbf{X}} \newcommand{\bphi}{\boldsymbol{\phi}} \newcommand{\bPhi}{\boldsymbol{\Phi}}
    \]'
  prefs: []
  type: TYPE_NORMAL
- en: Model Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Linear regression* is a relatively simple method that is extremely widely-used.
    It is also a great stepping stone for more sophisticated methods, making it a
    natural algorithm to study first.'
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, the target variable \(y\) is assumed to follow a linear
    function of one or more predictor variables, \(x_1, \dots, x_D\), plus some random
    error. Specifically, we assume the model for the \(n^\text{th}\) observation in
    our sample is of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(\beta_0\) is the intercept term, \(\beta_1\) through \(\beta_D\) are
    the coefficients on our feature variables, and \(\epsilon\) is an error term that
    represents the difference between the true \(y\) value and the linear function
    of the predictors. Note that the terms with an \(n\) in the subscript differ between
    observations while the terms without (namely the \(\beta\text{s}\)) do not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The math behind linear regression often becomes easier when we use vectors
    to represent our predictors and coefficients. Let’s define \(\bx_n\) and \(\bbeta\)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(\bx_n\) includes a leading 1, corresponding to the intercept term
    \(\beta_0\). Using these definitions, we can equivalently express \(y_n\) as
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of a dataset designed for linear regression. The input variable
    is generated randomly and the target variable is generated as a linear combination
    of that input variable plus an error term.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameter Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section covers the entire structure we assume our data follows
    in linear regression. The machine learning task is then to estimate the parameters
    in \(\bbeta\). These estimates are represented by \(\hat{\beta}_0, \dots, \hat{\beta}_D\)
    or \(\bbetahat\). The estimates give us *fitted values* for our target variable,
    represented by \(\hat{y}_n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'This task can be accomplished in two ways which, though slightly different
    conceptually, are identical mathematically. The first approach is through the
    lens of *minimizing loss*. A common practice in machine learning is to choose
    a loss function that defines how well a model with a given set of parameter estimates
    the observed data. The most common loss function for linear regression is squared
    error loss. This says the *loss* of our model is proportional to the sum of squared
    differences between the true \(y_n\) values and the fitted values, \(\hat{y}_n\).
    We then *fit* the model by finding the estimates \(\bbetahat\) that minimize this
    loss function. This approach is covered in the subsection [Approach 1: Minimizing
    Loss](s1/loss_minimization.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The second approach is through the lens of *maximizing likelihood*. Another
    common practice in machine learning is to model the target as a random variable
    whose distribution depends on one or more parameters, and then find the parameters
    that maximize its likelihood. Under this approach, we will represent the target
    with \(Y_n\) since we are treating it as a random variable. The most common model
    for \(Y_n\) in linear regression is a Normal random variable with mean \(E(Y_n)
    = \bbeta^\top \bx_n\). That is, we assume
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
  prefs: []
  type: TYPE_NORMAL
- en: 'and we find the values of \(\bbetahat\) to maximize the likelihood. This approach
    is covered in subsection [Approach 2: Maximizing Likelihood](s1/likelihood_maximization.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve estimated \(\bbeta\), our model is *fit* and we can make predictions.
    The below graph is the same as the one above but includes our estimated line-of-best-fit,
    obtained by calculating \(\hat{\beta}_0\) and \(\hat{\beta}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Extensions of Ordinary Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many important extensions to linear regression which make the model
    more flexible. Those include [Regularized Regression](../c2/s1/regularized.html)—which
    balances the bias-variance tradeoff for high-dimensional regression models—[Bayesian
    Regression](../c2/s1/bayesian.html)—which allows for prior distributions on the
    coefficients—and [GLMs](../c2/s1/GLMs.html)—which introduce non-linearity to regression
    models. These extensions are discussed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Model Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Linear regression* is a relatively simple method that is extremely widely-used.
    It is also a great stepping stone for more sophisticated methods, making it a
    natural algorithm to study first.'
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, the target variable \(y\) is assumed to follow a linear
    function of one or more predictor variables, \(x_1, \dots, x_D\), plus some random
    error. Specifically, we assume the model for the \(n^\text{th}\) observation in
    our sample is of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_n = \beta_0 + \beta_1 x_{n1} + \dots + \beta_Dx_{nD} + \epsilon_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(\beta_0\) is the intercept term, \(\beta_1\) through \(\beta_D\) are
    the coefficients on our feature variables, and \(\epsilon\) is an error term that
    represents the difference between the true \(y\) value and the linear function
    of the predictors. Note that the terms with an \(n\) in the subscript differ between
    observations while the terms without (namely the \(\beta\text{s}\)) do not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The math behind linear regression often becomes easier when we use vectors
    to represent our predictors and coefficients. Let’s define \(\bx_n\) and \(\bbeta\)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{align} \bx_n &= \begin{pmatrix} 1 & x_{n1} & \dots &
    x_{nD} \end{pmatrix}^\top \\ \bbeta &= \begin{pmatrix} \beta_0 & \beta_1 & \dots
    & \beta_D \end{pmatrix}^\top. \end{align} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that \(\bx_n\) includes a leading 1, corresponding to the intercept term
    \(\beta_0\). Using these definitions, we can equivalently express \(y_n\) as
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_n = \bbeta^\top \bx_n + \epsilon_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of a dataset designed for linear regression. The input variable
    is generated randomly and the target variable is generated as a linear combination
    of that input variable plus an error term.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/concept_2_0.png](../Images/60ca0e10dab7626e0f855f43f7f346e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameter Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous section covers the entire structure we assume our data follows
    in linear regression. The machine learning task is then to estimate the parameters
    in \(\bbeta\). These estimates are represented by \(\hat{\beta}_0, \dots, \hat{\beta}_D\)
    or \(\bbetahat\). The estimates give us *fitted values* for our target variable,
    represented by \(\hat{y}_n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'This task can be accomplished in two ways which, though slightly different
    conceptually, are identical mathematically. The first approach is through the
    lens of *minimizing loss*. A common practice in machine learning is to choose
    a loss function that defines how well a model with a given set of parameter estimates
    the observed data. The most common loss function for linear regression is squared
    error loss. This says the *loss* of our model is proportional to the sum of squared
    differences between the true \(y_n\) values and the fitted values, \(\hat{y}_n\).
    We then *fit* the model by finding the estimates \(\bbetahat\) that minimize this
    loss function. This approach is covered in the subsection [Approach 1: Minimizing
    Loss](s1/loss_minimization.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The second approach is through the lens of *maximizing likelihood*. Another
    common practice in machine learning is to model the target as a random variable
    whose distribution depends on one or more parameters, and then find the parameters
    that maximize its likelihood. Under this approach, we will represent the target
    with \(Y_n\) since we are treating it as a random variable. The most common model
    for \(Y_n\) in linear regression is a Normal random variable with mean \(E(Y_n)
    = \bbeta^\top \bx_n\). That is, we assume
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y_n|\bx_n \sim \mathcal{N}(\bbeta^\top \bx_n, \sigma^2), \]
  prefs: []
  type: TYPE_NORMAL
- en: 'and we find the values of \(\bbetahat\) to maximize the likelihood. This approach
    is covered in subsection [Approach 2: Maximizing Likelihood](s1/likelihood_maximization.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve estimated \(\bbeta\), our model is *fit* and we can make predictions.
    The below graph is the same as the one above but includes our estimated line-of-best-fit,
    obtained by calculating \(\hat{\beta}_0\) and \(\hat{\beta}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/concept_4_0.png](../Images/c220c51daba256b0e1531ca574fb37aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Extensions of Ordinary Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many important extensions to linear regression which make the model
    more flexible. Those include [Regularized Regression](../c2/s1/regularized.html)—which
    balances the bias-variance tradeoff for high-dimensional regression models—[Bayesian
    Regression](../c2/s1/bayesian.html)—which allows for prior distributions on the
    coefficients—and [GLMs](../c2/s1/GLMs.html)—which introduce non-linearity to regression
    models. These extensions are discussed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
