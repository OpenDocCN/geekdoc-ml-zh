<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Training and Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Training and Tuning</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_training_tuning.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
¬© Copyright 2024.</p>
<p>This chapter is a summary of <strong>Machine Learning Training and Tuning</strong> including essential concepts:</p>
<ul class="simple">
<li><p>Model Parameter Training and Hyperparameter Tuning</p></li>
<li><p>Model Goodness Metrics</p></li>
<li><p>Cross Validation Workflows</p></li>
<li><p>Limitations of Cross Validation</p></li>
</ul>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/owOSiKT3K8E?si=PrY5lL4Dbi2Ix7fu">Training and Testing</a>.</p></li>
<li><p><a class="reference external" href="https://youtu.be/g38sEpFOX-0?si=XPC18zNMCxaIZCOF">Model Goodness Metrics</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/FiX8IWPhTcg?si=K4A3W0zTaiypm7n7">Cross Validation Considerations</a></p></li>
</ul>
<p>For your convenience here‚Äôs a summary of salient points.</p>
<section id="training-and-tuning-predictive-machine-learning-models">
<h2>Training and Tuning Predictive Machine Learning Models</h2>
<p>In predictive machine learning, we follow a standard model training and testing workflow. This process ensures that our model generalizes well to new data, rather than just fitting the training data perfectly.</p>
<figure style="text-align: center;">
  <img src="../Images/31abfecc21c246ba7144dfb847ab4378.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/ML_workflow_portrait.png"/>
  <figcaption style="text-align: center;">Standard predictive machine learning modeling workflow.</figcaption>
</figure>
<p>Let‚Äôs walk through the key steps,</p>
<ol class="arabic simple">
<li><p><strong>Train and Test Split</strong> - divide the available data into mutually exclusive, exhaustive subsets: a training set and a testing set.</p></li>
</ol>
<ul class="simple">
<li><p>typically, 15%‚Äì30% of the data is held out for testing</p></li>
<li><p>the remaining 70%‚Äì85% is used for training the model</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Define a range of hyperparameter(s)</strong> values to explore, ranging from,</p></li>
</ol>
<ul class="simple">
<li><p>simple models with low flexibility</p></li>
<li><p>to complex models with high flexibility</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> This step may involve tuning multiple hyperparameters, in which case efficient sampling methods (e.g., grid search, random search, or Bayesian optimization) are often used.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Train Model Parameters for Each Hyperparameter Setting</strong> - for each set of hyperparameters, train a model on the training data. This yields:</p></li>
</ol>
<ul class="simple">
<li><p>a suite of trained models, each with different complexity</p></li>
<li><p>each model has parameters optimized to minimize error on the training data</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Evaluate Each Model on the Withheld Testing Data</strong> - using the testing data,</p></li>
</ol>
<ul class="simple">
<li><p>evaluate how each trained model performs on unseen data</p></li>
<li><p>summarize prediction error (for example, root mean square error (RMSE), mean absolute error (MAE), classification accuracy) for each model</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p><strong>Select the Hyperparameters That Minimize Test Error</strong> - this is the hyperparameter tuning step:</p></li>
</ol>
<ul class="simple">
<li><p>choose the model hyperparaemter(s) that performs best on the test data</p></li>
<li><p>these are your tuned hyperparameters</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p><strong>Retrain the Final Model on All Data Using Tuned Hyperparameters</strong> - now that the best model complexity has been identified,</p></li>
</ol>
<ul class="simple">
<li><p>retrain the model using both the training and test sets</p></li>
<li><p>this maximizes the amount of data used for final model parameter estimation</p></li>
<li><p>the resulting model is the one you deploy in real-world applications</p></li>
</ul>
</section>
<section id="common-questions-about-the-model-training-and-tuning-workflow">
<h2>Common Questions About the Model Training and Tuning Workflow</h2>
<p>As a professor, I often hear these questions when I introduce the above machine learning model training and tuning workflow.</p>
<ul class="simple">
<li><p><strong>What is the main outcome of steps 1‚Äì5?</strong> - the only reliable outcome is the tuned hyperparameters.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> we do not use the model trained in step 3 or 4 directly, because it was trained without access to all available data. Instead, we retrain the final model using all data with the selected hyperparameters.</p>
<ul class="simple">
<li><p><strong>Why not train the model on all the data from the beginning?</strong> - because if we do that, we have no independent way to evaluate the model‚Äôs generalization. A very complex model can easily overfit‚Äîfitting the training data perfectly, but performing poorly on new, unseen data.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> overfitting happens when model flexibility is too high‚Äîit captures noise instead of the underlying pattern. Without a withheld test set, we can‚Äôt detect this.</p>
<p>This workflow for training and tuning predictive machine learning models is,</p>
<ul class="simple">
<li><p>an empirical, cross-validation-based process</p></li>
<li><p>a practical simulation of real-world model use</p></li>
<li><p>a method to identify the model complexity that best balances fit and generalization</p></li>
</ul>
<p>I‚Äôve said model parameters and model hyperparameters a bunch of times, so I owe you their definitions.</p>
</section>
<section id="model-parameters-and-model-hyperparameters">
<h2>Model Parameters and Model Hyperparameters</h2>
<p><strong>Model parameters</strong> are fit during training phase to minimize error at the training data, i.e., model parameters are trained with training data and control model fit to the data. For example,</p>
<ul class="simple">
<li><p>for the polynomial predictive machine learning model from the machine learning workflow example above, the model parameters are the polynomial coefficients, e.g., <span class="math notranslate nohighlight">\(b_3\)</span>, <span class="math notranslate nohighlight">\(b_2\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (often called <span class="math notranslate nohighlight">\(b_0\)</span>) for the third order polynomial model.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/85cce1589249c12a11ff52ea10e6c893.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/parameters.png"/>
  <figcaption style="text-align: center;">Model parameters are adjusted to fit of the model to the data, i.e., model parameters are trained to minimize error over the training data (x markers).</figcaption>
</figure>
<p><strong>Model hyperparameters</strong> are very different. They do not constrain the model fit to the data directly, instead they constrain the model complexity.
The model hyperparameters are selected (call tuning) to minimize error at the withheld testing data. Going back to our polynomial predictive machine learning example, the choice of polynomial order is the model hyperparameter.</p>
<figure style="text-align: center;">
  <img src="../Images/aef7a81bd36cd7f2b25e1574784b4932.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/hyperparameters.png"/>
  <figcaption style="text-align: center;">Model hyperparameters are adjusted to change the model complexity / flexibility, i.e., model hyperparameters are tuned to minimize error over the withheld testing data (solid circles).</figcaption>
</figure>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Model parameters vs. model hyperparameters</p>
<p>Model parameters control the model fit and are trained with training data. Model hyperparameters control the model complexity and are tuned with testing data.</p>
</div>
</section>
<section id="regression-and-classification">
<h2>Regression and Classification</h2>
<p>Before we proceed, we need to define regression and classification.</p>
<ul class="simple">
<li><p><strong>Regression</strong> - a predictive machine learning model where the response feature(s) is continuous.</p></li>
<li><p><strong>Classification</strong> - a predictive machine learning model where the response feature(s) is categorical.</p></li>
</ul>
<p>It turns out that for each of these we need to build different models and use different methods to score these models.</p>
<ul class="simple">
<li><p>for the remainder of this discussion we will focus on regression, but in later chapters we introduce classification models as well.</p></li>
</ul>
<p>Now, to better understand predictive machine learning model tuning, i.e., the empirical approach to tune model complexity to minimize testing error, we need to understand the sources of testing error.</p>
<ul class="simple">
<li><p>the causes of the thing that we are attempting to minimize!</p></li>
</ul>
</section>
<section id="training-and-testing-data">
<h2>Training and Testing Data</h2>
<p>For clarity, consider this is schematic of the flow of training and testing data in the predictive machine learning model parameter training and hyperparameter tuning workflow,</p>
<figure style="text-align: center;">
  <img src="../Images/454aec4a3d657f072406100f036592b6.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/traintestdata.png"/>
  <figcaption style="text-align: center;">The flow of training and testing data in the predictive machine learning model parameter training and hyperparameter tuning workflow.</figcaption>
</figure>
<p><strong>Training Data</strong>,</p>
<ul class="simple">
<li><p>trains model parameters</p></li>
<li><p>trains the final model for real world use</p></li>
</ul>
<p><strong>Testing Data</strong>,</p>
<ul class="simple">
<li><p>withheld from training model parameters to avoid model overfit</p></li>
<li><p>tunes model hyperparameters</p></li>
<li><p>returned to train the final tuned model for deployment</p></li>
</ul>
</section>
<section id="how-much-data-should-be-withheld-for-testing">
<h2>How Much Data Should be Withheld for Testing?</h2>
<p>The proportion in testing is recommended by various sources from 15% - 30% of the total dataset. This is a compromise,</p>
<ul class="simple">
<li><p>data withheld for testing reduces the data available for training; therefore, reduces the accuracy of the model.</p></li>
<li><p>data withheld for testing improves the accuracy of the assessment of the model performance.</p></li>
</ul>
<p>Various authors have experimented on a variety of training and testing ratios and have recommended splits for their applications,</p>
<ul class="simple">
<li><p>the optimum ratio of training and testing split depends on problem setting</p></li>
</ul>
<p>To determine the proportion of testing data to withheld we could consider the difficulty in model parameter training (e.g., the number of model parameters) and the difficulty in model hyperparameter tuning (e.g., number of hyperparameters, range of response feature outcomes).</p>
</section>
<section id="fair-train-and-test-splits">
<h2>Fair Train and Test Splits</h2>
<p>Dr. Julian Salazar suggests that for spatial prediction problems that random train and test data splits may not be fair.</p>
<ul class="simple">
<li><p>proposed a <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521015023">fair train and test split method</a> for spatial prediction models that splits the data based on the difficulty of the planned use of the model.</p></li>
<li><p>prediction difficulty is related to kriging variance that accounts for spatial continuity and distance offset, i.e., the difficulty of the estimate.</p></li>
<li><p>the testing split is iterated to match the distribution of kriging variance for planned real world use of the model</p></li>
</ul>
<p>To illustrate this concept of prediction difficulty, consider this set of well logs with both random assignment of testing data and withholding an entire contiguous region of the well log for testing data.</p>
<ul class="simple">
<li><p><strong>easy prediction problem</strong> - for random assignment, usually training data are available very close and very similar to the withheld testing data</p></li>
<li><p><strong>difficult prediction problem</strong> - for removal of the contiguous region, there are no similar nor close training data to the withheld testing data</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/5aa194417f3fa3435f02717796814e62.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/fairtraintest.png"/>
  <figcaption style="text-align: center;">Two cases for train and test data split, random (left) and by-region (right).</figcaption>
</figure>
<p>Consider the following prediction cases, i.e., planned real world use of the models, and some practical suggestions for fair train and test split.</p>
<ol class="arabic simple">
<li><p>If the model will be used to impute data with small offsets from available data then construct a train and test split with train data close to test data - random assignment of withheld testing data is likely sufficient.</p></li>
<li><p>if the model will be used to predict a large distance offsets then perform splits the result is large offsets between train and test data - withhold entire wells, drill holes or spatial regions.</p></li>
</ol>
<ul class="simple">
<li><p>Note, with fair train and test splits the tuned model may vary based on the planned use for the model.</p></li>
</ul>
<p>Use a simple method like withholding entire wells for a predrill prediction model, or use the Dr. Salazar workflow, but don‚Äôt ignore this issue and just use random selection by default.</p>
<ul class="simple">
<li><p>admittedly, throughout this e-book for demonstration workflow brevity and clarity I have just used random training and testing data assignments.</p></li>
</ul>
</section>
<section id="model-metrics">
<h2>Model Metrics</h2>
<p>Since we have covered the workflows for training and tuning, now we can specify the model metrics that are applied for,</p>
<ul class="simple">
<li><p>training model parameters</p></li>
<li><p>tuning model hyperparameters</p></li>
<li><p>model checking and comparison</p></li>
</ul>
<p>Here‚Äôs an flowchart indicating how these metrics fit into the machine learning modeling workflow.</p>
<figure style="text-align: center;">
  <img src="../Images/d311f90f3b59dd6738b0ce7a12614995.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/metrics.png"/>
  <figcaption style="text-align: center;">Various applications for model metrics in machine learning modeling workflows.</figcaption>
</figure>
<p>Choice of model metric depends primarily on the context of the prediction problem,</p>
<ul class="simple">
<li><p>classification vs. regression</p></li>
<li><p>individual estimates vs. entire subsets in space (images) or time (signals)</p></li>
<li><p>estimation vs. uncertainty</p></li>
</ul>
<p>There are additional considerations, for example,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L^1\)</span> vs <span class="math notranslate nohighlight">\(L^2\)</span> norms with their differences, for example, in robustness with outliers, stability of solutions and solution sparsity</p></li>
<li><p>consistency with model assumptions, for example, <span class="math notranslate nohighlight">\(r^2\)</span> is only valid for linear models</p></li>
</ul>
<p>Let‚Äôs review some of the common model metrics for regression models and then for classification models.</p>
</section>
<section id="mean-square-error-mse">
<h2>Mean Square Error (MSE)</h2>
<p>Is sensitive to outliers, but is continuously differentiable, leading to a closed-form expression for model training. Since the error is squared the error units are squared and this may be less interpretable, for example, MSE of 23,543 <span class="math notranslate nohighlight">\(mD^2\)</span>. The equation is,</p>
<div class="math notranslate nohighlight">
\[
\text{Test MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta y_i)^2
\]</div>
</section>
<section id="mean-absolute-error-mae">
<h2>Mean Absolute Error (MAE)</h2>
<p>Is robust in the presence of outliers, but is not continuously differentiable; therefore, there is no closed-form expression for model training and training is generally accomplished by iterative optimization. The equation is,</p>
<div class="math notranslate nohighlight">
\[
\text{Test MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |y_i - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta y_i|
\]</div>
</section>
<section id="variance-explained">
<h2>Variance Explained</h2>
<p>The proportion of variance of the response feature captured by the model. Assumes additivity of variance; therefore, we only use this model metric for linear models.</p>
<p>First we calculate the variance explained by the model, simply as the variance of the model predictions,</p>
<div class="math notranslate nohighlight">
\[
\sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} ( \hat{y}_i - \bar{y} )^2
\]</div>
<p>then we calculate the variance not explained by the model as the variance of the error over the model predictions,</p>
<div class="math notranslate nohighlight">
\[
\sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2
\]</div>
<p>then under the assumption of additivity of variance, we calculate the ratio of variance explained over all variance, variance explained plus variance not explained,</p>
<div class="math notranslate nohighlight">
\[
r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2}
\]</div>
<p>For linear regression, recall <span class="math notranslate nohighlight">\(r^2 =  \left( \rho_(X,y) \right)^2\)</span>; therefore, like correlation coefficients, <span class="math notranslate nohighlight">\(r^2\)</span>,</p>
<ul class="simple">
<li><p>has similar issues as correlation with respect to outliers and mixing multiple populations, e.g., Simpson‚Äôs Paradox</p></li>
<li><p>for nonlinear models consider pseudo-R-square methods</p></li>
</ul>
<p>Also, even a linear model can have a negative <span class="math notranslate nohighlight">\(r^2\)</span> if the model trend contradicts the data trend, for example, if you fit data with a negative slope with a linear model with a positive slope!</p>
</section>
<section id="inlier-ratio">
<h2>Inlier Ratio</h2>
<p>The proportion of testing data, <span class="math notranslate nohighlight">\(y_i\)</span> within a margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>, of the model, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, calculated as,</p>
<div class="math notranslate nohighlight">
\[ 
I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
\]</div>
<p>where the indicator transform, <span class="math notranslate nohighlight">\(I_R\)</span> is defined as,</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
I(y_i, \hat{y}_i) = 
\begin{cases} 
1, &amp; \text{if } |y_i - \hat{y}_i| \leq \epsilon \\
0, &amp; \text{otherwise}
\end{cases} 
\end{split}\]</div>
<p>Here‚Äôs an illustration of the inlier ratio model metric, <span class="math notranslate nohighlight">\(I_R\)</span> model metric,</p>
<figure style="text-align: center;">
  <img src="../Images/3ddaa66ff3998a59b318297be09c60b8.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/inliers.png"/>
  <figcaption style="text-align: center;">Testing data, model with margin, \(\epsilon\), and outliers (white) and inliers (red) identified, 16 inliers out of 25 data samples, \(ùêºùëÖ = 0.64\). </figcaption>
</figure>
<p>While the illustration is a linear model, this metric may be applied to any model. Although there is some subjectivity with the inlier ratio model metric,</p>
<ul class="simple">
<li><p>what is the best selection for the margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>?</p></li>
</ul>
</section>
<section id="common-classification-model-metrics">
<h2>Common Classification Model Metrics</h2>
<p>Let‚Äôs review some of the common model metrics for classification models. Classification is potentially more complicated than regression, since instead of a single model metric, we actually calculate an entire confusion matrix,</p>
<ul class="simple">
<li><p>a <span class="math notranslate nohighlight">\(K \times K\)</span> matrix with frequencies of predicted (x axis) vs. actual (y axis) categories to visualize the performance of a classification model, where <span class="math notranslate nohighlight">\(K\)</span> is the response feature cardinality, i.e., the number of possible categories</p></li>
<li><p>visualize and diagnose all the combinations of correct and misclassification with the classification model, for example, category 1 is often misclassified as category 3,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/0c2a856f882f5bcc7e9df675ea5afb21.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/confusion.png"/>
  <figcaption style="text-align: center;">Example confusion matrix for a classification model, 2D matrix with the frequencies of all cases of truth and predicted categories. </figcaption>
</figure>
<ul class="simple">
<li><p>perfect accuracy is number of each class, <span class="math notranslate nohighlight">\(n_1, n_2, \ldots, n_K\)</span> on the diagonal, i.e., category 1 is always predicted as category 1, etc.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/34df964c246951e764fc3a7e8fecbf96.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/perfect_confusion.png"/>
  <figcaption style="text-align: center;">Example confusion matrix for perfectly accurate classification model. </figcaption>
</figure>
<ul class="simple">
<li><p>the confusion matrix is applied to calculate a single summary of categorical accuracy, for example, precision, recall, etc.</p></li>
<li><p>model metrics are specific to the specific category and may significantly vary over categories, i.e., we can predict well for category <span class="math notranslate nohighlight">\(k=1\)</span> but not for category <span class="math notranslate nohighlight">\(k=3\)</span>.</p></li>
</ul>
</section>
<section id="precision">
<h2>Precision</h2>
<p>For category <span class="math notranslate nohighlight">\(ùëò\)</span>, precision is the ratio of true positive over all positives,</p>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all positives}}
\]</div>
<p>we can intuitively describe precision as the conditional probability,</p>
<div class="math notranslate nohighlight">
\[
\text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says } k \text{ is happening}\right) 
\]</div>
<figure style="text-align: center;">
  <img src="../Images/020824af3f57f97af1ee2ba6ca6de684.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/precision.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the precision model metric for each category, \(k\).</figcaption>
</figure>
<p>For this example, we can calculate the precision for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 
\]</div>
</section>
<section id="recall-called-sensitivity-in-medical">
<h2>Recall (called sensitivity in medical)</h2>
<p>Recall for group <span class="math notranslate nohighlight">\(ùëò\)</span> is the ratio of true positives over all cases of <span class="math notranslate nohighlight">\(ùëò\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}} + n_{k \text{ false negative}}}
\]</div>
<p>We can intuitively describe recall as, how many of group ùëò did we catch?</p>
<ul class="simple">
<li><p>Note, recall does not account for false positives.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/recall.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the recall model metric for each category, \(k\). </figcaption>
</figure>
<p>For this example, we can calculate the recall for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 
\]</div>
</section>
<section id="specificity">
<h2>Specificity</h2>
<p>Specificity for group <span class="math notranslate nohighlight">\(ùëò\)</span> is the ratio of true negatives over all negative cases of <span class="math notranslate nohighlight">\(n \ne ùëò\)</span>.</p>
<div class="math notranslate nohighlight">
\[ 
\text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k \text{ true negative}} + n_{k \text{ false positive}}} 
\]</div>
<p>We can intuitively describe specificity as, how many of not group <span class="math notranslate nohighlight">\(k\)</span> did we catch?</p>
<ul class="simple">
<li><p>Note, recall does not account for true positives.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/22a61637f89b9d1f446385497e2e9b65.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/specificity.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the recall model metric for each category, \(k\). </figcaption>
</figure>
<p>For this example, we can calculate the recall for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5 + 7)} = \frac{50}{62} = 0.81 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 + 15)} = \frac{28}{58} = 0.48 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2 + 9)} = \frac{57}{68} = 0.84
\]</div>
</section>
<section id="f1-score">
<h2>f1-score</h2>
<p>f1-score is the harmonic mean of precision and recall for each <span class="math notranslate nohighlight">\(k\)</span> category,</p>
<div class="math notranslate nohighlight">
\[ 
\text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}} 
\]</div>
<p>The idea is to combine precision and recall into a single metric since they both see different aspects of the classification model accuracy.</p>
<ul class="simple">
<li><p>the harmonic mean is sensitive the to lowest score; therefore, good performance in one score cannot average out or make up for bad performance in the other</p></li>
</ul>
</section>
<section id="train-and-test-hold-out-cross-validation">
<h2>Train and Test Hold Out Cross Validation</h2>
<p>If only one train and test data split is applied to tune our machine learning model hyperparameters then we are applying the hold out cross validation approach.</p>
<ul class="simple">
<li><p>we split the data into training and testing data, these are exhaustive and mutually exclusive groups.</p></li>
<li><p>but this cross validation method is not exahustive, we only consider the one split for testing, most data are not tested. Also, we do not explore the full combinatorial of possible splits (more about this as we compare with other cross validation methods)</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ada2e0334323368c40690e596e78d464.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/test.png"/>
  <figcaption style="text-align: center;">The train and test data hold out cross validation.</figcaption>
</figure>
<p>The workflow is,</p>
<ol class="arabic simple">
<li><p>withhold the testing data subset from model training</p></li>
<li><p>train models with the remaining training data with various hyperparameters representing simple to complicated models</p></li>
<li><p>then test the suite of simple to complicated trained models with withheld testing data</p></li>
<li><p>select the model hyperparameters (complexity) with lowest testing error</p></li>
<li><p>retrain the model with the turned hyperparameters and all of the data for deployment</p></li>
</ol>
<p>The advantage of this approach is that we can readily evaluate the training and testing data split.</p>
<ul class="simple">
<li><p>since there is only one split, we can easily visualize and evaluate the train and test data cases, coverage and balance</p></li>
</ul>
<p>The disadvantage is that this method may be sensitive to the specific selection of testing data</p>
<ul class="simple">
<li><p>as a result hold out cross validation may result in a noisy plot of testing error vs. the hyperparameter</p></li>
</ul>
</section>
<section id="train-validate-and-test-hold-out-cross-validation">
<h2>Train, Validate and Test Hold Out Cross Validation</h2>
<p>There is a more complete hold out cross validation workflow commonly applied,</p>
<ol class="arabic simple">
<li><p><strong>Train with training data split</strong> - models sees and learns from this data to train the model parameters.</p></li>
<li><p><strong>Validate with the validation data split</strong> - evaluation of model complexity vs. accuracy with data withheld from model parameter training to tune the model hyperparameters. The same as testing data in train and test workflow.</p></li>
<li><p><strong>Test model performance with testing data</strong> - data withheld until the model is complete to provide a final evaluation of model performance.  This data had no role in building the model and is commonly applied to compare multiple competing models.</p></li>
</ol>
<figure style="text-align: center;">
  <img src="../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/validation.png"/>
  <figcaption style="text-align: center;">The train, validate and test hold out cross validation.</figcaption>
</figure>
<p>I understand the motivation for the training, validation and testing cross validation workflow. It is an attempt to check our models, objectively, with cases that,</p>
<ul class="simple">
<li><p>we know the truth and can access accuracy accurately</p></li>
<li><p>had nothing to do with the model construction, training model parameters nor tuning model hyperparameters</p></li>
</ul>
<p>I appreciate this, but I have some concerns,</p>
<ol class="arabic simple">
<li><p>We are further reducing the number of samples available to training model parameters and to tuning model hyperparameters.</p></li>
<li><p>Eventually we will retrain the tuned model with all the data, so the model we test is not actually the final deployed model.</p></li>
<li><p>What do we do if the testing data is not accurately predicted? Do we include another round of testing with another withheld subset of the data? Ad infinitum?</p></li>
</ol>
</section>
<section id="load-the-required-libraries">
<h2>Load the Required Libraries</h2>
<p>The following code loads the required libraries. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>                     <span class="c1"># K-fold cross validation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># color map</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>                                                     <span class="c1"># random number seed</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>I also added a convenience function to add major and minor gridlines to improve plot interpretability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks  </span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-data-to-demonstration-cross-validation-methods">
<h2>Load Data to Demonstration Cross Validation Methods</h2>
<p>Let‚Äôs load a spatial dataset and select 2 predictor features to visualize cross validation methods.</p>
<ul class="simple">
<li><p>we will focus on the data splits and not the actual model training and tuning. Later when we cover predictive machine learning methods we will add the model component of the workflow.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>

<span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span>                                          <span class="c1"># make predictor and response DataFrames</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>

<span class="n">Xname</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>                             <span class="c1"># store the names of the features</span>
<span class="n">yname</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span>

<span class="n">Xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">];</span> <span class="n">Xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">24.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">]</span>                           <span class="c1"># set the minimum and maximum values for plotting</span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">500.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">9000.0</span>

<span class="n">Xlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">]</span>
<span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production (MCFPD)'</span>

<span class="n">Xtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">]</span>
<span class="n">ytitle</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production'</span>

<span class="n">Xunit</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'%'</span><span class="p">,</span><span class="sa">r</span><span class="s1">'$kg/m^3 x m/s x 10^3$'</span><span class="p">];</span> <span class="n">yunit</span> <span class="o">=</span> <span class="s1">'MCFPD'</span>
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span/><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">23</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>
<span class="ne">---&gt; </span><span class="mi">23</span> <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span> <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>

<span class="ne">NameError</span>: name 'pred' is not defined
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-train-and-test-hold-out-cross-validation">
<h2>Visualize Train and Test Hold Out Cross Validation</h2>
<p>Let‚Äôs compare the train and test with train, validate and test hold out data splits.</p>
<ul class="simple">
<li><p>first we plot a train and test data split and then a train, validate and test split.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.15</span>                                              <span class="c1"># set the proportion of test data to withhold</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># make one train DataFrame with both X and y (remove all other features)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># make one testin DataFrame with both X and y (remove all other features)</span>

<span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ytitle</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png" src="../Images/a54bde3cc01f9083714572540a961fb0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png"/>
</div>
</div>
<p>It is a good idea to visualize the train and test split,</p>
<ul class="simple">
<li><p>histograms for each predictor feature and the response feature to ensure that the train and test cover the range of possible outcomes and are balanced</p></li>
<li><p>if the number of predictor features is 2 then we can actually plot the predictor feature space to check coverage and balance of train and test data splits</p></li>
</ul>
<p>Now let‚Äôs repeat this for the train, validate and test data split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">;</span> <span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.15</span>                             <span class="c1"># set the proportion of test data to withhold</span>
<span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">val_prop</span> <span class="o">+</span> <span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="n">val_prop</span> <span class="o">+</span> <span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">df_val</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq3</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq3</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ytitle</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png" src="../Images/8537d6caff0c1936ede31daca961765a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png"/>
</div>
</div>
<p>Once again we can visualize the splits, now train, validate and test,</p>
<ul class="simple">
<li><p>histograms for each predictor feature and the response feature to ensure that the train and test cover the range of possible outcomes and are balanced</p></li>
<li><p>if the number of predictor features is 2 then we can actually plot the predictor feature space to check coverage and balance of train and test data splits</p></li>
</ul>
</section>
<section id="leave-one-out-cross-validation-loo-cv">
<h2>Leave-one-out Cross Validation (LOO CV)</h2>
<p>Leave-one-out cross validation is an exhaustive cross validation method, i.e., all data gets tested by loop over all the data.</p>
<ul class="simple">
<li><p>we train and tune <span class="math notranslate nohighlight">\(n\)</span> models, for each model a single datum is withheld as testing and the <span class="math notranslate nohighlight">\(n-1\)</span> data are assigned as training data</p></li>
<li><p>we will calculate <span class="math notranslate nohighlight">\(n\)</span> training and testing errors that will be aggregated over all <span class="math notranslate nohighlight">\(n\)</span> models, for example, the average of the mean square error.</p></li>
</ul>
<p>In the case of leave-one-out cross validation,</p>
<ul class="simple">
<li><p>we test at only one datum so the test error is just a single error at the single withheld datum, so we just use standard MSE over the <span class="math notranslate nohighlight">\(n\)</span> models</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Test MSE Aggregate} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta y_i)^2
\]</div>
<ul class="simple">
<li><p>but, we have <span class="math notranslate nohighlight">\(n-1\)</span> training data for each model, so we aggregate, by averageing the mean square error of each model,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Train MSE Aggregate} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i
\]</div>
<p>Here‚Äôs the leave-one-out cross validation steps,</p>
<ol class="arabic simple">
<li><p>Loop over all <span class="math notranslate nohighlight">\(n\)</span> data, and withhold that data</p></li>
<li><p>Train on the remaining <span class="math notranslate nohighlight">\(n‚àí1\)</span> data and test on the withheld single data</p></li>
<li><p>Calculate model goodness metric, MSE for a single test data is the square error</p></li>
<li><p>Goto 1</p></li>
<li><p>Aggregate model goodness metric over all data, <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ol>
<p>Typically, leave-one-out cross validation is too easy of a prediction problem; therefore, it is not commonly used,</p>
<ul class="simple">
<li><p>but it introduces the concept of exhaustive cross validation, i.e., all data gets tested!</p></li>
</ul>
<p>Leave-one-out cross validation is also exhaustive in the sense that the full combinatorial of <span class="math notranslate nohighlight">\(n\)</span> data choose <span class="math notranslate nohighlight">\(p\)</span> where <span class="math notranslate nohighlight">\(p=1\)</span> are explored,</p>
<div class="math notranslate nohighlight">
\[
\binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n - 1)!} = n 
\]</div>
<p>where the full combinatorial is the <span class="math notranslate nohighlight">\(n\)</span> models that we built above!</p>
</section>
<section id="k-fold-cross-validation-k-fold-cv">
<h2>K-fold Cross Validation (k-fold CV)</h2>
<p>K-fold is a more general, efficient and robust approach.</p>
<ul class="simple">
<li><p>a exhaustive cross validation approach (all data are tested), but it samples a limited set of the possible combinatorial of prediction problems, unlike Leave-one-out cross validation where we attempt every possible case on data withheld for testing</p></li>
<li><p>for K-fold cross validation we assign a single set of K equal size splits and we loop over the splits, withholding the <span class="math notranslate nohighlight">\(k\)</span> split for testing data and using the data outside the split for training</p></li>
<li><p>the testing proportion is <span class="math notranslate nohighlight">\(\frac{1}{K}\)</span>, e.g., for <span class="math notranslate nohighlight">\(K=3\)</span>, 33.3% is withheld for testing, for <span class="math notranslate nohighlight">\(K=4\)</span>, 25% is withheld for testing and for <span class="math notranslate nohighlight">\(K=5\)</span>, 20% is withheld for testing</p></li>
</ul>
<p>We call it K-fold cross validation, because each of the splits is known as a fold. Here‚Äôs the steps for K-fold cross validation,</p>
<ol class="arabic simple">
<li><p>Select <span class="math notranslate nohighlight">\(K\)</span>, integer number of folds</p></li>
<li><p>Split the data into <span class="math notranslate nohighlight">\(K\)</span> equal size folds</p></li>
<li><p>Loop over each <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span> fold</p></li>
<li><p>Assign the data outside the <span class="math notranslate nohighlight">\(k\)</span> fold as training data and inside the <span class="math notranslate nohighlight">\(k\)</span> fold as testing data</p></li>
<li><p>Train and test the prediction model and calculated the testing model metric</p></li>
<li><p>Goto 3</p></li>
<li><p>Aggregate testing model metric over all K folds</p></li>
</ol>
<p>As you can see above k-fold cross validation is exhaustive, since all data is tested, i.e., withheld as testing data, but the method is not exhaustive in that all possible <span class="math notranslate nohighlight">\(\frac{n}{K}\)</span> data subsets are not considered.</p>
<p>To calculated the combinatorial for exhaustive K folds we used the multinomial coefficient,</p>
<div class="math notranslate nohighlight">
\[
\frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!}
\]</div>
<p>For example, if there are <span class="math notranslate nohighlight">\(n=100\)</span> data and <span class="math notranslate nohighlight">\(K=4\)</span> folds, there are <span class="math notranslate nohighlight">\(6.72 \times 10^55\)</span> possible combinations. I vote that we stick with regular K-fold cross validation.</p>
<p>Let‚Äôs visualize K-fold cross validation splits, for the case of <span class="math notranslate nohighlight">\(K=4\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">K</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">fold_number</span><span class="p">,</span> <span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">test_index</span><span class="p">,</span> <span class="s1">'Fold'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fold_number</span>  <span class="c1"># Assign fold number to test set</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="n">df_in</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="p">];</span> <span class="n">df_out</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">!=</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_in</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_in</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_out</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_out</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'K-fold #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">+</span> <span class="s1">', '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png" src="../Images/dd257beaa821a963fc15b3318a7bd9f2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png"/>
</div>
</div>
</section>
<section id="leave-p-out-cross-validation-lpo-cv">
<h2>Leave-p-out Cross Validation (LpO-CV)</h2>
<p>This is the variant of K-fold cross validation that exhaustively samples the full combinatorial of withholding <span class="math notranslate nohighlight">\(p\)</span> testing data.</p>
<ol class="arabic simple">
<li><p>Select <span class="math notranslate nohighlight">\(p\)</span>, integer number of testing data to withhold</p></li>
<li><p>For all possible <span class="math notranslate nohighlight">\(p\)</span> subsets of <span class="math notranslate nohighlight">\(n\)</span>,</p></li>
<li><p>Assign the data outside the <span class="math notranslate nohighlight">\(p\)</span> as training data and inside the <span class="math notranslate nohighlight">\(p\)</span> as testing data</p></li>
<li><p>Train and test the prediction model and calculated the testing model metric</p></li>
<li><p>Goto 2</p></li>
<li><p>Aggregate testing model metric over the combinatorial</p></li>
</ol>
<p>For this case the combinatorial of cases is, <span class="math notranslate nohighlight">\(n\)</span> choose <span class="math notranslate nohighlight">\(p\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\binom{n}{p} = \frac{n!}{p!(n - p)!}
\]</div>
<p>For <span class="math notranslate nohighlight">\(n=100\)</span> and <span class="math notranslate nohighlight">\(p=20\)</span>, we have <span class="math notranslate nohighlight">\(5.36 \times 10^{20}\)</span> combinations to check!</p>
</section>
<section id="limitations-of-cross-validation">
<h2>Limitations of Cross Validation</h2>
<p>Here are some additional issues with the model cross validation approach in general,</p>
<ul class="simple">
<li><p><strong>Peeking, Information Leakage</strong> ‚Äì some information is transmitted from the withheld data into the model, some model decision(s) use all the data. Pipelines and wrappers help with this.</p></li>
<li><p><strong>Black Swans / Stationarity</strong> ‚Äì the model cannot be tested for data events not available in the data. This is also known as the ‚ÄòNo Free Lunch Theorem‚Äô in machine learning</p></li>
</ul>
<p>Consider the words of Hume,</p>
<p>‚Äúeven after the observation of the frequent or constant conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience‚Äù - Hume (1739‚Äì1740)</p>
<ul class="simple">
<li><p>We cannot predict things that we have never seen in our data!</p></li>
</ul>
<p>here‚Äôs a quote from the famous Oreskes et al. (1994) paper on subsurface validation and verification,</p>
<p>‚ÄúVerification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always nonunique. Models can be confirmed by the demonstration of agreement between observation and prediction, but confirmation is inherently partial. Complete confirmation is logically precluded by the fallacy of affirming the consequent and by incomplete access to natural phenomena. Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.‚Äù</p>
<ul class="simple">
<li><p>Oreskes et al. (1994)</p></li>
</ul>
<p>all of this is summed up very well with,</p>
<p>‚ÄòAll models are wrong, but some are useful‚Äô ‚Äì George Box</p>
<p>and a reminder of,</p>
<p><strong>Parsimony</strong> ‚Äì since all models are wrong, an economical description of the system. Occam‚Äôs Razor</p>
<p>resulting in a pragmatic approach of,</p>
<p><strong>Worrying Selectively</strong> ‚Äì since all models are wrong, figure out what is most importantly wrong.</p>
<p>finally, I add my own words,</p>
<p>‚ÄòBe humble, the earth will surprise you!‚Äô ‚Äì Michael Pyrcz</p>
</section>
<section id="comments">
<h2>Comments</h2>
<p>This was a basic description of machine learning concepts. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this was helpful,</p>
<p><em>Michael</em></p>
</section>
<section id="about-the-author">
<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
&#13;

<h2>Training and Tuning Predictive Machine Learning Models</h2>
<p>In predictive machine learning, we follow a standard model training and testing workflow. This process ensures that our model generalizes well to new data, rather than just fitting the training data perfectly.</p>
<figure style="text-align: center;">
  <img src="../Images/31abfecc21c246ba7144dfb847ab4378.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/ML_workflow_portrait.png"/>
  <figcaption style="text-align: center;">Standard predictive machine learning modeling workflow.</figcaption>
</figure>
<p>Let‚Äôs walk through the key steps,</p>
<ol class="arabic simple">
<li><p><strong>Train and Test Split</strong> - divide the available data into mutually exclusive, exhaustive subsets: a training set and a testing set.</p></li>
</ol>
<ul class="simple">
<li><p>typically, 15%‚Äì30% of the data is held out for testing</p></li>
<li><p>the remaining 70%‚Äì85% is used for training the model</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Define a range of hyperparameter(s)</strong> values to explore, ranging from,</p></li>
</ol>
<ul class="simple">
<li><p>simple models with low flexibility</p></li>
<li><p>to complex models with high flexibility</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> This step may involve tuning multiple hyperparameters, in which case efficient sampling methods (e.g., grid search, random search, or Bayesian optimization) are often used.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Train Model Parameters for Each Hyperparameter Setting</strong> - for each set of hyperparameters, train a model on the training data. This yields:</p></li>
</ol>
<ul class="simple">
<li><p>a suite of trained models, each with different complexity</p></li>
<li><p>each model has parameters optimized to minimize error on the training data</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Evaluate Each Model on the Withheld Testing Data</strong> - using the testing data,</p></li>
</ol>
<ul class="simple">
<li><p>evaluate how each trained model performs on unseen data</p></li>
<li><p>summarize prediction error (for example, root mean square error (RMSE), mean absolute error (MAE), classification accuracy) for each model</p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p><strong>Select the Hyperparameters That Minimize Test Error</strong> - this is the hyperparameter tuning step:</p></li>
</ol>
<ul class="simple">
<li><p>choose the model hyperparaemter(s) that performs best on the test data</p></li>
<li><p>these are your tuned hyperparameters</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p><strong>Retrain the Final Model on All Data Using Tuned Hyperparameters</strong> - now that the best model complexity has been identified,</p></li>
</ol>
<ul class="simple">
<li><p>retrain the model using both the training and test sets</p></li>
<li><p>this maximizes the amount of data used for final model parameter estimation</p></li>
<li><p>the resulting model is the one you deploy in real-world applications</p></li>
</ul>
&#13;

<h2>Common Questions About the Model Training and Tuning Workflow</h2>
<p>As a professor, I often hear these questions when I introduce the above machine learning model training and tuning workflow.</p>
<ul class="simple">
<li><p><strong>What is the main outcome of steps 1‚Äì5?</strong> - the only reliable outcome is the tuned hyperparameters.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> we do not use the model trained in step 3 or 4 directly, because it was trained without access to all available data. Instead, we retrain the final model using all data with the selected hyperparameters.</p>
<ul class="simple">
<li><p><strong>Why not train the model on all the data from the beginning?</strong> - because if we do that, we have no independent way to evaluate the model‚Äôs generalization. A very complex model can easily overfit‚Äîfitting the training data perfectly, but performing poorly on new, unseen data.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\quad\)</span> overfitting happens when model flexibility is too high‚Äîit captures noise instead of the underlying pattern. Without a withheld test set, we can‚Äôt detect this.</p>
<p>This workflow for training and tuning predictive machine learning models is,</p>
<ul class="simple">
<li><p>an empirical, cross-validation-based process</p></li>
<li><p>a practical simulation of real-world model use</p></li>
<li><p>a method to identify the model complexity that best balances fit and generalization</p></li>
</ul>
<p>I‚Äôve said model parameters and model hyperparameters a bunch of times, so I owe you their definitions.</p>
&#13;

<h2>Model Parameters and Model Hyperparameters</h2>
<p><strong>Model parameters</strong> are fit during training phase to minimize error at the training data, i.e., model parameters are trained with training data and control model fit to the data. For example,</p>
<ul class="simple">
<li><p>for the polynomial predictive machine learning model from the machine learning workflow example above, the model parameters are the polynomial coefficients, e.g., <span class="math notranslate nohighlight">\(b_3\)</span>, <span class="math notranslate nohighlight">\(b_2\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(c\)</span> (often called <span class="math notranslate nohighlight">\(b_0\)</span>) for the third order polynomial model.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/85cce1589249c12a11ff52ea10e6c893.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/parameters.png"/>
  <figcaption style="text-align: center;">Model parameters are adjusted to fit of the model to the data, i.e., model parameters are trained to minimize error over the training data (x markers).</figcaption>
</figure>
<p><strong>Model hyperparameters</strong> are very different. They do not constrain the model fit to the data directly, instead they constrain the model complexity.
The model hyperparameters are selected (call tuning) to minimize error at the withheld testing data. Going back to our polynomial predictive machine learning example, the choice of polynomial order is the model hyperparameter.</p>
<figure style="text-align: center;">
  <img src="../Images/aef7a81bd36cd7f2b25e1574784b4932.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/concepts/hyperparameters.png"/>
  <figcaption style="text-align: center;">Model hyperparameters are adjusted to change the model complexity / flexibility, i.e., model hyperparameters are tuned to minimize error over the withheld testing data (solid circles).</figcaption>
</figure>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Model parameters vs. model hyperparameters</p>
<p>Model parameters control the model fit and are trained with training data. Model hyperparameters control the model complexity and are tuned with testing data.</p>
</div>
&#13;

<h2>Regression and Classification</h2>
<p>Before we proceed, we need to define regression and classification.</p>
<ul class="simple">
<li><p><strong>Regression</strong> - a predictive machine learning model where the response feature(s) is continuous.</p></li>
<li><p><strong>Classification</strong> - a predictive machine learning model where the response feature(s) is categorical.</p></li>
</ul>
<p>It turns out that for each of these we need to build different models and use different methods to score these models.</p>
<ul class="simple">
<li><p>for the remainder of this discussion we will focus on regression, but in later chapters we introduce classification models as well.</p></li>
</ul>
<p>Now, to better understand predictive machine learning model tuning, i.e., the empirical approach to tune model complexity to minimize testing error, we need to understand the sources of testing error.</p>
<ul class="simple">
<li><p>the causes of the thing that we are attempting to minimize!</p></li>
</ul>
&#13;

<h2>Training and Testing Data</h2>
<p>For clarity, consider this is schematic of the flow of training and testing data in the predictive machine learning model parameter training and hyperparameter tuning workflow,</p>
<figure style="text-align: center;">
  <img src="../Images/454aec4a3d657f072406100f036592b6.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/traintestdata.png"/>
  <figcaption style="text-align: center;">The flow of training and testing data in the predictive machine learning model parameter training and hyperparameter tuning workflow.</figcaption>
</figure>
<p><strong>Training Data</strong>,</p>
<ul class="simple">
<li><p>trains model parameters</p></li>
<li><p>trains the final model for real world use</p></li>
</ul>
<p><strong>Testing Data</strong>,</p>
<ul class="simple">
<li><p>withheld from training model parameters to avoid model overfit</p></li>
<li><p>tunes model hyperparameters</p></li>
<li><p>returned to train the final tuned model for deployment</p></li>
</ul>
&#13;

<h2>How Much Data Should be Withheld for Testing?</h2>
<p>The proportion in testing is recommended by various sources from 15% - 30% of the total dataset. This is a compromise,</p>
<ul class="simple">
<li><p>data withheld for testing reduces the data available for training; therefore, reduces the accuracy of the model.</p></li>
<li><p>data withheld for testing improves the accuracy of the assessment of the model performance.</p></li>
</ul>
<p>Various authors have experimented on a variety of training and testing ratios and have recommended splits for their applications,</p>
<ul class="simple">
<li><p>the optimum ratio of training and testing split depends on problem setting</p></li>
</ul>
<p>To determine the proportion of testing data to withheld we could consider the difficulty in model parameter training (e.g., the number of model parameters) and the difficulty in model hyperparameter tuning (e.g., number of hyperparameters, range of response feature outcomes).</p>
&#13;

<h2>Fair Train and Test Splits</h2>
<p>Dr. Julian Salazar suggests that for spatial prediction problems that random train and test data splits may not be fair.</p>
<ul class="simple">
<li><p>proposed a <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0920410521015023">fair train and test split method</a> for spatial prediction models that splits the data based on the difficulty of the planned use of the model.</p></li>
<li><p>prediction difficulty is related to kriging variance that accounts for spatial continuity and distance offset, i.e., the difficulty of the estimate.</p></li>
<li><p>the testing split is iterated to match the distribution of kriging variance for planned real world use of the model</p></li>
</ul>
<p>To illustrate this concept of prediction difficulty, consider this set of well logs with both random assignment of testing data and withholding an entire contiguous region of the well log for testing data.</p>
<ul class="simple">
<li><p><strong>easy prediction problem</strong> - for random assignment, usually training data are available very close and very similar to the withheld testing data</p></li>
<li><p><strong>difficult prediction problem</strong> - for removal of the contiguous region, there are no similar nor close training data to the withheld testing data</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/5aa194417f3fa3435f02717796814e62.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/fairtraintest.png"/>
  <figcaption style="text-align: center;">Two cases for train and test data split, random (left) and by-region (right).</figcaption>
</figure>
<p>Consider the following prediction cases, i.e., planned real world use of the models, and some practical suggestions for fair train and test split.</p>
<ol class="arabic simple">
<li><p>If the model will be used to impute data with small offsets from available data then construct a train and test split with train data close to test data - random assignment of withheld testing data is likely sufficient.</p></li>
<li><p>if the model will be used to predict a large distance offsets then perform splits the result is large offsets between train and test data - withhold entire wells, drill holes or spatial regions.</p></li>
</ol>
<ul class="simple">
<li><p>Note, with fair train and test splits the tuned model may vary based on the planned use for the model.</p></li>
</ul>
<p>Use a simple method like withholding entire wells for a predrill prediction model, or use the Dr. Salazar workflow, but don‚Äôt ignore this issue and just use random selection by default.</p>
<ul class="simple">
<li><p>admittedly, throughout this e-book for demonstration workflow brevity and clarity I have just used random training and testing data assignments.</p></li>
</ul>
&#13;

<h2>Model Metrics</h2>
<p>Since we have covered the workflows for training and tuning, now we can specify the model metrics that are applied for,</p>
<ul class="simple">
<li><p>training model parameters</p></li>
<li><p>tuning model hyperparameters</p></li>
<li><p>model checking and comparison</p></li>
</ul>
<p>Here‚Äôs an flowchart indicating how these metrics fit into the machine learning modeling workflow.</p>
<figure style="text-align: center;">
  <img src="../Images/d311f90f3b59dd6738b0ce7a12614995.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/metrics.png"/>
  <figcaption style="text-align: center;">Various applications for model metrics in machine learning modeling workflows.</figcaption>
</figure>
<p>Choice of model metric depends primarily on the context of the prediction problem,</p>
<ul class="simple">
<li><p>classification vs. regression</p></li>
<li><p>individual estimates vs. entire subsets in space (images) or time (signals)</p></li>
<li><p>estimation vs. uncertainty</p></li>
</ul>
<p>There are additional considerations, for example,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L^1\)</span> vs <span class="math notranslate nohighlight">\(L^2\)</span> norms with their differences, for example, in robustness with outliers, stability of solutions and solution sparsity</p></li>
<li><p>consistency with model assumptions, for example, <span class="math notranslate nohighlight">\(r^2\)</span> is only valid for linear models</p></li>
</ul>
<p>Let‚Äôs review some of the common model metrics for regression models and then for classification models.</p>
&#13;

<h2>Mean Square Error (MSE)</h2>
<p>Is sensitive to outliers, but is continuously differentiable, leading to a closed-form expression for model training. Since the error is squared the error units are squared and this may be less interpretable, for example, MSE of 23,543 <span class="math notranslate nohighlight">\(mD^2\)</span>. The equation is,</p>
<div class="math notranslate nohighlight">
\[
\text{Test MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta y_i)^2
\]</div>
&#13;

<h2>Mean Absolute Error (MAE)</h2>
<p>Is robust in the presence of outliers, but is not continuously differentiable; therefore, there is no closed-form expression for model training and training is generally accomplished by iterative optimization. The equation is,</p>
<div class="math notranslate nohighlight">
\[
\text{Test MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |y_i - \hat{y}_i| = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} |\Delta y_i|
\]</div>
&#13;

<h2>Variance Explained</h2>
<p>The proportion of variance of the response feature captured by the model. Assumes additivity of variance; therefore, we only use this model metric for linear models.</p>
<p>First we calculate the variance explained by the model, simply as the variance of the model predictions,</p>
<div class="math notranslate nohighlight">
\[
\sigma_{\text{explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} ( \hat{y}_i - \bar{y} )^2
\]</div>
<p>then we calculate the variance not explained by the model as the variance of the error over the model predictions,</p>
<div class="math notranslate nohighlight">
\[
\sigma_{\text{not explained}}^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2
\]</div>
<p>then under the assumption of additivity of variance, we calculate the ratio of variance explained over all variance, variance explained plus variance not explained,</p>
<div class="math notranslate nohighlight">
\[
r^2 = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{explained}}^2 + \sigma_{\text{not explained}}^2} = \frac{\sigma_{\text{explained}}^2}{\sigma_{\text{total}}^2}
\]</div>
<p>For linear regression, recall <span class="math notranslate nohighlight">\(r^2 =  \left( \rho_(X,y) \right)^2\)</span>; therefore, like correlation coefficients, <span class="math notranslate nohighlight">\(r^2\)</span>,</p>
<ul class="simple">
<li><p>has similar issues as correlation with respect to outliers and mixing multiple populations, e.g., Simpson‚Äôs Paradox</p></li>
<li><p>for nonlinear models consider pseudo-R-square methods</p></li>
</ul>
<p>Also, even a linear model can have a negative <span class="math notranslate nohighlight">\(r^2\)</span> if the model trend contradicts the data trend, for example, if you fit data with a negative slope with a linear model with a positive slope!</p>
&#13;

<h2>Inlier Ratio</h2>
<p>The proportion of testing data, <span class="math notranslate nohighlight">\(y_i\)</span> within a margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>, of the model, <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>, calculated as,</p>
<div class="math notranslate nohighlight">
\[ 
I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
\]</div>
<p>where the indicator transform, <span class="math notranslate nohighlight">\(I_R\)</span> is defined as,</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
I(y_i, \hat{y}_i) = 
\begin{cases} 
1, &amp; \text{if } |y_i - \hat{y}_i| \leq \epsilon \\
0, &amp; \text{otherwise}
\end{cases} 
\end{split}\]</div>
<p>Here‚Äôs an illustration of the inlier ratio model metric, <span class="math notranslate nohighlight">\(I_R\)</span> model metric,</p>
<figure style="text-align: center;">
  <img src="../Images/3ddaa66ff3998a59b318297be09c60b8.png" style="display: block; margin: 0 auto; width: 80%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/inliers.png"/>
  <figcaption style="text-align: center;">Testing data, model with margin, \(\epsilon\), and outliers (white) and inliers (red) identified, 16 inliers out of 25 data samples, \(ùêºùëÖ = 0.64\). </figcaption>
</figure>
<p>While the illustration is a linear model, this metric may be applied to any model. Although there is some subjectivity with the inlier ratio model metric,</p>
<ul class="simple">
<li><p>what is the best selection for the margin, <span class="math notranslate nohighlight">\(\epsilon\)</span>?</p></li>
</ul>
&#13;

<h2>Common Classification Model Metrics</h2>
<p>Let‚Äôs review some of the common model metrics for classification models. Classification is potentially more complicated than regression, since instead of a single model metric, we actually calculate an entire confusion matrix,</p>
<ul class="simple">
<li><p>a <span class="math notranslate nohighlight">\(K \times K\)</span> matrix with frequencies of predicted (x axis) vs. actual (y axis) categories to visualize the performance of a classification model, where <span class="math notranslate nohighlight">\(K\)</span> is the response feature cardinality, i.e., the number of possible categories</p></li>
<li><p>visualize and diagnose all the combinations of correct and misclassification with the classification model, for example, category 1 is often misclassified as category 3,</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/0c2a856f882f5bcc7e9df675ea5afb21.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/confusion.png"/>
  <figcaption style="text-align: center;">Example confusion matrix for a classification model, 2D matrix with the frequencies of all cases of truth and predicted categories. </figcaption>
</figure>
<ul class="simple">
<li><p>perfect accuracy is number of each class, <span class="math notranslate nohighlight">\(n_1, n_2, \ldots, n_K\)</span> on the diagonal, i.e., category 1 is always predicted as category 1, etc.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/34df964c246951e764fc3a7e8fecbf96.png" style="display: block; margin: 0 auto; width: 40%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/perfect_confusion.png"/>
  <figcaption style="text-align: center;">Example confusion matrix for perfectly accurate classification model. </figcaption>
</figure>
<ul class="simple">
<li><p>the confusion matrix is applied to calculate a single summary of categorical accuracy, for example, precision, recall, etc.</p></li>
<li><p>model metrics are specific to the specific category and may significantly vary over categories, i.e., we can predict well for category <span class="math notranslate nohighlight">\(k=1\)</span> but not for category <span class="math notranslate nohighlight">\(k=3\)</span>.</p></li>
</ul>
&#13;

<h2>Precision</h2>
<p>For category <span class="math notranslate nohighlight">\(ùëò\)</span>, precision is the ratio of true positive over all positives,</p>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}} + n_{k \text{ false positive}}} = \frac{\text{true positive}}{\text{all positives}}
\]</div>
<p>we can intuitively describe precision as the conditional probability,</p>
<div class="math notranslate nohighlight">
\[
\text{Precision}_k = P \left(k \text{ is happening} \mid \text{model says } k \text{ is happening}\right) 
\]</div>
<figure style="text-align: center;">
  <img src="../Images/020824af3f57f97af1ee2ba6ca6de684.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/precision.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the precision model metric for each category, \(k\).</figcaption>
</figure>
<p>For this example, we can calculate the precision for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=1} = \frac{15}{15 + (5 + 7)} = \frac{15}{27} = 0.56 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=2} = \frac{22}{22 + (15 + 15)} = \frac{22}{52} = 0.42 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Precision}_{k=3} = \frac{4}{4 + (2 + 9)} = \frac{4}{15} = 0.27 
\]</div>
&#13;

<h2>Recall (called sensitivity in medical)</h2>
<p>Recall for group <span class="math notranslate nohighlight">\(ùëò\)</span> is the ratio of true positives over all cases of <span class="math notranslate nohighlight">\(ùëò\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\text{Recall}_k = \frac{n_{k \text{ true positive}}}{n_{k \text{ true positive}} + n_{k \text{ false negative}}}
\]</div>
<p>We can intuitively describe recall as, how many of group ùëò did we catch?</p>
<ul class="simple">
<li><p>Note, recall does not account for false positives.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/94f40c5cd8c775dabd2e31e85c21a9cf.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/recall.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the recall model metric for each category, \(k\). </figcaption>
</figure>
<p>For this example, we can calculate the recall for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=1} = \frac{15}{15 + (15 + 2)} = \frac{15}{32} = 0.47 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=2} = \frac{22}{22 + (5 + 9)} = \frac{22}{36} = 0.61 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\text{Recall}_{k=3} = \frac{4}{4 + (7 + 15)} = \frac{4}{26} = 0.15 
\]</div>
&#13;

<h2>Specificity</h2>
<p>Specificity for group <span class="math notranslate nohighlight">\(ùëò\)</span> is the ratio of true negatives over all negative cases of <span class="math notranslate nohighlight">\(n \ne ùëò\)</span>.</p>
<div class="math notranslate nohighlight">
\[ 
\text{Specificity}_k = \frac{n_{k \text{ true negative}}}{n_{\neq k} \, n_{k \text{ true negative}} + n_{k \text{ false positive}}} 
\]</div>
<p>We can intuitively describe specificity as, how many of not group <span class="math notranslate nohighlight">\(k\)</span> did we catch?</p>
<ul class="simple">
<li><p>Note, recall does not account for true positives.</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/22a61637f89b9d1f446385497e2e9b65.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/specificity.png"/>
  <figcaption style="text-align: center;">Example confusion matrix with illustration of the recall model metric for each category, \(k\). </figcaption>
</figure>
<p>For this example, we can calculate the recall for each category as,</p>
<ul class="simple">
<li><p>Category k=1</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=1} = \frac{22 + 9 + 15 + 4}{(22 + 9 + 15 + 4) + (5 + 7)} = \frac{50}{62} = 0.81 
\]</div>
<ul class="simple">
<li><p>Category k = 2,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=2} = \frac{15 + 2 + 7 + 4}{(15 + 2 + 7 + 4) + (15 + 15)} = \frac{28}{58} = 0.48 
\]</div>
<ul class="simple">
<li><p>Category k = 3,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Specificity}_{k=3} = \frac{15 + 15 + 5 + 22}{(15 + 15 + 5 + 22) + (2 + 9)} = \frac{57}{68} = 0.84
\]</div>
&#13;

<h2>f1-score</h2>
<p>f1-score is the harmonic mean of precision and recall for each <span class="math notranslate nohighlight">\(k\)</span> category,</p>
<div class="math notranslate nohighlight">
\[ 
\text{F1-Score}_k = \frac{2}{\frac{1}{\text{Precision}_k} + \frac{1}{\text{Recall}_k}} 
\]</div>
<p>The idea is to combine precision and recall into a single metric since they both see different aspects of the classification model accuracy.</p>
<ul class="simple">
<li><p>the harmonic mean is sensitive the to lowest score; therefore, good performance in one score cannot average out or make up for bad performance in the other</p></li>
</ul>
&#13;

<h2>Train and Test Hold Out Cross Validation</h2>
<p>If only one train and test data split is applied to tune our machine learning model hyperparameters then we are applying the hold out cross validation approach.</p>
<ul class="simple">
<li><p>we split the data into training and testing data, these are exhaustive and mutually exclusive groups.</p></li>
<li><p>but this cross validation method is not exahustive, we only consider the one split for testing, most data are not tested. Also, we do not explore the full combinatorial of possible splits (more about this as we compare with other cross validation methods)</p></li>
</ul>
<figure style="text-align: center;">
  <img src="../Images/ada2e0334323368c40690e596e78d464.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/test.png"/>
  <figcaption style="text-align: center;">The train and test data hold out cross validation.</figcaption>
</figure>
<p>The workflow is,</p>
<ol class="arabic simple">
<li><p>withhold the testing data subset from model training</p></li>
<li><p>train models with the remaining training data with various hyperparameters representing simple to complicated models</p></li>
<li><p>then test the suite of simple to complicated trained models with withheld testing data</p></li>
<li><p>select the model hyperparameters (complexity) with lowest testing error</p></li>
<li><p>retrain the model with the turned hyperparameters and all of the data for deployment</p></li>
</ol>
<p>The advantage of this approach is that we can readily evaluate the training and testing data split.</p>
<ul class="simple">
<li><p>since there is only one split, we can easily visualize and evaluate the train and test data cases, coverage and balance</p></li>
</ul>
<p>The disadvantage is that this method may be sensitive to the specific selection of testing data</p>
<ul class="simple">
<li><p>as a result hold out cross validation may result in a noisy plot of testing error vs. the hyperparameter</p></li>
</ul>
&#13;

<h2>Train, Validate and Test Hold Out Cross Validation</h2>
<p>There is a more complete hold out cross validation workflow commonly applied,</p>
<ol class="arabic simple">
<li><p><strong>Train with training data split</strong> - models sees and learns from this data to train the model parameters.</p></li>
<li><p><strong>Validate with the validation data split</strong> - evaluation of model complexity vs. accuracy with data withheld from model parameter training to tune the model hyperparameters. The same as testing data in train and test workflow.</p></li>
<li><p><strong>Test model performance with testing data</strong> - data withheld until the model is complete to provide a final evaluation of model performance.  This data had no role in building the model and is commonly applied to compare multiple competing models.</p></li>
</ol>
<figure style="text-align: center;">
  <img src="../Images/e1cf2df3d7bb949f562eecd4af03f6c0.png" style="display: block; margin: 0 auto; width: 100%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/training_tuning/validation.png"/>
  <figcaption style="text-align: center;">The train, validate and test hold out cross validation.</figcaption>
</figure>
<p>I understand the motivation for the training, validation and testing cross validation workflow. It is an attempt to check our models, objectively, with cases that,</p>
<ul class="simple">
<li><p>we know the truth and can access accuracy accurately</p></li>
<li><p>had nothing to do with the model construction, training model parameters nor tuning model hyperparameters</p></li>
</ul>
<p>I appreciate this, but I have some concerns,</p>
<ol class="arabic simple">
<li><p>We are further reducing the number of samples available to training model parameters and to tuning model hyperparameters.</p></li>
<li><p>Eventually we will retrain the tuned model with all the data, so the model we test is not actually the final deployed model.</p></li>
<li><p>What do we do if the testing data is not accurately predicted? Do we include another round of testing with another withheld subset of the data? Ad infinitum?</p></li>
</ol>
&#13;

<h2>Load the Required Libraries</h2>
<p>The following code loads the required libraries. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">ignore_warnings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span> <span class="n">AutoMinorLocator</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>                     <span class="c1"># K-fold cross validation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># plot all grids below the plot elements</span>
<span class="k">if</span> <span class="n">ignore_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                   
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># color map</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>                                                     <span class="c1"># random number seed</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Declare Functions</h2>
<p>I also added a convenience function to add major and minor gridlines to improve plot interpretability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks  </span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Load Data to Demonstration Cross Validation Methods</h2>
<p>Let‚Äôs load a spatial dataset and select 2 predictor features to visualize cross validation methods.</p>
<ul class="simple">
<li><p>we will focus on the data splits and not the actual model training and tuning. Later when we cover predictive machine learning methods we will add the model component of the workflow.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv'</span><span class="p">)</span> <span class="c1"># load data from Dr. Pyrcz's GitHub repository  </span>

<span class="n">response</span> <span class="o">=</span> <span class="s1">'Prod'</span>                                             <span class="c1"># specify the response feature</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span>                                          <span class="c1"># make predictor and response DataFrames</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">response</span><span class="p">]</span>

<span class="n">Xname</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>                             <span class="c1"># store the names of the features</span>
<span class="n">yname</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">name</span>

<span class="n">Xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">];</span> <span class="n">Xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">24.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">]</span>                           <span class="c1"># set the minimum and maximum values for plotting</span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">500.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">9000.0</span>

<span class="n">Xlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">]</span>
<span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production (MCFPD)'</span>

<span class="n">Xtitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Acoustic Impedance'</span><span class="p">]</span>
<span class="n">ytitle</span> <span class="o">=</span> <span class="s1">'Normalized Initial Production'</span>

<span class="n">Xunit</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'%'</span><span class="p">,</span><span class="sa">r</span><span class="s1">'$kg/m^3 x m/s x 10^3$'</span><span class="p">];</span> <span class="n">yunit</span> <span class="o">=</span> <span class="s1">'MCFPD'</span>
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span/><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">23</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>
<span class="ne">---&gt; </span><span class="mi">23</span> <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span> <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>

<span class="ne">NameError</span>: name 'pred' is not defined
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Visualize Train and Test Hold Out Cross Validation</h2>
<p>Let‚Äôs compare the train and test with train, validate and test hold out data splits.</p>
<ul class="simple">
<li><p>first we plot a train and test data split and then a train, validate and test split.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.15</span>                                              <span class="c1"># set the proportion of test data to withhold</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># make one train DataFrame with both X and y (remove all other features)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># make one testin DataFrame with both X and y (remove all other features)</span>

<span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ytitle</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png" src="../Images/a54bde3cc01f9083714572540a961fb0.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f0f58efa21786c689ea72d45aad30f90c3dd3c383729660c4fd8a44bd077385e.png"/>
</div>
</div>
<p>It is a good idea to visualize the train and test split,</p>
<ul class="simple">
<li><p>histograms for each predictor feature and the response feature to ensure that the train and test cover the range of possible outcomes and are balanced</p></li>
<li><p>if the number of predictor features is 2 then we can actually plot the predictor feature space to check coverage and balance of train and test data splits</p></li>
</ul>
<p>Now let‚Äôs repeat this for the train, validate and test data split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">val_prop</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">;</span> <span class="n">test_prop</span> <span class="o">=</span> <span class="mf">0.15</span>                             <span class="c1"># set the proportion of test data to withhold</span>
<span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">val_prop</span> <span class="o">+</span> <span class="n">test_prop</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_prop</span><span class="o">/</span><span class="p">(</span><span class="n">val_prop</span> <span class="o">+</span> <span class="n">test_prop</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">df_val</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq3</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq3</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xtitle</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_val</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Validate'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq3</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ytitle</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_val</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png" src="../Images/8537d6caff0c1936ede31daca961765a.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/79b0ada345bb17fe9e2e5450cdd37a1c3881bfd2ae3882674678aab8cad6597d.png"/>
</div>
</div>
<p>Once again we can visualize the splits, now train, validate and test,</p>
<ul class="simple">
<li><p>histograms for each predictor feature and the response feature to ensure that the train and test cover the range of possible outcomes and are balanced</p></li>
<li><p>if the number of predictor features is 2 then we can actually plot the predictor feature space to check coverage and balance of train and test data splits</p></li>
</ul>
&#13;

<h2>Leave-one-out Cross Validation (LOO CV)</h2>
<p>Leave-one-out cross validation is an exhaustive cross validation method, i.e., all data gets tested by loop over all the data.</p>
<ul class="simple">
<li><p>we train and tune <span class="math notranslate nohighlight">\(n\)</span> models, for each model a single datum is withheld as testing and the <span class="math notranslate nohighlight">\(n-1\)</span> data are assigned as training data</p></li>
<li><p>we will calculate <span class="math notranslate nohighlight">\(n\)</span> training and testing errors that will be aggregated over all <span class="math notranslate nohighlight">\(n\)</span> models, for example, the average of the mean square error.</p></li>
</ul>
<p>In the case of leave-one-out cross validation,</p>
<ul class="simple">
<li><p>we test at only one datum so the test error is just a single error at the single withheld datum, so we just use standard MSE over the <span class="math notranslate nohighlight">\(n\)</span> models</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Test MSE Aggregate} = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (y_i - \hat{y}_i)^2 = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\Delta y_i)^2
\]</div>
<ul class="simple">
<li><p>but, we have <span class="math notranslate nohighlight">\(n-1\)</span> training data for each model, so we aggregate, by averageing the mean square error of each model,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Train MSE Aggregate} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{n-1} \sum_{i=1}^{n-1} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} \text{Train MSE}_i
\]</div>
<p>Here‚Äôs the leave-one-out cross validation steps,</p>
<ol class="arabic simple">
<li><p>Loop over all <span class="math notranslate nohighlight">\(n\)</span> data, and withhold that data</p></li>
<li><p>Train on the remaining <span class="math notranslate nohighlight">\(n‚àí1\)</span> data and test on the withheld single data</p></li>
<li><p>Calculate model goodness metric, MSE for a single test data is the square error</p></li>
<li><p>Goto 1</p></li>
<li><p>Aggregate model goodness metric over all data, <span class="math notranslate nohighlight">\(n\)</span></p></li>
</ol>
<p>Typically, leave-one-out cross validation is too easy of a prediction problem; therefore, it is not commonly used,</p>
<ul class="simple">
<li><p>but it introduces the concept of exhaustive cross validation, i.e., all data gets tested!</p></li>
</ul>
<p>Leave-one-out cross validation is also exhaustive in the sense that the full combinatorial of <span class="math notranslate nohighlight">\(n\)</span> data choose <span class="math notranslate nohighlight">\(p\)</span> where <span class="math notranslate nohighlight">\(p=1\)</span> are explored,</p>
<div class="math notranslate nohighlight">
\[
\binom{n}{p} = \frac{n!}{p!(n - p)!} = \frac{n!}{1!(n - 1)!} = \frac{n!}{(n - 1)!} = n 
\]</div>
<p>where the full combinatorial is the <span class="math notranslate nohighlight">\(n\)</span> models that we built above!</p>
&#13;

<h2>K-fold Cross Validation (k-fold CV)</h2>
<p>K-fold is a more general, efficient and robust approach.</p>
<ul class="simple">
<li><p>a exhaustive cross validation approach (all data are tested), but it samples a limited set of the possible combinatorial of prediction problems, unlike Leave-one-out cross validation where we attempt every possible case on data withheld for testing</p></li>
<li><p>for K-fold cross validation we assign a single set of K equal size splits and we loop over the splits, withholding the <span class="math notranslate nohighlight">\(k\)</span> split for testing data and using the data outside the split for training</p></li>
<li><p>the testing proportion is <span class="math notranslate nohighlight">\(\frac{1}{K}\)</span>, e.g., for <span class="math notranslate nohighlight">\(K=3\)</span>, 33.3% is withheld for testing, for <span class="math notranslate nohighlight">\(K=4\)</span>, 25% is withheld for testing and for <span class="math notranslate nohighlight">\(K=5\)</span>, 20% is withheld for testing</p></li>
</ul>
<p>We call it K-fold cross validation, because each of the splits is known as a fold. Here‚Äôs the steps for K-fold cross validation,</p>
<ol class="arabic simple">
<li><p>Select <span class="math notranslate nohighlight">\(K\)</span>, integer number of folds</p></li>
<li><p>Split the data into <span class="math notranslate nohighlight">\(K\)</span> equal size folds</p></li>
<li><p>Loop over each <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span> fold</p></li>
<li><p>Assign the data outside the <span class="math notranslate nohighlight">\(k\)</span> fold as training data and inside the <span class="math notranslate nohighlight">\(k\)</span> fold as testing data</p></li>
<li><p>Train and test the prediction model and calculated the testing model metric</p></li>
<li><p>Goto 3</p></li>
<li><p>Aggregate testing model metric over all K folds</p></li>
</ol>
<p>As you can see above k-fold cross validation is exhaustive, since all data is tested, i.e., withheld as testing data, but the method is not exhaustive in that all possible <span class="math notranslate nohighlight">\(\frac{n}{K}\)</span> data subsets are not considered.</p>
<p>To calculated the combinatorial for exhaustive K folds we used the multinomial coefficient,</p>
<div class="math notranslate nohighlight">
\[
\frac{n!}{\left( \frac{n}{K}! \right)^K \cdot K!}
\]</div>
<p>For example, if there are <span class="math notranslate nohighlight">\(n=100\)</span> data and <span class="math notranslate nohighlight">\(K=4\)</span> folds, there are <span class="math notranslate nohighlight">\(6.72 \times 10^55\)</span> possible combinations. I vote that we stick with regular K-fold cross validation.</p>
<p>Let‚Äôs visualize K-fold cross validation splits, for the case of <span class="math notranslate nohighlight">\(K=4\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">K</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">fold_number</span><span class="p">,</span> <span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">test_index</span><span class="p">,</span> <span class="s1">'Fold'</span><span class="p">]</span> <span class="o">=</span> <span class="n">fold_number</span>  <span class="c1"># Assign fold number to test set</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
    <span class="n">df_in</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="p">];</span> <span class="n">df_out</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Fold'</span><span class="p">]</span> <span class="o">!=</span> <span class="n">k</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_in</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_in</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_out</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_out</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">color</span><span class="o">=</span><span class="s1">'white'</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'K-fold #'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">+</span> <span class="s1">', '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png" src="../Images/dd257beaa821a963fc15b3318a7bd9f2.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/614766507683133d44cfac6acbde2fe77809b31d326628000a3baa404147b18a.png"/>
</div>
</div>
&#13;

<h2>Leave-p-out Cross Validation (LpO-CV)</h2>
<p>This is the variant of K-fold cross validation that exhaustively samples the full combinatorial of withholding <span class="math notranslate nohighlight">\(p\)</span> testing data.</p>
<ol class="arabic simple">
<li><p>Select <span class="math notranslate nohighlight">\(p\)</span>, integer number of testing data to withhold</p></li>
<li><p>For all possible <span class="math notranslate nohighlight">\(p\)</span> subsets of <span class="math notranslate nohighlight">\(n\)</span>,</p></li>
<li><p>Assign the data outside the <span class="math notranslate nohighlight">\(p\)</span> as training data and inside the <span class="math notranslate nohighlight">\(p\)</span> as testing data</p></li>
<li><p>Train and test the prediction model and calculated the testing model metric</p></li>
<li><p>Goto 2</p></li>
<li><p>Aggregate testing model metric over the combinatorial</p></li>
</ol>
<p>For this case the combinatorial of cases is, <span class="math notranslate nohighlight">\(n\)</span> choose <span class="math notranslate nohighlight">\(p\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\binom{n}{p} = \frac{n!}{p!(n - p)!}
\]</div>
<p>For <span class="math notranslate nohighlight">\(n=100\)</span> and <span class="math notranslate nohighlight">\(p=20\)</span>, we have <span class="math notranslate nohighlight">\(5.36 \times 10^{20}\)</span> combinations to check!</p>
&#13;

<h2>Limitations of Cross Validation</h2>
<p>Here are some additional issues with the model cross validation approach in general,</p>
<ul class="simple">
<li><p><strong>Peeking, Information Leakage</strong> ‚Äì some information is transmitted from the withheld data into the model, some model decision(s) use all the data. Pipelines and wrappers help with this.</p></li>
<li><p><strong>Black Swans / Stationarity</strong> ‚Äì the model cannot be tested for data events not available in the data. This is also known as the ‚ÄòNo Free Lunch Theorem‚Äô in machine learning</p></li>
</ul>
<p>Consider the words of Hume,</p>
<p>‚Äúeven after the observation of the frequent or constant conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience‚Äù - Hume (1739‚Äì1740)</p>
<ul class="simple">
<li><p>We cannot predict things that we have never seen in our data!</p></li>
</ul>
<p>here‚Äôs a quote from the famous Oreskes et al. (1994) paper on subsurface validation and verification,</p>
<p>‚ÄúVerification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always nonunique. Models can be confirmed by the demonstration of agreement between observation and prediction, but confirmation is inherently partial. Complete confirmation is logically precluded by the fallacy of affirming the consequent and by incomplete access to natural phenomena. Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.‚Äù</p>
<ul class="simple">
<li><p>Oreskes et al. (1994)</p></li>
</ul>
<p>all of this is summed up very well with,</p>
<p>‚ÄòAll models are wrong, but some are useful‚Äô ‚Äì George Box</p>
<p>and a reminder of,</p>
<p><strong>Parsimony</strong> ‚Äì since all models are wrong, an economical description of the system. Occam‚Äôs Razor</p>
<p>resulting in a pragmatic approach of,</p>
<p><strong>Worrying Selectively</strong> ‚Äì since all models are wrong, figure out what is most importantly wrong.</p>
<p>finally, I add my own words,</p>
<p>‚ÄòBe humble, the earth will surprise you!‚Äô ‚Äì Michael Pyrcz</p>
&#13;

<h2>Comments</h2>
<p>This was a basic description of machine learning concepts. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a> and the YouTube lecture links at the start of this chapter with resource links in the videos‚Äô descriptions.</p>
<p>I hope this was helpful,</p>
<p><em>Michael</em></p>
&#13;

<h2>About the Author</h2>
<figure style="text-align: center;">
  <img src="../Images/eb709b2c0a0c715da01ae0165efdf3b2.png" style="display: block; margin: 0 auto; width: 70%;" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_static/intro/michael_pyrcz_officeshot_jacket.jpg"/>
  <figcaption style="text-align: center;"> Professor Michael Pyrcz in his office on the 40 acres, campus of The University of Texas at Austin.
</figcaption>
</figure>
<p>Michael Pyrcz is a professor in the <a class="reference external" href="https://cockrell.utexas.edu/faculty-directory/alphabetical/p">Cockrell School of Engineering</a>, and the <a class="reference external" href="https://www.jsg.utexas.edu/researcher/michael_pyrcz/">Jackson School of Geosciences</a>, at <a class="reference external" href="https://www.utexas.edu/">The University of Texas at Austin</a>, where he researches and teaches subsurface, spatial data analytics, geostatistics, and machine learning. Michael is also,</p>
<ul class="simple">
<li><p>the principal investigator of the <a class="reference external" href="https://fri.cns.utexas.edu/energy-analytics">Energy Analytics</a> freshmen research initiative and a core faculty in the Machine Learn Laboratory in the College of Natural Sciences, The University of Texas at Austin</p></li>
<li><p>an associate editor for <a class="reference external" href="https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board">Computers and Geosciences</a>, and a board member for <a class="reference external" href="https://link.springer.com/journal/11004/editorial-board">Mathematical Geosciences</a>, the International Association for Mathematical Geosciences.</p></li>
</ul>
<p>Michael has written over 70 <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en">peer-reviewed publications</a>, a <a class="reference external" href="https://pypi.org/project/geostatspy/">Python package</a> for spatial data analytics, co-authored a textbook on spatial data analytics, <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistical Reservoir Modeling</a> and author of two recently released e-books, <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy</a> and <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html">Applied Machine Learning in Python: a Hands-on Guide with Code</a>.</p>
<p>All of Michael‚Äôs university lectures are available on his <a class="reference external" href="https://www.youtube.com/@GeostatsGuyLectures">YouTube Channel</a> with links to 100s of Python interactive dashboards and well-documented workflows in over 40 repositories on his <a class="reference external" href="https://github.com/GeostatsGuy">GitHub account</a>, to support any interested students and working professionals with evergreen content. To find out more about Michael‚Äôs work and shared educational resources visit his <span class="xref myst">Website</span>.</p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
<p>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Geostatistics Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://geostatsguy.github.io/MachineLearningDemos_Book/">Applied Machine Learning in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
    
</body>
</html>