<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Gradient Boosting Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Gradient Boosting Trees</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_gradient_boosting.html">https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_gradient_boosting.html</a></blockquote>

<p>Michael J. Pyrcz, Professor, The University of Texas at Austin</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
<p>Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with Code‚Äù.</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite this e-Book as:</p>
<p>Pyrcz, M.J., 2024, <em>Applied Machine Learning in Python: A Hands-on Guide with Code</em> [e-book]. Zenodo. doi:10.5281/zenodo.15169138 <a class="reference external" href="https://doi.org/10.5281/zenodo.15169138"><img alt="DOI" src="../Images/7e4ea662f44af1eae87e87ecbb962ff4.png" data-original-src="https://zenodo.org/badge/863274676.svg"/></a></p>
</div>
<p>The workflows in this book and more are available here:</p>
<div class="remove-from-content-only admonition">
<p class="admonition-title">Cite the MachineLearningDemos GitHub Repository as:</p>
<p>Pyrcz, M.J., 2024, <em>MachineLearningDemos: Python Machine Learning Demonstration Workflows Repository</em> (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312. GitHub repository: <a class="github reference external" href="https://github.com/GeostatsGuy/MachineLearningDemos">GeostatsGuy/MachineLearningDemos</a> <a class="reference external" href="https://zenodo.org/doi/10.5281/zenodo.13835312"><img alt="DOI" src="../Images/4e3a59c17d684b06a170c4af84e0f631.png" data-original-src="https://zenodo.org/badge/862519860.svg"/></a></p>
</div>
<p>By Michael J. Pyrcz <br/>
¬© Copyright 2024.</p>
<p>This chapter is a tutorial for / demonstration of <strong>Gradient Boosting Trees</strong>.</p>
<p><strong>YouTube Lecture</strong>: check out my lectures on:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl">Introduction to Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/JUGo1Pu3QT4?si=ebQXv6Yglar0mYWp">Decision Tree</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/m5_wk310fho?si=up-mzVPHvniXsYE6">Random Forest</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/___T8_ixIwc?si=ozHR_eIuMF3SPTxJ">Gradient Boosting</a></p></li>
</ul>
<p>These lectures are all part of my <a class="reference external" href="https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&amp;si=XonjO2wHdXffMpeI">Machine Learning Course</a> on YouTube with linked well-documented Python workflows and interactive dashboards. My goal is to share accessible, actionable, and repeatable educational content. If you want to know about my motivation, check out <a class="reference external" href="https://michaelpyrcz.com/my-story">Michael‚Äôs Story</a>.</p>
<section id="motivations-for-gradient-boosting-trees">
<h2>Motivations for Gradient Boosting Trees</h2>
<p>Before we can understand gradient boosting trees we first need to cover decision trees. Here‚Äôs the critical concepts for decision trees.</p>
<section id="decision-tree">
<h3>Decision Tree</h3>
<p><strong>Prediction</strong></p>
<ul class="simple">
<li><p>estimate a function <span class="math notranslate nohighlight">\(\hat{f}\)</span> such that we predict a response feature <span class="math notranslate nohighlight">\(Y\)</span> from a set of predictor features <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>.</p></li>
<li><p>the prediction is of the form <span class="math notranslate nohighlight">\(\hat{Y} = \hat{f}(X_1,\ldots,X_m)\)</span></p></li>
</ul>
<p><strong>Supervised Learning</strong></p>
<ul class="simple">
<li><p>the response feature label, <span class="math notranslate nohighlight">\(Y\)</span>, is available over the training and testing data</p></li>
</ul>
<p><strong>Based on an Ensemble of Decision Trees</strong></p>
<p>These are the concepts related to decision tree.</p>
<p><strong>Hierarchical, Binary Segmentation of the Feature Space</strong></p>
<p>The fundamental idea is to divide the predictor space, <span class="math notranslate nohighlight">\(ùëã_1,\ldots,X_m\)</span>, into <span class="math notranslate nohighlight">\(J\)</span> mutually exclusive, exhaustive regions</p>
<ul class="simple">
<li><p><strong>mutually exclusive</strong> ‚Äì any combination of predictors only belongs to a single region, <span class="math notranslate nohighlight">\(R_j\)</span></p></li>
<li><p><strong>exhaustive</strong> ‚Äì all combinations of predictors belong a region, <span class="math notranslate nohighlight">\(R_j\)</span>, regions cover entire feature space (range of the variables being considered)</p></li>
</ul>
<p>For every observation in a region, <span class="math notranslate nohighlight">\(R_j\)</span>, we use the same prediction, <span class="math notranslate nohighlight">\(\hat{Y}(R_j)\)</span></p>
<p>For example predict production, <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, from porosity, <span class="math notranslate nohighlight">\({X_1}\)</span></p>
<ul class="simple">
<li><p>given the data within a mD feature space, <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, find that boundary maximizes the gap between the two categories</p></li>
<li><p>new cases are classified based on where they fall relative to this boundary</p></li>
</ul>
<p><strong>Procedure for Tree Construction</strong></p>
<p>The tree is constructed from the top down.  We begin with a single region that covers the entire feature space and then proceed with a sequence of splits.</p>
<ul class="simple">
<li><p><strong>Scan All Possible Splits</strong> over all regions and over all features.</p></li>
<li><p><strong>Greedy Optimization</strong>  The method proceeds by finding the first segmentation (split) in any feature that minimizes the residual sum of squares of errors over all the training data <span class="math notranslate nohighlight">\(y_i\)</span> over all of the regions <span class="math notranslate nohighlight">\(j = 1,\ldots,J\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<ul class="simple">
<li><p><strong>Stopping Criteria</strong> is typically based on minimum number of training data in each region for a robust estimation and / or minimum reduction in RSS for the next split</p></li>
</ul>
<p>Now we can cover gradient boosting trees that build on the concept of decision trees.</p>
</section>
</section>
<section id="boosting-models">
<h2>Boosting Models</h2>
<p>Boosting additively applies multiple week learners to build a stronger learner.</p>
<ul class="simple">
<li><p>a weak learner is one that offers predictions just marginally better than random selection</p></li>
</ul>
<p>I‚Äôll explain the method with words and then with equations.</p>
<ul class="simple">
<li><p>build a simple model with a high error rate, the model can be quite inaccurate, but moves in the correct direction</p></li>
<li><p>calculate the error from the model</p></li>
<li><p>fit another model to the error</p></li>
<li><p>calculate the error from this addition of the first and second model</p></li>
<li><p>repeat until the desired accuracy is obtained or some other stopping criteria</p></li>
</ul>
<p>The general workflow for predicting <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> is:</p>
<ul class="simple">
<li><p>build a week learner to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
<li><p>loop over number of desired estimators, <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span></p>
<ol class="arabic simple">
<li><p>calculate the residuals at the training data, <span class="math notranslate nohighlight">\(h_k(x_{i}) = y_i - \hat{F}_k(x_{i})\)</span></p></li>
<li><p>fit another week learner to predict <span class="math notranslate nohighlight">\(h_k\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
</ol>
</li>
</ul>
<p>We have a hierarchy of simple <span class="math notranslate nohighlight">\(K\)</span> models.</p>
<ul class="simple">
<li><p>each model builds on the previous to improve the accuracy</p></li>
</ul>
<p>Our regression estimator is the summation over the <span class="math notranslate nohighlight">\(K\)</span> simple models.</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m)
\]</div>
</section>
<section id="gradient-boosting-methods">
<h2>Gradient Boosting Methods</h2>
<p>If you look at the previous method, it becomes clear that it could be mapped to a gradient descent problem</p>
<p>At each step, <span class="math notranslate nohighlight">\(k\)</span>, a model is being fit, then the error is calculated, <span class="math notranslate nohighlight">\(h_k(X_1,\ldots,X_m)\)</span>.</p>
<p>We can assign a loss function</p>
<div class="math notranslate nohighlight">
\[
L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)^2}{2}
\]</div>
<p>So we want to minimize the <span class="math notranslate nohighlight">\(\ell2\)</span> loss function:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right)
\]</div>
<p>by adjusting our model result over our training data <span class="math notranslate nohighlight">\(F(x_1), F(x_2),\ldots,F(x_n)\)</span>.</p>
<p>We can take the partial derivative of the error vs. our model.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i
\]</div>
<p>We can interpret the residuals as negative gradients.</p>
<div class="math notranslate nohighlight">
\[
y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} 
\]</div>
<p>So now we have a gradient descent problem:</p>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + h(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)}
\]</div>
<p>Of the general form:</p>
<div class="math notranslate nohighlight">
\[
\phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k}
\]</div>
<p>where <span class="math notranslate nohighlight">\(phi_k\)</span> is the current state, <span class="math notranslate nohighlight">\(\rho\)</span> is the learning rate, <span class="math notranslate nohighlight">\(J\)</span> is the loss function, and <span class="math notranslate nohighlight">\(\phi_{k+1}\)</span> is the next state of our estimator.</p>
<p>If we consider our residual at training data to be a gradient then we are performing gradient descent.</p>
<ul class="simple">
<li><p>fitting a series of models to negative gradients</p></li>
</ul>
<p>By approaching the problem as a gradient decent problem we are able to apply a variety of loss functions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell2\)</span> is our <span class="math notranslate nohighlight">\(\frac{\left(y - F(X)\right)^2}{2}\)</span> is practical, but is not robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell1\)</span> is our <span class="math notranslate nohighlight">\(|y - F(X)|\)</span> is more robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i))
\]</div>
<ul class="simple">
<li><p>there are others like Huber Loss</p></li>
</ul>
<p><strong>Interpretability</strong></p>
<p>Compared to decision trees, the ensemble methods have reduced interpretability.  One tool to improve model interpretability is feature importance.</p>
<p>We calculate variable importance through calculating the average of:</p>
<ul class="simple">
<li><p>residual sum of square reduction for all splits involving each predictor feature for regression</p></li>
<li><p>the decrease in the Gini index for all splits involving each predictor feature for classification</p></li>
</ul>
<p>Both are standardized to sum to 1.0 over the features.</p>
</section>
<section id="classification">
<h2>Classification</h2>
<p>The response is a finite set of possible categories.</p>
<ul class="simple">
<li><p>For each training data the truth is 100% probability in the observed category and 0% otherwise</p></li>
<li><p>Estimate the probability of each category with the a decision tree</p></li>
<li><p>Use a measure of difference between the true and estimated distributions as the loss function to minimize</p></li>
</ul>
</section>
<section id="load-the-required-libraries">
<h2>Load the Required Libraries</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">math</span>                                                   <span class="c1"># square root operator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># arrays and matrix math</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>                                      <span class="c1"># statistical methods</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span><span class="n">AutoMinorLocator</span><span class="p">,</span><span class="n">FuncFormatter</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>                  <span class="c1"># custom color maps</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                                         <span class="c1"># for matrix scatter plots</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>                <span class="c1"># decision tree method</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>        <span class="c1"># tree-based gradient boosting</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">_tree</span>                                <span class="c1"># for accessing tree information</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>              <span class="c1"># standardize the features</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>                      <span class="c1"># graphical visualization of trees</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span><span class="n">cross_val_score</span><span class="p">,</span><span class="n">train_test_split</span><span class="p">,</span><span class="n">GridSearchCV</span><span class="p">,</span><span class="n">KFold</span><span class="p">)</span> <span class="c1"># model tuning</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Pipeline</span><span class="p">,</span><span class="n">make_pipeline</span><span class="p">)</span>         <span class="c1"># machine learning modeling pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>                     <span class="c1"># custom displays</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># suppress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> 
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>                                                     <span class="c1"># random number seed for workflow repeatability</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
</section>
<section id="declare-functions">
<h2>Declare Functions</h2>
<p>Let‚Äôs define a couple of functions to streamline plotting correlation matrices and visualization of a decision tree regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">comma_format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1">'</span>

<span class="k">def</span> <span class="nf">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">,</span><span class="n">nominal</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span> <span class="c1"># feature ranking plot</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">);</span> <span class="n">mask_low</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">-</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">nominal</span><span class="o">-</span><span class="n">mmin</span><span class="p">);</span> <span class="n">mask_high</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">mmax</span><span class="o">-</span><span class="n">nominal</span><span class="p">);</span> <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span><span class="s1">'r--'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'lightcoral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_low</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">mask_low</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_high</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">mask_high</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">limits</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>                 <span class="c1"># plots a graphical correlation matrix </span>
    <span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>          
    <span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
    <span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">white_low</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">);</span> <span class="n">white_high</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">newcolors</span><span class="p">[</span><span class="n">white_low</span><span class="p">:</span><span class="n">white_high</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                <span class="c1"># mask all correlations less than abs(0.8)</span>
    <span class="n">newcmp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">fignum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">limits</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">limits</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">newcmp</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_bottom</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    
<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks   </span>

<span class="k">def</span> <span class="nf">plot_CDF</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'none'</span><span class="p">):</span>
    <span class="n">cumprob</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">cumprob</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">cumprob</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">visualize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X1_train</span><span class="p">,</span><span class="n">X1_test</span><span class="p">,</span><span class="n">X2_train</span><span class="p">,</span><span class="n">X2_test</span><span class="p">,</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span>
                         <span class="n">ymax</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">annotate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span><span class="c1"># plots the data points and the decision tree prediction </span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>
    <span class="n">X1plot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="mf">300.0</span><span class="p">;</span> <span class="n">X2plot_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">300.0</span> <span class="c1"># resolution of the model visualization</span>
    <span class="n">XX1</span><span class="p">,</span> <span class="n">XX2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X1plot_step</span><span class="p">),</span> <span class="c1"># set up the mesh</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X2plot_step</span><span class="p">))</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">XX2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>    <span class="c1"># predict with our trained model over the mesh</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> 
        <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sp</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1_train</span><span class="p">,</span><span class="n">X2_train</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1_test</span><span class="p">,</span><span class="n">X2_test</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">sp</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>         <span class="c1"># add the color bar</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_hat</span>

<span class="k">def</span> <span class="nf">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">):</span> <span class="c1"># get OOB MSE and cross plot a decision tree </span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">MSE_test</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_hat</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Truth: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Estimated: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],[</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Testing MSE: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">MSE_test</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s1">,.0f</span><span class="si">}</span><span class="s1">'</span><span class="p">),[</span><span class="mi">4200</span><span class="p">,</span><span class="mi">2500</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">display_sidebyside</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>                                <span class="c1"># display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)</span>
    <span class="n">html_str</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">html_str</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span>  <span class="c1"># Using .head() for the first few rows</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s1">'&lt;div style="display: flex;"&gt;</span><span class="si">{</span><span class="n">html_str</span><span class="si">}</span><span class="s1">&lt;/div&gt;'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="set-the-working-directory">
<h2>Set the Working Directory</h2>
<p>I always like to do this so I don‚Äôt lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("c:/PGE383")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).</p>
</section>
<section id="loading-data">
<h2>Loading Data</h2>
<p>Let‚Äôs load the provided multivariate, spatial dataset <a class="reference external" href="https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv">unconv_MV.csv</a> available in my GeoDataSet repo. It is a comma delimited file with:</p>
<ul class="simple">
<li><p>well index (integer)</p></li>
<li><p>porosity (%)</p></li>
<li><p>permeability (<span class="math notranslate nohighlight">\(mD\)</span>)</p></li>
<li><p>acoustic impedance (<span class="math notranslate nohighlight">\(\frac{kg}{m^3} \cdot \frac{m}{s} \cdot 10^6\)</span>).</p></li>
<li><p>brittleness (%)</p></li>
<li><p>total organic carbon (%)</p></li>
<li><p>vitrinite reflectance (%)</p></li>
<li><p>initial gas production (90 day average) (MCFPD)</p></li>
</ul>
<p>We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô and then preview it to make sure it loaded correctly.</p>
<p><strong>Python Tip: using functions from a package</strong> just type the label for the package that we declared at the beginning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
<p>so we can access the pandas function ‚Äòread_csv‚Äô with the command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">()</span>
</pre></div>
</div>
<p>but read csv has required input parameters. The essential one is the name of the file. For our circumstance all the other default parameters are fine. If you want to see all the possible parameters for this function, just go to the docs <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">here</a>.</p>
<ul class="simple">
<li><p>The docs are always helpful</p></li>
<li><p>There is often a lot of flexibility for Python functions, possible through using various inputs parameters</p></li>
</ul>
<p>also, the program has an output, a pandas DataFrame loaded from the data.  So we have to specficy the name / variable representing that new object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"unconv_MV.csv"</span><span class="p">)</span>  
</pre></div>
</div>
<p>Let‚Äôs run this command to load the data and then this command to extract a random subset of the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">);</span> 
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="feature-engineering">
<h2>Feature Engineering</h2>
<p>Let‚Äôs make some changes to the data to improve the workflow:</p>
<ul class="simple">
<li><p><strong>Select the predictor features (x2) and the response feature (x1)</strong>, make sure the metadata is also consistent.</p></li>
<li><p><strong>Metadata</strong> encoding such as the units, labels and display ranges for each feature.</p></li>
<li><p><strong>Reduce the number of data</strong> for ease of visualization (hard to see if too many points on our plots).</p></li>
<li><p><strong>Train and test data split</strong> to demonstrate and visualize simple hyperparameter tuning.</p></li>
<li><p><strong>Add random noise to the data</strong> to demonstrate model overfit. The original data is error free and does not readily demonstrate overfit.</p></li>
</ul>
<p>Given this is properly set, one should be able to use any dataset and features for this demonstration.</p>
<ul class="simple">
<li><p>for brevity we don‚Äôt show any feature selection here. Previous chapter, e.g., k-nearest neighbours include some feature selection methods, but see the feature selection chapter for many possible methods with codes for feature selection.</p></li>
</ul>
</section>
<section id="optional-add-random-noise-to-the-response-feature">
<h2>Optional: Add Random Noise to the Response Feature</h2>
<p>We can do this to observe the impact of data noise on overfit and hyperparameter tuning.</p>
<ul class="simple">
<li><p>This is for experiential learning, of course we wouldn‚Äôt add random noise to our data</p></li>
<li><p>We set the random number seed for reproducibility</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">add_error</span> <span class="o">=</span> <span class="kc">True</span>                                              <span class="c1"># add random error to the response feature</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="mi">1000</span>                                               <span class="c1"># standard deviation of random error, for demonstration only</span>
<span class="n">idata</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv"</span><span class="p">)</span> <span class="c1"># load the data from my github repo</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    
<span class="k">elif</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv"</span><span class="p">)</span> <span class="c1"># load the data </span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.70</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">"Prod"</span><span class="p">:</span> <span class="s2">"Production"</span><span class="p">})</span>
    
<span class="n">yname</span> <span class="o">=</span> <span class="s1">'Production'</span><span class="p">;</span> <span class="n">Xname</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Por'</span><span class="p">,</span><span class="s1">'Brittle'</span><span class="p">]</span>               <span class="c1"># specify the predictor features (x2) and response feature (x1)</span>
<span class="n">Xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">];</span> <span class="n">Xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">25.0</span><span class="p">,</span><span class="mf">100.0</span><span class="p">]</span>                         <span class="c1"># set minimums and maximums for visualization </span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">1500.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">7000.0</span>
<span class="n">Xlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Brittleness'</span><span class="p">];</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Production'</span>    <span class="c1"># specify the feature labels for plotting</span>
<span class="n">Xunit</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'%'</span><span class="p">,</span><span class="s1">'%'</span><span class="p">];</span> <span class="n">yunit</span> <span class="o">=</span> <span class="s1">'MCFPD'</span>
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="k">if</span> <span class="n">add_error</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                         <span class="c1"># method to add error</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>                                 <span class="c1"># set random number seed</span>
    <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df_load</span><span class="p">))</span> <span class="c1"># add noise</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">_get_numeric_data</span><span class="p">();</span> <span class="n">values</span><span class="p">[</span><span class="n">values</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># set negative to 0 in a shallow copy ndarray</span>
    
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                              <span class="c1"># extract selected features as X and y DataFrames</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">Xname</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                                  <span class="c1"># make one DataFrame with both X and y (remove all other features)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs make sure that we have selected reasonable features to build a model</p>
<ul class="simple">
<li><p>the 2 predictor features are not collinear, as this would result in an unstable prediction model</p></li>
<li><p>each of the features are related to the response feature, the predictor features inform the response</p></li>
</ul>
</section>
<section id="calculate-the-correlation-matrix-and-correlation-with-response-ranking">
<h2>Calculate the Correlation Matrix and Correlation with Response Ranking</h2>
<p>Let‚Äôs start with correlation analysis. We can calculate and view the correlation matrix and correlation to the response features with these previously declared functions.</p>
<ul class="simple">
<li><p>correlation analysis is based on the assumption of linear relationships, but it is a good start</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlation</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="s1">'Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>           <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">Xname</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">yname</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f16d805e35f1c39ce293ec74df9132232e9f79c7ca0cb2ab8b63641a714d99c3.png" src="../Images/3e93a8978cec90fe97483ce65cda346f.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f16d805e35f1c39ce293ec74df9132232e9f79c7ca0cb2ab8b63641a714d99c3.png"/>
</div>
</div>
<p>Note the 1.0 diagonal resulting from the correlation of each variable with themselves.</p>
<p>This looks good.  There is a mix of correlation magnitudes. Of course, correlation coefficients are limited to degree of linear correlations.</p>
<ul class="simple">
<li><p>Let‚Äôs look at the matrix scatter plot to see the pairwise relationship between the features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="nb">vars</span><span class="o">=</span><span class="n">Xname</span><span class="o">+</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b641089892114f3044ca3ab9a43c2723da5e4105a26416ee749176d163822c57.png" src="../Images/8f6fc0dc9253e57bb674c41069015dde.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/b641089892114f3044ca3ab9a43c2723da5e4105a26416ee749176d163822c57.png"/>
</div>
</div>
</section>
<section id="train-and-test-split">
<h2>Train and Test Split</h2>
<p>For convenience and simplicity we use scikit-learn‚Äôs random train and test split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># make one train DataFrame with both X and y (remove all other features)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># make one testin DataFrame with both X and y (remove all other features)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-the-dataframe">
<h2>Visualize the DataFrame</h2>
<p>Visualizing the train and test DataFrame is useful check before we build our models.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice and clean format, see below).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'       Training DataFrame          Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span><span class="n">df_test</span><span class="p">)</span>                          <span class="c1"># custom function for side-by-side DataFrame display</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>       Training DataFrame          Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>86</th>
      <td>12.83</td>
      <td>29.87</td>
      <td>995.700671</td>
    </tr>
    <tr>
      <th>35</th>
      <td>17.39</td>
      <td>56.43</td>
      <td>6060.760806</td>
    </tr>
    <tr>
      <th>75</th>
      <td>12.23</td>
      <td>40.67</td>
      <td>3744.177137</td>
    </tr>
    <tr>
      <th>36</th>
      <td>13.72</td>
      <td>40.24</td>
      <td>4203.470533</td>
    </tr>
    <tr>
      <th>126</th>
      <td>12.83</td>
      <td>17.20</td>
      <td>2917.165695</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>15.55</td>
      <td>58.25</td>
      <td>5619.930037</td>
    </tr>
    <tr>
      <th>46</th>
      <td>20.21</td>
      <td>23.78</td>
      <td>3897.440411</td>
    </tr>
    <tr>
      <th>96</th>
      <td>15.07</td>
      <td>39.39</td>
      <td>4504.608029</td>
    </tr>
    <tr>
      <th>45</th>
      <td>12.10</td>
      <td>63.24</td>
      <td>3613.953926</td>
    </tr>
    <tr>
      <th>105</th>
      <td>19.54</td>
      <td>37.40</td>
      <td>5314.937997</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
</section>
<section id="summary-statistics-for-tabular-data">
<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames.</p>
<ul class="simple">
<li><p>The describe command provides count, mean, minimum, maximum in a nice data table.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'            Training DataFrame                      Testing DataFrame'</span><span class="p">)</span>    <span class="c1"># custom function for side-by-side summary statistics</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]],</span><span class="n">df_test</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>            Training DataFrame                      Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>105.000000</td>
      <td>105.000000</td>
      <td>105.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>14.859238</td>
      <td>48.861143</td>
      <td>4192.479746</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.057228</td>
      <td>14.432050</td>
      <td>1347.391355</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7.220000</td>
      <td>10.940000</td>
      <td>357.449794</td>
    </tr>
    <tr>
      <th>max</th>
      <td>23.550000</td>
      <td>84.330000</td>
      <td>7934.478879</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>35.000000</td>
      <td>35.000000</td>
      <td>35.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>15.011714</td>
      <td>46.798286</td>
      <td>4431.830496</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.574467</td>
      <td>13.380910</td>
      <td>1487.184992</td>
    </tr>
    <tr>
      <th>min</th>
      <td>6.550000</td>
      <td>20.120000</td>
      <td>1572.738774</td>
    </tr>
    <tr>
      <th>max</th>
      <td>20.860000</td>
      <td>68.760000</td>
      <td>7668.639376</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
<p>It is good that we checked the summary statistics.</p>
<ul class="simple">
<li><p>there are no obvious issues</p></li>
<li><p>check out the range of values for each feature to set up and adjust plotting limits. See above.</p></li>
</ul>
</section>
<section id="visualize-the-train-and-test-splits">
<h2>Visualize the Train and Test Splits</h2>
<p>Let‚Äôs check the consistency and coverage of training and testing with histograms and scatter plots.</p>
<ul class="simple">
<li><p>check to make sure the training and testing cover the range of possible feature combinations</p></li>
<li><p>ensure we are not extrapolating beyond the training data with the testing cases</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png" src="../Images/6c7c51e5c24b5320a250e5d2881b5021.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png"/>
</div>
</div>
<p>Sometimes I find it more convenient to compare distributions by looking at CDF‚Äôs instead of histograms.</p>
<ul class="simple">
<li><p>we avoid the arbitrary choice of histogram bin size, because CDF‚Äôs are at the data resolution.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>                                              <span class="c1"># response feature CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png" src="../Images/e1017bcd0dfe07284cc05b94b7dd3861.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png"/>
</div>
</div>
<p>Once again, the distributions are well behaved,</p>
<ul class="simple">
<li><p>we cannot observe obvious gaps nor truncations.</p></li>
<li><p>check coverage of the train and test data</p></li>
</ul>
<p>Let‚Äôs look at a scatter plot of Porosity vs. Brittleness with points colored by Production.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># visualize the train and test data in predictor feature space</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
    <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
    <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training '</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' vs. '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' and '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png" src="../Images/79e12ff1ac255ff9811cd9900912d141.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png"/>
</div>
</div>
</section>
<section id="tree-based-boosting">
<h2>Tree-based Boosting</h2>
<p>To perform tree-based boosting, gradient boosting tree regression we:</p>
<ol class="arabic simple">
<li><p>set the hyperparameters for our model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                 <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                      <span class="c1"># number of trees</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                     
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                            <span class="c1"># tree construction criterion</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>instantiate the model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">boost_tree</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>train the model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">boot_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>visualize the model result over the feature space (easy to do as we have only 2 predictor features)</p></li>
</ol>
</section>
<section id="demonstration-of-boosting">
<h2>Demonstration of Boosting</h2>
<p>For demonstration let‚Äôs set tree maximum depth to 1 and 6 tree-based boosting regression trees.</p>
<ul class="simple">
<li><p>each tree only has a single split, called decision stumps. This will prevent interaction between the predictor features and be highly interpretable</p></li>
</ul>
<p>You should be able to observe the additive nature of the trees, see the first tree and then the first plus the second tree and so on.</p>
<ul class="simple">
<li><p>recall the estimate is the summation of multiple trees</p></li>
<li><p>since we are working with fitting a gradient after the first tree, we can have negative and positive estimates</p></li>
<li><p>in this example we can see some production estimates that are actually negative</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                       
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
                                           
<span class="n">num_trees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>                              <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0689fdfd411d646efca1490a036e9354bc12c7bd5797bac40f5b0295fb0f11a3.png" src="../Images/b2a16fd01b6c4a7a3f642f8cf90844b3.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/0689fdfd411d646efca1490a036e9354bc12c7bd5797bac40f5b0295fb0f11a3.png"/>
</div>
</div>
<p>Notice that there is significant misfit with the data</p>
<ul class="simple">
<li><p>we have only used up to 6 decision stumps (1 decision tree)</p></li>
</ul>
<p>Let‚Äôs check the cross validation results with the withheld testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dca4db99b262b1382fe6624eace15ad6cfe1a52dd28209db869d61cb97d8a540.png" src="../Images/c19d0d925995b408d0e14db0f66b52ee.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/dca4db99b262b1382fe6624eace15ad6cfe1a52dd28209db869d61cb97d8a540.png"/>
</div>
</div>
<p>Of course with a single tree we do quite poorly, but by the time we get to 6 stump trees we cut the MSE almost in half.</p>
</section>
<section id="time-to-build-more-trees">
<h2>Time to Build More Trees</h2>
<p>Now let‚Äôs demonstrate the result of utilizing many more trees in our tree-based boosting model.</p>
<ul class="simple">
<li><p>we will still work with simple decision stumps, don‚Äôt worry we will add more later</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad50902645fae1091779c09e40ad78025d6ddf1261c9d5a405eb2af496ddd2df.png" src="../Images/3baea417052c8c32978d193e173228b1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ad50902645fae1091779c09e40ad78025d6ddf1261c9d5a405eb2af496ddd2df.png"/>
</div>
</div>
<p>See the plaid pattern? It is due to the use of decision stumps, and:</p>
<ul class="simple">
<li><p>an additive model</p></li>
<li><p>all models contribute to all predictions</p></li>
</ul>
<p>See the dark and bright regions?</p>
<ul class="simple">
<li><p>the additive model may extrapolate outside the data range</p></li>
</ul>
<p>Let‚Äôs cross validate with our testing data to see how our model has improved with more trees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f6fae7eb28cbf98d98c8bfbf82659a7d3bcb67136928521595c9e0374bdc1147.png" src="../Images/3f37a775a250322ae1595efdbfe1669d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f6fae7eb28cbf98d98c8bfbf82659a7d3bcb67136928521595c9e0374bdc1147.png"/>
</div>
</div>
<p>Around 20 trees we get our best performance and then we start to degrade, we are likely starting to overfit the training data.</p>
</section>
<section id="going-beyond-decision-stumps">
<h2>Going Beyond Decision Stumps</h2>
<p>As state before with decision stumps we prevent interactions between features.</p>
<p>Let‚Äôs extend to tree depth of 2</p>
<ul class="simple">
<li><p>two nested decisions resulting in 4 terminal nodes</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07951866b16ac8dd604974240c5a929632a2a45dccbbed9c7a5b1fcc6d5e3f6e.png" src="../Images/65161de226c63aae698a6005291afae4.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/07951866b16ac8dd604974240c5a929632a2a45dccbbed9c7a5b1fcc6d5e3f6e.png"/>
</div>
</div>
<p>We have much more flexibility now.</p>
<ul class="simple">
<li><p>with one tree we have 4 terminal nodes (regions)</p></li>
<li><p>with only 6 trees we are capturing some complicate features</p></li>
</ul>
<p>Let‚Äôs increase the tree depth one more time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aba462916f779681b9aa90d6b1a3a548e3d43d5ad379c5af304e09a38658a793.png" src="../Images/b36175195239341c3b7ea8c842eea5b7.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/aba462916f779681b9aa90d6b1a3a548e3d43d5ad379c5af304e09a38658a793.png"/>
</div>
</div>
<p>One more time, it is common to use trees with depths of 4-8, so let‚Äôs try 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1739c0986e82122c4de50870f59b64abac148392710ba70c39c9f279197c0704.png" src="../Images/d2ae5319fcaa86e15b8c1c77df29902e.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1739c0986e82122c4de50870f59b64abac148392710ba70c39c9f279197c0704.png"/>
</div>
</div>
<p>Let‚Äôs cross validate the model with testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/76eb33533b0470061aefadc2a8ad8b34dcd8adaa5de34b3a7fef0efc8af812ca.png" src="../Images/cdbb3edec9e81f128298ed05c7cd6189.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/76eb33533b0470061aefadc2a8ad8b34dcd8adaa5de34b3a7fef0efc8af812ca.png"/>
</div>
</div>
<p>With a max tree depth of 5 our model performance peaks early and the addition of more trees has no impact.</p>
<ul class="simple">
<li><p>of course this is not a thorough analysis</p></li>
</ul>
<p>Let‚Äôs try something more thorough</p>
<ul class="simple">
<li><p>we will cross validate models with <span class="math notranslate nohighlight">\(1,\ldots,100\)</span> trees with max tree depths of <span class="math notranslate nohighlight">\(1, 2, 3, 10\)</span>.</p></li>
<li><p>we also slow down the learning rate, I increased it above to amplify the difference of the outputs for demonstration, but know we want to see the best possible model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">num_trees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MSE1_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE2_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE3_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE4_list</span> <span class="o">=</span> <span class="p">[]</span> 

<span class="n">params1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>                                        <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees in our random forest</span>
    <span class="n">boosting_model1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test1_hat</span> <span class="o">=</span> <span class="n">boosting_model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE1_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test1_hat</span><span class="p">))</span>
    
    <span class="n">boosting_model2</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test2_hat</span> <span class="o">=</span> <span class="n">boosting_model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE2_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test2_hat</span><span class="p">))</span>

    <span class="n">boosting_model3</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test3_hat</span> <span class="o">=</span> <span class="n">boosting_model3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE3_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test3_hat</span><span class="p">))</span>

    <span class="n">boosting_model4</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test4_hat</span> <span class="o">=</span> <span class="n">boosting_model4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE4_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test4_hat</span><span class="p">))</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                            <span class="c1"># plot jackknife results for all cases</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE1_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE2_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE3_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 3"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE4_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 10"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Testing Mean Square Error vs. Number of Trees'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Number of Trees'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ae7e45a290e0fe1fb8cb9fe76b23eff4132746b7765f4629c874d06b593e0ca6.png" src="../Images/c2ee3e9f993c37a05e91acb8b4ce5baf.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ae7e45a290e0fe1fb8cb9fe76b23eff4132746b7765f4629c874d06b593e0ca6.png"/>
</div>
</div>
<p>That‚Äôs interesting:</p>
<ul class="simple">
<li><p>with increasing tree depths our model may improve</p></li>
<li><p>more tree depth requires fewer trees for improved accuracy</p></li>
<li><p>with tree depths of 2 and 3 the models behave the same and after 10-15 trees level off, they are resistant to overfit</p></li>
<li><p>with tree depth of 10, the number of trees has no impact of model performance</p></li>
</ul>
</section>
<section id="gradient-descent-hyperparameters">
<h2>Gradient Descent Hyperparameters</h2>
<p>The learning rate scales the additive impact of each additive tree to the overall model prediction.</p>
<ul class="simple">
<li><p>lower learning rate will slow the convergence to a solution</p></li>
<li><p>lower learning rate will help us not skip over an optimum solution</p></li>
</ul>
<p>We won‚Äôt spend much time on this, but let‚Äôs just try changing the learning rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">learning_rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">MSE1_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE2_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE3_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE4_list</span> <span class="o">=</span> <span class="p">[]</span> 

<span class="n">params1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                        <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">learning_rate</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees in our random forest</span>
    <span class="n">boosting_model1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test1_hat</span> <span class="o">=</span> <span class="n">boosting_model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE1_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test1_hat</span><span class="p">))</span>
    
    <span class="n">boosting_model2</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test2_hat</span> <span class="o">=</span> <span class="n">boosting_model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE2_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test2_hat</span><span class="p">))</span>

    <span class="n">boosting_model3</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test3_hat</span> <span class="o">=</span> <span class="n">boosting_model3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE3_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test3_hat</span><span class="p">))</span>

    <span class="n">boosting_model4</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test4_hat</span> <span class="o">=</span> <span class="n">boosting_model4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE4_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test4_hat</span><span class="p">))</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                            <span class="c1"># plot jackknife results for all cases</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE1_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE2_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE3_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 3"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE4_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 10"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Testing Mean Square Error vs. Learning Rate'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Learning Rate'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/011852fab9e64d42c5e7ac6bcaa69e4f061994730784f35584ff27e0d4ba9831.png" src="../Images/d130f276273e2b117adb7bf8d1768618.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/011852fab9e64d42c5e7ac6bcaa69e4f061994730784f35584ff27e0d4ba9831.png"/>
</div>
</div>
<p>Once again, a very interesting result.</p>
<ul class="simple">
<li><p>regardless of tree complexity, it is better to learn slowly!</p></li>
</ul>
</section>
<section id="machine-learning-pipelines-for-clean-compact-machine-learning-code">
<h2>Machine Learning Pipelines for Clean, Compact Machine Learning Code</h2>
<p>Pipelines are a scikit-learn class that allows for the encapsulation of a sequence of data preparation and modeling steps</p>
<ul class="simple">
<li><p>then we can treat the pipeline as an object in our much condensed workflow</p></li>
</ul>
<p>The pipeline class allows us to:</p>
<ul class="simple">
<li><p>improve code readability and to keep everything straight</p></li>
<li><p>build complete workflows with very few lines of readable code</p></li>
<li><p>avoid common workflow problems like data leakage, testing data informing model parameter training</p></li>
<li><p>abstract common machine learning modeling and focus on building the best model possible</p></li>
</ul>
<p>The fundamental philosophy is to treat machine learning as a combinatorial search to find the best model (AutoML)</p>
<p>For more information see my recorded lecture on <a class="reference external" href="https://www.youtube.com/watch?v=tYrPs8s1l9U&amp;list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&amp;index=5">Machine Learning Pipelines</a> and a well-documented demonstration <a class="reference external" href="http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb">Machine Learning Pipeline Workflow</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x1</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">;</span> <span class="n">x2</span> <span class="o">=</span> <span class="mf">0.3</span>                                           <span class="c1"># predictor values for the prediction</span>

<span class="n">pipe_boosting</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>                                      <span class="c1"># the machine learning workflow as a pipeline object</span>
    <span class="p">(</span><span class="s1">'boosting'</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>                                                    <span class="c1"># the machine learning workflow method's parameters to search</span>
    <span class="s1">'boosting__learning_rate'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="s1">'boosting__n_estimators'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">dtype</span> <span class="o">=</span> <span class="nb">int</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">tuned_boosting</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe_boosting</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">scoring</span> <span class="o">=</span> <span class="s1">'neg_mean_squared_error'</span><span class="p">,</span> <span class="c1"># hyperparameter tuning w. grid search k-fold cross validation </span>
                             <span class="n">refit</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tuned_boosting</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>                                         <span class="c1"># tune the model with k-fold and then train the model will the data </span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Tuned hyperparameter: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tuned_boosting</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>

<span class="n">estimate</span> <span class="o">=</span> <span class="n">tuned_boosting</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>                 <span class="c1"># make a prediction (no tuning shown)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Estimated '</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' for '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' and '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>  <span class="o">+</span> <span class="s1">' is '</span> <span class="o">+</span> 
      <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">estimate</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">yunit</span><span class="p">)</span>                     <span class="c1"># print results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Tuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD
</pre></div>
</div>
</div>
</div>
</section>
<section id="comments">
<h2>Comments</h2>
<p>I hope you found this chapter helpful. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a>,</p>
<p><em>Michael</em></p>
</section>
<section id="the-author">
<h2>The Author:</h2>
<p>Michael Pyrcz, Professor, The University of Texas at Austin
<em>Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions</em></p>
<p>With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers‚Äô and geoscientists‚Äô impact in subsurface resource development.</p>
<p>For more about Michael check out these links:</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a> | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
</section>
<section id="want-to-work-together">
<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
</section>
<section id="more-resources-available-at-twitter-github-website-googlescholar-book-youtube-applied-geostats-in-python-e-book-linkedin">
<h2>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></h2>
</section>
&#13;

<h2>Motivations for Gradient Boosting Trees</h2>
<p>Before we can understand gradient boosting trees we first need to cover decision trees. Here‚Äôs the critical concepts for decision trees.</p>
<section id="decision-tree">
<h3>Decision Tree</h3>
<p><strong>Prediction</strong></p>
<ul class="simple">
<li><p>estimate a function <span class="math notranslate nohighlight">\(\hat{f}\)</span> such that we predict a response feature <span class="math notranslate nohighlight">\(Y\)</span> from a set of predictor features <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>.</p></li>
<li><p>the prediction is of the form <span class="math notranslate nohighlight">\(\hat{Y} = \hat{f}(X_1,\ldots,X_m)\)</span></p></li>
</ul>
<p><strong>Supervised Learning</strong></p>
<ul class="simple">
<li><p>the response feature label, <span class="math notranslate nohighlight">\(Y\)</span>, is available over the training and testing data</p></li>
</ul>
<p><strong>Based on an Ensemble of Decision Trees</strong></p>
<p>These are the concepts related to decision tree.</p>
<p><strong>Hierarchical, Binary Segmentation of the Feature Space</strong></p>
<p>The fundamental idea is to divide the predictor space, <span class="math notranslate nohighlight">\(ùëã_1,\ldots,X_m\)</span>, into <span class="math notranslate nohighlight">\(J\)</span> mutually exclusive, exhaustive regions</p>
<ul class="simple">
<li><p><strong>mutually exclusive</strong> ‚Äì any combination of predictors only belongs to a single region, <span class="math notranslate nohighlight">\(R_j\)</span></p></li>
<li><p><strong>exhaustive</strong> ‚Äì all combinations of predictors belong a region, <span class="math notranslate nohighlight">\(R_j\)</span>, regions cover entire feature space (range of the variables being considered)</p></li>
</ul>
<p>For every observation in a region, <span class="math notranslate nohighlight">\(R_j\)</span>, we use the same prediction, <span class="math notranslate nohighlight">\(\hat{Y}(R_j)\)</span></p>
<p>For example predict production, <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, from porosity, <span class="math notranslate nohighlight">\({X_1}\)</span></p>
<ul class="simple">
<li><p>given the data within a mD feature space, <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, find that boundary maximizes the gap between the two categories</p></li>
<li><p>new cases are classified based on where they fall relative to this boundary</p></li>
</ul>
<p><strong>Procedure for Tree Construction</strong></p>
<p>The tree is constructed from the top down.  We begin with a single region that covers the entire feature space and then proceed with a sequence of splits.</p>
<ul class="simple">
<li><p><strong>Scan All Possible Splits</strong> over all regions and over all features.</p></li>
<li><p><strong>Greedy Optimization</strong>  The method proceeds by finding the first segmentation (split) in any feature that minimizes the residual sum of squares of errors over all the training data <span class="math notranslate nohighlight">\(y_i\)</span> over all of the regions <span class="math notranslate nohighlight">\(j = 1,\ldots,J\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<ul class="simple">
<li><p><strong>Stopping Criteria</strong> is typically based on minimum number of training data in each region for a robust estimation and / or minimum reduction in RSS for the next split</p></li>
</ul>
<p>Now we can cover gradient boosting trees that build on the concept of decision trees.</p>
</section>
&#13;

<h3>Decision Tree</h3>
<p><strong>Prediction</strong></p>
<ul class="simple">
<li><p>estimate a function <span class="math notranslate nohighlight">\(\hat{f}\)</span> such that we predict a response feature <span class="math notranslate nohighlight">\(Y\)</span> from a set of predictor features <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>.</p></li>
<li><p>the prediction is of the form <span class="math notranslate nohighlight">\(\hat{Y} = \hat{f}(X_1,\ldots,X_m)\)</span></p></li>
</ul>
<p><strong>Supervised Learning</strong></p>
<ul class="simple">
<li><p>the response feature label, <span class="math notranslate nohighlight">\(Y\)</span>, is available over the training and testing data</p></li>
</ul>
<p><strong>Based on an Ensemble of Decision Trees</strong></p>
<p>These are the concepts related to decision tree.</p>
<p><strong>Hierarchical, Binary Segmentation of the Feature Space</strong></p>
<p>The fundamental idea is to divide the predictor space, <span class="math notranslate nohighlight">\(ùëã_1,\ldots,X_m\)</span>, into <span class="math notranslate nohighlight">\(J\)</span> mutually exclusive, exhaustive regions</p>
<ul class="simple">
<li><p><strong>mutually exclusive</strong> ‚Äì any combination of predictors only belongs to a single region, <span class="math notranslate nohighlight">\(R_j\)</span></p></li>
<li><p><strong>exhaustive</strong> ‚Äì all combinations of predictors belong a region, <span class="math notranslate nohighlight">\(R_j\)</span>, regions cover entire feature space (range of the variables being considered)</p></li>
</ul>
<p>For every observation in a region, <span class="math notranslate nohighlight">\(R_j\)</span>, we use the same prediction, <span class="math notranslate nohighlight">\(\hat{Y}(R_j)\)</span></p>
<p>For example predict production, <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, from porosity, <span class="math notranslate nohighlight">\({X_1}\)</span></p>
<ul class="simple">
<li><p>given the data within a mD feature space, <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, find that boundary maximizes the gap between the two categories</p></li>
<li><p>new cases are classified based on where they fall relative to this boundary</p></li>
</ul>
<p><strong>Procedure for Tree Construction</strong></p>
<p>The tree is constructed from the top down.  We begin with a single region that covers the entire feature space and then proceed with a sequence of splits.</p>
<ul class="simple">
<li><p><strong>Scan All Possible Splits</strong> over all regions and over all features.</p></li>
<li><p><strong>Greedy Optimization</strong>  The method proceeds by finding the first segmentation (split) in any feature that minimizes the residual sum of squares of errors over all the training data <span class="math notranslate nohighlight">\(y_i\)</span> over all of the regions <span class="math notranslate nohighlight">\(j = 1,\ldots,J\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2
\]</div>
<ul class="simple">
<li><p><strong>Stopping Criteria</strong> is typically based on minimum number of training data in each region for a robust estimation and / or minimum reduction in RSS for the next split</p></li>
</ul>
<p>Now we can cover gradient boosting trees that build on the concept of decision trees.</p>
&#13;

<h2>Boosting Models</h2>
<p>Boosting additively applies multiple week learners to build a stronger learner.</p>
<ul class="simple">
<li><p>a weak learner is one that offers predictions just marginally better than random selection</p></li>
</ul>
<p>I‚Äôll explain the method with words and then with equations.</p>
<ul class="simple">
<li><p>build a simple model with a high error rate, the model can be quite inaccurate, but moves in the correct direction</p></li>
<li><p>calculate the error from the model</p></li>
<li><p>fit another model to the error</p></li>
<li><p>calculate the error from this addition of the first and second model</p></li>
<li><p>repeat until the desired accuracy is obtained or some other stopping criteria</p></li>
</ul>
<p>The general workflow for predicting <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> is:</p>
<ul class="simple">
<li><p>build a week learner to predict <span class="math notranslate nohighlight">\(Y\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
<li><p>loop over number of desired estimators, <span class="math notranslate nohighlight">\(k = 1,\ldots,K\)</span></p>
<ol class="arabic simple">
<li><p>calculate the residuals at the training data, <span class="math notranslate nohighlight">\(h_k(x_{i}) = y_i - \hat{F}_k(x_{i})\)</span></p></li>
<li><p>fit another week learner to predict <span class="math notranslate nohighlight">\(h_k\)</span> from <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>, <span class="math notranslate nohighlight">\(\hat{F}_k(X)\)</span> from the training data <span class="math notranslate nohighlight">\(x_{i,j}\)</span>.</p></li>
</ol>
</li>
</ul>
<p>We have a hierarchy of simple <span class="math notranslate nohighlight">\(K\)</span> models.</p>
<ul class="simple">
<li><p>each model builds on the previous to improve the accuracy</p></li>
</ul>
<p>Our regression estimator is the summation over the <span class="math notranslate nohighlight">\(K\)</span> simple models.</p>
<div class="math notranslate nohighlight">
\[
\hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m)
\]</div>
&#13;

<h2>Gradient Boosting Methods</h2>
<p>If you look at the previous method, it becomes clear that it could be mapped to a gradient descent problem</p>
<p>At each step, <span class="math notranslate nohighlight">\(k\)</span>, a model is being fit, then the error is calculated, <span class="math notranslate nohighlight">\(h_k(X_1,\ldots,X_m)\)</span>.</p>
<p>We can assign a loss function</p>
<div class="math notranslate nohighlight">
\[
L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)^2}{2}
\]</div>
<p>So we want to minimize the <span class="math notranslate nohighlight">\(\ell2\)</span> loss function:</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right)
\]</div>
<p>by adjusting our model result over our training data <span class="math notranslate nohighlight">\(F(x_1), F(x_2),\ldots,F(x_n)\)</span>.</p>
<p>We can take the partial derivative of the error vs. our model.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i
\]</div>
<p>We can interpret the residuals as negative gradients.</p>
<div class="math notranslate nohighlight">
\[
y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} 
\]</div>
<p>So now we have a gradient descent problem:</p>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + h(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i)
\]</div>
<div class="math notranslate nohighlight">
\[
F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)}
\]</div>
<p>Of the general form:</p>
<div class="math notranslate nohighlight">
\[
\phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k}
\]</div>
<p>where <span class="math notranslate nohighlight">\(phi_k\)</span> is the current state, <span class="math notranslate nohighlight">\(\rho\)</span> is the learning rate, <span class="math notranslate nohighlight">\(J\)</span> is the loss function, and <span class="math notranslate nohighlight">\(\phi_{k+1}\)</span> is the next state of our estimator.</p>
<p>If we consider our residual at training data to be a gradient then we are performing gradient descent.</p>
<ul class="simple">
<li><p>fitting a series of models to negative gradients</p></li>
</ul>
<p>By approaching the problem as a gradient decent problem we are able to apply a variety of loss functions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell2\)</span> is our <span class="math notranslate nohighlight">\(\frac{\left(y - F(X)\right)^2}{2}\)</span> is practical, but is not robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\ell1\)</span> is our <span class="math notranslate nohighlight">\(|y - F(X)|\)</span> is more robust with outliers</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
- 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i))
\]</div>
<ul class="simple">
<li><p>there are others like Huber Loss</p></li>
</ul>
<p><strong>Interpretability</strong></p>
<p>Compared to decision trees, the ensemble methods have reduced interpretability.  One tool to improve model interpretability is feature importance.</p>
<p>We calculate variable importance through calculating the average of:</p>
<ul class="simple">
<li><p>residual sum of square reduction for all splits involving each predictor feature for regression</p></li>
<li><p>the decrease in the Gini index for all splits involving each predictor feature for classification</p></li>
</ul>
<p>Both are standardized to sum to 1.0 over the features.</p>
&#13;

<h2>Classification</h2>
<p>The response is a finite set of possible categories.</p>
<ul class="simple">
<li><p>For each training data the truth is 100% probability in the observed category and 0% otherwise</p></li>
<li><p>Estimate the probability of each category with the a decision tree</p></li>
<li><p>Use a measure of difference between the true and estimated distributions as the loss function to minimize</p></li>
</ul>
&#13;

<h2>Load the Required Libraries</h2>
<p>We will also need some standard packages. These should have been installed with Anaconda 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="o">%</span><span class="k">matplotlib</span> inline                                         
<span class="n">suppress_warnings</span> <span class="o">=</span> <span class="kc">True</span>
<span class="kn">import</span> <span class="nn">os</span>                                                     <span class="c1"># to set current working directory </span>
<span class="kn">import</span> <span class="nn">math</span>                                                   <span class="c1"># square root operator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>                                            <span class="c1"># arrays and matrix math</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>                                      <span class="c1"># statistical methods</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>                                           <span class="c1"># DataFrames</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>                               <span class="c1"># for plotting</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="p">(</span><span class="n">MultipleLocator</span><span class="p">,</span><span class="n">AutoMinorLocator</span><span class="p">,</span><span class="n">FuncFormatter</span><span class="p">)</span> <span class="c1"># control of axes ticks</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>                  <span class="c1"># custom color maps</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>                                         <span class="c1"># for matrix scatter plots</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>                <span class="c1"># decision tree method</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>        <span class="c1"># tree-based gradient boosting</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">_tree</span>                                <span class="c1"># for accessing tree information</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>              <span class="c1"># standardize the features</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>                      <span class="c1"># graphical visualization of trees</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span><span class="n">cross_val_score</span><span class="p">,</span><span class="n">train_test_split</span><span class="p">,</span><span class="n">GridSearchCV</span><span class="p">,</span><span class="n">KFold</span><span class="p">)</span> <span class="c1"># model tuning</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Pipeline</span><span class="p">,</span><span class="n">make_pipeline</span><span class="p">)</span>         <span class="c1"># machine learning modeling pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>                                   <span class="c1"># measures to check our models</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>           <span class="c1"># multi-processor K-fold crossvalidation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>          <span class="c1"># train and test split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>                     <span class="c1"># custom displays</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>                                         <span class="c1"># default color bar, no bias and friendly for color vision defeciency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">axisbelow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                                <span class="c1"># grid behind plotting elements</span>
<span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>  
    <span class="kn">import</span> <span class="nn">warnings</span>                                           <span class="c1"># suppress any warnings for this demonstration</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> 
<span class="n">seed</span> <span class="o">=</span> <span class="mi">13</span>                                                     <span class="c1"># random number seed for workflow repeatability</span>
</pre></div>
</div>
</div>
</div>
<p>If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available with the respective package docs.</p>
&#13;

<h2>Declare Functions</h2>
<p>Let‚Äôs define a couple of functions to streamline plotting correlation matrices and visualization of a decision tree regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">comma_format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s1">,</span><span class="si">}</span><span class="s1">'</span>

<span class="k">def</span> <span class="nf">feature_rank_plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">,</span><span class="n">nominal</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span> <span class="c1"># feature ranking plot</span>
    <span class="n">mpred</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">);</span> <span class="n">mask_low</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">-</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">nominal</span><span class="o">-</span><span class="n">mmin</span><span class="p">);</span> <span class="n">mask_high</span> <span class="o">=</span> <span class="n">nominal</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="p">(</span><span class="n">mmax</span><span class="o">-</span><span class="n">nominal</span><span class="p">);</span> <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">metric</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span><span class="s1">'r--'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'dodgerblue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mpred</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">nominal</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'lightcoral'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_low</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&lt;</span> <span class="n">mask_low</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">mpred</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">mpred</span><span class="p">,</span><span class="n">mask_high</span><span class="p">),</span><span class="n">metric</span><span class="p">,</span><span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">metric</span> <span class="o">&gt;</span> <span class="n">mask_high</span><span class="p">),</span><span class="n">interpolate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Predictor Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span><span class="n">mmax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">1.5</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">limits</span><span class="p">,</span><span class="n">mask</span><span class="p">):</span>                 <span class="c1"># plots a graphical correlation matrix </span>
    <span class="n">my_colormap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'RdBu_r'</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>          
    <span class="n">newcolors</span> <span class="o">=</span> <span class="n">my_colormap</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
    <span class="n">white</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">white_low</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span> <span class="o">-</span> <span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">);</span> <span class="n">white_high</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">128</span><span class="o">+</span><span class="n">mask</span><span class="o">*</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">newcolors</span><span class="p">[</span><span class="n">white_low</span><span class="p">:</span><span class="n">white_high</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">white</span>                <span class="c1"># mask all correlations less than abs(0.8)</span>
    <span class="n">newcmp</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">(</span><span class="n">newcolors</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">fignum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">*</span><span class="n">limits</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">limits</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">newcmp</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s1">'bottom'</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_bottom</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],[</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    
<span class="k">def</span> <span class="nf">add_grid</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span><span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># add y grids</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'major'</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="mi">7</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">which</span><span class="o">=</span><span class="s1">'minor'</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">());</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_minor_locator</span><span class="p">(</span><span class="n">AutoMinorLocator</span><span class="p">())</span> <span class="c1"># turn on minor ticks   </span>

<span class="k">def</span> <span class="nf">plot_CDF</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'none'</span><span class="p">):</span>
    <span class="n">cumprob</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">cumprob</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">cumprob</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">visualize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X1_train</span><span class="p">,</span><span class="n">X1_test</span><span class="p">,</span><span class="n">X2_train</span><span class="p">,</span><span class="n">X2_test</span><span class="p">,</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span>
                         <span class="n">ymax</span><span class="p">,</span><span class="n">title</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">annotate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span><span class="c1"># plots the data points and the decision tree prediction </span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span>
    <span class="n">X1plot_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="mf">300.0</span><span class="p">;</span> <span class="n">X2plot_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">300.0</span> <span class="c1"># resolution of the model visualization</span>
    <span class="n">XX1</span><span class="p">,</span> <span class="n">XX2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X1plot_step</span><span class="p">),</span> <span class="c1"># set up the mesh</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X2plot_step</span><span class="p">))</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">XX2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>    <span class="c1"># predict with our trained model over the mesh</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> 
        <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sp</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1_train</span><span class="p">,</span><span class="n">X2_train</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X1_test</span><span class="p">,</span><span class="n">X2_test</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
        <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">sp</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>         <span class="c1"># add the color bar</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">ylabel</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_hat</span>

<span class="k">def</span> <span class="nf">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabel</span><span class="p">,</span><span class="n">title</span><span class="p">):</span> <span class="c1"># get OOB MSE and cross plot a decision tree </span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">MSE_test</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_hat</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_hat</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> 
                <span class="n">linewidths</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Truth: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Estimated: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ylabel</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],[</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">'black'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">'Testing MSE: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">MSE_test</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="si">:</span><span class="s1">,.0f</span><span class="si">}</span><span class="s1">'</span><span class="p">),[</span><span class="mi">4200</span><span class="p">,</span><span class="mi">2500</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">display_sidebyside</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>                                <span class="c1"># display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)</span>
    <span class="n">html_str</span> <span class="o">=</span> <span class="s1">''</span>
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">html_str</span> <span class="o">+=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_html</span><span class="p">()</span>  <span class="c1"># Using .head() for the first few rows</span>
    <span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="sa">f</span><span class="s1">'&lt;div style="display: flex;"&gt;</span><span class="si">{</span><span class="n">html_str</span><span class="si">}</span><span class="s1">&lt;/div&gt;'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Set the Working Directory</h2>
<p>I always like to do this so I don‚Äôt lose files and to simplify subsequent read and writes (avoid including the full address each time).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1">#os.chdir("c:/PGE383")                                        # set the working directory</span>
</pre></div>
</div>
</div>
</div>
<p>You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).</p>
&#13;

<h2>Loading Data</h2>
<p>Let‚Äôs load the provided multivariate, spatial dataset <a class="reference external" href="https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv">unconv_MV.csv</a> available in my GeoDataSet repo. It is a comma delimited file with:</p>
<ul class="simple">
<li><p>well index (integer)</p></li>
<li><p>porosity (%)</p></li>
<li><p>permeability (<span class="math notranslate nohighlight">\(mD\)</span>)</p></li>
<li><p>acoustic impedance (<span class="math notranslate nohighlight">\(\frac{kg}{m^3} \cdot \frac{m}{s} \cdot 10^6\)</span>).</p></li>
<li><p>brittleness (%)</p></li>
<li><p>total organic carbon (%)</p></li>
<li><p>vitrinite reflectance (%)</p></li>
<li><p>initial gas production (90 day average) (MCFPD)</p></li>
</ul>
<p>We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô and then preview it to make sure it loaded correctly.</p>
<p><strong>Python Tip: using functions from a package</strong> just type the label for the package that we declared at the beginning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
<p>so we can access the pandas function ‚Äòread_csv‚Äô with the command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">()</span>
</pre></div>
</div>
<p>but read csv has required input parameters. The essential one is the name of the file. For our circumstance all the other default parameters are fine. If you want to see all the possible parameters for this function, just go to the docs <a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">here</a>.</p>
<ul class="simple">
<li><p>The docs are always helpful</p></li>
<li><p>There is often a lot of flexibility for Python functions, possible through using various inputs parameters</p></li>
</ul>
<p>also, the program has an output, a pandas DataFrame loaded from the data.  So we have to specficy the name / variable representing that new object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"unconv_MV.csv"</span><span class="p">)</span>  
</pre></div>
</div>
<p>Let‚Äôs run this command to load the data and then this command to extract a random subset of the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">73073</span><span class="p">);</span> 
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
</pre></div>
</div>
&#13;

<h2>Feature Engineering</h2>
<p>Let‚Äôs make some changes to the data to improve the workflow:</p>
<ul class="simple">
<li><p><strong>Select the predictor features (x2) and the response feature (x1)</strong>, make sure the metadata is also consistent.</p></li>
<li><p><strong>Metadata</strong> encoding such as the units, labels and display ranges for each feature.</p></li>
<li><p><strong>Reduce the number of data</strong> for ease of visualization (hard to see if too many points on our plots).</p></li>
<li><p><strong>Train and test data split</strong> to demonstrate and visualize simple hyperparameter tuning.</p></li>
<li><p><strong>Add random noise to the data</strong> to demonstrate model overfit. The original data is error free and does not readily demonstrate overfit.</p></li>
</ul>
<p>Given this is properly set, one should be able to use any dataset and features for this demonstration.</p>
<ul class="simple">
<li><p>for brevity we don‚Äôt show any feature selection here. Previous chapter, e.g., k-nearest neighbours include some feature selection methods, but see the feature selection chapter for many possible methods with codes for feature selection.</p></li>
</ul>
&#13;

<h2>Optional: Add Random Noise to the Response Feature</h2>
<p>We can do this to observe the impact of data noise on overfit and hyperparameter tuning.</p>
<ul class="simple">
<li><p>This is for experiential learning, of course we wouldn‚Äôt add random noise to our data</p></li>
<li><p>We set the random number seed for reproducibility</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">add_error</span> <span class="o">=</span> <span class="kc">True</span>                                              <span class="c1"># add random error to the response feature</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="mi">1000</span>                                               <span class="c1"># standard deviation of random error, for demonstration only</span>
<span class="n">idata</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">if</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv"</span><span class="p">)</span> <span class="c1"># load the data from my github repo</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    
<span class="k">elif</span> <span class="n">idata</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="sa">r</span><span class="s2">"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv"</span><span class="p">)</span> <span class="c1"># load the data </span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">.70</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">);</span> <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span> <span class="c1"># extract 30% random to reduce the number of data</span>
    <span class="n">df_load</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">"Prod"</span><span class="p">:</span> <span class="s2">"Production"</span><span class="p">})</span>
    
<span class="n">yname</span> <span class="o">=</span> <span class="s1">'Production'</span><span class="p">;</span> <span class="n">Xname</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Por'</span><span class="p">,</span><span class="s1">'Brittle'</span><span class="p">]</span>               <span class="c1"># specify the predictor features (x2) and response feature (x1)</span>
<span class="n">Xmin</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">];</span> <span class="n">Xmax</span> <span class="o">=</span> <span class="p">[</span><span class="mf">25.0</span><span class="p">,</span><span class="mf">100.0</span><span class="p">]</span>                         <span class="c1"># set minimums and maximums for visualization </span>
<span class="n">ymin</span> <span class="o">=</span> <span class="mf">1500.0</span><span class="p">;</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">7000.0</span>
<span class="n">Xlabel</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Porosity'</span><span class="p">,</span><span class="s1">'Brittleness'</span><span class="p">];</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">'Production'</span>    <span class="c1"># specify the feature labels for plotting</span>
<span class="n">Xunit</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'%'</span><span class="p">,</span><span class="s1">'%'</span><span class="p">];</span> <span class="n">yunit</span> <span class="o">=</span> <span class="s1">'MCFPD'</span>
<span class="n">Xlabelunit</span> <span class="o">=</span> <span class="p">[</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">]</span>
<span class="n">ylabelunit</span> <span class="o">=</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span>

<span class="k">if</span> <span class="n">add_error</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>                                         <span class="c1"># method to add error</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>                                 <span class="c1"># set random number seed</span>
    <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">std_error</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df_load</span><span class="p">))</span> <span class="c1"># add noise</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">df_load</span><span class="o">.</span><span class="n">_get_numeric_data</span><span class="p">();</span> <span class="n">values</span><span class="p">[</span><span class="n">values</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># set negative to 0 in a shallow copy ndarray</span>
    
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df_load</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                              <span class="c1"># extract selected features as X and y DataFrames</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df_load</span><span class="p">[</span><span class="n">Xname</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                                  <span class="c1"># make one DataFrame with both X and y (remove all other features)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs make sure that we have selected reasonable features to build a model</p>
<ul class="simple">
<li><p>the 2 predictor features are not collinear, as this would result in an unstable prediction model</p></li>
<li><p>each of the features are related to the response feature, the predictor features inform the response</p></li>
</ul>
&#13;

<h2>Calculate the Correlation Matrix and Correlation with Response Ranking</h2>
<p>Let‚Äôs start with correlation analysis. We can calculate and view the correlation matrix and correlation to the response features with these previously declared functions.</p>
<ul class="simple">
<li><p>correlation analysis is based on the assumption of linear relationships, but it is a good start</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlation</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="s1">'Correlation Matrix'</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>           <span class="c1"># using our correlation matrix visualization function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Features'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">feature_rank_plot</span><span class="p">(</span><span class="n">Xname</span><span class="p">,</span><span class="n">correlation</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="s1">'Feature Ranking, Correlation with '</span> <span class="o">+</span> <span class="n">yname</span><span class="p">,</span><span class="s1">'Correlation'</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f16d805e35f1c39ce293ec74df9132232e9f79c7ca0cb2ab8b63641a714d99c3.png" src="../Images/3e93a8978cec90fe97483ce65cda346f.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f16d805e35f1c39ce293ec74df9132232e9f79c7ca0cb2ab8b63641a714d99c3.png"/>
</div>
</div>
<p>Note the 1.0 diagonal resulting from the correlation of each variable with themselves.</p>
<p>This looks good.  There is a mix of correlation magnitudes. Of course, correlation coefficients are limited to degree of linear correlations.</p>
<ul class="simple">
<li><p>Let‚Äôs look at the matrix scatter plot to see the pairwise relationship between the features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pairgrid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="nb">vars</span><span class="o">=</span><span class="n">Xname</span><span class="o">+</span><span class="p">[</span><span class="n">yname</span><span class="p">])</span>                <span class="c1"># matrix scatter plots</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">,</span> <span class="n">bins</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">)</span><span class="c1"># Map a density plot to the lower triangle</span>
<span class="n">pairgrid</span> <span class="o">=</span> <span class="n">pairgrid</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> 
                              <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">n_levels</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">pairgrid</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b641089892114f3044ca3ab9a43c2723da5e4105a26416ee749176d163822c57.png" src="../Images/8f6fc0dc9253e57bb674c41069015dde.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/b641089892114f3044ca3ab9a43c2723da5e4105a26416ee749176d163822c57.png"/>
</div>
</div>
&#13;

<h2>Train and Test Split</h2>
<p>For convenience and simplicity we use scikit-learn‚Äôs random train and test split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">73073</span><span class="p">)</span> <span class="c1"># train and test split</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                <span class="c1"># make one train DataFrame with both X and y (remove all other features)</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># make one testin DataFrame with both X and y (remove all other features)</span>
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Visualize the DataFrame</h2>
<p>Visualizing the train and test DataFrame is useful check before we build our models.</p>
<ul class="simple">
<li><p>many things can go wrong, e.g., we loaded the wrong data, all the features did not load, etc.</p></li>
</ul>
<p>We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice and clean format, see below).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'       Training DataFrame          Testing DataFrame'</span><span class="p">)</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span><span class="n">df_test</span><span class="p">)</span>                          <span class="c1"># custom function for side-by-side DataFrame display</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>       Training DataFrame          Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>86</th>
      <td>12.83</td>
      <td>29.87</td>
      <td>995.700671</td>
    </tr>
    <tr>
      <th>35</th>
      <td>17.39</td>
      <td>56.43</td>
      <td>6060.760806</td>
    </tr>
    <tr>
      <th>75</th>
      <td>12.23</td>
      <td>40.67</td>
      <td>3744.177137</td>
    </tr>
    <tr>
      <th>36</th>
      <td>13.72</td>
      <td>40.24</td>
      <td>4203.470533</td>
    </tr>
    <tr>
      <th>126</th>
      <td>12.83</td>
      <td>17.20</td>
      <td>2917.165695</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>15.55</td>
      <td>58.25</td>
      <td>5619.930037</td>
    </tr>
    <tr>
      <th>46</th>
      <td>20.21</td>
      <td>23.78</td>
      <td>3897.440411</td>
    </tr>
    <tr>
      <th>96</th>
      <td>15.07</td>
      <td>39.39</td>
      <td>4504.608029</td>
    </tr>
    <tr>
      <th>45</th>
      <td>12.10</td>
      <td>63.24</td>
      <td>3613.953926</td>
    </tr>
    <tr>
      <th>105</th>
      <td>19.54</td>
      <td>37.40</td>
      <td>5314.937997</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
&#13;

<h2>Summary Statistics for Tabular Data</h2>
<p>There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames.</p>
<ul class="simple">
<li><p>The describe command provides count, mean, minimum, maximum in a nice data table.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="s1">'            Training DataFrame                      Testing DataFrame'</span><span class="p">)</span>    <span class="c1"># custom function for side-by-side summary statistics</span>
<span class="n">display_sidebyside</span><span class="p">(</span><span class="n">df_train</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]],</span><span class="n">df_test</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">'count'</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'std'</span><span class="p">,</span> <span class="s1">'min'</span><span class="p">,</span> <span class="s1">'max'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>            Training DataFrame                      Testing DataFrame
</pre></div>
</div>
<div class="output text_html"><div style="display: flex;"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>105.000000</td>
      <td>105.000000</td>
      <td>105.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>14.859238</td>
      <td>48.861143</td>
      <td>4192.479746</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.057228</td>
      <td>14.432050</td>
      <td>1347.391355</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7.220000</td>
      <td>10.940000</td>
      <td>357.449794</td>
    </tr>
    <tr>
      <th>max</th>
      <td>23.550000</td>
      <td>84.330000</td>
      <td>7934.478879</td>
    </tr>
  </tbody>
</table><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>Por</th>
      <th>Brittle</th>
      <th>Production</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>35.000000</td>
      <td>35.000000</td>
      <td>35.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>15.011714</td>
      <td>46.798286</td>
      <td>4431.830496</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.574467</td>
      <td>13.380910</td>
      <td>1487.184992</td>
    </tr>
    <tr>
      <th>min</th>
      <td>6.550000</td>
      <td>20.120000</td>
      <td>1572.738774</td>
    </tr>
    <tr>
      <th>max</th>
      <td>20.860000</td>
      <td>68.760000</td>
      <td>7668.639376</td>
    </tr>
  </tbody>
</table></div></div></div>
</div>
<p>It is good that we checked the summary statistics.</p>
<ul class="simple">
<li><p>there are no obvious issues</p></li>
<li><p>check out the range of values for each feature to set up and adjust plotting limits. See above.</p></li>
</ul>
&#13;

<h2>Visualize the Train and Test Splits</h2>
<p>Let‚Äôs check the consistency and coverage of training and testing with histograms and scatter plots.</p>
<ul class="simple">
<li><p>check to make sure the training and testing cover the range of possible feature combinations</p></li>
<li><p>ensure we are not extrapolating beyond the training data with the testing cases</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">nbins</span> <span class="o">=</span> <span class="mi">20</span>                                                    <span class="c1"># number of histogram bins</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 histogram</span>
<span class="n">freq1</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">freq2</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">nbins</span><span class="p">),</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                     <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">max_freq</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">freq1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">,</span><span class="n">freq2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Frequency'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">max_freq</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">()</span>  
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper right'</span><span class="p">)</span>   

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>                                              <span class="c1"># predictor features #1 and #2 scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">df_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span><span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'black'</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' vs '</span> <span class="o">+</span>  <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png" src="../Images/6c7c51e5c24b5320a250e5d2881b5021.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/39becd8bd5d8101d8e6567bb5e26eb2b8fbe6499ce965fd4f72fdc3d8b7a3013.png"/>
</div>
</div>
<p>Sometimes I find it more convenient to compare distributions by looking at CDF‚Äôs instead of histograms.</p>
<ul class="simple">
<li><p>we avoid the arbitrary choice of histogram bin size, because CDF‚Äôs are at the data resolution.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>                                              <span class="c1"># predictor feature #1 CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>                                              <span class="c1"># predictor feature #2 CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabelunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>                                              <span class="c1"># response feature CDF</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="s1">'darkorange'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plot_CDF</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="s1">'red'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'solid'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">ylabelunit</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]);</span> <span class="n">add_grid</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' Train and Test CDFs'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="c1">#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf')   </span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png" src="../Images/e1017bcd0dfe07284cc05b94b7dd3861.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/26e6503a4d7debf39d1789ec8242383e943228b6a4f2f3dfc0a0f10cd99a5b46.png"/>
</div>
</div>
<p>Once again, the distributions are well behaved,</p>
<ul class="simple">
<li><p>we cannot observe obvious gaps nor truncations.</p></li>
<li><p>check coverage of the train and test data</p></li>
</ul>
<p>Let‚Äôs look at a scatter plot of Porosity vs. Brittleness with points colored by Production.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                              <span class="c1"># visualize the train and test data in predictor feature space</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
    <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Train'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> 
    <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">ymax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">'Test'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training '</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' vs. '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' and '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">Xunit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">Xmin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">Xmax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">'upper right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">orientation</span> <span class="o">=</span> <span class="s1">'vertical'</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' ('</span> <span class="o">+</span> <span class="n">yunit</span> <span class="o">+</span> <span class="s1">')'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png" src="../Images/79e12ff1ac255ff9811cd9900912d141.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1a847dd38c7158f07f58197cf7bfb88e46738dacb7622cd79a3a267a25a94494.png"/>
</div>
</div>
&#13;

<h2>Tree-based Boosting</h2>
<p>To perform tree-based boosting, gradient boosting tree regression we:</p>
<ol class="arabic simple">
<li><p>set the hyperparameters for our model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                 <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                      <span class="c1"># number of trees</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                     
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                            <span class="c1"># tree construction criterion</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>instantiate the model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">boost_tree</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>train the model</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">boot_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>visualize the model result over the feature space (easy to do as we have only 2 predictor features)</p></li>
</ol>
&#13;

<h2>Demonstration of Boosting</h2>
<p>For demonstration let‚Äôs set tree maximum depth to 1 and 6 tree-based boosting regression trees.</p>
<ul class="simple">
<li><p>each tree only has a single split, called decision stumps. This will prevent interaction between the predictor features and be highly interpretable</p></li>
</ul>
<p>You should be able to observe the additive nature of the trees, see the first tree and then the first plus the second tree and so on.</p>
<ul class="simple">
<li><p>recall the estimate is the summation of multiple trees</p></li>
<li><p>since we are working with fitting a gradient after the first tree, we can have negative and positive estimates</p></li>
<li><p>in this example we can see some production estimates that are actually negative</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                       
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
                                           
<span class="n">num_trees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>                              <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0689fdfd411d646efca1490a036e9354bc12c7bd5797bac40f5b0295fb0f11a3.png" src="../Images/b2a16fd01b6c4a7a3f642f8cf90844b3.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/0689fdfd411d646efca1490a036e9354bc12c7bd5797bac40f5b0295fb0f11a3.png"/>
</div>
</div>
<p>Notice that there is significant misfit with the data</p>
<ul class="simple">
<li><p>we have only used up to 6 decision stumps (1 decision tree)</p></li>
</ul>
<p>Let‚Äôs check the cross validation results with the withheld testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dca4db99b262b1382fe6624eace15ad6cfe1a52dd28209db869d61cb97d8a540.png" src="../Images/c19d0d925995b408d0e14db0f66b52ee.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/dca4db99b262b1382fe6624eace15ad6cfe1a52dd28209db869d61cb97d8a540.png"/>
</div>
</div>
<p>Of course with a single tree we do quite poorly, but by the time we get to 6 stump trees we cut the MSE almost in half.</p>
&#13;

<h2>Time to Build More Trees</h2>
<p>Now let‚Äôs demonstrate the result of utilizing many more trees in our tree-based boosting model.</p>
<ul class="simple">
<li><p>we will still work with simple decision stumps, don‚Äôt worry we will add more later</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad50902645fae1091779c09e40ad78025d6ddf1261c9d5a405eb2af496ddd2df.png" src="../Images/3baea417052c8c32978d193e173228b1.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ad50902645fae1091779c09e40ad78025d6ddf1261c9d5a405eb2af496ddd2df.png"/>
</div>
</div>
<p>See the plaid pattern? It is due to the use of decision stumps, and:</p>
<ul class="simple">
<li><p>an additive model</p></li>
<li><p>all models contribute to all predictions</p></li>
</ul>
<p>See the dark and bright regions?</p>
<ul class="simple">
<li><p>the additive model may extrapolate outside the data range</p></li>
</ul>
<p>Let‚Äôs cross validate with our testing data to see how our model has improved with more trees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f6fae7eb28cbf98d98c8bfbf82659a7d3bcb67136928521595c9e0374bdc1147.png" src="../Images/3f37a775a250322ae1595efdbfe1669d.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/f6fae7eb28cbf98d98c8bfbf82659a7d3bcb67136928521595c9e0374bdc1147.png"/>
</div>
</div>
<p>Around 20 trees we get our best performance and then we start to degrade, we are likely starting to overfit the training data.</p>
&#13;

<h2>Going Beyond Decision Stumps</h2>
<p>As state before with decision stumps we prevent interactions between features.</p>
<p>Let‚Äôs extend to tree depth of 2</p>
<ul class="simple">
<li><p>two nested decisions resulting in 4 terminal nodes</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07951866b16ac8dd604974240c5a929632a2a45dccbbed9c7a5b1fcc6d5e3f6e.png" src="../Images/65161de226c63aae698a6005291afae4.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/07951866b16ac8dd604974240c5a929632a2a45dccbbed9c7a5b1fcc6d5e3f6e.png"/>
</div>
</div>
<p>We have much more flexibility now.</p>
<ul class="simple">
<li><p>with one tree we have 4 terminal nodes (regions)</p></li>
<li><p>with only 6 trees we are capturing some complicate features</p></li>
</ul>
<p>Let‚Äôs increase the tree depth one more time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aba462916f779681b9aa90d6b1a3a548e3d43d5ad379c5af304e09a38658a793.png" src="../Images/b36175195239341c3b7ea8c842eea5b7.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/aba462916f779681b9aa90d6b1a3a548e3d43d5ad379c5af304e09a38658a793.png"/>
</div>
</div>
<p>One more time, it is common to use trees with depths of 4-8, so let‚Äôs try 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>
    
<span class="n">num_trees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">5000</span><span class="p">]</span>                          <span class="c1"># build a list of numbers of trees </span>
<span class="n">boosting_models</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">pred</span> <span class="o">=</span> <span class="p">[]</span>                 <span class="c1"># arrays for storage of models and model summaries</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees</span>
    <span class="n">boosting_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params</span><span class="p">))</span>
    <span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">visualize_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">X_train</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">X_test</span><span class="p">[</span><span class="n">Xname</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">Xmin</span><span class="p">,</span><span class="n">Xmax</span><span class="p">,</span>
        <span class="n">y_train</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">y_test</span><span class="p">[</span><span class="n">yname</span><span class="p">],</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="s1">'Gradient Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">,</span><span class="n">Xname</span><span class="p">,</span><span class="n">yname</span><span class="p">,</span><span class="n">Xlabelunit</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">))</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1739c0986e82122c4de50870f59b64abac148392710ba70c39c9f279197c0704.png" src="../Images/d2ae5319fcaa86e15b8c1c77df29902e.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/1739c0986e82122c4de50870f59b64abac148392710ba70c39c9f279197c0704.png"/>
</div>
</div>
<p>Let‚Äôs cross validate the model with testing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># check over number of trees</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>
    <span class="n">check_model</span><span class="p">(</span><span class="n">boosting_models</span><span class="p">[</span><span class="n">index</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">ymin</span><span class="p">,</span><span class="n">ymax</span><span class="p">,</span><span class="n">ylabelunit</span><span class="p">,</span><span class="s1">'Tree-Based Boosting with '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' Trees'</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/76eb33533b0470061aefadc2a8ad8b34dcd8adaa5de34b3a7fef0efc8af812ca.png" src="../Images/cdbb3edec9e81f128298ed05c7cd6189.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/76eb33533b0470061aefadc2a8ad8b34dcd8adaa5de34b3a7fef0efc8af812ca.png"/>
</div>
</div>
<p>With a max tree depth of 5 our model performance peaks early and the addition of more trees has no impact.</p>
<ul class="simple">
<li><p>of course this is not a thorough analysis</p></li>
</ul>
<p>Let‚Äôs try something more thorough</p>
<ul class="simple">
<li><p>we will cross validate models with <span class="math notranslate nohighlight">\(1,\ldots,100\)</span> trees with max tree depths of <span class="math notranslate nohighlight">\(1, 2, 3, 10\)</span>.</p></li>
<li><p>we also slow down the learning rate, I increased it above to amplify the difference of the outputs for demonstration, but know we want to see the best possible model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">num_trees</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MSE1_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE2_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE3_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE4_list</span> <span class="o">=</span> <span class="p">[]</span> 

<span class="n">params1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>                                        <span class="c1"># maximum depth per tree</span>
    <span class="s1">'learning_rate'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">num_tree</span> <span class="ow">in</span> <span class="n">num_trees</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees in our random forest</span>
    <span class="n">boosting_model1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test1_hat</span> <span class="o">=</span> <span class="n">boosting_model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE1_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test1_hat</span><span class="p">))</span>
    
    <span class="n">boosting_model2</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test2_hat</span> <span class="o">=</span> <span class="n">boosting_model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE2_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test2_hat</span><span class="p">))</span>

    <span class="n">boosting_model3</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test3_hat</span> <span class="o">=</span> <span class="n">boosting_model3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE3_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test3_hat</span><span class="p">))</span>

    <span class="n">boosting_model4</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_tree</span><span class="p">),</span><span class="o">**</span><span class="n">params4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test4_hat</span> <span class="o">=</span> <span class="n">boosting_model4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE4_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test4_hat</span><span class="p">))</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                            <span class="c1"># plot jackknife results for all cases</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE1_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE2_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE3_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 3"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">num_trees</span><span class="p">,</span><span class="n">MSE4_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 10"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Testing Mean Square Error vs. Number of Trees'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Number of Trees'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ae7e45a290e0fe1fb8cb9fe76b23eff4132746b7765f4629c874d06b593e0ca6.png" src="../Images/c2ee3e9f993c37a05e91acb8b4ce5baf.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/ae7e45a290e0fe1fb8cb9fe76b23eff4132746b7765f4629c874d06b593e0ca6.png"/>
</div>
</div>
<p>That‚Äôs interesting:</p>
<ul class="simple">
<li><p>with increasing tree depths our model may improve</p></li>
<li><p>more tree depth requires fewer trees for improved accuracy</p></li>
<li><p>with tree depths of 2 and 3 the models behave the same and after 10-15 trees level off, they are resistant to overfit</p></li>
<li><p>with tree depth of 10, the number of trees has no impact of model performance</p></li>
</ul>
&#13;

<h2>Gradient Descent Hyperparameters</h2>
<p>The learning rate scales the additive impact of each additive tree to the overall model prediction.</p>
<ul class="simple">
<li><p>lower learning rate will slow the convergence to a solution</p></li>
<li><p>lower learning rate will help us not skip over an optimum solution</p></li>
</ul>
<p>We won‚Äôt spend much time on this, but let‚Äôs just try changing the learning rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">learning_rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">MSE1_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE2_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE3_list</span> <span class="o">=</span> <span class="p">[];</span> <span class="n">MSE4_list</span> <span class="o">=</span> <span class="p">[]</span> 

<span class="n">params1</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                         <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">params4</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'loss'</span><span class="p">:</span> <span class="s1">'squared_error'</span><span class="p">,</span>                                           <span class="c1"># L2 Norm - least squares</span>
    <span class="s1">'max_depth'</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>                                        <span class="c1"># maximum depth per tree</span>
    <span class="s1">'n_estimators'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">'criterion'</span><span class="p">:</span> <span class="s1">'squared_error'</span>                                      <span class="c1"># tree construction criteria is mean square error over training</span>
<span class="p">}</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">learning_rate</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>                                  <span class="c1"># loop over number of trees in our random forest</span>
    <span class="n">boosting_model1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test1_hat</span> <span class="o">=</span> <span class="n">boosting_model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE1_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test1_hat</span><span class="p">))</span>
    
    <span class="n">boosting_model2</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test2_hat</span> <span class="o">=</span> <span class="n">boosting_model2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE2_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test2_hat</span><span class="p">))</span>

    <span class="n">boosting_model3</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test3_hat</span> <span class="o">=</span> <span class="n">boosting_model3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE3_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test3_hat</span><span class="p">))</span>

    <span class="n">boosting_model4</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span><span class="o">**</span><span class="n">params4</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test4_hat</span> <span class="o">=</span> <span class="n">boosting_model4</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">);</span> <span class="n">MSE4_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_test4_hat</span><span class="p">))</span>

    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>                                            <span class="c1"># plot jackknife results for all cases</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE1_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE2_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE3_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 3"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">learning_rates</span><span class="p">,</span><span class="n">MSE4_list</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s2">"Tree Depth = 10"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Testing Mean Square Error vs. Learning Rate'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Learning Rate'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Test Mean Square Error'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">);</span> <span class="n">add_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_formatter</span><span class="p">(</span><span class="n">FuncFormatter</span><span class="p">(</span><span class="n">comma_format</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/011852fab9e64d42c5e7ac6bcaa69e4f061994730784f35584ff27e0d4ba9831.png" src="../Images/d130f276273e2b117adb7bf8d1768618.png" data-original-src="https://geostatsguy.github.io/MachineLearningDemos_Book/_images/011852fab9e64d42c5e7ac6bcaa69e4f061994730784f35584ff27e0d4ba9831.png"/>
</div>
</div>
<p>Once again, a very interesting result.</p>
<ul class="simple">
<li><p>regardless of tree complexity, it is better to learn slowly!</p></li>
</ul>
&#13;

<h2>Machine Learning Pipelines for Clean, Compact Machine Learning Code</h2>
<p>Pipelines are a scikit-learn class that allows for the encapsulation of a sequence of data preparation and modeling steps</p>
<ul class="simple">
<li><p>then we can treat the pipeline as an object in our much condensed workflow</p></li>
</ul>
<p>The pipeline class allows us to:</p>
<ul class="simple">
<li><p>improve code readability and to keep everything straight</p></li>
<li><p>build complete workflows with very few lines of readable code</p></li>
<li><p>avoid common workflow problems like data leakage, testing data informing model parameter training</p></li>
<li><p>abstract common machine learning modeling and focus on building the best model possible</p></li>
</ul>
<p>The fundamental philosophy is to treat machine learning as a combinatorial search to find the best model (AutoML)</p>
<p>For more information see my recorded lecture on <a class="reference external" href="https://www.youtube.com/watch?v=tYrPs8s1l9U&amp;list=PLG19vXLQHvSAufDFgZEFAYQEwMJXklnQV&amp;index=5">Machine Learning Pipelines</a> and a well-documented demonstration <a class="reference external" href="http://localhost:8892/notebooks/OneDrive%20-%20The%20University%20of%20Texas%20at%20Austin/Courses/Workflows/PythonDataBasics_Pipelines.ipynb">Machine Learning Pipeline Workflow</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x1</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">;</span> <span class="n">x2</span> <span class="o">=</span> <span class="mf">0.3</span>                                           <span class="c1"># predictor values for the prediction</span>

<span class="n">pipe_boosting</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>                                      <span class="c1"># the machine learning workflow as a pipeline object</span>
    <span class="p">(</span><span class="s1">'boosting'</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>                                                    <span class="c1"># the machine learning workflow method's parameters to search</span>
    <span class="s1">'boosting__learning_rate'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">),</span>
    <span class="s1">'boosting__n_estimators'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">dtype</span> <span class="o">=</span> <span class="nb">int</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">tuned_boosting</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe_boosting</span><span class="p">,</span><span class="n">params</span><span class="p">,</span><span class="n">scoring</span> <span class="o">=</span> <span class="s1">'neg_mean_squared_error'</span><span class="p">,</span> <span class="c1"># hyperparameter tuning w. grid search k-fold cross validation </span>
                             <span class="n">refit</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">tuned_boosting</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>                                         <span class="c1"># tune the model with k-fold and then train the model will the data </span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Tuned hyperparameter: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tuned_boosting</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>

<span class="n">estimate</span> <span class="o">=</span> <span class="n">tuned_boosting</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>                 <span class="c1"># make a prediction (no tuning shown)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Estimated '</span> <span class="o">+</span> <span class="n">ylabel</span> <span class="o">+</span> <span class="s1">' for '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="s1">' and '</span> <span class="o">+</span> <span class="n">Xlabel</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>  <span class="o">+</span> <span class="s1">' is '</span> <span class="o">+</span> 
      <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">estimate</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">yunit</span><span class="p">)</span>                     <span class="c1"># print results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Tuned hyperparameter: {'boosting__learning_rate': 0.30000000000000004, 'boosting__n_estimators': 10}
Estimated Production for Porosity = 0.25 and Brittleness = 0.3 is 1979.7 MCFPD
</pre></div>
</div>
</div>
</div>
&#13;

<h2>Comments</h2>
<p>I hope you found this chapter helpful. Much more could be done and discussed, I have many more resources. Check out my <a class="reference external" href="https://michaelpyrcz.com/my-resources">shared resource inventory</a>,</p>
<p><em>Michael</em></p>
&#13;

<h2>The Author:</h2>
<p>Michael Pyrcz, Professor, The University of Texas at Austin
<em>Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions</em></p>
<p>With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers‚Äô and geoscientists‚Äô impact in subsurface resource development.</p>
<p>For more about Michael check out these links:</p>
<p><a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a> | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></p>
&#13;

<h2>Want to Work Together?</h2>
<p>I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.</p>
<ul class="simple">
<li><p>Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!</p></li>
<li><p>Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!</p></li>
<li><p>I can be reached at <a class="reference external" href="mailto:mpyrcz%40austin.utexas.edu">mpyrcz<span>@</span>austin<span>.</span>utexas<span>.</span>edu</a>.</p></li>
</ul>
<p>I‚Äôm always happy to discuss,</p>
<p><em>Michael</em></p>
<p>Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The Jackson School of Geosciences, The University of Texas at Austin</p>
&#13;

<h2>More Resources Available at: <a class="reference external" href="https://twitter.com/geostatsguy">Twitter</a> | <a class="reference external" href="https://github.com/GeostatsGuy">GitHub</a> | <a class="reference external" href="http://michaelpyrcz.com">Website</a> | <a class="reference external" href="https://scholar.google.com/citations?user=QVZ20eQAAAAJ&amp;hl=en&amp;oi=ao">GoogleScholar</a> | <a class="reference external" href="https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446">Book</a> | <a class="reference external" href="https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig">YouTube</a>  | <a class="reference external" href="https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html">Applied Geostats in Python e-book</a> | <a class="reference external" href="https://www.linkedin.com/in/michael-pyrcz-61a648a1">LinkedIn</a></h2>
    
</body>
</html>