["```py\ndate_range:from 2021-08-01 to 2022-08-31;abs:\"causal inference\" OR\n  \"causal network\" OR \"counterfactual\" OR \"causal reasoning\"\n```", "```py\nwith DAG('ingestion', ...) as dag:\n [...]\n get_article_urls = PythonOperator(\n task_id='query_arxiv_archive',\n python_callable=query_arxiv,\n op_kwargs={'query': query}\n )\n\n download_article = PythonOperator(\n task_id='download_from_arxiv_archive',\n python_callable=download_arxiv,\n op_kwargs={}\n )\n\n extract_text_from_article = PythonOperator(\n task_id='extract_text',\n python_callable=convert_pdf_to_text\n op_kwargs={},\n )\n [...]\n get_article_urls >> download_article\n download_article >> extract_text_from_article\n [...]\n```", "```py\nouts:\n- md5: 853c9693c5aac78162da1c3b46aec63e\n size: 2190841\n path: causal_inference.txt\n\nmeta:\n search_query: \"causal inference\"\n search_start: 1629410400\n search_end: 1672441200\n [...]\n```", "```py\n$ git log --oneline\n669a39e (HEAD -> master, tag: v0.0.1) - w2v baseline impl.\n[...]\n$ dvc remote list # list remote storage configured in DVC\nexp_bucket   s3://exp_bucket\n$ dvc pull # fetch data from remote storage into the project\nA       datasets/causal_inference.txt\nA       datasets/causal_inference_small.txt\n2 files added\n$ nvim src/train-cli.py # tune the training code\n$ pipenv run src/train-cli.py --dataset=datasets/causal_inference.txt\n ...\n$ git status -s\n M src/train.py\n$ git add src/train-cli.py\n$ git commit -m 'Changed word2vec window size to 4'\n$ git tag -a 'v0.0.2' -m 'Changed word2vec window size to 4'\n```", "```py\n[...]\nmodel = Word2Vec(\n callbacks=[Word2vecCallback()],\n compute_loss=True,\n vector_size=vector_size,\n min_count=min_count,\n window=window,\n workers=workers)\n)\n\nmodel.build_vocab(\n corpus_iterable=vocabulary,\n progress_per=1,\n trim_rule=_rule\n)\n\nmodel.train(\n sentences,\n total_examples=model.corpus_count,\n epochs=epochs,\n report_delay=1.0,\n compute_loss=True,\n callbacks=[Word2vecCallback()],\n)\n\nword_vectors = model.wv\n[...]\n```", "```py\nimport dvc.api\n...\ncorpus_path = dvc.api.get_url(\n path=corpus_path,\n repo=repo_path,\n rev=revision,\n remote=remote\n)\n...\n```", "```py\nfrom mlflow import (log_metric,log_param,log_artifacts,\n create_experiment,start_run,end_run)\n[...]\nexperiment_id = create_experiment(\n \"NLU experiments on causal inference corpus\",\n artifact_location=Path.cwd().joinpath(\"mlruns\").as_uri(),\n tags={},\n)\nstart_run(experiment_id=experiment_id)\nlog_param(\"query\", query)\n[...]\nlog_param(\"window\", window)\nlog_param(\"stop_date\", stop_data)\n[...]\nlog_metric(\"wv_size\", model.wv.vector_size)\n[...]\nlog_artifact(\"corpus.txt\")\nlog_artifact(\"keywords.txt\")\nlog_artifact(\"vectors.kv\")\nend_run()\n[...]\n```", "```py\nfrom __future__ import annotations\n\nimport io\nfrom typing import Any\nimport typing\n\nimport numpy as np\n\nimport bentoml\nfrom bentoml.io import File\nfrom bentoml.io import JSON\n\nnlu_runner = bentoml.picklable_model.get(\"nlu_exp:v0.0.2).to_runner()\nsvc = bentoml.Service(\"pdf_classifier\", runners=[nlu_runner])\n\n@svc.api(input=File(), output=JSON())\ndef classify(input_pdf: io.BytesIO[Any]) -> typing.List[float]:\n return nlu_runner.classify.run(input_pdf)\n```", "```py\n$ bentoml containerise nlu_exp:v0.0.2\n$ docker run -d --rm -p 3000:300 nlu_exp:v0.0.2\n```", "```py\n$ curl -H \"Content-Type: multipart/form-data\" \\\n -F 'fileobj=@good-article.pdf;type=application/octet-stream' \\\n http://remote:3000/classify\n{\"value\":0.8203434225167339}%\n```", "```py\n$ curl -H \"Content-Type: multipart/form-data\" \\\n -F 'fileobj=@bad-article.pdf;type=application/octet-stream' \\\n http://remote:3000/classify\n{\"value\":0.24675693999330117}%\n```", "```py\n[...]\n# load serialised KeyedVectors from the `knowledge_ww_fp` path\nknowledge_wv = KeyedVectors.load(knowledge_ww_fp, mmap=\"r\")\n# get the KeyedVectors pairs that match the\n# vocabulary word (the keyword list from the expert)\nknowledge_v = get_word2vec_vectors(\n word_vectors=knowledge_wv,\n vocabulary=vocab\n)\n# train on the fly a word2vec model on\n# the PDF converted into text\nmodel = word2vec(text, vocab)\ndocument_v = get_word2vec_vectors(\n word_vectors=model.wv,\n vocabulary=vocab\n)\n[...]\ndist = 1 - distance.cosine(document_v.mean(0), knowledge_v.mean(0))\n[...]\nlogger.warning(\"Classify request with distance %f for %s\",\n dist, metadata)\n[...]\nreturn dist\n```"]