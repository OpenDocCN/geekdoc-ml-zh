- en: Density-based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_density-based_clustering.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_density-based_clustering.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book “Applied Machine Learning in Python: a Hands-on Guide with
    Code”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This is a tutorial for / demonstration of **Density-based Clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cluster Analysis](https://youtu.be/oFE10cLl0Fs?si=AwmYnrYggtYWGV2n)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Issues with k-Means Clustering](https://youtu.be/ysJw8M_J40I?si=EIlg2941QrfAt7zE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Density-based Clustering](https://www.youtube.com/watch?v=3GaLe8HaDMc&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael’s Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Density-based Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models,
  prefs: []
  type: TYPE_NORMAL
- en: mixing distinct populations to train prediction models often reduces model accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: common methods for cluster analysis like k-means clustering are easy to apply
    but are quite inflexible; therefore, it is essential to add more flexible clustering
    methods, like density-based clustering, to our toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: always remember, the computer does not provide meaning nor description of the
    groups, that is our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s a simple workflow, demonstration of K-means, density- and spectral-based
    clustering (for automated category assignment) for subsurface modeling workflows.
    This should help you get started with inferential methods to find patterns in
    your subsurface data sets.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s recall K-means clustering, a common clustering approach for clustering,
    group assignment to unlabeled data, where dissimilarity within clustered groups
    is mini minimized. The loss function that is minimized is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‘inverse’
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the assumptions that result in significant reduction in flexibility
    of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: The clusters are spherical, convex, and isotropic within the predictor feature
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is due to the model loss based on minimizes difference within clusters,
    this is accomplished through this simple spherical geometry in the predictor feature
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is equal variance for all features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \sigma_{X_1}^2 = \sigma_{X_2}^2= \ldots = \sigma_{X_m}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumed with the between sample distance / dissimilarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters have similar sizes and frequency (number of samples in the clusters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: larger cluster are divided to minimize the overall squared difference within
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clusters with few samples in feature space are overwhelmed!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations are quite significant, it is useful to have a more flexible
    method for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN for Density-based Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For dense, nonlinear, and potentially overlapping groups we can use a form of
    nature inspired computing for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: did you know that your brain has built-in cluster analysis, based on density?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can prove this with a form of art know as pointillism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/09d143db8fd83558429e6fce57d02cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: “The banks of the Marne at dawn.”, by Albert Dubois-Pillet (1886), an example
    of pointillism. I removed the color to highlight the automated cluster analysis
    in our brains based on changes in point density.
  prefs: []
  type: TYPE_NORMAL
- en: Did it work? You grouped the trees, the clouds, the vegetated river banks via
    local density.
  prefs: []
  type: TYPE_NORMAL
- en: the only reason I know about pointillism is because my sister Susan Weckesser
    was an incredible artist and musician. Rest in peace, Suse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With density-based clustering the cluster groups are seeded or grown in feature
    space at locations with sufficient point density determined by hyperparameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f25368b953ac38bbbfc103ec47b2d5d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Density-based clustering hyperparameters require to check for sufficient local
    density to instantiate a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: \(\epsilon\) – the radius of the local neighbourhood in the metric of normalized
    features. The is the scale / resolution of the clusters. If this values is set
    too small, too many samples are left as outliers and if set too large, all the
    clusters merge to one single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: \(min_{Pts}\) – the minimum number of points to assign a core point, where core
    points are applied to initialize or grow a cluster group.
  prefs: []
  type: TYPE_NORMAL
- en: as you can see above, density is quantified by number of samples over a volume,
    where the volume is based on a radius over all dimensions of feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Automated or guided \(\epsilon\) parameter estimation is available by k-distance
    graph (in this case is k nearest neighbor),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a43305c81354672317592744b885b78c.png)'
  prefs: []
  type: TYPE_IMG
- en: k-distance graph with elbow about 0.07 distance in predictor feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the nearest neighbor distance in normalized feature space for all
    the sample data (1,700 in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort in ascending order and plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the distance that maximizes the positive curvature (the elbow).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a summary of the important aspects for DBSCAN clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '**DBSCAN** - stands for [Density-Based Spatial Clustering of Applications with
    Noise](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf) (Ester et al.,1996).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages** - include minimum domain knowledge to estimate hyperparameters,
    the ability to represent any arbitrary shape of cluster groups and efficient to
    apply for large data sets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Bottom-up / Agglomerative Clustering** – all data samples start
    as their own group, called ‘unvisited’ but practically as outliers until assigned
    to a group, and then the cluster group grow iteratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutually Exclusive** – like k-means clustering, all samples may only belong
    to a single cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_i \cap C_j | i \ne j) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-exhaustive** – some samples may be left as unassigned and assumed as
    outliers for the cluster group assignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Iterative Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All data samples are initialized as unvisited and then the method proceeds through
    the dataset and assigns the samples as,
  prefs: []
  type: TYPE_NORMAL
- en: '**core** point if there are \(\ge\) **min_samples** within **eps** distance
    from the sample, start of a new cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**border** point if there are \(lt\) **min_samples** within **eps** distance
    from the sample, but the sample is within **eps** distance of a core point, potential
    growth of an existing cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**outlier** point if there are \(lt\) **min_samples** within **eps** distance
    from the sample and the sample is not within **eps** distance of a core point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see DBSCAN in action, check out my [DBSCAN interactive
    Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_DBSCAN.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02615bd47a459eefbeef250260c431ac.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: Once the points are assigned these labels, all connected core points and their
    associate border points are assigned to an unique cluster. Outlier points are
    left as outliers without an assigned cluster. To understand the cluster assignments
    we should explain the following forms of connection.
  prefs: []
  type: TYPE_NORMAL
- en: '**directly density reachable** - point X is directly density reachable from
    A, if A is a core point and X belongs to the neighborhood (distance \(le\) eps)
    from A.'
  prefs: []
  type: TYPE_NORMAL
- en: '**density-reachable** - point Y is density reachable from A if Y belongs to
    a neighborhood of a core point that can reached from A. This would require a chain
    of core points each belonging the previous core points and the last core point
    including point Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dbca0ae43ddea0ec062c0109fc23a79.png)'
  prefs: []
  type: TYPE_IMG
- en: Directly density reachable and density reachable.
  prefs: []
  type: TYPE_NORMAL
- en: '**density-connected** - points A and B are density-connected if there is a
    point Z that is density-reachable from both points A and B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/601d90b59a9fbb894776c62ebf7e5537.png)'
  prefs: []
  type: TYPE_IMG
- en: Density connected.
  prefs: []
  type: TYPE_NORMAL
- en: '**density-based cluster** - a nonempty set where all points are density-connected
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1541a1e7a843de78416ea2d62714561.png)'
  prefs: []
  type: TYPE_IMG
- en: Density-based clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a concise summary of these steps for solving DBSCAN,
  prefs: []
  type: TYPE_NORMAL
- en: all points are labeled as unvisited
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: randomly visit an unvisited sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: check if a core point (\(\ge min_{Pts}\) within \(\epsilon\) distance), if so
    label as core otherwise label as outlier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: now visit all points within eps distance of the core point, determine if core,
    otherwise label as border point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: recursive operation where all points within eps distance of new core points
    are checked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: once this is exhausted then randomly visit an unvisited point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach may be thought of as identify and grow/merge clusters guided by
    local point density.
  prefs: []
  type: TYPE_NORMAL
- en: Load the required libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‘python -m pip install [package-name]’. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convenience function to add major and minor gridlines to plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Custom Colormap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom colormap for DBSCAN for several groups and 0 set as an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Set the working directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don’t lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s load one of the provided multivariate, spatial datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: you can select datasets 1-4 below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are diverse multivariate datasets with:'
  prefs: []
  type: TYPE_NORMAL
- en: facies (integers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (fraction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (\(mD\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, cluster analysis is unsupervised, inferential machine learning and does
    not use labels. We have facies available so we can evaluate our cluster groups
    with respect to the facies assigned.
  prefs: []
  type: TYPE_NORMAL
- en: We load the selected dataset with the pandas ‘read_csv’ function into a data
    frame we called ‘df’ and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‘read_csv’ with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: but read_csv() has required input parameters. The essential one is the name
    of the file. For our circumstance all the other default parameters are fine. If
    you want to see all the possible parameters for this function, just go to the
    [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Tip** - established Python packages like pandas have excellent documentation.
    They are an excellent first stop to learn new packages. I also find examples critical
    and this motivates all the workflow examples and codes in my courses!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data file.
    So we have to specficy the name / variable representing this new DataFrame object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can switch the dataset from \(1, \ldots, 4\) to try out different clustering
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Warning some of these datasets examples are quite difficult for clustering!
    I made a variety for you to test your skills!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|  | AI | Por | Facies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 381.296221 | 8.980678 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 304.449932 | 10.135657 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 193.607906 | 16.150132 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 205.778123 | 13.696509 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 290.854794 | 13.132821 | 3 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics and Histograms for the Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 1800.0 | 625.522917 | 247.547997 | 10.0 | 518.076017 | 683.865824 |
    783.315226 | 1200.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 1800.0 | 17.122778 | 4.778704 | 0.0 | 14.663434 | 17.674606 | 20.361860
    | 30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 1800.0 | 1.666667 | 0.816723 | 1.0 | 1.000000 | 1.000000 | 2.000000
    | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: Let’s also check the proportion of facies.
  prefs: []
  type: TYPE_NORMAL
- en: once again, we will not use facies, but facies indicates the proportion of samples
    natural cluster in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d433b8288a0a3df029306975e7de39b87ce67841a32778154ff039c43b6462c0.png](../Images/597839898a00cdf727aab95af331e2dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two considered features are quite incompatible. They have dramatically
    different:'
  prefs: []
  type: TYPE_NORMAL
- en: magnitudes / averages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the difference in ranges between the features is arbitrary we need to
    remove this by transforming each of the features. We normalize each of the features
    to range from 0.0 (minimum) to 1.0 (maximum).
  prefs: []
  type: TYPE_NORMAL
- en: Note, with normalization there is no distribution shape change so we would typically
    check for issues like outliers before the transformation, for brevity this step
    is not included here given there are no outliers in the provided datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we use these normalized feature values for the clustering workflows, e.g.,
    for calculating the required distances distance between samples and prototypes
    in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: note, for each data sample we have original feature and transformed feature
    values; therefore, it is very easy to go back and forth without the need for backtransformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|  | AI | Por | Facies | nAI | nPor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 381.296221 | 8.980678 | 3 | 0.312014 | 0.299356 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 304.449932 | 10.135657 | 3 | 0.247437 | 0.337855 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 193.607906 | 16.150132 | 3 | 0.154292 | 0.538338 |'
  prefs: []
  type: TYPE_TB
- en: Let’s confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 1800.0 | 625.522917 | 247.547997 | 10.0 | 518.076017 | 683.865824 |
    783.315226 | 1200.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 1800.0 | 17.122778 | 4.778704 | 0.0 | 14.663434 | 17.674606 | 20.361860
    | 30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 1800.0 | 1.666667 | 0.816723 | 1.0 | 1.000000 | 1.000000 | 2.000000
    | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| nAI | 1800.0 | 0.517246 | 0.208024 | 0.0 | 0.426955 | 0.566274 | 0.649845
    | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| nPor | 1800.0 | 0.570759 | 0.159290 | 0.0 | 0.488781 | 0.589154 | 0.678729
    | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: and let’s compare the original and transformed histograms for the features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/262590e49456fa3a3dfcfb8df4eedb2bc7e2381fde8f758cfb076cab46379e9f.png](../Images/6946eafd11051796db89f44d89a41b0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Quick Peek at Available Labels for Educational Purposes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I know this is cheating, but to calibrate our eye’s and to check the natural
    clusters in our dataset, let’s cheat and look at the features scatter plotted
    colored by the facies labels.
  prefs: []
  type: TYPE_NORMAL
- en: for this demonstration workflow this is our inaccessible truth model, we are
    using this as only a teaching tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: after this we will leave these labels out and attempt to automate their assignment
    with cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/652cda19b2c66a627a22fa17d9640e4e3745a8cf34e2a24347e8901aaf82fdc4.png](../Images/a385f8e325d6fef6ece0176c37991757.png)'
  prefs: []
  type: TYPE_IMG
- en: For just a moment, look at the truth model. Does it look like this will be difficult
    to reproduce and automate with cluster analysis?
  prefs: []
  type: TYPE_NORMAL
- en: are the natural groups separated - is there overlap?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what is the shape of the cluster groups - are the groups spherical?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what are the frequencies in each cluster group?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is instructive, since if the groups are not well separated, not spherical
    and have uneven frequencies within each natural group, then k-means clustering
    will not likely perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s ignore the truth facies categorical assignments and move forward
    with proper cluster analysis, unsupervised without labels.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the Unlabeled Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use clustering for automated assignment of facies
    based on two features.
  prefs: []
  type: TYPE_NORMAL
- en: I limit us two features for each of visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by visualizing the unlabeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We look at the data in original units and normalized units through this entire
    exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0dba4275f8b9d01f98df3f5f393358b936a2551c1053c568b18662686d49c08e.png](../Images/1beddf39d6d6aa601f72c6ced44757ef.png)'
  prefs: []
  type: TYPE_IMG
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we start with k-means clustering. From our previous analysis we may know
    that this will not go well!
  prefs: []
  type: TYPE_NORMAL
- en: I have an entire demonstration dedicated to [k-means cluster](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Clustering.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we use k-means Clustering to compare and contrast with the subsequent density-based
    method that we will attempt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once again, let’s be mindful of the assumptions of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: spherical variability over the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: same variability over each feature (after our min/max normalization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prior probability of membership in all clusters, equal number of samples in
    each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset violates these assumptions with non-spherical shapes, unequal proportions
    of samples in each cluster (we know this because we have the truth facies and
    we did peak).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/da99a7400002db428b69cf03b1f80d110a90cd9da772d44ceb538fe35ad1df0b.png](../Images/b6800135b0332bade66709fafc2729e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected there are issues:'
  prefs: []
  type: TYPE_NORMAL
- en: the non-convex, nonlinear shapes, and low-density representation of edges of
    clusters are not captured well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a results obvious connected geometries are broken up.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DBSCAN approach that seeds and grows clusters groups based on high density
    is well suited to working with data with an arbitrary (even non-convex) shape
    and with noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again the two parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**eps** - maximum distance between to samples in features space for a sample
    to be included in the neighbourhood of the other sample. Too small a value will
    result in many clusters and outliers, and too large a value will result in one
    cluster with all the samples together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_samples** - minimum number of samples in a neighbourhood to spawn a new
    cluster. Use a larger value for noisy data to prevent identifying clusters due
    to a few grouped outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a6d400b449a860659a58145ad193c6f51de9b329465afd97fd7e2cbb3204f27f.png](../Images/6bb64696f44116faf7cea700a0f22200.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This result is very different than k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: the cluster groups are not symmetric or spherical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we also have outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore the sensitivity in the parameters, by looking at the results over
    a \(3 \times 3\) combinatorial of hyperparameters. Run this the first time as
    is and then try making changes the values in this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3a9018a2b915e0bf08f57be33b055aefbf0440860254662a0505d30738c82ef9.png](../Images/6026c92be73d1f48a2bf8b744cf3ab03.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the DBSCAN Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s try to use a more repeatable method to calculate the DBSCAN hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**min samples** - we set with expert knowledge, there is a rule of thumb, \(min_{sample}
    \ge D + 1\) or \(min_{sample} \ge 2 \times D + 1\) for large, noisy datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eps** - we will use the nearest neighbour function to estimate it given the
    minimum number of samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s calculate the nearest neighbour distance for each sample data to the
    min sample nearest neighbour in normalized feature space with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a \(n \times k\) array with the distances and the sample data indices:'
  prefs: []
  type: TYPE_NORMAL
- en: for each \(n\) sample we have a row of nearest neighbour distances up to k nearest
    neighbours (closest to furthest in the columns), note the first neighbour is sample
    itself; therefore the first column is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we will then sort the rows in ascending order and plot relative to the sample
    index, \(i = 1,\ldots,n-1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we will plot the distances vs. the sample index in the ascending order
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/00f3b27f3c08d348670d975111110a79e3fd87b0b9515312a5f750c7be546819.png](../Images/d60288cd53277b9a6eb6b87f952c36fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we look for the nearest neighbour distance with the maximum positive curvature,
    known as the ‘elbow’ of the nearest neighbour plot
  prefs: []
  type: TYPE_NORMAL
- en: this provides an indication of the cluster sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we would estimate eps as just above 0.03 - 0.04 in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some observations from DBSCAN
  prefs: []
  type: TYPE_NORMAL
- en: the results are quite sensitive to the selection of **eps** and **min_samples**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in the sparse part of the ‘cluster’ many of the data are assigned to other groups
    are remain as outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the result with this estimated eps hyperparameter is not satisfying.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e79ecd502b7cf3f722b4eb588a2a2182dee76fa2b9e98bc6368f38a943962e1b.png](../Images/0304f745d4f6039f44c8361d5016c294.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of density-based clustering with DBSCAN. Much more
    could be done and discussed, I have many more resources. Check out my [shared
    resource inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture
    links at the start of this chapter with resource links in the videos’ descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael’s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael’s work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I’d be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Density-based Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models,
  prefs: []
  type: TYPE_NORMAL
- en: mixing distinct populations to train prediction models often reduces model accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: common methods for cluster analysis like k-means clustering are easy to apply
    but are quite inflexible; therefore, it is essential to add more flexible clustering
    methods, like density-based clustering, to our toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: always remember, the computer does not provide meaning nor description of the
    groups, that is our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s a simple workflow, demonstration of K-means, density- and spectral-based
    clustering (for automated category assignment) for subsurface modeling workflows.
    This should help you get started with inferential methods to find patterns in
    your subsurface data sets.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s recall K-means clustering, a common clustering approach for clustering,
    group assignment to unlabeled data, where dissimilarity within clustered groups
    is mini minimized. The loss function that is minimized is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‘inverse’
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the assumptions that result in significant reduction in flexibility
    of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: The clusters are spherical, convex, and isotropic within the predictor feature
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is due to the model loss based on minimizes difference within clusters,
    this is accomplished through this simple spherical geometry in the predictor feature
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is equal variance for all features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \sigma_{X_1}^2 = \sigma_{X_2}^2= \ldots = \sigma_{X_m}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumed with the between sample distance / dissimilarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters have similar sizes and frequency (number of samples in the clusters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: larger cluster are divided to minimize the overall squared difference within
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clusters with few samples in feature space are overwhelmed!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations are quite significant, it is useful to have a more flexible
    method for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN for Density-based Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For dense, nonlinear, and potentially overlapping groups we can use a form of
    nature inspired computing for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: did you know that your brain has built-in cluster analysis, based on density?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can prove this with a form of art know as pointillism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/09d143db8fd83558429e6fce57d02cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: “The banks of the Marne at dawn.”, by Albert Dubois-Pillet (1886), an example
    of pointillism. I removed the color to highlight the automated cluster analysis
    in our brains based on changes in point density.
  prefs: []
  type: TYPE_NORMAL
- en: Did it work? You grouped the trees, the clouds, the vegetated river banks via
    local density.
  prefs: []
  type: TYPE_NORMAL
- en: the only reason I know about pointillism is because my sister Susan Weckesser
    was an incredible artist and musician. Rest in peace, Suse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With density-based clustering the cluster groups are seeded or grown in feature
    space at locations with sufficient point density determined by hyperparameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f25368b953ac38bbbfc103ec47b2d5d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Density-based clustering hyperparameters require to check for sufficient local
    density to instantiate a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: \(\epsilon\) – the radius of the local neighbourhood in the metric of normalized
    features. The is the scale / resolution of the clusters. If this values is set
    too small, too many samples are left as outliers and if set too large, all the
    clusters merge to one single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: \(min_{Pts}\) – the minimum number of points to assign a core point, where core
    points are applied to initialize or grow a cluster group.
  prefs: []
  type: TYPE_NORMAL
- en: as you can see above, density is quantified by number of samples over a volume,
    where the volume is based on a radius over all dimensions of feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Automated or guided \(\epsilon\) parameter estimation is available by k-distance
    graph (in this case is k nearest neighbor),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a43305c81354672317592744b885b78c.png)'
  prefs: []
  type: TYPE_IMG
- en: k-distance graph with elbow about 0.07 distance in predictor feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the nearest neighbor distance in normalized feature space for all
    the sample data (1,700 in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort in ascending order and plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the distance that maximizes the positive curvature (the elbow).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a summary of the important aspects for DBSCAN clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '**DBSCAN** - stands for [Density-Based Spatial Clustering of Applications with
    Noise](https://cdn.aaai.org/KDD/1996/KDD96-037.pdf) (Ester et al.,1996).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advantages** - include minimum domain knowledge to estimate hyperparameters,
    the ability to represent any arbitrary shape of cluster groups and efficient to
    apply for large data sets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Bottom-up / Agglomerative Clustering** – all data samples start
    as their own group, called ‘unvisited’ but practically as outliers until assigned
    to a group, and then the cluster group grow iteratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutually Exclusive** – like k-means clustering, all samples may only belong
    to a single cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_i \cap C_j | i \ne j) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-exhaustive** – some samples may be left as unassigned and assumed as
    outliers for the cluster group assignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Iterative Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All data samples are initialized as unvisited and then the method proceeds through
    the dataset and assigns the samples as,
  prefs: []
  type: TYPE_NORMAL
- en: '**core** point if there are \(\ge\) **min_samples** within **eps** distance
    from the sample, start of a new cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**border** point if there are \(lt\) **min_samples** within **eps** distance
    from the sample, but the sample is within **eps** distance of a core point, potential
    growth of an existing cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**outlier** point if there are \(lt\) **min_samples** within **eps** distance
    from the sample and the sample is not within **eps** distance of a core point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see DBSCAN in action, check out my [DBSCAN interactive
    Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_DBSCAN.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02615bd47a459eefbeef250260c431ac.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for DBSCAN.
  prefs: []
  type: TYPE_NORMAL
- en: Once the points are assigned these labels, all connected core points and their
    associate border points are assigned to an unique cluster. Outlier points are
    left as outliers without an assigned cluster. To understand the cluster assignments
    we should explain the following forms of connection.
  prefs: []
  type: TYPE_NORMAL
- en: '**directly density reachable** - point X is directly density reachable from
    A, if A is a core point and X belongs to the neighborhood (distance \(le\) eps)
    from A.'
  prefs: []
  type: TYPE_NORMAL
- en: '**density-reachable** - point Y is density reachable from A if Y belongs to
    a neighborhood of a core point that can reached from A. This would require a chain
    of core points each belonging the previous core points and the last core point
    including point Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dbca0ae43ddea0ec062c0109fc23a79.png)'
  prefs: []
  type: TYPE_IMG
- en: Directly density reachable and density reachable.
  prefs: []
  type: TYPE_NORMAL
- en: '**density-connected** - points A and B are density-connected if there is a
    point Z that is density-reachable from both points A and B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/601d90b59a9fbb894776c62ebf7e5537.png)'
  prefs: []
  type: TYPE_IMG
- en: Density connected.
  prefs: []
  type: TYPE_NORMAL
- en: '**density-based cluster** - a nonempty set where all points are density-connected
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1541a1e7a843de78416ea2d62714561.png)'
  prefs: []
  type: TYPE_IMG
- en: Density-based clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a concise summary of these steps for solving DBSCAN,
  prefs: []
  type: TYPE_NORMAL
- en: all points are labeled as unvisited
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: randomly visit an unvisited sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: check if a core point (\(\ge min_{Pts}\) within \(\epsilon\) distance), if so
    label as core otherwise label as outlier
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: now visit all points within eps distance of the core point, determine if core,
    otherwise label as border point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: recursive operation where all points within eps distance of new core points
    are checked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: once this is exhausted then randomly visit an unvisited point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach may be thought of as identify and grow/merge clusters guided by
    local point density.
  prefs: []
  type: TYPE_NORMAL
- en: Load the required libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‘python -m pip install [package-name]’. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convenience function to add major and minor gridlines to plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Custom Colormap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom colormap for DBSCAN for several groups and 0 set as an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Set the working directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don’t lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s load one of the provided multivariate, spatial datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: you can select datasets 1-4 below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are diverse multivariate datasets with:'
  prefs: []
  type: TYPE_NORMAL
- en: facies (integers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (fraction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability (\(mD\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, cluster analysis is unsupervised, inferential machine learning and does
    not use labels. We have facies available so we can evaluate our cluster groups
    with respect to the facies assigned.
  prefs: []
  type: TYPE_NORMAL
- en: We load the selected dataset with the pandas ‘read_csv’ function into a data
    frame we called ‘df’ and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‘read_csv’ with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: but read_csv() has required input parameters. The essential one is the name
    of the file. For our circumstance all the other default parameters are fine. If
    you want to see all the possible parameters for this function, just go to the
    [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Tip** - established Python packages like pandas have excellent documentation.
    They are an excellent first stop to learn new packages. I also find examples critical
    and this motivates all the workflow examples and codes in my courses!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data file.
    So we have to specficy the name / variable representing this new DataFrame object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can switch the dataset from \(1, \ldots, 4\) to try out different clustering
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Warning some of these datasets examples are quite difficult for clustering!
    I made a variety for you to test your skills!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '|  | AI | Por | Facies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 381.296221 | 8.980678 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 304.449932 | 10.135657 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 193.607906 | 16.150132 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 205.778123 | 13.696509 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 290.854794 | 13.132821 | 3 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics and Histograms for the Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 1800.0 | 625.522917 | 247.547997 | 10.0 | 518.076017 | 683.865824 |
    783.315226 | 1200.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 1800.0 | 17.122778 | 4.778704 | 0.0 | 14.663434 | 17.674606 | 20.361860
    | 30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 1800.0 | 1.666667 | 0.816723 | 1.0 | 1.000000 | 1.000000 | 2.000000
    | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: Let’s also check the proportion of facies.
  prefs: []
  type: TYPE_NORMAL
- en: once again, we will not use facies, but facies indicates the proportion of samples
    natural cluster in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d433b8288a0a3df029306975e7de39b87ce67841a32778154ff039c43b6462c0.png](../Images/597839898a00cdf727aab95af331e2dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two considered features are quite incompatible. They have dramatically
    different:'
  prefs: []
  type: TYPE_NORMAL
- en: magnitudes / averages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the difference in ranges between the features is arbitrary we need to
    remove this by transforming each of the features. We normalize each of the features
    to range from 0.0 (minimum) to 1.0 (maximum).
  prefs: []
  type: TYPE_NORMAL
- en: Note, with normalization there is no distribution shape change so we would typically
    check for issues like outliers before the transformation, for brevity this step
    is not included here given there are no outliers in the provided datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we use these normalized feature values for the clustering workflows, e.g.,
    for calculating the required distances distance between samples and prototypes
    in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: note, for each data sample we have original feature and transformed feature
    values; therefore, it is very easy to go back and forth without the need for backtransformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '|  | AI | Por | Facies | nAI | nPor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 381.296221 | 8.980678 | 3 | 0.312014 | 0.299356 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 304.449932 | 10.135657 | 3 | 0.247437 | 0.337855 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 193.607906 | 16.150132 | 3 | 0.154292 | 0.538338 |'
  prefs: []
  type: TYPE_TB
- en: Let’s confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 1800.0 | 625.522917 | 247.547997 | 10.0 | 518.076017 | 683.865824 |
    783.315226 | 1200.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 1800.0 | 17.122778 | 4.778704 | 0.0 | 14.663434 | 17.674606 | 20.361860
    | 30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 1800.0 | 1.666667 | 0.816723 | 1.0 | 1.000000 | 1.000000 | 2.000000
    | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| nAI | 1800.0 | 0.517246 | 0.208024 | 0.0 | 0.426955 | 0.566274 | 0.649845
    | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| nPor | 1800.0 | 0.570759 | 0.159290 | 0.0 | 0.488781 | 0.589154 | 0.678729
    | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: and let’s compare the original and transformed histograms for the features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/262590e49456fa3a3dfcfb8df4eedb2bc7e2381fde8f758cfb076cab46379e9f.png](../Images/6946eafd11051796db89f44d89a41b0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Quick Peek at Available Labels for Educational Purposes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I know this is cheating, but to calibrate our eye’s and to check the natural
    clusters in our dataset, let’s cheat and look at the features scatter plotted
    colored by the facies labels.
  prefs: []
  type: TYPE_NORMAL
- en: for this demonstration workflow this is our inaccessible truth model, we are
    using this as only a teaching tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: after this we will leave these labels out and attempt to automate their assignment
    with cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/652cda19b2c66a627a22fa17d9640e4e3745a8cf34e2a24347e8901aaf82fdc4.png](../Images/a385f8e325d6fef6ece0176c37991757.png)'
  prefs: []
  type: TYPE_IMG
- en: For just a moment, look at the truth model. Does it look like this will be difficult
    to reproduce and automate with cluster analysis?
  prefs: []
  type: TYPE_NORMAL
- en: are the natural groups separated - is there overlap?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what is the shape of the cluster groups - are the groups spherical?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what are the frequencies in each cluster group?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is instructive, since if the groups are not well separated, not spherical
    and have uneven frequencies within each natural group, then k-means clustering
    will not likely perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s ignore the truth facies categorical assignments and move forward
    with proper cluster analysis, unsupervised without labels.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the Unlabeled Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use clustering for automated assignment of facies
    based on two features.
  prefs: []
  type: TYPE_NORMAL
- en: I limit us two features for each of visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by visualizing the unlabeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We look at the data in original units and normalized units through this entire
    exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/0dba4275f8b9d01f98df3f5f393358b936a2551c1053c568b18662686d49c08e.png](../Images/1beddf39d6d6aa601f72c6ced44757ef.png)'
  prefs: []
  type: TYPE_IMG
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we start with k-means clustering. From our previous analysis we may know
    that this will not go well!
  prefs: []
  type: TYPE_NORMAL
- en: I have an entire demonstration dedicated to [k-means cluster](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Clustering.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we use k-means Clustering to compare and contrast with the subsequent density-based
    method that we will attempt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once again, let’s be mindful of the assumptions of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: spherical variability over the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: same variability over each feature (after our min/max normalization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prior probability of membership in all clusters, equal number of samples in
    each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset violates these assumptions with non-spherical shapes, unequal proportions
    of samples in each cluster (we know this because we have the truth facies and
    we did peak).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/da99a7400002db428b69cf03b1f80d110a90cd9da772d44ceb538fe35ad1df0b.png](../Images/b6800135b0332bade66709fafc2729e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected there are issues:'
  prefs: []
  type: TYPE_NORMAL
- en: the non-convex, nonlinear shapes, and low-density representation of edges of
    clusters are not captured well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a results obvious connected geometries are broken up.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DBSCAN approach that seeds and grows clusters groups based on high density
    is well suited to working with data with an arbitrary (even non-convex) shape
    and with noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again the two parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**eps** - maximum distance between to samples in features space for a sample
    to be included in the neighbourhood of the other sample. Too small a value will
    result in many clusters and outliers, and too large a value will result in one
    cluster with all the samples together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**min_samples** - minimum number of samples in a neighbourhood to spawn a new
    cluster. Use a larger value for noisy data to prevent identifying clusters due
    to a few grouped outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a6d400b449a860659a58145ad193c6f51de9b329465afd97fd7e2cbb3204f27f.png](../Images/6bb64696f44116faf7cea700a0f22200.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This result is very different than k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: the cluster groups are not symmetric or spherical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we also have outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore the sensitivity in the parameters, by looking at the results over
    a \(3 \times 3\) combinatorial of hyperparameters. Run this the first time as
    is and then try making changes the values in this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/3a9018a2b915e0bf08f57be33b055aefbf0440860254662a0505d30738c82ef9.png](../Images/6026c92be73d1f48a2bf8b744cf3ab03.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating the DBSCAN Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s try to use a more repeatable method to calculate the DBSCAN hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**min samples** - we set with expert knowledge, there is a rule of thumb, \(min_{sample}
    \ge D + 1\) or \(min_{sample} \ge 2 \times D + 1\) for large, noisy datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eps** - we will use the nearest neighbour function to estimate it given the
    minimum number of samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s calculate the nearest neighbour distance for each sample data to the
    min sample nearest neighbour in normalized feature space with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a \(n \times k\) array with the distances and the sample data indices:'
  prefs: []
  type: TYPE_NORMAL
- en: for each \(n\) sample we have a row of nearest neighbour distances up to k nearest
    neighbours (closest to furthest in the columns), note the first neighbour is sample
    itself; therefore the first column is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we will then sort the rows in ascending order and plot relative to the sample
    index, \(i = 1,\ldots,n-1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we will plot the distances vs. the sample index in the ascending order
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/00f3b27f3c08d348670d975111110a79e3fd87b0b9515312a5f750c7be546819.png](../Images/d60288cd53277b9a6eb6b87f952c36fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we look for the nearest neighbour distance with the maximum positive curvature,
    known as the ‘elbow’ of the nearest neighbour plot
  prefs: []
  type: TYPE_NORMAL
- en: this provides an indication of the cluster sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we would estimate eps as just above 0.03 - 0.04 in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some observations from DBSCAN
  prefs: []
  type: TYPE_NORMAL
- en: the results are quite sensitive to the selection of **eps** and **min_samples**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in the sparse part of the ‘cluster’ many of the data are assigned to other groups
    are remain as outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the result with this estimated eps hyperparameter is not satisfying.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/e79ecd502b7cf3f722b4eb588a2a2182dee76fa2b9e98bc6368f38a943962e1b.png](../Images/0304f745d4f6039f44c8361d5016c294.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of density-based clustering with DBSCAN. Much more
    could be done and discussed, I have many more resources. Check out my [shared
    resource inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture
    links at the start of this chapter with resource links in the videos’ descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael’s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael’s work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I’d be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
