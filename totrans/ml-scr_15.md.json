["```py\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\n# import data\ncancer = datasets.load_breast_cancer()\nX_cancer = cancer['data']\ny_cancer = cancer['target']\nwine = datasets.load_wine()\nX_wine = wine['data']\ny_wine = wine['target'] \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nbinary_model = LogisticRegression(C = 10**5, max_iter = 1e5)\nbinary_model.fit(X_cancer, y_cancer) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = binary_model.predict(X_cancer)\np_hats = binary_model.predict_proba(X_cancer)\nprint(f'Training accuracy: {binary_model.score(X_cancer, y_cancer)}') \n```", "```py\nTraining accuracy: 0.984182776801406 \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmulticlass_model = LogisticRegression(multi_class = 'multinomial', C = 10**5, max_iter = 10**4)\nmulticlass_model.fit(X_wine, y_wine) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='multinomial', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = multiclass_model.predict(X_wine)\np_hats = multiclass_model.predict_proba(X_wine)\nprint(f'Training accuracy: {multiclass_model.score(X_wine, y_wine)}') \n```", "```py\nTraining accuracy: 1.0 \n```", "```py\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_cancer, y_cancer); \n```", "```py\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(n_components = 1)\nlda.fit(X_cancer, y_cancer);\n\nf0 = np.dot(X_cancer, lda.coef_[0])[y_cancer == 0]\nf1 = np.dot(X_cancer, lda.coef_[0])[y_cancer == 1]\nprint('Separated:', (min(f0) > max(f1)) | (max(f0) < min(f1))) \n```", "```py\nSeparated: False \n```", "```py\nfig, ax = plt.subplots(figsize = (7,5))\nsns.distplot(f0, bins = 25, kde = False, \n             color = 'cornflowerblue', label = 'Class 0')\nsns.distplot(f1, bins = 25, kde = False, \n             color = 'darkblue', label = 'Class 1')\nax.set_xlabel(r\"$f\\hspace{.25}(x_n)$\", size = 14)\nax.set_title(r\"Histogram of $f\\hspace{.25}(x_n)$ by Class\", size = 16)\nax.legend()\nsns.despine() \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nbinary_model = LogisticRegression(C = 10**5, max_iter = 1e5)\nbinary_model.fit(X_cancer, y_cancer) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = binary_model.predict(X_cancer)\np_hats = binary_model.predict_proba(X_cancer)\nprint(f'Training accuracy: {binary_model.score(X_cancer, y_cancer)}') \n```", "```py\nTraining accuracy: 0.984182776801406 \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmulticlass_model = LogisticRegression(multi_class = 'multinomial', C = 10**5, max_iter = 10**4)\nmulticlass_model.fit(X_wine, y_wine) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='multinomial', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = multiclass_model.predict(X_wine)\np_hats = multiclass_model.predict_proba(X_wine)\nprint(f'Training accuracy: {multiclass_model.score(X_wine, y_wine)}') \n```", "```py\nTraining accuracy: 1.0 \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nbinary_model = LogisticRegression(C = 10**5, max_iter = 1e5)\nbinary_model.fit(X_cancer, y_cancer) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100000.0,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = binary_model.predict(X_cancer)\np_hats = binary_model.predict_proba(X_cancer)\nprint(f'Training accuracy: {binary_model.score(X_cancer, y_cancer)}') \n```", "```py\nTraining accuracy: 0.984182776801406 \n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nmulticlass_model = LogisticRegression(multi_class = 'multinomial', C = 10**5, max_iter = 10**4)\nmulticlass_model.fit(X_wine, y_wine) \n```", "```py\nLogisticRegression(C=100000, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='multinomial', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) \n```", "```py\ny_hats = multiclass_model.predict(X_wine)\np_hats = multiclass_model.predict_proba(X_wine)\nprint(f'Training accuracy: {multiclass_model.score(X_wine, y_wine)}') \n```", "```py\nTraining accuracy: 1.0 \n```", "```py\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_cancer, y_cancer); \n```", "```py\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(n_components = 1)\nlda.fit(X_cancer, y_cancer);\n\nf0 = np.dot(X_cancer, lda.coef_[0])[y_cancer == 0]\nf1 = np.dot(X_cancer, lda.coef_[0])[y_cancer == 1]\nprint('Separated:', (min(f0) > max(f1)) | (max(f0) < min(f1))) \n```", "```py\nSeparated: False \n```", "```py\nfig, ax = plt.subplots(figsize = (7,5))\nsns.distplot(f0, bins = 25, kde = False, \n             color = 'cornflowerblue', label = 'Class 0')\nsns.distplot(f1, bins = 25, kde = False, \n             color = 'darkblue', label = 'Class 1')\nax.set_xlabel(r\"$f\\hspace{.25}(x_n)$\", size = 14)\nax.set_title(r\"Histogram of $f\\hspace{.25}(x_n)$ by Class\", size = 16)\nax.legend()\nsns.despine() \n```"]