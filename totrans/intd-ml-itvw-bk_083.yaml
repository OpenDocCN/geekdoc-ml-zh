- en: 6.2 Complexity and numerical analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.2 复杂度和数值分析
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/6.2-complexity-and-numerical-analysis.html](https://huyenchip.com/ml-interviews-book/contents/6.2-complexity-and-numerical-analysis.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huyenchip.com/ml-interviews-book/contents/6.2-complexity-and-numerical-analysis.html](https://huyenchip.com/ml-interviews-book/contents/6.2-complexity-and-numerical-analysis.html)
- en: '*If some characters seem to be missing, it''s because MathJax is not loaded
    correctly. Refreshing the page should fix it.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果某些字符似乎缺失，那是因为MathJax没有正确加载。刷新页面应该可以解决这个问题。*'
- en: Given that most of the recent breakthroughs in machine learning come from bigger
    models that require massive memory and computational power, it’s important to
    not only know how to implement a model but also how to scale it. To scale a model,
    we’d need to be able to estimate memory requirement and computational cost, as
    well as mitigate numerical instability when training and serving machine learning
    models. Here are some of the questions that can be asked to evaluate your understanding
    of numerical stability and scalability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最近在机器学习中的大多数突破都来自于需要大量内存和计算能力的更大模型，因此，不仅要知道如何实现模型，还要知道如何扩展模型。为了扩展模型，我们需要能够估计内存需求和计算成本，以及在训练和部署机器学习模型时减轻数值不稳定性。以下是一些可以提出的问题，以评估你对数值稳定性和可扩展性的理解。
- en: Matrix multiplication
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: '[E] You have three matrices:  and you need to calculate the product . In what
    order would you perform your multiplication and why?'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 你有三个矩阵：\(A\)、\(B\) 和 \(C\)，你需要计算 \(A \times B \times C\) 的乘积。你会按照什么顺序执行乘法，为什么？'
- en: '[M] Now you need to calculate the product of  matrices . How would you determine
    the order in which to perform the multiplication?'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 现在你需要计算 \(A\) 矩阵的乘积。你将如何确定执行乘法的顺序？'
- en: '[E] What are some of the causes for numerical instability in deep learning?'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 深度学习中数值不稳定的可能原因有哪些？'
- en: '[E] In many machine learning techniques (e.g. batch norm), we often see a small
    term  added to the calculation. What’s the purpose of that term?'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 在许多机器学习技术（例如批量归一化）中，我们经常看到在计算中添加了一个小的项。这个项的目的是什么？'
- en: '[E] What made GPUs popular for deep learning? How are they compared to TPUs?'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] 什么使得GPU在深度学习中变得流行？它们与TPU相比如何？'
- en: '[M] What does it mean when we say a problem is intractable?'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 当我们说一个问题是无解的时，这意味着什么？'
- en: '[H] What are the time and space complexity for doing backpropagation on a recurrent
    neural network?'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 对循环神经网络进行反向传播的时间复杂度和空间复杂度是多少？'
- en: '[H] Is knowing a model’s architecture and its hyperparameters enough to calculate
    the memory requirements for that model?'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 仅了解模型架构及其超参数是否足以计算该模型的内存需求？'
- en: '[H] Your model works fine on a single GPU but gives poor results when you train
    it on 8 GPUs. What might be the cause of this? What would you do to address it?'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 你的模型在单个GPU上运行良好，但在8个GPU上训练时给出较差的结果。这可能是由于什么原因？你会如何解决这个问题？'
- en: '[H] What benefits do we get from reducing the precision of our model? What
    problems might we run into? How to solve these problems?'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 降低模型精度我们能获得哪些好处？我们可能会遇到什么问题？如何解决这些问题？'
- en: '[H] How to calculate the average of 1M floating-point numbers with minimal
    loss of precision?'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 如何以最小的精度损失计算100万个浮点数的平均值？'
- en: '[H] How should we implement batch normalization if a batch is spread out over
    multiple GPUs?'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] 如果一个批次分散在多个GPU上，我们应该如何实现批量归一化？'
- en: '[M] Given the following code snippet. What might be a problem with it? How
    would you improve it? **Hint**: this is an actual question asked on [StackOverflow](https://stackoverflow.com/questions/39667089/python-vectorizing-nested-for-loops/39667342).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] 给定以下代码片段。它可能存在什么问题？你会如何改进它？**提示**：这是一个在[StackOverflow](https://stackoverflow.com/questions/39667089/python-vectorizing-nested-for-loops/39667342)上实际提出的问题。'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
