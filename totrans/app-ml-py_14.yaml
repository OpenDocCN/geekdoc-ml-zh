- en: Spectral Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_spectral_clustering.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_spectral_clustering.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **Spectral Clustering**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cluster Analysis](https://youtu.be/oFE10cLl0Fs?si=AwmYnrYggtYWGV2n)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Issues with k-Means Clustering](https://youtu.be/ysJw8M_J40I?si=EIlg2941QrfAt7zE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Density-based Clustering](https://www.youtube.com/watch?v=3GaLe8HaDMc&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral Clustering - I will get this recorded and updated ASAP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation for Spectral Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing distinct populations to train prediction models often reduces model accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common methods for cluster analysis like k-means clustering are easy to apply
    but are only based on proximity in the feature space and do not integrate information
    about the pairwise relationships between the data samples; therefore, it is essential
    to add clustering methods, like spectral clustering, to our toolkit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always remember, the computer does not provide meaning nor description of the
    groups, that is our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow demonstrating spectral clustering (for automated category
    assignment). This should help you get started with inferential methods to find
    patterns in your subsurface data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Methods Covered
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I cover the following clustering methods,
  prefs: []
  type: TYPE_NORMAL
- en: '**k-Means clustering** as a centroid-based clustering method with predetermined
    number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-based with DBSCAN**, density-based spatial clustering of applications
    with noise, as a density-based clustering method without predetermined number
    of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral clustering** as a hierarchical connectivity-based clustering method
    with predetermined number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs cover essential prerequisites before we get started with spectral cluster
    analysis. Let‚Äôs start with the concept of inferential machine learning and then
    briefly discuss k-means clustering for comparison to spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Inferential Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no response features, \(y\), just predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëã_1,\ldots,ùëã_ùëö \]
  prefs: []
  type: TYPE_NORMAL
- en: Machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We focus on inference of the population, the natural system, instead of prediction
    of response features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The K-means clustering approach is primarily applied as an unsupervised machine
    learning method for clustering, group assignment to unlabeled data, where dissimilarity
    within clustered groups is mini minimized. The loss function that is minimized
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the assumptions that result in significant reduction in flexibility
    of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: The clusters are spherical, convex, and isotropic within the predictor feature
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is due to the model loss based on minimizes difference within clusters,
    this is accomplished through this simple spherical geometry in the predictor feature
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is equal variance for all features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \sigma_{X_1}^2 = \sigma_{X_2}^2= \ldots = \sigma_{X_m}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumed with the between sample distance / dissimilarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters have similar sizes and frequency (number of samples in the clusters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: larger cluster are divided to minimize the overall squared difference within
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clusters with few samples in feature space are overwhelmed!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral Clustering, utilize the spectrum (eigenvalues) of a matrix that represents
    the pairwise relationships between the data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between the data samples are represented by a graph, here‚Äôs
    some prerequisite graph terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8c7d179397121ddb680407254988c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of a simple graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**vertices** are the data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**edges** as the pairwise connections between data samples. These connections
    may be represented as 0 or 1 (off or on) known as adjacency or as a degree of
    connection (larger number is more connected) known as affinity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9788590784dbae3dc41d751d93ff64ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of adjacency vs. affinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the graph we can calculate the adjacency matrix or affinity matrix (\(A\))
    as the \(n \times n\) matrix with:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{bmatrix} 0 & a_{1,2} & \dots & a_{1,n} \\ a_{2,1}
    & 0 & \dots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2}
    & \dots & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that the diagonal is 0 as the data samples are not considered to be connected
    to themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'For 4 data example above, we have samples 1, 2 and 3 connected to each other
    and sample 4 is not connected to any other samples, then the adjacency matrix
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_{adjacency} = \begin{bmatrix} 0 & 1 & 1 & 0 \\ 1 & 0 & 1 &
    0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to account for the degree of connectivity for each sample, the number
    of connections. This is call the degree matrix (\(D\)).
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate the degree from adjacency matrix as an marginalization over
    the rows or columns (doesn‚Äôt matter as the \(A\) is symmetric).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ D_{i,i} = \sum_{j=1}^n A_{i,j} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The off-diagonal values for the \(D\) matrix are set to zero. Here is the general
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{bmatrix} d_1 & 0 & \dots & 0 \\ 0 & d_2 & \dots &
    0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & d_n \\ \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For our 4 data samples the \(D\) matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{bmatrix} 2 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0
    & 2 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Data samples 1, 2, and 3 are connected to 2 other samples (they are all connected
    to each other), but data sample 4 is not connected to any other samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we combine the adjacency (or affinity) matrix with degree matrix to form
    the Graph Laplacian Matrix (\(L\)) through this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = D - A \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting matrix has the general form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{bmatrix} d_1 & -a_{1,2} & \dots & -a_{1,n} \\ -a_{2,1}
    & d_2 & \dots & -a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ -a_{n,1} & -a_{n,2}
    & \dots & d_n \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if we return to our 4 data sample example we get the graph Laplacian (\(L\))
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{bmatrix} 2 & -1 & -1 & 0 \\ -1 & 2 & -1 & 0 \\ -1
    & -1 & 2 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now have a single matrix that summarizes the graph, i.e., the pairwise connections
    between our data samples. But we need to summarize this such that we describe
    the most amount of information in the fewest dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: this sounds like a similar problem as the one we solved with principal component
    analysis, so let‚Äôs use the same mathematics, eigenvalues and eigenvectors, to
    give us an order set of vectors that describe the most amount of the system and
    are independent of each other so they each describe unique aspects of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we calculate the eigenvalues and eigenvectors of the graph Laplacian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs summarize the spectral clustering method:'
  prefs: []
  type: TYPE_NORMAL
- en: determine the number of clusters (like k-means clustering)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the graph, pairwise data connections are compiled in the graph Laplacian matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dimensionality reduction of the graph Laplacian matrix is accomplished with
    eigenvalues, eigenvectors linear, orthogonal projection, rotation to best describe
    the variance of the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: k-means clustering is applied on the \(2, 3, \ldots, k\) eigenvectors. For example,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for \(k=2\) clusters we just assign the \(2^{nd}\) eigenvector to the data samples,
    \(1, \ldots, n\) and find the 2 k-means clusters in 1D with k-means clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(k=3\) clusters we assign the \(2^{nd}\) and \(3^{rd}\) eigenvector to
    the data samples, \(1, \ldots, n\) and find the 3 k-means clusters in 2D with
    k-means clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What are the advantages of spectral clustering?:'
  prefs: []
  type: TYPE_NORMAL
- en: the ability to encode pairwise relationships, integrate expert knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues provide useful information on the number of clusters, based on the
    degree of ‚Äòcutting‚Äô required to make k clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lower dimensional representation for the sample data pairwise relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the eigenvalues and eigenvectors can be interpreted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see spectral clustering in action, check out my [spectral
    clustering interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spectral_Clustering.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/413e65c3d1a2ce361ed996b9051b0bda.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: This there are additional dasboards that include illustration and interpretation
    of the eigenvalues and eigenvectors from spectral clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95105bcced36853896189e850db9f424.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for spectral clustering with graph Laplacian
    and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs load a dataset and try out spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Set Up Custom Colormaps for Plotting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we specify custom colormap with discretization for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convenience functions to add commas to numbers and major and minor gridlines
    to plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial datasets. It is a comma delimited
    file with:'
  prefs: []
  type: TYPE_NORMAL
- en: you can select datasets 1-4 below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‚Äòread_csv‚Äô with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: but read csv has required input parameters. The essential one is the name of
    the file. For our circumstance all the other default parameters are fine. If you
    want to see all the possible parameters for this function, just go to the docs
    [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: The docs are always helpful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is often a lot of flexibility for Python functions, possible through using
    various inputs parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data. So
    we have to specficy the name / variable representing that new object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs run this command to load the data and then this command to extract a random
    subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We do this to reduce the number of data for ease of visualization (hard to see
    if too many points on our plots) and in the case of spectral clustering so we
    can visualize the affinity, degree and graph Laplacian matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | Facies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 24.751762 | 1199.611948 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.152299 | 530.125860 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 21.259112 | 1137.369924 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20.203508 | 1122.143340 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4.145138 | 737.744165 | 3 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 34.0 | 16.880290 | 7.045527 | 1.405136 | 12.192611 | 19.562295 | 22.544500
    | 26.296139 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 34.0 | 1007.408007 | 218.666676 | 245.633475 | 935.089796 | 1084.143851
    | 1172.372323 | 1199.611948 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 34.0 | 1.529412 | 0.748141 | 1.000000 | 1.000000 | 1.000000 | 2.000000
    | 3.000000 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs also check the proportion of facies.
  prefs: []
  type: TYPE_NORMAL
- en: we will not use these, but they indicate the proportion of samples in each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c4b8e90c46f5544496d52ad9f232bc98e8a5223ce25c3724419509e4d8a3ec9c.png](../Images/45f496bd6ac7bccb45c72f2c0c708880.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two considered features are quite incompatible. They have dramatically
    different:'
  prefs: []
  type: TYPE_NORMAL
- en: magnitudes / averages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the difference in ranges between the features is arbitrary we need to
    remove this by transforming each of the features. We normalize each of the features
    to range from 0.0 (minimum) to 1.0 (maximum).
  prefs: []
  type: TYPE_NORMAL
- en: Note, with normalization there is no distribution shape change so we would typically
    check for issues like outliers before the transformation, for brevity this step
    is not included here given there are no outliers in the provided datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we use these normalized feature values for the clustering workflows, e.g.,
    for calculating the required distances distance between samples and prototypes
    in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: note, for each data sample we have original feature and transformed feature
    values; therefore, it is very easy to go back and forth without the need for backtransformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | Facies | nPor | nPerm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 24.751762 | 1199.611948 | 1 | 0.937954 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.152299 | 530.125860 | 3 | 0.150543 | 0.298217 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 21.259112 | 1137.369924 | 1 | 0.797637 | 0.934755 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 34.0 | 16.880290 | 7.045527 | 1.405136 | 12.192611 | 19.562295 | 22.544500
    | 26.296139 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 34.0 | 1007.408007 | 218.666676 | 245.633475 | 935.089796 | 1084.143851
    | 1172.372323 | 1199.611948 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 34.0 | 1.529412 | 0.748141 | 1.000000 | 1.000000 | 1.000000 | 2.000000
    | 3.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| nPor | 34.0 | 0.621717 | 0.283055 | 0.000000 | 0.433388 | 0.729467 | 0.849277
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| nPerm | 34.0 | 0.798524 | 0.229216 | 0.000000 | 0.722717 | 0.878962 | 0.971446
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: and let‚Äôs compare the original and transformed histograms for the features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/acf5703f2a31373c27f22f9dfe1c826ef93ce18789b5e3f47ed5587755d86f9a.png](../Images/6f32faac5df4a794dba6448b5603e081.png)'
  prefs: []
  type: TYPE_IMG
- en: Quick Peek at Available Labels for Educational Purposes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I know this is cheating, but to calibrate our eye‚Äôs and to check the natural
    clusters in our dataset, let‚Äôs cheat and look at the features scatter plotted
    colored by the facies labels.
  prefs: []
  type: TYPE_NORMAL
- en: for this demonstration workflow this is our inaccessible truth model, we are
    using this as only a teaching tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: after this we will leave these labels out and attempt to automate their assignment
    with cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8be02d8288ec5cac5639f85fe3f99882ec9ab50c9883e51272accce539ab53f8.png](../Images/5ccd6d442120c520ebfb6f6523300ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use K-means clustering provide facies based on
    porosity and brittleness features.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by looking at the scatterplot of our training data features, porosity
    and brittleness.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the data in original units and normalized units through this
    entire exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d6d1f0ed5f9c9e518a0670b99b89a3775cc689e6a6479629b897594ee325bc27.png](../Images/7a931fd464f1fbc195783a4acaa06ff8.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculate Adjacency, Degree and Graph Laplacian Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first step is to calculate the affinity matrix. While kernel and nearest
    neighbour approaches are common, these methods only integrate proximity information,
    this step could include integration of various information sources.
  prefs: []
  type: TYPE_NORMAL
- en: to demonstrate this here we will assign adjacency by-hand. Let‚Äôs specify data
    samples that are connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, in practice we would use additional information to build the adjacency
    or affinity matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For demonstration purposes, let‚Äôs look at the data samples in feature space
    with indices plotted to assign some connections.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/66dade4ca857fd8365f9662e55cc3f1ce0d5a35cbb139bb143854ac9f2c77d3d.png](../Images/f830e1cc7f9fe6a3d86401dacd292eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we specify the connected data pairs by index and build the adjacency, degree
    and graph Laplacian matrices.
  prefs: []
  type: TYPE_NORMAL
- en: in practice we only build the adjacency (or affinity) matrix. The others matrices
    are calculated automatically by the spectral clustering function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/fdaaeebf599bf85787590f1efc74a462b02a6461821ea3b94577821e97f17c34.png](../Images/680fefa78db7bb2de50eb4473168da9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Spectral Clustering with Custom Adjacency Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs run spectral clustering with our adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: to do this we set the parameter ‚Äòaffinity‚Äô as precomputed and then train on
    the adjacency matrix instead of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/001f2f92cbb8990bace5fde37acc03c9b6881278353fdbf0929638f1d79b1fcd.png](../Images/bd2c005d0f9d7776fcefd18f10cec75b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: unsurprisingly the spectral groups are aligned with the connected groups that
    we specified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs try something else, we add connections between the 3 group with 2 new
    vertices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b392eaffd64400bd04bb8e50a651c5a08db8bdfbd2751bf354c688c836a5bb0c.png](../Images/45b914a7bc66e9c00906f9617b485508.png)'
  prefs: []
  type: TYPE_IMG
- en: See the added vertices are shown as red bold lines above. Note the shift in
    the cluster groups across that gap that we connected.
  prefs: []
  type: TYPE_NORMAL
- en: even though all the data samples are connects, spectral clustering provides
    rational groups as it learns the difference between direct connections (two data
    samples connected by a vertex) and indirect connections (two data samples connected
    via multiple vertices passing through one or more other data samples).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, there is no meaning to the actual cluster group numbers, so don‚Äôt
    be concerned with the switch in the cluster group numerical and color assignments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral Clustering with Affinity Matrix from Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a variety of methods to calculate an affinity matrix from the data
    sample locations in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'a common method is to use kernel functions, like a radial basis function (RBF)
    kernel, parameterized by \(\gamma\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ K(x_i,x_j) = exp \left(- \gamma || x_i - x_j ||^2 \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(|| x_i - x_j ||\) is the distance between data samples \(x_i\) and \(x_j\)
    in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: since we are using a distance calculation, we are careful to ensure that normalized
    or standardize features are used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**large \(\gamma\)** - similarity is assigned locally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**small \(\gamma\)** - similarity persists over long distances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4ab466d66a474f22326f87525595960bb38698a43b7d9097ff234185b72cfdc6.png](../Images/40b2f8acc469e88a2d931a24e25f1b19.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs look at the affinity matrix that was calculated by scikit-learn‚Äôs SpectralClustering
    function to achieve this clustering result.
  prefs: []
  type: TYPE_NORMAL
- en: this is the pairwise connection between all of the sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a26ea28f26564381944024963e1cfddfdb181e67cb3d201ae5ee0983ae380ed3.png](../Images/53361211caca5ff75eaadd9e66ea6014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here‚Äôs some general observations:'
  prefs: []
  type: TYPE_NORMAL
- en: there is a large number of pairwise connections between the sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs demonstrate the sensitivity of the solution to the model hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs explore the sensitivity in the parameters, by looking at the results over
    a \(3 \times 3\) combinatorial of hyperparameters. Run this the first time as
    is and then try making changes to these values in this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/32f66e1c345a7062dc8136bbfb43b739043b2fd9898edff4eb326237afe87ead.png](../Images/fcc840ff81dd74729548746086f57b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An observation from these spectral cluster group assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: as gamma increases, range of similarity is reduced and the model is more sensitive
    to gaps in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of cluster analysis with spectral clustering. Much
    more could be done and discussed, I have many more resources. Check out my [shared
    resource inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture
    links at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Spectral Cluster Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to learn and segment distinct populations to improve our prediction
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing distinct populations to train prediction models often reduces model accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering is an inferential machine learning method to automate the segmentation
    of the dataset into separate groups, known as clusters and specified by an integer
    index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common methods for cluster analysis like k-means clustering are easy to apply
    but are only based on proximity in the feature space and do not integrate information
    about the pairwise relationships between the data samples; therefore, it is essential
    to add clustering methods, like spectral clustering, to our toolkit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always remember, the computer does not provide meaning nor description of the
    groups, that is our job!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow demonstrating spectral clustering (for automated category
    assignment). This should help you get started with inferential methods to find
    patterns in your subsurface data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Methods Covered
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I cover the following clustering methods,
  prefs: []
  type: TYPE_NORMAL
- en: '**k-Means clustering** as a centroid-based clustering method with predetermined
    number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density-based with DBSCAN**, density-based spatial clustering of applications
    with noise, as a density-based clustering method without predetermined number
    of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral clustering** as a hierarchical connectivity-based clustering method
    with predetermined number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs cover essential prerequisites before we get started with spectral cluster
    analysis. Let‚Äôs start with the concept of inferential machine learning and then
    briefly discuss k-means clustering for comparison to spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Inferential Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no response features, \(y\), just predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëã_1,\ldots,ùëã_ùëö \]
  prefs: []
  type: TYPE_NORMAL
- en: Machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We focus on inference of the population, the natural system, instead of prediction
    of response features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The K-means clustering approach is primarily applied as an unsupervised machine
    learning method for clustering, group assignment to unlabeled data, where dissimilarity
    within clustered groups is mini minimized. The loss function that is minimized
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of import aspects for k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prototype Method** - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative Solution** - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning** - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Weighting** - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the assumptions that result in significant reduction in flexibility
    of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: The clusters are spherical, convex, and isotropic within the predictor feature
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is due to the model loss based on minimizes difference within clusters,
    this is accomplished through this simple spherical geometry in the predictor feature
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is equal variance for all features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \sigma_{X_1}^2 = \sigma_{X_2}^2= \ldots = \sigma_{X_m}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: this assumed with the between sample distance / dissimilarity calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters have similar sizes and frequency (number of samples in the clusters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: larger cluster are divided to minimize the overall squared difference within
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clusters with few samples in feature space are overwhelmed!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral Clustering, utilize the spectrum (eigenvalues) of a matrix that represents
    the pairwise relationships between the data samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between the data samples are represented by a graph, here‚Äôs
    some prerequisite graph terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8c7d179397121ddb680407254988c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of a simple graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**vertices** are the data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**edges** as the pairwise connections between data samples. These connections
    may be represented as 0 or 1 (off or on) known as adjacency or as a degree of
    connection (larger number is more connected) known as affinity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9788590784dbae3dc41d751d93ff64ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of adjacency vs. affinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the graph we can calculate the adjacency matrix or affinity matrix (\(A\))
    as the \(n \times n\) matrix with:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{bmatrix} 0 & a_{1,2} & \dots & a_{1,n} \\ a_{2,1}
    & 0 & \dots & a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n,1} & a_{n,2}
    & \dots & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Note that the diagonal is 0 as the data samples are not considered to be connected
    to themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'For 4 data example above, we have samples 1, 2 and 3 connected to each other
    and sample 4 is not connected to any other samples, then the adjacency matrix
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_{adjacency} = \begin{bmatrix} 0 & 1 & 1 & 0 \\ 1 & 0 & 1 &
    0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to account for the degree of connectivity for each sample, the number
    of connections. This is call the degree matrix (\(D\)).
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate the degree from adjacency matrix as an marginalization over
    the rows or columns (doesn‚Äôt matter as the \(A\) is symmetric).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ D_{i,i} = \sum_{j=1}^n A_{i,j} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The off-diagonal values for the \(D\) matrix are set to zero. Here is the general
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{bmatrix} d_1 & 0 & \dots & 0 \\ 0 & d_2 & \dots &
    0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & d_n \\ \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For our 4 data samples the \(D\) matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{bmatrix} 2 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0
    & 2 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Data samples 1, 2, and 3 are connected to 2 other samples (they are all connected
    to each other), but data sample 4 is not connected to any other samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we combine the adjacency (or affinity) matrix with degree matrix to form
    the Graph Laplacian Matrix (\(L\)) through this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = D - A \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting matrix has the general form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{bmatrix} d_1 & -a_{1,2} & \dots & -a_{1,n} \\ -a_{2,1}
    & d_2 & \dots & -a_{2,n} \\ \vdots & \vdots & \ddots & \vdots \\ -a_{n,1} & -a_{n,2}
    & \dots & d_n \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if we return to our 4 data sample example we get the graph Laplacian (\(L\))
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{bmatrix} 2 & -1 & -1 & 0 \\ -1 & 2 & -1 & 0 \\ -1
    & -1 & 2 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now have a single matrix that summarizes the graph, i.e., the pairwise connections
    between our data samples. But we need to summarize this such that we describe
    the most amount of information in the fewest dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: this sounds like a similar problem as the one we solved with principal component
    analysis, so let‚Äôs use the same mathematics, eigenvalues and eigenvectors, to
    give us an order set of vectors that describe the most amount of the system and
    are independent of each other so they each describe unique aspects of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we calculate the eigenvalues and eigenvectors of the graph Laplacian.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs summarize the spectral clustering method:'
  prefs: []
  type: TYPE_NORMAL
- en: determine the number of clusters (like k-means clustering)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the graph, pairwise data connections are compiled in the graph Laplacian matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dimensionality reduction of the graph Laplacian matrix is accomplished with
    eigenvalues, eigenvectors linear, orthogonal projection, rotation to best describe
    the variance of the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: k-means clustering is applied on the \(2, 3, \ldots, k\) eigenvectors. For example,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for \(k=2\) clusters we just assign the \(2^{nd}\) eigenvector to the data samples,
    \(1, \ldots, n\) and find the 2 k-means clusters in 1D with k-means clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(k=3\) clusters we assign the \(2^{nd}\) and \(3^{rd}\) eigenvector to
    the data samples, \(1, \ldots, n\) and find the 3 k-means clusters in 2D with
    k-means clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What are the advantages of spectral clustering?:'
  prefs: []
  type: TYPE_NORMAL
- en: the ability to encode pairwise relationships, integrate expert knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues provide useful information on the number of clusters, based on the
    degree of ‚Äòcutting‚Äô required to make k clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lower dimensional representation for the sample data pairwise relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the eigenvalues and eigenvectors can be interpreted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to see spectral clustering in action, check out my [spectral
    clustering interactive Python dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spectral_Clustering.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/413e65c3d1a2ce361ed996b9051b0bda.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: This there are additional dasboards that include illustration and interpretation
    of the eigenvalues and eigenvectors from spectral clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95105bcced36853896189e850db9f424.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard for spectral clustering with graph Laplacian
    and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs load a dataset and try out spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following code loads the required libraries. These should have been installed
    with Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Set Up Custom Colormaps for Plotting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we specify custom colormap with discretization for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convenience functions to add commas to numbers and major and minor gridlines
    to plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Loading Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial datasets. It is a comma delimited
    file with:'
  prefs: []
  type: TYPE_NORMAL
- en: you can select datasets 1-4 below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a data frame we called ‚Äòdf‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python Tip: using functions from a package** just type the label for the
    package that we declared at the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'so we can access the pandas function ‚Äòread_csv‚Äô with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: but read csv has required input parameters. The essential one is the name of
    the file. For our circumstance all the other default parameters are fine. If you
    want to see all the possible parameters for this function, just go to the docs
    [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: The docs are always helpful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is often a lot of flexibility for Python functions, possible through using
    various inputs parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, the program has an output, a pandas DataFrame loaded from the data. So
    we have to specficy the name / variable representing that new object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs run this command to load the data and then this command to extract a random
    subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We do this to reduce the number of data for ease of visualization (hard to see
    if too many points on our plots) and in the case of spectral clustering so we
    can visualize the affinity, degree and graph Laplacian matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | Facies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 24.751762 | 1199.611948 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.152299 | 530.125860 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 21.259112 | 1137.369924 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20.203508 | 1122.143340 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 4.145138 | 737.744165 | 3 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table includes porosity (fraction) and acoustic impedance (\(\frac{kg}{m^3}
    \cdot \frac{m}{s} \cdot 10^3\)) that we will work with in the demonstration below.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum,
    and quartiles all in a nice data table. We use transpose just to flip the table
    so that features are on the rows and the statistics are on the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 34.0 | 16.880290 | 7.045527 | 1.405136 | 12.192611 | 19.562295 | 22.544500
    | 26.296139 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 34.0 | 1007.408007 | 218.666676 | 245.633475 | 935.089796 | 1084.143851
    | 1172.372323 | 1199.611948 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 34.0 | 1.529412 | 0.748141 | 1.000000 | 1.000000 | 1.000000 | 2.000000
    | 3.000000 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs also check the proportion of facies.
  prefs: []
  type: TYPE_NORMAL
- en: we will not use these, but they indicate the proportion of samples in each cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c4b8e90c46f5544496d52ad9f232bc98e8a5223ce25c3724419509e4d8a3ec9c.png](../Images/45f496bd6ac7bccb45c72f2c0c708880.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two considered features are quite incompatible. They have dramatically
    different:'
  prefs: []
  type: TYPE_NORMAL
- en: magnitudes / averages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variances / ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the difference in ranges between the features is arbitrary we need to
    remove this by transforming each of the features. We normalize each of the features
    to range from 0.0 (minimum) to 1.0 (maximum).
  prefs: []
  type: TYPE_NORMAL
- en: Note, with normalization there is no distribution shape change so we would typically
    check for issues like outliers before the transformation, for brevity this step
    is not included here given there are no outliers in the provided datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we use these normalized feature values for the clustering workflows, e.g.,
    for calculating the required distances distance between samples and prototypes
    in our workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: note, for each data sample we have original feature and transformed feature
    values; therefore, it is very easy to go back and forth without the need for backtransformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Por | Perm | Facies | nPor | nPerm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 24.751762 | 1199.611948 | 1 | 0.937954 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.152299 | 530.125860 | 3 | 0.150543 | 0.298217 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 21.259112 | 1137.369924 | 1 | 0.797637 | 0.934755 |'
  prefs: []
  type: TYPE_TB
- en: Let‚Äôs confirm that our normalized porosity and acoustic impedance now range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '|  | count | mean | std | min | 25% | 50% | 75% | max |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Por | 34.0 | 16.880290 | 7.045527 | 1.405136 | 12.192611 | 19.562295 | 22.544500
    | 26.296139 |'
  prefs: []
  type: TYPE_TB
- en: '| Perm | 34.0 | 1007.408007 | 218.666676 | 245.633475 | 935.089796 | 1084.143851
    | 1172.372323 | 1199.611948 |'
  prefs: []
  type: TYPE_TB
- en: '| Facies | 34.0 | 1.529412 | 0.748141 | 1.000000 | 1.000000 | 1.000000 | 2.000000
    | 3.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| nPor | 34.0 | 0.621717 | 0.283055 | 0.000000 | 0.433388 | 0.729467 | 0.849277
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| nPerm | 34.0 | 0.798524 | 0.229216 | 0.000000 | 0.722717 | 0.878962 | 0.971446
    | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: and let‚Äôs compare the original and transformed histograms for the features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/acf5703f2a31373c27f22f9dfe1c826ef93ce18789b5e3f47ed5587755d86f9a.png](../Images/6f32faac5df4a794dba6448b5603e081.png)'
  prefs: []
  type: TYPE_IMG
- en: Quick Peek at Available Labels for Educational Purposes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I know this is cheating, but to calibrate our eye‚Äôs and to check the natural
    clusters in our dataset, let‚Äôs cheat and look at the features scatter plotted
    colored by the facies labels.
  prefs: []
  type: TYPE_NORMAL
- en: for this demonstration workflow this is our inaccessible truth model, we are
    using this as only a teaching tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: after this we will leave these labels out and attempt to automate their assignment
    with cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/8be02d8288ec5cac5639f85fe3f99882ec9ab50c9883e51272accce539ab53f8.png](../Images/5ccd6d442120c520ebfb6f6523300ae1.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we want to use K-means clustering provide facies based on
    porosity and brittleness features.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by looking at the scatterplot of our training data features, porosity
    and brittleness.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at the data in original units and normalized units through this
    entire exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d6d1f0ed5f9c9e518a0670b99b89a3775cc689e6a6479629b897594ee325bc27.png](../Images/7a931fd464f1fbc195783a4acaa06ff8.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculate Adjacency, Degree and Graph Laplacian Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first step is to calculate the affinity matrix. While kernel and nearest
    neighbour approaches are common, these methods only integrate proximity information,
    this step could include integration of various information sources.
  prefs: []
  type: TYPE_NORMAL
- en: to demonstrate this here we will assign adjacency by-hand. Let‚Äôs specify data
    samples that are connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, in practice we would use additional information to build the adjacency
    or affinity matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For demonstration purposes, let‚Äôs look at the data samples in feature space
    with indices plotted to assign some connections.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/66dade4ca857fd8365f9662e55cc3f1ce0d5a35cbb139bb143854ac9f2c77d3d.png](../Images/f830e1cc7f9fe6a3d86401dacd292eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we specify the connected data pairs by index and build the adjacency, degree
    and graph Laplacian matrices.
  prefs: []
  type: TYPE_NORMAL
- en: in practice we only build the adjacency (or affinity) matrix. The others matrices
    are calculated automatically by the spectral clustering function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/fdaaeebf599bf85787590f1efc74a462b02a6461821ea3b94577821e97f17c34.png](../Images/680fefa78db7bb2de50eb4473168da9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Spectral Clustering with Custom Adjacency Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs run spectral clustering with our adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: to do this we set the parameter ‚Äòaffinity‚Äô as precomputed and then train on
    the adjacency matrix instead of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/001f2f92cbb8990bace5fde37acc03c9b6881278353fdbf0929638f1d79b1fcd.png](../Images/bd2c005d0f9d7776fcefd18f10cec75b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some observations:'
  prefs: []
  type: TYPE_NORMAL
- en: unsurprisingly the spectral groups are aligned with the connected groups that
    we specified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs try something else, we add connections between the 3 group with 2 new
    vertices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/b392eaffd64400bd04bb8e50a651c5a08db8bdfbd2751bf354c688c836a5bb0c.png](../Images/45b914a7bc66e9c00906f9617b485508.png)'
  prefs: []
  type: TYPE_IMG
- en: See the added vertices are shown as red bold lines above. Note the shift in
    the cluster groups across that gap that we connected.
  prefs: []
  type: TYPE_NORMAL
- en: even though all the data samples are connects, spectral clustering provides
    rational groups as it learns the difference between direct connections (two data
    samples connected by a vertex) and indirect connections (two data samples connected
    via multiple vertices passing through one or more other data samples).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, there is no meaning to the actual cluster group numbers, so don‚Äôt
    be concerned with the switch in the cluster group numerical and color assignments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral Clustering with Affinity Matrix from Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a variety of methods to calculate an affinity matrix from the data
    sample locations in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'a common method is to use kernel functions, like a radial basis function (RBF)
    kernel, parameterized by \(\gamma\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ K(x_i,x_j) = exp \left(- \gamma || x_i - x_j ||^2 \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(|| x_i - x_j ||\) is the distance between data samples \(x_i\) and \(x_j\)
    in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: since we are using a distance calculation, we are careful to ensure that normalized
    or standardize features are used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**large \(\gamma\)** - similarity is assigned locally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**small \(\gamma\)** - similarity persists over long distances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/4ab466d66a474f22326f87525595960bb38698a43b7d9097ff234185b72cfdc6.png](../Images/40b2f8acc469e88a2d931a24e25f1b19.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs look at the affinity matrix that was calculated by scikit-learn‚Äôs SpectralClustering
    function to achieve this clustering result.
  prefs: []
  type: TYPE_NORMAL
- en: this is the pairwise connection between all of the sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/a26ea28f26564381944024963e1cfddfdb181e67cb3d201ae5ee0983ae380ed3.png](../Images/53361211caca5ff75eaadd9e66ea6014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here‚Äôs some general observations:'
  prefs: []
  type: TYPE_NORMAL
- en: there is a large number of pairwise connections between the sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs demonstrate the sensitivity of the solution to the model hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs explore the sensitivity in the parameters, by looking at the results over
    a \(3 \times 3\) combinatorial of hyperparameters. Run this the first time as
    is and then try making changes to these values in this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/32f66e1c345a7062dc8136bbfb43b739043b2fd9898edff4eb326237afe87ead.png](../Images/fcc840ff81dd74729548746086f57b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An observation from these spectral cluster group assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: as gamma increases, range of similarity is reduced and the model is more sensitive
    to gaps in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of cluster analysis with spectral clustering. Much
    more could be done and discussed, I have many more resources. Check out my [shared
    resource inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture
    links at the start of this chapter with resource links in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
