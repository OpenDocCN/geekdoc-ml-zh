- en: Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œ
- en: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_CNN.html)
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Michael J. Pyrczï¼Œæ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°ç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Python
    ä¸­åº”ç”¨åœ°ç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Python
    ä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
- en: 'Chapter of e-book â€œApplied Machine Learning in Python: a Hands-on Guide with
    Codeâ€.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­ä¹¦â€œPython ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹åŠ¨æŒ‡å—â€çš„ç« èŠ‚ã€‚
- en: 'Cite this e-Book as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤ç”µå­ä¹¦å¼•ç”¨å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Pyrcz, M.J., 2024, *Python ä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå¸¦ä»£ç çš„æ‰‹åŠ¨æŒ‡å—* [ç”µå­ä¹¦]. Zenodo. doi:10.5281/zenodo.15169138
    [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)
- en: 'The workflows in this book and more are available here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦ä¸­çš„å·¥ä½œæµç¨‹ä»¥åŠæ›´å¤šå†…å®¹å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°† MachineLearningDemos GitHub ä»“åº“å¼•ç”¨å¦‚ä¸‹ï¼š
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python æœºå™¨å­¦ä¹ æ¼”ç¤ºå·¥ä½œæµç¨‹ä»“åº“* (0.0.3) [è½¯ä»¶].
    Zenodo. DOI: 10.5281/zenodo.13835312\. GitHub ä»“åº“ï¼š[GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
- en: By Michael J. Pyrcz
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…ï¼šMichael J. Pyrcz
- en: Â© Copyright 2024.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Â© ç‰ˆæƒæ‰€æœ‰ 2024ã€‚
- en: This chapter is a tutorial for / demonstration of **Convolutional Neural Networks**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ˜¯å…³äº/æ¼”ç¤º**å·ç§¯ç¥ç»ç½‘ç»œ**çš„æ•™ç¨‹ã€‚
- en: '**YouTube Lecture**: check out my lectures on:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube è®²åº§**ï¼šæŸ¥çœ‹æˆ‘å…³äºä»¥ä¸‹å†…å®¹çš„è®²åº§ï¼š'
- en: '[Artificial Neural Networks](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äººå·¥ç¥ç»ç½‘ç»œ](https://youtu.be/A9PiCMY_6nM?si=NxWSU_5RgQ4w55EL)'
- en: '[Convolutional Neural Networks](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å·ç§¯ç¥ç»ç½‘ç»œ](https://youtu.be/za2my_XDoOs?si=LeHU6p2_fc9dX4Yt)'
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michaelâ€™s Story](https://michaelpyrcz.com/my-story).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®²åº§éƒ½æ˜¯æˆ‘ YouTube ä¸Šçš„ [æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…å«æœ‰è‰¯å¥½æ–‡æ¡£è®°å½•çš„ Python å·¥ä½œæµç¨‹å’Œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚æˆ‘çš„ç›®æ ‡æ˜¯åˆ†äº«æ˜“äºç†è§£ã€å¯æ“ä½œå’Œå¯é‡å¤çš„æ•™è‚²å†…å®¹ã€‚å¦‚æœä½ æƒ³çŸ¥é“æˆ‘çš„åŠ¨æœºï¼Œè¯·æŸ¥çœ‹ [Michael
    çš„æ•…äº‹](https://michaelpyrcz.com/my-story)ã€‚
- en: Motivation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: Convolutional neural networks are very powerful, nature inspired computing deep
    learning method based on an analogy of visual cortex extending the ability of
    our artificial neural networks to better work with images.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„ã€å—è‡ªç„¶å¯å‘çš„åŸºäºè§†è§‰çš®å±‚ç±»æ¯”çš„è®¡ç®—æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå®ƒæ‰©å±•äº†æˆ‘ä»¬äººå·¥ç¥ç»ç½‘ç»œçš„å›¾åƒå¤„ç†èƒ½åŠ›ã€‚
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶å¯å‘çš„è®¡ç®—æ­£åœ¨å¯»æ‰¾è‡ªç„¶ç•Œçš„çµæ„Ÿæ¥å¼€å‘æ–°çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œ
- en: '**artificial neural networks** are inspired by biological neural networks'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äººå·¥ç¥ç»ç½‘ç»œ**å—ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å¯å‘'
- en: '**nodes** in our model are artificial neurons, simple processors'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹**åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ˜¯äººå·¥ç¥ç»å…ƒï¼Œç®€å•çš„å¤„ç†å™¨'
- en: '**connections** between nodes are artificial synapses'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥**æ˜¯äººå·¥çªè§¦ã€‚'
- en: '**perceptive fields** regularization to improve generalization and efficiency'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ„ŸçŸ¥åœº**æ­£åˆ™åŒ–ä»¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚'
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our convolutional
    neural network.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ™ºèƒ½æ˜¯ä»è®¸å¤šè¿æ¥çš„ç®€å•å¤„ç†å™¨ä¸­äº§ç”Ÿçš„ã€‚åœ¨æœ¬ç« çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ä½¿ç”¨èŠ‚ç‚¹å’Œè¿æ¥æœ¯è¯­æ¥æè¿°æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: Concepts in Common with Artificial Neural Networks
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸äººå·¥ç¥ç»ç½‘ç»œå…±æœ‰çš„æ¦‚å¿µã€‚
- en: Here are some key aspects of artificial neural networks (ANNs),
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰çš„å…³é”®æ–¹é¢ï¼Œ
- en: '**Basic Design** - *â€œâ€¦a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.â€* Caudill (1989).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬è®¾è®¡** - *â€œâ€¦â€¦ä¸€ä¸ªç”±è®¸å¤šç®€å•ã€é«˜åº¦äº’è”çš„å¤„ç†å…ƒç´ ç»„æˆçš„è®¡ç®—ç³»ç»Ÿï¼Œå®ƒä»¬é€šè¿‡å¯¹å¤–éƒ¨è¾“å…¥çš„åŠ¨æ€çŠ¶æ€å“åº”æ¥å¤„ç†ä¿¡æ¯ã€‚â€* Caudill (1989)ã€‚'
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»ç„¶æ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹** - è™½ç„¶è¿™äº›æ¨¡å‹å¯èƒ½éå¸¸å¤æ‚ï¼Œç”šè‡³æœ‰æ•°ç™¾ä¸‡ä¸ªå¯è®­ç»ƒæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯ä¸€ä¸ªå°†é¢„æµ‹ç‰¹å¾æ˜ å°„åˆ°å“åº”ç‰¹å¾çš„å‡½æ•°ï¼Œ'
- en: \[ Y=f(X)+\epsilon \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y=f(X)+\epsilon \]
- en: '**Supervised learning** â€“ we provide training data with predictor features,
    \(X_1,\ldots,ğ‘‹_ğ‘š\) and response feature(s), \(ğ‘Œ_1,\ldots,ğ‘Œ_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å­¦ä¹ ** â€“ æˆ‘ä»¬æä¾›å¸¦æœ‰é¢„æµ‹ç‰¹å¾\(X_1,\ldots,ğ‘‹_ğ‘š\)å’Œå“åº”ç‰¹å¾ï¼ˆ\(ğ‘Œ_1,\ldots,ğ‘Œ_K\)ï¼‰çš„è®­ç»ƒæ•°æ®ï¼ŒæœŸæœ›æŸç§æ¨¡å‹é¢„æµ‹è¯¯å·®\(\epsilon\)ã€‚'
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** - é€šè¿‡åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°æ¥æ¨¡å‹éçº¿æ€§å…³ç³»ï¼Œå°†éçº¿æ€§å¼•å…¥åˆ°ç³»ç»Ÿä¸­ã€‚'
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - artificial
    neural networks have the ability to learn any possible function shape of \(f\)
    over an interval, for an arbitrary wide (single hidden layer) by Cybenko (1989)
    and arbitrary depth by Lu and others (2017)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼ˆé€šç”¨é€¼è¿‘å®šç†ï¼‰** - äººå·¥ç¥ç»ç½‘ç»œå…·æœ‰é€šè¿‡Cybenko (1989)æå‡ºçš„ä»»æ„å®½ï¼ˆå•éšè—å±‚ï¼‰å’ŒLuç­‰äººï¼ˆ2017ï¼‰æå‡ºçš„ä»»æ„æ·±åº¦å­¦ä¹ åŒºé—´å†…ä»»ä½•å¯èƒ½çš„å‡½æ•°å½¢çŠ¶\(f\)çš„èƒ½åŠ›ã€‚'
- en: For brevity, I will not repeat all the fundamental concepts from the artificial
    neural network chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å°†ä¸ä¼šé‡å¤äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚çš„æ‰€æœ‰åŸºæœ¬æ¦‚å¿µã€‚
- en: it may be a good idea to review that chapter before starting with this one
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è¿™ä¸€ç« ä¹‹å‰å›é¡¾é‚£ç« å¯èƒ½æ˜¯ä¸ªå¥½ä¸»æ„ã€‚
- en: Convolutional Neural Networks Concepts
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæ¦‚å¿µã€‚
- en: '**Regularization** - artificial neural networks are prone to overfitting; therefore,
    we need a form of regularization to prevent this'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–** - äººå·¥ç¥ç»ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ­£åˆ™åŒ–å½¢å¼æ¥é˜²æ­¢è¿™ç§æƒ…å†µã€‚'
- en: for example, ridge and LASSO integrate regularization into the loss function
    through the shrinkage term to reduce overfit
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå²­å›å½’å’ŒLASSOé€šè¿‡æ”¶ç¼©é¡¹å°†æ­£åˆ™åŒ–é›†æˆåˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥å‡å°‘è¿‡æ‹Ÿåˆã€‚
- en: With convolutional neural networks we take a different approach to regularization,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬é‡‡å–äº†ä¸åŒçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œ
- en: with image data we have an implicit hierarchy / proximity and relative position
    of pixels, flattening our 2D images into 1D vectors to pass through multiple fully
    connected artificial neural network layers would destroy this information
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å›¾åƒæ•°æ®æ—¶ï¼Œæˆ‘ä»¬æœ‰éšå«çš„å±‚æ¬¡/é‚»è¿‘æ€§å’Œåƒç´ çš„ç›¸å¯¹ä½ç½®ï¼Œå°†æˆ‘ä»¬çš„äºŒç»´å›¾åƒå±•å¹³ä¸º1Då‘é‡ä»¥é€šè¿‡å¤šä¸ªå…¨è¿æ¥äººå·¥ç¥ç»ç½‘ç»œå±‚ä¼šç ´åè¿™äº›ä¿¡æ¯ã€‚
- en: so we remove, regularize to 0.0, all connections outside of perceptive fields
    to preserve proximity and relative position of pixels information while avoiding
    overfit
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ç§»é™¤ï¼Œæ­£åˆ™åŒ–åˆ°0.0ï¼Œæ‰€æœ‰æ„ŸçŸ¥åœºå¤–çš„è¿æ¥ï¼Œä»¥ä¿æŒåƒç´ ä¿¡æ¯ä¹‹é—´çš„é‚»è¿‘æ€§å’Œç›¸å¯¹ä½ç½®ï¼ŒåŒæ—¶é¿å…è¿‡æ‹Ÿåˆã€‚
- en: these perceptive fields are regularization through extraction of smaller pixel
    subsets and simpler patterns from the images
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ„ŸçŸ¥åœºé€šè¿‡ä»å›¾åƒä¸­æå–è¾ƒå°çš„åƒç´ å­é›†å’Œæ›´ç®€å•çš„æ¨¡å¼æ¥è¿›è¡Œæ­£åˆ™åŒ–ã€‚
- en: Note, we also include dropout, random removal of connections, as another form
    of regularization
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬dropoutï¼ˆéšæœºç§»é™¤è¿æ¥ï¼‰ä½œä¸ºå¦ä¸€ç§æ­£åˆ™åŒ–çš„å½¢å¼ã€‚
- en: Image Data with Artificial Neural Networks
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œçš„å›¾åƒæ•°æ®ã€‚
- en: We could use image data with our artificial neural networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›¾åƒæ•°æ®ä¸æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: '![](../Images/a7d555024d6a9f3a61c7fd816bd8bf6a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7d555024d6a9f3a61c7fd816bd8bf6a.png)'
- en: Illustration of artificial neural network to classify an image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œåˆ†ç±»å›¾åƒçš„ç¤ºæ„å›¾ã€‚
- en: What is the issue with this approach?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- en: '**Massive number of model parameters** - the model will likely be difficult
    to train and overfit.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å‚æ•°æ•°é‡å·¨å¤§** - æ¨¡å‹å¯èƒ½éš¾ä»¥è®­ç»ƒä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚'
- en: '**Very sensitive to location** - we would like to learn from our images with
    location invariance, i.e., for the example above the location of the channels
    should not matter.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¯¹ä½ç½®éå¸¸æ•æ„Ÿ** - æˆ‘ä»¬å¸Œæœ›ä»å…·æœ‰ä½ç½®ä¸å˜æ€§çš„å›¾åƒä¸­å­¦ä¹ ï¼Œå³ï¼Œå¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œé€šé“çš„ä½ç½®ä¸åº”é‡è¦ã€‚'
- en: '**Flattening the image to a vector** - this is required by a artificial neural
    network, but if we first flatten our images to a vector we lose important information
    about the inter-pixel patterns, ordering, adjacency, etc.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å°†å›¾åƒå±•å¹³ä¸ºå‘é‡** - è¿™æ˜¯äººå·¥ç¥ç»ç½‘ç»œæ‰€å¿…éœ€çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬é¦–å…ˆå°†å›¾åƒå±•å¹³ä¸ºå‘é‡ï¼Œæˆ‘ä»¬å°±ä¼šä¸¢å¤±å…³äºåƒç´ æ¨¡å¼ã€é¡ºåºã€ç›¸é‚»æ€§ç­‰é‡è¦ä¿¡æ¯ã€‚'
- en: In summary, artificial neural networks are very inefficient for images and overfit
    and donâ€™t generalize well! Instead, letâ€™s be inspired by our visual cortex, with
    our vision we do not perceive all â€˜pixelsâ€™, instead we segment the field of view
    into receptive fields.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œäººå·¥ç¥ç»ç½‘ç»œåœ¨å›¾åƒå¤„ç†ä¸Šéå¸¸ä½æ•ˆï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆä¸”æ³›åŒ–èƒ½åŠ›å·®ï¼ç›¸åï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è§†è§‰çš®å±‚ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬çš„è§†è§‰ä¸ä¼šæ„ŸçŸ¥åˆ°æ‰€æœ‰â€˜åƒç´ â€™ï¼Œè€Œæ˜¯å°†è§†é‡åˆ†å‰²æˆæ„Ÿå—é‡ã€‚
- en: we extraction of features of interest from overlapping receptive fields, over
    a hierarchy (not shown) and then recompose the whole image, our perception.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»é‡å çš„æ„Ÿå—é‡ä¸­æå–æ„Ÿå…´è¶£çš„ç‰¹å¾ï¼Œåœ¨å±‚æ¬¡ç»“æ„ï¼ˆæœªæ˜¾ç¤ºï¼‰ä¸­é‡ç»„æ•´ä¸ªå›¾åƒï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥ã€‚
- en: we donâ€™t perceive all the â€˜pixelsâ€™ that would be exhausting for our brains,
    instead our visual cortex interprets and summarizes patterns.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šæ„ŸçŸ¥åˆ°æ‰€æœ‰â€˜åƒç´ â€™ï¼Œè¿™å¯¹æˆ‘ä»¬çš„å¤§è„‘æ¥è¯´ä¼šéå¸¸ç´¯ï¼Œç›¸åï¼Œæˆ‘ä»¬çš„è§†è§‰çš®å±‚ä¼šè§£é‡Šå’Œæ€»ç»“æ¨¡å¼ã€‚
- en: '![](../Images/57787e99367cbd5272ad8414cd3bfe82.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/57787e99367cbd5272ad8414cd3bfe82.png)'
- en: Illustration of overlapping receptive fields to break up a field of view, summarize
    each part and recombine into our perception.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é‡å çš„æ„Ÿå—é‡ç”¨äºåˆ†å‰²è§†é‡ï¼Œæ€»ç»“æ¯ä¸€éƒ¨åˆ†å¹¶é‡æ–°ç»„åˆæˆæˆ‘ä»¬çš„æ„ŸçŸ¥ã€‚
- en: Now letâ€™s compare artificial neural networks and convolutional neural networks
    with this concept of receptive fields.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç”¨æ„Ÿå—é‡çš„æ¦‚å¿µæ¥æ¯”è¾ƒäººå·¥ç¥ç»ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: Fully Connected, Feed Forward Artificial Neural Network
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨è¿æ¥ã€å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œ
- en: nodes in the next layer are connected to all nodes of the previous layer
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€å±‚çš„èŠ‚ç‚¹è¿æ¥åˆ°å‰ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹
- en: spatial information is lost, the image data is immediately flattened to a 1D
    vector and adjacency / ordering information is lost
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºé—´ä¿¡æ¯ä¸¢å¤±ï¼Œå›¾åƒæ•°æ®ç«‹å³å±•å¹³ä¸º1Då‘é‡ï¼Œç›¸é‚»æ€§å’Œé¡ºåºä¿¡æ¯ä¸¢å¤±
- en: Regularized with Receptive Fields Convolutional Neural Networks
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ„Ÿå—é‡å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–
- en: nodes in the next layer are mapped to specific regions of the previous layer,
    an image or feature map
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€å±‚çš„èŠ‚ç‚¹æ˜ å°„åˆ°å‰ä¸€å±‚ç‰¹å®šçš„åŒºåŸŸï¼Œå³å›¾åƒæˆ–ç‰¹å¾å›¾
- en: spatial information is preserved, data retains the original image 2D or 3D dimensionality
    in each layer, called feature maps.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºé—´ä¿¡æ¯å¾—åˆ°ä¿ç•™ï¼Œæ•°æ®åœ¨æ¯ä¸ªå±‚ä¸­ä¿ç•™äº†åŸå§‹å›¾åƒçš„2Dæˆ–3Dç»´åº¦æ€§ï¼Œç§°ä¸ºç‰¹å¾å›¾ã€‚
- en: '![](../Images/b56850a29d24dbccae1cabcfbdc9e7e7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/b56850a29d24dbccae1cabcfbdc9e7e7.png)'
- en: Illustration of overlapping receptive fields to break up a field of view, summarize
    each part and recombine into our perception.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é‡å çš„æ„Ÿå—é‡ç”¨äºåˆ†å‰²è§†é‡ï¼Œæ€»ç»“æ¯ä¸€éƒ¨åˆ†å¹¶é‡æ–°ç»„åˆæˆæˆ‘ä»¬çš„æ„ŸçŸ¥ã€‚
- en: Regularization for CNNs
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNçš„æ­£åˆ™åŒ–
- en: Letâ€™s do a quick recall on the concept of regularization for predictive machine
    learning models,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­æ­£åˆ™åŒ–çš„æ¦‚å¿µï¼Œ
- en: '**Regularization** - a constraint to reduce the sensitivity of the model to
    the data, i.e., to reduce model variance'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–** - ä¸€ç§çº¦æŸï¼Œä»¥å‡å°‘æ¨¡å‹å¯¹æ•°æ®çš„æ•æ„Ÿæ€§ï¼Œå³ï¼Œå‡å°‘æ¨¡å‹æ–¹å·®'
- en: '**Regularization with Receptive Fields** - the use of receptive fields is a
    form of regularization, resulting in,'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨æ„Ÿå—é‡è¿›è¡Œæ­£åˆ™åŒ–** - æ„Ÿå—é‡çš„ä½¿ç”¨æ˜¯ä¸€ç§æ­£åˆ™åŒ–å½¢å¼ï¼Œå¯¼è‡´ï¼Œ'
- en: massive reduction in connections, weights / model parameters
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿æ¥ã€æƒé‡/æ¨¡å‹å‚æ•°çš„å¤§é‡å‡å°‘
- en: effectively shrinking these potential weights to zero
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆåœ°å°†è¿™äº›æ½œåœ¨æƒé‡ç¼©å°åˆ°é›¶
- en: While integrating / focusing on pixel patterns!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶æ•´åˆ/å…³æ³¨åƒç´ æ¨¡å¼ï¼
- en: '**Regularization with Dropout** - during training epochs, randomly ignore or
    â€œdrop outâ€ a proportion of nodes. Each training epoch sees a different version
    / subset of the network'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨Dropoutè¿›è¡Œæ­£åˆ™åŒ–** - åœ¨è®­ç»ƒå‘¨æœŸä¸­ï¼Œéšæœºå¿½ç•¥æˆ–â€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†èŠ‚ç‚¹ã€‚æ¯ä¸ªè®­ç»ƒå‘¨æœŸéƒ½ä¼šçœ‹åˆ°ç½‘ç»œçš„ä¸åŒç‰ˆæœ¬/å­é›†'
- en: an additional form of regularization for CNN to prevent specific nodes from
    dominating the model
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNçš„ä¸€ç§é¢å¤–æ­£åˆ™åŒ–å½¢å¼ï¼Œä»¥é˜²æ­¢ç‰¹å®šèŠ‚ç‚¹ä¸»å¯¼æ¨¡å‹
- en: simulates training multiple models and averaging like ensemble learning. Note,
    it is generally not feasible to train multiple networks in parallel to apply the
    ensemble to calculate the prediction (like random forest) and after training all
    nodes are used.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿè®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶å¹³å‡ï¼Œç±»ä¼¼äºé›†æˆå­¦ä¹ ã€‚æ³¨æ„ï¼Œé€šå¸¸ä¸å¯è¡Œå¹¶è¡Œè®­ç»ƒå¤šä¸ªç½‘ç»œä»¥åº”ç”¨é›†æˆæ¥è®¡ç®—é¢„æµ‹ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒåæ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«ä½¿ç”¨ã€‚
- en: '![](../Images/2078f41aac92afeb57f8fdc28528b873.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2078f41aac92afeb57f8fdc28528b873.png)'
- en: Illustration of dropout to remove random connects from receptive field to nodes
    in the feature map.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ„Ÿå—é‡åˆ°ç‰¹å¾å›¾ä¸­çš„èŠ‚ç‚¹çš„éšæœºè¿æ¥çš„dropoutç¤ºæ„å›¾ã€‚
- en: '**Batch Normalization** - standardize the nodesâ€™ inputs / weights over a layer
    to center and rescale (mean of 0 and variance of 1) to optimize activation function
    sensitivity and model parameter training.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡å½’ä¸€åŒ–** - åœ¨å±‚ä¸Šæ ‡å‡†åŒ–èŠ‚ç‚¹çš„è¾“å…¥/æƒé‡ï¼Œä»¥ä¸­å¿ƒåŒ–å’Œç¼©æ”¾ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ï¼Œä»¥ä¼˜åŒ–æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§å’Œæ¨¡å‹å‚æ•°è®­ç»ƒã€‚'
- en: \[ \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]
- en: Then we add 2 model parameters, layer standard deviation, \(\gamma\), and mean,
    \(\beta\), to add control to improve the optimality of the train individual weights
    in the next layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ·»åŠ 2ä¸ªæ¨¡å‹å‚æ•°ï¼Œå±‚æ ‡å‡†å·®ï¼Œ\(\gamma\)ï¼Œå’Œå‡å€¼ï¼Œ\(\beta\)ï¼Œä»¥å¢åŠ æ§åˆ¶ï¼Œæé«˜ä¸‹ä¸€å±‚è®­ç»ƒå•ä¸ªæƒé‡çš„æœ€ä¼˜æ€§ã€‚
- en: \[ y_i \leftarrow \gamma \hat{x}_i + \beta \]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_i \leftarrow \gamma \hat{x}_i + \beta \]
- en: Building Blocks for CNNs
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNçš„æ„å»ºæ¨¡å—
- en: We have various operators to move from layer to layer (feature maps to feature
    maps) in our convolutional neural networks. The common operators include,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰å„ç§ç®—å­å¯ä»¥åœ¨æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ä»ä¸€å±‚ç§»åŠ¨åˆ°å¦ä¸€å±‚ï¼ˆç‰¹å¾å›¾åˆ°ç‰¹å¾å›¾ï¼‰ã€‚å¸¸è§çš„ç®—å­åŒ…æ‹¬ï¼Œ
- en: '**Convolution** â€“ a weighting window, kernel / filter designed to extract spatial
    information'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å·ç§¯** â€“ ä¸€ä¸ªåŠ æƒçª—å£ï¼Œæ ¸/æ»¤æ³¢å™¨ï¼Œç”¨äºæå–ç©ºé—´ä¿¡æ¯'
- en: '**Pooling** â€“ reduction in dimensionality, increase local translation invariance'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ± åŒ–** â€“ ç»´åº¦é™ä½ï¼Œå¢åŠ å±€éƒ¨å¹³ç§»ä¸å˜æ€§'
- en: '**Depth-wise Pooling, Down Sampling** â€“ 1x1 filter that combine channels /
    feature maps to learn over multiple kernels'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ·±åº¦æ± åŒ–ï¼Œä¸‹é‡‡æ ·** â€“ 1x1æ»¤æ³¢å™¨ï¼Œå°†é€šé“/ç‰¹å¾å›¾ç»„åˆèµ·æ¥ï¼Œä»¥åœ¨å¤šä¸ªæ ¸ä¸Šå­¦ä¹ '
- en: '**Activation** â€“ use of an activation function to apply a nonlinear transformation
    to impart nonlinearity to the system and to prevent collapse of the system to
    a simple linear model'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¿€æ´»** â€“ ä½¿ç”¨æ¿€æ´»å‡½æ•°å¯¹ç³»ç»Ÿåº”ç”¨éçº¿æ€§å˜æ¢ï¼Œä»¥èµ‹äºˆç³»ç»Ÿéçº¿æ€§ï¼Œå¹¶é˜²æ­¢ç³»ç»Ÿç®€åŒ–ä¸ºç®€å•çš„çº¿æ€§æ¨¡å‹'
- en: '**Full-connected, Feed Forward** â€“ see previous lecture on artificial neural
    networks'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…¨è¿æ¥ï¼Œå‰é¦ˆ** â€“ å‚è§ä¹‹å‰å…³äºäººå·¥ç¥ç»ç½‘ç»œçš„è®²åº§'
- en: Now we will describe, interpret and demonstrate these operators.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†æè¿°ã€è§£é‡Šå’Œæ¼”ç¤ºè¿™äº›ç®—å­ã€‚
- en: Convolution
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯
- en: Convolution is the integral product of two functions, after one is reversed
    and shifted by \(\Delta\).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ˜¯ä¸¤ä¸ªå‡½æ•°çš„ç§¯åˆ†ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸€ä¸ªå‡½æ•°è¢«åè½¬å¹¶æ²¿ \(\Delta\) å¹³ç§»ã€‚
- en: one interpretation is smoothing a function with weighting function, \(ğ‘“(\Delta)\),
    is applied to calculate the weighted average of function, \(ğ‘”(x)\),
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§è§£é‡Šæ˜¯ä½¿ç”¨åŠ æƒå‡½æ•°ï¼Œ\(ğ‘“(\Delta)\)ï¼Œå¯¹å‡½æ•°è¿›è¡Œå¹³æ»‘ï¼Œä»¥è®¡ç®—å‡½æ•°ï¼Œ\(ğ‘”(x)\)ï¼Œçš„åŠ æƒå¹³å‡å€¼ï¼Œ
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
- en: this easily extends into any dimensionality, for example 2D for images,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå®¹æ˜“æ‰©å±•åˆ°ä»»ä½•ç»´åº¦ï¼Œä¾‹å¦‚2Dç”¨äºå›¾åƒï¼Œ
- en: \[ (f * g)(x, y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x,
    \Delta_y) g(x - \Delta_x, y - \Delta_y) \, d\Delta_x \, d\Delta_y \]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x, y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x,
    \Delta_y) g(x - \Delta_x, y - \Delta_y) \, d\Delta_x \, d\Delta_y \]
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç§¯åˆ†ä¹‹å‰é€‰æ‹©å“ªä¸ªå‡½æ•°å…ˆå¹³ç§»ï¼Œä¸ä¼šæ”¹å˜ç»“æœï¼Œå·ç§¯ç®—å­å…·æœ‰äº¤æ¢æ€§ï¼Œ
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä»»ä¸€å‡½æ•°è¢«åå°„ï¼Œå·ç§¯å°±ç­‰åŒäºäº¤å‰ç›¸å…³ï¼Œå®ƒæ˜¯ä¸¤ä¸ªä¿¡å·ä½œä¸ºä½ç§»å‡½æ•°ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚
- en: To demonstrate convolution with an exhaustive \(g(x)\) and sparsely sampled
    \(g(x)\) I built out an [interactive Python convolution dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb),
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¼”ç¤ºä½¿ç”¨è¯¦å°½çš„ \(g(x)\) å’Œç¨€ç–é‡‡æ ·çš„ \(g(x)\) çš„å·ç§¯ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ª[äº¤äº’å¼Pythonå·ç§¯ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb)ï¼Œ
- en: '![](../Images/f55c37e0f7da98a233affd5fbd5ba38c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/f55c37e0f7da98a233affd5fbd5ba38c.png)'
- en: Interactive Python dashboard to demonstrate convolution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤äº’å¼Pythonä»ªè¡¨æ¿ç”¨äºæ¼”ç¤ºå·ç§¯ã€‚
- en: For convolution operations, a trainable weighting function, \(g(\Tau)\), is
    learned during model training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­¦ä¹ ä¸€ä¸ªå¯è®­ç»ƒçš„åŠ æƒå‡½æ•° \(g(\Tau)\) ç”¨äºå·ç§¯æ“ä½œã€‚
- en: '**Filter/Kernel** â€“ the weights assigned over the convolution window to calculate
    the next feature map. By training the weights the filter(s) may extract specific
    spatial features.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ»¤æ³¢å™¨/æ ¸** â€“ åœ¨å·ç§¯çª—å£ä¸­åˆ†é…çš„æƒé‡ï¼Œç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªç‰¹å¾å›¾ã€‚é€šè¿‡è®­ç»ƒæƒé‡ï¼Œæ»¤æ³¢å™¨å¯ä»¥æå–ç‰¹å®šçš„ç©ºé—´ç‰¹å¾ã€‚'
- en: '![](../Images/1955083a83351548602c54c6f2f6d86b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/1955083a83351548602c54c6f2f6d86b.png)'
- en: Two examples of convolution calculations. .
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯è®¡ç®—çš„ä¸¤ç§ç¤ºä¾‹ã€‚
- en: Letâ€™s look at a specific kernel form, the blur filter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç‰¹å®šçš„æ ¸å½¢å¼ï¼Œå³æ¨¡ç³Šæ»¤æ³¢å™¨ã€‚
- en: the next feature map receives the local averages over the previous feature map
    or image.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªç‰¹å¾å›¾æ¥æ”¶å…ˆå‰ç‰¹å¾å›¾æˆ–å›¾åƒä¸Šçš„å±€éƒ¨å¹³å‡å€¼ã€‚
- en: '![](../Images/f41a6a56aa0cf6c53667cad2b0ae52c8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/f41a6a56aa0cf6c53667cad2b0ae52c8.png)'
- en: Example filter, the blur / local average filter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ»¤æ³¢å™¨ï¼Œæ¨¡ç³Š/å±€éƒ¨å¹³å‡æ»¤æ³¢å™¨ã€‚
- en: Some observations about the filters,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ»¤æ³¢å™¨çš„ä¸€äº›è§‚å¯Ÿï¼Œ
- en: size of the filter is related to the scale of the features that we are extracting
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ»¤æ³¢å™¨çš„å¤§å°ä¸æˆ‘ä»¬æå–çš„ç‰¹å¾çš„å°ºåº¦ç›¸å…³
- en: larger kernels increase the number of connections, model weights and ultimately
    the computational complexity
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾ƒå¤§çš„æ ¸å¢åŠ äº†è¿æ¥æ•°ã€æ¨¡å‹æƒé‡ï¼Œæœ€ç»ˆå¢åŠ äº†è®¡ç®—å¤æ‚åº¦
- en: odd numbers for kernel size to avoid distortion, asymmetric kernels are possible
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ ¸å¤§å°ä½¿ç”¨å¥‡æ•°ä»¥é¿å…æ‰­æ›²ï¼Œå¯èƒ½ä½¿ç”¨éå¯¹ç§°æ ¸
- en: sum to one prevent bias (shifting in the mean from one image or feature map
    to another feature map
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚å’Œä¸º1å¯ä»¥é˜²æ­¢åå·®ï¼ˆä»ä¸€å¹…å›¾åƒæˆ–ç‰¹å¾å›¾åˆ°å¦ä¸€å¹…ç‰¹å¾å›¾çš„å‡å€¼åç§»ï¼‰
- en: '**Padding** - our next feature map has a reduced size by 1 on all the edges
    to avoid the overlapping outside the feature map, i.e., no padding, while padding
    extrapolates outside the feature map or image, preventing reduction in size of
    the next feature map.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¡«å……** - æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªç‰¹å¾å›¾åœ¨æ‰€æœ‰è¾¹ç¼˜ä¸Šå‡å°‘äº†1ï¼Œä»¥é¿å…ç‰¹å¾å›¾å¤–éƒ¨çš„é‡å ï¼Œå³æ— å¡«å……ï¼Œè€Œå¡«å……å°†æ‰©å±•åˆ°ç‰¹å¾å›¾æˆ–å›¾åƒå¤–éƒ¨ï¼Œé˜²æ­¢ä¸‹ä¸€ä¸ªç‰¹å¾å›¾å¤§å°çš„å‡å°‘ã€‚'
- en: there are varirous methods for padding, including assuming zero, constant value,
    and nearest value in feature map.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­˜åœ¨å¤šç§å¡«å……æ–¹æ³•ï¼ŒåŒ…æ‹¬å‡è®¾é›¶ã€å¸¸æ•°å€¼å’Œç‰¹å¾å›¾ä¸­çš„æœ€è¿‘å€¼ã€‚
- en: '![](../Images/58368b8a8cebacdde3ae4116d6462f35.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/58368b8a8cebacdde3ae4116d6462f35.png)'
- en: Convolution without padding, resulting in feature map size reduction (above)
    and convolution with padding (assuming 0 outside the feature map) for no feature
    map size reduction (below).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ— å¡«å……çš„å·ç§¯å¯¼è‡´ç‰¹å¾å›¾å¤§å°å‡å°‘ï¼ˆä¸Šæ–¹ï¼‰å’Œæœ‰å¡«å……çš„å·ç§¯ï¼ˆå‡è®¾ç‰¹å¾å›¾å¤–éƒ¨ä¸º0ï¼‰ä»¥é¿å…ç‰¹å¾å›¾å¤–éƒ¨çš„é‡å ï¼Œå³æ— å¡«å……ï¼Œè€Œå¡«å……å°†æ‰©å±•åˆ°ç‰¹å¾å›¾æˆ–å›¾åƒå¤–éƒ¨ï¼Œé˜²æ­¢ä¸‹ä¸€ä¸ªç‰¹å¾å›¾å¤§å°çš„å‡å°‘ã€‚
- en: '**Stride** - the steps of the convolution filter / kernel through the previous
    feature map.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥é•¿** - å·ç§¯æ»¤æ³¢å™¨/æ ¸åœ¨å…ˆå‰ç‰¹å¾å›¾ä¸Šçš„æ­¥è¿›ã€‚'
- en: for a stride of 1 there is no implicit reduction in feature map size.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ­¥é•¿ä¸º1ï¼Œç‰¹å¾å›¾å¤§å°æ²¡æœ‰éšå¼å‡å°‘ã€‚
- en: for a stride of > 2 there is a reduction in feature map size.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ­¥é•¿å¤§äº2ï¼Œç‰¹å¾å›¾å¤§å°ä¼šå‡å°‘ã€‚
- en: '![](../Images/58300f18f8744327620c90fcc1cc392f.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/58300f18f8744327620c90fcc1cc392f.png)'
- en: Convolution with stride of 1 (above) and convolution with a stride of 2 (below)
    with a reduction in feature map size.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥é•¿ä¸º1çš„å·ç§¯ï¼ˆä¸Šæ–¹ï¼‰å’Œæ­¥é•¿ä¸º2çš„å·ç§¯ï¼ˆä¸‹æ–¹ï¼‰å¯¼è‡´ç‰¹å¾å›¾å¤§å°å‡å°‘ã€‚
- en: '**Size of Next Feature Map** - the next feature map size is determined by the
    hyperparameters, previous feature map size, \(n_{in}\), convolution kernel size,
    \(k\), convolution padding size, \(p\) and convolution stride size, \(s\), by
    this equation,'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„å¤§å°** - ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„å¤§å°ç”±è¶…å‚æ•°ã€å…ˆå‰ç‰¹å¾å›¾å¤§å° \(n_{in}\)ã€å·ç§¯æ ¸å¤§å° \(k\)ã€å·ç§¯å¡«å……å¤§å° \(p\) å’Œå·ç§¯æ­¥é•¿å¤§å°
    \(s\) å†³å®šï¼Œé€šè¿‡æ­¤æ–¹ç¨‹ï¼Œ'
- en: \[ n_{\text{out}} = \left\lfloor \frac{n_{\text{in}} + 2p - k}{s} \right\rfloor
    + 1 \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{n_{\text{in}} + 2p - k}{s} \right\rfloor
    + 1 \]
- en: For example, if,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ
- en: \(n_{in} = 4\)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(n_{in} = 4\)
- en: \(k = 2\)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(k = 2\)
- en: \(p = 0\)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(p = 0\)
- en: \(s = 1\)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(s = 1\)
- en: Then we can substitute into the equation,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å°†æ­¤ä»£å…¥æ–¹ç¨‹ä¸­ï¼Œ
- en: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
- en: Now compare this to a visualization of this example,
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†æ­¤ä¸è¯¥ç¤ºä¾‹çš„è§†è§‰è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œ
- en: '![](../Images/ad9c65a10c9c4afb28b77bd7cbfc6017.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ad9c65a10c9c4afb28b77bd7cbfc6017.png)'
- en: Example kernel, input and output feature maps for the example above.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç¤ºä¾‹çš„ç¤ºä¾‹æ ¸ã€è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾ã€‚
- en: 'We calculate the next feature map size as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—ä¸‹ä¸€ä¸ªç‰¹å¾å›¾å¤§å°å¦‚ä¸‹ï¼š
- en: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
- en: '**Filter / Kernel Design** â€“ by training the weights the filter may extract
    specific features. Consider these example filter types with simple 1D examples
    of input and output feature â€˜mapsâ€™.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ»¤æ³¢å™¨/æ ¸è®¾è®¡** â€“ é€šè¿‡è®­ç»ƒæƒé‡ï¼Œæ»¤æ³¢å™¨å¯ä»¥æå–ç‰¹å®šçš„ç‰¹å¾ã€‚è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹æ»¤æ³¢å™¨ç±»å‹ï¼Œä»¥åŠè¾“å…¥å’Œè¾“å‡ºç‰¹å¾â€˜å›¾â€™çš„ç®€å•1Dç¤ºä¾‹ã€‚'
- en: '![](../Images/363089e3910cc726998fcf261fba68ef.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/363089e3910cc726998fcf261fba68ef.png)'
- en: Filters (above, and 1D illustrations of convolution, original function (black)
    and convolution (red) (below).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ»¤æ³¢å™¨ï¼ˆä¸Šé¢ï¼Œä»¥åŠå·ç§¯çš„1Dç¤ºæ„å›¾ï¼ŒåŸå§‹å‡½æ•°ï¼ˆé»‘è‰²ï¼‰å’Œå·ç§¯ï¼ˆçº¢è‰²ï¼‰ä¸‹é¢ï¼‰ã€‚
- en: '**Multiple Filters / Kernels** - typically multiple filters, \(n_k\), are trained
    on each convolutional layer to extract various structures from the image.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šä¸ªæ»¤æ³¢å™¨/æ ¸** - é€šå¸¸åœ¨æ¯ä¸ªå·ç§¯å±‚ä¸Šè®­ç»ƒå¤šä¸ªæ»¤æ³¢å™¨ï¼Œ\(n_k\)ï¼Œä»¥ä»å›¾åƒä¸­æå–ä¸åŒçš„ç»“æ„ã€‚'
- en: this increases feature map depth or channels, \(n_k\)
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å¢åŠ äº†ç‰¹å¾å›¾çš„æ·±åº¦æˆ–é€šé“æ•°ï¼Œ\(n_k\)
- en: '![](../Images/401b5ed57a2efd08be6f224d335f76da.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/401b5ed57a2efd08be6f224d335f76da.png)'
- en: Feature map has a depth, also known as number of channels, $n_k$ due to application
    of more than 1 filter to the previous image or feature map. .
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåº”ç”¨äº†å¤šä¸ªæ»¤æ³¢å™¨åˆ°å‰ä¸€ä¸ªå›¾åƒæˆ–ç‰¹å¾å›¾ï¼Œç‰¹å¾å›¾å…·æœ‰æ·±åº¦ï¼Œä¹Ÿç§°ä¸ºé€šé“æ•°ï¼Œ\(n_k\)ã€‚
- en: note, the original image may have multiple channels, depth > 1, for example,
    a RGB image with 3 channels, one for each red, blue and green.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼ŒåŸå§‹å›¾åƒå¯èƒ½å…·æœ‰å¤šä¸ªé€šé“ï¼Œæ·±åº¦>1ï¼Œä¾‹å¦‚ï¼Œå…·æœ‰3ä¸ªé€šé“çš„RGBå›¾åƒï¼Œæ¯ä¸ªé€šé“åˆ†åˆ«å¯¹åº”çº¢è‰²ã€è“è‰²å’Œç»¿è‰²ã€‚
- en: Finally here are some examples of convolution filters applied to an image of
    a brick wall on The University of Texas at Austin, codes are available at [SubsurfaceDataAnalytics_Convolution_Operators.ipynb.](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Convolution_Operators.ipynb).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¿™é‡Œæœ‰ä¸€äº›å·ç§¯æ»¤æ³¢å™¨åº”ç”¨äºå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ç –å¢™å›¾åƒçš„ç¤ºä¾‹ï¼Œä»£ç å¯åœ¨[SubsurfaceDataAnalytics_Convolution_Operators.ipynb.](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Convolution_Operators.ipynb)æ‰¾åˆ°ã€‚
- en: '![](../Images/48b85ce0016245754f632d2d7034335d.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48b85ce0016245754f632d2d7034335d.png)'
- en: Convolution examples from a brick wall on the campus of The University of Texas
    at Austin. .
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ä¸­ç –å¢™çš„å·ç§¯ç¤ºä¾‹ã€‚
- en: Activation Functions
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: See the artificial neural network chapter for more details, but for a reminder
    considerations for selecting activation functions,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜…äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œä½†ä¸ºäº†æé†’é€‰æ‹©æ¿€æ´»å‡½æ•°çš„è€ƒè™‘å› ç´ ï¼Œ
- en: '**Nonlinear** â€“ required to impose nonlinearity into the predictor. Proved
    to be a universal function approximator if at least 1 hidden layer (Cybenko, 1989).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** â€“ å°†éçº¿æ€§å¼ºåŠ åˆ°é¢„æµ‹å™¨ä¸­ã€‚å¦‚æœè‡³å°‘æœ‰1ä¸ªéšè—å±‚ï¼ˆCybenkoï¼Œ1989å¹´ï¼‰ï¼Œå·²è¢«è¯æ˜æ˜¯é€šç”¨çš„å‡½æ•°é€¼è¿‘å™¨ã€‚'
- en: '**Range** â€“ finite for more stability gradient-based learning, infinite for
    more efficient training (but requires a slower learning rate)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŒƒå›´** â€“ å¯¹äºæ›´ç¨³å®šçš„åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ˜¯æœ‰é™çš„ï¼Œå¯¹äºæ›´æœ‰æ•ˆçš„è®­ç»ƒæ˜¯æ— é™çš„ï¼ˆä½†éœ€è¦è¾ƒæ…¢çš„å­¦ä¹ ç‡ï¼‰'
- en: '**Continuously Differentiable** â€“ required for stable gradient-based optimization'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿ç»­å¯å¾®** â€“ å¯¹äºç¨³å®šçš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ˜¯å¿…éœ€çš„'
- en: '**Smooth functions with Monotonic Derivative** â€“ may generalize better'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…·æœ‰å•è°ƒå¯¼æ•°çš„å¹³æ»‘å‡½æ•°** â€“ å¯èƒ½å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›'
- en: '**Monotonic** â€“ guaranteed convexity of error surface of a single layer model
    (global minimum for loss function)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å•è°ƒ** â€“ ä¿è¯å•å±‚æ¨¡å‹è¯¯å·®è¡¨é¢çš„å‡¸æ€§ï¼ˆæŸå¤±å‡½æ•°çš„å…¨å±€æœ€å°å€¼ï¼‰'
- en: '**Approximates Identity at the Origin** â€“ well learn efficiently with the weights
    initialized with small random values'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœ¨åŸç‚¹è¿‘ä¼¼æ’ç­‰** â€“ ä½¿ç”¨å°éšæœºå€¼åˆå§‹åŒ–æƒé‡æ—¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ '
- en: '![](../Images/cb1d19a7978ea9f1f58b12b229835dcc.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb1d19a7978ea9f1f58b12b229835dcc.png)'
- en: Convolution examples from a brick wall on the campus of The University of Texas
    at Austin. .
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ä¸­ç –å¢™çš„å·ç§¯ç¤ºä¾‹ã€‚
- en: Pooling
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ± åŒ–
- en: Summarization over a filter / kernel with a single value. The impact of pooling
    includes,
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å•ä¸ªå€¼è¿›è¡Œæ»¤æ³¢å™¨/æ ¸çš„æ±‡æ€»ã€‚æ± åŒ–çš„å½±å“åŒ…æ‹¬ï¼Œ
- en: down sample the detection of features in feature maps
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹é‡‡æ ·ç‰¹å¾å›¾ä¸­ç‰¹å¾æ£€æµ‹
- en: reduces the dimensionality of the feature map
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘äº†ç‰¹å¾å›¾çš„ç»´åº¦
- en: integrate translation invariance, pattern detection insensitive to location
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•´åˆå¹³ç§»ä¸å˜æ€§ï¼Œå¯¹ä½ç½®ä¸æ•æ„Ÿçš„æ¨¡å¼æ£€æµ‹
- en: Two common pooling methods are average and max pooling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¸¸è§çš„æ± åŒ–æ–¹æ³•æ˜¯å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ã€‚
- en: '![](../Images/4b2fd11462cbb82e0cb09ac3e2d938ff.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b2fd11462cbb82e0cb09ac3e2d938ff.png)'
- en: Schematic of pooling operation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ± åŒ–æ“ä½œçš„ç¤ºæ„å›¾ã€‚
- en: Hereâ€™s an example of pooling with a 2x2 filter and a stride of 2.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªä½¿ç”¨2x2æ»¤æ³¢å™¨å’Œæ­¥é•¿ä¸º2çš„æ± åŒ–ç¤ºä¾‹ã€‚
- en: '![](../Images/81532b0b04b5a5af52f9b81557967b34.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81532b0b04b5a5af52f9b81557967b34.png)'
- en: Example of pooling operation for reducing the extent of the feature map.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ± åŒ–æ“ä½œçš„ç¤ºä¾‹ï¼Œç”¨äºå‡å°‘ç‰¹å¾å›¾çš„å°ºå¯¸ã€‚
- en: with 2 x 2 filter, stride of 2 the dimension is reduced Â½ per axis, Â¼ for 2D
    images.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨2 x 2æ»¤æ³¢å™¨ï¼Œæ­¥é•¿ä¸º2ï¼Œæ¯ä¸ªè½´çš„ç»´åº¦å‡å°‘ä¸€åŠï¼Œå¯¹äº2Då›¾åƒä¸ºå››åˆ†ä¹‹ä¸€ã€‚
- en: for example, the value of 16 is location independent within the filter. This
    introduces translational invariance.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ16çš„å€¼åœ¨æ»¤æ³¢å™¨å†…æ˜¯ä½ç½®æ— å…³çš„ã€‚è¿™å¼•å…¥äº†å¹³ç§»ä¸å˜æ€§ã€‚
- en: '![](../Images/a3f5e46b161a0d2f7e0f99574f3db593.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/a3f5e46b161a0d2f7e0f99574f3db593.png)'
- en: Max and average pooling examples from a brick wall on the campus of The University
    of Texas at Austin..
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ç –å¢™çš„æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–ç¤ºä¾‹ã€‚
- en: Depth-wise Pooling
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±åº¦æ± åŒ–
- en: Summarization over feature maps with a single value.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•ä¸ªå€¼å¯¹ç‰¹å¾å›¾è¿›è¡Œæ€»ç»“ã€‚
- en: down sample the detection over feature maps
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾å›¾ä¸Šå¯¹æ£€æµ‹è¿›è¡Œä¸‹é‡‡æ ·
- en: combine information learned from multiple kernels
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“åˆæ¥è‡ªå¤šä¸ªæ ¸çš„ä¿¡æ¯
- en: reduces the depth of the feature map, next layer
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘äº†ç‰¹å¾å›¾çš„æ·±åº¦ï¼Œä¸‹ä¸€å±‚
- en: Two common pooling methods are average and max pooling.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¸¸è§çš„æ± åŒ–æ–¹æ³•æ˜¯å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ã€‚
- en: '![](../Images/1786e9cb04c2dac2297ddc607d4b0649.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/1786e9cb04c2dac2297ddc607d4b0649.png)'
- en: Schematic of depthwise pooling operation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦æ± åŒ–æ“ä½œçš„ç¤ºæ„å›¾ã€‚
- en: Common CNN Architecture
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§çš„CNNæ¶æ„
- en: The following is a common workflow for convolutional neural networks,
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œçš„å¸¸è§å·¥ä½œæµç¨‹ï¼Œ
- en: '![](../Images/fcb53216a4534c9da9053f990e48aa90.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/fcb53216a4534c9da9053f990e48aa90.png)'
- en: The common CNN workflow.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„CNNå·¥ä½œæµç¨‹ã€‚
- en: We can illustrate this common workflow with an illustration of the common architecture.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç”¨å¸¸è§æ¶æ„çš„ç¤ºæ„å›¾æ¥è¡¨ç¤ºè¿™ä¸ªå¸¸è§çš„å·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/618c123a572e16a0e12819e75b283099.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/618c123a572e16a0e12819e75b283099.png)'
- en: The common CNN architecture.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„CNNæ¶æ„ã€‚
- en: By-Hand CNN Architecture
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹å·¥å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„
- en: Just like the artificial neural network chapter, I build a simple convolutional
    neural network by-hand.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒäººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä¸€æ ·ï¼Œæˆ‘é€šè¿‡æ‰‹å·¥åˆ¶ä½œäº†ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: For simplicity and brevity, I have made the following architectural choices,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´å’Œç®€æ´ï¼Œæˆ‘åšå‡ºäº†ä»¥ä¸‹æ¶æ„é€‰æ‹©ï¼Œ
- en: 1D images with height of 5
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜åº¦ä¸º5çš„1Då›¾åƒ
- en: 1 convolutional layer with 1 kernel of size 3
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªå·ç§¯å±‚ï¼Œå¤§å°ä¸º3çš„1ä¸ªæ ¸
- en: activation with sigmoid
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨sigmoidæ¿€æ´»
- en: stride of 1 and no padding so the feature map has a size of 3
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­¥é•¿ä¸º1ä¸”æ— å¡«å……ï¼Œå› æ­¤ç‰¹å¾å›¾çš„å¤§å°ä¸º3
- en: artificial neural network from feature map immediately to output node with linear
    activation
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»ç‰¹å¾å›¾ç›´æ¥åˆ°è¾“å‡ºèŠ‚ç‚¹ï¼Œå…·æœ‰çº¿æ€§æ¿€æ´»çš„äººå·¥ç¥ç»ç½‘ç»œ
- en: This minimalist architecture demonstrates many of the salient concepts for convolutional
    neural networks while being very easy to visualize and very fast to train.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç®€çº¦æ¶æ„å±•ç¤ºäº†å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è®¸å¤šæ˜¾è‘—æ¦‚å¿µï¼ŒåŒæ—¶éå¸¸æ˜“äºå¯è§†åŒ–ä¸”è®­ç»ƒé€Ÿåº¦å¿«ã€‚
- en: '![](../Images/c18c0d3da8b925d617dbd7cad6b9d1fa.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/c18c0d3da8b925d617dbd7cad6b9d1fa.png)'
- en: Schematic illustration of our by-hand CNN.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹å·¥åˆ¶ä½œçš„å·ç§¯ç¥ç»ç½‘ç»œçš„ç¤ºæ„å›¾ã€‚
- en: Now, we convert this schematic of our by-hand CNN to a diagram of the actual
    nodes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æ‰‹å·¥åˆ¶ä½œçš„å·ç§¯ç¥ç»ç½‘ç»œçš„ç¤ºæ„å›¾è½¬æ¢ä¸ºå®é™…èŠ‚ç‚¹çš„å›¾ã€‚
- en: '![](../Images/aaaec452d8f375bf00b4df41c281e46c.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/aaaec452d8f375bf00b4df41c281e46c.png)'
- en: Architecture of our by-hand CNN.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹å·¥åˆ¶ä½œçš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚
- en: To demonstrate the practicality of working with this architecture, letâ€™s add
    the labels of the model parameters,
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºä½¿ç”¨æ­¤æ¶æ„çš„å®ç”¨æ€§ï¼Œè®©æˆ‘ä»¬æ·»åŠ æ¨¡å‹å‚æ•°çš„æ ‡ç­¾ï¼Œ
- en: kernel weights, \(\lambda_6\), \(\lambda_7\), and \(\lambda_8\)
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸æƒé‡ï¼Œ\(\lambda_6\)ï¼Œ\(\lambda_7\)ï¼Œå’Œ \(\lambda_8\)
- en: kernel bias, \(b_{conv}\)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸åç½®ï¼Œ\(b_{conv}\)
- en: artificial neural network weights, \(\lambda_{9,12}\), \(\lambda_{10,12}\),
    and \(\lambda_{11,12}\)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œæƒé‡ï¼Œ\(\lambda_{9,12}\)ï¼Œ\(\lambda_{10,12}\)ï¼Œå’Œ \(\lambda_{11,12}\)
- en: artificial neural network bias, \(b_{12}\)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œåå·®ï¼Œ\(b_{12}\)
- en: '![](../Images/11f0651d909bf7e40e4ebdeca86bab61.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/11f0651d909bf7e40e4ebdeca86bab61.png)'
- en: Architecture of our by-hand CNN with all trainable model parameters. Note, kernel
    weights are only shown in the first kernel position to avoid clutter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…å«æ‰€æœ‰å¯è®­ç»ƒæ¨¡å‹å‚æ•°çš„æˆ‘ä»¬æ‰‹å·¥åˆ¶ä½œçš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚æ³¨æ„ï¼Œä»…æ˜¾ç¤ºäº†ç¬¬ä¸€ä¸ªæ ¸ä½ç½®ä¸­çš„æ ¸æƒé‡ä»¥é¿å…æ‚ä¹±ã€‚
- en: Training Model Parameters
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: Training a convolutional neural network proceeds iteratively by these steps,
    the same as discussed in the artificial neural network chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œï¼Œä¸äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä¸­è®¨è®ºçš„ç›¸åŒã€‚
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
- en: Training an artificial neural network proceeds iteratively by, 1\. forward pass
    to make a prediction, 2\. calculate the error derivative based on the prediction
    and truth over training data, 3\. backpropagate the error derivative back through
    the artificial neural network to calculate the derivatives of the error over all
    the model weights and biases parameters, 4\. update the model parameters based
    on the derivatives and learning rates, 5\. repeat until convergence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œé€šè¿‡è¿­ä»£è¿›è¡Œï¼Œ1. å‰å‘ä¼ æ’­è¿›è¡Œé¢„æµ‹ï¼Œ2. æ ¹æ®è®­ç»ƒæ•°æ®ä¸­çš„é¢„æµ‹å’ŒçœŸå®å€¼è®¡ç®—è¯¯å·®å¯¼æ•°ï¼Œ3. é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œ4.
    æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œ5. é‡å¤ç›´åˆ°æ”¶æ•›ã€‚
- en: Hereâ€™s some details on each step with a focus on differences from artificial
    neural networks, for more details see the artificial neural network chapter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œé‡ç‚¹å…³æ³¨ä¸äººå·¥ç¥ç»ç½‘ç»œçš„å·®å¼‚ï¼Œæ›´å¤šç»†èŠ‚è¯·å‚é˜…äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ã€‚
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Hereâ€™s a couple common methods,'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚è¿™é‡Œæœ‰ä¸€äº›å¸¸è§çš„æ–¹æ³•ï¼Œ'
- en: '**Xavier Glorot Uniform Initialization** - for Tanh and sigmoid activation,
    random realizations from uniform distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier Glorot å‡åŒ€åˆå§‹åŒ–** - å¯¹äº Tanh å’Œ sigmoid æ¿€æ´»å‡½æ•°ï¼Œä»ç”± \(U[\text{min}, \text{max}]\)
    æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå–å€¼ï¼Œ'
- en: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}},\ \sqrt{\frac{6}{n_{\text{in}}
    + n_{\text{out}}}}\right]^{-1}(p^\ell) \]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}},\ \sqrt{\frac{6}{n_{\text{in}}
    + n_{\text{out}}}}\right]^{-1}(p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\) ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: For example, given a \(3 \times 3\) kernel with 1 channel in and 9 channel out,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é€šé“ä¸º1ï¼Œè¾“å‡ºé€šé“ä¸º9çš„ \(3 \times 3\) å·ç§¯æ ¸ï¼Œ
- en: \[ n_{in} = k \times k \times C = 3 \times 3 \times 1 \]\[ n_out = k \times
    k \times C = 3 \times 3 \times 9 \]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{in} = k \times k \times C = 3 \times 3 \times 1 \]\[ n_out = k \times
    k \times C = 3 \times 3 \times 9 \]
- en: '**He Kaiming Weight Initialization** - for ReLU and leaky ReLU activation,
    random realizations from uniform distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**He Kaiming æƒé‡åˆå§‹åŒ–** - å¯¹äº ReLU å’Œ leaky ReLU æ¿€æ´»å‡½æ•°ï¼Œä»ç”± \(U[\text{min}, \text{max}]\)
    æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºå–å€¼ï¼Œ'
- en: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}}}},\ \sqrt{\frac{6}{n_{\text{in}}}}\right]^{-1}(p^\ell)
    \]
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}}}},\ \sqrt{\frac{6}{n_{\text{in}}}}\right]^{-1}(p^\ell)
    \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥æ•°é‡ï¼Œ\(k\) æ˜¯è¾“å‡ºæ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\)
    ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‰å‘ä¼ æ’­** - è¿›è¡Œé¢„æµ‹ï¼Œ\(\hat{y}\)ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šæ”¹è¿›ã€‚'
- en: '![](../Images/08556ecbd47d143019d0163dc95761cf.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08556ecbd47d143019d0163dc95761cf.png)'
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®çš„äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ã€‚
- en: '**Calculate the Error Derivative** - given a loss of, \(P = \frac{1}{2} \left(\hat{y}
    - y \right)^2\), the error derivative, i.e., rate of change of in error given
    a change in model estimate is \(\frac{\partial P}{\partial \hat{y}} = \hat{Y}
    - Y\).'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—è¯¯å·®å¯¼æ•°** - ç»™å®šæŸå¤± \(P = \frac{1}{2} \left(\hat{y} - y \right)^2\)ï¼Œè¯¯å·®å¯¼æ•°ï¼Œå³è¯¯å·®éšæ¨¡å‹ä¼°è®¡å˜åŒ–çš„å˜åŒ–ç‡æ˜¯
    \(\frac{\partial P}{\partial \hat{y}} = \hat{Y} - Y\)ã€‚'
- en: For now, letâ€™s only consider a single estimate, and we will address more than
    1 training data later.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œæˆ‘ä»¬åªè€ƒè™‘å•ä¸ªä¼°è®¡å€¼ï¼Œç¨åæˆ‘ä»¬å°†è®¨è®ºè¶…è¿‡1ä¸ªè®­ç»ƒæ•°æ®ã€‚
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, to accomplish this we use the chain rule,'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¸ºæ­¤æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œ'
- en: \[ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot
    \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} \]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot
    \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} \]
- en: '**Update the Model Parameters** - based on the derivatives, \frac{\partial
    P}{\partial \lambda_{i,j}} and learning rates, \(\eta\), like this,'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´æ–°æ¨¡å‹å‚æ•°** - åŸºäºå¯¼æ•°ï¼Œ\(\frac{\partial P}{\partial \lambda_{i,j}}\) å’Œå­¦ä¹ ç‡ï¼Œ\(\eta\)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ'
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    P}{\partial \lambda_{i,j}} \]
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    P}{\partial \lambda_{i,j}} \]
- en: '**Repeat Until Convergence** - return to step 1\. until the error, \(P\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›æ­¥éª¤1ï¼Œç›´åˆ°è¯¯å·®ï¼Œ\(P\)ï¼Œé™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶'
- en: Backpropagation
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­
- en: For brevity, I refer you to the artificial neural network chapter for a walkthrough
    of backpropagating the error gradient through a neural network.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å»ºè®®æ‚¨å‚è€ƒäººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ï¼Œäº†è§£é€šè¿‡ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®æ¢¯åº¦çš„è¿‡ç¨‹ã€‚
- en: Updating Model Parameters
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°æ¨¡å‹å‚æ•°
- en: The derivatives for each of the model parameters are the error gradients, so
    we are ready to use gradient descent optimization with the addition of,
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ¨¡å‹å‚æ•°çš„å¯¼æ•°æ˜¯è¯¯å·®æ¢¯åº¦ï¼Œå› æ­¤æˆ‘ä»¬å‡†å¤‡å¥½ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå¹¶æ·»åŠ ï¼Œ
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡** - ä¸ºäº†ç¼©æ”¾æ¨¡å‹æ›´æ–°çš„å˜åŒ–ç‡ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œ\(\eta\)ã€‚å¯¹äºæˆ‘ä»¬ä¸Šé¢çš„æ¨¡å‹å‚æ•°ç¤ºä¾‹ï¼Œ'
- en: \[ \lambda_{9,12}^{\ell} = \lambda_{9,12}^{\ell - 1} - \eta \cdot \frac{\partial
    P}{\partial \lambda_{9,12}} \]\[ \lambda_{10,12}^{\ell} = \lambda_{10,12}^{\ell
    - 1} - \eta \cdot \frac{\partial P}{\partial \lambda_{10,12}} \]\[ b_{12}^{\ell}
    = b_{12}^{\ell - 1} + \eta \cdot \frac{\partial P}{\partial b_{12}} \]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{9,12}^{\ell} = \lambda_{9,12}^{\ell - 1} - \eta \cdot \frac{\partial
    P}{\partial \lambda_{9,12}} \]\[ \lambda_{10,12}^{\ell} = \lambda_{10,12}^{\ell
    - 1} - \eta \cdot \frac{\partial P}{\partial \lambda_{10,12}} \]\[ b_{12}^{\ell}
    = b_{12}^{\ell - 1} + \eta \cdot \frac{\partial P}{\partial b_{12}} \]
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›æƒ³ä¸€ä¸‹ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯æ¢¯åº¦è®¡ç®—å’Œæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®çš„æ›´æ–°ï¼Œè¿­ä»£å¹¶è¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ¢ç´¢æŸå¤±è¶…æ›²é¢ï¼Œé¿å…å¹¶é€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼Œæœ€ç»ˆæ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ï¼Œä¹Ÿç§°ä¸ºæ­¥é•¿ï¼Œé€šå¸¸è®¾ç½®åœ¨0.0åˆ°1.0ä¹‹é—´ï¼Œæ³¨æ„åœ¨TensorFlowçš„Kerasæ¨¡å—ä¸­é»˜è®¤ä¸º0.01
- en: '**Low Learning Rate** â€“ more stable, but a slower solution, may get stuck in
    a local minimum'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½å­¦ä¹ ç‡** â€“ æ›´ç¨³å®šï¼Œä½†è§£çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼'
- en: '**High Learning Rate** â€“ may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å­¦ä¹ ç‡** â€“ å¯èƒ½ä¸ç¨³å®šï¼Œä½†å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¿«çš„è§£å†³æ–¹æ¡ˆï¼Œå¯èƒ½ä¼šå‘æ•£åˆ°å…¨å±€æœ€å°å€¼ä¹‹å¤–'
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç­–ç•¥æ˜¯ä»ä¸€ä¸ªé«˜çš„å­¦ä¹ ç‡å¼€å§‹ï¼Œç„¶ååœ¨è¿­ä»£è¿‡ç¨‹ä¸­é€æ¸é™ä½å­¦ä¹ ç‡
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡è¡°å‡** - è®¾ç½®ä¸º> 0ä»¥é¿å…å‡è½»æŒ¯è¡ï¼Œ'
- en: Training Epochs
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒå‘¨æœŸ
- en: This is a good time to talk about stochastic gradient descent optimization,
    first letâ€™s define some common terms,
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯è®¨è®ºéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„å¥½æ—¶æœºï¼Œé¦–å…ˆè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å¸¸è§æœ¯è¯­ï¼Œ
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡æ‰€æœ‰æ•°æ®åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éšæœºæ¢¯åº¦ä¸‹é™** - åœ¨æ¯ä¸ªæ ·æœ¬æ•°æ®ä¸Šæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡å•ä¸ªæ‰¹æ¬¡åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸­ï¼Œé€šè¿‡ä½¿ç”¨æ•°æ®å­é›†ï¼Œç§°ä¸ºæ‰¹æ¬¡ï¼Œå¼•å…¥äº†éšæœºæ€§ï¼Œ
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŠŠæˆ‘ä»¬çš„100ä¸ªæ ·æœ¬åˆ†æˆ4ä¸ªæ‰¹æ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ†åˆ«è¿­ä»£æ¯ä¸ªæ‰¹æ¬¡
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŠ å¿«äº†å•ä¸ªæ›´æ–°çš„é€Ÿåº¦ï¼Œæ•°æ®æ›´å°‘ï¼Œè®¡ç®—æ›´å¿«ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¼•å…¥äº†æ›´å¤šçš„è¯¯å·®
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸æœ‰åŠ©äºè®­ç»ƒæ¢ç´¢å…¨å±€æœ€å°å€¼ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼å’ŒæŸå¤±è¶…æ›²é¢ä¸Šçš„è„Š
- en: Finally our last definition here,
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰çš„æœ€åä¸€ä¸ªæœ¯è¯­ï¼Œ
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epoch** - æ˜¯éå†æ‰€æœ‰æ•°æ®çš„ä¸€æ¬¡ï¼Œå› æ­¤å¦‚æœæœ‰ 4 ä¸ª mini-batchesï¼Œé‚£ä¹ˆæ¨¡å‹å‚æ•°çš„æ›´æ–°å°†ä¼šè¿›è¡Œ 4 æ¬¡è¿­ä»£ã€‚'
- en: There are many other considerations that I will add later including,
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰è®¸å¤šå…¶ä»–è€ƒè™‘å› ç´ ï¼Œæˆ‘å°†åœ¨ä»¥åæ·»åŠ ï¼ŒåŒ…æ‹¬ï¼Œ
- en: momentum
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨é‡
- en: adaptive optimization
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ä¼˜åŒ–
- en: Now letâ€™s build the above artificial neural network by-hand and visualize the
    solution!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸Šè¿°äººå·¥ç¥ç»ç½‘ç»œå¹¶å¯è§†åŒ–è§£å†³æ–¹æ¡ˆï¼
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ‰‹åŠ¨æ“ä½œï¼Œè¿™æ ·ä½ å¯ä»¥çœ‹åˆ°æ¯ä¸€ä¸ªè®¡ç®—ã€‚æˆ‘æ•…æ„é¿å…ä½¿ç”¨ TensorFlow æˆ– PyTorchã€‚
- en: Training with Multiple Training Images
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªè®­ç»ƒå›¾åƒè¿›è¡Œè®­ç»ƒ
- en: The backpropagation is based on a single sample, i.e., training image and paired
    response feature value; therefore, to train over multiple images we must cycle
    over the,
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸºäºå•ä¸ªæ ·æœ¬ï¼Œå³è®­ç»ƒå›¾åƒå’Œé…å¯¹çš„å“åº”ç‰¹å¾å€¼ï¼›å› æ­¤ï¼Œè¦è®­ç»ƒå¤šä¸ªå›¾åƒï¼Œæˆ‘ä»¬å¿…é¡»å¾ªç¯éå†å®ƒä»¬ï¼Œ
- en: forward pass
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ æ’­
- en: calculate error derivative
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—è¯¯å·®å¯¼æ•°
- en: back propagate
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­
- en: '![](../Images/911e3eed1fd228a0da4716b460c82d38.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/911e3eed1fd228a0da4716b460c82d38.png)'
- en: Batch training process.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡è®­ç»ƒè¿‡ç¨‹ã€‚
- en: For each image the weights and biases gradients are stored. Then the gradients
    are summed over the images in the batch and this sum is applied with the learning
    rate to update the weights and biases.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå›¾åƒï¼Œå­˜å‚¨æƒé‡å’Œåç½®çš„æ¢¯åº¦ã€‚ç„¶åï¼Œåœ¨æ‰¹æ¬¡çš„å›¾åƒä¸Šå¯¹æ¢¯åº¦è¿›è¡Œæ±‚å’Œï¼Œå¹¶å°†è¿™ä¸ªå’Œä¸å­¦ä¹ ç‡ä¸€èµ·åº”ç”¨ä»¥æ›´æ–°æƒé‡å’Œåç½®ã€‚
- en: Forward Pass
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ æ’­
- en: For clarity, letâ€™s walk through the convolutional neural network, starting with
    image input.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œè®©æˆ‘ä»¬ä»å›¾åƒè¾“å…¥å¼€å§‹éå†å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: The input nodes receives the input from the image in to the convolution layer,
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹æ¥æ”¶ä»å›¾åƒåˆ°å·ç§¯å±‚çš„è¾“å…¥ï¼Œ
- en: node order is retained to preserve spatial, location information from the image
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿ç•™èŠ‚ç‚¹é¡ºåºä»¥ä¿ç•™ä»å›¾åƒä¸­è·å–çš„ç©ºé—´ã€ä½ç½®ä¿¡æ¯ã€‚
- en: For continuous feature images,
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿ç»­ç‰¹å¾å›¾åƒï¼Œ
- en: the continuous predictor feature values are normalized to a min / max of [0,1]
    or [-1,1] to improve sensitivity for the specific activation function
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿ç»­é¢„æµ‹ç‰¹å¾å€¼è¢«å½’ä¸€åŒ–åˆ° [0,1] æˆ– [-1,1]ï¼Œä»¥æ”¹å–„ç‰¹å®šæ¿€æ´»å‡½æ•°çš„çµæ•åº¦ã€‚
- en: for color images, the RGB channels may be each normalized and included as 3
    input channels
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå½©è‰²å›¾åƒï¼ŒRGB é€šé“å¯èƒ½æ¯ä¸ªéƒ½è¢«å½’ä¸€åŒ–å¹¶ä½œä¸º 3 ä¸ªè¾“å…¥é€šé“åŒ…å«åœ¨å†…ã€‚
- en: For categorical feature images,
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»ç‰¹å¾å›¾åƒï¼Œ
- en: for binary, cardinality of 2, the values may be reassigned by indicator transform
    to 0 or 1
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºäºŒå…ƒï¼ŒåŸºæ•°ä¸º 2 çš„æƒ…å†µï¼Œå€¼å¯ä»¥é€šè¿‡æŒ‡ç¤ºå˜æ¢é‡æ–°åˆ†é…ä¸º 0 æˆ– 1
- en: for cardinality > 2, one-hot-encoding may be applied resulting in \(k\) input
    channels
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåŸºæ•°å¤§äº 2 çš„æƒ…å†µï¼Œå¯èƒ½åº”ç”¨ one-hot-encodingï¼Œä»è€Œäº§ç”Ÿ \(k\) ä¸ªè¾“å…¥é€šé“ã€‚
- en: See the feature transformation chapter for more details about these transformations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è¿™äº›å˜æ¢çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç‰¹å¾å˜æ¢ç« èŠ‚ã€‚
- en: Now we pass through a convolution layer, with convolution and activation, resulting
    in a new feature map.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå·ç§¯å±‚ï¼ŒåŒ…æ‹¬å·ç§¯å’Œæ¿€æ´»ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾å›¾ã€‚
- en: '![](../Images/02745c4fdecaad32e59683bafa3d0820.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02745c4fdecaad32e59683bafa3d0820.png)'
- en: Walk-through of our by-hand convolutional neural network, through the convolution
    layer.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ‰‹åŠ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvolutional neural networkï¼‰çš„éå†ï¼Œé€šè¿‡å·ç§¯å±‚ã€‚
- en: take linearly weighted combinations based on the kernel(s) of input image or
    previous feature map, add a bias term and then nonlinearly transform the result,
    this transform is call the activation function, \(\alpha\).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®è¾“å…¥å›¾åƒæˆ–å‰ä¸€ä¸ªç‰¹å¾å›¾çš„æ ¸ï¼ˆkernelï¼‰è¿›è¡Œçº¿æ€§åŠ æƒçš„ç»„åˆï¼Œæ·»åŠ ä¸€ä¸ªåç½®é¡¹ï¼Œç„¶åå¯¹ç»“æœè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œè¿™ç§å˜æ¢ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼Œ\(\alpha\)ã€‚
- en: \[ C_{j_{\text{in}}} = \sum_{i=1}^{n} \left( K_{i+5} \cdot I_{j-8} \right) +
    b_{conv} \]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_{j_{\text{in}}} = \sum_{i=1}^{n} \left( K_{i+5} \cdot I_{j-8} \right) +
    b_{conv} \]
- en: where, \(K_6\), \(K_7\) and \(K_8\) are kernel weights, \(b_{conv}\) is the
    kernel bias, and the \(I\) are the input nodes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ\(K_6\)ã€\(K_7\) å’Œ \(K_8\) æ˜¯æ ¸æƒé‡ï¼Œ\(b_{conv}\) æ˜¯æ ¸åç½®ï¼Œ\(I\) æ˜¯è¾“å…¥èŠ‚ç‚¹ã€‚
- en: Please, excuse the strange indices in the equation, I like using unique node
    integers for every node to avoid mixing up nodes in my notes and codes, but this
    does complicate the index assignments.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·åŸè°…æ–¹ç¨‹ä¸­çš„å¥‡æ€ªç´¢å¼•ï¼Œæˆ‘å–œæ¬¢ä¸ºæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„èŠ‚ç‚¹æ•´æ•°ï¼Œä»¥é¿å…åœ¨ç¬”è®°å’Œä»£ç ä¸­æ··æ·†èŠ‚ç‚¹ï¼Œä½†è¿™ç¡®å®ä½¿ç´¢å¼•åˆ†é…å¤æ‚åŒ–ã€‚
- en: \[ C_{9_{\text{in}}} = I_1 \cdot K_6 + I_2 \cdot K_7 + I_3 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{10_{\text{in}}} = I_2 \cdot K_6 + I_3 \cdot K_7 + I_4 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{11_{\text{in}}} = I_3 \cdot K_6 + I_4 \cdot K_7 + I_5 \cdot K_8 + b_{\text{conv}}
    \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_{9_{\text{in}}} = I_1 \cdot K_6 + I_2 \cdot K_7 + I_3 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{10_{\text{in}}} = I_2 \cdot K_6 + I_3 \cdot K_7 + I_4 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{11_{\text{in}}} = I_3 \cdot K_6 + I_4 \cdot K_7 + I_5 \cdot K_8 + b_{\text{conv}}
    \]
- en: then nonlinear activation is applied to each,
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¹æ¯ä¸ªèŠ‚ç‚¹åº”ç”¨éçº¿æ€§æ¿€æ´»ï¼Œ
- en: \[ C_j = \alpha \left( C_{j_{in}} \right) \quad j = 9, \ldots 11 \]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_j = \alpha \left( C_{j_{in}} \right) \quad j = 9, \ldots 11 \]
- en: Now we proceed from the feature map through the artificial neural network to
    the output.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä»ç‰¹å¾å›¾é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåˆ°è¾“å‡ºã€‚
- en: This is just a standard artificial neural network that takes the feature map
    flattened and moves it to the output.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªæ ‡å‡†çš„å…·æœ‰å±•å¹³ç‰¹å¾å›¾å¹¶å°†å…¶ç§»åŠ¨åˆ°è¾“å‡ºçš„ç®€å•äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: in this example for brevity we show the simplest possible artificial neural
    network, i.e., the next layer after the feature map is the output. More complicated
    architectures with hidden layers are often applied.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€ç®€å•çš„å¯èƒ½çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œå³ç‰¹å¾å›¾ä¹‹åçš„ä¸‹ä¸€å±‚æ˜¯è¾“å‡ºã€‚æ›´å¤æ‚çš„å…·æœ‰éšè—å±‚çš„æ¶æ„é€šå¸¸è¢«åº”ç”¨ã€‚
- en: also, since our images are 1D we do not require a flattening step
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„å›¾åƒæ˜¯1Dçš„ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œå±•å¹³æ­¥éª¤
- en: The output node is a standard output node from an artificial neural network,
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹æ˜¯äººå·¥ç¥ç»ç½‘ç»œçš„æ ‡å‡†è¾“å‡ºèŠ‚ç‚¹ï¼Œ
- en: input is a linear combination of the nodes from the previous layer with an added
    bias term.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥æ˜¯å‰ä¸€å±‚èŠ‚ç‚¹çš„çº¿æ€§ç»„åˆï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªåç½®é¡¹ã€‚
- en: \[ O_{j_{\text{in}}} = \sum_{j=1}^{m} \left( \lambda_{i,j} \cdot H_i \right)
    + b_j \]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{j_{\text{in}}} = \sum_{j=1}^{m} \left( \lambda_{i,j} \cdot H_i \right)
    + b_j \]
- en: and then an activation is applied,
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååº”ç”¨ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œ
- en: \[ O_j = \alpha \left( O_{j_{in}} \right) \]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha \left( O_{j_{in}} \right) \]
- en: For the case of a regression model, with continuous response feature, linear
    or identity activation is applied,
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›å½’æ¨¡å‹çš„æƒ…å†µï¼Œå…·æœ‰è¿ç»­å“åº”ç‰¹å¾ï¼Œåº”ç”¨çº¿æ€§æˆ–æ’ç­‰æ¿€æ´»å‡½æ•°ï¼Œ
- en: \[ O_j = \alpha \left( O_{j_{in}} \right) = O_{j_{in}} \]![](../Images/3becddf21de74c01ec2c585108079e0e.png)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha \left( O_{j_{in}} \right) = O_{j_{in}} \]![](../Images/3becddf21de74c01ec2c585108079e0e.png)
- en: Walk-through of our by-hand convolutional neural network, from feature map to
    the output for continuous output.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œçš„æ‰‹åŠ¨å®ç°è¿‡ç¨‹ï¼Œä»ç‰¹å¾å›¾åˆ°è¾“å‡ºï¼Œç”¨äºè¿ç»­è¾“å‡ºã€‚
- en: and for the case of a classification model, with categorical response feature,
    softmax activation is applied over \(K\) nodes equal to the cardinality of the
    response feature.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»æ¨¡å‹çš„æƒ…å†µï¼Œå…·æœ‰åˆ†ç±»å“åº”ç‰¹å¾ï¼Œåœ¨ç­‰äºå“åº”ç‰¹å¾åŸºæ•°\(K\)çš„èŠ‚ç‚¹ä¸Šåº”ç”¨softmaxæ¿€æ´»å‡½æ•°ã€‚
- en: the output is a probability for each category that honors non-negativity and
    closure constraints
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç¬¦åˆéè´Ÿæ€§å’Œé—­åˆçº¦æŸ
- en: '![](../Images/439615adbac8db26678e58e258f9105f.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439615adbac8db26678e58e258f9105f.png)'
- en: Walk-through of our by-hand convolutional neural network, from feature map to
    the output for categorical output.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œçš„æ‰‹åŠ¨å®ç°è¿‡ç¨‹ï¼Œä»ç‰¹å¾å›¾åˆ°è¾“å‡ºï¼Œç”¨äºåˆ†ç±»è¾“å‡ºã€‚
- en: \[ O_j = g_k(O_{j_{\text{in}}}) = \frac{e^{O_{j_{\text{in}}}}}{\sum_{\iota=1}^{K}
    e^{O_{\iota_{\text{in}}}}} \]
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = g_k(O_{j_{\text{in}}}) = \frac{e^{O_{j_{\text{in}}}}}{\sum_{\iota=1}^{K}
    e^{O_{\iota_{\text{in}}}}} \]
- en: Import Required Packages
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ‰€éœ€çš„åŒ…
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: recall our goal is to build a convolutional neural network by-hand with only
    basic math and array operations, so we only need NumPy along with matplotlib for
    plotting.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°ä½æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰‹åŠ¨æ„å»ºä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œåªä½¿ç”¨åŸºæœ¬çš„æ•°å­¦å’Œæ•°ç»„æ“ä½œï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦NumPyä»¥åŠmatplotlibè¿›è¡Œç»˜å›¾ã€‚
- en: '[PRE0]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing â€˜python -m pip install [package-name]â€™. More assistance is available
    with the respective package docs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œæ‚¨å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„ä¸€äº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨Windowsä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install
    [package-name]â€™æ¥å®Œæˆã€‚æ›´å¤šå¸®åŠ©å¯ä»¥åœ¨ç›¸åº”åŒ…çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ã€‚
- en: Declare Functions
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: Hereâ€™s the functions to make, train and visualize our convoluational neural
    network.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ›å»ºã€è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œçš„å‡½æ•°ã€‚
- en: '[PRE1]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Simple By-hand CNN
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•çš„æ‰‹åŠ¨CNN
- en: I wrote this code to specify a simple CNN,
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¼–å†™äº†è¿™æ®µä»£ç æ¥æŒ‡å®šä¸€ä¸ªç®€å•çš„CNNï¼Œ
- en: five input nodes, 1 convolution layer with a kernel of 3 resulting in a 3 nodes
    in the feature map and 1 output node
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº”ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œ1ä¸ªå…·æœ‰3ä¸ªæ ¸çš„å·ç§¯å±‚ï¼Œåœ¨ç‰¹å¾å›¾ä¸­äº§ç”Ÿ3ä¸ªèŠ‚ç‚¹å’Œ1ä¸ªè¾“å‡ºèŠ‚ç‚¹
- en: 'and to train the CNN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶é€šè¿‡è¿­ä»£æ‰§è¡Œå‰å‘è®¡ç®—å’Œåå‘ä¼ æ’­æ¥è®­ç»ƒCNNã€‚æˆ‘è®¡ç®—ï¼š
- en: the error and then propagate it to each node
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åä¼ æ’­é”™è¯¯åˆ°æ¯ä¸ªèŠ‚ç‚¹
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚è§£è¯¯å·®ç›¸å¯¹äºæ¯ä¸ªæƒé‡å’Œåç½®çš„åå¯¼æ•°
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æƒé‡ã€åç½®å’Œæ‰€æœ‰epochçš„åå¯¼æ•°éƒ½è®°å½•åœ¨å‘é‡ä¸­ä»¥ä¾¿ç»˜å›¾
- en: '[PRE2]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Visualize By-hand CNN
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨å¯è§†åŒ–CNN
- en: Letâ€™s visualize our convolutional neural network.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: note, I will used this code latter to make interactive dashboards.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘å°†åœ¨ä»¥åä½¿ç”¨æ­¤ä»£ç åˆ¶ä½œäº¤äº’å¼ä»ªè¡¨æ¿ã€‚
- en: '[PRE3]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![_images/bd6f0d76fef9a0c1d4062d2fed4028a098a6ba3f59c0dff50e67370b90502ba7.png](../Images/0b3e6b1109144cd7a0b01bc9d699e751.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![_images/bd6f0d76fef9a0c1d4062d2fed4028a098a6ba3f59c0dff50e67370b90502ba7.png](../Images/0b3e6b1109144cd7a0b01bc9d699e751.png)'
- en: and now we can visualize the model training results including,
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ¨¡å‹è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬ï¼Œ
- en: weights and biases vs. training epochs
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒé‡å’Œåå·®ä¸è®­ç»ƒè½®æ•°
- en: CNN prediction vs. training epochs
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNé¢„æµ‹ä¸è®­ç»ƒè½®æ•°
- en: '[PRE4]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![_images/4cea6b946d67f8e2cb6605415803c148ac63ebfb43f8034a7285880405bb88d3.png](../Images/7dcc0007f13a7312fec3085ca24fbf79.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![_images/4cea6b946d67f8e2cb6605415803c148ac63ebfb43f8034a7285880405bb88d3.png](../Images/7dcc0007f13a7312fec3085ca24fbf79.png)'
- en: Of course, the results above are not very practical, in fact we need our convolutional
    neural network to generalize by learning over many images, not just 1 as demonstrated
    above. Letâ€™s,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¸Šè¿°ç»“æœå¹¶ä¸éå¸¸å®ç”¨ï¼Œå®é™…ä¸Šæˆ‘ä»¬éœ€è¦æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡å­¦ä¹ è®¸å¤šå›¾åƒæ¥æ³›åŒ–ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸Šé¢æ¼”ç¤ºçš„1ä¸ªã€‚è®©æˆ‘ä»¬ï¼Œ
- en: make a suite of synthetic image data with labels
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ¶ä½œä¸€ç³»åˆ—å¸¦æœ‰æ ‡ç­¾çš„åˆæˆå›¾åƒæ•°æ®
- en: apply the batch training method to train on this ensemble of training images.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰¹é‡è®­ç»ƒæ–¹æ³•åº”ç”¨äºè®­ç»ƒè¿™ä¸ªè®­ç»ƒå›¾åƒé›†åˆã€‚
- en: Make Synthetic Training Images with Label
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ¶ä½œå¸¦æœ‰æ ‡ç­¾çš„åˆæˆè®­ç»ƒå›¾åƒ
- en: The following code makes random configurations of 5 points with variable slope
    and additive noise and then retains the slope linear regression model of the data
    as the label.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ç”Ÿæˆå…·æœ‰å¯å˜æ–œç‡å’ŒåŠ æ€§å™ªå£°çš„5ä¸ªç‚¹çš„éšæœºé…ç½®ï¼Œç„¶åä¿ç•™æ•°æ®çš„æ–œç‡çº¿æ€§å›å½’æ¨¡å‹ä½œä¸ºæ ‡ç­¾ã€‚
- en: The workflow inludes these steps,
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼Œ
- en: draw a random slope, \(m^{\ell} \sim U\left[-2.0,2.0\right]\)
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç”»ä¸€ä¸ªéšæœºæ–œç‡ï¼Œ\(m^{\ell} \sim U\left[-2.0,2.0\right]\)
- en: sample 5 values on this slope centered at the middle of the values
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å€¼çš„ä¸­é—´é‡‡æ ·5ä¸ªå€¼
- en: \[ Z^{\ell} = \left(x-2.5\right)*m^{\ell} \]
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z^{\ell} = \left(x-2.5\right)*m^{\ell} \]
- en: add a random errror to the values, \(\epsilon \sim U\left[-\Delta,\Delta\right]\)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘å€¼æ·»åŠ éšæœºè¯¯å·®ï¼Œ\(\epsilon \sim U\left[-\Delta,\Delta\right]\)
- en: \[ Z_{\epsilon}^{\ell} = \left(x-2.5\right)*m^{\ell} + U^{-1}\left[-\Delta,\Delta\right](p^{\ell})
    \]
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z_{\epsilon}^{\ell} = \left(x-2.5\right)*m^{\ell} + U^{-1}\left[-\Delta,\Delta\right](p^{\ell})
    \]
- en: calculate the linear regression model slope to retain as the response feature
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—çº¿æ€§å›å½’æ¨¡å‹æ–œç‡ä»¥ä¿ç•™ä½œä¸ºå“åº”ç‰¹å¾
- en: \[ m = \frac{<Z_{\epsilon}^{\ell} \cdot x>}{<x \cdot x>} \]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: \[ m = \frac{<Z_{\epsilon}^{\ell} \cdot x>}{<x \cdot x>} \]
- en: '[PRE5]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![_images/60bea9aeec98af43e38abc10f6ce1302ee8c10ff095a8bfdef8fb4231fc1be32.png](../Images/e763c68a0e61054a9b3854f895c7ec02.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![_images/60bea9aeec98af43e38abc10f6ce1302ee8c10ff095a8bfdef8fb4231fc1be32.png](../Images/e763c68a0e61054a9b3854f895c7ec02.png)'
- en: Training the Simple CNN on Many Training Images
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šè®­ç»ƒå›¾åƒä¸Šè®­ç»ƒç®€å•çš„CNN
- en: I modified the code above to loops over the batch of training images, sums the
    error gradients and updates over all the weights and biases for each training
    epoch.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¿®æ”¹äº†ä¸Šé¢çš„ä»£ç ï¼Œä½¿å…¶åœ¨è®­ç»ƒå›¾åƒæ‰¹æ¬¡ä¸Šå¾ªç¯ï¼Œç´¯åŠ è¯¯å·®æ¢¯åº¦ï¼Œå¹¶åœ¨æ¯ä¸ªè®­ç»ƒè½®æ•°æ›´æ–°æ‰€æœ‰æƒé‡å’Œåå·®ã€‚
- en: '[PRE6]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now again we can visualize the model performance,
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å†æ¬¡å¯è§†åŒ–æ¨¡å‹æ€§èƒ½ï¼Œ
- en: predictions over all training images vs. training epochs
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è®­ç»ƒå›¾åƒçš„é¢„æµ‹ä¸è®­ç»ƒè½®æ•°
- en: model weights and biases vs. training epochs
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æƒé‡å’Œåå·®ä¸è®­ç»ƒè½®æ•°
- en: '[PRE7]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![_images/ba623428124e0113a09fcf1b71bb1dacaf21bee4f296116c527e30bf0e957edb.png](../Images/e175db3feac32085e697870180f28aab.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![_images/ba623428124e0113a09fcf1b71bb1dacaf21bee4f296116c527e30bf0e957edb.png](../Images/e175db3feac32085e697870180f28aab.png)'
- en: Comments
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³¨é‡Š
- en: This was a basic treatment of convolutional neural networks. Much more could
    be done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´çš„YouTubeè®²åº§é“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚
- en: I hope this is helpful,
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­å†…è¿ˆå…‹å°”Â·çš®å°”èŒ¨æ•™æˆçš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡å…¹æ˜¯[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„Cockrellå·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨[å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡](https://www.utexas.edu/)ç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œä»¥åŠå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚'
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚'
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”å·²ç»æ’°å†™äº†è¶…è¿‡70ç¯‡[åŒè¡Œè¯„å®¡çš„å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦ã€Š[åœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ã€‹ï¼Œå¹¶ä¸”æ˜¯ä¸¤æœ¬è¿‘æœŸå‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œåˆ†åˆ«æ˜¯ã€Š[Pythonåº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPyå®è·µæŒ‡å—](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)ã€‹å’Œã€Š[Pythonåº”ç”¨æœºå™¨å­¦ä¹ ï¼šä»£ç å®è·µæŒ‡å—](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‹ã€‚
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œé™„æœ‰100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†è®°å½•å·¥ä½œæµç¨‹çš„é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHubè´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³è¦ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³è¦äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œå­¦ä¹ æœºå™¨å­¦ä¹ çš„äººæ¥è¯´æ˜¯æœ‰å¸®åŠ©çš„ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«éƒ½æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„Ÿå…´è¶£åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»åˆ°æˆ‘ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Pyrczï¼Œåšå£«ï¼ŒP.Eng. æ•™æˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡Cockrellå·¥ç¨‹å­¦é™¢å’ŒJacksonåœ°çƒç§‘å­¦å­¦é™¢
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°çƒç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
- en: Motivation
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: Convolutional neural networks are very powerful, nature inspired computing deep
    learning method based on an analogy of visual cortex extending the ability of
    our artificial neural networks to better work with images.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„åŸºäºè§†è§‰çš®å±‚ç±»æ¯”çš„è‡ªç„¶å¯å‘è®¡ç®—æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå®ƒæ‰©å±•äº†æˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œåœ¨å¤„ç†å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚
- en: Nature inspired computing is looking to nature for inspiration to develop novel
    problem-solving methods,
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶å¯å‘è®¡ç®—æ­£åœ¨å¯»æ‰¾è‡ªç„¶ç•Œçš„çµæ„Ÿæ¥å¼€å‘æ–°çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œ
- en: '**artificial neural networks** are inspired by biological neural networks'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äººå·¥ç¥ç»ç½‘ç»œ**å—åˆ°ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å¯å‘'
- en: '**nodes** in our model are artificial neurons, simple processors'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹**åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ˜¯äººå·¥ç¥ç»å…ƒï¼Œç®€å•çš„å¤„ç†å™¨'
- en: '**connections** between nodes are artificial synapses'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥æ˜¯äººå·¥çªè§¦**'
- en: '**perceptive fields** regularization to improve generalization and efficiency'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ„ŸçŸ¥åŸŸæ­£åˆ™åŒ–**ç”¨äºæé«˜æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡'
- en: intelligence emerges from many connected simple processors. For the remainder
    of this chapter, I will used the terms nodes and connections to describe our convolutional
    neural network.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: æ™ºèƒ½æ˜¯ä»è®¸å¤šç›¸äº’è¿æ¥çš„ç®€å•å¤„ç†å™¨ä¸­äº§ç”Ÿçš„ã€‚åœ¨æœ¬ç« çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ä½¿ç”¨èŠ‚ç‚¹å’Œè¿æ¥è¿™ä¸¤ä¸ªæœ¯è¯­æ¥æè¿°æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: Concepts in Common with Artificial Neural Networks
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸äººå·¥ç¥ç»ç½‘ç»œå…±æœ‰çš„æ¦‚å¿µ
- en: Here are some key aspects of artificial neural networks (ANNs),
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰çš„å…³é”®æ–¹é¢ï¼Œ
- en: '**Basic Design** - *â€œâ€¦a computing system made up of a number of simple, highly
    interconnected processing elements, which process information by their dynamic
    state response to external inputs.â€* Caudill (1989).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬è®¾è®¡** - *â€œâ€¦â€¦ä¸€ä¸ªç”±è®¸å¤šç®€å•ã€é«˜åº¦äº’è¿çš„å¤„ç†å…ƒç´ ç»„æˆçš„è®¡ç®—ç³»ç»Ÿï¼Œè¿™äº›å¤„ç†å…ƒç´ é€šè¿‡å…¶å¯¹å¤–éƒ¨è¾“å…¥çš„åŠ¨æ€çŠ¶æ€å“åº”æ¥å¤„ç†ä¿¡æ¯ã€‚â€* Caudill
    (1989)ã€‚'
- en: '**Still a Prediction Model** - while these models may be quite complicated
    with even millions of trainable model parameters, weights and biases, they are
    still a function that maps from predictor features to response features,'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»ç„¶æ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹** - è™½ç„¶è¿™äº›æ¨¡å‹å¯èƒ½éå¸¸å¤æ‚ï¼Œç”šè‡³æœ‰æ•°ç™¾ä¸‡ä¸ªå¯è®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€æƒé‡å’Œåå·®ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯ä¸€ä¸ªä»é¢„æµ‹ç‰¹å¾æ˜ å°„åˆ°å“åº”ç‰¹å¾çš„å‡½æ•°ï¼Œ'
- en: \[ Y=f(X)+\epsilon \]
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Y=f(X)+\epsilon \]
- en: '**Supervised learning** â€“ we provide training data with predictor features,
    \(X_1,\ldots,ğ‘‹_ğ‘š\) and response feature(s), \(ğ‘Œ_1,\ldots,ğ‘Œ_K\), with the expectation
    of some model prediction error, \(\epsilon\).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç›‘ç£å­¦ä¹ ** â€“ æˆ‘ä»¬æä¾›å¸¦æœ‰é¢„æµ‹ç‰¹å¾ \(X_1,\ldots,ğ‘‹_ğ‘š\) å’Œå“åº”ç‰¹å¾ \(ğ‘Œ_1,\ldots,ğ‘Œ_K\) çš„è®­ç»ƒæ•°æ®ï¼ŒæœŸæœ›æ¨¡å‹é¢„æµ‹è¯¯å·®
    \(\epsilon\)ã€‚'
- en: '**Nonlinearity** - nonlinearity is imparted to the system through the application
    of nonlinear activation functions to model nonlinear relationships'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** - é€šè¿‡åº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°æ¥æ¨¡å‹éçº¿æ€§å…³ç³»ï¼Œå°†éçº¿æ€§å¼•å…¥åˆ°ç³»ç»Ÿä¸­'
- en: '**Universal Function Approximator (Universal Approximation Theorem)** - artificial
    neural networks have the ability to learn any possible function shape of \(f\)
    over an interval, for an arbitrary wide (single hidden layer) by Cybenko (1989)
    and arbitrary depth by Lu and others (2017)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼ˆé€šç”¨é€¼è¿‘å®šç†ï¼‰** - äººå·¥ç¥ç»ç½‘ç»œå…·æœ‰å­¦ä¹  \(f\) åœ¨ä»»æ„å®½ï¼ˆå•éšè—å±‚ï¼‰åŒºé—´å†…ä»»ä½•å¯èƒ½å‡½æ•°å½¢çŠ¶çš„èƒ½åŠ›ï¼Œç”± Cybenko (1989)
    æå‡ºï¼Œå¹¶ç”± Lu å’Œå…¶ä»–äºº (2017) æå‡ºä»»æ„æ·±åº¦'
- en: For brevity, I will not repeat all the fundamental concepts from the artificial
    neural network chapter.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å°†ä¸ä¼šé‡å¤äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä¸­çš„æ‰€æœ‰åŸºæœ¬æ¦‚å¿µã€‚
- en: it may be a good idea to review that chapter before starting with this one
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è¿™ä¸€ç« ä¹‹å‰å›é¡¾ä¸€ä¸‹é‚£ä¸€ç« å¯èƒ½æ˜¯ä¸ªå¥½ä¸»æ„ã€‚
- en: Convolutional Neural Networks Concepts
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæ¦‚å¿µ
- en: '**Regularization** - artificial neural networks are prone to overfitting; therefore,
    we need a form of regularization to prevent this'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–** - äººå·¥ç¥ç»ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆï¼›å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ­£åˆ™åŒ–å½¢å¼æ¥é˜²æ­¢è¿™ç§æƒ…å†µã€‚'
- en: for example, ridge and LASSO integrate regularization into the loss function
    through the shrinkage term to reduce overfit
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå²­å›å½’å’ŒLASSOé€šè¿‡æ”¶ç¼©é¡¹å°†æ­£åˆ™åŒ–é›†æˆåˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥å‡å°‘è¿‡æ‹Ÿåˆã€‚
- en: With convolutional neural networks we take a different approach to regularization,
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯¹æ­£åˆ™åŒ–é‡‡å–äº†ä¸åŒçš„æ–¹æ³•ã€‚
- en: with image data we have an implicit hierarchy / proximity and relative position
    of pixels, flattening our 2D images into 1D vectors to pass through multiple fully
    connected artificial neural network layers would destroy this information
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å›¾åƒæ•°æ®æ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåƒç´ çš„éšå¼å±‚æ¬¡/é‚»è¿‘æ€§å’Œç›¸å¯¹ä½ç½®ï¼Œå°†æˆ‘ä»¬çš„äºŒç»´å›¾åƒå±•å¹³æˆä¸€ç»´å‘é‡ä»¥é€šè¿‡å¤šä¸ªå…¨è¿æ¥äººå·¥ç¥ç»ç½‘ç»œå±‚ä¼šç ´åè¿™äº›ä¿¡æ¯ã€‚
- en: so we remove, regularize to 0.0, all connections outside of perceptive fields
    to preserve proximity and relative position of pixels information while avoiding
    overfit
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ç§»é™¤ï¼Œæ­£åˆ™åŒ–åˆ°0.0ï¼Œæ„ŸçŸ¥åŸŸä¹‹å¤–çš„æ‰€æœ‰è¿æ¥ï¼Œä»¥ä¿ç•™åƒç´ çš„é‚»è¿‘æ€§å’Œç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼ŒåŒæ—¶é¿å…è¿‡æ‹Ÿåˆã€‚
- en: these perceptive fields are regularization through extraction of smaller pixel
    subsets and simpler patterns from the images
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›æ„ŸçŸ¥åŸŸæ˜¯é€šè¿‡ä»å›¾åƒä¸­æå–è¾ƒå°çš„åƒç´ å­é›†å’Œæ›´ç®€å•çš„æ¨¡å¼æ¥è¿›è¡Œæ­£åˆ™åŒ–çš„ã€‚
- en: Note, we also include dropout, random removal of connections, as another form
    of regularization
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬dropoutï¼ˆéšæœºç§»é™¤è¿æ¥ï¼‰ï¼Œä½œä¸ºå¦ä¸€ç§æ­£åˆ™åŒ–å½¢å¼ã€‚
- en: Image Data with Artificial Neural Networks
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œå¤„ç†å›¾åƒæ•°æ®
- en: We could use image data with our artificial neural networks.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›¾åƒæ•°æ®æ¥è®­ç»ƒæˆ‘ä»¬çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
- en: '![](../Images/a7d555024d6a9f3a61c7fd816bd8bf6a.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7d555024d6a9f3a61c7fd816bd8bf6a.png)'
- en: Illustration of artificial neural network to classify an image.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œç”¨äºåˆ†ç±»å›¾åƒçš„ç¤ºæ„å›¾ã€‚
- en: What is the issue with this approach?
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- en: '**Massive number of model parameters** - the model will likely be difficult
    to train and overfit.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤§é‡æ¨¡å‹å‚æ•°** - æ¨¡å‹å¯èƒ½éš¾ä»¥è®­ç»ƒå¹¶ä¸”å®¹æ˜“è¿‡æ‹Ÿåˆã€‚'
- en: '**Very sensitive to location** - we would like to learn from our images with
    location invariance, i.e., for the example above the location of the channels
    should not matter.'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¯¹ä½ç½®éå¸¸æ•æ„Ÿ** - æˆ‘ä»¬å¸Œæœ›ä»å…·æœ‰ä½ç½®ä¸å˜æ€§çš„å›¾åƒä¸­å­¦ä¹ ï¼Œå³å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œé€šé“çš„ä½ç½®ä¸åº”å¾ˆé‡è¦ã€‚'
- en: '**Flattening the image to a vector** - this is required by a artificial neural
    network, but if we first flatten our images to a vector we lose important information
    about the inter-pixel patterns, ordering, adjacency, etc.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å°†å›¾åƒå±•å¹³ä¸ºå‘é‡** - è¿™æ˜¯äººå·¥ç¥ç»ç½‘ç»œæ‰€å¿…éœ€çš„ï¼Œä½†å¦‚æœæˆ‘ä»¬é¦–å…ˆå°†å›¾åƒå±•å¹³ä¸ºå‘é‡ï¼Œæˆ‘ä»¬å°±ä¼šå¤±å»å…³äºåƒç´ æ¨¡å¼ã€é¡ºåºã€ç›¸é‚»æ€§ç­‰é‡è¦ä¿¡æ¯ã€‚'
- en: In summary, artificial neural networks are very inefficient for images and overfit
    and donâ€™t generalize well! Instead, letâ€™s be inspired by our visual cortex, with
    our vision we do not perceive all â€˜pixelsâ€™, instead we segment the field of view
    into receptive fields.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œäººå·¥ç¥ç»ç½‘ç»œåœ¨å¤„ç†å›¾åƒæ—¶éå¸¸ä½æ•ˆï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”æ³›åŒ–èƒ½åŠ›ä¸å¥½ï¼ç›¸åï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è§†è§‰çš®å±‚ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬çš„è§†è§‰å¹¶ä¸æ„ŸçŸ¥æ‰€æœ‰â€œåƒç´ â€ï¼Œè€Œæ˜¯å°†è§†é‡åˆ†å‰²æˆæ„ŸçŸ¥åŸŸã€‚
- en: we extraction of features of interest from overlapping receptive fields, over
    a hierarchy (not shown) and then recompose the whole image, our perception.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»é‡å çš„æ„ŸçŸ¥åŸŸä¸­æå–æ„Ÿå…´è¶£çš„ç‰¹å¾ï¼Œåœ¨ä¸€ä¸ªå±‚æ¬¡ç»“æ„ï¼ˆæœªæ˜¾ç¤ºï¼‰ä¸­ï¼Œç„¶åé‡æ–°ç»„åˆæ•´ä¸ªå›¾åƒï¼Œå³æˆ‘ä»¬çš„æ„ŸçŸ¥ã€‚
- en: we donâ€™t perceive all the â€˜pixelsâ€™ that would be exhausting for our brains,
    instead our visual cortex interprets and summarizes patterns.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šæ„ŸçŸ¥æ‰€æœ‰â€œåƒç´ â€ï¼Œè¿™å¯¹æˆ‘ä»¬çš„å¤§è„‘æ¥è¯´ä¼šéå¸¸ç´¯ï¼Œç›¸åï¼Œæˆ‘ä»¬çš„è§†è§‰çš®å±‚è§£é‡Šå¹¶æ€»ç»“æ¨¡å¼ã€‚
- en: '![](../Images/57787e99367cbd5272ad8414cd3bfe82.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57787e99367cbd5272ad8414cd3bfe82.png)'
- en: Illustration of overlapping receptive fields to break up a field of view, summarize
    each part and recombine into our perception.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å æ„ŸçŸ¥åŸŸçš„ç¤ºæ„å›¾ï¼Œç”¨äºåˆ†å‰²è§†é‡ï¼Œæ€»ç»“æ¯ä¸ªéƒ¨åˆ†ï¼Œç„¶åé‡æ–°ç»„åˆåˆ°æˆ‘ä»¬çš„æ„ŸçŸ¥ä¸­ã€‚
- en: Now letâ€™s compare artificial neural networks and convolutional neural networks
    with this concept of receptive fields.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç”¨è¿™ä¸ªæ„ŸçŸ¥åŸŸçš„æ¦‚å¿µæ¥æ¯”è¾ƒäººå·¥ç¥ç»ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: Fully Connected, Feed Forward Artificial Neural Network
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨è¿æ¥ã€å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œ
- en: nodes in the next layer are connected to all nodes of the previous layer
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€å±‚çš„èŠ‚ç‚¹è¿æ¥åˆ°å‰ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚
- en: spatial information is lost, the image data is immediately flattened to a 1D
    vector and adjacency / ordering information is lost
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºé—´ä¿¡æ¯ä¸¢å¤±ï¼Œå›¾åƒæ•°æ®ç«‹å³å±•å¹³ä¸º1Då‘é‡ï¼Œç›¸é‚»/é¡ºåºä¿¡æ¯ä¸¢å¤±ã€‚
- en: Regularized with Receptive Fields Convolutional Neural Networks
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ„Ÿå—é‡æ­£åˆ™åŒ–çš„å·ç§¯ç¥ç»ç½‘ç»œ
- en: nodes in the next layer are mapped to specific regions of the previous layer,
    an image or feature map
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå±‚ä¸­çš„èŠ‚ç‚¹æ˜ å°„åˆ°å‰ä¸€ä¸ªå±‚çš„ç‰¹å®šåŒºåŸŸï¼Œä¸€ä¸ªå›¾åƒæˆ–ç‰¹å¾å›¾
- en: spatial information is preserved, data retains the original image 2D or 3D dimensionality
    in each layer, called feature maps.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºé—´ä¿¡æ¯å¾—åˆ°ä¿ç•™ï¼Œæ•°æ®åœ¨æ¯ä¸ªå±‚ä¸­ä¿ç•™äº†åŸå§‹å›¾åƒçš„2Dæˆ–3Dç»´æ•°ï¼Œç§°ä¸ºç‰¹å¾å›¾ã€‚
- en: '![](../Images/b56850a29d24dbccae1cabcfbdc9e7e7.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b56850a29d24dbccae1cabcfbdc9e7e7.png)'
- en: Illustration of overlapping receptive fields to break up a field of view, summarize
    each part and recombine into our perception.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºäº†é‡å çš„æ„Ÿå—é‡æ¥åˆ†å‰²è§†é‡ï¼Œæ€»ç»“æ¯ä¸ªéƒ¨åˆ†å¹¶å°†å®ƒä»¬é‡æ–°ç»„åˆåˆ°æˆ‘ä»¬çš„æ„ŸçŸ¥ä¸­ã€‚
- en: Regularization for CNNs
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNçš„æ­£åˆ™åŒ–
- en: Letâ€™s do a quick recall on the concept of regularization for predictive machine
    learning models,
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹æ­£åˆ™åŒ–çš„æ¦‚å¿µï¼Œ
- en: '**Regularization** - a constraint to reduce the sensitivity of the model to
    the data, i.e., to reduce model variance'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£åˆ™åŒ–** - ä¸€ç§çº¦æŸï¼Œç”¨äºå‡å°‘æ¨¡å‹å¯¹æ•°æ®çš„æ•æ„Ÿæ€§ï¼Œå³å‡å°‘æ¨¡å‹æ–¹å·®'
- en: '**Regularization with Receptive Fields** - the use of receptive fields is a
    form of regularization, resulting in,'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºäºæ„Ÿå—é‡çš„æ­£åˆ™åŒ–** - ä½¿ç”¨æ„Ÿå—é‡æ˜¯ä¸€ç§æ­£åˆ™åŒ–å½¢å¼ï¼Œå¯¼è‡´ï¼Œ'
- en: massive reduction in connections, weights / model parameters
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿æ¥ã€æƒé‡/æ¨¡å‹å‚æ•°çš„å¤§é‡å‡å°‘
- en: effectively shrinking these potential weights to zero
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆåœ°å°†è¿™äº›æ½œåœ¨æƒé‡ç¼©å°åˆ°é›¶
- en: While integrating / focusing on pixel patterns!
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶å…³æ³¨åƒç´ æ¨¡å¼ï¼
- en: '**Regularization with Dropout** - during training epochs, randomly ignore or
    â€œdrop outâ€ a proportion of nodes. Each training epoch sees a different version
    / subset of the network'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºäºdropoutçš„æ­£åˆ™åŒ–** - åœ¨è®­ç»ƒå‘¨æœŸä¸­ï¼Œéšæœºå¿½ç•¥æˆ–â€œdropoutâ€ä¸€éƒ¨åˆ†èŠ‚ç‚¹ã€‚æ¯ä¸ªè®­ç»ƒå‘¨æœŸçœ‹åˆ°ç½‘ç»œçš„ä¸åŒç‰ˆæœ¬/å­é›†'
- en: an additional form of regularization for CNN to prevent specific nodes from
    dominating the model
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNçš„å¦ä¸€ç§æ­£åˆ™åŒ–å½¢å¼ï¼Œç”¨äºé˜²æ­¢ç‰¹å®šèŠ‚ç‚¹ä¸»å¯¼æ¨¡å‹
- en: simulates training multiple models and averaging like ensemble learning. Note,
    it is generally not feasible to train multiple networks in parallel to apply the
    ensemble to calculate the prediction (like random forest) and after training all
    nodes are used.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿè®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶å¹³å‡ï¼Œç±»ä¼¼äºé›†æˆå­¦ä¹ ã€‚æ³¨æ„ï¼Œé€šå¸¸ä¸å¯èƒ½å¹¶è¡Œè®­ç»ƒå¤šä¸ªç½‘ç»œä»¥åº”ç”¨é›†æˆæ¥è®¡ç®—é¢„æµ‹ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒåæ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«ä½¿ç”¨ã€‚
- en: '![](../Images/2078f41aac92afeb57f8fdc28528b873.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2078f41aac92afeb57f8fdc28528b873.png)'
- en: Illustration of dropout to remove random connects from receptive field to nodes
    in the feature map.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºäº†dropoutå¦‚ä½•ä»æ„Ÿå—é‡åˆ°ç‰¹å¾å›¾èŠ‚ç‚¹ä¸­ç§»é™¤éšæœºè¿æ¥ã€‚
- en: '**Batch Normalization** - standardize the nodesâ€™ inputs / weights over a layer
    to center and rescale (mean of 0 and variance of 1) to optimize activation function
    sensitivity and model parameter training.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡å½’ä¸€åŒ–** - åœ¨å±‚ä¸Šæ ‡å‡†åŒ–èŠ‚ç‚¹çš„è¾“å…¥/æƒé‡ï¼Œä»¥å±…ä¸­å’Œç¼©æ”¾ï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ï¼Œä»¥ä¼˜åŒ–æ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§å’Œæ¨¡å‹å‚æ•°è®­ç»ƒã€‚'
- en: \[ \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \]
- en: Then we add 2 model parameters, layer standard deviation, \(\gamma\), and mean,
    \(\beta\), to add control to improve the optimality of the train individual weights
    in the next layer.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æ·»åŠ 2ä¸ªæ¨¡å‹å‚æ•°ï¼Œå±‚æ ‡å‡†å·®ï¼Œ\(\gamma\)ï¼Œå’Œå‡å€¼ï¼Œ\(\beta\)ï¼Œä»¥å¢åŠ æ§åˆ¶ï¼Œæé«˜ä¸‹ä¸€å±‚è®­ç»ƒå•ä¸ªæƒé‡çš„æœ€ä¼˜æ€§ã€‚
- en: \[ y_i \leftarrow \gamma \hat{x}_i + \beta \]
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_i \leftarrow \gamma \hat{x}_i + \beta \]
- en: Building Blocks for CNNs
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNçš„æ„å»ºæ¨¡å—
- en: We have various operators to move from layer to layer (feature maps to feature
    maps) in our convolutional neural networks. The common operators include,
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰å¤šç§ç®—å­å¯ä»¥åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ä»ä¸€å±‚ç§»åŠ¨åˆ°å¦ä¸€å±‚ï¼ˆç‰¹å¾å›¾åˆ°ç‰¹å¾å›¾ï¼‰ã€‚å¸¸è§çš„ç®—å­åŒ…æ‹¬ï¼Œ
- en: '**Convolution** â€“ a weighting window, kernel / filter designed to extract spatial
    information'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å·ç§¯** â€“ ä¸€ä¸ªåŠ æƒçª—å£ï¼Œæ ¸/æ»¤æ³¢å™¨ï¼Œç”¨äºæå–ç©ºé—´ä¿¡æ¯'
- en: '**Pooling** â€“ reduction in dimensionality, increase local translation invariance'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ± åŒ–** â€“ ç»´åº¦å‡å°‘ï¼Œå¢åŠ å±€éƒ¨å¹³ç§»ä¸å˜æ€§'
- en: '**Depth-wise Pooling, Down Sampling** â€“ 1x1 filter that combine channels /
    feature maps to learn over multiple kernels'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ·±åº¦æ± åŒ–ï¼Œä¸‹é‡‡æ ·** â€“ 1x1æ»¤æ³¢å™¨ï¼Œå°†é€šé“/ç‰¹å¾å›¾ç»„åˆåˆ°å¤šä¸ªæ ¸ä¸Šå­¦ä¹ '
- en: '**Activation** â€“ use of an activation function to apply a nonlinear transformation
    to impart nonlinearity to the system and to prevent collapse of the system to
    a simple linear model'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¿€æ´»** â€“ ä½¿ç”¨æ¿€æ´»å‡½æ•°å¯¹ç³»ç»Ÿåº”ç”¨éçº¿æ€§å˜æ¢ï¼Œèµ‹äºˆç³»ç»Ÿéçº¿æ€§ï¼Œå¹¶é˜²æ­¢ç³»ç»Ÿç®€åŒ–ä¸ºç®€å•çš„çº¿æ€§æ¨¡å‹'
- en: '**Full-connected, Feed Forward** â€“ see previous lecture on artificial neural
    networks'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…¨è¿æ¥ï¼Œå‰é¦ˆ** â€“ å‚è§ä¹‹å‰å…³äºäººå·¥ç¥ç»ç½‘ç»œçš„è®²åº§'
- en: Now we will describe, interpret and demonstrate these operators.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†æè¿°ã€è§£é‡Šå¹¶æ¼”ç¤ºè¿™äº›ç®—å­ã€‚
- en: Convolution
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å·ç§¯
- en: Convolution is the integral product of two functions, after one is reversed
    and shifted by \(\Delta\).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ˜¯ä¸¤ä¸ªå‡½æ•°çš„ç§¯åˆ†ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸€ä¸ªå‡½æ•°è¢«åè½¬å¹¶å¹³ç§» \(\Delta\)ã€‚
- en: one interpretation is smoothing a function with weighting function, \(ğ‘“(\Delta)\),
    is applied to calculate the weighted average of function, \(ğ‘”(x)\),
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç§è§£é‡Šæ˜¯å°†åŠ æƒå‡½æ•° \(ğ‘“(\Delta)\) åº”ç”¨äºå¹³æ»‘å‡½æ•°ï¼Œä»¥è®¡ç®—å‡½æ•° \(ğ‘”(x)\) çš„åŠ æƒå¹³å‡å€¼ï¼Œ
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
- en: this easily extends into any dimensionality, for example 2D for images,
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆå®¹æ˜“æ‰©å±•åˆ°ä»»ä½•ç»´åº¦ï¼Œä¾‹å¦‚å¯¹äºå›¾åƒæ˜¯2Dï¼Œ
- en: \[ (f * g)(x, y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x,
    \Delta_y) g(x - \Delta_x, y - \Delta_y) \, d\Delta_x \, d\Delta_y \]
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x, y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(\Delta_x,
    \Delta_y) g(x - \Delta_x, y - \Delta_y) \, d\Delta_x \, d\Delta_y \]
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity,
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç§¯åˆ†ä¹‹å‰é€‰æ‹©å“ªä¸ªå‡½æ•°è¢«å¹³ç§»ï¼Œä¸ä¼šæ”¹å˜ç»“æœï¼Œå·ç§¯ç®—å­å…·æœ‰äº¤æ¢æ€§ï¼Œ
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä»»ä¸€å‡½æ•°è¢«åå°„ï¼Œåˆ™å·ç§¯ç­‰åŒäºç›¸å…³å·ç§¯ï¼Œå®ƒæ˜¯ä¸¤ä¸ªä¿¡å·ç›¸ä¼¼åº¦çš„åº¦é‡ï¼Œä½œä¸ºä½ç§»çš„å‡½æ•°ã€‚
- en: To demonstrate convolution with an exhaustive \(g(x)\) and sparsely sampled
    \(g(x)\) I built out an [interactive Python convolution dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb),
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¼”ç¤ºä½¿ç”¨è¯¦å°½ \(g(x)\) å’Œç¨€ç–é‡‡æ · \(g(x)\) çš„å·ç§¯ï¼Œæˆ‘æ„å»ºäº†ä¸€ä¸ª[äº¤äº’å¼Pythonå·ç§¯ä»ªè¡¨æ¿](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Convolution_kNearest.ipynb)ï¼Œ
- en: '![](../Images/f55c37e0f7da98a233affd5fbd5ba38c.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f55c37e0f7da98a233affd5fbd5ba38c.png)'
- en: Interactive Python dashboard to demonstrate convolution.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤äº’å¼Pythonä»ªè¡¨æ¿ç”¨äºæ¼”ç¤ºå·ç§¯ã€‚
- en: For convolution operations, a trainable weighting function, \(g(\Tau)\), is
    learned during model training.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå·ç§¯æ“ä½œï¼Œåœ¨æ¨¡å‹è®­ç»ƒæœŸé—´å­¦ä¹ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„åŠ æƒå‡½æ•° \(g(\Tau)\)ã€‚
- en: '**Filter/Kernel** â€“ the weights assigned over the convolution window to calculate
    the next feature map. By training the weights the filter(s) may extract specific
    spatial features.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ»¤æ³¢å™¨/æ ¸** â€“ åœ¨å·ç§¯çª—å£ä¸Šåˆ†é…çš„æƒé‡ï¼Œç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªç‰¹å¾å›¾ã€‚é€šè¿‡è®­ç»ƒè¿™äº›æƒé‡ï¼Œæ»¤æ³¢å™¨å¯ä»¥æå–ç‰¹å®šçš„ç©ºé—´ç‰¹å¾ã€‚'
- en: '![](../Images/1955083a83351548602c54c6f2f6d86b.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1955083a83351548602c54c6f2f6d86b.png)'
- en: Two examples of convolution calculations. .
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå·ç§¯è®¡ç®—çš„ç¤ºä¾‹ã€‚
- en: Letâ€™s look at a specific kernel form, the blur filter.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç‰¹å®šçš„æ ¸å½¢å¼ï¼Œå³æ¨¡ç³Šæ»¤æ³¢å™¨ã€‚
- en: the next feature map receives the local averages over the previous feature map
    or image.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªç‰¹å¾å›¾æ¥æ”¶å‰ä¸€ä¸ªç‰¹å¾å›¾æˆ–å›¾åƒä¸Šçš„å±€éƒ¨å¹³å‡å€¼ã€‚
- en: '![](../Images/f41a6a56aa0cf6c53667cad2b0ae52c8.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f41a6a56aa0cf6c53667cad2b0ae52c8.png)'
- en: Example filter, the blur / local average filter.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ»¤æ³¢å™¨ï¼Œæ¨¡ç³Š/å±€éƒ¨å¹³å‡æ»¤æ³¢å™¨ã€‚
- en: Some observations about the filters,
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ»¤æ³¢å™¨çš„ä¸€äº›è§‚å¯Ÿï¼Œ
- en: size of the filter is related to the scale of the features that we are extracting
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ»¤æ³¢å™¨çš„å¤§å°ä¸æˆ‘ä»¬æå–çš„ç‰¹å¾çš„å°ºåº¦ç›¸å…³
- en: larger kernels increase the number of connections, model weights and ultimately
    the computational complexity
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾ƒå¤§çš„æ ¸å¢åŠ äº†è¿æ¥æ•°ã€æ¨¡å‹æƒé‡ï¼Œæœ€ç»ˆå¢åŠ äº†è®¡ç®—å¤æ‚åº¦
- en: odd numbers for kernel size to avoid distortion, asymmetric kernels are possible
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸å¤§å°ä½¿ç”¨å¥‡æ•°ä»¥é¿å…å¤±çœŸï¼Œå¯èƒ½å­˜åœ¨éå¯¹ç§°æ ¸
- en: sum to one prevent bias (shifting in the mean from one image or feature map
    to another feature map
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç´¯åŠ åˆ°1å¯ä»¥é˜²æ­¢åå·®ï¼ˆä»ä¸€å¹…å›¾åƒæˆ–ç‰¹å¾å›¾åˆ°å¦ä¸€å¹…ç‰¹å¾å›¾çš„å‡å€¼å¹³ç§»ï¼‰
- en: '**Padding** - our next feature map has a reduced size by 1 on all the edges
    to avoid the overlapping outside the feature map, i.e., no padding, while padding
    extrapolates outside the feature map or image, preventing reduction in size of
    the next feature map.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¡«å……** - æˆ‘ä»¬ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„æ‰€æœ‰è¾¹ç¼˜éƒ½å‡å°‘äº†1çš„å¤§å°ï¼Œä»¥é¿å…ç‰¹å¾å›¾å¤–çš„é‡å ï¼Œå³æ²¡æœ‰å¡«å……ï¼Œè€Œå¡«å……å°†è¶…å‡ºç‰¹å¾å›¾æˆ–å›¾åƒï¼Œé˜²æ­¢ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„å¤§å°å‡å°‘ã€‚'
- en: there are varirous methods for padding, including assuming zero, constant value,
    and nearest value in feature map.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¡«å……æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼ŒåŒ…æ‹¬åœ¨ç‰¹å¾å›¾ä¸Šå‡è®¾é›¶å€¼ã€å¸¸æ•°å€¼å’Œæœ€è¿‘å€¼ã€‚
- en: '![](../Images/58368b8a8cebacdde3ae4116d6462f35.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58368b8a8cebacdde3ae4116d6462f35.png)'
- en: Convolution without padding, resulting in feature map size reduction (above)
    and convolution with padding (assuming 0 outside the feature map) for no feature
    map size reduction (below).
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: æ— å¡«å……çš„å·ç§¯ï¼Œå¯¼è‡´ç‰¹å¾å›¾å¤§å°å‡å°ï¼ˆä¸Šæ–¹ï¼‰å’Œå¡«å……å·ç§¯ï¼ˆå‡è®¾ç‰¹å¾å›¾å¤–éƒ¨ä¸º0ï¼‰ä»¥æ— ç‰¹å¾å›¾å¤§å°å‡å°ï¼ˆä¸‹æ–¹ï¼‰ã€‚
- en: '**Stride** - the steps of the convolution filter / kernel through the previous
    feature map.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥é•¿** - å·ç§¯æ»¤æ³¢å™¨/æ ¸é€šè¿‡å‰ä¸€ä¸ªç‰¹å¾å›¾çš„æ­¥æ•°ã€‚'
- en: for a stride of 1 there is no implicit reduction in feature map size.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ­¥é•¿ä¸º1çš„æƒ…å†µï¼Œæ²¡æœ‰éšå¼å‡å°‘ç‰¹å¾å›¾å¤§å°ã€‚
- en: for a stride of > 2 there is a reduction in feature map size.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ­¥é•¿å¤§äº2çš„æƒ…å†µï¼Œç‰¹å¾å›¾å¤§å°ä¼šå‡å°ã€‚
- en: '![](../Images/58300f18f8744327620c90fcc1cc392f.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/58300f18f8744327620c90fcc1cc392f.png)'
- en: Convolution with stride of 1 (above) and convolution with a stride of 2 (below)
    with a reduction in feature map size.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥é•¿ä¸º1çš„å·ç§¯ï¼ˆä¸Šæ–¹ï¼‰å’Œæ­¥é•¿ä¸º2çš„å·ç§¯ï¼ˆä¸‹æ–¹ï¼‰ä»¥åŠç‰¹å¾å›¾å¤§å°çš„å‡å°ã€‚
- en: '**Size of Next Feature Map** - the next feature map size is determined by the
    hyperparameters, previous feature map size, \(n_{in}\), convolution kernel size,
    \(k\), convolution padding size, \(p\) and convolution stride size, \(s\), by
    this equation,'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„å¤§å°** - ä¸‹ä¸€ä¸ªç‰¹å¾å›¾çš„å¤§å°ç”±è¶…å‚æ•°ã€å‰ä¸€ä¸ªç‰¹å¾å›¾å¤§å° \(n_{in}\)ã€å·ç§¯æ ¸å¤§å° \(k\)ã€å·ç§¯å¡«å……å¤§å° \(p\)
    å’Œå·ç§¯æ­¥é•¿å¤§å° \(s\) å†³å®šï¼Œé€šè¿‡æ­¤æ–¹ç¨‹ï¼Œ'
- en: \[ n_{\text{out}} = \left\lfloor \frac{n_{\text{in}} + 2p - k}{s} \right\rfloor
    + 1 \]
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{n_{\text{in}} + 2p - k}{s} \right\rfloor
    + 1 \]
- en: For example, if,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœï¼Œ
- en: \(n_{in} = 4\)
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(n_{in} = 4\)
- en: \(k = 2\)
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(k = 2\)
- en: \(p = 0\)
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(p = 0\)
- en: \(s = 1\)
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(s = 1\)
- en: Then we can substitute into the equation,
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä»£å…¥æ–¹ç¨‹ä¸­ï¼Œ
- en: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
- en: Now compare this to a visualization of this example,
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†æ­¤ä¸è¯¥ç¤ºä¾‹çš„å¯è§†åŒ–è¿›è¡Œæ¯”è¾ƒï¼Œ
- en: '![](../Images/ad9c65a10c9c4afb28b77bd7cbfc6017.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ad9c65a10c9c4afb28b77bd7cbfc6017.png)'
- en: Example kernel, input and output feature maps for the example above.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç¤ºä¾‹çš„ç¤ºä¾‹å†…æ ¸ã€è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾ã€‚
- en: 'We calculate the next feature map size as:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¡ç®—ä¸‹ä¸€ä¸ªç‰¹å¾å›¾å¤§å°å¦‚ä¸‹ï¼š
- en: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{\text{out}} = \left\lfloor \frac{4 + 2(0) - 2}{1} \right\rfloor + 1 =
    3 \]
- en: '**Filter / Kernel Design** â€“ by training the weights the filter may extract
    specific features. Consider these example filter types with simple 1D examples
    of input and output feature â€˜mapsâ€™.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ»¤æ³¢å™¨/æ ¸è®¾è®¡** â€“ é€šè¿‡è®­ç»ƒæƒé‡ï¼Œæ»¤æ³¢å™¨å¯ä»¥æå–ç‰¹å®šçš„ç‰¹å¾ã€‚è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹æ»¤æ³¢å™¨ç±»å‹ï¼Œä»¥åŠè¾“å…¥å’Œè¾“å‡ºç‰¹å¾â€˜å›¾â€™çš„ç®€å•1Dç¤ºä¾‹ã€‚'
- en: '![](../Images/363089e3910cc726998fcf261fba68ef.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/363089e3910cc726998fcf261fba68ef.png)'
- en: Filters (above, and 1D illustrations of convolution, original function (black)
    and convolution (red) (below).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: æ»¤æ³¢å™¨ï¼ˆä¸Šæ–¹ï¼Œä»¥åŠå·ç§¯çš„1Dç¤ºæ„å›¾ï¼ŒåŸå§‹å‡½æ•°ï¼ˆé»‘è‰²ï¼‰å’Œå·ç§¯ï¼ˆçº¢è‰²ï¼‰ï¼ˆä¸‹æ–¹ï¼‰ã€‚
- en: '**Multiple Filters / Kernels** - typically multiple filters, \(n_k\), are trained
    on each convolutional layer to extract various structures from the image.'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šä¸ªæ»¤æ³¢å™¨/æ ¸** - é€šå¸¸åœ¨æ¯ä¸ªå·ç§¯å±‚ä¸Šè®­ç»ƒå¤šä¸ªæ»¤æ³¢å™¨ \(n_k\)ï¼Œä»¥ä»å›¾åƒä¸­æå–å„ç§ç»“æ„ã€‚'
- en: this increases feature map depth or channels, \(n_k\)
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å¢åŠ äº†ç‰¹å¾å›¾æ·±åº¦æˆ–é€šé“æ•°ï¼Œ\(n_k\)
- en: '![](../Images/401b5ed57a2efd08be6f224d335f76da.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/401b5ed57a2efd08be6f224d335f76da.png)'
- en: Feature map has a depth, also known as number of channels, $n_k$ due to application
    of more than 1 filter to the previous image or feature map. .
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåº”ç”¨äº†å¤šä¸ªæ»¤æ³¢å™¨åˆ°å‰ä¸€ä¸ªå›¾åƒæˆ–ç‰¹å¾å›¾ï¼Œç‰¹å¾å›¾å…·æœ‰æ·±åº¦ï¼Œä¹Ÿç§°ä¸ºé€šé“æ•°ï¼Œ\(n_k\)ã€‚
- en: note, the original image may have multiple channels, depth > 1, for example,
    a RGB image with 3 channels, one for each red, blue and green.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼ŒåŸå§‹å›¾åƒå¯èƒ½å…·æœ‰å¤šä¸ªé€šé“ï¼Œæ·±åº¦å¤§äº1ï¼Œä¾‹å¦‚ï¼Œä¸€ä¸ªRGBå›¾åƒå…·æœ‰3ä¸ªé€šé“ï¼Œæ¯ä¸ªé€šé“åˆ†åˆ«å¯¹åº”çº¢è‰²ã€è“è‰²å’Œç»¿è‰²ã€‚
- en: Finally here are some examples of convolution filters applied to an image of
    a brick wall on The University of Texas at Austin, codes are available at [SubsurfaceDataAnalytics_Convolution_Operators.ipynb.](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Convolution_Operators.ipynb).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè¿™é‡Œæœ‰ä¸€äº›å°†å·ç§¯æ»¤æ³¢å™¨åº”ç”¨äºå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡çš„ç –å¢™å›¾åƒçš„ç¤ºä¾‹ï¼Œä»£ç å¯åœ¨[SubsurfaceDataAnalytics_Convolution_Operators.ipynb](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Convolution_Operators.ipynb)æ‰¾åˆ°ã€‚
- en: '![](../Images/48b85ce0016245754f632d2d7034335d.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/48b85ce0016245754f632d2d7034335d.png)'
- en: Convolution examples from a brick wall on the campus of The University of Texas
    at Austin. .
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ç –å¢™çš„å·ç§¯ç¤ºä¾‹ã€‚
- en: Activation Functions
  id: totrans-509
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: See the artificial neural network chapter for more details, but for a reminder
    considerations for selecting activation functions,
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜…äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œä½†ä¸ºäº†æé†’é€‰æ‹©æ¿€æ´»å‡½æ•°çš„è€ƒè™‘å› ç´ ï¼Œ
- en: '**Nonlinear** â€“ required to impose nonlinearity into the predictor. Proved
    to be a universal function approximator if at least 1 hidden layer (Cybenko, 1989).'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éçº¿æ€§** â€“ éœ€è¦å‘é¢„æµ‹å™¨æ–½åŠ éçº¿æ€§ã€‚å¦‚æœè‡³å°‘æœ‰1ä¸ªéšè—å±‚ï¼ˆCybenkoï¼Œ1989å¹´ï¼‰ï¼Œå·²è¢«è¯æ˜æ˜¯é€šç”¨çš„å‡½æ•°é€¼è¿‘å™¨ã€‚'
- en: '**Range** â€“ finite for more stability gradient-based learning, infinite for
    more efficient training (but requires a slower learning rate)'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŒƒå›´** â€“ å¯¹äºæ›´ç¨³å®šçš„åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ˜¯æœ‰é™çš„ï¼Œå¯¹äºæ›´æœ‰æ•ˆçš„è®­ç»ƒæ˜¯æ— é™çš„ï¼ˆä½†éœ€è¦è¾ƒæ…¢çš„å­¦ä¹ ç‡ï¼‰'
- en: '**Continuously Differentiable** â€“ required for stable gradient-based optimization'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿ç»­å¯å¾®** â€“ å¯¹äºåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ˜¯å¿…éœ€çš„'
- en: '**Smooth functions with Monotonic Derivative** â€“ may generalize better'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…·æœ‰å•è°ƒå¯¼æ•°çš„å¹³æ»‘å‡½æ•°** â€“ å¯èƒ½å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›'
- en: '**Monotonic** â€“ guaranteed convexity of error surface of a single layer model
    (global minimum for loss function)'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å•è°ƒæ€§** â€“ ä¿è¯å•å±‚æ¨¡å‹è¯¯å·®è¡¨é¢çš„å‡¸æ€§ï¼ˆæŸå¤±å‡½æ•°çš„å…¨å±€æœ€å°å€¼ï¼‰'
- en: '**Approximates Identity at the Origin** â€“ well learn efficiently with the weights
    initialized with small random values'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœ¨åŸç‚¹è¿‘ä¼¼ä¸ºå•ä½çŸ©é˜µ** â€“ ä½¿ç”¨å°éšæœºå€¼åˆå§‹åŒ–æƒé‡å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ '
- en: '![](../Images/cb1d19a7978ea9f1f58b12b229835dcc.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/cb1d19a7978ea9f1f58b12b229835dcc.png)'
- en: Convolution examples from a brick wall on the campus of The University of Texas
    at Austin. .
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ç –å¢™çš„å·ç§¯ç¤ºä¾‹ã€‚
- en: Pooling
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ± åŒ–
- en: Summarization over a filter / kernel with a single value. The impact of pooling
    includes,
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å•ä¸ªå€¼è¿›è¡Œæ»¤æ³¢å™¨/æ ¸çš„æ±‡æ€»ã€‚æ± åŒ–çš„å½±å“åŒ…æ‹¬ï¼Œ
- en: down sample the detection of features in feature maps
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾å›¾ä¸­å¯¹ç‰¹å¾æ£€æµ‹è¿›è¡Œä¸‹é‡‡æ ·
- en: reduces the dimensionality of the feature map
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘äº†ç‰¹å¾å›¾çš„ç»´åº¦
- en: integrate translation invariance, pattern detection insensitive to location
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•´åˆå¹³ç§»ä¸å˜æ€§ï¼Œå¯¹ä½ç½®ä¸æ•æ„Ÿçš„æ¨¡å¼æ£€æµ‹
- en: Two common pooling methods are average and max pooling.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¸¸è§çš„æ± åŒ–æ–¹æ³•æ˜¯å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ã€‚
- en: '![](../Images/4b2fd11462cbb82e0cb09ac3e2d938ff.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/4b2fd11462cbb82e0cb09ac3e2d938ff.png)'
- en: Schematic of pooling operation.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: æ± åŒ–æ“ä½œçš„ç¤ºæ„å›¾ã€‚
- en: Hereâ€™s an example of pooling with a 2x2 filter and a stride of 2.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªä½¿ç”¨2x2æ»¤æ³¢å™¨å’Œæ­¥é•¿ä¸º2çš„æ± åŒ–ç¤ºä¾‹ã€‚
- en: '![](../Images/81532b0b04b5a5af52f9b81557967b34.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/81532b0b04b5a5af52f9b81557967b34.png)'
- en: Example of pooling operation for reducing the extent of the feature map.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘ç‰¹å¾å›¾èŒƒå›´çš„æ± åŒ–æ“ä½œç¤ºä¾‹ã€‚
- en: with 2 x 2 filter, stride of 2 the dimension is reduced Â½ per axis, Â¼ for 2D
    images.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨2x2æ»¤æ³¢å™¨ï¼Œæ­¥é•¿ä¸º2ï¼Œæ¯ä¸ªè½´çš„ç»´åº¦å‡å°‘ä¸€åŠï¼Œå¯¹äº2Då›¾åƒä¸ºå››åˆ†ä¹‹ä¸€ã€‚
- en: for example, the value of 16 is location independent within the filter. This
    introduces translational invariance.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ16çš„å€¼åœ¨æ»¤æ³¢å™¨å†…æ˜¯ä½ç½®æ— å…³çš„ã€‚è¿™å¼•å…¥äº†å¹³ç§»ä¸å˜æ€§ã€‚
- en: '![](../Images/a3f5e46b161a0d2f7e0f99574f3db593.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/a3f5e46b161a0d2f7e0f99574f3db593.png)'
- en: Max and average pooling examples from a brick wall on the campus of The University
    of Texas at Austin..
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡æ ¡å›­ç –å¢™çš„æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–ç¤ºä¾‹ã€‚
- en: Depth-wise Pooling
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±åº¦æ± åŒ–
- en: Summarization over feature maps with a single value.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•ä¸ªå€¼å¯¹ç‰¹å¾å›¾è¿›è¡Œæ±‡æ€»ã€‚
- en: down sample the detection over feature maps
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾å›¾ä¸Šå¯¹æ£€æµ‹è¿›è¡Œä¸‹é‡‡æ ·ã€‚
- en: combine information learned from multiple kernels
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“åˆä»å¤šä¸ªæ ¸å­¦ä¹ åˆ°çš„ä¿¡æ¯
- en: reduces the depth of the feature map, next layer
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘äº†ç‰¹å¾å›¾çš„æ·±åº¦ï¼Œä¸‹ä¸€å±‚ã€‚
- en: Two common pooling methods are average and max pooling.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¸¸è§çš„æ± åŒ–æ–¹æ³•æ˜¯å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–ã€‚
- en: '![](../Images/1786e9cb04c2dac2297ddc607d4b0649.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/1786e9cb04c2dac2297ddc607d4b0649.png)'
- en: Schematic of depthwise pooling operation.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦æ± åŒ–æ“ä½œçš„ç¤ºæ„å›¾ã€‚
- en: Common CNN Architecture
  id: totrans-542
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¸¸è§çš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„
- en: The following is a common workflow for convolutional neural networks,
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¸¸è§çš„å·ç§¯ç¥ç»ç½‘ç»œå·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/fcb53216a4534c9da9053f990e48aa90.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/fcb53216a4534c9da9053f990e48aa90.png)'
- en: The common CNN workflow.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„å·ç§¯ç¥ç»ç½‘ç»œå·¥ä½œæµç¨‹ã€‚
- en: We can illustrate this common workflow with an illustration of the common architecture.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å¸¸è§æ¶æ„çš„ç¤ºæ„å›¾æ¥å±•ç¤ºè¿™ä¸ªå¸¸è§çš„å·¥ä½œæµç¨‹ã€‚
- en: '![](../Images/618c123a572e16a0e12819e75b283099.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/618c123a572e16a0e12819e75b283099.png)'
- en: The common CNN architecture.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¸è§çš„å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚
- en: By-Hand CNN Architecture
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œæ¶æ„
- en: Just like the artificial neural network chapter, I build a simple convolutional
    neural network by-hand.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒäººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä¸€æ ·ï¼Œæˆ‘æ‰‹åŠ¨æ„å»ºäº†ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: For simplicity and brevity, I have made the following architectural choices,
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€å•å’Œç®€æ´ï¼Œæˆ‘åšå‡ºäº†ä»¥ä¸‹æ¶æ„é€‰æ‹©ï¼Œ
- en: 1D images with height of 5
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜åº¦ä¸º5çš„ä¸€ç»´å›¾åƒ
- en: 1 convolutional layer with 1 kernel of size 3
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ä¸ªå·ç§¯å±‚ï¼Œ1ä¸ªå¤§å°ä¸º3çš„æ ¸
- en: activation with sigmoid
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨sigmoidæ¿€æ´»
- en: stride of 1 and no padding so the feature map has a size of 3
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­¥é•¿ä¸º1ä¸”æ²¡æœ‰å¡«å……ï¼Œå› æ­¤ç‰¹å¾å›¾çš„å¤§å°ä¸º3ã€‚
- en: artificial neural network from feature map immediately to output node with linear
    activation
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»ç‰¹å¾å›¾ç›´æ¥åˆ°è¾“å‡ºèŠ‚ç‚¹çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œå…·æœ‰çº¿æ€§æ¿€æ´»
- en: This minimalist architecture demonstrates many of the salient concepts for convolutional
    neural networks while being very easy to visualize and very fast to train.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç®€çº¦æ¶æ„å±•ç¤ºäº†å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è®¸å¤šå…³é”®æ¦‚å¿µï¼ŒåŒæ—¶éå¸¸æ˜“äºå¯è§†åŒ–ï¼Œä¸”è®­ç»ƒé€Ÿåº¦éå¸¸å¿«ã€‚
- en: '![](../Images/c18c0d3da8b925d617dbd7cad6b9d1fa.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c18c0d3da8b925d617dbd7cad6b9d1fa.png)'
- en: Schematic illustration of our by-hand CNN.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºçš„CNNçš„ç¤ºæ„å›¾ã€‚
- en: Now, we convert this schematic of our by-hand CNN to a diagram of the actual
    nodes.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºçš„CNNçš„ç¤ºæ„å›¾è½¬æ¢ä¸ºå®é™…èŠ‚ç‚¹çš„å›¾ã€‚
- en: '![](../Images/aaaec452d8f375bf00b4df41c281e46c.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aaaec452d8f375bf00b4df41c281e46c.png)'
- en: Architecture of our by-hand CNN.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºçš„CNNæ¶æ„ã€‚
- en: To demonstrate the practicality of working with this architecture, letâ€™s add
    the labels of the model parameters,
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºä½¿ç”¨æ­¤æ¶æ„çš„å®ç”¨æ€§ï¼Œè®©æˆ‘ä»¬æ·»åŠ æ¨¡å‹å‚æ•°çš„æ ‡ç­¾ï¼Œ
- en: kernel weights, \(\lambda_6\), \(\lambda_7\), and \(\lambda_8\)
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¸æƒé‡ï¼Œ\(\lambda_6\)ï¼Œ\(\lambda_7\)ï¼Œå’Œ \(\lambda_8\)
- en: kernel bias, \(b_{conv}\)
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·ç§¯æ ¸åç½®ï¼Œ\(b_{conv}\)
- en: artificial neural network weights, \(\lambda_{9,12}\), \(\lambda_{10,12}\),
    and \(\lambda_{11,12}\)
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œæƒé‡ï¼Œ\(\lambda_{9,12}\)ï¼Œ\(\lambda_{10,12}\)ï¼Œå’Œ \(\lambda_{11,12}\)
- en: artificial neural network bias, \(b_{12}\)
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äººå·¥ç¥ç»ç½‘ç»œåç½®ï¼Œ\(b_{12}\)
- en: '![](../Images/11f0651d909bf7e40e4ebdeca86bab61.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11f0651d909bf7e40e4ebdeca86bab61.png)'
- en: Architecture of our by-hand CNN with all trainable model parameters. Note, kernel
    weights are only shown in the first kernel position to avoid clutter.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºçš„CNNæ¶æ„ï¼ŒåŒ…å«æ‰€æœ‰å¯è®­ç»ƒçš„æ¨¡å‹å‚æ•°ã€‚æ³¨æ„ï¼Œæ ¸æƒé‡ä»…æ˜¾ç¤ºåœ¨ç¬¬ä¸€ä¸ªæ ¸ä½ç½®ï¼Œä»¥é¿å…æ··ä¹±ã€‚
- en: Training Model Parameters
  id: totrans-570
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹å‚æ•°
- en: Training a convolutional neural network proceeds iteratively by these steps,
    the same as discussed in the artificial neural network chapter.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œï¼Œä¸äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ä¸­è®¨è®ºçš„ç›¸åŒã€‚
- en: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3a5bc8956f8ceda05ddf9b582cd141d.png)'
- en: Training an artificial neural network proceeds iteratively by, 1\. forward pass
    to make a prediction, 2\. calculate the error derivative based on the prediction
    and truth over training data, 3\. backpropagate the error derivative back through
    the artificial neural network to calculate the derivatives of the error over all
    the model weights and biases parameters, 4\. update the model parameters based
    on the derivatives and learning rates, 5\. repeat until convergence.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œï¼Œ1. å‰å‘ä¼ æ’­ä»¥è¿›è¡Œé¢„æµ‹ï¼Œ2. æ ¹æ®é¢„æµ‹å’Œè®­ç»ƒæ•°æ®ä¸­çš„çœŸå®å€¼è®¡ç®—è¯¯å·®å¯¼æ•°ï¼Œ3. é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®å¯¼æ•°ï¼Œä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œ4.
    æ ¹æ®å¯¼æ•°å’Œå­¦ä¹ ç‡æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œ5. é‡å¤ç›´åˆ°æ”¶æ•›ã€‚
- en: Hereâ€™s some details on each step with a focus on differences from artificial
    neural networks, for more details see the artificial neural network chapter.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ï¼Œé‡ç‚¹å…³æ³¨ä¸äººå·¥ç¥ç»ç½‘ç»œçš„å·®å¼‚ï¼Œæ›´å¤šç»†èŠ‚è¯·å‚é˜…äººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ã€‚
- en: '**Initializing the Model Parameters** - initialize all model parameters with
    typically small (near zero) random values. Hereâ€™s a couple common methods,'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆå§‹åŒ–æ¨¡å‹å‚æ•°** - é€šå¸¸ä½¿ç”¨æ¥è¿‘é›¶çš„å°éšæœºå€¼åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§æ–¹æ³•ï¼Œ'
- en: '**Xavier Glorot Uniform Initialization** - for Tanh and sigmoid activation,
    random realizations from uniform distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Xavier Glorot å‡åŒ€åˆå§‹åŒ–** - å¯¹äºTanhå’Œsigmoidæ¿€æ´»å‡½æ•°ï¼Œä»ç”± \(U[\text{min}, \text{max}]\)
    æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºç”Ÿæˆï¼Œ'
- en: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}},\ \sqrt{\frac{6}{n_{\text{in}}
    + n_{\text{out}}}}\right]^{-1}(p^\ell) \]
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}},\ \sqrt{\frac{6}{n_{\text{in}}
    + n_{\text{out}}}}\right]^{-1}(p^\ell) \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    and \(p^{\ell}\) is a random cumulative probability value drawn from the uniform
    distribution, \(U[0,1]\).
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯CDFçš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥æ•°é‡ï¼Œ\(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\) ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: For example, given a \(3 \times 3\) kernel with 1 channel in and 9 channel out,
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é€šé“ä¸º1ï¼Œè¾“å‡ºé€šé“ä¸º9çš„ \(3 \times 3\) å·ç§¯æ ¸ï¼Œ
- en: \[ n_{in} = k \times k \times C = 3 \times 3 \times 1 \]\[ n_out = k \times
    k \times C = 3 \times 3 \times 9 \]
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: \[ n_{in} = k \times k \times C = 3 \times 3 \times 1 \]\[ n_out = k \times
    k \times C = 3 \times 3 \times 9 \]
- en: '**He Kaiming Weight Initialization** - for ReLU and leaky ReLU activation,
    random realizations from uniform distributions specified by \(U[\text{min}, \text{max}]\),'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**He Kaiming æƒé‡åˆå§‹åŒ–** - å¯¹äºReLUå’Œleaky ReLUæ¿€æ´»å‡½æ•°ï¼Œä»ç”± \(U[\text{min}, \text{max}]\)
    æŒ‡å®šçš„å‡åŒ€åˆ†å¸ƒä¸­éšæœºç”Ÿæˆï¼Œ'
- en: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}}}},\ \sqrt{\frac{6}{n_{\text{in}}}}\right]^{-1}(p^\ell)
    \]
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_i = F_U\left[-\sqrt{\frac{6}{n_{\text{in}}}},\ \sqrt{\frac{6}{n_{\text{in}}}}\right]^{-1}(p^\ell)
    \]
- en: where \(F^{-1}_U\) is the inverse of the CDF, \(p\) is the number of inputs,
    \(k\) is the number of outputs, and \(p^{\ell}\) is a random cumulative probability
    value drawn from the uniform distribution, \(U[0,1]\).
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä¸­ \(F^{-1}_U\) æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„é€†ï¼Œ\(p\) æ˜¯è¾“å…¥çš„æ•°é‡ï¼Œ\(k\) æ˜¯è¾“å‡ºçš„æ•°é‡ï¼Œè€Œ \(p^{\ell}\) æ˜¯ä»å‡åŒ€åˆ†å¸ƒ \(U[0,1]\)
    ä¸­æŠ½å–çš„éšæœºç´¯ç§¯æ¦‚ç‡å€¼ã€‚
- en: '**Forward Pass** - to make a prediction, \(\hat{y}\). Initial predictions will
    be random for the first iteration, but will improve.'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­£å‘ä¼ æ’­** - è¿›è¡Œé¢„æµ‹ï¼Œ\(\hat{y}\)ã€‚åˆå§‹é¢„æµ‹åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å°†æ˜¯éšæœºçš„ï¼Œä½†ä¼šé€æ¸æ”¹è¿›ã€‚'
- en: '![](../Images/08556ecbd47d143019d0163dc95761cf.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/08556ecbd47d143019d0163dc95761cf.png)'
- en: Prediction with our artificial neural network initialized with random model
    parameters, weights and biases.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®çš„äººå·¥ç¥ç»ç½‘ç»œè¿›è¡Œé¢„æµ‹ã€‚
- en: '**Calculate the Error Derivative** - given a loss of, \(P = \frac{1}{2} \left(\hat{y}
    - y \right)^2\), the error derivative, i.e., rate of change of in error given
    a change in model estimate is \(\frac{\partial P}{\partial \hat{y}} = \hat{Y}
    - Y\).'
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—è¯¯å·®å¯¼æ•°** - ç»™å®šæŸå¤± \(P = \frac{1}{2} \left(\hat{y} - y \right)^2\)ï¼Œè¯¯å·®å¯¼æ•°ï¼Œå³è¯¯å·®éšæ¨¡å‹ä¼°è®¡å˜åŒ–çš„å˜åŒ–ç‡æ˜¯
    \(\frac{\partial P}{\partial \hat{y}} = \hat{Y} - Y\)ã€‚'
- en: For now, letâ€™s only consider a single estimate, and we will address more than
    1 training data later.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åªè€ƒè™‘ä¸€ä¸ªä¼°è®¡å€¼ï¼Œç¨åæˆ‘ä»¬å°†è§£å†³è¶…è¿‡1ä¸ªè®­ç»ƒæ•°æ®ã€‚
- en: '**Backpropagate the Error Derivative** - we shift back through the artificial
    neural network to calculate the derivatives of the error over all the model weights
    and biases parameters, to accomplish this we use the chain rule,'
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­è¯¯å·®å¯¼æ•°** - æˆ‘ä»¬é€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåå‘ç§»åŠ¨ä»¥è®¡ç®—æ‰€æœ‰æ¨¡å‹æƒé‡å’Œåç½®å‚æ•°çš„è¯¯å·®å¯¼æ•°ï¼Œä¸ºæ­¤æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œ'
- en: \[ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot
    \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} \]
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial}{\partial x} f(g(h(x))) = \frac{\partial f}{\partial g} \cdot
    \frac{\partial g}{\partial h} \cdot \frac{\partial h}{\partial x} \]
- en: '**Update the Model Parameters** - based on the derivatives, \frac{\partial
    P}{\partial \lambda_{i,j}} and learning rates, \(\eta\), like this,'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ›´æ–°æ¨¡å‹å‚æ•°** - æ ¹æ®å¯¼æ•°ï¼Œ\(\frac{\partial P}{\partial \lambda_{i,j}}\) å’Œå­¦ä¹ ç‡ \(\eta\)ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ'
- en: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    P}{\partial \lambda_{i,j}} \]
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{i,j}^{\ell} = \lambda_{i,j}^{\ell - 1} + \eta \cdot \frac{\partial
    P}{\partial \lambda_{i,j}} \]
- en: '**Repeat Until Convergence** - return to step 1\. until the error, \(P\), is
    reduced to an acceptable level, i.e., model convergence is the condition to stop
    the iterations'
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é‡å¤ç›´åˆ°æ”¶æ•›** - è¿”å›æ­¥éª¤ 1ï¼Œç›´åˆ°è¯¯å·® \(P\) é™ä½åˆ°å¯æ¥å—çš„æ°´å¹³ï¼Œå³æ¨¡å‹æ”¶æ•›æ˜¯åœæ­¢è¿­ä»£çš„æ¡ä»¶'
- en: Backpropagation
  id: totrans-594
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­
- en: For brevity, I refer you to the artificial neural network chapter for a walkthrough
    of backpropagating the error gradient through a neural network.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å»ºè®®æ‚¨å‚è€ƒäººå·¥ç¥ç»ç½‘ç»œç« èŠ‚ï¼Œäº†è§£é€šè¿‡ç¥ç»ç½‘ç»œåå‘ä¼ æ’­è¯¯å·®æ¢¯åº¦çš„è¿‡ç¨‹ã€‚
- en: Updating Model Parameters
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°æ¨¡å‹å‚æ•°
- en: The derivatives for each of the model parameters are the error gradients, so
    we are ready to use gradient descent optimization with the addition of,
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°çš„å¯¼æ•°æ˜¯è¯¯å·®æ¢¯åº¦ï¼Œå› æ­¤æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œå¹¶æ·»åŠ ï¼Œ
- en: '**learning rate** - to scale the rate of change of the model updates we assign
    a learning rate, \(\eta\). For our model parameter examples from above,'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡** - ä¸ºäº†ç¼©æ”¾æ¨¡å‹æ›´æ–°çš„å˜åŒ–ç‡ï¼Œæˆ‘ä»¬åˆ†é…ä¸€ä¸ªå­¦ä¹ ç‡ \(\eta\)ã€‚å¯¹äºä¸Šé¢æåˆ°çš„æ¨¡å‹å‚æ•°ç¤ºä¾‹ï¼Œ'
- en: \[ \lambda_{9,12}^{\ell} = \lambda_{9,12}^{\ell - 1} - \eta \cdot \frac{\partial
    P}{\partial \lambda_{9,12}} \]\[ \lambda_{10,12}^{\ell} = \lambda_{10,12}^{\ell
    - 1} - \eta \cdot \frac{\partial P}{\partial \lambda_{10,12}} \]\[ b_{12}^{\ell}
    = b_{12}^{\ell - 1} + \eta \cdot \frac{\partial P}{\partial b_{12}} \]
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \lambda_{9,12}^{\ell} = \lambda_{9,12}^{\ell - 1} - \eta \cdot \frac{\partial
    P}{\partial \lambda_{9,12}} \]\[ \lambda_{10,12}^{\ell} = \lambda_{10,12}^{\ell
    - 1} - \eta \cdot \frac{\partial P}{\partial \lambda_{10,12}} \]\[ b_{12}^{\ell}
    = b_{12}^{\ell - 1} + \eta \cdot \frac{\partial P}{\partial b_{12}} \]
- en: recall, this process of gradient calculation and model parameters, weights and
    biases, updating is iterated and is known as gradient descent optimization.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›æƒ³ï¼Œè¿™ä¸ªæ¢¯åº¦è®¡ç®—å’Œæ¨¡å‹å‚æ•°ã€æƒé‡å’Œåç½®æ›´æ–°è¿‡ç¨‹æ˜¯è¿­ä»£çš„ï¼Œè¢«ç§°ä¸ºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
- en: the goal is to explore the loss hypersurface, avoiding and escaping local minimums
    and ultimately finding the global minimum.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ¢ç´¢æŸå¤±è¶…æ›²é¢ï¼Œé¿å…å’Œé€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼Œæœ€ç»ˆæ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚
- en: learning rate, also known as step size is commonly set between 0.0 and 1.0,
    note 0.01 is the default in Keras module of TensorFlow
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ï¼Œä¹Ÿç§°ä¸ºæ­¥é•¿ï¼Œé€šå¸¸è®¾ç½®åœ¨ 0.0 å’Œ 1.0 ä¹‹é—´ï¼Œæ³¨æ„åœ¨ TensorFlow çš„ Keras æ¨¡å—ä¸­é»˜è®¤å€¼ä¸º 0.01
- en: '**Low Learning Rate** â€“ more stable, but a slower solution, may get stuck in
    a local minimum'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä½å­¦ä¹ ç‡** â€“ æ›´ç¨³å®šï¼Œä½†è§£çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼'
- en: '**High Learning Rate** â€“ may be unstable, but perhaps a faster solution, may
    diverge out of the global minimum'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é«˜å­¦ä¹ ç‡** â€“ å¯èƒ½ä¸ç¨³å®šï¼Œä½†å¯èƒ½æ˜¯ä¸€ä¸ªæ›´å¿«çš„è§£å†³æ–¹æ¡ˆï¼Œå¯èƒ½ä¼šåç¦»å…¨å±€æœ€å°å€¼'
- en: One strategy is to start with a high learning rate and then to decrease the
    learning rate over the iterations
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç­–ç•¥æ˜¯å¼€å§‹ä½¿ç”¨é«˜å­¦ä¹ ç‡ï¼Œç„¶ååœ¨è¿­ä»£è¿‡ç¨‹ä¸­é€æ¸é™ä½å­¦ä¹ ç‡
- en: '**Learning Rate Decay** - set as > 0 to avoid mitigate oscillations,'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç‡è¡°å‡** - è®¾ç½®ä¸º>0ä»¥é¿å…å‡è½»æŒ¯è¡ï¼Œ'
- en: Training Epochs
  id: totrans-607
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒepoch
- en: This is a good time to talk about stochastic gradient descent optimization,
    first letâ€™s define some common terms,
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯è®¨è®ºéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„å¥½æ—¶æœºï¼Œé¦–å…ˆè®©æˆ‘ä»¬å®šä¹‰ä¸€äº›å¸¸è§æœ¯è¯­ï¼Œ
- en: '**Batch Gradient Descent** - updates the model parameters after passing through
    all of the data'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ‰¹é‡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡æ‰€æœ‰æ•°æ®åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Stochastic Gradient Descent** - updates the model parameters over each sample
    data'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**éšæœºæ¢¯åº¦ä¸‹é™** - åœ¨æ¯ä¸ªæ ·æœ¬æ•°æ®ä¸Šæ›´æ–°æ¨¡å‹å‚æ•°'
- en: '**Mini-batch Gradient Descent** - updates the model parameter after passing
    through a single batch'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿·ä½ æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™** - åœ¨é€šè¿‡å•ä¸ªæ‰¹æ¬¡åæ›´æ–°æ¨¡å‹å‚æ•°'
- en: With mini-batch gradient descent stochasticity is introduced through the use
    of subsets of the data, known as batches,
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿·ä½ æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™ï¼Œé€šè¿‡ä½¿ç”¨æ•°æ®å­é›†ï¼ˆç§°ä¸ºæ‰¹æ¬¡ï¼‰å¼•å…¥äº†éšæœºæ€§ï¼Œ
- en: for example, if we divide our 100 samples into 4 batches, then we iterate over
    each batch separately
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°†æˆ‘ä»¬çš„100ä¸ªæ ·æœ¬åˆ†æˆ4ä¸ªæ‰¹æ¬¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å•ç‹¬è¿­ä»£æ¯ä¸ªæ‰¹æ¬¡
- en: we speed up the individual updates, fewer data are faster to calculate, but
    we introduce more error
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡åŠ å¿«å•ä¸ªæ›´æ–°æ¥åŠ é€Ÿï¼Œè¾ƒå°‘çš„æ•°æ®è®¡ç®—å¾—æ›´å¿«ï¼Œä½†å¼•å…¥äº†æ›´å¤šçš„é”™è¯¯
- en: this often helps the training explore for the global minimum and avoid getting
    stuck in local minimums and along ridges in the loss hypersurface
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™é€šå¸¸æœ‰åŠ©äºè®­ç»ƒæ¢ç´¢å…¨å±€æœ€å°å€¼ï¼Œå¹¶é¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼ä»¥åŠæŸå¤±è¶…æ›²é¢ä¸Šçš„è„Š
- en: Finally our last definition here,
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰æœ€åä¸€ä¸ªæœ¯è¯­ï¼Œ
- en: '**epoch** - is one pass over all of the data, so that would be 4 iterations
    of updating the model parameters if we have 4 mini-batches'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epoch** - æ˜¯å¯¹æ‰€æœ‰æ•°æ®çš„å•æ¬¡éå†ï¼Œå› æ­¤å¦‚æœæœ‰4ä¸ªè¿·ä½ æ‰¹æ¬¡ï¼Œé‚£ä¹ˆå°†ä¼šæœ‰4æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°çš„è¿­ä»£'
- en: There are many other considerations that I will add later including,
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†åœ¨ä»¥åæ·»åŠ è®¸å¤šå…¶ä»–è€ƒè™‘å› ç´ ï¼ŒåŒ…æ‹¬ï¼Œ
- en: momentum
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨é‡
- en: adaptive optimization
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªé€‚åº”ä¼˜åŒ–
- en: Now letâ€™s build the above artificial neural network by-hand and visualize the
    solution!
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ‰‹åŠ¨æ„å»ºä¸Šè¿°äººå·¥ç¥ç»ç½‘ç»œå¹¶å¯è§†åŒ–è§£å†³æ–¹æ¡ˆï¼
- en: this is by-hand so that you can see every calculation. I intentionally avoided
    using TensorFlow or PyTorch.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ºäº†æ‰‹åŠ¨æ“ä½œï¼Œä»¥ä¾¿ä½ å¯ä»¥çœ‹åˆ°æ¯ä¸ªè®¡ç®—ã€‚æˆ‘æ•…æ„é¿å…äº†ä½¿ç”¨TensorFlowæˆ–PyTorchã€‚
- en: Training with Multiple Training Images
  id: totrans-623
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªè®­ç»ƒå›¾åƒè¿›è¡Œè®­ç»ƒ
- en: The backpropagation is based on a single sample, i.e., training image and paired
    response feature value; therefore, to train over multiple images we must cycle
    over the,
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­åŸºäºå•ä¸ªæ ·æœ¬ï¼Œå³è®­ç»ƒå›¾åƒå’Œé…å¯¹çš„å“åº”ç‰¹å¾å€¼ï¼›å› æ­¤ï¼Œä¸ºäº†åœ¨å¤šä¸ªå›¾åƒä¸Šè®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»å¾ªç¯éå†ï¼Œ
- en: forward pass
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ æ’­
- en: calculate error derivative
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—è¯¯å·®å¯¼æ•°
- en: back propagate
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­
- en: '![](../Images/911e3eed1fd228a0da4716b460c82d38.png)'
  id: totrans-628
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/911e3eed1fd228a0da4716b460c82d38.png)'
- en: Batch training process.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡è®­ç»ƒè¿‡ç¨‹ã€‚
- en: For each image the weights and biases gradients are stored. Then the gradients
    are summed over the images in the batch and this sum is applied with the learning
    rate to update the weights and biases.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯å¼ å›¾åƒï¼Œå­˜å‚¨æƒé‡å’Œåå·®æ¢¯åº¦ã€‚ç„¶åï¼Œå°†æ¢¯åº¦åœ¨æ‰¹æ¬¡ä¸­çš„å›¾åƒä¸Šæ±‚å’Œï¼Œå¹¶å°†æ­¤æ€»å’Œåº”ç”¨äºå­¦ä¹ ç‡ä»¥æ›´æ–°æƒé‡å’Œåå·®ã€‚
- en: Forward Pass
  id: totrans-631
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰å‘ä¼ æ’­
- en: For clarity, letâ€™s walk through the convolutional neural network, starting with
    image input.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œè®©æˆ‘ä»¬ä»å›¾åƒè¾“å…¥å¼€å§‹ï¼Œé€æ­¥è®²è§£å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: The input nodes receives the input from the image in to the convolution layer,
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥èŠ‚ç‚¹æ¥æ”¶å›¾åƒè¾“å…¥åˆ°å·ç§¯å±‚çš„è¾“å…¥ï¼Œ
- en: node order is retained to preserve spatial, location information from the image
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿ç•™èŠ‚ç‚¹é¡ºåºä»¥ä¿ç•™ä»å›¾åƒä¸­è·å–çš„ç©ºé—´ã€ä½ç½®ä¿¡æ¯
- en: For continuous feature images,
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿ç»­ç‰¹å¾å›¾åƒï¼Œ
- en: the continuous predictor feature values are normalized to a min / max of [0,1]
    or [-1,1] to improve sensitivity for the specific activation function
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿ç»­é¢„æµ‹å™¨çš„ç‰¹å¾å€¼è¢«å½’ä¸€åŒ–åˆ°[0,1]æˆ–[-1,1]çš„æœ€å°/æœ€å¤§å€¼ï¼Œä»¥æé«˜ç‰¹å®šæ¿€æ´»å‡½æ•°çš„æ•æ„Ÿæ€§
- en: for color images, the RGB channels may be each normalized and included as 3
    input channels
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå½©è‰²å›¾åƒï¼ŒRGBé€šé“å¯ä»¥åˆ†åˆ«å½’ä¸€åŒ–å¹¶ä½œä¸º3ä¸ªè¾“å…¥é€šé“åŒ…å«
- en: For categorical feature images,
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»ç‰¹å¾å›¾åƒï¼Œ
- en: for binary, cardinality of 2, the values may be reassigned by indicator transform
    to 0 or 1
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºäºŒè¿›åˆ¶ï¼ŒåŸºæ•°2ï¼Œå€¼å¯ä»¥é€šè¿‡æŒ‡ç¤ºå˜æ¢é‡æ–°åˆ†é…ä¸º0æˆ–1
- en: for cardinality > 2, one-hot-encoding may be applied resulting in \(k\) input
    channels
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºåŸºæ•°å¤§äº2çš„æƒ…å†µï¼Œå¯èƒ½åº”ç”¨one-hotç¼–ç ï¼Œä»è€Œäº§ç”Ÿ\(k\)ä¸ªè¾“å…¥é€šé“
- en: See the feature transformation chapter for more details about these transformations.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è¿™äº›è½¬æ¢çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç‰¹å¾è½¬æ¢ç« èŠ‚ã€‚
- en: Now we pass through a convolution layer, with convolution and activation, resulting
    in a new feature map.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå·ç§¯å±‚ï¼ŒåŒ…å«å·ç§¯å’Œæ¿€æ´»æ“ä½œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾å›¾ã€‚
- en: '![](../Images/02745c4fdecaad32e59683bafa3d0820.png)'
  id: totrans-643
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02745c4fdecaad32e59683bafa3d0820.png)'
- en: Walk-through of our by-hand convolutional neural network, through the convolution
    layer.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„å·ç§¯ç¥ç»ç½‘ç»œçš„è®²è§£ï¼Œé€šè¿‡å·ç§¯å±‚ã€‚
- en: take linearly weighted combinations based on the kernel(s) of input image or
    previous feature map, add a bias term and then nonlinearly transform the result,
    this transform is call the activation function, \(\alpha\).
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ¹æ®è¾“å…¥å›¾åƒæˆ–å‰ä¸€ä¸ªç‰¹å¾å›¾çš„æ ¸è¿›è¡Œçº¿æ€§åŠ æƒç»„åˆï¼Œæ·»åŠ ä¸€ä¸ªåç½®é¡¹ï¼Œç„¶åå¯¹ç»“æœè¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œè¿™ç§å˜æ¢ç§°ä¸ºæ¿€æ´»å‡½æ•° \(\alpha\)ã€‚
- en: \[ C_{j_{\text{in}}} = \sum_{i=1}^{n} \left( K_{i+5} \cdot I_{j-8} \right) +
    b_{conv} \]
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_{j_{\text{in}}} = \sum_{i=1}^{n} \left( K_{i+5} \cdot I_{j-8} \right) +
    b_{conv} \]
- en: where, \(K_6\), \(K_7\) and \(K_8\) are kernel weights, \(b_{conv}\) is the
    kernel bias, and the \(I\) are the input nodes.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ\(K_6\)ã€\(K_7\) å’Œ \(K_8\) æ˜¯æ ¸æƒé‡ï¼Œ\(b_{conv}\) æ˜¯æ ¸åç½®ï¼Œ\(I\) æ˜¯è¾“å…¥èŠ‚ç‚¹ã€‚
- en: Please, excuse the strange indices in the equation, I like using unique node
    integers for every node to avoid mixing up nodes in my notes and codes, but this
    does complicate the index assignments.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·åŸè°…æ–¹ç¨‹ä¸­å¥‡æ€ªçš„ç´¢å¼•ï¼Œæˆ‘å–œæ¬¢ä¸ºæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨å”¯ä¸€çš„èŠ‚ç‚¹æ•´æ•°ï¼Œä»¥é¿å…åœ¨æˆ‘çš„ç¬”è®°å’Œä»£ç ä¸­æ··æ·†èŠ‚ç‚¹ï¼Œä½†è¿™ç¡®å®ä½¿ç´¢å¼•åˆ†é…å˜å¾—å¤æ‚ã€‚
- en: \[ C_{9_{\text{in}}} = I_1 \cdot K_6 + I_2 \cdot K_7 + I_3 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{10_{\text{in}}} = I_2 \cdot K_6 + I_3 \cdot K_7 + I_4 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{11_{\text{in}}} = I_3 \cdot K_6 + I_4 \cdot K_7 + I_5 \cdot K_8 + b_{\text{conv}}
    \]
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_{9_{\text{in}}} = I_1 \cdot K_6 + I_2 \cdot K_7 + I_3 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{10_{\text{in}}} = I_2 \cdot K_6 + I_3 \cdot K_7 + I_4 \cdot K_8 + b_{\text{conv}}
    \]\[ C_{11_{\text{in}}} = I_3 \cdot K_6 + I_4 \cdot K_7 + I_5 \cdot K_8 + b_{\text{conv}}
    \]
- en: then nonlinear activation is applied to each,
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¹æ¯ä¸ªåº”ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œ
- en: \[ C_j = \alpha \left( C_{j_{in}} \right) \quad j = 9, \ldots 11 \]
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: \[ C_j = \alpha \left( C_{j_{in}} \right) \quad j = 9, \ldots 11 \]
- en: Now we proceed from the feature map through the artificial neural network to
    the output.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä»ç‰¹å¾å›¾å¼€å§‹ï¼Œé€šè¿‡äººå·¥ç¥ç»ç½‘ç»œåˆ°è¾“å‡ºå±‚ã€‚
- en: This is just a standard artificial neural network that takes the feature map
    flattened and moves it to the output.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªæ ‡å‡†çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œå®ƒå°†å±•å¹³çš„ç‰¹å¾å›¾ç§»åŠ¨åˆ°è¾“å‡ºå±‚ã€‚
- en: in this example for brevity we show the simplest possible artificial neural
    network, i.e., the next layer after the feature map is the output. More complicated
    architectures with hidden layers are often applied.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¾‹ä¸­ä¸ºäº†ç®€æ´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€ç®€å•çš„å¯èƒ½çš„äººå·¥ç¥ç»ç½‘ç»œï¼Œå³ç‰¹å¾å›¾ä¹‹åçš„ä¸‹ä¸€å±‚å°±æ˜¯è¾“å‡ºå±‚ã€‚æ›´å¤æ‚çš„æ¶æ„ï¼Œé€šå¸¸åŒ…å«éšè—å±‚ã€‚
- en: also, since our images are 1D we do not require a flattening step
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„å›¾åƒæ˜¯1ç»´çš„ï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œå±•å¹³æ­¥éª¤
- en: The output node is a standard output node from an artificial neural network,
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºèŠ‚ç‚¹æ˜¯äººå·¥ç¥ç»ç½‘ç»œçš„æ ‡å‡†è¾“å‡ºèŠ‚ç‚¹ï¼Œ
- en: input is a linear combination of the nodes from the previous layer with an added
    bias term.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥æ˜¯å‰ä¸€å±‚èŠ‚ç‚¹çš„çº¿æ€§ç»„åˆï¼Œå¹¶æ·»åŠ ä¸€ä¸ªåç½®é¡¹ã€‚
- en: \[ O_{j_{\text{in}}} = \sum_{j=1}^{m} \left( \lambda_{i,j} \cdot H_i \right)
    + b_j \]
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_{j_{\text{in}}} = \sum_{j=1}^{m} \left( \lambda_{i,j} \cdot H_i \right)
    + b_j \]
- en: and then an activation is applied,
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œ
- en: \[ O_j = \alpha \left( O_{j_{in}} \right) \]
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha \left( O_{j_{in}} \right) \]
- en: For the case of a regression model, with continuous response feature, linear
    or identity activation is applied,
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›å½’æ¨¡å‹çš„æƒ…å†µï¼Œå…·æœ‰è¿ç»­å“åº”ç‰¹å¾ï¼Œåº”ç”¨çº¿æ€§æˆ–æ’ç­‰æ¿€æ´»å‡½æ•°ï¼Œ
- en: \[ O_j = \alpha \left( O_{j_{in}} \right) = O_{j_{in}} \]![](../Images/3becddf21de74c01ec2c585108079e0e.png)
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = \alpha \left( O_{j_{in}} \right) = O_{j_{in}} \]![](../Images/3becddf21de74c01ec2c585108079e0e.png)
- en: Walk-through of our by-hand convolutional neural network, from feature map to
    the output for continuous output.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„å·ç§¯ç¥ç»ç½‘ç»œçš„è®²è§£ï¼Œä»ç‰¹å¾å›¾åˆ°è¿ç»­è¾“å‡ºçš„è¾“å‡ºå±‚ã€‚
- en: and for the case of a classification model, with categorical response feature,
    softmax activation is applied over \(K\) nodes equal to the cardinality of the
    response feature.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†ç±»æ¨¡å‹çš„æƒ…å†µï¼Œå…·æœ‰åˆ†ç±»å“åº”ç‰¹å¾ï¼Œåœ¨ç­‰äºå“åº”ç‰¹å¾åŸºæ•° \(K\) çš„ \(K\) ä¸ªèŠ‚ç‚¹ä¸Šåº”ç”¨softmaxæ¿€æ´»å‡½æ•°ï¼Œ
- en: the output is a probability for each category that honors non-negativity and
    closure constraints
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç¬¦åˆéè´Ÿæ€§å’Œé—­åˆçº¦æŸ
- en: '![](../Images/439615adbac8db26678e58e258f9105f.png)'
  id: totrans-666
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/439615adbac8db26678e58e258f9105f.png)'
- en: Walk-through of our by-hand convolutional neural network, from feature map to
    the output for categorical output.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹åŠ¨å®ç°çš„å·ç§¯ç¥ç»ç½‘ç»œçš„è®²è§£ï¼Œä»ç‰¹å¾å›¾åˆ°åˆ†ç±»è¾“å‡ºçš„è¾“å‡ºå±‚ã€‚
- en: \[ O_j = g_k(O_{j_{\text{in}}}) = \frac{e^{O_{j_{\text{in}}}}}{\sum_{\iota=1}^{K}
    e^{O_{\iota_{\text{in}}}}} \]
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: \[ O_j = g_k(O_{j_{\text{in}}}) = \frac{e^{O_{j_{\text{in}}}}}{\sum_{\iota=1}^{K}
    e^{O_{\iota_{\text{in}}}}} \]
- en: Import Required Packages
  id: totrans-669
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥æ‰€éœ€çš„åŒ…
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦ä¸€äº›æ ‡å‡†åŒ…ã€‚è¿™äº›åº”è¯¥å·²ç»ä¸Anaconda 3ä¸€èµ·å®‰è£…ã€‚
- en: recall our goal is to build a convolutional neural network by-hand with only
    basic math and array operations, so we only need NumPy along with matplotlib for
    plotting.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰‹åŠ¨æ„å»ºä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œåªä½¿ç”¨åŸºæœ¬çš„æ•°å­¦å’Œæ•°ç»„è¿ç®—ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦NumPyä»¥åŠmatplotlibè¿›è¡Œç»˜å›¾ã€‚
- en: '[PRE8]'
  id: totrans-672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing â€˜python -m pip install [package-name]â€™. More assistance is available
    with the respective package docs.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ é‡åˆ°åŒ…å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦é¦–å…ˆå®‰è£…è¿™äº›åŒ…ä¸­çš„æŸäº›ã€‚è¿™é€šå¸¸å¯ä»¥é€šè¿‡åœ¨Windowsä¸Šæ‰“å¼€å‘½ä»¤çª—å£ç„¶åè¾“å…¥â€˜python -m pip install
    [package-name]â€™æ¥å®Œæˆã€‚æœ‰å…³ç›¸åº”åŒ…çš„æ–‡æ¡£å¯ä»¥æä¾›æ›´å¤šå¸®åŠ©ã€‚
- en: Declare Functions
  id: totrans-674
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å£°æ˜å‡½æ•°
- en: Hereâ€™s the functions to make, train and visualize our convoluational neural
    network.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ›å»ºã€è®­ç»ƒå’Œå¯è§†åŒ–æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œçš„å‡½æ•°ã€‚
- en: '[PRE9]'
  id: totrans-676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Simple By-hand CNN
  id: totrans-677
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•çš„æ‰‹åŠ¨CNN
- en: I wrote this code to specify a simple CNN,
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç¼–å†™äº†è¿™æ®µä»£ç æ¥æŒ‡å®šä¸€ä¸ªç®€å•çš„CNNï¼Œ
- en: five input nodes, 1 convolution layer with a kernel of 3 resulting in a 3 nodes
    in the feature map and 1 output node
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº”ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œ1ä¸ªå…·æœ‰3ä¸ªæ ¸çš„å·ç§¯å±‚ï¼Œåœ¨ç‰¹å¾å›¾ä¸­æœ‰3ä¸ªèŠ‚ç‚¹ï¼Œ1ä¸ªè¾“å‡ºèŠ‚ç‚¹
- en: 'and to train the CNN by iteratively performing the forward calculation and
    backpropagation. I calculate:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”é€šè¿‡è¿­ä»£æ‰§è¡Œå‰å‘è®¡ç®—å’Œåå‘ä¼ æ’­æ¥è®­ç»ƒCNNã€‚æˆ‘è®¡ç®—ï¼š
- en: the error and then propagate it to each node
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åå°†å®ƒä¼ æ’­åˆ°æ¯ä¸ªèŠ‚ç‚¹
- en: solve for the partial derivatives of the error with respect to each weight and
    bias
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ±‚è§£è¯¯å·®ç›¸å¯¹äºæ¯ä¸ªæƒé‡å’Œåå·®çš„åå¯¼æ•°
- en: all weights, biases and partial derivatives for all epoch are recorded in vectors
    for plotting
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æƒé‡ã€åå·®å’Œæ‰€æœ‰epochçš„åå¯¼æ•°éƒ½è®°å½•åœ¨å‘é‡ä¸­ä»¥è¿›è¡Œç»˜å›¾ã€‚
- en: '[PRE10]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Visualize By-hand CNN
  id: totrans-685
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨å¯è§†åŒ–CNN
- en: Letâ€™s visualize our convolutional neural network.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚
- en: note, I will used this code latter to make interactive dashboards.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘å°†åœ¨ä»¥åä½¿ç”¨æ­¤ä»£ç åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿ã€‚
- en: '[PRE11]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![_images/bd6f0d76fef9a0c1d4062d2fed4028a098a6ba3f59c0dff50e67370b90502ba7.png](../Images/0b3e6b1109144cd7a0b01bc9d699e751.png)'
  id: totrans-689
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/0b3e6b1109144cd7a0b01bc9d699e751.png)'
- en: and now we can visualize the model training results including,
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–æ¨¡å‹è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬ï¼Œ
- en: weights and biases vs. training epochs
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒé‡å’Œåå·®ä¸è®­ç»ƒepochçš„å…³ç³»
- en: CNN prediction vs. training epochs
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNé¢„æµ‹ä¸è®­ç»ƒepochçš„å…³ç³»
- en: '[PRE12]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![_images/4cea6b946d67f8e2cb6605415803c148ac63ebfb43f8034a7285880405bb88d3.png](../Images/7dcc0007f13a7312fec3085ca24fbf79.png)'
  id: totrans-694
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/7dcc0007f13a7312fec3085ca24fbf79.png)'
- en: Of course, the results above are not very practical, in fact we need our convolutional
    neural network to generalize by learning over many images, not just 1 as demonstrated
    above. Letâ€™s,
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä¸Šè¿°ç»“æœå¹¶ä¸éå¸¸å®ç”¨ï¼Œå®é™…ä¸Šæˆ‘ä»¬éœ€è¦æˆ‘ä»¬çš„å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡å­¦ä¹ è®¸å¤šå›¾åƒæ¥æ³›åŒ–ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸Šé¢æ¼”ç¤ºçš„1ä¸ªã€‚è®©æˆ‘ä»¬ï¼Œ
- en: make a suite of synthetic image data with labels
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ ‡ç­¾çš„åˆæˆå›¾åƒæ•°æ®é›†
- en: apply the batch training method to train on this ensemble of training images.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰¹é‡è®­ç»ƒæ–¹æ³•åº”ç”¨äºè®­ç»ƒè¿™äº›è®­ç»ƒå›¾åƒé›†åˆã€‚
- en: Make Synthetic Training Images with Label
  id: totrans-698
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¸¦æœ‰æ ‡ç­¾çš„åˆæˆè®­ç»ƒå›¾åƒ
- en: The following code makes random configurations of 5 points with variable slope
    and additive noise and then retains the slope linear regression model of the data
    as the label.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç åˆ›å»ºå…·æœ‰å¯å˜æ–œç‡å’ŒåŠ æ€§å™ªå£°çš„5ä¸ªç‚¹çš„éšæœºé…ç½®ï¼Œç„¶åä¿ç•™æ•°æ®çš„æ–œç‡çº¿æ€§å›å½’æ¨¡å‹ä½œä¸ºæ ‡ç­¾ã€‚
- en: The workflow inludes these steps,
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œæµç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼Œ
- en: draw a random slope, \(m^{\ell} \sim U\left[-2.0,2.0\right]\)
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶ä¸€ä¸ªéšæœºçš„æ–œç‡ï¼Œ\(m^{\ell} \sim U\left[-2.0,2.0\right]\)
- en: sample 5 values on this slope centered at the middle of the values
  id: totrans-702
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ­¤æ–œç‡çš„ä¸­é—´å€¼å¤„é‡‡æ ·5ä¸ªå€¼
- en: \[ Z^{\ell} = \left(x-2.5\right)*m^{\ell} \]
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z^{\ell} = \left(x-2.5\right)*m^{\ell} \]
- en: add a random errror to the values, \(\epsilon \sim U\left[-\Delta,\Delta\right]\)
  id: totrans-704
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨å€¼çš„ä¸­å€¼å¤„æ·»åŠ éšæœºè¯¯å·®ï¼Œ\(\epsilon \sim U\left[-\Delta,\Delta\right]\)
- en: \[ Z_{\epsilon}^{\ell} = \left(x-2.5\right)*m^{\ell} + U^{-1}\left[-\Delta,\Delta\right](p^{\ell})
    \]
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: \[ Z_{\epsilon}^{\ell} = \left(x-2.5\right)*m^{\ell} + U^{-1}\left[-\Delta,\Delta\right](p^{\ell})
    \]
- en: calculate the linear regression model slope to retain as the response feature
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—çº¿æ€§å›å½’æ¨¡å‹çš„æ–œç‡ï¼Œå°†å…¶ä½œä¸ºå“åº”ç‰¹å¾ä¿ç•™ã€‚
- en: \[ m = \frac{<Z_{\epsilon}^{\ell} \cdot x>}{<x \cdot x>} \]
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: \[ m = \frac{<Z_{\epsilon}^{\ell} \cdot x>}{<x \cdot x>} \]
- en: '[PRE13]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![_images/60bea9aeec98af43e38abc10f6ce1302ee8c10ff095a8bfdef8fb4231fc1be32.png](../Images/e763c68a0e61054a9b3854f895c7ec02.png)'
  id: totrans-709
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/e763c68a0e61054a9b3854f895c7ec02.png)'
- en: Training the Simple CNN on Many Training Images
  id: totrans-710
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šè®­ç»ƒå›¾åƒä¸Šè®­ç»ƒç®€å•çš„CNN
- en: I modified the code above to loops over the batch of training images, sums the
    error gradients and updates over all the weights and biases for each training
    epoch.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¿®æ”¹äº†ä¸Šé¢çš„ä»£ç ï¼Œä½¿å…¶å¾ªç¯éå†è®­ç»ƒå›¾åƒæ‰¹æ¬¡ï¼Œç´¯åŠ è¯¯å·®æ¢¯åº¦ï¼Œå¹¶åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸæ›´æ–°æ‰€æœ‰æƒé‡å’Œåç½®ã€‚
- en: '[PRE14]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now again we can visualize the model performance,
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡å¯ä»¥å¯è§†åŒ–æ¨¡å‹æ€§èƒ½ï¼Œ
- en: predictions over all training images vs. training epochs
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è®­ç»ƒå›¾åƒçš„é¢„æµ‹ä¸è®­ç»ƒå‘¨æœŸ
- en: model weights and biases vs. training epochs
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æƒé‡å’Œåç½®ä¸è®­ç»ƒå‘¨æœŸ
- en: '[PRE15]'
  id: totrans-716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![_images/ba623428124e0113a09fcf1b71bb1dacaf21bee4f296116c527e30bf0e957edb.png](../Images/e175db3feac32085e697870180f28aab.png)'
  id: totrans-717
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡é“¾æ¥](../Images/e175db3feac32085e697870180f28aab.png)'
- en: Comments
  id: totrans-718
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„è®º
- en: This was a basic treatment of convolutional neural networks. Much more could
    be done and discussed, I have many more resources. Check out my [shared resource
    inventory](https://michaelpyrcz.com/my-resources) and the YouTube lecture links
    at the start of this chapter with resource links in the videosâ€™ descriptions.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å¤„ç†ã€‚å¯ä»¥åšå’Œè®¨è®ºçš„è¿˜æœ‰å¾ˆå¤šï¼Œæˆ‘æœ‰å¾ˆå¤šæ›´å¤šçš„èµ„æºã€‚æŸ¥çœ‹æˆ‘çš„[å…±äº«èµ„æºæ¸…å•](https://michaelpyrcz.com/my-resources)ä»¥åŠæœ¬ç« å¼€å¤´å¸¦æœ‰èµ„æºé“¾æ¥çš„YouTubeè®²åº§é“¾æ¥ï¼Œè§†é¢‘æè¿°ä¸­åŒ…å«èµ„æºé“¾æ¥ã€‚
- en: I hope this is helpful,
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©ï¼Œ
- en: '*Michael*'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: About the Author
  id: totrans-722
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºä½œè€…
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  id: totrans-723
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡é“¾æ¥](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡40è‹±äº©æ ¡å›­å†…ï¼Œè¿ˆå…‹å°”Â·çš®å°”å¥‡æ•™æˆçš„åŠå…¬å®¤ã€‚
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡[ç§‘å…‹é›·å°”å·¥ç¨‹å­¦é™¢](https://cockrell.utexas.edu/faculty-directory/alphabetical/p)å’Œ[æ°å…‹é€Šåœ°çƒç§‘å­¦å­¦é™¢](https://www.jsg.utexas.edu/researcher/michael_pyrcz/)çš„æ•™æˆï¼Œä»–åœ¨é‚£é‡Œç ”ç©¶å¹¶æ•™æˆåœ°ä¸‹ã€ç©ºé—´æ•°æ®åˆ†æã€åœ°ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ã€‚è¿ˆå…‹å°”è¿˜æ˜¯ï¼Œ
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[èƒ½æºåˆ†æ](https://fri.cns.utexas.edu/energy-analytics)æ–°ç”Ÿç ”ç©¶é¡¹ç›®çš„é¦–å¸­ç ”ç©¶å‘˜ï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡è‡ªç„¶ç§‘å­¦é™¢æœºå™¨å­¦ä¹ å®éªŒå®¤çš„æ ¸å¿ƒæ•™å‘˜ã€‚'
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ã€Šè®¡ç®—æœºä¸åœ°çƒç§‘å­¦ã€‹](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board)çš„å‰¯ç¼–è¾‘ï¼Œä»¥åŠå›½é™…æ•°å­¦åœ°çƒç§‘å­¦åä¼š[ã€Šæ•°å­¦åœ°çƒç§‘å­¦ã€‹](https://link.springer.com/journal/11004/editorial-board)çš„è‘£äº‹ä¼šæˆå‘˜ã€‚'
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”å·²ç»æ’°å†™äº†70å¤šç¯‡[åŒè¡Œè¯„å®¡å‡ºç‰ˆç‰©](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en)ï¼Œä¸€ä¸ªç”¨äºç©ºé—´æ•°æ®åˆ†æçš„[PythonåŒ…](https://pypi.org/project/geostatspy/)ï¼Œåˆè‘—äº†ä¸€æœ¬å…³äºç©ºé—´æ•°æ®åˆ†æçš„æ•™ç§‘ä¹¦[ã€Šåœ°ç»Ÿè®¡å­¦å‚¨å±‚å»ºæ¨¡ã€‹](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)ï¼Œå¹¶æ˜¯ä¸¤æœ¬æœ€è¿‘å‘å¸ƒçš„ç”µå­ä¹¦çš„ä½œè€…ï¼Œ[ã€ŠPythonä¸­çš„åº”ç”¨åœ°ç»Ÿè®¡å­¦ï¼šGeostatsPyå®è·µæŒ‡å—ã€‹](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)å’Œ[ã€ŠPythonä¸­çš„åº”ç”¨æœºå™¨å­¦ä¹ ï¼šå®è·µæŒ‡å—ä¸ä»£ç ã€‹](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html)ã€‚
- en: All of Michaelâ€™s university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michaelâ€™s work and shared educational resources visit his
    Website.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”çš„æ‰€æœ‰å¤§å­¦è®²åº§éƒ½å¯ä»¥åœ¨ä»–çš„[YouTubeé¢‘é“](https://www.youtube.com/@GeostatsGuyLectures)ä¸Šæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…å«100å¤šä¸ªPythonäº¤äº’å¼ä»ªè¡¨æ¿å’Œ40å¤šä¸ªå­˜å‚¨åº“ä¸­çš„è¯¦ç»†å·¥ä½œæµç¨‹é“¾æ¥ï¼Œè¿™äº›å­˜å‚¨åº“ä½äºä»–çš„[GitHubè´¦æˆ·](https://github.com/GeostatsGuy)ï¼Œä»¥æ”¯æŒä»»ä½•æ„Ÿå…´è¶£çš„å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«ï¼Œæä¾›å¸¸é’å†…å®¹ã€‚è¦äº†è§£æ›´å¤šå…³äºè¿ˆå…‹å°”çš„å·¥ä½œå’Œå…±äº«æ•™è‚²èµ„æºï¼Œè¯·è®¿é—®ä»–çš„ç½‘ç«™ã€‚
- en: Want to Work Together?
  id: totrans-730
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³ä¸€èµ·å·¥ä½œå—ï¼Ÿ
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™äº›å†…å®¹å¯¹é‚£äº›æƒ³äº†è§£æ›´å¤šå…³äºåœ°ä¸‹å»ºæ¨¡ã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ çš„äººæœ‰æ‰€å¸®åŠ©ã€‚å­¦ç”Ÿå’Œåœ¨èŒä¸“ä¸šäººå£«æ¬¢è¿å‚åŠ ã€‚
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? Iâ€™d be happy to drop by and work with you!
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒ³é‚€è¯·æˆ‘åˆ°è´µå…¬å¸è¿›è¡ŒåŸ¹è®­ã€è¾…å¯¼ã€é¡¹ç›®å®¡æŸ¥ã€å·¥ä½œæµç¨‹è®¾è®¡å’Œ/æˆ–å’¨è¯¢ï¼Ÿæˆ‘å¾ˆä¹æ„æ‹œè®¿å¹¶ä¸æ‚¨åˆä½œï¼
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„Ÿå…´è¶£ä¸æˆ‘åˆä½œã€æ”¯æŒæˆ‘çš„ç ”ç©¶ç”Ÿç ”ç©¶æˆ–æˆ‘çš„åœ°ä¸‹æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ è”ç›Ÿï¼ˆå…±åŒè´Ÿè´£äººæ˜¯çº¦ç¿°Â·ç¦æ–¯ç‰¹æ•™æˆï¼‰å—ï¼Ÿæˆ‘çš„ç ”ç©¶å°†æ•°æ®åˆ†æã€éšæœºå»ºæ¨¡å’Œæœºå™¨å­¦ä¹ ç†è®ºä¸å®è·µç›¸ç»“åˆï¼Œä»¥å¼€å‘æ–°çš„æ–¹æ³•å’Œå·¥ä½œæµç¨‹ï¼Œå¢åŠ ä»·å€¼ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°ä¸‹é—®é¢˜ï¼
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡[mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu)è”ç³»åˆ°æˆ‘ã€‚
- en: Iâ€™m always happy to discuss,
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ€»æ˜¯å¾ˆé«˜å…´è®¨è®ºï¼Œ
- en: '*Michael*'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”*'
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ˆå…‹å°”Â·çš®å°”å¥‡ï¼Œåšå£«ï¼Œæ³¨å†Œå·¥ç¨‹å¸ˆï¼Œå¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡ Cockrell å·¥ç¨‹å­¦é™¢å’Œ Jackson åœ°çƒç§‘å­¦å­¦é™¢æ•™æˆ
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šèµ„æºå¯åœ¨ä»¥ä¸‹ä½ç½®è·å–ï¼š[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [ç½‘ç«™](http://michaelpyrcz.com) | [Google Scholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [åœ°çƒç»Ÿè®¡å­¦ä¹¦ç±](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Pythonä¸­åº”ç”¨åœ°çƒç»Ÿè®¡å­¦ç”µå­ä¹¦](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Pythonä¸­åº”ç”¨æœºå™¨å­¦ä¹ ç”µå­ä¹¦](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)
