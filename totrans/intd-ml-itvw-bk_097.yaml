- en: 8.2.3 Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html](https://huyenchip.com/ml-interviews-book/contents/8.2.3-reinforcement-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ðŸŒ³ **Tip** ðŸŒ³
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To refresh your knowledge on deep RL, checkout [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)
    (OpenAI)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[E] Explain the explore vs exploit tradeoff with examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How would a finite or infinite horizon affect our algorithms?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why do we need the discount term for objective functions?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Fill in the empty circles using the minimax algorithm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Minimax algorithm](../Images/24e7be9d8e9ad4ff02ee1ee2e821bff8.png "image_tooltip")'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[M] Fill in the alpha and beta values as you traverse the minimax tree from
    left to right.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Alpha-beta pruning](../Images/e5c5778c32bf07f69195be40a13b8d14.png "image_tooltip")'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[E] Given a policy, derive the reward function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Pros and cons of on-policy vs. off-policy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Whatâ€™s the difference between model-based and model-free? Which one is
    more data-efficient?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
