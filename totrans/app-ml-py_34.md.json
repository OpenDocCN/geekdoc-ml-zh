["```py\n%matplotlib inline                                         \nsuppress_warnings = True                                      # toggle to supress warnings\nimport os                                                     # set working directory\nimport numpy as np                                            # arrays and matrix math\nimport matplotlib.pyplot as plt                               # for plotting\nimport matplotlib.patches as patches                          # fancy box around agents\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom matplotlib.ticker import FuncFormatter\nimport copy                                                   # deep copy dictionaries\nimport math\nseed = 42\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 42                                                     # random number seed for workflow repeatability \n```", "```py\ndef sigmoid(x):                                               # sigmoid activation function\n    return 1 / (1 + np.exp(-x))\n\ndef generate_real_data(batch_size, slope_range=(-0.4, -0.7), residual_std=0.02): # make a synthetic training image set\n  \"\"\"\n Generate real 3-node images with decreasing linear trend plus noise.\n Standardize each to have mean 0.5.\n Returns shape (batch_size, 3)\n \"\"\"\n    slopes = np.random.uniform(slope_range[0], slope_range[1], size=batch_size)\n    base = np.array([1, 2, 3])  # node positions for linear trend\n    data = np.zeros((batch_size, 3))\n    for i in range(batch_size):\n        trend = slopes[i] * base\n        residual = np.random.normal(0, residual_std, size=3)\n        sample = trend + residual\n        sample += 0.5 - np.mean(sample)                       # standardize to mean 0.5\n        data[i] = sample\n    return data\n\ndef initialize_generator_weights():                           # initialize the generator weights and return as a dictionary\n    # Small random weights and bias for generator\n    return {\n        'lambda_12': np.random.randn() * 0.1,\n        'lambda_13': np.random.randn() * 0.1,\n        'lambda_14': np.random.randn() * 0.1,\n        'b': 0.0\n    }\n\ndef generator_forward(L1_latent, weights,return_pre_activation=False): # given latent vector and generator weights return a set of fake images\n  \"\"\"\n L1_latent: ndarray shape (batch_size,)\n weights: dict with keys 'lambda_1_2', 'lambda_1_3', 'lambda_1_4', 'b'\n Returns output ndarray shape (batch_size, 3)\n \"\"\"\n    O2in = weights['lambda_12'] * L1_latent + weights['b']\n    O3in = weights['lambda_13'] * L1_latent + weights['b']\n    O4in = weights['lambda_14'] * L1_latent + weights['b']\n\n    O2 = sigmoid(O2in)\n    O3 = sigmoid(O3in)\n    O4 = sigmoid(O4in)\n\n    Oin = np.vstack([O2in, O3in, O4in]).T; O = np.vstack([O2, O3, O4]).T; \n\n    if return_pre_activation: \n        return np.vstack([O2in, O3in, O4in]).T, np.vstack([O2, O3, O4]).T # shape (batch_size, 3)\n    else:\n        return np.vstack([O2, O3, O4]).T # shape (batch_size, 3)\n\ndef initialize_discriminator_weights():                       # initialize the discriminator weights and return as a dictionary \n    return {\n        'lambda_58': np.random.randn() * 0.1,\n        'lambda_68': np.random.randn() * 0.1,\n        'lambda_78': np.random.randn() * 0.1,\n        'c': 0.0\n    }\n\ndef discriminator_forward(I5, I6, I7, weights):               # given a set of images return the discriminator probability of real image\n  \"\"\"\n Inputs: I5, I6, I7 shape (batch_size,)\n weights: dict with keys 'lambda_58', 'lambda_68', 'lambda_78', 'c'\n Returns probability ndarray shape (batch_size,)\n \"\"\"\n    dO8in = (weights['lambda_58'] * I5 + \n         weights['lambda_68'] * I6 + \n         weights['lambda_78'] * I7 + \n         weights['c'])\n    dO8 = sigmoid(dO8in)\n    return dO8, dO8in\n\ndef discriminator_gradients(I5, I6, I7, y_true, weights):     # given set of images, true labels, and discriminator weights return the gradients\n  \"\"\"\n Compute discriminator gradients averaged over batch.\n y_true: labels (1 for real, 0 for fake), shape (batch_size,)\n \"\"\"\n    batch_size = y_true.shape[0]\n    y_pred, z = discriminator_forward(I5, I6, I7, weights)\n    dO8in = (y_pred - y_true)                                  # shape (batch_size,), note this solution integrates the sigmoid activation at O8\n\n    grad_lambda_58 = np.mean(dO8in * I5)\n    grad_lambda_68 = np.mean(dO8in * I6)\n    grad_lambda_78 = np.mean(dO8in * I7)\n    grad_c = np.mean(dO8in)\n\n    return {\n        'lambda_58': grad_lambda_58,\n        'lambda_68': grad_lambda_68,\n        'lambda_78': grad_lambda_78,\n        'c': grad_c\n    }\n\ndef generator_gradients(L1_latent, weights_g, weights_d):     # given latent vector, generator and discriminator weights return the gradients\n  \"\"\"\n Compute gradients of generator weights using discriminator feedback.\n L1_latent: shape (batch_size,)\n weights_g: generator weights dict\n weights_d: discriminator weights dict\n \"\"\"\n    batch_size = L1_latent.shape[0]\n\n    #O = generator_forward(L1_latent, weights_g)               # generator outputs, shape (batch_size, 3)\n    O_in, O = generator_forward(L1_latent, weights_g, return_pre_activation=True)\n\n    O2_in, O3_in, O4_in = O_in[:, 0], O_in[:, 1], O_in[:, 2]\n    O2, O3, O4 = O[:,0], O[:,1], O[:,2]\n    I5, I6, I7 = O[:,0], O[:,1], O[:,2]\n\n    y_pred, z = discriminator_forward(I5, I6, I7, weights_d)  # discriminator forward pass\n\n    dO8in = y_pred - 1                                           # gradient of loss w.r.t discriminator logit for generator loss, shape (batch_size,)\n\n    dO2 = dO8in * weights_d['lambda_58']                         # gradients w.r.t generator outputs\n    dO3 = dO8in * weights_d['lambda_68']\n    dO4 = dO8in * weights_d['lambda_78']\n\n    dO2in = dO2 * O2 * (1 - O2)                                  # Backprop through generator sigmoid activation\n    dO3in = dO3 * O3 * (1 - O3)\n    dO4in = dO4 * O4 * (1 - O4)\n\n    grad_lambda_12 = np.mean(dO2in * L1_latent)                 # gradients w.r.t generator weights and bias\n    grad_lambda_13 = np.mean(dO3in * L1_latent)\n    grad_lambda_14 = np.mean(dO4in * L1_latent)\n    grad_b = np.mean(dO2in + dO3in + dO4in)\n\n    return {\n        'lambda_12': grad_lambda_12,\n        'lambda_13': grad_lambda_13,\n        'lambda_14': grad_lambda_14,\n        'b': grad_b\n    }\n\ndef fancybox(ax, xy, width, height, label=\"\", edgecolor=\"black\", text_color=None): # a dashed fancy box for the GAN plot\n  \"\"\"\n Draws a dashed, rounded rectangle on a given axes.\n\n Parameters:\n - ax: The matplotlib axes to draw on\n - xy: (x, y) tuple for bottom-left corner of the box\n - width: Width of the box\n - height: Height of the box\n - label: Optional label text to display centered above the box\n - edgecolor: Border color of the box\n - text_color: Color of the label text (defaults to edgecolor)\n \"\"\"\n    if text_color is None:\n        text_color = edgecolor\n\n    box = patches.FancyBboxPatch(                             # draw box\n        xy,\n        width,\n        height,\n        boxstyle=\"round,pad=0.02\",\n        linewidth=2,\n        edgecolor=edgecolor,\n        facecolor=\"none\",\n        linestyle='--'\n    )\n    ax.add_patch(box)\n\n    x_center = xy[0] + width / 2                              # add label text above the box\n    y_top = xy[1] + height + 0.02\n    ax.text(x_center, y_top + 0.02, label, ha='center', va='bottom', fontsize=16, color=text_color)\n\ndef add_grid2():                                              # add grid lines\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ncolors = ['darksalmon','deepskyblue','gold']                  # line colors for latent to fake to probability flow\ncolors_real = ['orangered','dodgerblue','goldenrod']          # line colors for real image to probability flow\n\ndef draw_gan_architecture_full():                             # function to draw the GAN demonstrated in this workflow\n    fig, ax = plt.subplots(figsize=(12, 7))\n\n    ax.text(0.1, 0.5, r\"$L_1$\", fontsize=14, ha='center', va='center', # generator latent node\n            bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n\n    gen_outputs = [                                           # generator output nodes locations and labels\n        (0.3, 0.7, r\"$O_2$\", r\"$\\lambda_{1,2}$\"),\n        (0.3, 0.5, r\"$O_3$\", r\"$\\lambda_{1,3}$\"),\n        (0.3, 0.3, r\"$O_4$\", r\"$\\lambda_{1,4}$\")\n    ]\n    for i, (x, y, label, weight) in enumerate(gen_outputs):   # generator output node labels\n        ax.text(x, y, label, fontsize=14, ha='center', va='center',\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\", xy=(x - 0.05, y), xytext=(0.15, 0.5),   # generator output node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n        ax.text((x + 0.11) / 2, (y + 0.5) / 2, weight, fontsize=12, ha='center', va='bottom')\n\n    disc_inputs = [                                           # discriminator input nodes locations and labels\n        (0.5, 0.7, r\"$I_5$\", r\"$\\lambda_{5,8}$\"),\n        (0.5, 0.5, r\"$I_6$\", r\"$\\lambda_{6,8}$\"),\n        (0.5, 0.3, r\"$I_7$\", r\"$\\lambda_{7,8}$\")\n    ]\n    for i, (x, y, label, _) in enumerate(disc_inputs):        # discriminator input node labels\n        ax.text(x, y, label, fontsize=14, ha='center', va='center',\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\", xy=(x - 0.05, y), xytext=(0.35, gen_outputs[i][1]), # discriminator input node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n\n    ax.text(0.7, 0.5, r\"$D_8$\", fontsize=14, ha='center', va='center', # discriminator decision node label\n            bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n    for i, (x, y, _, weight) in enumerate(disc_inputs):\n        ax.annotate(\"\",xy=(0.65, 0.5), xytext=(x + 0.05, y),  # discriminator output node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n        ax.text((x + 0.7) / 2, (y + 0.5) / 2, weight, fontsize=12, ha='center', va='bottom')\n\n    real_inputs = [                                           # real data path below generator\n        (0.3, 0.1, r\"$I_5^{real}$\"),\n        (0.3, -0.1, r\"$I_6^{real}$\"),\n        (0.3, -0.3, r\"$I_7^{real}$\")\n    ]\n    for i, (x, y, label) in enumerate(real_inputs):\n        ax.text(x, y, label, fontsize=14, ha='center', va='center', # label real data node labels\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\",                                       # arrow to discriminator inputs\n                    xy=(disc_inputs[i][0] - 0.05, disc_inputs[i][1]), \n                    xytext=(x + 0.05, y),\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors_real[i], linestyle=\"--\"))\n\n    ax.text(0.1, 0.57, \"Latent\", fontsize=16, ha='center')    # GAN part labels\n    ax.text(0.7, 0.57, \"Classification\", fontsize=16, ha='center')\n    ax.text(0.26, -0.18, \"Real Images\", fontsize=14, ha='center', color=\"black\",rotation=90.0)\n    ax.text(0.26, 0.38, \"Fake Images\", fontsize=14, ha='center', color=\"black\",rotation=90.0)\n\n    fancybox(ax, xy=(0.07, 0.24), width=0.25, height=0.5, label=\"Generator\", edgecolor=\"grey\") # fancy boxes around generator and discriminator\n    fancybox(ax, xy=(0.48, 0.24), width=0.29, height=0.5, label=\"Discriminator\", edgecolor=\"grey\")\n\n    ax.text(0.68,0.4,'c',fontsize=12,color='black')           # draw the biases in O2, O3, O4 and D8\n    ax.annotate(\"\", xy=(0.695, 0.47), xytext=(0.685, 0.42),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.63,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.65), xytext=(0.29, 0.68),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.43,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.45), xytext=(0.29, 0.48),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.23,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.25), xytext=(0.29, 0.28),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndraw_gan_architecture_full()                                  # draw the GAN \n```", "```py\namin = -5.0; amax = 5.0\n\ninput = np.linspace(amin,amax,100); output = 1.0/(1.0 + np.exp(-1*np.linspace(amin,amax,100))); derriv = output * (1 - output)\n\nplt.subplot(111)\nplt.plot(input,output,color='forestgreen',lw=2,label='Activation'); add_grid2(); plt.xlabel('Input'); plt.ylabel('Output')\nplt.plot(input,derriv,color='deepskyblue',lw=2,label='Derrivative'); add_grid2(); plt.xlabel('Input'); plt.ylabel('Output') \nplt.title('Sigmoid Activation'); plt.legend(loc='upper left'); plt.xlim([amin,amax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndef train_gan(epochs=1000,batch_size=32,lr_g=0.1,lr_d=0.1,verbose=True): # function for training the GAN\n    weights_epoch_list = []; weights_g_list = []\n    # Initialize weights\n    weights_g = initialize_generator_weights()\n    weights_d = initialize_discriminator_weights()\n\n    # Tracking losses\n    generator_losses = []\n    discriminator_losses = []\n\n    real_images = generate_real_data(batch_size) # one set of images\n    for epoch in range(epochs):\n        # Step 1: Generate real data\n\n        I5_real, I6_real, I7_real = real_images[:,0], real_images[:,1], real_images[:,2]\n        y_real = np.ones(batch_size)\n\n        # Step 2: Generate fake data\n        L1_fake = np.random.uniform(0.4, 1, batch_size)\n        fake_images = generator_forward(L1_fake, weights_g,return_pre_activation=False)\n        I5_fake, I6_fake, I7_fake = fake_images[:,0], fake_images[:,1], fake_images[:,2]\n        y_fake = np.zeros(batch_size)\n\n        # Combine for discriminator training\n        I5_combined = np.concatenate([I5_real, I5_fake])\n        I6_combined = np.concatenate([I6_real, I6_fake])\n        I7_combined = np.concatenate([I7_real, I7_fake])\n        y_combined = np.concatenate([y_real, y_fake])\n\n        # Step 3: Train discriminator\n        grads_d = discriminator_gradients(I5_combined, I6_combined, I7_combined, y_combined, weights_d)\n        for key in weights_d:\n            weights_d[key] -= lr_d * grads_d[key]\n\n        # Step 4: Train generator\n        L1_gen = np.random.uniform(0.4, 1, batch_size)\n        grads_g = generator_gradients(L1_gen, weights_g, weights_d)\n        for key in weights_g:\n            weights_g[key] -= lr_g * grads_g[key]\n        # if epoch in [1000,2500,5000]: # save the weights to visualize model improvement over epochs\n            # weights_g_list.append(weights_g)\n\n        # Step 5: Calculate and store losses\n        y_pred_real, _ = discriminator_forward(I5_real, I6_real, I7_real, weights_d)\n        y_pred_fake, _ = discriminator_forward(I5_fake, I6_fake, I7_fake, weights_d)\n        loss_d_real = -np.mean(np.log(y_pred_real + 1e-8))\n        loss_d_fake = -np.mean(np.log(1 - y_pred_fake + 1e-8))\n        loss_d = loss_d_real + loss_d_fake\n        discriminator_losses.append(loss_d)\n\n        y_pred_gen, _ = discriminator_forward(*generator_forward(L1_gen, weights_g).T, weights_d)\n        loss_g = -np.mean(np.log(y_pred_gen + 1e-8))\n        generator_losses.append(loss_g)\n\n        # Print progress\n        if verbose and (epoch % 5000 == 0 or epoch == epochs - 1):\n            weights_epoch_list.append(epoch)\n            weights_g_list.append(copy.deepcopy(weights_g))\n            print(f\"Epoch {epoch}: D_loss = {loss_d:.4f}, G_loss = {loss_g:.4f}\")\n\n    # # Final output\n    # print(\"\\nTraining complete.\\nFinal Generator Weights:\")\n    # for k, v in weights_g.items():\n    #     print(f\"  {k}: {v:.4f}\")\n\n    # print(\"\\nFinal Discriminator Weights:\")\n    # for k, v in weights_d.items():\n    #     print(f\"  {k}: {v:.4f}\")\n\n    return weights_g, weights_d, generator_losses, discriminator_losses, real_images, weights_g_list, weights_epoch_list\n\nnp.random.seed(seed=seed)\nepochs = 50000\nbatch_size = 32\nfinal_weights_g, final_weights_d, loss_g, loss_d, real_images, weights_g_list, weights_epoch_list = train_gan(epochs=epochs, batch_size=batch_size, lr_g=0.001, lr_d=0.001)\n\nplt.plot(np.arange(1,epochs+1,1),loss_g,color='green',label='Generator')\nplt.plot(np.arange(1,epochs+1,1),loss_d,color='red',label='Discriminator')\nplt.xlim(1,epochs); plt.ylim(0,2.0); plt.ylabel('Loss'); plt.xlabel('Epoch'); plt.legend(loc='upper right')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nEpoch 0: D_loss = 1.3433, G_loss = 0.6666 \n```", "```py\nEpoch 5000: D_loss = 1.0994, G_loss = 0.7903 \n```", "```py\nEpoch 10000: D_loss = 1.0035, G_loss = 0.8478 \n```", "```py\nEpoch 15000: D_loss = 0.9899, G_loss = 0.8271 \n```", "```py\nEpoch 20000: D_loss = 1.0441, G_loss = 0.8124 \n```", "```py\nEpoch 25000: D_loss = 1.0621, G_loss = 0.8129 \n```", "```py\nEpoch 30000: D_loss = 1.1218, G_loss = 0.8277 \n```", "```py\nEpoch 35000: D_loss = 1.1224, G_loss = 0.8411 \n```", "```py\nEpoch 40000: D_loss = 1.1631, G_loss = 0.7914 \n```", "```py\nEpoch 45000: D_loss = 1.1779, G_loss = 0.7685 \n```", "```py\nEpoch 49999: D_loss = 1.1607, G_loss = 0.7617 \n```", "```py\nL1_test = np.random.uniform(0.4, 1, batch_size) \ntrained_fake = generator_forward(L1_test, weights_g_list[-1])\nuntrained_fake = generator_forward(L1_test, weights_g_list[0])\n\nplt.subplot(121)\nfor i in range(0,batch_size):\n    plt.plot(real_images[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(real_images[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Real Images'); plt.xlabel('Value'); plt.ylabel('Node')\n    plt.ylabel('Pixel'); plt.yticks([1, 2, 3]); plt.ylim([3.2,0.8])\n\nplt.subplot(122)\nfor i in range(0,batch_size):\n    plt.plot(trained_fake[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(trained_fake[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Trained Generator Fake Images'); plt.xlabel('Value'); plt.ylabel('Node')\n    plt.yticks([1, 2, 3], [r'$O_2$', r'$O_3$', r'$O_4$']); plt.ylim([3.2,0.8])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfor i,weights_g in enumerate(weights_g_list):\n    fake = generator_forward(L1_test, weights_g_list[i])\n    plt.subplot(4,3,i+1)\n    for j in range(0,batch_size):\n        plt.plot(fake[j],np.arange(1,4,1),alpha=0.3)\n        plt.scatter(fake[j],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Fake Images: Training Epoch ' + str(weights_epoch_list[i])); plt.xlabel('Value'); plt.ylabel('Output Node')\n    plt.yticks([1, 2, 3], [r'$O_2$', r'$O_3$', r'$O_4$']); plt.ylim([3.2,0.8])\n\nplt.subplot(4,3,12)\nfor i in range(0,batch_size):\n    plt.plot(real_images[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(real_images[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Real Images'); plt.xlabel('Value'); plt.ylabel('Pixel'); plt.yticks([1, 2, 3]); plt.ylim([3.2,0.8])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=3.2, wspace=0.1, hspace=0.3); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True                                      # toggle to supress warnings\nimport os                                                     # set working directory\nimport numpy as np                                            # arrays and matrix math\nimport matplotlib.pyplot as plt                               # for plotting\nimport matplotlib.patches as patches                          # fancy box around agents\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom matplotlib.ticker import FuncFormatter\nimport copy                                                   # deep copy dictionaries\nimport math\nseed = 42\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 42                                                     # random number seed for workflow repeatability \n```", "```py\ndef sigmoid(x):                                               # sigmoid activation function\n    return 1 / (1 + np.exp(-x))\n\ndef generate_real_data(batch_size, slope_range=(-0.4, -0.7), residual_std=0.02): # make a synthetic training image set\n  \"\"\"\n Generate real 3-node images with decreasing linear trend plus noise.\n Standardize each to have mean 0.5.\n Returns shape (batch_size, 3)\n \"\"\"\n    slopes = np.random.uniform(slope_range[0], slope_range[1], size=batch_size)\n    base = np.array([1, 2, 3])  # node positions for linear trend\n    data = np.zeros((batch_size, 3))\n    for i in range(batch_size):\n        trend = slopes[i] * base\n        residual = np.random.normal(0, residual_std, size=3)\n        sample = trend + residual\n        sample += 0.5 - np.mean(sample)                       # standardize to mean 0.5\n        data[i] = sample\n    return data\n\ndef initialize_generator_weights():                           # initialize the generator weights and return as a dictionary\n    # Small random weights and bias for generator\n    return {\n        'lambda_12': np.random.randn() * 0.1,\n        'lambda_13': np.random.randn() * 0.1,\n        'lambda_14': np.random.randn() * 0.1,\n        'b': 0.0\n    }\n\ndef generator_forward(L1_latent, weights,return_pre_activation=False): # given latent vector and generator weights return a set of fake images\n  \"\"\"\n L1_latent: ndarray shape (batch_size,)\n weights: dict with keys 'lambda_1_2', 'lambda_1_3', 'lambda_1_4', 'b'\n Returns output ndarray shape (batch_size, 3)\n \"\"\"\n    O2in = weights['lambda_12'] * L1_latent + weights['b']\n    O3in = weights['lambda_13'] * L1_latent + weights['b']\n    O4in = weights['lambda_14'] * L1_latent + weights['b']\n\n    O2 = sigmoid(O2in)\n    O3 = sigmoid(O3in)\n    O4 = sigmoid(O4in)\n\n    Oin = np.vstack([O2in, O3in, O4in]).T; O = np.vstack([O2, O3, O4]).T; \n\n    if return_pre_activation: \n        return np.vstack([O2in, O3in, O4in]).T, np.vstack([O2, O3, O4]).T # shape (batch_size, 3)\n    else:\n        return np.vstack([O2, O3, O4]).T # shape (batch_size, 3)\n\ndef initialize_discriminator_weights():                       # initialize the discriminator weights and return as a dictionary \n    return {\n        'lambda_58': np.random.randn() * 0.1,\n        'lambda_68': np.random.randn() * 0.1,\n        'lambda_78': np.random.randn() * 0.1,\n        'c': 0.0\n    }\n\ndef discriminator_forward(I5, I6, I7, weights):               # given a set of images return the discriminator probability of real image\n  \"\"\"\n Inputs: I5, I6, I7 shape (batch_size,)\n weights: dict with keys 'lambda_58', 'lambda_68', 'lambda_78', 'c'\n Returns probability ndarray shape (batch_size,)\n \"\"\"\n    dO8in = (weights['lambda_58'] * I5 + \n         weights['lambda_68'] * I6 + \n         weights['lambda_78'] * I7 + \n         weights['c'])\n    dO8 = sigmoid(dO8in)\n    return dO8, dO8in\n\ndef discriminator_gradients(I5, I6, I7, y_true, weights):     # given set of images, true labels, and discriminator weights return the gradients\n  \"\"\"\n Compute discriminator gradients averaged over batch.\n y_true: labels (1 for real, 0 for fake), shape (batch_size,)\n \"\"\"\n    batch_size = y_true.shape[0]\n    y_pred, z = discriminator_forward(I5, I6, I7, weights)\n    dO8in = (y_pred - y_true)                                  # shape (batch_size,), note this solution integrates the sigmoid activation at O8\n\n    grad_lambda_58 = np.mean(dO8in * I5)\n    grad_lambda_68 = np.mean(dO8in * I6)\n    grad_lambda_78 = np.mean(dO8in * I7)\n    grad_c = np.mean(dO8in)\n\n    return {\n        'lambda_58': grad_lambda_58,\n        'lambda_68': grad_lambda_68,\n        'lambda_78': grad_lambda_78,\n        'c': grad_c\n    }\n\ndef generator_gradients(L1_latent, weights_g, weights_d):     # given latent vector, generator and discriminator weights return the gradients\n  \"\"\"\n Compute gradients of generator weights using discriminator feedback.\n L1_latent: shape (batch_size,)\n weights_g: generator weights dict\n weights_d: discriminator weights dict\n \"\"\"\n    batch_size = L1_latent.shape[0]\n\n    #O = generator_forward(L1_latent, weights_g)               # generator outputs, shape (batch_size, 3)\n    O_in, O = generator_forward(L1_latent, weights_g, return_pre_activation=True)\n\n    O2_in, O3_in, O4_in = O_in[:, 0], O_in[:, 1], O_in[:, 2]\n    O2, O3, O4 = O[:,0], O[:,1], O[:,2]\n    I5, I6, I7 = O[:,0], O[:,1], O[:,2]\n\n    y_pred, z = discriminator_forward(I5, I6, I7, weights_d)  # discriminator forward pass\n\n    dO8in = y_pred - 1                                           # gradient of loss w.r.t discriminator logit for generator loss, shape (batch_size,)\n\n    dO2 = dO8in * weights_d['lambda_58']                         # gradients w.r.t generator outputs\n    dO3 = dO8in * weights_d['lambda_68']\n    dO4 = dO8in * weights_d['lambda_78']\n\n    dO2in = dO2 * O2 * (1 - O2)                                  # Backprop through generator sigmoid activation\n    dO3in = dO3 * O3 * (1 - O3)\n    dO4in = dO4 * O4 * (1 - O4)\n\n    grad_lambda_12 = np.mean(dO2in * L1_latent)                 # gradients w.r.t generator weights and bias\n    grad_lambda_13 = np.mean(dO3in * L1_latent)\n    grad_lambda_14 = np.mean(dO4in * L1_latent)\n    grad_b = np.mean(dO2in + dO3in + dO4in)\n\n    return {\n        'lambda_12': grad_lambda_12,\n        'lambda_13': grad_lambda_13,\n        'lambda_14': grad_lambda_14,\n        'b': grad_b\n    }\n\ndef fancybox(ax, xy, width, height, label=\"\", edgecolor=\"black\", text_color=None): # a dashed fancy box for the GAN plot\n  \"\"\"\n Draws a dashed, rounded rectangle on a given axes.\n\n Parameters:\n - ax: The matplotlib axes to draw on\n - xy: (x, y) tuple for bottom-left corner of the box\n - width: Width of the box\n - height: Height of the box\n - label: Optional label text to display centered above the box\n - edgecolor: Border color of the box\n - text_color: Color of the label text (defaults to edgecolor)\n \"\"\"\n    if text_color is None:\n        text_color = edgecolor\n\n    box = patches.FancyBboxPatch(                             # draw box\n        xy,\n        width,\n        height,\n        boxstyle=\"round,pad=0.02\",\n        linewidth=2,\n        edgecolor=edgecolor,\n        facecolor=\"none\",\n        linestyle='--'\n    )\n    ax.add_patch(box)\n\n    x_center = xy[0] + width / 2                              # add label text above the box\n    y_top = xy[1] + height + 0.02\n    ax.text(x_center, y_top + 0.02, label, ha='center', va='bottom', fontsize=16, color=text_color)\n\ndef add_grid2():                                              # add grid lines\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\ncolors = ['darksalmon','deepskyblue','gold']                  # line colors for latent to fake to probability flow\ncolors_real = ['orangered','dodgerblue','goldenrod']          # line colors for real image to probability flow\n\ndef draw_gan_architecture_full():                             # function to draw the GAN demonstrated in this workflow\n    fig, ax = plt.subplots(figsize=(12, 7))\n\n    ax.text(0.1, 0.5, r\"$L_1$\", fontsize=14, ha='center', va='center', # generator latent node\n            bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n\n    gen_outputs = [                                           # generator output nodes locations and labels\n        (0.3, 0.7, r\"$O_2$\", r\"$\\lambda_{1,2}$\"),\n        (0.3, 0.5, r\"$O_3$\", r\"$\\lambda_{1,3}$\"),\n        (0.3, 0.3, r\"$O_4$\", r\"$\\lambda_{1,4}$\")\n    ]\n    for i, (x, y, label, weight) in enumerate(gen_outputs):   # generator output node labels\n        ax.text(x, y, label, fontsize=14, ha='center', va='center',\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\", xy=(x - 0.05, y), xytext=(0.15, 0.5),   # generator output node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n        ax.text((x + 0.11) / 2, (y + 0.5) / 2, weight, fontsize=12, ha='center', va='bottom')\n\n    disc_inputs = [                                           # discriminator input nodes locations and labels\n        (0.5, 0.7, r\"$I_5$\", r\"$\\lambda_{5,8}$\"),\n        (0.5, 0.5, r\"$I_6$\", r\"$\\lambda_{6,8}$\"),\n        (0.5, 0.3, r\"$I_7$\", r\"$\\lambda_{7,8}$\")\n    ]\n    for i, (x, y, label, _) in enumerate(disc_inputs):        # discriminator input node labels\n        ax.text(x, y, label, fontsize=14, ha='center', va='center',\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\", xy=(x - 0.05, y), xytext=(0.35, gen_outputs[i][1]), # discriminator input node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n\n    ax.text(0.7, 0.5, r\"$D_8$\", fontsize=14, ha='center', va='center', # discriminator decision node label\n            bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n    for i, (x, y, _, weight) in enumerate(disc_inputs):\n        ax.annotate(\"\",xy=(0.65, 0.5), xytext=(x + 0.05, y),  # discriminator output node connections\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors[i]))\n        ax.text((x + 0.7) / 2, (y + 0.5) / 2, weight, fontsize=12, ha='center', va='bottom')\n\n    real_inputs = [                                           # real data path below generator\n        (0.3, 0.1, r\"$I_5^{real}$\"),\n        (0.3, -0.1, r\"$I_6^{real}$\"),\n        (0.3, -0.3, r\"$I_7^{real}$\")\n    ]\n    for i, (x, y, label) in enumerate(real_inputs):\n        ax.text(x, y, label, fontsize=14, ha='center', va='center', # label real data node labels\n                bbox=dict(boxstyle=\"circle\", fc=\"white\", ec=\"black\"))\n        ax.annotate(\"\",                                       # arrow to discriminator inputs\n                    xy=(disc_inputs[i][0] - 0.05, disc_inputs[i][1]), \n                    xytext=(x + 0.05, y),\n                    arrowprops=dict(arrowstyle=\"->\", lw=2, color=colors_real[i], linestyle=\"--\"))\n\n    ax.text(0.1, 0.57, \"Latent\", fontsize=16, ha='center')    # GAN part labels\n    ax.text(0.7, 0.57, \"Classification\", fontsize=16, ha='center')\n    ax.text(0.26, -0.18, \"Real Images\", fontsize=14, ha='center', color=\"black\",rotation=90.0)\n    ax.text(0.26, 0.38, \"Fake Images\", fontsize=14, ha='center', color=\"black\",rotation=90.0)\n\n    fancybox(ax, xy=(0.07, 0.24), width=0.25, height=0.5, label=\"Generator\", edgecolor=\"grey\") # fancy boxes around generator and discriminator\n    fancybox(ax, xy=(0.48, 0.24), width=0.29, height=0.5, label=\"Discriminator\", edgecolor=\"grey\")\n\n    ax.text(0.68,0.4,'c',fontsize=12,color='black')           # draw the biases in O2, O3, O4 and D8\n    ax.annotate(\"\", xy=(0.695, 0.47), xytext=(0.685, 0.42),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.63,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.65), xytext=(0.29, 0.68),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.43,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.45), xytext=(0.29, 0.48),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.text(0.27,0.23,'b',fontsize=12,color='black')\n    ax.annotate(\"\", xy=(0.28, 0.25), xytext=(0.29, 0.28),arrowprops=dict(arrowstyle=\"-\", color=\"black\", lw=1))\n    ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndraw_gan_architecture_full()                                  # draw the GAN \n```", "```py\namin = -5.0; amax = 5.0\n\ninput = np.linspace(amin,amax,100); output = 1.0/(1.0 + np.exp(-1*np.linspace(amin,amax,100))); derriv = output * (1 - output)\n\nplt.subplot(111)\nplt.plot(input,output,color='forestgreen',lw=2,label='Activation'); add_grid2(); plt.xlabel('Input'); plt.ylabel('Output')\nplt.plot(input,derriv,color='deepskyblue',lw=2,label='Derrivative'); add_grid2(); plt.xlabel('Input'); plt.ylabel('Output') \nplt.title('Sigmoid Activation'); plt.legend(loc='upper left'); plt.xlim([amin,amax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndef train_gan(epochs=1000,batch_size=32,lr_g=0.1,lr_d=0.1,verbose=True): # function for training the GAN\n    weights_epoch_list = []; weights_g_list = []\n    # Initialize weights\n    weights_g = initialize_generator_weights()\n    weights_d = initialize_discriminator_weights()\n\n    # Tracking losses\n    generator_losses = []\n    discriminator_losses = []\n\n    real_images = generate_real_data(batch_size) # one set of images\n    for epoch in range(epochs):\n        # Step 1: Generate real data\n\n        I5_real, I6_real, I7_real = real_images[:,0], real_images[:,1], real_images[:,2]\n        y_real = np.ones(batch_size)\n\n        # Step 2: Generate fake data\n        L1_fake = np.random.uniform(0.4, 1, batch_size)\n        fake_images = generator_forward(L1_fake, weights_g,return_pre_activation=False)\n        I5_fake, I6_fake, I7_fake = fake_images[:,0], fake_images[:,1], fake_images[:,2]\n        y_fake = np.zeros(batch_size)\n\n        # Combine for discriminator training\n        I5_combined = np.concatenate([I5_real, I5_fake])\n        I6_combined = np.concatenate([I6_real, I6_fake])\n        I7_combined = np.concatenate([I7_real, I7_fake])\n        y_combined = np.concatenate([y_real, y_fake])\n\n        # Step 3: Train discriminator\n        grads_d = discriminator_gradients(I5_combined, I6_combined, I7_combined, y_combined, weights_d)\n        for key in weights_d:\n            weights_d[key] -= lr_d * grads_d[key]\n\n        # Step 4: Train generator\n        L1_gen = np.random.uniform(0.4, 1, batch_size)\n        grads_g = generator_gradients(L1_gen, weights_g, weights_d)\n        for key in weights_g:\n            weights_g[key] -= lr_g * grads_g[key]\n        # if epoch in [1000,2500,5000]: # save the weights to visualize model improvement over epochs\n            # weights_g_list.append(weights_g)\n\n        # Step 5: Calculate and store losses\n        y_pred_real, _ = discriminator_forward(I5_real, I6_real, I7_real, weights_d)\n        y_pred_fake, _ = discriminator_forward(I5_fake, I6_fake, I7_fake, weights_d)\n        loss_d_real = -np.mean(np.log(y_pred_real + 1e-8))\n        loss_d_fake = -np.mean(np.log(1 - y_pred_fake + 1e-8))\n        loss_d = loss_d_real + loss_d_fake\n        discriminator_losses.append(loss_d)\n\n        y_pred_gen, _ = discriminator_forward(*generator_forward(L1_gen, weights_g).T, weights_d)\n        loss_g = -np.mean(np.log(y_pred_gen + 1e-8))\n        generator_losses.append(loss_g)\n\n        # Print progress\n        if verbose and (epoch % 5000 == 0 or epoch == epochs - 1):\n            weights_epoch_list.append(epoch)\n            weights_g_list.append(copy.deepcopy(weights_g))\n            print(f\"Epoch {epoch}: D_loss = {loss_d:.4f}, G_loss = {loss_g:.4f}\")\n\n    # # Final output\n    # print(\"\\nTraining complete.\\nFinal Generator Weights:\")\n    # for k, v in weights_g.items():\n    #     print(f\"  {k}: {v:.4f}\")\n\n    # print(\"\\nFinal Discriminator Weights:\")\n    # for k, v in weights_d.items():\n    #     print(f\"  {k}: {v:.4f}\")\n\n    return weights_g, weights_d, generator_losses, discriminator_losses, real_images, weights_g_list, weights_epoch_list\n\nnp.random.seed(seed=seed)\nepochs = 50000\nbatch_size = 32\nfinal_weights_g, final_weights_d, loss_g, loss_d, real_images, weights_g_list, weights_epoch_list = train_gan(epochs=epochs, batch_size=batch_size, lr_g=0.001, lr_d=0.001)\n\nplt.plot(np.arange(1,epochs+1,1),loss_g,color='green',label='Generator')\nplt.plot(np.arange(1,epochs+1,1),loss_d,color='red',label='Discriminator')\nplt.xlim(1,epochs); plt.ylim(0,2.0); plt.ylabel('Loss'); plt.xlabel('Epoch'); plt.legend(loc='upper right')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nEpoch 0: D_loss = 1.3433, G_loss = 0.6666 \n```", "```py\nEpoch 5000: D_loss = 1.0994, G_loss = 0.7903 \n```", "```py\nEpoch 10000: D_loss = 1.0035, G_loss = 0.8478 \n```", "```py\nEpoch 15000: D_loss = 0.9899, G_loss = 0.8271 \n```", "```py\nEpoch 20000: D_loss = 1.0441, G_loss = 0.8124 \n```", "```py\nEpoch 25000: D_loss = 1.0621, G_loss = 0.8129 \n```", "```py\nEpoch 30000: D_loss = 1.1218, G_loss = 0.8277 \n```", "```py\nEpoch 35000: D_loss = 1.1224, G_loss = 0.8411 \n```", "```py\nEpoch 40000: D_loss = 1.1631, G_loss = 0.7914 \n```", "```py\nEpoch 45000: D_loss = 1.1779, G_loss = 0.7685 \n```", "```py\nEpoch 49999: D_loss = 1.1607, G_loss = 0.7617 \n```", "```py\nL1_test = np.random.uniform(0.4, 1, batch_size) \ntrained_fake = generator_forward(L1_test, weights_g_list[-1])\nuntrained_fake = generator_forward(L1_test, weights_g_list[0])\n\nplt.subplot(121)\nfor i in range(0,batch_size):\n    plt.plot(real_images[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(real_images[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Real Images'); plt.xlabel('Value'); plt.ylabel('Node')\n    plt.ylabel('Pixel'); plt.yticks([1, 2, 3]); plt.ylim([3.2,0.8])\n\nplt.subplot(122)\nfor i in range(0,batch_size):\n    plt.plot(trained_fake[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(trained_fake[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Trained Generator Fake Images'); plt.xlabel('Value'); plt.ylabel('Node')\n    plt.yticks([1, 2, 3], [r'$O_2$', r'$O_3$', r'$O_4$']); plt.ylim([3.2,0.8])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nfor i,weights_g in enumerate(weights_g_list):\n    fake = generator_forward(L1_test, weights_g_list[i])\n    plt.subplot(4,3,i+1)\n    for j in range(0,batch_size):\n        plt.plot(fake[j],np.arange(1,4,1),alpha=0.3)\n        plt.scatter(fake[j],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Fake Images: Training Epoch ' + str(weights_epoch_list[i])); plt.xlabel('Value'); plt.ylabel('Output Node')\n    plt.yticks([1, 2, 3], [r'$O_2$', r'$O_3$', r'$O_4$']); plt.ylim([3.2,0.8])\n\nplt.subplot(4,3,12)\nfor i in range(0,batch_size):\n    plt.plot(real_images[i],np.arange(1,4,1),alpha=0.3)\n    plt.scatter(real_images[i],np.arange(1,4,1),edgecolor='black',zorder=10)\n    plt.title('Real Images'); plt.xlabel('Value'); plt.ylabel('Pixel'); plt.yticks([1, 2, 3]); plt.ylim([3.2,0.8])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=3.2, wspace=0.1, hspace=0.3); plt.show() \n```"]