- en: 8.3 Training neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8.3 è®­ç»ƒç¥ç»ç½‘ç»œ
- en: åŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html](https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html](https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html)
- en: ğŸŒ³ **Tip** ğŸŒ³
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸŒ³ **æç¤º** ğŸŒ³
- en: 'For more tips on training neural networks, check out:'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šå…³äºè®­ç»ƒç¥ç»ç½‘ç»œçš„æŠ€å·§ï¼Œè¯·æŸ¥çœ‹ï¼š
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
    (Karpathy 2019)'
  id: totrans-5
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜ç±](http://karpathy.github.io/2019/04/25/recipe/) (Karpathy 2019)'
- en: '[NLP''s Clever Hans Moment has Arrived](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)
    (Heinzerling 2019): an excellent writeup on trying to understand what exactly
    your neural network learns, and techniques to ensure that your model works correctly
    with textual data.'
  id: totrans-6
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ Clever Hans æ—¶åˆ»å·²ç»åˆ°æ¥](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)
    (Heinzerling 2019)ï¼šä¸€ç¯‡å…³äºå°è¯•ç†è§£ä½ çš„ç¥ç»ç½‘ç»œç©¶ç«Ÿå­¦ä¹ äº†ä»€ä¹ˆï¼Œä»¥åŠç¡®ä¿ä½ çš„æ¨¡å‹åœ¨æ–‡æœ¬æ•°æ®ä¸Šæ­£ç¡®å·¥ä½œçš„æŠ€æœ¯çš„ä¼˜ç§€æ–‡ç« ã€‚'
- en: '[An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html)
    (Ruder 2016)'
  id: totrans-7
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•æ¦‚è¿°](http://ruder.io/optimizing-gradient-descent/index.html) (Ruder
    2016)'
- en: '[E] When building a neural network, should you overfit or underfit it first?'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] å½“æ„å»ºç¥ç»ç½‘ç»œæ—¶ï¼Œä½ åº”è¯¥å…ˆè¿‡åº¦æ‹Ÿåˆè¿˜æ˜¯æ¬ æ‹Ÿåˆï¼Ÿ'
- en: '[E] Write the vanilla gradient update.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] å†™å‡ºvanillaæ¢¯åº¦æ›´æ–°ã€‚'
- en: Neural network in simple Numpy.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç®€å•Numpyä¸­çš„ç¥ç»ç½‘ç»œã€‚
- en: '[E] Write in plain NumPy the forward and backward pass for a two-layer feed-forward
    neural network with a ReLU layer in between.'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ç”¨çº¯NumPyç¼–å†™å…·æœ‰ReLUå±‚çš„ä¸¤å±‚å‰é¦ˆç¥ç»ç½‘ç»œçš„æ­£å‘å’Œåå‘ä¼ é€’ã€‚'
- en: '[M] Implement vanilla dropout for the forward and backward pass in NumPy.'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åœ¨NumPyä¸­å®ç°vanilla dropoutçš„å‰å‘å’Œåå‘ä¼ é€’ã€‚'
- en: Activation functions.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°ã€‚
- en: '[E] Draw the graphs for sigmoid, tanh, ReLU, and leaky ReLU.'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ç»˜åˆ¶sigmoidã€tanhã€ReLUå’Œleaky ReLUçš„å›¾è¡¨ã€‚'
- en: '[E] Pros and cons of each activation function.'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æ¯ä¸ªæ¿€æ´»å‡½æ•°çš„ä¼˜ç¼ºç‚¹ã€‚'
- en: '[E] Is ReLU differentiable? What to do when itâ€™s not differentiable?'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ReLUå¯å¾®å—ï¼Ÿå½“å®ƒä¸å¯å¾®æ—¶æ€ä¹ˆåŠï¼Ÿ'
- en: '[M] Derive derivatives for sigmoid function  when  is a vector.'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] å½“æ˜¯å‘é‡æ—¶ï¼Œæ¨å¯¼sigmoidå‡½æ•°çš„å¯¼æ•°ã€‚'
- en: '[E] Whatâ€™s the motivation for skip connection in neural works?'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] è·³è·ƒè¿æ¥åœ¨ç¥ç»ç½‘ç»œä¸­çš„åŠ¨æœºæ˜¯ä»€ä¹ˆï¼Ÿ'
- en: Vanishing and exploding gradients.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¶ˆå¤±å’Œçˆ†ç‚¸æ¢¯åº¦ã€‚
- en: '[E] How do we know that gradients are exploding? How do we prevent it?'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æˆ‘ä»¬å¦‚ä½•çŸ¥é“æ¢¯åº¦æ­£åœ¨çˆ†ç‚¸ï¼Ÿæˆ‘ä»¬å¦‚ä½•é˜²æ­¢å®ƒï¼Ÿ'
- en: '[E] Why are RNNs especially susceptible to vanishing and exploding gradients?'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä¸ºä»€ä¹ˆRNNç‰¹åˆ«å®¹æ˜“å—åˆ°æ¶ˆå¤±å’Œçˆ†ç‚¸æ¢¯åº¦çš„å½±å“ï¼Ÿ'
- en: '[M] Weight normalization separates a weight vectorâ€™s norm from its gradient.
    How would it help with training?'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] æƒé‡å½’ä¸€åŒ–å°†æƒé‡å‘é‡çš„èŒƒæ•°ä¸å…¶æ¢¯åº¦åˆ†å¼€ã€‚è¿™å¦‚ä½•æœ‰åŠ©äºè®­ç»ƒï¼Ÿ'
- en: '[M] When training a large neural network, say a language model with a billion
    parameters, you evaluate your model on a validation set at the end of every epoch.
    You realize that your validation loss is often lower than your train loss. What
    might be happening?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] å½“è®­ç»ƒä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œï¼Œæ¯”å¦‚ä¸€ä¸ªæ‹¥æœ‰åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹æ—¶ï¼Œä½ ä¼šåœ¨æ¯ä¸ªepochç»“æŸæ—¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ä½ çš„æ¨¡å‹ã€‚ä½ å‘ç°éªŒè¯æŸå¤±é€šå¸¸ä½äºè®­ç»ƒæŸå¤±ã€‚å¯èƒ½å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ'
- en: '[E] What criteria would you use for early stopping?'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä½ ä¼šä½¿ç”¨ä»€ä¹ˆæ ‡å‡†æ¥å†³å®šæå‰åœæ­¢ï¼Ÿ'
- en: '[E] Gradient descent vs SGD vs mini-batch SGD.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æ¢¯åº¦ä¸‹é™ vs SGD vs å°æ‰¹é‡SGDã€‚'
- en: '[H] Itâ€™s a common practice to train deep learning models using epochs: we sample
    batches from data **without** replacement. Why would we use epochs instead of
    just sampling data **with** replacement?'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] ä½¿ç”¨epochæ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼šæˆ‘ä»¬ä»æ•°æ®ä¸­é‡‡æ ·æ‰¹æ¬¡è€Œä¸è¿›è¡Œæ›¿æ¢ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šä½¿ç”¨epochè€Œä¸æ˜¯ä»…ä»…ç”¨æ›¿æ¢çš„æ–¹å¼æ¥é‡‡æ ·æ•°æ®ï¼Ÿ'
- en: '[M] Your modelâ€™ weights fluctuate a lot during training. How does that affect
    your modelâ€™s performance? What to do about it?'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä½ çš„æ¨¡å‹æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¢åŠ¨å¾ˆå¤§ã€‚è¿™å¦‚ä½•å½±å“ä½ çš„æ¨¡å‹æ€§èƒ½ï¼Ÿä½ è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ'
- en: Learning rate.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ã€‚
- en: '[E] Draw a graph number of training epochs vs training error for when the learning
    rate is:'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ç»˜åˆ¶å½“å­¦ä¹ ç‡ä¸ºæ—¶çš„è®­ç»ƒè½®æ•°ä¸è®­ç»ƒè¯¯å·®çš„å›¾è¡¨ã€‚'
- en: too high
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤ªé«˜
- en: too low
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¤ªä½
- en: acceptable.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯æ¥å—çš„ã€‚
- en: '[E] Whatâ€™s learning rate warmup? Why do we need it?'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] å­¦ä¹ ç‡é¢„çƒ­æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å®ƒï¼Ÿ'
- en: '[E] Compare batch norm and layer norm.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æ¯”è¾ƒæ‰¹å½’ä¸€åŒ–å’Œå±‚å½’ä¸€åŒ–ã€‚'
- en: '[M] Why is squared L2 norm sometimes preferred to L2 norm for regularizing
    neural networks?'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä¸ºä»€ä¹ˆå¹³æ–¹L2èŒƒæ•°æœ‰æ—¶æ¯”L2èŒƒæ•°æ›´é€‚åˆç”¨äºæ­£åˆ™åŒ–ç¥ç»ç½‘ç»œï¼Ÿ'
- en: '[E] Some models use weight decay: after each gradient update, the weights are
    multiplied by a factor slightly less than 1\. What is this useful for?'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä¸€äº›æ¨¡å‹ä½¿ç”¨æƒé‡è¡°å‡ï¼šåœ¨æ¯æ¬¡æ¢¯åº¦æ›´æ–°åï¼Œæƒé‡ä¹˜ä»¥ä¸€ä¸ªç•¥å°äº1çš„å› å­ã€‚è¿™æœ‰ä»€ä¹ˆç”¨ï¼Ÿ'
- en: Itâ€™s a common practice for the learning rate to be reduced throughout the training.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é™ä½å­¦ä¹ ç‡æ˜¯ä¸€ç§å¸¸è§åšæ³•ã€‚
- en: '[E] Whatâ€™s the motivation?'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æœ‰ä»€ä¹ˆåŠ¨æœºï¼Ÿ'
- en: '[M] What might be the exceptions?'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] å¯èƒ½æœ‰å“ªäº›ä¾‹å¤–æƒ…å†µï¼Ÿ'
- en: Batch size.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰¹å¤§å°ã€‚
- en: '[E] What happens to your model training when you decrease the batch size to
    1?'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] å½“ä½ å°†æ‰¹å¤§å°å‡å°‘åˆ° 1 æ—¶ï¼Œä½ çš„æ¨¡å‹è®­ç»ƒä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ'
- en: '[E] What happens when you use the entire training data in a batch?'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] å½“ä½ åœ¨ä¸€ä¸ªæ‰¹ä¸­ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ'
- en: '[M] How should we adjust the learning rate as we increase or decrease the batch
    size?'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] éšç€æ‰¹å¤§å°çš„å¢åŠ æˆ–å‡å°‘ï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ'
- en: '[M] Why is Adagrad sometimes favored in problems with sparse gradients?'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä¸ºä»€ä¹ˆ Adagrad åœ¨ç¨€ç–æ¢¯åº¦é—®é¢˜ä¸­æœ‰æ—¶æ›´å—æ¬¢è¿ï¼Ÿ'
- en: Adam vs. SGD.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adam ä¸ SGDã€‚
- en: '[M] What can you say about the ability to converge and generalize of Adam vs.
    SGD?'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä½ èƒ½å¯¹ Adam ä¸ SGD çš„æ”¶æ•›èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›è¯´äº›ä»€ä¹ˆï¼Ÿ'
- en: '[M] What else can you say about the difference between these two optimizers?'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä½ è¿˜èƒ½å¯¹è¿™ä¸¤ç§ä¼˜åŒ–å™¨çš„åŒºåˆ«è¯´äº›ä»€ä¹ˆï¼Ÿ'
- en: '[M] With model parallelism, you might update your model weights using the gradients
    from each machine asynchronously or synchronously. What are the pros and cons
    of asynchronous SGD vs. synchronous SGD?'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åœ¨æ¨¡å‹å¹¶è¡ŒåŒ–ä¸­ï¼Œä½ å¯èƒ½ä½¿ç”¨æ¯ä¸ªæœºå™¨çš„æ¢¯åº¦å¼‚æ­¥æˆ–åŒæ­¥åœ°æ›´æ–°ä½ çš„æ¨¡å‹æƒé‡ã€‚å¼‚æ­¥ SGD ä¸åŒæ­¥ SGD çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ'
- en: '[M] Why shouldnâ€™t we have two consecutive linear layers in a neural network?'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸åº”è¯¥åœ¨ç¥ç»ç½‘ç»œä¸­æœ‰ä¸¤ä¸ªè¿ç»­çš„çº¿æ€§å±‚ï¼Ÿ'
- en: '[M] Can a neural network with only RELU (non-linearity) act as a linear classifier?'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åªä½¿ç”¨ RELU (éçº¿æ€§) çš„ç¥ç»ç½‘ç»œèƒ½ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨å—ï¼Ÿ'
- en: '[M] Design the smallest neural network that can function as an XOR gate.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] è®¾è®¡ä¸€ä¸ªæœ€å°çš„ç¥ç»ç½‘ç»œï¼Œä½¿å…¶èƒ½ä½œä¸º XOR é—¨ä½¿ç”¨ã€‚'
- en: '[E] Why donâ€™t we just initialize all weights in a neural network to zero?'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ç›´æ¥å°†ç¥ç»ç½‘ç»œä¸­çš„æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸ºé›¶ï¼Ÿ'
- en: Stochasticity.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: éšæœºæ€§ã€‚
- en: '[M] What are some sources of randomness in a neural network?'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ç¥ç»ç½‘ç»œä¸­çš„ä¸€äº›éšæœºæ€§æ¥æºæœ‰å“ªäº›ï¼Ÿ'
- en: '[M] Sometimes stochasticity is desirable when training neural networks. Why
    is that?'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œéšæœºæ€§æœ‰æ—¶æ˜¯å¯å–çš„ã€‚ä¸ºä»€ä¹ˆï¼Ÿ'
- en: Dead neuron.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ­»ç¥ç»å…ƒã€‚
- en: '[E] Whatâ€™s a dead neuron?'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æ­»ç¥ç»å…ƒæ˜¯ä»€ä¹ˆï¼Ÿ'
- en: '[E] How do we detect them in our neural network?'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E] æˆ‘ä»¬å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œä¸­æ£€æµ‹å®ƒä»¬ï¼Ÿ'
- en: '[M] How to prevent them?'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] å¦‚ä½•é˜²æ­¢å®ƒä»¬ï¼Ÿ'
- en: Pruning.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‰ªæã€‚
- en: '[M] Pruning is a popular technique where certain weights of a neural network
    are set to 0\. Why is it desirable?'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] å‰ªææ˜¯ä¸€ç§æµè¡Œçš„æŠ€æœ¯ï¼Œå…¶ä¸­ç¥ç»ç½‘ç»œçš„ä¸€äº›æƒé‡è¢«è®¾ç½®ä¸º 0ã€‚ä¸ºä»€ä¹ˆè¿™æ˜¯å¯å–çš„ï¼Ÿ'
- en: '[M] How do you choose what to prune from a neural network?'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M] ä½ å¦‚ä½•é€‰æ‹©ä»ç¥ç»ç½‘ç»œä¸­å‰ªæçš„å†…å®¹ï¼Ÿ'
- en: '[H] Under what conditions would it be possible to recover training data from
    the weight checkpoints?'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] åœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹å¯ä»¥ä»æƒé‡æ£€æŸ¥ç‚¹ä¸­æ¢å¤è®­ç»ƒæ•°æ®ï¼Ÿ'
- en: '[H] Why do we try to reduce the size of a big trained model through techniques
    such as knowledge distillation instead of just training a small model from the
    beginning?'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H] ä¸ºä»€ä¹ˆæˆ‘ä»¬è¯•å›¾é€šè¿‡çŸ¥è¯†è’¸é¦ç­‰æŠ€æœ¯æ¥å‡å°å¤§å‹è®­ç»ƒæ¨¡å‹çš„å¤§å°ï¼Œè€Œä¸æ˜¯ä»ä¸€å¼€å§‹å°±è®­ç»ƒä¸€ä¸ªå°çš„æ¨¡å‹ï¼Ÿ'
- en: '* * *'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright Â©2021 Chip Huyen.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿™æœ¬ä¹¦æ˜¯ç”± [Chip Huyen](https://huyenchip.com) åœ¨ä¼—å¤šæœ‹å‹çš„å¸®åŠ©ä¸‹åˆ›ä½œçš„ã€‚å¯¹äºåé¦ˆã€å‹˜è¯¯å’Œå»ºè®®ï¼Œä½œè€…å¯ä»¥é€šè¿‡[è¿™é‡Œ](https://huyenchip.com/communication/)è”ç³»ã€‚ç‰ˆæƒÂ©2021
    Chip Huyenã€‚*'
