- en: 8.3 Training neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html](https://huyenchip.com/ml-interviews-book/contents/8.3-training-neural-networks.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ðŸŒ³ **Tip** ðŸŒ³
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For more tips on training neural networks, check out:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
    (Karpathy 2019)'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '[NLP''s Clever Hans Moment has Arrived](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)
    (Heinzerling 2019): an excellent writeup on trying to understand what exactly
    your neural network learns, and techniques to ensure that your model works correctly
    with textual data.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html)
    (Ruder 2016)'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '[E] When building a neural network, should you overfit or underfit it first?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Write the vanilla gradient update.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural network in simple Numpy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Write in plain NumPy the forward and backward pass for a two-layer feed-forward
    neural network with a ReLU layer in between.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Implement vanilla dropout for the forward and backward pass in NumPy.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Draw the graphs for sigmoid, tanh, ReLU, and leaky ReLU.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Pros and cons of each activation function.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Is ReLU differentiable? What to do when itâ€™s not differentiable?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Derive derivatives for sigmoid function  when  is a vector.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Whatâ€™s the motivation for skip connection in neural works?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How do we know that gradients are exploding? How do we prevent it?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why are RNNs especially susceptible to vanishing and exploding gradients?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Weight normalization separates a weight vectorâ€™s norm from its gradient.
    How would it help with training?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] When training a large neural network, say a language model with a billion
    parameters, you evaluate your model on a validation set at the end of every epoch.
    You realize that your validation loss is often lower than your train loss. What
    might be happening?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What criteria would you use for early stopping?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Gradient descent vs SGD vs mini-batch SGD.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Itâ€™s a common practice to train deep learning models using epochs: we sample
    batches from data **without** replacement. Why would we use epochs instead of
    just sampling data **with** replacement?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Your modelâ€™ weights fluctuate a lot during training. How does that affect
    your modelâ€™s performance? What to do about it?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Draw a graph number of training epochs vs training error for when the learning
    rate is:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: too high
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: too low
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: acceptable.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Whatâ€™s learning rate warmup? Why do we need it?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Compare batch norm and layer norm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why is squared L2 norm sometimes preferred to L2 norm for regularizing
    neural networks?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Some models use weight decay: after each gradient update, the weights are
    multiplied by a factor slightly less than 1\. What is this useful for?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Itâ€™s a common practice for the learning rate to be reduced throughout the training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Whatâ€™s the motivation?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What might be the exceptions?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Batch size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What happens to your model training when you decrease the batch size to
    1?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] What happens when you use the entire training data in a batch?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How should we adjust the learning rate as we increase or decrease the batch
    size?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why is Adagrad sometimes favored in problems with sparse gradients?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adam vs. SGD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What can you say about the ability to converge and generalize of Adam vs.
    SGD?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What else can you say about the difference between these two optimizers?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] With model parallelism, you might update your model weights using the gradients
    from each machine asynchronously or synchronously. What are the pros and cons
    of asynchronous SGD vs. synchronous SGD?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Why shouldnâ€™t we have two consecutive linear layers in a neural network?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Can a neural network with only RELU (non-linearity) act as a linear classifier?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Design the smallest neural network that can function as an XOR gate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Why donâ€™t we just initialize all weights in a neural network to zero?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stochasticity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] What are some sources of randomness in a neural network?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Sometimes stochasticity is desirable when training neural networks. Why
    is that?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Dead neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] Whatâ€™s a dead neuron?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[E] How do we detect them in our neural network?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How to prevent them?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pruning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] Pruning is a popular technique where certain weights of a neural network
    are set to 0\. Why is it desirable?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[M] How do you choose what to prune from a neural network?'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Under what conditions would it be possible to recover training data from
    the weight checkpoints?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[H] Why do we try to reduce the size of a big trained model through techniques
    such as knowledge distillation instead of just training a small model from the
    beginning?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright Â©2021 Chip Huyen.*'
  prefs: []
  type: TYPE_NORMAL
