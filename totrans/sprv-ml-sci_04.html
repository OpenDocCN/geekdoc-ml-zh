<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2  Bare-Bones Supervised Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2  Bare-Bones Supervised Machine Learning</h1>
<blockquote>原文：<a href="https://ml-science-book.com/supervised-ml.html">https://ml-science-book.com/supervised-ml.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter looks at supervised machine learning, stripping it to its bare-bones. Why supervised machine learning? Of course, the applications of unsupervised learning and reinforcement learning in science are no less interesting. However, after looking at many scientific applications, we realized that the primary goal was often to solve a supervised learning problem. Unsupervised and reinforcement learning techniques have often been used to support this goal, e.g. by providing powerful representations or fine-tuning models.</p>
<p>What is supervised machine learning about? Think back to the tornado prediction example from the introduction.<br/>
Supervised machine learning produces models that provide <em>output predictions</em>, for example, concerning the occurrence of a tornado in the next hour. To obtain these predictions, models must be fed with so-called <em>input feature values</em>, e.g. storm-centered radar images and short-range soundings. The <em>search for models</em> is done via a <em>learning algorithm</em> and a <em>labeled dataset</em>, consisting of <em>input-output pairs</em>, also called training data.</p>
<div class="raven-box">
<p>A young Raven called Rattle was the first to adopt supervised machine learning. At first, the other Raven scientists were skeptical. Too new. Unproven. Risky. Not the way of Raven Science. Nevertheless, Rattle began to explain to the first interested Ravens what machine learning was all about.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../Images/2871cf0d19e22bd49260fcc1f5670a1c.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:35.0%" data-original-src="https://ml-science-book.com/images/raven-supervised.jpg"/></p>
</figure>
</div>
</div>
<section id="describe-the-prediction-task" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="describe-the-prediction-task"><span class="header-section-number">2.1</span> Describe the prediction task</h2>
<p>To use supervised machine learning, you first have to translate your problem into a prediction task with the following ingredients:</p>
<ul>
<li><strong>Pick a target variable <span class="math inline">\(Y\)</span> to predict.</strong> For example, the occurrence of tornadoes in a 1-hour time window, coded as 0 (no tornado) and 1 (tornado).</li>
<li><strong>Define the task.</strong> The task is related to the target and can range from classification and regression to survival analysis and recommendation. Depending on how you frame the tornado prediction problem, you end up with different task types:
<ul>
<li>Will a tornado occur within the next hour? <span class="math inline">\(\Rightarrow\)</span> classification (Tornado Y/N)</li>
<li>How many tornadoes will occur this year? <span class="math inline">\(\Rightarrow\)</span> regression (0, 1, 2, …)</li>
<li>How long until the next tornado occurs? <span class="math inline">\(\Rightarrow\)</span> survival analysis (e.g. 2h)</li>
</ul></li>
<li><strong>Decide on the evaluation metric.</strong> This metric defines what counts as a good or bad prediction. To classify tornadoes in 1-hour windows, you could use metrics such as F1 score<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> or accuracy.</li>
<li><strong>Choose features <span class="math inline">\(X\)</span> from which to predict the target.</strong> Features could be hourly measurements from radar stations, like wind speeds and precipitation.</li>
</ul>
<p>Once the task is defined, you need data.</p>
</section>
<section id="get-the-data" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="get-the-data"><span class="header-section-number">2.2</span> Get the data</h2>
<p>Machine learning requires data. Data points are represented as pairs of feature and target values <span class="math inline">\((x^{(i)}, y^{(i)})_{i=1, \ldots, n}\)</span>. In the tornado case, <span class="math inline">\(x^{(145)}\)</span> might be radar measurements from 10 AM to 11 AM on May 25th, 2021, in a specific 3km by 3km patch in the United States. Accordingly, <span class="math inline">\(y^{(145)}\)</span> would indicate whether a tornado occurred in this time and place.</p>
<p>After cleaning and pre-processing the data, it is typically randomly split into three subsets for machine learning:</p>
<ul>
<li>A training dataset, <span class="math inline">\(D_{train}\)</span>, used to train the model.</li>
<li>A validation dataset, <span class="math inline">\(D_{val}\)</span>, used to validate modeling choices and selection.</li>
<li>A testing dataset, <span class="math inline">\(D_{test}\)</span>, used for final performance evaluation.</li>
</ul>
<p>The simplest way is to randomly split your dataset into these three buckets. In reality, you have to adapt the splitting mechanism based on data and tasks. For example, time series and clustered data require splitting schemes that respect the data structure. For classification with imbalanced data, you might want to use mechanisms that ensure that the minority class occurs often enough in each split. Anyways, in a completely random 3-way split, our data point <span class="math inline">\((x^{(145)}, y^{(145)})\)</span> would fall into one of these buckets.</p>
</section>
<section id="train-the-model" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="train-the-model"><span class="header-section-number">2.3</span> Train the model</h2>
<p>Training a machine learning model means running an algorithm that takes as input the training data and outputs the model. The model describes a function that outputs predictions based on input features.</p>
<p>The training requires making some choices:</p>
<ol type="1">
<li>Select a class of models <span class="math inline">\(F\)</span>. For example, decision trees or neural networks. Usually, you have to specify this class further, e.g., by choosing the maximal depth of a tree or the specific architecture of the neural network. Radar readings might be on a spatial level, so you might pick a convolutional neural network such as ResNet for the tornado prediction task.</li>
<li>Choose a training algorithm <span class="math inline">\(I\)</span>. The training algorithm takes the training data and produces a prediction model <span class="math inline">\(\hat{f}\)</span>. For example, neural networks typically use stochastic gradient descent with backpropagation as the training algorithm. But you could also train a neural network using genetic algorithms.</li>
<li>Set hyperparameters. Hyperparameters control the training algorithm <span class="math inline">\(I\)</span> and affect the models that the training produces. Some hyperparameters are related to the model class, like the number of layers in your neural net or the number of neighbors in the k-nearest-neighbors algorithm. Others are connected to the training algorithm, such as the learning rate or the batch size in stochastic gradient descent.</li>
</ol>
<p>For example, when you train a convolutional neural network to predict tornadoes, you use stochastic gradient descent (<span class="math inline">\(I\)</span>) with the training data <span class="math inline">\(D_{train}\)</span>. To do this, you have to set the hyperparameters. After the training process, you get a trained CNN <span class="math inline">\(\hat{f}\)</span> from the model class of CNNs.</p>
<p>The training process seeks to produce a model <span class="math inline">\(\hat{f}\)</span> that makes a minimal error on the training data:</p>
<p><span class="math display">\[\epsilon_{train}(\hat{f}):= \frac{1}{|D_{train}|} \sum_{x,y \,\in D_{train}} L(\hat{f}(x), y),\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the loss function the model optimizes for. Some training algorithms can optimize arbitrary (well, some constraints remain) loss functions directly, like neural networks using gradient descent, while other training algorithms have built-in and sometimes less explicit losses they optimize for, like greedy splits in CART decision trees <span class="citation" data-cites="breiman2017classification"><a href="references.html#ref-breiman2017classification" role="doc-biblioref">[1]</a></span>.</p>
</section>
<section id="validate-modeling-choices" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="validate-modeling-choices"><span class="header-section-number">2.4</span> Validate modeling choices</h2>
<p>Training doesn’t guarantee that it will produce the best-performing model. For example, you might not have picked the best model class, because you have used linear regression but the best model for that task might be a tree ensemble. And even if you picked the best model class, you might have set the hyperparameters non-optimally. You need a procedure to both pick a model class and the hyperparameters. A naive approach would be to compute the evaluation metric for the training data, but this would be a bad idea since some models may overfit the training data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Overfitting
</div>
</div>
<div class="callout-body-container callout-body">
<p>We say a model <em>overfits</em> when it has good performance on the training data but performs poorly with new unseen data from the same distribution. A model that overfits is good at reproducing the random errors in training data but fails to capture the general patterns.</p>
</div>
</div>
<p>You typically compute the evaluation metric using a separate validation dataset <span class="math inline">\(D_{val}\)</span>. Since the model wasn’t trained using the validation data, you get a fair assessment of its performance. With the validation data, you can compare multiple models and hyperparameter configurations and pick the best-performing one. This allows you to detect underfitting and overfitting and to guard against these problems by properly regularizing and tuning the model.</p>
</section>
<section id="evaluate-the-model-on-test-data" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="evaluate-the-model-on-test-data"><span class="header-section-number">2.5</span> Evaluate the model on test data</h2>
<p>How well would your final model perform? Unfortunately, you can’t use the evaluation metrics you have computed from training and validation data. Both will be too optimistic since you already used the training data to train the model and the validation data to make modeling choices. Instead, you have to evaluate the model performance on test data <span class="math inline">\(D_{test}\)</span>. This gives you a realistic estimate of the model performance.</p>
</section>
<section id="and-repeat-the-role-of-resampling-data" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="and-repeat-the-role-of-resampling-data"><span class="header-section-number">2.6</span> And repeat: the role of resampling data</h2>
<p>Having just one split into training, validation, and testing is not very data efficient. Typically, the data is split multiple times. A common technique is cross-validation, which splits the data into k different parts. Let’s say you use <span class="math inline">\(k=10\)</span>. Nine out of ten folds might be used for training and validation, and the tenth for test data. You cycle through the folds so that each fold is used as test data once. This way, you always use “fresh” data for evaluating the model. Other sampling methods such as bootstrapping and subsampling can be used here as well. But even having multiple folds may not give stable results – would you generate the fold splitting again, you may get different estimates. So another established method is to repeat the sampling.</p>
<p>You have another choice to make, and that is how to split training and validation data into nine parts. You could either do a single split or do cross-validation again. Like in the movie <em>Inception</em>, which is about dreams within dreams, you go one level deeper and do cross-validation within cross-validation, a procedure called nested cross-validation <span class="citation" data-cites="bates2023cross"><a href="references.html#ref-bates2023cross" role="doc-biblioref">[2]</a></span>. The advantage of (nested) cross-validation is better estimates of the model performance, and better models since you use the data more efficiently.</p>
</section>
<section id="bare-bones-machine-learning-can-be-automated" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="bare-bones-machine-learning-can-be-automated"><span class="header-section-number">2.7</span> Bare-bones machine learning can be automated</h2>
<p>Once you have defined the prediction task and your data, the entire training process can be completely automated. The subfield of machine learning called AutoML aims to completely automate the machine learning training process and make machine learning engineers redundant <span class="citation" data-cites="hutter2019automated"><a href="references.html#ref-hutter2019automated" role="doc-biblioref">[3]</a></span>. Upload your data, pick a column as your target, and pick an evaluation metric. Click a button. And the machine does everything for you. Data splitting, hyperparameter tuning, model selection, model evaluation. We call this automatable practice of machine learning “bare-bones machine learning”. The big question is: How does such an optimization-focused approach mix with a complex practice like science?</p>


<div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman2017classification" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">L. Breiman, <em>Classification and <span>Regression</span> <span>Trees</span></em>. New York: Routledge, 2017. doi: <a href="https://doi.org/10.1201/9781315139470">10.1201/9781315139470</a>.</div>
</div>
<div id="ref-bates2023cross" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">S. Bates, T. Hastie, and R. Tibshirani, <span>“Cross-validation: What does it estimate and how well does it do it?”</span> <em>Journal of the American Statistical Association</em>, pp. 1–12, 2023, doi: <a href="https://doi.org/10.1080/01621459.2023.2197686">10.1080/01621459.2023.2197686</a>.</div>
</div>
<div id="ref-hutter2019automated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">F. Hutter, L. Kotthoff, and J. Vanschoren, <em>Automated machine learning: Methods, systems, challenges</em>. Springer Nature, 2019. doi: <a href="https://doi.org/10.1007/978-3-030-05318-5">10.1007/978-3-030-05318-5</a>.</div>
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>The F1 score is a metric that balances precision and recall, particularly useful for evaluating performance on imbalanced datasets.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

    
</body>
</html>