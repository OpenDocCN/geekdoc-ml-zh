["```py\n%matplotlib inline                                         \nsuppress_warnings = False\nimport os                                                     # to set current working directory \nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn import linear_model                              # linear regression\nfrom sklearn.linear_model import Ridge                        # ridge regression implemented in scikit-learn\nfrom sklearn.linear_model import Lasso                        # LASSO regression implemented in scikit-learn\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"C:\\PGE337\")                                        # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1.0; seed = 71071\n\nyname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)\nxmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization \nymin = 0.0; ymax = 25.0    \nxlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting\nyunit = '%'; xunit = '$g/cm^{3}$'    \nXlabelunit = xlabel + ' (' + xunit + ')'\nylabelunit = ylabel + ' (' + yunit + ')'\n\n#df = pd.read_csv(\"Density_Por_data.csv\")                     # load the data from local current directory\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv\") # load the data from my github repo\ndf = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise\n    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray \n```", "```py\nx_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split\n# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame\n\ny = df[yname].values.reshape(len(df))                         # features as 1D vectors\nx = df[xname].values.reshape(len(df))\n\ndf_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames\ndf_test = pd.concat([x_test,y_test],axis=1) \n```", "```py\nprint('   Training DataFrame      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test) \n```", "```py\n Training DataFrame      Testing DataFrame \n```", "```py\nprint('     Training DataFrame         Testing DataFrame')\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame         Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nfreq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([xmin,xmax]); plt.legend(loc='upper right')   \n\nplt.subplot(222)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # plot the model\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title('Porosity vs Density')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nlinear_reg = linear_model.LinearRegression()                  # instantiate the model\n\nlinear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n```", "```py\ny_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared_linear = metrics.r2_score(df_test[yname].values, y_pred_linear)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)\nplt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.annotate(r'$r^2$ :' + str(np.round(r_squared_linear,2)),[1.97,15])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 1.0                                                     # lambda hyperparameter\n\nridge_reg = Ridge(alpha=lam)                                  # instantiate the model\n\nridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_ridge = ridge_reg.predict(x_model.reshape(10,1)) # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = metrics.r2_score(df_test[yname].values, y_pred_ridge)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.1                                                     # lambda hyperparameter\n\nlasso_reg = Lasso(alpha=lam)                                  # instantiate the model\n\nlasso_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_lasso = lasso_reg.predict(x_model.reshape(10,1))      # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(lasso_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(lasso_reg.intercept_,2)),[1.97,16])\nplt.title('LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,nlambda):\n    lasso_reg = Lasso(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=lasso_reg, X= df['Density'].values.reshape(-1, 1), \n                             y=df['Porosity'].values, cv=10, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')\nplt.vlines(0.1,0,20,color='red',lw=2); plt.vlines(0.9,0,20,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.075,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[1.06,11.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.01,0.1],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.9,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.01                                                      # tuned hyperparameter\nlasso_tuned = Lasso(alpha=lam)                                  # instantiate the model\nlasso_tuned.fit(df[xname].values.reshape(len(df),1), df[yname]) # train the model parameters on all data\n\ny_pred_lasso_tuned = lasso_tuned.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = metrics.r2_score(df_test[yname].values, y_pred_lasso_tuned)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Tuned LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.01 \n```", "```py\ndf_sample = df.copy(deep=True).sample(n=10,random_state=11)\nnoise_stdev = 3.0\nnp.random.seed(seed=15)\ndf_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))\n\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-3,5,nlambda)\nfor ilam in range(0,nlambda):\n    lasso_reg = Lasso(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=lasso_reg, X= df_sample['Density'].values.reshape(-1, 1), \n                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-3,1.0e5); plt.ylim(0.001,20.0); \nplt.xscale('log')\nplt.vlines(0.003,0,20,color='red',lw=2); plt.vlines(0.4,0,20,color='red',lw=2,zorder=10); plt.vlines(0.1,0,20,color='red',lw=2,zorder=10);\nplt.annotate('Linear Regression',[0.0022,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate(r'LASSO Tuned $\\lambda$',[0.075,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[0.46,11.7],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.001,0.003],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.4,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmv_data = 2\n\nif mv_data == 1:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\")\n    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature\nelif mv_data == 2:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\")\n    df_mv = df_mv.rename({'Prod':'Production'},axis=1)\n    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature\ndf_mv.head()                                                  # load the comma delimited data file \n```", "```py\ndf_mv.describe().transpose() \n```", "```py\nscaler = StandardScaler() \n```", "```py\nsfeatures = scaler.fit_transform(df_mv.values) \n```", "```py\ndf_nmv = pd.DataFrame() \n```", "```py\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) \n```", "```py\nscaler = StandardScaler()                                     # instantiate the scaler \n\nsfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame \ndf_nmv = pd.DataFrame()                                       # instantiate a new DataFrame\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame\ndf_nmv.head()                                                 # preview the new DataFrame \n```", "```py\ndf_nmv.describe().transpose()                                 # summary statistics from the new DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), \n                                                    test_size=0.33, random_state=73073)\nprint('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) \n```", "```py\nNumber of training data = 134 and number of testing data = 66 \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \ndf_nmv.describe().transpose()                               # summary statistics from the new DataFrame \nlams = np.logspace(-3,1,nbins)                              # make a list of lambda values\ncoefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    lasso_reg = Lasso(alpha=lam)                            # instantiate the model\n    lasso_reg.fit(X_train, y_train)                         # fit model\n    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)                                            # plot the results\nfor ifeature in range(0,6):\n    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \nlams = np.logspace(-2,5,nbins)       \nridge_coefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    ridge_reg = Ridge(alpha=lam)\n    ridge_reg.fit(X_train, y_train) # fit model\n    ridge_coefs[index,:] = ridge_reg.coef_\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)\nfor ifeature in range(0,6):\n    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \ndf_nmv.describe().transpose()                               # summary statistics from the new DataFrame \nlams = np.logspace(-4,6,nbins)                              # make a list of lambda values\ncoefs = np.ndarray((nbins,6))\nestimates_ridge = np.zeros((nbins,len(X_test)))\nestimates_lasso = np.zeros((nbins,len(X_test)))\n\nindex = 0\nfor lam in lams:\n    lasso_reg = Lasso(alpha=lam)                            # instantiate the model\n    lasso_reg.fit(X_train, y_train)                         # fit model\n    ridge_reg = Ridge(alpha=lam)\n    ridge_reg.fit(X_train, y_train) # fit model\n    estimates_ridge[index] = ridge_reg.predict(X_test).flatten() # predict at test data\n    estimates_lasso[index] = lasso_reg.predict(X_test).flatten() # predict at test data\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(211)                                            # plot the results\nfor iest in range(0,6):\n    plt.semilogx(lams,estimates_ridge[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)\nplt.title('Ridge Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); \nplt.ylabel(r'Predictions, $\\hat{y}_i$')\nplt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\nplt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(1.0e5,-0.1,1.5,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[110000,0.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([1.0e5,1.0e7],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\n\nplt.subplot(212)                                            # plot the results\nfor iest in range(0,6):\n    plt.semilogx(lams,estimates_lasso[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)\nplt.title('LASSO Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); \nplt.ylabel(r'Predictions, $\\hat{y}_i$')\nplt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\nplt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(0.90,-1.0,1.5,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[1.05,0.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.9,100000],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = False\nimport os                                                     # to set current working directory \nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn import linear_model                              # linear regression\nfrom sklearn.linear_model import Ridge                        # ridge regression implemented in scikit-learn\nfrom sklearn.linear_model import Lasso                        # LASSO regression implemented in scikit-learn\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"C:\\PGE337\")                                        # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1.0; seed = 71071\n\nyname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)\nxmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization \nymin = 0.0; ymax = 25.0    \nxlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting\nyunit = '%'; xunit = '$g/cm^{3}$'    \nXlabelunit = xlabel + ' (' + xunit + ')'\nylabelunit = ylabel + ' (' + yunit + ')'\n\n#df = pd.read_csv(\"Density_Por_data.csv\")                     # load the data from local current directory\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv\") # load the data from my github repo\ndf = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise\n    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray \n```", "```py\nx_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split\n# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame\n\ny = df[yname].values.reshape(len(df))                         # features as 1D vectors\nx = df[xname].values.reshape(len(df))\n\ndf_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames\ndf_test = pd.concat([x_test,y_test],axis=1) \n```", "```py\nprint('   Training DataFrame      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test) \n```", "```py\n Training DataFrame      Testing DataFrame \n```", "```py\nprint('     Training DataFrame         Testing DataFrame')\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame         Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nfreq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([xmin,xmax]); plt.legend(loc='upper right')   \n\nplt.subplot(222)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # plot the model\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title('Porosity vs Density')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.3, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nlinear_reg = linear_model.LinearRegression()                  # instantiate the model\n\nlinear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n```", "```py\ny_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared_linear = metrics.r2_score(df_test[yname].values, y_pred_linear)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)\nplt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.annotate(r'$r^2$ :' + str(np.round(r_squared_linear,2)),[1.97,15])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 1.0                                                     # lambda hyperparameter\n\nridge_reg = Ridge(alpha=lam)                                  # instantiate the model\n\nridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_ridge = ridge_reg.predict(x_model.reshape(10,1)) # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = metrics.r2_score(df_test[yname].values, y_pred_ridge)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Ridge Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.1                                                     # lambda hyperparameter\n\nlasso_reg = Lasso(alpha=lam)                                  # instantiate the model\n\nlasso_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_lasso = lasso_reg.predict(x_model.reshape(10,1))      # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(lasso_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(lasso_reg.intercept_,2)),[1.97,16])\nplt.title('LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,nlambda):\n    lasso_reg = Lasso(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=lasso_reg, X= df['Density'].values.reshape(-1, 1), \n                             y=df['Porosity'].values, cv=10, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')\nplt.vlines(0.1,0,20,color='red',lw=2); plt.vlines(0.9,0,20,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.075,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[1.06,11.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.01,0.1],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.9,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.01                                                      # tuned hyperparameter\nlasso_tuned = Lasso(alpha=lam)                                  # instantiate the model\nlasso_tuned.fit(df[xname].values.reshape(len(df),1), df[yname]) # train the model parameters on all data\n\ny_pred_lasso_tuned = lasso_tuned.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = metrics.r2_score(df_test[yname].values, y_pred_lasso_tuned)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_lasso, color='red', linewidth=2,label='LASSO Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nplt.annotate('LASSO Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Tuned LASSO Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 0.01 \n```", "```py\ndf_sample = df.copy(deep=True).sample(n=10,random_state=11)\nnoise_stdev = 3.0\nnp.random.seed(seed=15)\ndf_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))\n\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-3,5,nlambda)\nfor ilam in range(0,nlambda):\n    lasso_reg = Lasso(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=lasso_reg, X= df_sample['Density'].values.reshape(-1, 1), \n                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('LASSO Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-3,1.0e5); plt.ylim(0.001,20.0); \nplt.xscale('log')\nplt.vlines(0.003,0,20,color='red',lw=2); plt.vlines(0.4,0,20,color='red',lw=2,zorder=10); plt.vlines(0.1,0,20,color='red',lw=2,zorder=10);\nplt.annotate('Linear Regression',[0.0022,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate(r'LASSO Tuned $\\lambda$',[0.075,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[0.46,11.7],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.001,0.003],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.4,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmv_data = 2\n\nif mv_data == 1:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\")\n    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature\nelif mv_data == 2:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\")\n    df_mv = df_mv.rename({'Prod':'Production'},axis=1)\n    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature\ndf_mv.head()                                                  # load the comma delimited data file \n```", "```py\ndf_mv.describe().transpose() \n```", "```py\nscaler = StandardScaler() \n```", "```py\nsfeatures = scaler.fit_transform(df_mv.values) \n```", "```py\ndf_nmv = pd.DataFrame() \n```", "```py\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) \n```", "```py\nscaler = StandardScaler()                                     # instantiate the scaler \n\nsfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame \ndf_nmv = pd.DataFrame()                                       # instantiate a new DataFrame\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame\ndf_nmv.head()                                                 # preview the new DataFrame \n```", "```py\ndf_nmv.describe().transpose()                                 # summary statistics from the new DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), \n                                                    test_size=0.33, random_state=73073)\nprint('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) \n```", "```py\nNumber of training data = 134 and number of testing data = 66 \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \ndf_nmv.describe().transpose()                               # summary statistics from the new DataFrame \nlams = np.logspace(-3,1,nbins)                              # make a list of lambda values\ncoefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    lasso_reg = Lasso(alpha=lam)                            # instantiate the model\n    lasso_reg.fit(X_train, y_train)                         # fit model\n    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)                                            # plot the results\nfor ifeature in range(0,6):\n    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \nlams = np.logspace(-2,5,nbins)       \nridge_coefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    ridge_reg = Ridge(alpha=lam)\n    ridge_reg.fit(X_train, y_train) # fit model\n    ridge_coefs[index,:] = ridge_reg.coef_\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)\nfor ifeature in range(0,6):\n    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmv_data = 2\n\nif mv_data == 1:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\")\n    df_mv = df_mv.drop('WellIndex',axis = 1)                  # remove the well index feature\nelif mv_data == 2:\n    df_mv = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\")\n    df_mv = df_mv.rename({'Prod':'Production'},axis=1)\n    df_mv = df_mv.drop('Well',axis = 1)                       # remove the well index feature\ndf_mv.head()                                                  # load the comma delimited data file \n```", "```py\ndf_mv.describe().transpose() \n```", "```py\nscaler = StandardScaler() \n```", "```py\nsfeatures = scaler.fit_transform(df_mv.values) \n```", "```py\ndf_nmv = pd.DataFrame() \n```", "```py\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) \n```", "```py\nscaler = StandardScaler()                                     # instantiate the scaler \n\nsfeatures = scaler.fit_transform(df_mv.values)                # standardize all the values extracted from the DataFrame \ndf_nmv = pd.DataFrame()                                       # instantiate a new DataFrame\ndf_nmv = pd.DataFrame(sfeatures, index=df_mv.index, columns=df_mv.columns) # copy the standardized values into the new DataFrame\ndf_nmv.head()                                                 # preview the new DataFrame \n```", "```py\ndf_nmv.describe().transpose()                                 # summary statistics from the new DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(df_nmv.iloc[:,:6], pd.DataFrame({'Production':df_nmv['Production']}), \n                                                    test_size=0.33, random_state=73073)\nprint('Number of training data = ' + str(len(X_train)) + ' and number of testing data = ' + str(len(X_test))) \n```", "```py\nNumber of training data = 134 and number of testing data = 66 \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \ndf_nmv.describe().transpose()                               # summary statistics from the new DataFrame \nlams = np.logspace(-3,1,nbins)                              # make a list of lambda values\ncoefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    lasso_reg = Lasso(alpha=lam)                            # instantiate the model\n    lasso_reg.fit(X_train, y_train)                         # fit model\n    coefs[index,:] = lasso_reg.coef_                        # retrieve the coefficients\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)                                            # plot the results\nfor ifeature in range(0,6):\n    plt.semilogx(lams,coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-3,1.0e1); plt.ylim(-1.0,1.0); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \nlams = np.logspace(-2,5,nbins)       \nridge_coefs = np.ndarray((nbins,6))\n\nindex = 0\nfor lam in lams:\n    ridge_reg = Ridge(alpha=lam)\n    ridge_reg.fit(X_train, y_train) # fit model\n    ridge_coefs[index,:] = ridge_reg.coef_\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(111)\nfor ifeature in range(0,6):\n    plt.semilogx(lams,ridge_coefs[:,ifeature], label = df_mv.columns[ifeature], c = color[ifeature], linewidth = 3.0)\n\nplt.title('Standardized Model Coefficients vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); plt.ylabel('Standardized Model Coefficients')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(-1.0,1.0); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1., wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nnbins = 1000                                                # number of bins to explore the hyperparameter \ndf_nmv.describe().transpose()                               # summary statistics from the new DataFrame \nlams = np.logspace(-4,6,nbins)                              # make a list of lambda values\ncoefs = np.ndarray((nbins,6))\nestimates_ridge = np.zeros((nbins,len(X_test)))\nestimates_lasso = np.zeros((nbins,len(X_test)))\n\nindex = 0\nfor lam in lams:\n    lasso_reg = Lasso(alpha=lam)                            # instantiate the model\n    lasso_reg.fit(X_train, y_train)                         # fit model\n    ridge_reg = Ridge(alpha=lam)\n    ridge_reg.fit(X_train, y_train) # fit model\n    estimates_ridge[index] = ridge_reg.predict(X_test).flatten() # predict at test data\n    estimates_lasso[index] = lasso_reg.predict(X_test).flatten() # predict at test data\n    index = index + 1\n\ncolor = ['black','blue','green','red','orange','grey']\nplt.subplot(211)                                            # plot the results\nfor iest in range(0,6):\n    plt.semilogx(lams,estimates_ridge[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)\nplt.title('Ridge Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); \nplt.ylabel(r'Predictions, $\\hat{y}_i$')\nplt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\nplt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(1.0e5,-0.1,1.5,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[110000,0.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([1.0e5,1.0e7],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\n\nplt.subplot(212)                                            # plot the results\nfor iest in range(0,6):\n    plt.semilogx(lams,estimates_lasso[:,iest], label = 'Estimate #' + str(iest+1), c = color[iest], linewidth = 3.0)\nplt.title('LASSO Regression - 6 Example Predictions vs. Lambda Hyperparameter'); plt.xlabel('Lambda Hyperparameter'); \nplt.ylabel(r'Predictions, $\\hat{y}_i$')\nplt.xlim(1.0e-4,1.0e6); plt.ylim(-1.0,1.5); plt.grid(); plt.legend(loc = 'lower right')\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linewidth=0.5)\nplt.vlines(0.001,-1,1.5,color='red',lw=2); plt.vlines(0.90,-1.0,1.5,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.0007,0.6],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean of Response Feature',[1.05,0.4],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([0.9,100000],[-1,-1],[1.5,1.5],color='grey',alpha=0.3,zorder=1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=2.0, wspace=0.2, hspace=0.2); plt.show() \n```"]