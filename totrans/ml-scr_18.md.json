["```py\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\nwine = datasets.load_wine()\nX, y = wine.data, wine.target \n```", "```py\nclass LDA:\n\n    ## Fitting the model \n    def fit(self, X, y):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Get mu for each class and overall Sigma\n        self.mu_ks = []\n        self.Sigma = np.zeros((self.D, self.D))        \n        for i, k in enumerate(self.unique_y):\n\n            X_k = self.X[self.y == k]\n            mu_k = X_k.mean(0).reshape(self.D, 1)\n            self.mu_ks.append(mu_k)\n\n            for x_n in X_k:\n                x_n = x_n.reshape(-1,1)\n                x_n_minus_mu_k = (x_n - mu_k)\n                self.Sigma += np.dot(x_n_minus_mu_k, x_n_minus_mu_k.T)\n\n        self.Sigma /= self.N\n\n    ## Making classifications\n\n    def _mvn_density(self, x_n, mu_k, Sigma):\n        x_n_minus_mu_k = (x_n - mu_k)\n        density = np.exp(-(1/2)*x_n_minus_mu_k.T @ np.linalg.inv(Sigma) @ x_n_minus_mu_k)\n        return density\n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test):\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y):\n                p_x_given_y = self._mvn_density(x_n, self.mu_ks[j], self.Sigma)\n                p_y_given_x = self.pi_ks[j]*p_x_given_y\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nlda = LDA()\nlda.fit(X, y)\nyhat = lda.classify(X)\nnp.mean(yhat == y) \n```", "```py\n1.0 \n```", "```py\ndef graph_boundaries(X, model, model_title, n0 = 100, n1 = 100, figsize = (7, 5), label_every = 4):\n\n        # Generate X for plotting \n        d0_range = np.linspace(X[:,0].min(), X[:,0].max(), n0)\n        d1_range = np.linspace(X[:,1].min(), X[:,1].max(), n1)\n        X_plot = np.array(np.meshgrid(d0_range, d1_range)).T.reshape(-1, 2)\n\n        # Get class predictions\n        y_plot = model.classify(X_plot).astype(int)\n\n        # Plot \n        fig, ax = plt.subplots(figsize = figsize)\n        sns.heatmap(y_plot.reshape(n0, n1).T,\n                   cmap = sns.color_palette('Pastel1', 3),\n                   cbar_kws = {'ticks':sorted(np.unique(y_plot))})\n        xticks, yticks = ax.get_xticks(), ax.get_yticks()\n        ax.set(xticks = xticks[::label_every], xticklabels = d0_range.round(2)[::label_every],\n               yticks = yticks[::label_every], yticklabels = d1_range.round(2)[::label_every])\n        ax.set(xlabel = 'X1', ylabel = 'X2', title = model_title + ' Predictions by X1 and X2')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=0) \n```", "```py\nX_2d = X.copy()[:,2:4]\nlda_2d = LDA()\nlda_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, lda_2d, 'LDA') \n```", "```py\nclass QDA:\n\n    ## Fitting the model\n\n    def fit(self, X, y):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Get mu and Sigma for each class\n        self.mu_ks = []\n        self.Sigma_ks = []\n        for i, k in enumerate(self.unique_y):\n\n            X_k = self.X[self.y == k]\n            mu_k = X_k.mean(0).reshape(self.D, 1)\n            self.mu_ks.append(mu_k)\n\n            Sigma_k = np.zeros((self.D, self.D))\n            for x_n in X_k:\n                x_n = x_n.reshape(-1,1)\n                x_n_minus_mu_k = (x_n - mu_k)\n                Sigma_k += np.dot(x_n_minus_mu_k, x_n_minus_mu_k.T)\n            self.Sigma_ks.append(Sigma_k/len(X_k))\n\n    ## Making classifications \n\n    def _mvn_density(self, x_n, mu_k, Sigma_k):\n        x_n_minus_mu_k = (x_n - mu_k)\n        density = np.linalg.det(Sigma_k)**(-1/2) * np.exp(-(1/2)*x_n_minus_mu_k.T @ np.linalg.inv(Sigma_k) @ x_n_minus_mu_k)\n        return density\n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test):\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y):\n\n                p_x_given_y = self._mvn_density(x_n, self.mu_ks[j], self.Sigma_ks[j])\n                p_y_given_x = self.pi_ks[j]*p_x_given_y\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nqda = QDA()\nqda.fit(X, y)\nyhat = qda.classify(X)\nnp.mean(yhat == y) \n```", "```py\n0.9943820224719101 \n```", "```py\nqda_2d = QDA()\nqda_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, qda_2d, 'QDA') \n```", "```py\nclass NaiveBayes:\n\n    ######## Fit Model ########\n\n    def _estimate_class_parameters(self, X_k):\n\n        class_parameters = []\n\n        for d in range(self.D):\n            X_kd = X_k[:,d] # only the dth column and the kth class\n\n            if self.distributions[d] == 'normal':\n                mu = np.mean(X_kd)\n                sigma2 = np.var(X_kd)\n                class_parameters.append([mu, sigma2])\n\n            if self.distributions[d] == 'bernoulli':\n                p = np.mean(X_kd)\n                class_parameters.append(p)\n\n            if self.distributions[d] == 'poisson':\n                lam = np.mean(X_kd)\n                class_parameters.append(p)\n\n        return class_parameters\n\n    def fit(self, X, y, distributions = None):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n        if distributions is None:\n            distributions = ['normal' for i in range(len(y))]\n        self.distributions = distributions\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Estimate parameters\n        self.parameters = []\n        for i, k in enumerate(self.unique_y):\n            X_k = self.X[self.y == k]\n            self.parameters.append(self._estimate_class_parameters(X_k))\n\n    ######## Make Classifications ########\n\n    def _get_class_probability(self, x_n, j):\n\n        class_parameters = self.parameters[j] # j is index of kth class\n        class_probability = 1 \n\n        for d in range(self.D):\n            x_nd = x_n[d] # just the dth variable in observation x_n\n\n            if self.distributions[d] == 'normal':\n                mu, sigma2 = class_parameters[d]\n                class_probability *= sigma2**(-1/2)*np.exp(-(x_nd - mu)**2/sigma2)\n\n            if self.distributions[d] == 'bernoulli':\n                p = class_parameters[d]\n                class_probability *= (p**x_nd)*(1-p)**(1-x_nd)\n\n            if self.distributions[d] == 'poisson':\n                lam = class_parameters[d]\n                class_probability *= np.exp(-lam)*lam**x_nd\n\n        return class_probability \n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test): # loop through test observations\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y): # loop through classes\n\n                p_x_given_y = self._get_class_probability(x_n, j)\n                p_y_given_x = self.pi_ks[j]*p_x_given_y # bayes' rule\n\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nnb = NaiveBayes()\nnb.fit(X, y)\nyhat = nb.classify(X)\nnp.mean(yhat == y) \n```", "```py\n0.9775280898876404 \n```", "```py\nnb_2d = NaiveBayes()\nnb_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, nb_2d, 'Naive Bayes') \n```", "```py\nclass LDA:\n\n    ## Fitting the model \n    def fit(self, X, y):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Get mu for each class and overall Sigma\n        self.mu_ks = []\n        self.Sigma = np.zeros((self.D, self.D))        \n        for i, k in enumerate(self.unique_y):\n\n            X_k = self.X[self.y == k]\n            mu_k = X_k.mean(0).reshape(self.D, 1)\n            self.mu_ks.append(mu_k)\n\n            for x_n in X_k:\n                x_n = x_n.reshape(-1,1)\n                x_n_minus_mu_k = (x_n - mu_k)\n                self.Sigma += np.dot(x_n_minus_mu_k, x_n_minus_mu_k.T)\n\n        self.Sigma /= self.N\n\n    ## Making classifications\n\n    def _mvn_density(self, x_n, mu_k, Sigma):\n        x_n_minus_mu_k = (x_n - mu_k)\n        density = np.exp(-(1/2)*x_n_minus_mu_k.T @ np.linalg.inv(Sigma) @ x_n_minus_mu_k)\n        return density\n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test):\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y):\n                p_x_given_y = self._mvn_density(x_n, self.mu_ks[j], self.Sigma)\n                p_y_given_x = self.pi_ks[j]*p_x_given_y\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nlda = LDA()\nlda.fit(X, y)\nyhat = lda.classify(X)\nnp.mean(yhat == y) \n```", "```py\n1.0 \n```", "```py\ndef graph_boundaries(X, model, model_title, n0 = 100, n1 = 100, figsize = (7, 5), label_every = 4):\n\n        # Generate X for plotting \n        d0_range = np.linspace(X[:,0].min(), X[:,0].max(), n0)\n        d1_range = np.linspace(X[:,1].min(), X[:,1].max(), n1)\n        X_plot = np.array(np.meshgrid(d0_range, d1_range)).T.reshape(-1, 2)\n\n        # Get class predictions\n        y_plot = model.classify(X_plot).astype(int)\n\n        # Plot \n        fig, ax = plt.subplots(figsize = figsize)\n        sns.heatmap(y_plot.reshape(n0, n1).T,\n                   cmap = sns.color_palette('Pastel1', 3),\n                   cbar_kws = {'ticks':sorted(np.unique(y_plot))})\n        xticks, yticks = ax.get_xticks(), ax.get_yticks()\n        ax.set(xticks = xticks[::label_every], xticklabels = d0_range.round(2)[::label_every],\n               yticks = yticks[::label_every], yticklabels = d1_range.round(2)[::label_every])\n        ax.set(xlabel = 'X1', ylabel = 'X2', title = model_title + ' Predictions by X1 and X2')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=0) \n```", "```py\nX_2d = X.copy()[:,2:4]\nlda_2d = LDA()\nlda_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, lda_2d, 'LDA') \n```", "```py\nclass QDA:\n\n    ## Fitting the model\n\n    def fit(self, X, y):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Get mu and Sigma for each class\n        self.mu_ks = []\n        self.Sigma_ks = []\n        for i, k in enumerate(self.unique_y):\n\n            X_k = self.X[self.y == k]\n            mu_k = X_k.mean(0).reshape(self.D, 1)\n            self.mu_ks.append(mu_k)\n\n            Sigma_k = np.zeros((self.D, self.D))\n            for x_n in X_k:\n                x_n = x_n.reshape(-1,1)\n                x_n_minus_mu_k = (x_n - mu_k)\n                Sigma_k += np.dot(x_n_minus_mu_k, x_n_minus_mu_k.T)\n            self.Sigma_ks.append(Sigma_k/len(X_k))\n\n    ## Making classifications \n\n    def _mvn_density(self, x_n, mu_k, Sigma_k):\n        x_n_minus_mu_k = (x_n - mu_k)\n        density = np.linalg.det(Sigma_k)**(-1/2) * np.exp(-(1/2)*x_n_minus_mu_k.T @ np.linalg.inv(Sigma_k) @ x_n_minus_mu_k)\n        return density\n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test):\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y):\n\n                p_x_given_y = self._mvn_density(x_n, self.mu_ks[j], self.Sigma_ks[j])\n                p_y_given_x = self.pi_ks[j]*p_x_given_y\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nqda = QDA()\nqda.fit(X, y)\nyhat = qda.classify(X)\nnp.mean(yhat == y) \n```", "```py\n0.9943820224719101 \n```", "```py\nqda_2d = QDA()\nqda_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, qda_2d, 'QDA') \n```", "```py\nclass NaiveBayes:\n\n    ######## Fit Model ########\n\n    def _estimate_class_parameters(self, X_k):\n\n        class_parameters = []\n\n        for d in range(self.D):\n            X_kd = X_k[:,d] # only the dth column and the kth class\n\n            if self.distributions[d] == 'normal':\n                mu = np.mean(X_kd)\n                sigma2 = np.var(X_kd)\n                class_parameters.append([mu, sigma2])\n\n            if self.distributions[d] == 'bernoulli':\n                p = np.mean(X_kd)\n                class_parameters.append(p)\n\n            if self.distributions[d] == 'poisson':\n                lam = np.mean(X_kd)\n                class_parameters.append(p)\n\n        return class_parameters\n\n    def fit(self, X, y, distributions = None):\n\n        ## Record info\n        self.N, self.D = X.shape\n        self.X = X\n        self.y = y\n        if distributions is None:\n            distributions = ['normal' for i in range(len(y))]\n        self.distributions = distributions\n\n        ## Get prior probabilities \n        self.unique_y, unique_y_counts = np.unique(self.y, return_counts = True) # returns unique y and counts\n        self.pi_ks = unique_y_counts/self.N\n\n        ## Estimate parameters\n        self.parameters = []\n        for i, k in enumerate(self.unique_y):\n            X_k = self.X[self.y == k]\n            self.parameters.append(self._estimate_class_parameters(X_k))\n\n    ######## Make Classifications ########\n\n    def _get_class_probability(self, x_n, j):\n\n        class_parameters = self.parameters[j] # j is index of kth class\n        class_probability = 1 \n\n        for d in range(self.D):\n            x_nd = x_n[d] # just the dth variable in observation x_n\n\n            if self.distributions[d] == 'normal':\n                mu, sigma2 = class_parameters[d]\n                class_probability *= sigma2**(-1/2)*np.exp(-(x_nd - mu)**2/sigma2)\n\n            if self.distributions[d] == 'bernoulli':\n                p = class_parameters[d]\n                class_probability *= (p**x_nd)*(1-p)**(1-x_nd)\n\n            if self.distributions[d] == 'poisson':\n                lam = class_parameters[d]\n                class_probability *= np.exp(-lam)*lam**x_nd\n\n        return class_probability \n\n    def classify(self, X_test):\n\n        y_n = np.empty(len(X_test))\n        for i, x_n in enumerate(X_test): # loop through test observations\n\n            x_n = x_n.reshape(-1, 1)\n            p_ks = np.empty(len(self.unique_y))\n\n            for j, k in enumerate(self.unique_y): # loop through classes\n\n                p_x_given_y = self._get_class_probability(x_n, j)\n                p_y_given_x = self.pi_ks[j]*p_x_given_y # bayes' rule\n\n                p_ks[j] = p_y_given_x\n\n            y_n[i] = self.unique_y[np.argmax(p_ks)]\n\n        return y_n \n```", "```py\nnb = NaiveBayes()\nnb.fit(X, y)\nyhat = nb.classify(X)\nnp.mean(yhat == y) \n```", "```py\n0.9775280898876404 \n```", "```py\nnb_2d = NaiveBayes()\nnb_2d.fit(X_2d, y)\ngraph_boundaries(X_2d, nb_2d, 'Naive Bayes') \n```"]