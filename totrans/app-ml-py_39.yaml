- en: Machine Learning Glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_glossary.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_glossary.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Geostatistics in Python: a Hands-on Guide with GeostatsPy‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy,
    [https://geostatsguy.github.io/GeostatsPyDemos_Book](https://geostatsguy.github.io/GeostatsPyDemos_Book).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the GeostatsPyDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, GeostatsPyDemos: GeostatsPy Python Package for Spatial Data
    Analytics and Geostatistics Demonstration Workflows Repository (0.0.1). Zenodo.
    [https://zenodo.org/doi/10.5281/zenodo.12667035](https://zenodo.org/doi/10.5281/zenodo.12667035)'
  prefs: []
  type: TYPE_NORMAL
- en: '[![DOI](../Images/16a74734a4e89db11a986fb62be91669.png)](https://zenodo.org/doi/10.5281/zenodo.12667035)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a summary of essential **Machine Learning Terminology**.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Machine Learning Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, why do this? I have received the request for a course glossary from
    the students in my **Subsurface Machine Learning** combined undergraduate and
    graduate course. While I usually dedicate a definition slide in the lecture slide
    decks for salient terms, some of my students have requested course glossary, list
    of terminology for their course review. The e-book provides a great vehicle and
    motivation to finally complete this.
  prefs: []
  type: TYPE_NORMAL
- en: Let me begin with a confession. There is a [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary)
    written by Google developers. For those seeking the in depth, comprehensive list
    of geostatistical terms please use this book!
  prefs: []
  type: TYPE_NORMAL
- en: By writing my own glossary I can limit the scope and descriptions to course
    content. I fear that many students would be overwhelmed by the size and mathematical
    notation of a standard machine learning glossary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, by including a glossary in the e-book I can link from glossary entries
    to the chapters in the e-book for convenience. I will eventual populate all the
    chapters with hyperlinks to the glossary to enable moving back and forth between
    the chapters and the glossary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, like the rest of the book, I want the glossary to be a evergreen living
    document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adjacency Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the pairwise connections between all pairwise combinations of graph
    nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: the values are indicators, 0 if not connected, 1 if connected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, node self connections are set to 0 in the adjacency matrix
  prefs: []
  type: TYPE_NORMAL
- en: '**Addition Rule** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): when we add probabilities
    (the union of outcomes), the probability of \(A\) or \(B\) is calculated with
    the probability addition rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: given mutually exclusive events we can generalize the addition rule as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Affine Correction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*). For the case of affine correction
    of \(X\) to \(Y\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{\sigma_y}{\sigma_x}(x_i - \overline{x}) + \overline{y}, \quad
    \forall \quad i, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\sigma_x\) are the original mean and variance,
    and \(\overline{y}\) and \(\sigma_y\) are the new mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: We can see above that the affine correlation method first centers the distribution
    (by subtracting the original mean), then rescales the dispersion (distribution
    spread) by the ratio of the new standard deviation to the original standard deviation
    and then shifts the distribution to centered on the new mean.
  prefs: []
  type: TYPE_NORMAL
- en: there is no shape change for affine correction. For shape change consider *Distribution
    Transformation* like *Gaussian Anamorphosis*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Affinity Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the degree of pairwise connections between all pairwise combinations
    of graph nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: values indicate the strength of the connection, unlike adjacency matrix with
    indicators, 0 if not connected, 1 if connected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, node self connections are set to 0 in the adjacency matrix
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): the
    application of bootstrap to obtain data realizations,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y^b, X_1^b, \dots, X_m^b, \quad b = 1, \dots, B \]
  prefs: []
  type: TYPE_NORMAL
- en: to train predictive model realizations,
  prefs: []
  type: TYPE_NORMAL
- en: \(\hat{Y}^b = \hat{f}^b (X_1^b, \dots, X_m^b)\)
  prefs: []
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: \((X_1^b, \dots, X_m^b)\) - the bootstrap predictor features in the \(b^{th}\)
    bootstrapped dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\hat{f}^b\) - the \(b^{th}\) bootstrapped model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\hat{Y}^b\) - predicted value for the model in the \(b^{th}\) bootstrapped
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to calculate prediction realizations. The ensemble of prediction realizations
    are aggregated to reduce model variance. The aggregation includes,
  prefs: []
  type: TYPE_NORMAL
- en: '*regression* - the average of the predictions $\( \hat{Y} = \frac{1}{B} \sum_{b=1}^{B}
    \hat{Y}^b \)$'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*classification* - the mode of the predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{Y} = \text{argmax}(\hat{Y}^b) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can perform bagging with any prediction model, in fact the BaggingClassifier
    and BaggingRegressor functions in scikit-learn are wrappers that take the prediction
    model as an input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Basis Expansion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): to add
    flexibility to our model, for example, to capture non-linearity in our model for
    regression, classification, we expand the features with a set of basis functions'
  prefs: []
  type: TYPE_NORMAL
- en: in mathematics basis expansion is the approach of representing a more complicated
    function with a linear combination of simpler basis functions that make the problem
    easier to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with basis expansion we expand the dimensionality of the problem with basis
    functions of the original features, but still use linear methods on the transformed
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ‚Ñé(ùë•_ùëñ )=\left( ‚Ñé_1(ùë•_ùëñ ),‚Ñé_2(ùë•_ùëñ ),\ldots,‚Ñé_ùëò(ùë•_ùëñ ) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here an example of basis expansion, the set of basis functions for polynomial
    basis expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3,
    \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Basis Function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): to add
    flexibility to our model, for example, to capture non-linearity in our model for
    regression, classification, we expand the features with a set of basis functions'
  prefs: []
  type: TYPE_NORMAL
- en: in mathematics basis expansion is the approach of representing a more complicated
    function with a linear combination of simpler basis functions that make the problem
    easier to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with basis expansion we expand the dimensionality of the problem with basis
    functions of the original features, but still use linear methods on the transformed
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ‚Ñé(ùë•_ùëñ )=\left( ‚Ñé_1(ùë•_ùëñ ),‚Ñé_2(ùë•_ùëñ ),\ldots,‚Ñé_ùëò(ùë•_ùëñ ) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'were each of \(h_1\), \ldots, \(h_k\) are basis functions. For example, here
    are the basis functions for polynomial basis expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3,
    \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes‚Äô Theorem** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the mathematical
    model central to Bayesian probability for the Bayesian updating from prior probability,
    with likelihood probability from new information to posterior probability.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P(A)\) is the prior, \(P(B|A)\) is the likelihood, \(P(B)\) is the evidence
    term and \(P(A|B)\) is the posterior. If is convenient to substitute more descriptive
    labels for \(A\) and \(B\) to better conceptualize this approach,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{Model} | \text{New Data}) = \frac{P(\text{New Data} | \text{Model})
    \cdot P(\text{Model})}{P(\text{New Data})} \]
  prefs: []
  type: TYPE_NORMAL
- en: demonstrating that we are updating our model with new data
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probabilities based
    on a degree of belief (expert judgement and experience) in the likelihood of an
    event. The general approach,'
  prefs: []
  type: TYPE_NORMAL
- en: start with prior probability, prior to the collection of new information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: formulate a likelihood probability, based on new information alone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update prior with likelihood to calculate the updated posterior probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: continue to update as new information is available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: solve probability problems that we cannot use simple frequencies, i.e., *frequentist
    probability* approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian updating is modeled with *Bayes‚Äô Theorem*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian Updating for Classification**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): this is how we pose the classification
    prediction problem from the perspective of Bayesian updating, based on the conditional
    probability of a category, \(k\), given \(n\) features, \(x_1, \dots , x_n\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can solve for this posterior with Bayesian updating,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = \frac{P(x_1, \dots , x_n | C_k) P(C_k)}{P(x_1,
    \dots , x_n)} \]
  prefs: []
  type: TYPE_NORMAL
- en: let‚Äôs combine the likelihood and prior for the moment,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1, \dots , x_n | C_k) P(C_k) = P(x_1, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can expand the full joint distribution recursively as follows,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: expansion of the joint with the conditional and prior,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: continue recursively expanding,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3, \dots
    , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots
    , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n}
    | C_k) P(C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Linear Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    the frequentist formulation of the linear regression model is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_1 \times x + b_0 + \sigma \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(x\) is the predictor feature, \(b_1\) is the slope parameter, \(b_0\)
    is the intercept parameter and \(\sigma\) is the error or noise. There is an analytical
    form for the ordinary least squares solution to fit the available data while minimizing
    the \(L^2\) norm of the data error vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Bayesian formulation of linear regression is we pose the model as a
    prediction of the distribution of the response, \(Y\), now a random variable:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y \sim N(\beta^{T}X, \sigma^{2} I) \]
  prefs: []
  type: TYPE_NORMAL
- en: We estimate the model parameter distributions through Bayesian updating for
    inferring the model parameters from a prior and likelihood from training data.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\beta | y, X) = \frac{P(y,X| \beta) P(\beta)}{P(y,X)} \]
  prefs: []
  type: TYPE_NORMAL
- en: In general for continuous features we are not able to directly calculate the
    posterior and we must use a sampling method, such as Markov chain Monte Carlo
    (McMC) to sample the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): you have big data
    if your data has a combination of these criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Volume* - many data samples and features, difficult to store, transmit
    and visualize'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Velocity* - high-rate collection, continuous data collection relative
    to decision making cycles, challenges keeping up with the new data while updating
    the models'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Variety* - data form various sources, with various types of data, types
    of information, and scales'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Variability* - data acquisition changes during the project, even for
    a single feature there may be multiple vintages of data with different scales,
    distributions, and veracity'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Veracity* - data has various levels of accuracy, the data is not certain'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For common subsurface applications most, if not all, of these criteria are met.
    Subsurface engineering and geoscience are often working with big data!
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Data Analytics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the process of
    examining large and varied data (*big data*) sets to discover patterns and make
    decisions, the application of statistics to big data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Transform** (also Indicator Transform)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): indicator
    coding a random variable to a probability relative to a category or a threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a categorical variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization equal to a category?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k
    \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_2 = 2\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 2\),
    then \(i(bf{u}_1; z_2) = 1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 1\), and a RV away from data, \(Z(\bf{u}_2)\) then
    is calculated as \(F^{-1}_{\bf{u}_2}(z_1)\) of the RV as \(i(\bf{u}_2; z_1) =
    0.23\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(I\{\bf{u}:z_k\}\) is an indicator for a continuous variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization less than or equal to a threshold?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le
    z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 6\%\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 8\%\),
    then \(i(\bf{u}_1; z_1) = 0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_4 = 18\%\), and a RV away from data, \(Z(\bf{u}_2) = N\left[\mu
    = 16\%,\sigma = 3\%\right]\) then \(i(\bf{u}_2; z_4) = 0.75\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indicator coding may be applied over an entire random function by indicator
    transform of all the random variables at each location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): addition of multiple
    week learners to build a stronger learner.'
  prefs: []
  type: TYPE_NORMAL
- en: a weak learner is one that offers predictions just marginally better than random
    selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the method in words, and then with equations,
  prefs: []
  type: TYPE_NORMAL
- en: build a simple model with a high error rate, the model can be quite inaccurate,
    but moves in the correct direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the error from the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fit another model to the error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the error from this addition of the first and second model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat until the desired accuracy is obtained or some other stopping criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now with equations, the general workflow for predicting \(Y\) from \(X_1,\ldots,X_m\)
    is,
  prefs: []
  type: TYPE_NORMAL
- en: build a week learner to predict \(Y\) from \(X_1,\ldots,X_m\), \(\hat{F}_k(X)\)
    from the training data \(x_{i,j}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loop over number of desired estimators, \(k = 1,\ldots,K\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the residuals at the training data, \(h_k(x_{i}) = y_i - \hat{F}_k(x_{i})\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: fit another week learner to predict \(h_k\) from \(X_1,\ldots,X_m\), \(\hat{F}_k(X)\)
    from the training data \(x_{i,j}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: each model builds on the previous to improve the accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regression estimator is the summation over the \(K\) simple models,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): a statistical
    resampling procedure to calculate uncertainty in a calculated statistic from the
    sample data itself. Some general comments,'
  prefs: []
  type: TYPE_NORMAL
- en: '*sampling with replacement* - \(n\) (number of data samples) *Monte Carlo simulation*s
    from the dataset *cumulative distribution function*, this results in a new realization
    of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*simulates the data collection process* - the fundamental idea is to simulate
    the original data collection process. Instead of actually collecting new sample
    sets, we randomly select from the data to get data realizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bootstrap any statistic* - this approach is very flexible as we can calculate
    realizations of any statistics from the data realizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*computationally cheap* - repeat this approach to get realizations of the statistic
    to build a complete distribution of uncertainty. Use a large number of realizations,
    \(L\), for a reliable uncertainty model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*calculates the entire distribution of uncertainty* - for any statistic, you
    calculate any summary statistic for the uncertainty model, e.g., mean, P10 and
    P90 of the uncertainty in the mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bagging for machine learning* - is the application of bootstrap to obtain
    data realizations to train predictive model realizations to aggregate predictions
    over ensembles of prediction models to reduce model variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the limitations of bootstrap?
  prefs: []
  type: TYPE_NORMAL
- en: biased sample data will likely result in a biased bootstrapped uncertainty model,
    you must first debias the samples, e.g., *declustering*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you must have a sufficient sample size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integrates uncertainty due to sparse samples in space only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: does not account for the spatial context of the data, i.e., sample data locations,
    volume of interest nor the spatial continuity. There is a variant of bootstrap
    called [spatial bootstrap](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spatial_Bootstrap.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    can only take one of a limited, and usually fixed, number of possible values'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical Nominal Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical*
    feature without any natural ordering, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: facies = {boundstone, wackystone, packstone, brecia}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minerals = {quartz, feldspar, calcite}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical Ordinal Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical*
    feature with a natural ordering, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: geologic age = {Miocene, Pliocene, Pleistocene} - ordered from older to younger
    rock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohs hardness = \(\{1, 2, \ldots, 10\}\) - ordered from softer to harder rock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Causation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a relationship
    where a change in one or more feature(s) directly leads to a change in one or
    more other feature(s).'
  prefs: []
  type: TYPE_NORMAL
- en: Some important aspects of causal relationships,
  prefs: []
  type: TYPE_NORMAL
- en: '*Asymmetry and temporal precedence* - \(A\) is caused by \(B\) does not indicate
    that \(B\) is caused by \(A\)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Non-spurious* - not due to random effect or confounding features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mechanism and explanation* - a plausible mechanism or process is available
    to explain the relationship'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Consistency* - the relationship is observable over a range of conditions,
    times, locations, populations, etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Strength* - stronger relationships increase the likelihood of causation given
    all the previous 1-5 hold'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing causation is very difficult,
  prefs: []
  type: TYPE_NORMAL
- en: in this course we typically avoid causation and causal analysis, and emphasize
    this with statements such as correlation does not imply causation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell-based Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned such that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of declustering is for the sample statistics to be independent of sample
    locations, e.g., infill drilling or blast hole samples should not change the statistics
    for the area of interest due to increased local sample density.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cell-based declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: a cell mesh is placed over the spatial data and weights are set as proportional
    to the inverse of the number of samples in the cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the cell mesh size is varied, and the cell size that minimizes the declustered
    mean (in the sample mean is biased high) or maximizes the declustered mean (if
    the sample mean is biased low) is selected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: to remove the impact of cell mesh position, the cell mesh is randomly moved
    several times and the resulting declustering weights are averaged for each datum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The weights are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = \frac{1}{n_l} \cdot \frac{n}{L_o} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n_l\) is the number of data in the current cell, \(L_o\) is the number
    of cells with data, and \(n\) is the total number of data.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some highlights for cell-based declustering,
  prefs: []
  type: TYPE_NORMAL
- en: expert judgement to assign cell size based on the nominal sample spacing (e.g.,
    data spacing before infill drilling) will improve the performance over the automated
    method for cell size selection based on minimum or maximum declustered mean (mentioned
    above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cell-based declustering is not aware of the boundaries of the area of interest;
    therefore, data near the boundary of the area of interest may appear to be more
    sparsely sampled and receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cell-based was developed by Professor Andre Journel in 1983, []
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cognitive Biases**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): an automated (subconscious)
    thought process used by human brain to simplify information processing from large
    amount of personal experience and learned preferences. While these have been critical
    for our evolution and survival on this planet, they can lead to the following
    issues in data science:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Anchoring Bias*, too much emphasis on the first piece of information. Studies
    have shown that the first piece of information could be irrelevant as we are beginning
    to learn about a topic, and often the earliest data in a project has the largest
    uncertainty. Address anchoring bias by curating all data, integrating uncertainty,
    fostering open discussion and debate on your project team.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Availability Heuristic*, overestimate importance of easily available information,
    for example, grandfather smoked 3 packs a day and lived to 100 years old, i.e.,
    relying on anecdotes. Address availability heuristic by ensuring the project team
    documents all available information and applies quantitative analysis to move
    beyond anecdotes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Bandwagon Effect*, assessed probability increases with the number of people
    holding the same belief. Watch out for everyone jumping on board or the loudest
    voice influencing all others on your project teams. Encouraging all members of
    the project team to contribute and even separate meetings may be helpful to address
    bandwagon effect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Blind-spot Effect*, fail to see your own cognitive biases. This is the hardest
    cognitive bias of all. One possible solution is to invite arms length review of
    your project team‚Äôs methods, results and decisions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Choice-supportive Bias*, probability increases after a commitment, i.e., a
    decision is made. For example, it was good that I bought that car supported by
    focusing on positive information about the car. This is a specific case of confirmation
    bias.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Clustering Illusion*, seeing patterns in random events. Yes, this heuristic
    helped us stay alive when large predictors hunted us, i.e., false positives are
    much better than false negatives! The solution is to model uncertainty confidence
    intervals and test all data and results against random effect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Confirmation Bias*, only consider new information that supports current model.
    Choice-supportive bias is a specific case of confirmation bias. The solution to
    confirmation bias is to seek out people that you will likely disagree with and
    build skilled project teams that hold diverse technical opinions and have different
    expert experience. My approach is to get nervous if everyone in the room agrees
    with me!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Conservatism Bias*, favor old data to newly collected data. Data curation
    and quantitative analysis are helpful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Recency Bias*, favor the most recently collected data. Ensure your team documents
    previous data and choices to enhance team memory. Just like conservative bias,
    data curation and quantitative analysis are our first line of defense.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Survivorship Bias*, focus on success cases only. Check for any possible pre-selection
    or filters on the data available to your team.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Robust use of statistics / data analytics protects use from bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Complimentary Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the NOT operator
    for probability, if we define A then A compliment, \(A^c\), is not A and we have
    this resulting closure relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: complimentary events may be considered for beyond univariate problems, for example
    consider this bivariate closure,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) + P(A^c|B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, the given term must be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational Complexity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): represents the
    computer resources for a method, we use it in machine learning to understand how
    our machine learning methods scale as we change the dimensionality, number of
    features, and the number of training data, represented by,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëÇ(ùëì(ùëõ)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùëõ\) represents size of the problem. There are 2 components of computational
    complexity,
  prefs: []
  type: TYPE_NORMAL
- en: '*time complexity* - refers to computational time and the scaling of this time
    to the size of the problem for a given algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*space complexity* - refers to computer memory required and the scaling of
    storage to the size of the problem for a given algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if time complexity is \(O(n^3)\), where is \(n\) is number of training
    data, then if we double the number of data the run time increases eight times.
  prefs: []
  type: TYPE_NORMAL
- en: Additional salient points about computational complexity,
  prefs: []
  type: TYPE_NORMAL
- en: '*default to worst-case complexity* - the worst case for complexity given a
    specific problem size, provides an upper bound'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*asymptotic complexity* - where \(ùëõ\) is large. Some algorithms have speed-up
    for small datasets, this is not used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumes all steps are required, e.g., data is not presorted, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time complexity examples,
  prefs: []
  type: TYPE_NORMAL
- en: '*quadratic time*, \(ùë∂(ùíè^ùüê)\) - for example, integer multiplication, bubble
    sort'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*linear time*, \(ùë∂(ùíè)\) - for example, finding the min or max in an unsorted
    array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*fractional power*, \(ùë∂(ùíè^ùíÑ )\) - where \([0 < c < 1]\), for example, searching
    in a kd-tree, \(ùëÇ(ùëõ^(\frac{1}{2}))\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exponential Time*, \(ùë∂(ùüê^ùíè)\) - for example, traveling salesman problem with
    dynamic programing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the probability of
    an event, given another event has occurred,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: we read this as the probability of A given B has occurred as the joint divided
    by the marginal. We can extend conditional probabilities to any multivariate case
    by adding joints to either component. For example,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B,A) = \frac{P(A,B,C)}{P(B,C)} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Interval**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the uncertainty
    in a summary statistic or model parameter represented as a range, lower and upper
    bound, based on a specified probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: We communicate confidence intervals like this,
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that model slope is between
    0.5 and 0.7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other salient points about confidence intervals,
  prefs: []
  type: TYPE_NORMAL
- en: calculated by analytical methods, when available, or with more general and flexible
    bootstrap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for Bayesian methods we refer credibility intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion Matrix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a matrix with frequencies
    of predicted (x axis) vs. actual (y axis) categories to visualize the performance
    of a classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: visualize and diagnose all the combinations of correct and misclassification
    with the classification model, for example, category 1 is often misclassified
    as category 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perfect accuracy is number of each class on the diagonal, category 1 is always
    predicted as category 1, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the classification matrix is applied to calculate a single summary of categorical
    accuracy, for example, precision, recall, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    can take any value between a lower and upper bound. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity = \(\{13.01\%, 5.23\%, 24.62\%\}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gold grade = \(\{4.56 \text{ g/t}, 8.72 \text{ g/t}, 12.45 \text{ g/t} \}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous, Interval Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *continuous feature*
    where the intervals between numbers are equal, for example, the difference between
    1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the actual
    values do not have an objective, physical reality (exist on an arbitrary scale),
    i.e., do not have a true zero point, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: Celsius scale of temperature (an arbitrary scale based on water freezing at
    0 and boiling at 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calendar year (there is no true zero year)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use addition and subtraction operations to compare continuous, interval
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous, Ratio Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *continuous feature*
    where the intervals between numbers are equal, for example, the difference between
    1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the values
    do have an objective reality (measure an actual physical phenomenon), i.e., do
    have true zero point, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: Kelvin scale of temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saturation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is a true zero, continuous, ratio features can be compared with
    multiplication and division mathematical operations (in addition to addition and
    subtraction), e.g., twice as much porosity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuously Differentiable**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Training and Tuning](MachineLearning_training_tuning.html):
    a function is continuously differentiable if it satisfies two key conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The function is differentiable, the derivative of the function exists at every
    point in its domain, i.e., the function has a well-defined slope at every possible
    point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The derivative is continuous, the derivative of the function does not have any
    jumps, discontinuities, or abrupt changes, i.e, the derivative function itself
    is continuous at every point in its domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a machine learning example,
  prefs: []
  type: TYPE_NORMAL
- en: the \(L^2\) norm is continuously differentiable and as a result for linear and
    ridge regression we can apply partial derivatives to the loss function to calculate
    a closed form of training the model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the \(L^1\) norm is not continuously differentiable and as a result for LASSO
    regression we cannot apply partial derivatives to the loss function to calculate
    a closed form of training the model parameters. We must use iterative optimization
    to train the model parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): Integral
    product of two functions, after one is reversed and shifted by \(\Delta\).'
  prefs: []
  type: TYPE_NORMAL
- en: one interpretation is smoothing a function with weighting function, \(ùëì(\Delta)\),
    is applied to calculate the weighted average of function, \(ùëî(x)\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: this easily extends into multidimensional
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \,
    d\Delta_x \, d\Delta_y \, d\Delta_z \]
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity.
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Core Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the primary sampling
    method for direct measure for subsurface resources (recovered drill cuttings are
    also direct measures with greater uncertainty and smaller, irregular scale). Comments
    on core data,'
  prefs: []
  type: TYPE_NORMAL
- en: expensive / time consuming to collect for oil and gas, interrupt drilling operations,
    sparse and selective (very biased) coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: very common in mining (diamond drill holes) for grade control with regular patterns
    and tight spacing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gravity, piston, etc. coring are used to sample sediments in lakes and oceans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do we learn from core data?
  prefs: []
  type: TYPE_NORMAL
- en: petrological features (sedimentary structures, mineral grades), petrophysical
    features (porosity, permeability), and mechanical features (elastic modulas, Poisson‚Äôs
    ratio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stratigraphy and ore body geometry through interpolation between wells and drill
    holes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core data are critical to support subsurface resource interpretations. They
    anchor the entire reservoir concept and framework for prediction,
  prefs: []
  type: TYPE_NORMAL
- en: for example, core data collocated with well log data are used to calibrate (ground
    truth) facies, porosity from well logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): the Pearson‚Äôs
    product-moment correlation coefficient is a measure of the degree of linear relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x
    \sigma_y} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\overline{y}\) are the means of features \(x\)
    and \(y\). The measure is bound \(\[-1,1\]\).
  prefs: []
  type: TYPE_NORMAL
- en: correlation coefficient is a standardized covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Person‚Äôs correlation coefficient is quite sensitive to outliers and departure
    from linear behavior (in the bivariate sense). We have an alternative known as
    the Spearman‚Äôs rank correlations coefficient,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i}
    - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le
    1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The rank correlation applies the rank transform to the data prior to calculating
    the correlation coefficient. To calculate the rank transform simply replace the
    data values with the rank \(R_x = 1,\dots,n\), where \(n\) is the maximum value
    and \(1\) is the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall
    \, i \gt j \]\[ R_{x_i} = i \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a measure
    of how two features vary together,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ C_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\overline{y}\) are the means of features \(x\)
    and \(y\). The measure is bound \(\[-\sigma_x \cdot \sigm_y,\sigma_x \cdot \sigm_y\]\).
  prefs: []
  type: TYPE_NORMAL
- en: correlation coefficient is a standardized covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Person‚Äôs correlation coefficient is quite sensitive to outliers and departure
    from linear behavior (in the bivariate sense). We have an alternative known as
    the Spearman‚Äôs rank correlations coefficient,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i}
    - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le
    1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The rank correlation applies the rank transform to the data prior to calculating
    the correlation coefficient. To calculate the rank transform simply replace the
    data values with the rank \(R_x = 1,\dots,n\), where \(n\) is the maximum value
    and \(1\) is the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall
    \, i \gt j \]\[ R_{x_i} = i \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross Validation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): withholding a portion
    of the data from the model parameter training to test the ability of the model
    to predict for cases not used to train the model'
  prefs: []
  type: TYPE_NORMAL
- en: this is typically conducted by a train and test data split, with 15% - 30% of
    data assigned to testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a dress rehearsal for real-world model use, the train-test split must be fair,
    resulting in similar prediction difficulty to the planned use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are more complicated designs such as k-fold cross validation that allows
    testing over all data via k-folds each with trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cross validation may be applied to check model performance for estimation accuracy
    (most common) and uncertainty model goodness ([Maldonado-Cruz and Pyrcz, 2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cumulative Distribution Function** (CDF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): the sum of
    a discrete PDF or the integral of a continuous PDF. Here are the important concepts,'
  prefs: []
  type: TYPE_NORMAL
- en: the CDF is stated as \(F_x(x)\), note the PDF is stated as \(f_x(x)\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: is the probability that a random sample, \(X\), is less than or equal to a specific
    value \(x\); therefore, the y axis is cumulative probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ F_x(x) = P(X \le x) = \int_{-infty}^x f(u) du \]
  prefs: []
  type: TYPE_NORMAL
- en: for CDFs there is no bin assumption; therefore, bins are at the resolution of
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monotonically non-decreasing function, because a negative slope would indicate
    negative probability over an interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requirements for a valid CDF include,
  prefs: []
  type: TYPE_NORMAL
- en: 'non-negativity constraint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_x(x) = P(X \le x) \ge 0.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'valid probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ 0.0 \le F_x(x) \le 1.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'cannot have negative slope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \frac{dF_x(x)}{dx} \ge 0.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'minimum and maximum (ensuring probability closure) values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{min}(F_x(x)) = 0.0 \quad \text{max}(F_x(x)) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Curse of Dimensionality**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): the suite of challenges
    associated with working with many features, i.e., high dimensional space, including,'
  prefs: []
  type: TYPE_NORMAL
- en: impossible to visualize data and model in high dimensionality space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: usually insufficient sampling for statistical inference in vast high dimensional
    space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: low coverage of high dimensional predictor feature space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: distorted feature space, including warped space dominated by corners and distances
    lose sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multicollinearity between features is more likely as the dimensionality increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data** (data aspects)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): when describing spatial
    dataset these are the fundamental aspects,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data coverage* - what proportion of the population has been sampled for this?'
  prefs: []
  type: TYPE_NORMAL
- en: In general, hard data has high resolution (small scale, volume support), but
    with poor data coverage (measure only an extremely small proportion of the population,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Core coverage deepwater oil and gas* - well core only sample one five hundred
    millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter
    cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Core coverage mining grade control* - diamond drill hole cores sample one
    eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter
    cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft data tend to have excellent (often complete) coverage, but with low resolution,
  prefs: []
  type: TYPE_NORMAL
- en: '*Seismic reflection surveys and gradiometric surveys* - data is generally available
    over the entire volume of interest, but resolution is low and generally decreasing
    with depth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Scale* (support size) - What is the scale or volume sampled by the individual
    samples? For example,'
  prefs: []
  type: TYPE_NORMAL
- en: core tomography images of core samples at the pore scale, 1 - 50 \(\mu m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gamma ray well log sampled at 0.3 m intervals with 1 m penetration away from
    the bore hole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ground-based gravity gradiometry map with 20 m x 20 m x 100 m resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Information Type* - What does the data tell us about the subsurface?
    For example,'
  prefs: []
  type: TYPE_NORMAL
- en: grain size distribution that may be applied to calibrate permeability and saturations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fluid type to assess the location of the oil water contact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dip and continuity of important reservoir layers to access connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mineral grade to map high, mid and low grade ore shells for mine planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Convexity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    a subset, \(A\), of Euclidean feature space is convex if, for any two points \(ùë•_1\)
    and \(ùë•_2\) within \(ùê¥\), the entire line segment connecting these points is within
    \(A\), \(\left[ùë•_1,ùë•_2\right] \in A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: a convenient Pandas class
    for working with data tables with rows for each sample and columns for each feature,
    due to,'
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to load data from a variety of file types, Python classes and
    even directly from Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to calculate summary statistics and visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data queries, sort, data filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in attributes to store information about the data, e.g. size, number nulls
    and null value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Analytics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the use of statistics
    with visualization to support decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: Dr. Pyrcz says that data analytics is the same as statistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any workflow steps
    to enhance, improve raw data to be model ready.'
  prefs: []
  type: TYPE_NORMAL
- en: data-driven science needs data, data preparation remains essential
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\gt >80\%\) of any subsurface study is data preparation and interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We continue to face a challenge with data:'
  prefs: []
  type: TYPE_NORMAL
- en: data curation - format standards, version control, storage, transmission, security
    and documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: large volume to manage - visualization, availability and data mining and exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: large volumes of metadata - lack of platforms, standards and formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: engineering integration, variety of data, scale, interpretation and uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean databases are prerequisite to all data analytics and machine learning
  prefs: []
  type: TYPE_NORMAL
- en: must start with this foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: garbage in, garbage out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Degree Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the number of connections for each graph nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: diagonal matrix with integer for number of connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DBSCAN for Density-based Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    an density-based clustering algorithm, groups are seeded or grown in feature space
    at locations with sufficient point density determined by hyperparameters,'
  prefs: []
  type: TYPE_NORMAL
- en: \(\epsilon\) ‚Äì the radius of the local neighbourhood in the metric of normalized
    features. The is the scale / resolution of the clusters. If this values is set
    too small, too many samples are left as outliers and if set too large, all the
    clusters merge to one single cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(min_{Pts}\) ‚Äì the minimum number of points to assign a core point, where core
    points are applied to initialize or grow a cluster group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density is quantified by number of samples over a volume, where the volume is
    based on a radius over all dimensions of feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Automated or guided \(\epsilon\) parameter estimation is available by k-distance
    graph (in this case is k nearest neighbor).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the nearest neighbor distance in normalized feature space for all
    the sample data (1,700 in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort in ascending order and plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the distance that maximizes the positive curvature (the elbow).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a summary of salient aspects for DBSCAN clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '*DBSCAN* - stands for Density-Based Spatial Clustering of Applications with
    Noise (Ester et al.,1996).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantages* - include minimum domain knowledge to estimate hyperparameters,
    the ability to represent any arbitrary shape of cluster groups and efficient to
    apply for large data sets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hierarchical Bottom-up / Agglomerative Clustering* ‚Äì all data samples start
    as their own group, called ‚Äòunvisited‚Äô but practically as outliers until assigned
    to a group, and then the cluster group grow iteratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mutually Exclusive* ‚Äì like k-means clustering, all samples may only belong
    to a single cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_i \cap C_j | i \ne j) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Non-exhaustive* ‚Äì some samples may be left as unassigned and assumed as outliers
    for the cluster group assignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Criteria**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    is calculated by applying the transfer function to the subsurface model(s) to
    support decision making. The decision criteria represents value, health, environment
    and safety. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: contaminant recovery rate to support design of a pump and treat soil remediation
    project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: oil-in-place resources to determine if a reservoir should be developed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lorenz coefficient heterogeneity measure to classify a reservoir and determine
    mature analogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recovery factor or production rate to schedule production and determine optimum
    facilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recovered mineral grade and tonnage to determine economic ultimate pit shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Tree**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Decision Tree](MachineLearning_decision_tree.html): a intuitive, regression
    and classification predictive machine learning model that devides the predictor
    space, \(ùëã_1,‚Ä¶,ùëã_ùëö\), into \(ùêΩ\) mutually exclusive, exhaustive regions, \(ùëÖ_ùëó\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*mutually exclusive* ‚Äì any combination of predictors only belongs to a single
    region, \(ùëÖ_ùëó\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exhaustive* ‚Äì all combinations of predictors belong a region, \(ùëÖ_ùëó\), regions
    cover entire feature space, range of the variables being considered'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same prediction in each region, mean of training data in region, \(\hat{Y}(ùëÖ_ùëó)
    = \overline{Y}(ùëÖ_ùëó)\)
  prefs: []
  type: TYPE_NORMAL
- en: for classification the most common, mode-based or argmax operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other salient points about decision tree,
  prefs: []
  type: TYPE_NORMAL
- en: '*supervised Learning* - the response feature label, \(Y\), is available over
    the training and testing data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*hierarchical, binary segmentation* - of the predictor feature space, start
    with 1 region and sequentially divide, creating new regions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*compact, interpretable model* - since the classification is based on a hierarchy
    of binary segmentations of the feature space (one feature at a time) the model
    can be specified in a intuitive manner as a tree with binary branches**, hence
    the name decision tree. The code for the model is nested if statements, for example,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The decision tree is constructed from the top down. We begin with a single region
    that covers the entire feature space and then proceed with a sequence of splits,
  prefs: []
  type: TYPE_NORMAL
- en: '*scan all possible splits* - over all regions and over all features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*greedy optimization* - proceeds by finding the best split in any feature that
    minimizes the residual sum of squares of errors over all the training data \(y_i\)
    over all of the regions \(j = 1,\ldots,J\). There is no other information shared
    between subsequent splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters include,
  prefs: []
  type: TYPE_NORMAL
- en: '*number of regions* ‚Äì very easy to understand, you know what the model will
    be'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*minimum reduction in RSS* ‚Äì could stop early, e.g., a low reduction in RSS
    split could lead to a subsequent split with a larger reduction in RSS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*minimum number of training data in each region* ‚Äì related to the concept of
    accuracy of the region mean prediction, i.e., we need at least ùëõ data for a reliable
    mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*maximum number of levels* ‚Äì forces symmetric trees, similar number of splits
    to get to each region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: various methods that assign weights to spatial samples based
    on local sampling density, such that the weighted statistics are likely more representative
    of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various declustering methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*cell-based declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*polygonal declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kriging-based declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that no declustering method can prove that for every
    data set the resulting weighted statistics will improve the prediction of the
    population parameters, but in expectation these methods tend to reduce the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Declustering** (statistics)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: once declustering weights are calculated for a spatial dataset,
    then declustered statistics are applied as input for only subsequent analysis
    or modeling. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: the declustered mean is assigned as the stationary, global mean for simple kriging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the weighted CDF from all the data with weights are applied to sequential Gaussian
    simulation to ensure the back-transformed realizations approach the declustered
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any statistic can be weighted, including the entire CDF! Here are some examples
    of weighted statistics, given declustering weights, \(w(\bf{u}_j)\), for all data
    \(j=1,\ldots,n\).
  prefs: []
  type: TYPE_NORMAL
- en: weighted sample mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \overline{x}_{wt} = \frac{\sum_{i=1}^n w(\bf{u}_j) \cdot z(\bf{u}_j)}{\sum_{i=1}^n
    w(\bf{u}_j)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n\) is the number of data.
  prefs: []
  type: TYPE_NORMAL
- en: weighted sample variance,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ s^2_{x_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) - 1} \cdot \sum_{i=1}^n
    w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}_{wt}\) is the declustered mean.
  prefs: []
  type: TYPE_NORMAL
- en: weighted covariance,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ C_{x,y_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) } \cdot \sum_{i=1}^n w(\bf{u}_j)
    \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right) \cdot \left( y(\bf{u}_j)
    - \overline{y}_{wt} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}_{wt}\) and \(\overline{y}_{wt}\) are the declustered means
    for features \(X\) and \(Y\).
  prefs: []
  type: TYPE_NORMAL
- en: the entire CDF,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ F_z(z) \approx \sum_{j=1}^{n(Z<z)} w(\bf{u}_j) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n(Z<z)\) is the number of sorted ascending data less than threshold
    \(z\). We show this as approximative as this is simplified and at data resolution
    and without an interpolation model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that no declustering method can prove that for every
    data set the resulting weighted statistics will improve the prediction of the
    population parameters, but in expectation these methods tend to reduce the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-Connected** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    points \(A\) and \(B\) are density-connected if there is a point \(Z\) that is
    density-reachable from both points \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-based Cluster** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    a nonempty set where all points are density-connected to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-Reachable** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    point \(Y\) is density reachable from \(A\) if \(Y\) belongs to a neighborhood
    of a core point that can reached from \(A\). This would require a chain of core
    points each belonging the previous core points and the last core point including
    point Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that assumes
    the system or process that is completely predictable'
  prefs: []
  type: TYPE_NORMAL
- en: often-based on engineering and geoscience physics and expert judgement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, numerical flow simulation or stratigraphic bounding surfaces interpreted
    from seismic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for this course we also state that data-driven estimation models like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: integration of physics and expert knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integration of various information sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: often quite time consuming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: often no assessment of uncertainty, focus on building one model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): methods to reduce
    the number of features within a data science workflow. There are 2 primary methods,'
  prefs: []
  type: TYPE_NORMAL
- en: '*features Selection* ‚Äì find the subset of original features that are most important
    for the problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*feature projection* ‚Äì transform the data from a higher to lower dimensional
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known as dimension reduction or dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: motivated by the curse of dimensionality and multicollinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: applied in statistics, machine learning and information theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Directly Density Reachable** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    point \(X\) is directly density reachable from \(A\), if \(A\) is a core point
    and \(X\) belongs to the neighborhood, distance \(le \epsilon\) from \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discrete Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical
    feature* or a *continuous feature* that is binned or grouped, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity between 0 and 20% assigned to 10 bins = {0 - 2%, 2% - 4%, \ldots ,20%}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohs hardness = \(\{1, 2, \ldots, 10\}\) (same at *categorical feature*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution Transformations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    mapping from one distribution to another distribution through percentile values,
    resulting in a new histogram, PDF, and CDF. We perform distribution transformations
    in geostatistical methods and workflows because,'
  prefs: []
  type: TYPE_NORMAL
- en: '*inference* - to correct a feature distribution to an expected shape, for example,
    correcting for too few or biased data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*theory* - a specific distribution assumption is required for a workflow step,
    for example, Gaussian distribution with mean of 0.0 and variance of 1.0 is required
    for sequential Gaussian simulation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*data preparation or cleaning* - to correct for outliers, the transformation
    will map the outlier into the target distribution no longer as an outlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we perform distribution transformations?
  prefs: []
  type: TYPE_NORMAL
- en: 'We transform the values from the cumulative distribution function (CDF), \(F_{X}\),
    to a new CDF , \(G_{Y}\). This can be generalized with the quantile - quantile
    transformation applied to all the sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward transform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Y = G_{Y}^{-1}(F_{X}(X)) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse transform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ X = F_{X}^{-1}(G_{Y}(Y)) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This may be applied to any data, including parametric or nonparametric distributions.
    We just need to be able to map from one distribution to another through percentiles,
    so it is a:'
  prefs: []
  type: TYPE_NORMAL
- en: rank preserving transform, for example, P25 remains P25 after distribution transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eager Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): Model is
    a generalization of the training data constructed prior to queries'
  prefs: []
  type: TYPE_NORMAL
- en: the model is input-independent after parameter training and hyperparameter tuning,
    i.e., the training data does not need to be available to make new predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opposite is lazy learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is process of obtaining
    the single best value to represent a feature at an unsampled location, or time.
    Some additional concepts,'
  prefs: []
  type: TYPE_NORMAL
- en: local accuracy takes precedence over global spatial variability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too smooth, not appropriate for any transform function that is sensitive to
    heterogeneity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, inverse distance and kriging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: many predictive machine learning models focus on estimation (e.g., k-nearest
    neighbours, decision tree, random forest, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**f1-score** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the harmonic mean of recall and precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ f1-score_k = \frac{2} { \frac{1}{Precision_k} + \frac{1}{Recall_k} } \]
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder,
  prefs: []
  type: TYPE_NORMAL
- en: '*recall* - the ratio of true positives divided by all cases of the category
    in the testing dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*precision* - the ratio of true positives divided by all positives, true positives
    + false positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature** (also variable)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any property measured
    or observed in a study'
  prefs: []
  type: TYPE_NORMAL
- en: for example, porosity, permeability, mineral concentrations, saturations, contaminant
    concentration, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in data mining / machine learning this is known as a feature, statisticians
    call these variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measure often requires significant analysis, interpretation, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when features are modified and combined to improve our models we call this feature
    engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Engineering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): using
    domain expertise to extract improved predictor or response features from raw data,'
  prefs: []
  type: TYPE_NORMAL
- en: improve the performance, accuracy and convergency, of inferential or predictive
    machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: improve model interpretability (or may worsen interpretability if our engineered
    features are in unfamiliar units)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mitigate outliers & bias, consistency with assumptions such as Gaussianity,
    linearization, dimensional expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformation and feature selection are two forms of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Importance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a variety of machine
    learning methods to provide measures for feature ranking, for example decision
    trees summarize the reduction in mean square error through inclusion of each feature
    and is summarized as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(T_f\) are all nodes with feature \(x\) as the split, \(N_t\) is the
    number of training samples reaching node \(t\), \(N\) is the total number of samples
    in the dataset and \(\Delta_{MSE_t}\) is the reduction in MSE with the \(t\) split.
  prefs: []
  type: TYPE_NORMAL
- en: Note, feature importance can be calculated in a similar manner to MSE above
    for the case of classification trees with *Gini Impurity*.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance is part of model-based feature ranking,
  prefs: []
  type: TYPE_NORMAL
- en: the accuracy of the feature importance depends on the accuracy of the model,
    i.e., an inaccurate model will likely provide incorrect feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Imputation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): replacing null
    values in the data table, samples that do not have values for all features with
    plausible values for 2 reasons,'
  prefs: []
  type: TYPE_NORMAL
- en: enable statistical calculations and models that require complete data tables,
    i.e., cannot work with missing feature values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maximize model accuracy, increasing the number of reliable samples available
    for training and testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mitigate model bias that may occur with likewise deletion in feature values
    are not missing at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature imputation methods include,
  prefs: []
  type: TYPE_NORMAL
- en: '*constant value imputation* - replace null values with feature mean or mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model-based imputation* - replace null values with a prediction of the missing
    feature with available feature values for the same sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also an iterative methods that depend on convergence,
  prefs: []
  type: TYPE_NORMAL
- en: '*Multiple Imputation by Chained Equations (MICE)* - assign random values and
    then iterate over the missing values predicting new values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of this method is to obtain reasonable imputed values that account
    for the relationships between all the features and all the available and missing
    values
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Projection**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): a transforms original
    \(m\) features to \(p\) features, where \(p << m\) for dimensionality reduction'
  prefs: []
  type: TYPE_NORMAL
- en: given features, \(ùëã_1,\ldots,ùëã_ùëö\) we would require \(\binom{m}{2} = \frac{ùëö(ùëö‚àí1)}{2}\)
    scatter plots to visualize just the two-dimensional scatter plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these representations would not capture \(> 2\) dimensional structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once we have 4 or more variables understanding our data gets very difficult.
    Recall the curse of dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: principal component analysis, multidimensional scaling and random projection
    are examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature selection is an alternative method for dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Space**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): commonly feature space
    only refers to the predictor features and does not include the response feature(s),
    i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: all possible combinations of predictor features for which we need to make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may be referred to as predictor feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we train and test our machines‚Äô predictions over the predictor feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: the space is typically a hypercuboid with each axis representing a predictor
    feature and extending from the minimum to maximum, over the range of each predictor
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more complicated shapes of predictor feature space are possible, e.g., we could
    mask or remove subsets with poor data coverage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Ranking**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): part of feature engineering,
    feature ranking is a set of methods that assign relative importance or value to
    each feature with respect to information contained for inference and importance
    in predicting a response feature.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a wide variety of possible methods to accomplish this. My recommendation
    is a wide-array approach with multiple metric, while understanding the assumptions
    and limitations of each method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the general types of metrics that we will consider for feature ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual Inspection* - including data distributions, scatter plots and violin
    plots'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Statistical Summaries* - correlation analysis, mutual information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model-based* - including model parameters, feature importance scores and global
    Shapley values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive feature elimination* - and other methods that perform trail and
    error to find optimum parameters sets through withheld testing data cross validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature ranking is primarily motivated by the curse of dimensionality, i.e.,
    work with the fewest, most informative predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Transformations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    type of feature engineering involving mathematical operation applied to a feature
    to improve the value of the feature in a workflow. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: feature truncation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature normalization or standardization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature distribution transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many reasons that we may want to perform feature transformations.
  prefs: []
  type: TYPE_NORMAL
- en: the make the features consistent for visualization and comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to avoid bias or impose feature weighting for methods (e.g. k nearest neighbours
    regression) that rely on distances calculated in predictor feature space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the method requires the variables to have a specific range or distribution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: artificial neural networks may require all features to range from [-1,1]
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: partial correlation coefficients require a Gaussian distribution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical tests may require a specific distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: geostatistical sequential simulation requires an indicator or Gaussian transform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformations is a common basic building blocks in many machine learning
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fourth Paradigm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the data-driven
    paradigm for scientific discovery building from the,'
  prefs: []
  type: TYPE_NORMAL
- en: First Paradigm - empirical science - experiments and observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second Paradigm - theoretical science - analytical expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third Paradigm - computation science - numeric simulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We augment with new scientific paradigms, we don‚Äôt replace older paradigms.
    Each of the previous paradigm are supported by the previous paradigms, for example,
  prefs: []
  type: TYPE_NORMAL
- en: theoretical science is build on empirical science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: numerical simulations integrate analytical expressions and calibrated equations
    from experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequentist Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): measure of the likelihood
    that an event will occur based on frequencies observed from an experiment. For
    random experiments and well-defined settings (such as coin tosses),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Prob}(A) = P(A) = \lim_{n \to \infty} \frac{n(A)}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(n(A)\) = number of times event \(A\) occurred \(n\) = number of trails
  prefs: []
  type: TYPE_NORMAL
- en: For example, possibility of drilling a dry hole for the next well, encountering
    sandstone at a location (\(\bf{u}_{\alpha}\)), exceeding a rock porosity of \(15
    \%\) at a location (\(\bf{u}_{\alpha}\)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian Anamorphosis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    quantile transformation to a Gaussian distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping feature values through their cumulative probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = G_y^{-1}\left( F_x(x)\right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùêπ_ùë•\) is the original feature cumulative distribution function (CDF)
    and \(ùê∫_ùë¶\) is the Gaussian CDF probability density function
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left[-1 \frac{1}{2} \left(\frac{x-\mu}{\sigma}
    \right)^2 \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: shorthand for a normal distribution is
  prefs: []
  type: TYPE_NORMAL
- en: \[ N[\mu,\sigma^2] \]
  prefs: []
  type: TYPE_NORMAL
- en: for example \(N[0,1]\) is standard normal
  prefs: []
  type: TYPE_NORMAL
- en: much of natural variation or measurement error is Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: parameterized fully by mean, variance and correlation coefficient (if multivariate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: distribution is unbounded, no min nor max, extremes are very unlikely, some
    type of truncation is often applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning, many workflows apply univariate Gaussian anamorphosis and then assume
    bivariate or multivariate Gaussian, this is not correct, but it is generally too
    difficult to transform our data to multivariate Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: Methods that require a Gaussian distribution,
  prefs: []
  type: TYPE_NORMAL
- en: Pearson product-moment correlation coefficients completely characterize multivariate
    relationships when data are multivariate Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: partial correlations require bivariate Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sequential simulation (geostatistics) assumes Gaussian to reproduce the global
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Student‚Äôs t test for difference in means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-square distributions is derived from sum of squares of Gaussian distributed
    random variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian naive Bayes classification assumes Gaussian conditionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gibbs Sampler** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics, based on,'
  prefs: []
  type: TYPE_NORMAL
- en: sequentially sampling from conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since only the conditional probability density functions are required, the system
    is simplified as the full joint probability density function is not needed
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the basic steps of the Gibbs MCMC Sampler for a bivariate case,
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for \(ùëã(0)\), \(ùëå(0)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from \(ùëì(ùëã|ùëå(0))\) to get \(ùëã(1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from \(ùëì(ùëå|ùëã(1))\) to get \(ùëå(1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for the next steps for samples, \(\ell = 1,\ldots,ùêø\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting samples will have the correct joint distribution,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëì(ùëã,ùëå) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Boosting Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): a prediction model
    that results from posing a boosting model as gradient descent problem'
  prefs: []
  type: TYPE_NORMAL
- en: At each step, \(k\), a model is being fit, then the error is calculated, \(h_k(X_1,\ldots,X_m)\).
  prefs: []
  type: TYPE_NORMAL
- en: We can assign a loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)^2}{2} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So we want to minimize the \(\ell2\) loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: by adjusting our model result over our training data \(F(x_1), F(x_2),\ldots,F(x_n)\).
  prefs: []
  type: TYPE_NORMAL
- en: We can take the partial derivative of the error vs. our model,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We can interpret the residuals as negative gradients.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we have a gradient descent problem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ F_{k+1}(X_i) = F_k(X_i) + h(X_i) \]\[ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i)
    \]\[ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the general form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(phi_k\) is the current state, \(\rho\) is the learning rate, \(J\) is
    the loss function, and \(\phi_{k+1}\) is the next state of our estimator.
  prefs: []
  type: TYPE_NORMAL
- en: The error residual at training data is the gradient, then we are performing
    gradient descent,
  prefs: []
  type: TYPE_NORMAL
- en: fitting a series of models to negative gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By approaching the problem as a gradient decent problem we are able to apply
    a variety of loss functions,
  prefs: []
  type: TYPE_NORMAL
- en: \(\ell2\) is our \(\frac{\left(y - F(X)\right)^2}{2}\) is practical, but is
    not robust with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\ell1\) is our \(|y - F(X)|\) is more robust with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) \]
  prefs: []
  type: TYPE_NORMAL
- en: there are others like Huber Loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph Laplacian** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph by integrating connections between graph nodes, samples, number of connections
    for each graph nodes, samples. Calculated as degree matrix minus adjacency matrix.
    Where,'
  prefs: []
  type: TYPE_NORMAL
- en: '*degree matrix*, \(ùê∑\) - degree of connection for each node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adjacency matrix, \(ùê¥\) - specific connections between nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geostatistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a branch of applied
    statistics that integrates:'
  prefs: []
  type: TYPE_NORMAL
- en: the spatial (geological) context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the spatial relationship
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: volumetric support / scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: uncertainty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I include all spatial statistics with geostatistics, some disagree with me on
    this. From my experience, any useful statistical method for modeling spatial phenomenon
    is adopted and added to the geostatistics toolkit! Geostatistics is an expanding
    and evolving field of study.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient-based Optimization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): a method to solve
    for model parameters by iteratively minimizing the loss function. The steps include,'
  prefs: []
  type: TYPE_NORMAL
- en: start with random model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function for the model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function gradient, generally don‚Äôt have an equation for the
    loss function, sampling with numerical calculation of the local loss function
    derivative,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha},
    b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} \]
  prefs: []
  type: TYPE_NORMAL
- en: update the parameter estimate by stepping down slope / gradient,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(r\) is the learning rate/step size, \(\hat{b}(1,ùë°)\), is the current
    model parameter estimate and \(\hat{b}(1,ùë°+1)\) is the updated parameter estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Some important comments about gradient-based optimization,
  prefs: []
  type: TYPE_NORMAL
- en: '*gradient search convergence* - the method will find a local or global minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*gradient search step size* - impact of step size, \(r\) too small, takes too
    long to converge to a solution and \(r\) too large, the solution may skip over/miss
    a global minimum or diverge'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multiple model parameters* - calculate and decompose the gradient over multiple
    model parameters, with a vector representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{matrix} \nabla
    L(y_{\alpha}, F(X_{\alpha}, b_1)) & \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{matrix}
    \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: '*exploration of parameter space* - optimization for training machine learning
    model parameters is exploration of a high dimensional model parameter space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a diagram
    that represents data in an organized manner, each sample as a node with vertices
    indicating pairwise relationships between samples.'
  prefs: []
  type: TYPE_NORMAL
- en: for an undirected graph, vertices are bidirectional, i.e., the connection is
    symmetric, both ways with the same strength
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gridded Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: generally exhaustive, regularly
    spaced data over 2D or 3D, representing maps and models'
  prefs: []
  type: TYPE_NORMAL
- en: stored as a .csv comma delimited file, with \(ùëõ_ùë¶\) rows and \(ùëõ_ùë•\) columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may also be saved/loaded as also binary for a more compact, but not human readable
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: commonly visualized directly, for example, matplotlib‚Äôs imshow function, or
    as contour maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data that has a
    high degree of certainty, usually from a direct measurement from the rock'
  prefs: []
  type: TYPE_NORMAL
- en: for example, well core-based and well log-based porosity and lithofacies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, hard data has high resolution (small scale, volume support), but
    with poor coverage (measure only an extremely small proportion of the population,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Core coverage deepwater oil and gas* - well core only sample one five hundred
    millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter
    cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Core coverage mining grade control* - diamond drill hole cores sample one
    eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter
    cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hermite Polynomials**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): a family
    of orthogonal polynomials on the real number line.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Order | Hermite Polynomial \(H_e(x)\) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0th Order | \(H_{e_0}(x) = 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 1st Order | \(H_{e_1}(x) = x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 2nd Order | \(H_{e_2}(x) = x^2 - 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 3rd Order | \(H_{e_3}(x) = x^3 - 3x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 4th Order | \(H_{e_4}(x) = x^4 - 6x^2 + 3\) |'
  prefs: []
  type: TYPE_TB
- en: These polynomials are orthogonal with respect to a weighting function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùë§(ùë•)=ùëí^{‚àí\frac{ùë•^2}{2}} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the standard Gaussian probability density function without the scaler,
    \(\frac{1}{\sqrt{2\pi}}\). The definition of orthogonality is stated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The Hermite polynomials are orthogonal over the interval \([‚àí\infty,\infty]\)
    for the standard normal probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: By applying hermite polynomials instead of regular polynomials for polynomial
    basis expansion in polynomial regression were remove the multicollinearity between
    the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: recall, independence of the predictor features is an assumption of the linear
    system applied in polynomial regression with the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heuristic Algorithm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: a shortcut solution to solve a difficult problem a compromise
    of optimality and accuracy for speed and practicality.'
  prefs: []
  type: TYPE_NORMAL
- en: this general approach is common in machine learning, computer science and mathematical
    optimization, for example, the solution for k-mean clustering a \(k^n\) solution
    space is practically solved with an heuristic algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: all cluster group assignments are determined iteratively,
    as opposed to an partitional clustering method that determine cluster groups all
    at once. Including,'
  prefs: []
  type: TYPE_NORMAL
- en: '*agglomerative hierarchical clustering* - start with \(n\) clusters, each data
    sample in its own cluster, and then iteratively merges clusters into larger clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*divisive hierarchical clustering* - start with all data in one cluster, and
    then iteratively divide off new clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering is partitional clustering, while the solution heuristic to
    find the solution is iterative, the solution is actually all at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: difficult to update, once a series of splits or mergers are made it is difficult
    to go back and modify the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): a representation
    of the univariate statistical distribution with a plot of frequency over an exhaustive
    set of bins over the range of possible values. These are the steps to build a
    histogram,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the continuous feature range of possible values into \(K\) equal size
    bins, \(\delta x\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: or use available category labels for categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Count the number of samples (frequency) in each bin, \(n_k\), \quad \(\forall
    \quad k=1,\ldots,K\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the frequency vs. the bin label (use bin centroid if continuous)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note, histograms are typically plotted as a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): system or process
    that includes a combination of both *deterministic model* and *stochastic model*'
  prefs: []
  type: TYPE_NORMAL
- en: most geostatistical models are hybrid models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, additive deterministic trend models and stochastic residual models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): events \(A\) and
    \(B\) are independent if and only if the following relations are true,'
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A \cap B) = P(A) \cdot P(B)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(A|B) = P(A)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B|A) = P(B)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any of these are violated we suspect that there exists some form of relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '**Indicator Transform** (also Binary Transform)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): indicator
    coding a random variable to a probability relative to a category or a threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a categorical variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization equal to a category?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k
    \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_2 = 2\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 2\),
    then \(i(bf{u}_1; z_2) = 1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 1\), and a RV away from data, \(Z(\bf{u}_2)\) then
    is calculated as \(F^{-1}_{\bf{u}_2}(z_1)\) of the RV as \(i(\bf{u}_2; z_1) =
    0.23\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a continuous variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization less than or equal to a threshold?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le
    z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 6\%\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 8\%\),
    then \(i(\bf{u}_1; z_1) = 0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_4 = 18\%\), and a RV away from data, \(Z(\bf{u}_2) = N\left[\mu
    = 16\%,\sigma = 3\%\right]\) then \(i(\bf{u}_2; z_4) = 0.75\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indicator coding may be applied over an entire random function by indicator
    transform of all the random variables at each location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Indicator Variogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): varogram‚Äôs
    calculated and modelled from the *indicator transform* of spatial data and used
    for indicator kriging. The indicator variogram is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \gamma_i(\mathbf{h}; z_k) = \frac{1}{2N(\mathbf{h})} \sum_{\alpha=1}^{N(\mathbf{h})}
    \left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]^2
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i(\mathbf{u}_\alpha; z_k)\) and \(i(\mathbf{u}_\alpha + \mathbf{h};
    z_k)\) are the indicator transforms for the \(z_k\) threshold at the tail location
    \(\mathbf{u}_\alpha\) and head location \(\mathbf{u}_\alpha + \mathbf{h}\) respectively.
  prefs: []
  type: TYPE_NORMAL
- en: for hard data the indicator transform \(i(\bf{u},z_k)\) is either 0 or 1, in
    which case the \(\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h};
    z_k) \right]^2\) is equal to 0 when the values at head and tail are both \(\le
    z_k\) (for continuous features) or \(= z_k\) (for categorical features), the same
    relative to the threshold, or 1 when they are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: therefore, the indicator variogram is \(\frac{1}{2}\) the proportion of pairs
    that change! The indicator variogram can be related to probability of change over
    a lag distance, \(h\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the sill of an indicator variogram is the indicator variance calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma_i^2 = p \cdot (1 - p) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(p\) is the proportion of 1‚Äôs (or zeros as the function is symmetric
    over proportion)
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference, Inferential Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): this is a big topic,
    but for the course I provide this simplified, functional definition, given a random
    sample from a population, describe the population, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: given the well samples, describe the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given the drill hole samples, describe the ore body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inlier**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: a regression model accuracy metric, the proportion of testing data within a
    margin, \(\epsilon\), of the model predictions, \(\hat{y}_i\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: given the indicator transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful, intuitive measure of accuracy, the proportion of training
    or testing data with predictions that are good enough.
  prefs: []
  type: TYPE_NORMAL
- en: but, there is a choice of the size of the margin, \(\epsilon\), that could be
    related to the accuracy required for the specific application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance-based Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): also known
    as memory-based learning, compares new prediction problems (as set of predictors,
    \(ùë•_1,\ldots,ùë•_ùëö\)) with the cases observed in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: model requires access to the training data, acting as a library of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prediction directly from the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prediction complexity grows with the number of training data, \(ùëõ\), number
    of neighbors, \(ùëò\), and number of features, \(ùëö\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a specific case of lazy learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intersection of Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the intersection
    of outcomes, the probability of \(A\) and \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: under the assumption of independence of \(A\) and \(B\) the probability of \(A\)
    and \(B\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Irreducible Error**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    data limitations, including missing features and missing samples, for example,
    the full predictor feature space is not adequately sampled'
  prefs: []
  type: TYPE_NORMAL
- en: irreducible error is not impacted by model complexity, it is a limitation of
    the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_e^2\) is irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inertia** (clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: the k-means clustering loss function summarizing the difference
    between samples within the same group over all the groups,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum_{i=1}^{K} \sum_{x_j \in C_i} \| x_j - \mu_i \|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(K\) is the total number of clusters, \(C_i\) represents the set of samples
    in the \(i^{th}\) cluster, \(x_j\) represents a data sample in cluster, \(C_i\),
    \(mu_i\) is the prototype of cluster \(C_i\),\(\| x_j - \mu_i \|^2\) is the squared
    Euclidean distance between sample \(x_j\) and the cluster prototype \(\mu_i \).
    The samples and prototypes and distance calculations in mD space, with \(1,\ldots,m\)
    features.
  prefs: []
  type: TYPE_NORMAL
- en: by minimizing inertia k-means clusters minimizes difference within groups while
    maximizing difference between groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Joint Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probability that
    considers more than one event occurring together, the probability of \(A\) and
    \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: or the probability of \(A\), \(B\) and \(C\) is represented as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B \cap C) = P(A,B,C) \]
  prefs: []
  type: TYPE_NORMAL
- en: under the assumption of independence of \(A\), \(B\) and \(C\) the joint probability
    may be calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B,C) = P(A) \cdot P(B) \cdot P(C) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**K Bins Discretization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): bin
    the range of the feature into K bins, then for each sample assignment of a value
    of 1 if the sample is within a bin and 0 if outsize the bin'
  prefs: []
  type: TYPE_NORMAL
- en: binning strategies include uniform width bins (uniform) and uniform number of
    data in each bin (quantile)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also known as one hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods that require K bins discretization,
  prefs: []
  type: TYPE_NORMAL
- en: basis expansion to work in a higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discretization of continuous features to categorical features for categorical
    methods such as naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: histogram construction and Chi-square test for difference in distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mutual information binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-fold Cross Validation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): partitioning the
    data into K folds, and looping over the folds training the model with reminder
    of the data and testing the model with the data in the fold. Then aggregating
    the testing accuracy over all the folds.'
  prefs: []
  type: TYPE_NORMAL
- en: the train and test data split is based on K, for example, K = 4, is 25% testing
    for each fold and K = 5, is 20% testing for each fold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is an improvement over cross validation that only applies one train and
    test split to build a single model. The K-fold approach allows testing of all
    data and the aggregation of accuracy over all the folds tends to smooth the accuracy
    vs. hyperparameter plot for more reliable hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-fold cross validation may be applied to check model performance for estimation
    accuracy (most common) and uncertainty model goodness ([Maldonado-Cruz and Pyrcz,
    2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-Means Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: an unsupervised machine learning method for partitional clustering,
    group assignment to unlabeled data, where dissimilarity within clustered groups
    is mini minimized. The loss function that is minimized is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is a summary of import aspects for k-means clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '*k* - is given as a model hyperparameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exhaustive and mutually exclusive groups* - all data assigned to a single
    group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*prototype method* - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iterative solution* - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*unsupervised learning* - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*feature weighting* - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-Nearest Neighbours**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): a simple,
    interpretable and flexible, nonparametric predictive machine learning model based
    on a local weighting window applied to \(k\) nearest training data'
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbours approach is similar to a convolution approach for spatial
    interpolation. Convolution is the integral product of two functions, after one
    is reversed and shifted by \(\Delta\).
  prefs: []
  type: TYPE_NORMAL
- en: one interpretation is smoothing a function with weighting function, \(ùëì(\Delta)\),
    is applied to calculate the weighted average of function, \(ùëî(x)\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: this easily extends into multidimensional
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \,
    d\Delta_x \, d\Delta_y \, d\Delta_z \]
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity.
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for k-nearest neighbours the use of \(k\) results in a locally adaptive window
    size, different from standard convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbours is an instance-based, lazy learning method, the model training
    is postponed until prediction is required, no precalculation of the model. i.e.,
    prediction requires access to the data.
  prefs: []
  type: TYPE_NORMAL
- en: to make new predictions that training data must be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameters include,
  prefs: []
  type: TYPE_NORMAL
- en: '*k number of nearest data* to utilize for prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*data weighting*, for example uniform weighting with the local training data
    average, or inverse distance weighting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, for the case of inverse distance weighting, the method is analogous to
    inverse distance weighted interpolation with a maximum number of local data constraint
    commonly applied for spatial interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: inverse distance is available in GeostatsPy for spatial mapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too find the k-nearest data a distance metric is needed,
  prefs: []
  type: TYPE_NORMAL
- en: training data within the predictor feature space are ranked by distance (closest
    to farthest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a variety of distance metrics may be applied, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidian distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \begin{equation}
  prefs: []
  type: TYPE_NORMAL
- en: d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)^2}
    \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Minkowski Distance - a general expression for distance with well-known Manhattan
    and Euclidean distances are special cases,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p
    \right)^{\frac{1}{p}} \]
  prefs: []
  type: TYPE_NORMAL
- en: when \(p=2\), this becomes the Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when \(p=1\) it becomes the Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel Trick** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): we
    can incorporate our basis expansion in our method without ever needing to transform
    the training data to this higher dimensional space,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x) \]
  prefs: []
  type: TYPE_NORMAL
- en: We only need the inner product over the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle \]
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the actual values in the transformed space, we just need the ‚Äòsimilarity‚Äô
    between all available training data in that transformed space!
  prefs: []
  type: TYPE_NORMAL
- en: we training our support vector machines with only a similarity matrix between
    training data that will be projected to the higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we never actually need to calculate the training data values in the higher dimensional
    space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kriging**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: spatial estimation approach that relies on linear weights
    that account for spatial continuity, data closeness and redundancy. The kriging
    estimate is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ z^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot z(\bf{u}_{\alpha})
    + \left( 1.0 - \sum_{\alpha=1}^n \lambda_{\alpha} \right) \cdot m_z \]
  prefs: []
  type: TYPE_NORMAL
- en: the right term is the unbiasedness constraint, one minus the sum of the weights
    is applied to the global mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case where the trend, \(t(\bf{u})\), is removed, we now have a residual,
    \(y(\bf{u})\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ y(\bf{u}) = z(\bf{u}) - t(\bf{u}) \]
  prefs: []
  type: TYPE_NORMAL
- en: the residual mean is zero so we can simplify our kriging estimate as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot y(\bf{u}_{\alpha})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The simple kriging weights are calculated by solving a linear system of equations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^n \lambda_j C(\bf{u}_i,\bf{u}_j) = C(\bf{u},\bf{u}_i), \quad i=1,\ldots,n
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that may be represented with matrix notation as,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} C(\bf{u}_1,\bf{u}_1) & C(\bf{u}_1,\bf{u}_2)
    & \dots & C(\bf{u}_1,\bf{u}_n) \\ C(\bf{u}_2,\bf{u}_1) & C(\bf{u}_2,\bf{u}_2)
    & \dots & C(\bf{u}_2,\bf{u}_n) \\ \vdots & \vdots & \ddots & \vdots \\ C(\bf{u}_n,\bf{u}_1)
    & C(\bf{u}_n,\bf{u}_2) & \dots & C(\bf{u}_n,\bf{u}_n) \\ \end{bmatrix} \cdot \begin{bmatrix}
    \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \\ \end{bmatrix} = \begin{bmatrix}
    C(\bf{u}_1,\bf{u}) \\ C(\bf{u}_2,\bf{u}) \\ \vdots \\ C(\bf{u}_n,\bf{u}) \\ \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This system may be derived by substituting the equation for kriging estimates
    into the equation for estimation variance, and then setting the partial derivative
    with respect to the weights to zero.
  prefs: []
  type: TYPE_NORMAL
- en: we are optimizing the weights to minimize the estimation variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this system integrates the,
  prefs: []
  type: TYPE_NORMAL
- en: '*spatial continuity* as quantified by the variogram (and covariance function
    to calculate the covariance, \(C\), values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*redundancy* the degree of spatial continuity between all of the available
    data with themselves, \(C(\bf{u}_i,\bf{u}_j)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*closeness* the degree of spatial continuity between the available data and
    the estimation location, \(C(\bf{u}_i,\bf{u})\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kriging provides a measure of estimation accuracy known as kriging variance
    (a specific case of estimation variance).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^{2}_{E}(\bf{u}) = C(0) - \sum^{n}_{\alpha = 1} \lambda_{\alpha} C(\bf{u}_0
    - \bf{u}_{\alpha}) \]
  prefs: []
  type: TYPE_NORMAL
- en: Kriging estimates are best in that they minimize the above estimation variance.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of kriging estimates include,
  prefs: []
  type: TYPE_NORMAL
- en: '*Exact interpolator* - kriging estimates with the data values at the data locations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kriging variance* - a measure of uncertainty in a kriging estimate. Can be
    calculated before getting the sample information, as the kriging estimation variance
    is not dependent on the values of the data nor the kriging estimate, i.e. the
    kriging estimator is homoscedastic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spatial context* - kriging takes integrates spatial continuity, closeness
    and redundancy; therefore, kriging accounts for the configuration of the data
    and structural continuity of the feature being estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scale* - kriging by default assumes the estimate and data are at the same
    point support, i.e., mathematically represented as points in space with zero volume.
    Kriging may be generalized to account for the support volume of the data and estimate,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multivariate* - kriging may be generalized to account for multiple secondary
    data in the spatial estimate with the cokriging system. We will cover this later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smoothing effect* - of kriging can be forecasted as the missing variance.
    The missing variance over local estimates is the kriging variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kriging-based Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kriging-based declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: calculate and model the experimental variogram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply kriging to calculate estimates over a high-resolution grid covering the
    area of interest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the sum of the weights assigned to each data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: assign data weights proportional to this sum of weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The weights are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = n \cdot \frac{\sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_j}{\sum_{i=1}^n
    \left[ \sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_{j,ix,iy} \right]} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(nx\) and \(ny\) are the number of cells in the grid, \(n\) is the number
    of data, and \(\lambda_{j,ix,iy}\) is the weight assigned to the \(j\) data at
    the \(ix,iy\) grid cell.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an important point for kriging-based declustering,
  prefs: []
  type: TYPE_NORMAL
- en: like polygonal declustering, kriging-based declustering is sensitive to the
    boundaries of the area of interest; therefore, the weights assigned to the data
    near the boundary of the area of interest may change radically as the area of
    interest is expanded or contracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, kriging-based declustering integrates the spatial continuity model from
    variogram model. Consider the following possible impacts of the variogram model
    on the declustering weights,
  prefs: []
  type: TYPE_NORMAL
- en: if there is 100% relative nugget effect, there is no spatial continuity and
    therefore, all data receives equal weight. Note for the equation above this results
    in a divide by 0.0 error that must be checked for in the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: geometric anisotropy may significantly impact the weights as data aligned over
    specific azimuths are assessed as closer or further in terms of covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kolmogorov‚Äôs 3 Probability Axioms**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probability Concepts: these are Kolmogorov‚Äôs 3 axioms for valid probabilities,'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of an event is a non-negative number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(ùê¥) \ge 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Probability of the entire sample space, all possible outcomes, \(\Omega\), is
    one (unity), also known as probability closure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: Additivity of mutually exclusive events for unions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: e.g., probability of \(A_1\) and \(A_2\) mutual exclusive events is, \(P(A_1
    + A_2) = P(A_1) + P(A_2)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^1\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): known as Manhattan
    norm or sum of absolute residual (SAR),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n |\Delta y_i | \]
  prefs: []
  type: TYPE_NORMAL
- en: also expressed as the mean absolute error (MAE),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n |\Delta y_i | \]
  prefs: []
  type: TYPE_NORMAL
- en: Minimization with \(L^1\) norm is known as minimum absolute difference.
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^2\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): known as sum of
    square residual (SSR),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \sqrt{\Delta y_i} \]
  prefs: []
  type: TYPE_NORMAL
- en: also expressed as the mean square error (MSE),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n \left( \Delta y_i \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the Euclidian norm,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sqrt{ \sum_{i=1}^n \sqrt{\Delta y_i} } \]
  prefs: []
  type: TYPE_NORMAL
- en: Minimization with \(L^2\) norm is known as the method of least squares.
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^1\) vs. \(L^2\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): the choice of \(L^1\)
    and \(L^2\) norm is important in machine learning. To explain this let‚Äôs compare
    the performance of \(L^1\) and \(L^2\) norms in loss functions while training
    model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Least Absolute Deviations (L1) | Least Squares (L2) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness* | Robust | Not very robust |'
  prefs: []
  type: TYPE_TB
- en: '| Solution Stability | Unstable solution | Stable solution |'
  prefs: []
  type: TYPE_TB
- en: '| Number of Solutions | Possibly multiple solutions | Always one solution |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Selection | Built-in feature selection | No feature selection |'
  prefs: []
  type: TYPE_TB
- en: '| Output Sparsity | Sparse outputs | Non-sparse outputs |'
  prefs: []
  type: TYPE_TB
- en: '| Analytical Solutions | No analytical solutions | Analytical solutions |'
  prefs: []
  type: TYPE_TB
- en: Here‚Äôs some important points,
  prefs: []
  type: TYPE_NORMAL
- en: '*robust* - resistant to outliers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*unstable* - for small changes in training the trained model predictions may
    jump'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multiple solutions* - different solution have similar or the same loss, resulting
    in solutions jumping with small changes to the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*output sparsity* and *feature selection* - model parameters tend to 0.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*analytical solutions* - an analytical solution is available to solve for the
    optimum model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**\(L^1\) or \(L^2\) Normalizer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): is
    performed across features over individual samples to constrain the sum'
  prefs: []
  type: TYPE_NORMAL
- en: The L1 Norm has the following constraint across samples,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\alpha = 1}^m x^{\prime}_{i,\alpha} = 1.0, \quad i = 1, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: The L1 normalizer transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[ x^{\prime}_{i,\alpha} = \frac{x_{i,\alpha}}{\sum_{\alpha=1}^m x_{i,\alpha}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The L2 Norm has the following constraint across samples,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\alpha = 1}^m \left( x^{\prime}_{i,\alpha} \right)^2 = 1.0, \quad i
    = 1, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: The L2 normalizer transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[ x^{\prime}_{i,\alpha} = \sqrt{\frac{(x_{i,\alpha})^2}{\sum_{\alpha=1}^m (x_{i,\alpha})^2}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Example, applied in text classification and clustering, and L1 for compositional
    data (sum 1.0 constraint)
  prefs: []
  type: TYPE_NORMAL
- en: '**LASSO Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): linear regression
    with \(L^1\) regularization term and regularization hyperparameter \(\lambda\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: As a result, LASSO regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The only difference between LASSO and ridge regression is:'
  prefs: []
  type: TYPE_NORMAL
- en: for LASSO the shrinkage term is posed as an \(\ell_1\) penalty,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda \sum_{\alpha=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: for ridge regression the shrinkage term is posed as an \(\ell_2\) penalty,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'While both ridge regression and the LASSO shrink the model parameters (\(b_{\alpha},
    \alpha = 1,\ldots,m\)) towards zero:'
  prefs: []
  type: TYPE_NORMAL
- en: LASSO parameters reach zero at different rates for each predictor feature as
    the lambda, \(\lambda\), hyperparameter increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a result LASSO provides a method for feature ranking and selection!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lambda, \(\lambda\), hyperparameter controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the prediction model approaches linear regression,
    there is lower model bias, but the model variance is higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the coefficients all become 0.0 and the model
    is the training data response feature mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lazy Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): model is
    a generalization of the training data and calculation is delayed until query is
    made of the model'
  prefs: []
  type: TYPE_NORMAL
- en: the model is the training data and selected hyperparameters, to make new predictions
    the training data must be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opposite is eager learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate** (gradient boosting)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): controls the rate
    of updating with each new model.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_m = f_{m-1} - \rho_m \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial
    F(X_\alpha)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\rho_m\) is the learning rate, \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial
    F(X_\alpha)} is the gradient, error, \(f_{m-1}\) is the previous estimate, and
    \(f_m\) is the new estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Some salient points about learning rate,
  prefs: []
  type: TYPE_NORMAL
- en: without learning rate, the boosting models learn too quickly and will have too
    high model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: slow down learning for a more robust model, balanced to ensure good performance,
    too small rate will require very large number of models to reach convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Likewise Deletion** (MRMR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): removal of any sample
    with any missing feature values'
  prefs: []
  type: TYPE_NORMAL
- en: if missing feature values are not missing at random (MAR) this may impart a
    bias in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: will result in a decrease in the effective data size and increase in model uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): a linear, parametric
    prediction model,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n (\Delta y_i)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '*Error-free* - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linearity* - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Constant Variance* - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Independence of Error* - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*No multicollinearity* - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location Map**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Loading and Plotting Data and Models](MachineLearning_plotting_data_models.html):
    a data plot where the 2 axes are locations, e.g., \(X\) and \(Y\), Easting and
    Northing, Latitude and Longitude, etc., to show the locations and magnitudes of
    the spatial data.'
  prefs: []
  type: TYPE_NORMAL
- en: often the data points are colored to represent the scale of feature to visualize
    the sampled feature over the area or volume of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: advantage, visualize the data without any model that may bias our impression
    of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disadvantage, may be difficult to visualize large datasets and data in 3D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss Function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): the equation that
    is minimized to train the model parameters. For example, the loss function for
    linear regression includes residual sum of square, the \(L^2\) error norm,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0)
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for LASSO regression the loss function includes residual sum of square, the
    \(L^2\) error norm, plus a \(L^1\) regularization term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: for k-means clustering the loss function is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum^k_{i=1} \sum_{\alpha \in C_i} \sqrt{ \sum_{j = 1}^m X_{\alpha,m}
    - \mu_{i,m} } \]
  prefs: []
  type: TYPE_NORMAL
- en: The method to minimize loss functions depends on the type of norm,
  prefs: []
  type: TYPE_NORMAL
- en: with \(L^2\) norms we apply differentiation to the loss function with respect
    to the model parameter and set it equal to zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with \(L^1\) norms in our loss functions we lose access to an analytical solution
    and use iterative optimization, e.g., steepest descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine Learning Workflow Design**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: is based on the following
    steps,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Specify the Goals* - for example,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: build a numerical model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: evaluate different recovery processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specify the Data* - what is available and what is missing?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Design a Set of Steps to Accomplish the Goal* - common steps include,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: load data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: format, check and clean data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: run operation, including, statistical calculation, model or visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: transfer function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Develop Documentation* - including implementation details, defense of decisions,
    metadata, limitations and future work'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Flow* - data and information flow, learning while modeling with branches and
    loop backs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Uncertainty* - summarize all uncertainty sources, include methods to integrate
    uncertainty, defend the uncertainty models and aspects deemed certain'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Margin** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): when
    the training data include overlapping categories it would not be possible, nor
    desirable, to develop a decision boundary that perfectly separates the categories
    for which this condition would hold,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: We need a model that allows for some misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We introduce the concept of a margin, \(ùëÄ\), and a distance from the margin
    (error, ùúâ_ùëñ).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N
    \xi_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: The loss function includes the margin term, \(M\), and hence attempts to minimize
    margin while minimizing classification error weighted by hyperparameter, \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginal Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probability that
    considers only one event occurring, the probability of \(A\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities may be calculated from joint probabilities through the
    process of marginalization,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) = \int_{-\infty}^{\infty} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: where we integrate over all cases of the other event, \(B\), to remove its influence.
    Given discrete possible cases of event \(B\) we can simply sum the probabilities
    over all possible cases of \(B\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) = \sum_{i=1}^{k_B} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix Scatter Plots**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): composite
    plot including the combinatorial of all pair-wise scatter plots for all features.'
  prefs: []
  type: TYPE_NORMAL
- en: given \(m\) features, there are \(m \times m\) scatter plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the scatter plots are ordered, y-axis feature from \(X_1,\ldots,X_m\) over the
    rows and x-axis feature from \(X_1,\ldots,X_m\) over the columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the diagonal is the features plotted with themselves and are often replaced
    with feature histograms or probability density functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use matrix scatter plots to,
  prefs: []
  type: TYPE_NORMAL
- en: look for bivariate linear or nonlinear structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: look for bivariate homoscedasticity (constant conditional variance) and heteroscedasticity
    (conditional variance changes with value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: look for bivariate constraints, such as sum constraints with compositional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, the other features are marginalized, this is not a full m-D visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum Relevance Minimum Redundancy** (MRMR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a mutual information-based
    approach for feature ranking that accounts for feature relevance and redundancy.'
  prefs: []
  type: TYPE_NORMAL
- en: one example is a relevance minus redundancy summary,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MRMR = max \left[ frac{1}{|S|} \sum_{X_i \in S} I(X_i,Y) - \frac{1}{|S|^2}
    \sum_{X_i \in S} \sum_{X_j, i \ne j} I(X_i,X_j) \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùëÜ\) is the predictor feature subset and \(|ùëÜ|\) is the number of features
    in the subset \(ùëÜ\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Metropolis-Hastings MCMC Sampler**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    The basic steps of the Metropolis-Hastings MCMC Sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\ell = 1, \ldots, L\):'
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for the initial sample of model parameters, \(\beta(\ell
    = 1) = b_1(\ell = 1)\), \(b_0(\ell = 1)\) and \(\sigma^2(\ell = 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propose new model parameters based on a proposal function, \(\beta^{\prime}
    = b_1\), \(b_0\) and \(\sigma^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate probability of acceptance of the new proposal, as the ratio of the
    posterior probability of the new model parameters given the data to the previous
    model parameters given the data multiplied by the probability of the old step
    given the new step divided by the probability of the new step given the old.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X)
    }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply Monte Carlo simulation to conditionally accept the proposal, if accepted,
    \(\ell = \ell + 1\), and sample \(\beta(\ell) = \beta^{\prime}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minkowski Distance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): a general
    expression for distance with well-known Manhattan and Euclidean distances are
    special cases,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p
    \right)^{\frac{1}{p}} \]
  prefs: []
  type: TYPE_NORMAL
- en: when \(p=2\), this becomes the Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when \(p=1\) it becomes the Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing Feature Values**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): null values
    in the data table, samples that do not have values for all features'
  prefs: []
  type: TYPE_NORMAL
- en: There are many causes of missing feature values, for example,
  prefs: []
  type: TYPE_NORMAL
- en: sampling cost, e.g., low permeability test takes too long
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rock rheology sample filter, e.g., can‚Äôt recover the mudstone samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sampling to reduce uncertainty and maximize profitability instead of statistical
    representativity, dual purpose samples for information and production
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing data consequences, more than reducing the amount of training and testing
    data, missing data, if not completely at random may result in,
  prefs: []
  type: TYPE_NORMAL
- en: biased sample statistics resulting in biased model training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: biased models with biased predictions with potentially no indication of the
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing at Random** (MAR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): missing feature
    values are distributed randomly, uniform coverage over the predictor feature space,
    i.e., all values have likelihood to be missing, and no correlation between missing
    feature values.'
  prefs: []
  type: TYPE_NORMAL
- en: This is typically not the case as missing data often has a confounding feature,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: sampling cost, e.g., low permeability test takes too long
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rock rheology sample filter, e.g., can‚Äôt recover the mudstone samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sampling to reduce uncertainty and maximize profitability instead of statistical
    representativity, dual purpose samples for information and production
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing data consequences, more than reducing the amount of training and testing
    data, missing data, if not completely at random may result in,
  prefs: []
  type: TYPE_NORMAL
- en: biased sample statistics resulting in biased model training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: biased models with biased predictions with potentially no indication of the
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Bias**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    insufficient complexity and flexibility to fit the natural setting'
  prefs: []
  type: TYPE_NORMAL
- en: increasing model complexity usually results in decreasing model bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model bias variance trade-off* - as complexity increases, model variance increases
    and model bias decreases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0)
    \right)^2\) is model bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-Bias Variance Trade-off**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): as complexity increases,
    model variance increases and model bias decreases.'
  prefs: []
  type: TYPE_NORMAL
- en: as model variance and model bias are both components of expected test square
    error, the balancing of model bias and model variance results in an optimum level
    of complexity to minimize the testing error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Checking**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is a critical last
    step for any spatial modeling workflow. Here are the critical aspects of model
    checking,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model Inputs* - data and statistics integration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: check the model to ensure the model inputs are honored in the models, generally
    checked over all the realizations, for example, the output histograms and matches
    the input histogram over the realizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accurate Spatial Estimates* - ability of the model to accurately predict away
    from the available sample data, over a variety of configurations, with accuracy'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: by cross validation, withholding some of the data, check the model‚Äôs ability
    to predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generally, summarized with a truth vs. predicted cross plot and measures such
    as mean square error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE = \frac{1}{n} \sum_{\alpha = 1}^{n} \left(z^{*}(\bf{u}_{\alpha}) - z(\bf{u}_{\alpha})
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Accurate and Precise Uncertainty Models* - uncertainty model is fair given
    the amount of information available and various sources of uncertainty'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: also checked through cross validation, withholding some of the data, but by
    checking the proportion of the data in specific probability intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarized with a proportion of withheld data in interval vs. the probability
    interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points on the 45 degree line indicate accurate and precise uncertainty model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points above the 45 degree line indicate accurate and imprecise uncertainty
    model, uncertainty is too wide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points below the 45 degree line indicate inaccurate uncertainty model, uncertainty
    is too narrow or model is biased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Complexity or Flexibility**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the ability of
    a model to fit to data and to be interpreted.'
  prefs: []
  type: TYPE_NORMAL
- en: A variety of concepts may be used to describe model complexity,
  prefs: []
  type: TYPE_NORMAL
- en: the number of features, predictor variables are in the model, dimensionality
    of the model, usually resulting in more model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of parameters, the order applied for each term, e.g. linear, quadratic,
    thresholds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the format of the model, i.e., a compact equation with polynomial regression
    vs. nested conditional statements with decision tree vs. thousands of weights
    and bias model parameters for a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, more complexity with a high order polynomial, larger decision trees
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, more complicated or flexible models are more difficult to interpret,
  prefs: []
  type: TYPE_NORMAL
- en: linear regression and the associated model parameters can be analyzed and even
    applied for feature ranking, while support vector machines with radial basis functions
    are a linear model in the nD high dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Generalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the ability of
    a model to predict away from training data.'
  prefs: []
  type: TYPE_NORMAL
- en: the model learns the structure in the data and does not just memorize the training
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models that do not generalize well,
  prefs: []
  type: TYPE_NORMAL
- en: overfit models have high accuracy at training data and low accuracy away from
    training data, demonstrated with low testing accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: underfit models are too simple or inflexible for the natural phenomenon and
    have low training and testing accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Hyperparameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): constrain the model
    complexity. Hyperparameters are tuned to maximize accuracy with the withheld testing
    data to prevent model overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: For a set of polynomial models from \(4^{th}\) to \(1^{st}\) order,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_4 \cdot x^4 + b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]\[
    y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]\[ y = b_2 \cdot x^2 +
    b_1 \cdot x + b_0 \]\[ y = b_1 \cdot x + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: the choice of polynomial order is the hyperparameter, i.e., the first order
    model is most simple and the fourth order model is most complicated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Parameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): trainable coefficients
    for a machine learning model that control the fit to the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: For a polynomial model,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: \(b_3\), \(b_2\), \(b_1\), and \(b_0\) are model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '*training model parameters* - model parameters are calculated by optimization
    to minimize error and regularization terms over the training data through analytical
    solution or iterative solution, e.g., gradient descent optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ridge Regression](MachineLearning_ridge_regression.html): adding information
    to prevent overfit (or underfit), improve model generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: this information is known as a regularization term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this represents a penalty for complexity that is tuned with a regularization
    hyperparameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the ridge regression loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda \sum_{j=1}^m b_{\alpha}^2\) is the regularization term and \(\lambda\)
    is the regularization hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of regularization is quite general and choices in machine learning
    architecture, such as,
  prefs: []
  type: TYPE_NORMAL
- en: use of receptive fields for convolutional neural networks (CNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the choice to limit decision trees to a maximum number of levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a couple of useful perspectives on model regularization,
  prefs: []
  type: TYPE_NORMAL
- en: '*Occam‚Äôs razor* - regularization tunes model complexity to the simplest effective
    solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian perspective* - regularization is imposing a prior on the solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Variance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    sensitivity to the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: increasing model complexity usually results in increasing model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ensemble machine learning, for example, model bagging reduce model variance
    by averaging over multiple estimators trained on bootstrap realizations of the
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model bias variance trade-off* - as complexity increases, model variance increases
    and model bias decreases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[
    \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right]\) is model variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum** (optimization)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): update the previous
    step with the new step, momentum, \(\lambda\), is the weight applied to the previous
    step while \(1 - \lambda\) is the weight applied to the current step,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \left( r \cdot \nabla L \right)_{t-1} \right)^m = \lambda \cdot r
    \cdot \nabla L_{t-2} + (1 - \lambda) \cdot r \cdot \nabla L_{t-1} \]
  prefs: []
  type: TYPE_NORMAL
- en: the gradients calculated from the partial derivatives of the loss function for
    each model parameter have noise. Momentum smooths out, reduces the impact of this
    noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: momentum helps the solution proceed down the general slope of the loss function,
    rather than oscillating in local ravines or dimples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markov Chain Monte Carlo** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Markov* - screening assumption, the next sample is only dependent on the previous
    sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chain* - the samples form a sequence often demonstrating a transition from
    burn-in chain with inaccurate statistics and equilibrium chain with accurate statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monte Carlo* - use of Monte Carlo simulation, random sampling from a statistical
    distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is this useful?
  prefs: []
  type: TYPE_NORMAL
- en: we often don‚Äôt have the target distribution, it is unknown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we can sample with the correct frequencies with other form of information
    such as conditional probability density functions, Gibbs sampler, or the likelihood
    ratios of the candidate next sample and the current sample, Metropolis-Hastings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metropolis-Hastings Sampling** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics, based on,'
  prefs: []
  type: TYPE_NORMAL
- en: the likelihood ratios of the candidate next sample and the current sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a rejection sampler based on this likelihood ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since only the ratio of likelihood is required, the system is simplified as
    the evidence term cancels out from the Bayesian probability
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the basic steps of the Metropolis-Hastings MCMC Sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\ell = 1, \ldots, L\):'
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for the initial sample of model parameters, \(\beta(\ell
    = 1) = b_1(\ell = 1)\), \(b_0(\ell = 1)\) and \(\sigma^2(\ell = 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propose new model parameters based on a proposal function, \(\beta^{\prime}
    = b_1\), \(b_0\) and \(\sigma^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate probability of acceptance of the new proposal, as the ratio of the
    posterior probability of the new model parameters given the data to the previous
    model parameters given the data multiplied by the probability of the old step
    given the new step divided by the probability of the new step given the old.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X)
    }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply Monte Carlo simulation to conditionally accept the proposal, if accepted,
    \(\ell = \ell + 1\), and sample \(\beta(\ell) = \beta^{\prime}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Monte Carlo Simulation (MCS)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a random sample from a statistical distribution, random variable. The steps for
    MCS are:'
  prefs: []
  type: TYPE_NORMAL
- en: model the feature cumulative distribution function, \(F_x(x)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: draw random value from a uniform [0,1] distribution, this is a random cumulative
    probability value, known as a p-value, \(p^{\ell}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply the inverse of the cumulative distribution function to calculate the associated
    realization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x^{\ell} = F_x^{-1} (p^{\ell}) \]
  prefs: []
  type: TYPE_NORMAL
- en: repeat to calculate enough realizations for the subsequent analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monte Carlo simulation is the basic building block of stochastic simulation
    workflows, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Monte Carlo simulation workflows* - apply Monte Carlo simulation many over
    all features to the transfer function to calculate a realization of the decision
    criteria, repeated for many realizations, to propagate uncertainty through a transfer
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bootstrap* - applies Monte Carlo simulation to acquire realizations of the
    data to calculate uncertainty in sample statistics or ensembles of prediction
    models for ensemble-based machine learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monte Carlo methods* - applies Monte Carlo simulation to speed up an expensive
    calculation with a limited random sample that converges on the solution as the
    number of random samples increases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo Simulation Workflow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a convenient stochastic workflow for propagating uncertainty through a transfer
    function through sampling with Monte Carlo Simulation (MCS). The workflow includes
    the following steps,'
  prefs: []
  type: TYPE_NORMAL
- en: Model all the input features‚Äô distributions, cumulative distribution functions,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_{x_1}(x_1), \quad F_{x_2}(x_2), \quad \dots \quad , F_{x_m}(x_m) \]
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo simulate a realizations for all the inputs,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x_1^{\ell}, \quad x_2^{\ell}, \quad \ldots \quad , x_m^{\ell} \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply to the transfer function to get a realization of the transfer function
    output, often the *decision criteria*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ y^{\ell} = f \left(x_1^{\ell},x_2^{\ell}, \quad \ldots \quad, x_m^{\ell}
    \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 1-3 to calculate enough realizations to model the transfer function
    output distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_y(y) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplication Rule** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): we can calculate
    the joint probability of \(A\) and \(B\) as the product of the conditional probability
    of \(B\) given \(A\) with the marginal probability of \(A\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: The multiplication rule is derived as a simple manipulation of the definition
    of conditional probability, in this case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutual Information**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a generalized approach
    that quantifies the mutual dependence between two features.'
  prefs: []
  type: TYPE_NORMAL
- en: quantifies the amount of information gained from observing one feature about
    the other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoids any assumption about the form of the relationship (e.g. no assumption
    of linear relationship)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: units are Shannons or bits
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compares the joint probabilities to the product of the marginal probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarizes the difference between the joint \(P(x,y)\) and the product of the
    marginals \(P(x)\cdot P(y)\), integrated over all \(x \in ùëã\) and \(y \in Y\),
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For discrete or binned continuous features \(X\) and \(Y\), mutual information
    is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'recall, given independence between \(X\) and \(Y\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) \]
  prefs: []
  type: TYPE_NORMAL
- en: therefore if the two features are independent then the \(log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: The joint probability \(P_{X,Y}(x,y)\) is a weighting term on the sum and enforces
    closure.
  prefs: []
  type: TYPE_NORMAL
- en: parts of the joint distribution with greater density have greater impact on
    the mutual information metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For continuous (and nonbinned) features we can applied the integral form.
  prefs: []
  type: TYPE_NORMAL
- en: \[ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) dx dy \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutually Exclusive Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the events do not
    intersect, i.e., do not have any common outcomes. We represent this as,'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability for mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Multidimensional Scaling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multidimensional Scaling](MachineLearning_multidimensional_scaling.html):
    a method in inferential statistics / information visualization for exploring /
    visualizing the similarity (conversely the difference) between individual samples
    from a high dimensional dataset in a low dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional scaling (MDS) projects the \(m\) dimensional data to \(p\)
    dimensions such that \(p << m\).
  prefs: []
  type: TYPE_NORMAL
- en: while attempting to preserve the pairwise dissimilarity between the data samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ideally we are able to project to \(p=2\) to easily explore the relationships
    between the samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While principal component analysis (PCA) operates with the covariance matrix,
    multidimensional scaling operates with the distance or dissimilarity matrix. For
    multidimensional scaling,
  prefs: []
  type: TYPE_NORMAL
- en: you don‚Äôt need to know the actual feature values, just the distance or dissimilarity
    between the samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as with any distance in feature space, we consider feature standardization to
    ensure that features with larger variance do not dominate the calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we may work with a variety of dissimilarity measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison between multidimensional scaling and principal component analysis,
  prefs: []
  type: TYPE_NORMAL
- en: principal component analysis takes the covariance matrix (\(m \times m\)) between
    all the features and finds the linear, orthogonal rotation such that the *variance
    is maximized* over the ordered principle components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multidimensional scaling takes the matrix of the pairwise distances (\(n \times
    n\)) between all the samples in feature space and finds the nonlinear projection
    such that the *error in the pairwise distances is minimized*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some have suggest that visualizing data or models in a multidimensional scaling
    space is visualizing the space of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): the application of the assumption
    of conditional independence to simplify the classification prediction problem
    from the perspective of Bayesian updating, based on the conditional probability
    of a category, \(k\), given \(n\) features, \(x_1, \dots , x_n\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4,
    \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood, conditional probability with the joint conditional is difficult,
    likely impossible to calculate. It requires information about the joint relationship
    between \(x_1, \dots , x_n\) features. As \(n\) increases this requires a lot
    of data to inform the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: With the naive Bayes approach we make the ‚Äònaive‚Äô assumption that the features
    are all *conditionally independent**. This entails,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_i | x_{i+1}, \ldots , x_n, C_k) = P(x_i | C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i = 1, \ldots, n\) features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now solve for the needed conditional probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = \frac{P(C_k) \prod_{i=1}^{n} P(x_i | C_k)}{P(x_1,
    \dots , x_n)} \]
  prefs: []
  type: TYPE_NORMAL
- en: We only need the prior, \(P(C_k)\), and a set of conditionals, \(P(x_i | C_k)\),
    for all predictor features, \(i = 1,\ldots,n\) and all categories, \(k = 1,\ldots,K\).
  prefs: []
  type: TYPE_NORMAL
- en: The evidence term, \(P(x_1, \dots , x_n)\), is only based on the features \(x_1,
    \dots , x_n\); therefore, is a constant over the categories \(k = 1,\ldots,n\).
  prefs: []
  type: TYPE_NORMAL
- en: it ensures closure - probabilities over all categories sum to one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we simply standardize the numerators to sum to one over the categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The naive Bayes approach is:'
  prefs: []
  type: TYPE_NORMAL
- en: simple to understand, builds on fundamental Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: practical even with small datasets since with the conditional independence we
    only need to estimate simple conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ndarray**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: Numpy‚Äôs convenient class
    for working with grids, exhaustive, regularly spaced data over 2D or 3D, representing
    maps and models, due to,'
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate gridded data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods to load from a variety of file types, Python classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods to calculate multidimensional summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods for data queries, filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in attributes to store information about the nD array, for example, size
    and shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nonparametric Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that makes
    no assumption about the functional form, shape of the natural setting.'
  prefs: []
  type: TYPE_NORMAL
- en: learns the shape from the training data, more flexibility to fit a variety of
    shapes for natural systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: less risk that the model is a poor fit for the natural settings than with parametric
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically need a lot more data for an accurate estimate of nonparametric models,
  prefs: []
  type: TYPE_NORMAL
- en: nonparametric often have many trainable parameters, i.e., nonparametric models
    are actually parametric rich!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): norm of a vector
    maps vector values to a summary measure \([ùüé,\infty)\), that indicates size or
    length.'
  prefs: []
  type: TYPE_NORMAL
- en: To train our models to training data, we require a single summary measure of
    mismatch with the training data, training error. The error is observed at each
    training data location,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta y_i = y_i - \hat{y}_i, \quad \forall \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: as an error vector. We need a single value to summarize over all training data,
    that we can minimize!
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*) to a minimum of 0.0 and a maximum
    of 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: this is a shift and stretch / squeeze of the original property distribution
    assumes no shape change, rank preserving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y_i = \frac{x_i - min(x)}{max(x) - min(x)}, \quad \forall \quad i, \ldots,
    n \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Methods that require standardization and min/max normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering, k-nearest neighbour regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\beta\) coefficient‚Äôs for feature ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: artificial neural networks forward transform of predictor features and back
    transform of response features to improve activation function sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalized Histogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): is a representation
    of the univariate statistical distribution with a plot of probability over an
    exhaustive set of bins over the range of possible values. These are the steps
    to build a normalized histogram,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the continuous feature range of possible values into \(K\) equal size
    bins, \(\delta x\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: or use available categories for categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Count the number of samples (frequency) in each bin, \(n_k\), \(\forall k=1,\ldots,K\)
    and divide each by the total number of data, \(n\), to calculate the probability
    of each bin,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ p_k = \frac{n_k}{n}, \forall \quad k = 1,\ldots,L \]
  prefs: []
  type: TYPE_NORMAL
- en: Plot the probability vs. the bin label (use bin centroid if continuous)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note, normalized histograms are typically plotted as a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: '**One Hot Encoding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): bin
    the range of the feature into K bins, then for each sample assignment of a value
    of 1 if the sample is within a bin and 0 if outsize the bin'
  prefs: []
  type: TYPE_NORMAL
- en: binning strategies include uniform width bins (uniform) and uniform number of
    data in each bin (quantile)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also known as K bins discretization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods that require K bins discretization,
  prefs: []
  type: TYPE_NORMAL
- en: basis expansion to work in a higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discretization of continuous features to categorical features for categorical
    methods such as naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: histogram construction and Chi-square test for difference in distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mutual information binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-Bag Sample**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): with
    bootstrap resampling of the data, it can be shown that about \(\frac{2}{3}\) of
    the data will be included (in expectation). For bagging-based ensemble prediction
    models,'
  prefs: []
  type: TYPE_NORMAL
- en: therefore are \(\frac{1}{3}\) of the data (in expectation) unused in training
    each model realization, these are know as out-of-bag observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for every response feature observation, \(y_{\alpha}\), there are \(\frac{B}{3}\)
    out-of-bag predictions, \(y^{*,b}_{\alpha}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can aggregate this ensemble of prediction realizations, average for regression
    or mode for classification, to calculate a single out-of-bag prediction, \(y^{*}_{\alpha}
    = \sum_{\alpha = 1}^{\frac{B}{3}} y^{*,b}_{\alpha}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: from these single out-of-bag predictions over all data, the out-of-bag mean
    square error (MSE) is calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE_{OOB} = \sum_{\alpha = 1}^{\frac{B}{3}} \left[ y^{*}_{\alpha} - y_{\alpha}
    \right]^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: For bagging-based ensemble predictive machine learning, there is no need to
    perform training and testing splits, hyperparameter tuning can be applied with
    out-of-bag MSE.
  prefs: []
  type: TYPE_NORMAL
- en: this is equivalent to random train and test split that may not be fair, same
    difficulty as the planned use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this freezes the test proportion at about \(\frac{1}{3}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfit Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a machine learning
    model that is fit to data noise or data idiosyncrasies'
  prefs: []
  type: TYPE_NORMAL
- en: increased complexity will generally decrease error with respect to the training
    dataset but, may result in increase error with testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: over the region of model complexity with rising testing error and falling training
    error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues of an overfit machine learning model,
  prefs: []
  type: TYPE_NORMAL
- en: more model complexity and flexibility than can be justified with the available
    data, data accuracy, frequency and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: high accuracy in training, but low accuracy in testing representing real-world
    use away from training data cases, indicating poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters** (statistics)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a summary measure
    of a population'
  prefs: []
  type: TYPE_NORMAL
- en: for example, population mean, population standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We very rarely have access to actual population parameters, in general we infer
    population parameters with available sample statistics
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters** (machine learning)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): trainable coefficients
    for a machine learning model that control the fit to the training data'
  prefs: []
  type: TYPE_NORMAL
- en: model parameters are calculated by optimization to minimize error over the training
    data through, analytical solution, or iterative solution, e.g., gradient descent
    optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that makes
    an assumption about the functional form, shape of the natural system.'
  prefs: []
  type: TYPE_NORMAL
- en: we gain simplicity and advantage of only a few parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for is a linear model we only have \(m+1\) model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a risk that our model is quite different than the natural setting,
    resulting in a poor model, for example, a linear model applied to a nonlinear
    phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: '**Partial Correlation Coefficient**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a method
    to calculate the correlation between \(ùëø\) and \(ùíÄ\) after controlling for the
    influence of \(ùíÅ_ùüè,\ldots,ùíÅ_(ùíé‚àíùüê)\) other features on both \(ùëø\) and \(ùëå\). Note,
    I use \(m-2\) to account for \(X\) and \(Y\) removed.'
  prefs: []
  type: TYPE_NORMAL
- en: For \(\rho_(ùëã,ùëå.ùëç_1,‚Ä¶,ùëç_(ùëö‚àí2) )\),
  prefs: []
  type: TYPE_NORMAL
- en: perform linear, least-squares regression to predict \(ùëø\) from \(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\).
    \(ùëø\) is regressed on the predictors to calculate the estimate, \(ùëø^‚àó\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: perform linear, least-squares regression to predict \(ùíÄ\) from \(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\).
    \(ùíÄ\) is regressed on the predictors to calculate the estimate, ùíÄ^‚àó
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the residuals in Step #1, \(ùëø ‚àí ùëø^‚àó\), where \(ùëø^‚àó=ùíá(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
    linear regression model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the residuals in Step #2, \(ùíÄ ‚àí ùíÄ^‚àó\), where \(ùíÄ^‚àó=ùíá(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
    linear regression model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the correlation coefficient between the residuals from Steps #3 and
    #4, \(\rho_{ùëø ‚àíùëø^‚àó,ùíÄ ‚àí ùíÄ^‚àó}\)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assumptions of Partial Correlation, for \(ùùÜ_(ùëø,ùíÄ.ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
  prefs: []
  type: TYPE_NORMAL
- en: \(ùëø,ùíÄ,ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\) have linear relationships, i.e., all pairwise relationships
    are linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no outliers for any of the univariate distributions (univariate outliers) and
    pairwise relationships (bivariate outliers). Partial correlation is very sensitive
    to outliers like regular correlation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian distributed, univariate and pairwise bivariate distributions Gaussian
    distributed. Bivariate should be linearly related and homoscedastic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitional Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: all cluster group assignments are determined at once, as
    opposed to an agglomerative hierarchical clustering method that starts with \(n\)
    clusters and then iteratively merges clusters into larger clusters'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering is partitional clustering, while the solution heuristic to
    find the solution is iterative, the solution is actually all at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: easy to update, for example, by modifying the prototype locations and recalculating
    the group assignments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polygonal Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polygonal declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split up the area of interest with Voronoi polygons. These are constructed by
    intersected perpendicular bisectors between adjacent data points. The polygons
    group the area of interest by nearest data point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign weight to each datum proportional to the area of the associated Voronoi
    polygon
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = n \cdot \frac{A_j}{\sum_{j=1}^n} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(w(\bf{u}_j)\) is the weight for the \(j\) data. Note, the sum of the
    weights is \(n\); therefore, \(w(\bf{u}_j)\) is nominal weight of 1.0, sample
    density if the data were equally spaced over the area of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some highlights for polygonal declustering,
  prefs: []
  type: TYPE_NORMAL
- en: polygonal declustering is sensitive to the boundaries of the area of interest;
    therefore, the weights assigned to the data near the boundary of the area of interest
    may change radically as the area of interest is expanded or contracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: polygonal declustering is the same as the Theissen polygon method for calculation
    of precipitation averages developed by Afred H. Thiessen in 1911, []
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): application
    of polynomial basis expansion to the predictor features before linear regression,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{l=1}^{k} \sum_{j=1}^{m} \beta_{j,l} h_l (X_j) + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where the h transforms over training data, \(ùëñ=1,\ldots,n\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(x_i) = x_i, \quad h_2(x_i) = x_i^2, \quad h_3(x_i) = x_i^3, \quad h_4(x_i)
    = x_i^4, \dots, h_k(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: up to the specified order \(ùëò\).
  prefs: []
  type: TYPE_NORMAL
- en: For example, with a single predictor feature, \(ùëö = 1\), up to the \(4^{th}\)
    order,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1} X + \beta_{1,2} X^2 + \beta_{1,3} X^3 + \beta_{1,4} X^4 +
    \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: After the \(ùíâ_ùíç\), \(ùëô=1,\ldots,ùëò\) transforms, over the \(ùëó=1,\dots,ùëö\) predictor
    features we have the same linear equation and the ability to utilize the previously
    discussed analytical solution.
  prefs: []
  type: TYPE_NORMAL
- en: we are assuming linearity after application of our basis expansion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now the model parameters, \(\beta_(ùíç,ùíä)\), relate to a transformed version of
    the initial predictor feature, \(ùíâ_ùíç (ùëø_ùíã)\).
  prefs: []
  type: TYPE_NORMAL
- en: we lose the ability to interpret the coefficients, for example, what is ùëùùëíùëüùëöùëíùëéùëèùëñùëôùëñùë°ùë¶\(^4\)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generally, significantly higher model variance, i.e., may have unstable interpolation
    and especially extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression model assumptions,
  prefs: []
  type: TYPE_NORMAL
- en: '*error-free* - predictor features basis expansions are error free, not random
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*constant variance* - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*linearity* - response is linear combination of basis features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*polynomial* - relationships between ùëã and Y is polynomial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*independence of error* - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no multicollinearity* - none of the basis feature expansions are linearly redundant
    with other features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Population**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): exhaustive, finite
    list of property of interest over area of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: for example, exhaustive set of porosity measures at every location within a
    reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, the entire population is not generally accessible and we use a limited
    sample to make inference concerning the population
  prefs: []
  type: TYPE_NORMAL
- en: '**Power Law Average**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    general form for averaging based scale up, aggregation of smaller scale measures
    in a larger volume into a single value representative of the larger volume'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{x}_p = \left(\frac{1}{n}\sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: useful to calculate effective permeability where flow is not parallel nor perpendicular
    to distinct permeability layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flow simulation may be applied to calibrate (calculate the appropriate power
    for power law averaging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the ratio of true positives divided by all positives, true positives + false
    positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Precision_k = \frac{ n_{k,\text{true positives}} }{ n_{k,\text{true positives}}
    + n_{k,\text{false positives}}} = \frac{ n_{k,\text{true positives}} }{ n_{k,
    \text{all positives}} } \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Interval**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the uncertainty
    in the next prediction represented as a range, lower and upper bound, based on
    a specified probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: We communicate confidence intervals like this,
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that the true reservoir NTG
    is between 13% and 17%, given, predictor feature values, \(ùëã_1=ùë•_1,\ldots,ùëã_ùëö=ùë•_ùëö\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the uncertainty in our prediction, for prediction intervals we integrate,
  prefs: []
  type: TYPE_NORMAL
- en: uncertainty in the model \(ùê∏{\hat{ùëå}|ùëã=ùë•}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: error in the model, conditional distribution \(\hat{Y}|X=x\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction, Predictive Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): estimate the next
    sample(s) given assumptions about or a model of the population'
  prefs: []
  type: TYPE_NORMAL
- en: for example, given our model of the reservoir, predict the next well (pre-drill
    assessment) sample, e.g., porosity, permeability, production rate, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictor Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the input feature
    for a predictive machine learning model. We can generalize a predictive machine
    learning model as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \hat{f}(x_1,\ldots,x_m) + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: where the response feature is \(y\), the predictor features are \(x_1,\ldots,x_m\),
    and \(\epsilon\) is model error
  prefs: []
  type: TYPE_NORMAL
- en: traditional statistics uses the term independent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictor Feature Space**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): refers to the predictor
    features and does not include the response feature(s), i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: all possible combinations of predictor features for which we need to make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may be referred to as predictor feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we train and test our machines‚Äô predictions over the predictor feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: the space is typically a hypercuboid with each axis representing a predictor
    feature and extending from the minimum to maximum, over the range of each predictor
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more complicated shapes of predictor feature space are possible, e.g., we could
    mask or remove subsets with poor data coverage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data samples for
    the feature of interest, the target feature for building a model, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity measures from cores and logs used to build a full 3D porosity model.
    Any samples of porosity are the primary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as opposed to secondary feature, e.g., if we have facies data to help predict
    porosity, the facies data are secondary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): one of a variety
    of methods for dimensional reduction, transform the data to a lower dimension'
  prefs: []
  type: TYPE_NORMAL
- en: given features, \(ùëã_1,\dots,ùëã_ùëö\) we would require \({m \choose 2}=\frac{ùëö \cdot
    (ùëö‚àí1)}{2}\) scatter plots to visualize just the two-dimensional scatter plots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once we have 4 or more variables understanding our data gets very hard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recall the curse of dimensionality, impact inference, modeling and visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One solution, is to find a good lower dimensional, \(ùëù\), representation of
    the original dimensions \(ùëö\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefits of Working in a Reduced Dimensional Representation:'
  prefs: []
  type: TYPE_NORMAL
- en: data storage / Computational Time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: easier visualization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: also takes care of multicollinearity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Salient points of principal component analysis,
  prefs: []
  type: TYPE_NORMAL
- en: '*orthogonal transformation* - convert a set of observations into a set of linearly
    uncorrelated variables known as principal components, the transformation retains
    pairwise distance, i.e., is a rotation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*number of principal components (\(k\)) available* - is min‚Å°(\(ùëõ‚àí1,ùëö\)), limited
    by the variables/features, \(ùëö\), and the number of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components are ordered,
  prefs: []
  type: TYPE_NORMAL
- en: first component describes the larges possible variance / accounts for as much
    variability as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: next component describes the largest possible remaining variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: up to the maximum number of principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors-based,
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the data covariance matrix, the pairwise covariance for the combinatorial
    of features and then calculate the eigenvectors and eigenvalues from the covariance
    matrix,
  prefs: []
  type: TYPE_NORMAL
- en: the eigenvalues are the variance explained for each component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the eigenvectors of the data covariance matrix are the principal components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability Density Function** (PDF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): a representation
    of a statistical distribution with a function, \(f(x)\), of probability density
    over the range of all possible feature values, \(x\). These are the concepts for
    PDFs,'
  prefs: []
  type: TYPE_NORMAL
- en: non-negativity constraint, the density cannot be negative,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0.0 \le f(x) \]
  prefs: []
  type: TYPE_NORMAL
- en: for continuous features the density may be > 1.0, because density is a measure
    of likelihood and not of probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integrate density over a range of \(x\) to calculate probability,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0 \le \int_a^b f(x) dx = P(a \le x \le b) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability closure, the sum of the area under the PDF curve is equal to 1.0,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \int_{-infty}^{\infty} f(x) dx = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Nonparametric PDFs are calculated with kernels (usual a small Gaussian distribution)
    that is summed over all data; therefore, there is an implicitly scale (smoothness)
    parameter when calculating a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: To large of kernels will smooth out important information about the univariate
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too narrow will result in an overly noisy PDF that is difficult to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is analogous to the choice of bin size for a histogram or normalized histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric PDFs are possible but require model fitting to the data, the steps
    are,
  prefs: []
  type: TYPE_NORMAL
- en: Select a parametric distribution, e.g., Gaussian, log normal, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the parameters for the parametric distribution based on the available
    data, by methods such as least squares or maximum likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probability Non-negativity, Normalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probability Concepts: fundamental constraints on probability including,'
  prefs: []
  type: TYPE_NORMAL
- en: Bounded, \(0.0 \le P(A) \le 1.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Closure, \(P(\Omega) = 1.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Null Sets, \(P(\emptyset) = 0.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probability of Acceptance** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    applied in a rejection sampler as the likelihood of a candidate sample being added
    to the sample.'
  prefs: []
  type: TYPE_NORMAL
- en: conditional acceptance is performed by Monte Carlo simulation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sequentially sampling from conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The acceptance rule is,
  prefs: []
  type: TYPE_NORMAL
- en: if \(ùëÉ(ùëéùëêùëêùëíùëùùë°) \ge 1\), accept ‚Äì accept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if \(ùëÉ(ùëéùëêùëêùëíùëùùë°) \lt 1\), conditionally accept, draw \(ùëù ‚àº U[0,1]\), and accept
    if \(ùëù \le ùõº\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability Operators**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): common probability
    operators that are essential to working with probability and uncertainty problems,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Union of Events* - the union of outcomes, the probability of \(A\) or \(B\)
    is calculated with the probability addition rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Intersection of Events* - the intersection of outcomes, the probability of
    \(A\) and \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: only under the assumption of independence of \(A\) and \(B\) can it be calculate
    from the probabilities of \(A\) and \(B\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: if there is dependence between \(A\) and \(B\) then we need the conditional
    probability, \(P(A|B)\) instead of the marginal, \(P(A)\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Complimentary Events* - is the NOT operator for probability, if we define
    \(A\) then \(A\) compliment, \(A^c\) is not \(A\) and we have this resulting closure
    relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: complimentary events may be considered for beyond univariate problems, for example
    consider this bivariate closure,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) + P(A^c|B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, the given term must be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '*Mutually Exclusive Events* - the events that do not intersect or do not have
    any common outcomes. We represent this with set notation as,'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the joint probability of \(A\) and \(B\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability Perspectives**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the 3 primary perspectives
    for calculating probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Long-term frequencies* - probability as ratio of outcomes, requires repeated
    observations of an experiment. The basis for *frequentist probability*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Physical tendencies or propensities* - probability from knowledge about or
    modeling the system, e.g., we could know the probability of a heads outcome from
    a coin toss without the experiment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Degrees of belief* - reflect our certainty about a result, very flexible,
    assign probability to anything, and updating with new information. The basis for
    *Bayesian probability*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prototype** (clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: represent the sample data with set of points in the feature
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: prototypes are typically not actual samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample data are often assigned to the nearest (Euclidean) distance prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualitative Features**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): information about
    quantities that you cannot directly measure, require interpretation of measurement,
    and are described with words (not numbers), for example,'
  prefs: []
  type: TYPE_NORMAL
- en: rock type = sandstone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: zonation = bornite-chalcopyrite-gold higher grade copper zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative Features**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): features that can
    be measured and represented by numbers, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: age = 10 Ma (millions of years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity = 0.134 (fraction of volume is void space)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saturation = 80.5% (volume percentage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like *qualitative features*, there is often the requirement for interpretation,
    for example, total porosity may be measured but should be converted to effective
    porosity through interpretation or a model
  prefs: []
  type: TYPE_NORMAL
- en: '**\(r^2\)** (also coefficient of determination)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the proportion
    of variance explained by the model in linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'This works only for linear models, where:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^2_{tot} = \sigma^2_{reg} + \sigma^2_{res} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma^2_{tot}\) is variance of response feature training, \(y_i\),
    \(\sigma^2_{reg}\) is variance of the model predictions, \(\hat{y}_i\), and \(\sigma^2_{res}\)
    is the variance of the errors, \(\Delta y_i\).
  prefs: []
  type: TYPE_NORMAL
- en: for linear regression, \(r^2 = \left( \rho_{x,y} \right)^2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For nonlinear models this will not likely hold, then \(\frac{\sigma^2_{ùëüùëíùëî}}{\sigma^2_{ùë°ùëúùë°}}\)
    may exceed \([0,1]\), for our nonlinear models regression models we will use more
    robust measures, e.g. mean square error (MSE)
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): a ensemble
    prediction model that is based on the standard bagging approach, specifically,'
  prefs: []
  type: TYPE_NORMAL
- en: with decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with diversification of the individual trees by restricting each split to consider
    a \(p\) random subset of the \(ùëö\) available predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are various methods to calculate \(p\) from \(m\) available features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ p = \sqrt{m} \]
  prefs: []
  type: TYPE_NORMAL
- en: is common. Note, if \(p = m\) then random forest is tree bagging.
  prefs: []
  type: TYPE_NORMAL
- en: More comments on the benefit of ensemble model diversification,
  prefs: []
  type: TYPE_NORMAL
- en: the reduction in model variance by ensemble estimation, as represented by standard
    error in the mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma_{\overline{x}}^2 = fracc{\sigma_{s}^2}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: is under the assumption that the samples are uncorrelated. One issue with tree
    bagging is the trees in the ensemble may be highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: this occurs when there is a dominant predictor feature as it will always be
    applied to the top split(s), the result is all the trees in the ensemble are very
    similar (i.e. correlated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with highly correlated trees, there is significantly less reduction in model
    variance with the ensemble
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this forces each tree in the ensemble to evolve in dissimilar, decorrelated,
    manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): an outcome from
    a *random variable* or a joint outcome from a *random function*.'
  prefs: []
  type: TYPE_NORMAL
- en: an outcome from a random variable, \(X\), (or joint set of outcomes from a random
    function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: represented with lower case, e.g., \(x\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for spatial settings it is common to include a location vector, \(\bf{u}\),
    to describe the location, e.g., \(x(\bf{u})\), as \(X(\bf{u})\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: resulting from simulation, e.g., Monte Carlo simulation, sequential Gaussian
    simulation, a method to sample (jointly) from the RV (RF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in general, we assume all realizations are equiprobable, i.e., have the same
    probability of occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realizations** (uncertainty)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): multiple spatial,
    subsurface models calculated by stochastic simulation by holding input parameters
    and model choices constant and only changing the random number seed'
  prefs: []
  type: TYPE_NORMAL
- en: these models represent spatial uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, hold the porosity mean constant and observe changes in porosity
    away from the wells over multiple realizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasons to Learn Some Coding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Workflow Construction and Coding](MachineLearning_workflow_construction.html):
    Professor Pyrcz‚Äôs reasons for all scientists and engineers to learn some coding,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transparency* ‚Äì no compiler accepts hand waiving! Coding forces your logic
    to be uncovered for any other scientist or engineer to review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reproducibility* ‚Äì run it and get an answer, hand it over to a peer, they
    run it and they get the same answer. This is a principle of the scientific method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quantification* ‚Äì programs need numbers and drive us from qualitative to quantitative.
    Feed the program and discover new ways to look at the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open-source* ‚Äì leverage a world of brilliance. Check out packages, snippets
    and be amazed with what great minds have freely shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Break Down Barriers* ‚Äì don‚Äôt throw it over the fence. Sit at the table with
    the developers and share more of your subject matter expertise for a better deployed
    product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment* ‚Äì share your code with others and multiply your impact. Performance
    metrics or altruism, your good work benefits many others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficiency* ‚Äì minimize the boring parts of the job. Build a suite of scripts
    for automation of common tasks and spend more time doing science and engineering!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Always Time to Do it Again!* ‚Äì how many times did you only do it once? It
    probably takes 2-4 times as long to script and automate a workflow. Usually, worth
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be Like Us ‚Äì it will change you. Users feel limited, programmers truly harness
    the power of their applications and hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the ratio of true positives divided by all cases of the category in the testing
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Recursive Feature Elimination**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a method works by
    recursively removing features and building a model with the remaining features.'
  prefs: []
  type: TYPE_NORMAL
- en: build a model with all features, calculate a feature ranking metric, e.g., coefficient
    or feature importance, depending on which is available with the modeling method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove the feature with the lowest feature importance and rebuild the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat the process until only one feature remains
  prefs: []
  type: TYPE_NORMAL
- en: Any model predictive model could be used,
  prefs: []
  type: TYPE_NORMAL
- en: the method assigns rank \(1,\ldots,ùëö\) for all features as reverse order of
    removal, i.e., last remaining feature is most important and first removed is least
    important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reservoir Modeling Workflow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Workflow Construction and Coding](MachineLearning_workflow_construction.html):
    the following is the common geostatistical reservoir modeling workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate all available information to build multiple subsurface scenarios and
    realizations to sample the uncertainty space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply all the models to the transfer function to sample the decision criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assemble the distribution of the decision criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the optimum reservoir development decisions accounting for this uncertainty
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Response Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): output feature
    for a predictive machine learning model. We can generalize a predictive machine
    learning model as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \hat{f}(x_1,\ldots,x_m) + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: where the response feature is \(y\), the predictor features are \(x_1,\ldots,x_m\),
    and \(\epsilon\) is model error
  prefs: []
  type: TYPE_NORMAL
- en: traditional statistics uses the term dependent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge Regression** (Tikhonov Regularization)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ridge Regression](MachineLearning_ridge_regression.html): a linear, parametric
    prediction model,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize a loss function including the error, residual sum of squares (RSS)
    over the training data and a regularization term:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0)
    \right)^2 + \lambda \sum_{\alpha = 1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data, and \(\lambda \sum_{\alpha = 1}^m b_{\alpha}^2\) is the shrinkage
    penalty.
  prefs: []
  type: TYPE_NORMAL
- en: With ridge regression we add a hyperparameter, \(\lambda\), to our minimization,
    with a shrinkage penalty term, \(\sum_{j=1}^m b_{\alpha}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: As a result, ridge regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: lambda does not include the intercept, \(b_0\).'
  prefs: []
  type: TYPE_NORMAL
- en: The \(\lambda\) is a hyperparameter that controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the solution approaches linear regression, there
    is no bias (relative to a linear model fit), but the model variance is likely
    higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases,
    the model becomes simpler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the model parameters \(b_1,\ldots,b_m\) shrink
    to 0.0 and the model predictions approaches the training data response feature
    mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the set of values,
    locations that have been measured'
  prefs: []
  type: TYPE_NORMAL
- en: for example, 1,000 porosity measures from well-logs over the wells in the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or 1,000,000 acoustic impedance measurements over a 1000 x 1000 2D grid for
    a reservoir unit of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenarios** (uncertainty)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): multiple spatial,
    subsurface models calculated by stochastic simulation by changing the input parameters
    or other modeling choices to represent the uncertainty due to inference of model
    parameters and model choices'
  prefs: []
  type: TYPE_NORMAL
- en: for example, model three porosity input distribution, porosity mean low, mid
    and high, and vary the input distribution to calculate new subsurface models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data samples for
    another feature, not the feature of interest, the target feature for building
    a model, but are used to improve the prediction of the target feature.'
  prefs: []
  type: TYPE_NORMAL
- en: requires a model of the relationship between the primary and secondary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, samples in space of,
  prefs: []
  type: TYPE_NORMAL
- en: acoustic impedance (secondary data) to support calculation of a model of porosity,
    the feature of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (secondary data) to support calculation of a model of permeability,
    the feature of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seismic Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): indirect measurement
    with remote sensing, reflection seismic applies acoustic source(s) and receivers
    (geophones) to map acoustic reflections with high coverage and generally low resolution.
    Some more details,'
  prefs: []
  type: TYPE_NORMAL
- en: seismic reflections (amplitude) data are inverted to rock properties, e.g.,
    acoustic impedance, consistent with and positionally anchored with well sonic
    logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: provides framework, bounding surfaces for extents and shapes of reservoirs along
    with soft information on reservoir properties, e.g., porosity and facies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shapley Value**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): model-based, local
    (for a single prediction) and global (over a suit of predictions) feature importance
    by learning contribution of each feature to the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: A explainable machine learning method to support complicated models are often
    required but have low interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Two choices to improve model interpretability,
  prefs: []
  type: TYPE_NORMAL
- en: reduce the complexity of the models, but may also reduce model accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: develop improved, agnostic (for any model) model diagnostics, i.e., Shapley
    value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shapley value is a cooperative game theory approach that,
  prefs: []
  type: TYPE_NORMAL
- en: for allocating resources between players based on a summarization of marginal
    contributions, i.e., dividing up payment between players
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculates the contribution of each predictor feature to push the response prediction
    away from the mean value of the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: marginal contributions and Shapley values are in units of the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in the units of the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simpson‚Äôs Paradox**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): data trend
    reverses or disappears when groups are combined (or separated). Often observed
    in correlation analysis when grouping data, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: groups each have a negative correlation, but the whole has a positive correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soft Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data that has a
    high degree of uncertainty, such that data uncertainty must be integrated into
    the model'
  prefs: []
  type: TYPE_NORMAL
- en: for example, probability density function for local porosity calibrated from
    acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft data integration requires workflows like *indicator kriging*, *indicator
    simulation* and *p-field simulation* or workflows that randomize the data with
    standard simulation methods that assume hard data like *sequential Gaussian simulation*.
  prefs: []
  type: TYPE_NORMAL
- en: soft data integration is an advanced topic and a focus of ongoing research,
    but is often to done with standard, subsurface modeling software packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (biased)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: sample such that the sample statistics are not representative
    of the population parameters. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: the sample mean is not the same as the population mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the sample variance is not the same as the population variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the population parameters are not accessible, so we cannot directly
    calculate sampling bias, i.e., the difference between the sample statistics and
    the population parameters. Methods we can use to check for biased sampling,
  prefs: []
  type: TYPE_NORMAL
- en: evaluate the samples for preferential sampling, clustering, filtering, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apply *declustering* and check the results for a major change in the summary
    statistics, this is using declustering diagnostically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (clustered)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: spatial samples with locations preferentially selected, i.e.,
    clustered, resulting in biased statistics,'
  prefs: []
  type: TYPE_NORMAL
- en: typically spatial samples are clustered in locations with higher value samples,
    e.g., high porosity and permeability, good quality shale for unconventional reservoirs,
    low acoustic impedance indicating higher porosity, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the population parameters are not accessible, so we cannot directly
    calculate sampling bias, i.e., the difference between the sample statistics and
    the population parameters. Methods we can use to check for biased sampling,
  prefs: []
  type: TYPE_NORMAL
- en: evaluate the samples for preferential sampling, clustering, filtering, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apply *declustering* and check the results for a major change in the summary
    statistics, this is using declustering diagnostically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (common practice)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: sample locations are selected to,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reduce uncertainty* - by answering questions, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: how far does the contaminant plume extend? ‚Äì sample peripheries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where is the fault? ‚Äì drill based on seismic interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what is the highest mineral grade? ‚Äì sample the best part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: who far does the reservoir extend? ‚Äì offset drilling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Directly maximize net present value* - while collecting information, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: maximize production rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maximize tonnage of mineral extracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, often our samples are dual purpose, e.g., wells that are drilled
    for exploration and appraisal information are subsequently utilized for production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (representative)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: if we are sampling for representativity, i.e., the sample
    set and resulting sample statistics are representative of the population, by sampling
    theory we have 2 options:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Random sampling* - each potential sample from the population is equally likely
    to be sampled as samples are collected. This includes,'
  prefs: []
  type: TYPE_NORMAL
- en: selecting a specific location has no impact on the selection of subsequent locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumption that the population size that is much larger than the sample size;
    therefore, significant correlation between samples is not imposed due to without
    replacement sampling (the constraint that you can only sample a location once).
    Note, generally this is not an issue for the subsurface due to the sparsely sampled
    massive populations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regular sampling* - sampling at equal space or time intervals. While random
    sampling is preferred, regular sampling is robust as long as,'
  prefs: []
  type: TYPE_NORMAL
- en: the regular sampling intervals do not align with natural periodicity in the
    data, for example, the crests are systematically samples resulting in biased high
    sample statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a partitional
    clustering method that utilizes the spectrum, eigenvalues and eigenvectors, of
    a matrix that represents the pairwise relationships between the data.'
  prefs: []
  type: TYPE_NORMAL
- en: dimensionality reduction from data samples pairwise relationships characterized
    by the graph Laplacian matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues, eigenvectors are equivalent to principal component analysis dimensionality
    reduction by linear, orthogonal feature projection and rotation to best describe
    the variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of spectral clustering,
  prefs: []
  type: TYPE_NORMAL
- en: the ability to encode pairwise relationships, integrate expert knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues provide useful information on the number of clusters, based on the
    degree of ‚Äòcutting‚Äô required to make k clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lower dimensional representation for the sample data pairwise relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the resulting eigenvalues and eigenvectors can be interpreted, eigenvalues describe
    the amount of connection for each number of groups and eigenvectors are grouped
    to form the clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*) to a mean of 0.0 and a variance
    of 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{1}{\sigma_x}(x_i - \overline{x}), \quad \forall \quad i, \ldots,
    n \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\sigma_x\) are the original mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: this is a shift and stretch / squeeze of the original property distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumes no shape change, rank preserving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient-based Optimization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): a method to solve
    for model parameters by iteratively minimizing the loss function. Stochasticity
    and improve computational efficiency are added to gradient descent through the
    use of batches,'
  prefs: []
  type: TYPE_NORMAL
- en: a batch is a random subset of the training data with specified size, \(n_{batch}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: resulting in stochastic approximations of the loss function gradient, that are
    faster to calculate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: batches reduce accuracy in the gradient descent, but speed up the calculation
    and can perform more steps, often faster than gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increase \(ùëõ_{ùëèùëéùë°ùëê‚Ñé}\) for more accuracy of gradient estimation, and decrease
    \(ùëõ_{ùëèùëéùë°ùëê‚Ñé}\) to speed up the steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robbins-Siegmund (1971) Theorem - converge to global minimum for convex loss
    functions and either a global or local minimum for nonconvex loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: The steps include,
  prefs: []
  type: TYPE_NORMAL
- en: start with random model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: select a random subset of training data, \(n_{batch}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function and loss function gradient for the model parameters
    over the random batch,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha},
    b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} \]
  prefs: []
  type: TYPE_NORMAL
- en: update the parameter estimate by stepping down slope / gradient,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(r\) is the learning rate/step size, \(\hat{b}(1,ùë°)\), is the current
    model parameter estimate and \(\hat{b}(1,ùë°+1)\) is the updated parameter estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): system or process
    that is uncertain and is represented by multiple models, *realizations* and *scenarios*
    constrained by statistics,'
  prefs: []
  type: TYPE_NORMAL
- en: for example, data-driven models that integrate uncertainty like geostatistical
    simulation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: report significance, confidence / prediction intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: honor many types of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data-driven approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: limited physics used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical model assumptions / simplification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the alternative to stochastic models see *deterministic models*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistics** (practice)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the theory and
    practice for collecting, organizing, and interpreting data, as well as drawing
    conclusions and making decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistics** (measurement)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): summary measure
    of a sample, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: sample mean - \(\overline{x}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample standard deviation - \(s\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we use statistics as estimates of the model parameters that summarize the population
    (*inference*)
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical Distribution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): for a feature
    a description of the probability of occurrence over the range of possible values.
    We represent the univariate statistical distribution with,'
  prefs: []
  type: TYPE_NORMAL
- en: '*histogram*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*normalized histogram*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*probability density function* (PDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*cumulative distribution function* (CDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do we learn from a statistical distribution? For example,
  prefs: []
  type: TYPE_NORMAL
- en: what is the minimum and maximum?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have a lot of low values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have a lot of high values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have outliers, and any other values that don‚Äôt make sense and need explaining?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): training
    data within the margin or misclassified and update the support vector machines
    classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: with a support vector machines model, training data well within the correct
    region, are not support vectors, and have no impact on the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector Machines**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): a
    predictive, binary classification machine learning method that is a good classification
    method when there is poor separation of groups.'
  prefs: []
  type: TYPE_NORMAL
- en: projects the original predictor features to higher dimensional space and then
    applies a linear, plane or hyperplane,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ùëì(ùë•) = ùë•^ùëá \beta +\beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\beta\) is a vector and together with \(\beta\) are the hyperplane model
    parameters, while \(x\) is the matrix of predictor features, all are in the high
    dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùê∫(ùë•)=\text{ùë†ùëñùëîùëõ}\left( ùëì(ùë•) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: \(ùëì(ùë•)\) is proportional to the signed distance from the decision boundary,
    and \(ùê∫(ùë•)\) is the side of the decision boundary, \(‚àí\) one side and \(+\) the
    other, \(f(x) = 0\) is on the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: We represent the constraint, all data of each category must be on the correct
    side of the boundary, by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where this holds if the categories, \(y_i\), are -1 or 1\. We need a model that
    allows for some misclassification,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We introduce the concept of a margin, \(ùëÄ\), and a distance from the margin,
    the error as \(\xi_i\). Now we can pose our loss function as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N
    \xi_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: subject to, \(\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq
    M - \xi_i\).
  prefs: []
  type: TYPE_NORMAL
- en: This is the support vector machine loss function in the higher dimensional space,
    where ùõΩ,ùõΩ_0 are the multilinear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training the support vector machine, by finding the model parameters of the
    plane to maximize the margin, \(M\), while minimizing the error, \(\sum_{i=1}^N
    \xi_i\)
  prefs: []
  type: TYPE_NORMAL
- en: \(ùë™\) hyperparameter weights the sum of errors, \(xi_ùëñ\), higher \(ùê∂\), will
    result in reduced margin, \(M\), and lead to overfit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: smaller margin, fewer data used to constrain the boundary, known as support
    vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training data well within the correct side of the boundary have no influence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are some key aspects of support vector machines,
  prefs: []
  type: TYPE_NORMAL
- en: known as support vector machines, and not machine, because with a new kernel
    you get a new machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are many kernels available including polynomial and radial basis functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary hyperparameter is \(C\), the cost of
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are related to the choice of kernel, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*polynomial* - polynomial order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*radial basis function* - \(\gamma\) inversely proportional to the distance
    influence of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tabular Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: data table with rows for
    each sample and columns for each feature'
  prefs: []
  type: TYPE_NORMAL
- en: Pandas‚Äô DataFrames are a convenient class for working with tabular data, due
    to,
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to load data from a variety of file types, Python classes and
    even directly from Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to calculate summary statistics and visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data queries, sort, data filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in attributes to store information about the data, e.g. size, number nulls
    and null value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and Testing Splits**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): for model cross
    validation, prior to predictive model training withhold a proportion of the data
    as testing data.'
  prefs: []
  type: TYPE_NORMAL
- en: training data are applied to train the model parameters, while withheld testing
    data are applied to tune the model hyperparameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hyperparameter tuning is selecting the hyperparameter combination that minimizes
    the error norm over the withheld testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common approach is random selection, this may not be fair testing,
  prefs: []
  type: TYPE_NORMAL
- en: the range of testing difficulty is similar to the real-world use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too easy ‚Äì testing cases are the same or almost the same as training cases,
    random sampling is often too easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too hard ‚Äì testing cases are very different from the training cases, the model
    is expected to severely extrapolate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative methods such as k-fold cross validation provide the opportunity
    for testing over all available data but require,
  prefs: []
  type: TYPE_NORMAL
- en: the training k predictive machine learning models over the hyperparameter combinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: aggregation of the testing error over the k models for selection of the optimum
    hyperparameters, hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, there are alternative workflow that include, training, validation and
    testing subsets of the data
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer Function** (reservoir modeling workflow)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): calculation applied
    to the spatial, subsurface model realizations and scenarios to calculate a decision
    criterion, a metric that is used to support decision making representing value,
    and health, environment and safety. Example transfer functions include,'
  prefs: []
  type: TYPE_NORMAL
- en: '*transport and bioattenuation* - numerical simulation to model soil contaminant
    concentrations over time during a pump and treat operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*volumetric calculation* - for total oil-in-place to calculate resource in
    place'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*heterogeneity metric* - as an indicator of recovery factor to estimate reserves
    from resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*flow simulation* - for pre-drill production forecast for a planned well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Whittle‚Äôs pit optimization* - to calculate mineral resources and ultimate
    pit shell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncertainty Modeling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): calculation of
    the range of possible values for a feature at a location or jointly over many
    locations at the sample time. Some considerations,'
  prefs: []
  type: TYPE_NORMAL
- en: quantification of the limitation in the precision of our samples and model predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is a model, there is no objective uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is caused by our ignorance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is caused by sparse sampling, measurement error and bias, and heterogeneity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'we represent uncertainty by multiple models, scenarios and realizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Scenarios - multiple spatial, subsurface models calculated by stochastic simulation
    by changing the input parameters or other modeling choices to represent the uncertainty
    due to inference of model parameters and model choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realizations - multiple spatial, subsurface models calculated by stochastic
    simulation by holding input parameters and model choices constant and only changing
    the random number seed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfit Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a machine learning
    model that too simple, too low complexity and flexibility, to fit the natural
    phenomenon resulting in very high model bias.'
  prefs: []
  type: TYPE_NORMAL
- en: underfit models often approach the response feature global mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: underfit models have high error over training and testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increased complexity will generally decrease error with respect to the training
    and testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: over the region of model complexity with falling training and testing error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues of an underfit machine learning model,
  prefs: []
  type: TYPE_NORMAL
- en: more model complexity and flexibility is insufficient given the available data,
    data accuracy, frequency and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: low accuracy in training and testing representing real-world use away from training
    data cases, indicating poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Union of Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the union of outcomes,
    the probability of \(A\) or \(B\) is calculated with the probability addition
    rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate Parameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): summary measures
    based on one feature measured over the population'
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): summary measures
    based on one feature measured over the samples'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: learning patterns in data from unlabeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: no response features, \(ùëå\), instead only predictor features, \(ùëã_1,ldots,ùëã_ùëö\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: focus on inference of the population, the natural system, instead of prediction
    of response features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this course we use the terms inferential and predictive machine learning,
    all the covered inferential machine learning methods are unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variable** (also feature)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any property measured
    or observed in a study, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity, permeability, mineral concentrations, saturations, contaminant concentration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in data mining / machine learning this is known as a *feature*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measure often requires significant analysis, interpretation, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance Inflation Factor** (VIF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a measure of linear
    multicollinearity between a predictor feature (\(X_i\)) a nd all other predictor
    features (\(X_j, \forall j \ne i\)).'
  prefs: []
  type: TYPE_NORMAL
- en: First we calculate a linear regression for a predictor feature given all the
    other predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_i = \sum_{j, j \ne i}^m X_j + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: From this model we determine the coefficient of determination, \(R^2\), known
    as variance explained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we calculate the Variance Inflation Factor as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ VIF = \frac{1}{1 - R^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume-Variance Relations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): as
    the *volume support* (scale) increases the variance reduces'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting volume-variance relations is central to handling multiple scales
    of data and models. Some general observations and assumptions,
  prefs: []
  type: TYPE_NORMAL
- en: the mean does not change as the volume support, scale changes. Only the variance
    changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there may be shape change (we will not tackle that here). Best practice is to
    check shape change empirically. It is common to assume no shape change (*affine
    correction*) or to use a shape change model (indirect lognormal correction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the variance reduction in the distribution is inversely proportional to the
    range of spatial continuity. Variance reduces faster (over smaller volume increase)
    for shorter spatial continuity ranges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over common changes in scale this impact may be significant; therefore, it is
    not appropriate to ignore volume-variance relations,
  prefs: []
  type: TYPE_NORMAL
- en: we don‚Äôt do this scale up, change in volume support perfectly, and this is why
    it is still called the missing scale. We rarely have enough data to model this
    rigorously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we need a model to predict this change in variance with change in volume support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some change in volume support, scale models,
  prefs: []
  type: TYPE_NORMAL
- en: '*Empirical* - build a small scale, high resolution model and scale it up numerically.
    For example, calculate a high resolution model of permeability, apply flow simulation
    to calculate effective permeability over \(v\) scale blocks'
  prefs: []
  type: TYPE_NORMAL
- en: '*Power Law Averaging* - there is a flexible approach known as power law averaging.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ z_V = \left[ \frac{1}{n} \sum z_v^{\omega} \right] ^{\frac{1}{\omega}} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\omega\) is the power of averaging. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\omega = 1\) is a regular linear averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\omega = -1\) is a harmonic averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\omega = 0\) is a geometric averaging (this is proved in the limit as \(\omega
    \rightarrow 0\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate \(\omega\)?
  prefs: []
  type: TYPE_NORMAL
- en: for some cases we know from theory the correct \(\omega\) value, for example,
    for flow orthogonal to beds we select \(\omega = -1.0\) to scale up permeability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flow simulation may be applied to numerically scale up permeability and then
    to back-calculate a calibrated \(\omega\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model* - directly adjust the statistics for change in scale. For example,
    under the assumption of linear averaging and a stationary variogram and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = 1 - \frac{\overline{\gamma}(v,v)}{\sigma^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(f\) is variance reduction factor,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = \frac{D^2(v,V)}{D^2(\cdot,V)} = \frac{D^2(v,V)}{\sigma^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: in other words, \(f\) is the ratio of the variance at scale \(v\) to the variance
    at the original data point support scale based on,
  prefs: []
  type: TYPE_NORMAL
- en: the variogram model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the scale of the data, \(\cdot\) and the scale of \(v\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Venn Diagrams**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): a plot, visual tool
    for communicating probability. What do we learn from a Venn diagram?'
  prefs: []
  type: TYPE_NORMAL
- en: size of regions \(\propto\) probability of occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: proportion of \(\Omega\), all possible outcomes represented by a box, i.e.,
    probability of \(1.0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: overlap \(\propto\) probability of joint occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venn diagrams are an excellent tool to visualize marginal, joint and conditional
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well Log Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): as a much cheaper
    method to sample wells that does not interrupt drilling operations, well logs
    are very common over the wells. Often all wells have various well logs available.
    For example,'
  prefs: []
  type: TYPE_NORMAL
- en: gamma ray on pilot vertical wells to assess the locations and quality of shales
    for targeting (landing) horizontal wells
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: neutron porosity to assess location high porosity reservoir sands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gamma ray in drill holes to map thorium mineralization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well log data are critical to support subsurface resource interpretations. Once
    anchored by core data they provide the essential coverage and resolution to model
    the entire reservoir concept / framework for prediction, for example,
  prefs: []
  type: TYPE_NORMAL
- en: well log data calibrated by core data collocated with well log data are used
    to map the critical stratigraphic layers, including reservoir and seal units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: well logs are applied to depth correct features inverted from seismic that have
    location imprecision due to uncertainty in the rock velocity over the volume of
    interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weak Learner**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): the prediction
    model performs only marginally better than random'
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëå = \hat{f}_ùëò(ùëã_1,\ldots,ùëã_ùëö) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{f}_ùëò\) is the \(ùëò^{th}\) weak learner, \(ùëã_1,\ldots,ùëã_ùëö\) are the
    predictor features, \(\hat{Y}\) is the prediction of the response feature.
  prefs: []
  type: TYPE_NORMAL
- en: The term weak predictor is often used, and specifically the term weak classifier
    for the case of classification models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well Log Data, Image Logs**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a special case
    of *well logs* where the well logs are repeated at various azimuthal intervals
    within the well bore resulting in a 2D (unwrapped) image instead of a 1D line
    along the well bore. For example, Fullbore formation MicroImager (FMI) with:'
  prefs: []
  type: TYPE_NORMAL
- en: with 80% bore hole coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.2 inch (0.5 cm) resolution vertical and horizontal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 inch (79 cm) depth of investigation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: can be applied to observe lithology change, bed dips and sedimentary structures.
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic introduction to geostatistics. If you would like more on these
    fundamental concepts I recommend the Introduction, Modeling Principles and Modeling
    Prerequisites chapters from my text book, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446){cite}`pyrcz2014‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Author:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Michael Pyrcz, Professor, The University of Texas at Austin *Novel Data Analytics,
    Geostatistics and Machine Learning Subsurface Solutions*
  prefs: []
  type: TYPE_NORMAL
- en: With over 17 years of experience in subsurface consulting, research and development,
    Michael has returned to academia driven by his passion for teaching and enthusiasm
    for enhancing engineers‚Äô and geoscientists‚Äô impact in subsurface resource development.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more about Michael check out these links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Machine Learning Concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Firstly, why do this? I have received the request for a course glossary from
    the students in my **Subsurface Machine Learning** combined undergraduate and
    graduate course. While I usually dedicate a definition slide in the lecture slide
    decks for salient terms, some of my students have requested course glossary, list
    of terminology for their course review. The e-book provides a great vehicle and
    motivation to finally complete this.
  prefs: []
  type: TYPE_NORMAL
- en: Let me begin with a confession. There is a [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary)
    written by Google developers. For those seeking the in depth, comprehensive list
    of geostatistical terms please use this book!
  prefs: []
  type: TYPE_NORMAL
- en: By writing my own glossary I can limit the scope and descriptions to course
    content. I fear that many students would be overwhelmed by the size and mathematical
    notation of a standard machine learning glossary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, by including a glossary in the e-book I can link from glossary entries
    to the chapters in the e-book for convenience. I will eventual populate all the
    chapters with hyperlinks to the glossary to enable moving back and forth between
    the chapters and the glossary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, like the rest of the book, I want the glossary to be a evergreen living
    document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adjacency Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the pairwise connections between all pairwise combinations of graph
    nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: the values are indicators, 0 if not connected, 1 if connected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, node self connections are set to 0 in the adjacency matrix
  prefs: []
  type: TYPE_NORMAL
- en: '**Addition Rule** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): when we add probabilities
    (the union of outcomes), the probability of \(A\) or \(B\) is calculated with
    the probability addition rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: given mutually exclusive events we can generalize the addition rule as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Affine Correction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*). For the case of affine correction
    of \(X\) to \(Y\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{\sigma_y}{\sigma_x}(x_i - \overline{x}) + \overline{y}, \quad
    \forall \quad i, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\sigma_x\) are the original mean and variance,
    and \(\overline{y}\) and \(\sigma_y\) are the new mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: We can see above that the affine correlation method first centers the distribution
    (by subtracting the original mean), then rescales the dispersion (distribution
    spread) by the ratio of the new standard deviation to the original standard deviation
    and then shifts the distribution to centered on the new mean.
  prefs: []
  type: TYPE_NORMAL
- en: there is no shape change for affine correction. For shape change consider *Distribution
    Transformation* like *Gaussian Anamorphosis*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Affinity Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the degree of pairwise connections between all pairwise combinations
    of graph nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: values indicate the strength of the connection, unlike adjacency matrix with
    indicators, 0 if not connected, 1 if connected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, node self connections are set to 0 in the adjacency matrix
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): the
    application of bootstrap to obtain data realizations,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y^b, X_1^b, \dots, X_m^b, \quad b = 1, \dots, B \]
  prefs: []
  type: TYPE_NORMAL
- en: to train predictive model realizations,
  prefs: []
  type: TYPE_NORMAL
- en: \(\hat{Y}^b = \hat{f}^b (X_1^b, \dots, X_m^b)\)
  prefs: []
  type: TYPE_NORMAL
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: \((X_1^b, \dots, X_m^b)\) - the bootstrap predictor features in the \(b^{th}\)
    bootstrapped dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\hat{f}^b\) - the \(b^{th}\) bootstrapped model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\hat{Y}^b\) - predicted value for the model in the \(b^{th}\) bootstrapped
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to calculate prediction realizations. The ensemble of prediction realizations
    are aggregated to reduce model variance. The aggregation includes,
  prefs: []
  type: TYPE_NORMAL
- en: '*regression* - the average of the predictions $\( \hat{Y} = \frac{1}{B} \sum_{b=1}^{B}
    \hat{Y}^b \)$'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*classification* - the mode of the predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{Y} = \text{argmax}(\hat{Y}^b) \]
  prefs: []
  type: TYPE_NORMAL
- en: We can perform bagging with any prediction model, in fact the BaggingClassifier
    and BaggingRegressor functions in scikit-learn are wrappers that take the prediction
    model as an input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Basis Expansion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): to add
    flexibility to our model, for example, to capture non-linearity in our model for
    regression, classification, we expand the features with a set of basis functions'
  prefs: []
  type: TYPE_NORMAL
- en: in mathematics basis expansion is the approach of representing a more complicated
    function with a linear combination of simpler basis functions that make the problem
    easier to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with basis expansion we expand the dimensionality of the problem with basis
    functions of the original features, but still use linear methods on the transformed
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ‚Ñé(ùë•_ùëñ )=\left( ‚Ñé_1(ùë•_ùëñ ),‚Ñé_2(ùë•_ùëñ ),\ldots,‚Ñé_ùëò(ùë•_ùëñ ) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here an example of basis expansion, the set of basis functions for polynomial
    basis expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3,
    \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Basis Function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): to add
    flexibility to our model, for example, to capture non-linearity in our model for
    regression, classification, we expand the features with a set of basis functions'
  prefs: []
  type: TYPE_NORMAL
- en: in mathematics basis expansion is the approach of representing a more complicated
    function with a linear combination of simpler basis functions that make the problem
    easier to solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with basis expansion we expand the dimensionality of the problem with basis
    functions of the original features, but still use linear methods on the transformed
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ‚Ñé(ùë•_ùëñ )=\left( ‚Ñé_1(ùë•_ùëñ ),‚Ñé_2(ùë•_ùëñ ),\ldots,‚Ñé_ùëò(ùë•_ùëñ ) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'were each of \(h_1\), \ldots, \(h_k\) are basis functions. For example, here
    are the basis functions for polynomial basis expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_{i,1}(x_i) = x_i, \quad h_{i,2}(x_i) = x_i^2, \quad h_{i,3}(x_i) = x_i^3,
    \quad h_{i,4}(x_i) = x_i^4, \dots, \quad h_{i,k}(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayes‚Äô Theorem** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the mathematical
    model central to Bayesian probability for the Bayesian updating from prior probability,
    with likelihood probability from new information to posterior probability.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P(A)\) is the prior, \(P(B|A)\) is the likelihood, \(P(B)\) is the evidence
    term and \(P(A|B)\) is the posterior. If is convenient to substitute more descriptive
    labels for \(A\) and \(B\) to better conceptualize this approach,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{Model} | \text{New Data}) = \frac{P(\text{New Data} | \text{Model})
    \cdot P(\text{Model})}{P(\text{New Data})} \]
  prefs: []
  type: TYPE_NORMAL
- en: demonstrating that we are updating our model with new data
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probabilities based
    on a degree of belief (expert judgement and experience) in the likelihood of an
    event. The general approach,'
  prefs: []
  type: TYPE_NORMAL
- en: start with prior probability, prior to the collection of new information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: formulate a likelihood probability, based on new information alone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update prior with likelihood to calculate the updated posterior probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: continue to update as new information is available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: solve probability problems that we cannot use simple frequencies, i.e., *frequentist
    probability* approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian updating is modeled with *Bayes‚Äô Theorem*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian Updating for Classification**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): this is how we pose the classification
    prediction problem from the perspective of Bayesian updating, based on the conditional
    probability of a category, \(k\), given \(n\) features, \(x_1, \dots , x_n\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can solve for this posterior with Bayesian updating,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = \frac{P(x_1, \dots , x_n | C_k) P(C_k)}{P(x_1,
    \dots , x_n)} \]
  prefs: []
  type: TYPE_NORMAL
- en: let‚Äôs combine the likelihood and prior for the moment,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1, \dots , x_n | C_k) P(C_k) = P(x_1, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can expand the full joint distribution recursively as follows,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: expansion of the joint with the conditional and prior,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2, \dots , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: continue recursively expanding,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3, \dots
    , x_n, C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots
    , x_n, C_k) P(x_3 | x_4, \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n}
    | C_k) P(C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Linear Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    the frequentist formulation of the linear regression model is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_1 \times x + b_0 + \sigma \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(x\) is the predictor feature, \(b_1\) is the slope parameter, \(b_0\)
    is the intercept parameter and \(\sigma\) is the error or noise. There is an analytical
    form for the ordinary least squares solution to fit the available data while minimizing
    the \(L^2\) norm of the data error vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Bayesian formulation of linear regression is we pose the model as a
    prediction of the distribution of the response, \(Y\), now a random variable:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Y \sim N(\beta^{T}X, \sigma^{2} I) \]
  prefs: []
  type: TYPE_NORMAL
- en: We estimate the model parameter distributions through Bayesian updating for
    inferring the model parameters from a prior and likelihood from training data.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\beta | y, X) = \frac{P(y,X| \beta) P(\beta)}{P(y,X)} \]
  prefs: []
  type: TYPE_NORMAL
- en: In general for continuous features we are not able to directly calculate the
    posterior and we must use a sampling method, such as Markov chain Monte Carlo
    (McMC) to sample the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): you have big data
    if your data has a combination of these criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Volume* - many data samples and features, difficult to store, transmit
    and visualize'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Velocity* - high-rate collection, continuous data collection relative
    to decision making cycles, challenges keeping up with the new data while updating
    the models'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Variety* - data form various sources, with various types of data, types
    of information, and scales'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Variability* - data acquisition changes during the project, even for
    a single feature there may be multiple vintages of data with different scales,
    distributions, and veracity'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Data Veracity* - data has various levels of accuracy, the data is not certain'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For common subsurface applications most, if not all, of these criteria are met.
    Subsurface engineering and geoscience are often working with big data!
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Data Analytics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the process of
    examining large and varied data (*big data*) sets to discover patterns and make
    decisions, the application of statistics to big data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Transform** (also Indicator Transform)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): indicator
    coding a random variable to a probability relative to a category or a threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a categorical variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization equal to a category?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k
    \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_2 = 2\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 2\),
    then \(i(bf{u}_1; z_2) = 1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 1\), and a RV away from data, \(Z(\bf{u}_2)\) then
    is calculated as \(F^{-1}_{\bf{u}_2}(z_1)\) of the RV as \(i(\bf{u}_2; z_1) =
    0.23\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(I\{\bf{u}:z_k\}\) is an indicator for a continuous variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization less than or equal to a threshold?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le
    z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 6\%\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 8\%\),
    then \(i(\bf{u}_1; z_1) = 0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_4 = 18\%\), and a RV away from data, \(Z(\bf{u}_2) = N\left[\mu
    = 16\%,\sigma = 3\%\right]\) then \(i(\bf{u}_2; z_4) = 0.75\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indicator coding may be applied over an entire random function by indicator
    transform of all the random variables at each location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): addition of multiple
    week learners to build a stronger learner.'
  prefs: []
  type: TYPE_NORMAL
- en: a weak learner is one that offers predictions just marginally better than random
    selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the method in words, and then with equations,
  prefs: []
  type: TYPE_NORMAL
- en: build a simple model with a high error rate, the model can be quite inaccurate,
    but moves in the correct direction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the error from the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fit another model to the error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the error from this addition of the first and second model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat until the desired accuracy is obtained or some other stopping criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now with equations, the general workflow for predicting \(Y\) from \(X_1,\ldots,X_m\)
    is,
  prefs: []
  type: TYPE_NORMAL
- en: build a week learner to predict \(Y\) from \(X_1,\ldots,X_m\), \(\hat{F}_k(X)\)
    from the training data \(x_{i,j}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loop over number of desired estimators, \(k = 1,\ldots,K\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the residuals at the training data, \(h_k(x_{i}) = y_i - \hat{F}_k(x_{i})\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: fit another week learner to predict \(h_k\) from \(X_1,\ldots,X_m\), \(\hat{F}_k(X)\)
    from the training data \(x_{i,j}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: each model builds on the previous to improve the accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regression estimator is the summation over the \(K\) simple models,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{Y} =\sum_{k=1}^{K} F_k(X_1,\ldots,X_m) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): a statistical
    resampling procedure to calculate uncertainty in a calculated statistic from the
    sample data itself. Some general comments,'
  prefs: []
  type: TYPE_NORMAL
- en: '*sampling with replacement* - \(n\) (number of data samples) *Monte Carlo simulation*s
    from the dataset *cumulative distribution function*, this results in a new realization
    of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*simulates the data collection process* - the fundamental idea is to simulate
    the original data collection process. Instead of actually collecting new sample
    sets, we randomly select from the data to get data realizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bootstrap any statistic* - this approach is very flexible as we can calculate
    realizations of any statistics from the data realizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*computationally cheap* - repeat this approach to get realizations of the statistic
    to build a complete distribution of uncertainty. Use a large number of realizations,
    \(L\), for a reliable uncertainty model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*calculates the entire distribution of uncertainty* - for any statistic, you
    calculate any summary statistic for the uncertainty model, e.g., mean, P10 and
    P90 of the uncertainty in the mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*bagging for machine learning* - is the application of bootstrap to obtain
    data realizations to train predictive model realizations to aggregate predictions
    over ensembles of prediction models to reduce model variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the limitations of bootstrap?
  prefs: []
  type: TYPE_NORMAL
- en: biased sample data will likely result in a biased bootstrapped uncertainty model,
    you must first debias the samples, e.g., *declustering*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: you must have a sufficient sample size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integrates uncertainty due to sparse samples in space only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: does not account for the spatial context of the data, i.e., sample data locations,
    volume of interest nor the spatial continuity. There is a variant of bootstrap
    called [spatial bootstrap](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Spatial_Bootstrap.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    can only take one of a limited, and usually fixed, number of possible values'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical Nominal Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical*
    feature without any natural ordering, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: facies = {boundstone, wackystone, packstone, brecia}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minerals = {quartz, feldspar, calcite}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical Ordinal Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical*
    feature with a natural ordering, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: geologic age = {Miocene, Pliocene, Pleistocene} - ordered from older to younger
    rock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohs hardness = \(\{1, 2, \ldots, 10\}\) - ordered from softer to harder rock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Causation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a relationship
    where a change in one or more feature(s) directly leads to a change in one or
    more other feature(s).'
  prefs: []
  type: TYPE_NORMAL
- en: Some important aspects of causal relationships,
  prefs: []
  type: TYPE_NORMAL
- en: '*Asymmetry and temporal precedence* - \(A\) is caused by \(B\) does not indicate
    that \(B\) is caused by \(A\)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Non-spurious* - not due to random effect or confounding features'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mechanism and explanation* - a plausible mechanism or process is available
    to explain the relationship'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Consistency* - the relationship is observable over a range of conditions,
    times, locations, populations, etc.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Strength* - stronger relationships increase the likelihood of causation given
    all the previous 1-5 hold'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing causation is very difficult,
  prefs: []
  type: TYPE_NORMAL
- en: in this course we typically avoid causation and causal analysis, and emphasize
    this with statements such as correlation does not imply causation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell-based Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned such that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of declustering is for the sample statistics to be independent of sample
    locations, e.g., infill drilling or blast hole samples should not change the statistics
    for the area of interest due to increased local sample density.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cell-based declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: a cell mesh is placed over the spatial data and weights are set as proportional
    to the inverse of the number of samples in the cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the cell mesh size is varied, and the cell size that minimizes the declustered
    mean (in the sample mean is biased high) or maximizes the declustered mean (if
    the sample mean is biased low) is selected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: to remove the impact of cell mesh position, the cell mesh is randomly moved
    several times and the resulting declustering weights are averaged for each datum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The weights are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = \frac{1}{n_l} \cdot \frac{n}{L_o} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n_l\) is the number of data in the current cell, \(L_o\) is the number
    of cells with data, and \(n\) is the total number of data.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some highlights for cell-based declustering,
  prefs: []
  type: TYPE_NORMAL
- en: expert judgement to assign cell size based on the nominal sample spacing (e.g.,
    data spacing before infill drilling) will improve the performance over the automated
    method for cell size selection based on minimum or maximum declustered mean (mentioned
    above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cell-based declustering is not aware of the boundaries of the area of interest;
    therefore, data near the boundary of the area of interest may appear to be more
    sparsely sampled and receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cell-based was developed by Professor Andre Journel in 1983, []
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cognitive Biases**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): an automated (subconscious)
    thought process used by human brain to simplify information processing from large
    amount of personal experience and learned preferences. While these have been critical
    for our evolution and survival on this planet, they can lead to the following
    issues in data science:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Anchoring Bias*, too much emphasis on the first piece of information. Studies
    have shown that the first piece of information could be irrelevant as we are beginning
    to learn about a topic, and often the earliest data in a project has the largest
    uncertainty. Address anchoring bias by curating all data, integrating uncertainty,
    fostering open discussion and debate on your project team.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Availability Heuristic*, overestimate importance of easily available information,
    for example, grandfather smoked 3 packs a day and lived to 100 years old, i.e.,
    relying on anecdotes. Address availability heuristic by ensuring the project team
    documents all available information and applies quantitative analysis to move
    beyond anecdotes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Bandwagon Effect*, assessed probability increases with the number of people
    holding the same belief. Watch out for everyone jumping on board or the loudest
    voice influencing all others on your project teams. Encouraging all members of
    the project team to contribute and even separate meetings may be helpful to address
    bandwagon effect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Blind-spot Effect*, fail to see your own cognitive biases. This is the hardest
    cognitive bias of all. One possible solution is to invite arms length review of
    your project team‚Äôs methods, results and decisions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Choice-supportive Bias*, probability increases after a commitment, i.e., a
    decision is made. For example, it was good that I bought that car supported by
    focusing on positive information about the car. This is a specific case of confirmation
    bias.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Clustering Illusion*, seeing patterns in random events. Yes, this heuristic
    helped us stay alive when large predictors hunted us, i.e., false positives are
    much better than false negatives! The solution is to model uncertainty confidence
    intervals and test all data and results against random effect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Confirmation Bias*, only consider new information that supports current model.
    Choice-supportive bias is a specific case of confirmation bias. The solution to
    confirmation bias is to seek out people that you will likely disagree with and
    build skilled project teams that hold diverse technical opinions and have different
    expert experience. My approach is to get nervous if everyone in the room agrees
    with me!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Conservatism Bias*, favor old data to newly collected data. Data curation
    and quantitative analysis are helpful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Recency Bias*, favor the most recently collected data. Ensure your team documents
    previous data and choices to enhance team memory. Just like conservative bias,
    data curation and quantitative analysis are our first line of defense.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Survivorship Bias*, focus on success cases only. Check for any possible pre-selection
    or filters on the data available to your team.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Robust use of statistics / data analytics protects use from bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Complimentary Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the NOT operator
    for probability, if we define A then A compliment, \(A^c\), is not A and we have
    this resulting closure relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: complimentary events may be considered for beyond univariate problems, for example
    consider this bivariate closure,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) + P(A^c|B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, the given term must be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational Complexity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): represents the
    computer resources for a method, we use it in machine learning to understand how
    our machine learning methods scale as we change the dimensionality, number of
    features, and the number of training data, represented by,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëÇ(ùëì(ùëõ)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùëõ\) represents size of the problem. There are 2 components of computational
    complexity,
  prefs: []
  type: TYPE_NORMAL
- en: '*time complexity* - refers to computational time and the scaling of this time
    to the size of the problem for a given algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*space complexity* - refers to computer memory required and the scaling of
    storage to the size of the problem for a given algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if time complexity is \(O(n^3)\), where is \(n\) is number of training
    data, then if we double the number of data the run time increases eight times.
  prefs: []
  type: TYPE_NORMAL
- en: Additional salient points about computational complexity,
  prefs: []
  type: TYPE_NORMAL
- en: '*default to worst-case complexity* - the worst case for complexity given a
    specific problem size, provides an upper bound'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*asymptotic complexity* - where \(ùëõ\) is large. Some algorithms have speed-up
    for small datasets, this is not used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumes all steps are required, e.g., data is not presorted, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time complexity examples,
  prefs: []
  type: TYPE_NORMAL
- en: '*quadratic time*, \(ùë∂(ùíè^ùüê)\) - for example, integer multiplication, bubble
    sort'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*linear time*, \(ùë∂(ùíè)\) - for example, finding the min or max in an unsorted
    array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*fractional power*, \(ùë∂(ùíè^ùíÑ )\) - where \([0 < c < 1]\), for example, searching
    in a kd-tree, \(ùëÇ(ùëõ^(\frac{1}{2}))\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exponential Time*, \(ùë∂(ùüê^ùíè)\) - for example, traveling salesman problem with
    dynamic programing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the probability of
    an event, given another event has occurred,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: we read this as the probability of A given B has occurred as the joint divided
    by the marginal. We can extend conditional probabilities to any multivariate case
    by adding joints to either component. For example,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B,A) = \frac{P(A,B,C)}{P(B,C)} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence Interval**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the uncertainty
    in a summary statistic or model parameter represented as a range, lower and upper
    bound, based on a specified probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: We communicate confidence intervals like this,
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that model slope is between
    0.5 and 0.7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other salient points about confidence intervals,
  prefs: []
  type: TYPE_NORMAL
- en: calculated by analytical methods, when available, or with more general and flexible
    bootstrap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for Bayesian methods we refer credibility intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion Matrix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a matrix with frequencies
    of predicted (x axis) vs. actual (y axis) categories to visualize the performance
    of a classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: visualize and diagnose all the combinations of correct and misclassification
    with the classification model, for example, category 1 is often misclassified
    as category 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perfect accuracy is number of each class on the diagonal, category 1 is always
    predicted as category 1, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the classification matrix is applied to calculate a single summary of categorical
    accuracy, for example, precision, recall, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    can take any value between a lower and upper bound. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity = \(\{13.01\%, 5.23\%, 24.62\%\}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gold grade = \(\{4.56 \text{ g/t}, 8.72 \text{ g/t}, 12.45 \text{ g/t} \}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous, Interval Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *continuous feature*
    where the intervals between numbers are equal, for example, the difference between
    1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the actual
    values do not have an objective, physical reality (exist on an arbitrary scale),
    i.e., do not have a true zero point, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: Celsius scale of temperature (an arbitrary scale based on water freezing at
    0 and boiling at 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calendar year (there is no true zero year)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use addition and subtraction operations to compare continuous, interval
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous, Ratio Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *continuous feature*
    where the intervals between numbers are equal, for example, the difference between
    1.50 and 2.50 is the same as the difference between 2.50 and 3.50, but the values
    do have an objective reality (measure an actual physical phenomenon), i.e., do
    have true zero point, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: Kelvin scale of temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: permeability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saturation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is a true zero, continuous, ratio features can be compared with
    multiplication and division mathematical operations (in addition to addition and
    subtraction), e.g., twice as much porosity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuously Differentiable**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Training and Tuning](MachineLearning_training_tuning.html):
    a function is continuously differentiable if it satisfies two key conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The function is differentiable, the derivative of the function exists at every
    point in its domain, i.e., the function has a well-defined slope at every possible
    point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The derivative is continuous, the derivative of the function does not have any
    jumps, discontinuities, or abrupt changes, i.e, the derivative function itself
    is continuous at every point in its domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a machine learning example,
  prefs: []
  type: TYPE_NORMAL
- en: the \(L^2\) norm is continuously differentiable and as a result for linear and
    ridge regression we can apply partial derivatives to the loss function to calculate
    a closed form of training the model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the \(L^1\) norm is not continuously differentiable and as a result for LASSO
    regression we cannot apply partial derivatives to the loss function to calculate
    a closed form of training the model parameters. We must use iterative optimization
    to train the model parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): Integral
    product of two functions, after one is reversed and shifted by \(\Delta\).'
  prefs: []
  type: TYPE_NORMAL
- en: one interpretation is smoothing a function with weighting function, \(ùëì(\Delta)\),
    is applied to calculate the weighted average of function, \(ùëî(x)\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: this easily extends into multidimensional
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \,
    d\Delta_x \, d\Delta_y \, d\Delta_z \]
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity.
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Core Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the primary sampling
    method for direct measure for subsurface resources (recovered drill cuttings are
    also direct measures with greater uncertainty and smaller, irregular scale). Comments
    on core data,'
  prefs: []
  type: TYPE_NORMAL
- en: expensive / time consuming to collect for oil and gas, interrupt drilling operations,
    sparse and selective (very biased) coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: very common in mining (diamond drill holes) for grade control with regular patterns
    and tight spacing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gravity, piston, etc. coring are used to sample sediments in lakes and oceans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do we learn from core data?
  prefs: []
  type: TYPE_NORMAL
- en: petrological features (sedimentary structures, mineral grades), petrophysical
    features (porosity, permeability), and mechanical features (elastic modulas, Poisson‚Äôs
    ratio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stratigraphy and ore body geometry through interpolation between wells and drill
    holes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core data are critical to support subsurface resource interpretations. They
    anchor the entire reservoir concept and framework for prediction,
  prefs: []
  type: TYPE_NORMAL
- en: for example, core data collocated with well log data are used to calibrate (ground
    truth) facies, porosity from well logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): the Pearson‚Äôs
    product-moment correlation coefficient is a measure of the degree of linear relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)\sigma_x
    \sigma_y} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\overline{y}\) are the means of features \(x\)
    and \(y\). The measure is bound \(\[-1,1\]\).
  prefs: []
  type: TYPE_NORMAL
- en: correlation coefficient is a standardized covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Person‚Äôs correlation coefficient is quite sensitive to outliers and departure
    from linear behavior (in the bivariate sense). We have an alternative known as
    the Spearman‚Äôs rank correlations coefficient,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i}
    - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le
    1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The rank correlation applies the rank transform to the data prior to calculating
    the correlation coefficient. To calculate the rank transform simply replace the
    data values with the rank \(R_x = 1,\dots,n\), where \(n\) is the maximum value
    and \(1\) is the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall
    \, i \gt j \]\[ R_{x_i} = i \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a measure
    of how two features vary together,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ C_{x,y} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{(n-1)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\overline{y}\) are the means of features \(x\)
    and \(y\). The measure is bound \(\[-\sigma_x \cdot \sigm_y,\sigma_x \cdot \sigm_y\]\).
  prefs: []
  type: TYPE_NORMAL
- en: correlation coefficient is a standardized covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Person‚Äôs correlation coefficient is quite sensitive to outliers and departure
    from linear behavior (in the bivariate sense). We have an alternative known as
    the Spearman‚Äôs rank correlations coefficient,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \rho_{R_x R_y} = \frac{\sum_{i=1}^{n} (R_{x_i} - \overline{R_x})(R_{y_i}
    - \overline{R_y})}{(n-1)\sigma_{R_x} \sigma_{R_y}}, \, -1.0 \le \rho_{xy} \le
    1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The rank correlation applies the rank transform to the data prior to calculating
    the correlation coefficient. To calculate the rank transform simply replace the
    data values with the rank \(R_x = 1,\dots,n\), where \(n\) is the maximum value
    and \(1\) is the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\alpha, \, \forall \alpha = 1,\dots, n, \, | \, x_i \ge x_j \, \forall
    \, i \gt j \]\[ R_{x_i} = i \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross Validation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): withholding a portion
    of the data from the model parameter training to test the ability of the model
    to predict for cases not used to train the model'
  prefs: []
  type: TYPE_NORMAL
- en: this is typically conducted by a train and test data split, with 15% - 30% of
    data assigned to testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a dress rehearsal for real-world model use, the train-test split must be fair,
    resulting in similar prediction difficulty to the planned use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are more complicated designs such as k-fold cross validation that allows
    testing over all data via k-folds each with trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cross validation may be applied to check model performance for estimation accuracy
    (most common) and uncertainty model goodness ([Maldonado-Cruz and Pyrcz, 2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cumulative Distribution Function** (CDF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): the sum of
    a discrete PDF or the integral of a continuous PDF. Here are the important concepts,'
  prefs: []
  type: TYPE_NORMAL
- en: the CDF is stated as \(F_x(x)\), note the PDF is stated as \(f_x(x)\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: is the probability that a random sample, \(X\), is less than or equal to a specific
    value \(x\); therefore, the y axis is cumulative probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ F_x(x) = P(X \le x) = \int_{-infty}^x f(u) du \]
  prefs: []
  type: TYPE_NORMAL
- en: for CDFs there is no bin assumption; therefore, bins are at the resolution of
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monotonically non-decreasing function, because a negative slope would indicate
    negative probability over an interval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requirements for a valid CDF include,
  prefs: []
  type: TYPE_NORMAL
- en: 'non-negativity constraint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_x(x) = P(X \le x) \ge 0.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'valid probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ 0.0 \le F_x(x) \le 1.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'cannot have negative slope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \frac{dF_x(x)}{dx} \ge 0.0, \quad \forall x \]
  prefs: []
  type: TYPE_NORMAL
- en: 'minimum and maximum (ensuring probability closure) values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{min}(F_x(x)) = 0.0 \quad \text{max}(F_x(x)) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Curse of Dimensionality**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): the suite of challenges
    associated with working with many features, i.e., high dimensional space, including,'
  prefs: []
  type: TYPE_NORMAL
- en: impossible to visualize data and model in high dimensionality space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: usually insufficient sampling for statistical inference in vast high dimensional
    space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: low coverage of high dimensional predictor feature space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: distorted feature space, including warped space dominated by corners and distances
    lose sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multicollinearity between features is more likely as the dimensionality increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data** (data aspects)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): when describing spatial
    dataset these are the fundamental aspects,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data coverage* - what proportion of the population has been sampled for this?'
  prefs: []
  type: TYPE_NORMAL
- en: In general, hard data has high resolution (small scale, volume support), but
    with poor data coverage (measure only an extremely small proportion of the population,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Core coverage deepwater oil and gas* - well core only sample one five hundred
    millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter
    cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Core coverage mining grade control* - diamond drill hole cores sample one
    eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter
    cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft data tend to have excellent (often complete) coverage, but with low resolution,
  prefs: []
  type: TYPE_NORMAL
- en: '*Seismic reflection surveys and gradiometric surveys* - data is generally available
    over the entire volume of interest, but resolution is low and generally decreasing
    with depth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Scale* (support size) - What is the scale or volume sampled by the individual
    samples? For example,'
  prefs: []
  type: TYPE_NORMAL
- en: core tomography images of core samples at the pore scale, 1 - 50 \(\mu m\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gamma ray well log sampled at 0.3 m intervals with 1 m penetration away from
    the bore hole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ground-based gravity gradiometry map with 20 m x 20 m x 100 m resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Information Type* - What does the data tell us about the subsurface?
    For example,'
  prefs: []
  type: TYPE_NORMAL
- en: grain size distribution that may be applied to calibrate permeability and saturations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fluid type to assess the location of the oil water contact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dip and continuity of important reservoir layers to access connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mineral grade to map high, mid and low grade ore shells for mine planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Convexity**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    a subset, \(A\), of Euclidean feature space is convex if, for any two points \(ùë•_1\)
    and \(ùë•_2\) within \(ùê¥\), the entire line segment connecting these points is within
    \(A\), \(\left[ùë•_1,ùë•_2\right] \in A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: a convenient Pandas class
    for working with data tables with rows for each sample and columns for each feature,
    due to,'
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to load data from a variety of file types, Python classes and
    even directly from Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to calculate summary statistics and visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data queries, sort, data filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in attributes to store information about the data, e.g. size, number nulls
    and null value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Analytics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the use of statistics
    with visualization to support decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: Dr. Pyrcz says that data analytics is the same as statistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any workflow steps
    to enhance, improve raw data to be model ready.'
  prefs: []
  type: TYPE_NORMAL
- en: data-driven science needs data, data preparation remains essential
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\gt >80\%\) of any subsurface study is data preparation and interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We continue to face a challenge with data:'
  prefs: []
  type: TYPE_NORMAL
- en: data curation - format standards, version control, storage, transmission, security
    and documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: large volume to manage - visualization, availability and data mining and exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: large volumes of metadata - lack of platforms, standards and formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: engineering integration, variety of data, scale, interpretation and uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean databases are prerequisite to all data analytics and machine learning
  prefs: []
  type: TYPE_NORMAL
- en: must start with this foundation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: garbage in, garbage out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Degree Matrix** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph with the number of connections for each graph nodes, samples.'
  prefs: []
  type: TYPE_NORMAL
- en: diagonal matrix with integer for number of connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DBSCAN for Density-based Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    an density-based clustering algorithm, groups are seeded or grown in feature space
    at locations with sufficient point density determined by hyperparameters,'
  prefs: []
  type: TYPE_NORMAL
- en: \(\epsilon\) ‚Äì the radius of the local neighbourhood in the metric of normalized
    features. The is the scale / resolution of the clusters. If this values is set
    too small, too many samples are left as outliers and if set too large, all the
    clusters merge to one single cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(min_{Pts}\) ‚Äì the minimum number of points to assign a core point, where core
    points are applied to initialize or grow a cluster group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density is quantified by number of samples over a volume, where the volume is
    based on a radius over all dimensions of feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Automated or guided \(\epsilon\) parameter estimation is available by k-distance
    graph (in this case is k nearest neighbor).
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the nearest neighbor distance in normalized feature space for all
    the sample data (1,700 in this case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort in ascending order and plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the distance that maximizes the positive curvature (the elbow).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a summary of salient aspects for DBSCAN clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '*DBSCAN* - stands for Density-Based Spatial Clustering of Applications with
    Noise (Ester et al.,1996).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advantages* - include minimum domain knowledge to estimate hyperparameters,
    the ability to represent any arbitrary shape of cluster groups and efficient to
    apply for large data sets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hierarchical Bottom-up / Agglomerative Clustering* ‚Äì all data samples start
    as their own group, called ‚Äòunvisited‚Äô but practically as outliers until assigned
    to a group, and then the cluster group grow iteratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mutually Exclusive* ‚Äì like k-means clustering, all samples may only belong
    to a single cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_i \cap C_j | i \ne j) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Non-exhaustive* ‚Äì some samples may be left as unassigned and assumed as outliers
    for the cluster group assignment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(C_1 \cup C_2 \cup \dots C_k) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Criteria**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a feature that
    is calculated by applying the transfer function to the subsurface model(s) to
    support decision making. The decision criteria represents value, health, environment
    and safety. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: contaminant recovery rate to support design of a pump and treat soil remediation
    project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: oil-in-place resources to determine if a reservoir should be developed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lorenz coefficient heterogeneity measure to classify a reservoir and determine
    mature analogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recovery factor or production rate to schedule production and determine optimum
    facilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recovered mineral grade and tonnage to determine economic ultimate pit shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Tree**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Decision Tree](MachineLearning_decision_tree.html): a intuitive, regression
    and classification predictive machine learning model that devides the predictor
    space, \(ùëã_1,‚Ä¶,ùëã_ùëö\), into \(ùêΩ\) mutually exclusive, exhaustive regions, \(ùëÖ_ùëó\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*mutually exclusive* ‚Äì any combination of predictors only belongs to a single
    region, \(ùëÖ_ùëó\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exhaustive* ‚Äì all combinations of predictors belong a region, \(ùëÖ_ùëó\), regions
    cover entire feature space, range of the variables being considered'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same prediction in each region, mean of training data in region, \(\hat{Y}(ùëÖ_ùëó)
    = \overline{Y}(ùëÖ_ùëó)\)
  prefs: []
  type: TYPE_NORMAL
- en: for classification the most common, mode-based or argmax operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other salient points about decision tree,
  prefs: []
  type: TYPE_NORMAL
- en: '*supervised Learning* - the response feature label, \(Y\), is available over
    the training and testing data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*hierarchical, binary segmentation* - of the predictor feature space, start
    with 1 region and sequentially divide, creating new regions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*compact, interpretable model* - since the classification is based on a hierarchy
    of binary segmentations of the feature space (one feature at a time) the model
    can be specified in a intuitive manner as a tree with binary branches**, hence
    the name decision tree. The code for the model is nested if statements, for example,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The decision tree is constructed from the top down. We begin with a single region
    that covers the entire feature space and then proceed with a sequence of splits,
  prefs: []
  type: TYPE_NORMAL
- en: '*scan all possible splits* - over all regions and over all features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*greedy optimization* - proceeds by finding the best split in any feature that
    minimizes the residual sum of squares of errors over all the training data \(y_i\)
    over all of the regions \(j = 1,\ldots,J\). There is no other information shared
    between subsequent splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum^{J}_{j=1} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters include,
  prefs: []
  type: TYPE_NORMAL
- en: '*number of regions* ‚Äì very easy to understand, you know what the model will
    be'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*minimum reduction in RSS* ‚Äì could stop early, e.g., a low reduction in RSS
    split could lead to a subsequent split with a larger reduction in RSS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*minimum number of training data in each region* ‚Äì related to the concept of
    accuracy of the region mean prediction, i.e., we need at least ùëõ data for a reliable
    mean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*maximum number of levels* ‚Äì forces symmetric trees, similar number of splits
    to get to each region'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: various methods that assign weights to spatial samples based
    on local sampling density, such that the weighted statistics are likely more representative
    of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various declustering methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '*cell-based declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*polygonal declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kriging-based declustering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that no declustering method can prove that for every
    data set the resulting weighted statistics will improve the prediction of the
    population parameters, but in expectation these methods tend to reduce the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Declustering** (statistics)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: once declustering weights are calculated for a spatial dataset,
    then declustered statistics are applied as input for only subsequent analysis
    or modeling. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: the declustered mean is assigned as the stationary, global mean for simple kriging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the weighted CDF from all the data with weights are applied to sequential Gaussian
    simulation to ensure the back-transformed realizations approach the declustered
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any statistic can be weighted, including the entire CDF! Here are some examples
    of weighted statistics, given declustering weights, \(w(\bf{u}_j)\), for all data
    \(j=1,\ldots,n\).
  prefs: []
  type: TYPE_NORMAL
- en: weighted sample mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \overline{x}_{wt} = \frac{\sum_{i=1}^n w(\bf{u}_j) \cdot z(\bf{u}_j)}{\sum_{i=1}^n
    w(\bf{u}_j)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n\) is the number of data.
  prefs: []
  type: TYPE_NORMAL
- en: weighted sample variance,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ s^2_{x_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) - 1} \cdot \sum_{i=1}^n
    w(\bf{u}_j) \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}_{wt}\) is the declustered mean.
  prefs: []
  type: TYPE_NORMAL
- en: weighted covariance,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ C_{x,y_{wt}} = \frac{1}{\sum_{i=1}^n w(\bf{u}_j) } \cdot \sum_{i=1}^n w(\bf{u}_j)
    \cdot \left( x(\bf{u}_j) - \overline{x}_{wt} \right) \cdot \left( y(\bf{u}_j)
    - \overline{y}_{wt} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}_{wt}\) and \(\overline{y}_{wt}\) are the declustered means
    for features \(X\) and \(Y\).
  prefs: []
  type: TYPE_NORMAL
- en: the entire CDF,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ F_z(z) \approx \sum_{j=1}^{n(Z<z)} w(\bf{u}_j) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(n(Z<z)\) is the number of sorted ascending data less than threshold
    \(z\). We show this as approximative as this is simplified and at data resolution
    and without an interpolation model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that no declustering method can prove that for every
    data set the resulting weighted statistics will improve the prediction of the
    population parameters, but in expectation these methods tend to reduce the bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-Connected** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    points \(A\) and \(B\) are density-connected if there is a point \(Z\) that is
    density-reachable from both points \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-based Cluster** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    a nonempty set where all points are density-connected to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Density-Reachable** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    point \(Y\) is density reachable from \(A\) if \(Y\) belongs to a neighborhood
    of a core point that can reached from \(A\). This would require a chain of core
    points each belonging the previous core points and the last core point including
    point Y.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that assumes
    the system or process that is completely predictable'
  prefs: []
  type: TYPE_NORMAL
- en: often-based on engineering and geoscience physics and expert judgement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, numerical flow simulation or stratigraphic bounding surfaces interpreted
    from seismic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for this course we also state that data-driven estimation models like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: integration of physics and expert knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integration of various information sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: often quite time consuming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: often no assessment of uncertainty, focus on building one model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality Reduction**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): methods to reduce
    the number of features within a data science workflow. There are 2 primary methods,'
  prefs: []
  type: TYPE_NORMAL
- en: '*features Selection* ‚Äì find the subset of original features that are most important
    for the problem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*feature projection* ‚Äì transform the data from a higher to lower dimensional
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Known as dimension reduction or dimensionality reduction
  prefs: []
  type: TYPE_NORMAL
- en: motivated by the curse of dimensionality and multicollinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: applied in statistics, machine learning and information theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Directly Density Reachable** (DBSCAN)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Density-based Clustering](MachineLearning_density-based_clustering.html):
    point \(X\) is directly density reachable from \(A\), if \(A\) is a core point
    and \(X\) belongs to the neighborhood, distance \(le \epsilon\) from \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discrete Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a *categorical
    feature* or a *continuous feature* that is binned or grouped, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity between 0 and 20% assigned to 10 bins = {0 - 2%, 2% - 4%, \ldots ,20%}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohs hardness = \(\{1, 2, \ldots, 10\}\) (same at *categorical feature*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution Transformations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    mapping from one distribution to another distribution through percentile values,
    resulting in a new histogram, PDF, and CDF. We perform distribution transformations
    in geostatistical methods and workflows because,'
  prefs: []
  type: TYPE_NORMAL
- en: '*inference* - to correct a feature distribution to an expected shape, for example,
    correcting for too few or biased data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*theory* - a specific distribution assumption is required for a workflow step,
    for example, Gaussian distribution with mean of 0.0 and variance of 1.0 is required
    for sequential Gaussian simulation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*data preparation or cleaning* - to correct for outliers, the transformation
    will map the outlier into the target distribution no longer as an outlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we perform distribution transformations?
  prefs: []
  type: TYPE_NORMAL
- en: 'We transform the values from the cumulative distribution function (CDF), \(F_{X}\),
    to a new CDF , \(G_{Y}\). This can be generalized with the quantile - quantile
    transformation applied to all the sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward transform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Y = G_{Y}^{-1}(F_{X}(X)) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse transform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ X = F_{X}^{-1}(G_{Y}(Y)) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This may be applied to any data, including parametric or nonparametric distributions.
    We just need to be able to map from one distribution to another through percentiles,
    so it is a:'
  prefs: []
  type: TYPE_NORMAL
- en: rank preserving transform, for example, P25 remains P25 after distribution transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eager Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): Model is
    a generalization of the training data constructed prior to queries'
  prefs: []
  type: TYPE_NORMAL
- en: the model is input-independent after parameter training and hyperparameter tuning,
    i.e., the training data does not need to be available to make new predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opposite is lazy learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is process of obtaining
    the single best value to represent a feature at an unsampled location, or time.
    Some additional concepts,'
  prefs: []
  type: TYPE_NORMAL
- en: local accuracy takes precedence over global spatial variability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too smooth, not appropriate for any transform function that is sensitive to
    heterogeneity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, inverse distance and kriging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: many predictive machine learning models focus on estimation (e.g., k-nearest
    neighbours, decision tree, random forest, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**f1-score** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the harmonic mean of recall and precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ f1-score_k = \frac{2} { \frac{1}{Precision_k} + \frac{1}{Recall_k} } \]
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder,
  prefs: []
  type: TYPE_NORMAL
- en: '*recall* - the ratio of true positives divided by all cases of the category
    in the testing dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*precision* - the ratio of true positives divided by all positives, true positives
    + false positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature** (also variable)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any property measured
    or observed in a study'
  prefs: []
  type: TYPE_NORMAL
- en: for example, porosity, permeability, mineral concentrations, saturations, contaminant
    concentration, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in data mining / machine learning this is known as a feature, statisticians
    call these variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measure often requires significant analysis, interpretation, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when features are modified and combined to improve our models we call this feature
    engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Engineering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): using
    domain expertise to extract improved predictor or response features from raw data,'
  prefs: []
  type: TYPE_NORMAL
- en: improve the performance, accuracy and convergency, of inferential or predictive
    machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: improve model interpretability (or may worsen interpretability if our engineered
    features are in unfamiliar units)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mitigate outliers & bias, consistency with assumptions such as Gaussianity,
    linearization, dimensional expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformation and feature selection are two forms of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Importance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a variety of machine
    learning methods to provide measures for feature ranking, for example decision
    trees summarize the reduction in mean square error through inclusion of each feature
    and is summarized as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ FI(x) = \sum_{t \in T_f} \frac{N_t}{N} \Delta_{MSE_t} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(T_f\) are all nodes with feature \(x\) as the split, \(N_t\) is the
    number of training samples reaching node \(t\), \(N\) is the total number of samples
    in the dataset and \(\Delta_{MSE_t}\) is the reduction in MSE with the \(t\) split.
  prefs: []
  type: TYPE_NORMAL
- en: Note, feature importance can be calculated in a similar manner to MSE above
    for the case of classification trees with *Gini Impurity*.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance is part of model-based feature ranking,
  prefs: []
  type: TYPE_NORMAL
- en: the accuracy of the feature importance depends on the accuracy of the model,
    i.e., an inaccurate model will likely provide incorrect feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Imputation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): replacing null
    values in the data table, samples that do not have values for all features with
    plausible values for 2 reasons,'
  prefs: []
  type: TYPE_NORMAL
- en: enable statistical calculations and models that require complete data tables,
    i.e., cannot work with missing feature values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maximize model accuracy, increasing the number of reliable samples available
    for training and testing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mitigate model bias that may occur with likewise deletion in feature values
    are not missing at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature imputation methods include,
  prefs: []
  type: TYPE_NORMAL
- en: '*constant value imputation* - replace null values with feature mean or mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model-based imputation* - replace null values with a prediction of the missing
    feature with available feature values for the same sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also an iterative methods that depend on convergence,
  prefs: []
  type: TYPE_NORMAL
- en: '*Multiple Imputation by Chained Equations (MICE)* - assign random values and
    then iterate over the missing values predicting new values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of this method is to obtain reasonable imputed values that account
    for the relationships between all the features and all the available and missing
    values
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Projection**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): a transforms original
    \(m\) features to \(p\) features, where \(p << m\) for dimensionality reduction'
  prefs: []
  type: TYPE_NORMAL
- en: given features, \(ùëã_1,\ldots,ùëã_ùëö\) we would require \(\binom{m}{2} = \frac{ùëö(ùëö‚àí1)}{2}\)
    scatter plots to visualize just the two-dimensional scatter plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: these representations would not capture \(> 2\) dimensional structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once we have 4 or more variables understanding our data gets very difficult.
    Recall the curse of dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: principal component analysis, multidimensional scaling and random projection
    are examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature selection is an alternative method for dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Space**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): commonly feature space
    only refers to the predictor features and does not include the response feature(s),
    i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: all possible combinations of predictor features for which we need to make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may be referred to as predictor feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we train and test our machines‚Äô predictions over the predictor feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: the space is typically a hypercuboid with each axis representing a predictor
    feature and extending from the minimum to maximum, over the range of each predictor
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more complicated shapes of predictor feature space are possible, e.g., we could
    mask or remove subsets with poor data coverage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Ranking**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): part of feature engineering,
    feature ranking is a set of methods that assign relative importance or value to
    each feature with respect to information contained for inference and importance
    in predicting a response feature.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a wide variety of possible methods to accomplish this. My recommendation
    is a wide-array approach with multiple metric, while understanding the assumptions
    and limitations of each method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the general types of metrics that we will consider for feature ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Visual Inspection* - including data distributions, scatter plots and violin
    plots'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Statistical Summaries* - correlation analysis, mutual information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model-based* - including model parameters, feature importance scores and global
    Shapley values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recursive feature elimination* - and other methods that perform trail and
    error to find optimum parameters sets through withheld testing data cross validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature ranking is primarily motivated by the curse of dimensionality, i.e.,
    work with the fewest, most informative predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Transformations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    type of feature engineering involving mathematical operation applied to a feature
    to improve the value of the feature in a workflow. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: feature truncation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature normalization or standardization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature distribution transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many reasons that we may want to perform feature transformations.
  prefs: []
  type: TYPE_NORMAL
- en: the make the features consistent for visualization and comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to avoid bias or impose feature weighting for methods (e.g. k nearest neighbours
    regression) that rely on distances calculated in predictor feature space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the method requires the variables to have a specific range or distribution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: artificial neural networks may require all features to range from [-1,1]
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: partial correlation coefficients require a Gaussian distribution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical tests may require a specific distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: geostatistical sequential simulation requires an indicator or Gaussian transform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformations is a common basic building blocks in many machine learning
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fourth Paradigm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the data-driven
    paradigm for scientific discovery building from the,'
  prefs: []
  type: TYPE_NORMAL
- en: First Paradigm - empirical science - experiments and observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second Paradigm - theoretical science - analytical expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third Paradigm - computation science - numeric simulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We augment with new scientific paradigms, we don‚Äôt replace older paradigms.
    Each of the previous paradigm are supported by the previous paradigms, for example,
  prefs: []
  type: TYPE_NORMAL
- en: theoretical science is build on empirical science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: numerical simulations integrate analytical expressions and calibrated equations
    from experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequentist Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): measure of the likelihood
    that an event will occur based on frequencies observed from an experiment. For
    random experiments and well-defined settings (such as coin tosses),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Prob}(A) = P(A) = \lim_{n \to \infty} \frac{n(A)}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(n(A)\) = number of times event \(A\) occurred \(n\) = number of trails
  prefs: []
  type: TYPE_NORMAL
- en: For example, possibility of drilling a dry hole for the next well, encountering
    sandstone at a location (\(\bf{u}_{\alpha}\)), exceeding a rock porosity of \(15
    \%\) at a location (\(\bf{u}_{\alpha}\)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Gaussian Anamorphosis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    quantile transformation to a Gaussian distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping feature values through their cumulative probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = G_y^{-1}\left( F_x(x)\right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùêπ_ùë•\) is the original feature cumulative distribution function (CDF)
    and \(ùê∫_ùë¶\) is the Gaussian CDF probability density function
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = \frac{1}{\sigma \sqrt{2 \pi}} exp \left[-1 \frac{1}{2} \left(\frac{x-\mu}{\sigma}
    \right)^2 \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: shorthand for a normal distribution is
  prefs: []
  type: TYPE_NORMAL
- en: \[ N[\mu,\sigma^2] \]
  prefs: []
  type: TYPE_NORMAL
- en: for example \(N[0,1]\) is standard normal
  prefs: []
  type: TYPE_NORMAL
- en: much of natural variation or measurement error is Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: parameterized fully by mean, variance and correlation coefficient (if multivariate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: distribution is unbounded, no min nor max, extremes are very unlikely, some
    type of truncation is often applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning, many workflows apply univariate Gaussian anamorphosis and then assume
    bivariate or multivariate Gaussian, this is not correct, but it is generally too
    difficult to transform our data to multivariate Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: Methods that require a Gaussian distribution,
  prefs: []
  type: TYPE_NORMAL
- en: Pearson product-moment correlation coefficients completely characterize multivariate
    relationships when data are multivariate Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: partial correlations require bivariate Gaussian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sequential simulation (geostatistics) assumes Gaussian to reproduce the global
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Student‚Äôs t test for difference in means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-square distributions is derived from sum of squares of Gaussian distributed
    random variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian naive Bayes classification assumes Gaussian conditionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gibbs Sampler** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics, based on,'
  prefs: []
  type: TYPE_NORMAL
- en: sequentially sampling from conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since only the conditional probability density functions are required, the system
    is simplified as the full joint probability density function is not needed
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the basic steps of the Gibbs MCMC Sampler for a bivariate case,
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for \(ùëã(0)\), \(ùëå(0)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from \(ùëì(ùëã|ùëå(0))\) to get \(ùëã(1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from \(ùëì(ùëå|ùëã(1))\) to get \(ùëå(1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for the next steps for samples, \(\ell = 1,\ldots,ùêø\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting samples will have the correct joint distribution,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëì(ùëã,ùëå) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Boosting Models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): a prediction model
    that results from posing a boosting model as gradient descent problem'
  prefs: []
  type: TYPE_NORMAL
- en: At each step, \(k\), a model is being fit, then the error is calculated, \(h_k(X_1,\ldots,X_m)\).
  prefs: []
  type: TYPE_NORMAL
- en: We can assign a loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ L\left(y,F(X)\right) = \frac{\left(y - F(X)\right)^2}{2} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So we want to minimize the \(\ell2\) loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \sum_{i=1}^{n} L\left(y_i, F_k(X) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: by adjusting our model result over our training data \(F(x_1), F(x_2),\ldots,F(x_n)\).
  prefs: []
  type: TYPE_NORMAL
- en: We can take the partial derivative of the error vs. our model,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We can interpret the residuals as negative gradients.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i - F(x_i) = -1 \frac{\partial J}{\partial F(x_i)} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we have a gradient descent problem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ F_{k+1}(X_i) = F_k(X_i) + h(X_i) \]\[ F_{k+1}(X_i) = F_k(X_i) + y_i - F_k(X_i)
    \]\[ F_{k+1}(X_i) = F_k(X_i) - 1 \frac{\partial J}{\partial F_k(X_i)} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the general form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi_{k+1} = \phi_k - \rho \frac{\partial J}{\partial \phi_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(phi_k\) is the current state, \(\rho\) is the learning rate, \(J\) is
    the loss function, and \(\phi_{k+1}\) is the next state of our estimator.
  prefs: []
  type: TYPE_NORMAL
- en: The error residual at training data is the gradient, then we are performing
    gradient descent,
  prefs: []
  type: TYPE_NORMAL
- en: fitting a series of models to negative gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By approaching the problem as a gradient decent problem we are able to apply
    a variety of loss functions,
  prefs: []
  type: TYPE_NORMAL
- en: \(\ell2\) is our \(\frac{\left(y - F(X)\right)^2}{2}\) is practical, but is
    not robust with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ - 1 \frac{\partial J}{\partial F_k(X_i)} = y_i - F_k(X_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\ell1\) is our \(|y - F(X)|\) is more robust with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ - 1 \frac{\partial J}{\partial F_k(X_i)} = sign(y_i - F_k(X_i)) \]
  prefs: []
  type: TYPE_NORMAL
- en: there are others like Huber Loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph Laplacian** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a matrix representing
    a graph by integrating connections between graph nodes, samples, number of connections
    for each graph nodes, samples. Calculated as degree matrix minus adjacency matrix.
    Where,'
  prefs: []
  type: TYPE_NORMAL
- en: '*degree matrix*, \(ùê∑\) - degree of connection for each node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adjacency matrix, \(ùê¥\) - specific connections between nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geostatistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a branch of applied
    statistics that integrates:'
  prefs: []
  type: TYPE_NORMAL
- en: the spatial (geological) context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the spatial relationship
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: volumetric support / scale
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: uncertainty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I include all spatial statistics with geostatistics, some disagree with me on
    this. From my experience, any useful statistical method for modeling spatial phenomenon
    is adopted and added to the geostatistics toolkit! Geostatistics is an expanding
    and evolving field of study.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient-based Optimization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): a method to solve
    for model parameters by iteratively minimizing the loss function. The steps include,'
  prefs: []
  type: TYPE_NORMAL
- en: start with random model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function for the model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function gradient, generally don‚Äôt have an equation for the
    loss function, sampling with numerical calculation of the local loss function
    derivative,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha},
    b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} \]
  prefs: []
  type: TYPE_NORMAL
- en: update the parameter estimate by stepping down slope / gradient,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(r\) is the learning rate/step size, \(\hat{b}(1,ùë°)\), is the current
    model parameter estimate and \(\hat{b}(1,ùë°+1)\) is the updated parameter estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Some important comments about gradient-based optimization,
  prefs: []
  type: TYPE_NORMAL
- en: '*gradient search convergence* - the method will find a local or global minimum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*gradient search step size* - impact of step size, \(r\) too small, takes too
    long to converge to a solution and \(r\) too large, the solution may skip over/miss
    a global minimum or diverge'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multiple model parameters* - calculate and decompose the gradient over multiple
    model parameters, with a vector representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1, b_2)) = \left[ \begin{matrix} \nabla
    L(y_{\alpha}, F(X_{\alpha}, b_1)) & \nabla L(y_{\alpha}, F(X_{\alpha}, b_2)) \end{matrix}
    \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: '*exploration of parameter space* - optimization for training machine learning
    model parameters is exploration of a high dimensional model parameter space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph** (spectral clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a diagram
    that represents data in an organized manner, each sample as a node with vertices
    indicating pairwise relationships between samples.'
  prefs: []
  type: TYPE_NORMAL
- en: for an undirected graph, vertices are bidirectional, i.e., the connection is
    symmetric, both ways with the same strength
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gridded Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: generally exhaustive, regularly
    spaced data over 2D or 3D, representing maps and models'
  prefs: []
  type: TYPE_NORMAL
- en: stored as a .csv comma delimited file, with \(ùëõ_ùë¶\) rows and \(ùëõ_ùë•\) columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may also be saved/loaded as also binary for a more compact, but not human readable
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: commonly visualized directly, for example, matplotlib‚Äôs imshow function, or
    as contour maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data that has a
    high degree of certainty, usually from a direct measurement from the rock'
  prefs: []
  type: TYPE_NORMAL
- en: for example, well core-based and well log-based porosity and lithofacies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, hard data has high resolution (small scale, volume support), but
    with poor coverage (measure only an extremely small proportion of the population,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Core coverage deepwater oil and gas* - well core only sample one five hundred
    millionth to one five billionth of a deepwater reservoir, assuming 3 inch diameter
    cores with 10% core coverage in vertical wells with 500 m to 1,500 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Core coverage mining grade control* - diamond drill hole cores sample one
    eight thousandth to one thirty thousandth of ore body, assuming HQ 63.5 mm diameter
    cores with 100% core coverage in vertical drill holes with 5 m to 10 m spacing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hermite Polynomials**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): a family
    of orthogonal polynomials on the real number line.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Order | Hermite Polynomial \(H_e(x)\) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0th Order | \(H_{e_0}(x) = 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 1st Order | \(H_{e_1}(x) = x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 2nd Order | \(H_{e_2}(x) = x^2 - 1\) |'
  prefs: []
  type: TYPE_TB
- en: '| 3rd Order | \(H_{e_3}(x) = x^3 - 3x\) |'
  prefs: []
  type: TYPE_TB
- en: '| 4th Order | \(H_{e_4}(x) = x^4 - 6x^2 + 3\) |'
  prefs: []
  type: TYPE_TB
- en: These polynomials are orthogonal with respect to a weighting function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùë§(ùë•)=ùëí^{‚àí\frac{ùë•^2}{2}} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the standard Gaussian probability density function without the scaler,
    \(\frac{1}{\sqrt{2\pi}}\). The definition of orthogonality is stated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \int_{-\infty}^{\infty} H_m(x) H_n(x) w(x) \, dx = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The Hermite polynomials are orthogonal over the interval \([‚àí\infty,\infty]\)
    for the standard normal probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: By applying hermite polynomials instead of regular polynomials for polynomial
    basis expansion in polynomial regression were remove the multicollinearity between
    the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: recall, independence of the predictor features is an assumption of the linear
    system applied in polynomial regression with the polynomial basis expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heuristic Algorithm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: a shortcut solution to solve a difficult problem a compromise
    of optimality and accuracy for speed and practicality.'
  prefs: []
  type: TYPE_NORMAL
- en: this general approach is common in machine learning, computer science and mathematical
    optimization, for example, the solution for k-mean clustering a \(k^n\) solution
    space is practically solved with an heuristic algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: all cluster group assignments are determined iteratively,
    as opposed to an partitional clustering method that determine cluster groups all
    at once. Including,'
  prefs: []
  type: TYPE_NORMAL
- en: '*agglomerative hierarchical clustering* - start with \(n\) clusters, each data
    sample in its own cluster, and then iteratively merges clusters into larger clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*divisive hierarchical clustering* - start with all data in one cluster, and
    then iteratively divide off new clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering is partitional clustering, while the solution heuristic to
    find the solution is iterative, the solution is actually all at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: difficult to update, once a series of splits or mergers are made it is difficult
    to go back and modify the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): a representation
    of the univariate statistical distribution with a plot of frequency over an exhaustive
    set of bins over the range of possible values. These are the steps to build a
    histogram,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the continuous feature range of possible values into \(K\) equal size
    bins, \(\delta x\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: or use available category labels for categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Count the number of samples (frequency) in each bin, \(n_k\), \quad \(\forall
    \quad k=1,\ldots,K\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the frequency vs. the bin label (use bin centroid if continuous)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note, histograms are typically plotted as a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): system or process
    that includes a combination of both *deterministic model* and *stochastic model*'
  prefs: []
  type: TYPE_NORMAL
- en: most geostatistical models are hybrid models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, additive deterministic trend models and stochastic residual models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): events \(A\) and
    \(B\) are independent if and only if the following relations are true,'
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A \cap B) = P(A) \cdot P(B)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(A|B) = P(A)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B|A) = P(B)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any of these are violated we suspect that there exists some form of relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '**Indicator Transform** (also Binary Transform)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): indicator
    coding a random variable to a probability relative to a category or a threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a categorical variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization equal to a category?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) = z_k
    \\ 0, & \text{if } Z(\bf{u}) \ne z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_2 = 2\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 2\),
    then \(i(bf{u}_1; z_2) = 1\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 1\), and a RV away from data, \(Z(\bf{u}_2)\) then
    is calculated as \(F^{-1}_{\bf{u}_2}(z_1)\) of the RV as \(i(\bf{u}_2; z_1) =
    0.23\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(i(\bf{u}:z_k)\) is an indicator for a continuous variable,
  prefs: []
  type: TYPE_NORMAL
- en: what is the probability of a realization less than or equal to a threshold?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[\begin{split} i(\bf{u}; z_k) = \begin{cases} 1, & \text{if } Z(\bf{u}) \le
    z_k \\ 0, & \text{if } Z(\bf{u}) > z_k \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for example,
  prefs: []
  type: TYPE_NORMAL
- en: given threshold, \(z_1 = 6\%\), and data at \(\bf{u}_1\), \(z(\bf{u}_1) = 8\%\),
    then \(i(\bf{u}_1; z_1) = 0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given threshold, \(z_4 = 18\%\), and a RV away from data, \(Z(\bf{u}_2) = N\left[\mu
    = 16\%,\sigma = 3\%\right]\) then \(i(\bf{u}_2; z_4) = 0.75\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indicator coding may be applied over an entire random function by indicator
    transform of all the random variables at each location.
  prefs: []
  type: TYPE_NORMAL
- en: '**Indicator Variogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): varogram‚Äôs
    calculated and modelled from the *indicator transform* of spatial data and used
    for indicator kriging. The indicator variogram is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \gamma_i(\mathbf{h}; z_k) = \frac{1}{2N(\mathbf{h})} \sum_{\alpha=1}^{N(\mathbf{h})}
    \left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h}; z_k) \right]^2
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i(\mathbf{u}_\alpha; z_k)\) and \(i(\mathbf{u}_\alpha + \mathbf{h};
    z_k)\) are the indicator transforms for the \(z_k\) threshold at the tail location
    \(\mathbf{u}_\alpha\) and head location \(\mathbf{u}_\alpha + \mathbf{h}\) respectively.
  prefs: []
  type: TYPE_NORMAL
- en: for hard data the indicator transform \(i(\bf{u},z_k)\) is either 0 or 1, in
    which case the \(\left[ i(\mathbf{u}_\alpha; z_k) - i(\mathbf{u}_\alpha + \mathbf{h};
    z_k) \right]^2\) is equal to 0 when the values at head and tail are both \(\le
    z_k\) (for continuous features) or \(= z_k\) (for categorical features), the same
    relative to the threshold, or 1 when they are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: therefore, the indicator variogram is \(\frac{1}{2}\) the proportion of pairs
    that change! The indicator variogram can be related to probability of change over
    a lag distance, \(h\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the sill of an indicator variogram is the indicator variance calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma_i^2 = p \cdot (1 - p) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(p\) is the proportion of 1‚Äôs (or zeros as the function is symmetric
    over proportion)
  prefs: []
  type: TYPE_NORMAL
- en: '**Inference, Inferential Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): this is a big topic,
    but for the course I provide this simplified, functional definition, given a random
    sample from a population, describe the population, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: given the well samples, describe the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given the drill hole samples, describe the ore body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inlier**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: a regression model accuracy metric, the proportion of testing data within a
    margin, \(\epsilon\), of the model predictions, \(\hat{y}_i\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_R = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} I(y_i, \hat{y}_i)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: given the indicator transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} I(y_i, \hat{y}_i) = \begin{cases} 1, & \text{if } |y_i - \hat{y}_i|
    \leq \epsilon \\ 0, & \text{otherwise} \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful, intuitive measure of accuracy, the proportion of training
    or testing data with predictions that are good enough.
  prefs: []
  type: TYPE_NORMAL
- en: but, there is a choice of the size of the margin, \(\epsilon\), that could be
    related to the accuracy required for the specific application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance-based Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): also known
    as memory-based learning, compares new prediction problems (as set of predictors,
    \(ùë•_1,\ldots,ùë•_ùëö\)) with the cases observed in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: model requires access to the training data, acting as a library of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prediction directly from the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prediction complexity grows with the number of training data, \(ùëõ\), number
    of neighbors, \(ùëò\), and number of features, \(ùëö\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a specific case of lazy learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intersection of Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the intersection
    of outcomes, the probability of \(A\) and \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: under the assumption of independence of \(A\) and \(B\) the probability of \(A\)
    and \(B\) is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Irreducible Error**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    data limitations, including missing features and missing samples, for example,
    the full predictor feature space is not adequately sampled'
  prefs: []
  type: TYPE_NORMAL
- en: irreducible error is not impacted by model complexity, it is a limitation of
    the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_e^2\) is irreducible error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inertia** (clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: the k-means clustering loss function summarizing the difference
    between samples within the same group over all the groups,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum_{i=1}^{K} \sum_{x_j \in C_i} \| x_j - \mu_i \|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(K\) is the total number of clusters, \(C_i\) represents the set of samples
    in the \(i^{th}\) cluster, \(x_j\) represents a data sample in cluster, \(C_i\),
    \(mu_i\) is the prototype of cluster \(C_i\),\(\| x_j - \mu_i \|^2\) is the squared
    Euclidean distance between sample \(x_j\) and the cluster prototype \(\mu_i \).
    The samples and prototypes and distance calculations in mD space, with \(1,\ldots,m\)
    features.
  prefs: []
  type: TYPE_NORMAL
- en: by minimizing inertia k-means clusters minimizes difference within groups while
    maximizing difference between groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Joint Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probability that
    considers more than one event occurring together, the probability of \(A\) and
    \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: or the probability of \(A\), \(B\) and \(C\) is represented as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B \cap C) = P(A,B,C) \]
  prefs: []
  type: TYPE_NORMAL
- en: under the assumption of independence of \(A\), \(B\) and \(C\) the joint probability
    may be calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B,C) = P(A) \cdot P(B) \cdot P(C) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**K Bins Discretization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): bin
    the range of the feature into K bins, then for each sample assignment of a value
    of 1 if the sample is within a bin and 0 if outsize the bin'
  prefs: []
  type: TYPE_NORMAL
- en: binning strategies include uniform width bins (uniform) and uniform number of
    data in each bin (quantile)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also known as one hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods that require K bins discretization,
  prefs: []
  type: TYPE_NORMAL
- en: basis expansion to work in a higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discretization of continuous features to categorical features for categorical
    methods such as naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: histogram construction and Chi-square test for difference in distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mutual information binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K-fold Cross Validation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): partitioning the
    data into K folds, and looping over the folds training the model with reminder
    of the data and testing the model with the data in the fold. Then aggregating
    the testing accuracy over all the folds.'
  prefs: []
  type: TYPE_NORMAL
- en: the train and test data split is based on K, for example, K = 4, is 25% testing
    for each fold and K = 5, is 20% testing for each fold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is an improvement over cross validation that only applies one train and
    test split to build a single model. The K-fold approach allows testing of all
    data and the aggregation of accuracy over all the folds tends to smooth the accuracy
    vs. hyperparameter plot for more reliable hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-fold cross validation may be applied to check model performance for estimation
    accuracy (most common) and uncertainty model goodness ([Maldonado-Cruz and Pyrcz,
    2021](https://www.sciencedirect.com/science/article/pii/S0920410521006343))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-Means Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: an unsupervised machine learning method for partitional clustering,
    group assignment to unlabeled data, where dissimilarity within clustered groups
    is mini minimized. The loss function that is minimized is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum^k_{i=1} \sum_{\alpha \in C_i} || X_{\alpha} - \mu_i || \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(i\) is the cluster index, \(\alpha\) is the data sample index, \(X\)
    is the data sample and \(\mu_i\) is the \(i\) cluster prototype, \(k\) is the
    total number of clusters, and \(|| X_m - \mu_m ||\) is the Euclidean distance
    from a sample to the cluster prototype in \(M\) dimensional space calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ || X_{m,\alpha} - \mu_i || = \sqrt{ \sum_m^M \left( X_{m,\alpha} - \mu_{m,i}
    \right)^2 } \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is a summary of import aspects for k-means clustering,
  prefs: []
  type: TYPE_NORMAL
- en: '*k* - is given as a model hyperparameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*exhaustive and mutually exclusive groups* - all data assigned to a single
    group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*prototype method* - represents the training data with number of synthetic
    cases in the features space. For K-means clustering we assign and iteratively
    update \(K\) prototypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iterative solution* - the initial prototypes are assigned randomly in the
    feature space, the labels for each training sample are updated to the nearest
    prototype, then the prototypes are adjusted to the centroid of their assigned
    training data, repeat until there is no further update to the training data assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*unsupervised learning* - the training data are not labeled and are assigned
    \(K\) labels based on their proximity to the prototypes in the feature space.
    The idea is that similar things, proximity in feature space, should belong to
    the same cluster group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*feature weighting* - the procedure depends on the Euclidian distance between
    training samples and prototypes in feature space. Distance is treated as the ‚Äòinverse‚Äô
    of similarity. If the features have significantly different magnitudes, the feature(s)
    with the largest magnitudes and ranges will dominate the loss function and cluster
    groups will become anisotropic aligned orthogonal to the high range feature(s).
    While the common approach is to standardize / normalize the variables, by-feature
    weighting may be applied through unequal variances. Note, in this demonstration
    we normalize the features to range from 0.0 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-Nearest Neighbours**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): a simple,
    interpretable and flexible, nonparametric predictive machine learning model based
    on a local weighting window applied to \(k\) nearest training data'
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbours approach is similar to a convolution approach for spatial
    interpolation. Convolution is the integral product of two functions, after one
    is reversed and shifted by \(\Delta\).
  prefs: []
  type: TYPE_NORMAL
- en: one interpretation is smoothing a function with weighting function, \(ùëì(\Delta)\),
    is applied to calculate the weighted average of function, \(ùëî(x)\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: this easily extends into multidimensional
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x, y, z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
    f(\Delta_x, \Delta_y, \Delta_z) g(x - \Delta_x, y - \Delta_y, z - \Delta_z) \,
    d\Delta_x \, d\Delta_y \, d\Delta_z \]
  prefs: []
  type: TYPE_NORMAL
- en: The choice of which function is shifted before integration does not change the
    result, the convolution operator has commutativity.
  prefs: []
  type: TYPE_NORMAL
- en: \[ (f * g)(x) = \int_{-\infty}^{\infty} f(\Delta) g(x - \Delta) \, d\Delta \]\[
    (f * g)(x) = \int_{-\infty}^{\infty} f(x - \Delta) g(\Delta) \, d\Delta \]
  prefs: []
  type: TYPE_NORMAL
- en: if either function is reflected then convolution is equivalent to cross-correlation,
    measure of similarity between 2 signals as a function of displacement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for k-nearest neighbours the use of \(k\) results in a locally adaptive window
    size, different from standard convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-nearest neighbours is an instance-based, lazy learning method, the model training
    is postponed until prediction is required, no precalculation of the model. i.e.,
    prediction requires access to the data.
  prefs: []
  type: TYPE_NORMAL
- en: to make new predictions that training data must be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameters include,
  prefs: []
  type: TYPE_NORMAL
- en: '*k number of nearest data* to utilize for prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*data weighting*, for example uniform weighting with the local training data
    average, or inverse distance weighting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, for the case of inverse distance weighting, the method is analogous to
    inverse distance weighted interpolation with a maximum number of local data constraint
    commonly applied for spatial interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: inverse distance is available in GeostatsPy for spatial mapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too find the k-nearest data a distance metric is needed,
  prefs: []
  type: TYPE_NORMAL
- en: training data within the predictor feature space are ranked by distance (closest
    to farthest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a variety of distance metrics may be applied, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidian distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \begin{equation}
  prefs: []
  type: TYPE_NORMAL
- en: d_i = \sqrt{\sum_{\alpha = 1}^{m} \left(x_{\alpha,i} - x_{\alpha,0}\right)^2}
    \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: Minkowski Distance - a general expression for distance with well-known Manhattan
    and Euclidean distances are special cases,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p
    \right)^{\frac{1}{p}} \]
  prefs: []
  type: TYPE_NORMAL
- en: when \(p=2\), this becomes the Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when \(p=1\) it becomes the Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel Trick** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): we
    can incorporate our basis expansion in our method without ever needing to transform
    the training data to this higher dimensional space,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x) \]
  prefs: []
  type: TYPE_NORMAL
- en: We only need the inner product over the predictor features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ h(x) \left( h(x') \right)^T = \langle h(x), h(x') \rangle \]
  prefs: []
  type: TYPE_NORMAL
- en: Instead of the actual values in the transformed space, we just need the ‚Äòsimilarity‚Äô
    between all available training data in that transformed space!
  prefs: []
  type: TYPE_NORMAL
- en: we training our support vector machines with only a similarity matrix between
    training data that will be projected to the higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we never actually need to calculate the training data values in the higher dimensional
    space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kriging**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: spatial estimation approach that relies on linear weights
    that account for spatial continuity, data closeness and redundancy. The kriging
    estimate is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ z^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot z(\bf{u}_{\alpha})
    + \left( 1.0 - \sum_{\alpha=1}^n \lambda_{\alpha} \right) \cdot m_z \]
  prefs: []
  type: TYPE_NORMAL
- en: the right term is the unbiasedness constraint, one minus the sum of the weights
    is applied to the global mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case where the trend, \(t(\bf{u})\), is removed, we now have a residual,
    \(y(\bf{u})\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ y(\bf{u}) = z(\bf{u}) - t(\bf{u}) \]
  prefs: []
  type: TYPE_NORMAL
- en: the residual mean is zero so we can simplify our kriging estimate as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y^*(\bf{u}) = \sum_{\alpha = 1}^{n} \lambda_{\alpha} \cdot y(\bf{u}_{\alpha})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The simple kriging weights are calculated by solving a linear system of equations,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{j=1}^n \lambda_j C(\bf{u}_i,\bf{u}_j) = C(\bf{u},\bf{u}_i), \quad i=1,\ldots,n
    \]
  prefs: []
  type: TYPE_NORMAL
- en: that may be represented with matrix notation as,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{bmatrix} C(\bf{u}_1,\bf{u}_1) & C(\bf{u}_1,\bf{u}_2)
    & \dots & C(\bf{u}_1,\bf{u}_n) \\ C(\bf{u}_2,\bf{u}_1) & C(\bf{u}_2,\bf{u}_2)
    & \dots & C(\bf{u}_2,\bf{u}_n) \\ \vdots & \vdots & \ddots & \vdots \\ C(\bf{u}_n,\bf{u}_1)
    & C(\bf{u}_n,\bf{u}_2) & \dots & C(\bf{u}_n,\bf{u}_n) \\ \end{bmatrix} \cdot \begin{bmatrix}
    \lambda_1 \\ \lambda_2 \\ \vdots \\ \lambda_n \\ \end{bmatrix} = \begin{bmatrix}
    C(\bf{u}_1,\bf{u}) \\ C(\bf{u}_2,\bf{u}) \\ \vdots \\ C(\bf{u}_n,\bf{u}) \\ \end{bmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This system may be derived by substituting the equation for kriging estimates
    into the equation for estimation variance, and then setting the partial derivative
    with respect to the weights to zero.
  prefs: []
  type: TYPE_NORMAL
- en: we are optimizing the weights to minimize the estimation variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this system integrates the,
  prefs: []
  type: TYPE_NORMAL
- en: '*spatial continuity* as quantified by the variogram (and covariance function
    to calculate the covariance, \(C\), values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*redundancy* the degree of spatial continuity between all of the available
    data with themselves, \(C(\bf{u}_i,\bf{u}_j)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*closeness* the degree of spatial continuity between the available data and
    the estimation location, \(C(\bf{u}_i,\bf{u})\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kriging provides a measure of estimation accuracy known as kriging variance
    (a specific case of estimation variance).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^{2}_{E}(\bf{u}) = C(0) - \sum^{n}_{\alpha = 1} \lambda_{\alpha} C(\bf{u}_0
    - \bf{u}_{\alpha}) \]
  prefs: []
  type: TYPE_NORMAL
- en: Kriging estimates are best in that they minimize the above estimation variance.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of kriging estimates include,
  prefs: []
  type: TYPE_NORMAL
- en: '*Exact interpolator* - kriging estimates with the data values at the data locations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kriging variance* - a measure of uncertainty in a kriging estimate. Can be
    calculated before getting the sample information, as the kriging estimation variance
    is not dependent on the values of the data nor the kriging estimate, i.e. the
    kriging estimator is homoscedastic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spatial context* - kriging takes integrates spatial continuity, closeness
    and redundancy; therefore, kriging accounts for the configuration of the data
    and structural continuity of the feature being estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scale* - kriging by default assumes the estimate and data are at the same
    point support, i.e., mathematically represented as points in space with zero volume.
    Kriging may be generalized to account for the support volume of the data and estimate,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multivariate* - kriging may be generalized to account for multiple secondary
    data in the spatial estimate with the cokriging system. We will cover this later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smoothing effect* - of kriging can be forecasted as the missing variance.
    The missing variance over local estimates is the kriging variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kriging-based Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kriging-based declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: calculate and model the experimental variogram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply kriging to calculate estimates over a high-resolution grid covering the
    area of interest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the sum of the weights assigned to each data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: assign data weights proportional to this sum of weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The weights are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = n \cdot \frac{\sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_j}{\sum_{i=1}^n
    \left[ \sum_{iy}^{ny} \sum_{ix}^{nx} \lambda_{j,ix,iy} \right]} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(nx\) and \(ny\) are the number of cells in the grid, \(n\) is the number
    of data, and \(\lambda_{j,ix,iy}\) is the weight assigned to the \(j\) data at
    the \(ix,iy\) grid cell.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an important point for kriging-based declustering,
  prefs: []
  type: TYPE_NORMAL
- en: like polygonal declustering, kriging-based declustering is sensitive to the
    boundaries of the area of interest; therefore, the weights assigned to the data
    near the boundary of the area of interest may change radically as the area of
    interest is expanded or contracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, kriging-based declustering integrates the spatial continuity model from
    variogram model. Consider the following possible impacts of the variogram model
    on the declustering weights,
  prefs: []
  type: TYPE_NORMAL
- en: if there is 100% relative nugget effect, there is no spatial continuity and
    therefore, all data receives equal weight. Note for the equation above this results
    in a divide by 0.0 error that must be checked for in the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: geometric anisotropy may significantly impact the weights as data aligned over
    specific azimuths are assessed as closer or further in terms of covariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kolmogorov‚Äôs 3 Probability Axioms**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probability Concepts: these are Kolmogorov‚Äôs 3 axioms for valid probabilities,'
  prefs: []
  type: TYPE_NORMAL
- en: Probability of an event is a non-negative number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(ùê¥) \ge 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Probability of the entire sample space, all possible outcomes, \(\Omega\), is
    one (unity), also known as probability closure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: Additivity of mutually exclusive events for unions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: e.g., probability of \(A_1\) and \(A_2\) mutual exclusive events is, \(P(A_1
    + A_2) = P(A_1) + P(A_2)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^1\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): known as Manhattan
    norm or sum of absolute residual (SAR),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n |\Delta y_i | \]
  prefs: []
  type: TYPE_NORMAL
- en: also expressed as the mean absolute error (MAE),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n |\Delta y_i | \]
  prefs: []
  type: TYPE_NORMAL
- en: Minimization with \(L^1\) norm is known as minimum absolute difference.
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^2\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): known as sum of
    square residual (SSR),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \sqrt{\Delta y_i} \]
  prefs: []
  type: TYPE_NORMAL
- en: also expressed as the mean square error (MSE),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{n} \sum_{i=1}^n \left( \Delta y_i \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: and the Euclidian norm,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sqrt{ \sum_{i=1}^n \sqrt{\Delta y_i} } \]
  prefs: []
  type: TYPE_NORMAL
- en: Minimization with \(L^2\) norm is known as the method of least squares.
  prefs: []
  type: TYPE_NORMAL
- en: '**\(L^1\) vs. \(L^2\) Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): the choice of \(L^1\)
    and \(L^2\) norm is important in machine learning. To explain this let‚Äôs compare
    the performance of \(L^1\) and \(L^2\) norms in loss functions while training
    model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Least Absolute Deviations (L1) | Least Squares (L2) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness* | Robust | Not very robust |'
  prefs: []
  type: TYPE_TB
- en: '| Solution Stability | Unstable solution | Stable solution |'
  prefs: []
  type: TYPE_TB
- en: '| Number of Solutions | Possibly multiple solutions | Always one solution |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Selection | Built-in feature selection | No feature selection |'
  prefs: []
  type: TYPE_TB
- en: '| Output Sparsity | Sparse outputs | Non-sparse outputs |'
  prefs: []
  type: TYPE_TB
- en: '| Analytical Solutions | No analytical solutions | Analytical solutions |'
  prefs: []
  type: TYPE_TB
- en: Here‚Äôs some important points,
  prefs: []
  type: TYPE_NORMAL
- en: '*robust* - resistant to outliers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*unstable* - for small changes in training the trained model predictions may
    jump'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multiple solutions* - different solution have similar or the same loss, resulting
    in solutions jumping with small changes to the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*output sparsity* and *feature selection* - model parameters tend to 0.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*analytical solutions* - an analytical solution is available to solve for the
    optimum model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**\(L^1\) or \(L^2\) Normalizer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): is
    performed across features over individual samples to constrain the sum'
  prefs: []
  type: TYPE_NORMAL
- en: The L1 Norm has the following constraint across samples,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\alpha = 1}^m x^{\prime}_{i,\alpha} = 1.0, \quad i = 1, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: The L1 normalizer transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[ x^{\prime}_{i,\alpha} = \frac{x_{i,\alpha}}{\sum_{\alpha=1}^m x_{i,\alpha}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The L2 Norm has the following constraint across samples,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{\alpha = 1}^m \left( x^{\prime}_{i,\alpha} \right)^2 = 1.0, \quad i
    = 1, \ldots, n \]
  prefs: []
  type: TYPE_NORMAL
- en: The L2 normalizer transform,
  prefs: []
  type: TYPE_NORMAL
- en: \[ x^{\prime}_{i,\alpha} = \sqrt{\frac{(x_{i,\alpha})^2}{\sum_{\alpha=1}^m (x_{i,\alpha})^2}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Example, applied in text classification and clustering, and L1 for compositional
    data (sum 1.0 constraint)
  prefs: []
  type: TYPE_NORMAL
- en: '**LASSO Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): linear regression
    with \(L^1\) regularization term and regularization hyperparameter \(\lambda\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: As a result, LASSO regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The only difference between LASSO and ridge regression is:'
  prefs: []
  type: TYPE_NORMAL
- en: for LASSO the shrinkage term is posed as an \(\ell_1\) penalty,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda \sum_{\alpha=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: for ridge regression the shrinkage term is posed as an \(\ell_2\) penalty,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda \sum_{\alpha=1}^m \left(b_{\alpha}\right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'While both ridge regression and the LASSO shrink the model parameters (\(b_{\alpha},
    \alpha = 1,\ldots,m\)) towards zero:'
  prefs: []
  type: TYPE_NORMAL
- en: LASSO parameters reach zero at different rates for each predictor feature as
    the lambda, \(\lambda\), hyperparameter increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as a result LASSO provides a method for feature ranking and selection!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lambda, \(\lambda\), hyperparameter controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the prediction model approaches linear regression,
    there is lower model bias, but the model variance is higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the coefficients all become 0.0 and the model
    is the training data response feature mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lazy Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): model is
    a generalization of the training data and calculation is delayed until query is
    made of the model'
  prefs: []
  type: TYPE_NORMAL
- en: the model is the training data and selected hyperparameters, to make new predictions
    the training data must be available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opposite is eager learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate** (gradient boosting)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): controls the rate
    of updating with each new model.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_m = f_{m-1} - \rho_m \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial
    F(X_\alpha)} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\rho_m\) is the learning rate, \frac{\partial L(y_\alpha, F(X_\alpha))}{\partial
    F(X_\alpha)} is the gradient, error, \(f_{m-1}\) is the previous estimate, and
    \(f_m\) is the new estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Some salient points about learning rate,
  prefs: []
  type: TYPE_NORMAL
- en: without learning rate, the boosting models learn too quickly and will have too
    high model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: slow down learning for a more robust model, balanced to ensure good performance,
    too small rate will require very large number of models to reach convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Likewise Deletion** (MRMR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): removal of any sample
    with any missing feature values'
  prefs: []
  type: TYPE_NORMAL
- en: if missing feature values are not missing at random (MAR) this may impart a
    bias in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: will result in a decrease in the effective data size and increase in model uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): a linear, parametric
    prediction model,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n (\Delta y_i)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '*Error-free* - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linearity* - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Constant Variance* - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Independence of Error* - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*No multicollinearity* - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location Map**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Loading and Plotting Data and Models](MachineLearning_plotting_data_models.html):
    a data plot where the 2 axes are locations, e.g., \(X\) and \(Y\), Easting and
    Northing, Latitude and Longitude, etc., to show the locations and magnitudes of
    the spatial data.'
  prefs: []
  type: TYPE_NORMAL
- en: often the data points are colored to represent the scale of feature to visualize
    the sampled feature over the area or volume of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: advantage, visualize the data without any model that may bias our impression
    of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disadvantage, may be difficult to visualize large datasets and data in 3D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss Function**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): the equation that
    is minimized to train the model parameters. For example, the loss function for
    linear regression includes residual sum of square, the \(L^2\) error norm,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0)
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: for LASSO regression the loss function includes residual sum of square, the
    \(L^2\) error norm, plus a \(L^1\) regularization term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m |b_{\alpha}| \]
  prefs: []
  type: TYPE_NORMAL
- en: for k-means clustering the loss function is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ I = \sum^k_{i=1} \sum_{\alpha \in C_i} \sqrt{ \sum_{j = 1}^m X_{\alpha,m}
    - \mu_{i,m} } \]
  prefs: []
  type: TYPE_NORMAL
- en: The method to minimize loss functions depends on the type of norm,
  prefs: []
  type: TYPE_NORMAL
- en: with \(L^2\) norms we apply differentiation to the loss function with respect
    to the model parameter and set it equal to zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with \(L^1\) norms in our loss functions we lose access to an analytical solution
    and use iterative optimization, e.g., steepest descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine Learning Workflow Design**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: is based on the following
    steps,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Specify the Goals* - for example,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: build a numerical model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: evaluate different recovery processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specify the Data* - what is available and what is missing?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Design a Set of Steps to Accomplish the Goal* - common steps include,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: load data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: format, check and clean data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: run operation, including, statistical calculation, model or visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: transfer function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Develop Documentation* - including implementation details, defense of decisions,
    metadata, limitations and future work'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Flow* - data and information flow, learning while modeling with branches and
    loop backs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Uncertainty* - summarize all uncertainty sources, include methods to integrate
    uncertainty, defend the uncertainty models and aspects deemed certain'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Margin** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): when
    the training data include overlapping categories it would not be possible, nor
    desirable, to develop a decision boundary that perfectly separates the categories
    for which this condition would hold,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: We need a model that allows for some misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We introduce the concept of a margin, \(ùëÄ\), and a distance from the margin
    (error, ùúâ_ùëñ).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N
    \xi_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: The loss function includes the margin term, \(M\), and hence attempts to minimize
    margin while minimizing classification error weighted by hyperparameter, \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginal Probability**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): probability that
    considers only one event occurring, the probability of \(A\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities may be calculated from joint probabilities through the
    process of marginalization,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) = \int_{-\infty}^{\infty} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: where we integrate over all cases of the other event, \(B\), to remove its influence.
    Given discrete possible cases of event \(B\) we can simply sum the probabilities
    over all possible cases of \(B\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) = \sum_{i=1}^{k_B} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix Scatter Plots**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): composite
    plot including the combinatorial of all pair-wise scatter plots for all features.'
  prefs: []
  type: TYPE_NORMAL
- en: given \(m\) features, there are \(m \times m\) scatter plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the scatter plots are ordered, y-axis feature from \(X_1,\ldots,X_m\) over the
    rows and x-axis feature from \(X_1,\ldots,X_m\) over the columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the diagonal is the features plotted with themselves and are often replaced
    with feature histograms or probability density functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use matrix scatter plots to,
  prefs: []
  type: TYPE_NORMAL
- en: look for bivariate linear or nonlinear structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: look for bivariate homoscedasticity (constant conditional variance) and heteroscedasticity
    (conditional variance changes with value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: look for bivariate constraints, such as sum constraints with compositional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, the other features are marginalized, this is not a full m-D visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum Relevance Minimum Redundancy** (MRMR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a mutual information-based
    approach for feature ranking that accounts for feature relevance and redundancy.'
  prefs: []
  type: TYPE_NORMAL
- en: one example is a relevance minus redundancy summary,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MRMR = max \left[ frac{1}{|S|} \sum_{X_i \in S} I(X_i,Y) - \frac{1}{|S|^2}
    \sum_{X_i \in S} \sum_{X_j, i \ne j} I(X_i,X_j) \right] \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(ùëÜ\) is the predictor feature subset and \(|ùëÜ|\) is the number of features
    in the subset \(ùëÜ\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Metropolis-Hastings MCMC Sampler**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    The basic steps of the Metropolis-Hastings MCMC Sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\ell = 1, \ldots, L\):'
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for the initial sample of model parameters, \(\beta(\ell
    = 1) = b_1(\ell = 1)\), \(b_0(\ell = 1)\) and \(\sigma^2(\ell = 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propose new model parameters based on a proposal function, \(\beta^{\prime}
    = b_1\), \(b_0\) and \(\sigma^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate probability of acceptance of the new proposal, as the ratio of the
    posterior probability of the new model parameters given the data to the previous
    model parameters given the data multiplied by the probability of the old step
    given the new step divided by the probability of the new step given the old.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X)
    }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply Monte Carlo simulation to conditionally accept the proposal, if accepted,
    \(\ell = \ell + 1\), and sample \(\beta(\ell) = \beta^{\prime}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minkowski Distance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[k-Nearest Neighbours](MachineLearning_knearest_neighbours.html): a general
    expression for distance with well-known Manhattan and Euclidean distances are
    special cases,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ d_{(i,i')} = \left( \sum_{j=1}^{m} \left( x_{(j,i)} - x_{(j,i')} \right)^p
    \right)^{\frac{1}{p}} \]
  prefs: []
  type: TYPE_NORMAL
- en: when \(p=2\), this becomes the Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when \(p=1\) it becomes the Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing Feature Values**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): null values
    in the data table, samples that do not have values for all features'
  prefs: []
  type: TYPE_NORMAL
- en: There are many causes of missing feature values, for example,
  prefs: []
  type: TYPE_NORMAL
- en: sampling cost, e.g., low permeability test takes too long
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rock rheology sample filter, e.g., can‚Äôt recover the mudstone samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sampling to reduce uncertainty and maximize profitability instead of statistical
    representativity, dual purpose samples for information and production
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing data consequences, more than reducing the amount of training and testing
    data, missing data, if not completely at random may result in,
  prefs: []
  type: TYPE_NORMAL
- en: biased sample statistics resulting in biased model training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: biased models with biased predictions with potentially no indication of the
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing at Random** (MAR)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Imputation](MachineLearning_feature_imputation.html): missing feature
    values are distributed randomly, uniform coverage over the predictor feature space,
    i.e., all values have likelihood to be missing, and no correlation between missing
    feature values.'
  prefs: []
  type: TYPE_NORMAL
- en: This is typically not the case as missing data often has a confounding feature,
    for example,
  prefs: []
  type: TYPE_NORMAL
- en: sampling cost, e.g., low permeability test takes too long
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: rock rheology sample filter, e.g., can‚Äôt recover the mudstone samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sampling to reduce uncertainty and maximize profitability instead of statistical
    representativity, dual purpose samples for information and production
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing data consequences, more than reducing the amount of training and testing
    data, missing data, if not completely at random may result in,
  prefs: []
  type: TYPE_NORMAL
- en: biased sample statistics resulting in biased model training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: biased models with biased predictions with potentially no indication of the
    bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Bias**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    insufficient complexity and flexibility to fit the natural setting'
  prefs: []
  type: TYPE_NORMAL
- en: increasing model complexity usually results in decreasing model bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model bias variance trade-off* - as complexity increases, model variance increases
    and model bias decreases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\left(E [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0)
    \right)^2\) is model bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-Bias Variance Trade-off**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): as complexity increases,
    model variance increases and model bias decreases.'
  prefs: []
  type: TYPE_NORMAL
- en: as model variance and model bias are both components of expected test square
    error, the balancing of model bias and model variance results in an optimum level
    of complexity to minimize the testing error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Checking**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is a critical last
    step for any spatial modeling workflow. Here are the critical aspects of model
    checking,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model Inputs* - data and statistics integration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: check the model to ensure the model inputs are honored in the models, generally
    checked over all the realizations, for example, the output histograms and matches
    the input histogram over the realizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accurate Spatial Estimates* - ability of the model to accurately predict away
    from the available sample data, over a variety of configurations, with accuracy'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: by cross validation, withholding some of the data, check the model‚Äôs ability
    to predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generally, summarized with a truth vs. predicted cross plot and measures such
    as mean square error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE = \frac{1}{n} \sum_{\alpha = 1}^{n} \left(z^{*}(\bf{u}_{\alpha}) - z(\bf{u}_{\alpha})
    \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Accurate and Precise Uncertainty Models* - uncertainty model is fair given
    the amount of information available and various sources of uncertainty'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: also checked through cross validation, withholding some of the data, but by
    checking the proportion of the data in specific probability intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarized with a proportion of withheld data in interval vs. the probability
    interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points on the 45 degree line indicate accurate and precise uncertainty model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points above the 45 degree line indicate accurate and imprecise uncertainty
    model, uncertainty is too wide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: points below the 45 degree line indicate inaccurate uncertainty model, uncertainty
    is too narrow or model is biased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Complexity or Flexibility**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the ability of
    a model to fit to data and to be interpreted.'
  prefs: []
  type: TYPE_NORMAL
- en: A variety of concepts may be used to describe model complexity,
  prefs: []
  type: TYPE_NORMAL
- en: the number of features, predictor variables are in the model, dimensionality
    of the model, usually resulting in more model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of parameters, the order applied for each term, e.g. linear, quadratic,
    thresholds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the format of the model, i.e., a compact equation with polynomial regression
    vs. nested conditional statements with decision tree vs. thousands of weights
    and bias model parameters for a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, more complexity with a high order polynomial, larger decision trees
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, more complicated or flexible models are more difficult to interpret,
  prefs: []
  type: TYPE_NORMAL
- en: linear regression and the associated model parameters can be analyzed and even
    applied for feature ranking, while support vector machines with radial basis functions
    are a linear model in the nD high dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Generalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the ability of
    a model to predict away from training data.'
  prefs: []
  type: TYPE_NORMAL
- en: the model learns the structure in the data and does not just memorize the training
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models that do not generalize well,
  prefs: []
  type: TYPE_NORMAL
- en: overfit models have high accuracy at training data and low accuracy away from
    training data, demonstrated with low testing accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: underfit models are too simple or inflexible for the natural phenomenon and
    have low training and testing accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Hyperparameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): constrain the model
    complexity. Hyperparameters are tuned to maximize accuracy with the withheld testing
    data to prevent model overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: For a set of polynomial models from \(4^{th}\) to \(1^{st}\) order,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_4 \cdot x^4 + b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]\[
    y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]\[ y = b_2 \cdot x^2 +
    b_1 \cdot x + b_0 \]\[ y = b_1 \cdot x + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: the choice of polynomial order is the hyperparameter, i.e., the first order
    model is most simple and the fourth order model is most complicated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Parameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): trainable coefficients
    for a machine learning model that control the fit to the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: For a polynomial model,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = b_3 \cdot x^3 + b_2 \cdot x^2 + b_1 \cdot x + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: \(b_3\), \(b_2\), \(b_1\), and \(b_0\) are model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '*training model parameters* - model parameters are calculated by optimization
    to minimize error and regularization terms over the training data through analytical
    solution or iterative solution, e.g., gradient descent optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ridge Regression](MachineLearning_ridge_regression.html): adding information
    to prevent overfit (or underfit), improve model generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: this information is known as a regularization term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this represents a penalty for complexity that is tuned with a regularization
    hyperparameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the ridge regression loss function,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda \sum_{j=1}^m b_{\alpha}^2\) is the regularization term and \(\lambda\)
    is the regularization hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of regularization is quite general and choices in machine learning
    architecture, such as,
  prefs: []
  type: TYPE_NORMAL
- en: use of receptive fields for convolutional neural networks (CNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the choice to limit decision trees to a maximum number of levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a couple of useful perspectives on model regularization,
  prefs: []
  type: TYPE_NORMAL
- en: '*Occam‚Äôs razor* - regularization tunes model complexity to the simplest effective
    solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bayesian perspective* - regularization is imposing a prior on the solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Variance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): is error due to
    sensitivity to the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: increasing model complexity usually results in increasing model variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ensemble machine learning, for example, model bagging reduce model variance
    by averaging over multiple estimators trained on bootstrap realizations of the
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*model bias variance trade-off* - as complexity increases, model variance increases
    and model bias decreases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the three components of expected test square error, including model variance,
    model bias and irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E \left[ \left(y_0 - \hat{f}(x_1^0, \ldots, x_m,^0 \right)^2 \right] = \left(E
    [\hat{f}(x_1^0, \ldots, x_m,^0)] - f(x_1^0, \ldots, x_m,^0) \right)^2 + \]\[ E
    \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[ \hat{f}(x_1^0,
    \ldots, x_m,^0) \right] \right)^2 \right] + \sigma_e^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(E \left[ \left( \hat{f} \left(x_1^0, \ldots, x_m,^0 \right) - E \left[
    \hat{f}(x_1^0, \ldots, x_m,^0) \right] \right)^2 \right]\) is model variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum** (optimization)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): update the previous
    step with the new step, momentum, \(\lambda\), is the weight applied to the previous
    step while \(1 - \lambda\) is the weight applied to the current step,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \left( r \cdot \nabla L \right)_{t-1} \right)^m = \lambda \cdot r
    \cdot \nabla L_{t-2} + (1 - \lambda) \cdot r \cdot \nabla L_{t-1} \]
  prefs: []
  type: TYPE_NORMAL
- en: the gradients calculated from the partial derivatives of the loss function for
    each model parameter have noise. Momentum smooths out, reduces the impact of this
    noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: momentum helps the solution proceed down the general slope of the loss function,
    rather than oscillating in local ravines or dimples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markov Chain Monte Carlo** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Markov* - screening assumption, the next sample is only dependent on the previous
    sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chain* - the samples form a sequence often demonstrating a transition from
    burn-in chain with inaccurate statistics and equilibrium chain with accurate statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monte Carlo* - use of Monte Carlo simulation, random sampling from a statistical
    distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is this useful?
  prefs: []
  type: TYPE_NORMAL
- en: we often don‚Äôt have the target distribution, it is unknown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we can sample with the correct frequencies with other form of information
    such as conditional probability density functions, Gibbs sampler, or the likelihood
    ratios of the candidate next sample and the current sample, Metropolis-Hastings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metropolis-Hastings Sampling** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a set of algorithms to sample from a probability distribution such that the samples
    match the distribution statistics, based on,'
  prefs: []
  type: TYPE_NORMAL
- en: the likelihood ratios of the candidate next sample and the current sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a rejection sampler based on this likelihood ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since only the ratio of likelihood is required, the system is simplified as
    the evidence term cancels out from the Bayesian probability
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the basic steps of the Metropolis-Hastings MCMC Sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\ell = 1, \ldots, L\):'
  prefs: []
  type: TYPE_NORMAL
- en: Assign random values for the initial sample of model parameters, \(\beta(\ell
    = 1) = b_1(\ell = 1)\), \(b_0(\ell = 1)\) and \(\sigma^2(\ell = 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propose new model parameters based on a proposal function, \(\beta^{\prime}
    = b_1\), \(b_0\) and \(\sigma^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate probability of acceptance of the new proposal, as the ratio of the
    posterior probability of the new model parameters given the data to the previous
    model parameters given the data multiplied by the probability of the old step
    given the new step divided by the probability of the new step given the old.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\beta \rightarrow \beta^{\prime}) = min\left(\frac{P(\beta^{\prime}|y,X)
    }{ P(\beta | y,X)} \cdot \frac{P(\beta^{\prime}|\beta) }{ P(\beta | \beta^{\prime})},1\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply Monte Carlo simulation to conditionally accept the proposal, if accepted,
    \(\ell = \ell + 1\), and sample \(\beta(\ell) = \beta^{\prime}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Monte Carlo Simulation (MCS)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a random sample from a statistical distribution, random variable. The steps for
    MCS are:'
  prefs: []
  type: TYPE_NORMAL
- en: model the feature cumulative distribution function, \(F_x(x)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: draw random value from a uniform [0,1] distribution, this is a random cumulative
    probability value, known as a p-value, \(p^{\ell}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply the inverse of the cumulative distribution function to calculate the associated
    realization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x^{\ell} = F_x^{-1} (p^{\ell}) \]
  prefs: []
  type: TYPE_NORMAL
- en: repeat to calculate enough realizations for the subsequent analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monte Carlo simulation is the basic building block of stochastic simulation
    workflows, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*Monte Carlo simulation workflows* - apply Monte Carlo simulation many over
    all features to the transfer function to calculate a realization of the decision
    criteria, repeated for many realizations, to propagate uncertainty through a transfer
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bootstrap* - applies Monte Carlo simulation to acquire realizations of the
    data to calculate uncertainty in sample statistics or ensembles of prediction
    models for ensemble-based machine learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Monte Carlo methods* - applies Monte Carlo simulation to speed up an expensive
    calculation with a limited random sample that converges on the solution as the
    number of random samples increases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monte Carlo Simulation Workflow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    a convenient stochastic workflow for propagating uncertainty through a transfer
    function through sampling with Monte Carlo Simulation (MCS). The workflow includes
    the following steps,'
  prefs: []
  type: TYPE_NORMAL
- en: Model all the input features‚Äô distributions, cumulative distribution functions,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_{x_1}(x_1), \quad F_{x_2}(x_2), \quad \dots \quad , F_{x_m}(x_m) \]
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo simulate a realizations for all the inputs,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x_1^{\ell}, \quad x_2^{\ell}, \quad \ldots \quad , x_m^{\ell} \]
  prefs: []
  type: TYPE_NORMAL
- en: Apply to the transfer function to get a realization of the transfer function
    output, often the *decision criteria*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ y^{\ell} = f \left(x_1^{\ell},x_2^{\ell}, \quad \ldots \quad, x_m^{\ell}
    \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 1-3 to calculate enough realizations to model the transfer function
    output distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ F_y(y) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplication Rule** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): we can calculate
    the joint probability of \(A\) and \(B\) as the product of the conditional probability
    of \(B\) given \(A\) with the marginal probability of \(A\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: The multiplication rule is derived as a simple manipulation of the definition
    of conditional probability, in this case,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutual Information**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a generalized approach
    that quantifies the mutual dependence between two features.'
  prefs: []
  type: TYPE_NORMAL
- en: quantifies the amount of information gained from observing one feature about
    the other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: avoids any assumption about the form of the relationship (e.g. no assumption
    of linear relationship)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: units are Shannons or bits
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compares the joint probabilities to the product of the marginal probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: summarizes the difference between the joint \(P(x,y)\) and the product of the
    marginals \(P(x)\cdot P(y)\), integrated over all \(x \in ùëã\) and \(y \in Y\),
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For discrete or binned continuous features \(X\) and \(Y\), mutual information
    is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I(X;Y) = \sum_{y \in Y} \sum_{x \in X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: 'recall, given independence between \(X\) and \(Y\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y) \]
  prefs: []
  type: TYPE_NORMAL
- en: therefore if the two features are independent then the \(log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) = 0\)
  prefs: []
  type: TYPE_NORMAL
- en: The joint probability \(P_{X,Y}(x,y)\) is a weighting term on the sum and enforces
    closure.
  prefs: []
  type: TYPE_NORMAL
- en: parts of the joint distribution with greater density have greater impact on
    the mutual information metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For continuous (and nonbinned) features we can applied the integral form.
  prefs: []
  type: TYPE_NORMAL
- en: \[ I(X;Y) = \int_{Y} \int_{X}P_{X,Y}(x,y) log \left( \frac{P_{X,Y}(x,y)}{P_X(x)
    \cdot P_Y(y)} \right) dx dy \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutually Exclusive Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the events do not
    intersect, i.e., do not have any common outcomes. We represent this as,'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability for mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Multidimensional Scaling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multidimensional Scaling](MachineLearning_multidimensional_scaling.html):
    a method in inferential statistics / information visualization for exploring /
    visualizing the similarity (conversely the difference) between individual samples
    from a high dimensional dataset in a low dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional scaling (MDS) projects the \(m\) dimensional data to \(p\)
    dimensions such that \(p << m\).
  prefs: []
  type: TYPE_NORMAL
- en: while attempting to preserve the pairwise dissimilarity between the data samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ideally we are able to project to \(p=2\) to easily explore the relationships
    between the samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While principal component analysis (PCA) operates with the covariance matrix,
    multidimensional scaling operates with the distance or dissimilarity matrix. For
    multidimensional scaling,
  prefs: []
  type: TYPE_NORMAL
- en: you don‚Äôt need to know the actual feature values, just the distance or dissimilarity
    between the samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as with any distance in feature space, we consider feature standardization to
    ensure that features with larger variance do not dominate the calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we may work with a variety of dissimilarity measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison between multidimensional scaling and principal component analysis,
  prefs: []
  type: TYPE_NORMAL
- en: principal component analysis takes the covariance matrix (\(m \times m\)) between
    all the features and finds the linear, orthogonal rotation such that the *variance
    is maximized* over the ordered principle components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: multidimensional scaling takes the matrix of the pairwise distances (\(n \times
    n\)) between all the samples in feature space and finds the nonlinear projection
    such that the *error in the pairwise distances is minimized*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some have suggest that visualizing data or models in a multidimensional scaling
    space is visualizing the space of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): the application of the assumption
    of conditional independence to simplify the classification prediction problem
    from the perspective of Bayesian updating, based on the conditional probability
    of a category, \(k\), given \(n\) features, \(x_1, \dots , x_n\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_1 | x_2, \dots , x_n, C_k) P(x_2 | x_3, \dots , x_n, C_k) P(x_3 | x_4,
    \dots , x_n, C_k) \ldots P(x_{n-1} | x_n, C_k) (x_{n} | C_k) P(C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood, conditional probability with the joint conditional is difficult,
    likely impossible to calculate. It requires information about the joint relationship
    between \(x_1, \dots , x_n\) features. As \(n\) increases this requires a lot
    of data to inform the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: With the naive Bayes approach we make the ‚Äònaive‚Äô assumption that the features
    are all *conditionally independent**. This entails,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(x_i | x_{i+1}, \ldots , x_n, C_k) = P(x_i | C_k) \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i = 1, \ldots, n\) features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now solve for the needed conditional probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C_k | x_1, \dots , x_n) = \frac{P(C_k) \prod_{i=1}^{n} P(x_i | C_k)}{P(x_1,
    \dots , x_n)} \]
  prefs: []
  type: TYPE_NORMAL
- en: We only need the prior, \(P(C_k)\), and a set of conditionals, \(P(x_i | C_k)\),
    for all predictor features, \(i = 1,\ldots,n\) and all categories, \(k = 1,\ldots,K\).
  prefs: []
  type: TYPE_NORMAL
- en: The evidence term, \(P(x_1, \dots , x_n)\), is only based on the features \(x_1,
    \dots , x_n\); therefore, is a constant over the categories \(k = 1,\ldots,n\).
  prefs: []
  type: TYPE_NORMAL
- en: it ensures closure - probabilities over all categories sum to one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we simply standardize the numerators to sum to one over the categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The naive Bayes approach is:'
  prefs: []
  type: TYPE_NORMAL
- en: simple to understand, builds on fundamental Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: practical even with small datasets since with the conditional independence we
    only need to estimate simple conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ndarray**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: Numpy‚Äôs convenient class
    for working with grids, exhaustive, regularly spaced data over 2D or 3D, representing
    maps and models, due to,'
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate gridded data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods to load from a variety of file types, Python classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods to calculate multidimensional summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods for data queries, filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built in attributes to store information about the nD array, for example, size
    and shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nonparametric Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that makes
    no assumption about the functional form, shape of the natural setting.'
  prefs: []
  type: TYPE_NORMAL
- en: learns the shape from the training data, more flexibility to fit a variety of
    shapes for natural systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: less risk that the model is a poor fit for the natural settings than with parametric
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically need a lot more data for an accurate estimate of nonparametric models,
  prefs: []
  type: TYPE_NORMAL
- en: nonparametric often have many trainable parameters, i.e., nonparametric models
    are actually parametric rich!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Norm**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): norm of a vector
    maps vector values to a summary measure \([ùüé,\infty)\), that indicates size or
    length.'
  prefs: []
  type: TYPE_NORMAL
- en: To train our models to training data, we require a single summary measure of
    mismatch with the training data, training error. The error is observed at each
    training data location,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Delta y_i = y_i - \hat{y}_i, \quad \forall \quad i = 1,\ldots,n \]
  prefs: []
  type: TYPE_NORMAL
- en: as an error vector. We need a single value to summarize over all training data,
    that we can minimize!
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*) to a minimum of 0.0 and a maximum
    of 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: this is a shift and stretch / squeeze of the original property distribution
    assumes no shape change, rank preserving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y_i = \frac{x_i - min(x)}{max(x) - min(x)}, \quad \forall \quad i, \ldots,
    n \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Methods that require standardization and min/max normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering, k-nearest neighbour regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\beta\) coefficient‚Äôs for feature ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: artificial neural networks forward transform of predictor features and back
    transform of response features to improve activation function sensitivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalized Histogram**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): is a representation
    of the univariate statistical distribution with a plot of probability over an
    exhaustive set of bins over the range of possible values. These are the steps
    to build a normalized histogram,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the continuous feature range of possible values into \(K\) equal size
    bins, \(\delta x\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \Delta x = \left( \frac{x_{max} - x_{min}}{K} \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: or use available categories for categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Count the number of samples (frequency) in each bin, \(n_k\), \(\forall k=1,\ldots,K\)
    and divide each by the total number of data, \(n\), to calculate the probability
    of each bin,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ p_k = \frac{n_k}{n}, \forall \quad k = 1,\ldots,L \]
  prefs: []
  type: TYPE_NORMAL
- en: Plot the probability vs. the bin label (use bin centroid if continuous)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note, normalized histograms are typically plotted as a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: '**One Hot Encoding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): bin
    the range of the feature into K bins, then for each sample assignment of a value
    of 1 if the sample is within a bin and 0 if outsize the bin'
  prefs: []
  type: TYPE_NORMAL
- en: binning strategies include uniform width bins (uniform) and uniform number of
    data in each bin (quantile)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also known as K bins discretization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods that require K bins discretization,
  prefs: []
  type: TYPE_NORMAL
- en: basis expansion to work in a higher dimensional space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: discretization of continuous features to categorical features for categorical
    methods such as naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: histogram construction and Chi-square test for difference in distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mutual information binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-Bag Sample**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): with
    bootstrap resampling of the data, it can be shown that about \(\frac{2}{3}\) of
    the data will be included (in expectation). For bagging-based ensemble prediction
    models,'
  prefs: []
  type: TYPE_NORMAL
- en: therefore are \(\frac{1}{3}\) of the data (in expectation) unused in training
    each model realization, these are know as out-of-bag observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for every response feature observation, \(y_{\alpha}\), there are \(\frac{B}{3}\)
    out-of-bag predictions, \(y^{*,b}_{\alpha}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can aggregate this ensemble of prediction realizations, average for regression
    or mode for classification, to calculate a single out-of-bag prediction, \(y^{*}_{\alpha}
    = \sum_{\alpha = 1}^{\frac{B}{3}} y^{*,b}_{\alpha}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: from these single out-of-bag predictions over all data, the out-of-bag mean
    square error (MSE) is calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ MSE_{OOB} = \sum_{\alpha = 1}^{\frac{B}{3}} \left[ y^{*}_{\alpha} - y_{\alpha}
    \right]^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: For bagging-based ensemble predictive machine learning, there is no need to
    perform training and testing splits, hyperparameter tuning can be applied with
    out-of-bag MSE.
  prefs: []
  type: TYPE_NORMAL
- en: this is equivalent to random train and test split that may not be fair, same
    difficulty as the planned use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this freezes the test proportion at about \(\frac{1}{3}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfit Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a machine learning
    model that is fit to data noise or data idiosyncrasies'
  prefs: []
  type: TYPE_NORMAL
- en: increased complexity will generally decrease error with respect to the training
    dataset but, may result in increase error with testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: over the region of model complexity with rising testing error and falling training
    error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues of an overfit machine learning model,
  prefs: []
  type: TYPE_NORMAL
- en: more model complexity and flexibility than can be justified with the available
    data, data accuracy, frequency and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: high accuracy in training, but low accuracy in testing representing real-world
    use away from training data cases, indicating poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters** (statistics)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a summary measure
    of a population'
  prefs: []
  type: TYPE_NORMAL
- en: for example, population mean, population standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We very rarely have access to actual population parameters, in general we infer
    population parameters with available sample statistics
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters** (machine learning)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): trainable coefficients
    for a machine learning model that control the fit to the training data'
  prefs: []
  type: TYPE_NORMAL
- en: model parameters are calculated by optimization to minimize error over the training
    data through, analytical solution, or iterative solution, e.g., gradient descent
    optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a model that makes
    an assumption about the functional form, shape of the natural system.'
  prefs: []
  type: TYPE_NORMAL
- en: we gain simplicity and advantage of only a few parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for is a linear model we only have \(m+1\) model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a risk that our model is quite different than the natural setting,
    resulting in a poor model, for example, a linear model applied to a nonlinear
    phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: '**Partial Correlation Coefficient**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): a method
    to calculate the correlation between \(ùëø\) and \(ùíÄ\) after controlling for the
    influence of \(ùíÅ_ùüè,\ldots,ùíÅ_(ùíé‚àíùüê)\) other features on both \(ùëø\) and \(ùëå\). Note,
    I use \(m-2\) to account for \(X\) and \(Y\) removed.'
  prefs: []
  type: TYPE_NORMAL
- en: For \(\rho_(ùëã,ùëå.ùëç_1,‚Ä¶,ùëç_(ùëö‚àí2) )\),
  prefs: []
  type: TYPE_NORMAL
- en: perform linear, least-squares regression to predict \(ùëø\) from \(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\).
    \(ùëø\) is regressed on the predictors to calculate the estimate, \(ùëø^‚àó\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: perform linear, least-squares regression to predict \(ùíÄ\) from \(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\).
    \(ùíÄ\) is regressed on the predictors to calculate the estimate, ùíÄ^‚àó
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the residuals in Step #1, \(ùëø ‚àí ùëø^‚àó\), where \(ùëø^‚àó=ùíá(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
    linear regression model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the residuals in Step #2, \(ùíÄ ‚àí ùíÄ^‚àó\), where \(ùíÄ^‚àó=ùíá(ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
    linear regression model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'calculate the correlation coefficient between the residuals from Steps #3 and
    #4, \(\rho_{ùëø ‚àíùëø^‚àó,ùíÄ ‚àí ùíÄ^‚àó}\)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assumptions of Partial Correlation, for \(ùùÜ_(ùëø,ùíÄ.ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê})\),
  prefs: []
  type: TYPE_NORMAL
- en: \(ùëø,ùíÄ,ùíÅ_ùüè,\ldots,ùíÅ_{ùíé‚àíùüê}\) have linear relationships, i.e., all pairwise relationships
    are linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no outliers for any of the univariate distributions (univariate outliers) and
    pairwise relationships (bivariate outliers). Partial correlation is very sensitive
    to outliers like regular correlation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian distributed, univariate and pairwise bivariate distributions Gaussian
    distributed. Bivariate should be linearly related and homoscedastic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitional Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: all cluster group assignments are determined at once, as
    opposed to an agglomerative hierarchical clustering method that starts with \(n\)
    clusters and then iteratively merges clusters into larger clusters'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering is partitional clustering, while the solution heuristic to
    find the solution is iterative, the solution is actually all at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: easy to update, for example, by modifying the prototype locations and recalculating
    the group assignments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polygonal Declustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: a declustering method to assign weights to spatial samples
    based on local sampling density, such that the weighted statistics are likely
    more representative of the population. Data weights are assigned so that,'
  prefs: []
  type: TYPE_NORMAL
- en: samples in densely sampled areas receive less weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: samples in sparsely sampled areas receive more weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polygonal declustering proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split up the area of interest with Voronoi polygons. These are constructed by
    intersected perpendicular bisectors between adjacent data points. The polygons
    group the area of interest by nearest data point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign weight to each datum proportional to the area of the associated Voronoi
    polygon
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w(\bf{u}_j) = n \cdot \frac{A_j}{\sum_{j=1}^n} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(w(\bf{u}_j)\) is the weight for the \(j\) data. Note, the sum of the
    weights is \(n\); therefore, \(w(\bf{u}_j)\) is nominal weight of 1.0, sample
    density if the data were equally spaced over the area of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some highlights for polygonal declustering,
  prefs: []
  type: TYPE_NORMAL
- en: polygonal declustering is sensitive to the boundaries of the area of interest;
    therefore, the weights assigned to the data near the boundary of the area of interest
    may change radically as the area of interest is expanded or contracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: polygonal declustering is the same as the Theissen polygon method for calculation
    of precipitation averages developed by Afred H. Thiessen in 1911, []
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial Regression**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Polynomial Regression](MachineLearning_polynomial_regression.html): application
    of polynomial basis expansion to the predictor features before linear regression,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{l=1}^{k} \sum_{j=1}^{m} \beta_{j,l} h_l (X_j) + \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where the h transforms over training data, \(ùëñ=1,\ldots,n\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ h_1(x_i) = x_i, \quad h_2(x_i) = x_i^2, \quad h_3(x_i) = x_i^3, \quad h_4(x_i)
    = x_i^4, \dots, h_k(x_i) = x_i^k \]
  prefs: []
  type: TYPE_NORMAL
- en: up to the specified order \(ùëò\).
  prefs: []
  type: TYPE_NORMAL
- en: For example, with a single predictor feature, \(ùëö = 1\), up to the \(4^{th}\)
    order,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \beta_{1,1} X + \beta_{1,2} X^2 + \beta_{1,3} X^3 + \beta_{1,4} X^4 +
    \beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: After the \(ùíâ_ùíç\), \(ùëô=1,\ldots,ùëò\) transforms, over the \(ùëó=1,\dots,ùëö\) predictor
    features we have the same linear equation and the ability to utilize the previously
    discussed analytical solution.
  prefs: []
  type: TYPE_NORMAL
- en: we are assuming linearity after application of our basis expansion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now the model parameters, \(\beta_(ùíç,ùíä)\), relate to a transformed version of
    the initial predictor feature, \(ùíâ_ùíç (ùëø_ùíã)\).
  prefs: []
  type: TYPE_NORMAL
- en: we lose the ability to interpret the coefficients, for example, what is ùëùùëíùëüùëöùëíùëéùëèùëñùëôùëñùë°ùë¶\(^4\)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generally, significantly higher model variance, i.e., may have unstable interpolation
    and especially extrapolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression model assumptions,
  prefs: []
  type: TYPE_NORMAL
- en: '*error-free* - predictor features basis expansions are error free, not random
    variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*constant variance* - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*linearity* - response is linear combination of basis features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*polynomial* - relationships between ùëã and Y is polynomial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*independence of error* - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: no multicollinearity* - none of the basis feature expansions are linearly redundant
    with other features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Population**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): exhaustive, finite
    list of property of interest over area of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: for example, exhaustive set of porosity measures at every location within a
    reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, the entire population is not generally accessible and we use a limited
    sample to make inference concerning the population
  prefs: []
  type: TYPE_NORMAL
- en: '**Power Law Average**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    general form for averaging based scale up, aggregation of smaller scale measures
    in a larger volume into a single value representative of the larger volume'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \overline{x}_p = \left(\frac{1}{n}\sum_{i=1}^n x_i^p \right)^{\frac{1}{p}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: useful to calculate effective permeability where flow is not parallel nor perpendicular
    to distinct permeability layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flow simulation may be applied to calibrate (calculate the appropriate power
    for power law averaging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the ratio of true positives divided by all positives, true positives + false
    positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Precision_k = \frac{ n_{k,\text{true positives}} }{ n_{k,\text{true positives}}
    + n_{k,\text{false positives}}} = \frac{ n_{k,\text{true positives}} }{ n_{k,
    \text{all positives}} } \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction Interval**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the uncertainty
    in the next prediction represented as a range, lower and upper bound, based on
    a specified probability interval known as the confidence level.'
  prefs: []
  type: TYPE_NORMAL
- en: We communicate confidence intervals like this,
  prefs: []
  type: TYPE_NORMAL
- en: there is a 95% probability (or 19 times out of 20) that the true reservoir NTG
    is between 13% and 17%, given, predictor feature values, \(ùëã_1=ùë•_1,\ldots,ùëã_ùëö=ùë•_ùëö\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the uncertainty in our prediction, for prediction intervals we integrate,
  prefs: []
  type: TYPE_NORMAL
- en: uncertainty in the model \(ùê∏{\hat{ùëå}|ùëã=ùë•}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: error in the model, conditional distribution \(\hat{Y}|X=x\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction, Predictive Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): estimate the next
    sample(s) given assumptions about or a model of the population'
  prefs: []
  type: TYPE_NORMAL
- en: for example, given our model of the reservoir, predict the next well (pre-drill
    assessment) sample, e.g., porosity, permeability, production rate, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictor Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the input feature
    for a predictive machine learning model. We can generalize a predictive machine
    learning model as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \hat{f}(x_1,\ldots,x_m) + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: where the response feature is \(y\), the predictor features are \(x_1,\ldots,x_m\),
    and \(\epsilon\) is model error
  prefs: []
  type: TYPE_NORMAL
- en: traditional statistics uses the term independent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictor Feature Space**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): refers to the predictor
    features and does not include the response feature(s), i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: all possible combinations of predictor features for which we need to make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: may be referred to as predictor feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we train and test our machines‚Äô predictions over the predictor feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: the space is typically a hypercuboid with each axis representing a predictor
    feature and extending from the minimum to maximum, over the range of each predictor
    feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more complicated shapes of predictor feature space are possible, e.g., we could
    mask or remove subsets with poor data coverage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primary Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data samples for
    the feature of interest, the target feature for building a model, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity measures from cores and logs used to build a full 3D porosity model.
    Any samples of porosity are the primary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as opposed to secondary feature, e.g., if we have facies data to help predict
    porosity, the facies data are secondary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](MachineLearning_PCA.html): one of a variety
    of methods for dimensional reduction, transform the data to a lower dimension'
  prefs: []
  type: TYPE_NORMAL
- en: given features, \(ùëã_1,\dots,ùëã_ùëö\) we would require \({m \choose 2}=\frac{ùëö \cdot
    (ùëö‚àí1)}{2}\) scatter plots to visualize just the two-dimensional scatter plots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once we have 4 or more variables understanding our data gets very hard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recall the curse of dimensionality, impact inference, modeling and visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One solution, is to find a good lower dimensional, \(ùëù\), representation of
    the original dimensions \(ùëö\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefits of Working in a Reduced Dimensional Representation:'
  prefs: []
  type: TYPE_NORMAL
- en: data storage / Computational Time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: easier visualization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: also takes care of multicollinearity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Salient points of principal component analysis,
  prefs: []
  type: TYPE_NORMAL
- en: '*orthogonal transformation* - convert a set of observations into a set of linearly
    uncorrelated variables known as principal components, the transformation retains
    pairwise distance, i.e., is a rotation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*number of principal components (\(k\)) available* - is min‚Å°(\(ùëõ‚àí1,ùëö\)), limited
    by the variables/features, \(ùëö\), and the number of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components are ordered,
  prefs: []
  type: TYPE_NORMAL
- en: first component describes the larges possible variance / accounts for as much
    variability as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: next component describes the largest possible remaining variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: up to the maximum number of principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors-based,
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the data covariance matrix, the pairwise covariance for the combinatorial
    of features and then calculate the eigenvectors and eigenvalues from the covariance
    matrix,
  prefs: []
  type: TYPE_NORMAL
- en: the eigenvalues are the variance explained for each component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the eigenvectors of the data covariance matrix are the principal components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability Density Function** (PDF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): a representation
    of a statistical distribution with a function, \(f(x)\), of probability density
    over the range of all possible feature values, \(x\). These are the concepts for
    PDFs,'
  prefs: []
  type: TYPE_NORMAL
- en: non-negativity constraint, the density cannot be negative,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0.0 \le f(x) \]
  prefs: []
  type: TYPE_NORMAL
- en: for continuous features the density may be > 1.0, because density is a measure
    of likelihood and not of probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integrate density over a range of \(x\) to calculate probability,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0 \le \int_a^b f(x) dx = P(a \le x \le b) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability closure, the sum of the area under the PDF curve is equal to 1.0,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \int_{-infty}^{\infty} f(x) dx = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Nonparametric PDFs are calculated with kernels (usual a small Gaussian distribution)
    that is summed over all data; therefore, there is an implicitly scale (smoothness)
    parameter when calculating a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: To large of kernels will smooth out important information about the univariate
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too narrow will result in an overly noisy PDF that is difficult to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is analogous to the choice of bin size for a histogram or normalized histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric PDFs are possible but require model fitting to the data, the steps
    are,
  prefs: []
  type: TYPE_NORMAL
- en: Select a parametric distribution, e.g., Gaussian, log normal, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the parameters for the parametric distribution based on the available
    data, by methods such as least squares or maximum likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probability Non-negativity, Normalization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probability Concepts: fundamental constraints on probability including,'
  prefs: []
  type: TYPE_NORMAL
- en: Bounded, \(0.0 \le P(A) \le 1.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Closure, \(P(\Omega) = 1.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Null Sets, \(P(\emptyset) = 0.0\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probability of Acceptance** (MCMC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bayesian Linear Regression](MachineLearning_Bayesian_linear_regression.html):
    applied in a rejection sampler as the likelihood of a candidate sample being added
    to the sample.'
  prefs: []
  type: TYPE_NORMAL
- en: conditional acceptance is performed by Monte Carlo simulation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sequentially sampling from conditional distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The acceptance rule is,
  prefs: []
  type: TYPE_NORMAL
- en: if \(ùëÉ(ùëéùëêùëêùëíùëùùë°) \ge 1\), accept ‚Äì accept
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if \(ùëÉ(ùëéùëêùëêùëíùëùùë°) \lt 1\), conditionally accept, draw \(ùëù ‚àº U[0,1]\), and accept
    if \(ùëù \le ùõº\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability Operators**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): common probability
    operators that are essential to working with probability and uncertainty problems,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Union of Events* - the union of outcomes, the probability of \(A\) or \(B\)
    is calculated with the probability addition rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Intersection of Events* - the intersection of outcomes, the probability of
    \(A\) and \(B\) is represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: only under the assumption of independence of \(A\) and \(B\) can it be calculate
    from the probabilities of \(A\) and \(B\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: if there is dependence between \(A\) and \(B\) then we need the conditional
    probability, \(P(A|B)\) instead of the marginal, \(P(A)\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '*Complimentary Events* - is the NOT operator for probability, if we define
    \(A\) then \(A\) compliment, \(A^c\) is not \(A\) and we have this resulting closure
    relationship,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: complimentary events may be considered for beyond univariate problems, for example
    consider this bivariate closure,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) + P(A^c|B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, the given term must be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '*Mutually Exclusive Events* - the events that do not intersect or do not have
    any common outcomes. We represent this with set notation as,'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the joint probability of \(A\) and \(B\) as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability Perspectives**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the 3 primary perspectives
    for calculating probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Long-term frequencies* - probability as ratio of outcomes, requires repeated
    observations of an experiment. The basis for *frequentist probability*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Physical tendencies or propensities* - probability from knowledge about or
    modeling the system, e.g., we could know the probability of a heads outcome from
    a coin toss without the experiment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Degrees of belief* - reflect our certainty about a result, very flexible,
    assign probability to anything, and updating with new information. The basis for
    *Bayesian probability*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prototype** (clustering)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: represent the sample data with set of points in the feature
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: prototypes are typically not actual samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample data are often assigned to the nearest (Euclidean) distance prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualitative Features**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): information about
    quantities that you cannot directly measure, require interpretation of measurement,
    and are described with words (not numbers), for example,'
  prefs: []
  type: TYPE_NORMAL
- en: rock type = sandstone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: zonation = bornite-chalcopyrite-gold higher grade copper zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative Features**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): features that can
    be measured and represented by numbers, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: age = 10 Ma (millions of years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity = 0.134 (fraction of volume is void space)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: saturation = 80.5% (volume percentage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like *qualitative features*, there is often the requirement for interpretation,
    for example, total porosity may be measured but should be converted to effective
    porosity through interpretation or a model
  prefs: []
  type: TYPE_NORMAL
- en: '**\(r^2\)** (also coefficient of determination)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear Regression](MachineLearning_linear_regression.html): the proportion
    of variance explained by the model in linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'This works only for linear models, where:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma^2_{tot} = \sigma^2_{reg} + \sigma^2_{res} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma^2_{tot}\) is variance of response feature training, \(y_i\),
    \(\sigma^2_{reg}\) is variance of the model predictions, \(\hat{y}_i\), and \(\sigma^2_{res}\)
    is the variance of the errors, \(\Delta y_i\).
  prefs: []
  type: TYPE_NORMAL
- en: for linear regression, \(r^2 = \left( \rho_{x,y} \right)^2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For nonlinear models this will not likely hold, then \(\frac{\sigma^2_{ùëüùëíùëî}}{\sigma^2_{ùë°ùëúùë°}}\)
    may exceed \([0,1]\), for our nonlinear models regression models we will use more
    robust measures, e.g. mean square error (MSE)
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forest**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Bagging Tree and Random Forest](MachineLearning_ensemble_trees.html): a ensemble
    prediction model that is based on the standard bagging approach, specifically,'
  prefs: []
  type: TYPE_NORMAL
- en: with decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with diversification of the individual trees by restricting each split to consider
    a \(p\) random subset of the \(ùëö\) available predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are various methods to calculate \(p\) from \(m\) available features,
  prefs: []
  type: TYPE_NORMAL
- en: \[ p = \sqrt{m} \]
  prefs: []
  type: TYPE_NORMAL
- en: is common. Note, if \(p = m\) then random forest is tree bagging.
  prefs: []
  type: TYPE_NORMAL
- en: More comments on the benefit of ensemble model diversification,
  prefs: []
  type: TYPE_NORMAL
- en: the reduction in model variance by ensemble estimation, as represented by standard
    error in the mean,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma_{\overline{x}}^2 = fracc{\sigma_{s}^2}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: is under the assumption that the samples are uncorrelated. One issue with tree
    bagging is the trees in the ensemble may be highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: this occurs when there is a dominant predictor feature as it will always be
    applied to the top split(s), the result is all the trees in the ensemble are very
    similar (i.e. correlated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with highly correlated trees, there is significantly less reduction in model
    variance with the ensemble
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this forces each tree in the ensemble to evolve in dissimilar, decorrelated,
    manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): an outcome from
    a *random variable* or a joint outcome from a *random function*.'
  prefs: []
  type: TYPE_NORMAL
- en: an outcome from a random variable, \(X\), (or joint set of outcomes from a random
    function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: represented with lower case, e.g., \(x\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for spatial settings it is common to include a location vector, \(\bf{u}\),
    to describe the location, e.g., \(x(\bf{u})\), as \(X(\bf{u})\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: resulting from simulation, e.g., Monte Carlo simulation, sequential Gaussian
    simulation, a method to sample (jointly) from the RV (RF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in general, we assume all realizations are equiprobable, i.e., have the same
    probability of occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Realizations** (uncertainty)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): multiple spatial,
    subsurface models calculated by stochastic simulation by holding input parameters
    and model choices constant and only changing the random number seed'
  prefs: []
  type: TYPE_NORMAL
- en: these models represent spatial uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, hold the porosity mean constant and observe changes in porosity
    away from the wells over multiple realizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasons to Learn Some Coding**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Workflow Construction and Coding](MachineLearning_workflow_construction.html):
    Professor Pyrcz‚Äôs reasons for all scientists and engineers to learn some coding,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transparency* ‚Äì no compiler accepts hand waiving! Coding forces your logic
    to be uncovered for any other scientist or engineer to review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reproducibility* ‚Äì run it and get an answer, hand it over to a peer, they
    run it and they get the same answer. This is a principle of the scientific method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quantification* ‚Äì programs need numbers and drive us from qualitative to quantitative.
    Feed the program and discover new ways to look at the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open-source* ‚Äì leverage a world of brilliance. Check out packages, snippets
    and be amazed with what great minds have freely shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Break Down Barriers* ‚Äì don‚Äôt throw it over the fence. Sit at the table with
    the developers and share more of your subject matter expertise for a better deployed
    product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deployment* ‚Äì share your code with others and multiply your impact. Performance
    metrics or altruism, your good work benefits many others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficiency* ‚Äì minimize the boring parts of the job. Build a suite of scripts
    for automation of common tasks and spend more time doing science and engineering!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Always Time to Do it Again!* ‚Äì how many times did you only do it once? It
    probably takes 2-4 times as long to script and automate a workflow. Usually, worth
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be Like Us ‚Äì it will change you. Users feel limited, programmers truly harness
    the power of their applications and hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** (classification accuracy metric)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Naive Bayes](MachineLearning_naive_Bayes.html): a categorical classification
    prediction model measure of accuracy, a single summary metric for each \(k\) category
    from the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: the ratio of true positives divided by all cases of the category in the testing
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ Recall_k = \frac{ n_{k, \text{true positives}} }{n_k} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Recursive Feature Elimination**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a method works by
    recursively removing features and building a model with the remaining features.'
  prefs: []
  type: TYPE_NORMAL
- en: build a model with all features, calculate a feature ranking metric, e.g., coefficient
    or feature importance, depending on which is available with the modeling method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: remove the feature with the lowest feature importance and rebuild the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat the process until only one feature remains
  prefs: []
  type: TYPE_NORMAL
- en: Any model predictive model could be used,
  prefs: []
  type: TYPE_NORMAL
- en: the method assigns rank \(1,\ldots,ùëö\) for all features as reverse order of
    removal, i.e., last remaining feature is most important and first removed is least
    important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reservoir Modeling Workflow**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Workflow Construction and Coding](MachineLearning_workflow_construction.html):
    the following is the common geostatistical reservoir modeling workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate all available information to build multiple subsurface scenarios and
    realizations to sample the uncertainty space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply all the models to the transfer function to sample the decision criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assemble the distribution of the decision criteria
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the optimum reservoir development decisions accounting for this uncertainty
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Response Feature**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): output feature
    for a predictive machine learning model. We can generalize a predictive machine
    learning model as,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \hat{f}(x_1,\ldots,x_m) + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: where the response feature is \(y\), the predictor features are \(x_1,\ldots,x_m\),
    and \(\epsilon\) is model error
  prefs: []
  type: TYPE_NORMAL
- en: traditional statistics uses the term dependent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge Regression** (Tikhonov Regularization)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ridge Regression](MachineLearning_ridge_regression.html): a linear, parametric
    prediction model,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize a loss function including the error, residual sum of squares (RSS)
    over the training data and a regularization term:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i} + b_0)
    \right)^2 + \lambda \sum_{\alpha = 1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data, and \(\lambda \sum_{\alpha = 1}^m b_{\alpha}^2\) is the shrinkage
    penalty.
  prefs: []
  type: TYPE_NORMAL
- en: With ridge regression we add a hyperparameter, \(\lambda\), to our minimization,
    with a shrinkage penalty term, \(\sum_{j=1}^m b_{\alpha}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: As a result, ridge regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: lambda does not include the intercept, \(b_0\).'
  prefs: []
  type: TYPE_NORMAL
- en: The \(\lambda\) is a hyperparameter that controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the solution approaches linear regression, there
    is no bias (relative to a linear model fit), but the model variance is likely
    higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases,
    the model becomes simpler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the model parameters \(b_1,\ldots,b_m\) shrink
    to 0.0 and the model predictions approaches the training data response feature
    mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the set of values,
    locations that have been measured'
  prefs: []
  type: TYPE_NORMAL
- en: for example, 1,000 porosity measures from well-logs over the wells in the reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: or 1,000,000 acoustic impedance measurements over a 1000 x 1000 2D grid for
    a reservoir unit of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenarios** (uncertainty)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): multiple spatial,
    subsurface models calculated by stochastic simulation by changing the input parameters
    or other modeling choices to represent the uncertainty due to inference of model
    parameters and model choices'
  prefs: []
  type: TYPE_NORMAL
- en: for example, model three porosity input distribution, porosity mean low, mid
    and high, and vary the input distribution to calculate new subsurface models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data samples for
    another feature, not the feature of interest, the target feature for building
    a model, but are used to improve the prediction of the target feature.'
  prefs: []
  type: TYPE_NORMAL
- en: requires a model of the relationship between the primary and secondary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, samples in space of,
  prefs: []
  type: TYPE_NORMAL
- en: acoustic impedance (secondary data) to support calculation of a model of porosity,
    the feature of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (secondary data) to support calculation of a model of permeability,
    the feature of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seismic Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): indirect measurement
    with remote sensing, reflection seismic applies acoustic source(s) and receivers
    (geophones) to map acoustic reflections with high coverage and generally low resolution.
    Some more details,'
  prefs: []
  type: TYPE_NORMAL
- en: seismic reflections (amplitude) data are inverted to rock properties, e.g.,
    acoustic impedance, consistent with and positionally anchored with well sonic
    logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: provides framework, bounding surfaces for extents and shapes of reservoirs along
    with soft information on reservoir properties, e.g., porosity and facies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shapley Value**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): model-based, local
    (for a single prediction) and global (over a suit of predictions) feature importance
    by learning contribution of each feature to the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: A explainable machine learning method to support complicated models are often
    required but have low interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Two choices to improve model interpretability,
  prefs: []
  type: TYPE_NORMAL
- en: reduce the complexity of the models, but may also reduce model accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: develop improved, agnostic (for any model) model diagnostics, i.e., Shapley
    value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shapley value is a cooperative game theory approach that,
  prefs: []
  type: TYPE_NORMAL
- en: for allocating resources between players based on a summarization of marginal
    contributions, i.e., dividing up payment between players
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculates the contribution of each predictor feature to push the response prediction
    away from the mean value of the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: marginal contributions and Shapley values are in units of the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in the units of the response feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simpson‚Äôs Paradox**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Multivariate Analysis](MachineLearning_multivariate_analysis.html): data trend
    reverses or disappears when groups are combined (or separated). Often observed
    in correlation analysis when grouping data, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: groups each have a negative correlation, but the whole has a positive correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soft Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): data that has a
    high degree of uncertainty, such that data uncertainty must be integrated into
    the model'
  prefs: []
  type: TYPE_NORMAL
- en: for example, probability density function for local porosity calibrated from
    acoustic impedance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft data integration requires workflows like *indicator kriging*, *indicator
    simulation* and *p-field simulation* or workflows that randomize the data with
    standard simulation methods that assume hard data like *sequential Gaussian simulation*.
  prefs: []
  type: TYPE_NORMAL
- en: soft data integration is an advanced topic and a focus of ongoing research,
    but is often to done with standard, subsurface modeling software packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (biased)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: sample such that the sample statistics are not representative
    of the population parameters. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: the sample mean is not the same as the population mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the sample variance is not the same as the population variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the population parameters are not accessible, so we cannot directly
    calculate sampling bias, i.e., the difference between the sample statistics and
    the population parameters. Methods we can use to check for biased sampling,
  prefs: []
  type: TYPE_NORMAL
- en: evaluate the samples for preferential sampling, clustering, filtering, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apply *declustering* and check the results for a major change in the summary
    statistics, this is using declustering diagnostically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (clustered)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: spatial samples with locations preferentially selected, i.e.,
    clustered, resulting in biased statistics,'
  prefs: []
  type: TYPE_NORMAL
- en: typically spatial samples are clustered in locations with higher value samples,
    e.g., high porosity and permeability, good quality shale for unconventional reservoirs,
    low acoustic impedance indicating higher porosity, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the population parameters are not accessible, so we cannot directly
    calculate sampling bias, i.e., the difference between the sample statistics and
    the population parameters. Methods we can use to check for biased sampling,
  prefs: []
  type: TYPE_NORMAL
- en: evaluate the samples for preferential sampling, clustering, filtering, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: apply *declustering* and check the results for a major change in the summary
    statistics, this is using declustering diagnostically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (common practice)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: sample locations are selected to,'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reduce uncertainty* - by answering questions, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: how far does the contaminant plume extend? ‚Äì sample peripheries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where is the fault? ‚Äì drill based on seismic interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what is the highest mineral grade? ‚Äì sample the best part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: who far does the reservoir extend? ‚Äì offset drilling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Directly maximize net present value* - while collecting information, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: maximize production rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maximize tonnage of mineral extracted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, often our samples are dual purpose, e.g., wells that are drilled
    for exploration and appraisal information are subsequently utilized for production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial Sampling** (representative)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data Preparation: if we are sampling for representativity, i.e., the sample
    set and resulting sample statistics are representative of the population, by sampling
    theory we have 2 options:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Random sampling* - each potential sample from the population is equally likely
    to be sampled as samples are collected. This includes,'
  prefs: []
  type: TYPE_NORMAL
- en: selecting a specific location has no impact on the selection of subsequent locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumption that the population size that is much larger than the sample size;
    therefore, significant correlation between samples is not imposed due to without
    replacement sampling (the constraint that you can only sample a location once).
    Note, generally this is not an issue for the subsurface due to the sparsely sampled
    massive populations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regular sampling* - sampling at equal space or time intervals. While random
    sampling is preferred, regular sampling is robust as long as,'
  prefs: []
  type: TYPE_NORMAL
- en: the regular sampling intervals do not align with natural periodicity in the
    data, for example, the crests are systematically samples resulting in biased high
    sample statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral Clustering**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Spectral Clustering](MachineLearning_spectral_clustering.html): a partitional
    clustering method that utilizes the spectrum, eigenvalues and eigenvectors, of
    a matrix that represents the pairwise relationships between the data.'
  prefs: []
  type: TYPE_NORMAL
- en: dimensionality reduction from data samples pairwise relationships characterized
    by the graph Laplacian matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues, eigenvectors are equivalent to principal component analysis dimensionality
    reduction by linear, orthogonal feature projection and rotation to best describe
    the variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of spectral clustering,
  prefs: []
  type: TYPE_NORMAL
- en: the ability to encode pairwise relationships, integrate expert knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues provide useful information on the number of clusters, based on the
    degree of ‚Äòcutting‚Äô required to make k clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lower dimensional representation for the sample data pairwise relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the resulting eigenvalues and eigenvectors can be interpreted, eigenvalues describe
    the amount of connection for each number of groups and eigenvectors are grouped
    to form the clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): a
    distribution rescaling that can be thought of as shifting, and stretching or squeezing
    of a univariate distribution (e.g., *histogram*) to a mean of 0.0 and a variance
    of 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i = \frac{1}{\sigma_x}(x_i - \overline{x}), \quad \forall \quad i, \ldots,
    n \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\overline{x}\) and \(\sigma_x\) are the original mean and variance.
  prefs: []
  type: TYPE_NORMAL
- en: this is a shift and stretch / squeeze of the original property distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: assumes no shape change, rank preserving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient-based Optimization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LASSO Regression](MachineLearning_LASSO_regression.html): a method to solve
    for model parameters by iteratively minimizing the loss function. Stochasticity
    and improve computational efficiency are added to gradient descent through the
    use of batches,'
  prefs: []
  type: TYPE_NORMAL
- en: a batch is a random subset of the training data with specified size, \(n_{batch}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: resulting in stochastic approximations of the loss function gradient, that are
    faster to calculate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: batches reduce accuracy in the gradient descent, but speed up the calculation
    and can perform more steps, often faster than gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increase \(ùëõ_{ùëèùëéùë°ùëê‚Ñé}\) for more accuracy of gradient estimation, and decrease
    \(ùëõ_{ùëèùëéùë°ùëê‚Ñé}\) to speed up the steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robbins-Siegmund (1971) Theorem - converge to global minimum for convex loss
    functions and either a global or local minimum for nonconvex loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: The steps include,
  prefs: []
  type: TYPE_NORMAL
- en: start with random model parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: select a random subset of training data, \(n_{batch}\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: calculate the loss function and loss function gradient for the model parameters
    over the random batch,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \nabla L(y_{\alpha}, F(X_{\alpha}, b_1)) = \frac{L(y_{\alpha}, F(X_{\alpha},
    b_1 - \epsilon)) - L(y_{\alpha}, F(X_{\alpha}, b_1 + \epsilon))}{2\epsilon} \]
  prefs: []
  type: TYPE_NORMAL
- en: update the parameter estimate by stepping down slope / gradient,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \hat{b}_{1,t+1} = \hat{b}_{1,t} - r \nabla L(y_{\alpha}, F(X_{\alpha}, b_1))
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(r\) is the learning rate/step size, \(\hat{b}(1,ùë°)\), is the current
    model parameter estimate and \(\hat{b}(1,ùë°+1)\) is the updated parameter estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): system or process
    that is uncertain and is represented by multiple models, *realizations* and *scenarios*
    constrained by statistics,'
  prefs: []
  type: TYPE_NORMAL
- en: for example, data-driven models that integrate uncertainty like geostatistical
    simulation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: report significance, confidence / prediction intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: honor many types of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data-driven approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: limited physics used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical model assumptions / simplification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the alternative to stochastic models see *deterministic models*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistics** (practice)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): the theory and
    practice for collecting, organizing, and interpreting data, as well as drawing
    conclusions and making decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistics** (measurement)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): summary measure
    of a sample, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: sample mean - \(\overline{x}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sample standard deviation - \(s\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we use statistics as estimates of the model parameters that summarize the population
    (*inference*)
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical Distribution**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): for a feature
    a description of the probability of occurrence over the range of possible values.
    We represent the univariate statistical distribution with,'
  prefs: []
  type: TYPE_NORMAL
- en: '*histogram*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*normalized histogram*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*probability density function* (PDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*cumulative distribution function* (CDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do we learn from a statistical distribution? For example,
  prefs: []
  type: TYPE_NORMAL
- en: what is the minimum and maximum?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have a lot of low values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have a lot of high values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do we have outliers, and any other values that don‚Äôt make sense and need explaining?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector** (support vector machines)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): training
    data within the margin or misclassified and update the support vector machines
    classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: with a support vector machines model, training data well within the correct
    region, are not support vectors, and have no impact on the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support Vector Machines**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Support Vector Machines](MachineLearning_support_vector_machines.html): a
    predictive, binary classification machine learning method that is a good classification
    method when there is poor separation of groups.'
  prefs: []
  type: TYPE_NORMAL
- en: projects the original predictor features to higher dimensional space and then
    applies a linear, plane or hyperplane,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ ùëì(ùë•) = ùë•^ùëá \beta +\beta_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\beta\) is a vector and together with \(\beta\) are the hyperplane model
    parameters, while \(x\) is the matrix of predictor features, all are in the high
    dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùê∫(ùë•)=\text{ùë†ùëñùëîùëõ}\left( ùëì(ùë•) \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: \(ùëì(ùë•)\) is proportional to the signed distance from the decision boundary,
    and \(ùê∫(ùë•)\) is the side of the decision boundary, \(‚àí\) one side and \(+\) the
    other, \(f(x) = 0\) is on the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: We represent the constraint, all data of each category must be on the correct
    side of the boundary, by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: where this holds if the categories, \(y_i\), are -1 or 1\. We need a model that
    allows for some misclassification,
  prefs: []
  type: TYPE_NORMAL
- en: \[ y_i \left( x_i^T \beta + \beta_0 \right) \geq M - \xi_i \]
  prefs: []
  type: TYPE_NORMAL
- en: We introduce the concept of a margin, \(ùëÄ\), and a distance from the margin,
    the error as \(\xi_i\). Now we can pose our loss function as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \underset{\beta, \beta_0}{\text{min}} \left( \frac{1}{2M^2} + C \sum_{i=1}^N
    \xi_i \right) \]
  prefs: []
  type: TYPE_NORMAL
- en: subject to, \(\xi_i \geq 0, \quad y_i \left( x_i^T \beta + \beta_0 \right) \geq
    M - \xi_i\).
  prefs: []
  type: TYPE_NORMAL
- en: This is the support vector machine loss function in the higher dimensional space,
    where ùõΩ,ùõΩ_0 are the multilinear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training the support vector machine, by finding the model parameters of the
    plane to maximize the margin, \(M\), while minimizing the error, \(\sum_{i=1}^N
    \xi_i\)
  prefs: []
  type: TYPE_NORMAL
- en: \(ùë™\) hyperparameter weights the sum of errors, \(xi_ùëñ\), higher \(ùê∂\), will
    result in reduced margin, \(M\), and lead to overfit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: smaller margin, fewer data used to constrain the boundary, known as support
    vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training data well within the correct side of the boundary have no influence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are some key aspects of support vector machines,
  prefs: []
  type: TYPE_NORMAL
- en: known as support vector machines, and not machine, because with a new kernel
    you get a new machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are many kernels available including polynomial and radial basis functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary hyperparameter is \(C\), the cost of
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are related to the choice of kernel, for example,
  prefs: []
  type: TYPE_NORMAL
- en: '*polynomial* - polynomial order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*radial basis function* - \(\gamma\) inversely proportional to the distance
    influence of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tabular Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine Learning Workflow Construction and Coding: data table with rows for
    each sample and columns for each feature'
  prefs: []
  type: TYPE_NORMAL
- en: Pandas‚Äô DataFrames are a convenient class for working with tabular data, due
    to,
  prefs: []
  type: TYPE_NORMAL
- en: convenient data structure to store, access, manipulate tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to load data from a variety of file types, Python classes and
    even directly from Excel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods to calculate summary statistics and visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data queries, sort, data filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in methods for data manipulation, cleaning, reformatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: built-in attributes to store information about the data, e.g. size, number nulls
    and null value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and Testing Splits**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): for model cross
    validation, prior to predictive model training withhold a proportion of the data
    as testing data.'
  prefs: []
  type: TYPE_NORMAL
- en: training data are applied to train the model parameters, while withheld testing
    data are applied to tune the model hyperparameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hyperparameter tuning is selecting the hyperparameter combination that minimizes
    the error norm over the withheld testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common approach is random selection, this may not be fair testing,
  prefs: []
  type: TYPE_NORMAL
- en: the range of testing difficulty is similar to the real-world use of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too easy ‚Äì testing cases are the same or almost the same as training cases,
    random sampling is often too easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: too hard ‚Äì testing cases are very different from the training cases, the model
    is expected to severely extrapolate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative methods such as k-fold cross validation provide the opportunity
    for testing over all available data but require,
  prefs: []
  type: TYPE_NORMAL
- en: the training k predictive machine learning models over the hyperparameter combinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: aggregation of the testing error over the k models for selection of the optimum
    hyperparameters, hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, there are alternative workflow that include, training, validation and
    testing subsets of the data
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer Function** (reservoir modeling workflow)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): calculation applied
    to the spatial, subsurface model realizations and scenarios to calculate a decision
    criterion, a metric that is used to support decision making representing value,
    and health, environment and safety. Example transfer functions include,'
  prefs: []
  type: TYPE_NORMAL
- en: '*transport and bioattenuation* - numerical simulation to model soil contaminant
    concentrations over time during a pump and treat operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*volumetric calculation* - for total oil-in-place to calculate resource in
    place'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*heterogeneity metric* - as an indicator of recovery factor to estimate reserves
    from resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*flow simulation* - for pre-drill production forecast for a planned well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Whittle‚Äôs pit optimization* - to calculate mineral resources and ultimate
    pit shell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncertainty Modeling**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): calculation of
    the range of possible values for a feature at a location or jointly over many
    locations at the sample time. Some considerations,'
  prefs: []
  type: TYPE_NORMAL
- en: quantification of the limitation in the precision of our samples and model predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is a model, there is no objective uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is caused by our ignorance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uncertainty is caused by sparse sampling, measurement error and bias, and heterogeneity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'we represent uncertainty by multiple models, scenarios and realizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Scenarios - multiple spatial, subsurface models calculated by stochastic simulation
    by changing the input parameters or other modeling choices to represent the uncertainty
    due to inference of model parameters and model choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realizations - multiple spatial, subsurface models calculated by stochastic
    simulation by holding input parameters and model choices constant and only changing
    the random number seed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfit Model**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a machine learning
    model that too simple, too low complexity and flexibility, to fit the natural
    phenomenon resulting in very high model bias.'
  prefs: []
  type: TYPE_NORMAL
- en: underfit models often approach the response feature global mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: underfit models have high error over training and testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: increased complexity will generally decrease error with respect to the training
    and testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: over the region of model complexity with falling training and testing error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues of an underfit machine learning model,
  prefs: []
  type: TYPE_NORMAL
- en: more model complexity and flexibility is insufficient given the available data,
    data accuracy, frequency and coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: low accuracy in training and testing representing real-world use away from training
    data cases, indicating poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Union of Events** (probability)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): the union of outcomes,
    the probability of \(A\) or \(B\) is calculated with the probability addition
    rule,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate Parameters**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): summary measures
    based on one feature measured over the population'
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate Statistics**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Univariate Analysis](MachineLearning_univariate_analysis.html): summary measures
    based on one feature measured over the samples'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised Learning**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cluster Analysis: learning patterns in data from unlabeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: no response features, \(ùëå\), instead only predictor features, \(ùëã_1,ldots,ùëã_ùëö\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: machine learns by mimicry a compact representation of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: captures patterns as feature projections, group assignments, neural network
    latent features, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: focus on inference of the population, the natural system, instead of prediction
    of response features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this course we use the terms inferential and predictive machine learning,
    all the covered inferential machine learning methods are unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variable** (also feature)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): any property measured
    or observed in a study, for example,'
  prefs: []
  type: TYPE_NORMAL
- en: porosity, permeability, mineral concentrations, saturations, contaminant concentration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in data mining / machine learning this is known as a *feature*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: measure often requires significant analysis, interpretation, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance Inflation Factor** (VIF)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Ranking](MachineLearning_feature_ranking.html): a measure of linear
    multicollinearity between a predictor feature (\(X_i\)) a nd all other predictor
    features (\(X_j, \forall j \ne i\)).'
  prefs: []
  type: TYPE_NORMAL
- en: First we calculate a linear regression for a predictor feature given all the
    other predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_i = \sum_{j, j \ne i}^m X_j + \epsilon \]
  prefs: []
  type: TYPE_NORMAL
- en: From this model we determine the coefficient of determination, \(R^2\), known
    as variance explained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we calculate the Variance Inflation Factor as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ VIF = \frac{1}{1 - R^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume-Variance Relations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature Transformations](MachineLearning_feature_transformations.html): as
    the *volume support* (scale) increases the variance reduces'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting volume-variance relations is central to handling multiple scales
    of data and models. Some general observations and assumptions,
  prefs: []
  type: TYPE_NORMAL
- en: the mean does not change as the volume support, scale changes. Only the variance
    changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there may be shape change (we will not tackle that here). Best practice is to
    check shape change empirically. It is common to assume no shape change (*affine
    correction*) or to use a shape change model (indirect lognormal correction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the variance reduction in the distribution is inversely proportional to the
    range of spatial continuity. Variance reduces faster (over smaller volume increase)
    for shorter spatial continuity ranges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over common changes in scale this impact may be significant; therefore, it is
    not appropriate to ignore volume-variance relations,
  prefs: []
  type: TYPE_NORMAL
- en: we don‚Äôt do this scale up, change in volume support perfectly, and this is why
    it is still called the missing scale. We rarely have enough data to model this
    rigorously
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we need a model to predict this change in variance with change in volume support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some change in volume support, scale models,
  prefs: []
  type: TYPE_NORMAL
- en: '*Empirical* - build a small scale, high resolution model and scale it up numerically.
    For example, calculate a high resolution model of permeability, apply flow simulation
    to calculate effective permeability over \(v\) scale blocks'
  prefs: []
  type: TYPE_NORMAL
- en: '*Power Law Averaging* - there is a flexible approach known as power law averaging.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ z_V = \left[ \frac{1}{n} \sum z_v^{\omega} \right] ^{\frac{1}{\omega}} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\omega\) is the power of averaging. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\omega = 1\) is a regular linear averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\omega = -1\) is a harmonic averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\omega = 0\) is a geometric averaging (this is proved in the limit as \(\omega
    \rightarrow 0\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate \(\omega\)?
  prefs: []
  type: TYPE_NORMAL
- en: for some cases we know from theory the correct \(\omega\) value, for example,
    for flow orthogonal to beds we select \(\omega = -1.0\) to scale up permeability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: flow simulation may be applied to numerically scale up permeability and then
    to back-calculate a calibrated \(\omega\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model* - directly adjust the statistics for change in scale. For example,
    under the assumption of linear averaging and a stationary variogram and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = 1 - \frac{\overline{\gamma}(v,v)}{\sigma^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(f\) is variance reduction factor,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f = \frac{D^2(v,V)}{D^2(\cdot,V)} = \frac{D^2(v,V)}{\sigma^2} \]
  prefs: []
  type: TYPE_NORMAL
- en: in other words, \(f\) is the ratio of the variance at scale \(v\) to the variance
    at the original data point support scale based on,
  prefs: []
  type: TYPE_NORMAL
- en: the variogram model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the scale of the data, \(\cdot\) and the scale of \(v\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Venn Diagrams**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Probability Concepts](MachineLearning_probability.html): a plot, visual tool
    for communicating probability. What do we learn from a Venn diagram?'
  prefs: []
  type: TYPE_NORMAL
- en: size of regions \(\propto\) probability of occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: proportion of \(\Omega\), all possible outcomes represented by a box, i.e.,
    probability of \(1.0\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: overlap \(\propto\) probability of joint occurrence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venn diagrams are an excellent tool to visualize marginal, joint and conditional
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well Log Data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): as a much cheaper
    method to sample wells that does not interrupt drilling operations, well logs
    are very common over the wells. Often all wells have various well logs available.
    For example,'
  prefs: []
  type: TYPE_NORMAL
- en: gamma ray on pilot vertical wells to assess the locations and quality of shales
    for targeting (landing) horizontal wells
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: neutron porosity to assess location high porosity reservoir sands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gamma ray in drill holes to map thorium mineralization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well log data are critical to support subsurface resource interpretations. Once
    anchored by core data they provide the essential coverage and resolution to model
    the entire reservoir concept / framework for prediction, for example,
  prefs: []
  type: TYPE_NORMAL
- en: well log data calibrated by core data collocated with well log data are used
    to map the critical stratigraphic layers, including reservoir and seal units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: well logs are applied to depth correct features inverted from seismic that have
    location imprecision due to uncertainty in the rock velocity over the volume of
    interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weak Learner**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Gradient Boosting](MachineLearning_gradient_boosting.html): the prediction
    model performs only marginally better than random'
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùëå = \hat{f}_ùëò(ùëã_1,\ldots,ùëã_ùëö) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\hat{f}_ùëò\) is the \(ùëò^{th}\) weak learner, \(ùëã_1,\ldots,ùëã_ùëö\) are the
    predictor features, \(\hat{Y}\) is the prediction of the response feature.
  prefs: []
  type: TYPE_NORMAL
- en: The term weak predictor is often used, and specifically the term weak classifier
    for the case of classification models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Well Log Data, Image Logs**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Machine Learning Concepts](MachineLearning_concepts.html): a special case
    of *well logs* where the well logs are repeated at various azimuthal intervals
    within the well bore resulting in a 2D (unwrapped) image instead of a 1D line
    along the well bore. For example, Fullbore formation MicroImager (FMI) with:'
  prefs: []
  type: TYPE_NORMAL
- en: with 80% bore hole coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.2 inch (0.5 cm) resolution vertical and horizontal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 inch (79 cm) depth of investigation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: can be applied to observe lithology change, bed dips and sedimentary structures.
  prefs: []
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic introduction to geostatistics. If you would like more on these
    fundamental concepts I recommend the Introduction, Modeling Principles and Modeling
    Prerequisites chapters from my text book, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446){cite}`pyrcz2014‚Äô.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Author:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Michael Pyrcz, Professor, The University of Texas at Austin *Novel Data Analytics,
    Geostatistics and Machine Learning Subsurface Solutions*
  prefs: []
  type: TYPE_NORMAL
- en: With over 17 years of experience in subsurface consulting, research and development,
    Michael has returned to academia driven by his passion for teaching and enthusiasm
    for enhancing engineers‚Äô and geoscientists‚Äô impact in subsurface resource development.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more about Michael check out these links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
