- en: Data Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程
- en: '*DALL·E 3 Prompt: Create a rectangular illustration visualizing the concept
    of data engineering. Include elements such as raw data sources, data processing
    pipelines, storage systems, and refined datasets. Show how raw data is transformed
    through cleaning, processing, and storage to become valuable information that
    can be analyzed and used for decision-making.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：创建一个矩形插图，可视化数据工程的概念。包括原始数据源、数据处理管道、存储系统和精炼数据集。展示原始数据如何通过清理、处理和存储转化为有价值的信息，这些信息可以用于分析和决策制定。*'
- en: '![](../media/file71.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file71.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why does data quality serve as the foundation that determines whether machine
    learning systems succeed or fail in production environments?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么数据质量作为基础，决定了机器学习系统在生产环境中成功或失败？*'
- en: 'Machine learning systems depend on data quality: no algorithm can overcome
    poor data, but excellent data engineering enables even simple models to achieve
    remarkable results. Unlike traditional software where logic is explicit, ML systems
    derive behavior from data patterns, making quality the primary determinant of
    system trustworthiness. Understanding data engineering principles provides the
    foundation for building ML systems that operate consistently across diverse production
    environments, maintain performance over time, and scale effectively as data volumes
    and complexity increase.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统依赖于数据质量：没有算法可以克服糟糕的数据，但优秀的数据工程甚至可以使简单的模型取得显著的结果。与逻辑明确的传统软件不同，机器学习系统从数据模式中推导行为，使质量成为系统可靠性的主要决定因素。理解数据工程原理为构建在多样化生产环境中一致运行、随时间保持性能并随着数据量和复杂性的增加而有效扩展的机器学习系统提供了基础。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Apply the four pillars framework (Quality, Reliability, Scalability, Governance)
    to evaluate data engineering decisions systematically
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用四支柱框架（质量、可靠性、可扩展性、治理）来系统地评估数据工程决策
- en: Calculate infrastructure requirements for ML systems including storage capacity,
    processing throughput, and labeling costs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机器学习系统的基础设施需求，包括存储容量、处理吞吐量和标注成本
- en: Design data pipelines that maintain training-serving consistency to prevent
    the primary cause of production ML failures
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计数据管道，以保持训练-服务一致性，防止生产中机器学习失败的主要原因
- en: Evaluate acquisition strategies (existing datasets, web scraping, crowdsourcing,
    synthetic data) based on quality-cost-scale trade-offs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据质量-成本-规模权衡来评估获取策略（现有数据集、网络抓取、众包、合成数据）
- en: Architect storage systems (databases, data warehouses, data lakes, feature stores)
    appropriate for different ML workload patterns
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计适合不同机器学习工作负载模式的存储系统（数据库、数据仓库、数据湖、特征存储）
- en: Implement data governance practices including lineage tracking, privacy protection,
    and bias mitigation throughout the data lifecycle
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据生命周期中实施数据治理实践，包括血缘跟踪、隐私保护和偏差缓解
- en: Data Engineering as a Systems Discipline
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程作为系统学科
- en: 'The systematic methodologies examined in the previous chapter establish the
    procedural foundations of machine learning development, yet underlying each phase
    of these workflows exists a fundamental prerequisite: robust data infrastructure.
    In traditional software, computational logic is defined by code. In machine learning,
    system behavior is defined by data. This paradigm shift makes data a first-class
    citizen in the engineering process, akin to source code, requiring a new discipline,
    data engineering, to manage it with the same rigor we apply to code.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中探讨的系统方法为机器学习发展的程序基础奠定了基础，然而，在这些工作流程的每个阶段都存在一个基本前提：强大的数据基础设施。在传统软件中，计算逻辑由代码定义。在机器学习中，系统行为由数据定义。这种范式转变使得数据成为工程过程中的第一公民，类似于源代码，需要一个新的学科，即数据工程，以同样的严谨性来管理它。
- en: While workflow methodologies provide the organizational framework for constructing
    ML systems, data engineering provides the technical substrate that enables effective
    implementation of these methodologies. Advanced modeling techniques and rigorous
    validation procedures cannot compensate for deficient data infrastructure, whereas
    well-engineered data systems enable even conventional approaches to achieve substantial
    performance gains.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然工作流程方法为构建机器学习系统提供了组织框架，但数据工程提供了实现这些方法的技术基础。高级建模技术和严格的验证程序无法弥补数据基础设施的不足，而精心设计的数据系统甚至可以使传统方法实现显著的性能提升。
- en: This chapter examines data engineering as a systematic engineering discipline
    focused on the design, construction, and maintenance of infrastructure that transforms
    heterogeneous raw information into reliable, high-quality datasets suitable for
    machine learning applications. In contrast to traditional software systems where
    computational logic remains explicit and deterministic, machine learning systems
    derive their behavioral characteristics from underlying data patterns, establishing
    data infrastructure quality as the principal determinant of system efficacy. Consequently,
    architectural decisions concerning data acquisition, processing, storage, and
    governance influence whether ML systems achieve expected performance in production
    environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了数据工程作为一门系统性的工程学科，其重点在于设计、构建和维护将异构原始信息转化为适合机器学习应用的高质量、可靠数据集的基础设施。与计算逻辑保持明确和确定性的传统软件系统不同，机器学习系统从其底层数据模式中推导出其行为特征，将数据基础设施质量作为系统效能的主要决定因素。因此，关于数据获取、处理、存储和治理的架构决策影响着机器学习系统在生产环境中是否能够实现预期的性能。
- en: '***Data Engineering*** is the systematic discipline of designing and maintaining
    *data infrastructure* that transforms *raw data* into *reliable*, *accessible*,
    and *analysis-ready* datasets through principled acquisition, processing, storage,
    and governance practices.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据工程**是设计和维护将**原始数据**转化为**可靠**、**可访问**和**分析就绪**数据集的**数据基础设施**的系统学科，通过原则性的获取、处理、存储和治理实践实现。'
- en: 'The critical importance of data engineering decisions becomes evident when
    examining how data quality issues propagate through machine learning systems.
    Traditional software systems typically generate predictable error responses or
    explicit rejections when encountering malformed input, enabling developers to
    implement immediate corrective measures. Machine learning systems present different
    challenges: data quality deficiencies manifest as subtle performance degradations
    that accumulate throughout the processing pipeline and frequently remain undetected
    until catastrophic system failures occur in production environments. While individual
    mislabeled training instances may appear inconsequential, systematic labeling
    inconsistencies systematically corrupt model behavior across entire feature spaces.
    Similarly, gradual data distribution shifts in production environments can progressively
    degrade system performance until comprehensive model retraining becomes necessary.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当考察数据质量问题如何在机器学习系统中传播时，数据工程决策的关键重要性变得显而易见。传统的软件系统在遇到格式不正确的输入时通常会生成可预测的错误响应或明确的拒绝，使开发者能够实施立即的纠正措施。机器学习系统则提出了不同的挑战：数据质量缺陷表现为微妙的性能下降，这些下降在整个处理管道中累积，通常在生产环境中直到系统发生灾难性故障才被发现。虽然单个错误标记的训练实例可能看似无关紧要，但系统性的标记不一致会系统地破坏整个特征空间中的模型行为。同样，生产环境中的数据分布逐渐变化会逐渐降低系统性能，直到需要进行全面模型重新训练。
- en: These challenges require systematic engineering approaches that transcend ad-hoc
    solutions and reactive interventions. Effective data engineering demands systematic
    analysis of infrastructure requirements that parallels the disciplined methodologies
    applied to workflow design. This chapter develops a principled theoretical framework
    for data engineering decision-making, organized around four foundational pillars
    (Quality, Reliability, Scalability, and Governance) that provide systematic guidance
    for technical choices spanning initial data acquisition through production deployment.
    We examine how these engineering principles manifest throughout the complete data
    lifecycle, clarifying the systems-level thinking required to construct data infrastructure
    that supports current ML workflows while maintaining adaptability and scalability
    as system requirements evolve.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战需要超越临时解决方案和反应性干预的系统工程方法。有效的数据工程需要对基础设施需求进行系统分析，这与应用于工作流程设计的纪律性方法相平行。本章发展了一个数据工程决策的原则性理论框架，围绕四个基础支柱（质量、可靠性、可扩展性和治理）组织，为从初始数据获取到生产部署的技术选择提供系统指导。我们考察了这些工程原则在整个数据生命周期中的体现，明确了构建支持当前机器学习工作流程的数据基础设施所需的系统级思维，同时保持系统需求演变时的适应性和可扩展性。
- en: Rather than analyzing individual technical components in isolation, we examine
    the systemic interdependencies among engineering decisions, demonstrating the
    inherently interconnected nature of data infrastructure systems. This integrated
    analytical perspective is particularly significant as we prepare to examine the
    computational frameworks that process these carefully engineered datasets, the
    primary focus of subsequent chapters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是孤立地分析单个技术组件，而是检查工程决策之间的系统性相互依赖关系，展示了数据基础设施系统的内在互联性。这种综合分析视角在我们准备检查处理这些精心设计的计算数据集的计算框架时尤为重要，这是后续章节的主要关注点。
- en: Four Pillars Framework
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 四支柱框架
- en: Building effective ML systems requires understanding not only what data engineering
    is but also implementing a structured framework for making principled decisions
    about data infrastructure. Choices regarding storage formats, ingestion patterns,
    processing architectures, and governance policies require systematic evaluation
    rather than ad-hoc selection. This framework organizes data engineering around
    four foundational pillars that ensure systems achieve functionality, robustness,
    scalability, and trustworthiness.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 构建有效的机器学习系统不仅需要理解数据工程是什么，还需要实施一个结构化的框架，以做出关于数据基础设施的原则性决策。关于存储格式、摄取模式、处理架构和治理政策的决策需要系统性的评估，而不是临时选择。这个框架围绕四个基础支柱组织数据工程，确保系统实现功能性、鲁棒性、可扩展性和可靠性。
- en: The Four Foundational Pillars
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 四个基础支柱
- en: Every data engineering decision, from choosing storage formats to designing
    ingestion pipelines, should be evaluated against four foundational principles.
    Each pillar contributes to system success through systematic decision-making.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据工程决策，从选择存储格式到设计摄取管道，都应该与四个基础原则进行评估。每个支柱通过系统性的决策制定为系统成功做出贡献。
- en: First, data quality provides the foundation for system success. Quality issues
    compound throughout the ML lifecycle through a phenomenon termed “Data Cascades”
    ([Section 6.3](ch012.xhtml#sec-data-engineering-data-cascades-need-systematic-foundations-e6f5)),
    wherein early failures propagate and amplify downstream. Quality includes accuracy,
    completeness, consistency, and fitness for the intended ML task. High-quality
    data is essential for model success, with the mathematical foundations of this
    relationship explored in [Chapter 3](ch009.xhtml#sec-dl-primer) and [Chapter 4](ch010.xhtml#sec-dnn-architectures).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据质量是系统成功的基础。在整个机器学习生命周期中，质量问题会通过称为“数据级联”的现象（[第6.3节](ch012.xhtml#sec-data-engineering-data-cascades-need-systematic-foundations-e6f5)）累积放大，其中早期失败会传播并放大到下游。质量包括准确性、完整性、一致性和适合于预期机器学习任务的程度。高质量的数据对于模型成功至关重要，这种关系的数学基础在[第3章](ch009.xhtml#sec-dl-primer)和[第4章](ch010.xhtml#sec-dnn-architectures)中进行了探讨。
- en: Building upon this quality foundation, ML systems require consistent, predictable
    data processing that handles failures gracefully. Reliability means building systems
    that continue operating despite component failures, data anomalies, or unexpected
    load patterns. This includes implementing comprehensive error handling, monitoring,
    and recovery mechanisms throughout the data pipeline.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个质量基础上，机器学习系统需要一致、可预测的数据处理，能够优雅地处理失败。可靠性意味着构建即使在组件故障、数据异常或意外负载模式的情况下也能继续运行的系统。这包括在整个数据管道中实施全面的错误处理、监控和恢复机制。
- en: While reliability ensures consistent operation, scalability addresses the challenge
    of growth. As ML systems grow from prototypes to production services, data volumes
    and processing requirements increase dramatically. Scalability involves designing
    systems that can handle growing data volumes, user bases, and computational demands
    without requiring complete system redesigns.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可靠性确保了持续运行，但可扩展性解决了增长挑战。随着机器学习系统从原型发展到生产服务，数据量和处理需求急剧增加。可扩展性涉及设计能够处理增长的数据量、用户基础和计算需求，而无需进行完整的系统重新设计。
- en: Finally, governance provides the framework within which quality, reliability,
    and scalability operate. Data governance ensures systems operate within legal,
    ethical, and business constraints while maintaining transparency and accountability.
    This includes privacy protection, bias mitigation, regulatory compliance, and
    establishing clear data ownership and access controls.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，治理提供了质量、可靠性和可扩展性运行的框架。数据治理确保系统在法律、伦理和业务约束下运行，同时保持透明度和问责制。这包括隐私保护、偏差缓解、合规监管以及建立明确的数据所有权和访问控制。
- en: '![](../media/file72.svg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file72.svg)'
- en: 'Figure 6.1: **The Four Pillars of Data Engineering**: Quality, Reliability,
    Scalability, and Governance form the foundational framework for ML data systems.
    Each pillar contributes essential capabilities (solid arrows), while trade-offs
    between pillars (dashed lines) require careful balancing: validation overhead
    affects throughput, consistency constraints limit distributed scale, privacy requirements
    impact performance, and bias mitigation may reduce available training data. Effective
    data engineering requires managing these tensions systematically rather than optimizing
    any single pillar in isolation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图6.1：**数据工程四支柱**：质量、可靠性、可扩展性和治理构成了机器学习数据系统的基本框架。每个支柱都贡献了基本能力（实线箭头），而支柱之间的权衡（虚线）需要仔细平衡：验证开销影响吞吐量，一致性约束限制分布式扩展，隐私要求影响性能，而偏差缓解可能会减少可用的训练数据。有效的数据工程需要系统地管理这些紧张关系，而不是孤立地优化任何单个支柱。 '
- en: Integrating the Pillars Through Systems Thinking
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过系统思维整合支柱
- en: Although understanding each pillar individually provides important insights,
    recognizing their individual importance is only the first step toward effective
    data engineering. As illustrated in [Figure 6.1](ch012.xhtml#fig-four-pillars),
    these four pillars are not independent components but interconnected aspects of
    a unified system where decisions in one area affect all others. Quality improvements
    must account for scalability constraints, reliability requirements influence governance
    implementations, and governance policies shape quality metrics. This systems perspective
    guides our exploration of data engineering, examining how each technical topic
    supports and balances these foundational principles while managing their inherent
    tensions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单独理解每个支柱提供了重要的见解，但认识到它们的个体重要性只是有效数据工程的第一步。如图6.1所示，这四个支柱不是独立的组件，而是统一系统的相互关联的方面，其中某一领域的决策会影响所有其他领域。质量改进必须考虑到可扩展性约束，可靠性要求会影响治理实施，而治理政策会塑造质量指标。这种系统视角指导我们对数据工程的探索，检查每个技术主题如何支持并平衡这些基础原则，同时管理它们固有的紧张关系。
- en: As [Figure 6.2](ch012.xhtml#fig-ds-time) illustrates, data scientists spend
    60-80% of their time on data preparation tasks according to various industry surveys[1](#fn1).
    This statistic reflects the current state where data engineering practices are
    often ad-hoc rather than systematic. By applying the four-pillar framework consistently
    to address this overhead, teams can reduce data preparation time while building
    more reliable and maintainable systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如图6.2所示，根据各种行业调查，数据科学家将60-80%的时间花在数据准备任务上[1](#fn1)。这一统计数据反映了当前的状态，即数据工程实践往往是临时的而不是系统的。通过一致地应用四支柱框架来解决这个问题，团队可以减少数据准备时间，同时构建更可靠和可维护的系统。
- en: '![](../media/file73.svg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file73.svg)'
- en: 'Figure 6.2: **Data Scientist Time Allocation**: Data preparation consumes a
    majority of data science effort, up to 60%, underscoring the need for systematic
    data engineering practices to prevent downstream model failures and ensure project
    success. Prioritizing data quality and pipeline development yields greater returns
    than solely focusing on advanced algorithms. Source: Various industry reports.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：**数据科学家时间分配**：数据准备消耗了数据科学工作的大部分努力，高达60%，强调了系统化数据工程实践的需求，以防止下游模型失败并确保项目成功。优先考虑数据质量和管道开发比仅仅关注高级算法能带来更大的回报。来源：各种行业报告。
- en: Framework Application Across Data Lifecycle
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 框架在数据生命周期中的应用
- en: This four-pillar framework guides our exploration of data engineering systems
    from problem definition through production operations. We begin by establishing
    clear problem definitions and governance principles that shape all subsequent
    technical decisions. The framework then guides us through data acquisition strategies,
    where quality and reliability requirements determine how we source and validate
    data. Processing and storage decisions follow naturally from scalability and governance
    constraints, while operational practices ensure all four pillars are maintained
    throughout the system lifecycle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个四支柱框架指导我们从问题定义到生产操作的整个数据工程系统探索过程。我们首先建立清晰的问题定义和治理原则，这些原则塑造了所有后续的技术决策。然后，框架引导我们通过数据获取策略，其中质量和可靠性要求决定了我们如何获取和验证数据。处理和存储决策自然地遵循可扩展性和治理约束，而运营实践确保整个系统生命周期中维护这四个支柱。
- en: 'This framework guides our systematic exploration through each major component
    of data engineering. As we examine data acquisition, ingestion, processing, and
    storage in subsequent sections, we examine how these pillars manifest in specific
    technical decisions: sourcing techniques that balance quality with scalability,
    storage architectures that support performance within governance constraints,
    and processing pipelines that maintain reliability while handling massive scale.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此框架指导我们系统地探索数据工程的每个主要组件。在后续章节中，当我们检查数据获取、摄取、处理和存储时，我们考察这些支柱如何在具体技术决策中体现：平衡质量与可扩展性的来源技术、支持治理约束下的性能的存储架构，以及处理管道在处理大规模数据时保持可靠性。
- en: '[Table 6.1](ch012.xhtml#tbl-four-pillars-matrix) provides a comprehensive view
    of how each pillar manifests across the major stages of the data pipeline. This
    matrix serves both as a planning tool for system design and as a reference for
    troubleshooting when issues arise at different pipeline stages.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.1](ch012.xhtml#tbl-four-pillars-matrix)提供了每个支柱如何在数据管道主要阶段体现的全面视图。此矩阵既作为系统设计的规划工具，也作为在不同管道阶段出现问题时进行故障排除的参考。'
- en: 'Table 6.1: **Four Pillars Applied Across Data Pipeline Stages**: This matrix
    illustrates how Quality, Reliability, Scalability, and Governance principles manifest
    in each major stage of the data engineering pipeline. Each cell shows specific
    techniques and practices that implement the corresponding pillar at that stage,
    providing a comprehensive framework for systematic decision-making and troubleshooting.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：**数据管道各阶段应用的四项支柱**：此矩阵说明了质量、可靠性、可扩展性和治理原则如何在数据工程管道的每个主要阶段体现。每个单元格显示了在该阶段实施相应支柱的具体技术和实践，提供了一个全面的框架，用于系统决策和故障排除。
- en: '| **Stage** | **Quality** | **Reliability** | **Scalability** | **Governance**
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **阶段** | **质量** | **可靠性** | **可扩展性** | **治理** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Acquisition** | Representative sampling, bias detection | Diverse sources,
    redundant collection strategies | Web scraping, synthetic data generation | Consent,
    anonymization, ethical sourcing |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **获取** | 典型抽样，偏差检测 | 多样化来源，冗余收集策略 | 网络爬虫，合成数据生成 | 同意，匿名化，道德来源 |'
- en: '| **Ingestion** | Schema validation, data profiling | Dead letter queues, graceful
    degradation | Batch vs stream processing, autoscaling pipelines | Access controls,
    audit logs, data lineage |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **摄取** | 架构验证，数据概要分析 | 死信队列，优雅降级 | 批量处理与流处理，自动扩展管道 | 访问控制，审计日志，数据血缘 |'
- en: '| **Processing** | Consistency validation, training-serving parity | Idempotent
    transformations, retry mechanisms | Distributed frameworks, horizontal scaling
    | Lineage tracking, privacy preservation, bias monitoring |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **处理** | 一致性验证，训练-服务一致性 | 幂等变换，重试机制 | 分布式框架，横向扩展 | 血缘追踪，隐私保护，偏差监控 |'
- en: '| **Storage** | Data validation checks, freshness monitoring | Backups, replication,
    disaster recovery | Tiered storage, partitioning, compression optimization | Access
    audits, encryption, retention policies |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **存储** | 数据验证检查，新鲜度监控 | 备份，复制，灾难恢复 | 分层存储，分区，压缩优化 | 访问审计，加密，保留策略 |'
- en: To ground these concepts in practical reality, we follow a Keyword Spotting
    (KWS) system throughout as our running case study, demonstrating how framework
    principles translate into engineering decisions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些概念扎根于实际现实，我们以关键词检测（KWS）系统作为贯穿的案例研究，展示框架原则如何转化为工程决策。
- en: Data Cascades and the Need for Systematic Foundations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据级联和系统基础的需求
- en: 'Machine learning systems face a unique failure pattern that distinguishes them
    from traditional software engineering: “Data Cascades,”[2](#fn2) the phenomenon
    identified by Sambasivan et al. ([2021](ch058.xhtml#ref-sambasivan2021everyone))
    where poor data quality in early stages amplifies throughout the entire pipeline,
    causing downstream model failures, project termination, and potential user harm.
    Unlike traditional software where bad inputs typically produce immediate errors,
    ML systems degrade silently until quality issues become severe enough to necessitate
    complete system rebuilds.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统面临一种独特的失败模式，这使它们与传统软件工程区别开来：“数据级联”，[2](#fn2) 由Sambasivan等人[2021](ch058.xhtml#ref-sambasivan2021everyone)提出的现象，其中早期阶段的数据质量问题在整个管道中放大，导致下游模型失败、项目终止和潜在的用户伤害。与传统软件中不良输入通常产生即时错误不同，ML系统在质量问题时会默默退化，直到问题严重到需要完全重建系统。
- en: 'Data cascades occur when teams skip establishing clear quality criteria, reliability
    requirements, and governance principles before beginning data collection and processing
    work. This fundamental vulnerability motivates our Four Pillars framework: Quality,
    Reliability, Scalability, and Governance provide the systematic foundation needed
    to prevent cascade failures and build robust ML systems.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当团队在开始数据收集和处理工作之前跳过建立明确的质量标准、可靠性要求和治理原则时，就会发生数据级联。这种基本漏洞促使我们提出四支柱框架：质量、可靠性、可扩展性和治理提供了防止级联失败和构建稳健机器学习系统所需的系统基础。
- en: '[Figure 6.3](ch012.xhtml#fig-cascades) illustrates these potential data pitfalls
    at every stage and how they influence the entire process down the line. The influence
    of data collection errors is especially pronounced. As illustrated in the figure,
    any lapses in this initial stage will become apparent during model evaluation
    and deployment phases discussed in [Chapter 8](ch014.xhtml#sec-ai-training) and
    [Chapter 13](ch019.xhtml#sec-ml-operations), potentially leading to costly consequences
    such as abandoning the entire model and restarting anew. Therefore, investing
    in data engineering techniques from the onset will help us detect errors early,
    mitigating these cascading effects.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.3](ch012.xhtml#fig-cascades)展示了每个阶段潜在的数据陷阱以及它们如何影响后续整个流程。数据收集错误的影響尤为明显。如图所示，任何在这个初始阶段的疏忽将在第8章（ch014.xhtml#sec-ai-training）和第13章（ch019.xhtml#sec-ml-operations）中讨论的模型评估和部署阶段变得明显，可能导致放弃整个模型并重新开始的昂贵后果。因此，从一开始就投资于数据工程技术将帮助我们及早发现错误，减轻这些级联效应。'
- en: '![](../media/file74.svg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file74.svg)'
- en: 'Figure 6.3: **Data Quality Cascades**: Errors introduced early in the machine
    learning workflow amplify across subsequent stages, increasing costs and potentially
    leading to flawed predictions or harmful outcomes. Recognizing these cascades
    motivates proactive investment in data engineering and quality control to mitigate
    risks and ensure reliable system performance. Source: ([Sambasivan et al. 2021](ch058.xhtml#ref-sambasivan2021everyone)).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：**数据质量级联**：在机器学习工作流程早期引入的错误会在后续阶段放大，增加成本，并可能导致预测错误或有害结果。认识到这些级联现象会促使我们积极投资于数据工程和质量控制，以减轻风险并确保系统性能可靠。来源：([Sambasivan等人2021](ch058.xhtml#ref-sambasivan2021everyone))。
- en: Establishing Governance Principles Early
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早期建立治理原则
- en: With this understanding of how quality issues cascade through ML systems, we
    must establish governance principles that ensure our data engineering systems
    operate within ethical, legal, and business constraints. These principles are
    not afterthoughts to be applied later but foundational requirements that shape
    every technical decision from the outset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了质量问题如何在机器学习系统中级联之后，我们必须建立治理原则，确保我们的数据工程系统在道德、法律和商业约束范围内运行。这些原则不是事后才考虑的应用，而是塑造每个技术决策的基础性要求。
- en: Central to these governance principles, data systems must protect user privacy
    and maintain security throughout their lifecycle. This means implementing access
    controls, encryption, and data minimization practices from the initial system
    design, not adding them as later enhancements. Privacy requirements directly influence
    data collection methods, storage architectures, and processing approaches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些治理原则的核心是，数据系统必须在整个生命周期中保护用户隐私并维护安全性。这意味着从初始系统设计开始实施访问控制、加密和数据最小化实践，而不是作为后续的增强功能添加。隐私要求直接影响数据收集方法、存储架构和处理方法。
- en: Beyond privacy protection, data engineering systems must actively work to identify
    and mitigate bias in data collection, labeling, and processing. This requires
    diverse data collection strategies, representative sampling approaches, and systematic
    bias detection throughout the pipeline. Technical choices about data sources,
    labeling methodologies, and quality metrics all impact system fairness. Hidden
    stratification in data—where subpopulations are underrepresented or exhibit different
    patterns, can cause systematic failures even in well-performing models ([Oakden-Rayner
    et al. 2020](ch058.xhtml#ref-oakden2020hidden)), underscoring why demographic
    balance and representation requires engineering into data collection from the
    outset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了隐私保护之外，数据工程系统必须积极努力识别和减轻数据收集、标注和处理中的偏差。这需要多样化的数据收集策略、代表性的抽样方法，以及在整个流程中的系统性偏差检测。关于数据来源、标注方法和质量指标的技术选择都会影响系统的公平性。数据中的隐藏分层——即子群体代表性不足或表现出不同模式，即使在表现良好的模型中也可能导致系统性失败([Oakden-Rayner等人
    2020](ch058.xhtml#ref-oakden2020hidden))，这强调了为什么人口平衡和代表性需要从一开始就融入数据收集。
- en: Complementing these fairness efforts, systems must maintain clear documentation
    about data sources, processing decisions, and quality criteria. This includes
    implementing data lineage tracking, maintaining processing logs, and establishing
    clear ownership and responsibility for data quality decisions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 补充这些公平性努力，系统必须保持关于数据来源、处理决策和质量标准的清晰文档。这包括实施数据血缘跟踪、维护处理日志，并明确数据质量决策的所有权和责任。
- en: Finally, data systems must comply with relevant regulations such as GDPR, CCPA,
    and domain-specific requirements. Compliance requirements influence data retention
    policies, user consent mechanisms, and cross-border data transfer protocols.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据系统必须遵守相关的法规，如GDPR、CCPA和特定领域的需求。合规要求影响数据保留政策、用户同意机制和跨境数据传输协议。
- en: These governance principles work hand-in-hand with our technical pillars of
    quality, reliability, and scalability. A system cannot be truly reliable if it
    violates user privacy, and quality metrics are meaningless if they perpetuate
    unfair outcomes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些治理原则与我们的质量、可靠性和可扩展性等技术支柱相辅相成。如果一个系统侵犯了用户隐私，那么它就不再是真正可靠的，如果质量指标持续产生不公平的结果，那么它们也就没有意义。
- en: Structured Approach to Problem Definition
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化问题定义方法
- en: Building on these governance foundations, we need a systematic approach to problem
    definition. As Sculley et al. ([2021](ch058.xhtml#ref-sculley2015hidden)) emphasize,
    ML systems require problem framing that goes beyond traditional software development
    approaches. Whether developing recommendation engines processing millions of user
    interactions, computer vision systems analyzing medical images, or natural language
    models handling diverse text data, each system brings unique challenges that require
    careful consideration within our governance and technical framework.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些治理基础之上，我们需要一个系统性的方法来定义问题。正如Sculley等人([2021](ch058.xhtml#ref-sculley2015hidden))强调的，机器学习系统需要的问题框架超越了传统的软件开发方法。无论是开发处理数百万用户交互的推荐引擎，分析医学图像的计算机视觉系统，还是处理多样化文本数据的自然语言模型，每个系统都带来了独特挑战，需要在我们的治理和技术框架内进行仔细考虑。
- en: Within this context, establishing clear objectives provides unified direction
    that guides the entire project, from data collection strategies through deployment
    operations. These objectives must balance technical performance with governance
    requirements, creating measurable outcomes that include both accuracy metrics
    and fairness criteria.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，确立明确的目标提供了统一的方向，指导整个项目，从数据收集策略到部署操作。这些目标必须在技术性能和治理要求之间取得平衡，创造可衡量的成果，包括准确性指标和公平性标准。
- en: 'This systematic approach to problem definition ensures that governance principles
    and technical requirements are integrated from the start rather than retrofitted
    later. To achieve this integration, we identify the key steps that must precede
    any data collection effort:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统性的问题定义方法确保了治理原则和技术要求从一开始就得到整合，而不是事后进行修改。为了实现这种整合，我们确定了任何数据收集工作之前必须采取的关键步骤：
- en: Identify and clearly state the problem definition
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定并明确表述问题定义
- en: Set clear objectives to meet
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设定明确的目标以满足
- en: Establish success benchmarks
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立成功基准
- en: Understand end-user engagement/use
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解最终用户的参与/使用情况
- en: Understand the constraints and limitations of deployment
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解部署的约束和限制
- en: Perform data collection.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行数据收集。
- en: Iterate and refine.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代和改进。
- en: Framework Application Through Keyword Spotting Case Study
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过关键词检测案例研究应用框架
- en: To demonstrate how these systematic principles work in practice, Keyword Spotting
    (KWS) systems provide an ideal case study for applying our four-pillar framework
    to real-world data engineering challenges. These systems, which power voice-activated
    devices like smartphones and smart speakers, must detect specific wake words (such
    as “OK, Google” or “Alexa”) within continuous audio streams while operating under
    strict resource constraints.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这些系统化原则在实际中的运作方式，关键词检测（KWS）系统为我们提供了一个理想的案例研究，用于将我们的四支柱框架应用于现实世界的数据工程挑战。这些系统为智能手机和智能扬声器等语音激活设备提供动力，必须在连续的音频流中检测特定的唤醒词（如“OK,
    Google”或“Alexa”），同时在严格的资源限制下运行。
- en: 'As shown in [Figure 6.4](ch012.xhtml#fig-keywords), KWS systems operate as
    lightweight, always-on front-ends that trigger more complex voice processing systems.
    These systems demonstrate the interconnected challenges across all four pillars
    of our framework ([Section 6.2](ch012.xhtml#sec-data-engineering-four-pillars-framework-5cab)):
    Quality (accuracy across diverse environments), Reliability (consistent battery-powered
    operation), Scalability (severe memory constraints), and Governance (privacy protection).
    These constraints explain why many KWS systems support only a limited number of
    languages: collecting high-quality, representative voice data for smaller linguistic
    populations proves prohibitively difficult given governance and scalability challenges,
    demonstrating how all four pillars must work together to achieve successful deployment.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.4](ch012.xhtml#fig-keywords)所示，KWS系统作为轻量级、始终开启的前端运行，触发更复杂的语音处理系统。这些系统展示了我们框架四个支柱之间的相互挑战（[第6.2节](ch012.xhtml#sec-data-engineering-four-pillars-framework-5cab)）：质量（跨不同环境的准确性）、可靠性（一致的电池供电操作）、可扩展性（严重的内存限制）和治理（隐私保护）。这些限制解释了为什么许多KWS系统只支持有限的语言：由于治理和可扩展性的挑战，为较小的语言群体收集高质量、代表性的语音数据变得难以承受，这证明了所有四个支柱必须协同工作才能实现成功的部署。
- en: '![](../media/file75.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file75.png)'
- en: 'Figure 6.4: **Keyword Spotting System**: A typical deployment of keyword spotting
    (KWS) technology in a voice-activated device, where a constantly-listening system
    detects a wake word to initiate further processing. this example demonstrates
    how KWS serves as a lightweight, always-on front-end for more complex voice interfaces.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：**关键词检测系统**：在语音激活设备中典型部署的关键词检测（KWS）技术，其中持续监听的系统检测到唤醒词以启动进一步处理。本例演示了KWS如何作为轻量级、始终开启的前端，为更复杂的语音界面服务。
- en: 'With this framework understanding established, we can apply our problem definition
    approach to our KWS example, demonstrating how the four pillars guide practical
    engineering decisions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立这个框架理解之后，我们可以将我们的问题定义方法应用于我们的KWS示例，展示四个支柱如何指导实际工程决策：
- en: '**Identifying the Problem**: KWS detects specific keywords amidst ambient sounds
    and other spoken words. The primary problem is to design a system that can recognize
    these keywords with high accuracy, low latency, and minimal false positives or
    negatives, especially when deployed on devices with limited computational resources.
    A well-specified problem definition for developing a new KWS model should identify
    the desired keywords along with the envisioned application and deployment scenario.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别问题**：KWS在环境声音和其他说话声中检测特定关键词。主要问题在于设计一个能够以高精度、低延迟和最小化误报或漏报识别这些关键词的系统，尤其是在计算资源有限的设备上。为开发新的KWS模型制定一个明确的问题定义，应确定所需的关键词以及预期的应用和部署场景。'
- en: '**Setting Clear Objectives**: The objectives for a KWS system must balance
    multiple competing requirements. Performance targets include achieving high accuracy
    rates (98% accuracy in keyword detection) while ensuring low latency (keyword
    detection and response within 200 milliseconds). Resource constraints demand minimizing
    power consumption to extend battery life on embedded devices and ensuring the
    model size is optimized for available memory on the device.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设定明确目标**：KWS系统的目标必须平衡多个相互竞争的要求。性能目标包括实现高准确率（关键词检测准确率达到98%）的同时确保低延迟（关键词检测和响应在200毫秒内完成）。资源限制要求最小化功耗以延长嵌入式设备的电池寿命，并确保模型大小针对设备上可用的内存进行了优化。'
- en: '**Benchmarks for Success**: Establish clear metrics to measure the success
    of the KWS system. Key performance indicators include true positive rate (the
    percentage of correctly identified keywords relative to all spoken keywords) and
    false positive rate (the percentage of non-keywords including silence, background
    noise, and out-of-vocabulary words incorrectly identified as keywords). Detection/error
    tradeoff curves evaluate KWS on streaming audio representative of real-world deployment
    scenarios by comparing false accepts per hour (false positives over total evaluation
    audio duration) against false rejection rate (missed keywords relative to spoken
    keywords in evaluation audio), as demonstrated by Nayak et al. ([2022](ch058.xhtml#ref-nayak2022improving)).
    Operational metrics track response time (keyword utterance to system response)
    and power consumption (average power used during keyword detection).'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成功的基准**：建立明确的指标来衡量KWS系统的成功。关键性能指标包括真正阳性率（正确识别的关键词占所有说话关键词的百分比）和假阳性率（包括静音、背景噪音和不在词汇表中的词被错误地识别为关键词的百分比）。检测/错误权衡曲线通过比较每小时假接受率（总评估音频持续时间内的假阳性）与假拒绝率（评估音频中未识别的关键词与说话关键词的比例）来评估KWS在代表现实部署场景的流式音频上的表现，如Nayak等人所展示的([2022](ch058.xhtml#ref-nayak2022improving))。操作指标跟踪响应时间（关键词语音到系统响应）和功耗（关键词检测期间的平均功耗）。'
- en: '**Stakeholder Engagement and Understanding**: Engage with stakeholders, which
    include device manufacturers, hardware and software developers, and end-users.
    Understand their needs, capabilities, and constraints. Different stakeholders
    bring competing priorities: device manufacturers might prioritize low power consumption,
    software developers might emphasize ease of integration, and end-users would prioritize
    accuracy and responsiveness. Balancing these competing requirements shapes system
    architecture decisions throughout development.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利益相关者参与和理解**：与利益相关者进行互动，这些利益相关者包括设备制造商、硬件和软件开发者以及最终用户。了解他们的需求、能力和限制。不同的利益相关者会带来相互竞争的优先级：设备制造商可能会优先考虑低功耗，软件开发者可能会强调易于集成，而最终用户则会优先考虑准确性和响应速度。在整个开发过程中，平衡这些相互竞争的要求将塑造系统架构决策。'
- en: '**Understanding the Constraints and Limitations of Embedded Systems**: Embedded
    devices come with their own set of challenges that shape KWS system design. Memory
    limitations require extremely lightweight models, typically as small as 16 KB
    to fit in the always-on island of the SoC[3](#fn3), with this constraint covering
    only model weights while preprocessing code must also fit within tight memory
    bounds. Processing power constraints from limited computational capabilities (a
    few hundred MHz of clock speed) demand aggressive model optimization for efficiency.
    Power consumption becomes critical since most embedded devices run on batteries,
    requiring the KWS system to achieve sub-milliwatt power consumption during continuous
    listening. Environmental challenges add another layer of complexity, as devices
    must function effectively across diverse deployment scenarios ranging from quiet
    bedrooms to noisy industrial settings.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**理解嵌入式系统的限制和局限性**：嵌入式设备自带一套挑战，这些挑战塑造了关键词识别系统（KWS）的设计。内存限制需要极其轻量级的模型，通常小至16
    KB以适应SoC的始终在线岛[3](#fn3)，这一限制仅涵盖模型权重，而预处理代码也必须适应严格的内存限制。来自有限的计算能力（几百兆赫的时钟速度）的处理能力限制要求对模型进行激进优化以提高效率。功耗变得至关重要，因为大多数嵌入式设备都使用电池供电，要求KWS系统在持续监听期间实现亚毫瓦的功耗。环境挑战又增加了一层复杂性，因为设备必须在从安静的卧室到嘈杂的工业环境等不同的部署场景中有效运行。'
- en: '**Data Collection and Analysis**: For a KWS system, data quality and diversity
    determine success. The dataset must capture demographic diversity by including
    speakers with various accents across age and gender to ensure wide-ranging recognition
    support. Keyword variations require attention since people pronounce wake words
    differently, requiring the dataset to capture these pronunciation nuances and
    slight variations. Background noise diversity proves essential, necessitating
    data samples that include or are augmented with different ambient noises to train
    the model for real-world scenarios ranging from quiet environments to noisy conditions.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集和分析**：对于一个KWS系统来说，数据质量和多样性决定了成功。数据集必须通过包括不同年龄和性别的各种口音的说话者来捕捉人口统计学上的多样性，以确保广泛的识别支持。关键词的变化需要关注，因为人们发音唤醒词的方式不同，需要数据集捕捉这些发音细微差别和轻微变化。背景噪声的多样性证明是至关重要的，需要包括或增强不同环境噪声的数据样本，以训练模型适应从安静环境到嘈杂条件等现实世界的场景。'
- en: '**Iterative Feedback and Refinement**: Finally, once a prototype KWS system
    is developed, teams must ensure the system remains aligned with the defined problem
    and objectives as deployment scenarios change over time and use-cases evolve.
    This requires testing in real-world scenarios, gathering feedback about whether
    some users or deployment scenarios encounter underperformance relative to others,
    and iteratively refining both the dataset and model based on observed failure
    patterns.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代反馈和改进**：最后，一旦开发出一个原型KWS系统，团队必须确保系统随着时间的推移和用例的发展与定义的问题和目标保持一致。这需要在现实世界场景中进行测试，收集有关某些用户或部署场景相对于其他用户或场景是否遇到性能不足的反馈，并根据观察到的失败模式迭代地改进数据集和模型。'
- en: Building on this problem definition foundation, our KWS system demonstrates
    how different data collection approaches combine effectively across the project
    lifecycle. Pre-existing datasets like Google’s Speech Commands ([Warden 2018](ch058.xhtml#ref-warden2018speech))
    provide a foundation for initial development, offering carefully curated voice
    samples for common wake words. However, these datasets often lack diversity in
    accents, environments, and languages, necessitating additional strategies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题定义的基础上，我们的关键词唤醒系统（KWS）展示了不同的数据收集方法如何在项目生命周期中有效地结合。现有的数据集，如谷歌的语音命令（[Warden
    2018](ch058.xhtml#ref-warden2018speech)），为初始开发提供了基础，提供了精心挑选的常见唤醒词的语音样本。然而，这些数据集通常在口音、环境和语言方面缺乏多样性，需要额外的策略。
- en: To address coverage gaps, web scraping supplements baseline datasets by gathering
    diverse voice samples from video platforms and speech databases, capturing natural
    speech patterns and wake word variations. Crowdsourcing platforms like Amazon
    Mechanical Turk[4](#fn4) enable targeted collection of wake word samples across
    different demographics and environments, particularly valuable for underrepresented
    languages or specific acoustic conditions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决覆盖范围上的差距，网络爬虫通过从视频平台和语音数据库收集多样化的语音样本来补充基线数据集，捕捉自然语言模式和唤醒词的变化。众包平台如亚马逊机械师（[4](#fn4)）允许针对不同人口统计学和环境收集唤醒词样本，对于代表性不足的语言或特定的声学条件特别有价值。
- en: Finally, synthetic data generation fills remaining gaps through speech synthesis
    ([Werchniak et al. 2021](ch058.xhtml#ref-werchniak2021exploring)) and audio augmentation,
    creating unlimited wake word variations across acoustic environments, speaker
    characteristics, and background conditions. This comprehensive approach enables
    KWS systems that perform robustly across diverse real-world conditions while demonstrating
    how systematic problem definition guides data strategy throughout the project
    lifecycle.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过语音合成（[Werchniak et al. 2021](ch058.xhtml#ref-werchniak2021exploring)）和音频增强，合成数据生成填补了剩余的空白，在声学环境、说话者特征和背景条件下创建无限的唤醒词变化。这种全面的方法使得KWS系统能够在各种现实世界条件下稳健地运行，同时展示了如何通过系统性的问题定义指导整个项目生命周期中的数据策略。
- en: With our framework principles established through the KWS case study, we now
    examine how these abstract concepts translate into operational reality through
    data pipeline architecture.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过KWS案例研究建立我们的框架原则，我们现在探讨这些抽象概念如何通过数据管道架构转化为操作现实。
- en: Data Pipeline Architecture
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道架构
- en: Data pipelines serve as the systematic implementation of our four-pillar framework,
    transforming raw data into ML-ready formats while maintaining quality, reliability,
    scalability, and governance standards. Rather than simple linear data flows, these
    are complex systems that must orchestrate multiple data sources, transformation
    processes, and storage systems while ensuring consistent performance under varying
    load conditions. Pipeline architecture translates our abstract framework principles
    into operational reality, where each pillar manifests as concrete engineering
    decisions about validation strategies, error handling mechanisms, throughput optimization,
    and observability infrastructure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道是我们四支柱框架的系统化实现，将原始数据转换为机器学习准备格式，同时保持质量、可靠性、可扩展性和治理标准。这些不是简单的线性数据流，而是复杂的系统，必须协调多个数据源、转换过程和存储系统，同时确保在不同负载条件下的性能一致性。管道架构将我们的抽象框架原则转化为操作现实，其中每一支柱都表现为关于验证策略、错误处理机制、吞吐量优化和可观察性基础设施的具体工程决策。
- en: To illustrate these concepts, our KWS system pipeline architecture must handle
    continuous audio streams, maintain low-latency processing for real-time keyword
    detection, and ensure privacy-preserving data handling. The pipeline must scale
    from development environments processing sample audio files to production deployments
    handling millions of concurrent audio streams while maintaining strict quality
    and governance standards.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些概念，我们的关键词提取系统（KWS）的管道架构必须处理连续的音频流，保持低延迟处理以实现实时关键词检测，并确保数据处理的隐私保护。该管道必须从处理样本音频文件的开发环境扩展到处理数百万并发音频流的生产部署，同时保持严格的质量和治理标准。
- en: '![](../media/file76.svg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file76.svg)'
- en: 'Figure 6.5: **Data Pipeline Architecture**: Modular pipelines ingest, process,
    and deliver data for machine learning tasks, enabling independent scaling of components
    and improved data quality control. Distinct stages (ingestion, storage, and preparation)
    transform raw data into a format suitable for model training and validation, forming
    the foundation of reliable ML systems.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：**数据管道架构**：模块化管道摄取、处理和交付数据以供机器学习任务使用，使组件能够独立扩展，并提高数据质量控制。不同的阶段（摄取、存储和准备）将原始数据转换为适合模型训练和验证的格式，形成可靠机器学习系统的基础。
- en: 'As shown in the architecture diagram, ML data pipelines consist of several
    distinct layers: data sources, ingestion, processing, labeling, storage, and ML
    training ([Figure 6.5](ch012.xhtml#fig-pipeline-flow)). Each layer plays a specific
    role in the data preparation workflow, and selecting appropriate technologies
    for each layer requires understanding how our four framework pillars manifest
    at each stage. Rather than treating these layers as independent components to
    be optimized separately, we examine how quality requirements at one stage affect
    scalability constraints at another, how reliability needs shape governance implementations,
    and how the pillars interact to determine overall system effectiveness.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如架构图所示，机器学习数据管道由几个不同的层组成：数据源、摄取、处理、标记、存储和机器学习训练（[图6.5](ch012.xhtml#fig-pipeline-flow)）。每一层在数据准备工作流程中扮演着特定的角色，为每一层选择适当的技术需要理解我们的四个框架支柱在每个阶段的表现。我们不是将这些层视为独立的组件分别优化，而是考察一个阶段的质量要求如何影响另一个阶段的可扩展性约束，可靠性需求如何塑造治理实施，以及支柱如何相互作用以确定整体系统有效性。
- en: Central to these design decisions, data pipeline design is constrained by storage
    hierarchies and I/O bandwidth limitations rather than CPU capacity. Understanding
    these constraints enables building efficient systems that can handle modern ML
    workloads. Storage hierarchy trade-offs, ranging from high-latency object storage
    (ideal for archival) to low-latency in-memory stores (essential for real-time
    serving), and bandwidth limitations (spinning disks at 100-200 MB/s versus RAM
    at 50-200 GB/s) shape every pipeline decision. Detailed storage architecture considerations
    are covered in [Section 6.9](ch012.xhtml#sec-data-engineering-strategic-storage-architecture-87b1).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设计决策的核心是，数据管道设计受限于存储层次结构和I/O带宽限制，而不是CPU容量。理解这些限制能够构建能够处理现代机器学习工作负载的高效系统。从高延迟对象存储（适用于归档）到低延迟内存存储（对于实时服务至关重要）的存储层次结构权衡，以及带宽限制（100-200
    MB/s的旋转磁盘与50-200 GB/s的RAM）塑造了每个管道决策。详细的存储架构考虑因素在[第6.9节](ch012.xhtml#sec-data-engineering-strategic-storage-architecture-87b1)中有所介绍。
- en: Given these performance constraints, design decisions should align with specific
    requirements. For streaming data, consider whether you need message durability
    (ability to replay failed processing), ordering guarantees (maintaining event
    sequence), or geographic distribution. For batch processing, the key decision
    factors include data volume relative to memory, processing complexity, and whether
    computation must be distributed. Single-machine tools suffice for gigabyte-scale
    data, but terabyte-scale processing needs distributed frameworks that partition
    work across clusters. The interactions between these layers, viewed through our
    four-pillar lens, determine the system’s overall effectiveness and guide the specific
    engineering decisions we examine in the following subsections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些性能限制，设计决策应与特定要求一致。对于流数据，考虑你是否需要消息持久性（重放失败处理的能 力）、排序保证（保持事件序列）或地理分布。对于批量处理，关键决策因素包括与内存相比的数据量、处理复杂性和计算是否必须分布式。对于千兆级数据，单机工具就足够了，但对于太字节级处理，则需要分布式框架，将工作分区到集群中。通过我们的四支柱视角来看这些层之间的交互，决定了系统的整体有效性，并指导我们在以下小节中检查的具体工程决策。
- en: Quality Through Validation and Monitoring
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过验证和监控实现质量
- en: Quality represents the foundation of reliable ML systems, and pipelines implement
    quality through systematic validation and monitoring at every stage. Production
    experience shows that data pipeline issues represent a major source of ML failures,
    with studies citing 30-70% attribution rates for schema changes breaking downstream
    processing, distribution drift degrading model accuracy, or data corruption silently
    introducing errors ([Sculley et al. 2021](ch058.xhtml#ref-sculley2015hidden)).
    These failures prove particularly insidious because they often don’t cause obvious
    system crashes but instead slowly degrade model performance in ways that become
    apparent only after affecting users. The quality pillar demands proactive monitoring
    and validation that catches issues before they cascade into model failures.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 质量是可靠机器学习系统的基石，管道通过在每个阶段进行系统性的验证和监控来实现质量。生产经验表明，数据管道问题代表机器学习失败的主要来源，研究表明，模式更改导致下游处理中断、分布漂移降低模型精度或数据损坏静默引入错误的归因率在30-70%之间([Sculley等人2021](ch058.xhtml#ref-sculley2015hidden))。这些故障证明尤其狡猾，因为它们通常不会引起明显的系统崩溃，而是以只有影响用户后才会显现的方式缓慢降低模型性能。质量支柱要求主动监控和验证，以在问题演变成模型失败之前捕捉到它们。
- en: 'Understanding these metrics in practice requires examining how production teams
    implement monitoring at scale. Most organizations adopt severity-based alerting
    systems where different types of failures trigger different response protocols.
    The most critical alerts indicate complete system failure: the pipeline has stopped
    processing entirely, showing zero throughput for more than 5 minutes, or a primary
    data source has become completely unavailable. These situations demand immediate
    attention because they halt all downstream model training or serving. More subtle
    degradation patterns require different detection strategies. When throughput drops
    to 80% of baseline levels, or error rates climb above 5%, or quality metrics drift
    more than 2 standard deviations from training data characteristics, the system
    signals degradation requiring urgent but not immediate attention. These gradual
    failures often prove more dangerous than complete outages because they can persist
    undetected for hours or days, silently corrupting model inputs and degrading prediction
    quality.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中理解这些指标需要检查生产团队如何实施大规模监控。大多数组织采用基于严重程度的警报系统，不同类型的故障触发不同的响应协议。最关键的警报表明系统完全故障：管道完全停止处理，超过5分钟内显示零吞吐量，或者主要数据源完全不可用。这些情况需要立即关注，因为它们会停止所有下游模型训练或服务。更微妙的退化模式需要不同的检测策略。当吞吐量降至基线水平的80%或错误率上升至5%以上，或者质量指标偏离训练数据特征超过2个标准差时，系统会发出需要紧急但非立即关注的退化信号。这些渐进性故障往往比完全中断更危险，因为它们可以在数小时或数天内未被检测到，默默地破坏模型输入并降低预测质量。
- en: 'Consider how these principles apply to a recommendation system processing user
    interaction events. With a baseline throughput of 50,000 records per second, the
    monitoring system tracks several interdependent signals. Instantaneous throughput
    alerts fire if processing drops below 40,000 records per second for more than
    10 minutes, accounting for normal traffic variation while catching genuine capacity
    or processing problems. Each feature in the data stream has its own quality profile:
    if a feature like user_age shows null values in more than 5% of records when the
    training data contained less than 1% nulls, something has likely broken in the
    upstream data source. Duplicate detection runs on sampled data, watching for the
    same event appearing multiple times—a pattern that might indicate retry logic
    gone wrong or a database query accidentally returning the same records repeatedly.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些原则如何应用于处理用户交互事件的推荐系统。在基准吞吐量为每秒50,000条记录的情况下，监控系统跟踪几个相互依赖的信号。如果处理速度低于每秒40,000条记录超过10分钟，即时吞吐量警报就会触发，这既考虑了正常交通变化，也捕捉到了真正的容量或处理问题。数据流中的每个特征都有自己的质量配置文件：如果一个特征，如用户年龄，在训练数据中少于1%的空值时，在超过5%的记录中显示空值，那么上游数据源可能出现了问题。在样本数据上运行重复检测，观察同一事件出现多次——这可能是重试逻辑出错或数据库查询意外重复返回相同记录的模式。
- en: 'These monitoring dimensions become particularly important when considering
    end-to-end latency. The system must track not just whether data arrives, but how
    long it takes to flow through the entire pipeline from the moment an event occurs
    to when the resulting features become available for model inference. When 95th
    percentile[5](#fn5) latency exceeds 30 seconds in a system with a 10-second service
    level agreement, the monitoring system needs to pinpoint which pipeline stage
    introduced the delay: ingestion, transformation, validation, or storage.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑端到端延迟时，这些监控维度变得尤为重要。系统必须跟踪的不仅是数据是否到达，还有从事件发生到结果特征可用于模型推理的整个管道的流动时间。当一个具有10秒服务级别协议的系统在95%的百分位延迟超过30秒时，监控系统需要确定是哪个管道阶段引入了延迟：摄取、转换、验证或存储。
- en: Quality monitoring extends beyond simple schema validation to statistical properties
    that capture whether serving data resembles training data. Rather than just checking
    that values fall within valid ranges, production systems track rolling statistics
    over 24-hour windows. For numerical features like transaction_amount or session_duration,
    the system computes means and standard deviations continuously, then applies statistical
    tests like the Kolmogorov-Smirnov test[6](#fn6) to compare serving distributions
    against training distributions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 质量监控不仅限于简单的模式验证，还包括统计属性，这些属性可以捕捉到服务数据是否类似于训练数据。生产系统不仅仅是检查值是否在有效范围内，而是在24小时窗口内跟踪滚动统计。对于像交易金额或会话持续时间这样的数值特征，系统会持续计算平均值和标准差，然后应用如柯尔莫哥洛夫-斯米诺夫检验[6](#fn6)等统计检验，以比较服务分布与训练分布。
- en: 'Categorical features require different statistical approaches. Instead of comparing
    means and variances, monitoring systems track category frequency distributions.
    When new categories appear that never existed in training data, or when existing
    categories shift substantially in relative frequency—say, the proportion of “mobile”
    versus “desktop” traffic changes by more than 20%, the system flags potential
    data quality issues or genuine distribution shifts. This statistical vigilance
    catches subtle problems that simple schema validation misses entirely: imagine
    if age values remain in the valid range of 18-95, but the distribution shifts
    from primarily 25-45 year olds to primarily 65+ year olds, indicating the data
    source has changed in ways that will affect model performance.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 分类别特征需要不同的统计方法。监控系统不是比较平均值和方差，而是跟踪类别频率分布。当出现训练数据中从未存在的新类别，或者当现有类别在相对频率上发生实质性变化——例如，“移动”与“桌面”流量的比例变化超过20%，系统会标记潜在的数据质量问题和真正的分布变化。这种统计警觉性可以捕捉到简单模式验证完全忽略的微妙问题：想象一下，如果年龄值保持在18-95的有效范围内，但分布从主要25-45岁的人转变为主要65岁以上的人，这表明数据源发生了会影响模型性能的变化。
- en: Validation at the pipeline level encompasses multiple strategies working together.
    Schema validation executes synchronously as data enters the pipeline, rejecting
    malformed records immediately before they can propagate downstream. Modern tools
    like TensorFlow Data Validation (TFDV)[7](#fn7) automatically infer schemas from
    training data, capturing expected data types, value ranges, and presence requirements.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道级别的验证包括多种策略协同工作。模式验证在数据进入管道时同步执行，立即拒绝格式不正确的记录，防止它们传播到下游。现代工具如TensorFlow数据验证（TFDV）[7](#fn7)自动从训练数据推断模式，捕获预期的数据类型、值范围和存在要求。
- en: This synchronous validation necessarily remains simple and fast, checking properties
    that can be evaluated on individual records in microseconds. More sophisticated
    validation that requires comparing serving data against training data distributions
    or aggregating statistics across many records must run asynchronously to avoid
    blocking the ingestion pipeline. Statistical validation systems typically sample
    1-10% of serving traffic—enough to detect meaningful shifts while avoiding the
    computational cost of analyzing every record. These samples accumulate in rolling
    windows, commonly 1 hour, 24 hours, and 7 days, with different windows revealing
    different patterns. Hourly windows detect sudden shifts like a data source failing
    over to a backup with different characteristics, while weekly windows reveal gradual
    drift in user populations or behavior.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种同步验证必然保持简单和快速，在微秒级检查可以评估的属性。更复杂的验证，需要比较服务数据与训练数据分布或跨多个记录汇总统计，必须异步运行以避免阻塞数据摄取管道。统计验证系统通常抽样1-10%的服务流量——足以检测到有意义的变动，同时避免分析每条记录的计算成本。这些样本累积在滚动窗口中，通常是1小时、24小时和7天，不同的窗口揭示不同的模式。每小时窗口检测到数据源突然切换到具有不同特性的备份，而每周窗口揭示用户群体或行为的逐渐漂移。
- en: Perhaps the most insidious validation challenge arises from training-serving
    skew[8](#fn8), where the same features get computed differently in training versus
    serving environments. This typically happens when training pipelines process data
    in batch using one set of libraries or logic, while serving systems compute features
    in real-time using different implementations. A recommendation system might compute
    “user_lifetime_purchases” in training by joining user profiles against complete
    transaction histories, while the serving system inadvertently uses a cached materialized
    view[9](#fn9) updated only weekly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最隐蔽的验证挑战来自训练-服务偏差[8](#fn8)，在训练与服务环境中，相同的特征计算方式不同。这通常发生在训练管道使用一组库或逻辑批量处理数据时，而服务系统使用不同的实现实时计算特征。推荐系统可能在训练中通过将用户配置文件与完整的交易历史记录连接来计算“user_lifetime_purchases”，而服务系统无意中使用了仅每周更新的缓存物化视图[9](#fn9)。
- en: Reliability Through Graceful Degradation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过优雅降级实现可靠性
- en: 'While quality monitoring detects issues, reliability ensures systems continue
    operating effectively when problems occur. Pipelines face constant challenges:
    data sources become temporarily unavailable, network partitions separate components,
    upstream schema changes break parsing logic, or unexpected load spikes exhaust
    resources. The reliability pillar demands systems that handle these failures gracefully
    rather than cascading into complete outage. This resilience comes from systematic
    failure analysis, intelligent error handling, and automated recovery strategies
    that maintain service continuity even under adverse conditions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当质量监控检测到问题时，可靠性确保系统在出现问题时仍能有效地继续运行。管道面临持续的挑战：数据源暂时不可用，网络分区分离组件，上游模式变更破坏了解析逻辑，或意外的负载峰值耗尽资源。可靠性支柱要求系统能优雅地处理这些故障，而不是导致完全中断。这种弹性来自系统的故障分析、智能错误处理和自动恢复策略，即使在不利条件下也能保持服务连续性。
- en: Systematic failure mode analysis for ML data pipelines reveals predictable patterns
    that require specific engineering countermeasures. Data corruption failures occur
    when upstream systems introduce subtle format changes, encoding issues, or field
    value modifications that pass basic validation but corrupt model inputs. A date
    field switching from “YYYY-MM-DD” to “MM/DD/YYYY” format might not trigger schema
    validation but will break any date-based feature computation. Schema evolution[10](#fn10)
    failures happen when source systems add fields, rename columns, or change data
    types without coordination, breaking downstream processing assumptions that expected
    specific field names or types. Resource exhaustion manifests as gradually degrading
    performance when data volume growth outpaces capacity planning, eventually causing
    pipeline failures during peak load periods.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习数据管道的系统故障模式分析揭示了需要特定工程对策的预测性模式。数据损坏故障发生在上游系统引入微妙的格式变化、编码问题或字段值修改时，这些修改通过了基本验证但破坏了模型输入。日期字段从“YYYY-MM-DD”格式切换到“MM/DD/YYYY”格式可能不会触发模式验证，但会破坏任何基于日期的特征计算。当源系统添加字段、重命名列或更改数据类型时，如果没有协调，就会发生模式演变[10](#fn10)故障，这会破坏下游处理假设，这些假设期望特定的字段名称或类型。资源耗尽表现为当数据量增长超过容量规划时，性能逐渐下降，最终在高峰负载期间导致管道故障。
- en: Building on this failure analysis, effective error handling strategies ensure
    problems are contained and recovered from systematically. Implementing intelligent
    retry logic for transient errors, such as network interruptions or temporary service
    outages, requires exponential backoff strategies to avoid overwhelming recovering
    services. A simple linear retry that attempts reconnection every second would
    flood a struggling service with connection attempts, potentially preventing its
    recovery. Exponential backoff—retrying after 1 second, then 2 seconds, then 4
    seconds, doubling with each attempt—gives services breathing room to recover while
    still maintaining persistence. Many ML systems employ the concept of dead letter
    queues[11](#fn11), using separate storage for data that fails processing after
    multiple retry attempts. This allows for later analysis and potential reprocessing
    of problematic data without blocking the main pipeline ([Kleppmann 2016](ch058.xhtml#ref-kleppmann2017designing)).
    A pipeline processing financial transactions that encounters malformed data can
    route it to a dead letter queue rather than losing critical records or halting
    all processing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这次故障分析，有效的错误处理策略确保问题得到系统性控制并恢复。对于瞬时错误，如网络中断或临时服务中断，实现智能重试逻辑需要指数退避策略以避免压倒恢复中的服务。简单的线性重试每秒尝试重新连接，可能会使服务陷入连接尝试的洪流中，从而可能阻止其恢复。指数退避——在1秒后重试，然后2秒，然后4秒，每次尝试翻倍——给服务提供恢复的空间，同时仍然保持持续性。许多机器学习系统采用死信队列的概念[11](#fn11)，为多次重试后处理失败的数据使用单独的存储。这允许在不妨碍主管道的情况下，对有问题的数据进行后续分析和可能的重新处理([Kleppmann
    2016](ch058.xhtml#ref-kleppmann2017designing))。处理金融交易的管道遇到格式错误的数据时，可以将其路由到死信队列，而不是丢失关键记录或停止所有处理。
- en: Moving beyond ad-hoc error handling, cascade failure prevention requires circuit
    breaker[12](#fn12) patterns and bulkhead isolation to prevent single component
    failures from propagating throughout the system. When a feature computation service
    fails, the circuit breaker pattern stops calling that service after detecting
    repeated failures, preventing the caller from waiting on timeouts that would cascade
    into its own failure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 超越临时的错误处理，级联故障预防需要断路器[12](#fn12)模式和舱壁隔离来防止单个组件故障在整个系统中传播。当一个特征计算服务失败时，断路器模式在检测到重复失败后会停止调用该服务，防止调用者等待可能导致其自身失败的超时。
- en: 'Automated recovery engineering implements sophisticated strategies beyond simple
    retry logic. Progressive timeout increases prevent overwhelming struggling services
    while maintaining rapid recovery for transient issues—initial requests timeout
    after 1 second, but after detecting service degradation, timeouts extend to 5
    seconds, then 30 seconds, giving the service time to stabilize. Multi-tier fallback
    systems provide degraded service when primary data sources fail: serving slightly
    stale cached features when real-time computation fails, or using approximate features
    when exact computation times out. A recommendation system unable to compute user
    preferences from the past 30 days might fall back to preferences from the past
    90 days, providing somewhat less accurate but still useful recommendations rather
    than failing entirely. Comprehensive alerting and escalation procedures ensure
    human intervention occurs when automated recovery fails, with sufficient diagnostic
    information captured during the failure to enable rapid debugging.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 自动恢复工程实现了超越简单重试逻辑的复杂策略。渐进式超时增加防止压倒努力的服务，同时保持对短暂问题的快速恢复——初始请求在1秒后超时，但在检测到服务退化后，超时时间延长到5秒，然后是30秒，给服务稳定的时间。多层回退系统在主数据源失败时提供降级服务：当实时计算失败时，提供稍微陈旧的缓存特征，或者当精确计算超时时使用近似特征。一个无法从过去30天内计算用户偏好的推荐系统可能会回退到过去90天的偏好，提供稍微不准确但仍然有用的推荐，而不是完全失败。全面的警报和升级程序确保在自动化恢复失败时发生人工干预，在失败期间捕获足够的诊断信息，以便快速调试。
- en: These concepts become concrete when considering a financial ML system ingesting
    market data. Error handling might involve falling back to slightly delayed data
    sources if real-time feeds fail, while simultaneously alerting the operations
    team to the issue. Dead letter queues capture malformed price updates for investigation
    rather than dropping them silently. Circuit breakers prevent the system from overwhelming
    a struggling market data provider during recovery. This comprehensive approach
    to error management ensures that downstream processes have access to reliable,
    high-quality data for training and inference tasks, even in the face of the inevitable
    failures that occur in distributed systems at scale.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑一个处理市场数据的金融机器学习系统时，这些概念变得具体。错误处理可能涉及在实时数据源失败时回退到稍微延迟的数据源，同时同时通知操作团队该问题。死信队列捕获异常的价格更新以供调查，而不是静默地丢弃它们。断路器在恢复期间防止系统压倒一个努力的市场数据提供商。这种全面的错误管理方法确保了即使面对分布式系统在规模上不可避免的故障，下游过程也能访问到可靠、高质量的数据，用于训练和推理任务。
- en: Scalability Patterns
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性模式
- en: 'While quality and reliability ensure correct system operation, scalability
    addresses a different challenge: how systems evolve as data volumes grow and ML
    systems mature from prototypes to production services. Pipelines that work effectively
    at gigabyte scale often break at terabyte scale without architectural changes
    that enable distributed processing. Scalability involves designing systems that
    handle growing data volumes, user bases, and computational demands without requiring
    complete redesigns. The key insight is that scalability constraints manifest differently
    across pipeline stages, requiring different architectural patterns for ingestion,
    processing, and storage.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然质量和可靠性确保了系统的正确运行，但可扩展性解决了一个不同的挑战：随着数据量的增长和机器学习系统从原型发展到生产服务，系统如何演变。在千兆规模上有效运行的管道，如果没有能够实现分布式处理的架构变化，通常会在太字节规模上失效。可扩展性涉及设计能够处理不断增长的数据量、用户基础和计算需求，而不需要完全重新设计的系统。关键洞察是，可扩展性约束在管道的不同阶段表现出不同的形式，需要不同的架构模式来处理摄取、处理和存储。
- en: ML systems typically follow two primary ingestion patterns, each with distinct
    scalability characteristics. Batch ingestion involves collecting data in groups
    over a specified period before processing. This method proves appropriate when
    real-time data processing is not critical and data can be processed at scheduled
    intervals. A retail company might use batch ingestion to process daily sales data
    overnight, updating ML models for inventory prediction each morning. Batch processing
    enables efficient use of computational resources by amortizing startup costs across
    large data volumes—a job processing one terabyte might use 100 machines for 10
    minutes, achieving better resource efficiency than maintaining always-on infrastructure.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统通常遵循两种主要的摄入模式，每种模式都具有独特的可扩展性特征。批处理摄入涉及在指定时间段内收集数据组，然后再进行处理。当实时数据处理不是关键且数据可以按计划间隔处理时，这种方法是合适的。一家零售公司可能会使用批处理摄入在夜间处理每日销售数据，每天早上更新库存预测的机器学习模型。批处理通过在大数据量上分摊启动成本，从而有效地利用计算资源——处理一个千兆字节的工作可能需要100台机器10分钟，比维护始终在线的基础设施具有更好的资源效率。
- en: In contrast to this scheduled approach, stream ingestion processes data in real-time
    as it arrives. This pattern proves crucial for applications requiring immediate
    data processing, scenarios where data loses value quickly, and systems that need
    to respond to events as they occur. A financial institution might use stream ingestion
    for real-time fraud detection, processing each transaction as it occurs to flag
    suspicious activity immediately. However, stream processing must handle backpressure[13](#fn13)
    when downstream systems cannot keep pace—when a sudden traffic spike produces
    data faster than processing capacity, the system must either buffer data (requiring
    memory), sample (losing some data), or push back to producers (potentially causing
    failures). Data freshness Service Level Agreements (SLAs)[14](#fn14) formalize
    these requirements, specifying maximum acceptable delays between data generation
    and availability for processing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种计划方法相反，流式处理在数据到达时实时处理数据。这种模式对于需要立即数据处理的应用程序至关重要，在数据价值迅速丧失的场景中，以及需要响应事件的应用程序中。金融机构可能会使用流式处理进行实时欺诈检测，在交易发生时立即处理每个交易以标记可疑活动。然而，当下游系统无法跟上时，流式处理必须处理背压[13](#fn13)——当突然的交通高峰产生比处理能力更快的数据时，系统必须缓冲数据（需要内存）、采样（丢失一些数据）或将数据推回生产者（可能造成故障）。数据新鲜度服务级别协议（SLA）[14](#fn14)正式化了这些要求，指定了数据生成与可用处理之间的最大可接受延迟。
- en: 'Recognizing the limitations of either approach alone, many modern ML systems
    employ hybrid approaches, combining both batch and stream ingestion to handle
    different data velocities and use cases. This flexibility allows systems to process
    both historical data in batches and real-time data streams, providing a comprehensive
    view of the data landscape. Production systems must balance cost versus latency
    trade-offs: real-time processing can cost 10-100x more than batch processing.
    This cost differential arises from several factors: streaming systems require
    always-on infrastructure rather than schedulable resources, maintain redundant
    processing for fault tolerance, need low-latency networking and storage, and cannot
    benefit from the economies of scale that batch processing achieves by amortizing
    startup costs across large data volumes. Techniques for managing streaming systems
    at scale, including backpressure handling and cost optimization, are detailed
    in [Chapter 13](ch019.xhtml#sec-ml-operations).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到单独采用任何一种方法的局限性，许多现代机器学习系统采用混合方法，结合批处理和流式处理来处理不同的数据速度和用例。这种灵活性使得系统可以同时处理批量历史数据和实时数据流，从而提供对数据景观的全面视图。生产系统必须在成本与延迟之间进行权衡：实时处理可能比批处理成本高10-100倍。这种成本差异源于几个因素：流式系统需要始终在线的基础设施而不是可调度资源，需要维护冗余处理以实现容错，需要低延迟的网络和存储，并且无法从批处理通过在大数据量上分摊启动成本所实现的规模经济中受益。在[第13章](ch019.xhtml#sec-ml-operations)中详细介绍了在规模上管理流式系统的技术，包括背压处理和成本优化。
- en: 'Beyond ingestion patterns, distributed processing becomes necessary when single
    machines cannot handle data volumes or processing complexity. The challenge in
    distributed systems is that data must be partitioned across multiple computing
    resources, which introduces coordination overhead. Distributed coordination is
    limited by network round-trip times: local operations complete in microseconds
    while network coordination requires milliseconds, creating a 1000x latency difference.
    This constraint explains why operations requiring global coordination, like computing
    normalization statistics across 100 machines, create bottlenecks. Each partition
    computes local statistics quickly, but combining them requires information from
    all partitions, and the round-trip time for gathering results dominates total
    execution time.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了摄取模式之外，当单机无法处理数据量或处理复杂性时，分布式处理变得必要。分布式系统中的挑战在于数据必须在多个计算资源之间分区，这引入了协调开销。分布式协调受网络往返时间的限制：本地操作在微秒内完成，而网络协调需要毫秒，这造成了1000倍的延迟差异。这一限制解释了为什么需要全局协调的操作，如跨100台机器计算归一化统计信息，会形成瓶颈。每个分区可以快速计算本地统计信息，但合并它们需要所有分区的信息，而收集结果的网络往返时间决定了总执行时间。
- en: 'Data locality becomes critical at this scale. Moving one terabyte of training
    data across the network takes 100+ seconds at 10GB/s, while local SSD access requires
    only 200 seconds at 5GB/s. This similar performance between network transfer and
    local storage drives ML system design toward compute-follows-data architectures
    where processing moves to data rather than data moving to processing. When processing
    nodes access local data at RAM speeds (50-200 GB/s) but must coordinate over networks
    limited to 1-10 GB/s, the bandwidth mismatch creates fundamental bottlenecks.
    Geographic distribution amplifies these challenges: cross-datacenter coordination
    must handle network latency (50-200ms between regions), partial failures during
    network partitions, and regulatory constraints preventing data from crossing borders.
    Understanding which operations parallelize easily versus those requiring expensive
    coordination determines system architecture and performance characteristics.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个规模上，数据局部性变得至关重要。在10GB/s的速度下，将1TB的训练数据通过网络传输需要100多秒，而本地SSD访问只需要5GB/s的速度下200秒。网络传输和本地存储之间相似的性能推动了机器学习系统设计向计算跟随数据架构的转变，即处理移动到数据而不是数据移动到处理。当处理节点以RAM速度（50-200
    GB/s）访问本地数据，但必须通过限于1-10 GB/s的网络进行协调时，带宽不匹配创造了基本的瓶颈。地理分布放大了这些挑战：跨数据中心协调必须处理网络延迟（区域间50-200ms）、网络分区期间的局部故障，以及防止数据跨境的监管限制。了解哪些操作容易并行化，而哪些操作需要昂贵的协调，决定了系统架构和性能特征。
- en: For our KWS system, these scalability patterns manifest concretely through quantitative
    capacity planning that dimensions infrastructure appropriately for workload requirements.
    Development uses batch processing on sample datasets to iterate on model architectures
    rapidly. Training scales to distributed processing across GPU clusters when model
    complexity or dataset size (23 million examples) exceeds single-machine capacity.
    Production deployment requires stream processing for real-time wake word detection
    on millions of concurrent devices. The system must handle traffic spikes when
    news events trigger synchronized usage—millions of users simultaneously asking
    about breaking news.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的KWS系统，这些可扩展性模式通过定量容量规划具体体现，为工作负载需求适当地调整基础设施。开发使用样本数据集的批量处理来快速迭代模型架构。当模型复杂性或数据集大小（2300万个示例）超过单机容量时，训练扩展到GPU集群的分布式处理。生产部署需要流处理，以在数百万并发设备上实现实时唤醒词检测。当新闻事件触发同步使用时，系统必须处理流量高峰——数百万用户同时询问突发新闻。
- en: 'To make these scaling challenges concrete, consider the engineering calculations
    required to dimension our KWS training infrastructure. With 23 million audio samples
    averaging 1 second each at 16 kHz sampling rate (16-bit PCM[15](#fn15)), raw storage
    requires approximately:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些扩展挑战具体化，考虑一下为我们的关键词语音识别（KWS）训练基础设施进行尺寸计算所需的工程计算。在16 kHz采样率（16位PCM[15](#fn15)）下，平均每秒1秒的2300万音频样本，原始存储需要大约：
- en: <semantics><mrow><mtext mathvariant="normal">Storage</mtext><mo>=</mo><mn>23</mn><mo>×</mo><msup><mn>10</mn><mn>6</mn></msup>
    <mrow><mtext mathvariant="normal">samples</mtext></mrow><mo>×</mo><mn>1</mn> <mrow><mtext
    mathvariant="normal">sec</mtext></mrow><mo>×</mo><mn>16</mn><mo>,</mo><mn>000</mn>
    <mrow><mtext mathvariant="normal">samples/sec</mtext></mrow><mo>×</mo><mn>2</mn>
    <mrow><mtext mathvariant="normal">bytes</mtext></mrow><mo>=</mo><mn>736</mn> <mrow><mtext
    mathvariant="normal">GB</mtext></mrow></mrow> <annotation encoding="application/x-tex">\text{Storage}
    = 23 \times 10^6 \text{ samples} \times 1 \text{ sec} \times 16,000 \text{ samples/sec}
    \times 2 \text{ bytes} = 736 \text{ GB}</annotation></semantics>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">存储</mtext><mo>=</mo><mn>23</mn><mo>×</mo><msup><mn>10</mn><mn>6</mn></msup>
    <mrow><mtext mathvariant="normal">样本</mtext></mrow><mo>×</mo><mn>1</mn> <mrow><mtext
    mathvariant="normal">秒</mtext></mrow><mo>×</mo><mn>16</mn><mo>,</mo><mn>000</mn>
    <mrow><mtext mathvariant="normal">样本/秒</mtext></mrow><mo>×</mo><mn>2</mn> <mrow><mtext
    mathvariant="normal">字节</mtext></mrow><mo>=</mo><mn>736</mn> <mrow><mtext mathvariant="normal">GB</mtext></mrow></mrow>
    <annotation encoding="application/x-tex">\text{存储} = 23 \times 10^6 \text{样本}
    \times 1 \text{秒} \times 16,000 \text{样本/秒} \times 2 \text{字节} = 736 \text{GB}</annotation></semantics>
- en: 'Processing these samples into MFCC features (13 coefficients, 100 frames per
    second) reduces storage but increases computational requirements. Feature extraction
    on a modern CPU processes approximately 100x real-time (100 seconds of audio per
    second of computation), requiring:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些样本处理成MFCC特征（13个系数，每秒100帧）可以减少存储需求，但会增加计算需求。在现代CPU上进行的特征提取大约是实时处理速度的100倍（每秒计算100秒的音频），需要：
- en: <semantics><mrow><mtext mathvariant="normal">Processing time</mtext><mo>=</mo><mfrac><mrow><mn>23</mn><mo>×</mo><msup><mn>10</mn><mn>6</mn></msup>
    <mrow><mtext mathvariant="normal">sec of audio</mtext></mrow></mrow><mrow><mn>100</mn>
    <mrow><mtext mathvariant="normal">speedup</mtext></mrow></mrow></mfrac><mo>=</mo><mn>230</mn><mo>,</mo><mn>000</mn>
    <mrow><mtext mathvariant="normal">sec</mtext></mrow><mo>≈</mo><mn>64</mn> <mrow><mtext
    mathvariant="normal">hours on single core</mtext></mrow></mrow><annotation encoding="application/x-tex">\text{Processing
    time} = \frac{23 \times 10^6 \text{ sec of audio}}{100 \text{ speedup}} = 230,000
    \text{ sec} \approx 64 \text{ hours on single core}</annotation></semantics>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">处理时间</mtext><mo>=</mo><mfrac><mrow><mn>23</mn><mo>×</mo><msup><mn>10</mn><mn>6</mn></msup>
    <mrow><mtext mathvariant="normal">秒音频</mtext></mrow></mrow><mrow><mn>100</mn>
    <mrow><mtext mathvariant="normal">加速比</mtext></mrow></mrow></mfrac><mo>=</mo><mn>230</mn><mo>,</mo><mn>000</mn>
    <mrow><mtext mathvariant="normal">秒</mtext></mrow><mo>≈</mo><mn>64</mn> <mrow><mtext
    mathvariant="normal">小时单核处理时间</mtext></mrow></mrow><annotation encoding="application/x-tex">\text{处理时间}
    = \frac{23 \times 10^6 \text{秒音频}}{100 \text{加速比}} = 230,000 \text{秒} \approx
    64 \text{小时单核处理时间}</annotation></semantics>
- en: Distributing across 64 cores reduces this to one hour, demonstrating how parallelization
    enables rapid iteration. Network bandwidth becomes the bottleneck when transferring
    training data from storage to GPU servers—at 10 GB/s network throughput, transferring
    736 GB requires 74 seconds, comparable to the training epoch time itself. This
    analysis reveals why high-throughput storage (NVMe SSDs achieving 5-7 GB/s) and
    network infrastructure (25-100 Gbps interconnects) prove essential for ML workloads
    where data movement time rivals computation time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 分布在64个核心上可以将时间缩短到一小时，展示了并行化如何实现快速迭代。当将训练数据从存储传输到GPU服务器时，网络带宽成为瓶颈——在10 GB/s的网络吞吐量下，传输736
    GB需要74秒，与训练周期时间相当。这一分析揭示了为什么高吞吐量存储（实现5-7 GB/s的NVMe SSD）和网络基础设施（25-100 Gbps互连）对于数据移动时间与计算时间相当的人工智能工作负载至关重要。
- en: Scalability architecture enables this range from development through production
    while maintaining efficiency at each stage, with capacity planning ensuring infrastructure
    appropriately dimensions for workload requirements.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性架构使得从开发到生产的整个过程中都能保持效率，同时通过容量规划确保基础设施能够适当地适应工作负载需求。
- en: Governance Through Observability
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过可观察性进行治理
- en: Having addressed functional requirements through quality, reliability, and scalability,
    we turn to the governance pillar. The governance pillar manifests in pipelines
    as comprehensive observability—the ability to understand what data flows through
    the system, how it transforms, and who accesses it. Effective governance requires
    tracking data lineage from sources through transformations to final datasets,
    maintaining audit trails for compliance, and implementing access controls that
    enforce organizational policies. Unlike the other pillars that focus primarily
    on system functionality, governance ensures operations occur within legal, ethical,
    and business constraints while maintaining transparency and accountability.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过质量、可靠性和可扩展性解决了功能需求后，我们转向治理支柱。治理支柱在管道中表现为全面的可观察性——理解数据通过系统流动、如何转换以及谁访问它的能力。有效的治理需要跟踪数据来源通过转换到最终数据集，维护审计跟踪以符合规定，并实施访问控制以执行组织政策。与其他主要关注系统功能的支柱不同，治理确保操作在法律、伦理和商业约束内进行，同时保持透明度和问责制。
- en: 'Data lineage tracking captures the complete provenance of every dataset: which
    raw sources contributed data, what transformations were applied, when processing
    occurred, and what version of processing code executed. For ML systems, lineage
    becomes essential for debugging model behavior and ensuring reproducibility. When
    a model prediction proves incorrect, engineers need to trace back through the
    pipeline: which training data contributed to this prediction, what quality metrics
    did that data have, what transformations were applied, and can we recreate this
    exact scenario for investigation? Modern lineage systems like Apache Atlas, Amundsen,
    or commercial offerings instrument pipelines to automatically capture this flow.
    Each pipeline stage annotates data with metadata describing its provenance, creating
    an audit trail that enables both debugging and compliance.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来源跟踪捕获每个数据集的完整来源：哪些原始来源贡献了数据，应用了哪些转换，何时进行加工，以及执行了哪个版本的加工代码。对于机器学习系统，来源对于调试模型行为和确保可重复性至关重要。当一个模型预测被证明是错误的时，工程师需要通过管道回溯：哪些训练数据贡献了这个预测，这些数据有什么质量指标，应用了哪些转换，我们能否为调查重新创建这个确切场景？现代来源系统如Apache
    Atlas、Amundsen或商业产品通过为管道添加元数据来自动捕获这种流动。每个管道阶段都会用描述其来源的元数据注释数据，创建一个审计跟踪，既可用于调试又可用于合规性。
- en: 'Audit trails complement lineage by recording who accessed data and when. Regulatory
    frameworks like GDPR require organizations to demonstrate appropriate data handling,
    including tracking access to personal information. ML pipelines implement audit
    logging at data access points: when training jobs read datasets, when serving
    systems retrieve features, or when engineers query data for analysis. These logs
    typically capture user identity, timestamp, data accessed, and purpose. For a
    healthcare ML system, audit trails demonstrate compliance by showing that only
    authorized personnel accessed patient data, that access occurred for legitimate
    medical purposes, and that data wasn’t retained longer than allowed. The scale
    of audit logging in production systems can be substantial—a high-traffic recommendation
    system might generate millions of audit events daily—requiring efficient log storage
    and querying infrastructure.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 审计跟踪通过记录谁访问了数据以及何时访问来补充数据来源。像GDPR这样的监管框架要求组织证明适当的数据处理，包括跟踪对个人信息访问的记录。机器学习管道在数据访问点实现审计日志记录：当训练作业读取数据集时，当服务系统检索特征时，或者当工程师查询数据进行分析时。这些日志通常捕获用户身份、时间戳、访问的数据和目的。对于医疗保健机器学习系统，审计跟踪通过显示只有授权人员访问了患者数据，访问是出于合法的医疗目的，并且数据没有保留超过允许的时间来证明合规性。生产系统中的审计日志规模可能很大——高流量的推荐系统每天可能会生成数百万审计事件——需要高效的日志存储和查询基础设施。
- en: Access controls enforce policies about who can read, write, or transform data
    at each pipeline stage. Rather than simple read/write permissions, ML systems
    often implement attribute-based access control where policies consider data sensitivity,
    user roles, and access context. A data scientist might access anonymized training
    data freely but require approval for raw data containing personal information.
    Production serving systems might read feature data but never write it, preventing
    accidental corruption. Access controls integrate with data catalogs that maintain
    metadata about data sensitivity, compliance requirements, and usage restrictions,
    enabling automated policy enforcement as data flows through pipelines.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制强制执行关于谁可以在每个管道阶段读取、写入或转换数据的策略。除了简单的读写权限之外，机器学习系统通常实现基于属性的访问控制，其中策略考虑数据敏感性、用户角色和访问上下文。数据科学家可以自由访问匿名化训练数据，但需要批准包含个人信息的原始数据。生产服务系统可能读取特征数据，但从不写入，以防止意外损坏。访问控制与维护数据敏感度、合规性要求和使用限制元数据的数据目录集成，使得在数据通过管道流动时能够实现自动策略执行。
- en: 'Provenance metadata enables reproducibility essential for both debugging and
    compliance. When a model trained six months ago performed better than current
    models, teams need to recreate that training environment: exact data version,
    transformation parameters, and code versions. ML systems implement this through
    comprehensive metadata capture: training jobs record dataset checksums, transformation
    parameter values, random seeds for reproducibility, and code version hashes. Feature
    stores maintain historical feature values, enabling point-in-time reconstruction
    of training conditions. For our KWS system, this means tracking which version
    of forced alignment generated labels, what audio normalization parameters were
    applied, what synthetic data generation settings were used, and which crowdsourcing
    batches contributed to training data.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 版本元数据使可重复性成为调试和合规性的必要条件。当六个月前训练的模型比当前模型表现更好时，团队需要重新创建那个训练环境：确切的数据版本、转换参数和代码版本。机器学习系统通过全面的元数据捕获来实现这一点：训练作业记录数据集校验和、转换参数值、用于可重复性的随机种子和代码版本哈希。特征存储维护历史特征值，使得可以重建训练条件。对于我们关键词识别系统来说，这意味着跟踪哪个版本的强制对齐生成了标签，应用了哪些音频归一化参数，使用了哪些合成数据生成设置，以及哪些众包批次贡献了训练数据。
- en: The integration of these governance mechanisms transforms pipelines from opaque
    data transformers into auditable, reproducible systems that can demonstrate appropriate
    data handling. This governance infrastructure proves essential not just for regulatory
    compliance but for maintaining trust in ML systems as they make increasingly consequential
    decisions affecting users’ lives.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些治理机制的集成将管道从透明度低的数据转换器转变为可审计、可重复的系统，可以证明适当的数据处理。这种治理基础设施不仅对于合规性至关重要，而且对于维护机器学习系统在做出越来越影响用户生活的关键决策时的信任至关重要。
- en: With comprehensive pipeline architecture established—quality through validation
    and monitoring, reliability through graceful degradation, scalability through
    appropriate patterns, and governance through observability—we must now determine
    what actually flows through these carefully designed systems. The data sources
    we choose shape every downstream characteristic of our ML systems.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立全面的管道架构之后——通过验证和监控确保质量，通过优雅降级确保可靠性，通过适当的模式确保可扩展性，通过可观察性确保治理——我们现在必须确定实际上通过这些精心设计的系统流动的是什么。我们选择的数据源塑造了我们机器学习系统的每个下游特征。
- en: Strategic Data Acquisition
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 战略数据获取
- en: Data acquisition represents more than simply gathering training examples. It
    is a strategic decision that determines our system’s capabilities and limitations.
    The approaches we choose for sourcing training data directly shape our quality
    foundation, reliability characteristics, scalability potential, and governance
    compliance. Rather than treating data sources as independent options to be selected
    based on convenience or familiarity, we examine them as strategic choices that
    must align with our established framework requirements. Each sourcing strategy
    (existing datasets, web scraping, crowdsourcing, synthetic generation) offers
    different trade-offs across quality, cost, scale, and ethical considerations.
    The key insight is that no single approach satisfies all requirements; successful
    ML systems typically combine multiple strategies, balancing their complementary
    strengths against competing constraints.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采集不仅仅是简单地收集训练示例。它是一个战略决策，决定了我们系统的能力和局限性。我们选择用于获取训练数据的方法直接塑造了我们的质量基础、可靠性特征、可扩展潜力以及治理合规性。我们不是将数据来源视为基于便利性或熟悉度选择的独立选项，而是将它们视为必须与我们的既定框架要求相一致的战略选择。每种数据获取策略（现有数据集、网络爬取、众包、合成生成）在质量、成本、规模和伦理考量方面都提供了不同的权衡。关键洞察是没有任何单一方法能满足所有要求；成功的机器学习系统通常结合多种策略，平衡它们的互补优势与竞争性约束。
- en: Returning to our KWS system, data source decisions have profound implications
    across all our framework pillars, as demonstrated in our integrated case study
    in [Section 6.3.3](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff).
    Achieving 98% accuracy across diverse acoustic environments (quality pillar) requires
    representative data spanning accents, ages, and recording conditions. Maintaining
    consistent detection despite device variations (reliability pillar) demands data
    from varied hardware. Supporting millions of concurrent users (scalability pillar)
    requires data volumes that manual collection cannot economically provide. Protecting
    user privacy in always-listening systems (governance pillar) constrains collection
    methods and requires careful anonymization. These interconnected requirements
    demonstrate why acquisition strategy must be evaluated systematically rather than
    through ad-hoc source selection.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的关键词检测系统，数据来源决策对我们框架的所有支柱都有深远的影响，如我们在[第6.3.3节](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)的综合案例研究中所示。在多样化的声学环境中实现98%的准确性（质量支柱）需要涵盖口音、年龄和录音条件的代表性数据。尽管设备存在差异，但保持一致的检测（可靠性支柱）需要来自不同硬件的数据。支持数百万并发用户（可扩展性支柱）需要手动收集无法经济提供的数据量。在始终倾听的系统中保护用户隐私（治理支柱）限制了收集方法，并需要仔细的匿名化。这些相互关联的要求说明了为什么获取策略必须通过系统评估而不是通过临时的来源选择来评估。
- en: Data Source Evaluation and Selection
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据来源评估和选择
- en: Having established the strategic importance of data acquisition, we begin with
    quality as the primary driver. When quality requirements dominate acquisition
    decisions, the choice between curated datasets, expert crowdsourcing, and controlled
    web scraping depends on the accuracy targets, domain expertise needed, and benchmark
    requirements that guide model development. The quality pillar demands understanding
    not just that data appears correct but that it accurately represents the deployment
    environment and provides sufficient coverage of edge cases that might cause failures.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在确立了数据采集的战略重要性之后，我们以质量作为首要驱动力开始。当质量要求主导获取决策时，在精选数据集、专家众包和受控网络爬取之间的选择取决于准确性目标、所需的领域专业知识以及指导模型开发的基准要求。质量支柱要求我们不仅理解数据看起来是正确的，而且它准确地代表了部署环境，并提供了足够的边缘案例覆盖，这些边缘案例可能导致失败。
- en: Platforms like [Kaggle](https://www.kaggle.com/) and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)
    provide ML practitioners with ready-to-use datasets that can jumpstart system
    development. These pre-existing datasets are particularly valuable when building
    ML systems as they offer immediate access to cleaned, formatted data with established
    benchmarks. One of their primary advantages is cost efficiency, as creating datasets
    from scratch requires significant time and resources, especially when building
    production ML systems that need large amounts of high-quality training data. Building
    on this cost efficiency, many of these datasets, such as [ImageNet](https://www.image-net.org/),
    have become standard benchmarks in the machine learning community, enabling consistent
    performance comparisons across different models and architectures. For ML system
    developers, this standardization provides clear metrics for evaluating model improvements
    and system performance. The immediate availability of these datasets allows teams
    to begin experimentation and prototyping without delays in data collection and
    preprocessing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 [Kaggle](https://www.kaggle.com/) 和 [UCI 机器学习仓库](https://archive.ics.uci.edu/)
    这样的平台为机器学习从业者提供了现成的数据集，这些数据集可以加速系统开发。这些现有的数据集在构建机器学习系统时尤其有价值，因为它们提供了立即访问经过清洗、格式化的数据以及已建立的基准。它们的主要优势之一是成本效益，因为从头开始创建数据集需要大量的时间和资源，尤其是在构建需要大量高质量训练数据的生产级机器学习系统时。在此基础上，许多这些数据集，例如
    [ImageNet](https://www.image-net.org/)，已经成为机器学习社区的标准基准，使得在不同模型和架构之间进行一致的性能比较成为可能。对于机器学习系统开发者来说，这种标准化提供了明确的指标来评估模型改进和系统性能。这些数据集的即时可用性使得团队可以开始实验和原型设计，而无需在数据收集和预处理上延迟。
- en: Despite these advantages, ML practitioners must carefully consider the quality
    assurance aspects of pre-existing datasets. For instance, the ImageNet dataset
    was found to have label errors on 3.4% of the validation set ([Northcutt, Athalye,
    and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive)). While popular datasets
    benefit from community scrutiny that helps identify and correct errors and biases,
    most datasets remain “untended gardens” where quality issues can significantly
    impact downstream system performance if not properly addressed. As ([Gebru et
    al. 2021a](ch058.xhtml#ref-gebru2018datasheets)) highlighted in her paper, simply
    providing a dataset without documentation can lead to misuse and misinterpretation,
    potentially amplifying biases present in the data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些优势，机器学习从业者必须仔细考虑现有数据集的质量保证方面。例如，ImageNet 数据集被发现验证集中有 3.4% 的标签错误 ([Northcutt,
    Athalye, and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive))。虽然流行的数据集受益于社区审查，有助于识别和纠正错误和偏差，但大多数数据集仍然是“无人照料的花园”，如果未能妥善解决质量问题，可能会对下游系统性能产生重大影响。正如
    ([Gebru et al. 2021a](ch058.xhtml#ref-gebru2018datasheets)) 在她的论文中指出的那样，仅仅提供数据集而不提供文档可能会导致误用和误解，从而放大数据中存在的偏差。
- en: Beyond quality concerns, supporting documentation accompanying existing datasets
    is invaluable, yet is often only present in widely-used datasets. Good documentation
    provides insights into the data collection process and variable definitions and
    sometimes even offers baseline model performances. This information not only aids
    understanding but also promotes reproducibility in research, a cornerstone of
    scientific integrity; currently, there is a crisis around improving reproducibility
    in machine learning systems ([Pineau et al. 2021](ch058.xhtml#ref-pineau2021improving);
    [Henderson et al. 2018](ch058.xhtml#ref-henderson2018deep)). When other researchers
    have access to the same data, they can validate findings, test new hypotheses,
    or apply different methodologies, thus allowing us to build on each other’s work
    more rapidly. The challenges of data quality extend particularly to big data scenarios
    where volume and variety compound quality concerns ([Gudivada, Rao, et al. 2017](ch058.xhtml#ref-gudivada2017data)),
    requiring systematic approaches to quality validation at scale.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了质量方面的担忧之外，现有数据集的伴随文档是无价的，但通常只存在于广泛使用的数据集中。良好的文档提供了对数据收集过程和变量定义的见解，有时甚至提供基线模型性能。这些信息不仅有助于理解，而且促进了研究的可重复性，这是科学诚信的基石；目前，机器学习系统中提高可重复性的危机正在加剧（[Pineau等人2021](ch058.xhtml#ref-pineau2021improving)；[Henderson等人2018](ch058.xhtml#ref-henderson2018deep)）。当其他研究人员能够访问相同的数据时，他们可以验证发现，测试新的假设，或应用不同的方法，从而使我们能够更快地构建在彼此工作之上的成果。数据质量的问题尤其扩展到大数据场景，其中体积和多样性加剧了质量担忧（[Gudivada，Rao等人2017](ch058.xhtml#ref-gudivada2017data)），需要在大规模上采用系统性的质量验证方法。
- en: Even with proper documentation, understanding the context in which the data
    was collected becomes necessary. Researchers must avoid potential overfitting
    when using popular datasets such as ImageNet ([Beyer et al. 2020](ch058.xhtml#ref-beyer2020we)),
    which can lead to inflated performance metrics. Sometimes, these [datasets do
    not reflect the real-world data](https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有适当的文档，理解数据收集的上下文也变得必要。研究人员在使用像ImageNet（[Beyer等人2020](ch058.xhtml#ref-beyer2020we)）这样的流行数据集时必须避免潜在的过拟合，这些数据集可能导致性能指标膨胀。有时，这些[数据集并不能反映真实世界的数据](https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/)。
- en: Central to these contextual concerns, a key consideration for ML systems is
    how well pre-existing datasets reflect real-world deployment conditions. Relying
    on standard datasets can create a concerning disconnect between training and production
    environments. This misalignment becomes particularly problematic when multiple
    ML systems are trained on the same datasets ([Figure 6.6](ch012.xhtml#fig-misalignment)),
    potentially propagating biases and limitations throughout an entire ecosystem
    of deployed models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些上下文担忧的核心，对于机器学习系统的一个关键考虑是现有数据集如何好地反映现实世界的部署条件。依赖标准数据集可能会在训练和生产环境之间产生令人担忧的脱节。当多个机器学习系统在相同的数据集上训练时，这种不匹配变得尤其有问题（[图6.6](ch012.xhtml#fig-misalignment)），可能会在整个部署模型生态系统中传播偏差和局限性。
- en: '![](../media/file77.svg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file77.svg)'
- en: 'Figure 6.6: **Dataset Convergence**: Shared datasets can mask limitations and
    propagate biases across multiple machine learning systems, potentially leading
    to overoptimistic performance evaluations and reduced generalization to unseen
    data. Reliance on common datasets creates a false sense of progress within an
    ecosystem of models, hindering the development of robust and reliable AI applications.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：**数据集收敛**：共享数据集可能会掩盖局限性并传播偏差到多个机器学习系统中，可能导致过度乐观的性能评估和降低对未见数据的泛化能力。依赖公共数据集会在模型生态系统中产生一种虚假的进步感，阻碍稳健和可靠的AI应用的发展。
- en: 'For our KWS system, pre-existing datasets like Google’s Speech Commands ([Warden
    2018](ch058.xhtml#ref-warden2018speech)) provide essential starting points, offering
    carefully curated voice samples for common wake words. These datasets enable rapid
    prototyping and establish baseline performance metrics. However, evaluating them
    against our quality requirements immediately reveals coverage gaps: limited accent
    diversity, predominantly quiet recording environments, and support for only major
    languages. Quality-driven acquisition strategy recognizes these limitations and
    plans complementary approaches to address them, demonstrating how framework-based
    thinking guides source selection beyond simply choosing available datasets.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词识别系统，像Google的Speech Commands([Warden 2018](ch058.xhtml#ref-warden2018speech))这样的现有数据集提供了基本起点，提供了精心挑选的常见唤醒词的语音样本。这些数据集使得快速原型设计成为可能，并建立了基线性能指标。然而，一旦将它们与我们的质量要求进行比较，就会立即发现覆盖范围不足：有限的口音多样性、主要是在安静的环境中录音，以及仅支持主要语言。以质量为导向的获取策略认识到这些限制，并计划补充方法来应对这些限制，展示了基于框架的思维方式如何指导源选择，而不仅仅是选择可用的数据集。
- en: Scalability and Cost Optimization
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可扩展性和成本优化
- en: 'While quality-focused approaches excel at creating accurate, well-curated datasets,
    they face inherent scaling limitations. When scale requirements dominate—needing
    millions or billions of examples that manual curation cannot economically provide—web
    scraping and synthetic generation offer paths to massive datasets. The scalability
    pillar demands understanding the economic models underlying different acquisition
    strategies: cost per labeled example, throughput limitations, and how these scale
    with data volume. What proves cost-effective at thousand-example scale often becomes
    prohibitive at million-example scale, while approaches that require high setup
    costs amortize favorably across large volumes.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然以质量为导向的方法擅长创建准确、精心整理的数据集，但它们面临着固有的可扩展性限制。当可扩展性需求占主导地位——需要数百万或数十亿个示例，而人工整理在经济上无法提供时——网络爬取和合成生成提供了通往大规模数据集的途径。可扩展性支柱要求理解不同获取策略背后的经济模型：每标注一个示例的成本、吞吐量限制以及这些如何随着数据量的增加而变化。在千个示例规模上证明是成本效益的方法，在百万个示例规模上往往变得难以承受，而需要高设置成本的方法在大规模上则有利可图。
- en: 'Web scraping offers a powerful approach to gathering training data at scale,
    particularly in domains where pre-existing datasets are insufficient. This automated
    technique for extracting data from websites has become essential for modern ML
    system development, enabling teams to build custom datasets tailored to their
    specific needs. When human-labeled data is scarce, web scraping demonstrates its
    value. Consider computer vision systems: major datasets like [ImageNet](https://www.image-net.org/)
    and [OpenImages](https://storage.googleapis.com/openimages/web/index.html) were
    built through systematic web scraping, significantly advancing the field of computer
    vision.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬取提供了一种强大的方法，用于大规模收集训练数据，尤其是在现有数据集不足的领域。这种从网站提取数据的自动化技术已成为现代机器学习系统开发的关键，使团队能够构建符合其特定需求的定制数据集。当人工标注数据稀缺时，网络爬取展示了其价值。以计算机视觉系统为例：像[ImageNet](https://www.image-net.org/)和[OpenImages](https://storage.googleapis.com/openimages/web/index.html)这样的大型数据集就是通过系统性的网络爬取构建的，极大地推动了计算机视觉领域的发展。
- en: Expanding beyond these computer vision applications, the impact of web scraping
    extends well beyond image recognition systems. In natural language processing,
    web-scraped data has enabled the development of increasingly sophisticated ML
    systems. Large language models, such as ChatGPT and Claude, rely on vast amounts
    of text scraped from the public internet and media to learn language patterns
    and generate responses ([Groeneveld et al. 2024](ch058.xhtml#ref-groeneveld2024olmo)).
    Similarly, specialized ML systems like GitHub’s Copilot demonstrate how targeted
    web scraping, in this case of code repositories, can create powerful domain-specific
    assistants ([M. Chen et al. 2021](ch058.xhtml#ref-chen2021evaluating)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 超越这些计算机视觉应用，网络爬取的影响远远超出了图像识别系统。在自然语言处理领域，网络爬取数据已促使越来越复杂的机器学习系统的发展。大型语言模型，如ChatGPT和Claude，依赖于从公共互联网和媒体爬取的大量文本来学习语言模式并生成响应([Groeneveld等人
    2024](ch058.xhtml#ref-groeneveld2024olmo))。同样，像GitHub的Copilot这样的专业机器学习系统展示了如何通过有针对性的网络爬取（在这种情况下是代码仓库），可以创建强大的特定领域助手([陈等人
    2021](ch058.xhtml#ref-chen2021evaluating))。
- en: Building on these foundational developments, production ML systems often require
    continuous data collection to maintain relevance and performance. Web scraping
    facilitates this by gathering structured data like stock prices, weather patterns,
    or product information for analytical applications. This continuous collection
    introduces unique challenges for ML systems. Data consistency becomes crucial,
    as variations in website structure or content formatting can disrupt the data
    pipeline and affect model performance. Proper data management through databases
    or warehouses becomes essential not just for storage, but for maintaining data
    quality and enabling model updates.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在这些基础发展之上，生产级机器学习系统通常需要持续的数据收集来保持相关性和性能。网络爬虫通过收集如股价、天气模式或产品信息等结构化数据，为分析应用提供了便利。这种持续收集为机器学习系统带来了独特的挑战。数据一致性变得至关重要，因为网站结构或内容格式的变化可能会中断数据管道并影响模型性能。通过数据库或仓库进行适当的数据管理不仅对于存储至关重要，而且对于维护数据质量和实现模型更新也变得至关重要。
- en: However, alongside these powerful capabilities, web scraping presents several
    challenges that ML system developers must carefully consider. Legal and ethical
    constraints can limit data collection, as not all websites permit scraping, and
    violating these restrictions can have [serious consequences](https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/).
    When building ML systems with scraped data, teams must carefully document data
    sources and ensure compliance with terms of service and copyright laws. Privacy
    considerations become important when dealing with user-generated content, often
    requiring systematic anonymization procedures.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着这些强大功能的出现，网络爬虫也带来了几个挑战，机器学习系统开发者必须仔细考虑。法律和伦理限制可能会限制数据收集，因为并非所有网站都允许爬取，违反这些限制可能会产生[严重后果](https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/)。当使用爬取的数据构建机器学习系统时，团队必须仔细记录数据来源，并确保遵守服务条款和版权法。在处理用户生成内容时，隐私考虑变得重要，通常需要系统性的匿名化程序。
- en: Complementing these legal and ethical constraints, technical limitations also
    affect the reliability of web-scraped training data. Rate limiting by websites
    can slow data collection, while the dynamic nature of web content can introduce
    inconsistencies that impact model training. As shown in [Figure 6.7](ch012.xhtml#fig-traffic-light),
    web scraping can yield unexpected or irrelevant data, for example, historical
    images appearing in contemporary image searches, that can pollute training datasets
    and degrade model performance. These issues highlight the importance of thorough
    data validation and cleaning processes in ML pipelines built on web-scraped data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些法律和伦理限制之外，技术限制也会影响网络爬取训练数据的可靠性。网站的速率限制可能会减慢数据收集，而网络内容的动态性可能会引入影响模型训练的不一致性。如图6.7所示，网络爬取可能会产生意外或不相关的数据，例如，在当代图像搜索中出现的旧图像，这可能会污染训练数据集并降低模型性能。这些问题突出了在基于网络爬取数据的机器学习管道中，进行彻底的数据验证和清理过程的重要性。
- en: '![](../media/file78.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file78.jpg)'
- en: 'Figure 6.7: **Data Source Noise**: Web scraping introduces irrelevant or outdated
    data into training sets, requiring systematic data validation and cleaning to
    maintain model performance and prevent spurious correlations. Historical images
    appearing in contemporary searches exemplify this noise, underscoring the need
    for careful filtering and quality control in web-sourced datasets. Source: Vox.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：**数据源噪声**：网络爬虫将无关或过时的数据引入训练集，需要系统性的数据验证和清理，以维持模型性能并防止虚假相关性。在当代搜索中出现的旧图像就是这种噪声的例子，强调了在来自网络的数据集中进行仔细过滤和质量控制的需要。来源：Vox。
- en: Crowdsourcing offers another scalable approach, leveraging distributed human
    computation to accelerate dataset creation. Platforms like [Amazon Mechanical
    Turk](https://www.mturk.com/) exemplify how crowdsourcing facilitates this process
    by distributing annotation tasks to a global workforce. This enables rapid collection
    of labels for complex tasks such as sentiment analysis, image recognition, and
    speech transcription, significantly expediting the data preparation phase. One
    of the most impactful examples of crowdsourcing in machine learning is the creation
    of the [ImageNet dataset](https://image-net.org/). ImageNet, which revolutionized
    computer vision, was built by distributing image labeling tasks to contributors
    via Amazon Mechanical Turk. The contributors categorized millions of images into
    thousands of classes, enabling researchers to train and benchmark models for a
    wide variety of visual recognition tasks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 众包提供了一种可扩展的方法，利用分布式的人类计算来加速数据集的创建。例如，[Amazon Mechanical Turk](https://www.mturk.com/)平台展示了众包如何通过将标注任务分配给全球劳动力来促进这一过程。这使得能够快速收集复杂任务（如情感分析、图像识别和语音转录）的标签，显著加快了数据准备阶段。机器学习领域众包最具影响力的例子之一是[ImageNet数据集](https://image-net.org/)的创建。ImageNet通过将图像标注任务分配给贡献者（通过Amazon
    Mechanical Turk）而建立，这些贡献者将数百万张图像分类成数千个类别，使研究人员能够训练和基准测试用于广泛视觉识别任务的模型。
- en: Building on this massive labeling effort, the dataset’s availability spurred
    advancements in deep learning, including the breakthrough AlexNet model in 2012
    ([Krizhevsky, Sutskever, and Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet))
    that demonstrated the power of large-scale neural networks and showed how large-scale,
    crowdsourced datasets could drive innovation. ImageNet’s success highlights how
    leveraging a diverse group of contributors for annotation can enable machine learning
    systems to achieve unprecedented performance. Extending beyond academic research,
    another example of crowdsourcing’s potential is Google’s [Crowdsource](https://crowdsource.google.com/),
    a platform where volunteers contribute labeled data to improve AI systems in applications
    like language translation, handwriting recognition, and image understanding.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在这次大规模标注工作之上，数据集的可用性推动了深度学习的发展，包括2012年取得的突破性成果AlexNet模型([Krizhevsky, Sutskever,
    and Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet))，该模型展示了大规模神经网络的威力，并展示了大规模、众包数据集如何推动创新。ImageNet的成功突显了利用多元化贡献者进行标注如何使机器学习系统实现前所未有的性能。在学术研究之外，众包潜力的另一个例子是谷歌的[Crowdsource](https://crowdsource.google.com/)平台，这是一个志愿者贡献标注数据以改善人工智能系统在语言翻译、手写识别和图像理解等应用中的平台。
- en: 'Beyond these static dataset creation efforts, crowdsourcing has also been instrumental
    in applications beyond traditional dataset annotation. For instance, the navigation
    app [Waze](https://www.waze.com/) uses crowdsourced data from its users to provide
    real-time traffic updates, route suggestions, and incident reporting. These diverse
    applications highlight one of the primary advantages of crowdsourcing: its scalability.
    By distributing microtasks to a large audience, projects can process enormous
    volumes of data quickly and cost-effectively. This scalability is particularly
    beneficial for machine learning systems that require extensive datasets to achieve
    high performance. The diversity of contributors introduces a wide range of perspectives,
    cultural insights, and linguistic variations, enriching datasets and improving
    models’ ability to generalize across populations.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些静态数据集创建的努力之外，众包还在传统数据集标注之外的应用中发挥了重要作用。例如，导航应用[Waze](https://www.waze.com/)使用用户提供的众包数据提供实时交通更新、路线建议和事故报告。这些多样化的应用突显了众包的一个主要优势：其可扩展性。通过将微任务分配给大量受众，项目可以快速且经济高效地处理大量数据。这种可扩展性对于需要大量数据集以实现高性能的机器学习系统尤其有益。贡献者的多样性引入了广泛的视角、文化洞察和语言变体，丰富了数据集并提高了模型在人群中的泛化能力。
- en: Complementing this scalability advantage, flexibility is a key benefit of crowdsourcing.
    Tasks can be adjusted dynamically based on initial results, allowing for iterative
    improvements in data collection. For example, Google’s [reCAPTCHA](https://www.google.com/recaptcha/about/)
    system uses crowdsourcing to verify human users while simultaneously labeling
    datasets for training machine learning models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 补充这一可扩展性优势，灵活性是众包的关键好处。任务可以根据初始结果动态调整，从而允许在数据收集中进行迭代改进。例如，谷歌的[reCAPTCHA](https://www.google.com/recaptcha/about/)系统利用众包来验证人类用户，同时为训练机器学习模型的数据集进行标注。
- en: Moving beyond human-generated data entirely, synthetic data generation represents
    the ultimate scalability solution, creating unlimited training examples through
    algorithmic generation rather than manual collection. This approach changes the
    economics of data acquisition by removing human labor from the equation. As [Figure 6.8](ch012.xhtml#fig-synthetic-data)
    illustrates, synthetic data combines with historical datasets to create larger,
    more diverse training sets that would be impractical to collect manually.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 超越了人类生成数据，合成数据生成代表了终极可扩展性解决方案，通过算法生成而不是手动收集来创建无限的训练示例。这种方法通过从方程式中去除人力劳动而改变了数据获取的经济性。如图6.8[图6.8](ch012.xhtml#fig-synthetic-data)所示，合成数据与历史数据集相结合，创建了更大、更多样化的训练集，这些训练集手动收集是不切实际的。
- en: '![](../media/file79.svg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file79.svg)'
- en: 'Figure 6.8: **Synthetic Data Augmentation**: Combining algorithmically generated
    data with historical datasets expands training set size and diversity, mitigating
    limitations caused by scarce or biased real-world data and improving model generalization.
    This approach enables robust machine learning system development when acquiring
    sufficient real-world data is impractical or unethical. Source: [anylogic](HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：**合成数据增强**：将算法生成数据与历史数据集相结合，扩大了训练集的大小和多样性，减轻了由稀缺或偏颇的实际情况数据造成的限制，并提高了模型泛化能力。这种方法在获取足够真实世界数据不切实际或不道德时，能够促进稳健的机器学习系统开发。来源：[anylogic](HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/).
- en: Building on this foundation, advancements in generative modeling techniques
    have greatly enhanced the quality of synthetic data. Modern AI systems can produce
    data that closely resembles real-world distributions, making it suitable for applications
    ranging from computer vision to natural language processing. For example, generative
    models have been used to create synthetic images for object recognition tasks,
    producing diverse datasets that closely match real-world images. Similarly, synthetic
    data has been leveraged to simulate speech patterns, enhancing the robustness
    of voice recognition systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基础上，生成建模技术的进步大大提高了合成数据的质量。现代人工智能系统可以生成与真实世界分布非常相似的数据，使其适用于从计算机视觉到自然语言处理的各种应用。例如，生成模型已被用于创建用于物体识别任务的合成图像，产生了与真实世界图像非常接近的多样化数据集。同样，合成数据已被用于模拟语音模式，增强了语音识别系统的鲁棒性。
- en: Beyond these quality improvements, synthetic data has become particularly valuable
    in domains where obtaining real-world data is either impractical or costly. The
    automotive industry has embraced synthetic data to train autonomous vehicle systems;
    there are only so many cars you can physically crash to get crash-test data that
    might help an ML system know how to avoid crashes in the first place. Capturing
    real-world scenarios, especially rare edge cases such as near-accidents or unusual
    road conditions, is inherently difficult. Synthetic data allows researchers to
    [simulate these scenarios in a controlled virtual environment](https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/),
    ensuring that models are trained to handle a wide range of conditions. This approach
    has proven invaluable for advancing the capabilities of self-driving cars.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些质量改进之外，合成数据在获取真实世界数据既不切实际又昂贵的情况下变得特别有价值。汽车行业已经采用合成数据来训练自动驾驶车辆系统；你只能物理撞击有限数量的汽车来获取可能帮助机器学习系统了解如何避免碰撞的碰撞测试数据。捕捉真实世界场景，特别是罕见边缘情况，如接近事故或异常道路条件，本质上很困难。合成数据允许研究人员在[受控的虚拟环境中模拟这些场景](https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/)，确保模型被训练以处理广泛的条件。这种方法对于提高自动驾驶汽车的能力已被证明非常有价值。
- en: Complementing these safety-critical applications, another important application
    of synthetic data lies in augmenting existing datasets. Introducing variations
    into datasets enhances model robustness by exposing the model to diverse conditions.
    For instance, in speech recognition, data augmentation techniques like SpecAugment
    ([D. S. Park et al. 2019](ch058.xhtml#ref-park2019specaugment)) introduce noise,
    shifts, or pitch variations, enabling models to generalize better across different
    environments and speaker styles. This principle extends to other domains as well,
    where synthetic data can fill gaps in underrepresented scenarios or edge cases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 补充这些安全关键应用，合成数据另一个重要的应用在于增强现有数据集。在数据集中引入变化可以通过使模型接触到不同的条件来增强模型的鲁棒性。例如，在语音识别中，像SpecAugment（[D.
    S. Park等人 2019](ch058.xhtml#ref-park2019specaugment)）这样的数据增强技术引入噪声、偏移或音调变化，使模型能够更好地在不同环境和说话人风格之间泛化。这一原则也适用于其他领域，其中合成数据可以填补在代表性不足的场景或边缘情况中的空白。
- en: For our KWS system, the scalability pillar drove the need for 23 million training
    examples across 50 languages—a volume that manual collection cannot economically
    provide. Web scraping supplements baseline datasets with diverse voice samples
    from video platforms. Crowdsourcing enables targeted collection for underrepresented
    languages. Synthetic data generation fills remaining gaps through speech synthesis
    ([Werchniak et al. 2021](ch058.xhtml#ref-werchniak2021exploring)) and audio augmentation,
    creating unlimited wake word variations across acoustic environments, speaker
    characteristics, and background conditions. This comprehensive multi-source strategy
    demonstrates how scalability requirements shape acquisition decisions, with each
    approach contributing specific capabilities to the overall data ecosystem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词识别（KWS）系统，可扩展性支柱推动了在50种语言中收集2300万个训练样本的需求——这是一个手工收集在经济上无法提供的数量。网络爬虫通过从视频平台补充基线数据集，增加了多样化的声音样本。众包使得对代表性不足的语言进行有针对性的收集成为可能。通过语音合成（[Werchniak等人
    2021](ch058.xhtml#ref-werchniak2021exploring)）和音频增强，合成数据生成填补了剩余的空白，在声学环境、说话人特征和背景条件下创造了无限的唤醒词变化。这种综合的多源策略展示了可扩展性要求如何塑造获取决策，每种方法都为整体数据生态系统贡献了特定的能力。
- en: Reliability Across Diverse Conditions
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多样化条件下的可靠性
- en: 'Beyond quality and scale considerations, the reliability pillar addresses a
    critical question: will our collected data enable models that perform consistently
    across the deployment environment’s full range of conditions? A dataset might
    achieve high quality by established metrics yet fail to support reliable production
    systems if it doesn’t capture the diversity encountered during deployment. Coverage
    requirements for robust models extend beyond simple volume to encompass geographic
    diversity, demographic representation, temporal variation, and edge case inclusion
    that stress-test model behavior.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 除了质量和规模考虑之外，可靠性支柱解决了一个关键问题：我们收集的数据是否能够使模型在部署环境的全部条件下保持一致的性能？一个数据集可能通过既定的指标达到高质量，但如果它没有捕捉到部署过程中遇到的各种多样性，则可能无法支持可靠的系统。鲁棒模型的覆盖要求不仅包括简单的数量，还包括地理多样性、人口代表性、时间变化和边缘情况包含，这些都会对模型行为进行压力测试。
- en: Understanding coverage requirements requires examining potential failure modes.
    Geographic bias occurs when training data comes predominantly from specific regions,
    causing models to underperform in other areas. A study of image datasets found
    significant geographic skew, with image recognition systems trained on predominantly
    Western imagery performing poorly on images from other regions ([T. Wang et al.
    2019](ch058.xhtml#ref-wang2019balanced)). Demographic bias emerges when training
    data doesn’t represent the full user population, potentially causing discriminatory
    outcomes. Temporal variation matters when phenomena change over time—a fraud detection
    model trained only on historical data may fail against new fraud patterns. Edge
    case collection proves particularly challenging yet critical, as rare scenarios
    often represent high-stakes situations where failures cause the most harm.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 理解覆盖范围需求需要检查潜在的失败模式。当训练数据主要来自特定地区时，会出现地理偏差，导致模型在其他地区表现不佳。一项关于图像数据集的研究发现，地理偏差显著，主要使用西方图像进行训练的图像识别系统在其他地区的图像上表现不佳（[T.
    Wang等，2019](ch058.xhtml#ref-wang2019balanced)）。当训练数据不代表完整用户群体时，会出现人口统计偏差，可能导致歧视性结果。当现象随时间变化时，时间变化很重要——仅使用历史数据训练的欺诈检测模型可能无法应对新的欺诈模式。边缘案例收集特别具有挑战性但至关重要，因为罕见场景通常代表高风险情况，失败会造成最大的损害。
- en: The challenge of edge case collection becomes apparent in autonomous vehicle
    development. While normal driving conditions are easy to capture through test
    fleet operation, near-accidents, unusual pedestrian behavior, or rare weather
    conditions occur infrequently. Synthetic data generation helps address this by
    simulating rare scenarios, but validating that synthetic examples accurately represent
    real edge cases requires careful engineering. Some organizations employ targeted
    data collection where test drivers deliberately create edge cases or where engineers
    identify scenarios from incident reports that need better coverage.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘案例收集的挑战在自动驾驶车辆开发中变得明显。虽然通过测试车队运营可以轻松捕捉正常驾驶条件，但接近事故、不寻常的行人行为或罕见的天气条件发生的频率较低。合成数据生成通过模拟罕见场景来帮助解决这个问题，但验证合成示例是否准确代表真实边缘案例需要仔细的工程。一些组织采用针对性的数据收集，测试驾驶员故意创建边缘案例，或者工程师从事故报告中识别需要更好覆盖的场景。
- en: Dataset convergence, illustrated in [Figure 6.6](ch012.xhtml#fig-misalignment)
    earlier, represents another reliability challenge. When multiple systems train
    on identical datasets, they inherit identical blind spots and biases. An entire
    ecosystem of models may fail on the same edge cases because all trained on data
    with the same coverage gaps. This systemic risk motivates diverse data sourcing
    strategies where each organization collects supplementary data beyond common benchmarks,
    ensuring their models develop different strengths and weaknesses rather than shared
    failure modes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集收敛性，如前文[图6.6](ch012.xhtml#fig-misalignment)所示，代表了另一个可靠性挑战。当多个系统在相同的数据集上训练时，它们会继承相同的盲点和偏见。整个模型生态系统可能因为所有模型都是在具有相同覆盖范围缺失的数据上训练的，而在相同的边缘情况下失败。这种系统性风险促使采用多样化的数据来源策略，其中每个组织收集超出常见基准的补充数据，确保其模型发展出不同的优势和劣势，而不是共享的失败模式。
- en: 'For our KWS system, reliability manifests as consistent wake word detection
    across acoustic environments from quiet bedrooms to noisy streets, across accents
    from various geographic regions, and across age ranges from children to elderly
    speakers. The data sourcing strategy explicitly addresses these diversity requirements:
    web scraping captures natural speech variation from diverse video sources, crowdsourcing
    targets underrepresented demographics and environments, and synthetic data systematically
    explores the parameter space of acoustic conditions. Without this deliberate diversity
    in sourcing, the system might achieve high accuracy on test sets while failing
    unreliably in production deployment.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词唤醒系统（KWS），可靠性表现为在从安静的卧室到嘈杂的街道等不同的声学环境中，从各种地理区域的口音，以及从儿童到老年人的年龄范围内，都能保持一致的唤醒词检测。数据来源策略明确地解决了这些多样性需求：网络爬虫从多样化的视频来源中捕获自然语言变化，众包针对代表性不足的人口和环境，合成数据系统地探索声学条件的参数空间。如果没有这种在数据来源上的故意多样性，系统可能在测试集上实现高精度，但在生产部署中却可能不可靠。
- en: Governance and Ethics in Sourcing
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据来源的治理与伦理
- en: The governance pillar in data acquisition encompasses legal compliance, ethical
    treatment of data contributors, privacy protection, and transparency about data
    origins and limitations. Unlike the other pillars that focus on system capabilities,
    governance ensures data sourcing occurs within appropriate legal and ethical boundaries.
    The consequences of governance failures extend beyond system performance to reputational
    damage, legal liability, and potential harm to individuals whose data was improperly
    collected or used.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 数据获取的治理支柱包括法律合规、对数据贡献者的道德待遇、隐私保护和关于数据来源和限制的透明度。与其他侧重于系统能力的支柱不同，治理确保数据来源在适当的法律和道德边界内进行。治理失败带来的后果不仅限于系统性能，还包括声誉损害、法律责任，以及可能对不恰当地收集或使用其数据的个人造成的潜在伤害。
- en: Legal constraints significantly limit data collection methods across different
    jurisdictions and domains. Not all websites permit scraping, and violating these
    restrictions can have serious consequences, as ongoing litigation around training
    data for large language models demonstrates. Copyright law governs what publicly
    available content can be used for training, with different standards emerging
    across jurisdictions. Terms of service agreements may prohibit using data for
    ML training even when technically accessible. Privacy regulations like GDPR in
    Europe and CCPA in California impose strict requirements on personal data collection,
    requiring consent, enabling deletion requests, and sometimes demanding explanations
    of algorithmic decisions ([Wachter, Mittelstadt, and Russell 2017](ch058.xhtml#ref-wachter2017counterfactual)).
    Healthcare data falls under additional regulations like HIPAA in the United States,
    requiring specific safeguards for patient information. Organizations must carefully
    navigate these legal frameworks, documenting data sources and ensuring compliance
    throughout the acquisition process.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 法律约束在不同司法管辖区和领域中对数据收集方法产生了显著限制。并非所有网站都允许抓取数据，违反这些限制可能会产生严重后果，正如围绕大型语言模型训练数据的持续诉讼所展示的那样。版权法规定了哪些公开内容可用于训练，不同司法管辖区出现了不同的标准。服务条款协议可能禁止使用数据进行机器学习训练，即使技术上可以访问。欧洲的GDPR和加州的CCPA等隐私法规对个人数据收集提出了严格的要求，包括需要同意、允许删除请求，有时还要求对算法决策进行解释([Wachter,
    Mittelstadt, and Russell 2017](ch058.xhtml#ref-wachter2017counterfactual))。医疗数据在美国属于HIPAA等额外法规的管辖，要求对病人信息采取特定的保护措施。组织必须谨慎地处理这些法律框架，记录数据来源，并在整个获取过程中确保合规。
- en: Beyond legal compliance, ethical sourcing requires fair treatment of human contributors.
    The crowdsourcing example we examined earlier—where [OpenAI outsourced data annotation
    to workers in Kenya](https://time.com/6247678/openai-chatgpt-kenya-workers/) paying
    as little as $1.32 per hour for reviewing traumatic content—highlights governance
    failures that can occur when economic pressures override ethical considerations.
    Many workers reportedly suffered psychological harm from exposure to disturbing
    material without adequate mental health support. This case underscores power imbalances
    that can emerge when outsourcing data work to economically disadvantaged regions.
    The lack of fair compensation, inadequate support for workers dealing with traumatic
    content, and insufficient transparency about working conditions represent governance
    failures that affect human welfare beyond just system performance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了法律合规之外，道德来源还需要公平对待人类贡献者。我们之前考察的众包示例——[OpenAI将数据标注外包给肯尼亚的工人](https://time.com/6247678/openai-chatgpt-kenya-workers/)，每小时支付低至1.32美元来审查创伤性内容——突显了当经济压力压倒道德考量时可能发生的治理失败。据报道，许多工人因接触令人不安的材料而遭受心理伤害，而没有得到足够的心理健康支持。这一案例强调了当将数据工作外包给经济不发达地区时可能出现的权力失衡。缺乏公平的补偿、对处理创伤性内容的工人支持不足，以及对工作条件缺乏足够的透明度，这些都是影响人类福利的治理失败，而不仅仅是系统性能。
- en: Industry-wide standards for ethical crowdsourcing have begun emerging in response
    to such concerns. Fair compensation means paying at least local minimum wages,
    ideally benchmarked against comparable work in workers’ regions. Worker wellbeing
    requires providing mental health resources for those dealing with sensitive content,
    limiting exposure to traumatic material, and ensuring reasonable working conditions.
    Transparency demands clear communication about task purposes, how contributions
    will be used, and worker rights. Organizations like the Partnership on AI have
    published guidelines for ethical crowdwork, establishing baselines for acceptable
    practices.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这种担忧，行业范围内的道德众包标准已经开始出现。公平补偿意味着至少支付当地最低工资，理想情况下与工人所在地区的类似工作进行比较。工人的福祉需要为处理敏感内容的人提供心理健康资源，限制接触创伤性材料，并确保合理的工作条件。透明度要求明确沟通任务目的、贡献如何被使用以及工人的权利。像人工智能伙伴关系这样的组织已经发布了道德众包指南，确立了可接受实践的基础。
- en: 'While quality, scalability, and reliability focus on system capabilities, the
    governance pillar ensures our data acquisition occurs within appropriate ethical
    and legal boundaries. Privacy protection forms another critical governance concern,
    particularly when sourcing data involving individuals who didn’t explicitly consent
    to ML training use. Anonymization emerges as a critical capability when handling
    sensitive data. From a systems engineering perspective, anonymization represents
    more than regulatory compliance; it constitutes a core design constraint affecting
    data pipeline architecture, storage strategies, and processing efficiency. ML
    systems must handle sensitive data throughout their lifecycle: during collection,
    storage, transformation, model training, and even in error logs and debugging
    outputs. A single privacy breach can compromise not just individual records but
    entire datasets, making the system unusable for future development.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，质量、可扩展性和可靠性关注系统能力，治理支柱确保我们的数据采集在适当的道德和法律边界内进行。隐私保护是另一个关键的治理问题，尤其是当涉及未明确同意用于机器学习训练的个人数据时。在处理敏感数据时，匿名化成为一个关键能力。从系统工程的角度来看，匿名化不仅代表法规遵从，它构成了一个核心设计约束，影响着数据管道架构、存储策略和处理效率。机器学习系统必须在整个生命周期中处理敏感数据：在收集、存储、转换、模型训练甚至在错误日志和调试输出中。一次隐私泄露不仅会损害单个记录，还可能损害整个数据集，使系统在未来开发中无法使用。
- en: Practitioners have developed a range of anonymization techniques to mitigate
    privacy risks. The most straightforward approach, masking, involves altering or
    obfuscating sensitive values so that they cannot be directly traced back to the
    original data subject. For instance, digits in financial account numbers or credit
    card numbers can be replaced with asterisks, fixed dummy characters, or hashed
    values to protect sensitive information during display or logging.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 实践者已经开发了一系列匿名化技术来减轻隐私风险。最直接的方法是掩码，它涉及更改或模糊敏感值，以便它们不能直接追溯到原始数据主体。例如，金融账户号码或信用卡号码中的数字可以用星号、固定哑字符或散列值替换，以在显示或记录过程中保护敏感信息。
- en: Building on this direct protection approach, generalization reduces the precision
    or granularity of data to decrease the likelihood of re-identification. Instead
    of revealing an exact date of birth or address, the data is aggregated into broader
    categories such as age ranges or zip code prefixes. For example, a user’s exact
    age of 37 might be generalized to an age range of 30-39, while their exact address
    might be bucketed to city-level granularity. This technique reduces re-identification
    risk by sharing data in aggregated form, though careful granularity selection
    is crucial—too coarse loses analytical value while too fine may still enable re-identification
    under certain conditions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种直接保护方法，泛化通过降低数据的精确度或粒度来减少重新识别的可能性。而不是透露确切的出生日期或地址，数据被汇总到更广泛的类别，例如年龄范围或邮政编码前缀。例如，一个用户确切的37岁年龄可能被泛化到30-39岁的年龄范围，而他们的确切地址可能被归类到城市级别的粒度。这种技术通过以汇总形式共享数据来降低重新识别风险，尽管仔细选择粒度至关重要——过于粗糙会失去分析价值，而过于精细在某些条件下仍可能使重新识别成为可能。
- en: While generalization reduces data precision, pseudonymization takes a different
    approach by replacing direct identifiers—names, Social Security numbers, email
    addresses—with artificial identifiers or “pseudonyms.” These pseudonyms must not
    reveal or be easily traceable to the original data subject, enabling analysis
    that links records for the same individual without exposing their identity.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当泛化降低数据精度时，伪匿名化通过用人工标识符或“假名”替换直接标识符——姓名、社会保障号码、电子邮件地址——采取不同的方法。这些假名不得揭示或容易被追踪到原始数据主体，从而允许分析将同一个人的记录联系起来，而不暴露他们的身份。
- en: Moving beyond simple identifier replacement, k-anonymity provides a more formal
    approach, ensuring that each record in a dataset is indistinguishable from at
    least k-1 other records. This is achieved by suppressing or generalizing quasi-identifiers—attributes
    that in combination could be used to re-identify individuals, such as zip code,
    age, and gender. For example, if k=5, every record must share the same combination
    of quasi-identifiers with at least four other records, preventing attackers from
    pinpointing individuals simply by looking at these attributes. This approach provides
    formal privacy guarantees but may require significant data distortion and doesn’t
    protect against homogeneity or background knowledge attacks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 超越简单的标识符替换，k-匿名性提供了一种更正式的方法，确保数据集中的每个记录至少与其他k-1个记录不可区分。这是通过抑制或泛化准标识符——这些属性组合起来可能被用来重新识别个人，例如邮政编码、年龄和性别——来实现的。例如，如果k=5，每个记录必须与至少四个其他记录共享相同的准标识符组合，从而防止攻击者仅通过查看这些属性就定位个人。这种方法提供了正式的隐私保证，但可能需要显著的数据扭曲，并且不能防止同质性或背景知识攻击。
- en: At the most sophisticated end of this spectrum, differential privacy ([Dwork,
    n.d.](ch058.xhtml#ref-dwork2008differential)) adds carefully calibrated noise
    or randomized data perturbations to query results or datasets. The goal is to
    ensure that including or excluding any single individual’s data doesn’t significantly
    affect outputs, thereby concealing their presence. Introduced noise is controlled
    by the ε parameter in ε-Differential Privacy, balancing data utility and privacy
    guarantees. This approach provides strong mathematical privacy guarantees and
    sees wide use in academic and industrial settings, though added noise can affect
    data accuracy and model performance, requiring careful parameter tuning to balance
    privacy and usefulness.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个谱系的最高端，差分隐私（[Dwork, n.d.](ch058.xhtml#ref-dwork2008differential)）向查询结果或数据集添加精心校准的噪声或随机数据扰动。目标是确保包括或排除任何单个个体的数据不会显著影响输出，从而隐藏他们的存在。引入的噪声由ε-Differential
    Privacy中的ε参数控制，平衡数据效用和隐私保证。这种方法提供了强大的数学隐私保证，在学术和工业环境中得到广泛应用，尽管添加的噪声可能会影响数据准确性和模型性能，需要仔细的参数调整以平衡隐私和实用性。
- en: '[Table 6.2](ch012.xhtml#tbl-anonymization-comparison) summarizes the key characteristics
    of each anonymization approach to help practitioners select appropriate techniques
    based on their specific privacy requirements and data utility needs.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.2](ch012.xhtml#tbl-anonymization-comparison)总结了每种匿名化方法的关键特征，以帮助从业者根据他们特定的隐私要求和数据效用需求选择适当的技巧。'
- en: 'Table 6.2: Anonymization Techniques Comparison'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：匿名化技术比较
- en: '| **Technique** | **Data Utility** | **Privacy Level** | **Implementation**
    | **Best Use Case** |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **数据效用** | **隐私级别** | **实现** | **最佳用例** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Masking** | High | Low-Medium | Simple | Displaying sensitive data |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **掩码** | 高 | 低-中等 | 简单 | 显示敏感数据 |'
- en: '| **Generalization** | Medium | Medium | Moderate | Age ranges, location bucketing
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **泛化** | 中等 | 中等 | 中等 | 年龄范围、位置分桶 |'
- en: '| **Pseudonymization** | High | Medium | Moderate | Individual tracking needed
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **伪匿名化** | 高 | 中等 | 中等 | 需要个人跟踪 |'
- en: '| **K-anonymity** | Low-Medium | High | Complex | Formal privacy guarantees
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **k-匿名性** | 低-中等 | 高 | 复杂 | 正式的隐私保证 |'
- en: '| **Differential Privacy** | Medium | Very High | Complex | Statistical guarantees
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **差分隐私** | 中等 | 非常高 | 复杂 | 统计保证 |'
- en: As the comparison table illustrates, effective data anonymization balances privacy
    and utility. Techniques such as masking, generalization, pseudonymization, k-anonymity,
    and differential privacy each target different aspects of re-identification risk.
    By carefully selecting and combining these methods, organizations can responsibly
    derive value from sensitive datasets while respecting the privacy rights and expectations
    of the individuals represented within them.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如比较表所示，有效的数据匿名化在隐私和效用之间取得平衡。如掩码、泛化、匿名化、k-匿名性和差分隐私等技术分别针对重新识别风险的不同方面。通过精心选择和组合这些方法，组织可以在尊重包含在敏感数据集中个人的隐私权利和期望的同时，负责任地从这些数据集中提取价值。
- en: For our KWS system, governance constraints shape acquisition throughout. Voice
    data inherently contains biometric information requiring privacy protection, driving
    decisions about anonymization, consent requirements, and data retention policies.
    Multilingual support raises equity concerns—will the system work only for commercially
    valuable languages or also serve smaller linguistic communities? Fair crowdsourcing
    practices ensure that annotators providing voice samples or labeling receive appropriate
    compensation and understand how their contributions will be used. Transparency
    about data sources and limitations enables users to understand system capabilities
    and potential biases. These governance considerations don’t just constrain acquisition
    but shape which approaches are ethically acceptable and legally permissible.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词语音识别（KWS）系统，治理约束贯穿了整个获取过程。语音数据本身包含需要隐私保护的生物识别信息，这推动了匿名化、同意要求和数据保留政策等决策。多语言支持引发了公平性问题——系统是否仅适用于商业价值高的语言，还是也能服务于较小的语言社区？公平的众包实践确保提供语音样本或标签的标注者获得适当的补偿，并了解他们的贡献将如何被使用。关于数据来源和局限性的透明度使用户能够理解系统的能力及其潜在偏见。这些治理考虑不仅限制了获取，还塑造了哪些方法在伦理上可接受和合法上可行。
- en: Integrated Acquisition Strategy
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成获取策略
- en: Having examined how each pillar shapes acquisition choices, we now see why real-world
    ML systems rarely use a single acquisition method in isolation. Instead, they
    combine approaches strategically to balance competing pillar requirements, recognizing
    that each method contributes complementary strengths. The art of data acquisition
    lies in understanding how these sources work together to create datasets that
    satisfy quality, scalability, reliability, and governance constraints simultaneously.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察了每一根支柱如何塑造获取选择之后，我们现在明白为什么现实世界的机器学习（ML）系统很少单独使用一种获取方法。相反，它们战略性地结合方法，以平衡相互竞争的支柱要求，认识到每种方法都贡献了互补的优势。数据获取的艺术在于理解这些来源如何协同工作，以创建同时满足质量、可扩展性、可靠性和治理约束的数据集。
- en: 'Our KWS system exemplifies this integrated approach. Google’s Speech Commands
    dataset provides a quality-assured baseline enabling rapid prototyping and establishing
    performance benchmarks. However, evaluating this against our requirements reveals
    gaps: limited accent diversity, coverage of only major languages, predominantly
    clean recording environments. Web scraping addresses some gaps by gathering diverse
    voice samples from video platforms and speech databases, capturing natural speech
    patterns across varied acoustic conditions. This scales beyond what manual collection
    could provide while maintaining reasonable quality through automated filtering.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的KWS系统展示了这种集成方法的典范。谷歌的语音命令（Speech Commands）数据集提供了一个质量保证的基线，使得快速原型设计和建立性能基准成为可能。然而，将之与我们的需求进行对比时，我们发现存在差距：口音多样性有限，仅覆盖主要语言，录音环境以清洁为主。通过从视频平台和语音数据库收集多样化的语音样本，网络抓取解决了部分差距，捕捉了在不同声学条件下的自然语音模式。这种方法在规模上超越了人工收集所能提供的，同时通过自动化过滤保持了合理的质量。
- en: 'Crowdsourcing fills targeted gaps that neither existing datasets nor web scraping
    adequately address: underrepresented accents, specific demographic groups, or
    particular acoustic environments identified as weak points. By carefully designing
    crowdsourcing tasks with clear guidelines and quality control, the system balances
    scale with quality while ensuring ethical treatment of contributors. Synthetic
    data generation completes the picture by systematically exploring the parameter
    space: varying background noise levels, speaker ages, microphone characteristics,
    and wake word pronunciations. This addresses the long tail of rare conditions
    that are impractical to collect naturally while enabling controlled experiments
    about which acoustic variations most affect model performance.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 群智外包填补了现有数据集和网页抓取都无法充分解决的针对性缺口：代表性不足的口音、特定的群体或被识别为薄弱环节的特定声学环境。通过精心设计具有明确指导和质量控制的群智外包任务，系统在平衡规模和质量的同时，确保对贡献者的道德待遇。通过系统地探索参数空间：变化的后景噪声水平、说话人年龄、麦克风特性和唤醒词发音，合成数据生成技术完善了这一过程。这解决了难以自然收集的罕见条件长尾问题，同时使关于哪些声学变化最影响模型性能的受控实验成为可能。
- en: The synthesis of these approaches demonstrates how our framework guides strategy.
    Quality requirements drive use of curated datasets and expert review. Scalability
    needs motivate synthetic generation and web scraping. Reliability demands mandate
    diverse sourcing across demographics and environments. Governance constraints
    shape consent requirements, anonymization practices, and fair compensation policies.
    Rather than selecting sources based on convenience, the integrated strategy systematically
    addresses each pillar’s requirements through complementary methods.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的综合展示了我们的框架如何指导策略。质量要求驱动了精选数据集和专家审查的使用。可扩展性需求促使合成生成和网页抓取。可靠性需求要求在人口统计和环境中的多样化来源。治理约束塑造了同意要求、匿名化实践和公平补偿政策。而不是基于便利性选择来源，综合策略通过互补方法系统地解决每个支柱的要求。
- en: The diversity achieved through multi-source acquisition—crowdsourced audio with
    varying quality, synthetic data with perfect consistency, web-scraped content
    with unpredictable formats—creates specific challenges at the boundary where external
    data enters our controlled pipeline environment.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多源获取实现的多样性——质量不一的群智外包音频、完美一致性的合成数据、格式不可预测的网页抓取内容——在外部数据进入我们受控管道环境的边界处创造了特定的挑战。
- en: Data Ingestion
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data ingestion represents the critical junction where carefully acquired data
    enters our ML systems, transforming from diverse external formats into standardized
    pipeline inputs. This boundary layer must handle the heterogeneity resulting from
    our multi-source acquisition strategy while maintaining the quality, reliability,
    scalability, and governance standards we’ve established. This transformation from
    external sources into controlled pipeline environments presents several challenges
    that manifest distinctly across our framework pillars. The quality pillar demands
    validation that catches issues at the entry point before they propagate downstream.
    The reliability pillar requires error handling that maintains operation despite
    source failures and data anomalies. The scalability pillar necessitates throughput
    optimization that handles growing data volumes and velocity. The governance pillar
    enforces access controls and audit trails at the system boundary where external
    data enters trusted environments. Ingestion represents a critical boundary where
    careful engineering prevents problems from entering the pipeline while enabling
    the data flow that ML systems require.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取代表了关键节点，精心获取的数据进入我们的机器学习系统，从各种外部格式转换为标准化的管道输入。这个边界层必须处理由于我们的多源获取策略而产生的异质性，同时保持我们已建立的质量、可靠性、可扩展性和治理标准。从外部来源到受控管道环境的这种转变提出了几个挑战，这些挑战在我们框架的各个支柱中明显不同。质量支柱要求在问题传播到下游之前在入口点进行验证。可靠性支柱需要错误处理，即使在源失败和数据异常的情况下也能保持操作。可扩展性支柱需要吞吐量优化，以处理不断增长的数据量和速度。治理支柱在系统边界执行访问控制和审计跟踪，外部数据进入可信环境。摄取代表了关键边界，精心工程可以防止问题进入管道，同时使机器学习系统所需的数据流得以实现。
- en: Batch vs. Streaming Ingestion Patterns
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量与流式摄取模式
- en: To address these ingestion challenges systematically, ML systems typically follow
    two primary patterns that reflect different approaches to data flow timing and
    processing. Each pattern has distinct characteristics and use cases that shape
    how systems balance latency, throughput, cost, and complexity. Understanding when
    to apply batch versus streaming ingestion—or combinations thereof—requires analyzing
    workload characteristics against our framework requirements.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统地解决摄取挑战，机器学习系统通常遵循两种主要模式，这些模式反映了不同的数据流时序和处理方法。每种模式都有独特的特性和用例，这些特性和用例塑造了系统如何平衡延迟、吞吐量、成本和复杂性。了解何时应用批量摄取与流摄取，或者它们的组合，需要分析工作负载特性与我们的框架要求之间的对应关系。
- en: Batch ingestion involves collecting data in groups or batches over a specified
    period before processing. This method proves appropriate when real-time data processing
    is not critical and data can be processed at scheduled intervals. The batch approach
    enables efficient use of computational resources by amortizing startup costs across
    large data volumes and processing when resources are available or least expensive.
    For example, a retail company might use batch ingestion to process daily sales
    data overnight, updating their ML models for inventory prediction each morning
    ([Akidau et al. 2015](ch058.xhtml#ref-akidau2015dataflow)). The batch job might
    process gigabytes of transaction data using dozens of machines for 30 minutes,
    then release those resources for other workloads. This scheduled processing proves
    far more cost-effective than maintaining always-on infrastructure, particularly
    when slight staleness in predictions doesn’t affect business outcomes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 批量摄取涉及在指定时间段内收集数据分组或批次，然后再进行处理。当实时数据处理不是关键且数据可以按预定间隔处理时，这种方法是合适的。批量方法通过在大数据量上分摊启动成本并在资源可用或成本最低时处理数据，从而有效地利用计算资源。例如，一家零售公司可能会使用批量摄取在夜间处理每日销售数据，每天早上更新其库存预测的机器学习模型（[Akidau
    等人 2015](ch058.xhtml#ref-akidau2015dataflow)）。批量作业可能使用数十台机器处理数吉字节的事务数据 30 分钟，然后释放这些资源供其他工作负载使用。这种计划处理比保持始终在线的基础设施更具成本效益，尤其是在预测的轻微滞后不会影响业务结果的情况下。
- en: Batch processing also simplifies error handling and recovery. When a batch job
    fails midway, the system can retry the entire batch or resume from checkpoints
    without complex state management. Data scientists can easily inspect failed batches,
    understand what went wrong, and reprocess after fixes. The deterministic nature
    of batch processing—processing the same input data always produces the same output—simplifies
    debugging and validation. These characteristics make batch ingestion attractive
    for ML workflows even when real-time processing is technically feasible but not
    required.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理也简化了错误处理和恢复。当一个批处理作业在中间失败时，系统可以重试整个批次或从检查点恢复，而不需要复杂的状态管理。数据科学家可以轻松检查失败的批次，了解出了什么问题，并在修复后重新处理。批量处理的确定性特性——处理相同的输入数据总是产生相同的输出——简化了调试和验证。这些特性使得即使在实时处理技术上可行但不是必需的情况下，批量摄取对机器学习工作流程仍然具有吸引力。
- en: In contrast to this scheduled approach, stream ingestion processes data in real-time
    as it arrives, consuming events continuously rather than waiting to accumulate
    batches. This pattern proves crucial for applications requiring immediate data
    processing, scenarios where data loses value quickly, and systems that need to
    respond to events as they occur. A financial institution might use stream ingestion
    for real-time fraud detection, processing each transaction as it occurs to flag
    suspicious activity immediately before completing the transaction. The value of
    fraud detection drops dramatically if detection occurs hours after the fraudulent
    transaction completes—by then money has been transferred and accounts compromised.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种计划方法相反，流摄取在数据到达时实时处理数据，持续消耗事件而不是等待积累批次。这种模式对于需要立即数据处理的用例至关重要，例如数据价值迅速丧失的场景，以及需要响应事件发生的系统。金融机构可能会使用流摄取进行实时欺诈检测，在交易发生时处理每笔交易，以便在交易完成前立即标记可疑活动。如果欺诈检测发生在欺诈交易完成后数小时，那么欺诈检测的价值将大幅下降——到那时，资金已经转移，账户已被破坏。
- en: However, stream processing introduces complexity that batch processing avoids.
    The system must handle backpressure when downstream systems cannot keep pace with
    incoming data rates. During traffic spikes, when a sudden surge produces data
    faster than processing capacity, the system must either buffer data (requiring
    memory and introducing latency), sample (losing some data), or push back to producers
    (potentially causing their failures). Data freshness Service Level Agreements
    (SLAs) formalize these requirements, specifying maximum acceptable delays between
    data generation and availability for processing. Meeting a 100-millisecond freshness
    SLA requires different infrastructure than meeting a 1-hour SLA, affecting everything
    from networking to storage to processing architectures.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，流式处理引入了批量处理所避免的复杂性。系统必须处理下游系统无法跟上数据流入速率时的背压。在流量峰值期间，当突然的激增产生比处理能力更快的数据时，系统必须要么缓冲数据（需要内存并引入延迟），要么采样（丢失一些数据），或者将数据推回生产者（可能造成其失败）。数据新鲜度服务级别协议（SLA）正式化了这些要求，指定了数据生成与可用处理之间的最大可接受延迟。满足100毫秒的新鲜度SLA所需的架构与满足1小时SLA所需的架构不同，这影响着从网络到存储再到处理架构的各个方面。
- en: Recognizing the limitations of either approach alone, many modern ML systems
    employ hybrid approaches, combining both batch and stream ingestion to handle
    different data velocities and use cases. This flexibility allows systems to process
    both historical data in batches and real-time data streams, providing a comprehensive
    view of the data landscape. A recommendation system might use streaming ingestion
    for real-time user interactions—clicks, views, purchases—to update session-based
    recommendations immediately, while using batch ingestion for overnight processing
    of user profiles, item features, and collaborative filtering models that don’t
    require real-time updates.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到单独采用任何一种方法的局限性，许多现代机器学习系统采用混合方法，结合批量和流式摄取来处理不同的数据速度和用例。这种灵活性允许系统同时处理历史数据批量处理和实时数据流，提供全面的数据景观视图。推荐系统可能会使用流式摄取来实时更新基于会话的推荐，例如点击、查看、购买，而使用批量摄取来处理用户资料、项目特征和不需要实时更新的协同过滤模型，这些模型可以在夜间进行处理。
- en: 'Production systems must balance cost versus latency trade-offs when selecting
    patterns: real-time processing can cost 10-100x more than batch processing. This
    cost differential arises from several factors: streaming systems require always-on
    infrastructure rather than schedulable resources that can spin up and down based
    on workload; maintain redundant processing for fault tolerance to ensure no events
    are lost; need low-latency networking and storage to meet millisecond-scale SLAs;
    and cannot benefit from economies of scale that batch processing achieves by amortizing
    startup costs across large data volumes. A batch job processing one terabyte might
    use 100 machines for 10 minutes, while a streaming system processing the same
    data over 24 hours needs dedicated resources continuously available. This 100x
    difference in cost per byte processed drives many architectural decisions about
    which data truly requires real-time processing versus what can tolerate batch
    delays.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 生产系统在选择模式时必须平衡成本与延迟之间的权衡：实时处理可能比批量处理贵10-100倍。这种成本差异源于几个因素：流式系统需要始终在线的基础设施，而不是可以根据工作负载启动和关闭的可调度资源；为了容错而维护冗余处理以确保不丢失任何事件；需要低延迟的网络和存储以满足毫秒级的服务水平协议（SLA）；并且无法从批量处理通过在大数据量中分摊启动成本所实现的规模经济中受益。处理一个千兆字节的批量作业可能需要100台机器运行10分钟，而处理相同数据24小时的流式系统则需要持续可用的专用资源。每处理一个字节的100倍成本差异驱动了许多关于哪些数据真正需要实时处理以及哪些可以容忍批量延迟的架构决策。
- en: ETL and ELT Comparison
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ETL与ELT比较
- en: Beyond choosing ingestion patterns based on timing requirements, designing effective
    data ingestion pipelines requires understanding the differences between Extract,
    Transform, Load (ETL)[16](#fn16) and Extract, Load, Transform (ELT)[17](#fn17)
    approaches, as illustrated in [Figure 6.9](ch012.xhtml#fig-etl-vs-elt). These
    paradigms determine when data transformations occur relative to the loading phase,
    significantly impacting the flexibility and efficiency of ML pipelines. The choice
    between ETL and ELT affects where computational resources are consumed, how quickly
    data becomes available for analysis, and how easily transformation logic can evolve
    as requirements change.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了根据时间要求选择摄取模式之外，设计有效的数据摄取管道还需要了解提取、转换、加载（ETL）[16](#fn16) 和提取、加载、转换（ELT）[17](#fn17)
    方法之间的差异，如图6.9所示。这些范式决定了数据转换相对于加载阶段发生的时间，这对机器学习管道的灵活性和效率有重大影响。ETL和ELT之间的选择会影响计算资源消耗的位置、数据变得可用于分析的速度以及转换逻辑如何随着需求的变化而演变。
- en: '![](../media/file80.svg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file80.svg)'
- en: 'Figure 6.9: **Data Pipeline Architectures**: ETL pipelines transform data *before*
    loading it into a data warehouse, while ELT pipelines load raw data first and
    transform it within the warehouse, impacting system flexibility and resource allocation
    for machine learning workflows. Choosing between ETL and ELT depends on data volume,
    transformation complexity, and the capabilities of the target data storage system.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：**数据管道架构**：ETL管道在将数据加载到数据仓库之前对其进行转换，而ELT管道首先加载原始数据，然后在仓库内进行转换，这影响了机器学习工作流程的系统灵活性和资源分配。选择ETL和ELT取决于数据量、转换复杂性和目标数据存储系统的功能。
- en: ETL is a well-established paradigm in which data is first gathered from a source,
    then transformed to match the target schema or model, and finally loaded into
    a data warehouse or other repository. This approach typically results in data
    being stored in a ready-to-query format, which can be advantageous for ML systems
    that require consistent, pre-processed data. The transformation step occurs in
    a separate processing layer before data reaches the warehouse, enabling validation
    and standardization before persistence. For instance, an ML system predicting
    customer churn might use ETL to standardize and aggregate customer interaction
    data from multiple sources—converting different timestamp formats to UTC, normalizing
    text encodings to UTF-8, and computing aggregate features like “total purchases
    last 30 days”—before loading into a format suitable for model training ([Inmon
    2005](ch058.xhtml#ref-inmon2005building)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ETL是一种成熟的方法论，其中数据首先从源收集，然后转换以匹配目标模式或模型，最后加载到数据仓库或其他存储库。这种方法通常导致数据以可查询的格式存储，这对于需要一致、预处理的机器学习系统来说可能是有利的。转换步骤在数据到达仓库之前在单独的处理层中发生，使得在持久化之前可以进行验证和标准化。例如，一个预测客户流失的机器学习系统可能会使用ETL来标准化和汇总来自多个来源的客户交互数据——将不同的时间戳格式转换为UTC，将文本编码标准化为UTF-8，并计算汇总特征，如“过去30天的总购买额”——然后再加载到适合模型训练的格式中（[Inmon
    2005](ch058.xhtml#ref-inmon2005building)）。
- en: The advantages of ETL become apparent in scenarios with well-defined schemas
    and transformation requirements. Only cleaned, validated, transformed data reaches
    the warehouse, reducing storage requirements and simplifying downstream queries.
    Security and privacy compliance can be enforced during transformation, ensuring
    sensitive data is masked or encrypted before reaching storage. Quality validation
    occurs before loading, preventing corrupted or invalid data from entering the
    warehouse. For ML systems with stable feature pipelines and clear data quality
    requirements, ETL provides a clean separation between messy source data and curated
    training data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有明确定义的模式和转换要求的场景中，ETL的优势变得明显。只有经过清洗、验证和转换的数据才会进入仓库，从而减少了存储需求并简化了下游查询。在转换过程中可以强制执行安全性和隐私合规性，确保敏感数据在到达存储之前被屏蔽或加密。在加载之前进行质量验证，防止损坏或无效数据进入仓库。对于具有稳定特征管道和明确数据质量要求的机器学习系统，ETL在杂乱的数据源和精心整理的培训数据之间提供了清晰的分离。
- en: However, ETL can be less flexible when schemas or requirements change frequently,
    a common occurrence in evolving ML projects. When transformation logic changes—adding
    new features, modifying aggregations, or correcting bugs—all source data must
    be reprocessed through the ETL pipeline to update the warehouse. This reprocessing
    can take hours or days for large datasets, slowing iteration velocity during ML
    development. The transformation layer requires dedicated infrastructure and expertise,
    adding operational complexity and cost to the data pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当模式或需求频繁变化时，ETL可能不够灵活，这在不断发展的机器学习项目中是一个常见现象。当转换逻辑发生变化——添加新特征、修改聚合或纠正错误时，所有源数据都必须通过ETL管道重新处理以更新数据仓库。这种重新处理可能需要数小时或数天，对于大型数据集来说，会减慢机器学习开发中的迭代速度。转换层需要专用基础设施和专业知识，增加了数据管道的操作复杂性和成本。
- en: This is where the ELT approach offers advantages. ELT reverses the order by
    first loading raw data and then applying transformations as needed within the
    target system. This method is often seen in modern data lake or schema-on-read
    environments, allowing for a more agile approach when addressing evolving analytical
    needs in ML systems. Raw source data is loaded quickly into scalable storage,
    with transformations applied using the warehouse’s computational resources. Modern
    cloud data warehouses like BigQuery, Snowflake, and Redshift provide massive computational
    capacity that can execute complex transformations on terabyte-scale data in minutes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是ELT方法提供优势的地方。ELT通过首先加载原始数据，然后在目标系统内按需应用转换来反转顺序。这种方法通常在现代数据湖或读取模式环境中看到，当处理机器学习系统中的不断发展的分析需求时，允许采取更敏捷的方法。原始源数据快速加载到可扩展的存储中，使用仓库的计算资源应用转换。现代云数据仓库如BigQuery、Snowflake和Redshift提供巨大的计算能力，可以在几分钟内执行复杂转换，处理PB级数据。
- en: By deferring transformations, ELT can accommodate varying uses of the same dataset,
    which is particularly useful in exploratory data analysis phases of ML projects
    or when multiple models with different data requirements are being developed simultaneously.
    One team might compute daily aggregates while another computes hourly aggregates,
    each transforming the same raw data differently. When transformation logic bugs
    are discovered, teams can reprocess data by simply rerunning transformation queries
    rather than re-ingesting from sources. This flexibility accelerates ML experimentation
    where feature engineering requirements evolve rapidly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过推迟转换，ELT可以适应同一数据集的多种用途，这在机器学习项目的探索性数据分析阶段或同时开发多个具有不同数据需求模型时特别有用。一个团队可能计算每日汇总，而另一个团队可能计算每小时汇总，每个团队以不同的方式转换相同的原始数据。当发现转换逻辑错误时，团队可以通过简单地重新运行转换查询来重新处理数据，而不是从源重新摄取。这种灵活性加速了机器学习实验，其中特征工程需求迅速演变。
- en: However, ELT places greater demands on storage systems and query engines, which
    must handle large amounts of unprocessed information. Raw data storage grows larger
    than transformed data, increasing costs. Query performance may suffer when transformations
    execute repeatedly on the same raw data rather than reading pre-computed results.
    Privacy and compliance become more complex when raw sensitive data persists in
    storage rather than being masked during ingestion.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ELT对存储系统和查询引擎的要求更高，它们必须处理大量未处理的信息。原始数据存储的增长大于转换数据，增加了成本。当转换在相同的原始数据上重复执行而不是读取预计算的结果时，查询性能可能会受到影响。当原始敏感数据在存储中持续存在而不是在摄取时被掩码时，隐私和合规性变得更加复杂。
- en: In practice, many ML systems employ hybrid approaches, selecting ETL or ELT
    on a case-by-case basis depending on the specific requirements of each data source
    or ML model. For example, a system might use ETL for structured data from relational
    databases where schemas are well-defined and stable, while employing ELT for unstructured
    data like text or images where transformation requirements may evolve as the ML
    models are refined. High-volume clickstream data might use ELT to enable rapid
    loading and flexible transformation, while sensitive financial data might use
    ETL to enforce encryption and masking before persistence.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，许多机器学习系统采用混合方法，根据每个数据源或机器学习模型的具体需求，逐个案例选择ETL或ELT。例如，一个系统可能会使用ETL来处理来自关系数据库的结构化数据，其中模式定义良好且稳定，而对于文本或图像等非结构化数据，可能会采用ELT，因为随着机器学习模型的优化，转换需求可能会发生变化。高流量的点击流数据可能会使用ELT来实现快速加载和灵活的转换，而敏感的财务数据可能会使用ETL在持久化之前进行加密和掩码。
- en: When implementing streaming components within ETL/ELT architectures, distributed
    systems principles become critical. The CAP theorem[18](#fn18) fundamentally constrains
    streaming system design choices. Apache Kafka[19](#fn19) prioritizes consistency
    and partition tolerance, making it ideal for reliable event ordering but potentially
    experiencing availability issues during network partitions. Apache Pulsar emphasizes
    availability and partition tolerance, providing better fault tolerance but with
    relaxed consistency guarantees. Amazon Kinesis balances all three properties through
    careful configuration but requires understanding these trade-offs for proper deployment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在ETL/ELT架构中实现流组件时，分布式系统原则变得至关重要。CAP定理[18](#fn18)从根本上限制了流系统设计的选择。Apache Kafka[19](#fn19)优先考虑一致性和分区容错性，使其在可靠的事件排序方面表现理想，但在网络分区期间可能会出现可用性问题。Apache
    Pulsar强调可用性和分区容错性，提供更好的容错性，但一致性保证有所放宽。Amazon Kinesis通过仔细配置平衡所有三个属性，但需要理解这些权衡以进行适当的部署。
- en: Multi-Source Integration Strategies
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多源集成策略
- en: Regardless of whether ETL or ELT approaches are used, integrating diverse data
    sources represents a key challenge in data ingestion for ML systems. Data may
    originate from various sources including databases, APIs, file systems, and IoT
    devices. Each source may have its own data format, access protocol, and update
    frequency. The integration challenge lies not just in connecting to these sources
    but in normalizing their disparate characteristics into a unified pipeline that
    subsequent processing stages can consume reliably.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是使用ETL还是ELT方法，将多样化的数据源集成到机器学习系统中代表数据摄取的关键挑战。数据可能来自各种来源，包括数据库、API、文件系统和物联网设备。每个源可能有自己的数据格式、访问协议和更新频率。集成挑战不仅在于连接到这些源，而且在于将它们不同的特性规范化到一个后续处理阶段可以可靠消费的统一管道中。
- en: Given this source diversity, ML engineers must develop robust connectors or
    adapters for each data source to effectively integrate these sources. These connectors
    handle the specifics of data extraction, including authentication, rate limiting,
    and error handling. For example, when integrating with a REST API, the connector
    would manage API keys, respect rate limits specified in API documentation or HTTP
    headers, and handle HTTP status codes appropriately—retrying on transient errors
    (500, 503), aborting on authentication failures (401, 403), and backing off when
    rate limited (429). A well-designed connector abstracts these details from downstream
    processing, presenting a consistent interface regardless of whether data originates
    from APIs, databases, or file systems.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种数据源多样性，机器学习工程师必须为每个数据源开发健壮的连接器或适配器，以有效地集成这些源。这些连接器处理数据提取的细节，包括身份验证、速率限制和错误处理。例如，当与REST
    API集成时，连接器将管理API密钥，遵守API文档或HTTP头中指定的速率限制，并适当地处理HTTP状态码——在短暂错误（500、503）上重试，在身份验证失败（401、403）上中止，并在速率限制（429）时退避。一个设计良好的连接器将这些细节从下游处理中抽象出来，无论数据是否来自API、数据库或文件系统，都提供一个一致的接口。
- en: Beyond basic connectivity, source integration often involves data transformation
    at the ingestion point. This might include parsing JSON[20](#fn20) or XML responses
    into structured formats, converting timestamps to a standard timezone and format
    (typically UTC and ISO 8601), or performing basic data cleaning operations like
    trimming whitespace or normalizing text encodings. The goal is to standardize
    the data format as it enters the ML pipeline, simplifying downstream processing.
    These transformations differ from the business logic transformations in ETL or
    ELT—they address technical format variations rather than semantic transformation
    of content.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的连接性之外，源集成通常涉及在摄取点进行数据转换。这可能包括将JSON[20](#fn20)或XML响应解析为结构化格式，将时间戳转换为标准时区和格式（通常是UTC和ISO
    8601），或执行基本的数据清理操作，如修剪空白或规范化文本编码。目标是标准化数据格式，使其进入机器学习管道，简化下游处理。这些转换不同于ETL或ELT中的业务逻辑转换——它们解决的是技术格式变化，而不是内容语义转换。
- en: 'In addition to data format standardization, it’s essential to consider the
    reliability and availability of data sources. Some sources may experience downtime
    or have inconsistent data quality. Implementing retry mechanisms with exponential
    backoff handles transient failures gracefully. Data quality checks at ingestion
    catch systematic problems early—if a source suddenly starts producing null values
    for previously required fields, immediate detection prevents corrupted data from
    flowing downstream. Fallback procedures enable continued operation when primary
    sources fail: switching to backup data sources, serving cached data, or degrading
    gracefully rather than failing completely. A stock price ingestion system might
    fall back to delayed prices if real-time feeds fail, maintaining service with
    slightly stale data rather than complete outage.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据格式标准化外，考虑数据源的可靠性和可用性至关重要。一些来源可能会出现停机或数据质量不一致的情况。实现具有指数退避的重试机制可以优雅地处理暂时性故障。在摄取时进行数据质量检查可以早期捕捉到系统性问题——如果某个来源突然开始为之前必需的字段产生空值，立即检测可以防止损坏的数据流向下游。后备程序可以在主要来源失败时继续操作：切换到备用数据源，提供缓存数据，或者优雅地降级而不是完全失败。如果实时数据源失败，股价摄取系统可能会回退到延迟价格，以略微陈旧的数据维持服务，而不是完全中断。
- en: 'Case Study: Selecting Ingestion Patterns for KWS'
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：为KWS选择摄取模式
- en: 'Applying these ingestion concepts to our KWS system, production implementations
    demonstrate both streaming and batch patterns working in concert, reflecting the
    dual operational modes we established during problem definition. The ingestion
    architecture directly implements requirements from our four-pillar framework:
    quality through validation of audio characteristics, reliability through consistent
    operation despite source diversity, scalability through handling millions of concurrent
    streams, and governance through source authentication and tracking.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些摄取概念应用于我们的KWS系统，生产实现展示了流式和批量模式协同工作，反映了我们在问题定义期间建立的两种操作模式。摄取架构直接实现了我们四支柱框架的要求：通过验证音频特征来保证质量，通过一致操作来保证可靠性，尽管来源多样化；通过处理数百万并发流来保证可扩展性，通过来源认证和跟踪来保证治理。
- en: 'The streaming ingestion pattern handles real-time audio data from active devices
    where wake words must be detected within our 200 millisecond latency requirement.
    This requires careful implementation of publish-subscribe mechanisms using systems
    like Apache Kafka that buffer incoming audio data and enable parallel processing
    across multiple inference servers. The streaming path prioritizes our reliability
    and scalability pillars: maintaining consistent low-latency operation despite
    varying device loads and network conditions while handling millions of concurrent
    audio streams from deployed devices.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 流式摄取模式处理来自活跃设备的实时音频数据，在这些设备上必须在我们的200毫秒延迟要求内检测到唤醒词。这需要谨慎实现发布-订阅机制，使用如Apache
    Kafka这样的系统来缓冲传入的音频数据，并允许多个推理服务器之间的并行处理。流式路径优先考虑我们的可靠性和可扩展性支柱：在设备负载和网络条件变化的情况下保持一致的低延迟操作，同时处理部署设备上的数百万并发音频流。
- en: 'Parallel to this real-time processing, batch ingestion handles data for model
    training and updates. This includes the diverse data sources we established during
    acquisition: new wake word recordings from crowdsourcing efforts discussed in
    [Section 6.5](ch012.xhtml#sec-data-engineering-strategic-data-acquisition-9ff8),
    synthetic data from voice generation systems that address coverage gaps we identified,
    and validated user interactions that provide real-world examples of both successful
    detections and false rejections. The batch processing typically follows an ETL
    pattern where audio data undergoes preprocessing—normalization to standard volume
    levels, filtering to remove extreme noise, and segmentation into consistent durations—before
    being stored in formats optimized for model training. This processing addresses
    our quality pillar by ensuring training data undergoes consistent transformations
    that preserve the acoustic characteristics distinguishing wake words from background
    speech.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，批量处理处理模型训练和更新的数据。这包括我们在获取过程中建立的各种数据源：来自众包努力的新的唤醒词录音，如[第6.5节](ch012.xhtml#sec-data-engineering-strategic-data-acquisition-9ff8)中讨论的，以及解决我们识别的覆盖范围差距的语音生成系统的合成数据，以及验证过的用户交互，这些交互提供了成功检测和错误拒绝的真实世界示例。批量处理通常遵循ETL模式，其中音频数据经过预处理——标准化到标准音量水平，过滤以去除极端噪声，并分割成一致的时间段——然后存储在针对模型训练优化的格式中。此处理通过确保训练数据经历一致的转换来维护区分唤醒词和背景语音的声学特征，从而解决我们的质量支柱。
- en: 'Integrating these diverse data sources presents unique challenges for KWS systems.
    Real-time audio streams require rate limiting to prevent system overload during
    usage spikes—imagine millions of users simultaneously asking their voice assistants
    about breaking news. Crowdsourced data needs systematic validation to ensure recording
    quality meets the specifications we established during problem definition: adequate
    signal-to-noise ratios, appropriate speaker distances, and correct labeling. Synthetic
    data must be verified for realistic representation of wake word variations rather
    than generating acoustically implausible samples that would mislead model training.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些不同的数据源集成到KWS系统中带来了独特的挑战。实时音频流需要速率限制，以防止在使用高峰期间系统过载——想象数百万用户同时向他们的语音助手询问突发新闻。众包数据需要系统性的验证，以确保录音质量符合我们在问题定义期间设定的规格：足够的信号与噪声比，适当的说话人距离，以及正确的标签。合成数据必须经过验证，以确保唤醒词变体的现实表示，而不是生成声学上不可能的样本，这些样本会误导模型训练。
- en: 'The sophisticated error handling mechanisms required by voice interaction systems
    become apparent when processing real-time audio. Dead letter queues store failed
    recognition attempts for subsequent analysis, helping identify patterns in false
    negatives or system failures that might indicate acoustic conditions we didn’t
    adequately cover during data collection. For example, a smart home device processing
    the wake word “Alexa” must validate several audio quality metrics: signal-to-noise
    ratio above our minimum threshold established during requirements definition,
    appropriate sample rate matching training data specifications, recording duration
    within expected bounds of one to two seconds, and speaker proximity indicators
    suggesting the utterance was directed at the device rather than incidental speech.
    Invalid samples route to dead letter queues for analysis rather than discarding
    them entirely—these failures often reveal edge cases requiring attention in the
    next model iteration. Valid samples flow through to real-time processing for wake
    word detection while simultaneously being logged for potential inclusion in future
    training data, demonstrating how production systems continuously improve through
    careful data engineering.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理实时音频时，语音交互系统所需的复杂错误处理机制变得明显。死信队列存储失败的识别尝试，以便后续分析，有助于识别假阴性或系统故障中的模式，这些模式可能表明我们在数据收集期间未能充分覆盖的声学条件。例如，处理唤醒词“Alexa”的智能家居设备必须验证几个音频质量指标：信号与噪声比高于我们在需求定义期间设定的最低阈值，适当的采样率与训练数据规格相匹配，录音时长在一到两秒的预期范围内，以及说话人距离指示器表明话语是针对设备而非偶然的言语。无效样本被路由到死信队列进行分析，而不是完全丢弃——这些失败通常揭示了需要在下一个模型迭代中注意的边缘情况。有效样本流向实时处理以进行唤醒词检测，同时被记录下来，以备将来可能包含在训练数据中，展示了生产系统如何通过仔细的数据工程不断改进。
- en: This ingestion architecture completes the boundary layer where external data
    enters our controlled pipeline. With reliable ingestion established—validating
    data quality, handling errors gracefully, scaling to required throughput, and
    maintaining governance controls—we now turn to systematic data processing that
    transforms ingested raw data into ML-ready features while maintaining the training-serving
    consistency essential for production systems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种摄取架构完成了外部数据进入我们控制管道的边界层。在建立了可靠的数据摄取——验证数据质量、优雅地处理错误、扩展到所需的吞吐量以及维护治理控制后——我们现在转向系统化数据处理，将摄取的原始数据转换为机器学习准备好的特征，同时保持生产系统所必需的训练-服务一致性。
- en: Systematic Data Processing
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统化数据处理
- en: 'With reliable data ingestion established, we enter the most technically challenging
    phase of the pipeline: systematic data processing. Here, a fundamental requirement—applying
    identical transformations during training and serving—becomes the source of approximately
    70% of production ML failures ([Sculley et al. 2021](ch058.xhtml#ref-sculley2015hidden)).
    This striking statistic underscores why training-serving consistency must serve
    as the central organizing principle for all processing decisions.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了可靠的数据摄取后，我们进入了管道中最具技术挑战性的阶段：系统化数据处理。在这里，一个基本要求——在训练和部署时应用相同的转换——成为大约70%的生产机器学习失败的原因（[Sculley等人，2021](ch058.xhtml#ref-sculley2015hidden)）。这个引人注目的统计数据强调了为什么训练-服务一致性必须成为所有处理决策的核心组织原则。
- en: 'Data processing implements the quality requirements defined in our problem
    definition phase, transforming raw data into ML-ready formats while maintaining
    reliability and scalability standards. Processing decisions must preserve data
    integrity while improving model readiness, all while adhering to governance principles
    throughout the transformation pipeline. Every transformation—from normalization
    parameters to categorical encodings to feature engineering logic—must be applied
    identically in both contexts. Consider a simple example: normalizing transaction
    amounts during training by removing currency symbols and converting to floats,
    but forgetting to apply identical preprocessing during serving. This seemingly
    minor inconsistency can degrade model accuracy by 20-40%, as the model receives
    differently formatted inputs than it was trained on. The severity of this problem
    makes training-serving consistency the central organizing principle for processing
    system design.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理实现了我们在问题定义阶段定义的质量要求，在保持可靠性和可扩展性标准的同时，将原始数据转换为机器学习准备好的格式。处理决策必须在提高模型准备度的同时保持数据完整性，并且在整个转换管道中遵守治理原则。每个转换——从归一化参数到分类编码再到特征工程逻辑——都必须在两个上下文中应用一致。考虑一个简单的例子：在训练过程中通过去除货币符号并将金额转换为浮点数来归一化交易金额，但忘记在服务过程中应用相同的预处理。这种看似微小的不一致性可能会使模型准确性降低20-40%，因为模型接收到的输入格式与其训练时不同。这个问题的严重性使得训练-服务一致性成为处理系统设计的核心组织原则。
- en: For our KWS system, processing decisions directly impact all four pillars as
    established in our problem definition ([Section 6.3.3](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)).
    Quality transformations must preserve acoustic characteristics essential for wake
    word detection while standardizing across diverse recording conditions. Reliability
    requires consistent processing despite varying audio formats collected through
    our multi-source acquisition strategy. Scalability demands efficient algorithms
    that handle millions of audio streams from deployed devices. Governance ensures
    privacy-preserving transformations that protect user voice data throughout processing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词检测系统（KWS），处理决策直接影响到我们在问题定义中确立的四个支柱（[第6.3.3节](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)）。质量转换必须保留对唤醒词检测至关重要的声学特征，并在不同的录音条件下进行标准化。可靠性要求在通过我们的多源采集策略收集到的不同音频格式下保持一致的处理。可扩展性需要处理部署设备上数百万个音频流的效率算法。治理确保在处理过程中保护用户语音数据隐私的转换。
- en: Ensuring Training-Serving Consistency
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确保训练-服务一致性
- en: We begin with quality as the cornerstone of data processing. Here, the quality
    pillar manifests as ensuring that transformations applied during training match
    exactly those applied during serving. This consistency challenge extends beyond
    just applying the same code—it requires that parameters computed on training data
    (normalization constants, encoding dictionaries, vocabulary mappings) are stored
    and reused during serving. Without this discipline, models receive fundamentally
    different inputs during serving than they were trained on, causing performance
    degradation that’s often subtle and difficult to debug.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将质量视为数据处理的基础。在这里，质量支柱表现为确保在训练期间应用的转换与在服务期间应用的转换完全匹配。这一致性挑战不仅限于应用相同的代码——它要求在训练数据上计算出的参数（归一化常数、编码字典、词汇映射）在服务期间被存储和重用。如果没有这种纪律，模型在服务期间接收到的输入与它们训练时的输入根本不同，导致性能下降，这种下降往往是微妙且难以调试的。
- en: 'Data cleaning involves identifying and correcting errors, inconsistencies,
    and inaccuracies in datasets. Raw data frequently contains issues such as missing
    values, duplicates, or outliers that can significantly impact model performance
    if left unaddressed. The key insight is that cleaning operations must be deterministic
    and reproducible: given the same input, cleaning must produce the same output
    whether executed during training or serving. This requirement shapes which cleaning
    techniques are safe to use in production ML systems.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗涉及识别和纠正数据集中的错误、不一致性和不准确之处。原始数据经常包含缺失值、重复项或异常等问题，如果未解决，这些都会严重影响模型性能。关键见解是，清洗操作必须是确定性和可重复的：给定相同的输入，清洗必须在训练或服务期间执行时产生相同的输出。这一要求决定了哪些清洗技术在生产机器学习系统中是安全的。
- en: Data cleaning might involve removing duplicate records based on deterministic
    keys, handling missing values through imputation or deletion using rules that
    can be applied consistently, and correcting formatting inconsistencies systematically.
    For instance, in a customer database, names might be inconsistently capitalized
    or formatted. A data cleaning process would standardize these entries, ensuring
    that “John Doe,” “john doe,” and “DOE, John” are all treated as the same entity.
    The cleaning rules—convert to title case, reorder to “First Last” format—must
    be captured in code that executes identically in training and serving. As emphasized
    throughout this chapter, every cleaning operation must be applied identically
    in both contexts to maintain system reliability.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗可能包括基于确定性键删除重复记录，通过规则进行插补或删除以处理缺失值，以及系统地纠正格式不一致性。例如，在一个客户数据库中，姓名可能不一致地大写或格式化。数据清洗过程将标准化这些条目，确保“John
    Doe”、“john doe”和“DOE, John”都被视为同一实体。清洗规则——转换为标题格式、重新排序为“First Last”格式——必须被捕获在代码中，该代码在训练和服务期间执行相同。正如本章所强调的，每个清洗操作都必须在这两个环境中以相同的方式应用，以保持系统可靠性。
- en: Outlier detection and treatment is another important aspect of data cleaning,
    but one that introduces consistency challenges. Outliers can sometimes represent
    valuable information about rare events, but they can also result from measurement
    errors or data corruption. ML practitioners must carefully consider the nature
    of their data and the requirements of their models when deciding how to handle
    outliers. Simple threshold-based outlier removal (removing values more than 3
    standard deviations from the mean) maintains training-serving consistency if the
    mean and standard deviation are computed on training data and reused during serving.
    However, more sophisticated outlier detection methods that consider relationships
    between features or temporal patterns require careful engineering to ensure consistent
    application.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测和处理是数据清洗的另一个重要方面，但同时也引入了一致性挑战。异常有时可以代表关于罕见事件的宝贵信息，但它们也可能是由测量错误或数据损坏引起的。机器学习从业者必须在决定如何处理异常时仔细考虑其数据的性质和其模型的要求。基于阈值的简单异常移除（移除比平均值超过3个标准差的值）如果平均值和标准差是在训练数据上计算并在服务期间重用的，则可以保持训练-服务一致性。然而，更复杂的异常检测方法，考虑特征之间的关系或时间模式，需要仔细的工程来实现一致的应用。
- en: 'Quality assessment goes hand in hand with data cleaning, providing a systematic
    approach to evaluating the reliability and usefulness of data. This process involves
    examining various aspects of data quality, including accuracy, completeness, consistency,
    and timeliness. In production systems, data quality degrades in subtle ways that
    basic metrics miss: fields that never contain nulls suddenly show sparse patterns,
    numeric distributions drift from their training ranges, or categorical values
    appear that weren’t present during model development.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 质量评估与数据清洗相辅相成，提供了一种系统性的方法来评估数据的可靠性和有用性。这个过程包括检查数据质量的各个方面，包括准确性、完整性、一致性和时效性。在生产系统中，数据质量以基本指标无法捕捉的微妙方式下降：从未包含空值的字段突然出现稀疏模式，数值分布偏离其训练范围，或者在模型开发期间未出现过的分类值。
- en: To address these subtle degradation patterns, production quality monitoring
    requires specific metrics beyond simple missing value counts as discussed in [Section 6.4.1](ch012.xhtml#sec-data-engineering-quality-validation-monitoring-5f2a).
    Critical indicators include null value patterns by feature (sudden increases suggest
    upstream failures), count anomalies (10x increases often indicate data duplication
    or pipeline errors), value range violations (prices becoming negative, ages exceeding
    realistic bounds), and join failure rates between data sources. Statistical drift
    detection[21](#fn21) becomes essential by monitoring means, variances, and quantiles
    of features over time to catch gradual degradation before it impacts model performance.
    For example, in an e-commerce recommendation system, the average user session
    length might gradually increase from 8 minutes to 12 minutes over six months due
    to improved site design, but a sudden drop to 3 minutes suggests a data collection
    bug.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些微妙的退化模式，生产质量监控需要超越简单缺失值计数之外的特定指标，正如[第6.4.1节](ch012.xhtml#sec-data-engineering-quality-validation-monitoring-5f2a)中讨论的那样。关键指标包括按特征（突然增加可能表明上游故障）的空值模式、计数异常（10倍增加通常表明数据重复或管道错误）、值范围违规（价格变为负数，年龄超过现实界限）以及数据源之间的连接失败率。通过监控特征随时间的变化，包括均值、方差和分位数，进行统计漂移检测[21](#fn21)变得至关重要，以便在它影响模型性能之前捕捉到渐进式退化。例如，在一个电子商务推荐系统中，由于网站设计的改进，平均用户会话长度可能在六个月内从8分钟逐渐增加到12分钟，但突然下降到3分钟可能表明存在数据收集错误。
- en: Supporting these monitoring requirements, quality assessment tools range from
    simple statistical measures to complex machine learning-based approaches. Data
    profiling tools provide summary statistics and visualizations that help identify
    potential quality issues, while advanced techniques employ unsupervised learning
    algorithms to detect anomalies or inconsistencies in large datasets. Establishing
    clear quality metrics and thresholds ensures that data entering the ML pipeline
    meets necessary standards for reliable model training and inference. The key is
    maintaining the same quality standards and validation logic across training and
    serving to prevent quality issues from creating training-serving skew.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 支持这些监控需求，质量评估工具从简单的统计指标到基于复杂机器学习的方法都有。数据概要分析工具提供总结统计和可视化，有助于识别潜在的质量问题，而高级技术则采用无监督学习算法来检测大型数据集中的异常或不一致性。建立明确的质量指标和阈值确保进入机器学习管道的数据满足可靠模型训练和推理的必要标准。关键是保持训练和部署过程中的相同质量标准和验证逻辑，以防止质量问题的出现导致训练-部署偏差。
- en: Transformation techniques convert data from its raw form into a format more
    suitable for analysis and modeling. This process can include a wide range of operations,
    from simple conversions to complex mathematical transformations. Central to effective
    transformation, common transformation tasks include normalization and standardization,
    which scale numerical features to a common range or distribution. For example,
    in a housing price prediction model, features like square footage and number of
    rooms might be on vastly different scales. Normalizing these features ensures
    that they contribute more equally to the model’s predictions ([Bishop 2006](ch058.xhtml#ref-bishop2006pattern)).
    Maintaining training-serving consistency requires that normalization parameters
    (mean, standard deviation) computed on training data be stored and applied identically
    during serving. This means persisting these parameters alongside the model itself—often
    in the model artifact or a separate parameter file—and loading them during serving
    initialization.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 转换技术将数据从其原始形式转换为更适合分析和建模的格式。这个过程可以包括从简单的转换到复杂的数学变换的广泛操作。有效的转换的核心任务包括归一化和标准化，这些任务将数值特征缩放到一个共同的范围或分布。例如，在房价预测模型中，像面积和房间数量这样的特征可能在很大程度上有不同的尺度。归一化这些特征确保它们对模型预测的贡献更加平等([Bishop
    2006](ch058.xhtml#ref-bishop2006pattern))。保持训练-服务一致性需要将训练数据上计算的归一化参数（均值、标准差）存储并应用于服务期间完全相同。这意味着将这些参数与模型本身一起持久化——通常在模型工件或单独的参数文件中——并在服务初始化期间加载它们。
- en: Beyond numerical scaling, other transformations might involve encoding categorical
    variables, handling date and time data, or creating derived features. For instance,
    one-hot encoding is often used to convert categorical variables into a format
    that can be readily understood by many machine learning algorithms. Categorical
    encodings must handle both the categories present during training and unknown
    categories encountered during serving. A robust approach computes the category
    vocabulary during training (the set of all observed categories), persists it with
    the model, and during serving either maps unknown categories to a special “unknown”
    token or uses default values. Without this discipline, serving encounters categories
    the model never saw during training, potentially causing errors or degraded performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数值缩放之外，其他转换可能包括对分类变量进行编码、处理日期和时间数据，或创建派生特征。例如，独热编码通常用于将分类变量转换为许多机器学习算法可以轻松理解的格式。分类编码必须处理训练期间存在的类别以及服务期间遇到的未知类别。一种稳健的方法是在训练期间计算类别词汇表（所有观察到的类别集合），将其与模型一起持久化，并在服务期间将未知类别映射到特殊的“未知”标记或使用默认值。如果没有这种纪律，服务期间可能会遇到模型在训练期间从未见过的类别，这可能导致错误或性能下降。
- en: Feature engineering is the process of using domain knowledge to create new features
    that make machine learning algorithms work more effectively. This step is often
    considered more of an art than a science, requiring creativity and deep understanding
    of both the data and the problem at hand. Feature engineering might involve combining
    existing features, extracting information from complex data types, or creating
    entirely new features based on domain insights. For example, in a retail recommendation
    system, engineers might create features that capture the recency, frequency, and
    monetary value of customer purchases, known as RFM analysis ([Kuhn and Johnson
    2013](ch058.xhtml#ref-kuhn2013applied)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是使用领域知识创建新特征的过程，这些特征可以使机器学习算法更有效地工作。这一步骤通常被认为更多的是一种艺术而不是科学，需要创造力和对数据和待解决问题有深刻的理解。特征工程可能涉及组合现有特征、从复杂的数据类型中提取信息，或根据领域洞察创建全新的特征。例如，在零售推荐系统中，工程师可能会创建捕捉客户购买的新近性、频率和货币价值的特征，这被称为RFM分析([Kuhn和Johnson
    2013](ch058.xhtml#ref-kuhn2013applied))。
- en: Given these creative possibilities, the importance of feature engineering cannot
    be overstated. Well-engineered features can often lead to significant improvements
    in model performance, sometimes outweighing the impact of algorithm selection
    or hyperparameter tuning. However, the creativity required for feature engineering
    must be balanced against the consistency requirements of production systems. Every
    engineered feature must be computed identically during training and serving. This
    means that feature engineering logic should be implemented in libraries or modules
    that can be shared between training and serving code, rather than being reimplemented
    separately. Many organizations build feature stores, discussed in [Section 6.9.4](ch012.xhtml#sec-data-engineering-feature-stores-bridging-training-serving-fce5),
    specifically to ensure feature computation consistency across environments.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些创意可能性面前，特征工程的重要性不容小觑。精心设计的特征往往能显著提升模型性能，有时甚至超过算法选择或超参数调整的影响。然而，特征工程所需的创造力必须与生产系统的一致性要求相平衡。每个工程化的特征在训练和部署时都必须以相同的方式进行计算。这意味着特征工程逻辑应该实现于库或模块中，这些库或模块可以在训练和部署代码之间共享，而不是分别重新实现。许多组织构建特征存储库，如[第6.9.4节](ch012.xhtml#sec-data-engineering-feature-stores-bridging-training-serving-fce5)所述，专门用于确保跨环境中的特征计算一致性。
- en: 'Applying these processing concepts to our KWS system, the audio recordings
    flowing through our ingestion pipeline—whether from crowdsourcing, synthetic generation,
    or real-world captures—require careful cleaning to ensure reliable wake word detection.
    Raw audio data often contains imperfections that our problem definition anticipated:
    background noise from various environments (quiet bedrooms to noisy industrial
    settings), clipped signals from recording level issues, varying volumes across
    different microphones and speakers, and inconsistent sampling rates from diverse
    capture devices. The cleaning pipeline must standardize these variations while
    preserving the acoustic characteristics that distinguish wake words from background
    speech—a quality-preservation requirement that directly impacts our 98% accuracy
    target.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些处理概念应用于我们的KWS系统，流经我们摄入管道的音频录音——无论来自众包、合成生成还是现实世界的捕捉——都需要进行仔细的清理，以确保可靠的唤醒词检测。原始音频数据通常包含我们的问题定义所预期的缺陷：来自各种环境的背景噪声（从安静的卧室到嘈杂的工业环境）、录音电平问题导致的剪辑信号、不同麦克风和扬声器的不同音量，以及来自不同捕获设备的采样率不一致。清理管道必须标准化这些变化，同时保留区分唤醒词和背景语音的声学特征——这是一个直接影响我们98%准确率目标的保质量要求。
- en: Quality assessment for KWS extends the general principles with audio-specific
    metrics. Beyond checking for null values or schema conformance, our system tracks
    background noise levels (signal-to-noise ratio above 20 decibels), audio clarity
    scores (frequency spectrum analysis), and speaking rate consistency (wake word
    duration within 500-800 milliseconds). The quality assessment pipeline automatically
    flags recordings where background noise would prevent accurate detection, where
    wake words are spoken too quickly or unclearly for the model to distinguish them,
    or where clipping or distortion has corrupted the audio signal. This automated
    filtering ensures only high-quality samples reach model development, preventing
    the “garbage in, garbage out” cascade we identified in [Figure 6.3](ch012.xhtml#fig-cascades).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键词语音识别（KWS）的质量评估，在通用原则的基础上增加了音频特定的指标。除了检查空值或模式符合性之外，我们的系统还跟踪背景噪声水平（信噪比超过20分贝）、音频清晰度评分（频率谱分析）和说话速率一致性（唤醒词持续时间在500-800毫秒之间）。质量评估管道自动标记那些背景噪声会妨碍准确检测、唤醒词说话过快或不够清晰以至于模型无法区分，或者剪辑或失真已损坏音频信号的录音。这种自动过滤确保只有高质量的样本进入模型开发，防止我们已在[图6.3](ch012.xhtml#fig-cascades)中识别出的“垃圾输入，垃圾输出”级联。
- en: Transforming audio data for KWS involves converting raw waveforms into formats
    suitable for ML models while maintaining training-serving consistency. As shown
    in [Figure 6.10](ch012.xhtml#fig-spectrogram-example), the transformation pipeline
    converts audio signals into standardized feature representations—typically Mel-frequency
    cepstral coefficients (MFCCs)[22](#fn22) or spectrograms[23](#fn23)—that emphasize
    speech-relevant characteristics while reducing noise and variability across different
    recording conditions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键词检测（KWS）的音频数据处理，涉及将原始波形转换为适合机器学习模型的格式，同时保持训练和服务的连贯性。如图6.10[图6.10](ch012.xhtml#fig-spectrogram-example)所示，转换管道将音频信号转换为标准化的特征表示——通常是梅尔频率倒谱系数（MFCCs)[22](#fn22)或频谱图[23](#fn23)，这些表示强调了与语音相关的特征，同时减少了不同录制条件下的噪声和可变性。
- en: '![](../media/file81.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file81.png)'
- en: 'Figure 6.10: **Audio Feature Transformation**: Advanced audio features compress
    raw audio waveforms into representations that emphasize perceptually relevant
    characteristics for machine learning tasks. This transformation reduces noise
    and data dimensionality while preserving essential speech information, improving
    model performance in applications like keyword spotting.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：**音频特征转换**：高级音频特征将原始音频波形压缩成强调机器学习任务中感知相关特性的表示。这种转换减少了噪声和数据维度，同时保留了重要的语音信息，提高了在关键词检测等应用中的模型性能。
- en: Building Idempotent Data Transformations
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建幂等数据转换
- en: Building on quality foundations, we turn to reliability. While quality focuses
    on what transformations produce, reliability ensures how consistently they operate.
    Processing reliability means transformations produce identical outputs given identical
    inputs, regardless of when, where, or how many times they execute. This property,
    called idempotency, proves essential for production ML systems where processing
    may be retried due to failures, where data may be reprocessed to fix bugs, or
    where the same data flows through multiple processing paths.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在质量基础之上，我们转向可靠性。虽然质量关注的是哪些转换会产生什么结果，但可靠性确保它们如何持续稳定地运行。处理可靠性意味着在相同的输入下，转换会产生相同的输出，无论何时、何地或执行多少次。这种特性，称为幂等性，对于可能因故障而重试处理、可能因修复错误而重新处理数据，或相同数据通过多个处理路径流动的生产级机器学习系统来说至关重要。
- en: 'To understand idempotency intuitively, consider a light switch. Flipping the
    switch to the “on” position turns the light on. Flipping it to “on” again leaves
    the light on; the operation can be repeated without changing the outcome. This
    is idempotent behavior. In contrast, a toggle switch that changes state with each
    press is not idempotent: pressing it repeatedly alternates between on and off
    states. In data processing, we want light switch behavior where reapplying the
    same transformation yields the same result, not toggle switch behavior where repeated
    application changes the outcome unpredictably.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观理解幂等性，考虑一个开关。将开关翻转到“开”的位置会打开灯。再次将其翻转到“开”的位置，灯仍然亮着；操作可以重复进行而不会改变结果。这是幂等行为。相比之下，每次按下都会改变状态的切换开关不是幂等的：重复按下会在开和关的状态之间交替。在数据处理中，我们希望有开关的行为，即重新应用相同的转换会产生相同的结果，而不是切换开关的行为，重复应用会不可预测地改变结果。
- en: Idempotent transformations enable reliable error recovery. When a processing
    job fails midway, the system can safely retry processing the same data without
    worrying about duplicate transformations or inconsistent state. A non-idempotent
    transformation might append data to existing records, so retrying would create
    duplicates. An idempotent transformation would upsert data (insert if not exists,
    update if exists), so retrying produces the same final state. This distinction
    becomes critical in distributed systems where partial failures are common and
    retries are the primary recovery mechanism.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等转换使可靠的错误恢复成为可能。当处理作业中途失败时，系统可以安全地重试处理相同的数据，而不用担心重复的转换或不一致的状态。非幂等转换可能会向现有记录中添加数据，因此重试会创建重复项。幂等转换会更新数据（如果不存在则插入，如果存在则更新），因此重试会产生相同的状态。这种区别在分布式系统中变得至关重要，因为在分布式系统中，部分故障很常见，重试是主要的恢复机制。
- en: Handling partial processing failures requires careful state management. Processing
    pipelines should be designed so that each stage can be retried independently without
    affecting other stages. Checkpoint-restart mechanisms enable recovery from the
    last successful processing state rather than restarting from scratch. For long-running
    data processing jobs operating on terabyte-scale datasets, checkpointing progress
    every few minutes means a failure near the end requires reprocessing only recent
    data rather than the entire dataset. The checkpoint logic must carefully track
    what data has been processed and what remains, ensuring no data is lost or processed
    twice.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 处理部分失败需要仔细的状态管理。处理管道应该设计成每个阶段可以独立重试，而不会影响其他阶段。检查点-重启机制可以从最后一个成功的处理状态恢复，而不是从头开始重新启动。对于在TB级数据集上运行的长运行数据处理作业，每几分钟检查点进度意味着在接近结束时失败只需要重新处理最近的数据，而不是整个数据集。检查点逻辑必须仔细跟踪已处理的数据和剩余的数据，确保没有数据丢失或重复处理。
- en: 'Deterministic transformations are those that always produce the same output
    for the same input, without dependence on external factors like time, random numbers,
    or mutable global state. Transformations that depend on current time (e.g., computing
    “days since event” based on current date) break determinism—reprocessing historical
    data would produce different results. The solution is to capture temporal reference
    points explicitly: instead of “days since event,” compute “days from event to
    reference date” where reference date is fixed and persisted. Random operations
    should use seeded random number generators where the seed is derived deterministically
    from input data, ensuring reproducibility.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性转换是指对于相同的输入总是产生相同输出的转换，而不依赖于时间、随机数或可变的全局状态等外部因素。依赖于当前时间的转换（例如，基于当前日期计算“事件以来的天数”）会破坏确定性——重新处理历史数据会产生不同的结果。解决方案是明确捕获时间参考点：而不是“事件以来的天数”，计算“事件到参考日期的天数”，其中参考日期是固定的并持续存在。随机操作应使用带种子的随机数生成器，其中种子可以从输入数据中确定性推导出来，确保可重复性。
- en: 'For our KWS system, reliability requires reproducible feature extraction. Audio
    preprocessing must be deterministic: given the same raw audio file, the same MFCC
    features are always computed regardless of when processing occurs or which server
    executes it. This enables debugging model behavior (can always recreate exact
    features for a problematic example), reprocessing data when bugs are fixed (produces
    consistent results), and distributed processing (different workers produce identical
    features from the same input). The processing code captures all parameters—FFT
    window size, hop length, number of MFCC coefficients—in configuration that’s versioned
    alongside the code, ensuring reproducibility across time and execution environments.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词识别系统，可靠性需要可重复的特征提取。音频预处理必须是确定性的：对于相同的原始音频文件，无论何时处理或哪个服务器执行，总是计算相同的MFCC特征。这使调试模型行为（可以始终为有问题的示例重新创建精确的特征）成为可能，当错误被修复时重新处理数据（产生一致的结果），以及分布式处理（不同的工作者从相同的输入产生相同特征）。处理代码捕获所有参数——FFT窗口大小、跳长、MFCC系数的数量——在配置中版本化，与代码一起版本化，确保跨时间和执行环境的可重复性。
- en: Scaling Through Distributed Processing
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过分布式处理进行扩展
- en: With quality and reliability established, we face the challenge of scale. As
    datasets grow larger and ML systems become more complex, the scalability of data
    processing becomes the limiting factor. Consider the data processing stages we’ve
    discussed—cleaning, quality assessment, transformation, and feature engineering.
    When these operations must handle terabytes of data, a single machine becomes
    insufficient. The cleaning techniques that work on gigabytes of data in memory
    must be redesigned to work across distributed systems.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在质量和可靠性得到确立之后，我们面临着规模化的挑战。随着数据集的增长和机器学习系统的复杂性增加，数据处理的可扩展性成为限制因素。考虑我们讨论过的数据处理阶段——清理、质量评估、转换和特征工程。当这些操作必须处理TB级的数据时，单台机器就不够用了。在内存中处理GB级数据的清理技术必须重新设计，以便在分布式系统中工作。
- en: These challenges manifest when quality assessment must process data faster than
    it arrives, when feature engineering operations require computing statistics across
    entire datasets before transforming individual records, and when transformation
    pipelines create bottlenecks at massive volumes. Processing must scale from development
    (gigabytes on laptops) through production (terabytes across clusters) while maintaining
    consistent behavior.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当质量评估必须处理数据比其到达得更快时，当特征工程操作需要在转换单个记录之前计算整个数据集的统计信息时，以及当转换管道在大量数据中创建瓶颈时，这些挑战就会显现出来。处理必须从开发（笔记本电脑上的千兆字节）扩展到生产（集群上的太字节），同时保持一致的行为。
- en: 'To address these scaling bottlenecks, data must be partitioned across multiple
    computing resources, which introduces coordination challenges. Distributed coordination
    is fundamentally limited by network round-trip times: local operations complete
    in microseconds while network coordination requires milliseconds, creating a 1000x
    latency difference. This constraint explains why operations requiring global coordination
    (like computing normalization statistics across 100 machines) create bottlenecks.
    Each partition computes local statistics quickly, but combining them requires
    information from all partitions.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些扩展瓶颈，数据必须在多个计算资源之间分区，这引入了协调挑战。分布式协调从根本上受到网络往返时间的限制：本地操作在微秒内完成，而网络协调需要毫秒，这造成了1000倍的延迟差异。这一限制解释了为什么需要全局协调的操作（如跨100台机器计算归一化统计信息）会形成瓶颈。每个分区可以快速计算本地统计信息，但合并它们需要所有分区的信息。
- en: 'Data locality becomes critical at this scale. Moving one terabyte of training
    data across the network takes 100+ seconds at 10 gigabytes per second, while local
    SSD access requires only 200 seconds at 5 gigabytes per second, driving ML system
    design toward compute-follows-data architectures. When processing nodes access
    local data at RAM speeds (50-200 gigabytes per second) but must coordinate over
    networks limited to 1-10 gigabytes per second, the bandwidth mismatch creates
    fundamental bottlenecks. Geographic distribution amplifies these challenges: cross-datacenter
    coordination must handle network latency (50-200 milliseconds between regions),
    partial failures, and regulatory constraints preventing data from crossing borders.
    Understanding which operations parallelize easily versus those requiring expensive
    coordination determines system architecture and performance characteristics.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个规模下，数据局部性变得至关重要。在每秒10千兆字节的速率下，将1太字节的训练数据通过网络传输需要100多秒，而本地SSD访问仅需200秒（每秒5千兆字节），这促使机器学习系统设计转向数据跟随计算架构。当处理节点以RAM速度（每秒50-200千兆字节）访问本地数据，但必须通过每秒限制在1-10千兆字节的网络进行协调时，带宽不匹配会形成基本瓶颈。地理分布放大了这些挑战：跨数据中心协调必须处理网络延迟（区域间50-200毫秒）、部分故障，以及防止数据跨境的监管限制。理解哪些操作可以轻松并行化，而哪些操作需要昂贵的协调，决定了系统架构和性能特征。
- en: 'Single-machine processing suffices for surprisingly large workloads when engineered
    carefully. Modern servers with 256 gigabytes RAM can process datasets of several
    terabytes using out-of-core processing that streams data from disk. Libraries
    like Dask or Vaex enable pandas-like APIs that automatically stream and parallelize
    computations across multiple cores. Before investing in distributed processing
    infrastructure, teams should exhaust single-machine optimization: using efficient
    data formats (Parquet[24](#fn24) instead of CSV), minimizing memory allocations,
    leveraging vectorized operations, and exploiting multi-core parallelism. The operational
    simplicity of single-machine processing—no network coordination, no partial failures,
    simple debugging—makes it preferable when performance is adequate.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当设计得当时，单机处理对于出人意料大的工作负载是足够的。现代服务器具有256千兆字节的RAM，可以使用脱核处理来处理数个太字节的数集，该处理方式从磁盘流式传输数据。Dask或Vaex等库能够启用类似pandas的API，自动在多个核心上流式传输和并行化计算。在投资分布式处理基础设施之前，团队应该充分利用单机优化：使用高效的数据格式（Parquet[24](#fn24)而不是CSV），最小化内存分配，利用向量操作，并利用多核并行性。单机处理的操作简单性——无需网络协调、无部分故障、调试简单——使得在性能足够时更受欢迎。
- en: 'Distributed processing frameworks become necessary when data volumes or computational
    requirements exceed single-machine capacity, but the speedup achievable through
    parallelization faces fundamental limits described by Amdahl’s Law:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量或计算需求超过单机容量时，分布式处理框架变得必要，但通过并行化实现的加速速度受到Amdahl定律描述的根本限制：
- en: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>S</mi><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{Speedup} \leq \frac{1}{S + \frac{P}{N}}</annotation></semantics>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">Speedup</mtext><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>S</mi><mo>+</mo><mfrac><mi>P</mi><mi>N</mi></mfrac></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\text{Speedup} \leq \frac{1}{S + \frac{P}{N}}</annotation></semantics>
- en: 'where <semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    represents the serial fraction of work that cannot parallelize, <semantics><mi>P</mi><annotation
    encoding="application/x-tex">P</annotation></semantics> the parallel fraction,
    and <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    the number of processors. This explains why distributing our KWS feature extraction
    across 64 cores achieves only a 64x speedup when the work is embarrassingly parallel
    (<semantics><mrow><mi>S</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">S
    \approx 0</annotation></semantics>), but coordination-heavy operations like computing
    global normalization statistics might achieve only 10x speedup even with 64 cores
    due to the serial aggregation phase. Understanding this relationship guides architectural
    decisions: operations with high serial fractions should run on fewer, faster cores
    rather than many slower cores, while highly parallel workloads benefit from maximum
    distribution as examined further in [Chapter 8](ch014.xhtml#sec-ai-training).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，<semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics>
    表示无法并行化的工作序列分数，<semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics>
    表示并行分数，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    表示处理器数量。这解释了为什么将我们的关键词提取特征提取分布在 64 个核心上，当工作具有明显的并行性时（<semantics><mrow><mi>S</mi><mo>≈</mo><mn>0</mn></mrow><annotation
    encoding="application/x-tex">S \approx 0</annotation></semantics>），只能实现 64 倍的速度提升，但像计算全局归一化统计这样的协调密集型操作，即使有
    64 个核心，也可能只能实现 10 倍的速度提升，这是由于序列聚合阶段。理解这种关系指导了架构决策：具有高序列分数的操作应该在较少、较快的核心上运行，而不是在许多较慢的核心上运行，而高度并行的作业从最大分布中受益，这在第
    8 章中进一步探讨。
- en: Apache Spark provides a distributed computing framework that parallelizes transformations
    across clusters of machines, handling data partitioning, task scheduling, and
    fault tolerance automatically. Beam provides a unified API for both batch and
    streaming processing, enabling the same transformation logic to run on multiple
    execution engines (Spark, Flink, Dataflow). TensorFlow’s tf.data API optimizes
    data loading pipelines for ML training, supporting distributed reading, prefetching,
    and transformation. The choice of framework depends on whether processing is batch
    or streaming, how transformations parallelize, and what execution environment
    is available.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了一个分布式计算框架，该框架可以在机器集群之间并行化转换，自动处理数据分区、任务调度和容错。Beam 提供了一个统一的 API，用于批处理和流处理，使得相同的转换逻辑可以在多个执行引擎（Spark、Flink、Dataflow）上运行。TensorFlow
    的 tf.data API 优化了机器学习训练中的数据加载管道，支持分布式读取、预取和转换。框架的选择取决于处理是批处理还是流处理，转换如何并行化，以及可用的执行环境。
- en: Another important consideration is the balance between preprocessing and on-the-fly
    computation. While extensive preprocessing can speed up model training and inference,
    it can also lead to increased storage requirements and potential data staleness.
    Production systems often implement hybrid approaches, preprocessing computationally
    expensive features while computing rapidly changing features on-the-fly. This
    balance depends on storage costs, computation resources, and freshness requirements
    specific to each use case. Features that are expensive to compute but change slowly
    (user demographic summaries, item popularity scores) benefit from preprocessing.
    Features that change rapidly (current session state, real-time inventory levels)
    must be computed on-the-fly despite computational cost.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是预处理和即时计算的平衡。虽然大量的预处理可以加快模型训练和推理的速度，但它也可能导致存储需求增加和潜在的数据过时。生产系统通常实施混合方法，预处理计算成本高昂的特征，同时在即时计算快速变化的特征。这种平衡取决于存储成本、计算资源以及每个用例特定的数据新鲜度要求。计算成本高昂但变化缓慢的特征（如用户人口统计摘要、项目流行度得分）从预处理中受益。变化快速的特征（如当前会话状态、实时库存水平）必须即时计算，尽管计算成本较高。
- en: 'For our KWS system, scalability manifests at multiple stages. Development uses
    single-machine processing on sample datasets to iterate rapidly. Training at scale
    requires distributed processing when dataset size (23 million examples) exceeds
    single-machine capacity or when multiple experiments run concurrently. The processing
    pipeline parallelizes naturally: audio files are independent, so transforming
    them requires no coordination between workers. Each worker reads its assigned
    audio files from distributed storage, computes features, and writes results back—a
    trivially parallel pattern achieving near-linear scalability. Production deployment
    requires real-time processing on edge devices with severe resource constraints
    (our 16 kilobyte memory limit), necessitating careful optimization and quantization
    to fit processing within device capabilities.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词识别（KWS）系统，可扩展性在多个阶段显现。开发阶段使用单机处理在样本数据集上快速迭代。大规模训练需要分布式处理，当数据集大小（2300万示例）超过单机容量或多个实验同时运行时。处理管道自然并行化：音频文件是独立的，因此转换它们不需要在工作者之间进行协调。每个工作者从分布式存储中读取分配给它的音频文件，计算特征，并将结果写回——这是一个简单的并行模式，实现了接近线性的可扩展性。生产部署需要在边缘设备上进行实时处理，这些设备资源受限（我们的16千字节内存限制），需要仔细优化和量化，以确保处理在设备能力范围内。
- en: Tracking Data Transformation Lineage
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪数据转换血缘
- en: Completing our four-pillar view of data processing, governance ensures accountability
    and reproducibility. The governance pillar requires tracking what transformations
    were applied, when they executed, which version of processing code ran, and what
    parameters were used. This transformation lineage enables reproducibility essential
    for debugging, compliance with regulations requiring explainability, and iterative
    improvement when transformation bugs are discovered. Without comprehensive lineage,
    teams cannot reproduce training data, cannot explain why models make specific
    predictions, and cannot safely fix processing bugs without risking inconsistency.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 完善我们关于数据处理四支柱的观点，治理确保了问责制和可重复性。治理支柱要求跟踪应用了哪些转换、何时执行、运行了哪个版本的代码处理以及使用了哪些参数。这种转换血缘使得可重复性成为调试、符合需要可解释性的法规以及发现转换错误时的迭代改进所必需的。没有全面的血缘，团队无法重现训练数据，无法解释模型为何做出特定的预测，也无法在没有风险不一致的情况下安全地修复处理错误。
- en: 'Transformation versioning captures which version of processing code produced
    each dataset. When transformation logic changes—fixing a bug, adding features,
    or improving quality—the version number increments. Datasets are tagged with the
    transformation version that created them, enabling identification of all data
    requiring reprocessing when bugs are fixed. This versioning extends beyond just
    code versions to capture the entire processing environment: library versions (different
    NumPy versions may produce slightly different numerical results), runtime configurations
    (environment variables affecting behavior), and execution infrastructure (CPU
    architecture affecting floating-point precision).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 转换版本控制记录了哪个版本的代码处理生成了每个数据集。当转换逻辑发生变化——修复错误、添加功能或提高质量时——版本号会增加。数据集会标记上创建它们的转换版本，以便在修复错误时识别所有需要重新处理的数据。这种版本控制不仅限于代码版本，还涵盖了整个处理环境：库版本（不同的NumPy版本可能会产生略微不同的数值结果）、运行时配置（影响行为的环境变量）和执行基础设施（影响浮点精度的CPU架构）。
- en: Parameter tracking maintains the specific values used during transformation.
    For normalization, this means storing the mean and standard deviation computed
    on training data. For categorical encoding, this means storing the vocabulary
    (set of all observed categories). For feature engineering, this means storing
    any constants, thresholds, or parameters used in feature computation. These parameters
    are typically serialized alongside model artifacts, ensuring serving uses identical
    parameters to training. Modern ML frameworks like TensorFlow and PyTorch provide
    mechanisms for bundling preprocessing parameters with models, simplifying deployment
    and ensuring consistency.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数跟踪维护转换过程中使用的特定值。对于归一化，这意味着存储在训练数据上计算出的均值和标准差。对于分类编码，这意味着存储词汇表（所有观察到的类别的集合）。对于特征工程，这意味着存储在特征计算中使用的任何常数、阈值或参数。这些参数通常与模型工件一起序列化，确保服务使用与训练相同的参数。现代机器学习框架如TensorFlow和PyTorch提供了将预处理参数与模型捆绑的机制，简化了部署并确保了一致性。
- en: 'Processing lineage for reproducibility tracks the complete transformation history
    from raw data to final features. This includes which raw data files were read,
    what transformations were applied in what order, what parameters were used, and
    when processing occurred. Lineage systems like Apache Atlas, Amundsen, or commercial
    offerings instrument pipelines to automatically capture this flow. When model
    predictions prove incorrect, engineers can trace back through lineage: which training
    data contributed to this behavior, what quality scores did that data have, what
    transformations were applied, and can we recreate this exact scenario to investigate?'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性处理谱系追踪从原始数据到最终特征的完整转换历史。这包括读取了哪些原始数据文件，应用了哪些转换以及它们的顺序，使用了哪些参数，以及处理发生的时间。Apache
    Atlas、Amundsen或商业产品等谱系系统通过自动捕获此流程来对管道进行工具化。当模型预测证明不正确时，工程师可以通过谱系回溯：哪些训练数据导致了这种行为，这些数据有什么质量评分，应用了哪些转换，并且能否重现这一确切场景以进行调查？
- en: 'Code version ties processing results to the exact code that produced them.
    When processing code lives in version control (Git), each dataset should record
    the commit hash of the code that created it. This enables recreating the exact
    processing environment: checking out the specific code version, installing dependencies
    listed at that version, and running processing with identical parameters. Container
    technologies like Docker simplify this by capturing the entire processing environment
    (code, dependencies, system libraries) in an immutable image that can be rerun
    months or years later with identical results.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 代码版本将处理结果与生成它们的精确代码关联起来。当处理代码存在于版本控制（Git）中时，每个数据集应记录创建它的代码的提交哈希。这允许重现精确的处理环境：检出特定的代码版本，安装该版本列出的依赖项，并使用相同的参数运行处理。容器技术如Docker通过捕获整个处理环境（代码、依赖项、系统库）在一个不可变镜像中，可以几个月或几年后以相同的结果重新运行来简化这一过程。
- en: 'For our KWS system, transformation governance tracks audio processing parameters
    that critically affect model behavior. When audio is normalized to standard volume,
    the reference volume level is persisted. When FFT transforms audio to frequency
    domain, the window size, hop length, and window function (Hamming, Hanning, etc.)
    are recorded. When MFCCs are computed, the number of coefficients, frequency range,
    and mel filterbank parameters are captured. This comprehensive parameter tracking
    enables several critical capabilities: reproducing training data exactly when
    debugging model failures, validating that serving uses identical preprocessing
    to training, and systematically studying how preprocessing choices affect model
    accuracy. Without this governance infrastructure, teams resort to manual documentation
    that inevitably becomes outdated or incorrect, leading to subtle training-serving
    skew that degrades production performance.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的关键词识别（KWS）系统，转换治理跟踪对模型行为有重要影响的音频处理参数。当音频被标准化到标准音量时，参考音量级别被持久化。当FFT将音频转换到频域时，记录窗口大小、跳长和窗口函数（汉明、汉宁等）。当计算MFCC时，捕获系数数量、频率范围和梅尔滤波器组参数。这种全面的参数跟踪使几个关键功能成为可能：在调试模型故障时精确重现训练数据，验证服务使用与训练相同的预处理，以及系统地研究预处理选择如何影响模型精度。没有这种治理基础设施，团队将求助于手动文档，这些文档不可避免地会过时或错误，导致微妙的训练-服务偏差，降低生产性能。
- en: End-to-End Processing Pipeline Design
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端处理管道设计
- en: Integrating these cleaning, assessment, transformation, and feature engineering
    steps, processing pipelines bring together the various data processing steps into
    a coherent, reproducible workflow. These pipelines ensure that data is consistently
    prepared across training and inference stages, reducing the risk of data leakage
    and improving the reliability of ML systems. Pipeline design determines how easily
    teams can iterate on processing logic, how well processing scales as data grows,
    and how reliably systems maintain training-serving consistency.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 整合这些清理、评估、转换和特征工程步骤，处理管道将各种数据处理步骤组合成一个连贯、可重现的工作流程。这些管道确保数据在训练和推理阶段的一致性准备，降低数据泄露的风险，并提高机器学习系统的可靠性。管道设计决定了团队迭代处理逻辑的容易程度，处理随数据增长而扩展的效果，以及系统维护训练-服务一致性的可靠性。
- en: Modern ML frameworks and tools often provide capabilities for building and managing
    data processing pipelines. For instance, Apache Beam and TensorFlow Transform
    allow developers to define data processing steps that can be applied consistently
    during both model training and serving. The choice of data processing framework
    must align with the broader ML framework ecosystem discussed in [Chapter 7](ch013.xhtml#sec-ai-frameworks),
    where framework-specific data loaders and preprocessing utilities can significantly
    impact development velocity and system performance.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的机器学习框架和工具通常提供构建和管理数据处理管道的能力。例如，Apache Beam和TensorFlow Transform允许开发者定义可以在模型训练和服务期间一致应用的数据处理步骤。数据处理框架的选择必须与第7章（ch013.xhtml#sec-ai-frameworks）中讨论的更广泛的机器学习框架生态系统相一致，其中框架特定的数据加载器和预处理实用程序可以显著影响开发速度和系统性能。
- en: Beyond tool selection, effective pipeline design involves considerations such
    as modularity, scalability, and version control. Modular pipelines allow for easy
    updates and maintenance of individual processing steps. Each transformation stage
    should be implemented as an independent module with clear inputs and outputs,
    enabling testing in isolation and replacement without affecting other stages.
    Version control for pipelines is crucial, ensuring that changes in data processing
    can be tracked and correlated with changes in model performance. When model accuracy
    drops, version control enables identifying whether processing changes contributed
    to the degradation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 除了工具选择之外，有效的管道设计还涉及模块化、可扩展性和版本控制等方面的考虑。模块化管道允许轻松更新和维护单个处理步骤。每个转换阶段应实现为一个独立的模块，具有清晰的输入和输出，以便在隔离状态下进行测试和替换，而不会影响其他阶段。管道的版本控制至关重要，确保数据处理的变化可以被跟踪并与模型性能的变化相关联。当模型精度下降时，版本控制可以确定处理变化是否导致了退化。
- en: This modular breakdown of pipeline components is well illustrated by TensorFlow
    Extended in [Figure 6.11](ch012.xhtml#fig-tfx-pipeline-example), which shows the
    complete flow from initial data ingestion through to final model deployment. The
    figure demonstrates how data flows through validation, transformation, and feature
    engineering stages before reaching model training. Each component in the pipeline
    can be versioned, tested, and scaled independently while maintaining overall system
    consistency.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对管道组件的模块化分解在[图6.11](ch012.xhtml#fig-tfx-pipeline-example)中得到了很好的体现，该图展示了从初始数据摄取到最终模型部署的完整流程。该图演示了数据如何通过验证、转换和特征工程阶段，在达到模型训练之前流动。管道中的每个组件都可以独立地进行版本控制、测试和扩展，同时保持整体系统的一致性。
- en: '![](../media/file82.svg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file82.svg)'
- en: 'Figure 6.11: **Data Processing Pipeline**: A modular end-to-end ML pipeline,
    as implemented in TensorFlow extended, highlighting key stages from raw data ingestion
    to trained model deployment and serving. this decomposition enables independent
    development, versioning, and scaling of each component, improving maintainability
    and reproducibility of ML systems.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：**数据处理管道**：一个模块化的端到端机器学习管道，如TensorFlow Extended中实现的那样，突出了从原始数据摄取到训练模型部署和服务的各个关键阶段。这种分解使得每个组件可以独立地进行开发、版本控制和扩展，从而提高了机器学习系统的可维护性和可重复性。
- en: 'Integrating these processing components, our KWS processing pipelines must
    handle both batch processing for training and real-time processing for inference
    while maintaining consistency between these modes. The pipeline design ensures
    that the same normalization parameters computed on training data—mean volume levels,
    frequency response curves, and duration statistics—are stored and applied identically
    during serving. This architectural decision reflects our reliability pillar: users
    expect consistent wake word detection regardless of when their device was manufactured
    or which model version it runs, requiring processing pipelines that maintain stable
    behavior across training iterations and deployment environments.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成这些处理组件时，我们的关键词语音识别（KWS）处理管道必须同时处理训练的批量处理和推理的实时处理，同时保持这两种模式之间的一致性。管道设计确保了在服务期间存储和应用与训练数据上计算出的相同归一化参数——平均音量水平、频率响应曲线和持续时间统计——保持一致。这一架构决策反映了我们的可靠性支柱：用户期望无论其设备是在何时制造的或运行的是哪个模型版本，都能实现一致的唤醒词检测，这需要处理管道在训练迭代和部署环境中保持稳定行为。
- en: Effective data processing is the cornerstone of successful ML systems. By carefully
    cleaning, transforming, and engineering data through the lens of our four-pillar
    framework—quality through training-serving consistency, reliability through idempotent
    transformations, scalability through distributed processing, and governance through
    comprehensive lineage—practitioners can significantly improve the performance
    and reliability of their models. As the field of machine learning continues to
    evolve, so too do the techniques and tools for data processing, making this an
    exciting and dynamic area of study and practice. With systematic processing established,
    we now examine data labeling, which introduces human judgment into our otherwise
    automated pipelines while maintaining the same framework discipline across quality,
    reliability, scalability, and governance dimensions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的数据处理是成功机器学习系统的基石。通过仔细清洗、转换和通过我们的四支柱框架进行数据工程——通过训练-服务一致性保证质量，通过幂等转换保证可靠性，通过分布式处理保证可扩展性，通过全面溯源保证治理——从业者可以显著提高其模型的表现力和可靠性。随着机器学习领域的持续发展，数据处理的技术和工具也在不断发展，这使得这一领域成为了一个既令人兴奋又充满活力的研究领域和实践领域。在系统化处理建立之后，我们现在来考察数据标注，它将人类判断引入了我们原本自动化的管道中，同时在质量、可靠性、可扩展性和治理维度上保持了相同的框架纪律。
- en: Data Labeling
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标注
- en: With systematic data processing established, data labeling emerges as a particularly
    complex systems challenge within the broader data engineering landscape. As training
    datasets grow to millions or billions of examples, the infrastructure supporting
    labeling operations becomes increasingly critical to system performance. Labeling
    represents human-in-the-loop system engineering where our four pillars guide infrastructure
    decisions in fundamentally different ways than in automated pipeline stages. The
    quality pillar manifests as ensuring label accuracy through consensus mechanisms
    and gold standard validation. The reliability pillar demands platform architecture
    that coordinates thousands of concurrent annotators without data loss or corruption.
    The scalability pillar drives AI assistance to amplify human judgment rather than
    replace it. The governance pillar requires fair compensation, bias mitigation,
    and ethical treatment of human contributors whose labor creates the training data
    enabling ML systems.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统化数据处理建立之后，数据标注作为数据工程领域中的一个特别复杂的系统挑战显现出来。随着训练数据集增长到数百万或数十亿个示例，支持标注操作的基础设施对系统性能变得越来越关键。标注代表了人机交互的系统工程，我们的四个支柱以与自动化管道阶段截然不同的方式指导基础设施决策。质量支柱表现为通过共识机制和黄金标准验证确保标签准确性。可靠性支柱要求平台架构能够协调数千个并发标注者，而不会出现数据丢失或损坏。可扩展性支柱推动人工智能辅助增强人类判断，而不是取代它。治理支柱要求公平补偿、偏见缓解和对创造训练数据以使机器学习系统得以运行的人类贡献者的道德待遇。
- en: Modern machine learning systems must efficiently handle the creation, storage,
    and management of labels across their data pipeline. The systems architecture
    must support various labeling workflows while maintaining data consistency, ensuring
    quality, and managing computational resources effectively. These requirements
    compound when dealing with large-scale datasets or real-time labeling needs. The
    systematic challenges extend beyond just storing and managing labels—production
    ML systems need robust pipelines that integrate labeling workflows with data ingestion,
    preprocessing, and training components while maintaining high throughput and adapting
    to changing requirements.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习系统必须高效地处理其数据管道中标签的创建、存储和管理。系统架构必须支持各种标注工作流程，同时保持数据一致性、确保质量和有效管理计算资源。当处理大规模数据集或实时标注需求时，这些要求会变得更加复杂。系统性的挑战不仅限于存储和管理标签——生产级机器学习系统需要强大的管道，将标注工作流程与数据摄取、预处理和训练组件集成，同时保持高吞吐量和适应不断变化的需求。
- en: Label Types and Their System Requirements
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注类型及其系统要求
- en: 'To build effective labeling systems, we must first understand how different
    types of labels affect our system architecture and resource requirements. Consider
    a practical example: building a smart city system that needs to detect and track
    various objects like vehicles, pedestrians, and traffic signs from video feeds.
    Labels capture information about key tasks or concepts, with each label type imposing
    distinct storage, computation, and validation requirements.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建有效的标签系统，我们首先必须理解不同类型的标签如何影响我们的系统架构和资源需求。考虑一个实际例子：构建一个智能城市系统，该系统需要从视频流中检测和跟踪各种对象，如车辆、行人和交通标志。标签捕捉关于关键任务或概念的信息，每种标签类型都施加独特的存储、计算和验证需求。
- en: 'Classification labels represent the simplest form, categorizing images with
    a specific tag or (in multi-label classification) tags such as labeling an image
    as “car” or “pedestrian.” While conceptually straightforward, a production system
    processing millions of video frames must efficiently store and retrieve these
    labels. Storage requirements are modest—a single integer or string per image—but
    retrieval patterns matter: training often samples random subsets while validation
    requires sequential access to all labels, driving different indexing strategies.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 分类标签代表最简单的形式，通过特定的标签或（在多标签分类中）标签如将图像标记为“汽车”或“行人”来对图像进行分类。虽然概念上简单明了，但处理数百万视频帧的生产系统必须有效地存储和检索这些标签。存储需求是适度的——每张图像一个整数或字符串——但检索模式很重要：训练通常随机采样子集，而验证需要按顺序访问所有标签，这推动了不同的索引策略。
- en: Bounding boxes extend beyond simple classification by identifying object locations,
    drawing a box around each object of interest. Our system now needs to track not
    just what objects exist, but where they are in each frame. This spatial information
    introduces new storage and processing challenges, especially when tracking moving
    objects across video frames. Each bounding box requires storing four coordinates
    (x, y, width, height) plus the object class, multiplying storage by 5x compared
    to classification. More importantly, bounding box annotation requires pixel-precise
    positioning that takes 10-20x longer than classification, dramatically affecting
    labeling throughput and cost.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框通过识别对象位置，围绕每个感兴趣的对象画一个框，超越了简单的分类。我们的系统现在不仅需要跟踪存在哪些对象，还需要跟踪它们在每个帧中的位置。这种空间信息引入了新的存储和处理挑战，尤其是在跟踪视频帧中的移动对象时。每个边界框需要存储四个坐标（x，y，宽度，高度）以及对象类别，与分类相比，存储增加了5倍。更重要的是，边界框标注需要比分类精确10-20倍的像素级定位，这极大地影响了标注吞吐量和成本。
- en: Segmentation maps provide the most comprehensive information by classifying
    objects at the pixel level, highlighting each object in a distinct color. For
    our traffic monitoring system, this might mean precisely outlining each vehicle,
    pedestrian, and road sign. These detailed annotations significantly increase our
    storage and processing requirements. A segmentation mask for a 1920x1080 image
    requires 2 million labels (one per pixel), compared to perhaps 10 bounding boxes
    or a single classification label. This 100,000x storage increase and the hours
    required per image for manual segmentation make this approach suitable only when
    pixel-level precision is essential.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 分割图通过在像素级别对对象进行分类，以不同的颜色突出显示每个对象，提供了最全面的信息。对于我们的交通监控系统，这可能意味着精确地勾勒出每一辆车、行人和交通标志。这些详细的标注显著增加了我们的存储和处理需求。一个1920x1080图像的分割掩码需要200万个标签（每个像素一个），而可能只有10个边界框或一个分类标签。这种10万倍存储的增加以及每张图像手动分割所需的小时数，使得这种方法仅在像素级精度至关重要时适用。
- en: '![](../media/file83.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file83.png)'
- en: 'Figure 6.12: **Data Annotation Granularity**: Increasing levels of detail in
    data labeling—from bounding boxes to pixel-level segmentation—impact both annotation
    cost and potential model accuracy. Fine-grained segmentation provides richer information
    for training but demands significantly more labeling effort and storage capacity
    than coarser annotations.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：**数据标注粒度**：数据标注的细节级别从边界框到像素级分割，既影响标注成本也影响潜在模型精度。细粒度分割为训练提供了更丰富的信息，但比粗粒度标注需要显著更多的标注努力和存储容量。
- en: '[Figure 6.12](ch012.xhtml#fig-labels) illustrates these common label types
    and their increasing complexity. Given these increasing complexity levels, the
    choice of label format depends heavily on our system requirements and resource
    constraints ([Johnson-Roberson et al. 2017](ch058.xhtml#ref-10.1109/ICRA.2017.7989092)).
    While classification labels might suffice for simple traffic counting, autonomous
    vehicles need detailed segmentation maps to make precise navigation decisions.
    Leading autonomous vehicle companies often maintain hybrid systems that store
    multiple label types for the same data, allowing flexible use across different
    applications. A single camera frame might have classification labels (scene type:
    highway, urban, rural), bounding boxes (vehicles and pedestrians for obstacle
    detection), and segmentation masks (road surface for path planning), with each
    label type serving distinct downstream models.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12](ch012.xhtml#fig-labels) 展示了这些常见的标签类型及其不断增加的复杂性。考虑到这些不断增加的复杂性级别，标签格式的选择在很大程度上取决于我们的系统需求和资源限制
    ([Johnson-Roberson等人 2017](ch058.xhtml#ref-10.1109/ICRA.2017.7989092))。虽然分类标签可能足以满足简单的交通计数，但自动驾驶汽车需要详细的分割地图来做出精确的导航决策。领先的自动驾驶汽车公司通常维护混合系统，为相同的数据存储多个标签类型，从而在不同应用中实现灵活使用。一个单独的相机帧可能包含分类标签（场景类型：高速公路、城市、乡村）、边界框（用于障碍物检测的车辆和行人）以及分割掩码（用于路径规划的路面），每种标签类型都服务于不同的下游模型。'
- en: 'Extending beyond these basic label types, production systems must also handle
    rich metadata essential for maintaining data quality and debugging model behavior.
    The Common Voice dataset ([Ardila et al. 2020](ch058.xhtml#ref-ardila2020common))
    exemplifies sophisticated metadata management in speech recognition: tracking
    speaker demographics for model fairness, recording quality metrics for data filtering,
    validation status for label reliability, and language information for multilingual
    support. If our traffic monitoring system performs poorly in rainy conditions,
    weather condition metadata during data collection helps identify and address the
    issue. Modern labeling platforms have built sophisticated metadata management
    systems that efficiently index and query this metadata alongside primary labels,
    enabling filtering during training data selection and post-hoc analysis when model
    failures are discovered.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 超出这些基本标签类型，生产系统还必须处理对维护数据质量和调试模型行为至关重要的丰富元数据。Common Voice数据集 ([Ardila等人 2020](ch058.xhtml#ref-ardila2020common))
    体现了语音识别中复杂的元数据管理：跟踪说话者人口统计信息以实现模型公平性、记录质量指标以进行数据过滤、验证状态以验证标签可靠性以及语言信息以支持多语言。如果我们的交通监控系统在雨天表现不佳，数据收集期间的天气条件元数据有助于识别和解决问题。现代标签平台已经建立了复杂的元数据管理系统，能够高效地索引和查询这些元数据，与主要标签一起，在训练数据选择期间进行过滤，并在模型失败被发现时进行事后分析。
- en: These metadata requirements demonstrate how label type choice cascades through
    entire system design. A system built for simple classification labels would need
    significant modifications to handle segmentation maps efficiently. The infrastructure
    must optimize storage systems for the chosen label format, implement efficient
    data retrieval patterns for training, maintain quality control pipelines for validation
    as established in [Section 6.7.1](ch012.xhtml#sec-data-engineering-ensuring-trainingserving-consistency-f3b7),
    and manage version control for label updates. When labels are corrected or refined,
    the system must track which model versions used which label versions, enabling
    correlation between label quality improvements and model performance gains.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元数据需求展示了标签类型选择如何贯穿整个系统设计。为简单分类标签构建的系统需要对分割地图进行重大修改才能有效处理。基础设施必须优化所选标签格式的存储系统，实现高效的数据检索模式以进行训练，维护在[第6.7.1节](ch012.xhtml#sec-data-engineering-ensuring-trainingserving-consistency-f3b7)中建立的验证质量控制管道，并管理标签更新的版本控制。当标签被纠正或细化时，系统必须跟踪哪些模型版本使用了哪些标签版本，以便在标签质量改进和模型性能提升之间建立关联。
- en: Achieving Label Accuracy and Consensus
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现标签准确性和共识
- en: In the labeling domain, quality takes on unique challenges. The quality pillar
    here focuses on ensuring label accuracy despite the inherent subjectivity and
    ambiguity in many labeling tasks. Even with clear guidelines and careful system
    design, some fraction of labels will inevitably be incorrect Thyagarajan et al.
    ([2022](ch058.xhtml#ref-thyagarajan2023multilabel)). The challenge is not eliminating
    labeling errors entirely—an impossible goal—but systematically measuring and managing
    error rates to keep them within bounds that don’t degrade model performance.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注领域，质量面临独特的挑战。这里的质量支柱侧重于确保标签的准确性，尽管许多标注任务固有的主观性和模糊性。即使有明确的指南和仔细的系统设计，一些标签不可避免地会错误——Thyagarajan等人([2022](ch058.xhtml#ref-thyagarajan2023multilabel))。挑战不是完全消除标注错误——这是一个不可能的目标，而是系统地测量和管理错误率，以保持它们在不会降低模型性能的范围内。
- en: As [Figure 6.13](ch012.xhtml#fig-hard-labels) illustrates, labeling failures
    arise from two distinct sources requiring different engineering responses. Some
    errors stem from data quality issues where the underlying data is genuinely ambiguous
    or corrupted—like the blurred frog image where even expert annotators cannot determine
    the species with certainty. Other errors require deep domain expertise where the
    correct label is determinable but only by experts with specialized knowledge,
    as with the black stork identification. These different failure modes drive architectural
    decisions about annotator qualification, task routing, and consensus mechanisms.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.13](ch012.xhtml#fig-hard-labels)所示，标签错误源于两个不同的来源，需要不同的工程响应。一些错误源于数据质量问题，其中底层数据确实模糊不清或损坏——就像模糊的青蛙图像，即使专家标注者也无法确定物种。其他错误需要深厚的领域专业知识，正确的标签只能由具有专门知识的专家确定，例如黑鹳的识别。这些不同的失败模式推动了关于标注者资格、任务路由和共识机制的建筑决策。
- en: '![](../media/file84.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file84.png)'
- en: 'Figure 6.13: **Labeling Ambiguity**: How subjective or difficult examples,
    such as blurry images or rare species, can introduce errors during data labeling,
    highlighting the need for careful quality control and potentially expert annotation.
    Source: ([Northcutt, Athalye, and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive)).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：**标签模糊性**：主观或困难的例子，如模糊图像或稀有物种，如何在数据标注过程中引入错误，突出了仔细的质量控制和可能需要专家标注的必要性。来源：([Northcutt,
    Athalye, and Mueller 2021](ch058.xhtml#ref-northcutt2021pervasive))。
- en: Given these fundamental quality challenges, production ML systems implement
    multiple layers of quality control. Systematic quality checks continuously monitor
    the labeling pipeline through random sampling of labeled data for expert review
    and statistical methods to flag potential errors. The infrastructure must efficiently
    process these checks across millions of examples without creating bottlenecks.
    Sampling strategies typically validate 1-10% of labels, balancing detection sensitivity
    against review costs. Higher-risk applications like medical diagnosis or autonomous
    vehicles may validate 100% of labels through multiple independent reviews, while
    lower-stakes applications like product recommendations may validate only 1% through
    spot checks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些基本的质量挑战，生产级机器学习系统实施了多层质量控制。系统性的质量检查通过随机抽样标注数据供专家审查和统计方法来标记潜在错误，持续监控标注流程。基础设施必须高效地处理这些检查，跨越数百万个示例，而不造成瓶颈。抽样策略通常验证1-10%的标签，在检测灵敏度和审查成本之间取得平衡。高风险应用，如医疗诊断或自动驾驶汽车，可能通过多次独立审查验证100%的标签，而低风险应用，如产品推荐，可能只通过抽查验证1%的标签。
- en: Beyond random sampling approaches, collecting multiple labels per data point,
    often referred to as “consensus labeling,” can help identify controversial or
    ambiguous cases. Professional labeling companies have developed sophisticated
    infrastructure for this process. For example, [Labelbox](https://labelbox.com/)
    has consensus tools that track inter-annotator agreement rates and automatically
    route controversial cases for expert review. [Scale AI](https://scale.com) implements
    tiered quality control, where experienced annotators verify the work of newer
    team members. The consensus infrastructure typically collects 3-5 labels per example,
    computing inter-annotator agreement using metrics like Fleiss’ kappa which measures
    agreement beyond what would occur by chance. Examples with low agreement (kappa
    below 0.4) route to expert review rather than forcing consensus from genuinely
    ambiguous cases.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机抽样方法之外，对每个数据点收集多个标签，通常称为“共识标注”，有助于识别有争议或模糊不清的案例。专业的标注公司已经为这一过程开发了复杂的基础设施。例如，[Labelbox](https://labelbox.com/)
    提供了共识工具，可以跟踪标注者之间的协议率，并自动将争议案例路由给专家审查。[Scale AI](https://scale.com) 实施了分级质量控制，经验丰富的标注者会验证新团队成员的工作。共识基础设施通常对每个示例收集3-5个标签，使用如Fleiss的kappa等指标来计算标注者之间的协议，这些指标衡量的是超出偶然发生的协议。协议度低的示例（kappa低于0.4）将被路由到专家审查，而不是强迫从真正模糊不清的案例中达成共识。
- en: The consensus approach reflects an economic trade-off essential for scalable
    systems. Expert review costs 10-50x more per example than crowdsourced labeling,
    but forcing agreement on ambiguous examples through majority voting of non-experts
    produces systematically biased labels. By routing only genuinely ambiguous cases
    to experts—often 5-15% of examples identified through low inter-annotator agreement—systems
    balance cost against quality. This tiered approach enables processing millions
    of examples economically while maintaining quality standards through targeted
    expert intervention.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 共识方法反映了可扩展系统中必要的经济权衡。专家审查的成本比众包标注高出10-50倍，但通过非专家的多数投票强迫对模糊案例达成一致会产生系统性的偏差标签。通过仅将真正模糊不清的案例路由给专家——通常是通过低标注者之间协议率识别的5-15%的示例——系统在成本和质量之间取得平衡。这种分级方法使得以经济的方式处理数百万个示例，同时通过有针对性的专家干预保持质量标准。
- en: While technical infrastructure provides the foundation for quality control,
    successful labeling systems must also consider human factors. When working with
    annotators, organizations need robust systems for training and guidance. This
    includes good documentation with clear examples of correct labeling, visual demonstrations
    of edge cases and how to handle them, regular feedback mechanisms showing annotators
    their accuracy on gold standard examples, and calibration sessions where annotators
    discuss ambiguous cases to develop shared understanding. For complex or domain-specific
    tasks, the system might implement tiered access levels, routing challenging cases
    to annotators with appropriate expertise based on their demonstrated accuracy
    on similar examples.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然技术基础设施为质量控制提供了基础，但成功的标注系统还必须考虑人为因素。在与标注者合作时，组织需要强大的培训和指导系统。这包括良好的文档，其中包含正确标注的清晰示例，边缘案例的视觉演示以及如何处理这些案例，以及定期的反馈机制，显示标注者在黄金标准示例上的准确性，还有校准会议，标注者在此讨论模糊不清的案例以发展共同理解。对于复杂或特定领域的任务，系统可能会实施分级访问级别，将具有适当专业知识的标注者路由到具有类似示例上展示的准确性的挑战性案例。
- en: Quality monitoring generates substantial data that must be efficiently processed
    and tracked. Organizations typically monitor inter-annotator agreement rates (tracking
    whether multiple annotators agree on the same example), label confidence scores
    (how certain annotators are about their labels), time spent per annotation (both
    too fast suggesting careless work and too slow suggesting confusion), error patterns
    and types (systematic biases or misunderstandings), annotator performance metrics
    (accuracy on gold standard examples), and bias indicators (whether certain annotator
    demographics systematically label differently). These metrics must be computed
    and updated efficiently across millions of examples, often requiring dedicated
    analytics pipelines that process labeling data in near real-time to catch quality
    issues before they affect large volumes of data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 质量监控生成大量数据，这些数据必须被高效地处理和跟踪。组织通常监控标注者之间的互评一致性比率（跟踪多个标注者是否对同一示例达成一致），标签置信度分数（标注者对其标签的确定性），每个标注所花费的时间（过快可能表明工作粗心，过慢可能表明困惑），错误模式和类型（系统性的偏差或误解），标注者性能指标（在黄金标准示例上的准确性），以及偏差指标（某些标注者的人口统计特征是否系统地以不同的方式标注）。这些指标必须在数百万个示例中高效地计算和更新，通常需要专门的分析管道，以近乎实时地处理标注数据，以便在这些问题影响大量数据之前捕捉到质量问题。
- en: Building Reliable Labeling Platforms
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建可靠的标注平台
- en: Moving from label quality to system reliability, we examine how platform architecture
    supports consistent operations. While quality focuses on label accuracy, reliability
    ensures the platform architecture itself operates consistently at scale. Scaling
    labeling from hundreds to millions of examples while maintaining quality requires
    understanding how production labeling systems separate concerns across multiple
    architectural components. The fundamental challenge is that labeling represents
    a human-in-the-loop workflow where system performance depends not just on infrastructure
    but on managing human attention, expertise, and consistency.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 从标签质量转向系统可靠性，我们考察平台架构如何支持一致的操作。虽然质量关注标签准确性，但可靠性确保平台架构本身在规模上保持一致。在保持质量的同时，将标注从数百个示例扩展到数百万个示例需要理解生产标注系统如何在多个架构组件之间分离关注点。基本挑战在于，标注代表一个有人工智能参与的流程，其中系统性能不仅取决于基础设施，还取决于管理人类注意力、专业知识和一致性。
- en: 'At the foundation sits a durable task queue that stores labeling tasks persistently,
    ensuring no work gets lost when systems restart or annotators disconnect. Most
    production systems use message queues like Apache Kafka or RabbitMQ rather than
    databases for this purpose, since message queues provide natural ordering, parallel
    consumption, and replay capabilities that databases don’t easily support. Each
    task carries metadata beyond just the data to be labeled: what type of task it
    is (classification, bounding boxes, segmentation), what expertise level it requires,
    how urgent it is, and any context needed for accurate labeling—perhaps related
    examples or relevant documentation.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础层是一个耐用的任务队列，它持久地存储标注任务，确保在系统重启或标注者断开连接时不会丢失任何工作。大多数生产系统使用像Apache Kafka或RabbitMQ这样的消息队列而不是数据库来达到这个目的，因为消息队列提供了自然排序、并行消费和重放功能，而这些是数据库难以支持的。每个任务都携带除要标注的数据之外的元数据：任务的类型（分类、边界框、分割），所需的专业知识水平，紧急程度，以及任何用于准确标注的上下文——可能是相关示例或相关文档。
- en: The assignment service that routes tasks to annotators implements matching logic
    that’s more sophisticated than simple round-robin distribution. Medical image
    labeling systems route chest X-rays specifically to annotators who have demonstrated
    radiology expertise, measured by their agreement with expert labels on gold standard
    examples. But expertise matching alone isn’t sufficient—annotators who see only
    chest images or only a specific pathology can develop blind spots, performing
    well on familiar examples but poorly on less common cases. Production systems
    therefore constraint assignment to ensure no annotator receives more than 30%
    of their tasks from a single category, maintaining breadth of exposure that prevents
    overspecialization from degrading quality on less-familiar examples.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 将任务路由到标注者的任务分配服务实现了比简单的轮询分配更复杂的匹配逻辑。医学图像标注系统会将胸部X光片特别路由到那些已经证明具有放射学专业知识且与专家标签在黄金标准示例上达成一致性的标注者。但仅凭专业知识匹配是不够的——只看到胸部图像或只看到特定病理的标注者可能会形成盲点，在熟悉的例子上表现良好，但在不常见的案例上表现较差。因此，生产系统对分配进行约束，确保没有标注者从单一类别中接收超过30%的任务，保持广泛的接触，防止过度专业化降低不熟悉例子上的质量。
- en: 'When tasks require multiple annotations to ensure quality, the consensus engine
    determines both when sufficient labels have been collected and how to aggregate
    potentially conflicting opinions. Simple majority voting works for clear-cut classification
    tasks where most annotators naturally agree: identifying whether an image contains
    a car rarely produces disagreement. But more subjective tasks like sentiment analysis
    or identifying nuanced image attributes produce legitimate disagreement between
    thoughtful annotators. A common pattern addresses this by collecting 3-5 labels
    per example, computing inter-annotator agreement using Fleiss’ kappa (which measures
    agreement beyond chance), and routing examples with low agreement—typically kappa
    below 0.4—to expert review rather than forcing consensus from genuinely ambiguous
    cases.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务需要多个标注以确保质量时，共识引擎确定何时收集了足够的标签以及如何汇总可能存在冲突的意见。简单的多数投票适用于大多数标注者自然达成一致明确的分类任务：确定图像中是否包含汽车很少产生分歧。但像情感分析或识别细微图像属性这样的更主观的任务会在深思熟虑的标注者之间产生合法的分歧。一种常见的模式是通过每个例子收集3-5个标签，使用Fleiss的kappa（它衡量的是超出偶然性的协议）来计算标注者间的一致性，并将一致性低的例子——通常是kappa低于0.4的例子——路由到专家评审，而不是从真正模糊的案例中强制达成共识。
- en: This tiered approach reflects a fundamental economic trade-off that shapes platform
    architecture. Expert review costs 10-50x more per example than crowdsourced labeling,
    but forcing agreement on ambiguous examples through majority voting of non-experts
    produces systematically biased labels—biased toward easier-to-label patterns that
    may not reflect the complexity important for model robustness. By routing only
    genuinely ambiguous cases to experts—often 5-15% of examples identified through
    low inter-annotator agreement—systems balance cost against quality. The platform
    must implement this routing logic efficiently, tracking which examples need expert
    review and ensuring they’re delivered to appropriately qualified annotators without
    creating bottlenecks.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层方法反映了塑造平台架构的基本经济权衡。专家评审的成本比众包标注每例高出10-50倍，但通过非专家的多数投票来强制对模糊例子达成一致会产生系统性的偏差——倾向于更容易标注的模式，这可能无法反映对模型鲁棒性重要性的复杂性。通过仅将真正模糊的案例路由到专家那里——通常是通过低标注者间一致性识别出的5-15%的例子——系统在成本和质量之间取得平衡。平台必须有效地实施这种路由逻辑，跟踪哪些例子需要专家评审，并确保它们被适当地合格的标注者接收，同时避免形成瓶颈。
- en: 'Maintaining quality at scale requires continuous measurement through gold standard
    injection. The system periodically inserts examples with known correct labels
    into the task stream without revealing which examples are gold standard. This
    enables computing per-annotator accuracy without the Hawthorne effect where measurement
    changes behavior—annotators can’t “try harder” on gold standard examples if they
    don’t know which ones they are. Annotators consistently scoring below 85% on gold
    standards receive additional training materials, more detailed guidelines, or
    removal from the pool if performance doesn’t improve. Beyond simple accuracy,
    systems track quality across multiple dimensions: agreement with peer annotators
    on the same tasks (detecting systematic disagreement suggesting misunderstanding
    of guidelines), time per task (both too fast suggesting careless work and too
    slow suggesting confusion), and consistency where the same annotator sees similar
    examples shown days apart to measure whether they apply labels reliably over time.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模下保持质量需要通过黄金标准注入进行持续测量。系统定期将已知正确标签的示例插入到任务流中，而不透露哪些示例是黄金标准。这使计算每个标注者的准确率成为可能，避免了霍桑效应，即测量改变行为——如果标注者不知道哪些是黄金标准示例，他们就不能在黄金标准示例上“更加努力”。那些在黄金标准上得分持续低于85%的标注者将获得额外的培训材料、更详细的指南，或者如果表现没有改善，则从池中移除。除了简单的准确性之外，系统还从多个维度跟踪质量：与同一任务的同行标注者达成一致（检测到系统性的不一致表明对指南的理解有误），每个任务的时间（太快表明工作粗心大意，太慢表明困惑），以及一致性，即同一标注者在几天后看到相似的示例，以衡量他们是否能够可靠地随时间应用标签。
- en: 'The performance requirements of these systems become demanding at scale. A
    labeling platform processing 10,000 annotations per hour must balance latency
    requirements against database write capacity. Writing each annotation immediately
    to a persistent database like PostgreSQL for durability would require 2-3 writes
    per second, well within database capacity. But task serving—delivering new tasks
    to 100,000 concurrent annotators requesting work—requires subsecond response times
    that databases struggle to provide when serving requests fan out across many annotators.
    Production systems therefore maintain a two-tier storage architecture: Redis caches
    active tasks enabling sub-100ms task assignment latency, while annotations batch
    write to PostgreSQL every 100 annotations (typically every 30-60 seconds), providing
    durability without overwhelming the database with small writes.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统的性能要求在扩展时变得非常苛刻。每小时处理10,000个标注的平台必须在延迟要求和数据库写入容量之间取得平衡。将每个标注立即写入持久数据库（如PostgreSQL）以实现持久性将需要每秒2-3次写入，这在数据库容量范围内。但是，任务服务——向100,000个并发标注者交付新任务——需要亚秒级响应时间，而当请求分散到许多标注者时，数据库在提供服务时很难提供这样的响应时间。因此，生产系统维护一个双层存储架构：Redis缓存活动任务，以实现低于100毫秒的任务分配延迟，而标注则每100个标注（通常每30-60秒）批量写入PostgreSQL，提供持久性而不使数据库因小写入而超载。
- en: Horizontal scaling of these systems requires careful data partitioning. Tasks
    shard by task_id enabling independent task queue scaling, annotator performance
    metrics shard by annotator_id for fast lookup during assignment decisions, and
    aggregated labels shard by example_id for efficient retrieval during model training.
    This partitioning strategy enables systems handling millions of tasks daily to
    support 100,000+ concurrent annotators with median task assignment latency under
    50ms, proving that human-in-the-loop systems can scale to match fully automated
    pipelines when properly architected.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统的水平扩展需要仔细的数据分区。通过task_id对任务进行分片，以实现独立的任务队列扩展，通过annotator_id对标注者性能指标进行分片，以便在分配决策期间快速查找，以及通过example_id对聚合标签进行分片，以便在模型训练期间高效检索。这种分区策略使每天处理数百万个任务的系统能够支持10,000+个并发标注者，平均任务分配延迟低于50毫秒，证明了当适当架构时，人机交互系统可以扩展以匹配全自动化管道。
- en: Beyond these architectural considerations, understanding the economics of labeling
    operations reveals why scalability through AI assistance becomes essential. Data
    labeling represents one of ML systems’ largest hidden costs, yet it is frequently
    overlooked in project planning that focuses primarily on compute infrastructure
    and model training expenses. While teams carefully optimize GPU utilization and
    track training costs measured in dollars per hour, labeling expenses measured
    in dollars per example often receive less scrutiny despite frequently exceeding
    compute costs by orders of magnitude. Understanding the full economic model reveals
    why scalability through AI assistance becomes not just beneficial but economically
    necessary as ML systems mature and data requirements grow to millions or billions
    of labeled examples, which [Chapter 13](ch019.xhtml#sec-ml-operations) examines
    where operational costs compound across the ML lifecycle.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些架构考虑因素之外，了解标签操作的经济效益揭示了为什么通过人工智能辅助实现的可扩展性变得至关重要。数据标注代表了机器学习系统最大的隐藏成本之一，然而，在主要关注计算基础设施和模型训练费用的项目规划中，它往往被忽视。虽然团队会仔细优化GPU利用率并跟踪以每小时美元计算的培训成本，但以每个示例美元计算的标注费用通常受到较少的审查，尽管它经常以数量级超过计算成本。了解完整的经济模型揭示了为什么随着机器学习系统的成熟和数据需求增长到数百万或数十亿个标注示例，通过人工智能辅助实现的可扩展性不仅有益，而且从经济角度来看是必要的，正如第[13章](ch019.xhtml#sec-ml-operations)所考察的，在机器学习生命周期中运营成本会叠加。
- en: 'The cost structure of labeling operations follows a multiplicative model capturing
    both direct annotation costs and quality control overhead:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 标签操作的成本结构遵循一个乘法模型，它同时捕捉直接标注成本和质量控制开销：
- en: <semantics><mrow><mtext mathvariant="normal">Total Cost</mtext><mo>=</mo><mi>N</mi><mo>×</mo><msub><mtext
    mathvariant="normal">Cost</mtext><mtext mathvariant="normal">label</mtext></msub><mo>×</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msub><mi>R</mi><mtext
    mathvariant="normal">review</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msub><mi>R</mi><mtext
    mathvariant="normal">rework</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{Total Cost} = N \times \text{Cost}_{\text{label}}
    \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})</annotation></semantics>
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">总成本</mtext><mo>=</mo><mi>N</mi><mo>×</mo><msub><mtext
    mathvariant="normal">成本</mtext><mtext mathvariant="normal">标注</mtext></msub><mo>×</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msub><mi>R</mi><mtext
    mathvariant="normal">审查</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><msub><mi>R</mi><mtext
    mathvariant="normal">返工</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{总成本} = N \times \text{Cost}_{\text{label}}
    \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})</annotation></semantics>
- en: 'where <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    represents the number of examples, <semantics><msub><mtext mathvariant="normal">Cost</mtext><mtext
    mathvariant="normal">label</mtext></msub><annotation encoding="application/x-tex">\text{Cost}_{\text{label}}</annotation></semantics>
    is the base cost per label, <semantics><msub><mi>R</mi><mtext mathvariant="normal">review</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{review}}</annotation></semantics> is the
    fraction requiring expert review (typically 0.05-0.15), and <semantics><msub><mi>R</mi><mtext
    mathvariant="normal">rework</mtext></msub><annotation encoding="application/x-tex">R_{\text{rework}}</annotation></semantics>
    accounts for labels requiring correction (typically 0.10-0.30). This equation
    reveals how quality requirements compound costs: a dataset requiring 1 million
    labels at $0.10 per label with 10% expert review (costing 5x more, or $0.50) and
    20% rework reaches $138,000, not the $100,000 that naive calculation suggests.
    For comparison, training a ResNet-50 model on this data might cost only $50 for
    compute—nearly 3,000x less than labeling, demonstrating why labeling economics
    dominate total system costs yet receive insufficient attention during planning
    phases.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    表示示例数量，<semantics><msub><mtext mathvariant="normal">Cost</mtext><mtext mathvariant="normal">label</mtext></msub><annotation
    encoding="application/x-tex">\text{Cost}_{\text{label}}</annotation></semantics>
    是每个标注的基础成本，<semantics><msub><mi>R</mi><mtext mathvariant="normal">review</mtext></msub><annotation
    encoding="application/x-tex">R_{\text{review}}</annotation></semantics> 是需要专家审查的比例（通常为0.05-0.15），而<semantics><msub><mi>R</mi><mtext
    mathvariant="normal">rework</mtext></msub><annotation encoding="application/x-tex">R_{\text{rework}}</annotation></semantics>
    考虑了需要修正的标注（通常为0.10-0.30）。此方程揭示了质量要求如何增加成本：一个需要100万个标注且每个标注成本为0.10美元的数据集，如果10%需要专家审查（成本增加5倍，或0.50美元）和20%需要重做，总成本将达到13.8万美元，而不是简单的计算所暗示的10万美元。相比之下，在这个数据集上训练一个ResNet-50模型可能只需50美元的计算成本——这比标注成本低近3000倍，这说明了为什么标注经济学在总系统成本中占主导地位，但在规划阶段却经常受到忽视。
- en: The cost per label varies dramatically by task complexity and required expertise.
    Simple image classification ranges from $0.01-0.05 per label when crowdsourced
    but rises to $0.50-2.00 when requiring expert verification. Bounding boxes cost
    $0.05-0.20 per box for straightforward cases but $1.00-5.00 for dense scenes with
    many overlapping objects. Semantic segmentation can reach $5-50 per image depending
    on precision requirements and object boundaries. Medical image annotation by radiologists
    costs $50-200 per study. When a computer vision system requires 10 million labeled
    images, the difference between $0.02 and $0.05 per label represents $300,000 in
    project costs—often more than the entire infrastructure budget yet frequently
    discovered only after labeling begins.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标注的成本因任务复杂性和所需的专业知识而大幅变化。简单的图像分类在众包时每个标注的成本在0.01-0.05美元之间，但需要专家验证时则上升至0.50-2.00美元。对于简单情况，边界框的成本为每个框0.05-0.20美元，但对于密集场景中许多重叠物体的场景，成本为1.00-5.00美元。语义分割的成本可能达到每张图像5-50美元，具体取决于精度要求和物体边界。放射科医生对医学图像的标注成本为每项研究50-200美元。当计算机视觉系统需要1000万个标注图像时，每标注0.02美元和0.05美元之间的差异代表着30万美元的项目成本——这通常超过了整个基础设施预算，但往往在标注开始后才被发现。
- en: Scaling with AI-Assisted Labeling
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用人工智能辅助标注进行扩展
- en: 'As labeling demands grow exponentially with modern ML systems, scalability
    becomes critical. The scalability pillar drives AI assistance as a force multiplier
    for human labeling rather than a replacement. Manual annotation alone cannot keep
    pace with modern ML systems’ data needs, while fully automated labeling lacks
    the nuanced judgment that humans provide. AI-assisted labeling finds the sweet
    spot: using automation to handle clear cases and accelerate annotation while preserving
    human judgment for ambiguous or high-stakes decisions. As illustrated in [Figure 6.14](ch012.xhtml#fig-weak-supervision),
    AI assistance offers several paths to scale labeling operations, each requiring
    careful system design to balance speed, quality, and resource usage.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代机器学习系统对标注需求呈指数级增长，可扩展性变得至关重要。可扩展性支柱推动人工智能辅助作为人类标注的倍增器，而不是替代品。仅靠人工标注无法跟上现代机器学习系统对数据的需求，而完全自动化的标注则缺乏人类提供的细微判断。人工智能辅助标注找到了最佳平衡点：利用自动化处理明确案例并加速标注，同时保留人类对模糊或高风险决策的判断。如图[图6.14](ch012.xhtml#fig-weak-supervision)所示，人工智能辅助提供了多种扩展标注操作的方法，每种方法都需要仔细的系统设计来平衡速度、质量和资源使用。
- en: '![](../media/file85.svg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file85.svg)'
- en: 'Figure 6.14: **AI-Augmented Labeling**: Programmatic labeling, distant supervision,
    and active learning scale data annotation by trading potential labeling errors
    for increased throughput, necessitating careful system design to balance labeling
    speed, cost, and model quality. These strategies enable machine learning systems
    to overcome limitations imposed by manual annotation alone, facilitating deployment
    in data-scarce environments. Source: Stanford AI Lab.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：**AI增强标注**：通过以潜在标注错误为代价提高吞吐量，程序化标注、远程监督和主动学习扩展了数据标注，这需要仔细的系统设计来平衡标注速度、成本和模型质量。这些策略使机器学习系统能够克服仅由人工标注带来的限制，从而促进在数据稀缺环境中的部署。来源：斯坦福AI实验室。
- en: Modern AI-assisted labeling typically employs a combination of approaches working
    together in the pipeline. Pre-annotation involves using AI models to generate
    preliminary labels for a dataset, which humans can then review and correct. Major
    labeling platforms have made significant investments in this technology. [Snorkel
    AI](https://snorkel.ai/) uses programmatic labeling ([Ratner et al. 2018](ch058.xhtml#ref-ratner2018snorkel))
    to automatically generate initial labels at scale through rule-based heuristics
    and weak supervision signals. Scale AI deploys pre-trained models to accelerate
    annotation in specific domains like autonomous driving, where object detection
    models pre-label vehicles and pedestrians that humans then verify and refine.
    Companies like [SuperAnnotate](https://www.superannotate.com/) provide automated
    pre-labeling tools that can reduce manual effort by 50-80% for computer vision
    tasks. This method, which often employs semi-supervised learning techniques ([Chapelle,
    Scholkopf, and Zien 2009](ch058.xhtml#ref-chapelle2009semisupervised)), can save
    significant time, especially for extremely large datasets.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现代AI辅助标注通常采用在管道中协同工作的多种方法的组合。预标注涉及使用AI模型为数据集生成初步标签，然后人类可以对其进行审查和纠正。主要的标注平台在这项技术上进行了大量投资。[Snorkel
    AI](https://snorkel.ai/)使用程序化标注（[Ratner等人，2018](ch058.xhtml#ref-ratner2018snorkel)）通过基于规则的启发式方法和弱监督信号自动生成初始标签。Scale
    AI部署预训练模型以加速特定领域（如自动驾驶）的标注，其中目标检测模型预先标注车辆和行人，然后由人类验证和改进。像[SuperAnnotate](https://www.superannotate.com/)这样的公司提供自动预标注工具，可以减少计算机视觉任务的50-80%的人工工作量。这种方法通常采用半监督学习技术（[Chapelle、Scholkopf和Zien，2009](ch058.xhtml#ref-chapelle2009semisupervised)），可以节省大量时间，尤其是在处理极其庞大的数据集时。
- en: The emergence of Large Language Models (LLMs) like ChatGPT has further transformed
    labeling pipelines. Beyond simple classification, LLMs can generate rich text
    descriptions, create labeling guidelines from examples, and even explain their
    reasoning for label assignments. For instance, content moderation systems use
    LLMs to perform initial content classification and generate explanations for policy
    violations that human reviewers can validate. However, integrating LLMs introduces
    new system challenges around inference costs (API calls can cost $0.01-$1 per
    example depending on complexity), rate limiting (cloud APIs typically limit to
    100-10,000 requests per minute), and output validation (LLMs occasionally produce
    confident but incorrect labels requiring systematic validation). Many organizations
    adopt a tiered approach, using smaller specialized models for routine cases while
    reserving larger LLMs for complex scenarios requiring nuanced judgment or rare
    domain expertise.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如ChatGPT的出现进一步改变了标注流程。除了简单的分类之外，LLMs可以生成丰富的文本描述，从示例中创建标注指南，甚至解释它们对标签分配的推理。例如，内容审查系统使用LLMs进行初始内容分类，并为政策违规生成解释，供人类审查员验证。然而，集成LLMs引入了新的系统挑战，包括推理成本（API调用可能根据复杂性每例花费0.01-1美元）、速率限制（云API通常限制每分钟100-10,000次请求）和输出验证（LLMs偶尔会生成自信但错误的标签，需要系统验证）。许多组织采用分层方法，使用较小的专用模型处理常规案例，而将较大的LLMs保留用于需要细微判断或罕见领域专业知识的复杂场景。
- en: 'Methods such as active learning complement these approaches by intelligently
    prioritizing which examples need human attention ([Coleman et al. 2022](ch058.xhtml#ref-coleman2022similarity)).
    These systems continuously analyze model uncertainty to identify valuable labeling
    candidates. Rather than labeling a random sample of unlabeled data, active learning
    selects examples where the current model is most uncertain or where labels would
    most improve model performance. The infrastructure must efficiently compute uncertainty
    metrics (often prediction entropy or disagreement between ensemble models), maintain
    task queues ordered by informativeness, and adapt prioritization strategies based
    on incoming labels. Consider a medical imaging system: active learning might identify
    unusual pathologies for expert review while handling routine cases through pre-annotation
    that experts merely verify. This approach can reduce required annotations by 50-90%
    compared to random sampling, though it requires careful engineering to prevent
    feedback loops where the model’s uncertainty biases which data gets labeled.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 活动学习等方法通过智能地优先处理哪些示例需要人类关注来补充这些方法（[Coleman等人2022](ch058.xhtml#ref-coleman2022similarity)）。这些系统持续分析模型的不确定性，以识别有价值的标注候选者。与其对未标记数据进行随机样本标注，活动学习会选择当前模型最不确定或标签最能提高模型性能的示例。基础设施必须有效地计算不确定性指标（通常是预测熵或集成模型之间的不一致性），维护按信息量排序的任务队列，并根据传入的标签调整优先级策略。考虑一个医学影像系统：活动学习可能会识别出需要专家审查的不寻常病理，同时通过预标注处理常规案例，专家只需验证。与随机抽样相比，这种方法可以将所需的标注减少50-90%，尽管它需要仔细的工程来防止模型的不确定性偏差导致哪些数据被标注的反馈循环。
- en: 'Quality control becomes increasingly crucial as these AI components interact.
    The system must monitor both AI and human performance through systematic metrics.
    Model confidence calibration matters: if the AI says it’s 95% confident but is
    actually only 75% accurate at that confidence level, pre-annotations mislead human
    reviewers. Human-AI agreement rates reveal whether AI assistance helps or hinders:
    when humans frequently override AI suggestions, the pre-annotations may be introducing
    bias rather than accelerating work. These metrics require careful instrumentation
    throughout the labeling pipeline, tracking not just final labels but the interaction
    between human and AI at each stage.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些AI组件的交互，质量控制变得越来越重要。系统必须通过系统指标监控AI和人类的表现。模型置信度校准很重要：如果AI表示有95%的信心，但实际上在该置信水平下只有75%的准确率，预标注会误导人类审查员。人机一致性比率揭示了AI辅助是否有助于或阻碍：当人类频繁地覆盖AI建议时，预标注可能会引入偏差而不是加速工作。这些指标需要在标注管道的每个阶段进行仔细的仪器化，不仅要跟踪最终标签，还要跟踪人类和AI在每个阶段的交互。
- en: In safety-critical domains like self-driving cars, these systems must maintain
    particularly rigorous standards while processing massive streams of sensor data.
    Waymo’s labeling infrastructure reportedly processes millions of sensor frames
    daily, using AI pre-annotation to label common objects (vehicles, pedestrians,
    traffic signs) while routing unusual scenarios (construction zones, emergency
    vehicles, unusual road conditions) to human experts. The system must maintain
    real-time performance despite this scale, using distributed architectures where
    pre-annotation runs on GPU clusters while human review scales horizontally across
    thousands of annotators, with careful load balancing ensuring neither component
    becomes a bottleneck.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶汽车等安全关键领域，这些系统在处理大量传感器数据时必须保持特别严格的标准。据报道，Waymo的标注基础设施每天处理数百万个传感器帧，使用AI预标注来标注常见物体（车辆、行人、交通标志），并将不寻常的场景（施工区域、紧急车辆、不寻常的道路状况）路由给人类专家。尽管规模如此之大，系统仍需保持实时性能，采用分布式架构，其中预标注在GPU集群上运行，而人类审查则横向扩展到数千名标注者，通过仔细的负载平衡确保没有任何组件成为瓶颈。
- en: 'Real-world deployments demonstrate these principles at scale in diverse domains.
    Medical imaging systems ([Krishnan, Rajpurkar, and Topol 2022](ch058.xhtml#ref-krishnan2022selfsupervised))
    combine pre-annotation for common conditions (identifying normal tissue, standard
    anatomical structures) with active learning for unusual cases (rare pathologies,
    ambiguous findings), all while maintaining strict patient privacy through secure
    annotation platforms with comprehensive audit trails. Self-driving vehicle systems
    coordinate multiple AI models to label diverse sensor data: one model pre-labels
    camera images, another handles lidar point clouds, a third processes radar data,
    with fusion logic combining predictions before human review. Social media platforms
    process millions of items hourly using tiered approaches where simpler models
    handle clear violations (spam, obvious hate speech) while complex content routes
    to more sophisticated models or human reviewers when initial classification is
    uncertain.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的部署在多个领域展示了这些原则的规模化应用。医学影像系统([Krishnan, Rajpurkar, and Topol 2022](ch058.xhtml#ref-krishnan2022selfsupervised))结合了对常见状况的预标注（识别正常组织、标准解剖结构）和针对不寻常案例的主动学习（罕见病理、模糊发现），同时通过具有全面审计跟踪的安全标注平台保持严格的病人隐私。自动驾驶车辆系统协调多个AI模型对多种传感器数据进行标注：一个模型预标注摄像头图像，另一个处理激光雷达点云，第三个处理雷达数据，融合逻辑在人工审查前结合预测。社交媒体平台每小时处理数百万条内容，采用分层方法，简单模型处理明显的违规行为（垃圾邮件、明显的仇恨言论），而复杂内容则路由到更复杂的模型或人工审查员，当初始分类不确定时。
- en: Ensuring Ethical and Fair Labeling
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确保道德和公平的标注
- en: Unlike previous sections where governance focused on data and processes, labeling
    governance centers on human welfare. The governance pillar here addresses ethical
    treatment of human contributors, bias mitigation, and fair compensation—challenges
    that manifest distinctly from governance in automated pipeline stages because
    human welfare is directly at stake. While governance in processing focuses on
    data lineage and compliance, governance in labeling requires ensuring that the
    humans creating training data are treated ethically, compensated fairly, and protected
    from harm.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前章节中治理重点在数据和流程不同，标注治理以人类福祉为中心。这里的治理支柱关注人类贡献者的道德待遇、偏差缓解和公平补偿——这些挑战在自动化管道阶段中与治理明显不同，因为人类的福祉直接受到威胁。虽然处理中的治理关注数据溯源和合规性，但标注中的治理需要确保创建训练数据的人类被道德对待、公平补偿并受到保护，免受伤害。
- en: However, alongside these compelling advantages of crowdsourcing, the challenges
    highlighted by real-world examples demonstrate why governance cannot be an afterthought.
    The issue of fair compensation and ethical data sourcing was brought into sharp
    focus during the development of large-scale AI systems like OpenAI’s ChatGPT.
    Reports revealed that [OpenAI outsourced data annotation tasks to workers in Kenya](https://time.com/6247678/openai-chatgpt-kenya-workers/),
    employing them to moderate content and identify harmful or inappropriate material
    that the model might generate. This involved reviewing and labeling distressing
    content, such as graphic violence and explicit material, to train the AI in recognizing
    and avoiding such outputs. While this approach enabled OpenAI to improve the safety
    and utility of ChatGPT, significant ethical concerns arose around the working
    conditions, the nature of the tasks, and the compensation provided to Kenyan workers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管众包具有这些令人信服的优势，但现实世界中的例子所凸显的挑战表明治理不能是事后之想。在开发像OpenAI的ChatGPT这样的大型AI系统期间，公平补偿和道德数据来源的问题被尖锐地聚焦。报告显示[OpenAI将数据标注任务外包给肯尼亚的工人](https://time.com/6247678/openai-chatgpt-kenya-workers/)，雇佣他们来审查内容并识别模型可能生成的有害或不适当的内容。这包括审查和标记令人不安的内容，如暴力画面和露骨材料，以训练AI识别并避免此类输出。虽然这种方法使OpenAI能够提高ChatGPT的安全性和实用性，但关于工作条件、任务性质以及给予肯尼亚工人的补偿的伦理问题引起了重大的关注。
- en: Many of the contributors were reportedly paid as little as $1.32 per hour for
    reviewing and labeling highly traumatic material. The emotional toll of such work,
    coupled with low wages, raised serious questions about the fairness and transparency
    of the crowdsourcing process. This controversy highlights a critical gap in ethical
    crowdsourcing practices. The workers, often from economically disadvantaged regions,
    were not adequately supported to cope with the psychological impact of their tasks.
    The lack of mental health resources and insufficient compensation underscored
    the power imbalances that can emerge when outsourcing data annotation tasks to
    lower-income regions.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 据报道，许多贡献者每小时仅获得1.32美元的报酬，用于审查和标记高度创伤性的材料。这种工作的情感负担，加上低工资，引发了关于众包过程公平性和透明度的严重质疑。这一争议突显了道德众包实践中的关键差距。这些工人，通常来自经济不发达地区，没有得到足够的支持来应对他们任务的心理影响。缺乏心理健康资源和补偿不足凸显了当将数据标注任务外包给低收入地区时可能出现的权力不平衡。
- en: Unfortunately, the challenges highlighted by the ChatGPT Kenya controversy are
    not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation
    face similar issues. As machine learning systems grow more complex and require
    larger datasets, the demand for annotated data will continue to increase. This
    shows the need for industry-wide standards and best practices to ensure ethical
    data sourcing. Fair compensation means paying at least local minimum wages, ideally
    benchmarked against comparable work in workers’ regions—not just the legally minimum
    but what would be considered fair for skilled work requiring sustained attention.
    For sensitive content moderation, this often means premium pay reflecting psychological
    burden, sometimes 2-3x base rates.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ChatGPT肯尼亚争议所凸显的挑战并不仅限于OpenAI。许多依赖众包进行数据标注的组织面临类似的问题。随着机器学习系统变得更加复杂并需要更大的数据集，对标注数据的需求将继续增加。这表明需要行业范围内的标准和最佳实践来确保道德数据来源。公平的补偿意味着至少支付当地最低工资，理想情况下与工人所在地区的类似工作进行比较——不仅仅是法律规定的最低工资，而是对于需要持续注意的熟练工作所认为的公平工资。对于敏感内容审核，这通常意味着反映心理负担的溢价支付，有时是基础工资的2-3倍。
- en: 'Worker wellbeing requires providing mental health resources for those dealing
    with sensitive content. Organizations like [Scale AI](https://scale.com) have
    implemented structured support including: limiting exposure to traumatic content
    (rotating annotators through different content types, capping hours per day on
    disturbing material), providing access to counseling services at no cost to workers,
    and offering immediate support channels when annotators encounter particularly
    disturbing content. These measures add operational cost but are essential for
    ethical operations. Transparency demands clear communication about task purposes,
    how contributions will be used, what kind of content workers might encounter,
    and worker rights including ability to skip tasks that cause distress.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 工人的福祉需要为处理敏感内容的人提供心理健康资源。像[Scale AI](https://scale.com)这样的组织已经实施了结构化的支持措施，包括：限制接触创伤性内容（通过不同内容类型轮换标注者，限制每天处理令人不安材料的时长），为工人提供免费的心理咨询服务，以及当标注者遇到特别令人不安的内容时提供即时支持渠道。这些措施增加了运营成本，但对于道德运营至关重要。透明度要求明确沟通任务目的、贡献如何被使用、工人可能遇到的内容类型，以及工人的权利，包括跳过引起不适的任务的能力。
- en: Beyond working conditions, bias in data labeling represents another critical
    governance concern. Annotators bring their own cultural, personal, and professional
    biases to the labeling process, which can be reflected in the resulting dataset.
    For example, T. Wang et al. ([2019](ch058.xhtml#ref-wang2019balanced)) found that
    image datasets labeled predominantly by annotators from one geographic region
    showed biases in object recognition tasks, performing poorly on images from other
    regions. This highlights the need for diverse annotator pools where demographic
    diversity among annotators helps counteract individual biases, though it doesn’t
    eliminate them. Regular bias audits examining whether label distributions differ
    systematically across annotator demographics, monitoring for patterns suggesting
    systematic bias (all images from certain regions receiving lower quality scores),
    and addressing identified biases through additional training or guideline refinement
    ensure labels support fair model behavior.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 除了工作条件之外，数据标注中的偏见代表了另一个关键的治理关注点。标注员在标注过程中会带入自己的文化、个人和专业偏见，这些偏见可能会反映在生成的数据集中。例如，T.
    王等人（[2019](ch058.xhtml#ref-wang2019balanced)）发现，主要由来自一个地理区域的标注员标注的图像数据集在物体识别任务中表现出偏见，对来自其他地区的图像表现不佳。这突显了需要多样化的标注员群体，其中标注员之间的人口统计学多样性有助于抵消个人偏见，尽管这并不能完全消除它们。定期进行偏见审计，检查标注员人口统计学中标签分布是否系统性地不同，监控可能表明系统性偏见的模式（来自某些地区的所有图像都获得较低的质量评分），以及通过额外的培训或指南细化来应对识别出的偏见，确保标签支持公平的模型行为。
- en: Data privacy and ethical considerations also pose challenges in data labeling.
    Leading data labeling companies have developed specialized solutions for these
    challenges. Scale AI, for instance, maintains dedicated teams and secure infrastructure
    for handling sensitive data in healthcare and finance, with HIPAA-compliant annotation
    platforms and strict data access controls. Appen implements strict data access
    controls and anonymization protocols, ensuring annotators never see personally
    identifiable information when unnecessary. Labelbox offers private cloud deployments
    for organizations with strict security requirements, enabling annotation without
    data leaving organizational boundaries. These privacy-preserving techniques connect
    directly to the security considerations we explore in future chapters[25](#fn25),
    where comprehensive approaches to protecting sensitive data throughout the ML
    lifecycle are examined.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私和伦理考量也在数据标注中提出了挑战。领先的数据标注公司已经为这些挑战开发了专门的解决方案。例如，Scale AI 维护了专门处理医疗保健和金融领域敏感数据的团队和安全的基础设施，拥有符合HIPAA标准的标注平台和严格的数据访问控制。Appen
    实施了严格的数据访问控制和匿名协议，确保标注员在不需要时永远不会看到个人可识别信息。Labelbox 为具有严格安全要求的组织提供私有云部署，允许在不离开组织边界的情况下进行标注。这些保护隐私的技术直接关联到我们在未来章节中探讨的安全考量[25](#fn25)，其中将检查保护整个机器学习生命周期中敏感数据的综合方法。
- en: Beyond privacy and working conditions, the dynamic nature of real-world data
    presents another limitation. Labels that are accurate at the time of annotation
    may become outdated or irrelevant as the underlying distribution of data changes
    over time. This concept, known as concept drift, necessitates ongoing labeling
    efforts and periodic re-evaluation of existing labels. Governance frameworks must
    account for label versioning (tracking when labels were created and by whom),
    re-annotation policies (systematically re-labeling data when concepts evolve),
    and retirement strategies (identifying when old labels should be deprecated rather
    than used for training).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 除了隐私和工作条件之外，现实世界数据的动态性也带来了另一个限制。在标注时准确无误的标签可能会随着时间推移，数据的基本分布发生变化而变得过时或不相关。这个概念被称为概念漂移，需要持续进行标注工作，并定期重新评估现有标签。治理框架必须考虑到标签版本控制（跟踪标签的创建时间和创建者）、重新标注政策（当概念演变时系统地重新标注数据），以及退役策略（确定何时应该弃用旧标签而不是用于训练）。
- en: 'Finally, the limitations of current labeling approaches become apparent when
    dealing with edge cases or rare events. In many real-world applications, it’s
    the unusual or rare instances that are often most critical (e.g., rare diseases
    in medical diagnosis, or unusual road conditions in autonomous driving). However,
    these cases are, by definition, underrepresented in most datasets and may be overlooked
    or mislabeled in large-scale annotation efforts. Governance requires explicit
    strategies for handling rare events: targeted collection campaigns for underrepresented
    scenarios, expert review requirements for rare cases, and systematic tracking
    ensuring rare events receive appropriate attention despite their low frequency.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在处理边缘案例或罕见事件时，当前标注方法的局限性变得明显。在许多实际应用中，不寻常或罕见的情况往往是至关重要的（例如，医学诊断中的罕见疾病，或自动驾驶中的不寻常道路条件）。然而，根据定义，这些案例在大多数数据集中代表性不足，可能在大规模标注工作中被忽视或错误标注。治理需要针对处理罕见事件的明确策略：针对代表性不足场景的目标收集活动，对罕见案例的专家审查要求，以及系统跟踪确保即使频率低，罕见事件也能得到适当的关注。
- en: This case emphasizes the importance of considering the human labor behind AI
    systems. While crowdsourcing offers scalability and diversity, it also brings
    ethical responsibilities that cannot be overlooked. Organizations must prioritize
    the well-being and fair treatment of contributors as they build the datasets that
    drive AI innovation. Governance in labeling ultimately means recognizing that
    training data isn’t just bits and bytes but the product of human labor deserving
    respect, fair compensation, and ethical treatment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这个案例强调了在考虑人工智能系统背后的人力劳动的重要性。虽然众包提供了可扩展性和多样性，但也带来了不能忽视的伦理责任。在构建推动人工智能创新的数据库时，组织必须优先考虑贡献者的福祉和公平待遇。在标签标注的治理最终意味着认识到训练数据不仅仅是比特和字节，而是值得尊重、公平补偿和道德对待的人力劳动成果。
- en: 'Case Study: Automated Labeling in KWS Systems'
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：KWS系统中的自动标注
- en: Continuing our KWS case study through the labeling stage—having established
    systematic problem definition ([Section 6.3.3](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)),
    diverse data collection strategies that address quality and coverage requirements,
    ingestion patterns handling both batch and streaming workflows, and processing
    pipelines ensuring training-serving consistency—we now confront a challenge unique
    to speech systems at scale. Generating millions of labeled wake word samples without
    proportional human annotation cost requires moving beyond the manual and crowdsourced
    approaches we examined earlier. The Multilingual Spoken Words Corpus (MSWC) ([Mazumder
    et al. 2021](ch058.xhtml#ref-mazumder2021multilingual)) demonstrates how automated
    labeling addresses this challenge through its innovative approach to generating
    labeled wake word data, containing over 23.4 million one-second spoken examples
    across 340,000 keywords in 50 different languages.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的KWS案例研究中继续通过标注阶段——在确立了系统性的问题定义（[第6.3.3节](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)）、解决质量和覆盖要求的多样化数据收集策略、处理批量和工作流流式传输的摄入模式，以及确保训练-服务一致性的处理管道之后——我们现在面临一个在规模化的语音系统中独特的挑战。在没有成比例的人力标注成本的情况下生成数百万个标注唤醒词样本，需要超越我们之前考察的手动和众包方法。多语言口语词汇语料库（MSWC）([Mazumder等人2021](ch058.xhtml#ref-mazumder2021multilingual))展示了通过其创新的方法生成标注唤醒词数据如何解决这一挑战，包含超过2340万个一秒钟的口语示例，涵盖了50种不同语言中的340000个关键词。
- en: This scale directly reflects our framework pillars in practice. Achieving our
    quality target of 98% accuracy across diverse environments requires millions of
    training examples covering acoustic variations we identified during problem definition.
    Reliability demands representation across varied acoustic conditions—different
    background noises, speaking styles, and recording environments. Scalability necessitates
    automation rather than manual labeling because 23.4 million examples would require
    approximately 2,600 person-years of effort at even 10 seconds per label, making
    manual annotation economically infeasible. Governance requirements mandate transparent
    sourcing and language diversity, ensuring voice-activated technology serves speakers
    of many languages rather than concentrating on only the most commercially valuable
    markets.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规模直接反映了我们在实践中框架的支柱。要实现我们在各种环境中达到98%准确性的质量目标，需要数百万个训练示例，这些示例涵盖了我们在问题定义期间识别出的声学变化。可靠性要求在多种声学条件下进行表示——不同的背景噪音、说话风格和录音环境。可扩展性需要自动化而不是人工标记，因为2340万个示例即使在每标签10秒的情况下也需要大约26000人年的人工，这使得人工标注在经济上不可行。治理要求透明的来源和语言多样性，确保语音激活技术服务于多种语言的说话者，而不仅仅是集中在最有商业价值的市场上。
- en: As illustrated in [Figure 6.15](ch012.xhtml#fig-mswc), this automated system
    begins with paired sentence audio recordings and corresponding transcriptions
    from projects like [Common Voice](https://commonvoice.mozilla.org/en) or multilingual
    captioned content platforms. The system processes these inputs through forced
    alignment[26](#fn26)—a computational technique that identifies precise word boundaries
    within continuous speech by analyzing both audio and transcription simultaneously.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.15](ch012.xhtml#fig-mswc)所示，这个自动化系统从配对句子音频记录和相应的转录开始，这些转录来自像[Common Voice](https://commonvoice.mozilla.org/en)或多语言字幕内容平台的项目。系统通过强制对齐[26](#fn26)对这些输入进行处理——这是一种计算技术，通过同时分析音频和转录来识别连续语音中的精确单词边界。
- en: '![](../media/file86.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file86.png)'
- en: 'Figure 6.15: **Multilingual Data Preparation**: Forced alignment and segmentation
    transform paired audio-text data into labeled one-second segments, creating a
    large-scale corpus for training keyword spotting models across 50+ languages.
    This automated process enables scalable development of KWS systems by efficiently
    generating training examples from readily available speech resources like common
    voice and multilingual captioned content.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15：**多语言数据准备**：强制对齐和分割将配对音频-文本数据转换为标记的一秒段，创建了一个大规模语料库，用于在50多种语言中训练关键词检测模型。这个自动化过程通过有效地从常见的语音资源（如common
    voice和多语言字幕内容）中生成训练示例，使得KWS系统的可扩展开发成为可能。
- en: 'Building on these precise timing markers, the extraction system generates clean
    keyword samples while handling engineering challenges our problem definition anticipated:
    background noise interfering with word boundaries, speakers stretching or compressing
    words unexpectedly beyond our target 500-800 millisecond duration, and longer
    words exceeding the one-second boundary. MSWC provides automated quality assessment
    that analyzes audio characteristics to identify potential issues with recording
    quality, speech clarity, or background noise—crucial for maintaining consistent
    standards across 23 million samples without the manual review expenses that would
    make this scale prohibitive.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些精确的时间标记的基础上，提取系统生成干净的关键词样本，同时处理我们的问题定义所预期的工程挑战：背景噪音干扰单词边界，说话者意外地拉伸或压缩单词，超出我们目标500-800毫秒的持续时间，以及超过一秒边界的长单词。MSWC提供自动质量评估，分析音频特征以识别录音质量、语音清晰度或背景噪音方面可能的问题——这对于在2.3亿个样本中保持一致标准至关重要，避免了手动审查费用，否则这种规模将是不可行的。
- en: 'Modern voice assistant developers often build upon this automated labeling
    foundation. While automated corpora may not contain the specific wake words a
    product requires, they provide starting points for KWS prototyping, particularly
    in underserved languages where commercial datasets don’t exist. Production systems
    typically layer targeted human recording and verification for challenging cases—unusual
    accents, rare words, or difficult acoustic environments that automated systems
    struggle with—requiring infrastructure that gracefully coordinates between automated
    processing and human expertise. This demonstrates how the four pillars guide integration:
    quality through targeted human verification, reliability through automated consistency,
    scalability through forced alignment, and governance through transparent sourcing
    and multilingual coverage.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现代语音助手开发者通常基于这个自动标注的基础进行构建。虽然自动语料库可能不包含产品所需的具体唤醒词，但它们为KWS原型设计提供了起点，尤其是在商业数据集不存在的欠发达语言中。生产系统通常针对具有挑战性的案例——如不寻常的口音、罕见词汇或自动化系统难以处理的困难声学环境——进行目标人类录音和验证，需要能够优雅地在自动化处理和人类专业知识之间协调的基础设施。这展示了四个支柱如何指导整合：通过目标人类验证保证质量，通过自动化一致性保证可靠性，通过强制对齐保证可扩展性，通过透明来源和多语言覆盖保证治理。
- en: The sophisticated orchestration of forced alignment, extraction, and quality
    control demonstrates how thoughtful data engineering directly impacts production
    machine learning systems. When a voice assistant responds to its wake word, it
    draws upon this labeling infrastructure combined with the collection strategies,
    pipeline architectures, and processing transformations we’ve examined throughout
    this chapter. Storage architecture, which we turn to next, completes this picture
    by determining how these carefully labeled datasets are organized, accessed, and
    maintained throughout the ML lifecycle, enabling efficient training iterations
    and reliable serving at scale.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 强制对齐、提取和质量控制的复杂编排展示了深思熟虑的数据工程如何直接影响生产机器学习系统。当语音助手对其唤醒词做出响应时，它会利用这个标注基础设施，结合我们在本章中考察的收集策略、管道架构和处理转换。我们接下来要讨论的存储架构，通过确定这些精心标注的数据集在整个机器学习生命周期中的组织、访问和维护方式，完成了这幅图景，使得高效的训练迭代和大规模可靠服务成为可能。
- en: Strategic Storage Architecture
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 战略存储架构
- en: After establishing systematic processing pipelines that transform raw data into
    ML-ready formats, we must design storage architectures that support the entire
    ML lifecycle while maintaining our four-pillar framework. Storage decisions determine
    how effectively we can maintain data quality over time, ensure reliable access
    under varying loads, scale to handle growing data volumes, and implement governance
    controls. The seemingly straightforward question of “where should we store this
    data” actually encompasses complex trade-offs between access patterns, cost constraints,
    consistency requirements, and performance characteristics that fundamentally shape
    how ML systems operate.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立将原始数据转换为机器学习准备格式的系统处理管道之后，我们必须设计支持整个机器学习生命周期的存储架构，同时保持我们的四支柱框架。存储决策决定了我们如何有效地维护数据质量，确保在变化负载下的可靠访问，扩展以处理不断增长的数据量，并实施治理控制。看似简单的问题“我们应该在哪里存储这些数据”，实际上涵盖了访问模式、成本限制、一致性要求和性能特性之间的复杂权衡，这些权衡从根本上塑造了机器学习系统的运行方式。
- en: ML storage requirements differ fundamentally from transactional systems that
    power traditional applications. Rather than optimizing for frequent small writes
    and point lookups that characterize e-commerce or banking systems, ML workloads
    prioritize high-throughput sequential reads over frequent writes, large-scale
    scans over row-level updates, and schema flexibility over rigid structures. A
    database serving an e-commerce application performs well with millions of individual
    product lookups per second, but an ML training job that needs to scan that entire
    product catalog repeatedly across training epochs requires completely different
    storage optimization. This section examines how to match storage architectures
    to ML workload characteristics, comparing databases, data warehouses, and data
    lakes before exploring specialized ML infrastructure like feature stores and examining
    how storage requirements evolve across the ML lifecycle.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ML 存储需求与驱动传统应用的交易系统存在根本区别。而不是优化频繁的小型写入和点查询，这些是电子商务或银行系统所特有的，ML 工作负载优先考虑高吞吐量顺序读取，而非频繁写入，大规模扫描而非行级更新，以及模式灵活性而非刚性结构。一个服务于电子商务应用的数据库每秒处理数百万个单独的产品查询表现良好，但需要在整个产品目录上重复扫描以进行训练的
    ML 训练作业则需要完全不同的存储优化。本节将探讨如何将存储架构与 ML 工作负载特性相匹配，在探讨专门的 ML 基础设施（如特征存储）之前，比较数据库、数据仓库和数据湖。
- en: ML Storage Systems Architecture Options
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML 存储系统架构选项
- en: Storage system selection represents a critical architectural decision that affects
    all aspects of the ML lifecycle from development through production operations.
    The choice between databases, data warehouses, and data lakes determines not just
    where data resides but how quickly teams can iterate during development, how models
    access training data, and how serving systems retrieve features in production.
    Understanding these trade-offs requires examining both fundamental storage characteristics
    and the specific access patterns of different ML tasks.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 存储系统选择是一个关键架构决策，它影响 ML 生命周期的各个方面，从开发到生产操作。数据库、数据仓库和数据湖之间的选择不仅决定了数据存放的位置，还决定了团队在开发期间可以多快迭代，模型如何访问训练数据，以及生产中服务系统如何检索特征。理解这些权衡需要检查基本存储特性以及不同
    ML 任务的特定访问模式。
- en: 'The key insight is that different ML workloads have fundamentally different
    storage requirements based on their access patterns and latency needs:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的洞察是，不同的 ML 工作负载根据其访问模式和延迟需求具有根本不同的存储需求：
- en: '**Databases (OLTP)**: Excel for online feature serving where you need low-latency,
    random access to individual records. A recommendation system looking up a user’s
    profile during real-time inference exemplifies this pattern: millisecond lookups
    of specific user features (age, location, preferences) to generate personalized
    recommendations.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库（OLTP）**：适用于需要低延迟、随机访问单个记录的在线特征服务。一个在实时推理期间查找用户配置文件的推荐系统就是这种模式的例子：对特定用户特征（年龄、位置、偏好）进行毫秒级查找以生成个性化推荐。'
- en: '**Data warehouses (OLAP)**: Optimize for model training on structured data
    where you need high-throughput, sequential scans over large, clean tables. Training
    a fraud detection model that processes millions of transactions with hundreds
    of features per transaction benefits from columnar storage that reads only relevant
    features efficiently.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据仓库（OLAP）**：优化用于结构化数据上的模型训练，其中需要高吞吐量、对大型、干净表进行顺序扫描。训练一个处理数百万交易且每笔交易有数百个特征的欺诈检测模型，得益于仅高效读取相关特征的列式存储。'
- en: '**Data lakes**: Handle exploratory data analysis and training on unstructured
    data (images, audio, text) where you need flexibility and low-cost storage for
    massive volumes. A computer vision system storing terabytes of raw images alongside
    metadata, annotations, and intermediate processing results requires the schema
    flexibility and cost efficiency that only data lakes provide.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖**：处理对非结构化数据（图像、音频、文本）的探索性分析和训练，其中需要灵活性和低成本存储以处理海量数据。一个计算机视觉系统存储了数以千计的原始图像以及元数据、注释和中间处理结果，需要数据湖提供的模式灵活性和成本效益。'
- en: 'Databases excel at operational and transactional purposes, maintaining product
    catalogs, user profiles, or transaction histories with strong consistency guarantees
    and low-latency point lookups. For ML workflows, databases serve specific roles
    well: storing feature metadata that changes frequently, managing experiment tracking
    where transactional consistency matters, or maintaining model registries that
    require atomic updates. A PostgreSQL database handling structured user attributes—user_id,
    age, country, preferences—provides millisecond lookups for serving systems that
    need individual user features in real-time. However, databases struggle when ML
    training requires scanning millions of records repeatedly across multiple epochs.
    The row-oriented storage that optimizes transactional lookups becomes inefficient
    when training needs only 20 of 100 columns from each record but must read entire
    rows to extract those columns.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库在操作和事务目的上表现出色，能够以强一致性保证和低延迟的点查询来维护产品目录、用户资料或交易历史。对于机器学习工作流程，数据库很好地扮演了特定角色：存储频繁变化的特征元数据、管理交易一致性至关重要的实验跟踪，或者维护需要原子更新的模型注册表。一个处理结构化用户属性（如user_id、年龄、国家、偏好）的PostgreSQL数据库，可以为需要实时提供单个用户特征的系统提供毫秒级的查询。然而，当机器学习训练需要跨多个epoch重复扫描数百万条记录时，数据库就力不从心。优化事务查询的行存储在训练只需要从每条记录中读取20个100个列时变得效率低下，因为必须读取整个行来提取这些列。
- en: Data warehouses fill this analytical gap, optimized for complex queries across
    integrated datasets transformed into standardized schemas. Modern warehouses like
    Google BigQuery, Amazon Redshift, and Snowflake use columnar storage formats ([Stonebraker
    et al. 2018](ch058.xhtml#ref-stonebraker2005cstore)) that enable reading specific
    features without loading entire records—essential when tables contain hundreds
    of columns but training needs only a subset. This columnar organization delivers
    five to ten times I/O reduction compared to row-based formats for typical ML workloads.
    Consider a fraud detection dataset with 100 columns where models typically use
    20 features—columnar storage reads only needed columns, achieving 80% I/O reduction
    before even considering compression. Many successful ML systems draw training
    data from warehouses because the structured environment simplifies exploratory
    analysis and iterative development. Data analysts can quickly compute aggregate
    statistics, identify correlations between features, and validate data quality
    using familiar SQL interfaces.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库填补了这一分析空白，优化了跨集成数据集的复杂查询，这些数据集已转换为标准化的模式。现代仓库如Google BigQuery、Amazon Redshift和Snowflake使用列式存储格式（[Stonebraker等人2018](ch058.xhtml#ref-stonebraker2005cstore)），允许读取特定特征而不需要加载整个记录——当表中包含数百个列但训练只需要子集时这一点至关重要。这种列式组织与基于行的格式相比，在典型的机器学习负载中可以减少五到十倍的I/O。考虑一个有100个列的欺诈检测数据集，其中模型通常使用20个特征——列式存储只读取所需的列，在考虑压缩之前就能实现80%的I/O减少。许多成功的机器学习系统从仓库中抽取训练数据，因为结构化环境简化了探索性分析和迭代开发。数据分析师可以快速计算汇总统计，识别特征之间的相关性，并使用熟悉的SQL接口验证数据质量。
- en: 'However, warehouses assume relatively stable schemas and struggle with truly
    unstructured data—images, audio, free-form text—or rapidly evolving formats common
    in experimental ML pipelines. When a computer vision team wants to store raw images
    alongside extracted features, multiple annotation formats from different labeling
    vendors, intermediate model predictions, and embedding vectors, forcing all these
    into rigid warehouse schemas creates more friction than value. Schema evolution
    becomes painful: adding new feature types requires ALTER TABLE operations that
    may take hours on large datasets, blocking other operations and slowing iteration
    velocity.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仓库假设相对稳定的模式，并且难以处理真正非结构化数据——图像、音频、自由文本，或者在实验性机器学习管道中常见的快速演变格式。当计算机视觉团队想要存储原始图像以及提取的特征、来自不同标注供应商的多个标注格式、中间模型预测和嵌入向量时，将这些全部强制放入严格的仓库模式中，产生的摩擦大于价值。模式演变变得痛苦：添加新的特征类型需要ALTER
    TABLE操作，在大型数据集上可能需要数小时，这会阻塞其他操作并减缓迭代速度。
- en: 'Data lakes address these limitations by storing structured, semi-structured,
    and unstructured data in native formats, deferring schema definitions until the
    point of reading—a pattern called schema-on-read. This flexibility proves valuable
    during early ML development when teams experiment with diverse data sources and
    aren’t certain which features will prove useful. A recommendation system might
    store in the same data lake: transaction logs as JSON, product images as JPEGs,
    user reviews as text files, clickstream data as Parquet, and model embeddings
    as NumPy arrays. Rather than forcing these heterogeneous types into a common schema
    upfront, the data lake preserves them in their native formats. Applications impose
    schema only when reading, enabling different consumers to interpret the same data
    differently—one team extracts purchase amounts from transaction logs while another
    analyzes temporal patterns, each applying schemas suited to their analysis.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖通过以原生格式存储结构化、半结构化和非结构化数据来解决这些限制，将模式定义推迟到读取点——这种模式称为读取时模式。这种灵活性在早期机器学习开发期间非常有价值，当时团队正在尝试不同的数据源，并且不确定哪些特征将是有用的。推荐系统可能存储在相同的数据湖中：事务日志作为JSON，产品图像作为JPEG，用户评论作为文本文件，点击流数据作为Parquet，模型嵌入作为NumPy数组。而不是一开始就将这些异构类型强制到共同的模式中，数据湖保留了它们的原生格式。应用程序仅在读取时施加模式，使不同的消费者能够以不同的方式解释相同的数据——一个团队从事务日志中提取购买金额，而另一个分析时间模式，每个团队都应用适合其分析的方案。
- en: This flexibility comes with serious governance challenges. Without disciplined
    metadata management and cataloging, data lakes degrade into “data swamps”—disorganized
    repositories where finding relevant data becomes nearly impossible, undermining
    the productivity benefits that motivated their adoption. A data lake might contain
    thousands of datasets across hundreds of directories with names like “userdata_v2_final”
    and “userdata_v2_final_ACTUALLY_FINAL”, where only the original authors (who have
    since left the company) understand what distinguishes them. Successful data lake
    implementations maintain searchable metadata about data lineage, quality metrics,
    update frequencies, ownership, and access patterns—essentially providing warehouse-like
    discoverability over lake-scale data. Tools like AWS Glue Data Catalog, Apache
    Atlas, or Databricks Unity Catalog provide this metadata layer, enabling teams
    to discover and understand data before investing effort in processing it.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性伴随着严重的治理挑战。如果没有纪律性的元数据管理和编目，数据湖会退化成“数据沼泽”——一个无序的存储库，在其中找到相关数据几乎是不可能的，从而削弱了推动其采用的生产力优势。一个数据湖可能包含成千上万的数据集，分布在数百个目录中，目录名称如“userdata_v2_final”和“userdata_v2_final_ACTUALLY_FINAL”，只有原始作者（他们已经离开公司）才理解它们之间的区别。成功的数据湖实施维护有关数据血缘、质量指标、更新频率、所有权和访问模式的可搜索元数据——本质上在湖规模数据上提供类似仓库的可发现性。像AWS
    Glue数据编目、Apache Atlas或Databricks Unity编目这样的工具提供了这一元数据层，使团队能够在投入处理之前发现和理解数据。
- en: '[Table 6.3](ch012.xhtml#tbl-storage) summarizes these fundamental trade-offs
    across storage system types:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.3](ch012.xhtml#tbl-storage)总结了跨存储系统类型的这些基本权衡：'
- en: 'Table 6.3: **Storage System Characteristics**: Different storage systems suit
    distinct stages of machine learning workflows based on data structure and purpose;
    databases manage transactional data, data warehouses support analytical reporting,
    and data lakes accommodate diverse, raw data for future processing. Understanding
    these characteristics enables efficient data management and supports the scalability
    of machine learning applications.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3：**存储系统特征**：不同的存储系统根据数据结构和目的适合机器学习工作流程的不同阶段；数据库管理事务数据，数据仓库支持分析报告，数据湖容纳多样化、原始数据以供未来处理。了解这些特征能够实现有效的数据管理，并支持机器学习应用程序的可扩展性。
- en: '| **Attribute** | **Conventional Database** | **Data Warehouse** | **Data Lake**
    |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **传统数据库** | **数据仓库** | **数据湖** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Purpose** | Operational and transactional | Analytical and reporting |
    Storage for raw and diverse data for future processing |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **目的** | 操作和事务 | 分析和报告 | 用于未来处理的原始和多样化数据的存储 |'
- en: '| **Data type** | Structured | Structured | Structured, semi-structured, and
    unstructured |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** | 结构化 | 结构化 | 结构化、半结构化和非结构化 |'
- en: '| **Scale** | Small to medium volumes | Medium to large volumes | Large volumes
    of diverse data |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| **规模** | 小到中等规模 | 中等到大规模 | 大量多样化数据 |'
- en: '| **Performance Optimization** | Optimized for transactional queries (OLTP)
    | Optimized for analytical queries (OLAP) | Optimized for scalable storage and
    retrieval |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| **性能优化** | 优化用于事务查询（OLTP） | 优化用于分析查询（OLAP） | 优化用于可扩展的存储和检索 |'
- en: '| **Examples** | MySQL, PostgreSQL, Oracle DB | Google BigQuery, Amazon Redshift,
    Microsoft Azure Synapse | Google Cloud Storage, AWS S3, Azure Data Lake Storage
    |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| **示例** | MySQL, PostgreSQL, Oracle DB | Google BigQuery, Amazon Redshift,
    Microsoft Azure Synapse | Google Cloud Storage, AWS S3, Azure Data Lake Storage
    |'
- en: 'Choosing appropriate storage requires systematic evaluation of workload requirements
    rather than following technology trends. Databases are optimal when data volume
    remains under one terabyte, query patterns involve frequent updates and complex
    joins, latency requirements demand subsecond response, and strong consistency
    is mandatory. A user profile store serving real-time recommendations exemplifies
    this pattern: small per-user records measured in kilobytes, frequent reads and
    writes as preferences update, strict consistency ensuring users see their own
    updates immediately, and latency requirements under 10 milliseconds. Databases
    become inadequate when analytical queries must span large datasets requiring table
    scans, schema evolution occurs frequently as feature requirements change, or storage
    costs exceed $500 per terabyte per month—the point where cheaper alternatives
    become economically compelling.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的存储需要系统地评估工作负载需求，而不是跟随技术趋势。当数据量保持在十太字节以下时，数据库是最优选择，查询模式涉及频繁的更新和复杂的连接，延迟需求要求亚秒级响应，并且强一致性是强制性的。一个为实时推荐服务的用户配置文件存储就是这种模式的例子：每个用户的记录量以千字节计算，随着偏好的更新而频繁读写，严格的致性确保用户能立即看到自己的更新，并且延迟需求低于10毫秒。当分析查询必须跨越需要表扫描的大型数据集，特征需求变化导致模式演变频繁，或者存储成本超过每月每太字节500美元时，数据库变得不适用——此时更便宜的替代方案在经济上变得有吸引力。
- en: Data warehouses excel when data volumes span one to 100 terabytes, analytical
    query patterns dominate transactional operations, batch processing latency measured
    in minutes to hours is acceptable, and structured data with relatively stable
    schemas represents the primary workload. Model training data preparation, batch
    feature engineering, and historical analysis fit this profile. The migration path
    from databases to warehouses typically occurs when query complexity increases—requiring
    aggregations or joins across tables totaling gigabytes rather than megabytes—or
    when analytical workloads start degrading transactional system performance. Warehouses
    become inadequate when real-time streaming ingestion is required with latency
    measured in seconds, or when unstructured data comprises more than 20% of workloads,
    as warehouse schema rigidity creates excessive friction for heterogeneous data.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库在数据量在一到一百太字节之间时表现卓越，分析查询模式主导事务操作，以分钟到小时计的批量处理延迟是可以接受的，且具有相对稳定模式的结构化数据是主要的工作负载。模型训练数据准备、批量特征工程和历史分析都符合这一特征。数据库向仓库的迁移通常发生在查询复杂性增加时——需要跨表进行聚合或连接，总表大小达到千兆字节而不是兆字节，或者当分析工作负载开始降低事务系统性能时。当需要以秒为延迟的实时流式摄取，或者非结构化数据占工作负载的20%以上时，仓库变得不适用，因为仓库模式刚性为异构数据创造了过多的摩擦。
- en: 'Data lakes become essential when data volumes exceed 100 terabytes, schema
    flexibility is critical for evolving data sources or experimental features, cost
    optimization is paramount (often 10 times cheaper than warehouses at scale), and
    diverse data types must coexist. Large-scale model training, particularly for
    multimodal systems combining text, images, audio, and structured features, requires
    data lake flexibility. Consider a self-driving car system storing: terabytes of
    camera images and lidar point clouds from test vehicles, vehicle telemetry as
    time-series data, manually-labeled annotations identifying objects and behaviors,
    automatically-generated synthetic data for rare scenarios, and model predictions
    for comparison against ground truth. Forcing these diverse types into warehouse
    schemas would require substantial transformation effort and discard nuances that
    native formats preserve. However, data lakes demand sophisticated catalog management
    and metadata governance to prevent quality degradation—the critical distinction
    between a productive data lake and an unusable data swamp.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量超过100太字节时，数据湖变得至关重要，因为数据模式的灵活性对于不断变化的数据源或实验性特征至关重要，成本优化至关重要（通常比仓库规模便宜10倍），并且必须共存多种数据类型。大规模模型训练，尤其是结合文本、图像、音频和结构化特征的复模系统，需要数据湖的灵活性。考虑一个自动驾驶汽车系统存储：来自测试车辆的数太字节相机图像和激光雷达点云，车辆遥测作为时间序列数据，手动标注的对象和行为识别，用于罕见场景的自动生成合成数据，以及与地面真相进行比较的模型预测。将这些多样化的类型强制纳入仓库模式将需要大量的转换工作，并丢弃原生格式保留的细微差别。然而，数据湖需要复杂的目录管理和元数据治理，以防止质量下降——这是生产性数据湖和不可用的数据沼泽之间的关键区别。
- en: 'Migration patterns between storage types follow predictable trajectories as
    ML systems mature and scale. Early-stage projects often start with databases,
    drawn by familiar SQL interfaces and existing organizational infrastructure. As
    datasets grow beyond database efficiency thresholds or analytical queries start
    affecting operational performance, teams migrate to warehouses. The warehouse
    serves well during stable production phases with established feature pipelines
    and relatively fixed schemas. When teams need to incorporate new data types—images
    for computer vision augmentation, unstructured text for natural language features,
    or audio for voice applications—or when cost optimization becomes critical at
    terabyte or petabyte scale, migration to data lakes occurs. Mature ML organizations
    typically employ all three storage types orchestrated through unified data catalogs:
    databases for operational data and real-time serving, warehouses for curated analytical
    data and feature engineering, and data lakes for raw heterogeneous data and large-scale
    training datasets.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 之间存储类型的迁移模式随着机器学习系统的成熟和扩展而遵循可预测的轨迹。早期项目通常从数据库开始，受到熟悉的SQL接口和现有组织基础设施的吸引。当数据集增长超过数据库效率阈值或分析查询开始影响操作性能时，团队会迁移到仓库。仓库在稳定的生产阶段表现良好，具有既定的特征管道和相对固定的模式。当团队需要整合新的数据类型——用于计算机视觉增强的图像、用于自然语言特征的未结构化文本或用于语音应用的音频，或者当在千兆或太字节规模上成本优化变得关键时，迁移到数据湖发生。成熟的机器学习组织通常采用所有三种存储类型，通过统一的数据目录进行协调：数据库用于操作数据和实时服务，仓库用于精选的分析数据和特征工程，数据湖用于原始异构数据和大规模训练数据集。
- en: ML Storage Requirements and Performance
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习存储需求与性能
- en: Beyond the functional differences between storage systems, cost and performance
    characteristics directly impact ML system economics and iteration speed. Understanding
    these quantitative trade-offs enables informed architectural decisions based on
    workload requirements.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储系统之间的功能差异外，成本和性能特征直接影响到机器学习系统的经济性和迭代速度。理解这些定量权衡有助于基于工作负载要求做出明智的架构决策。
- en: 'Table 6.4: **Storage Cost-Performance Trade-offs**: Different storage tiers
    provide distinct cost-performance characteristics that determine their suitability
    for specific ML workloads. Training data loading requires high-throughput sequential
    access, online serving needs low-latency random reads, while archival storage
    prioritizes cost over access speed for compliance and historical data.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4：**存储成本-性能权衡**：不同的存储层提供不同的成本-性能特征，这些特征决定了它们对特定机器学习工作负载的适用性。训练数据加载需要高吞吐量的顺序访问，在线服务需要低延迟的随机读取，而归档存储在合规性和历史数据方面优先考虑成本而非访问速度。
- en: '| **Storage Tier** | **Cost ($/TB/month)** | **Sequential Read** **Throughput**
    | **Random Read** **Latency** | **Typical ML Use Case** |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| **存储层级** | **成本（美元/每TB/每月）** | **顺序读取** **吞吐量** | **随机读取** **延迟** | **典型机器学习用例**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **NVME SSD (local)** | $100-300 | 5-7 GB/s | 10-100 μs | Training data loading,
    active feature serving |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| **NVMe SSD (本地)** | $100-300 | 5-7 GB/s | 10-100 μs | 训练数据加载，活跃特征服务 |'
- en: '| **Object Storage** **(S3, GCS)** | $20-25 | 100-500 MB/s (per connection)
    | 10-50 ms | Data lake raw storage, model artifacts |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| **对象存储** **(S3, GCS)** | $20-25 | 100-500 MB/s (每连接) | 10-50 ms | 数据湖原始存储，模型工件
    |'
- en: '| **Data Warehouse** **(BigQuery, Redshift)** | $20-40 | 1-5 GB/s (columnar
    scan) | 100-500 ms (query startup) | Training data queries, feature engineering
    |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| **数据仓库** **(BigQuery, Redshift)** | $20-40 | 1-5 GB/s (列式扫描) | 100-500 ms
    (查询启动) | 训练数据查询，特征工程 |'
- en: '| **In-Memory Cache** **(Redis, Memcached)** | $500-1000 | 20-50 GB/s | 1-10
    μs | Online feature serving, real-time inference |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| **内存缓存** **(Redis, Memcached)** | $500-1000 | 20-50 GB/s | 1-10 μs | 在线特征服务，实时推理
    |'
- en: '| **Archival Storage** **(Glacier, Nearline)** | $1-4 | 10-50 MB/s (after retrieval)
    | Hours (retrieval) | Historical retention, compliance archives |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| **归档存储** **(Glacier, Nearline)** | $1-4 | 10-50 MB/s (检索后) | 小时（检索） | 历史保留，合规归档
    |'
- en: 'As [Table 6.4](ch012.xhtml#tbl-storage-performance) illustrates, these metrics
    reveal why ML systems employ tiered storage architectures. Consider the economics
    of storing our KWS training dataset (736 GB): object storage costs $15-18/month,
    enabling affordable long-term retention of raw audio, while maintaining working
    datasets on NVMe for active training costs $74-220/month but provides 50x faster
    data loading. The performance difference directly impacts iteration velocity—training
    that loads data at 5 GB/s completes dataset loading in 150 seconds, compared to
    7,360 seconds at typical object storage speeds, a 50x difference that determines
    whether teams can iterate multiple times daily or must wait hours between experiments.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表6.4](ch012.xhtml#tbl-storage-performance)所示，这些指标揭示了为什么机器学习系统采用分层存储架构。考虑存储我们的KWS训练数据集（736
    GB）的经济性：对象存储每月成本为15-18美元，使得长期保留原始音频变得经济可行，同时在工作集上使用NVMe进行活跃训练的成本为每月74-220美元，但提供了50倍更快的数据加载速度。性能差异直接影响到迭代速度——以5
    GB/s的速度加载数据的训练在150秒内完成数据集加载，而典型对象存储速度下需要7,360秒，这是50倍的性能差异，决定了团队是否可以每天多次迭代，或者必须在实验之间等待数小时。
- en: Beyond the fundamental storage capabilities we’ve examined, ML workloads introduce
    unique requirements that conventional databases and warehouses weren’t designed
    to handle. Understanding these ML-specific needs and their performance implications
    shapes infrastructure decisions that cascade through the entire development lifecycle,
    from experimental notebooks to production serving systems handling millions of
    requests per second.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们已探讨的基本存储能力之外，机器学习工作负载引入了传统数据库和仓库未设计来处理的独特需求。理解这些特定于机器学习的需求和它们的性能影响，塑造了贯穿整个开发生命周期的基础设施决策，从实验笔记本到每秒处理数百万请求的生产服务系统。
- en: 'Modern ML models contain millions to billions of parameters requiring efficient
    storage and retrieval patterns quite different from traditional data. GPT-3 ([T.
    Brown et al. 2020](ch058.xhtml#ref-brown2020language)) requires approximately
    700 gigabytes for model weights when stored as 32-bit floats—larger than many
    organization’s entire operational databases. The trajectory reveals accelerating
    scale: from AlexNet’s 60 million parameters in 2012 ([Krizhevsky, Sutskever, and
    Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet)) to GPT-3’s 175 billion
    parameters in 2020, model size grew ~2,900-fold in eight years. Storage systems
    must handle these dense numerical arrays efficiently for both capacity and access
    speed. During distributed training where multiple workers need coordinated access
    to model checkpoints, storage bandwidth becomes critical. Unlike typical files
    where sequential organization matters for readability, model weights benefit from
    block-aligned storage enabling parallel reads across parameter groups. When 64
    GPUs simultaneously read different parameter shards from shared storage during
    distributed training initialization, storage systems must deliver aggregate bandwidth
    approaching the network interface limits—often 25 gigabits per second or higher—without
    introducing synchronization bottlenecks that would idle expensive compute resources.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型包含数百万到数十亿个参数，需要高效的存储和检索模式，这与传统数据大相径庭。GPT-3([T. Brown等人 2020](ch058.xhtml#ref-brown2020language))以32位浮点数存储时，模型权重大约需要700GB——比许多组织的整个运营数据库还要大。这一轨迹揭示了规模加速的趋势：从2012年AlexNet的6000万个参数([Krizhevsky,
    Sutskever, 和 Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet))到2020年GPT-3的1750亿个参数，模型大小在八年内增长了约2900倍。存储系统必须高效地处理这些密集的数值数组，以处理容量和访问速度。在分布式训练期间，多个工作者需要协调访问模型检查点，存储带宽变得至关重要。与典型的文件不同，其中顺序组织对可读性很重要，模型权重得益于块对齐的存储，这允许跨参数组的并行读取。当64个GPU在分布式训练初始化期间同时从共享存储中读取不同的参数碎片时，存储系统必须提供接近网络接口限制的总带宽——通常是每秒25吉比特或更高——而不引入会导致昂贵的计算资源闲置的同步瓶颈。
- en: The iterative nature of ML development introduces versioning requirements qualitatively
    different from traditional software. While Git excels at tracking code changes
    where files are predominantly text with small incremental modifications, it fails
    for large binary files where even small model changes result in entirely new checkpoints.
    Storing 10 versions of a 10 gigabyte model naively would consume 100 gigabytes,
    but most ML versioning systems store only deltas between versions, reducing storage
    proportionally to how much models actually change. Tools like DVC (Data Version
    Control) and MLflow maintain pointers to model artifacts rather than storing copies,
    enabling efficient versioning while preserving the ability to reproduce any historical
    model. A typical ML project generates hundreds of model versions during hyperparameter
    tuning—one version per training run as engineers explore learning rates, batch
    sizes, architectures, and regularization strategies. Without systematic versioning
    capturing training configuration, accuracy metrics, and training data version
    alongside model weights, reproducing results becomes impossible when yesterday’s
    model performed better than today’s but teams cannot identify which configuration
    produced it. This reproducibility challenge connects directly to the governance
    requirements [Section 6.10](ch012.xhtml#sec-data-engineering-data-governance-f561)
    examines where regulatory compliance often requires demonstrating exactly which
    data and process produced specific model predictions.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习发展的迭代性质引入了与传统软件截然不同的版本要求。虽然Git在跟踪以文本为主、增量修改较小的文件代码变化方面表现出色，但在处理大型二进制文件时却无能为力，即使是微小的模型变化也会导致全新的检查点。天真地存储一个10GB模型的10个版本将消耗100GB的空间，但大多数机器学习版本控制系统只存储版本之间的差异，按模型实际变化的比例减少存储空间。像DVC（数据版本控制）和MLflow这样的工具维护模型工件指针而不是存储副本，从而在保持重现任何历史模型能力的同时实现高效的版本控制。典型的机器学习项目在超参数调整期间会生成数百个模型版本——每个训练运行一个版本，工程师们在此过程中探索学习率、批量大小、架构和正则化策略。如果没有系统地记录训练配置、准确度指标和训练数据版本，与模型权重一起，当昨天的模型表现优于今天的模型但团队无法确定是哪种配置产生了它时，重现结果变得不可能。这一重现性挑战直接关联到治理要求[第6.10节](ch012.xhtml#sec-data-engineering-data-governance-f561)，其中监管合规通常要求证明具体模型预测是由哪些数据和过程产生的。
- en: 'Distributed training generates substantial intermediate data requiring storage
    systems to handle concurrent read/write operations at scale. When training ResNet-50
    across 64 GPUs, each processing unit works on its portion of data, requiring storage
    systems to handle 64 simultaneous writes of approximately 100 megabytes of intermediate
    results every few seconds during synchronization. Memory optimization strategies
    that trade computation for storage space reduce memory requirements but increase
    storage I/O as intermediate values write to disk. Storage systems must provide
    low-latency access to support efficient synchronization—if workers spend more
    time waiting for storage than performing computations, distributed processing
    becomes counterproductive. The synchronization pattern varies by parallelization
    strategy: some approaches require gathering results from all workers, others require
    sequential communication between workers, and mixed strategies combine both patterns
    with complex data dependencies.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练会产生大量的中间数据，需要存储系统处理大规模的并发读写操作。当在64个GPU上训练ResNet-50时，每个处理单元处理其数据的一部分，需要存储系统处理每几秒同步时的大约100兆字节的中间结果的64个并发写入。以存储空间换取计算空间的内存优化策略减少了内存需求，但增加了存储I/O，因为中间值写入磁盘。存储系统必须提供低延迟访问以支持高效的同步——如果工作者花费更多的时间等待存储而不是执行计算，分布式处理就会变得事倍功半。同步模式因并行化策略而异：一些方法需要从所有工作者那里收集结果，其他方法需要工作者之间的顺序通信，而混合策略将这两种模式与复杂的数据依赖相结合。
- en: The bandwidth hierarchy fundamentally constrains ML system design, creating
    bottlenecks that no amount of compute optimization can overcome. While RAM delivers
    50 to 200 gigabytes per second bandwidth on modern servers, network storage systems
    typically provide only one to 10 gigabytes per second, and even high-end NVMe
    SSDs max out at one to seven gigabytes per second sequential throughput. Modern
    GPUs can process data faster than storage can supply it, creating scenarios where
    expensive accelerators idle waiting for data. Consider ResNet-50 training where
    the model contains 25 million parameters totaling 100 megabytes, processing batches
    of 32 images consuming five megabytes of input data, performing four billion operations
    per forward pass. This yields 26 bytes moved per operation—extraordinarily high
    compared to traditional computing workloads operating below one byte per operation.
    When a GPU could theoretically process 10 gigabytes per second worth of computation
    but storage can only supply one gigabyte per second of data, the 10-fold bandwidth
    mismatch becomes the primary bottleneck limiting training throughput. No amount
    of GPU optimization—faster matrix multiplication kernels, improved memory access
    patterns, or better parallelization—can overcome this fundamental I/O constraint.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽层次结构从根本上限制了机器学习系统设计，创造了任何计算优化都无法克服的瓶颈。虽然RAM在现代服务器上提供每秒50到200千兆字节的带宽，但网络存储系统通常只提供每秒1到10千兆字节，即使是高端NVMe
    SSDs的顺序吞吐量也最多达到每秒1到7千兆字节。现代GPU可以比存储更快地处理数据，从而产生昂贵的加速器空闲等待数据的情况。考虑ResNet-50的训练，其中模型包含2500万个参数，总计100兆字节，处理每批32张图像，消耗5兆字节的输入数据，每前向传递执行400亿次操作。这产生了每次操作26字节的移动——与传统计算工作负载相比，这非常之高，传统计算工作负载的操作每秒低于1字节。当GPU理论上每秒可以处理10千兆字节的计算，但存储只能提供每秒1千兆字节的数据时，10倍的带宽不匹配成为限制训练吞吐量的主要瓶颈。无论多少GPU优化——更快的矩阵乘法内核、改进的内存访问模式或更好的并行化——都无法克服这种基本的I/O限制。
- en: 'Understanding these quantitative relationships enables informed architectural
    decisions about storage system selection and data pipeline optimization, which
    become even more critical during distributed training as examined in [Chapter 8](ch014.xhtml#sec-ai-training).
    The training throughput equation reveals the critical dependencies:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些定量关系，可以做出关于存储系统选择和数据管道优化的明智的架构决策，这在分布式训练中尤为重要，如第8章（ch014.xhtml#sec-ai-training）所述。训练吞吐量方程揭示了关键的依赖关系：
- en: <semantics><mrow><mtext mathvariant="normal">Training Throughput</mtext><mo>=</mo><mo>min</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">Compute Capacity</mtext><mo>,</mo><mtext
    mathvariant="normal">Data Supply Rate</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{Training Throughput} = \min(\text{Compute Capacity},
    \text{Data Supply Rate})</annotation></semantics>
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">训练吞吐量</mtext><mo>=</mo><mo>min</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">计算能力</mtext><mo>,</mo><mtext
    mathvariant="normal">数据供应速率</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{训练吞吐量} = \min(\text{计算能力}, \text{数据供应速率})</annotation></semantics>
- en: <semantics><mrow><mtext mathvariant="normal">Data Supply Rate</mtext><mo>=</mo><mtext
    mathvariant="normal">Storage Bandwidth</mtext><mo>×</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo>−</mo><mtext mathvariant="normal">Overhead</mtext><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Data
    Supply Rate} = \text{Storage Bandwidth} \times (1 - \text{Overhead})</annotation></semantics>
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">数据供应速率</mtext><mo>=</mo><mtext
    mathvariant="normal">存储带宽</mtext><mo>×</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mtext
    mathvariant="normal">开销</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\text{数据供应速率} = \text{存储带宽} \times (1 - \text{开销})</annotation></semantics>
- en: 'When storage bandwidth becomes the limiting factor, teams must either improve
    storage performance through faster media, parallelization, or caching, or reduce
    data movement requirements through compression, quantization, or architectural
    changes. Large language model training may require processing hundreds of gigabytes
    of text per hour, while computer vision models processing high-resolution imagery
    can demand sustained data rates exceeding 50 gigabytes per second across distributed
    clusters. These requirements explain the rise of specialized ML storage systems
    optimizing data loading pipelines: PyTorch DataLoader with multiple worker processes
    parallelizing I/O, TensorFlow tf.data API with prefetching and caching, and frameworks
    like NVIDIA DALI (Data Loading Library) that offload data augmentation to GPUs
    rather than loading pre-augmented data from storage.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 当存储带宽成为限制因素时，团队必须通过更快的介质、并行化或缓存来提高存储性能，或者通过压缩、量化或架构变更来减少数据移动需求。大型语言模型训练可能每小时需要处理数百吉字节文本，而处理高分辨率图像的计算机视觉模型可能需要分布式集群中超过每秒50吉字节的数据速率。这些需求解释了专用机器学习存储系统的兴起，这些系统优化了数据加载管道：PyTorch
    DataLoader通过多个工作进程并行化I/O，TensorFlow tf.data API通过预取和缓存，以及像NVIDIA DALI（数据加载库）这样的框架，将数据增强卸载到GPU上，而不是从存储中加载预增强的数据。
- en: 'File format selection dramatically impacts both throughput and latency through
    effects on I/O volume and decompression overhead. Columnar storage formats like
    Parquet or ORC deliver five to 10 times I/O reduction compared to row-based formats
    like CSV or JSON for typical ML workloads. The reduction comes from two mechanisms:
    reading only required columns rather than entire records, and column-level compression
    exploiting value patterns within columns. Consider a fraud detection dataset with
    100 columns where models typically use 20 features—columnar formats read only
    needed columns, achieving 80% I/O reduction before compression. Column compression
    proves particularly effective for categorical features with limited cardinality:
    a country code column with 200 unique values in 100 million records compresses
    20 to 50 times through dictionary encoding, while run-length encoding compresses
    sorted columns by storing only value changes. The combination can achieve total
    I/O reduction of 20 to 100 times compared to uncompressed row formats, directly
    translating to faster training iterations and reduced infrastructure costs.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 文件格式选择通过影响I/O体积和解压缩开销，对吞吐量和延迟产生重大影响。与CSV或JSON等基于行的格式相比，Parquet或ORC等列式存储格式在典型的机器学习工作负载中可以减少5到10倍的I/O。这种减少来自两种机制：只读取所需的列而不是整个记录，以及利用列内值模式进行列级压缩。考虑一个有100列的欺诈检测数据集，其中模型通常使用20个特征——列式格式只读取所需的列，在压缩前就能实现80%的I/O减少。列压缩对于有限基数分类特征特别有效：一个包含1000万条记录中200个唯一值的国家代码列，通过字典编码压缩20到50倍，而运行长度编码通过只存储值变化来压缩排序的列。这种组合可以将与未压缩行格式相比的总I/O减少20到100倍，这直接转化为更快的训练迭代和降低基础设施成本。
- en: 'Compression algorithm selection involves trade-offs between compression ratio
    and decompression speed. While gzip achieves higher compression ratios of six
    to eight times, Snappy achieves only two to three times compression but decompresses
    at 500 megabytes per second—roughly three to four times faster than gzip’s 120
    megabytes per second. For ML training where throughput matters more than storage
    costs, Snappy’s speed advantage often outweighs gzip’s space savings. Training
    on a 100 gigabyte dataset compressed with gzip requires 17 minutes of decompression
    time, while Snappy requires only five minutes. When training iterates over data
    for 50 epochs, this 12-minute difference per epoch compounds to 10 hours total—potentially
    the difference between running experiments overnight versus waiting multiple days
    for results. The choice cascades through the system: faster decompression enables
    higher batch sizes (fitting more examples in memory after decompression), reduced
    buffering requirements (less decompressed data needs staging), and better GPU
    utilization (less time idle waiting for data).'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩算法选择需要在压缩比和解压速度之间进行权衡。虽然gzip实现了六到八倍的更高压缩比，但Snappy只能实现两到三倍的压缩比，但解压速度达到每秒500兆字节——大约比gzip的每秒120兆字节快三到四倍。对于机器学习训练，其中吞吐量比存储成本更重要，Snappy的速度优势通常超过了gzip的空间节省。在用gzip压缩的100吉字节数据集上训练需要17分钟的解压时间，而Snappy只需要5分钟。当训练迭代50个epoch时，每个epoch的12分钟差异累积到总共10小时——这可能是夜间运行实验与等待多天结果之间的区别。这一选择会级联到整个系统中：更快的解压速度允许更大的批量大小（解压后在内存中容纳更多示例），减少缓冲需求（需要暂存的数据量更少），以及更好的GPU利用率（减少空闲等待数据的时间）。
- en: 'Storage performance optimization extends beyond format and compression to data
    layout strategies. Data partitioning based on frequently used query parameters
    dramatically improves retrieval efficiency. A recommendation system processing
    user interactions might partition data by date and user demographic attributes,
    enabling training on recent data subsets or specific user segments without scanning
    the entire dataset. Partitioning strategies interact with distributed training
    patterns: range partitioning by user ID enables data parallel training where each
    worker processes a consistent user subset, while random partitioning ensures workers
    see diverse data distributions. The partitioning granularity matters—too few partitions
    limit parallelism, while too many partitions increase metadata overhead and reduce
    efficiency of sequential reads within partitions.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 存储性能优化不仅限于格式和压缩，还包括数据布局策略。基于常用查询参数的数据分区显著提高了检索效率。一个处理用户交互的推荐系统可能会按日期和用户人口统计属性分区数据，使得在不需要扫描整个数据集的情况下，可以在最近的数据子集或特定用户群体上进行训练。分区策略与分布式训练模式相互作用：按用户ID进行范围分区可以实现数据并行训练，其中每个工作进程处理一致的用户子集，而随机分区确保工作进程看到不同的数据分布。分区粒度很重要——分区太少会限制并行性，而分区太多会增加元数据开销并降低分区内顺序读取的效率。
- en: Storage Across the ML Lifecycle
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习生命周期中的存储
- en: Storage requirements evolve substantially as ML systems progress from initial
    development through production deployment and ongoing maintenance. Understanding
    these changing requirements enables designing infrastructure that supports the
    full lifecycle efficiently rather than retrofitting storage later when systems
    scale or requirements change. The same dataset might be accessed very differently
    during exploratory analysis (random sampling for visualization), model training
    (sequential scanning for epochs), and production serving (random access for individual
    predictions), requiring storage architectures that accommodate these diverse patterns.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统从初始开发到生产部署和持续维护，存储需求发生了显著变化。了解这些变化的需求能够设计出支持整个生命周期的高效基础设施，而不是在系统扩展或需求变化后对存储进行改造。相同的数据集在探索性分析（用于可视化的随机抽样）、模型训练（epochs的顺序扫描）和生产服务（单个预测的随机访问）期间可能会有非常不同的访问方式，需要能够适应这些不同模式的存储架构。
- en: During development, storage systems must support exploratory data analysis and
    iterative model development where flexibility and collaboration matter more than
    raw performance. Data scientists work with various datasets simultaneously, experiment
    with feature engineering approaches, and rapidly iterate on model designs to refine
    approaches. The key challenge involves managing dataset versions without overwhelming
    storage capacity. A naive approach copying entire datasets for each experiment
    would exhaust storage quickly—10 experiments on a 100 gigabyte dataset would require
    one terabyte. Tools like DVC address this by tracking dataset versions through
    pointers and storing only deltas, enabling efficient experimentation. The system
    maintains lineage from raw data through transformations to final training datasets,
    supporting reproducibility when successful experiments need recreation months
    later.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，存储系统必须支持探索性数据分析与迭代模型开发，其中灵活性和协作比原始性能更重要。数据科学家同时处理各种数据集，实验特征工程方法，并快速迭代模型设计以完善方法。关键挑战在于管理数据集版本，而不会耗尽存储容量。一种天真地将整个数据集复制到每个实验中的方法会迅速耗尽存储空间——在100GB数据集上进行10次实验就需要1TB。像DVC这样的工具通过指针跟踪数据集版本，并仅存储差异，从而实现高效的实验。系统从原始数据通过转换到最终训练数据集维护了血缘关系，支持在成功实验需要几个月后重新创建时实现可重复性。
- en: Collaboration during development requires balancing data accessibility with
    security. Data scientists need efficient access to datasets for experimentation,
    but organizations must simultaneously safeguard sensitive information. Many teams
    implement tiered access controls where synthetic or anonymized datasets are broadly
    available for experimentation, while access to production data containing sensitive
    information requires approval and audit trails. This balances exploration velocity
    against governance requirements, enabling rapid iteration on representative data
    without exposing sensitive information unnecessarily.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 开发过程中的协作需要平衡数据可访问性与安全性。数据科学家需要高效地访问数据集进行实验，但组织机构必须同时保护敏感信息。许多团队实施了分级访问控制，其中合成或匿名数据集广泛可用以进行实验，而访问包含敏感信息的生产数据则需要批准和审计跟踪。这平衡了探索速度与治理要求，使得在无需不必要地暴露敏感信息的情况下，能够快速迭代代表性数据。
- en: Training phase requirements shift dramatically toward throughput optimization.
    Modern deep learning training processes massive datasets repeatedly across dozens
    or hundreds of epochs, making I/O efficiency critical for acceptable iteration
    speed. High-performance storage systems must provide throughput sufficient to
    feed data to multiple GPU or TPU accelerators simultaneously without creating
    bottlenecks. When training ResNet-50 on ImageNet’s 1.2 million images across 8
    GPUs, each GPU processes approximately 4,000 images per epoch at 256 image batch
    size. At 30 seconds per epoch, this requires loading 40,000 images per second
    across all GPUs—approximately 500 megabytes per second of decompressed image data.
    Storage systems unable to sustain this throughput cause GPUs to idle waiting for
    data, directly reducing training efficiency and increasing infrastructure costs.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 训练阶段的需求显著转向吞吐量优化。现代深度学习训练过程在数十或数百个epoch中重复处理大量数据集，使得I/O效率对于可接受的迭代速度至关重要。高性能存储系统必须提供足够的吞吐量，以同时向多个GPU或TPU加速器提供数据，而不会造成瓶颈。当在ImageNet的120万张图像上使用8个GPU训练ResNet-50时，每个GPU在每个epoch中以256个图像批次大小处理大约4,000张图像。每个epoch需要30秒，这意味着所有GPU每秒需要加载40,000张图像——大约每秒500兆字节的未压缩图像数据。无法维持这种吞吐量的存储系统会导致GPU空闲等待数据，直接降低训练效率并增加基础设施成本。
- en: 'The balance between preprocessing and on-the-fly computation becomes critical
    during training. Extensive preprocessing reduces training-time computation but
    increases storage requirements and risks staleness. Feature extraction for computer
    vision might precompute ResNet features from images, converting 150 kilobyte images
    to five kilobyte feature vectors—achieving 30-fold storage reduction and eliminating
    repeated computation. However, precomputed features become stale when feature
    extraction logic changes, requiring recomputation across the entire dataset. Production
    systems often implement hybrid approaches: precomputing expensive, stable transformations
    like feature extraction while computing rapidly-changing features on-the-fly during
    training. This balances storage costs, computation time, and freshness based on
    each feature’s specific characteristics.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，预处理和即时计算的平衡变得至关重要。广泛的预处理减少了训练时间的计算，但增加了存储需求并增加了数据过时的风险。计算机视觉的特征提取可能从图像中预计算
    ResNet 特征，将 150 千字节的图像转换为 5 千字节的特征向量——实现 30 倍的存储减少并消除重复计算。然而，当特征提取逻辑发生变化时，预计算的特征会过时，需要在整个数据集上重新计算。生产系统通常实施混合方法：预计算昂贵的、稳定的转换，如特征提取，同时在训练期间即时计算快速变化的特征。这根据每个特征的特定特性平衡了存储成本、计算时间和新鲜度。
- en: 'Deployment and serving requirements prioritize low-latency random access over
    high-throughput sequential scanning. Real-time inference demands storage solutions
    capable of retrieving model parameters and relevant features within millisecond
    timescales. For a recommendation system serving 10,000 requests per second with
    10 millisecond latency budgets, feature storage must support 100,000 random reads
    per second. In-memory databases like Redis or sophisticated caching strategies
    become essential for meeting these latency requirements. Edge deployment scenarios
    introduce additional constraints: limited storage capacity on embedded devices,
    intermittent connectivity to central data stores, and the need for model updates
    without disrupting inference. Many edge systems implement tiered storage where
    frequently-updated models cache locally while infrequently-changing reference
    data pulls from cloud storage periodically.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和服务的需求优先考虑低延迟随机访问而不是高吞吐量顺序扫描。实时推理需要能够在毫秒时间内检索模型参数和相关特征的存储解决方案。对于一个每秒处理 10,000
    个请求且具有 10 毫秒延迟预算的推荐系统，特征存储必须支持每秒 100,000 次随机读取。内存数据库如 Redis 或复杂的缓存策略对于满足这些延迟要求变得至关重要。边缘部署场景引入了额外的约束：嵌入式设备上的存储容量有限，与中央数据存储的间歇性连接，以及在不会中断推理的情况下进行模型更新的需求。许多边缘系统实施分层存储，其中频繁更新的模型本地缓存，而很少变化的参考数据定期从云存储中拉取。
- en: Model versioning becomes operationally critical during deployment. Storage systems
    must facilitate smooth transitions between model versions, ensuring minimal service
    disruption while enabling rapid rollback if new versions underperform. Shadow
    deployment patterns, where new models run alongside existing ones for validation,
    require storage systems to efficiently serve multiple model versions simultaneously.
    A/B testing frameworks require per-request model version selection, necessitating
    fast model loading without maintaining dozens of model versions in memory simultaneously.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本控制在部署过程中变得操作上至关重要。存储系统必须促进模型版本之间的平稳过渡，确保最小化服务中断，同时在新版本表现不佳时能够快速回滚。影子部署模式，即新模型与现有模型并行运行以进行验证，要求存储系统能够高效地为多个模型版本同时提供服务。A/B
    测试框架需要按请求选择模型版本，这需要快速加载模型而不需要在内存中同时维护数十个模型版本。
- en: 'Monitoring and maintenance phases introduce long-term storage considerations
    centered on debugging, compliance, and system improvement. Capturing incoming
    data alongside prediction results enables ongoing analysis detecting data drift,
    identifying model failures, and maintaining regulatory compliance. For edge and
    mobile deployments, storage constraints complicate data collection—systems must
    balance gathering sufficient data for drift detection against limited device storage
    and network bandwidth for uploading to central analysis systems. Regulated industries
    often require immutable storage supporting auditing: healthcare ML systems must
    retain not just predictions but complete data provenance showing which training
    data and model version produced each diagnostic recommendation, potentially for
    years or decades.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和维护阶段引入了长期存储考虑因素，这些考虑因素集中在调试、合规性和系统改进上。在预测结果的同时捕获传入数据，可以持续分析，检测数据漂移，识别模型故障，并保持法规合规。对于边缘和移动部署，存储限制使数据收集复杂化——系统必须在收集足够的数据以进行漂移检测与有限的设备存储和网络带宽上传到中央分析系统之间进行平衡。受监管的行业通常需要不可变存储以支持审计：医疗保健机器学习系统不仅需要保留预测，还需要保留完整的数据来源，显示哪些训练数据和模型版本产生了每个诊断建议，可能需要数年或数十年。
- en: 'Log and monitoring data volumes grow substantially in high-traffic production
    systems. A recommendation system serving 10 million users might generate terabytes
    of interaction logs daily. Storage strategies typically implement tiered retention:
    hot storage retains recent data (past week) for rapid analysis, warm storage keeps
    medium-term data (past quarter) for periodic analysis, and cold archive storage
    retains long-term data (past years) for compliance and rare deep analysis. The
    transitions between tiers involve trade-offs between access latency, storage costs,
    and retrieval complexity that systems must manage automatically as data ages.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在高流量生产系统中，日志和监控数据量显著增长。一个服务于1000万用户的推荐系统每天可能会生成数TB的交互日志。存储策略通常实施分层保留：热存储保留最近的数据（过去一周）以进行快速分析，温存储保留中期数据（过去一个季度）以进行定期分析，而冷存档存储保留长期数据（过去几年）以符合规定和进行罕见深度分析。层级之间的转换涉及访问延迟、存储成本和检索复杂性的权衡，系统必须随着数据老化自动管理。
- en: 'Feature Stores: Bridging Training and Serving'
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征存储：连接训练和部署
- en: Feature stores[27](#fn27) have emerged as critical infrastructure components
    addressing the unique challenge of maintaining consistency between training and
    serving environments while enabling feature reuse across models and teams. Traditional
    ML architectures often compute features differently offline during training versus
    online during serving, creating training-serving skew that silently degrades model
    performance.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储[27](#fn27)已成为解决在训练和部署环境中保持一致性以及跨模型和团队实现特征重用这一独特挑战的关键基础设施组件。传统的机器学习架构通常在训练期间离线计算特征，而在部署期间在线计算特征，这种训练-部署偏差会无声地降低模型性能。
- en: The fundamental problem feature stores address becomes clear when examining
    typical ML development workflows. During model development, data scientists write
    feature engineering logic in notebooks or scripts, often using different libraries
    and languages than production serving systems. Training might compute a user’s
    “total purchases last 30 days” using SQL aggregating historical data, while serving
    computes the same feature using a microservice that incrementally updates cached
    values. These implementations should produce identical results, but subtle differences—handling
    timezone conversions, dealing with missing data, or rounding numerical values—cause
    training and serving features to diverge. A study of production ML systems found
    that 30% to 40% of initial deployments at Uber suffered from training-serving
    skew, motivating development of their Michelangelo platform with integrated feature
    stores.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查典型的机器学习开发工作流程时，特征存储解决的根本问题变得清晰。在模型开发过程中，数据科学家在笔记本或脚本中编写特征工程逻辑，通常使用与生产部署系统不同的库和语言。训练可能使用SQL聚合历史数据来计算用户的“过去30天的总购买额”，而部署则使用微服务来逐步更新缓存的值。这些实现应该产生相同的结果，但细微的差异——处理时区转换、处理缺失数据或四舍五入数值——导致训练和部署特征发生偏差。一项针对生产机器学习系统的研究发现，Uber的30%到40%的初始部署都受到了训练-部署偏差的影响，这促使他们开发了集成了特征存储的Michelangelo平台。
- en: Feature stores provide a single source of truth for feature definitions, ensuring
    consistency across all stages of the ML lifecycle. When data scientists define
    a feature like “user_purchase_count_30d”, the feature store maintains both the
    definition (SQL query, transformation logic, or computation graph) and executes
    it consistently whether providing historical feature values for training or real-time
    values for serving. This architectural pattern eliminates an entire class of subtle
    bugs that prove notoriously difficult to debug because models train successfully
    but perform poorly in production without obvious errors.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储为特征定义提供了一个单一的真实来源，确保了机器学习生命周期所有阶段的连贯性。当数据科学家定义一个特征，如“user_purchase_count_30d”时，特征存储维护其定义（SQL查询、转换逻辑或计算图）并一致地执行它，无论是提供历史特征值用于训练还是提供实时值用于服务。这种架构模式消除了一个整个类别的微妙错误，这些错误证明非常难以调试，因为模型在训练中表现良好，但在生产中表现不佳，而没有明显的错误。
- en: Beyond consistency, feature stores enable feature reuse across models and teams,
    significantly reducing redundant work. When multiple teams build models requiring
    similar features—customer lifetime value for churn prediction and upsell models,
    user demographic features for recommendations and personalization, product attributes
    for search ranking and related item suggestions—the feature store prevents each
    team from reimplementing identical features with subtle variations. Centralized
    feature computation reduces both development time and infrastructure costs while
    improving consistency across models. A recommendation system might compute user
    embedding vectors representing preferences across hundreds of dimensions—expensive
    computation requiring aggregating months of interaction history. Rather than each
    model team recomputing embeddings, the feature store computes them once and serves
    them to all consumers.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一致性之外，特征存储能够在模型和团队之间实现特征重用，显著减少重复工作。当多个团队构建需要类似特征的模型时——例如，用于流失预测和交叉销售模型的客户终身价值，用于推荐和个性化的用户人口统计特征，用于搜索排名和相关商品推荐的商品属性——特征存储防止每个团队重新实现具有细微差异的相同特征。集中式特征计算减少了开发时间和基础设施成本，同时提高了模型之间的一致性。推荐系统可能会计算代表数百个维度的用户嵌入向量，这是一项昂贵的计算，需要汇总数月的交互历史。而不是每个模型团队重新计算嵌入向量，特征存储只计算一次并将它们提供给所有消费者。
- en: 'The architectural pattern typically implements dual storage modes optimized
    for different access patterns. The offline store uses columnar formats like Parquet
    on object storage, optimized for batch access during training where sequential
    scanning of millions of examples is common. The online store uses key-value systems
    like Redis, optimized for random access during serving where individual feature
    vectors must be retrieved in milliseconds. Synchronization between stores becomes
    critical: as training generates new models using current feature values, those
    models deploy to production expecting the online store to serve consistent features.
    Feature stores typically implement scheduled batch updates propagating new feature
    values from offline to online stores, with update frequencies depending on feature
    freshness requirements.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑模式通常实现针对不同访问模式优化的双重存储模式。离线存储使用类似Parquet的列式格式在对象存储上，优化了训练期间的批量访问，在此期间对数百万个示例进行顺序扫描是常见的。在线存储使用如Redis这样的键值系统，优化了服务期间的随机访问，在此期间必须以毫秒级检索单个特征向量。存储之间的同步变得至关重要：随着训练使用当前特征值生成新模型，这些模型部署到生产环境时，期望在线存储提供一致的特征。特征存储通常实现计划好的批量更新，将新的特征值从离线存储传播到在线存储，更新频率取决于特征新鲜度的要求。
- en: 'Time-travel capabilities distinguish sophisticated feature stores from simple
    caching layers. Training requires accessing feature values as they existed at
    specific points in time, not just current values. Consider training a churn prediction
    model: for users who churned on January 15th, the model should use features computed
    on January 14th, not current features reflecting their churned status. Point-in-time
    correctness ensures training data matches production conditions where predictions
    use currently-available features to forecast future outcomes. Implementing time-travel
    requires storing feature history, not just current values, substantially increasing
    storage requirements but enabling correct training on historical data.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 时间旅行功能将复杂特征存储与简单缓存层区分开来。训练需要访问特定时间点存在的特征值，而不仅仅是当前值。以训练流失预测模型为例：对于在1月15日流失的用户，模型应使用1月14日计算的特征值，而不是反映他们流失状态的当前特征值。时间点正确性确保训练数据与生产条件相匹配，在生产条件中，预测使用目前可用的特征来预测未来结果。实现时间旅行需要存储特征历史记录，而不仅仅是当前值，这大大增加了存储需求，但允许在历史数据上进行正确的训练。
- en: 'Feature store performance characteristics directly impact both training throughput
    and serving latency. For training, the offline store must support high-throughput
    batch reads, typically loading millions of feature vectors per minute when training
    begins epochs. Columnar storage formats enable efficient reads of specific features
    from wide feature tables containing hundreds of potential columns. For serving,
    the online store must support thousands to millions of reads per second with single-digit
    millisecond latency. This dual-mode optimization reflects fundamentally different
    access patterns: training performs large sequential scans while serving performs
    small random lookups, requiring different storage technologies optimized for each
    pattern.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储的性能特征直接影响训练吞吐量和服务延迟。对于训练，离线存储必须支持高吞吐量批量读取，通常在训练开始时每分钟加载数百万个特征向量。列式存储格式允许从包含数百个潜在列的宽特征表中高效地读取特定特征。对于服务，在线存储必须支持每秒数千到数百万次读取，以单数毫秒延迟。这种双模式优化反映了不同的访问模式：训练执行大顺序扫描，而服务执行小随机查找，需要针对每种模式优化不同的存储技术。
- en: Production deployments face additional challenges around feature freshness and
    cost management. Real-time features requiring immediate updates create pressure
    on online store capacity and synchronization logic. When users add items to shopping
    carts, recommendation systems want updated features reflecting current cart contents
    within seconds, not hours. Streaming feature computation pipelines process events
    in real-time, updating online stores continuously rather than through periodic
    batch jobs. However, streaming introduces complexity around exactly-once processing
    semantics, handling late-arriving events, and managing computation costs for features
    updated millions of times per second.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 生产部署在特征新鲜度和成本管理方面面临额外的挑战。需要立即更新的实时特征对在线存储容量和同步逻辑造成压力。当用户将商品添加到购物车时，推荐系统希望在几秒钟内而不是几小时内更新反映当前购物车内容的特征。流式特征计算管道实时处理事件，而不是通过定期批量作业不断更新在线存储。然而，流式处理引入了关于一次处理语义、处理迟到事件和管理每秒更新数百万次特征的计算成本的复杂性。
- en: 'Cost management for feature stores becomes significant at scale. Storing comprehensive
    feature history for time-travel capabilities multiplies storage requirements:
    retaining daily feature snapshots for one year requires 365 times the storage
    of keeping only current values. Production systems implement retention policies
    balancing point-in-time correctness against storage costs, perhaps retaining daily
    snapshots for one year, weekly snapshots for five years, and purging older history
    unless required for compliance. Online store costs grow with both feature dimensions
    and entity counts: storing 512-dimensional embedding vectors for 100 million users
    requires approximately 200 gigabytes at single-precision (32-bit floats), often
    replicated across regions for availability and low-latency access, multiplying
    costs substantially.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储的成本管理在规模扩大时变得重要。为了实现时间旅行功能而存储全面的特征历史会成倍增加存储需求：保留一年的每日特征快照需要仅保留当前值的存储量的365倍。生产系统实施保留策略，在时间点正确性和存储成本之间进行平衡，可能保留一年的每日快照，五年的每周快照，除非需要遵守规定，否则清除更早的历史记录。在线存储成本随着特征维度和实体数量的增加而增长：为1亿用户存储512维嵌入向量需要大约200千兆字节的单精度（32位浮点数），通常跨区域复制以提高可用性和低延迟访问，从而大幅增加成本。
- en: 'Feature store migration represents a significant undertaking for organizations
    with existing ML infrastructure. Legacy systems compute features ad-hoc across
    numerous repositories and pipelines, making centralization challenging. Successful
    migrations typically proceed incrementally: starting with new features in the
    feature store while gradually migrating high-value legacy features, prioritizing
    those used across multiple models or causing known training-serving skew issues.
    Maintaining abstraction layers that enable application-agnostic feature access
    prevents tight coupling to specific feature store implementations, facilitating
    future migrations when requirements evolve or better technologies emerge.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储迁移对于拥有现有机器学习基础设施的组织来说是一项重大任务。传统系统在众多存储库和管道中临时计算特征，使得集中化变得困难。成功的迁移通常逐步进行：从特征存储中的新特征开始，逐步迁移高价值的传统特征，优先考虑那些在多个模型中使用或导致已知的训练-服务偏差问题的特征。维护能够实现应用无关特征访问的抽象层，可以防止与特定特征存储实现紧密耦合，便于在需求演变或出现更优技术时进行未来的迁移。
- en: Modern feature store implementations include open-source projects like Feast
    and Tecton, commercial offerings from Databricks Feature Store and AWS SageMaker
    Feature Store, and custom-built solutions at major technology companies. Each
    makes different trade-offs between feature types supported (structured vs. unstructured),
    supported infrastructure (cloud-native vs. on-premise), and integration with ML
    frameworks. The convergence toward feature stores as essential ML infrastructure
    reflects recognition that feature engineering represents a substantial portion
    of ML development effort, and systematic infrastructure supporting features provides
    compounding benefits across an organization’s entire ML portfolio.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现代特征存储实现包括开源项目如Feast和Tecton，来自Databricks Feature Store和AWS SageMaker Feature
    Store的商业产品，以及主要技术公司定制的解决方案。每个解决方案在支持的特征类型（结构化对非结构化）、支持的基础设施（云原生对本地化）以及与机器学习框架的集成方面都有不同的权衡。向特征存储作为必要机器学习基础设施的趋同反映了这样一个认识，即特征工程代表了机器学习开发工作的大量努力，并且支持特征的系统化基础设施为整个组织的机器学习组合提供了累积效益。
- en: 'Case Study: Storage Architecture for KWS Systems'
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究：KWS系统的存储架构
- en: '[Use the KWS storage section we already created - lines from earlier]'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用我们之前已创建的KWS存储部分 - 早期内容]'
- en: Completing our comprehensive KWS case study—having traced the system from initial
    problem definition through data collection strategies, pipeline architectures,
    processing transformations, and labeling approaches—we now examine how storage
    architecture supports this entire data engineering lifecycle. The storage decisions
    made here directly reflect and enable choices made in earlier stages. Our crowdsourcing
    strategy established in [Section 6.3.3](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)
    determines raw audio volume and diversity requirements. Our processing pipeline
    designed in [Section 6.7](ch012.xhtml#sec-data-engineering-systematic-data-processing-e3d2)
    defines what intermediate features must be stored and retrieved efficiently. Our
    quality metrics from [Section 6.7.1](ch012.xhtml#sec-data-engineering-ensuring-trainingserving-consistency-f3b7)
    shape metadata storage needs for tracking data provenance and quality scores.
    Storage architecture weaves these threads together, enabling the system to function
    cohesively from development through production deployment.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成我们的全面KWS案例研究——从初始问题定义追踪到数据收集策略、管道架构、处理转换和标注方法——之后，我们现在考察存储架构如何支持整个数据工程生命周期。这里做出的存储决策直接反映了并支持了早期阶段做出的选择。我们在[第6.3.3节](ch012.xhtml#sec-data-engineering-framework-application-keyword-spotting-case-study-21ff)中确立的众包策略决定了原始音频的体积和多样性需求。我们在[第6.7节](ch012.xhtml#sec-data-engineering-systematic-data-processing-e3d2)中设计的处理管道定义了必须高效存储和检索的中间特征。我们从[第6.7.1节](ch012.xhtml#sec-data-engineering-ensuring-trainingserving-consistency-f3b7)中获得的质量指标塑造了元数据存储需求，以跟踪数据溯源和质量分数。存储架构将这些线索交织在一起，使得系统能够从开发到生产部署协同工作。
- en: 'A typical KWS storage architecture implements the tiered approach discussed
    earlier in this section, with each tier serving distinct purposes that emerged
    from our earlier engineering decisions. Raw audio files from various sources—crowd-sourced
    recordings collected through the campaigns we designed, synthetic data generated
    to fill coverage gaps, and real-world captures from deployed devices—reside in
    a data lake using cloud object storage services like S3 or Google Cloud Storage.
    This choice reflects our scalability pillar: audio files accumulate to hundreds
    of gigabytes or terabytes as we collect the millions of diverse examples needed
    for 98% accuracy across environments. The flexible schema of data lakes accommodates
    different sampling rates, audio formats, and recording conditions without forcing
    rigid structure on heterogeneous sources. Low cost per gigabyte that object storage
    provides—typically one-tenth the cost of database storage—enables retaining comprehensive
    data history for model improvement and debugging without prohibitive expense.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的关键词语音识别（KWS）存储架构实现了本节之前讨论过的分层方法，每个层级服务于从我们早期工程决策中产生的不同目的。来自各种来源的原始音频文件——通过我们设计的活动收集的众包录音、生成以填补覆盖范围空白的合成数据，以及从部署的设备中捕获的现实世界数据——存储在数据湖中，使用如S3或Google
    Cloud Storage这样的云对象存储服务。这一选择反映了我们的可扩展性支柱：随着我们收集数百万个不同示例以实现98%的环境准确性，音频文件积累到数百或数千吉字节。数据湖灵活的架构适应了不同的采样率、音频格式和录音条件，而不对异构源施加刚性结构。对象存储提供的每吉字节低成本——通常是数据库存储成本的十分之一——使得在不产生禁止性费用的情况下，能够保留全面的数据历史以用于模型改进和调试。
- en: The data lake stores comprehensive provenance metadata required by our governance
    pillar, metadata that proved essential during earlier pipeline stages. For each
    audio file, the system maintains source type (crowdsourced, synthetic, or real-world),
    collection date, demographic information when ethically collected and consented
    to, quality assessment scores computed by our validation pipeline, and processing
    history showing which transformations have been applied. This metadata enables
    filtering during training data selection and supports compliance requirements
    for privacy regulations and ethical AI practices [Section 6.10](ch012.xhtml#sec-data-engineering-data-governance-f561)
    examines.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖存储了我们治理支柱所需的综合溯源元数据，这些元数据在早期管道阶段已被证明至关重要。对于每个音频文件，系统维护着源类型（众包、合成或现实世界）、收集日期、在道德上收集并同意的时的人口统计信息、由我们的验证管道计算的质量评估分数，以及显示已应用哪些转换的处理历史。这些元数据使得在训练数据选择期间可以进行过滤，并支持隐私法规和道德人工智能实践的合规性要求[第6.10节](ch012.xhtml#sec-data-engineering-data-governance-f561)进行了考察。
- en: 'Processed features—spectrograms, MFCCs, and other ML-ready representations
    computed by our processing pipeline—move into a structured data warehouse optimized
    for training access. This addresses different performance requirements from raw
    storage: while raw audio is accessed infrequently (primarily during processing
    pipeline execution when we transform new data), processed features are read repeatedly
    during training epochs as models iterate over the dataset dozens of times. The
    warehouse uses columnar formats like Parquet, enabling efficient loading of specific
    features during training. For a dataset of 23 million examples like MSWC, columnar
    storage reduces training I/O by five to 10 times compared to row-based formats,
    directly impacting iteration speed during model development—the difference between
    training taking hours versus days.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的特征——包括由我们的处理流程计算出的频谱图、MFCCs和其他机器学习准备好的表示形式——进入一个针对训练访问优化的结构化数据仓库。这解决了从原始存储中不同的性能需求：虽然原始音频访问频率较低（主要在处理流程执行期间，当我们转换新数据时），但在训练周期中，处理后的特征会被反复读取，因为模型在数据集上迭代数十次。仓库使用如Parquet这样的列式格式，使得在训练期间高效加载特定特征成为可能。对于像MSWC这样的包含2300万个示例的数据集，与基于行的格式相比，列式存储可以将训练I/O减少五到十倍，直接影响到模型开发中的迭代速度——即训练时间从数小时到数天的差异。
- en: 'KWS systems benefit significantly from feature stores implementing the architecture
    patterns we’ve examined. Commonly used audio representations can be computed once
    and stored for reuse across different experiments or model versions, avoiding
    redundant computation. The feature store implements a dual architecture: an offline
    store using Parquet on object storage for training data, providing high throughput
    for sequential reads when training loads millions of examples, and an online store
    using Redis for low-latency inference, supporting our 200 millisecond latency
    requirement established during problem definition. This dual architecture addresses
    the fundamental tension between training’s batch access patterns—reading millions
    of examples sequentially—and serving’s random access patterns—retrieving features
    for individual audio snippets in real-time as users speak wake words.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: KWS系统从我们考察的架构模式实现的特征存储中受益显著。常用的音频表示可以一次性计算并存储，以便在不同实验或模型版本之间重复使用，从而避免冗余计算。特征存储实现了一种双架构：一个离线存储，使用对象存储上的Parquet来存储训练数据，提供高吞吐量以支持在训练负载数百万个示例时的顺序读取；以及一个在线存储，使用Redis来实现低延迟推理，支持我们在问题定义期间建立的200毫秒延迟要求。这种双架构解决了训练的批量访问模式——顺序读取数百万个示例——与服务的随机访问模式——在用户说出唤醒词时实时检索单个音频片段的特征——之间的基本紧张关系。
- en: 'In production, edge storage requirements become critical as our system deploys
    to resource-constrained devices. Models must be compact enough for devices with
    our 16 kilobyte memory constraint from the problem definition while maintaining
    quick parameter access for real-time wake word detection. Edge devices typically
    store quantized models using specialized formats like TensorFlow Lite’s FlatBuffers,
    which enable memory-mapped access without deserialization overhead that would
    violate latency requirements. Caching applies at multiple levels: frequently accessed
    model layers reside in SRAM for fastest access, the full model sits in flash storage
    for persistence across power cycles, and cloud-based model updates are fetched
    periodically to maintain current wake word detection patterns. This multi-tier
    caching ensures devices operate effectively even with intermittent network connectivity—a
    reliability requirement for consumer devices deployed in varied network environments
    from rural areas with limited connectivity to urban settings with congested networks.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，由于我们的系统部署到资源受限的设备上，边缘存储需求变得至关重要。模型必须足够紧凑，以满足从问题定义中我们16千字节内存限制的设备，同时保持快速参数访问以实现实时唤醒词检测。边缘设备通常使用如TensorFlow
    Lite的FlatBuffers这样的专用格式来存储量化模型，这允许通过内存映射访问，避免了违反延迟要求的反序列化开销。缓存应用于多个层级：频繁访问的模型层存储在SRAM中以实现最快的访问，完整模型存储在闪存中以实现跨电源周期的持久性，并且基于云的模型更新会定期获取，以保持当前的唤醒词检测模式。这种多级缓存确保设备即使在间歇性网络连接的情况下也能有效运行——这对于在网络环境多样的消费设备中部署的设备来说是一个可靠性要求，从网络连接有限的农村地区到网络拥堵的城市环境。
- en: Data Governance
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据治理
- en: 'The storage architectures we’ve examined—data lakes, warehouses, feature stores—are
    not merely technical infrastructure but governance enforcement mechanisms that
    determine who accesses data, how usage is tracked, and whether systems comply
    with regulatory requirements. Every architectural decision we’ve made throughout
    this chapter, from acquisition strategies through processing pipelines to storage
    design, carries governance implications that manifest most clearly when systems
    face regulatory audits, privacy violations, or ethical challenges. Data governance
    transforms from abstract policy into concrete engineering: access control systems
    that enforce who can read training data, audit infrastructure that tracks every
    data access for compliance, privacy-preserving techniques that protect individuals
    while enabling model training, and lineage systems that document how raw audio
    recordings become production models.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所考察的存储架构——数据湖、仓库、特征存储——不仅仅是技术基础设施，也是治理执行机制，它决定了谁可以访问数据，如何跟踪使用情况，以及系统是否遵守监管要求。我们在这章中做出的每一个架构决策，从获取策略到处理管道再到存储设计，都承载着治理含义，这些含义在系统面临监管审计、隐私侵犯或道德挑战时最为明显地表现出来。数据治理从抽象政策转变为具体工程：访问控制系统强制执行谁可以读取训练数据，审计基础设施跟踪每次数据访问以符合规定，隐私保护技术保护个人同时允许模型训练，以及记录原始音频录音如何成为生产模型的溯源系统。
- en: 'Our KWS system exemplifies governance challenges that arise when sophisticated
    storage meets sensitive data. The always-listening architecture that enables convenient
    voice activation creates profound privacy concerns: devices continuously process
    audio in users’ homes, feature stores maintain voice pattern histories across
    millions of users, and edge storage caches acoustic models derived from population-wide
    training data. These technical capabilities that enable our quality, reliability,
    and scalability requirements simultaneously create governance obligations around
    consent management, data minimization, access auditing, and deletion rights that
    require equally sophisticated engineering solutions. As shown in [Figure 6.16](ch012.xhtml#fig-data-governance-pillars),
    effective governance addresses these interconnected challenges through systematic
    implementation of privacy protection, security controls, compliance mechanisms,
    and accountability infrastructure throughout the ML lifecycle.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的KWS系统展示了当复杂的存储遇到敏感数据时出现的治理挑战。始终处于监听状态的架构使得便捷的语音激活成为可能，这引发了深刻的隐私担忧：设备在用户家中持续处理音频，特征存储维护着数百万用户的语音模式历史，边缘存储缓存了从全民训练数据中提取的声学模型。这些使我们能够满足质量、可靠性和可扩展性要求的技术能力，同时也在同意管理、数据最小化、访问审计和删除权等方面产生了治理义务，这些都需要同样复杂的工程解决方案。如图[图6.16](ch012.xhtml#fig-data-governance-pillars)所示，有效的治理通过在整个机器学习生命周期中系统地实施隐私保护、安全控制、合规机制和问责基础设施来解决这些相互关联的挑战。
- en: '![](../media/file87.svg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file87.svg)'
- en: 'Figure 6.16: **Data Governance Pillars**: Robust data governance establishes
    ethical and reliable machine learning systems by prioritizing privacy, fairness,
    transparency, and accountability throughout the data lifecycle. These interconnected
    pillars address unique challenges in ML workflows, ensuring responsible data usage
    and auditable decision-making processes.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：**数据治理支柱**：通过在整个数据生命周期中优先考虑隐私、公平性、透明度和问责制，稳健的数据治理建立了道德和可靠的机器学习系统。这些相互关联的支柱解决了机器学习工作流程中的独特挑战，确保了负责任的数据使用和可审计的决策过程。
- en: Security and Access Control Architecture
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全和访问控制架构
- en: 'Production ML systems implement layered security architectures where governance
    requirements translate into enforceable technical controls at each pipeline stage.
    Modern feature stores exemplify this integration by implementing role-based access
    control (RBAC) that maps organizational policies—data scientists can read training
    features, serving systems can read online features, but neither can modify raw
    source data—into database permissions that prevent unauthorized access. These
    access control systems operate across the storage tiers we examined: object storage
    like S3 enforces bucket policies that determine which services can read training
    data, data warehouses implement column-level security that hides sensitive fields
    like user identifiers from most queries, and feature stores maintain separate
    read/write paths with different permission requirements.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级机器学习系统实施分层安全架构，其中治理要求在各个管道阶段转化为可执行的技术控制。现代特征存储通过实施基于角色的访问控制（RBAC）来体现这种集成，该控制将组织政策映射到数据库权限，以防止未经授权的访问——数据科学家可以读取训练特征，服务系统可以读取在线特征，但两者都不能修改原始源数据。这些访问控制系统跨越了我们考察的存储层：对象存储如S3强制执行存储桶策略，以确定哪些服务可以读取训练数据；数据仓库实施列级安全，隐藏敏感字段如用户标识，以防止大多数查询；特征存储维护不同的读写路径，具有不同的权限要求。
- en: 'Our KWS system requires particularly sophisticated access controls because
    voice data flows across organizational and device boundaries. Edge devices store
    quantized models and cached audio features locally, requiring encryption to prevent
    extraction if devices are compromised—a voice assistant’s model parameters, though
    individually non-sensitive, could enable competitive reverse-engineering or reveal
    training data characteristics. The feature store maintains separate security zones:
    a production zone where serving systems retrieve real-time features using service
    credentials with read-only access, a training zone where data scientists access
    historical features using individual credentials tracked for audit purposes, and
    an operations zone where SRE teams can access pipeline health metrics without
    viewing actual voice data. This architectural separation, implemented through
    Kubernetes namespaces with separate IAM roles in cloud deployments, ensures that
    compromising one component—say, a serving system vulnerability—doesn’t expose
    training data or grant write access to production features.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的KWS系统需要特别复杂的访问控制，因为语音数据跨越了组织和设备边界。边缘设备在本地存储量化模型和缓存的音频特征，需要加密以防止在设备被破坏时提取——尽管语音助手的模型参数单独来看并不敏感，但可能允许竞争性的逆向工程或揭示训练数据特征。特征存储维护单独的安全区域：一个生产区域，其中服务系统使用具有只读访问权限的服务凭据检索实时特征；一个训练区域，其中数据科学家使用用于审计目的的个体凭据访问历史特征；一个操作区域，其中SRE团队可以访问管道健康指标，而无需查看实际的语音数据。这种架构分离通过在云部署中实施Kubernetes命名空间和具有单独IAM角色的IAM角色来实现，确保破坏一个组件——比如，服务系统的漏洞——不会暴露训练数据或授予对生产特征的写入访问权限。
- en: 'Access control systems integrate with encryption throughout the data lifecycle.
    Training data stored in data lakes uses server-side encryption with keys managed
    through dedicated key management services (AWS KMS, Google Cloud KMS) that enforce
    separation: training job credentials can decrypt current training data but not
    historical versions already used, implementing data minimization by limiting access
    scope. Feature stores implement encryption both at rest—storage encrypted using
    platform-managed keys—and in transit—TLS 1.3 for all communication between pipeline
    components and feature stores. For KWS edge devices, model updates transmitted
    from cloud training systems to millions of distributed devices require end-to-end
    encryption and code signing that verifies model integrity, preventing adversarial
    model injection that could compromise device security or user privacy.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制系统在整个数据生命周期中与加密集成。存储在数据湖中的训练数据使用服务器端加密，通过专用密钥管理服务（AWS KMS、Google Cloud KMS）管理密钥，以强制执行分离：训练作业凭据可以解密当前训练数据，但不能解密已使用的旧版本，通过限制访问范围实现数据最小化。特征存储在静态存储和传输过程中都实施加密——使用平台管理的密钥加密存储，以及使用TLS
    1.3加密管道组件和特征存储之间的所有通信。对于KWS边缘设备，从云训练系统传输到数百万分布式设备的模型更新需要端到端加密和代码签名，以验证模型完整性，防止可能损害设备安全或用户隐私的对抗性模型注入。
- en: Technical Privacy Protection Methods
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术隐私保护方法
- en: 'While access controls determine who can use data, privacy-preserving techniques
    determine what information systems expose even to authorized users. Differential
    privacy, which we examine in depth in [Chapter 17](ch023.xhtml#sec-responsible-ai),
    provides formal mathematical guarantees that individual training examples don’t
    leak through model behavior. Implementing differential privacy in production requires
    careful engineering: adding calibrated noise during model development, tracking
    privacy budgets across all data uses—each query or training run consumes budget,
    enforcing system-wide limits on total privacy loss—and validating that deployed
    models satisfy privacy guarantees through testing infrastructure that attempts
    to extract training data through membership inference attacks.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 当访问控制决定谁可以使用数据时，隐私保护技术则决定了信息系统能向授权用户暴露哪些信息。我们在[第17章](ch023.xhtml#sec-responsible-ai)中深入探讨了差分隐私，它提供了形式化的数学保证，确保单个训练示例不会通过模型行为泄露。在生产环境中实施差分隐私需要仔细的工程：在模型开发过程中添加校准噪声，跟踪所有数据使用中的隐私预算——每个查询或训练运行都会消耗预算，强制执行系统范围内的总隐私损失限制，并通过测试基础设施验证部署的模型是否满足隐私保证，该测试基础设施试图通过成员推理攻击提取训练数据。
- en: 'KWS systems face particularly acute privacy challenges because the always-listening
    architecture requires processing audio continuously while minimizing data retention
    and exposure. Production systems implement privacy through architectural choices:
    on-device processing where wake word detection runs entirely locally using models
    stored in edge flash memory, with audio never transmitted unless the wake word
    is detected; federated learning approaches where devices train on local audio
    to improve wake word detection but only share aggregated model updates, never
    raw audio, back to central servers; and automatic deletion policies where detected
    wake word audio is retained only briefly for quality monitoring before being permanently
    removed from storage. These aren’t just policy statements but engineering requirements
    that manifest in storage system design—data lakes implement lifecycle policies
    that automatically delete voice samples after 30 days unless explicitly tagged
    for long-term research use with additional consent, and feature stores implement
    time-to-live (TTL) fields that cause user voice patterns to expire and be purged
    from online serving stores.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: KWS系统面临特别严重的隐私挑战，因为始终处于监听状态的架构需要在最小化数据保留和暴露的同时连续处理音频。生产系统通过架构选择实现隐私保护：在设备上处理，其中唤醒词检测完全在本地运行，使用存储在边缘闪存中的模型，除非检测到唤醒词，否则不会传输音频；联邦学习方法，其中设备在本地音频上训练以改进唤醒词检测，但只共享聚合的模型更新，而不是原始音频，返回到中央服务器；以及自动删除策略，其中检测到的唤醒词音频仅保留很短的时间用于质量监控，然后永久从存储中删除。这些不仅仅是政策声明，而是体现在存储系统设计中的工程要求——数据湖实现生命周期策略，在30天后自动删除语音样本，除非明确标记为长期研究用途并附加额外同意，特征存储实现生存时间（TTL）字段，导致用户语音模式过期并被从在线服务存储中清除。
- en: The implementation complexity extends to handling deletion requests required
    by GDPR and similar regulations. When users invoke their “right to be forgotten,”
    systems must locate and remove not just source audio recordings but also derived
    features stored in feature stores, model embeddings that might encode voice characteristics,
    and audit logs that reference the user—while preserving audit integrity for compliance.
    This requires sophisticated data lineage tracking that we examine next, enabling
    systems to identify all data artifacts derived from a user’s voice samples across
    distributed storage tiers and pipeline stages.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 实施复杂性扩展到处理GDPR和类似法规要求的删除请求。当用户行使他们的“被遗忘权”时，系统必须定位并删除不仅包括源音频记录，还包括存储在特征存储中的派生特征、可能编码语音特征的模式嵌入以及引用用户的审计日志——同时保持审计完整性以符合法规。这需要复杂的数据血缘跟踪，我们将在下一部分探讨，使系统能够识别用户语音样本在分布式存储层和管道阶段中派生的所有数据工件。
- en: Architecting for Regulatory Compliance
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遵守法规的架构设计
- en: Compliance requirements transform from legal obligations into system architecture
    constraints that shape pipeline design, storage choices, and operational procedures.
    GDPR’s data minimization principle requires limiting collection and retention
    to what’s necessary for stated purposes—for KWS systems, this means justifying
    why voice samples need retention beyond training, documenting retention periods
    in system design documents, and implementing automated deletion once periods expire.
    The “right to access” requires systems to retrieve all data associated with a
    user—in practice, querying distributed storage systems (data lakes, warehouses,
    feature stores) and consolidating results, a capability that necessitates consistent
    user identifiers across all storage tiers and indexes that enable efficient user-level
    queries rather than full table scans.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 合规要求从法律义务转变为系统架构约束，这些约束塑造了管道设计、存储选择和操作程序。GDPR的数据最小化原则要求将收集和保留限制在实现声明的目的所必需的范围内——对于KWS系统，这意味着需要证明为什么语音样本需要保留在训练之外，记录保留期限在系统设计文档中，并在期限到期后实施自动删除。行使“访问权”要求系统检索与用户相关的所有数据——在实践中，查询分布式存储系统（数据湖、仓库、特征存储）并合并结果，这需要所有存储层和索引中的一致用户标识符，以及能够实现高效用户级查询而不是全表扫描的索引。
- en: Voice assistants operating globally face particularly complex compliance landscapes
    because regulatory requirements vary by jurisdiction and apply differently based
    on user age, data sensitivity, and processing location. California’s CCPA grants
    deletion rights similar to GDPR but with different timelines and exceptions. Children’s
    voice data triggers COPPA requirements in the United States, requiring verifiable
    parental consent before collecting data from users under 13—a technical challenge
    when voice characteristics don’t reliably reveal age, requiring supplementary
    authentication mechanisms. European requirements for cross-border data transfer
    restrict storing EU users’ voice data on servers outside designated countries
    unless specific safeguards exist, driving architectural decisions about regional
    data lakes, feature store replication strategies, and processing localization.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在全球范围内运营的语音助手面临着特别复杂的合规环境，因为监管要求因司法管辖区而异，并根据用户的年龄、数据敏感性和处理位置不同而有所不同。加利福尼亚州的CCPA赋予了类似于GDPR的删除权，但具有不同的时间表和例外情况。儿童的语音数据在美国引发了COPPA要求，在收集13岁以下用户的数据之前需要可验证的家长同意——当语音特征不能可靠地揭示年龄时，这是一个技术挑战，需要补充的认证机制。欧洲对跨境数据传输的要求限制了将欧盟用户的语音数据存储在指定国家以外的服务器上，除非存在特定的安全措施，这推动了关于区域数据湖、特征存储复制策略和处理本地化的架构决策。
- en: 'Standardized documentation frameworks like data cards ([Pushkarna, Zaldivar,
    and Kjartansson 2022](ch058.xhtml#ref-pushkarna2022data)) ([Figure 6.17](ch012.xhtml#fig-data-card))
    translate these compliance requirements into operational artifacts. Rather than
    legal documents maintained separately from systems, data cards become executable
    specifications: training pipelines check that input datasets have valid data cards
    before processing, model registries require data card references for all training
    data, and serving systems enforce that only models trained on compliant data can
    deploy to production. For our KWS training pipeline, data cards document not just
    the MSWC dataset characteristics but also consent basis (research use, commercial
    deployment), geographic restrictions (can train global models, cannot train region-specific
    models without additional consent), and retention commitments (audio deleted after
    feature extraction, features retained for model iteration).'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的文档框架，如数据卡片([Pushkarna, Zaldivar, and Kjartansson 2022](ch058.xhtml#ref-pushkarna2022data))
    ([图6.17](ch012.xhtml#fig-data-card))，将这些合规要求转化为操作性的工件。数据卡片不再是与系统分开维护的法律文件，而是可执行规范：训练管道在处理之前会检查输入数据集是否具有有效的数据卡片，模型注册要求所有训练数据都提供数据卡片引用，并且服务系统强制执行只有基于合规数据训练的模型才能部署到生产环境中。对于我们关键词语音识别(KWS)的训练管道，数据卡片不仅记录了MSWC数据集的特征，还包括同意基础(研究用途、商业部署)、地理限制(可以训练全球模型，但如果没有额外同意则不能训练特定区域的模型)，以及保留承诺(音频在特征提取后删除，特征保留以供模型迭代)。
- en: '![](../media/file88.svg)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file88.svg)'
- en: 'Figure 6.17: **Data Governance Documentation**: Data cards standardize critical
    dataset information, enabling transparency and accountability required for regulatory
    compliance with laws like GDPR and HIPAA. By providing a structured overview of
    dataset characteristics, intended uses, and potential risks, data cards facilitate
    responsible AI practices and support data subject rights.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：**数据治理文档**：数据卡标准化关键数据集信息，使透明度和问责制得以实现，这对于符合GDPR和HIPAA等法律的要求至关重要。通过提供数据集特征、预期用途和潜在风险的系统概述，数据卡促进了负责任的AI实践并支持数据主体的权利。
- en: Building Data Lineage Infrastructure
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建数据血缘基础设施
- en: 'Data lineage transforms from compliance documentation into operational infrastructure
    that powers governance capabilities across the ML lifecycle. Modern lineage systems
    like Apache Atlas and DataHub[28](#fn28) integrate with pipeline orchestrators
    (Airflow, Kubeflow) to automatically capture relationships: when an Airflow DAG
    reads audio files from S3, transforms them into spectrograms, and writes features
    to a warehouse, the lineage system records each step, creating a graph that traces
    any feature back to its source audio file and forward to all models trained using
    it. This automated tracking proves essential for deletion requests—when a user
    invokes GDPR rights, the lineage graph identifies all derived artifacts (extracted
    features, computed embeddings, trained model versions) that must be removed or
    retrained.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 数据血缘从合规文档转变为推动整个机器学习生命周期治理能力的运营基础设施。现代血缘系统，如Apache Atlas和DataHub[28](#fn28)，与管道编排器（Airflow、Kubeflow）集成，以自动捕获关系：当Airflow
    DAG从S3读取音频文件，将其转换为频谱图，并将特征写入仓库时，血缘系统会记录每个步骤，创建一个图，可以追踪任何特征回溯到其源音频文件，并向前追踪到使用该特征训练的所有模型。这种自动跟踪对于删除请求至关重要——当用户调用GDPR权利时，血缘图会识别所有必须删除或重新训练的派生工件（提取的特征、计算的嵌入、训练的模型版本）。
- en: Production KWS systems implement lineage tracking across all stages we’ve examined
    in this chapter. Source audio ingestion creates lineage records linking each audio
    file to its acquisition method (crowdsourced platform, web scraping source, synthetic
    generation parameters), enabling verification of consent requirements. Processing
    pipeline execution extends lineage graphs as audio becomes MFCC features, spectrograms,
    and embeddings—each transformation adds nodes that record not just output artifacts
    but also code versions, hyperparameters, and execution timestamps. Training jobs
    create lineage edges from feature collections to model artifacts, recording which
    data versions trained which model versions. When a voice assistant device downloads
    a model update, lineage tracking records the deployment, enabling recall if training
    data is later discovered to have quality or compliance issues.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 生产级的关键词语音识别（KWS）系统在本章所考察的所有阶段都实施了血缘跟踪。源音频摄取创建血缘记录，将每个音频文件与其获取方法（众包平台、网络爬虫源、合成生成参数）链接起来，从而验证同意要求。处理管道执行扩展血缘图，随着音频成为MFCC特征、频谱图和嵌入，每个转换都会添加节点，不仅记录输出工件，还包括代码版本、超参数和执行时间戳。训练作业从特征集合创建血缘边到模型工件，记录哪些数据版本训练了哪些模型版本。当语音助手设备下载模型更新时，血缘跟踪记录部署，以便在训练数据后来发现质量或合规性问题的情况下进行召回。
- en: 'The operational value extends beyond compliance to debugging and reproducibility.
    When KWS accuracy degrades for a specific accent, lineage systems enable tracing
    affected predictions back through deployed models to training features, identifying
    that the training data lacked sufficient representation of that accent. When research
    teams want to reproduce an experiment from six months ago, lineage graphs capture
    exact data versions, code commits, and hyperparameters that produced those results.
    Feature stores integrate lineage natively: each feature includes metadata about
    the source data, transformation logic, and computation time, enabling queries
    like “which models depend on user location data” to guide impact analysis when
    data sources change.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 运营价值不仅限于合规，还扩展到调试和可重现性。当针对特定口音的KWS准确性下降时，血缘系统可以追踪受影响的预测回溯到部署的模型到训练特征，确定训练数据缺乏对该口音的充分代表性。当研究团队想要重现六个月前的实验时，血缘图会捕获产生那些结果的确切数据版本、代码提交和超参数。特征存储集成血缘：每个特征都包含有关源数据、转换逻辑和计算时间的元数据，使得查询如“哪些模型依赖于用户位置数据”能够指导数据源更改时的影响分析。
- en: Audit Infrastructure and Accountability
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审计基础设施和问责制
- en: 'While lineage tracks what data exists and how it transforms, audit systems
    record who accessed data and when, creating accountability trails required by
    regulations like HIPAA and SOX[29](#fn29). Production ML systems generate enormous
    audit volumes—every training data access, feature store query, and model prediction
    can generate audit events, quickly accumulating to billions of events daily for
    large-scale systems. This scale necessitates specialized infrastructure: immutable
    append-only storage (often using cloud-native services like AWS CloudTrail or
    Google Cloud Audit Logs) that prevents tampering with historical records, efficient
    indexing (typically Elasticsearch or similar systems) that enables querying specific
    user or dataset accesses without full scans, and automated analysis that detects
    anomalous patterns indicating potential security breaches or policy violations.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然谱系追踪数据的存在和转换情况，但审计系统记录了谁访问了数据以及何时访问，创建了符合HIPAA和SOX等法规要求的问责制记录。[29](#fn29)。生产级机器学习系统生成巨大的审计量——每一次训练数据访问、特征存储查询和模型预测都可能生成审计事件，对于大规模系统来说，这些事件每天会迅速累积到数十亿个。这种规模需要专门的架构：不可变追加存储（通常使用如AWS
    CloudTrail或Google Cloud Audit Logs等云原生服务）以防止篡改历史记录，高效的索引（通常是Elasticsearch或类似系统）允许在不进行全面扫描的情况下查询特定用户或数据集的访问，以及自动分析，以检测异常模式，这些模式可能表明潜在的安全漏洞或违规行为。
- en: 'KWS systems implement multi-tier audit architectures that balance granularity
    against performance and cost. Edge devices log critical events locally—wake word
    detections, model updates, privacy setting changes—with logs periodically uploaded
    to centralized storage for compliance retention. Feature stores log every query
    with request metadata: which service requested features, which user IDs were accessed,
    and what features were retrieved, enabling analysis like “who accessed this specific
    user’s voice patterns” for security investigations. Training infrastructure logs
    dataset access, recording which jobs read which data partitions and when, implementing
    the accountability needed to demonstrate that deleted user data no longer appears
    in new model versions.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词识别（KWS）系统实施多级审计架构，在粒度、性能和成本之间取得平衡。边缘设备本地记录关键事件——唤醒词检测、模型更新、隐私设置更改，并将日志定期上传到集中存储以符合保留要求。特征存储记录每个查询，包括请求元数据：哪个服务请求了特征、哪些用户ID被访问以及检索了哪些特征，这为安全调查提供了分析，如“谁访问了这位特定用户的语音模式”。训练基础设施记录数据集访问，记录哪些作业读取了哪些数据分区以及何时读取，实施必要的问责制，以证明已删除的用户数据不再出现在新的模型版本中。
- en: The integration of lineage and audit systems creates comprehensive governance
    observability. When regulators audit a voice assistant provider, the combination
    of lineage graphs showing how user audio becomes models and audit logs proving
    who accessed that audio provides the transparency needed to demonstrate compliance.
    When security teams investigate suspected data exfiltration, audit logs identify
    suspicious access patterns while lineage graphs reveal what data the compromised
    credentials could reach. When ML teams debug model quality issues, lineage traces
    problems to specific training data while audit logs confirm no unauthorized modifications
    occurred. This operational governance infrastructure, built systematically throughout
    the data engineering practices we’ve examined in this chapter, transforms abstract
    compliance requirements into enforceable technical controls that maintain trust
    as ML systems scale in complexity and impact.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 谱系和审计系统的集成创建了全面的治理可观察性。当监管机构审计语音助手提供商时，显示用户音频如何成为模型的谱系图和证明谁访问了该音频的审计日志的结合，提供了证明合规性所需的透明度。当安全团队调查疑似数据泄露时，审计日志识别可疑的访问模式，而谱系图揭示了受损害凭证可能触及的数据。当机器学习团队调试模型质量问题时，谱系追踪将问题追溯到特定的训练数据，而审计日志确认没有发生未经授权的修改。这种操作治理基础设施，系统地构建在我们本章检查的数据工程实践中，将抽象的合规性要求转化为可执行的技术控制，以保持随着机器学习系统在复杂性和影响力方面的扩展而维持信任。
- en: As ML systems become increasingly embedded in high-stakes applications (healthcare
    diagnosis, financial decisions, autonomous vehicles), the engineering rigor applied
    to governance infrastructure will determine not just regulatory compliance but
    public trust and system accountability. Emerging approaches like blockchain-inspired
    tamper-evident logs[30](#fn30) and automated policy enforcement through infrastructure-as-code
    promise to make governance controls more robust and auditable, though they introduce
    their own complexity and cost trade-offs that organizations must carefully evaluate
    against their specific requirements.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统越来越多地嵌入高风险应用（如医疗诊断、金融决策、自动驾驶汽车），应用于治理基础设施的工程严谨性不仅将决定法规遵从性，还将决定公众信任和系统问责制。受区块链启发的可篡改日志[30](#fn30)和通过基础设施即代码的自动化政策执行等新兴方法承诺使治理控制更加稳健和可审计，尽管它们也引入了自身的复杂性和成本权衡，组织必须仔细评估这些权衡是否符合其具体需求。
- en: Fallacies and Pitfalls
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: Data engineering underpins every ML system, yet it remains one of the most underestimated
    aspects of ML development. The complexity of managing data pipelines, ensuring
    quality, and maintaining governance creates numerous opportunities for costly
    mistakes that can undermine even the most sophisticated models.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程是每个机器学习系统的基石，但它仍然是机器学习开发中最被低估的方面之一。管理数据管道、确保质量和维护治理的复杂性创造了众多可能导致成本高昂的错误的机会，这些错误甚至可能破坏最复杂的模型。
- en: '**Fallacy:** *More data always leads to better model performance.*'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *更多的数据总是导致更好的模型性能。*'
- en: This widespread belief drives teams to collect massive datasets without considering
    data quality or relevance. While more data can improve performance when properly
    curated, raw quantity often introduces noise, inconsistencies, and irrelevant
    examples that degrade model performance. A smaller, high-quality dataset with
    proper labeling and representative coverage typically outperforms a larger dataset
    with quality issues. The computational costs and storage requirements of massive
    datasets also create practical constraints that limit experimentation and deployment
    options. Effective data engineering prioritizes data quality and representativeness
    over sheer volume.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这种普遍的信念驱使团队收集大量数据集，而不考虑数据质量或相关性。虽然适当整理的数据可以改善性能，但原始数量的增加往往引入噪声、不一致性和不相关示例，从而降低模型性能。一个较小、高质量的数据集，经过适当的标注和代表性覆盖，通常优于一个存在质量问题的较大数据集。大量数据集的计算成本和存储需求也创造了实际限制，限制了实验和部署选项。有效的数据工程优先考虑数据质量和代表性，而不是单纯的数量。
- en: '**Pitfall:** *Treating data labeling as a simple mechanical task that can be
    outsourced without oversight.*'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *将数据标注视为一个简单的机械任务，可以外包而无需监督。*'
- en: Organizations often view data labeling as low-skill work that can be completed
    quickly by external teams or crowdsourcing platforms. This approach ignores the
    domain expertise, consistency requirements, and quality control necessary for
    reliable labels. Poor labeling guidelines, inadequate worker training, and insufficient
    quality validation lead to noisy labels that fundamentally limit model performance.
    The cost of correcting labeling errors after they affect model training far exceeds
    the investment in proper labeling infrastructure and oversight.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 组织通常将数据标注视为低技能工作，可以由外部团队或众包平台快速完成。这种方法忽略了领域专业知识、一致性要求和必要的质量控制，这些对于可靠的标签至关重要。糟糕的标注指南、不足的工人培训和不足的质量验证导致标签噪声，这从根本上限制了模型性能。在模型训练受到影响后纠正标注错误的花费远超过对适当的标注基础设施和监管的投资。
- en: '**Fallacy:** *Data engineering is a one-time setup that can be completed before
    model development begins.*'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *数据工程是一次性设置，可以在模型开发开始之前完成。*'
- en: This misconception treats data pipelines as static infrastructure rather than
    evolving systems that require continuous maintenance and adaptation. Real-world
    data sources change over time through schema evolution, quality degradation, and
    distribution shifts. Models deployed in production encounter new data patterns
    that require pipeline updates and quality checks. Teams that view data engineering
    as completed infrastructure rather than ongoing engineering practice often experience
    system failures when their pipelines cannot adapt to changing requirements.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解将数据管道视为静态基础设施，而不是需要持续维护和适应的演变系统。现实世界中的数据源会随着时间的推移通过模式演变、质量下降和分布变化而发生变化。在生产环境中部署的模型会遇到需要管道更新和质量检查的新数据模式。那些将数据工程视为已完成的基础设施而不是持续工程实践团队的，当他们的管道无法适应变化的需求时，往往会经历系统故障。
- en: '**Fallacy:** *Training and test data splitting is sufficient to ensure model
    generalization.*'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *训练和测试数据分割足以确保模型泛化。*'
- en: While proper train/test splitting prevents overfitting to training data, it
    doesn’t guarantee real-world performance. Production data often differs significantly
    from development datasets due to temporal shifts, geographic variations, or demographic
    changes. A model achieving 95% accuracy on a carefully curated test set may fail
    catastrophically when deployed to new regions or time periods. Robust evaluation
    requires understanding data collection biases, implementing continuous monitoring,
    and maintaining representative validation sets that reflect actual deployment
    conditions.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然适当的训练/测试分割可以防止过度拟合训练数据，但它并不能保证现实世界的性能。由于时间变化、地理差异或人口变化，生产数据通常与开发数据集有显著差异。一个在精心策划的测试集上达到95%准确率的模型，在部署到新地区或时间段时可能会失败得非常严重。稳健的评估需要理解数据收集偏差，实施持续监控，并维护代表性的验证集，以反映实际的部署条件。
- en: '**Pitfall:** *Building data pipelines without considering failure modes and
    recovery mechanisms.*'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *在构建数据管道时没有考虑故障模式和恢复机制。*'
- en: Data pipelines are often designed for the happy path where everything works
    correctly, ignoring the reality that data sources fail, formats change, and quality
    degrades. Teams discover these issues only when production systems crash or silently
    produce incorrect results. A pipeline processing financial transactions that lacks
    proper error handling for malformed data could lose critical records or duplicate
    transactions. Robust data engineering requires explicit handling of failures including
    data validation, checkpointing, rollback capabilities, and alerting mechanisms
    that detect anomalies before they impact downstream systems.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道通常是为一切正常工作的“快乐路径”而设计的，忽略了数据源会失败、格式会改变、质量会下降的现实。团队只有在生产系统崩溃或无声地产生错误结果时才会发现这些问题。一个处理金融交易的管道，如果没有对格式不正确的数据进行适当错误处理，可能会丢失关键记录或重复交易。稳健的数据工程需要明确处理失败，包括数据验证、检查点、回滚能力和在影响下游系统之前检测异常的警报机制。
- en: Summary
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data engineering serves as the foundational infrastructure that transforms raw
    information into the foundation of machine learning systems, determining not just
    model performance but also system reliability, ethical compliance, and long-term
    maintainability. This chapter revealed how every stage of the data pipeline, from
    initial problem definition through acquisition, storage, and governance, requires
    careful engineering decisions that cascade through the entire ML lifecycle. The
    seemingly straightforward task of “getting data ready” actually encompasses complex
    trade-offs between data quality and acquisition cost, real-time processing and
    batch efficiency, storage flexibility and query performance, and privacy protection
    and data utility.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程作为将原始信息转化为机器学习系统基础的基础设施，不仅决定了模型性能，还决定了系统可靠性、道德合规性和长期可维护性。本章揭示了数据管道的每个阶段，从初始问题定义到获取、存储和治理，都需要仔细的工程决策，这些决策贯穿整个机器学习生命周期。看似简单的“准备数据”任务实际上包含了数据质量与获取成本、实时处理与批量效率、存储灵活性与查询性能以及隐私保护和数据效用之间的复杂权衡。
- en: The technical architecture of data systems demonstrates how engineering decisions
    compound across the pipeline to create either robust, scalable foundations or
    brittle, maintenance-heavy technical debt. Data acquisition strategies must navigate
    the reality that perfect datasets rarely exist in nature, requiring sophisticated
    approaches ranging from crowdsourcing and synthetic generation to careful curation
    and active learning. Storage architectures from traditional databases to modern
    data lakes and feature stores represent fundamental choices about how data flows
    through the system, affecting everything from training speed to serving latency.
    The emergence of streaming data processing and real-time feature stores reflects
    the growing demand for ML systems that can adapt continuously to changing environments
    while maintaining consistency and reliability.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 数据系统的技术架构展示了工程决策如何在整个管道中累积，从而创建既强大又可扩展的基础设施，或者脆弱且维护成本高的技术债务。数据获取策略必须应对现实，即完美的数据集在自然界中很少存在，需要从众包和合成生成到精心管理和主动学习等复杂方法。从传统数据库到现代数据湖和特征存储的存储架构代表了关于数据如何通过系统的基本选择，影响着从训练速度到服务延迟的各个方面。流式数据处理和实时特征存储的出现反映了对于能够持续适应不断变化的环境同时保持一致性和可靠性的机器学习系统日益增长的需求。
- en: '**Key Takeaways**'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: The four pillars—Quality, Reliability, Scalability, and Governance—form an interconnected
    framework where optimizing one pillar creates trade-offs with others, requiring
    systematic balancing rather than isolated optimization.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个支柱——质量、可靠性、可扩展性和治理——构成一个相互关联的框架，其中优化一个支柱会与其他支柱产生权衡，需要系统性地平衡而不是孤立的优化。
- en: Training-serving consistency represents the most critical data engineering challenge,
    causing approximately 70% of production ML failures when transformation logic
    differs between training and serving environments.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练-服务一致性代表了最关键的数据工程挑战，当训练环境和服务环境之间的转换逻辑不同时，会导致大约70%的生产机器学习失败。
- en: Data labeling costs frequently exceed model training costs by 1,000-3,000x,
    yet receive insufficient attention during project planning. Understanding the
    full economic model (base cost × review overhead × rework multiplier) is essential
    for realistic budgeting.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标注成本通常比模型训练成本高1000-3000倍，但在项目规划期间却得到不足的关注。理解完整的经济模型（基础成本 × 审查开销 × 重新工作乘数）对于现实预算至关重要。
- en: Effective data acquisition requires strategically combining multiple approaches—existing
    datasets for quality baselines, web scraping for scale, crowdsourcing for coverage,
    and synthetic generation for edge cases—rather than relying on any single method.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的数据获取需要战略性地结合多种方法——现有数据集用于质量基准，网络爬虫用于规模，众包用于覆盖，以及合成生成用于边缘情况——而不是依赖任何单一方法。
- en: Storage architecture decisions cascade through the entire ML lifecycle, affecting
    training iteration speed, serving latency, feature consistency, and operational
    costs. Tiered storage strategies balance performance requirements against economic
    constraints.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储架构决策贯穿整个机器学习生命周期，影响着训练迭代速度、服务延迟、特征一致性和运营成本。分层存储策略在性能需求和经济约束之间取得平衡。
- en: 'Data governance extends beyond compliance to enable technical capabilities:
    lineage tracking enables debugging and reproducibility, access controls enable
    privacy-preserving architectures, and bias monitoring enables fairness improvements
    throughout system evolution.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理超越了合规性，以实现技术能力：血缘跟踪有助于调试和可重复性，访问控制有助于保护隐私的架构，偏差监测有助于在整个系统演变过程中提高公平性。
- en: The integration of robust data governance practices throughout the pipeline
    ensures that ML systems remain trustworthy, compliant, and transparent as they
    scale in complexity and impact. Data cards, lineage tracking, and automated monitoring
    create the observability needed to detect data drift, privacy violations, and
    quality degradation before they affect model behavior. These engineering foundations
    enable the distributed training strategies in [Chapter 8](ch014.xhtml#sec-ai-training),
    model optimization techniques in [Chapter 10](ch016.xhtml#sec-model-optimizations),
    and MLOps practices in [Chapter 13](ch019.xhtml#sec-ml-operations), where reliable
    data infrastructure becomes the prerequisite for scaling ML systems effectively.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个流程中整合稳健的数据治理实践确保了随着复杂性和影响的扩大，机器学习系统始终保持可信、合规和透明。数据卡片、血缘追踪和自动化监控创建了所需的可观察性，以便在它们影响模型行为之前检测到数据漂移、隐私违规和质量下降。这些工程基础使得[第8章](ch014.xhtml#sec-ai-training)中的分布式训练策略、[第10章](ch016.xhtml#sec-model-optimizations)中的模型优化技术和[第13章](ch019.xhtml#sec-ml-operations)中的MLOps实践成为可能，可靠的数据基础设施成为有效扩展机器学习系统的先决条件。
- en: '* * *'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
