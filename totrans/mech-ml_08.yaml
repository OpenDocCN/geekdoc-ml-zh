- en: 8 Bulldozer Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mlbook.explained.ai/bulldozer-feateng.html](https://mlbook.explained.ai/bulldozer-feateng.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Terence Parr](http://parrt.cs.usfca.edu) and [Jeremy Howard](http://www.fast.ai/about/#jeremy)'
  prefs: []
  type: TYPE_NORMAL
- en: Copyright © 2018-2019 Terence Parr. All rights reserved.
  prefs: []
  type: TYPE_NORMAL
- en: '*Please don''t replicate on web or redistribute in any way.*'
  prefs: []
  type: TYPE_NORMAL
- en: This book generated from markup+markdown+python+latex source with [Bookish](https://github.com/parrt/bookish).
  prefs: []
  type: TYPE_NORMAL
- en: You can make **comments or annotate** this page by going to the annotated version
    of this page. You'll see existing annotated bits highlighted in yellow. They are
    *PUBLICLY VISIBLE*. Or, you can send comments, suggestions, or fixes directly
    to [Terence](mailto:parrt@cs.usfca.edu).
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs: []
  type: TYPE_NORMAL
- en: '[Synthesizing date-related features](#sec:8.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ProductSize is an ordinal variable](#sec:8.2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[One-hot encoding Hydraulics_Flow](#onehot-hf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[One-hot encoding Enclosure](#sec:8.4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Splitting apart fiProductClassDesc](#sec:8.5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Training with log(price)](#sec:8.6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The effect of feature engineering on model performance](#sec:8.7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summary](#sec:8.8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the last chapter, we cleaned up the bulldozer dataset and fixed missing
    values. The resulting model''s OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png)
    score was good, but we can improve on that score by transforming some existing
    columns and synthesizing others. The techniques in this chapter extend and improve
    upon the feature engineering we did in **Chapter 6** *Categorically Speaking*.
    The features to focus on are derived from the feature importance graph from the
    last chapter, and our engineering process will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Shatter the `saledate` feature into its constituent components
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert `ProductSize` to an ordered categorical variable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*One hot encode* `Hydraulics_Flow` and `Enclosure`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split `fiProductClassDesc` into its constituent components
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the logarithm of `SalePrice` target variable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we go along, we'll examine the change in the model's OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png)
    score and see what pops up in the feature importance graph. At the end, we'll
    plot the feature transformations versus model scores to visualize the improvements
    gained from our handiwork.
  prefs: []
  type: TYPE_NORMAL
- en: '1If you get an error “`read_feather() got an unexpected keyword argument ''nthreads''`,”
    then try:'
  prefs: []
  type: TYPE_NORMAL
- en: '`import feather`'
  prefs: []
  type: TYPE_NORMAL
- en: '`feather.read_dataframe("data/bulldozer-train.feather")`'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started by loading the cleaned up dataframe we computed in the last
    chapter. We'll also load the original dataset so that we apply different transformations
    to `ProductSize`, `Hydraulics_Flow`, `fiProductClassDesc`, and `Enclosure`.1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2Don't forget the [notebooks](https://mlbook.explained.ai/notebooks/) aggregating
    the code snippets from the various chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We should get a new baseline OOB score, this time using a model with 150 trees
    in the forest, rather than the 50 we used previously. As we increase the number
    of trees, the accuracy of an RF tends to improve but only up to a certain point.
    More importantly for assessing model improvements, another interesting thing happens
    as we increase the number of trees. Scores from the larger and larger models start
    to converge to the same score for the same OOB test set, run after run. As the
    Random Forest averages more and more predictions from the individual trees, the
    variance of its combined predictions goes down and, hence, so does the variance
    of the overall OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png) scores.
    Here's how to get a stable baseline using the `test()` function used in previous
    chapters:2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.90318 using 15,454,126 tree nodes with 43.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that the OOB score of 0.903 slightly higher than the score we
    got at the end of the last chapter, due to the increased number of trees in the
    forest. Now let's dive into feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias and variance**'
  prefs: []
  type: TYPE_NORMAL
- en: Statisticians use the words *bias* and *variance* when evaluating the effect
    of model complexity on model accuracy. A model that systematically predicts values
    that differ from known true values is said to be *biased*. The larger that difference,
    the higher the bias. High bias models are not complex enough to capture the relationship
    between features and the target variable--they are *underfit*. If the model gives
    widely fluctuating results, depending on the test set, the model has high variance
    and is *overfit*. The simple way to think of these terms is that bias is accuracy
    and variance is generality.
  prefs: []
  type: TYPE_NORMAL
- en: The use of “variance” in this way is suboptimal, particularly when “overfit”
    is available and more explicit, because variance is used in so many contexts.
    For example, the term variance is also applicable when discussing the effect of
    increasing the number of trees in a Random Forest. Constructing an RF is an inherently
    random process and so multiple RF models trained on the exact same training set
    will be different. That means the prediction of these models on the exact same
    test set or OOB set will also be different. It's appropriate to describe the variation
    in the models' predictions as, well, variance.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we build a model with, say, 10 trees and get an OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png)
    score. Repeat that train-and-score process many times, and you will find lots
    of variation in the scores. Now, increase the number of trees to 100 and repeat
    the trials. You will notice that the variation between model OOB scores is significantly
    lower than models trained with only 10 trees. (The [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)
    comes into play when we average the predictions of multiple trees.)
  prefs: []
  type: TYPE_NORMAL
- en: So some people use variance to mean generality when comparing multiple test
    sets but also use variance to mean reduced prediction fluctuations on the same
    test set but larger forests. We recommend that you shy away from bias and variance,
    in favor of the more explicit accuracy and generality or underfit and overfit.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Synthesizing date-related features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Date columns in datasets are often predictive of target variables, such as
    the `saledate` in the bulldozer dataset. The date of sale and the year of manufacture
    together are strongly predictive of the sale price. As a general rule, we recommend
    shattering date columns into their constituent components to include: year, month,
    day, day of week (1..7), day of year (1..365), and even things like “end of quarter”
    and “end of month.” Pandas provides convenient functions to extract all of this
    information from a single `datetime64` entity. After extracting the components,
    convert the `datetime64` to an integer with the number of seconds since 1970 (the
    usual UNIX time measurement). Here''s a basic function that illustrates how to
    synthesize date-related features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After using the function, we can use Pandas'' `filter()` to examine the newly-created
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|   | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| saledate | 1232668800000000000 | 1232668800000000000 |'
  prefs: []
  type: TYPE_TB
- en: '| saleyear | 2009 | 2009 |'
  prefs: []
  type: TYPE_TB
- en: '| salemonth | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| saleday | 23 | 23 |'
  prefs: []
  type: TYPE_TB
- en: '| saledayofweek | 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| saledayofyear | 23 | 23 |'
  prefs: []
  type: TYPE_TB
- en: 'Since we don''t know which components, if any, will be predictive it''s a good
    idea to just add whatever you can derive from the date. For example, you might
    want to add a column indicating that a day was a business holiday or even whether
    there was a big storm. Beyond the usual year/month/day and other numeric components,
    the new columns you synthesize will be application-specific. RF models won''t
    get confused by the extra columns and we can excise useless features later, after
    finishing feature engineering. Let''s check the effect of date-splitting on model
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91315 using 14,917,750 tree nodes with 43.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: We get a nice bump from our clean baseline score of 0.903 to 0.913 and the number
    of nodes is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a `saleyear` column in addition to the `YearMade`, let''s
    create an `age` feature that explicitly states the age of a bulldozer for sale.
    The age of a vehicle is obviously important and, while the model has access to
    both fields already, it''s a good idea to make life as easy as possible on the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91281 using 14,896,494 tree nodes with 43.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: The OOB score is roughly the same after we add `age`, but the number of nodes
    is a little smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the feature importance graph, none of the date-related features we
    added appear to be important, other than the converted `saledate`. `YearMade`
    is still very important, but `age` appears to be not that important.
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/0b244dc0ef692025f47870f54222537c.png)](images/bulldozer-feateng/bulldozer-feateng_eng_11.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason for this is subtle but has to do with the fact that all of the date-related
    features are highly correlated, meaning that if we dropped one of them, the other
    features would “cover” for it. It''s better to treat all of those date-related
    features as a meta-feature for feature importance graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/d572c4140938a42d6074db3751782939.png)](images/bulldozer-feateng/bulldozer-feateng_eng_12.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: While `age` might not individually pop up in the importance graph, a graph of
    age in years versus sale price confirms our belief that older vehicles sell for
    less on average. That correlation (relationship between the age feature and target
    price) implies at least some predictability for `age`.
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/83fa5acb2086a22751da878cece7f956.png)](images/bulldozer-feateng/bulldozer-feateng_eng_13.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's keep all of these features for now and move on to the next task.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 ProductSize is an ordinal variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ProductSize` feature is important according to the feature importance
    graph so it''s worth revisiting the feature to see if we can improve upon the
    default label encoding. To get corroborating evidence of its importance, we can
    also look at the relationship between product size and sale price using Pandas''
    `groupby`. By grouping the data by `ProductSize` then calling `mean()`, we get
    the average `SalePrice` (and other columns) across product sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/732d1e2f7afad8e6b1d40c2db712dd88.png)](images/bulldozer-feateng/bulldozer-feateng_eng_14.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 3When copying a column from one dataframe to another, `df_raw` to `df`, using
    the assignment operator, Pandas can silently do weird things depending on how
    the two dataframes are indexed. The safest approach is to copy over the NumPy
    version of the column by using `.values`, as we have done here.
  prefs: []
  type: TYPE_NORMAL
- en: '(Here we''re using the built-in Pandas shortcut to matplotlib''s bar chart:
    `.plot.barh()`.) There''s a clear relationship between the size of the product
    and the sale price, as we would expect. Since `Large` is bigger than `Small`,
    the `ProductSize` feature is ordered, which means we can use an *ordinal encoding*.
    That just means that we assign numbers to the category values according to their
    size. A quick web search also shows that `Mini` and `Compact` bulldozers are the
    same size, leading to the following encoding:3'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[0 2 4 3 1 5]'
  prefs: []
  type: TYPE_NORMAL
- en: 'By using an ordinal encoding rather than a label encoding, we get a small bump
    in OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png) score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91526 using 14,873,450 tree nodes with 45.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: There are two other important features, `Hydraulics_Flow` and `Enclosure`, that
    we can easily encode in a more structured way than label encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 One-hot encoding Hydraulics_Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When in doubt, we encode categorical variables using label encoding. As we saw
    in the last section, however, if we notice that the variable is ordinal, we use
    that type of encoding. When the number of category levels is small, say, 10 or
    less we *one hot encode* the variable, assuming the category is important. One-hot
    encoding yields what people call *dummy variables*, boolean variables derived
    from a categorical variable where exactly one of the dummy variables is true for
    a given record. There is a new column for every categorical level. Missing category
    values yield 0 in each dummy variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to pickup the idea behind one-hot encoding is through a trivial
    example. Imagine we have a categorical variable with three levels (three departments).
    We start out with a dataframe like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|   | Dept |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Math |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | CS |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Physics |'
  prefs: []
  type: TYPE_TB
- en: '| 3 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Pandas can give us the dummy variables for that column and concatenate them
    onto the existing dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|   | Dept | CS | Math | Physics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Math | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | CS | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Physics | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 |  | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Now, instead of a number, the “hot” position indicates the category. Notice
    how the missing value ends up with none hot. If you're wondering what “hot” refers
    to, “one hot” is an electrical engineering term referring to multiple chip outputs
    where at most one output has a nonzero voltage at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature `Hydraulics_Flow` has “High Flow” and “Standard” levels, but the vast
    majority of records are missing a value for this feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'NaN 86819 Standard 12761 High Flow 402 None or Unspecified 18 Name: Hydraulics_Flow,
    dtype: int64'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before one-hot encoding let''s normalize the columns so string “None or Unspecified”
    is the same as missing (`np.nan`) then get dummy variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we replace column `Hydraulics_Flow` with the dummy variables by concatenating
    them onto the `df` DataFrame and deleting the unused column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Checking the OOB ![](../Images/ec985123b9b52e80981e6500795e8d16.png), we see
    that it's about the same as before and the feature importance graph shows that
    the most predictive category is `Standard`. (See feature `Hydraulics_Flow_Standard`
    in the graph.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91558 using 14,872,994 tree nodes with 45.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/9bb95a712bf0a082f161ed1d7be90d45.png)](images/bulldozer-feateng/bulldozer-feateng_eng_21.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 One-hot encoding Enclosure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s follow this same one-hot procedure for `Enclosure` because it''s also
    a categorical variable with only a few levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'OROPS 40904 EROPS w AC 34035 EROPS 24999 NaN 54 EROPS AC 6 NO ROPS 2 Name:
    Enclosure, dtype: int64'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s normalize the categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also look at the relationship between this variable and the sale price:'
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/3b87629a700271ec8ec57a10832a885e.png)](images/bulldozer-feateng/bulldozer-feateng_eng_24.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: That's interesting, “EROPS AC” gets, on average, twice the price of the other
    bulldozers. A web search reveals that ROP means “Roll Over Protection Structure,”
    so EROPS is an enclosed ROPS (a cabin) and OROP is an open protective cage. AC
    means “Air Conditioning.” The model suggests that a bulldozer with an enclosed
    cab gets a higher price and one with air conditioning gets the highest price,
    on average. Unfortunately, the OOB score drops a little bit but dummy variable
    `Enclosure_EROPS AC` is important per the importance graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91356 using 14,896,328 tree nodes with 44.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: » *Generated by code to left*
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/d4ca31a8f33d2a20dc751f16aa945cbf.png)](images/bulldozer-feateng/bulldozer-feateng_eng_26.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There are other important categorical variables, such as `fiSecondaryDesc`,
    but it has 148 levels, which would create 148 new columns, which would more than
    triple the number of overall columns in the dataframe. We recommend label encoding
    such categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Splitting apart fiProductClassDesc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature `fiProductClassSpec` is string variable rather than a categorical variable.
    The values are descriptions of the product class and some components of the string
    appear to correlate with higher prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![](../Images/983a2d4e06390a99351fcbed2edeac57.png)](images/bulldozer-feateng/bulldozer-feateng_eng_27.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bulldozers with higher operating capacity values seem to fetch higher prices.
    The model is clearly getting some kind of predictive power out of this feature
    when label encoded (per the feature important graph). But, we can make the information
    more explicit by splitting the description into four pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5312c2bf456281c7f30846ad789baff0.png)'
  prefs: []
  type: TYPE_IMG
- en: The description is a categorical variable, chosen from a finite set of categories
    such as “Skip Steer Loader.” The lower and upper components are numerical features
    and the units is a category, such as “Horsepower” or “Lb Operating Capacity.”
    We can call the latter three components the “spec”. Because the spec is sometimes
    `Unidentified`, the spec components could be missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To pull apart the description string, let''s do it in two steps. First, copy
    the original non-label-encoded string from `df_raw` and split it at the hyphen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[''Track Type Tractor, Dozer'' ''Hydraulic Excavator, Track'' ''Wheel Loader''
    ''Skid Steer Loader'' ''Backhoe Loader'' ''Motorgrader'']'
  prefs: []
  type: TYPE_NORMAL
- en: 'This leaves the right-hand side of the string as the spec string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[''20.0 to 75.0 Horsepower'' ''12.0 to 14.0 Metric Tons'' ''14.0 to 16.0 Metric
    Tons'' ''33.0 to 40.0 Metric Tons'' ''225.0 to 250.0 Horsepower'']'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, split that string using a regular expression that captures the two numbers
    and units to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|   | fiProductClassSpec_lower | fiProductClassSpec_upper | fiProductClassSpec_units
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 20.0000 | 75.0000 | Horsepower |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.0000 | 14.0000 | Metric Tons |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14.0000 | 16.0000 | Metric Tons |'
  prefs: []
  type: TYPE_TB
- en: 'Because we have introduced columns with potentially missing values and new
    categorical variables, we have to prepare the dataset following our usual procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We see a small bump in model performance from the transformation of this feature
    and a feature importance graph shows that the individual components we synthesized
    are important.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91429 using 14,873,080 tree nodes with 43.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Training with log(price)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The original Kaggle competition measured model performance based upon the logarithm
    of the price, so we should also do that because we''re going to compare our model''s
    performance to the competition leaders in the next chapter. Also, as we discussed
    in **Section 5.5** *Log in, exp out*, it often helps to take the logarithm of
    the target variable when dealing with prices. (We usually care more that two prices
    are different by 20% than by a fixed $20.) Transforming the target variable is
    a simple matter of calling the `log()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: OOB R^2 0.91881 using 14,865,740 tree nodes with 44.0 median tree height
  prefs: []
  type: TYPE_NORMAL
- en: That 0.919 score is a nice bump in accuracy, all from a simple mathematical
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 The effect of feature engineering on model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![](../Images/f0105091bb231aa27844a05f8860b28e.png)](images/bulldozer-feateng/bulldozer-feateng_eng_35.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: We've done a lot of work in this chapter to improve the features presented to
    the RF model, so let's compare the effect of these changes on model performance.
    The following figure zooms in on the range of OOB scores from our baseline to
    the final log feature improvement. (Code for this figure is in the [notebook](https://mlbook.explained.ai/notebooks/bulldozer-feateng/eng.ipynb)
    for this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/bf64f46c8edc9e22d895d496e163b021.png)](images/bulldozer-feateng/bulldozer-feateng_eng_36.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: Overall the model OOB score has improved from 0.903 to 0.919, a 16.141% improvement;
    `(oob_log-oob_clean)*100/(1-oob_clean)`).
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, our feature engineering efforts have paid off. The one-hot
    encoding of `Hydraulics_Flow` and `Enclosure`, however, doesn't seem to have improved
    model performance. In fact, `Enclosure`'s one-hot encoding seems to have hurt
    performance. But, remember, we are measuring accuracy and looking at a feature
    importance graph using the training set, not a validation set. As it turns out,
    one-hot encoding `Enclosure` does seem to improve metrics where it counts, on
    validation and other test sets. (While working on the next chapter, we compared
    scores from models with and without one-hot encoded `Enclosure` columns.) For
    the moment, it's best to keep all features available to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's summarize the techniques that we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dates**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule, break apart date columns into components such as day, month,
    year, day of week, day of year, and any other elements relevant to your application,
    such as “end of quarter” or “is holiday.” Synthesizing new columns based upon
    the date looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then convert the original date column to an integer representing the number
    of seconds since 1970 using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Ordinal encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical variables whose elements have order should be ordinal encoded using
    integers that mirror the relationship between category levels. For example, if
    a column has low, medium, and high levels, an encoding such as the following would
    work where missing values become 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**One-hot encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-ordered (nominal) categorical variables with about 10 or fewer levels can
    be one-hot encoded. Here is the basic procedure to replace a column with multiple
    dummy columns that one-hot encode the column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Split strings encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we split a string based upon the hyphen character using `split()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'and then used regular expressions to extract three components from the right-hand
    string using `extract()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We created new columns in `df` from the columns extracted in this way.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of other kinds of strings (in other datasets) you might want
    to split apart, such as URLs. You can create new columns that indicate `https`
    vs `http`, the top level domain, domain, filename, file extension etc.
  prefs: []
  type: TYPE_NORMAL
