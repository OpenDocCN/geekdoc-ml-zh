- en: Probability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率
- en: 原文：[https://dafriedman97.github.io/mlbook/content/appendix/probability.html](https://dafriedman97.github.io/mlbook/content/appendix/probability.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/appendix/probability.html](https://dafriedman97.github.io/mlbook/content/appendix/probability.html)
- en: Many machine learning methods are rooted in probability theory. Probabilistic
    methods in this book include [linear regression](../c1/concept.html), [Bayesian
    regression](../c2/s1/bayesian.html), and [generative classifiers](../c4/concept.html).
    This section covers the probability theory needed to understand those methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法都根植于概率论。本书中的概率方法包括[线性回归](../c1/concept.html)、[贝叶斯回归](../c2/s1/bayesian.html)和[生成分类器](../c4/concept.html)。本节涵盖了理解这些方法所需的概率论。
- en: 1\. Random Variables and Distributions
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 随机变量和分布
- en: Random Variables
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机变量
- en: A **random variable** is a variable whose value is randomly determined. The
    set of possible values a random variable can take on is called the variable’s
    **support**. An example of a random variable is the value on a die roll. This
    variable’s support is \(\{1, 2, 3, 4, 5, 6\}\). Random variables will be represented
    with uppercase letters and values in their support with lowercase letters. For
    instance \(X = x\) implies that a random variable \(X\) happened to take on value
    \(x\). Letting \(X\) be the value of a die roll, \(X = 4\) indicates that the
    die landed on 4.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机变量**是一个值随机确定的变量。随机变量可以取的可能值的集合被称为变量的**支撑集**。一个随机变量的例子是掷骰子的值。这个变量的支撑集是 \(\{1,
    2, 3, 4, 5, 6\}\)。随机变量将用大写字母表示，其支撑集中的值用小写字母表示。例如 \(X = x\) 意味着随机变量 \(X\) 偶然取了值
    \(x\)。让 \(X\) 表示掷骰子的值，\(X = 4\) 表示骰子落在 4 上。'
- en: Density Functions
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密度函数
- en: The likelihood that a random variable takes on a given value is determined through
    its density function. For a discrete random variable (one that can take on a finite
    set of values), this density function is called the **probability mass function**
    **(PMF)**. The PMF of a random variable \(X\) gives the probability that \(X\)
    will equal some value \(x\). We write it as \(f_X(x)\) or just \(f(x)\), and it
    is defined as
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量取特定值的可能性是通过其密度函数确定的。对于离散随机变量（可以取有限集合中的值），这个密度函数被称为**概率质量函数**（PMF）。随机变量 \(X\)
    的 PMF 给出 \(X\) 等于某个值 \(x\) 的概率。我们将其写作 \(f_X(x)\) 或简写为 \(f(x)\)，其定义如下
- en: \[ f(x) = P(X = x). \]
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = P(X = x). \]
- en: For a continuous random variable (one that can take on infinitely many values),
    the density function is called the **probability density function (PDF)**. The
    PDF \(f_X(x)\) of a continuous random variable \(X\) does not give \(P(X = x)\)
    but it does determine the probability that \(X\) lands in a certain range. Specifically,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量（可以取无限多个值），密度函数被称为**概率密度函数**（PDF）。连续随机变量 \(X\) 的 PDF \(f_X(x)\) 并不给出
    \(P(X = x)\)，但它确实确定了 \(X\) 落在某个范围内的概率。具体来说，
- en: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
- en: That is, integrating \(f(x)\) over a certain range gives the probability of
    \(X\) being in that range. While \(f(x)\) does not give the probability that \(X\)
    will equal a certain value, it does indicate the relative likelihood that it will
    be *around* that value. E.g. if \(f(a) > f(b)\), we can say \(X\) is more likely
    to be in an arbitrarily small area around the value \(a\) than around the value
    \(b\).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 即，将 \(f(x)\) 在某个范围内积分给出 \(X\) 落在该范围内的概率。虽然 \(f(x)\) 并不给出 \(X\) 等于某个特定值的概率，但它确实表明它将“**大约**”在该值周围的相对可能性。例如，如果
    \(f(a) > f(b)\)，我们可以说 \(X\) 更有可能位于值 \(a\) 附近的任意小区域内，而不是值 \(b\) 附近。
- en: Distributions
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布
- en: A random variable’s **distribution** is determined by its density function.
    Variables with the same density function are said to follow the same distributions.
    Certain families of distributions are very common in probability and machine learning.
    Two examples are given below.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量的**分布**由其密度函数决定。具有相同密度函数的变量被认为遵循相同的分布。某些分布族在概率和机器学习中非常常见。以下给出两个例子。
- en: The **Bernoulli** distribution is the most simple probability distribution and
    it describes the likelihood of the outcomes of a binary event. Let \(X\) be a
    random variable that equals 1 (representing “success”) with probability \(p\)
    and 0 (representing “failure”) with probability \(1-p\). Then, \(X\) is said to
    follow the Bernoulli distribution with probability parameter \(p\), written \(X
    \sim \text{Bern}(p)\), and its PMF is given by
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**伯努利**分布是最简单的概率分布，它描述了二元事件的结局的可能性。设 \(X\) 是一个随机变量，以概率 \(p\) 等于 1（代表“成功”），以概率
    \(1-p\) 等于 0（代表“失败”）。那么，\(X\) 被说成是服从概率参数 \(p\) 的伯努利分布，记作 \(X \sim \text{Bern}(p)\)，其概率质量函数（PMF）由以下公式给出'
- en: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
- en: We can check to see that for any valid value \(x\) in the support of \(X\)—i.e.,
    1 or 0—, \(f(x)\) gives \(P(X = x)\).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查，对于 \(X\) 的支持集内的任何有效值 \(x\)（即 1 或 0），\(f(x)\) 给出 \(P(X = x)\)。
- en: The **Normal** distribution is extremely common and will be used throughout
    this book. A random variable \(X\) follows the Normal distribution with mean parameter
    \(\mu \in \R\) and variance parameter \(\sigma^2 > 0\), written \(X \sim \mathcal{N}(\mu,
    \sigma^2)\), if its PDF is defined as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**正态**分布非常常见，本书将广泛使用。如果随机变量 \(X\) 的概率密度函数定义为具有均值参数 \(\mu \in \R\) 和方差参数 \(\sigma^2
    > 0\)，则 \(X\) 服从正态分布，记作 \(X \sim \mathcal{N}(\mu, \sigma^2)\)。'
- en: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
- en: The shape of the Normal random variable’s density function gives this distribution
    the name “the bell curve”, as shown below. Values closest to \(\mu\) are most
    likely and the density is symmetric around \(\mu\).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正态随机变量的密度函数的形状使得这种分布被称为“钟形曲线”，如下所示。最接近 \(\mu\) 的值最有可能，密度在 \(\mu\) 周围是对称的。
- en: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
- en: Independence
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立性
- en: So far we’ve discussed the density of individual random variables. The picture
    can get much more complicated when we want to study the behavior of multiple random
    variables simultaneously. The assumption of independence simplifies things greatly.
    Let’s start by defining independence in the discrete case.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了单个随机变量的密度。当我们想要同时研究多个随机变量的行为时，情况会变得更加复杂。独立性假设大大简化了问题。让我们首先从离散情况定义独立性。
- en: Two discrete random variables \(X\) and \(Y\) are **independent** if and only
    if
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个离散随机变量 \(X\) 和 \(Y\) 是**独立的**当且仅当
- en: \[ P(X = x, Y =y) = P(X = x)P(Y = y), \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(X = x, Y = y) = P(X = x)P(Y = y), \]
- en: for all \(x\) and \(y\). This says that if \(X\) and \(Y\) are independent,
    the probability that \(X = x\) and \(Y = y\) simultaneously is just the product
    of the probabilities that \(X = x\) and \(Y = y\) individually.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\)。这表示如果 \(X\) 和 \(Y\) 是独立的，那么 \(X = x\) 和 \(Y = y\) 同时发生的概率就是
    \(X = x\) 和 \(Y = y\) 分别发生的概率的乘积。
- en: To generalize this definition to continuous random variables, let’s first introduce
    *joint density function*. Quite simply, the joint density of two random variables
    \(X\) and \(Y\), written \(f_{X, Y}(x, y)\) gives the probability density of \(X\)
    and \(Y\) evaluated simultaneously at \(x\) and \(y\), respectively. We can then
    say that \(X\) and \(Y\) are independent if and only if
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个定义推广到连续随机变量，让我们首先介绍**联合密度函数**。简单地说，两个随机变量 \(X\) 和 \(Y\) 的联合密度，记作 \(f_{X,
    Y}(x, y)\)，给出了 \(X\) 和 \(Y\) 分别在 \(x\) 和 \(y\) 处同时评估的概率密度。然后我们可以这样说，\(X\) 和 \(Y\)
    是独立的当且仅当
- en: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
- en: for all \(x\) and \(y\).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\)。
- en: 2\. Maximum Likelihood Estimation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 最大似然估计
- en: Maximum likelihood estimation is used to understand the parameters of a distribution
    that gave rise to observed data. In order to model a data generating process,
    we often assume it comes from some family of distributions, such as the Bernoulli
    or Normal distributions. These distributions are indexed by certain parameters
    (\(p\) for the Bernoulli and \(\mu\) and \(\sigma^2\) for the Normal)—maximum
    likelihood estimation evaluates which parameters would be most consistent with
    the data we observed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计用于理解导致观察数据的分布的参数。为了模拟数据生成过程，我们通常假设它来自某些分布族，例如伯努利分布或正态分布。这些分布由某些参数索引（伯努利分布的
    \(p\)，正态分布的 \(\mu\) 和 \(\sigma^2\)）——最大似然估计评估哪些参数与我们观察到的数据最一致。
- en: 'Specifically, maximum likelihood estimation finds the values of unknown parameters
    that maximize the probability of observing the data we did. Basic maximum likelihood
    estimation can be broken into three steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，最大似然估计找到使观察到的数据的概率最大化的未知参数值。基本最大似然估计可以分为三个步骤：
- en: Find the joint density of the observed data, also called the *likelihood*
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到观察数据的联合密度，也称为*似然*
- en: Take the log of the likelihood, giving the *log-likelihood*.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对似然取对数，得到*对数似然*。
- en: Find the value of the parameter that maximizes the log-likelihood (and therefore
    the likelihood as well) by setting its derivative equal to 0.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将导数设为0来找到使对数似然（因此也是似然）最大化的参数值。
- en: Finding the value of the parameter to maximize the log-likelihood rather than
    the likelihood makes the math easier and gives us the same solution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过找到使对数似然最大化的参数值而不是似然值，可以使数学更容易，并给出相同的解。
- en: Let’s go through an example. Suppose we are interested in calculating the average
    weight of a Chihuahua. We assume the weight of any given Chihuahua is *independently*
    distributed Normally with \(\sigma^2 = 1\) but an unknown mean \(\mu\). So, we
    gather 10 Chihuahuas and weigh them. Denote the \(j^\text{th}\) Chihuahua weight
    with \(W_j \sim \mathcal{N}(\mu, 1)\). For step 1, let’s calculate the probability
    density of our data (i.e., the 10 Chihuahua weights). Since the weights are assumed
    to be independent, the densities multiply. Letting \(L(\mu)\) be the likelihood
    of \(\mu\), we have
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来解释。假设我们感兴趣的是计算吉娃娃的平均体重。我们假设任何给定吉娃娃的体重是*独立地*以 \(\sigma^2 = 1\) 正态分布，但均值
    \(\mu\) 未知。因此，我们收集了10只吉娃娃并称重。用 \(W_j \sim \mathcal{N}(\mu, 1)\) 表示第 \(j\) 只吉娃娃的体重。对于第1步，让我们计算我们数据（即10只吉娃娃的体重）的概率密度。由于假设体重是独立的，密度相乘。令
    \(L(\mu)\) 为 \(\mu\) 的似然，我们有
- en: \[\begin{split} \begin{align} L(\mu) &= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10})
    \\ &= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10}) \\ &= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot
    1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\ &\propto \exp\left(-\sum_{j =
    1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\ \end{align} \end{split}\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} L(\mu) &= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10})
    \\ &= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10}) \\ &= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot
    1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\ &\propto \exp\left(-\sum_{j =
    1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\ \end{align} \end{split}\]
- en: 'Note that we can work up to a constant of proportionality since the value of
    \(\mu\) that maximizes \(L(\mu)\) will also maximize anything proportional to
    \(L(\mu)\). For step 2, take the log:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以一直工作到比例常数，因为使 \(L(\mu)\) 最大的 \(\mu\) 值也会使任何与 \(L(\mu)\) 成比例的东西最大。对于第2步，取对数：
- en: \[ \log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c, \]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c, \]
- en: 'where \(c\) is some constant. For step 3, take the derivative:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(c\) 是某个常数。对于第3步，求导数：
- en: \[ \begin{align} \frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j
    - \mu). \end{align} \]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align} \frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j
    - \mu). \end{align} \]
- en: Setting this equal to 0, we find that the (log) likelihood is maximized with
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将其设为0，我们发现（对数）似然在以下情况下达到最大：
- en: \[ \hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. \]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. \]
- en: We put a hat over \(\mu\) to indicate that it is our *estimate* of the true
    \(\mu\). Note the sensible result—we estimate the true mean of the Chihuahua weight
    distribution to be the sample mean of our observed data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 \(\mu\) 上加一个帽子来表示它是我们对真实 \(\mu\) 的*估计*。注意合理的结果——我们估计吉娃娃体重分布的真实均值为我们观察数据的样本均值。
- en: 3\. Conditional Probability
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 条件概率
- en: Probabilistic machine learning methods typically consider the distribution of
    a target variable conditional on the value of one or more predictor variables.
    To understand these methods, let’s introduce some of the basic principles of conditional
    probability.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 概率机器学习方法通常考虑目标变量在给定一个或多个预测变量值条件下的分布。为了理解这些方法，让我们介绍一些条件概率的基本原理。
- en: Consider two events, \(A\) and \(B\). The **conditional probability** of \(A\)
    given \(B\) is the probability that \(A\) occurs given \(B\) occurs, written \(P(A|B)\).
    Closely related is the **joint probability** of \(A\) and \(B\), or the probability
    that both \(A\) and \(B\) occur, written \(P(A, B)\). We navigate between the
    conditional and joint probability with the following
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个事件，\(A\) 和 \(B\)。\(A\) 在 \(B\) 发生的条件下的**条件概率**是 \(A\) 在 \(B\) 发生的情况下发生的概率，写作
    \(P(A|B)\)。与之密切相关的是 \(A\) 和 \(B\) 的**联合概率**，即 \(A\) 和 \(B\) 同时发生的概率，写作 \(P(A,
    B)\)。我们通过以下方式在条件概率和联合概率之间进行转换
- en: \[ P(A, B) = P(A|B)P(B). \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(A, B) = P(A|B)P(B). \]
- en: 'The above equation leads to an extremely important principle in conditional
    probability: Bayes’ rule. **Bayes’ rule** states that'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程导出了一个在条件概率中极为重要的原理：贝叶斯定理。**贝叶斯定理**指出
- en: \[ P(A|B) = \frac{P(B|A)P(A)}{P(B)}. \]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(A|B) = \frac{P(B|A)P(A)}{P(B)}. \]
- en: Both of the above expressions work for random variables as well as events. For
    any two discrete random variables, \(X\) and \(Y\)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个表达式对随机变量和事件都适用。对于任何两个离散随机变量 \(X\) 和 \(Y\)
- en: \[\begin{split} \begin{align} P(X = x, Y = y) &= P(X = x|Y = y)P(Y = y) \\ P(X
    = x|Y = y) &= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}. \end{align} \end{split}\]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} P(X = x, Y = y) &= P(X = x|Y = y)P(Y = y) \\ P(X
    = x|Y = y) &= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}. \end{align} \end{split}\]
- en: The same is true for continuous random variables, replacing the PMFs with PDFs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量也是如此，用 PMF 代替 PDF。
- en: 1\. Random Variables and Distributions
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 随机变量和分布
- en: Random Variables
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机变量
- en: A **random variable** is a variable whose value is randomly determined. The
    set of possible values a random variable can take on is called the variable’s
    **support**. An example of a random variable is the value on a die roll. This
    variable’s support is \(\{1, 2, 3, 4, 5, 6\}\). Random variables will be represented
    with uppercase letters and values in their support with lowercase letters. For
    instance \(X = x\) implies that a random variable \(X\) happened to take on value
    \(x\). Letting \(X\) be the value of a die roll, \(X = 4\) indicates that the
    die landed on 4.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机变量**是一个值随机确定的变量。随机变量可以取的可能值的集合称为变量的**支撑集**。一个随机变量的例子是掷骰子的值。这个变量的支撑集是 \(\{1,
    2, 3, 4, 5, 6\}\)。随机变量将用大写字母表示，其支撑集中的值用小写字母表示。例如 \(X = x\) 意味着随机变量 \(X\) 偶然取了值
    \(x\)。如果让 \(X\) 表示掷骰子的值，\(X = 4\) 表示骰子落在 4 上。'
- en: Density Functions
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密度函数
- en: The likelihood that a random variable takes on a given value is determined through
    its density function. For a discrete random variable (one that can take on a finite
    set of values), this density function is called the **probability mass function**
    **(PMF)**. The PMF of a random variable \(X\) gives the probability that \(X\)
    will equal some value \(x\). We write it as \(f_X(x)\) or just \(f(x)\), and it
    is defined as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量取特定值的可能性是通过其密度函数确定的。对于一个离散随机变量（可以取有限个值的变量），这个密度函数被称为**概率质量函数**（PMF）。随机变量
    \(X\) 的 PMF 给出了 \(X\) 等于某个值 \(x\) 的概率。我们将其写作 \(f_X(x)\) 或简写为 \(f(x)\)，其定义如下
- en: \[ f(x) = P(X = x). \]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = P(X = x). \]
- en: For a continuous random variable (one that can take on infinitely many values),
    the density function is called the **probability density function (PDF)**. The
    PDF \(f_X(x)\) of a continuous random variable \(X\) does not give \(P(X = x)\)
    but it does determine the probability that \(X\) lands in a certain range. Specifically,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个连续随机变量（可以取无限多个值的变量），其密度函数被称为**概率密度函数（PDF）**。连续随机变量 \(X\) 的 PDF \(f_X(x)\)
    并不给出 \(P(X = x)\)，但它确实确定了 \(X\) 落在某个范围内的概率。具体来说，
- en: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
- en: That is, integrating \(f(x)\) over a certain range gives the probability of
    \(X\) being in that range. While \(f(x)\) does not give the probability that \(X\)
    will equal a certain value, it does indicate the relative likelihood that it will
    be *around* that value. E.g. if \(f(a) > f(b)\), we can say \(X\) is more likely
    to be in an arbitrarily small area around the value \(a\) than around the value
    \(b\).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 即，将 \(f(x)\) 在某个范围内积分给出 \(X\) 落在该范围内的概率。虽然 \(f(x)\) 并不给出 \(X\) 等于某个特定值的概率，但它确实表明它将“在”该值周围的相对可能性。例如，如果
    \(f(a) > f(b)\)，我们可以说 \(X\) 更有可能在值 \(a\) 附近的任意小区域内，而不是在值 \(b\) 附近。
- en: Distributions
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布
- en: A random variable’s **distribution** is determined by its density function.
    Variables with the same density function are said to follow the same distributions.
    Certain families of distributions are very common in probability and machine learning.
    Two examples are given below.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机变量的 **分布** 由其密度函数确定。具有相同密度函数的变量被认为遵循相同的分布。某些分布族在概率和机器学习中非常常见。以下给出两个例子。
- en: The **Bernoulli** distribution is the most simple probability distribution and
    it describes the likelihood of the outcomes of a binary event. Let \(X\) be a
    random variable that equals 1 (representing “success”) with probability \(p\)
    and 0 (representing “failure”) with probability \(1-p\). Then, \(X\) is said to
    follow the Bernoulli distribution with probability parameter \(p\), written \(X
    \sim \text{Bern}(p)\), and its PMF is given by
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**伯努利** 分布是最简单的概率分布，它描述了二元事件的结局可能性。设 \(X\) 是一个随机变量，以概率 \(p\) 等于 1（代表“成功”），以概率
    \(1-p\) 等于 0（代表“失败”）。那么，\(X\) 被说成是服从概率参数 \(p\) 的伯努利分布，记作 \(X \sim \text{Bern}(p)\)，其概率质量函数（PMF）如下所示'
- en: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
- en: We can check to see that for any valid value \(x\) in the support of \(X\)—i.e.,
    1 or 0—, \(f(x)\) gives \(P(X = x)\).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查，对于 \(X\) 支持集中的任何有效值 \(x\)（即 1 或 0），\(f(x)\) 给出 \(P(X = x)\)。
- en: The **Normal** distribution is extremely common and will be used throughout
    this book. A random variable \(X\) follows the Normal distribution with mean parameter
    \(\mu \in \R\) and variance parameter \(\sigma^2 > 0\), written \(X \sim \mathcal{N}(\mu,
    \sigma^2)\), if its PDF is defined as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**正态** 分布非常常见，本书中将广泛使用。一个随机变量 \(X\) 如果其概率密度函数（PDF）定义为'
- en: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
- en: The shape of the Normal random variable’s density function gives this distribution
    the name “the bell curve”, as shown below. Values closest to \(\mu\) are most
    likely and the density is symmetric around \(\mu\).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正态随机变量的密度函数的形状使得这个分布被称为“钟形曲线”，如下所示。最接近 \(\mu\) 的值最有可能，密度在 \(\mu\) 周围是对称的。
- en: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
- en: Independence
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立性
- en: So far we’ve discussed the density of individual random variables. The picture
    can get much more complicated when we want to study the behavior of multiple random
    variables simultaneously. The assumption of independence simplifies things greatly.
    Let’s start by defining independence in the discrete case.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了单个随机变量的密度。当我们想要同时研究多个随机变量的行为时，情况可能会变得更加复杂。独立性假设大大简化了问题。让我们首先在离散情况下定义独立性。
- en: Two discrete random variables \(X\) and \(Y\) are **independent** if and only
    if
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 两个离散随机变量 \(X\) 和 \(Y\) 是 **独立的** 当且仅当
- en: \[ P(X = x, Y =y) = P(X = x)P(Y = y), \]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(X = x, Y = y) = P(X = x)P(Y = y), \]
- en: for all \(x\) and \(y\). This says that if \(X\) and \(Y\) are independent,
    the probability that \(X = x\) and \(Y = y\) simultaneously is just the product
    of the probabilities that \(X = x\) and \(Y = y\) individually.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\)。这意味着如果 \(X\) 和 \(Y\) 是独立的，\(X = x\) 和 \(Y = y\) 同时发生的概率就是
    \(X = x\) 和 \(Y = y\) 单独发生的概率的乘积。
- en: To generalize this definition to continuous random variables, let’s first introduce
    *joint density function*. Quite simply, the joint density of two random variables
    \(X\) and \(Y\), written \(f_{X, Y}(x, y)\) gives the probability density of \(X\)
    and \(Y\) evaluated simultaneously at \(x\) and \(y\), respectively. We can then
    say that \(X\) and \(Y\) are independent if and only if
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个定义推广到连续随机变量，我们首先介绍 *联合密度函数*。简单来说，两个随机变量 \(X\) 和 \(Y\) 的联合密度，记作 \(f_{X,
    Y}(x, y)\)，给出了 \(X\) 和 \(Y\) 同时在 \(x\) 和 \(y\) 处的概率密度。然后我们可以说，\(X\) 和 \(Y\) 是独立的当且仅当
- en: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
- en: for all \(x\) and \(y\).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\).
- en: Random Variables
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机变量
- en: A **random variable** is a variable whose value is randomly determined. The
    set of possible values a random variable can take on is called the variable’s
    **support**. An example of a random variable is the value on a die roll. This
    variable’s support is \(\{1, 2, 3, 4, 5, 6\}\). Random variables will be represented
    with uppercase letters and values in their support with lowercase letters. For
    instance \(X = x\) implies that a random variable \(X\) happened to take on value
    \(x\). Letting \(X\) be the value of a die roll, \(X = 4\) indicates that the
    die landed on 4.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机变量**是一个值随机确定的变量。随机变量可以取的可能值的集合称为变量的**支持集**。一个随机变量的例子是掷骰子的值。这个变量的支持集是 \(\{1,
    2, 3, 4, 5, 6\}\)。随机变量将用大写字母表示，其支持集中的值用小写字母表示。例如 \(X = x\) 意味着随机变量 \(X\) 偶然取了值
    \(x\)。让 \(X\) 表示掷骰子的值，\(X = 4\) 表示骰子落在 4 上。'
- en: Density Functions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概率密度函数
- en: The likelihood that a random variable takes on a given value is determined through
    its density function. For a discrete random variable (one that can take on a finite
    set of values), this density function is called the **probability mass function**
    **(PMF)**. The PMF of a random variable \(X\) gives the probability that \(X\)
    will equal some value \(x\). We write it as \(f_X(x)\) or just \(f(x)\), and it
    is defined as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量取特定值的可能性是通过其密度函数确定的。对于离散随机变量（可以取有限个值的随机变量），这个密度函数称为**概率质量函数**（PMF）。随机变量
    \(X\) 的 PMF 给出 \(X\) 等于某个值 \(x\) 的概率。我们将其写作 \(f_X(x)\) 或简写为 \(f(x)\)，其定义为
- en: \[ f(x) = P(X = x). \]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = P(X = x). \]
- en: For a continuous random variable (one that can take on infinitely many values),
    the density function is called the **probability density function (PDF)**. The
    PDF \(f_X(x)\) of a continuous random variable \(X\) does not give \(P(X = x)\)
    but it does determine the probability that \(X\) lands in a certain range. Specifically,
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量（可以取无限多个值的随机变量），其密度函数称为**概率密度函数（PDF）**。连续随机变量 \(X\) 的 PDF \(f_X(x)\)
    不给出 \(P(X = x)\)，但它确实确定了 \(X\) 落在某个特定范围内的概率。具体来说，
- en: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(a \leq X \leq b) = \int_{x = a}^b f(x) dx. \]
- en: That is, integrating \(f(x)\) over a certain range gives the probability of
    \(X\) being in that range. While \(f(x)\) does not give the probability that \(X\)
    will equal a certain value, it does indicate the relative likelihood that it will
    be *around* that value. E.g. if \(f(a) > f(b)\), we can say \(X\) is more likely
    to be in an arbitrarily small area around the value \(a\) than around the value
    \(b\).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 即，对 \(f(x)\) 在某个范围内的积分给出 \(X\) 在该范围内的概率。虽然 \(f(x)\) 不给出 \(X\) 等于某个特定值的概率，但它确实表明它将“**大约**”在该值周围的相对可能性。例如，如果
    \(f(a) > f(b)\)，我们可以说 \(X\) 在值 \(a\) 附近的任意小区域内比在值 \(b\) 附近的区域内更有可能。
- en: Distributions
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布
- en: A random variable’s **distribution** is determined by its density function.
    Variables with the same density function are said to follow the same distributions.
    Certain families of distributions are very common in probability and machine learning.
    Two examples are given below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量的**分布**由其密度函数确定。具有相同密度函数的变量被认为遵循相同的分布。某些分布族在概率和机器学习中非常常见。以下给出两个例子。
- en: The **Bernoulli** distribution is the most simple probability distribution and
    it describes the likelihood of the outcomes of a binary event. Let \(X\) be a
    random variable that equals 1 (representing “success”) with probability \(p\)
    and 0 (representing “failure”) with probability \(1-p\). Then, \(X\) is said to
    follow the Bernoulli distribution with probability parameter \(p\), written \(X
    \sim \text{Bern}(p)\), and its PMF is given by
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**伯努利分布**是最简单的概率分布，它描述了二元事件的结局的可能性。设 \(X\) 是一个随机变量，以概率 \(p\) 等于 1（代表“成功”），以概率
    \(1-p\) 等于 0（代表“失败”）。那么，\(X\) 被说成是遵循概率参数 \(p\) 的伯努利分布，记作 \(X \sim \text{Bern}(p)\)，其
    PMF 如下所示'
- en: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_X(x) = p^x(1-p)^{(1-x)}. \]
- en: We can check to see that for any valid value \(x\) in the support of \(X\)—i.e.,
    1 or 0—, \(f(x)\) gives \(P(X = x)\).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查，对于 \(X\) 的支持集中任何有效的值 \(x\)（即 1 或 0），\(f(x)\) 给出 \(P(X = x)\)。
- en: The **Normal** distribution is extremely common and will be used throughout
    this book. A random variable \(X\) follows the Normal distribution with mean parameter
    \(\mu \in \R\) and variance parameter \(\sigma^2 > 0\), written \(X \sim \mathcal{N}(\mu,
    \sigma^2)\), if its PDF is defined as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**正态分布**极为常见，本书将贯穿使用。一个随机变量 \(X\) 服从均值为 \(\mu \in \R\) 和方差参数 \(\sigma^2 > 0\)
    的正态分布，记作 \(X \sim \mathcal{N}(\mu, \sigma^2)\)，如果其概率密度函数定义为'
- en: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}. \]
- en: The shape of the Normal random variable’s density function gives this distribution
    the name “the bell curve”, as shown below. Values closest to \(\mu\) are most
    likely and the density is symmetric around \(\mu\).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正态随机变量的密度函数的形状使得这个分布被称为“钟形曲线”，如下所示。最接近 \(\mu\) 的值最有可能，密度在 \(\mu\) 周围是对称的。
- en: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![normal](../Images/d421cf0f88db6f8c4a0271d87de7ab98.png)'
- en: Independence
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立性
- en: So far we’ve discussed the density of individual random variables. The picture
    can get much more complicated when we want to study the behavior of multiple random
    variables simultaneously. The assumption of independence simplifies things greatly.
    Let’s start by defining independence in the discrete case.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了单个随机变量的密度。当我们想要同时研究多个随机变量的行为时，情况可能会变得非常复杂。独立性假设大大简化了问题。让我们首先在离散情况下定义独立性。
- en: Two discrete random variables \(X\) and \(Y\) are **independent** if and only
    if
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 两个离散随机变量 \(X\) 和 \(Y\) 是**独立的**当且仅当
- en: \[ P(X = x, Y =y) = P(X = x)P(Y = y), \]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(X = x, Y = y) = P(X = x)P(Y = y), \]
- en: for all \(x\) and \(y\). This says that if \(X\) and \(Y\) are independent,
    the probability that \(X = x\) and \(Y = y\) simultaneously is just the product
    of the probabilities that \(X = x\) and \(Y = y\) individually.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\)。这意味着如果 \(X\) 和 \(Y\) 是独立的，那么 \(X = x\) 和 \(Y = y\) 同时发生的概率就是
    \(X = x\) 和 \(Y = y\) 单独发生的概率的乘积。
- en: To generalize this definition to continuous random variables, let’s first introduce
    *joint density function*. Quite simply, the joint density of two random variables
    \(X\) and \(Y\), written \(f_{X, Y}(x, y)\) gives the probability density of \(X\)
    and \(Y\) evaluated simultaneously at \(x\) and \(y\), respectively. We can then
    say that \(X\) and \(Y\) are independent if and only if
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个定义推广到连续随机变量，我们首先介绍**联合密度函数**。简单来说，两个随机变量 \(X\) 和 \(Y\) 的联合密度，记作 \(f_{X,
    Y}(x, y)\)，给出了 \(X\) 和 \(Y\) 在 \(x\) 和 \(y\) 处同时评估的概率密度。然后我们可以这样说，如果 \(X\) 和 \(Y\)
    是独立的，那么 \(X = x\) 和 \(Y = y\) 同时发生的概率就是 \(X = x\) 和 \(Y = y\) 单独发生的概率的乘积。
- en: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f_{X, Y}(x, y) = f_X(x) f_Y(y), \]
- en: for all \(x\) and \(y\).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 \(x\) 和 \(y\)。
- en: 2\. Maximum Likelihood Estimation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 最大似然估计
- en: Maximum likelihood estimation is used to understand the parameters of a distribution
    that gave rise to observed data. In order to model a data generating process,
    we often assume it comes from some family of distributions, such as the Bernoulli
    or Normal distributions. These distributions are indexed by certain parameters
    (\(p\) for the Bernoulli and \(\mu\) and \(\sigma^2\) for the Normal)—maximum
    likelihood estimation evaluates which parameters would be most consistent with
    the data we observed.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计用于理解导致观察数据的分布的参数。为了模拟数据生成过程，我们通常假设它来自某些分布族，例如伯努利分布或正态分布。这些分布由某些参数索引（伯努利分布的
    \(p\)，正态分布的 \(\mu\) 和 \(\sigma^2\)）——最大似然估计评估哪些参数与我们观察到的数据最一致。
- en: 'Specifically, maximum likelihood estimation finds the values of unknown parameters
    that maximize the probability of observing the data we did. Basic maximum likelihood
    estimation can be broken into three steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，最大似然估计找到未知参数的值，以最大化观察到的数据的概率。基本最大似然估计可以分为三个步骤：
- en: Find the joint density of the observed data, also called the *likelihood*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到观察数据的联合密度，也称为**似然**
- en: Take the log of the likelihood, giving the *log-likelihood*.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对似然取对数，得到**对数似然**。
- en: Find the value of the parameter that maximizes the log-likelihood (and therefore
    the likelihood as well) by setting its derivative equal to 0.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将导数设为0来找到使对数似然（因此也是似然）最大化的参数值。
- en: Finding the value of the parameter to maximize the log-likelihood rather than
    the likelihood makes the math easier and gives us the same solution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化对数似然而不是似然来找到参数值，这使得数学更容易，并且给出了相同的解。
- en: Let’s go through an example. Suppose we are interested in calculating the average
    weight of a Chihuahua. We assume the weight of any given Chihuahua is *independently*
    distributed Normally with \(\sigma^2 = 1\) but an unknown mean \(\mu\). So, we
    gather 10 Chihuahuas and weigh them. Denote the \(j^\text{th}\) Chihuahua weight
    with \(W_j \sim \mathcal{N}(\mu, 1)\). For step 1, let’s calculate the probability
    density of our data (i.e., the 10 Chihuahua weights). Since the weights are assumed
    to be independent, the densities multiply. Letting \(L(\mu)\) be the likelihood
    of \(\mu\), we have
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来讲解。假设我们想要计算吉娃娃的平均体重。我们假设任何一只吉娃娃的体重都是独立地服从均值为 \(\mu\)、方差为 \(\sigma^2
    = 1\) 的正态分布。因此，我们收集了10只吉娃娃并称量它们的体重。用 \(W_j\) 表示第 \(j\) 只吉娃娃的体重，\(W_j \sim \mathcal{N}(\mu,
    1)\)。对于第一步，让我们计算我们的数据（即10只吉娃娃的体重）的概率密度。由于假设体重是独立的，密度相乘。令 \(L(\mu)\) 为 \(\mu\)
    的似然函数，我们得到
- en: \[\begin{split} \begin{align} L(\mu) &= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10})
    \\ &= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10}) \\ &= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot
    1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\ &\propto \exp\left(-\sum_{j =
    1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\ \end{align} \end{split}\]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} L(\mu) &= f_{W_1, \dots, W_{10}}(w_1, \dots, w_{10})
    \\ &= f_{W_1}(w_1)\cdot...\cdot f_{W_{10}}(w_{10}) \\ &= \prod_{j = 1}^{10} \frac{1}{\sqrt{2\pi\cdot
    1}}\exp\left(-\frac{(w_j - \mu)^2}{2} \right) \\ &\propto \exp\left(-\sum_{j =
    1}^{10}\frac{(w_j - \mu)^2}{2} \right). \\ \end{align} \end{split}\]
- en: 'Note that we can work up to a constant of proportionality since the value of
    \(\mu\) that maximizes \(L(\mu)\) will also maximize anything proportional to
    \(L(\mu)\). For step 2, take the log:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以计算出比例常数，因为最大化 \(L(\mu)\) 的 \(\mu\) 值也将最大化与 \(L(\mu)\) 成比例的任何值。对于第二步，取对数：
- en: \[ \log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c, \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\mu) = -\sum_{j = 1}^{10}\frac{(w_j - \mu)^2}{2} + c, \]
- en: 'where \(c\) is some constant. For step 3, take the derivative:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(c\) 是某个常数。对于第三步，求导：
- en: \[ \begin{align} \frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j
    - \mu). \end{align} \]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align} \frac{\partial}{\partial \mu}\log L(\mu) = -\sum_{j = 1}^{10}(w_j
    - \mu). \end{align} \]
- en: Setting this equal to 0, we find that the (log) likelihood is maximized with
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将其设为0，我们发现（对数）似然函数在
- en: \[ \hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. \]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\mu} = \frac{1}{10}\sum_{j = 1}^{10} w_j = \bar{w}. \]
- en: We put a hat over \(\mu\) to indicate that it is our *estimate* of the true
    \(\mu\). Note the sensible result—we estimate the true mean of the Chihuahua weight
    distribution to be the sample mean of our observed data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 \(\mu\) 上加一个帽子来表示它是我们对真实 \(\mu\) 的估计。注意这个合理的结果——我们估计吉娃娃体重分布的真实均值为我们观察到的数据的样本均值。
- en: 3\. Conditional Probability
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 条件概率
- en: Probabilistic machine learning methods typically consider the distribution of
    a target variable conditional on the value of one or more predictor variables.
    To understand these methods, let’s introduce some of the basic principles of conditional
    probability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 概率机器学习方法通常考虑目标变量在给定一个或多个预测变量值条件下的分布。为了理解这些方法，让我们介绍一些条件概率的基本原理。
- en: Consider two events, \(A\) and \(B\). The **conditional probability** of \(A\)
    given \(B\) is the probability that \(A\) occurs given \(B\) occurs, written \(P(A|B)\).
    Closely related is the **joint probability** of \(A\) and \(B\), or the probability
    that both \(A\) and \(B\) occur, written \(P(A, B)\). We navigate between the
    conditional and joint probability with the following
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个事件 \(A\) 和 \(B\)。\(A\) 在 \(B\) 发生的条件下的**条件概率**是 \(A\) 发生在 \(B\) 发生的条件下的概率，表示为
    \(P(A|B)\)。与之密切相关的是 \(A\) 和 \(B\) 的**联合概率**，即 \(A\) 和 \(B\) 同时发生的概率，表示为 \(P(A,
    B)\)。我们通过以下方式在条件概率和联合概率之间进行转换
- en: \[ P(A, B) = P(A|B)P(B). \]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(A, B) = P(A|B)P(B). \]
- en: 'The above equation leads to an extremely important principle in conditional
    probability: Bayes’ rule. **Bayes’ rule** states that'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程导出了条件概率中的一个极其重要的原理：贝叶斯定理。**贝叶斯定理**指出
- en: \[ P(A|B) = \frac{P(B|A)P(A)}{P(B)}. \]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ P(A|B) = \frac{P(B|A)P(A)}{P(B)}. \]
- en: Both of the above expressions work for random variables as well as events. For
    any two discrete random variables, \(X\) and \(Y\)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个表达式同样适用于随机变量以及事件。对于任何两个离散随机变量 \(X\) 和 \(Y\)
- en: \[\begin{split} \begin{align} P(X = x, Y = y) &= P(X = x|Y = y)P(Y = y) \\ P(X
    = x|Y = y) &= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}. \end{align} \end{split}\]
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align} P(X = x, Y = y) &= P(X = x|Y = y)P(Y = y) \\ P(X
    = x|Y = y) &= \frac{P(Y = y|X = x)P(X = x)}{P(Y = y)}. \end{align} \end{split}\]
- en: The same is true for continuous random variables, replacing the PMFs with PDFs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续随机变量也是如此，用概率密度函数替换概率质量函数。
