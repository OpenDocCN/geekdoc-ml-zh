<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Chapter 4 Analysis of Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Chapter 4 Analysis of Algorithms</h1>
<blockquote>原文：<a href="https://ppml.dev/algorithms.html">https://ppml.dev/algorithms.html</a></blockquote>
<div id="algorithms" class="section level1 hasAnchor" number="4">

<!-- short-hands for mathematical notation -->
<p>In the previous chapters we discussed how the hardware architectures we use (Chapter <a href="hardware.html#hardware">2</a>), the nature of the
data we analyse and how we represent data with data structures (Chapter <a href="types-structures.html#types-structures">3</a>) contribute to the
performance of a machine learning pipeline. The last major piece of this puzzle is the algorithmic complexity of the
algorithms that power the machine learning models.</p>
<p>Algorithmic complexity is defined, in the abstract, as the amount of resources required by an algorithm to complete.
After using pseudocode to write a high-level description of the machine learning algorithm we would like to implement
(Section <a href="algorithms.html#pseudocode">4.1</a>), we can determine the complexity of its components and how they contribute to the complexity
of the algorithm as a whole. We represent and reason about complexity in mathematical terms with big-<span class="math inline">\(O\)</span> notation
(Section <a href="algorithms.html#bigO-intro">4.2</a>). Quantifying it (Section <a href="algorithms.html#bigO-benchmark">4.3</a>) presents several issues that are specific to
machine learning (Section <a href="algorithms.html#bigO-ml">4.4</a>), which we will illustrate with three examples (Section <a href="algorithms.html#bigO-examples">4.5</a>).</p>
<div id="pseudocode" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Writing Pseudocode<a href="algorithms.html#pseudocode" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>The first step in reasoning about an algorithm is to write it down using <em>pseudocode</em>, combining the understandability
of natural language and the precision of code to facilitate our analysis and the subsequent implementation in software.
Natural language is easier to read, but it is also ambiguous. Code, on the other hand, is too specific: it forces us
to think about implementation details and programming-language conventions thus making it harder to focus on the overall
picture.</p>
<p>Pseudocode is meant to offset the weaknesses of natural language and code while preserving their strong points. Ideally,
it provides a high-level description of an algorithm that facilitates its analysis and implementation by making the
intent of each step clear while suppressing those details that are irrelevant for understanding the algorithm. There is
no universal standard on how to write pseudocode, although some general guidelines exist (see, for instance, Chapter 9
in “Code Complete” <span class="citation">(McConnell <a href="#ref-codecomplete" role="doc-biblioref">2004</a>)</span>). The three key guidelines that are commonly accepted are:</p>
<ul>
<li>Each step of the algorithm should be a separate, self-contained item in an enumeration.</li>
<li>Pseudocode should combine the styles of good code and of good natural language to some extent. For instance, it may
fall short of full sentences. It should avoid idioms and conventions specific to any particular programming language,
using a lax syntax for code statements. For the same reason, variable names should come from the domain of the problem
the algorithm is trying to solve rather than from how they will be implemented (for instance, in terms of data
structures). We will return to this point in Section <a href="writing-code.html#naming">6.2</a>.</li>
<li>Pseudocode should ignore unnecessary details and use short-hand notation when possible, leaving the context to more
in-depth forms of documentation. In other words, the level of detail should be that of a high-level view of the
overall structure of the algorithm so that we can focus on its intent.</li>
</ul>
<p>Admittedly, these recommendations are vague because the best way of conveying a high-level view of an algorithm to the
reader depends on a combination of the pseudocode style and the reader’s background. As usual, knowing the audience is
key in communicating effectively.</p>
<p>Writing good pseudocode for machine learning algorithms, or for machine learning pipelines spanning multiple algorithms,
has two additional complications: the role played by the data and the need to integrate some amount of mathematical
notation.</p>
<p>Firstly, if we are to treat data as code (Section <a href="design-code.html#data-as-code">5.1</a>), we may want to include more details about them
in the pseudocode than we would in other settings. Such information may include, for example, the dimensions of the data
and those characteristics of its features that are crucial for the algorithm to function. In a sense, this is similar to
including some type information about key objects, and it is useful in clarifying what the inputs and the outputs of the
algorithm are as well as in giving more context to key steps.</p>
<p>Secondly, mathematical notation may be the best tool to describe key steps in a clear and readable way, so we may want
to integrate it with natural language and code to the best effect. In order to do that, we should define all the
variables and the functions used in the notation while leaving additional details to a separate document. For practical
purposes, mentioning the meaning of each symbol when introducing new mathematical notation (for instance, “the prior
Beta distribution <span class="math inline">\(\pi(\alpha, \beta) \sim \mathit{Be}(\alpha, \beta)\)</span>” as opposed to just “<span class="math inline">\(\pi(\alpha, \beta) \sim \mathit{Be}(\alpha, \beta)\)</span>”) is often enough to give context (what are properties of <span class="math inline">\(\pi\)</span>, how it will be used, etc.).
Complex formulas, derivations and formal proofs would reduce the readability of pseudocode by making it overlong and
forcing the reader to concentrate on understanding them instead of looking at the overall logic of the algorithm.</p>
<div style="page-break-after: always;"/>
<p>Describing all algorithms used in a machine learning pipeline using pseudocode has some further advantages we will touch
on in later chapters:</p>
<p/>
<ul>
<li>it makes code review easier (see Section <a href="writing-code.html#code-review">6.6</a>);</li>
<li>it facilitates iterative refinement because pseudocode is easier to modify than code (see Section
<a href="design-code.html#scoping-pipeline">5.3.1</a>);</li>
<li>it provides design documentation in a form that is easy to maintain (see Section <a href="documenting-code.html#designdocs">8.3</a>).</li>
</ul>
</div>
<div id="bigO-intro" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Computational Complexity and Big-<span class="math inline">\(O\)</span> Notation<a href="algorithms.html#bigO-intro" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>
Computational complexity is a branch of computer science that focuses on classifying computational problems according to
their inherent difficulty across three different dimensions:</p>
<ul>
<li>as a function of input size (say, the sample size or the number of variables);</li>
<li>as a function of how much resources will be used, in particular time (say, CPU time spent) and space (memory or
storage use);</li>
<li>on average (how long it will typically take), in the best case or in the worst case (how long it can possibly take).
</li>
</ul>
<p>
In other words, we would like to infer how much resources an algorithm will use just from its specification: this is
called <em>algorithm analysis</em>. Typically, the specification takes the form of pseudocode. As for the resources, we must
first choose our unit of measurement. In the case of space complexity, the choice is usually obvious: either an absolute
memory unit (such as MB, GB) or a relative one (such as the number of double-precision floating point values) for
various types of memory (RAM, GPU memory, storage). In the case of time complexity, we must choose a set of fundamental
operations that we consider to have a theoretical complexity of <span class="math inline">\(1\)</span>. Such operations can range from simple (arithmetic
operations) to complex (models trained) depending on the level of abstraction we would like to work at. On the one hand,
the lower the level of abstraction, the more we need to know about the specific implementation details of the algorithm.
This is only feasible up to a point because pseudocode will omit most such details. It is also undesirable to some
extent because it makes the analysis less general: a different implementation of the same algorithm may end up in a
different class of complexity while exhibiting about the same behaviour in practice. On the other hand, the higher the
level of abstraction, the higher the chance of obtaining an estimate of complexity that is only loosely connected to
reality. The more complex the operations, the more unlikely it will be that they have the same complexity and that their
complexity can be taken to be constant.</p>
<p>
The estimates of computational complexity produced by algorithm analysis are written using <em>big-<span class="math inline">\(O\)</span></em> and related
notations, which define the class of complexity in the limit of the input sizes <span class="citation">(Knuth <a href="#ref-knuth-big0" role="doc-biblioref">1976</a>, <a href="#ref-aocp" role="doc-biblioref">1997</a>)</span>. (All algorithms are
fast with small inputs.) More in detail:</p>
<ul>
<li>We describe the <em>worst-case</em> scenario using <em>big-<span class="math inline">\(O\)</span></em> notation. Formally, an algorithm with input of size <span class="math inline">\(N \to \infty\)</span> has a complexity <span class="math inline">\(f(N) = O(g(N))\)</span> if there exists a <span class="math inline">\(c_0 &gt; 0\)</span> such that <span class="math inline">\(f(N) \leqslant c_0 g(N)\)</span>. It
represents an upper bound in complexity.</li>
<li>We describe the <em>best-case</em> scenario using <em>big-<span class="math inline">\(\Omega\)</span></em> notation: <span class="math inline">\(f(N) = \Omega(g(N))\)</span>, <span class="math inline">\(f(N) \geqslant c_1 g(N)\)</span>
with <span class="math inline">\(c_1 &gt; 0\)</span>. It represents a lower bound in complexity.</li>
<li>We describe the <em>average case</em> using <em>big-<span class="math inline">\(\Theta\)</span></em> notation: <span class="math inline">\(f(N) = \Theta(g(N))\)</span>, <span class="math inline">\(c_2 g(N) \leqslant f(N) \leqslant c_3 g(N)\)</span> with <span class="math inline">\(c_2, c_3 &gt; 0\)</span>. It represents the average complexity.</li>
</ul>
<p>In practice, we often just write things like “it is <span class="math inline">\(O(g(N))\)</span> on average and <span class="math inline">\(O(h(N))\)</span> in the worst case” and use
big-<span class="math inline">\(O\)</span> for all three cases. If we are considering inputs that are best described with a combination of different sizes
(say, <span class="math inline">\(\{M, N, P\}\)</span>), <em>big-<span class="math inline">\(O\)</span></em> will be a multivariate function like <span class="math inline">\(O(g(M, N, P))\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:figure-complexity"/>
<img src="../Images/c15ab8b704fda37f4d619933b63cc935.png" alt="A graphical comparison of computational complexity classes." width="672" data-original-src="https://ppml.dev/chapter04_files/figure-html/figure-complexity-1.png"/>
<p class="caption">
Figure 4.1: A graphical comparison of computational complexity classes.
</p>
</div>
<p>Different classes of complexity in common use are shown in Figure <a href="algorithms.html#fig:figure-complexity">4.1</a>. Algorithms that belong
to <span class="math inline">\(O(1)\)</span>, <span class="math inline">\(O(\log N)\)</span>, <span class="math inline">\(O(N)\)</span> and <span class="math inline">\(O(N \log N)\)</span> are considered efficient, while those that belong to <span class="math inline">\(O(N^2)\)</span> or higher
classes of complexity are more demanding. In a sense, this classification reflects the economics of running compute
systems: it may be feasible to double our hardware requirements every time <span class="math inline">\(N\)</span> doubles, but increasing it by a power of
2 or more is rarely possible!
</p>
<p>How can we use big-<span class="math inline">\(O\)</span> notation? If we are comparing algorithms in different classes of complexity, we can concentrate
only on the leading term: <span class="math inline">\(O(3 \cdot 2^N + 3.42 N^2) \gg O(2 N^3 + 3 N^2)\)</span> is functionally equivalent to <span class="math inline">\(O(2^N) \gg O(N^3)\)</span> because the difference in the order of magnitude makes lower-order terms and even the coefficient of the leading
term irrelevant. If we are comparing algorithms in the same class of complexity, we only report the leading term
and its coefficient: <span class="math inline">\(O(1.2 N^2 + 3N) \gg O(0.9 N^2 + 2 \log N)\)</span> becomes <span class="math inline">\(O(1.2 N^2) \gg O(0.9 N^2)\)</span>. In the former
case, we can say algorithms scale in fundamentally different ways as their inputs grow in size. In the latter, we can
say that algorithms scale similarly but still rank them.</p>
<p>In most practical settings, however, interpreting big-<span class="math inline">\(O\)</span> notation requires a more nuanced approach because of its
intrinsic limitations.</p>
<ul>
<li>Big-<span class="math inline">\(O\)</span> notation does not include constant terms, so it may not necessarily map well to real-world performance.
Algorithms with complex initialisation phases that do not scale with input sizes may be slower than algorithms with
higher complexity for even moderate input sizes. This may be the case of algorithms that cache partial results or
sufficient statistics when the caching is more expensive than recomputing those quantities from scratch as needed.</li>
<li>Similarly, the coefficients in big-<span class="math inline">\(O\)</span> notation are usually not realistic: for instance, a time complexity <span class="math inline">\(O(2N)\)</span>
does not guarantee that doubling <span class="math inline">\(N\)</span> will double running time. How the algorithm is implemented, on what hardware,
etc. may not affect the class of complexity but they always have a strong effect on the associated coefficients. As a
result, we should estimate those coefficients from empirical run-times to obtain realistic performance curves, and use
the latter to compare algorithms within the same class of complexity.</li>
<li>There is a compromise between space and time complexity: we trade off one for the other. Arguably, space complexity is
more important than time complexity. In principle, we can wait a bit longer to get the results, but if our program
runs out of memory, it will crash and we will get no results at all.</li>
</ul>
</div>
<div id="bigO-benchmark" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Big-<span class="math inline">\(O\)</span> Notation and Benchmarking<a href="algorithms.html#bigO-benchmark" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Producing empirical performance curves for all relevant dimensions of computational complexity differs from other forms
of benchmarking in a few ways. If we know the theoretical class of complexity an algorithm belongs to, a simple linear
model will suffice for estimating the coefficients of the terms in its big-<span class="math inline">\(O\)</span> notation: we will show some examples in
Section <a href="algorithms.html#bigO-examples">4.5</a>. However, we should take some care in how performance is measured and in how we interpret
the empirical curves.</p>
<p>Firstly, we should plan the collection of the performance measurements following the best practices in the design of
physical <span class="citation">(Montgomery <a href="#ref-montgomery" role="doc-biblioref">20AD</a>)</span> and computer simulation <span class="citation">(Santner, Williams, and Notz <a href="#ref-santner" role="doc-biblioref">2018</a>)</span> experiments. If we are measuring complexity along a single
dimension, we should collect multiple performance measurements for each input size. The average performance at each
input size will provide a more stable estimate than a single measurement, and we can use the distribution of the
performance measurements to establish confidence bands around the average. Bands based on empirical quantiles (say, the
interval between the [5%, 95%] quantiles) are often preferable to bands based on the quantiles of the normal
distribution (say, average performance <span class="math inline">\(\pm\)</span> the standard deviation times the 95% quantile of the standard normal)
because the latter is symmetric around the average and fails to account for the fact that performance is skewed.
Performance is naturally bounded below by zero (instant execution!) and the average performance may be close enough to
zero that the bottom of the confidence band is negative!</p>
<p>If we are measuring complexity along multiple dimensions, it is best to use a single experimental design that involves
all of them in order to separate the main effect of each dimension from their interactions. Big-<span class="math inline">\(O\)</span> notation may not
include terms that contain more than one dimension, and in that case it is interesting to check whether that is true in
practice as well. Or big-<span class="math inline">\(O\)</span> notation may include such terms, and then the only consistent way of estimating their
coefficients is to vary all the involved input sizes simultaneously.</p>
<p>If we are comparing two algorithms in the same complexity class, we should do that on performance differences generated
on the same sets of inputs to increase the precision of our comparison. If we use different inputs, the performance
measures we collect for each input size are independent across algorithms: if those algorithms are <span class="math inline">\(O(f(N))\)</span> and
<span class="math inline">\(O(g(N))\)</span> respectively,
<span class="math display">\[\begin{equation*}
  \operatorname{VAR}(f(N) - g(N)) = \operatorname{VAR}(f(N)) + \operatorname{VAR}(g(N)).
\end{equation*}\]</span>
However, if we use the same inputs for both algorithms
<span class="math display">\[\begin{multline*}
  \operatorname{VAR}(f(N) - g(N)) = \operatorname{VAR}(f(N)) + \operatorname{VAR}(g(N)) - \\ 2\operatorname{COV}(f(N), g(N))
\end{multline*}\]</span>
because the performance measures are no longer independent. Since <span class="math inline">\(\operatorname{COV}(f(N), g(N)) &gt; 0\)</span>, the empirical differences in
performance will have smaller variability and thus greater precision.</p>
<p>Secondly, we should carefully choose the compute system we use. The system should “stand still” while we are taking
performance measurements: if other tasks are running at the same time, they may try to access shared resources that are
also involved in our performance measures. This has two negative effects: it makes performance measures noisier and
it inflates the estimated coefficients by making the average performance worse.</p>
<p>Thirdly, we should be careful in using performance curves to predict performance outside the range of the input sizes
(or the combinations of input sizes) we actually measured. In any compute system with finite resources, resource
contention will increase as the system reaches saturation. Hence we should use a compute system with enough resources
to handle all the input sizes we consider without going anywhere near capacity. We are interested in measuring the
performance of the algorithm, not of the compute system, so we should put stress on the former and not on the latter.</p>
<p>Finally, note that most experimental design approaches assume that performance measures are independent. Hence we should
strive to make our empirical measures as independent as possible by resetting the state of the compute system before
each run: for instance, we should remove all temporary objects.</p>
</div>
<div id="bigO-ml" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Algorithm Analysis for Machine Learning<a href="algorithms.html#bigO-ml" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Algorithm analysis presents some additional complications in the context of machine learning software.</p>
<p>The first set of complications is related to defining the size of the input. Machine learning algorithms typically have
a large number of different inputs, and the size of each input has several dimensions (such as the sample size and the
number of variables). Hence algorithms will belong to different classes of complexity for different dimensions, and it
will be unlikely that any will dominate the others in all dimensions. Sometimes we can reduce the number of dimensions
we are considering by assuming some are bounded because of the intrinsic nature of the inputs or by expressing one
dimension as a function of another (say, the number of variables is <span class="math inline">\(p \approx \sqrt{n}\)</span> where <span class="math inline">\(n\)</span> is the sample size).</p>
<p>Furthermore, computational complexity depends strongly on the assumptions we make on the distributions of the inputs and
not just on their sizes. More assumptions usually allow us to access algorithms with better performance, and make it
possible to use closed-form results: the more knowledge we put in the form of assumptions, the less we have to learn
empirical measures. Assuming some form of sparsity or regularity, or actively enforcing it in the learning process, will
reduce the complexity of machine learning models to the point they become tractable. In most settings, not making any
such assumption will mean exponential or combinatorial worst-case complexity. It is another trade-off: assumptions
versus complexity.</p>
<p>For stochastic algorithms, we can only meaningfully reason on the average case. Consider Markov chain Monte Carlo (MCMC)
posterior inference in Bayesian models, or stochastic gradient descent (SGD) for deep neural networks. Each time we run
them, they go through a different sequence of steps and they may produce a different posterior distribution or model. As
a consequence, each run will take a different amount of time and it will use a different amount of memory. The
construction of such algorithms gives convergence guarantees and convergence rates, so we have some expectations about
average complexity, but there is always some degree of uncertainty.</p>
</div>
<div id="bigO-examples" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Some Examples of Algorithm Analysis<a href="algorithms.html#bigO-examples" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>We will now apply the concepts we just introduced by investigating the time complexity of estimating the coefficients of
linear regression models (Section <a href="algorithms.html#bigO-lm">4.5.1</a>); the trade-off between time and space complexity in sparse matrices
(Section <a href="algorithms.html#bigO-sparsem">4.5.2</a>); and the time and space complexity of an MCMC algorithm to generate random directed
acyclic graphs from a uniform distribution (Section <a href="algorithms.html#bigO-dags">4.5.3</a>).</p>
<div id="bigO-lm" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Estimating Linear Regression Models<a href="algorithms.html#bigO-lm" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>Linear models are the foundation upon which most of statistics and machine learning are built: we often use them either
as standalone models or as part of more complex ones. Generally speaking, a linear model takes a vector <span class="math inline">\(\mathbf{y}\)</span> and a
matrix <span class="math inline">\(\mathbf{X}\)</span> of real numbers and tries to explain <span class="math inline">\(\mathbf{y}\)</span> as a function of <span class="math inline">\(\mathbf{X}\)</span> that is linear in the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>.
We can famously <span class="citation">(Weisberg <a href="#ref-weisberg" role="doc-biblioref">2014</a>)</span> estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> with the closed-form expression
<span class="math display">\[\begin{equation*}
  \underset{p \times 1}{\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}} =
    (\underset{p \times n}{\mathbf{X}^\mathrm{T}\vphantom{\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}}} \, \underset{n \times p}{\mathbf{X}\vphantom{\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}}})^{-1}
     \underset{p \times n}{\mathbf{X}^\mathrm{T}\vphantom{\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}}} \, \underset{n \times 1}{\mathbf{y}}
\end{equation*}\]</span>
which is, at the same time, the ordinary least squares estimate (from the orthogonal projection of <span class="math inline">\(\mathbf{y}\)</span> onto the space
spanned by the <span class="math inline">\(\mathbf{X}\)</span>) and the maximum likelihood estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span> (under the assumption residuals are independent and
normally distributed with a common variance). Note how we have annotated the formula above with the dimensions of both
<span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>. Those are our inputs: their dimensions depend on the sample size <span class="math inline">\(n\)</span> and on the number of variables
<span class="math inline">\(p\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> The
algorithmic complexity of estimating <span class="math inline">\(\boldsymbol{\beta}\)</span> will be a function of both.</p>
<p>Another option to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> is to use the QR decomposition of <span class="math inline">\(\mathbf{X}\)</span> <span class="citation">(Weisberg <a href="#ref-weisberg" role="doc-biblioref">2014</a>, Appendix A.9)</span>. Starting from the
<span class="math inline">\(\mathbf{X}\boldsymbol{\beta}= \mathbf{y}\)</span> formulation of the linear regression model, we perform the following steps:</p>
<ol style="list-style-type: decimal">
<li>compute the QR decomposition of <span class="math inline">\(\mathbf{X}\)</span> (<span class="math inline">\(\mathbf{Q}\)</span> is <span class="math inline">\(n \times p\)</span>, <span class="math inline">\(\mathbf{R}\)</span> is <span class="math inline">\(p \times p\)</span>);</li>
<li>rewrite the problem as <span class="math inline">\(\mathbf{R} \boldsymbol{\beta}= \mathbf{Q}^\mathrm{T}\mathbf{y}\)</span>;</li>
<li>compute <span class="math inline">\(\mathbf{Q}^\mathrm{T}\mathbf{y}\)</span>;</li>
<li>solve the resulting (triangular) linear system for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
</ol>
<p>Let’s call this estimator <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span>. If we discount numerical issues with pathological <span class="math inline">\(\mathbf{X}\)</span>s, <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span>
give identical estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span>: they are identical in terms of statistical properties, and neither makes any
assumption of the distribution of <span class="math inline">\(\mathbf{X}\)</span>. However, we may still prefer one to the other because of their time complexity.</p>
<p>Firstly, what is the time complexity of computing <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span>? The steps we would perform if we were estimating it by
hand are:</p>
<ol style="list-style-type: decimal">
<li>compute <span class="math inline">\(\mathbf{X}^\mathrm{T}\mathbf{X}\)</span>;</li>
<li>invert it and compute <span class="math inline">\((\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\)</span>;</li>
<li>compute <span class="math inline">\(\mathbf{X}^\mathrm{T}\mathbf{y}\)</span>;</li>
<li>multiply the results from steps 2 and 3.</li>
</ol>
<p>Given the simplicity of <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span>, this description will suffice as the pseudocode for our analysis.
From easily available sources (say, Wikipedia), we can find the time complexity of the operation in each step:</p>
<ul>
<li>multiplying an <span class="math inline">\(r \times s\)</span> matrix and an <span class="math inline">\(s \times t\)</span> matrix takes <span class="math inline">\(O(rst)\)</span> operations <span class="citation">(Wikipedia <a href="#ref-wiki-matmult" role="doc-biblioref">2021</a><a href="#ref-wiki-matmult" role="doc-biblioref">b</a>)</span>;</li>
<li>computing the inverse of an <span class="math inline">\(r \times r\)</span> matrix is <span class="math inline">\(O(r^3)\)</span> using a Cholesky decomposition <span class="citation">(Wikipedia <a href="#ref-wiki-chol" role="doc-biblioref">2021</a><a href="#ref-wiki-chol" role="doc-biblioref">a</a>)</span> or
Gram-Schmidt <span class="citation">(Wikipedia <a href="#ref-wiki-qr" role="doc-biblioref">2021</a><a href="#ref-wiki-qr" role="doc-biblioref">c</a>)</span>.</li>
</ul>
<p>These time complexities use arithmetic operations as the elementary operations, which is natural because matrices
are just sets of numbers that are combined and transformed using those operations. The actual contents of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>
are irrelevant: both matrices appear in the big-<span class="math inline">\(O\)</span> notation just with their dimensions.</p>
<p>Therefore, step 1 is <span class="math inline">\(O(pnp) = O(np^2)\)</span>, step 2 is <span class="math inline">\(O(p^3)\)</span>, step 3 is <span class="math inline">\(O(np)\)</span> and step 4 is <span class="math inline">\(O(p^2)\)</span>. The overall time
complexity is
<span class="math display" id="eq:betaEX">\[\begin{equation}
  O(np^2 + p^3 + np + p^2) = O(p^3 + (n + 1)p^2 + np)
\tag{4.1}
\end{equation}\]</span>
and it can be interpreted as follows:</p>
<ul>
<li>Estimating <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> is <span class="math inline">\(O(p^3)\)</span> in the number of parameters <span class="math inline">\(p\)</span>: if <span class="math inline">\(p\)</span> doubles, it takes eight times as long.</li>
<li>Estimating <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> is <span class="math inline">\(O(n)\)</span> in the sample size <span class="math inline">\(n\)</span>: if <span class="math inline">\(n\)</span> doubles, it takes twice as long.</li>
</ul>
<p>Let’s look now at <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span>. For time complexity we have that:</p>
<ul>
<li>solving an <span class="math inline">\(r \times s\)</span> linear system with Gram-Schmidt is <span class="math inline">\(O(rs^2)\)</span>;<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></li>
<li>back-substitution to solve the triangular linear system in step 4 is <span class="math inline">\(O(s^2)\)</span>.</li>
</ul>
<p>Therefore, step 1 is <span class="math inline">\(O(np^2)\)</span>, step 3 is <span class="math inline">\(O(np)\)</span> and step 4 is <span class="math inline">\(O(p^2)\)</span>. (Step 2 is merely for notation.) The overall
time complexity is then
<span class="math display" id="eq:betaQR">\[\begin{equation}
  O(np^2 + np + p^2) = O((n + 1)p^2 + np),
\tag{4.2}
\end{equation}\]</span>
making <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> quadratic in the number of parameters and linear in the sample size. We can expect it to be faster
than the closed-form estimator as <span class="math inline">\(p\)</span> grows (<span class="math inline">\(O(p^2)\)</span> instead of <span class="math inline">\(O(p^3)\)</span>), but we cannot say which approach is faster
as <span class="math inline">\(n\)</span> grows because they are both <span class="math inline">\(O(n)\)</span>.</p>
<div style="page-break-after: always;"/>
<p>Therefore, which algorithm is best depends on the data we expect to work on:</p>
<ul>
<li>if <span class="math inline">\(n \to \infty\)</span> but <span class="math inline">\(p\)</span> is bounded, <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> perform similarly well;</li>
<li>if <span class="math inline">\(p \to \infty\)</span>, <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> will be faster.</li>
</ul>
<p>How well do the time complexities in <a href="algorithms.html#eq:betaEX">(4.1)</a> and <a href="algorithms.html#eq:betaQR">(4.2)</a> map to real-world running times? We can
answer this question by benchmarking <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> as we discussed in Section <a href="algorithms.html#bigO-benchmark">4.3</a>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="algorithms.html#cb19-1" aria-hidden="true"/><span class="kw">library</span>(microbenchmark)</span>
<span id="cb19-2"><a href="algorithms.html#cb19-2" aria-hidden="true"/><span class="kw">library</span>(doBy)</span>
<span id="cb19-3"><a href="algorithms.html#cb19-3" aria-hidden="true"/></span>
<span id="cb19-4"><a href="algorithms.html#cb19-4" aria-hidden="true"/><span class="co"># define the estimators.</span></span>
<span id="cb19-5"><a href="algorithms.html#cb19-5" aria-hidden="true"/>betaEX =<span class="st"> </span><span class="cf">function</span>(y, X) <span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y</span>
<span id="cb19-6"><a href="algorithms.html#cb19-6" aria-hidden="true"/>betaQR =<span class="st"> </span><span class="cf">function</span>(y, X) <span class="kw">qr.solve</span>(X, y)</span>
<span id="cb19-7"><a href="algorithms.html#cb19-7" aria-hidden="true"/><span class="co"># define the grid of input sizes to examine.</span></span>
<span id="cb19-8"><a href="algorithms.html#cb19-8" aria-hidden="true"/>nn =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>) <span class="op">*</span><span class="st"> </span><span class="dv">1000</span></span>
<span id="cb19-9"><a href="algorithms.html#cb19-9" aria-hidden="true"/>pp =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>)</span>
<span id="cb19-10"><a href="algorithms.html#cb19-10" aria-hidden="true"/><span class="co"># a data frame to store the running times.</span></span>
<span id="cb19-11"><a href="algorithms.html#cb19-11" aria-hidden="true"/>time =<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb19-12"><a href="algorithms.html#cb19-12" aria-hidden="true"/>  <span class="kw">expand.grid</span>(<span class="dt">n =</span> nn, <span class="dt">p =</span> pp, <span class="dt">betahat =</span> <span class="kw">c</span>(<span class="st">"EX"</span>, <span class="st">"QR"</span>)),</span>
<span id="cb19-13"><a href="algorithms.html#cb19-13" aria-hidden="true"/>  <span class="dt">lq =</span> <span class="ot">NA</span>, <span class="dt">mean =</span> <span class="ot">NA</span>, <span class="dt">uq =</span> <span class="ot">NA</span></span>
<span id="cb19-14"><a href="algorithms.html#cb19-14" aria-hidden="true"/>)</span>
<span id="cb19-15"><a href="algorithms.html#cb19-15" aria-hidden="true"/><span class="co"># quantiles defining a 90% confidence band.</span></span>
<span id="cb19-16"><a href="algorithms.html#cb19-16" aria-hidden="true"/>lq =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">quantile</span>(x, <span class="fl">0.05</span>)</span>
<span id="cb19-17"><a href="algorithms.html#cb19-17" aria-hidden="true"/>uq =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">quantile</span>(x, <span class="fl">0.95</span>)</span>
<span id="cb19-18"><a href="algorithms.html#cb19-18" aria-hidden="true"/><span class="co"># for all combinations of input sizes...</span></span>
<span id="cb19-19"><a href="algorithms.html#cb19-19" aria-hidden="true"/><span class="cf">for</span> (n <span class="cf">in</span> nn) {</span>
<span id="cb19-20"><a href="algorithms.html#cb19-20" aria-hidden="true"/>  <span class="cf">for</span> (p <span class="cf">in</span> pp) {</span>
<span id="cb19-21"><a href="algorithms.html#cb19-21" aria-hidden="true"/>    <span class="co"># ... measure the running time averaging over 100 runs...</span></span>
<span id="cb19-22"><a href="algorithms.html#cb19-22" aria-hidden="true"/>    bench =<span class="st"> </span><span class="kw">microbenchmark</span>(<span class="kw">betaEX</span>(y, X), <span class="kw">betaQR</span>(y, X),</span>
<span id="cb19-23"><a href="algorithms.html#cb19-23" aria-hidden="true"/>              <span class="dt">times =</span> <span class="dv">100</span>,</span>
<span id="cb19-24"><a href="algorithms.html#cb19-24" aria-hidden="true"/>              <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">warmup =</span> <span class="dv">10</span>),</span>
<span id="cb19-25"><a href="algorithms.html#cb19-25" aria-hidden="true"/>              <span class="dt">setup =</span> {</span>
<span id="cb19-26"><a href="algorithms.html#cb19-26" aria-hidden="true"/>                X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow =</span> n, <span class="dt">ncol =</span> p)</span>
<span id="cb19-27"><a href="algorithms.html#cb19-27" aria-hidden="true"/>                y =<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">ncol</span>(X))</span>
<span id="cb19-28"><a href="algorithms.html#cb19-28" aria-hidden="true"/>              })</span>
<span id="cb19-29"><a href="algorithms.html#cb19-29" aria-hidden="true"/>    <span class="co"># ... and save the results for later analyses.</span></span>
<span id="cb19-30"><a href="algorithms.html#cb19-30" aria-hidden="true"/>    time[time<span class="op">$</span>n <span class="op">==</span><span class="st"> </span>n <span class="op">&amp;</span><span class="st"> </span>time<span class="op">$</span>p <span class="op">==</span><span class="st"> </span>p, <span class="kw">c</span>(<span class="st">"lq"</span>, <span class="st">"mean"</span>, <span class="st">"uq"</span>)] =</span>
<span id="cb19-31"><a href="algorithms.html#cb19-31" aria-hidden="true"/><span class="st">      </span><span class="kw">summaryBy</span>(time <span class="op">~</span><span class="st"> </span>expr, <span class="dt">data =</span> bench,</span>
<span id="cb19-32"><a href="algorithms.html#cb19-32" aria-hidden="true"/>                <span class="dt">FUN =</span> <span class="kw">c</span>(lq, mean, uq))[, <span class="dv">-1</span>]</span>
<span id="cb19-33"><a href="algorithms.html#cb19-33" aria-hidden="true"/>  }<span class="co">#FOR</span></span>
<span id="cb19-34"><a href="algorithms.html#cb19-34" aria-hidden="true"/>}<span class="co">#FOR</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ols-microbenchmark-plots"/>
<img src="../Images/83d64531eef1af66eb66b554672c31d1.png" alt="Marginal running times as a function of $n$ (with $p = 200$, top left) and of $p$ (with $n = 50000$, top right); the closed-form formula is shown in orange and the QR estimator in blue, each with 90% confidence bands. Joint running times in $(n, p)$ are shown in the bottom left and right panels for the closed-form formula and the QR estimator, respectively." width="672" data-original-src="https://ppml.dev/chapter04_files/figure-html/ols-microbenchmark-plots-1.png"/>
<p class="caption">
Figure 4.2: Marginal running times as a function of <span class="math inline">\(n\)</span> (with <span class="math inline">\(p = 200\)</span>, top left) and of <span class="math inline">\(p\)</span> (with <span class="math inline">\(n = 50000\)</span>, top right); the closed-form formula is shown in orange and the QR estimator in blue, each with 90% confidence bands. Joint running times in <span class="math inline">\((n, p)\)</span> are shown in the bottom left and right panels for the closed-form formula and the QR estimator, respectively.
</p>
</div>
<p>We plotted the results in Figure <a href="algorithms.html#fig:ols-microbenchmark-plots">4.2</a>. The top panels confirm the conclusions we reached
earlier: both <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> have a time complexity that is linear in <span class="math inline">\(n\)</span>, hence they scale similarly; but
<span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> has a cubic time complexity in <span class="math inline">\(p\)</span>, which makes it much slower than <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> as <span class="math inline">\(p\)</span> increases. The level
plots in the bottom panels show that <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> jointly influence running times, which we should expect since both
<a href="algorithms.html#eq:betaEX">(4.1)</a> and <a href="algorithms.html#eq:betaQR">(4.2)</a> contain mixed terms in which both <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> appear.</p>
<p>Considering how little noise is in the running times we measured, we can reliably estimate the coefficients of the terms
in <a href="algorithms.html#eq:betaEX">(4.1)</a> and <a href="algorithms.html#eq:betaQR">(4.2)</a> using a simple linear regression.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="algorithms.html#cb20-1" aria-hidden="true"/><span class="co"># rescale to make the coefficients easier to interpret.</span></span>
<span id="cb20-2"><a href="algorithms.html#cb20-2" aria-hidden="true"/>time<span class="op">$</span>mean =<span class="st"> </span>time<span class="op">$</span>mean <span class="op">*</span><span class="st"> </span><span class="dv">10</span><span class="op">^</span>(<span class="op">-</span><span class="dv">9</span>)</span>
<span id="cb20-3"><a href="algorithms.html#cb20-3" aria-hidden="true"/>time<span class="op">$</span>n =<span class="st"> </span>time<span class="op">$</span>n <span class="op">/</span><span class="st"> </span><span class="dv">1000</span></span>
<span id="cb20-4"><a href="algorithms.html#cb20-4" aria-hidden="true"/>bigO.EX =<span class="st"> </span><span class="kw">lm</span>(mean <span class="op">~</span><span class="st"> </span><span class="kw">I</span>(p<span class="op">^</span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>((n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>p<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(n <span class="op">*</span><span class="st"> </span>p),</span>
<span id="cb20-5"><a href="algorithms.html#cb20-5" aria-hidden="true"/>             <span class="dt">data =</span> <span class="kw">subset</span>(time, betahat <span class="op">==</span><span class="st"> "EX"</span>))</span>
<span id="cb20-6"><a href="algorithms.html#cb20-6" aria-hidden="true"/><span class="kw">coefficients</span>(bigO.EX)</span></code></pre></div>
<pre><code>##      (Intercept)           I(p^3) I((n + 1) * p^2)
##    -0.0120329302    -0.0000000025     0.0000017156
##         I(n * p)
##     0.0000244271</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="algorithms.html#cb22-1" aria-hidden="true"/>bigO.QR =<span class="st"> </span><span class="kw">lm</span>(mean <span class="op">~</span><span class="st"> </span><span class="kw">I</span>((n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>p<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(n <span class="op">*</span><span class="st"> </span>p),</span>
<span id="cb22-2"><a href="algorithms.html#cb22-2" aria-hidden="true"/>             <span class="dt">data =</span> <span class="kw">subset</span>(time, betahat <span class="op">==</span><span class="st"> "QR"</span>))</span>
<span id="cb22-3"><a href="algorithms.html#cb22-3" aria-hidden="true"/><span class="kw">coefficients</span>(bigO.QR)</span></code></pre></div>
<pre><code>##      (Intercept) I((n + 1) * p^2)         I(n * p)
##       -0.1103315        0.0000013        0.0000502</code></pre>
<p>The models in <code>bigO.EX</code> and <code>bigO.QR</code> allow us to predict the running times for any combinations of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. We
can also use them to check how much the empirical complexity curves they encode overlap the corresponding empirical
running times and check for outliers.</p>
<p>As a final note, there are several additional considerations we should weigh before choosing which algorithm to use: the
matrix inverse in <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> is known to be numerically unstable, which is why <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span> is preferred in scientific
software. <code>lm()</code> in both R and Julia, <code>fitlm()</code> in MATLAB are implemented on top of QR but, interestingly,
<code>LinearRegression()</code> in Scikit-learn <span class="citation">(Scikit-learn Developers <a href="#ref-sklearn" role="doc-biblioref">2022</a>)</span> is not.</p>
</div>
<div id="bigO-sparsem" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Sparse Matrices Representation<a href="algorithms.html#bigO-sparsem" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>
Matrices in which most cells are zero are called sparse matrices, as opposed to dense matrices in which most or all
elements are non-zero. In the case of dense (<span class="math inline">\(m \times n\)</span>) matrices, we need to store in memory the values of all the
cells, which means that their complexity is <span class="math inline">\(O(mn)\)</span>. However, in the case of sparse matrices, we can just store the
non-zero values and their coordinates, with the understanding that all other cells are equal to zero.</p>
<p>R provides several such representations in the Matrix package <span class="citation">(Bates and Maechler <a href="#ref-matrixpkg" role="doc-biblioref">2021</a>)</span>, and Python does the same in scipy <span class="citation">(Virtanen et al. <a href="#ref-scipy" role="doc-biblioref">2020</a>)</span>:
the default is the column-oriented, compressed format described in Section <a href="types-structures.html#matrices">3.2.3</a>. Consider the matrix
originally shown in Figure <a href="types-structures.html#fig:matrices">3.4</a>, in which 9 cells out of 15 are zeroes.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="algorithms.html#cb24-1" aria-hidden="true"/><span class="kw">library</span>(Matrix)</span>
<span id="cb24-2"><a href="algorithms.html#cb24-2" aria-hidden="true"/>m =<span class="st"> </span><span class="kw">Matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="op">:</span><span class="dv">0</span>), <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb24-3"><a href="algorithms.html#cb24-3" aria-hidden="true"/>m</span></code></pre></div>
<pre><code>## 3 x 5 sparse Matrix of class "dgCMatrix"
##
## [1,] . 1 . . 2
## [2,] . . 2 . 1
## [3,] 2 . 1 . .</code></pre>
<p>How are the elements of <code>m</code> stored in memory?</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="algorithms.html#cb26-1" aria-hidden="true"/><span class="kw">str</span>(m)</span></code></pre></div>
<pre><code>## Formal class 'dgCMatrix' [package "Matrix"] with 6 slots
##   ..@ i       : int [1:6] 2 0 1 2 0 1
##   ..@ p       : int [1:6] 0 1 2 4 4 6
##   ..@ Dim     : int [1:2] 3 5
##   ..@ Dimnames:List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##   ..@ x       : num [1:6] 2 1 2 1 2 1
##   ..@ factors : list()</code></pre>
<p>Comparing the notation in Section <a href="types-structures.html#matrices">3.2.3</a> and the documentation of Matrix, we can see that:</p>
<ul>
<li><code>i</code> contains the vector <span class="math inline">\(R\)</span>, row coordinates of the non-zero cells in the matrix;</li>
<li><code>p</code> contains the vector <span class="math inline">\(C\)</span>, the start and end indexes of the columns; and</li>
<li><code>x</code> contains the vector <span class="math inline">\(V\)</span> of the values of the non-zero cells.</li>
</ul>
<p>Note that both <code>i</code> and <code>p</code> are 0-based indexes to facilitate the use of the <code>dgCMatrix</code> class in the code inside
the Matrix package, while R uses 1-based indexing.</p>
<p>The overall space complexity of such a sparse matrix is then <span class="math inline">\(O(3z)\)</span>, where <span class="math inline">\(z\)</span> is the number of non-zero cells. R
stores real numbers in double precision (64 bits each) and indexes as 32-bits integers, which means <code>m</code> needs 128 bits
(16 bytes) of memory for each non-zero cell. So dense matrices use <span class="math inline">\(8mn\)</span> bytes of memory while sparse matrices use <span class="math inline">\(16z\)</span>
bytes; if <span class="math inline">\(z \ll mn\)</span> we can save most of the memory we would have allocated for a dense matrix.</p>
<p>The catch is that operations may have a higher time complexity for sparse matrices than for dense matrices. Even the
most simple: looking up the value of a cell <span class="math inline">\((i, j)\)</span> has time complexity <span class="math inline">\(O(1)\)</span> in a dense matrix, but for a sparse
matrix such as <code>m</code> we need to:</p>
<ul>
<li>Look up what are the first and the last values in <code>x</code> for the column <span class="math inline">\(j\)</span> by reading the <span class="math inline">\(j\)</span>th element of <code>p</code>.</li>
<li>Position ourselves on the row number for that first value in <code>i</code>, and read every successive number until we find the
row number <span class="math inline">\(j\)</span> or we reach the end of the column.</li>
<li>Read the value of the cell from <code>x</code>, which has the same position in <code>x</code> as the row number in <code>i</code>; or return zero if we
reach the end of the column.</li>
</ul>
<p>These three steps have an overall time complexity of <span class="math inline">\(O(1) + O(z/n) + O(1) = O(2 + z/n)\)</span> assuming <span class="math inline">\(z/n\)</span> non-zero
elements per column on average. Hence reading a cell from a sparse matrix appears to be more expensive than reading the
same cell from a dense matrix.</p>
<p>Assigning a non-zero value to a cell in a sparse matrix is more expensive as well. For each of <code>i</code> and <code>x</code> we need to:</p>
<ul>
<li>allocate a new array of length <span class="math inline">\(z + 1\)</span>;</li>
<li>copy the <span class="math inline">\(z\)</span> values from the old array;</li>
<li>add the values corresponding to the new cell;</li>
<li>replace the old array with the new one.</li>
</ul>
<p>Therefore, time and space complexity both are <span class="math inline">\(O(z)\)</span>. Handling <code>p</code> is also <span class="math inline">\(O(z)\)</span>, and it is more complex since we will
need to recompute half of the values on average.</p>
<p>This is troubling because we cannot predict the average time and space complexity just by looking at the input size: we
can only do that by making assumptions on the distribution of the values in the cells. Furthermore, this distribution
may change during the execution of our software as we assign new non-zero values to the sparse matrix. We can, however,
compare the empirical running times of read and write operations on dense and sparse matrices to get a practical
understanding of how they differ, as we did in the previous section with <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{EX}}\)</span> and <span class="math inline">\(\boldsymbol{\widehat{\beta}}_{\mathrm{QR}}\)</span>.</p>
<p>Consider a square matrix allocated as either a sparse or a dense matrix with <span class="math inline">\(n = 1000\)</span> and the proportions of non-zero
cells of <span class="math inline">\(z/n = \{ 0.01, 0.02, 0.05,\)</span> <span class="math inline">\(0.10, 0.20, 0.50, 1\}\)</span>. We can measure the time it takes to read (with the
<code>read10()</code> function) or write (with the <code>write10()</code> function) the values of 10 random cells for either type of matrix as
<span class="math inline">\(z\)</span> (<code>zz</code> in the code below) increases.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="algorithms.html#cb28-1" aria-hidden="true"/>read10 =<span class="st"> </span><span class="cf">function</span>(m) m[<span class="kw">sample</span>(<span class="kw">length</span>(m), <span class="dv">10</span>)]</span>
<span id="cb28-2"><a href="algorithms.html#cb28-2" aria-hidden="true"/>write10 =<span class="st"> </span><span class="cf">function</span>(m) m[<span class="kw">sample</span>(<span class="kw">length</span>(m), <span class="dv">10</span>)] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb28-3"><a href="algorithms.html#cb28-3" aria-hidden="true"/>[...]</span>
<span id="cb28-4"><a href="algorithms.html#cb28-4" aria-hidden="true"/><span class="cf">for</span> (z <span class="cf">in</span> zz) {</span>
<span id="cb28-5"><a href="algorithms.html#cb28-5" aria-hidden="true"/>  bench =<span class="st"> </span><span class="kw">microbenchmark</span>(</span>
<span id="cb28-6"><a href="algorithms.html#cb28-6" aria-hidden="true"/>            <span class="kw">read10</span>(sparse), <span class="kw">read10</span>(dense),</span>
<span id="cb28-7"><a href="algorithms.html#cb28-7" aria-hidden="true"/>            <span class="kw">write10</span>(sparse), <span class="kw">write10</span>(dense),</span>
<span id="cb28-8"><a href="algorithms.html#cb28-8" aria-hidden="true"/>            <span class="dt">times =</span> <span class="dv">200</span>,</span>
<span id="cb28-9"><a href="algorithms.html#cb28-9" aria-hidden="true"/>            <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">warmup =</span> <span class="dv">10</span>),</span>
<span id="cb28-10"><a href="algorithms.html#cb28-10" aria-hidden="true"/>            <span class="dt">setup =</span> {</span>
<span id="cb28-11"><a href="algorithms.html#cb28-11" aria-hidden="true"/>              sparse =<span class="st"> </span><span class="kw">Matrix</span>(<span class="dv">0</span>, n, n)</span>
<span id="cb28-12"><a href="algorithms.html#cb28-12" aria-hidden="true"/>              sparse[<span class="kw">sample</span>(<span class="kw">length</span>(sparse), <span class="kw">round</span>(z))] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb28-13"><a href="algorithms.html#cb28-13" aria-hidden="true"/>              dense =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, n)</span>
<span id="cb28-14"><a href="algorithms.html#cb28-14" aria-hidden="true"/>            })</span>
<span id="cb28-15"><a href="algorithms.html#cb28-15" aria-hidden="true"/>  [...]</span>
<span id="cb28-16"><a href="algorithms.html#cb28-16" aria-hidden="true"/>}<span class="co">#FOR</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:sparsem-microbenchmark-plots"/>
<img src="../Images/a176234975fbcaaf86df1216a8c59bb1.png" alt="Read (left) and write (right panel) performance for sparse (orange) and dense (blue) square matrices of size 1000. 90% confidence bars are so thin as to not be visible." width="672" data-original-src="https://ppml.dev/chapter04_files/figure-html/sparsem-microbenchmark-plots-1.png"/>
<p class="caption">
Figure 4.3: Read (left) and write (right panel) performance for sparse (orange) and dense (blue) square matrices of size 1000. 90% confidence bars are so thin as to not be visible.
</p>
</div>
<p>The resulting running times are shown in Figure <a href="algorithms.html#fig:sparsem-microbenchmark-plots">4.3</a>. As we expected, both reading
from and writing to a dense matrix is <span class="math inline">\(O(1)\)</span>: running times do not change as <span class="math inline">\(z\)</span> increases. This is not true in the case
of a sparse matrix. The running time of <code>write10()</code> increases linearly in <span class="math inline">\(z/n\)</span>, and so does that of <code>read10()</code> until
<span class="math inline">\(z/n = 0.5\)</span>. Reading times do not increase further for <span class="math inline">\(z/n = 1\)</span>: in fact, they decrease slightly. We can interpret this
as <span class="math inline">\(O(2 + z/n)\)</span> converging to a constant <span class="math inline">\(O(3)\)</span> as <span class="math inline">\(z/n \to 1\)</span>, making a (no longer) sparse matrix just an inefficient
dense matrix that requires extra coordinate look-ups.</p>
<p>Finally, we can regress the running times (in seconds) on <span class="math inline">\(O(z/n)\)</span> to determine the slope of the linear trends we see
for sparse matrices, that is, the orange lines in the two panels of Figure <a href="algorithms.html#fig:sparsem-microbenchmark-plots">4.3</a>. For
writing performance, the slope is
0.09; while for reading
performance it is 0.03
if we consider only <span class="math inline">\(z/n \leqslant 0.5\)</span>. Hence writing times increase by approximately 10% for every million cells in
the matrix, and read times by 3%. This, of course, is true for large matrices with millions of elements; performance may
very well be different for smaller matrices with just a few tens of elements.
</p>
</div>
<div id="bigO-dags" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Uniform Simulations of Directed Acyclic Graphs<a href="algorithms.html#bigO-dags" class="anchor-section" aria-label="Anchor link to header"/></h3>
<p>Many complex probabilistic models can be represented graphically as <em>directed acyclic graphs</em> (DAGs), in which each node
is associated with a random variable and arcs represent dependence relationships between those variables: notable
examples are Bayesian networks <span class="citation">(Scutari and Denis <a href="#ref-scutari" role="doc-biblioref">2021</a>)</span>, neural networks <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow" role="doc-biblioref">2016</a>)</span>, Bayesian hierarchical models <span class="citation">(Gelman et al. <a href="#ref-gelman" role="doc-biblioref">2013</a>)</span> and
vector auto-regressive (VAR) time series <span class="citation">(Tsay <a href="#ref-tsay" role="doc-biblioref">2010</a>)</span>. The DAGs make it possible to divide and conquer large multivariate
distributions into smaller ones in which each variable only depends on its parents.</p>
<p>When evaluating various aspects of these models, it may be useful to be able to generate random DAGs to use in
simulation studies. In particular, we may want to generate DAGs with uniform probability since this is considered a
non-informative prior distribution on the space of the possible model structures. A simple MCMC algorithm to do this is
illustrated in <span class="citation">(Melançon, Dutour, and Bousquet-Mélou <a href="#ref-melancon" role="doc-biblioref">2001</a>)</span>. We wrote it down as pseudocode in Algorithm <a href="algorithms.html#tab:melancon">4.1</a>. For simplicity, we omit
both <em>burn-in</em> (dropping the DAGs generated in the initial iterations to give time to the algorithm to converge to the
uniform distributions over DAGs) and <em>thinning</em> (returning only one DAG every several generated DAGs to return a set of
DAGs that are more nearly independent) even though both are standard practice in the literature.</p>
<table>
<caption><span id="tab:melancon">Table 4.1: </span>Random DAG Generation</caption>
<colgroup>
<col width="100%"/>
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><strong>Algorithm 4.1</strong> Random DAG Generation</td>
</tr>
<tr class="even">
<td align="left"><strong>Input:</strong> a set of nodes <span class="math inline">\(\mathbf{V}\)</span> (possibly with associated labels), the number <span class="math inline">\(N\)</span> of graphs to generate.</td>
</tr>
<tr class="odd">
<td align="left"><strong>Output:</strong> a set <span class="math inline">\(\mathbf{G}\)</span> of <span class="math inline">\(N\)</span> directed acyclic graphs.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\quad\)</span> 1. Initialise an empty graph with nodes <span class="math inline">\(\mathbf{V}\)</span> and arcs <span class="math inline">\(A_0 = \{\varnothing\}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\quad\)</span> 2. Initialise an empty set of graphs <span class="math inline">\(\mathbf{G}\)</span>.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\quad\)</span> 3. For a large number of iterations <span class="math inline">\(n = 1, \ldots, N\)</span>:</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\qquad\)</span> (a) Sample two random nodes <span class="math inline">\(v_i\)</span> and <span class="math inline">\(v_j \in \mathbf{V}\)</span> with <span class="math inline">\(v_i \neq v_j\)</span>.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\qquad\)</span> (b) If <span class="math inline">\(\{v_i \to v_j\} \in A_{n - 1}\)</span>, then <span class="math inline">\(A_{n} \leftarrow A_{n - 1} \setminus \{v_i \to v_j\}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\qquad\)</span> (c) If <span class="math inline">\(\{v_i \to v_j\} \not\in A_{n - 1}\)</span>, check whether the graph is still acyclic after adding <span class="math inline">\(\{v_i \to v_j\}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\qquad\qquad\)</span> i. If the graph is still acyclic, <span class="math inline">\(A_{n} \leftarrow A_{n - 1} \cup \{v_i \to v_j\}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\qquad\qquad\)</span> ii. If the graph is no longer acyclic, nothing is done.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\qquad\)</span> (d) <span class="math inline">\(\mathbf{G} \leftarrow \mathbf{G} \cup A_{n}\)</span>.</td>
</tr>
</tbody>
</table>
<p>How can we represent DAGs in this algorithm? Any graph is uniquely identified by its nodes <span class="math inline">\(\mathbf{V}\)</span> and its arc set <span class="math inline">\(A\)</span>. As
an example, consider a graph with nodes <span class="math inline">\(\mathbf{V}= \{ v_1, v_2, v_3, v_4 \}\)</span> and arcs <span class="math inline">\(\{ \{ v_1 \to v_3\},\)</span> <span class="math inline">\(\{v_2 \to v_3\},\)</span> <span class="math inline">\(\{v_3 \to v_4 \} \}\)</span>. Its <em>adjacency matrix</em> is a square matrix in which the cell <span class="math inline">\((i, j)\)</span> is equal to 1 if the
arc <span class="math inline">\(v_i \to v_j\)</span> is present in the DAG, and to 0 if it is not:
<span class="math display">\[\begin{equation*}
   \begin{bmatrix}
     0   &amp; 0   &amp; 1   &amp; 0 \\
     0   &amp; 0   &amp; 1   &amp; 0 \\
     0   &amp; 0   &amp; 0   &amp; 1 \\
     0   &amp; 0   &amp; 0   &amp; 0
   \end{bmatrix}.
 \end{equation*}\]</span>
We can store an adjacency matrix in a dense or a sparse matrix of size <span class="math inline">\(|\mathbf{V}|\)</span>: depending on how many arcs we expect to
see in the DAG, the trade-off between space and time complexity may be acceptable as discussed in Section
<a href="algorithms.html#bigO-sparsem">4.5.2</a>. The <em>adjacency list</em> of a graph is a set containing the children sets of each node:

<span class="math display">\[\begin{equation*}
  \left\{ v_1 = \{ v_3 \}, v_2 = \{ v_3 \}, v_3 = \{ v_4\}, v_4 = \varnothing \right\}.
\end{equation*}\]</span>
This representation is competitive with a sparse adjacency matrix in terms of space complexity: both are <span class="math inline">\(O(|A|)\)</span>, where
<span class="math inline">\(|A|\)</span> is the size of the arc set of the DAG (that is, the number of arcs it contains). If we assume that the DAGs
contain few arcs so that <span class="math inline">\(O(|A|) = O(|\mathbf{V}|)\)</span>, then space complexity is better than the <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> of adjacency matrices.
As for time complexity, path finding is <span class="math inline">\(O(|\mathbf{V}| + |A|)\)</span> in adjacency lists but <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> for adjacency matrices.
Adjacency matrices, on the other hand, allow for <span class="math inline">\(O(1)\)</span> arc insertion, arc deletion, and finding whether an arc is
present or not in the DAG. All these operations are either <span class="math inline">\(O(|\mathbf{V}|)\)</span> or <span class="math inline">\(O(|A|)\)</span> in adjacency lists.</p>
<p>For the moment, let’s represent DAGs with dense adjacency matrices. We can determine the time complexity of an MCMC step
in Algorithm <a href="algorithms.html#tab:melancon">4.1</a> as follows:</p>
<ul>
<li>For each iteration, adding and removing arcs is <span class="math inline">\(O(1)\)</span> since we just read or write a value in a specific cell of the
adjacency matrix.</li>
<li>Choosing a pair of nodes at random can also be considered <span class="math inline">\(O(1)\)</span>, since we choose two nodes regardless of <span class="math inline">\(|\mathbf{V}|\)</span> or
<span class="math inline">\(|A_n|\)</span>.</li>
<li>Both depth-first and breadth-first search have time complexity <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> since we have to scan the whole
adjacency matrix to look for each node’s children. We only perform such a search if we sample a candidate arc that
is not already present, because in order to include an arc <span class="math inline">\(v_i \to v_j\)</span> we must make sure that there is no path from
<span class="math inline">\(v_j\)</span> to <span class="math inline">\(v_i\)</span> in order to keep the DAG acyclic. That in turn happens with probability <span class="math inline">\(O(|A_n| / |\mathbf{V}|^2)\)</span>.</li>
</ul>
<div style="page-break-after: always;"/>
<p>The overall time complexity of Algorithm <a href="algorithms.html#tab:melancon">4.1</a> for <span class="math inline">\(N\)</span> MCMC iterations then is:
<span class="math display">\[\begin{equation*}
  O\left(N \left(1 + 1 + |\mathbf{V}|^2 \frac{|A_n|}{|\mathbf{V}|^2} \right)\right) \approx O(N|A_n|).
\end{equation*}\]</span>
However, we are assuming a uniform probability distribution over all possible DAGs: under this assumption <span class="citation">(Melançon, Dutour, and Bousquet-Mélou <a href="#ref-melancon" role="doc-biblioref">2001</a>)</span>
reports that <span class="math inline">\(O(|A_n|) \approx O(|\mathbf{V}|^2/4)\)</span>, making Algorithm <a href="algorithms.html#tab:melancon">4.1</a> <span class="math inline">\(O(N|\mathbf{V}|^2/4)\)</span>. If we assumed a
different probability distribution for the DAGs, the time complexity of the algorithm would change even if <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(N\)</span>
stayed the same because the average <span class="math inline">\(|A_n|\)</span> would be different. Furthermore, note that computing the overall time
complexity of Algorithm <a href="algorithms.html#tab:melancon">4.1</a> as <span class="math inline">\(N\)</span> times the complexity of an individual step implies that we are
assuming that all MCMC steps have the same time complexity. This is not exactly true because of the <span class="math inline">\(O(|A_n| / |\mathbf{V}|^2)\)</span>
term, which may be lower for early MCMC steps (when <span class="math inline">\(|A_n|\)</span> is bound by the number of steps <span class="math inline">\(n\)</span>) that for later steps
(when <span class="math inline">\(|A_n| \approx |\mathbf{V}|^2/4\)</span> because Algorithm <a href="algorithms.html#tab:melancon">4.1</a> has converged to the uniform distribution). It
is, however, a reasonable working assumption if <span class="math inline">\(N\)</span> is large enough and if most MCMC steps will be performed after
reaching the stationary distribution.</p>
<p>For each DAG we generate, we also have to consider the cost of saving it in a different data structure for later use.
Transforming the adjacency matrix into another data structure is necessarily <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> since we need to read every
cell in the adjacency matrix to find out which arcs are in the DAG. We do not always perform that transformation because
we may reject a new DAG instead of returning it, but it is difficult to evaluate how often that happens. A reasonable
guess is that we almost always save sparse graphs, since there will typically be no path between <span class="math inline">\(v_j\)</span> and <span class="math inline">\(v_i\)</span> (that
is the only case in which we do reject the current DAG proposal, and we do not need the transformation). As <span class="math inline">\(|A_n| \to |\mathbf{V}|\)</span> that condition will become easier to meet, so we can say that for a large number of iterations <span class="math inline">\(\approx O(|\mathbf{V}|^2 \cdot 0) = O(1)\)</span> for most iterations.</p>
<p>As for space complexity, the adjacency matrix uses <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> space: it is the most wasteful way of representing a
graph. Any other data structure we may save DAGs into will likely use less space.</p>
<p>We can investigate all the statements above as in Sections <a href="algorithms.html#bigO-lm">4.5.1</a> and <a href="algorithms.html#bigO-sparsem">4.5.2</a>.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="algorithms.html#cb29-1" aria-hidden="true"/><span class="kw">library</span>(bnlearn)</span>
<span id="cb29-2"><a href="algorithms.html#cb29-2" aria-hidden="true"/></span>
<span id="cb29-3"><a href="algorithms.html#cb29-3" aria-hidden="true"/>melancon =<span class="st"> </span><span class="cf">function</span>(nodes, n) {</span>
<span id="cb29-4"><a href="algorithms.html#cb29-4" aria-hidden="true"/>  <span class="co"># step (1)</span></span>
<span id="cb29-5"><a href="algorithms.html#cb29-5" aria-hidden="true"/>  dag =<span class="st"> </span><span class="kw">empty.graph</span>(nodes)</span>
<span id="cb29-6"><a href="algorithms.html#cb29-6" aria-hidden="true"/>  adjmat =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(nodes), <span class="dt">ncol =</span> <span class="kw">length</span>(nodes),</span>
<span id="cb29-7"><a href="algorithms.html#cb29-7" aria-hidden="true"/>             <span class="dt">dimnames =</span> <span class="kw">list</span>(nodes, nodes))</span>
<span id="cb29-8"><a href="algorithms.html#cb29-8" aria-hidden="true"/>  <span class="co"># step (2)</span></span>
<span id="cb29-9"><a href="algorithms.html#cb29-9" aria-hidden="true"/>  ret =<span class="st"> </span><span class="kw">vector</span>(n, <span class="dt">mode =</span> <span class="st">"list"</span>)</span>
<span id="cb29-10"><a href="algorithms.html#cb29-10" aria-hidden="true"/>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq</span>(n)) {</span>
<span id="cb29-11"><a href="algorithms.html#cb29-11" aria-hidden="true"/>    <span class="co"># step (3a)</span></span>
<span id="cb29-12"><a href="algorithms.html#cb29-12" aria-hidden="true"/>    candidate.arc =<span class="st"> </span><span class="kw">sample</span>(nodes, <span class="dv">2</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)</span>
<span id="cb29-13"><a href="algorithms.html#cb29-13" aria-hidden="true"/>    <span class="co"># step (3b)</span></span>
<span id="cb29-14"><a href="algorithms.html#cb29-14" aria-hidden="true"/>    <span class="cf">if</span> (adjmat[candidate.arc[<span class="dv">1</span>], candidate.arc[<span class="dv">2</span>]] <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</span>
<span id="cb29-15"><a href="algorithms.html#cb29-15" aria-hidden="true"/>      adjmat[candidate.arc[<span class="dv">1</span>], candidate.arc[<span class="dv">2</span>]] =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb29-16"><a href="algorithms.html#cb29-16" aria-hidden="true"/>      <span class="kw">amat</span>(dag) =<span class="st"> </span>adjmat</span>
<span id="cb29-17"><a href="algorithms.html#cb29-17" aria-hidden="true"/>    }<span class="co">#THEN</span></span>
<span id="cb29-18"><a href="algorithms.html#cb29-18" aria-hidden="true"/>    <span class="cf">else</span> {</span>
<span id="cb29-19"><a href="algorithms.html#cb29-19" aria-hidden="true"/>      <span class="co"># step (3c)</span></span>
<span id="cb29-20"><a href="algorithms.html#cb29-20" aria-hidden="true"/>      <span class="cf">if</span> (<span class="op">!</span><span class="kw">path.exists</span>(dag, <span class="dt">from =</span> candidate.arc[<span class="dv">2</span>],</span>
<span id="cb29-21"><a href="algorithms.html#cb29-21" aria-hidden="true"/>            <span class="dt">to =</span> candidate.arc[<span class="dv">1</span>])) {</span>
<span id="cb29-22"><a href="algorithms.html#cb29-22" aria-hidden="true"/>        adjmat[candidate.arc[<span class="dv">1</span>], candidate.arc[<span class="dv">2</span>]] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb29-23"><a href="algorithms.html#cb29-23" aria-hidden="true"/>        <span class="kw">amat</span>(dag) =<span class="st"> </span>adjmat</span>
<span id="cb29-24"><a href="algorithms.html#cb29-24" aria-hidden="true"/>      }<span class="co">#THEN</span></span>
<span id="cb29-25"><a href="algorithms.html#cb29-25" aria-hidden="true"/>    }<span class="co">#ELSE</span></span>
<span id="cb29-26"><a href="algorithms.html#cb29-26" aria-hidden="true"/>    <span class="co"># step (3d)</span></span>
<span id="cb29-27"><a href="algorithms.html#cb29-27" aria-hidden="true"/>    ret[[i]] =<span class="st"> </span>dag</span>
<span id="cb29-28"><a href="algorithms.html#cb29-28" aria-hidden="true"/>  }<span class="co">#FOR</span></span>
<span id="cb29-29"><a href="algorithms.html#cb29-29" aria-hidden="true"/>  <span class="kw">return</span>(ret)</span>
<span id="cb29-30"><a href="algorithms.html#cb29-30" aria-hidden="true"/>}<span class="co">#MELANCON</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:dags-plots"/>
<img src="../Images/5efa5dc6b2b3bb55321d8614e5e37a12.png" alt="Running times for Algorithm \@ref(tab:melancon) as a function of the number of nodes, generating 200 DAGs." width="576" data-original-src="https://ppml.dev/chapter04_files/figure-html/dags-plots-1.png"/>
<p class="caption">
Figure 4.4: Running times for Algorithm <a href="algorithms.html#tab:melancon">4.1</a> as a function of the number of nodes, generating 200 DAGs.
</p>
</div>
<p>How do time and space complexity change if we represent DAGs with adjacency lists instead? Both path finding (say, by
depth-first search) and saving DAGs in a different data structure have a time complexity of <span class="math inline">\(O(|\mathbf{V}| + |A_n|)\)</span> for
adjacency lists. However, the overall time complexity of each MCMC iteration is still quadratic:
<span class="math display">\[\begin{multline*}
  O\left(N \left(1 + 1 + (|\mathbf{V}| + |A_n|) \frac{|A_n|}{|\mathbf{V}|^2}\right)\right) \approx \\
    O\left(N \left(1 + 1 + (|\mathbf{V}| + |\mathbf{V}|^2) \frac{|\mathbf{V}|^2}{|\mathbf{V}|^2}\right)\right) \approx O(N|\mathbf{V}|^2),
\end{multline*}\]</span>
again assuming that <span class="math inline">\(O(|A_n|) \approx O(|\mathbf{V}|^2/4)\)</span>. The space complexity of an adjacency list is <span class="math inline">\(O(|\mathbf{V}| + |A_n|)\)</span>. On
average, this becomes <span class="math inline">\(O(|\mathbf{V}|^2)\)</span> under the same assumption.</p>
</div>
</div>
<div id="bigO-performance" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Big-<span class="math inline">\(O\)</span> Notation and Real-World Performance<a href="algorithms.html#bigO-performance" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>
Big-<span class="math inline">\(O\)</span> notation is a useful measure to assess scalability, and it can often be related to the practical performance of
real machine learning software. However, this becomes increasingly difficult as such software becomes more complex, for
several reasons. For instance:</p>
<ul>
<li>Software in a machine learning pipeline is heterogeneous: various parts are typically written in different programming
languages and are built on different libraries. Each part may be faster or slower than another because of that, even
when they belong to the same class of complexity. More on that in Section <a href="writing-code.html#programming-language">6.1</a>. Software
upgrades may also change the relative speeds of different parts of the pipeline and introduce new bottlenecks.</li>
<li>Both these things being equal, the same algorithm may be faster or slower depending on the data structures used to
store its inputs and outputs. We saw that to be the case in Section <a href="algorithms.html#bigO-sparsem">4.5.2</a>, and we touched on this point
in Section <a href="types-structures.html#right-data-structures">3.4</a> as well. The same is true for variable types, as discussed in Section
<a href="types-structures.html#right-variables">3.3</a>. High-level languages abstract these details to a large extent, which may lead to surprises
when benchmarking software.

</li>
<li>Differences in hardware can be impactful enough to completely hide differences in the class of complexity of different
algorithms for practical ranges of input sizes. Conversely, they can also introduce apparent discrepancies in
performance. This can realistically happen when the input sizes are small enough to make adjacent classes of
complexity comparable: an <span class="math inline">\(O(N^2)\)</span> algorithm on a CPU core will be slower than an <span class="math inline">\(O(N^3)\)</span> algorithm on a GPU with a
hundred of free units for <span class="math inline">\(N \leqslant 100\)</span>. Fixed costs that are ignored when deriving computational complexity may
also be relevant due to the relative difference, for instance, in the latency of different types of memory (Section
<a href="hardware.html#hardware-memory">2.1.2</a>).


</li>
<li>If we use any remote systems (Section <a href="hardware.html#hardware-cloud">2.3</a>), the hardware we are running a machine learning pipeline
on may vary without our knowledge, either in its configuration or in its overall load. Furthermore, benchmarking
remote systems accurately is inherently more difficult, as is troubleshooting them.
</li>
<li>The performance of some parts of the pipeline may be artificially limited by that of the external systems that provide
the inputs to the machine learning models or that consume their outputs. Individual parts of the same system may also
slow down each other as they consume each other’s outputs.</li>
</ul>
<p>To summarise, we may be able to map computational complexity to real-world performance for the individual components of
a machine learning pipeline running on simple hardware configurations. It is unlikely that we can do that with any degree
of accuracy when systems become larger and contain a larger number of software and hardware components. The resulting
complexity can easily make our expectations and intuitions about performance unreliable. In such a situation,
identifying performance issues requires measuring the current performance of each component as a baseline, identifying
which components are executed most often (which are sometimes said to be in the “critical path” or in the “hot path”)
and trying to redesign them to make them more efficient.
</p>
<!-- vim: set synmaxcol=600 textwidth=120 colorcolumn=120 spell wrap number: -->

</div>
</div>



    
</body>
</html>