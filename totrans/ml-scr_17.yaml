- en: Concept
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: 原文：[https://dafriedman97.github.io/mlbook/content/c4/concept.html](https://dafriedman97.github.io/mlbook/content/c4/concept.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://dafriedman97.github.io/mlbook/content/c4/concept.html](https://dafriedman97.github.io/mlbook/content/c4/concept.html)
- en: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\sumK}{\sum_{k
    = 1}^K} \newcommand{\sumk}{\sum_k} \newcommand{\prodN}{\prod_{n = 1}^N} \newcommand{\prodK}{\prod_{k
    = 1}^K} \newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bp}{\mathbf{p}} \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\btheta}{\boldsymbol{\theta}}
    \newcommand{\bmu}{\boldsymbol{\mu}} \newcommand{\bpi}{\boldsymbol{\pi}} \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
    \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}} \newcommand{\bSigma}{\boldsymbol{\Sigma}}
    \newcommand{\bT}{\mathbf{T}} \newcommand{\dadb}[2]{\frac{\partial #1}{\partial
    #2}} \newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}} \newcommand{\collection}{\{\bx_n,
    y_n\}_{n = 1}^N} \newcommand{\l}{\left(} \newcommand{\r}{\right)} \]'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \newcommand{\sumN}{\sum_{n = 1}^N} \newcommand{\sumn}{\sum_n} \newcommand{\sumK}{\sum_{k
    = 1}^K} \newcommand{\sumk}{\sum_k} \newcommand{\prodN}{\prod_{n = 1}^N} \newcommand{\prodK}{\prod_{k
    = 1}^K} \newcommand{\by}{\mathbf{y}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bx}{\mathbf{x}}
    \newcommand{\bp}{\mathbf{p}} \newcommand{\bbeta}{\boldsymbol{\beta}} \newcommand{\btheta}{\boldsymbol{\theta}}
    \newcommand{\bmu}{\boldsymbol{\mu}} \newcommand{\bpi}{\boldsymbol{\pi}} \newcommand{\bbetahat}{\boldsymbol{\hat{\beta}}}
    \newcommand{\bthetahat}{\boldsymbol{\hat{\theta}}} \newcommand{\bSigma}{\boldsymbol{\Sigma}}
    \newcommand{\bT}{\mathbf{T}} \newcommand{\dadb}[2]{\frac{\partial #1}{\partial
    #2}} \newcommand{\iid}{\overset{\small{\text{i.i.d.}}}{\sim}} \newcommand{\collection}{\{\bx_n,
    y_n\}_{n = 1}^N} \newcommand{\l}{\left(} \newcommand{\r}{\right)} \]'
- en: Discriminative classifiers, as we saw in the previous chapter, model a target
    variable as a direct function of one or more predictors. Generative classifiers,
    the subject of this chapter, instead view the predictors as being generated according
    to their class—i.e., they see the predictors as a function of the target, rather
    than the other way around. They then use Bayes’ rule to turn \(P(\bx_n|Y_n = k)\)
    into \(P(Y_n = k|\bx_n)\).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，判别分类器将目标变量建模为一个或多个预测器的直接函数。本章的主题生成分类器，则认为预测器是根据其类别生成的——即，它们将预测器视为目标函数，而不是相反。然后，它们使用贝叶斯定理将
    \(P(\bx_n|Y_n = k)\) 转换为 \(P(Y_n = k|\bx_n)\)。
- en: In generative classifiers, we view both the target and the predictors as random
    variables. We will therefore refer to the target variable with \(Y_n\), but in
    order to avoid confusing it with a matrix, we refer to the predictor vector with
    \(\bx_n\).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成分类器中，我们将目标和预测器视为随机变量。因此，我们将目标变量称为 \(Y_n\)，但为了避免与矩阵混淆，我们将预测器向量称为 \(\bx_n\)。
- en: Generative models can be broken down into the three following steps. Suppose
    we have a classification task with \(K\) unordered classes, represented by \(k
    = 1, \dots, K\).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以分解为以下三个步骤。假设我们有一个具有 \(K\) 个无序类别的分类任务，这些类别由 \(k = 1, \dots, K\) 表示。
- en: Estimate the density of the predictors conditional on the target belonging to
    each class. I.e., estimate \(p(\bx_n|Y_n = k)\) for \(k = 1, \dots, K\).
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标属于每个类别的条件下估计预测器的密度。即，估计 \(p(\bx_n|Y_n = k)\) 对于 \(k = 1, \dots, K\)。
- en: Estimate the prior probability that a target belongs to any given class. I.e.,
    estimate \(P(Y_n = k)\) for \(k = 1, \dots, K\). This is also written as \(p(Y_n)\).
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计目标属于任何给定类别的先验概率。即，估计 \(P(Y_n = k)\) 对于 \(k = 1, \dots, K\)。这也可以写成 \(p(Y_n)\)。
- en: Using Bayes’ rule, calculate the posterior probability that the target belongs
    to any given class. I.e., calculate \(p(Y_n = k|\bx_n) \propto p(\bx_n|Y_n = k)p(Y_n
    = k)\) for \(k = 1, \dots, K\).
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理计算目标属于任何给定类别的后验概率。即，计算 \(p(Y_n = k|\bx_n) \propto p(\bx_n|Y_n = k)p(Y_n
    = k)\) 对于 \(k = 1, \dots, K\)。
- en: We then classify observation \(n\) as being from the class for which \(P(Y_n
    = k|\bx_n)\) is greatest. In math,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将观察 \(n\) 分类为 \(P(Y_n = k|\bx_n)\) 最大的类别。在数学上，
- en: \[ \hat{Y}_n = \underset{k}{\text{arg max }} p(Y_n = k|\bx_n). \]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{Y}_n = \underset{k}{\text{arg max }} p(Y_n = k|\bx_n). \]
- en: Note that we do not need \(p(\bx_n)\), which would be the denominator in the
    Bayes’ rule formula, since it would be equal across classes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要 \(p(\bx_n)\)，这是贝叶斯定理公式中的分母，因为它在各个类别中是相等的。
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter is oriented differently from the others. The main methods discussed—Linear
    Discriminant Analysis, Quadratic Discriminant Analysis, and Naive Bayes—share
    much of the same structure. Rather than introducing each individually, we describe
    them together and note (in section 2.2) how they differ.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的侧重点与其他章节不同。讨论的主要方法——线性判别分析、二次判别分析和朴素贝叶斯——具有许多相同的结构。我们不是分别介绍它们，而是将它们一起描述，并在第2.2节中说明它们之间的区别。
- en: 1\. Model Structure
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 模型结构
- en: A generative classifier models two sources of randomness. First, we assume that
    out of the \(K\) possible classes, each observation belongs to class \(k\) independently
    with probability \(\pi_k\). In other words, letting \(\bpi =\begin{bmatrix} \pi_1
    & \dots & \pi_K\end{bmatrix}^\top \in \mathbb{R}^{K}\), we assume the prior
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式分类器模型了两种随机性来源。首先，我们假设在 \(K\) 个可能的类别中，每个观察值独立地属于类别 \(k\)，概率为 \(\pi_k\)。换句话说，让
    \(\bpi =\begin{bmatrix} \pi_1 & \dots & \pi_K\end{bmatrix}^\top \in \mathbb{R}^{K}\)，我们假设先验
- en: \[ y_n \iid \text{Cat}(\bpi). \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n \iid \text{Cat}(\bpi). \]
- en: See the math note below on the Categorical distribution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有关分类分布的数学注释请见下文。
- en: Math Note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: A random variable which takes on one of \(K\) discrete and unordered outcomes
    with probabilities \(\pi_1, \dots, \pi_K\) follows the Categorical distribution
    with parameter \(\bpi = \begin{bmatrix} \pi_1 & \dots & \pi_K \end{bmatrix}^\top\),
    written \(\text{Cat}(\bpi)\). For instance, a single die roll is distributed \(\text{Cat}(\bpi)\)
    for \(\bpi = \begin{bmatrix} 1/6 \dots 1/6 \end{bmatrix}^\top\).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机变量，取 \(K\) 个离散且无序的结果之一，其概率为 \(\pi_1, \dots, \pi_K\)，遵循参数为 \(\bpi = \begin{bmatrix}
    \pi_1 & \dots & \pi_K \end{bmatrix}^\top\) 的分类分布，记作 \(\text{Cat}(\bpi)\)。例如，单次掷骰子的分布为
    \(\text{Cat}(\bpi)\)，其中 \(\bpi = \begin{bmatrix} 1/6 \dots 1/6 \end{bmatrix}^\top\)。
- en: The density for \(Y \sim \text{Cat}(\bp)\) is defined as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \(Y \sim \text{Cat}(\bp)\) 的密度定义为
- en: \[\begin{split} \begin{align*} P(Y = 1) &= p_1 \\ &... \\ P(Y = K) &= p_K. \end{align*}
    \end{split}\]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} P(Y = 1) &= p_1 \\ &... \\ P(Y = K) &= p_K. \end{align*}
    \end{split}\]
- en: This can be written more compactly as
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以更简洁地写成
- en: \[ p(y) = \prod_{k = 1}^K p_k ^{I_k} \]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(y) = \prod_{k = 1}^K p_k ^{I_k} \]
- en: where \(I_k\) is an indicator that equals 1 if \(y = k\) and 0 otherwise.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(I_k\) 是一个指示变量，当 \(y = k\) 时等于 1，否则为 0。
- en: We then assume some distribution for \(\mathbf{x}_n\) conditional on observation
    \(n\)’s class, \(Y_n\). We typically assume all the \(\bx_n\) come from the same
    *family* of distributions, though the parameters depend on their class. For instance,
    we might have
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们假设在观察 \(n\) 的类别 \(Y_n\) 的条件下，\(\mathbf{x}_n\) 服从某种分布。我们通常假设所有 \(\bx_n\)
    都来自同一 *家族* 的分布，尽管参数依赖于它们的类别。例如，我们可能有
- en: \[\begin{split} \begin{align*} \bx_n|(Y_n = 1) &\sim \text{MVN}(\bmu_1, \bSigma_1),
    \\ &... \\ \bx_{n}|(Y_n = K) &\sim \text{MVN}(\bmu_K, \bSigma_K), \end{align*}
    \end{split}\]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \bx_n|(Y_n = 1) &\sim \text{MVN}(\bmu_1, \bSigma_1),
    \\ &... \\ \bx_{n}|(Y_n = K) &\sim \text{MVN}(\bmu_K, \bSigma_K), \end{align*}
    \end{split}\]
- en: though we wouldn’t let one conditional distribution be Multivariate Normal and
    another be Multivariate \(t\). Note that it is possible, however, for the individual
    variables within the random vector \(\bx_n\) to follow different distributions.
    For instance, if \(\bx_n = \begin{bmatrix} x_{n1} & x_{n2} \end{bmatrix}^\top\),
    we might have
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会让一个条件分布是多元正态分布，而另一个是多元 \(t\) 分布。注意，然而，随机向量 \(\bx_n\) 中的单个变量可以遵循不同的分布。例如，如果
    \(\bx_n = \begin{bmatrix} x_{n1} & x_{n2} \end{bmatrix}^\top\)，我们可能有
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \text{Bin}(n, p_k) \\
    x_{n2}|(Y_n = k) &\sim \mathcal{N}(\bmu_k, \bSigma_k) \end{align*} \end{split}\]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \text{Bin}(n, p_k) \\
    x_{n2}|(Y_n = k) &\sim \mathcal{N}(\bmu_k, \bSigma_k) \end{align*} \end{split}\]
- en: The machine learning task is to estimate the parameters of these models—\(\bpi\)
    for \(Y_n\) and whatever parameters might index the possible distributions of
    \(\bx_n|Y_n\), in this case \(\bmu_k\) and \(\bSigma_k\) for \(k = 1, \dots, K\).
    Once that’s done, we can estimate \(p(Y_n = k)\) and \(p(\bx_n|Y_n = k)\) for
    each class and, through Bayes’ rule, choose the class that maximizes \(p(Y_n =
    k|\bx_n)\).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务是对这些模型的参数进行估计——\(Y_n\) 的 \(\bpi\) 以及可能索引 \(\bx_n|Y_n\) 的可能分布的参数，在这种情况下是
    \(\bmu_k\) 和 \(\bSigma_k\) 对于 \(k = 1, \dots, K\)。一旦完成，我们就可以估计每个类别的 \(p(Y_n =
    k)\) 和 \(p(\bx_n|Y_n = k)\)，并通过贝叶斯定理选择最大化 \(p(Y_n = k|\bx_n)\) 的类别。
- en: 2\. Parameter Estimation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 参数估计
- en: 2.1 Class Priors
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 类别先验
- en: Let’s start by deriving the estimates for \(\bpi\), the class priors. Let \(I_{nk}\)
    be an indicator which equals 1 if \(Y_n = k\) and 0 otherwise. Then the joint
    likelihood and log-likelihood are given by
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先推导 \(\bpi\)，即类别先验的估计。令 \(I_{nk}\) 为一个指示变量，当 \(Y_n = k\) 时等于 1，否则为 0。那么联合似然和对数似然由以下给出
- en: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prodN \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n, Y_n\}_{n
    = 1}^N\right) &= \sumN \sum_{k = 1}^K I_{nk} \log(\pi_k) \\ &= \sum_{k = 1}^K
    N_k\log(\pi_k), \end{align*} \end{split}\]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prod_{n = 1}^N \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n,
    Y_n\}_{n = 1}^N\right) &= \sum_{n = 1}^N \sum_{k = 1}^K I_{nk} \log(\pi_k) \\
    &= \sum_{k = 1}^K N_k\log(\pi_k), \end{align*} \end{split}\]
- en: where \(N_k = \sumN I_{nk}\) gives the number of observations in class \(k\)
    for \(k = 1, \dots, K\).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(N_k = \sum_{n = 1}^N I_{nk}\) 给出了 \(k = 1, \dots, K\) 时类别 \(k\) 的观察数量。
- en: Math Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: The *Lagrangian function* provides a method for optimizing a function \(f(\bx)\)
    subject to the constraint \(g(\bx) = 0\). The Lagrangian is given by
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*拉格朗日函数* 提供了一种在约束 \(g(\bx) = 0\) 下优化函数 \(f(\bx)\) 的方法。拉格朗日函数由以下给出'
- en: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
- en: \(\lambda\) is known as the *Lagrange multiplier*. The critical points of \(f(\bx)\)
    (subject to the equality constraint) are found by setting the gradients of \(\mathcal{L}(\lambda,
    \bx)\) with respect to \(\lambda\) and \(\bx\) equal to 0.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lambda\) 被称为 *拉格朗日乘子*。通过将 \(\mathcal{L}(\lambda, \bx)\) 关于 \(\lambda\) 和
    \(\bx\) 的梯度设为 0，可以找到 \(f(\bx)\)（在等式约束下）的临界点。
- en: Noting the constraint \(\sum_{k = 1}^K \pi_k = 1\) (or equivalently \(\sum_{k
    = 1}^K\pi_k - 1 = 0\)), we can maximize the log-likelihood with the following
    Lagrangian.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到约束 \(\sum_{k = 1}^K \pi_k = 1\)（或者等价地 \(\sum_{k = 1}^K\pi_k - 1 = 0\)），我们可以通过以下拉格朗日乘子最大化对数似然。
- en: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \dadb{\mathcal{L}(\bpi)}{\pi_k} &= \frac{N_k}{\pi_k}
    - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\} \\ \dadb{\mathcal{L}(\bpi)}{\lambda}
    &= 1 - \sum_{k = 1}^K \pi_k. \\ \end{align*} \end{split}\]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \frac{\partial \mathcal{L}(\bpi)}{\partial
    \pi_k} &= \frac{N_k}{\pi_k} - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1,
    \dots, K\} \\ \frac{\partial \mathcal{L}(\bpi)}{\partial \lambda} &= 1 - \sum_{k
    = 1}^K \pi_k. \\ \end{align*} \end{split}\]
- en: 'This system of equations gives an intuitive solution:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程组给出了一种直观的解法：
- en: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
- en: which says that our estimate of \(p(Y_n = k)\) is just the sample fraction of
    observations from class \(k\).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们估计 \(p(Y_n = k)\) 的方法就是来自类别 \(k\) 的观察样本的样本比例。
- en: 2.2 Data Likelihood
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据似然
- en: The next step is to model the conditional distribution of \(\bx_n\) given \(Y_n\)
    so that we can estimate this distribution’s parameters. This of course depends
    on the family of distributions we choose to model \(\bx_n\). Three common approaches
    are detailed below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是建模给定 \(Y_n\) 的 \(\bx_n\) 的条件分布，以便我们可以估计这个分布的参数。这当然取决于我们选择用于建模 \(\bx_n\)
    的分布族。以下详细介绍了三种常见的方法。
- en: 2.2.1 Linear Discriminative Analysis (LDA)
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 线性判别分析 (LDA)
- en: In LDA, we assume
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LDA 中，我们假设
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
- en: for \(k = 1, \dots, K\). Note that each class has the same covariance matrix
    but a unique mean vector.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。注意，每个类别具有相同的协方差矩阵，但具有唯一的均值向量。
- en: Let’s derive the parameters in this case. First, let’s find the likelihood and
    log-likelihood. Note that we can write the joint likelihood as follows,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导出这种情况下的参数。首先，让我们找到似然和对数似然。注意，我们可以将联合似然写成以下形式，
- en: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prod_{n = 1}^N \prod_{k = 1}^K \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
- en: since \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) equals 1 if \(y_n
    \neq k\) and \(p(\bx_n|\bmu_k, \bSigma)\) otherwise. Then we plug in the Multivariate
    Normal PDF (dropping multiplicative constants) and take the log, as follows.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) 在 \(y_n \neq k\) 时等于
    1，而在 \(p(\bx_n|\bmu_k, \bSigma)\) 时则不等于 1。然后我们插入多元正态概率密度函数（省略乘性常数）并取对数，如下所示。
- en: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prod_{n
    = 1}^N \prod_{k = 1}^K \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n
    - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k
    = 1}^K, \bSigma\r &= \sum_{n = 1}^N \sum_{k = 1}^K I_{nk}\l-\frac{1}{2} \log|\bSigma|
    -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k) \r \end{align*}
    \end{split}\]
- en: Math Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: The following matrix derivatives will be of use for maximizing the above log-likelihood.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵导数将用于最大化上述对数似然。
- en: For any invertible matrix \(\mathbf{W}\),
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何可逆矩阵 \(\mathbf{W}\)，
- en: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
- en: where \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). It follows that
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). 因此，
- en: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
- en: We also have
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有
- en: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} \right] = -\mathbf{W}^{-\top}
    \bx \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
- en: For any symmetric matrix \(\mathbf{A}\),
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何对称矩阵 \(\mathbf{A}\)，
- en: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
- en: These results come from the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果来自 [矩阵手册](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)。
- en: Let’s start by estimating \(\bSigma\). First, simplify the log-likelihood to
    make the gradient with respect to \(\bSigma\) more apparent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先估计 \(\bSigma\)。首先，简化对数似然，以便更明显地看到关于 \(\bSigma\) 的梯度。
- en: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
- en: Then, using equations (2) and (3) from the *Math Note*, we get
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 *数学笔记* 中的方程 (2) 和 (3)，我们得到
- en: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
- en: 'Finally, we set this equal to 0 and multiply by \(\bSigma^{-1}\) on the left
    to solve for \(\hat{\bSigma}\):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将此等于 0 并将 \(\bSigma^{-1}\) 乘以左侧以求解 \(\hat{\bSigma}\)：
- en: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
- en: where \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: Now, to estimate the \(\bmu_k\), let’s look at each class individually. Let
    \(N_k\) be the number of observations in class \(k\) and \(C_k\) be the set of
    observations in class \(k\). Looking only at terms involving \(\bmu_k\), we get
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了估计 \(\bmu_k\)，让我们逐个查看每个类别。设 \(N_k\) 为类别 \(k\) 中的观测数，\(C_k\) 为类别 \(k\) 中的观测集合。仅考虑涉及
    \(\bmu_k\) 的项，我们得到
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
- en: Using equation (4) from the *Math Note*, we calculate the gradient as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *数学笔记* 中的方程 (4)，我们计算梯度如下
- en: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
- en: 'Setting this gradient equal to 0 and solving, we obtain our \(\bmu_k\) estimate:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将此梯度设为 0 并求解，我们得到 \(\bmu_k\) 的估计：
- en: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
- en: where \(\bar{\bx}_k\) is the element-wise sample mean of all \(\bx_n\) in class
    \(k\).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bar{\bx}_k\) 是类别 \(k\) 中所有 \(\bx_n\) 的逐元素样本均值。
- en: 2.2.2 Quadratic Discriminative Analysis (QDA)
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 二次判别分析 (QDA)
- en: 'QDA looks very similar to LDA but assumes each class has its *own* covariance
    matrix:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: QDA 看起来与 LDA 非常相似，但假设每个类别都有自己的协方差矩阵：
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
- en: 'for \(k = 1, \dots, K\). The log-likelihood is the same as in LDA except we
    replace \(\bSigma\) with \(\bSigma_k\):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。对数似然与 LDA 相同，只是将 \(\bSigma\) 替换为 \(\bSigma_k\)：
- en: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
- en: Again, let’s look at the parameters for each class individually. The log-likelihood
    for class \(k\) is given by
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们单独查看每个类别的参数。类别 \(k\) 的对数似然函数由以下给出
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
- en: We could take the gradient of this log-likelihood with respect to \(\bmu_k\)
    and set it equal to 0 to solve for \(\hat{\bmu}_k\). However, we can also note
    that our \(\hat{\bmu}_k\) estimate from the LDA approach will hold since this
    expression didn’t depend on the covariance term (which is the only thing we’ve
    changed). Therefore, we again get
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以取这个对数似然函数关于 \(\bmu_k\) 的梯度，并将其设为 0 来求解 \(\hat{\bmu}_k\)。然而，我们也可以注意到，从 LDA
    方法得到的 \(\hat{\bmu}_k\) 估计将保持不变，因为此表达式不依赖于协方差项（这是我们唯一改变的东西）。因此，我们再次得到
- en: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
- en: To estimate the \(\bSigma_k\), we take the gradient of the log-likelihood for
    class \(k\).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计 \(\bSigma_k\)，我们取类别 \(k\) 的对数似然函数的梯度。
- en: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
- en: 'Then we set this equal to 0 to solve for \(\hat{\bSigma}_k\):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这个梯度设为 0 来求解 \(\hat{\bSigma}_k\)：
- en: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
- en: where \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: 2.2.3 Naive Bayes
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 朴素贝叶斯
- en: Naive Bayes assumes the random variables within \(\bx_n\) are independent conditional
    on the class of observation \(n\). I.e. if \(\bx_n \in \mathbb{R}^D\), Naive Bayes
    assumes
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于朴素贝叶斯，假设在观察类别 \(n\) 的条件下，\(\bx_n\) 中的随机变量是独立的。即如果 \(\bx_n \in \mathbb{R}^D\)，朴素贝叶斯假设
- en: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
- en: This makes estimating \(p(\bx_n|Y_n)\) very easy—to estimate the parameters
    of \(p(x_{nd}|Y_n)\), we can ignore all the variables in \(\bx_{n}\) other than
    \(x_{nd}\)!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得估计 \(p(\bx_n|Y_n)\) 非常简单——为了估计 \(p(x_{nd}|Y_n)\) 的参数，我们可以忽略 \(\bx_{n}\) 中除了
    \(x_{nd}\) 以外的所有变量！
- en: As an example, assume \(\bx_n \in \mathbb{R}^2\) and we use the following model
    (where for simplicity \(n\) and \(\sigma^2_k\) are known).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 \(\bx_n \in \mathbb{R}^2\) 并且我们使用以下模型（为了简单起见，\(n\) 和 \(\sigma^2_k\) 是已知的）。
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
- en: Let the \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) contain all the parameters
    for class \(k\) . The joint likelihood function would become
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 令 \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) 包含类别 \(k\) 的所有参数。联合似然函数将变为
- en: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
- en: where the two are equal because of the Naive Bayes conditional independence
    assumption. This allows us to easily find maximum likelihood estimates. The rest
    of this sub-section demonstrates how those estimates would be found, though it
    is nothing beyond ordinary maximum likelihood estimation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯条件独立性假设，这两个是相等的。这使我们能够轻松找到最大似然估计。本小节的其余部分展示了如何找到这些估计，尽管这并不超出普通最大似然估计的范围。
- en: The log-likelihood is given by
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然由以下给出
- en: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
- en: 'As before, we estimate the parameters in each class by looking only at the
    terms in that class. Let’s look at the log-likelihood for class \(k\):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们通过只观察该类中的项来估计每个类别的参数。让我们看看类别 \(k\) 的对数似然：
- en: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
- en: Taking the derivative with respect to \(p_k\), we’re left with
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对 \(p_k\) 求导，我们得到
- en: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
- en: which, will give us \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) as
    usual. The same process would again give typical results for \(\mu_k\) and and
    \(\sigma^2_k\).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出 \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) 如常。同样的过程还会给出 \(\mu_k\)
    和 \(\sigma^2_k\) 的典型结果。
- en: 3\. Making Classifications
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 进行分类
- en: Regardless of our modeling choices for \(p(\bx_n|Y_n)\), classifying new observations
    is easy. Consider a test observation \(\bx_0\). For \(k = 1, \dots, K\), we use
    Bayes’ rule to calculate
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们对 \(p(\bx_n|Y_n)\) 的建模选择如何，对新观测值的分类都是容易的。考虑一个测试观测值 \(\bx_0\)。对于 \(k = 1,
    \dots, K\)，我们使用贝叶斯定理来计算
- en: \[\begin{split} \begin{align*} p(Y_0 = k|\bx_0) &\propto p(\bx_0|Y_0 = k)p(Y_0
    = k) \\ &= \hat{p}(\bx_0|Y_0 = k)\hat{\pi}_k, \end{align*} \end{split}\]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} p(Y_0 = k|\bx_0) &\propto p(\bx_0|Y_0 = k)p(Y_0
    = k) \\ &= \hat{p}(\bx_0|Y_0 = k)\hat{\pi}_k, \end{align*} \end{split}\]
- en: where \(\hat{p}\) gives the estimated density of \(\bx_0\) conditional on \(Y_0\).
    We then predict \(Y_0 = k\) for whichever value \(k\) maximizes the above expression.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\hat{p}\) 给出在 \(Y_0\) 条件下 \(\bx_0\) 的估计密度。然后我们预测 \(Y_0 = k\) 对于使上述表达式最大化的任何
    \(k\) 值。
- en: 1\. Model Structure
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 模型结构
- en: A generative classifier models two sources of randomness. First, we assume that
    out of the \(K\) possible classes, each observation belongs to class \(k\) independently
    with probability \(\pi_k\). In other words, letting \(\bpi =\begin{bmatrix} \pi_1
    & \dots & \pi_K\end{bmatrix}^\top \in \mathbb{R}^{K}\), we assume the prior
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生成分类器模型了两种随机性来源。首先，我们假设在 \(K\) 个可能的类别中，每个观测值独立地属于类别 \(k\) 的概率为 \(\pi_k\)。换句话说，让
    \(\bpi =\begin{bmatrix} \pi_1 & \dots & \pi_K\end{bmatrix}^\top \in \mathbb{R}^{K}\)，我们假设先验
- en: \[ y_n \iid \text{Cat}(\bpi). \]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: \[ y_n \iid \text{Cat}(\bpi). \]
- en: See the math note below on the Categorical distribution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有关分类分布的数学注释见下文。
- en: Math Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: A random variable which takes on one of \(K\) discrete and unordered outcomes
    with probabilities \(\pi_1, \dots, \pi_K\) follows the Categorical distribution
    with parameter \(\bpi = \begin{bmatrix} \pi_1 & \dots & \pi_K \end{bmatrix}^\top\),
    written \(\text{Cat}(\bpi)\). For instance, a single die roll is distributed \(\text{Cat}(\bpi)\)
    for \(\bpi = \begin{bmatrix} 1/6 \dots 1/6 \end{bmatrix}^\top\).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个取值为 \(K\) 个离散且无序结果的随机变量，其概率为 \(\pi_1, \dots, \pi_K\)，遵循参数为 \(\bpi = \begin{bmatrix}
    \pi_1 & \dots & \pi_K \end{bmatrix}^\top\) 的分类分布，写作 \(\text{Cat}(\bpi)\)。例如，单次掷骰子的分布为
    \(\text{Cat}(\bpi)\)，其中 \(\bpi = \begin{bmatrix} 1/6 \dots 1/6 \end{bmatrix}^\top\)。
- en: The density for \(Y \sim \text{Cat}(\bp)\) is defined as
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(Y \sim \text{Cat}(\bp)\) 的密度定义为
- en: \[\begin{split} \begin{align*} P(Y = 1) &= p_1 \\ &... \\ P(Y = K) &= p_K. \end{align*}
    \end{split}\]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} P(Y = 1) &= p_1 \\ &... \\ P(Y = K) &= p_K. \end{align*}
    \end{split}\]
- en: This can be written more compactly as
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以更紧凑地写成
- en: \[ p(y) = \prod_{k = 1}^K p_k ^{I_k} \]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(y) = \prod_{k = 1}^K p_k ^{I_k} \]
- en: where \(I_k\) is an indicator that equals 1 if \(y = k\) and 0 otherwise.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(I_k\) 是一个指示符，当 \(y = k\) 时等于 1，否则为 0。
- en: We then assume some distribution for \(\mathbf{x}_n\) conditional on observation
    \(n\)’s class, \(Y_n\). We typically assume all the \(\bx_n\) come from the same
    *family* of distributions, though the parameters depend on their class. For instance,
    we might have
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们假设在观察 \(n\) 的类别 \(Y_n\) 条件下，\(\mathbf{x}_n\) 服从某种分布。我们通常假设所有 \(\bx_n\)
    都来自同一 *族* 的分布，尽管参数取决于它们的类别。例如，我们可能会有
- en: \[\begin{split} \begin{align*} \bx_n|(Y_n = 1) &\sim \text{MVN}(\bmu_1, \bSigma_1),
    \\ &... \\ \bx_{n}|(Y_n = K) &\sim \text{MVN}(\bmu_K, \bSigma_K), \end{align*}
    \end{split}\]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \bx_n|(Y_n = 1) &\sim \text{MVN}(\bmu_1, \bSigma_1),
    \\ &... \\ \bx_{n}|(Y_n = K) &\sim \text{MVN}(\bmu_K, \bSigma_K), \end{align*}
    \end{split}\]
- en: though we wouldn’t let one conditional distribution be Multivariate Normal and
    another be Multivariate \(t\). Note that it is possible, however, for the individual
    variables within the random vector \(\bx_n\) to follow different distributions.
    For instance, if \(\bx_n = \begin{bmatrix} x_{n1} & x_{n2} \end{bmatrix}^\top\),
    we might have
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会让一个条件分布是多元正态分布，而另一个是多元 \(t\) 分布。然而，随机向量 \(\bx_n\) 中的各个变量可以遵循不同的分布。例如，如果
    \(\bx_n = \begin{bmatrix} x_{n1} & x_{n2} \end{bmatrix}^\top\)，我们可能会有
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \text{Bin}(n, p_k) \\
    x_{n2}|(Y_n = k) &\sim \mathcal{N}(\bmu_k, \bSigma_k) \end{align*} \end{split}\]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \text{Bin}(n, p_k) \\
    x_{n2}|(Y_n = k) &\sim \mathcal{N}(\bmu_k, \bSigma_k) \end{align*} \end{split}\]
- en: The machine learning task is to estimate the parameters of these models—\(\bpi\)
    for \(Y_n\) and whatever parameters might index the possible distributions of
    \(\bx_n|Y_n\), in this case \(\bmu_k\) and \(\bSigma_k\) for \(k = 1, \dots, K\).
    Once that’s done, we can estimate \(p(Y_n = k)\) and \(p(\bx_n|Y_n = k)\) for
    each class and, through Bayes’ rule, choose the class that maximizes \(p(Y_n =
    k|\bx_n)\).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务是要估计这些模型的参数——\(Y_n\) 的 \(\bpi\) 以及可能索引 \(\bx_n|Y_n\) 的可能分布的任何参数，在这种情况下是
    \(\bmu_k\) 和 \(\bSigma_k\) 对于 \(k = 1, \dots, K\)。一旦完成，我们就可以估计每个类别的 \(p(Y_n =
    k)\) 和 \(p(\bx_n|Y_n = k)\)，并通过贝叶斯定理选择最大化 \(p(Y_n = k|\bx_n)\) 的类别。
- en: 2\. Parameter Estimation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 参数估计
- en: 2.1 Class Priors
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 类别先验
- en: Let’s start by deriving the estimates for \(\bpi\), the class priors. Let \(I_{nk}\)
    be an indicator which equals 1 if \(Y_n = k\) and 0 otherwise. Then the joint
    likelihood and log-likelihood are given by
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先推导 \(\bpi\)，即类别先验的估计。令 \(I_{nk}\) 为一个指示变量，当 \(Y_n = k\) 时等于 1，否则为 0。然后，联合似然和对数似然由以下给出
- en: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prodN \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n, Y_n\}_{n
    = 1}^N\right) &= \sumN \sum_{k = 1}^K I_{nk} \log(\pi_k) \\ &= \sum_{k = 1}^K
    N_k\log(\pi_k), \end{align*} \end{split}\]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prodN \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n, Y_n\}_{n
    = 1}^N\right) &= \sumN \sum_{k = 1}^K I_{nk} \log(\pi_k) \\ &= \sum_{k = 1}^K
    N_k\log(\pi_k), \end{align*} \end{split}\]
- en: where \(N_k = \sumN I_{nk}\) gives the number of observations in class \(k\)
    for \(k = 1, \dots, K\).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(N_k = \sumN I_{nk}\) 给出了类别 \(k\) 中观察到的数量，对于 \(k = 1, \dots, K\)。
- en: Math Note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: The *Lagrangian function* provides a method for optimizing a function \(f(\bx)\)
    subject to the constraint \(g(\bx) = 0\). The Lagrangian is given by
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*拉格朗日函数* 提供了一种在约束 \(g(\bx) = 0\) 下优化函数 \(f(\bx)\) 的方法。拉格朗日函数由以下给出'
- en: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
- en: \(\lambda\) is known as the *Lagrange multiplier*. The critical points of \(f(\bx)\)
    (subject to the equality constraint) are found by setting the gradients of \(\mathcal{L}(\lambda,
    \bx)\) with respect to \(\lambda\) and \(\bx\) equal to 0.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lambda\) 被称为 *拉格朗日乘子*。通过将 \(\mathcal{L}(\lambda, \bx)\) 关于 \(\lambda\) 和
    \(\bx\) 的梯度设为 0，可以找到 \(f(\bx)\)（在等式约束下）的临界点。
- en: Noting the constraint \(\sum_{k = 1}^K \pi_k = 1\) (or equivalently \(\sum_{k
    = 1}^K\pi_k - 1 = 0\)), we can maximize the log-likelihood with the following
    Lagrangian.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到约束 \(\sum_{k = 1}^K \pi_k = 1\)（或等价地 \(\sum_{k = 1}^K\pi_k - 1 = 0\)），我们可以通过以下拉格朗日乘数最大化对数似然。
- en: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \dadb{\mathcal{L}(\bpi)}{\pi_k} &= \frac{N_k}{\pi_k}
    - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\} \\ \dadb{\mathcal{L}(\bpi)}{\lambda}
    &= 1 - \sum_{k = 1}^K \pi_k. \\ \end{align*} \end{split}\]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \dadb{\mathcal{L}(\bpi)}{\pi_k} &= \frac{N_k}{\pi_k}
    - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\} \\ \dadb{\mathcal{L}(\bpi)}{\lambda}
    &= 1 - \sum_{k = 1}^K \pi_k. \\ \end{align*} \end{split}\]
- en: 'This system of equations gives an intuitive solution:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程组给出了一种直观的解法：
- en: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
- en: which says that our estimate of \(p(Y_n = k)\) is just the sample fraction of
    observations from class \(k\).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们估计 \(p(Y_n = k)\) 的方法就是从类 \(k\) 中观察到的样本比例。
- en: 2.2 Data Likelihood
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据似然
- en: The next step is to model the conditional distribution of \(\bx_n\) given \(Y_n\)
    so that we can estimate this distribution’s parameters. This of course depends
    on the family of distributions we choose to model \(\bx_n\). Three common approaches
    are detailed below.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是建模给定 \(Y_n\) 的 \(\bx_n\) 的条件分布，以便我们可以估计该分布的参数。这当然取决于我们选择用于建模 \(\bx_n\) 的分布族。以下详细介绍了三种常见方法。
- en: 2.2.1 Linear Discriminative Analysis (LDA)
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 线性判别分析（LDA）
- en: In LDA, we assume
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性判别分析（LDA）中，我们假设
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
- en: for \(k = 1, \dots, K\). Note that each class has the same covariance matrix
    but a unique mean vector.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。注意，每个类具有相同的协方差矩阵，但具有唯一的均值向量。
- en: Let’s derive the parameters in this case. First, let’s find the likelihood and
    log-likelihood. Note that we can write the joint likelihood as follows,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导这个情况下的参数。首先，让我们找到似然和对数似然。注意，我们可以将联合似然写成以下形式，
- en: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
- en: since \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) equals 1 if \(y_n
    \neq k\) and \(p(\bx_n|\bmu_k, \bSigma)\) otherwise. Then we plug in the Multivariate
    Normal PDF (dropping multiplicative constants) and take the log, as follows.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) 在 \(y_n \neq k\) 时等于
    1，而在 \(p(\bx_n|\bmu_k, \bSigma)\) 时则不等于 1。然后我们插入多元正态概率密度函数（省略乘法常数）并取对数，如下所示。
- en: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
- en: Math Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数学笔记
- en: The following matrix derivatives will be of use for maximizing the above log-likelihood.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵导数将有助于最大化上述对数似然。
- en: For any invertible matrix \(\mathbf{W}\),
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何可逆矩阵 \(\mathbf{W}\)，
- en: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
- en: where \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). It follows that
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). 因此，
- en: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
- en: We also have
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有
- en: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
- en: For any symmetric matrix \(\mathbf{A}\),
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何对称矩阵 \(\mathbf{A}\)，
- en: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
- en: These results come from the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果来自[矩阵手册](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)。
- en: Let’s start by estimating \(\bSigma\). First, simplify the log-likelihood to
    make the gradient with respect to \(\bSigma\) more apparent.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从估计 \(\bSigma\) 开始。首先，简化对数似然，以便更明显地看到相对于 \(\bSigma\) 的梯度。
- en: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
- en: Then, using equations (2) and (3) from the *Math Note*, we get
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用数学笔记中的方程（2）和（3），我们得到
- en: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
- en: 'Finally, we set this equal to 0 and multiply by \(\bSigma^{-1}\) on the left
    to solve for \(\hat{\bSigma}\):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们将此设为 0 并在左侧乘以 \(\bSigma^{-1}\) 以求解 \(\hat{\bSigma}\):'
- en: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
- en: where \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: Now, to estimate the \(\bmu_k\), let’s look at each class individually. Let
    \(N_k\) be the number of observations in class \(k\) and \(C_k\) be the set of
    observations in class \(k\). Looking only at terms involving \(\bmu_k\), we get
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了估计 \(\bmu_k\)，让我们逐个查看每个类别。设 \(N_k\) 为类别 \(k\) 中的观测数，\(C_k\) 为类别 \(k\) 中的观测集合。仅查看涉及
    \(\bmu_k\) 的项，我们得到
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
- en: Using equation (4) from the *Math Note*, we calculate the gradient as
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *数学笔记* 中的方程 (4)，我们计算梯度如下
- en: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
- en: 'Setting this gradient equal to 0 and solving, we obtain our \(\bmu_k\) estimate:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将此梯度设为 0 并求解，我们得到我们的 \(\bmu_k\) 估计值：
- en: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
- en: where \(\bar{\bx}_k\) is the element-wise sample mean of all \(\bx_n\) in class
    \(k\).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bar{\bx}_k\) 是类 \(k\) 中所有 \(\bx_n\) 的元素样本均值。
- en: 2.2.2 Quadratic Discriminative Analysis (QDA)
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 二次判别分析（QDA）
- en: 'QDA looks very similar to LDA but assumes each class has its *own* covariance
    matrix:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: QDA 与 LDA 非常相似，但假设每个类别都有自己的协方差矩阵：
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
- en: 'for \(k = 1, \dots, K\). The log-likelihood is the same as in LDA except we
    replace \(\bSigma\) with \(\bSigma_k\):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 \(k = 1, \dots, K\)。对数似然与LDA相同，只是我们将 \(\bSigma\) 替换为 \(\bSigma_k\):'
- en: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
- en: Again, let’s look at the parameters for each class individually. The log-likelihood
    for class \(k\) is given by
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们逐个查看每个类别的参数。类别 \(k\) 的对数似然由以下给出
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
- en: We could take the gradient of this log-likelihood with respect to \(\bmu_k\)
    and set it equal to 0 to solve for \(\hat{\bmu}_k\). However, we can also note
    that our \(\hat{\bmu}_k\) estimate from the LDA approach will hold since this
    expression didn’t depend on the covariance term (which is the only thing we’ve
    changed). Therefore, we again get
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对此对数似然相对于 \(\bmu_k\) 求梯度，并将其设为 0 以求解 \(\hat{\bmu}_k\)。然而，我们也可以注意到，从 LDA
    方法得到的 \(\hat{\bmu}_k\) 估计将保持不变，因为此表达式不依赖于协方差项（这是我们唯一改变的东西）。因此，我们再次得到
- en: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
- en: To estimate the \(\bSigma_k\), we take the gradient of the log-likelihood for
    class \(k\).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计 \(\bSigma_k\)，我们取类别 \(k\) 的对数似然的梯度。
- en: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
- en: 'Then we set this equal to 0 to solve for \(\hat{\bSigma}_k\):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们将此设为 0 以求解 \(\hat{\bSigma}_k\):'
- en: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
- en: where \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: 2.2.3 Naive Bayes
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 简单贝叶斯
- en: Naive Bayes assumes the random variables within \(\bx_n\) are independent conditional
    on the class of observation \(n\). I.e. if \(\bx_n \in \mathbb{R}^D\), Naive Bayes
    assumes
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设 \(\bx_n\) 中的随机变量在观察类别 \(n\) 的条件下是独立的。即如果 \(\bx_n \in \mathbb{R}^D\)，朴素贝叶斯假设
- en: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
- en: This makes estimating \(p(\bx_n|Y_n)\) very easy—to estimate the parameters
    of \(p(x_{nd}|Y_n)\), we can ignore all the variables in \(\bx_{n}\) other than
    \(x_{nd}\)!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得估计 \(p(\bx_n|Y_n)\) 非常容易——为了估计 \(p(x_{nd}|Y_n)\) 的参数，我们可以忽略 \(\bx_{n}\) 中除了
    \(x_{nd}\) 以外的所有变量！
- en: As an example, assume \(\bx_n \in \mathbb{R}^2\) and we use the following model
    (where for simplicity \(n\) and \(\sigma^2_k\) are known).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 \(\bx_n \in \mathbb{R}^2\) 并且我们使用以下模型（其中为了简单起见 \(n\) 和 \(\sigma^2_k\)
    是已知的）。
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
- en: Let the \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) contain all the parameters
    for class \(k\) . The joint likelihood function would become
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让 \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) 包含类别 \(k\) 的所有参数。联合似然函数将变为
- en: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
- en: where the two are equal because of the Naive Bayes conditional independence
    assumption. This allows us to easily find maximum likelihood estimates. The rest
    of this sub-section demonstrates how those estimates would be found, though it
    is nothing beyond ordinary maximum likelihood estimation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯条件独立性假设，这两个是相等的。这使我们能够轻松找到最大似然估计。本小节其余部分将演示如何找到这些估计，尽管这并不超出普通最大似然估计的范围。
- en: The log-likelihood is given by
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然由以下给出
- en: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
- en: 'As before, we estimate the parameters in each class by looking only at the
    terms in that class. Let’s look at the log-likelihood for class \(k\):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们通过只查看该类别中的项来估计每个类别的参数。让我们看看类别 \(k\) 的对数似然：
- en: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
- en: Taking the derivative with respect to \(p_k\), we’re left with
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对 \(p_k\) 求导，我们得到
- en: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
- en: which, will give us \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) as
    usual. The same process would again give typical results for \(\mu_k\) and and
    \(\sigma^2_k\).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来 \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) 如常。同样的过程会再次为 \(\mu_k\)
    和 \(\sigma^2_k\) 提供典型结果。
- en: 2.1 Class Priors
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 类别先验
- en: Let’s start by deriving the estimates for \(\bpi\), the class priors. Let \(I_{nk}\)
    be an indicator which equals 1 if \(Y_n = k\) and 0 otherwise. Then the joint
    likelihood and log-likelihood are given by
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先推导 \(\bpi\)，即类先验的估计。令 \(I_{nk}\) 为一个指示变量，当 \(Y_n = k\) 时等于 1，否则为 0。然后联合似然和对数似然如下给出
- en: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prodN \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n, Y_n\}_{n
    = 1}^N\right) &= \sumN \sum_{k = 1}^K I_{nk} \log(\pi_k) \\ &= \sum_{k = 1}^K
    N_k\log(\pi_k), \end{align*} \end{split}\]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\left(\bpi; \{\bx_n, Y_n\}_{n = 1}^N\right)
    &= \prod_{n=1}^N \prod_{k = 1}^K \pi_k^{I_{nk}} \\ \log L\left(\bpi; \{\bx_n,
    Y_n\}_{n = 1}^N\right) &= \sum_{n=1}^N \sum_{k = 1}^K I_{nk} \log(\pi_k) \\ &=
    \sum_{k = 1}^K N_k\log(\pi_k), \end{align*} \end{split}\]
- en: where \(N_k = \sumN I_{nk}\) gives the number of observations in class \(k\)
    for \(k = 1, \dots, K\).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(N_k = \sum_{n=1}^N I_{nk}\) 给出了类 \(k\) 的观测数，对于 \(k = 1, \dots, K\)。
- en: Math Note
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: The *Lagrangian function* provides a method for optimizing a function \(f(\bx)\)
    subject to the constraint \(g(\bx) = 0\). The Lagrangian is given by
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*拉格朗日函数*提供了一种在约束 \(g(\bx) = 0\) 下优化函数 \(f(\bx)\) 的方法。拉格朗日函数如下给出'
- en: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\lambda, \bx) = f(\bx) - \lambda g(\bx). \]
- en: \(\lambda\) is known as the *Lagrange multiplier*. The critical points of \(f(\bx)\)
    (subject to the equality constraint) are found by setting the gradients of \(\mathcal{L}(\lambda,
    \bx)\) with respect to \(\lambda\) and \(\bx\) equal to 0.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \(\lambda\) 被称为 *拉格朗日乘子*。通过将 \(\mathcal{L}(\lambda, \bx)\) 关于 \(\lambda\) 和
    \(\bx\) 的梯度设置为 0，找到 \(f(\bx)\)（在等式约束下）的临界点。
- en: Noting the constraint \(\sum_{k = 1}^K \pi_k = 1\) (or equivalently \(\sum_{k
    = 1}^K\pi_k - 1 = 0\)), we can maximize the log-likelihood with the following
    Lagrangian.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到约束条件 \(\sum_{k = 1}^K \pi_k = 1\)（或等价地 \(\sum_{k = 1}^K\pi_k - 1 = 0\)），我们可以通过以下拉格朗日函数最大化对数似然。
- en: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \dadb{\mathcal{L}(\bpi)}{\pi_k} &= \frac{N_k}{\pi_k}
    - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\} \\ \dadb{\mathcal{L}(\bpi)}{\lambda}
    &= 1 - \sum_{k = 1}^K \pi_k. \\ \end{align*} \end{split}\]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \mathcal{L}(\bpi) &= \sum_{k = 1}^K N_k \log(\pi_k)
    - \lambda(\sum_{k = 1}^K \pi_k - 1). \\ \dadb{\mathcal{L}(\bpi)}{\pi_k} &= \frac{N_k}{\pi_k}
    - \lambda, \hspace{3mm}\forall \hspace{1mm}k \in \{1, \dots, K\} \\ \dadb{\mathcal{L}(\bpi)}{\lambda}
    &= 1 - \sum_{k = 1}^K \pi_k. \\ \end{align*} \end{split}\]
- en: 'This system of equations gives an intuitive solution:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程组给出了一种直观的解法：
- en: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\pi}_k = \frac{N_k}{N}, \hspace{1mm} \lambda = N, \]
- en: which says that our estimate of \(p(Y_n = k)\) is just the sample fraction of
    observations from class \(k\).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们 \(p(Y_n = k)\) 的估计只是来自类 \(k\) 的观测样本的样本比例。
- en: 2.2 Data Likelihood
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据似然
- en: The next step is to model the conditional distribution of \(\bx_n\) given \(Y_n\)
    so that we can estimate this distribution’s parameters. This of course depends
    on the family of distributions we choose to model \(\bx_n\). Three common approaches
    are detailed below.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是建模给定 \(Y_n\) 的 \(\bx_n\) 的条件分布，以便我们可以估计该分布的参数。这当然取决于我们选择用于建模 \(\bx_n\) 的分布族。以下详细介绍了三种常见方法。
- en: 2.2.1 Linear Discriminative Analysis (LDA)
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 线性判别分析 (LDA)
- en: In LDA, we assume
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LDA 中，我们假设
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
- en: for \(k = 1, \dots, K\). Note that each class has the same covariance matrix
    but a unique mean vector.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。注意，每个类都有相同的协方差矩阵，但具有唯一的均值向量。
- en: Let’s derive the parameters in this case. First, let’s find the likelihood and
    log-likelihood. Note that we can write the joint likelihood as follows,
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导这种情况下的参数。首先，让我们找到似然和对数似然。注意，我们可以将联合似然写成以下形式，
- en: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prod_{n=1}^N \prod_{k = 1}^K \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
- en: since \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) equals 1 if \(y_n
    \neq k\) and \(p(\bx_n|\bmu_k, \bSigma)\) otherwise. Then we plug in the Multivariate
    Normal PDF (dropping multiplicative constants) and take the log, as follows.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) 当 \(y_n \neq k\) 时等于
    1，而当 \(p(\bx_n|\bmu_k, \bSigma)\) 时为 1。然后我们插入多元正态概率密度函数（省略乘性常数）并取对数，如下所示。
- en: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
- en: Math Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 数学注释
- en: The following matrix derivatives will be of use for maximizing the above log-likelihood.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵导数将有助于最大化上述对数似然。
- en: For any invertible matrix \(\mathbf{W}\),
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何可逆矩阵 \(\mathbf{W}\),
- en: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
- en: where \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). It follows that
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). 因此，
- en: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
- en: We also have
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有
- en: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
- en: For any symmetric matrix \(\mathbf{A}\),
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何对称矩阵 \(\mathbf{A}\),
- en: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
- en: These results come from the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果来自 [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
- en: Let’s start by estimating \(\bSigma\). First, simplify the log-likelihood to
    make the gradient with respect to \(\bSigma\) more apparent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先估计 \(\bSigma\)。首先，简化对数似然函数，以便更明显地看到关于 \(\bSigma\) 的梯度。
- en: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
- en: Then, using equations (2) and (3) from the *Math Note*, we get
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 *Math Note* 中的方程 (2) 和 (3)，我们得到
- en: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
- en: 'Finally, we set this equal to 0 and multiply by \(\bSigma^{-1}\) on the left
    to solve for \(\hat{\bSigma}\):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们将这个等式设为0，并在左侧乘以 \(\bSigma^{-1}\) 来求解 \(\hat{\bSigma}\):'
- en: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
- en: where \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: Now, to estimate the \(\bmu_k\), let’s look at each class individually. Let
    \(N_k\) be the number of observations in class \(k\) and \(C_k\) be the set of
    observations in class \(k\). Looking only at terms involving \(\bmu_k\), we get
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了估计 \(\bmu_k\)，让我们逐个查看每个类别。设 \(N_k\) 为类别 \(k\) 中的观测数，\(C_k\) 为类别 \(k\) 中的观测集合。仅查看涉及
    \(\bmu_k\) 的项，我们得到
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
- en: Using equation (4) from the *Math Note*, we calculate the gradient as
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *Math Note* 中的方程 (4)，我们计算梯度如下
- en: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \left[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} \right] = \sum_{n \in C_k}
    \bSigma^{-1}(\bx_n - \bmu_k). \]
- en: 'Setting this gradient equal to 0 and solving, we obtain our \(\bmu_k\) estimate:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个梯度设为0并求解，我们得到我们的 \(\bmu_k\) 估计值：
- en: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
- en: where \(\bar{\bx}_k\) is the element-wise sample mean of all \(\bx_n\) in class
    \(k\).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bar{\bx}_k\) 是类别 \(k\) 中所有 \(\bx_n\) 的元素平均值。
- en: 2.2.2 Quadratic Discriminative Analysis (QDA)
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 二次判别分析 (QDA)
- en: 'QDA looks very similar to LDA but assumes each class has its *own* covariance
    matrix:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: QDA 与 LDA 非常相似，但假设每个类别都有自己的协方差矩阵：
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
- en: 'for \(k = 1, \dots, K\). The log-likelihood is the same as in LDA except we
    replace \(\bSigma\) with \(\bSigma_k\):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。对数似然与 LDA 相同，只是将 \(\bSigma\) 替换为 \(\bSigma_k\)：
- en: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
- en: Again, let’s look at the parameters for each class individually. The log-likelihood
    for class \(k\) is given by
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们分别查看每个类别的参数。类别 \(k\) 的对数似然由以下给出
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
- en: We could take the gradient of this log-likelihood with respect to \(\bmu_k\)
    and set it equal to 0 to solve for \(\hat{\bmu}_k\). However, we can also note
    that our \(\hat{\bmu}_k\) estimate from the LDA approach will hold since this
    expression didn’t depend on the covariance term (which is the only thing we’ve
    changed). Therefore, we again get
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这个对数似然相对于 \(\bmu_k\) 求导，并将其设为0来求解 \(\hat{\bmu}_k\)。然而，我们也可以注意到，由于这个表达式没有依赖于协方差项（这是我们唯一改变的东西），因此从
    LDA 方法得到的 \(\hat{\bmu}_k\) 估计将保持不变。因此，我们再次得到
- en: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
- en: To estimate the \(\bSigma_k\), we take the gradient of the log-likelihood for
    class \(k\).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计 \(\bSigma_k\)，我们取类别 \(k\) 的对数似然函数的梯度。
- en: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
- en: 'Then we set this equal to 0 to solve for \(\hat{\bSigma}_k\):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们将这个等式设为0来求解 \(\hat{\bSigma}_k\):'
- en: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
- en: where \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: 2.2.3 Naive Bayes
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 简单贝叶斯
- en: Naive Bayes assumes the random variables within \(\bx_n\) are independent conditional
    on the class of observation \(n\). I.e. if \(\bx_n \in \mathbb{R}^D\), Naive Bayes
    assumes
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯假设在观察 \(n\) 的类别条件下，\(\bx_n\) 中的随机变量是独立的。即如果 \(\bx_n \in \mathbb{R}^D\)，则简单贝叶斯假设
- en: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
- en: This makes estimating \(p(\bx_n|Y_n)\) very easy—to estimate the parameters
    of \(p(x_{nd}|Y_n)\), we can ignore all the variables in \(\bx_{n}\) other than
    \(x_{nd}\)!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得估计 \(p(\bx_n|Y_n)\) 非常简单——为了估计 \(p(x_{nd}|Y_n)\) 的参数，我们可以忽略 \(\bx_{n}\) 中除了
    \(x_{nd}\) 以外的所有变量！
- en: As an example, assume \(\bx_n \in \mathbb{R}^2\) and we use the following model
    (where for simplicity \(n\) and \(\sigma^2_k\) are known).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 \(\bx_n \in \mathbb{R}^2\)，并使用以下模型（为了简单起见，\(n\) 和 \(\sigma^2_k\) 是已知的）。
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
- en: Let the \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) contain all the parameters
    for class \(k\) . The joint likelihood function would become
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让 \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) 包含类 \(k\) 的所有参数。联合似然函数将变为
- en: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
- en: where the two are equal because of the Naive Bayes conditional independence
    assumption. This allows us to easily find maximum likelihood estimates. The rest
    of this sub-section demonstrates how those estimates would be found, though it
    is nothing beyond ordinary maximum likelihood estimation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯条件独立性假设，这两个是相等的。这使我们能够轻松地找到最大似然估计。本小节的其余部分将演示如何找到这些估计，尽管这并不超出普通最大似然估计的范围。
- en: The log-likelihood is given by
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然由以下给出
- en: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
- en: 'As before, we estimate the parameters in each class by looking only at the
    terms in that class. Let’s look at the log-likelihood for class \(k\):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们通过只查看该类中的项来估计每个类的参数。让我们看看类 \(k\) 的对数似然：
- en: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
- en: Taking the derivative with respect to \(p_k\), we’re left with
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对 \(p_k\) 求导，我们得到
- en: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
- en: which, will give us \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) as
    usual. The same process would again give typical results for \(\mu_k\) and and
    \(\sigma^2_k\).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出 \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) 的结果。同样的过程会再次给出 \(\mu_k\)
    和 \(\sigma^2_k\) 的典型结果。
- en: 2.2.1 Linear Discriminative Analysis (LDA)
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 线性判别分析 (LDA)
- en: In LDA, we assume
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LDA 中，我们假设
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma), \]
- en: for \(k = 1, \dots, K\). Note that each class has the same covariance matrix
    but a unique mean vector.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。注意，每个类都有相同的协方差矩阵，但具有唯一的均值向量。
- en: Let’s derive the parameters in this case. First, let’s find the likelihood and
    log-likelihood. Note that we can write the joint likelihood as follows,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导这种情况下的参数。首先，让我们找到似然和对数似然。注意，我们可以将联合似然写成以下形式，
- en: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = \prodN \prodK \Big(p\l\bx_n|\bmu_k,
    \bSigma\r\Big)^{I_{nk}}, \]
- en: since \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) equals 1 if \(y_n
    \neq k\) and \(p(\bx_n|\bmu_k, \bSigma)\) otherwise. Then we plug in the Multivariate
    Normal PDF (dropping multiplicative constants) and take the log, as follows.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 \(\left(p(\bx_{n}|\bmu_k, \bSigma)\right)^{I_{nk}}\) 在 \(y_n \neq k\) 时等于
    1，而在 \(p(\bx_n|\bmu_k, \bSigma)\) 时则不等于 1。然后我们插入多元正态概率密度函数（省略乘法常数）并取对数，如下所示。
- en: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &= \prodN\prodK
    \Big(\frac{1}{\sqrt{|\bSigma|}}\exp\left\{-\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k)\right\}\Big)^{I_{nk}} \\ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r &=
    \sumN\sumK I_{nk}\l-\frac{1}{2} \log|\bSigma| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n
    - \bmu_k) \r \end{align*} \end{split}\]
- en: Math Note
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 数学笔记
- en: The following matrix derivatives will be of use for maximizing the above log-likelihood.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵导数对于最大化上述对数似然是有用的。
- en: For any invertible matrix \(\mathbf{W}\),
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何可逆矩阵 \(\mathbf{W}\)，
- en: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{|\mathbf{W}|}{\mathbf{W}} = |\mathbf{W}|\mathbf{W}^{-\top}, \tag{1}
    \]
- en: where \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\). It follows that
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{W}^{-\top} = (\mathbf{W}^{-1})^\top\)。由此可得
- en: \[ \dadb{\log |\mathbf{W}|}{\mathbf{W}} = \mathbf{W}^{-\top}. \tag{2} \]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \log |\mathbf{W}|}{\partial \mathbf{W}} = \mathbf{W}^{-\top}.
    \tag{2} \]
- en: We also have
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有
- en: \[ \dadb{\bx^\top \mathbf{W}^{-1} \bx}{\mathbf{W}} = -\mathbf{W}^{-\top} \bx
    \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \left( \bx^\top \mathbf{W}^{-1} \bx \right)}{\partial \mathbf{W}}
    = -\mathbf{W}^{-\top} \bx \bx^\top \mathbf{W}^{-\top}. \tag{3} \]
- en: For any symmetric matrix \(\mathbf{A}\),
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何对称矩阵 \(\mathbf{A}\)，
- en: \[ \dadb{(\bx - \mathbf{s})^\top \mathbf{A} (\bx - \mathbf{s})}{\mathbf{s}}
    = -2\mathbf{A}(\bx - \mathbf{s}). \tag{4} \]
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\left[ \left( \bx - \mathbf{s} \right)^\top \mathbf{A} \left( \bx -
    \mathbf{s} \right) \right]}{\mathbf{s}} = -2\mathbf{A} \left( \bx - \mathbf{s}
    \right). \tag{4} \]
- en: These results come from the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果来自 [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)。
- en: Let’s start by estimating \(\bSigma\). First, simplify the log-likelihood to
    make the gradient with respect to \(\bSigma\) more apparent.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先估计 \(\bSigma\)。首先，简化对数似然，以便更明显地看到关于 \(\bSigma\) 的梯度。
- en: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sumN\sumK I_{nk}(\bx_n - \bmu_k)^\top\bSigma^{-1}(\bx_n - \bmu_k). \]
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r = - \frac{N}{2}\log |\bSigma| -\frac{1}{2}
    \sum_{n \in C_k} I_{nk} \left( \bx_n - \bmu_k \right)^\top \bSigma^{-1} \left(
    \bx_n - \bmu_k \right). \]
- en: Then, using equations (2) and (3) from the *Math Note*, we get
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用来自 *Math Note* 的方程 (2) 和 (3)，我们得到
- en: \[ \begin{align*} \dadb{\log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r }{\bSigma} &=
    -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sumN\sumK I_{nk}\bSigma^{-\top} (\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top\bSigma^{-\top}. \end{align*} \]
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \frac{\partial \log L\l\{\bmu_k\}_{k = 1}^K, \bSigma\r}{\bSigma}
    &= -\frac{N}{2}\bSigma^{-\top} + \frac{1}{2}\sum_{n \in C_k} I_{nk}\bSigma^{-\top}
    \left( \bx_n - \bmu_k \right) \left( \bx_n - \bmu_k \right)^\top \bSigma^{-\top}.
    \end{align*} \]
- en: 'Finally, we set this equal to 0 and multiply by \(\bSigma^{-1}\) on the left
    to solve for \(\hat{\bSigma}\):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将此设为 0 并在左侧乘以 \(\bSigma^{-1}\) 来求解 \(\hat{\bSigma}\)：
- en: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\l\sumN\sumK I_{nk}
    (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\r\bSigma^{-\top} \\ \bSigma^\top &= \frac{1}{N}\sumN
    \sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \\ \hat{\bSigma} &= \frac{1}{N}\mathbf{S}_T,
    \end{align*} \end{split}\]
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} 0 &= -\frac{N}{2} + \frac{1}{2}\sum_{n \in C_k}
    I_{nk} \left( \bx_n - \bmu_k \right) \left( \bx_n - \bmu_k \right)^\top \bSigma^{-\top}
    \\ \bSigma^\top &= \frac{1}{N} \sum_{n \in C_k} I_{nk} \left( \bx_n - \bmu_k \right)
    \left( \bx_n - \bmu_k \right)^\top \\ \hat{\bSigma} &= \frac{1}{N} \mathbf{S}_T,
    \end{align*} \end{split}\]
- en: where \(\mathbf{S}_T = \sumN\sumK I_{nk}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_T = \sum_{n \in C_k} I_{nk} \left( \bx_n - \bmu_k \right) \left(
    \bx_n - \bmu_k \right)^\top\).
- en: Now, to estimate the \(\bmu_k\), let’s look at each class individually. Let
    \(N_k\) be the number of observations in class \(k\) and \(C_k\) be the set of
    observations in class \(k\). Looking only at terms involving \(\bmu_k\), we get
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了估计 \(\bmu_k\)，让我们逐个查看每个类别。设 \(N_k\) 为类别 \(k\) 中的观测数，\(C_k\) 为类别 \(k\) 中的观测集合。仅查看涉及
    \(\bmu_k\) 的项，我们得到
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \Big(
    \log|\bSigma| + (\bx_n - \bmu_k)^\top \bSigma^{-1}(\bx_n - \bmu_k) \Big). \end{align*}
    \]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma) &= -\frac{1}{2} \sum_{n \in C_k} \left(
    \log |\bSigma| + \left( \bx_n - \bmu_k \right)^\top \bSigma^{-1} \left( \bx_n
    - \bmu_k \right) \right). \end{align*} \]
- en: Using equation (4) from the *Math Note*, we calculate the gradient as
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自 *Math Note* 的方程 (4)，我们计算梯度如下
- en: \[ \dadb{\log L(\bmu_k, \bSigma)}{\bmu_k} =\sum_{n \in C_k} \bSigma^{-1}(\bx_n
    - \bmu_k). \]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\partial \log L(\bmu_k, \bSigma)}{\partial \bmu_k} = \sum_{n \in C_k}
    \bSigma^{-1} \left( \bx_n - \bmu_k \right). \]
- en: 'Setting this gradient equal to 0 and solving, we obtain our \(\bmu_k\) estimate:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将此梯度设为 0 并求解，我们得到 \(\bmu_k\) 的估计值：
- en: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \frac{1}{N_k} \sum_{n \in C_k} \bx_n = \bar{\bx}_k, \]
- en: where \(\bar{\bx}_k\) is the element-wise sample mean of all \(\bx_n\) in class
    \(k\).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bar{\bx}_k\) 是类别 \(k\) 中所有 \(\bx_n\) 的逐元素样本均值。
- en: 2.2.2 Quadratic Discriminative Analysis (QDA)
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 二次判别分析 (QDA)
- en: 'QDA looks very similar to LDA but assumes each class has its *own* covariance
    matrix:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: QDA 与 LDA 非常相似，但假设每个类别都有自己的协方差矩阵：
- en: \[ \bx_n|(Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bx_n \mid (Y_n = k) \sim \text{MVN}(\bmu_k, \bSigma_k) \]
- en: 'for \(k = 1, \dots, K\). The log-likelihood is the same as in LDA except we
    replace \(\bSigma\) with \(\bSigma_k\):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(k = 1, \dots, K\)。对数似然与 LDA 相同，只是我们将 \(\bSigma\) 替换为 \(\bSigma_k\)：
- en: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sumN\sumK I_{nk}\l-\frac{1}{2}
    \log|\bSigma_k| -\frac{1}{2}(\bx_n - \bmu_k)^\top\bSigma_k^{-1}(\bx_n - \bmu_k)
    \r. \end{align*} \]
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L\l\{\bmu_k, \bSigma_k\}_{k = 1}^K\r &= \sum_{n \in C_k}
    I_{nk} \left( -\frac{1}{2} \log |\bSigma_k| -\frac{1}{2} \left( \bx_n - \bmu_k
    \right)^\top \bSigma_k^{-1} \left( \bx_n - \bmu_k \right) \right). \end{align*}
    \]
- en: Again, let’s look at the parameters for each class individually. The log-likelihood
    for class \(k\) is given by
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们逐个查看每个类别的参数。类别 \(k\) 的对数似然由以下给出
- en: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \log L(\bmu_k, \bSigma_k) &= -\frac{1}{2} \sum_{n \in C_k}
    \Big( \log|\bSigma_k| + (\bx_n - \bmu_k)^\top \bSigma_k^{-1}(\bx_n - \bmu_k) \Big).
    \end{align*} \]
- en: We could take the gradient of this log-likelihood with respect to \(\bmu_k\)
    and set it equal to 0 to solve for \(\hat{\bmu}_k\). However, we can also note
    that our \(\hat{\bmu}_k\) estimate from the LDA approach will hold since this
    expression didn’t depend on the covariance term (which is the only thing we’ve
    changed). Therefore, we again get
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对这个对数似然关于 \(\bmu_k\) 求梯度，并将其设为 0 来求解 \(\hat{\bmu}_k\)。然而，我们也可以注意到，由于这个表达式不依赖于协方差项（这是我们唯一改变的东西），我们使用
    LDA 方法得到的 \(\hat{\bmu}_k\) 估计将保持不变。因此，我们再次得到
- en: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\bmu}_k = \bar{\bx}_k. \]
- en: To estimate the \(\bSigma_k\), we take the gradient of the log-likelihood for
    class \(k\).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计 \(\bSigma_k\)，我们取类 \(k\) 的对数似然的梯度。
- en: \[ \begin{align*} \dadb{\log L(\bmu_k, \bSigma_k) }{\bSigma_k} &= -\frac{1}{2}\sum_{n
    \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n -
    \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \begin{align*} \frac{\partial \log L(\bmu_k, \bSigma_k)}{\partial \bSigma_k}
    &= -\frac{1}{2}\sum_{n \in C_k} \left( \bSigma_k^{-\top} - \bSigma_k^{-\top}(\bx_n
    - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top} \right). \end{align*} \]
- en: 'Then we set this equal to 0 to solve for \(\hat{\bSigma}_k\):'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这个值设为 0 来求解 \(\hat{\bSigma}_k\)：
- en: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \sum_{n \in C_k} \bSigma_k^{-\top} &= \sum_{n
    \in C_k} \bSigma_k^{-\top}(\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ N_k I &= \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top \bSigma_k^{-\top}
    \\ \bSigma_k^\top &= \frac{1}{N_k} \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top
    \\ \hat{\bSigma}_k &= \frac{1}{N_k} \mathbf{S}_k, \end{align*} \end{split}\]
- en: where \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{S}_k = \sum_{n \in C_k} (\bx_n - \bmu_k)(\bx_n - \bmu_k)^\top\).
- en: 2.2.3 Naive Bayes
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 简单贝叶斯
- en: Naive Bayes assumes the random variables within \(\bx_n\) are independent conditional
    on the class of observation \(n\). I.e. if \(\bx_n \in \mathbb{R}^D\), Naive Bayes
    assumes
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯假设 \(\bx_n\) 中的随机变量在观察类 \(n\) 的条件下是独立的。即如果 \(\bx_n \in \mathbb{R}^D\)，简单贝叶斯假设
- en: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n)\cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\bx_n|Y_n) = p(x_{n1}|Y_n) \cdot p(x_{n2}|Y_n) \cdot ... \cdot p(x_{nD}|Y_n).
    \]
- en: This makes estimating \(p(\bx_n|Y_n)\) very easy—to estimate the parameters
    of \(p(x_{nd}|Y_n)\), we can ignore all the variables in \(\bx_{n}\) other than
    \(x_{nd}\)!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得估计 \(p(\bx_n|Y_n)\) 非常容易——为了估计 \(p(x_{nd}|Y_n)\) 的参数，我们可以忽略 \(\bx_n\) 中除了
    \(x_{nd}\) 以外的所有变量！
- en: As an example, assume \(\bx_n \in \mathbb{R}^2\) and we use the following model
    (where for simplicity \(n\) and \(\sigma^2_k\) are known).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 \(\bx_n \in \mathbb{R}^2\) 并且我们使用以下模型（为了简单起见，\(n\) 和 \(\sigma^2_k\) 是已知的）。
- en: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} x_{n1}|(Y_n = k) &\sim \mathcal{N}(\mu_k, \sigma^2_k)
    \\ x_{n2}|(Y_n = k) &\sim \text{Bin}(n, p_k). \end{align*} \end{split}\]
- en: Let the \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) contain all the parameters
    for class \(k\) . The joint likelihood function would become
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让 \(\btheta_k = (\mu_k, \sigma_k^2, p_k)\) 包含类 \(k\) 的所有参数。联合似然函数将变为
- en: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK \l
    p(\bx|Y_n, \btheta_k)\r^{I_{nk}} \\ L(\{\btheta_k\}_{k = 1}^K) &= \prodN \prodK
    \l p(x_{n1}|\mu_k, \sigma^2_k) \cdot p(x_{n2}|p_k) \r^{I_{nk}}, \end{align*} \end{split}\]
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} L(\{\btheta_k\}_{k = 1}^K) &= \prod_{n=1}^{N}
    \prod_{k=1}^{K} \left( p(\bx|Y_n, \btheta_k) \right)^{I_{nk}} \\ L(\{\btheta_k\}_{k
    = 1}^K) &= \prod_{n=1}^{N} \prod_{k=1}^{K} \left( p(x_{n1}|\mu_k, \sigma^2_k)
    \cdot p(x_{n2}|p_k) \right)^{I_{nk}}, \end{align*} \end{split}\]
- en: where the two are equal because of the Naive Bayes conditional independence
    assumption. This allows us to easily find maximum likelihood estimates. The rest
    of this sub-section demonstrates how those estimates would be found, though it
    is nothing beyond ordinary maximum likelihood estimation.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 由于简单贝叶斯条件独立性假设，这两个是相等的。这使我们能够轻松找到最大似然估计。本小节的其余部分将演示如何找到这些估计，尽管这并不超出普通最大似然估计的范围。
- en: The log-likelihood is given by
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然由以下给出
- en: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sumN\sumK I_{nk}\l\log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \r. \]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log L(\{\btheta_k\}_{k = 1}^K) = \sum_{n=1}^{N}\sum_{k=1}^{K} I_{nk}\log
    p(x_{n1}|\mu_k, \sigma^2_k) + \log p(x_{n2}|p_k) \]
- en: 'As before, we estimate the parameters in each class by looking only at the
    terms in that class. Let’s look at the log-likelihood for class \(k\):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们通过只观察该类中的项来估计每个类别的参数。让我们看看类别 \(k\) 的对数似然：
- en: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} \log L(\btheta_k) &= \sum_{n \in C_k} \log p(x_{n1}|\mu_k,
    \sigma^2_k) + \log p(x_{n2}|p_k) \\ &= \sum_{n \in C_k} -\frac{(x_{n1} - \mu_k)^2}{2\sigma^2_k}
    + x_{n2}\log(p_k) + (1-x_{n2})\log(1-p_k). \end{align*} \end{split}\]
- en: Taking the derivative with respect to \(p_k\), we’re left with
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 对 \(p_k\) 求导，我们得到
- en: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \dadb{\log L(\btheta_k)}{p_k} = \sum_{n \in C_k}\frac{x_{n2}}{p_k} - \frac{1-x_{n2}}{1-p_k},
    \]
- en: which, will give us \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) as
    usual. The same process would again give typical results for \(\mu_k\) and and
    \(\sigma^2_k\).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出 \(\hat{p}_k = \frac{1}{N_k}\sum_{n \in C_k} x_{n2}\) 作为通常的结果。同样的过程会再次给出
    \(\mu_k\) 和 \(\sigma^2_k\) 的典型结果。
- en: 3\. Making Classifications
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 进行分类
- en: Regardless of our modeling choices for \(p(\bx_n|Y_n)\), classifying new observations
    is easy. Consider a test observation \(\bx_0\). For \(k = 1, \dots, K\), we use
    Bayes’ rule to calculate
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们对 \(p(\bx_n|Y_n)\) 的建模选择如何，对新观测的分类都是容易的。考虑一个测试观测 \(\bx_0\)。对于 \(k = 1, \dots,
    K\)，我们使用贝叶斯定理来计算
- en: \[\begin{split} \begin{align*} p(Y_0 = k|\bx_0) &\propto p(\bx_0|Y_0 = k)p(Y_0
    = k) \\ &= \hat{p}(\bx_0|Y_0 = k)\hat{\pi}_k, \end{align*} \end{split}\]
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \begin{align*} p(Y_0 = k|\bx_0) &\propto p(\bx_0|Y_0 = k)p(Y_0
    = k) \\ &= \hat{p}(\bx_0|Y_0 = k)\hat{\pi}_k, \end{align*} \end{split}\]
- en: where \(\hat{p}\) gives the estimated density of \(\bx_0\) conditional on \(Y_0\).
    We then predict \(Y_0 = k\) for whichever value \(k\) maximizes the above expression.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\hat{p}\) 给出了在 \(Y_0\) 条件下 \(\bx_0\) 的估计密度。然后我们预测 \(Y_0 = k\)，对于使上述表达式最大化的
    \(k\) 值。
