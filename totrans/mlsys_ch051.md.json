["```py\nsource ~/tflite/bin/activate\n```", "```py\ncd Documents/TFLITE/\nmkdir OBJ_DETECT\ncd OBJ_DETECT\nmkdir images\nmkdir models\ncd models\n```", "```py\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\n```", "```py\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n```", "```py\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n```", "```py\nimg_path = \"./images/cat_dog.jpeg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(8, 8))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n```", "```py\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\ninput_data = np.expand_dims(img, axis=0)\ninput_data.shape, input_data.dtype\n```", "```py\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\n```", "```py\nboxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[1][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[2][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[3][\"index\"])[0]\n)\n```", "```py\nfor i in range(num_detections):\n    if scores[i] > 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n```", "```py\nplt.figure(figsize=(12, 8))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] > 0.5:  # Adjust threshold as needed\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n```", "```py\npython3 get_img_data.py\n```", "```py\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport tflite_runtime.interpreter as tflite\n```", "```py\nmodel_path = \"./models/ei-raspi-object-detection-SSD-\\\n MobileNetv2-320x0320-int8.lite\"\nlabels = [\"box\", \"wheel\"]\n```", "```py\n# Load the TFLite model\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n```", "```py\ninput_dtype = input_details[0][\"dtype\"]\ninput_dtype\n```", "```py\nnumpy.int8\n```", "```py\n# Load the image\nimg_path = \"./images/box_2_wheel_2.jpg\"\norig_img = Image.open(img_path)\n\n# Display the image\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nplt.title(\"Original Image\")\nplt.show()\n```", "```py\nscale, zero_point = input_details[0][\"quantization\"]\nimg = orig_img.resize(\n    (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n)\nimg_array = np.array(img, dtype=np.float32) / 255.0\nimg_array = (\n    (img_array / scale + zero_point).clip(-128, 127).astype(np.int8)\n)\ninput_data = np.expand_dims(img_array, axis=0)\n```", "```py\ninput_data.shape, input_data.dtype\n```", "```py\n((1, 320, 320, 3), dtype('int8'))\n```", "```py\n# Inference on Raspi-Zero\nstart_time = time.time()\ninterpreter.set_tensor(input_details[0][\"index\"], input_data)\ninterpreter.invoke()\nend_time = time.time()\ninference_time = (\n    end_time - start_time\n) * 1000  # Convert to milliseconds\nprint(\"Inference time: {:.1f}ms\".format(inference_time))\n```", "```py\nboxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\nclasses = interpreter.get_tensor(output_details[3][\"index\"])[0]\nscores = interpreter.get_tensor(output_details[0][\"index\"])[0]\nnum_detections = int(\n    interpreter.get_tensor(output_details[2][\"index\"])[0]\n)\n```", "```py\nfor i in range(num_detections):\n    if scores[i] > 0.5:  # Confidence threshold\n        print(f\"Object {i}:\")\n        print(f\"  Bounding Box: {boxes[i]}\")\n        print(f\"  Confidence: {scores[i]}\")\n        print(f\"  Class: {classes[i]}\")\n```", "```py\nthreshold = 0.5\nplt.figure(figsize=(6, 6))\nplt.imshow(orig_img)\nfor i in range(num_detections):\n    if scores[i] > threshold:\n        ymin, xmin, ymax, xmax = boxes[i]\n        (left, right, top, bottom) = (\n            xmin * orig_img.width,\n            xmax * orig_img.width,\n            ymin * orig_img.height,\n            ymax * orig_img.height,\n        )\n        rect = plt.Rectangle(\n            (left, top),\n            right - left,\n            bottom - top,\n            fill=False,\n            color=\"red\",\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        class_id = int(classes[i])\n        class_name = labels[class_id]\n        plt.text(\n            left,\n            top - 10,\n            f\"{class_name}: {scores[i]:.2f}\",\n            color=\"red\",\n            fontsize=12,\n            backgroundcolor=\"white\",\n        )\n```", "```py\ndef non_max_suppression(boxes, scores, threshold):\n    # Convert to corner coordinates\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= threshold)[0]\n        order = order[inds + 1]\n\n    return keep\n```", "```py\ndef visualize_detections(\n    image, boxes, classes, scores, labels, threshold, iou_threshold\n):\n    if isinstance(image, Image.Image):\n        image_np = np.array(image)\n    else:\n        image_np = image\n    height, width = image_np.shape[:2]\n    # Convert normalized coordinates to pixel coordinates\n    boxes_pixel = boxes * np.array([height, width, height, width])\n    # Apply NMS\n    keep = non_max_suppression(boxes_pixel, scores, iou_threshold)\n    # Set the figure size to 12x8 inches\n    fig, ax = plt.subplots(1, figsize=(12, 8))\n    ax.imshow(image_np)\n    for i in keep:\n        if scores[i] > threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            rect = patches.Rectangle(\n                (xmin * width, ymin * height),\n                (xmax - xmin) * width,\n                (ymax - ymin) * height,\n                linewidth=2,\n                edgecolor=\"r\",\n                facecolor=\"none\",\n            )\n\n            ax.add_patch(rect)\n            class_name = labels[int(classes[i])]\n            ax.text(\n                xmin * width,\n                ymin * height - 10,\n                f\"{class_name}: {scores[i]:.2f}\",\n                color=\"red\",\n                fontsize=12,\n                backgroundcolor=\"white\",\n            )\n    plt.show()\n```", "```py\ndef detect_objects(img_path, conf=0.5, iou=0.5):\n    orig_img = Image.open(img_path)\n    scale, zero_point = input_details[0][\"quantization\"]\n    img = orig_img.resize(\n        (input_details[0][\"shape\"][1], input_details[0][\"shape\"][2])\n    )\n    img_array = np.array(img, dtype=np.float32) / 255.0\n    img_array = (\n        (img_array / scale + zero_point)\n        .clip(-128, 127)\n        .astype(np.int8)\n    )\n    input_data = np.expand_dims(img_array, axis=0)\n\n    # Inference on Raspi-Zero\n    start_time = time.time()\n    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n    interpreter.invoke()\n    end_time = time.time()\n    inference_time = (\n        end_time - start_time\n    ) * 1000  # Convert to milliseconds\n\n    print(\"Inference time: {:.1f}ms\".format(inference_time))\n\n    # Extract the outputs\n    boxes = interpreter.get_tensor(output_details[1][\"index\"])[0]\n    classes = interpreter.get_tensor(output_details[3][\"index\"])[0]\n    scores = interpreter.get_tensor(output_details[0][\"index\"])[0]\n    num_detections = int(\n        interpreter.get_tensor(output_details[2][\"index\"])[0]\n    )\n\n    visualize_detections(\n        orig_img,\n        boxes,\n        classes,\n        scores,\n        labels,\n        threshold=conf,\n        iou_threshold=iou,\n    )\n```", "```py\nimg_path = \"./images/box_2_wheel_2.jpg\"\ndetect_objects(img_path, conf=0.3, iou=0.05)\n```", "```py\ncd ~\ncd Documents\nmkdir EI_Linux\ncd EI_Linux\nmkdir models\nmkdir images\n```", "```py\npython3 -m venv ~/eilinux\nsource ~/eilinux/bin/activate\n```", "```py\nsudo apt-get update\nsudo apt-get install libatlas-base-dev\\\n                     libportaudio0 libportaudio2\nsudo apt-get installlibportaudiocpp0 portaudio19-dev\n\npip3 install edge_impulse_linux -i https://pypi.python.org/simple\npip3 install Pillow matplotlib pyaudio opencv-contrib-python\n\nsudo apt-get install portaudio19-dev\npip3 install pyaudio\npip3 install opencv-contrib-python\n```", "```py\nchmod +x raspi-object-detection-linux-aarch64-FOMO-int8.eim\n```", "```py\npip3 install jupyter\n```", "```py\njupyter notebook\n```", "```py\njupyter notebook --ip=192.168.4.210 --no-browser\n```", "```py\nimport sys, time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport cv2\nfrom edge_impulse_linux.image import ImageImpulseRunner\n```", "```py\nmodel_file = \"raspi-object-detection-linux-aarch64-int8.eim\"\nmodel_path = \"models/\" + model_file  # Trained ML model from\n# Edge Impulse\nlabels = [\"box\", \"wheel\"]\n```", "```py\n# Load the model file\nrunner = ImageImpulseRunner(model_path)\n\n# Initialize model\nmodel_info = runner.init()\n```", "```py\n# Load the image\nimg_path = \"./images/1_box_1_wheel.jpg\"\norig_img = cv2.imread(img_path)\nimg_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n\n# Display the image\nplt.imshow(img_rgb)\nplt.title(\"Original Image\")\nplt.show()\n```", "```py\nfeatures, cropped = (\n    runner.get_features_from_image_auto_studio_settings(img_rgb)\n)\n```", "```py\nres = runner.classify(features)\n```", "```py\nprint(\n    \"Found %d bounding boxes (%d ms.)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nfor bb in res[\"result\"][\"bounding_boxes\"]:\n    print(\n        \"\\t%s (%.2f): x=%d y=%d w=%d h=%d\"\n        % (\n            bb[\"label\"],\n            bb[\"value\"],\n            bb[\"x\"],\n            bb[\"y\"],\n            bb[\"width\"],\n            bb[\"height\"],\n        )\n    )\n```", "```py\nFound 2 bounding boxes (29 ms.)\n    1 (0.91): x=112 y=40 w=16 h=16\n    0 (0.75): x=48 y=56 w=8 h=8\n```", "```py\nprint(\n    \"\\tFound %d bounding boxes (latency: %d ms)\"\n    % (\n        len(res[\"result\"][\"bounding_boxes\"]),\n        res[\"timing\"][\"dsp\"] + res[\"timing\"][\"classification\"],\n    )\n)\nplt.figure(figsize=(5, 5))\nplt.imshow(cropped)\n\n# Go through each of the returned bounding boxes\nbboxes = res[\"result\"][\"bounding_boxes\"]\nfor bbox in bboxes:\n\n    # Get the corners of the bounding box\n    left = bbox[\"x\"]\n    top = bbox[\"y\"]\n    width = bbox[\"width\"]\n    height = bbox[\"height\"]\n\n    # Draw a circle centered on the detection\n    circ = plt.Circle(\n        (left + width // 2, top + height // 2),\n        5,\n        fill=False,\n        color=\"red\",\n        linewidth=3,\n    )\n    plt.gca().add_patch(circ)\n    class_id = int(bbox[\"label\"])\n    class_name = labels[class_id]\n    plt.text(\n        left,\n        top - 10,\n        f'{class_name}: {bbox[\"value\"]:.2f}',\n        color=\"red\",\n        fontsize=12,\n        backgroundcolor=\"white\",\n    )\nplt.show()\n```", "```py\ndeactivate\ncd ~\ncd Documents/\nmkdir YOLO\ncd YOLO\nmkdir models\nmkdir images\n```", "```py\npython3 -m venv ~/yolo\nsource ~/yolo/bin/activate\n```", "```py\nsudo apt update\nsudo apt install python3-pip -y\npip install -U pip\n```", "```py\npip install ultralytics[export]\n```", "```py\nsudo reboot\n```", "```py\nsource ~/yolo/bin/activate\ncd /Documents/YOLO\n```", "```py\nyolo predict model='yolov8n' \\\n     source='https://ultralytics.com/images/bus.jpg'\n```", "```py\nyolo export model=yolov8n.pt format=ncnn\n```", "```py\nyolo predict model='./yolov8n_ncnn_model' source='bus.jpg'\n```", "```py\npython3\n```", "```py\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n_ncnn_model\")\n```", "```py\nimg = \"bus.jpg\"\nresult = model.predict(img, save=True, imgsz=640, conf=0.5, iou=0.3)\n```", "```py\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\n```", "```py\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n```", "```py\nnano yolov8_tests.py\n```", "```py\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO(\"yolov8n_ncnn_model\")\n\n# Run inference\nimg = \"bus.jpg\"\nresult = model.predict(img, save=False, imgsz=640, conf=0.5, iou=0.3)\n\n# print the results\ninference_time = int(result[0].speed[\"inference\"])\nprint(f\"Inference Time: {inference_time} ms\")\nprint(f\"Number of objects: {len (result[0].boxes.cls)}\")\n```", "```py\npython yolov8_tests.py\n```", "```py\nnames:\n\n- box\n- wheel\nnc: 2\nroboflow:\n  license: CC BY 4.0\n  project: box-versus-wheel-auto-dataset\n  url: https://universe.roboflow.com/marcelo-rovai-riila/ \\\n     box-versus-wheel-auto-dataset/dataset/5\n  version: 5\n  workspace: marcelo-rovai-riila\ntest: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n      test/images\ntrain: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n       train/images\nval: /content/datasets/Box-versus-Wheel-auto-dataset-5/ \\\n     valid/images\n```", "```py\n    MODEL = 'yolov8n.pt'\n    IMG_SIZE = 640\n    EPOCHS = 25 # For a final project, you should consider\n                # at least 100 epochs\n    ```", "```py\n    !yolo task=detect mode=train model={MODEL} \\\n      data={dataset.location}/data.yaml \\\n      epochs={EPOCHS}\n      imgsz={IMG_SIZE} plots=True\n    ```", "```py\n!yolo task=detect mode=val model={HOME}/runs/detect/train/\\\n       weights/best.pt data={dataset.location}/data.yaml\n```", "```py\n!yolo task=detect mode=predict model={HOME}/runs/detect/train/\\\n     weights/best.pt conf=0.25 source={dataset.location}/test/\\\n     images save=True\n```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    ```", "```py\n    !scp -r /content/runs '/content/gdrive/MyDrive/\\\n     10_UNIFEI/Box_vs_Wheel_Project'\n    ```", "```py\ncd ..\npython\n```", "```py\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"./models/box_wheel_320_yolo.pt\")\n```", "```py\nimg = \"./images/1_box_1_wheel.jpg\"\nresult = model.predict(img, save=True, imgsz=320, conf=0.5, iou=0.3)\n```", "```py\nmodel_path = \"./models/ssd-mobilenet-v1-tflite-default-v1.tflite\"\n```", "```py\npython3 object_detection_app.py\n```", "```py\nmodel_path = \"./models/lite-model_efficientdet_lite0_\\\n detection_metadata_1.tflite\"\n```"]