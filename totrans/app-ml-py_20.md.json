["```py\n%matplotlib inline                                         \nsuppress_warnings = False\nimport os                                                     # to set current working directory \nimport os                                                     # to set current working directory \nimport numpy as np                                            # ndarrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn.metrics import mean_squared_error, r2_score      # specific measures to check our models\nfrom sklearn import linear_model                              # linear regression\nfrom sklearn.linear_model import Ridge                        # ridge regression implemented in scikit learn\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \n```", "```py\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n# Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"C:\\PGE337\")                                        # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1.0; seed = 71071\n\nyname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)\nxmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization \nymin = 0.0; ymax = 25.0    \nxlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting\nyunit = '%'; xunit = '$g/cm^{3}$'    \nXlabelunit = xlabel + ' (' + xunit + ')'\nylabelunit = ylabel + ' (' + yunit + ')'\n\n#df = pd.read_csv(\"Density_Por_data.csv\")                     # load the data from local current directory\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv\") # load the data from my github repo\ndf = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise\n    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray \n```", "```py\nx_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split\n# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame\n\ny = df[yname].values.reshape(len(df))                         # features as 1D vectors\nx = df[xname].values.reshape(len(df))\n\ndf_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames\ndf_test = pd.concat([x_test,y_test],axis=1) \n```", "```py\nprint('   Training DataFrame      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test) \n```", "```py\n Training DataFrame      Testing DataFrame \n```", "```py\nprint('     Training DataFrame         Testing DataFrame')\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame         Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nfreq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([xmin,xmax]); plt.legend(loc='upper right')   \n\nplt.subplot(222)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # plot the model\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title('Porosity vs Density')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nlinear_reg = linear_model.LinearRegression()                  # instantiate the model\n\nlinear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n```", "```py\ny_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared_linear = r2_score(df_test[yname].values, y_pred_linear)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)\nplt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nfor idata in range(0,len(df_test)):\n    if idata == 0:\n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_linear[idata]],color='grey',label=r'test $\\Delta_{y_i}$',zorder=1)\n    else:  \n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_linear[idata]],color='grey',zorder=1)\n\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.annotate(r'$r^2$ :' + str(np.round(r_squared_linear,2)),[1.97,15])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 13.0                                                     # lambda hyperparameter\n\nridge_reg = Ridge(alpha=lam)                                  # instantiate the model\n\nridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_ridge = ridge_reg.predict(x_model.reshape(10,1))      # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = r2_score(df_test[yname].values, y_pred_ridge)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='predictions')\n\nfor idata in range(0,len(df_test)):\n    if idata == 0:\n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_ridge[idata]],color='grey',label=r'test $\\Delta_{y_i}$',zorder=1)\n    else:  \n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_ridge[idata]],color='grey',zorder=1)\n\nplt.annotate('Ridge Regression Model Parameters:',[1.81,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='red',lw=2,zorder=1); plt.vlines(np.average(y_res_ridge),0,5.5,color='black',ls='--',zorder=10)\nplt.annotate('Residual Average = ' + str(np.round(np.average(y_res_ridge),2)),[np.average(y_res_ridge)+0.2,2.5],rotation=90.0)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n# Arrays to store the results\nncases = 6\nlamd_mat = np.logspace(-3,4,ncases)\nx_model = np.linspace(xmin,xmax,10)\nvar_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)\nmse_train = np.zeros(ncases); mse_test = np.zeros(ncases)\n\nfor ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values\n    ridge_reg = Ridge(alpha=lamd_mat[ilam])\n    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model\n\n    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model \n    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model \n    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)\n    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) \n\n    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))\n    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)\n    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test)    \n\n    if ilam <= 7:\n        plt.subplot(4,2,ilam+1)\n        plt.plot(x_model,y_model, color='red', linewidth=2,label='Linear Regression',zorder=100)\n        plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\n        plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n        plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\n        plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\n        plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\n        plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(round(lamd_mat[ilam],4)))\n        plt.xlabel(xname + ' (' + xunit + ')')\n        plt.ylabel(yname + ' (' + yunit + ')')\n        plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nncases = 100\nlamd_mat = np.logspace(-3,4,ncases)\nx_model = np.linspace(xmin,xmax,10)\nvar_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)\nmse_train = np.zeros(ncases); mse_test = np.zeros(ncases)\n\nfor ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values\n    ridge_reg = Ridge(alpha=lamd_mat[ilam])\n    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model\n\n    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model \n    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model \n    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)\n    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) \n\n    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))\n    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)\n    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test) \n\nplt.subplot(121)\nplt.plot(lamd_mat, var_explained_train,  color='darkorange', linewidth = 2, label = 'Train')\nplt.plot(lamd_mat, var_explained_test,  color='red', linewidth = 2, label = 'Test')\nplt.title('Variance Explained vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Variance Explained')\nplt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,1.0); \nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  \n\nplt.subplot(122)\nplt.plot(lamd_mat, mse_train,  color='darkorange', linewidth = 2, label = 'Train')\nplt.plot(lamd_mat, mse_test,  color='red', linewidth = 2, label = 'Test')\nplt.title('MSE vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Mean Square Error')\nplt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,15.0); plt.legend()\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nL = 200                                                       # the number of bootstrap realizations \n\nnsamples = 20                                                 # the number of samples in each bootstrap realization\nnlambda = 100                                                 # number of lambda values to evaluate\n\ncoef_mat = np.zeros(L)                                        # declare arrays to store the results\nvariance_coef = np.zeros(nlambda)\n\n#lamd_mat = np.linspace(0.0,100.0,nlambda) \nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,len(lambda_mat)):                         # loop over all lambda values \n    for l in range(0, L):                                     # loop over all bootstrap realizations\n        df_sample = df.sample(n = nsamples)                   # random sample (1 bootstrap)\n        ridge_reg = Ridge(alpha=lambda_mat[ilam])             # instantiate model\n        ridge_reg.fit(df_sample[xname].values.reshape(nsamples,1), df_sample[yname]) # fit model\n        coef_mat[l] = ridge_reg.coef_[0]                      # get the slope parameter\n    variance_coef[ilam] = np.var(coef_mat)                    # calculate the variance of the slopes over the L bootstraps\n\nplt.subplot(111)\nplt.plot(lambda_mat, variance_coef,  color='black', linewidth = 3, label = 'Slope Variance')\nplt.title('Model Variance vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Model Variance - Variance of the Slope Parameter, $b_1$')\nplt.xlim(0.01,100000.); plt.ylim(0.0,5.0); plt.xscale('log')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,nlambda):\n    ridge_reg = Ridge(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=ridge_reg, X= df['Density'].values.reshape(-1, 1), y=df['Porosity'].values, cv=10, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')\nplt.vlines(0.5,0,20,color='red',lw=2); plt.vlines(1000,0,20,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.4,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean',[1100,14.5],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.01,0.5],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([1000,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf_sample = df.copy(deep=True).sample(n=10,random_state=11)\nnoise_stdev = 3.0\nnp.random.seed(seed=15)\ndf_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))\n\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-4,3,nlambda)\nfor ilam in range(0,nlambda):\n    ridge_reg = Ridge(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=ridge_reg, X= df_sample['Density'].values.reshape(-1, 1), \n                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-4,1.0e3); plt.ylim(0.001,20.0); \nplt.xscale('log')\nplt.vlines(0.001,0,20,color='red',lw=2); plt.vlines(10,0,20,color='red',lw=2,zorder=10); \nplt.vlines(0.07,0,20,color='red',lw=2,zorder=10);\nplt.annotate('Linear Regression',[0.0007,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate(r'Ridge Tuned $\\lambda$',[0.055,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean',[7.4,14.5],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([10,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = False\nimport os                                                     # to set current working directory \nimport os                                                     # to set current working directory \nimport numpy as np                                            # ndarrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom sklearn.metrics import mean_squared_error, r2_score      # specific measures to check our models\nfrom sklearn import linear_model                              # linear regression\nfrom sklearn.linear_model import Ridge                        # ridge regression implemented in scikit learn\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \n```", "```py\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n# Function from iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"C:\\PGE337\")                                        # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 1.0; seed = 71071\n\nyname = 'Porosity'; xname = 'Density'                         # specify the predictor features (x2) and response feature (x1)\nxmin = 1.0; xmax = 2.5                                        # set minimums and maximums for visualization \nymin = 0.0; ymax = 25.0    \nxlabel = 'Porosity'; ylabel = 'Density'                       # specify the feature labels for plotting\nyunit = '%'; xunit = '$g/cm^{3}$'    \nXlabelunit = xlabel + ' (' + xunit + ')'\nylabelunit = ylabel + ' (' + yunit + ')'\n\n#df = pd.read_csv(\"Density_Por_data.csv\")                     # load the data from local current directory\ndf = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/Density_Por_data.csv\") # load the data from my github repo\ndf = df.sample(frac=1.0, random_state = 73073); df = df.reset_index() # extract 30% random to reduce the number of data\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df[yname] = df[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df)) # add noise\n    values = df._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray \n```", "```py\nx_train, x_test, y_train, y_test = train_test_split(df[xname],df[yname],test_size=0.25,random_state=73073) # train and test split\n# y_train = pd.DataFrame({yname:y_train.values}); y_test = pd.DataFrame({yname:y_test.values}) # optional to ensure response is a DataFrame\n\ny = df[yname].values.reshape(len(df))                         # features as 1D vectors\nx = df[xname].values.reshape(len(df))\n\ndf_train = pd.concat([x_train,y_train],axis=1)                # features as train and test DataFrames\ndf_test = pd.concat([x_test,y_test],axis=1) \n```", "```py\nprint('   Training DataFrame      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test) \n```", "```py\n Training DataFrame      Testing DataFrame \n```", "```py\nprint('     Training DataFrame         Testing DataFrame')\ndisplay_sidebyside(df_train.describe().loc[['count', 'mean', 'std', 'min', 'max']],df_test.describe().loc[['count', 'mean', 'std', 'min', 'max']]) \n```", "```py\n Training DataFrame         Testing DataFrame \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)\nfreq1,_,_ = plt.hist(x=df_train[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[xname],weights=None,bins=np.linspace(xmin,xmax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(xname + ' (' + xunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([xmin,xmax]); plt.legend(loc='upper right')   \n\nplt.subplot(222)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=True,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=True,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(yname + ' (' + yunit + ')'); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # plot the model\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.title('Porosity vs Density')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0.2, hspace=0.2)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nlinear_reg = linear_model.LinearRegression()                  # instantiate the model\n\nlinear_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_linear = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_linear_model = linear_reg.predict(x_model.reshape(-1, 1))   # predict at the withheld test data \n```", "```py\ny_pred_linear = linear_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared_linear = r2_score(df_test[yname].values, y_pred_linear)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_linear, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n# plt.scatter(df_test[xname], y_pred,color='grey',edgecolor='black',s = 40, alpha = 1.0, label = 'predictions',zorder=100)\nplt.scatter(df_test[xname], y_pred_linear,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_linear,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='Predictions')\n\nfor idata in range(0,len(df_test)):\n    if idata == 0:\n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_linear[idata]],color='grey',label=r'test $\\Delta_{y_i}$',zorder=1)\n    else:  \n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_linear[idata]],color='grey',zorder=1)\n\nplt.annotate('Linear Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(linear_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(linear_reg.intercept_,2)),[1.97,16])\nplt.annotate(r'$r^2$ :' + str(np.round(r_squared_linear,2)),[1.97,15])\nplt.title('Linear Regression Model, Porosity = f(Density)')\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_linear = y_pred_linear - df_test['Porosity'].values     # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_linear, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='black',ls='--',lw=2)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_linear),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_linear),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nlam = 13.0                                                     # lambda hyperparameter\n\nridge_reg = Ridge(alpha=lam)                                  # instantiate the model\n\nridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # train the model parameters\nx_model = np.linspace(xmin,xmax,10)\ny_model_ridge = ridge_reg.predict(x_model.reshape(10,1))      # predict with the fit model\n\nplt.subplot(111)                                              # plot the data, model with model parameters\nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ny_pred_ridge = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1)) # predict at test data\nr_squared = r2_score(df_test[yname].values, y_pred_ridge)\n\nplt.subplot(121)                                              # plot testing diagnostics \nplt.plot(x_model,y_model_ridge, color='red', linewidth=2,label='Linear Regression',zorder=100)\nplt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='train')\nplt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='test')\nplt.scatter(df_test[xname], y_pred_ridge,color='white',s=120,marker='o',linewidths=1.0, edgecolors=\"black\",zorder=300)\nplt.scatter(df_test[xname], y_pred_ridge,color='red',s=90,marker='*',linewidths=0.5, edgecolors=\"black\",zorder=320,label='predictions')\n\nfor idata in range(0,len(df_test)):\n    if idata == 0:\n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_ridge[idata]],color='grey',label=r'test $\\Delta_{y_i}$',zorder=1)\n    else:  \n        plt.plot([df_test.iloc[idata][xname],df_test.iloc[idata][xname]],[df_test.iloc[idata][yname],\n                        y_pred_ridge[idata]],color='grey',zorder=1)\n\nplt.annotate('Ridge Regression Model Parameters:',[1.81,18]) # add the model to the plot\nplt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\nplt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\nplt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(lam))\nplt.xlabel(xname + ' (' + xunit + ')')\nplt.ylabel(yname + ' (' + yunit + ')')\nplt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\ny_res_ridge = y_pred_ridge - df_test['Porosity'].values       # calculate the test residual\n\nplt.subplot(122)\nplt.hist(y_res_ridge, color = 'red', alpha = 0.8, edgecolor = 'black', bins = np.linspace(-5,5,20))\nplt.title(\"Error Residual at Testing Data\"); plt.xlabel(yname + ' True - Estimate (%)');plt.ylabel('Frequency')\nplt.vlines(0,0,5.5,color='red',lw=2,zorder=1); plt.vlines(np.average(y_res_ridge),0,5.5,color='black',ls='--',zorder=10)\nplt.annotate('Residual Average = ' + str(np.round(np.average(y_res_ridge),2)),[np.average(y_res_ridge)+0.2,2.5],rotation=90.0)\nplt.annotate('Test Error Residual:',[-4,4.7]) # add residual summary statistics\nplt.annotate(r'$\\overline{\\Delta{y}}$: ' + str(round(np.average(y_res_ridge),2)),[-4,4.4])\nplt.annotate(r'$\\sigma_{\\Delta{y}}$: ' + str(np.round(np.std(y_res_ridge),2)),[-4,4.1])\nadd_grid(); plt.xlim(-5,5); plt.ylim(0,5.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.1, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n# Arrays to store the results\nncases = 6\nlamd_mat = np.logspace(-3,4,ncases)\nx_model = np.linspace(xmin,xmax,10)\nvar_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)\nmse_train = np.zeros(ncases); mse_test = np.zeros(ncases)\n\nfor ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values\n    ridge_reg = Ridge(alpha=lamd_mat[ilam])\n    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model\n\n    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model \n    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model \n    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)\n    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) \n\n    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))\n    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)\n    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test)    \n\n    if ilam <= 7:\n        plt.subplot(4,2,ilam+1)\n        plt.plot(x_model,y_model, color='red', linewidth=2,label='Linear Regression',zorder=100)\n        plt.scatter(df_train[xname],df_train[yname],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\n        plt.scatter(df_test[xname],df_test[yname],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\n        plt.annotate('Ridge Regression Model Parameters:',[1.86,18]) # add the model to the plot\n        plt.annotate(r'$b_1$ :' + str(np.round(ridge_reg.coef_ [0],2)),[1.97,17])\n        plt.annotate(r'$b_0$ :' + str(np.round(ridge_reg.intercept_,2)),[1.97,16])\n        plt.title('Ridge Model, Regression of ' + yname + ' on ' + xname + r' with a $\\lambda = $' + str(round(lamd_mat[ilam],4)))\n        plt.xlabel(xname + ' (' + xunit + ')')\n        plt.ylabel(yname + ' (' + yunit + ')')\n        plt.legend(loc='upper right'); add_grid(); plt.xlim([xmin,xmax]); plt.ylim([ymin,ymax])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nncases = 100\nlamd_mat = np.logspace(-3,4,ncases)\nx_model = np.linspace(xmin,xmax,10)\nvar_explained_train = np.zeros(ncases); var_explained_test = np.zeros(ncases)\nmse_train = np.zeros(ncases); mse_test = np.zeros(ncases)\n\nfor ilam in range(0,len(lamd_mat)):                           # Loop over all lambda values\n    ridge_reg = Ridge(alpha=lamd_mat[ilam])\n    ridge_reg.fit(df_train[xname].values.reshape(len(df_train),1), df_train[yname]) # fit model\n\n    y_model = ridge_reg.predict(x_model.reshape(10,1))        # predict with the fit model \n    y_pred_train = ridge_reg.predict(df_train[xname].values.reshape(len(df_train),1)) # predict with the fit model \n    var_explained_train[ilam] = r2_score(df_train[yname].values, y_pred_train)\n    mse_train[ilam] = mean_squared_error(df_train[yname].values, y_pred_train) \n\n    y_pred_test = ridge_reg.predict(df_test[xname].values.reshape(len(df_test),1))\n    var_explained_test[ilam] = r2_score(df_test[yname].values, y_pred_test)\n    mse_test[ilam] = mean_squared_error(df_test[yname].values, y_pred_test) \n\nplt.subplot(121)\nplt.plot(lamd_mat, var_explained_train,  color='darkorange', linewidth = 2, label = 'Train')\nplt.plot(lamd_mat, var_explained_test,  color='red', linewidth = 2, label = 'Test')\nplt.title('Variance Explained vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Variance Explained')\nplt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,1.0); \nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  \n\nplt.subplot(122)\nplt.plot(lamd_mat, mse_train,  color='darkorange', linewidth = 2, label = 'Train')\nplt.plot(lamd_mat, mse_test,  color='red', linewidth = 2, label = 'Test')\nplt.title('MSE vs. Lambda'); plt.xlabel('Lambda'); plt.ylabel('Mean Square Error')\nplt.xlim(0.001,10000.); plt.xscale('log'); plt.ylim(0,15.0); plt.legend()\nplt.grid(True); plt.minorticks_on(); plt.grid(which='minor', linestyle=':', linewidth=0.5); plt.legend()  \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nL = 200                                                       # the number of bootstrap realizations \n\nnsamples = 20                                                 # the number of samples in each bootstrap realization\nnlambda = 100                                                 # number of lambda values to evaluate\n\ncoef_mat = np.zeros(L)                                        # declare arrays to store the results\nvariance_coef = np.zeros(nlambda)\n\n#lamd_mat = np.linspace(0.0,100.0,nlambda) \nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,len(lambda_mat)):                         # loop over all lambda values \n    for l in range(0, L):                                     # loop over all bootstrap realizations\n        df_sample = df.sample(n = nsamples)                   # random sample (1 bootstrap)\n        ridge_reg = Ridge(alpha=lambda_mat[ilam])             # instantiate model\n        ridge_reg.fit(df_sample[xname].values.reshape(nsamples,1), df_sample[yname]) # fit model\n        coef_mat[l] = ridge_reg.coef_[0]                      # get the slope parameter\n    variance_coef[ilam] = np.var(coef_mat)                    # calculate the variance of the slopes over the L bootstraps\n\nplt.subplot(111)\nplt.plot(lambda_mat, variance_coef,  color='black', linewidth = 3, label = 'Slope Variance')\nplt.title('Model Variance vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Model Variance - Variance of the Slope Parameter, $b_1$')\nplt.xlim(0.01,100000.); plt.ylim(0.0,5.0); plt.xscale('log')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-2,5,nlambda)\nfor ilam in range(0,nlambda):\n    ridge_reg = Ridge(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=ridge_reg, X= df['Density'].values.reshape(-1, 1), y=df['Porosity'].values, cv=10, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('Ridge Regression Test Mean Square Error vs. Lambda Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-2,1.0e5); plt.ylim(0.001,20.0); plt.xscale('log')\nplt.vlines(0.5,0,20,color='red',lw=2); plt.vlines(1000,0,20,color='red',lw=2,zorder=10)\nplt.annotate('Linear Regression',[0.4,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean',[1100,14.5],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.01,0.5],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([1000,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf_sample = df.copy(deep=True).sample(n=10,random_state=11)\nnoise_stdev = 3.0\nnp.random.seed(seed=15)\ndf_sample['Porosity'] = df_sample['Porosity'] + np.random.normal(0.0, noise_stdev, size=len(df_sample))\n\nscore = []                                                    # code modified from StackOverFlow by Dimosthenis\n\nnlambda = 100\nlambda_mat = np.logspace(-4,3,nlambda)\nfor ilam in range(0,nlambda):\n    ridge_reg = Ridge(alpha=lambda_mat[ilam])\n    scores = cross_val_score(estimator=ridge_reg, X= df_sample['Density'].values.reshape(-1, 1), \n                             y=df_sample['Porosity'].values, cv=2, n_jobs=4, scoring = \"neg_mean_squared_error\") # Perform 10-fold cross validation\n    score.append(abs(scores.mean()))\n\nplt.subplot(111)\nplt.plot(lambda_mat, score,  color='black', linewidth = 3, label = 'Test MSA',zorder=10)\nplt.title('Ridge Regression Test Mean Square Error vs. Ridge Hyperparameter'); plt.xlabel('Lambda'); plt.ylabel('Test Mean Square Error')\nplt.xlim(1.0e-4,1.0e3); plt.ylim(0.001,20.0); \nplt.xscale('log')\nplt.vlines(0.001,0,20,color='red',lw=2); plt.vlines(10,0,20,color='red',lw=2,zorder=10); \nplt.vlines(0.07,0,20,color='red',lw=2,zorder=10);\nplt.annotate('Linear Regression',[0.0007,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate(r'Ridge Tuned $\\lambda$',[0.055,12.5],rotation=90.0,color='red',zorder=10)\nplt.annotate('Mean',[7.4,14.5],rotation=90.0,color='red',zorder=10)\nplt.fill_between([0.0001,0.001],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.fill_between([10,100000],[0,0],[20,20],color='grey',alpha=0.3,zorder=1)\nplt.grid(which='both')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```"]