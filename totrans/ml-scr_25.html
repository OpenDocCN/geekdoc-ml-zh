<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Concept</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Concept</h1>
<blockquote>原文：<a href="https://dafriedman97.github.io/mlbook/content/c6/concept.html">https://dafriedman97.github.io/mlbook/content/c6/concept.html</a></blockquote>

<p>Due to their high variance, decision trees often fail to reach a level of precision comparable to other predictive algorithms. In the previous chapter, we introduced several ways to minimize the variance of a single decision tree, such as through pruning or direct size regulation. This chapter discusses another approach: <em>ensemble methods</em>. Ensemble methods combine the output of multiple simple models, often called “learners”, in order to create a final model with lower variance.</p>
<p>We will introduce ensemble methods in the context of tree-based learners, though ensemble methods can be applied to a wide range of learning algorithms. That said, the structure of decision trees makes ensemble methods particularly valuable. Here we discuss three tree-based ensemble methods: <em>bagging</em>, <em>random forests</em>, and <em>boosting</em>.</p>
<p>An example demonstrating the power of ensemble methods is given below. Using the <code class="docutils literal notranslate"><span class="pre">tips</span></code> dataset from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, we build several tree-based learners. Then for <span class="math notranslate nohighlight">\(b = 1, 2,  \dots, 30\)</span>, we use bagging to average the results of the <span class="math notranslate nohighlight">\(b\)</span> learners. For each bagged model, we also calculate the out-of-sample <span class="math notranslate nohighlight">\(RSS\)</span>. The blue line below shows the <span class="math notranslate nohighlight">\(RSS\)</span> for each bagged model—which clearly decreases with <span class="math notranslate nohighlight">\(b\)</span>—and the red line shows the <span class="math notranslate nohighlight">\(RSS\)</span> for a single decision tree. By averaging many trees, rather than relying on a single one, we are able to improve the precision of our model.</p>
<p><img alt="" src="../Images/5c969d05faacb005c4057705b08f0b77.png" data-original-src="https://dafriedman97.github.io/mlbook/_images/ensemble_accuracy.png"/></p>
<div class="toctree-wrapper compound">
</div>
    
</body>
</html>