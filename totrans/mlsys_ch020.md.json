["```py\n# Freeze all parameters\nfor name, param in model.named_parameters():\n    param.requires_grad = False\n\n# Enable gradients for bias parameters only\nfor name, param in model.named_parameters():\n    if \"bias\" in name:\n        param.requires_grad = True\n```", "```py\nclass Adapter(nn.Module):\n    def __init__(self, dim, bottleneck_dim):\n        super().__init__()\n        self.down = nn.Linear(dim, bottleneck_dim)\n        self.up = nn.Linear(bottleneck_dim, dim)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        return x + self.up(self.activation(self.down(x)))\n```", "```py\n# Assume model has named layers: ['conv1', 'conv2', 'fc']\n# We selectively update only conv2 and fc\n\nfor name, param in model.named_parameters():\n    if \"conv2\" in name or \"fc\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n```", "```py\n# Replay Buffer Techniques\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = []\n        self.index = 0\n\n    def store(self, feature_vec, label):\n        if len(self.buffer) < self.capacity:\n            self.buffer.append((feature_vec, label))\n        else:\n            self.buffer[self.index] = (feature_vec, label)\n        self.index = (self.index + 1) % self.capacity\n\n    def sample(self, k):\n        return random.sample(self.buffer, min(k, len(self.buffer)))\n```"]