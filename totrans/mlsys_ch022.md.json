["```py\ndef adversarial_training_step(model, data, labels, epsilon=0.1):\n    # Generate adversarial examples using FGSM\n    data.requires_grad_(True)\n    outputs = model(data)\n    loss = F.cross_entropy(outputs, labels)\n\n    model.zero_grad()\n    loss.backward()\n\n    # Create adversarial perturbation and mix with clean data\n    adv_data = data + epsilon * data.grad.sign()\n    adv_data = torch.clamp(adv_data, 0, 1)\n\n    mixed_data = torch.cat([data, adv_data])\n    mixed_labels = torch.cat([labels, labels])\n\n    return F.cross_entropy(model(mixed_data), mixed_labels)\n```", "```py\nfrom scipy.stats import ks_2samp\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\n\ndef detect_distribution_shift(\n    reference_data, new_data, threshold=0.05\n):\n    \"\"\"Detect distribution shift using statistical tests\"\"\"\n\n    # Kolmogorov-Smirnov test for feature-wise comparison\n    ks_pvalues = []\n    for feature_idx in range(new_data.shape[1]):\n        _, p_value = ks_2samp(\n            reference_data[:, feature_idx], new_data[:, feature_idx]\n        )\n        ks_pvalues.append(p_value)\n\n    # Domain classifier to detect overall distributional\n    # differences\n    X_combined = np.vstack([reference_data, new_data])\n    y_labels = np.concatenate(\n        [np.zeros(len(reference_data)), np.ones(len(new_data))]\n    )\n\n    clf = RandomForestClassifier(n_estimators=50, random_state=42)\n    clf.fit(X_combined, y_labels)\n    domain_auc = roc_auc_score(\n        y_labels, clf.predict_proba(X_combined)[:, 1]\n    )\n\n    return {\n        \"ks_shift_detected\": any(p < threshold for p in ks_pvalues),\n        \"domain_shift_detected\": domain_auc > 0.8,\n        \"severity_score\": domain_auc,\n    }\n```"]