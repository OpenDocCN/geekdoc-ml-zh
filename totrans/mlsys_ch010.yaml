- en: DNN Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN æ¶æ„
- en: '*DALLÂ·E 3 Prompt: A visually striking rectangular image illustrating the interplay
    between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected
    with machine learning systems. The composition features neural network diagrams
    blending seamlessly with representations of computational systems such as processors,
    graphs, and data streams. Bright neon tones contrast against a dark futuristic
    background, symbolizing cutting-edge technology and intricate system complexity.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALLÂ·E 3 æç¤ºï¼šä¸€ä¸ªè§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„çŸ©å½¢å›¾åƒï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ CNNã€RNN å’Œæ³¨æ„åŠ›ç½‘ç»œï¼‰ä¸æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ç›¸äº’ä½œç”¨ï¼Œç›¸äº’è¿æ¥ã€‚æ„å›¾åŒ…æ‹¬ç¥ç»ç½‘ç»œå›¾ä¸å¤„ç†å™¨ã€å›¾å’Œæ•°æ®æµç­‰è®¡ç®—ç³»ç»Ÿè¡¨ç¤ºçš„å®Œç¾èåˆã€‚æ˜äº®çš„éœ“è™¹è‰²è°ƒä¸é»‘æš—çš„æœªæ¥èƒŒæ™¯å½¢æˆå¯¹æ¯”ï¼Œè±¡å¾ç€å°–ç«¯æŠ€æœ¯å’Œå¤æ‚çš„ç³»ç»Ÿå¤æ‚æ€§ã€‚*'
- en: '![](../media/file53.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file53.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®çš„
- en: '*Why do architectural choices in neural networks affect system design decisions
    that determine computational feasibility, hardware requirements, and deployment
    constraints?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¸­çš„æ¶æ„é€‰æ‹©ä¼šå½±å“å†³å®šè®¡ç®—å¯è¡Œæ€§ã€ç¡¬ä»¶éœ€æ±‚å’Œéƒ¨ç½²çº¦æŸçš„ç³»ç»Ÿè®¾è®¡å†³ç­–ï¼Ÿ*'
- en: 'Neural network architectures represent engineering decisions that directly
    determine system performance and deployment viability. Each architectural choice
    creates cascading effects throughout the system stack: memory bandwidth demands,
    computational complexity patterns, parallelization opportunities, and hardware
    acceleration compatibility. Understanding these architectural implications enables
    engineers to make informed trade-offs between model capability and system constraints,
    predict computational bottlenecks before they occur, and select appropriate hardware
    platforms. Architectural decisions determine whether machine learning systems
    meet performance requirements within available computational resources. This understanding
    proves essential for building scalable AI systems that can be deployed effectively
    across diverse environments.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„ä»£è¡¨äº†ç›´æ¥å†³å®šç³»ç»Ÿæ€§èƒ½å’Œéƒ¨ç½²å¯è¡Œæ€§çš„å·¥ç¨‹å†³ç­–ã€‚æ¯ä¸ªæ¶æ„é€‰æ‹©éƒ½ä¼šåœ¨æ•´ä¸ªç³»ç»Ÿå †æ ˆä¸­äº§ç”Ÿçº§è”æ•ˆåº”ï¼šå†…å­˜å¸¦å®½éœ€æ±‚ã€è®¡ç®—å¤æ‚æ€§æ¨¡å¼ã€å¹¶è¡ŒåŒ–æœºä¼šå’Œç¡¬ä»¶åŠ é€Ÿå…¼å®¹æ€§ã€‚ç†è§£è¿™äº›æ¶æ„å½±å“ä½¿å·¥ç¨‹å¸ˆèƒ½å¤Ÿåœ¨æ¨¡å‹èƒ½åŠ›å’Œç³»ç»Ÿçº¦æŸä¹‹é—´åšå‡ºæ˜æ™ºçš„æƒè¡¡ï¼Œåœ¨é—®é¢˜å‘ç”Ÿä¹‹å‰é¢„æµ‹è®¡ç®—ç“¶é¢ˆï¼Œå¹¶é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶å¹³å°ã€‚æ¶æ„å†³ç­–å†³å®šäº†æœºå™¨å­¦ä¹ ç³»ç»Ÿæ˜¯å¦èƒ½åœ¨å¯ç”¨çš„è®¡ç®—èµ„æºå†…æ»¡è¶³æ€§èƒ½è¦æ±‚ã€‚è¿™ç§ç†è§£å¯¹äºæ„å»ºå¯æ‰©å±•çš„
    AI ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒä¸­æœ‰æ•ˆéƒ¨ç½²ã€‚
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ ç›®æ ‡**'
- en: Distinguish the computational characteristics and inductive biases of the four
    main neural network architectural families (MLPs, CNNs, RNNs, Transformers)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒºåˆ†å››ç§ä¸»è¦ç¥ç»ç½‘ç»œæ¶æ„å®¶æ—ï¼ˆMLPsã€CNNsã€RNNsã€Transformersï¼‰çš„è®¡ç®—ç‰¹æ€§å’Œå½’çº³åå·®
- en: Analyze how architectural design choices determine computational complexity,
    memory requirements, and parallelization opportunities
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ææ¶æ„è®¾è®¡é€‰æ‹©å¦‚ä½•å†³å®šè®¡ç®—å¤æ‚æ€§ã€å†…å­˜éœ€æ±‚å’Œå¹¶è¡ŒåŒ–æœºä¼š
- en: Evaluate the system-level implications of architectural patterns on hardware
    utilization, memory bandwidth, and deployment constraints
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¶æ„æ¨¡å¼å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€å†…å­˜å¸¦å®½å’Œéƒ¨ç½²çº¦æŸçš„ç³»ç»Ÿçº§å½±å“
- en: Apply the architecture selection framework to match data characteristics with
    appropriate neural network designs for specific applications
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ¶æ„é€‰æ‹©æ¡†æ¶åº”ç”¨äºåŒ¹é…æ•°æ®ç‰¹å¾ä¸ç‰¹å®šåº”ç”¨ä¸­é€‚å½“çš„ç¥ç»ç½‘ç»œè®¾è®¡
- en: Assess computational and memory trade-offs between different architectural approaches
    using complexity analysis
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤æ‚æ€§åˆ†æè¯„ä¼°ä¸åŒæ¶æ„æ–¹æ³•ä¹‹é—´çš„è®¡ç®—å’Œå†…å­˜æƒè¡¡
- en: Examine how fundamental computational primitives (matrix multiplication, convolution,
    attention) map to hardware acceleration opportunities
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ£€æŸ¥åŸºæœ¬è®¡ç®—åŸè¯­ï¼ˆçŸ©é˜µä¹˜æ³•ã€å·ç§¯ã€æ³¨æ„åŠ›ï¼‰å¦‚ä½•æ˜ å°„åˆ°ç¡¬ä»¶åŠ é€Ÿæœºä¼š
- en: Critique common architectural selection fallacies and their impact on system
    performance and deployment success
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰¹åˆ¤å¸¸è§çš„æ¶æ„é€‰æ‹©è°¬è¯¯åŠå…¶å¯¹ç³»ç»Ÿæ€§èƒ½å’Œéƒ¨ç½²æˆåŠŸçš„å½±å“
- en: Synthesize the unified inductive bias framework explaining architecture-data
    compatibility patterns
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»¼åˆç»Ÿä¸€çš„å½’çº³åå·®æ¡†æ¶ï¼Œè§£é‡Šæ¶æ„-æ•°æ®å…¼å®¹æ€§æ¨¡å¼
- en: Architectural Principles and Engineering Trade-offs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¶æ„åŸåˆ™ä¸å·¥ç¨‹æƒè¡¡
- en: The systematic organization of neural computations into effective architectures
    represents one of the most consequential developments in contemporary machine
    learning systems. Building on the mathematical foundations of neural computation
    established in [ChapterÂ 3](ch009.xhtml#sec-dl-primer), this chapter investigates
    the architectural principles that govern how operations (matrix multiplications,
    nonlinear activations, and gradient-based optimization) are structured to address
    complex computational problems. This architectural perspective bridges the gap
    between mathematical theory and practical systems implementation, examining how
    design choices at the network level determine system-wide performance characteristics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç¥ç»ç½‘ç»œè®¡ç®—ç³»ç»Ÿåœ°ç»„ç»‡æˆæœ‰æ•ˆçš„æ¶æ„ï¼Œæ˜¯å½“ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­æœ€å…·å½±å“åŠ›çš„è¿›å±•ä¹‹ä¸€ã€‚åŸºäº[ç¬¬3ç« ](ch009.xhtml#sec-dl-primer)ä¸­å»ºç«‹çš„ç¥ç»ç½‘ç»œè®¡ç®—çš„æ•°å­¦åŸºç¡€ï¼Œæœ¬ç« æ¢è®¨äº†æ”¯é…æ“ä½œï¼ˆçŸ©é˜µä¹˜æ³•ã€éçº¿æ€§æ¿€æ´»å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼‰å¦‚ä½•ç»“æ„åŒ–ä»¥è§£å†³å¤æ‚è®¡ç®—é—®é¢˜çš„æ¶æ„åŸåˆ™ã€‚è¿™ç§æ¶æ„è§†è§’æ¶èµ·äº†æ•°å­¦ç†è®ºä¸å®é™…ç³»ç»Ÿå®ç°ä¹‹é—´çš„æ¡¥æ¢ï¼Œè€ƒå¯Ÿäº†ç½‘ç»œå±‚é¢çš„è®¾è®¡é€‰æ‹©å¦‚ä½•å†³å®šæ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½ç‰¹å¾ã€‚
- en: 'This chapter centers on an engineering trade-off that permeates machine learning
    systems design. While mathematical theory, particularly universal approximation
    results, establishes that neural networks possess remarkable representational
    flexibility, practical deployment necessitates computational efficiency achievable
    only through judicious architectural specialization. This tension manifests across
    multiple dimensions: theoretical universality versus computational tractability,
    representational completeness versus memory efficiency, and mathematical generality
    versus domain-specific optimization. The resolution of these tensions through
    architectural innovation constitutes a primary driver of progress in machine learning
    systems.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« èšç„¦äºè´¯ç©¿æœºå™¨å­¦ä¹ ç³»ç»Ÿè®¾è®¡çš„å·¥ç¨‹æƒè¡¡ã€‚è™½ç„¶æ•°å­¦ç†è®ºï¼Œå°¤å…¶æ˜¯é€šç”¨é€¼è¿‘ç»“æœï¼Œè¡¨æ˜ç¥ç»ç½‘ç»œå…·æœ‰éå‡¡çš„è¡¨ç¤ºçµæ´»æ€§ï¼Œä½†å®é™…éƒ¨ç½²éœ€è¦é€šè¿‡æ˜æ™ºçš„æ¶æ„ä¸“é—¨åŒ–æ‰èƒ½å®ç°çš„è®¡ç®—æ•ˆç‡ã€‚è¿™ç§ç´§å¼ å…³ç³»åœ¨å¤šä¸ªç»´åº¦ä¸Šä½“ç°å‡ºæ¥ï¼šç†è®ºä¸Šçš„é€šç”¨æ€§ä¸è®¡ç®—ä¸Šçš„å¯å¤„ç†æ€§ï¼Œè¡¨ç¤ºçš„å®Œå¤‡æ€§ä¸å†…å­˜æ•ˆç‡ï¼Œä»¥åŠæ•°å­¦çš„æ™®éæ€§ä¸é¢†åŸŸç‰¹å®šä¼˜åŒ–ã€‚é€šè¿‡æ¶æ„åˆ›æ–°è§£å†³è¿™äº›ç´§å¼ å…³ç³»æ˜¯æ¨åŠ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿè¿›æ­¥çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚
- en: Contemporary neural architectures emerge from systematic responses to specific
    computational challenges encountered when deploying general mathematical frameworks
    on structured data. Each architectural paradigm embodies distinct inductive biases
    (implicit assumptions about data structure and relationships) that enable efficient
    learning while constraining the hypothesis space in domain-appropriate ways. These
    architectural innovations represent engineering solutions to the challenge of
    organizing computational primitives into patterns that achieve optimal balance
    between representational capacity and computational efficiency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä»£ç¥ç»ç½‘ç»œæ¶æ„æºäºå¯¹åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šéƒ¨ç½²é€šç”¨æ•°å­¦æ¡†æ¶æ—¶é‡åˆ°çš„å…·ä½“è®¡ç®—æŒ‘æˆ˜çš„ç³»ç»Ÿå“åº”ã€‚æ¯ä¸ªæ¶æ„èŒƒå¼éƒ½ä½“ç°äº†ç‹¬ç‰¹çš„å½’çº³åå·®ï¼ˆå…³äºæ•°æ®ç»“æ„å’Œå…³ç³»çš„éšå«å‡è®¾ï¼‰ï¼Œè¿™ä½¿é«˜æ•ˆå­¦ä¹ æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä»¥é¢†åŸŸé€‚å½“çš„æ–¹å¼çº¦æŸå‡è®¾ç©ºé—´ã€‚è¿™äº›æ¶æ„åˆ›æ–°ä»£è¡¨äº†å°†è®¡ç®—åŸè¯­ç»„ç»‡æˆæ¨¡å¼ä»¥å®ç°è¡¨ç¤ºèƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´æœ€ä½³å¹³è¡¡çš„å·¥ç¨‹è§£å†³æ–¹æ¡ˆã€‚
- en: This chapter examines four architectural families that collectively define the
    conceptual landscape of modern neural computation. Multi-Layer Perceptrons serve
    as the canonical implementation of universal approximation theory, demonstrating
    how dense connectivity enables general pattern recognition while illustrating
    the computational costs of architectural generality. Convolutional Neural Networks
    introduce the paradigm of spatial architectural specialization, exploiting translational
    invariance and local connectivity to achieve significant efficiency gains while
    preserving representational power for spatial data. Recurrent Neural Networks
    extend architectural specialization to temporal domains, incorporating explicit
    memory mechanisms that enable sequential processing capabilities absent from feedforward
    architectures. Attention mechanisms and Transformer architectures represent the
    current evolutionary frontier, replacing fixed structural assumptions with dynamic,
    content-dependent computation that achieves remarkable capability while maintaining
    computational efficiency through parallelizable operations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« è€ƒå¯Ÿäº†å››ä¸ªæ¶æ„å®¶æ—ï¼Œå®ƒä»¬å…±åŒå®šä¹‰äº†ç°ä»£ç¥ç»ç½‘ç»œçš„æ¦‚å¿µæ™¯è§‚ã€‚å¤šå±‚æ„ŸçŸ¥å™¨ä½œä¸ºé€šç”¨é€¼è¿‘ç†è®ºçš„å…¸èŒƒå®ç°ï¼Œå±•ç¤ºäº†å¯†é›†è¿æ¥å¦‚ä½•å®ç°é€šç”¨æ¨¡å¼è¯†åˆ«ï¼Œå¹¶è¯´æ˜äº†æ¶æ„é€šç”¨æ€§çš„è®¡ç®—æˆæœ¬ã€‚å·ç§¯ç¥ç»ç½‘ç»œå¼•å…¥äº†ç©ºé—´æ¶æ„ä¸“ä¸šåŒ–çš„èŒƒä¾‹ï¼Œåˆ©ç”¨å¹³ç§»ä¸å˜æ€§å’Œå±€éƒ¨è¿æ¥æ¥å®ç°æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒå¯¹ç©ºé—´æ•°æ®çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å¾ªç¯ç¥ç»ç½‘ç»œå°†æ¶æ„ä¸“ä¸šåŒ–æ‰©å±•åˆ°æ—¶é—´åŸŸï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼çš„è®°å¿†æœºåˆ¶ï¼Œä½¿åºåˆ—å¤„ç†èƒ½åŠ›æˆä¸ºå‰é¦ˆæ¶æ„æ‰€ä¸å…·å¤‡çš„ã€‚æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformeræ¶æ„ä»£è¡¨äº†å½“å‰çš„è¿›åŒ–å‰æ²¿ï¼Œç”¨åŠ¨æ€çš„ã€å†…å®¹ç›¸å…³çš„è®¡ç®—å–ä»£äº†å›ºå®šçš„ç»“æ„å‡è®¾ï¼Œåœ¨å¹¶è¡ŒåŒ–æ“ä½œä¸­ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚
- en: The systems engineering significance of these architectural patterns extends
    beyond mere algorithmic considerations. Each architectural choice creates distinct
    computational signatures that propagate through every level of the implementation
    stack, determining memory access patterns, parallelization strategies, hardware
    utilization characteristics, and ultimately system feasibility within resource
    constraints. Understanding these architectural implications proves essential for
    engineers responsible for system design, resource allocation, and performance
    optimization in production environments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¶æ„æ¨¡å¼åœ¨ç³»ç»Ÿå·¥ç¨‹ä¸­çš„é‡è¦æ€§ä¸ä»…é™äºç®—æ³•è€ƒè™‘ã€‚æ¯ä¸ªæ¶æ„é€‰æ‹©éƒ½ä¼šåˆ›å»ºç‹¬ç‰¹çš„è®¡ç®—ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¼šä¼ æ’­åˆ°å®ç°å †æ ˆçš„æ¯ä¸€å±‚ï¼Œä»è€Œç¡®å®šå†…å­˜è®¿é—®æ¨¡å¼ã€å¹¶è¡ŒåŒ–ç­–ç•¥ã€ç¡¬ä»¶åˆ©ç”¨ç‰¹æ€§ï¼Œå¹¶åœ¨èµ„æºçº¦æŸä¸‹æœ€ç»ˆå†³å®šç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚ç†è§£è¿™äº›æ¶æ„å½±å“å¯¹äºè´Ÿè´£ç³»ç»Ÿè®¾è®¡ã€èµ„æºåˆ†é…å’Œç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–çš„å·¥ç¨‹å¸ˆæ¥è¯´è‡³å…³é‡è¦ã€‚
- en: This chapter adopts a systems-oriented analytical framework that illuminates
    the relationships between architectural abstractions and concrete implementation
    requirements. For each architectural family, we systematically examine the computational
    primitives that determine hardware resource demands, the organizational principles
    that enable efficient algorithmic implementation, the memory hierarchy implications
    that affect system scalability, and the trade-offs between architectural sophistication
    and computational overhead.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« é‡‡ç”¨äº†ä¸€ç§é¢å‘ç³»ç»Ÿçš„åˆ†ææ¡†æ¶ï¼Œé˜æ˜äº†æ¶æ„æŠ½è±¡ä¸å…·ä½“å®ç°è¦æ±‚ä¹‹é—´çš„å…³ç³»ã€‚å¯¹äºæ¯ä¸ªæ¶æ„å®¶æ—ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ£€æŸ¥ç¡®å®šç¡¬ä»¶èµ„æºéœ€æ±‚çš„è®¡ç®—åŸè¯­ã€å®ç°é«˜æ•ˆç®—æ³•çš„ç»„ç»‡åŸåˆ™ã€å½±å“ç³»ç»Ÿå¯æ‰©å±•æ€§çš„å†…å­˜å±‚æ¬¡å½±å“ï¼Œä»¥åŠæ¶æ„å¤æ‚æ€§ä¸è®¡ç®—å¼€é”€ä¹‹é—´çš„æƒè¡¡ã€‚
- en: The analytical approach builds systematically upon the neural network foundations
    established in [ChapterÂ 3](ch009.xhtml#sec-dl-primer), extending core concepts
    of forward propagation, backpropagation, and gradient-based optimization by examining
    how architectural specialization organizes these operations to exploit problem-specific
    structure. Understanding the evolutionary relationships connecting these architectural
    paradigms and their distinct computational characteristics, practitioners develop
    the conceptual tools necessary for principled decision-making regarding architectural
    selection, resource planning, and system optimization in complex deployment scenarios.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ææ–¹æ³•ç³»ç»Ÿåœ°å»ºç«‹åœ¨[ç¬¬3ç« ](ch009.xhtml#sec-dl-primer)ä¸­å»ºç«‹çš„ç¥ç»ç½‘ç»œåŸºç¡€ä¹‹ä¸Šï¼Œé€šè¿‡è€ƒå¯Ÿæ¶æ„ä¸“ä¸šåŒ–å¦‚ä½•ç»„ç»‡è¿™äº›æ“ä½œä»¥åˆ©ç”¨ç‰¹å®šé—®é¢˜çš„ç»“æ„ï¼Œæ‰©å±•äº†å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚ç†è§£å°†è¿™äº›æ¶æ„èŒƒå¼åŠå…¶ç‹¬ç‰¹çš„è®¡ç®—ç‰¹æ€§è¿æ¥èµ·æ¥çš„è¿›åŒ–å…³ç³»ï¼Œä»ä¸šè€…å‘å±•äº†è¿›è¡Œæ¶æ„é€‰æ‹©ã€èµ„æºè§„åˆ’å’Œå¤æ‚éƒ¨ç½²åœºæ™¯ä¸­ç³»ç»Ÿä¼˜åŒ–çš„åŸåˆ™æ€§å†³ç­–æ‰€å¿…éœ€çš„æ¦‚å¿µå·¥å…·ã€‚
- en: 'Multi-Layer Perceptrons: Dense Pattern Processing'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ï¼šå¯†é›†æ¨¡å¼å¤„ç†
- en: 'Multi-Layer Perceptrons (MLPs) represent the fully-connected architectures
    introduced in [ChapterÂ 3](ch009.xhtml#sec-dl-primer), now examined through the
    lens of architectural choice and systems trade-offs. MLPs embody an inductive
    bias: **they assume no prior structure in the data, allowing any input to relate
    to any output**. This architectural choice enables maximum flexibility by treating
    all input relationships as equally plausible, making MLPs versatile but computationally
    intensive compared to specialized alternatives. Their computational power was
    established theoretically by the Universal Approximation Theorem (UAT)[1](#fn1)
    ([Cybenko 1989](ch058.xhtml#ref-cybenko1989approximation); [Hornik, Stinchcombe,
    and White 1989](ch058.xhtml#ref-hornik1989multilayer)), which we encountered as
    a footnote in [ChapterÂ 3](ch009.xhtml#sec-dl-primer). This theorem states that
    a sufficiently large MLP with non-linear activation functions can approximate
    any continuous function on a compact domain, given suitable weights and biases.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä»£è¡¨äº†åœ¨ç¬¬3ç« ï¼ˆch009.xhtml#sec-dl-primerï¼‰ä¸­å¼•å…¥çš„å…¨è¿æ¥æ¶æ„ï¼Œç°åœ¨é€šè¿‡æ¶æ„é€‰æ‹©å’Œç³»ç»Ÿæƒè¡¡çš„è§†è§’æ¥å®¡è§†ã€‚MLPä½“ç°äº†ä¸€ç§å½’çº³åå·®ï¼š**å®ƒä»¬å‡è®¾æ•°æ®ä¸­æ²¡æœ‰å…ˆéªŒç»“æ„ï¼Œå…è®¸ä»»ä½•è¾“å…¥ä¸ä»»ä½•è¾“å‡ºç›¸å…³è”**ã€‚è¿™ç§æ¶æ„é€‰æ‹©é€šè¿‡å°†æ‰€æœ‰è¾“å…¥å…³ç³»è§†ä¸ºåŒç­‰å¯èƒ½çš„ï¼Œä»è€Œæä¾›äº†æœ€å¤§çµæ´»æ€§ï¼Œä½¿MLPä¸ä¸“ç”¨æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”å…·æœ‰å¤šåŠŸèƒ½æ€§ä½†è®¡ç®—å¯†é›†ã€‚å®ƒä»¬çš„è®¡ç®—èƒ½åŠ›é€šè¿‡é€šç”¨é€¼è¿‘å®šç†ï¼ˆUATï¼‰[1](#fn1)ï¼ˆ[Cybenko
    1989](ch058.xhtml#ref-cybenko1989approximation)ï¼›[Hornik, Stinchcombe, and White
    1989](ch058.xhtml#ref-hornik1989multilayer)ï¼‰ä»ç†è®ºä¸Šå¾—åˆ°è¯å®ï¼Œæˆ‘ä»¬åœ¨ç¬¬3ç« ï¼ˆch009.xhtml#sec-dl-primerï¼‰ä¸­ä½œä¸ºè„šæ³¨é‡åˆ°äº†å®ƒã€‚è¯¥å®šç†æŒ‡å‡ºï¼Œå…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„è¶³å¤Ÿå¤§çš„MLPå¯ä»¥åœ¨ç´§å‡‘åŸŸä¸Šé€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ï¼Œå‰ææ˜¯ç»™å®šé€‚å½“çš„æƒé‡å’Œåå·®ã€‚
- en: '***Multi-Layer Perceptrons (MLPs)*** are *fully-connected neural networks*
    where every neuron connects to all neurons in adjacent layers, providing *maximum
    flexibility* through *universal approximation* at the cost of *high parameter
    counts* and *computational intensity*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰***æ˜¯*å…¨è¿æ¥ç¥ç»ç½‘ç»œ*ï¼Œå…¶ä¸­æ¯ä¸ªç¥ç»å…ƒéƒ½ä¸ç›¸é‚»å±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿æ¥ï¼Œé€šè¿‡*é€šç”¨é€¼è¿‘*æä¾›*æœ€å¤§çµæ´»æ€§*ï¼Œä½†ä»£ä»·æ˜¯*é«˜å‚æ•°è®¡æ•°*å’Œ*è®¡ç®—å¼ºåº¦*ã€‚'
- en: 'In practice, the UAT explains why MLPs succeed across diverse tasks while revealing
    the gap between theoretical capability and practical implementation. The theorem
    guarantees that *some* MLP can approximate any function, yet provides no guidance
    on requisite network size or weight determination. This gap becomes critical in
    real-world applications: while MLPs can theoretically solve any pattern recognition
    problem, achieving this capability may require impractically large networks or
    extensive computation. This theoretical power drives the selection of MLPs for
    tabular data, recommendation systems, and problems where input relationships are
    unknown, while these practical limitations motivated the development of specialized
    architectures that exploit data structure for computational efficiency, as detailed
    in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼ŒUATï¼ˆç”¨æˆ·æ¥å—æµ‹è¯•ï¼‰è§£é‡Šäº†ä¸ºä»€ä¹ˆMLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—æˆåŠŸï¼ŒåŒæ—¶æ­ç¤ºäº†ç†è®ºèƒ½åŠ›ä¸å®é™…å®æ–½ä¹‹é—´çš„å·®è·ã€‚å®šç†ä¿è¯äº†*æŸäº›*MLPå¯ä»¥é€¼è¿‘ä»»ä½•å‡½æ•°ï¼Œä½†å¹¶æœªæä¾›å…³äºæ‰€éœ€ç½‘ç»œå¤§å°æˆ–æƒé‡ç¡®å®šçš„æŒ‡å¯¼ã€‚è¿™ç§å·®è·åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­å˜å¾—è‡³å…³é‡è¦ï¼šè™½ç„¶MLPåœ¨ç†è®ºä¸Šå¯ä»¥è§£å†³ä»»ä½•æ¨¡å¼è¯†åˆ«é—®é¢˜ï¼Œä½†è¦å®ç°è¿™ç§èƒ½åŠ›å¯èƒ½éœ€è¦ä¸åˆ‡å®é™…çš„å¤§å‹ç½‘ç»œæˆ–å¤§é‡çš„è®¡ç®—ã€‚è¿™ç§ç†è®ºèƒ½åŠ›æ¨åŠ¨äº†MLPåœ¨è¡¨æ ¼æ•°æ®ã€æ¨èç³»ç»Ÿå’Œè¾“å…¥å…³ç³»æœªçŸ¥çš„é—®é¢˜ä¸­çš„åº”ç”¨é€‰æ‹©ï¼Œè€Œè¿™äº›å®é™…é™åˆ¶ä¿ƒä½¿å¼€å‘äº†ä¸“é—¨æ¶æ„ï¼Œè¿™äº›æ¶æ„åˆ©ç”¨æ•°æ®ç»“æ„ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œå¦‚[ç¬¬4.1èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)ä¸­è¯¦ç»†æ‰€è¿°ã€‚
- en: When applied to the MNIST handwritten digit recognition challenge[2](#fn2),
    an MLP demonstrates its computational approach by transforming a <semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixel image
    into digit classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å½“åº”ç”¨äºMNISTæ‰‹å†™æ•°å­—è¯†åˆ«æŒ‘æˆ˜[2](#fn2)æ—¶ï¼ŒMLPé€šè¿‡å°†ä¸€ä¸ª<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>åƒç´ å›¾åƒè½¬æ¢ä¸ºæ•°å­—åˆ†ç±»ï¼Œå±•ç¤ºäº†å…¶è®¡ç®—æ–¹æ³•ã€‚
- en: Pattern Processing Needs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å¼å¤„ç†éœ€æ±‚
- en: 'Deep learning models frequently encounter problems where any input feature
    may influence any output, absent inherent constraints on these relationships.
    Financial market analysis exemplifies this challenge: any economic indicator may
    affect any market outcome. Similarly, in natural language processing, the meaning
    of a word may depend on any other word in the sentence. These scenarios demand
    an architectural pattern capable of learning arbitrary relationships across all
    input features.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æ¨¡å‹ç»å¸¸é‡åˆ°ä»»ä½•è¾“å…¥ç‰¹å¾éƒ½å¯èƒ½å½±å“ä»»ä½•è¾“å‡ºçš„é—®é¢˜ï¼Œè¿™äº›å…³ç³»æ²¡æœ‰å†…åœ¨çš„çº¦æŸã€‚é‡‘èå¸‚åœºåˆ†ææ˜¯è¿™ä¸€æŒ‘æˆ˜çš„ä¾‹å­ï¼šä»»ä½•ç»æµæŒ‡æ ‡éƒ½å¯èƒ½å½±å“ä»»ä½•å¸‚åœºç»“æœã€‚åŒæ ·ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œä¸€ä¸ªè¯çš„æ„ä¹‰å¯èƒ½å–å†³äºå¥å­ä¸­çš„ä»»ä½•å…¶ä»–è¯ã€‚è¿™äº›åœºæ™¯éœ€è¦ä¸€ç§èƒ½å¤Ÿå­¦ä¹ æ‰€æœ‰è¾“å…¥ç‰¹å¾ä¹‹é—´ä»»æ„å…³ç³»çš„å»ºç­‘æ¨¡å¼ã€‚
- en: Dense pattern processing addresses these challenges through several key capabilities.
    First, it enables unrestricted feature interactions where each output can depend
    on any combination of inputs. Second, it supports learned feature importance,
    enabling the system to determine which connections matter rather than relying
    on prescribed relationships. Finally, it provides adaptive representation, enabling
    the network to reshape its internal representations based on the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯†é›†æ¨¡å¼å¤„ç†é€šè¿‡å‡ ä¸ªå…³é”®èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒå®ç°äº†æ— é™åˆ¶çš„ç‰¹å¾äº¤äº’ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å‡ºå¯ä»¥ä¾èµ–äºä»»ä½•è¾“å…¥ç»„åˆã€‚å…¶æ¬¡ï¼Œå®ƒæ”¯æŒå­¦ä¹ ç‰¹å¾é‡è¦æ€§ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿç¡®å®šå“ªäº›è¿æ¥æ˜¯é‡è¦çš„ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé¢„å®šçš„å…³ç³»ã€‚æœ€åï¼Œå®ƒæä¾›è‡ªé€‚åº”è¡¨ç¤ºï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿæ ¹æ®æ•°æ®é‡æ–°å¡‘é€ å…¶å†…éƒ¨è¡¨ç¤ºã€‚
- en: 'The MNIST digit recognition task illustrates this uncertainty: while humans
    might focus on specific parts of digits (loops in â€˜6â€™ or crossings in â€˜8â€™), the
    pixel combinations critical for classification remain indeterminate. A â€˜7â€™ written
    with a serif may share pixel patterns with a â€˜2â€™, while variations in handwriting
    mean discriminative features may appear anywhere in the image. This uncertainty
    about feature relationships necessitates a dense processing approach where every
    pixel can potentially influence the classification decision.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MNISTæ•°å­—è¯†åˆ«ä»»åŠ¡è¯´æ˜äº†è¿™ç§ä¸ç¡®å®šæ€§ï¼šè™½ç„¶äººç±»å¯èƒ½ä¸“æ³¨äºæ•°å­—çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆä¾‹å¦‚â€˜6â€™ä¸­çš„ç¯æˆ–â€˜8â€™ä¸­çš„äº¤å‰ï¼‰ï¼Œä½†ç”¨äºåˆ†ç±»çš„åƒç´ ç»„åˆä»ç„¶æ˜¯æœªç¡®å®šçš„ã€‚å¸¦æœ‰è£…é¥°çº¿çš„â€˜7â€™å¯èƒ½ä¸â€˜2â€™å…±äº«åƒç´ æ¨¡å¼ï¼Œè€Œæ‰‹å†™çš„å˜ä½“æ„å‘³ç€åˆ¤åˆ«ç‰¹å¾å¯èƒ½å‡ºç°åœ¨å›¾åƒçš„ä»»ä½•ä½ç½®ã€‚å…³äºç‰¹å¾å…³ç³»çš„è¿™ç§ä¸ç¡®å®šæ€§éœ€è¦å¯†é›†å¤„ç†æ–¹æ³•ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ éƒ½å¯èƒ½å½±å“åˆ†ç±»å†³ç­–ã€‚
- en: This requirement for unrestricted connectivity leads directly to the mathematical
    foundation of MLPs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ— é™åˆ¶è¿æ¥çš„è¦æ±‚ç›´æ¥å¯¼è‡´MLPsçš„æ•°å­¦åŸºç¡€ã€‚
- en: Algorithmic Structure
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•ç»“æ„
- en: 'MLPs enable unrestricted feature interactions through a direct algorithmic
    solution: complete connectivity between all nodes. This connectivity requirement
    manifests through a series of fully-connected layers, where each neuron connects
    to every neuron in adjacent layers, the â€œdenseâ€ connectivity pattern introduced
    in [ChapterÂ 3](ch009.xhtml#sec-dl-primer).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'MLPs enable unrestricted feature interactions through a direct algorithmic
    solution: complete connectivity between all nodes. This connectivity requirement
    manifests through a series of fully-connected layers, where each neuron connects
    to every neuron in adjacent layers, the â€œdenseâ€ connectivity pattern introduced
    in [ç¬¬3ç« ](ch009.xhtml#sec-dl-primer).'
- en: 'This architectural principle translates the dense connectivity pattern into
    matrix multiplication operations[3](#fn3), establishing the mathematical foundation
    that makes MLPs computationally tractable. As illustrated in [FigureÂ 4.1](ch010.xhtml#fig-mlp),
    each layer transforms its input through the fundamental operation introduced in
    [ChapterÂ 3](ch009.xhtml#sec-dl-primer):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å»ºç­‘åŸåˆ™å°†å¯†é›†è¿æ¥æ¨¡å¼è½¬æ¢ä¸ºçŸ©é˜µä¹˜æ³•æ“ä½œ[3](#fn3)ï¼Œå»ºç«‹äº†ä½¿MLPsåœ¨è®¡ç®—ä¸Šå¯å¤„ç†çš„æ•°å­¦åŸºç¡€ã€‚å¦‚å›¾[å›¾4.1](ch010.xhtml#fig-mlp)æ‰€ç¤ºï¼Œæ¯ä¸€å±‚é€šè¿‡åœ¨ç¬¬3ç« [æ·±åº¦å­¦ä¹ åŸºç¡€](ch009.xhtml#sec-dl-primer)ä¸­å¼•å…¥çš„åŸºæœ¬æ“ä½œè½¬æ¢å…¶è¾“å…¥ï¼š
- en: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mo minsize="1.2"
    maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>ğ¡</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>ğ›</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)}
    + \mathbf{b}^{(l)}\big)</annotation></semantics>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mo minsize="1.2"
    maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>ğ¡</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>ğ›</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)}
    + \mathbf{b}^{(l)}\big)</annotation></semantics>
- en: Recall that <semantics><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics>
    represents the layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    output (activation vector), <semantics><msup><mi>ğ¡</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathbf{h}^{(l-1)}</annotation></semantics> represents
    the input from the previous layer, <semantics><msup><mi>ğ–</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation
    encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics> denotes
    the weight matrix for layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>,
    <semantics><msup><mi>ğ›</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
    denotes the bias vector, and <semantics><mrow><mi>f</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>â‹…</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">f(\cdot)</annotation></semantics> denotes the activation
    function (such as ReLU, as detailed in [ChapterÂ 3](ch009.xhtml#sec-dl-primer)).
    This layer-wise transformation, while conceptually simple, creates computational
    patterns whose efficiency depends critically on how we organize these operations
    for different problem structures.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›æƒ³ä¸€ä¸‹ï¼Œ<semantics><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics>
    ä»£è¡¨ç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„è¾“å‡ºï¼ˆæ¿€æ´»å‘é‡ï¼‰ï¼Œ<semantics><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l-1)}</annotation></semantics>
    ä»£è¡¨æ¥è‡ªå‰ä¸€å±‚çš„æ•°æ®è¾“å…¥ï¼Œ<semantics><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>
    è¡¨ç¤ºç¬¬ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    å±‚çš„æƒé‡çŸ©é˜µï¼Œ<semantics><msup><mi>ğ›</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>
    è¡¨ç¤ºåç½®å‘é‡ï¼Œè€Œ <semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>â‹…</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot)</annotation></semantics>
    è¡¨ç¤ºæ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚ ReLUï¼Œè¯¦è§ç¬¬ 3 ç« ï¼‰ã€‚è¿™ç§å±‚çº§çš„è½¬æ¢åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†åˆ›å»ºçš„è®¡ç®—æ¨¡å¼æ•ˆç‡å–å†³äºæˆ‘ä»¬å¦‚ä½•é’ˆå¯¹ä¸åŒçš„é—®é¢˜ç»“æ„ç»„ç»‡è¿™äº›æ“ä½œã€‚
- en: '![](../media/file54.svg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file54.svg)'
- en: 'FigureÂ 4.1: **Layered Transformations**: Multi-Layer Perceptrons (MLPs) implement
    dense connectivity through sequential matrix multiplications and non-linear activations,
    supporting complex feature interactions and hierarchical representations of input
    data. Each layer transforms the input vector from the previous layer, producing
    a new vector that serves as input to the subsequent layer, as defined by the equation
    in the text. Source: ([Reagen et al. 2017](ch058.xhtml#ref-reagen2017deep)).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.1ï¼š**å±‚çŠ¶å˜æ¢**ï¼šå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰é€šè¿‡åºåˆ—çŸ©é˜µä¹˜æ³•å’Œéçº¿æ€§æ¿€æ´»å®ç°å¯†é›†è¿æ¥ï¼Œæ”¯æŒå¤æ‚ç‰¹å¾äº¤äº’å’Œè¾“å…¥æ•°æ®çš„å±‚æ¬¡è¡¨ç¤ºã€‚æ¯ä¸€å±‚å°†å‰ä¸€å±‚è¾“å…¥å‘é‡è½¬æ¢ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å‘é‡ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œå¦‚æ–‡æœ¬ä¸­çš„æ–¹ç¨‹æ‰€ç¤ºã€‚æ¥æºï¼š([Reagenç­‰äºº2017](ch058.xhtml#ref-reagen2017deep))ã€‚
- en: 'The dimensions of these operations reveal the computational scale of dense
    pattern processing:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ“ä½œçš„ç»´åº¦æ­ç¤ºäº†å¯†é›†æ¨¡å¼å¤„ç†çš„è®¡ç®—è§„æ¨¡ï¼š
- en: 'Input vector: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)}
    \in \mathbb{R}^{d_{\text{in}}}</annotation></semantics> (treated as a row vector
    in this formulation) represents all potential input features'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥å‘é‡ï¼š<semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)}
    \in \mathbb{R}^{d_{\text{in}}}</annotation></semantics>ï¼ˆåœ¨æœ¬å…¬å¼ä¸­ä½œä¸ºè¡Œå‘é‡å¤„ç†ï¼‰ä»£è¡¨æ‰€æœ‰æ½œåœ¨è¾“å…¥ç‰¹å¾
- en: 'Weight matrices: <semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times
    d_{\text{out}}}</annotation></semantics> capture all possible input-output relationships'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æƒé‡çŸ©é˜µï¼š<semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub></mrow></msup></mrow><annotation
    encoding="application/x-tex">\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times
    d_{\text{out}}}</annotation></semantics>æ•æ‰æ‰€æœ‰å¯èƒ½çš„è¾“å…¥è¾“å‡ºå…³ç³»
- en: 'Output vector: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>d</mi><mtext
    mathvariant="normal">out</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(l)}
    \in \mathbb{R}^{d_{\text{out}}}</annotation></semantics> produces transformed
    representations'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºå‘é‡ï¼š<semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>âˆˆ</mo><msup><mi>â„</mi><msub><mi>d</mi><mtext
    mathvariant="normal">out</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(l)}
    \in \mathbb{R}^{d_{\text{out}}}</annotation></semantics>äº§ç”Ÿè½¬æ¢åçš„è¡¨ç¤º
- en: 'Consider a simplified 4-pixel image processed by a 3-neuron hidden layer:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€ä¸ªç”±3ä¸ªç¥ç»å…ƒéšè—å±‚å¤„ç†çš„ç®€åŒ–4åƒç´ å›¾åƒï¼š
- en: '**Input**: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mn>0.8</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.9</mn><mo>,</mo><mn>0.1</mn><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)}
    = [0.8, 0.2, 0.9, 0.1]</annotation></semantics> (4 pixel intensities)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥**ï¼š<semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mn>0.8</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.9</mn><mo>,</mo><mn>0.1</mn><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)}
    = [0.8, 0.2, 0.9, 0.1]</annotation></semantics>ï¼ˆ4ä¸ªåƒç´ å¼ºåº¦ï¼‰'
- en: '**Weight matrix**: <semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.2</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.3</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mi>âˆ’</mi><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.7</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.1</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)}
    = \begin{bmatrix} 0.5 & 0.1 & -0.2 \\ -0.3 & 0.8 & 0.4 \\ 0.2 & -0.4 & 0.6 \\
    0.7 & 0.3 & -0.1 \end{bmatrix}</annotation></semantics> (4Ã—3 matrix)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒé‡çŸ©é˜µ**: <semantics><mrow><msup><mi>ğ–</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.1</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.2</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.3</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mn>0.8</mn></mtd><mtd columnalign="center"
    style="text-align: center"><mn>0.4</mn></mtd></mtr><mtr><mtd columnalign="center"
    style="text-align: center"><mn>0.2</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mi>âˆ’</mi><mn>0.4</mn></mtd><mtd columnalign="center" style="text-align:
    center"><mn>0.6</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.7</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0.3</mn></mtd><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.1</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)}
    = \begin{bmatrix} 0.5 & 0.1 & -0.2 \\ -0.3 & 0.8 & 0.4 \\ 0.2 & -0.4 & 0.6 \\
    0.7 & 0.3 & -0.1 \end{bmatrix}</annotation></semantics> (4Ã—3 çŸ©é˜µ)'
- en: '**Computation**: <semantics><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğ¡</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>âˆ’</mi><mn>0.3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mn>0.2</mn><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mn>0.7</mn><mo>Ã—</mo><mn>0.1</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.1</mn><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mn>0.8</mn><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.4</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mn>0.3</mn><mo>Ã—</mo><mn>0.1</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mn>0.4</mn><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mn>0.6</mn><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.1</mn></mtd></mtr></mtable><mo stretchy="true"
    form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.65</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.17</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.47</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    \mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5Ã—0.8
    + (-0.3)Ã—0.2 + 0.2Ã—0.9 + 0.7Ã—0.1 \\ 0.1Ã—0.8 + 0.8Ã—0.2 + (-0.4)Ã—0.9 + 0.3Ã—0.1 \\
    (-0.2)Ã—0.8 + 0.4Ã—0.2 + 0.6Ã—0.9 + (-0.1)Ã—0.1 \end{bmatrix} \\ = \begin{bmatrix}
    0.65 \\ -0.17 \\ 0.47 \end{bmatrix} \end{gather*}</annotation></semantics> **After
    ReLU**: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mn>0.65</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0.47</mn><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(1)}
    = [0.65, 0, 0.47]</annotation></semantics> (negative values zeroed)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¡ç®—**: <semantics><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><msup><mi>ğ³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>ğ¡</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>ğ–</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align:
    center"><mn>0.5</mn><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>âˆ’</mi><mn>0.3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mn>0.2</mn><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mn>0.7</mn><mo>Ã—</mo><mn>0.1</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.1</mn><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mn>0.8</mn><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.4</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mn>0.3</mn><mo>Ã—</mo><mn>0.1</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.2</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.8</mn><mo>+</mo><mn>0.4</mn><mo>Ã—</mo><mn>0.2</mn><mo>+</mo><mn>0.6</mn><mo>Ã—</mo><mn>0.9</mn><mo>+</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>0.1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>Ã—</mo><mn>0.1</mn></mtd></mtr></mtable><mo stretchy="true"
    form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align:
    center"><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.65</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mi>âˆ’</mi><mn>0.17</mn></mtd></mtr><mtr><mtd
    columnalign="center" style="text-align: center"><mn>0.47</mn></mtd></mtr></mtable><mo
    stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{gather*}
    \mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5Ã—0.8
    + (-0.3)Ã—0.2 + 0.2Ã—0.9 + 0.7Ã—0.1 \\ 0.1Ã—0.8 + 0.8Ã—0.2 + (-0.4)Ã—0.9 + 0.3Ã—0.1 \\
    (-0.2)Ã—0.8 + 0.4Ã—0.2 + 0.6Ã—0.9 + (-0.1)Ã—0.1 \end{bmatrix} \\ = \begin{bmatrix}
    0.65 \\ -0.17 \\ 0.47 \end{bmatrix} \end{gather*}</annotation></semantics> **ReLU
    å**: <semantics><mrow><msup><mi>ğ¡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo
    stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true"
    form="prefix">[</mo><mn>0.65</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0.47</mn><mo
    stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(1)}
    = [0.65, 0, 0.47]</annotation></semantics> (negative values zeroed)'
- en: Each hidden neuron combines ALL input pixels with different weights, demonstrating
    unrestricted feature interaction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªéšè—ç¥ç»å…ƒå°†æ‰€æœ‰è¾“å…¥åƒç´ ä»¥ä¸åŒçš„æƒé‡ç»„åˆèµ·æ¥ï¼Œå±•ç¤ºäº†æ— é™åˆ¶çš„ç‰¹å¾äº¤äº’ã€‚
- en: 'The MNIST example demonstrates the practical scale of these operations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MNISTç¤ºä¾‹å±•ç¤ºäº†è¿™äº›æ“ä½œçš„å®ç”¨è§„æ¨¡ï¼š
- en: Each 784-dimensional input (<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> pixels) connects
    to every neuron in the first hidden layer
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç”±784ç»´è¾“å…¥ï¼ˆ<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>åƒç´ ï¼‰è¿æ¥åˆ°ç¬¬ä¸€éšè—å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒ
- en: A hidden layer with 100 neurons requires a <semantics><mrow><mn>784</mn><mo>Ã—</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">784\times 100</annotation></semantics> weight matrix
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«100ä¸ªç¥ç»å…ƒçš„éšè—å±‚éœ€è¦ä¸€ä¸ª<semantics><mrow><mn>784</mn><mo>Ã—</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">784\times 100</annotation></semantics>æƒé‡çŸ©é˜µ
- en: Each weight in this matrix represents a learnable relationship between an input
    pixel and a hidden feature
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçŸ©é˜µä¸­çš„æ¯ä¸ªæƒé‡ä»£è¡¨ä¸€ä¸ªå¯å­¦ä¹ çš„è¾“å…¥åƒç´ å’Œéšè—ç‰¹å¾ä¹‹é—´çš„å…³ç³»
- en: This algorithmic structure addresses the need for arbitrary feature relationships
    while creating specific computational patterns that computer systems must accommodate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç®—æ³•ç»“æ„è§£å†³äº†ä»»æ„ç‰¹å¾å…³ç³»çš„éœ€æ±‚ï¼ŒåŒæ—¶åˆ›å»ºäº†è®¡ç®—æœºç³»ç»Ÿå¿…é¡»é€‚åº”çš„ç‰¹å®šè®¡ç®—æ¨¡å¼ã€‚
- en: Architectural Characteristics
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¶æ„ç‰¹æ€§
- en: This dense connectivity approach creates both advantages and trade-offs. Dense
    connectivity provides the universal approximation capability established earlier
    but introduces computational redundancy. While this theoretical power enables
    MLPs to model any continuous function given sufficient width, this flexibility
    necessitates numerous parameters to learn relatively simple patterns. The dense
    connections ensure that every input feature influences every output, yielding
    maximum expressiveness at the cost of maximum computational expense.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¯†é›†è¿æ¥æ–¹æ³•æ—¢å¸¦æ¥äº†ä¼˜åŠ¿ï¼Œä¹Ÿå¸¦æ¥äº†æƒè¡¡ã€‚å¯†é›†è¿æ¥æä¾›äº†ä¹‹å‰å»ºç«‹çš„é€šç”¨é€¼è¿‘èƒ½åŠ›ï¼Œä½†å¼•å…¥äº†è®¡ç®—å†—ä½™ã€‚è™½ç„¶è¿™ç§ç†è®ºä¸Šçš„èƒ½åŠ›ä½¿å¾—MLPåœ¨è¶³å¤Ÿå®½çš„æƒ…å†µä¸‹èƒ½å¤Ÿæ¨¡æ‹Ÿä»»ä½•è¿ç»­å‡½æ•°ï¼Œä½†è¿™ç§çµæ´»æ€§éœ€è¦å¤§é‡å‚æ•°æ¥å­¦ä¹ ç›¸å¯¹ç®€å•çš„æ¨¡å¼ã€‚å¯†é›†è¿æ¥ç¡®ä¿æ¯ä¸ªè¾“å…¥ç‰¹å¾å½±å“æ¯ä¸ªè¾“å‡ºï¼Œä»¥æœ€å¤§è®¡ç®—æˆæœ¬æ¢å–æœ€å¤§è¡¨è¾¾èƒ½åŠ›ã€‚
- en: These trade-offs motivate sophisticated optimization techniques that reduce
    computational demands while preserving model capability. Structured pruning can
    eliminate 80-90% of connections with minimal accuracy loss, while quantization
    reduces precision requirements from 32-bit to 8-bit or lower. While [ChapterÂ 10](ch016.xhtml#sec-model-optimizations)
    details these compression strategies, the architectural foundations established
    here determine which optimization approaches prove most effective for dense connectivity
    patterns, with [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration) exploring hardware-specific
    implementations that exploit regular matrix operation structure.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æƒè¡¡ä¿ƒä½¿äº†å¤æ‚çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›çš„åŒæ—¶å‡å°‘è®¡ç®—éœ€æ±‚ã€‚ç»“æ„åŒ–å‰ªæå¯ä»¥æ¶ˆé™¤80-90%çš„è¿æ¥ï¼ŒåŒæ—¶æœ€å°åŒ–ç²¾åº¦æŸå¤±ï¼Œè€Œé‡åŒ–å°†ç²¾åº¦è¦æ±‚ä»32ä½é™ä½åˆ°8ä½æˆ–æ›´ä½ã€‚è™½ç„¶[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)è¯¦ç»†ä»‹ç»äº†è¿™äº›å‹ç¼©ç­–ç•¥ï¼Œä½†è¿™é‡Œå»ºç«‹çš„æ¶æ„åŸºç¡€å†³å®šäº†å“ªç§ä¼˜åŒ–æ–¹æ³•å¯¹äºå¯†é›†è¿æ¥æ¨¡å¼æœ€ä¸ºæœ‰æ•ˆï¼Œè€Œ[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)æ¢è®¨äº†åˆ©ç”¨è§„åˆ™çŸ©é˜µæ“ä½œç»“æ„çš„ç‰¹å®šç¡¬ä»¶å®ç°ã€‚
- en: Computational Mapping
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æ˜ å°„
- en: The mathematical representation of dense matrix multiplication maps to specific
    computational patterns that systems must handle. This mapping progresses from
    mathematical abstraction to computational reality, as demonstrated in the first
    implementation shown in [ListingÂ 4.1](ch010.xhtml#lst-mlp_layer_matrix).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¯†é›†çŸ©é˜µä¹˜æ³•çš„æ•°å­¦è¡¨ç¤ºæ˜ å°„åˆ°ç³»ç»Ÿå¿…é¡»å¤„ç†çš„ç‰¹å®šè®¡ç®—æ¨¡å¼ã€‚è¿™ç§æ˜ å°„ä»æ•°å­¦æŠ½è±¡åˆ°è®¡ç®—ç°å®ï¼Œæ­£å¦‚åœ¨ç¬¬ä¸€ä¸ªå®ç°[åˆ—è¡¨4.1](ch010.xhtml#lst-mlp_layer_matrix)ä¸­æ‰€ç¤ºã€‚
- en: The function mlp_layer_matrix directly mirrors the mathematical equation, employing
    high-level matrix operations (`matmul`) to express the computation in a single
    line while abstracting the underlying complexity. This implementation style characterizes
    deep learning frameworks, where optimized libraries manage the actual computation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°mlp_layer_matrixç›´æ¥åæ˜ äº†æ•°å­¦æ–¹ç¨‹ï¼Œä½¿ç”¨é«˜çº§çŸ©é˜µæ“ä½œï¼ˆ`matmul`ï¼‰åœ¨å•è¡Œä¸­è¡¨è¾¾è®¡ç®—ï¼ŒåŒæ—¶æŠ½è±¡äº†åº•å±‚å¤æ‚æ€§ã€‚è¿™ç§å®ç°é£æ ¼æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ç‰¹å¾ï¼Œå…¶ä¸­ä¼˜åŒ–çš„åº“ç®¡ç†å®é™…çš„è®¡ç®—ã€‚
- en: 'ListingÂ 4.1: This implementation shows neural networks performing weighted
    sum and activation functions across layers using matrix operations. The code emphasizes
    the core computational pattern in multi-layer perceptrons.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.1ï¼šè¿™ä¸ªå®ç°å±•ç¤ºäº†ç¥ç»ç½‘ç»œé€šè¿‡çŸ©é˜µè¿ç®—åœ¨å±‚ä¹‹é—´æ‰§è¡ŒåŠ æƒæ±‚å’Œå’Œæ¿€æ´»å‡½æ•°ã€‚ä»£ç å¼ºè°ƒäº†å¤šå±‚æ„ŸçŸ¥å™¨ä¸­çš„æ ¸å¿ƒè®¡ç®—æ¨¡å¼ã€‚
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To understand the system implications of this architecture, we must look â€œunder
    the hoodâ€ of the high-level framework call. The elegant one-line matrix multiplication
    `output = matmul(X, W)` is, from the hardwareâ€™s perspective, a series of nested
    loops that expose the true computational demands on the system. This translation
    from logical model to physical execution reveals critical patterns that determine
    memory access, parallelization strategies, and hardware utilization.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç†è§£è¿™ç§æ¶æ„çš„ç³»ç»Ÿå½±å“ï¼Œæˆ‘ä»¬å¿…é¡»æŸ¥çœ‹é«˜çº§æ¡†æ¶è°ƒç”¨çš„â€œå†…éƒ¨ç»“æ„â€ã€‚ä»ç¡¬ä»¶çš„è§’åº¦æ¥çœ‹ï¼Œä¼˜é›…çš„å•è¡ŒçŸ©é˜µä¹˜æ³•`output = matmul(X, W)`å®é™…ä¸Šæ˜¯ä¸€ç³»åˆ—åµŒå¥—å¾ªç¯ï¼Œè¿™äº›å¾ªç¯æ­ç¤ºäº†ç³»ç»ŸçœŸæ­£çš„è®¡ç®—éœ€æ±‚ã€‚è¿™ç§ä»é€»è¾‘æ¨¡å‹åˆ°ç‰©ç†æ‰§è¡Œçš„è½¬æ¢æ­ç¤ºäº†å†³å®šå†…å­˜è®¿é—®ã€å¹¶è¡ŒåŒ–ç­–ç•¥å’Œç¡¬ä»¶åˆ©ç”¨çš„å…³é”®æ¨¡å¼ã€‚
- en: 'The second implementation, `mlp_layer_compute` (shown in [ListingÂ 4.2](ch010.xhtml#lst-mlp_layer_compute)),
    exposes the actual computational pattern through nested loops. This version reveals
    what really happens when we compute a layerâ€™s output: we process each sample in
    the batch, computing each output neuron by accumulating weighted contributions
    from all inputs.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§å®ç°æ–¹å¼ï¼Œ`mlp_layer_compute`ï¼ˆå¦‚[åˆ—è¡¨4.2](ch010.xhtml#lst-mlp_layer_compute)æ‰€ç¤ºï¼‰ï¼Œé€šè¿‡åµŒå¥—å¾ªç¯æ­ç¤ºäº†å®é™…çš„è®¡ç®—æ¨¡å¼ã€‚è¿™ä¸ªç‰ˆæœ¬æ­ç¤ºäº†å½“æˆ‘ä»¬è®¡ç®—å±‚çš„è¾“å‡ºæ—¶çœŸæ­£å‘ç”Ÿçš„äº‹æƒ…ï¼šæˆ‘ä»¬å¤„ç†æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œé€šè¿‡ç´¯ç§¯æ‰€æœ‰è¾“å…¥çš„åŠ æƒè´¡çŒ®æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚
- en: 'ListingÂ 4.2: This implementation computes each output neuron by accumulating
    weighted contributions from all inputs across the batch. The detailed step-by-step
    process exposes how a single layer in a neural network processes data, emphasizing
    the role of biases and weighted sums in producing outputs.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.2ï¼šè¿™ä¸ªå®ç°é€šè¿‡ç´¯ç§¯æ‰¹å¤„ç†ä¸­æ‰€æœ‰è¾“å…¥çš„åŠ æƒè´¡çŒ®æ¥è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒã€‚è¯¦ç»†çš„é€æ­¥è¿‡ç¨‹æ­ç¤ºäº†ç¥ç»ç½‘ç»œä¸­å•ä¸ªå±‚å¦‚ä½•å¤„ç†æ•°æ®ï¼Œå¼ºè°ƒäº†åå·®å’ŒåŠ æƒæ€»å’Œåœ¨äº§ç”Ÿè¾“å‡ºä¸­çš„ä½œç”¨ã€‚
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This translation from mathematical abstraction to concrete computation exposes
    how dense matrix multiplication decomposes into nested loops of simpler operations.
    The outer loop processes each sample in the batch, while the middle loop computes
    values for each output neuron. Within the innermost loop, the system performs
    repeated multiply-accumulate operations[4](#fn4), combining each input with its
    corresponding weight.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä»æ•°å­¦æŠ½è±¡åˆ°å…·ä½“è®¡ç®—çš„è½¬æ¢æ­ç¤ºäº†å¯†é›†çŸ©é˜µä¹˜æ³•å¦‚ä½•åˆ†è§£ä¸ºæ›´ç®€å•æ“ä½œçš„åµŒå¥—å¾ªç¯ã€‚å¤–å±‚å¾ªç¯å¤„ç†æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œä¸­é—´å¾ªç¯è®¡ç®—æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒçš„å€¼ã€‚åœ¨å†…å±‚å¾ªç¯ä¸­ï¼Œç³»ç»Ÿæ‰§è¡Œé‡å¤çš„ä¹˜åŠ æ“ä½œ[4](#fn4)ï¼Œå°†æ¯ä¸ªè¾“å…¥ä¸å…¶å¯¹åº”çš„æƒé‡ç»„åˆã€‚
- en: In the MNIST example, each output neuron requires 784 multiply-accumulate operations
    and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual
    implementations use optimizations through libraries like BLAS[5](#fn5) or cuBLAS,
    these patterns drive key system design decisions. The hardware architectures that
    accelerate these matrix operations, including GPU tensor cores[6](#fn6) and specialized
    AI accelerators, are covered in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨MNISTç¤ºä¾‹ä¸­ï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéœ€è¦784æ¬¡ä¹˜åŠ æ“ä½œå’Œè‡³å°‘1,568æ¬¡å†…å­˜è®¿é—®ï¼ˆ784æ¬¡ç”¨äºè¾“å…¥ï¼Œ784æ¬¡ç”¨äºæƒé‡ï¼‰ã€‚è™½ç„¶å®é™…å®ç°ä½¿ç”¨BLAS[5](#fn5)æˆ–cuBLASç­‰åº“è¿›è¡Œä¼˜åŒ–ï¼Œä½†è¿™äº›æ¨¡å¼é©±åŠ¨ç€å…³é”®çš„ç³»ç»Ÿè®¾è®¡å†³ç­–ã€‚åŠ é€Ÿè¿™äº›çŸ©é˜µæ“ä½œçš„ç¡¬ä»¶æ¶æ„ï¼ŒåŒ…æ‹¬GPUå¼ é‡æ ¸å¿ƒ[6](#fn6)å’Œä¸“é—¨çš„AIåŠ é€Ÿå™¨ï¼Œåœ¨[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­æœ‰æ‰€ä»‹ç»ã€‚
- en: System Implications
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: 'Neural network architectures exhibit distinct system-level characteristics
    that exhibit three core dimensions for systematic analysis: memory requirements,
    computation needs, and data movement. This framework enables consistent analysis
    of how algorithmic patterns influence system design decisions, revealing both
    commonalities and architecture-specific optimizations. We apply this framework
    throughout our analysis of each architecture family. These system-level considerations
    build directly on the foundational concepts of neural network computation patterns,
    memory systems, and system scaling discussed in [ChapterÂ 3](ch009.xhtml#sec-dl-primer).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„è¡¨ç°å‡ºç‹¬ç‰¹çš„ç³»ç»Ÿçº§ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åœ¨ç³»ç»Ÿåˆ†æä¸­å…·æœ‰ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼šå†…å­˜éœ€æ±‚ã€è®¡ç®—éœ€æ±‚å’Œæ•°æ®ç§»åŠ¨ã€‚è¿™ä¸ªæ¡†æ¶ä½¿å¾—å¯¹ç®—æ³•æ¨¡å¼å¦‚ä½•å½±å“ç³»ç»Ÿè®¾è®¡å†³ç­–è¿›è¡Œä¸€è‡´åˆ†ææˆä¸ºå¯èƒ½ï¼Œæ­ç¤ºäº†å…±æ€§å’Œæ¶æ„ç‰¹å®šçš„ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨åˆ†ææ¯ä¸ªæ¶æ„å®¶æ—æ—¶éƒ½åº”ç”¨äº†è¿™ä¸ªæ¡†æ¶ã€‚è¿™äº›ç³»ç»Ÿçº§è€ƒè™‘å› ç´ ç›´æ¥å»ºç«‹åœ¨ç¬¬ä¸‰ç« ä¸­è®¨è®ºçš„ç¥ç»ç½‘ç»œè®¡ç®—æ¨¡å¼ã€å†…å­˜ç³»ç»Ÿå’Œç³»ç»Ÿæ‰©å±•çš„åŸºç¡€æ¦‚å¿µä¹‹ä¸Šã€‚
- en: Memory Requirements
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜éœ€æ±‚
- en: For dense pattern processing, the memory requirements stem from storing and
    accessing weights, inputs, and intermediate results. In our MNIST example, connecting
    our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400
    weight parameters. Each forward pass must access all these weights, along with
    input data and intermediate results. The all-to-all connectivity pattern means
    thereâ€™s no inherent locality in these accesses; every output needs every input
    and its corresponding weights.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¯†é›†æ¨¡å¼å¤„ç†ï¼Œå†…å­˜éœ€æ±‚æºäºå­˜å‚¨å’Œè®¿é—®æƒé‡ã€è¾“å…¥å’Œä¸­é—´ç»“æœã€‚åœ¨æˆ‘ä»¬çš„MNISTç¤ºä¾‹ä¸­ï¼Œå°†784ç»´è¾“å…¥å±‚è¿æ¥åˆ°100ä¸ªç¥ç»å…ƒçš„éšè—å±‚éœ€è¦78,400ä¸ªæƒé‡å‚æ•°ã€‚æ¯æ¬¡æ­£å‘ä¼ é€’éƒ½å¿…é¡»è®¿é—®æ‰€æœ‰è¿™äº›æƒé‡ï¼Œä»¥åŠè¾“å…¥æ•°æ®å’Œä¸­é—´ç»“æœã€‚å…¨å…¨è¿æ¥æ¨¡å¼æ„å‘³ç€è¿™äº›è®¿é—®æ²¡æœ‰å›ºæœ‰çš„å±€éƒ¨æ€§ï¼›æ¯ä¸ªè¾“å‡ºéƒ½éœ€è¦æ¯ä¸ªè¾“å…¥åŠå…¶å¯¹åº”çš„æƒé‡ã€‚
- en: 'These memory access patterns enable optimization through careful data organization
    and reuse. Modern processors handle these dense access patterns through specialized
    approaches: CPUs leverage their cache hierarchy for data reuse, while GPUs employ
    memory architectures designed for high-bandwidth access to large parameter matrices.
    Frameworks abstract these optimizations through high-performance matrix operations
    (as detailed in our earlier analysis).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å­˜è®¿é—®æ¨¡å¼é€šè¿‡ç²¾å¿ƒç»„ç»‡æ•°æ®å’Œä½¿ç”¨ä¼˜åŒ–ã€‚ç°ä»£å¤„ç†å™¨é€šè¿‡ä¸“é—¨çš„æ–¹æ³•å¤„ç†è¿™äº›å¯†é›†è®¿é—®æ¨¡å¼ï¼šCPUåˆ©ç”¨å…¶ç¼“å­˜å±‚æ¬¡ç»“æ„è¿›è¡Œæ•°æ®é‡ç”¨ï¼Œè€ŒGPUé‡‡ç”¨ä¸“ä¸ºé«˜å¸¦å®½è®¿é—®å¤§å‚æ•°çŸ©é˜µè®¾è®¡çš„å†…å­˜æ¶æ„ã€‚æ¡†æ¶é€šè¿‡é«˜æ€§èƒ½çŸ©é˜µæ“ä½œï¼ˆå¦‚æˆ‘ä»¬æ—©æœŸåˆ†æä¸­æ‰€è¿°ï¼‰æŠ½è±¡è¿™äº›ä¼˜åŒ–ã€‚
- en: Computation Needs
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—éœ€æ±‚
- en: The core computation revolves around multiply-accumulate operations arranged
    in nested loops. Each output value requires as many multiply-accumulates as there
    are inputs. For MNIST, this requires 784 multiply-accumulates per output neuron.
    With 100 neurons in the hidden layer, 78,400 multiply-accumulates are performed
    for a single input image. While these operations are simple, their volume and
    arrangement create specific demands on processing resources.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒè®¡ç®—å›´ç»•åµŒå¥—å¾ªç¯ä¸­çš„ä¹˜åŠ æ“ä½œè¿›è¡Œã€‚æ¯ä¸ªè¾“å‡ºå€¼éœ€è¦çš„ä¹˜åŠ æ¬¡æ•°ä¸è¾“å…¥æ•°é‡ç›¸åŒã€‚å¯¹äºMNISTï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéœ€è¦784æ¬¡ä¹˜åŠ ã€‚åœ¨éšè—å±‚æœ‰100ä¸ªç¥ç»å…ƒçš„æƒ…å†µä¸‹ï¼Œå•ä¸ªè¾“å…¥å›¾åƒéœ€è¦è¿›è¡Œ78,400æ¬¡ä¹˜åŠ ã€‚è™½ç„¶è¿™äº›æ“ä½œå¾ˆç®€å•ï¼Œä½†å®ƒä»¬çš„æ•°é‡å’Œæ’åˆ—å¯¹å¤„ç†èµ„æºæå‡ºäº†ç‰¹å®šçš„è¦æ±‚ã€‚
- en: This computational structure enables specific optimization strategies in modern
    hardware. The dense matrix multiplication pattern parallelizes across multiple
    processing units, with each handling different subsets of neurons. Modern hardware
    accelerators take advantage of this through specialized matrix multiplication
    units, while software frameworks automatically convert these operations into optimized
    BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit
    cache locality by carefully tiling the computation to maximize data reuse, though
    their specific approaches differ based on their architectural strengths.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¡ç®—ç»“æ„ä½¿å¾—ç°ä»£ç¡¬ä»¶èƒ½å¤Ÿå®ç°ç‰¹å®šçš„ä¼˜åŒ–ç­–ç•¥ã€‚å¯†é›†çŸ©é˜µä¹˜æ³•æ¨¡å¼å¯ä»¥åœ¨å¤šä¸ªå¤„ç†å•å…ƒä¹‹é—´å¹¶è¡ŒåŒ–ï¼Œæ¯ä¸ªå•å…ƒå¤„ç†ä¸åŒçš„ç¥ç»å…ƒå­é›†ã€‚ç°ä»£ç¡¬ä»¶åŠ é€Ÿå™¨é€šè¿‡ä¸“é—¨çš„çŸ©é˜µä¹˜æ³•å•å…ƒåˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œè€Œè½¯ä»¶æ¡†æ¶è‡ªåŠ¨å°†è¿™äº›æ“ä½œè½¬æ¢ä¸ºä¼˜åŒ–çš„BLASï¼ˆåŸºæœ¬çº¿æ€§ä»£æ•°å­ç¨‹åºï¼‰è°ƒç”¨ã€‚CPUå’ŒGPUéƒ½å¯ä»¥é€šè¿‡ä»”ç»†åœ°åˆ†å—è®¡ç®—æ¥åˆ©ç”¨ç¼“å­˜å±€éƒ¨æ€§ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®é‡ç”¨ï¼Œå°½ç®¡å®ƒä»¬çš„å…·ä½“æ–¹æ³•åŸºäºå…¶æ¶æ„ä¼˜åŠ¿è€Œæœ‰æ‰€ä¸åŒã€‚
- en: Data Movement
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨
- en: 'The all-to-all connectivity pattern in MLPs creates significant data movement
    requirements. Each multiply-accumulate operation needs three pieces of data: an
    input value, a weight value, and the running sum. For our MNIST example layer,
    computing a single output value requires moving 784 inputs and 784 weights to
    wherever the computation occurs. This movement pattern repeats for each of the
    100 output neurons, creating large data transfer demands between memory and compute
    units.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: MLPsä¸­çš„å…¨å…¨è¿æ¥æ¨¡å¼äº§ç”Ÿäº†æ˜¾è‘—çš„æ•°æ®ç§»åŠ¨éœ€æ±‚ã€‚æ¯ä¸ªä¹˜åŠ æ“ä½œéœ€è¦ä¸‰ä»½æ•°æ®ï¼šä¸€ä¸ªè¾“å…¥å€¼ã€ä¸€ä¸ªæƒé‡å€¼å’Œè¿è¡Œæ€»å’Œã€‚å¯¹äºæˆ‘ä»¬çš„MNISTç¤ºä¾‹å±‚ï¼Œè®¡ç®—å•ä¸ªè¾“å‡ºå€¼éœ€è¦å°†784ä¸ªè¾“å…¥å’Œ784ä¸ªæƒé‡ç§»åŠ¨åˆ°è®¡ç®—å‘ç”Ÿçš„ä»»ä½•åœ°æ–¹ã€‚è¿™ç§ç§»åŠ¨æ¨¡å¼å¯¹æ¯ä¸ª100ä¸ªè¾“å‡ºç¥ç»å…ƒéƒ½é‡å¤ä¸€æ¬¡ï¼Œåœ¨å†…å­˜å’Œè®¡ç®—å•å…ƒä¹‹é—´äº§ç”Ÿäº†å¤§é‡çš„æ•°æ®ä¼ è¾“éœ€æ±‚ã€‚
- en: The predictable data movement patterns enable strategic data staging and transfer
    optimizations. Different architectures address this challenge through various
    mechanisms; CPUs use prefetching and multi-level caches, while GPUs employ high-bandwidth
    memory systems and latency hiding through massive threading. Software frameworks
    orchestrate these data movements through memory management systems that reduce
    redundant transfers and increase data reuse.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¯é¢„æµ‹çš„æ•°æ®ç§»åŠ¨æ¨¡å¼ä½¿å¾—æˆ˜ç•¥æ•°æ®è°ƒåº¦å’Œä¼ è¾“ä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚ä¸åŒçš„æ¶æ„é€šè¿‡å„ç§æœºåˆ¶æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼›CPUä½¿ç”¨é¢„å–å’Œå¤šçº§ç¼“å­˜ï¼Œè€ŒGPUåˆ™é‡‡ç”¨é«˜å¸¦å®½å†…å­˜ç³»ç»Ÿå’Œé€šè¿‡å¤§é‡çº¿ç¨‹éšè—å»¶è¿Ÿã€‚è½¯ä»¶æ¡†æ¶é€šè¿‡å†…å­˜ç®¡ç†ç³»ç»Ÿæ¥ç¼–æ’è¿™äº›æ•°æ®ç§»åŠ¨ï¼Œå‡å°‘å†—ä½™ä¼ è¾“å¹¶å¢åŠ æ•°æ®é‡ç”¨ã€‚
- en: 'This analysis of MLP computational demands reveals a crucial insight: while
    dense connectivity provides universal approximation capabilities, it creates significant
    inefficiencies when data exhibits inherent structure. This mismatch between architectural
    assumptions and data characteristics motivated the development of specialized
    approaches that could exploit structural patterns for computational gain.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹MLPè®¡ç®—éœ€æ±‚çš„è¿™ç§åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®è§è§£ï¼šè™½ç„¶å¯†é›†è¿æ¥æä¾›äº†é€šç”¨é€¼è¿‘èƒ½åŠ›ï¼Œä½†å½“æ•°æ®è¡¨ç°å‡ºå›ºæœ‰ç»“æ„æ—¶ï¼Œå®ƒä¼šäº§ç”Ÿæ˜¾è‘—çš„ä¸æ•ˆç‡ã€‚è¿™ç§æ¶æ„å‡è®¾ä¸æ•°æ®ç‰¹å¾ä¹‹é—´çš„ä¸åŒ¹é…ä¿ƒä½¿å¼€å‘å‡ºä¸“é—¨çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆ©ç”¨ç»“æ„æ¨¡å¼æ¥å®ç°è®¡ç®—ä¸Šçš„æ”¶ç›Šã€‚
- en: 'CNNs: Spatial Pattern Processing'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNsï¼šç©ºé—´æ¨¡å¼å¤„ç†
- en: The computational intensity and parameter requirements of MLPs reveal a mismatch
    when applied to structured data. Building on the computational complexity considerations
    outlined in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de),
    this inefficiency motivated the development of architectural patterns that exploit
    inherent data structure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: MLPsçš„è®¡ç®—å¼ºåº¦å’Œå‚æ•°éœ€æ±‚åœ¨åº”ç”¨äºç»“æ„åŒ–æ•°æ®æ—¶æ­ç¤ºäº†ä¸åŒ¹é…ã€‚åœ¨[ç¬¬4.1èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)ä¸­æ¦‚è¿°çš„è®¡ç®—å¤æ‚æ€§è€ƒè™‘çš„åŸºç¡€ä¸Šï¼Œè¿™ç§ä½æ•ˆä¿ƒä½¿å¼€å‘å‡ºåˆ©ç”¨å›ºæœ‰æ•°æ®ç»“æ„çš„æ¶æ„æ¨¡å¼ã€‚
- en: 'Convolutional Neural Networks emerged as the solution to this challenge ([Lecun
    et al. 1998](ch058.xhtml#ref-lecun1998gradient); [Krizhevsky, Sutskever, and Hinton
    2017a](ch058.xhtml#ref-krizhevsky2012imagenet)), embodying a specific inductive
    bias: they assume spatial locality and translation invariance, where nearby pixels
    are related and patterns can appear anywhere. This architectural assumption enables
    two key innovations that enhance efficiency for spatially structured data. Parameter
    sharing allows the same feature detector to be applied across different spatial
    positions, reducing parameters from millions to thousands while improving generalization.
    Local connectivity restricts connections to spatially adjacent regions, reflecting
    the insight that spatial proximity correlates with feature relevance.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ä½œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ–¹æ¡ˆå‡ºç°ï¼ˆ[Lecunç­‰äºº1998](ch058.xhtml#ref-lecun1998gradient)ï¼›[Krizhevsky,
    Sutskever, å’Œ Hinton 2017a](ch058.xhtml#ref-krizhevsky2012imagenet)ï¼‰ï¼Œä½“ç°äº†ä¸€ç§ç‰¹å®šçš„å½’çº³åå·®ï¼šå®ƒä»¬å‡è®¾ç©ºé—´å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§ï¼Œå³é™„è¿‘çš„åƒç´ ç›¸å…³ï¼Œæ¨¡å¼å¯ä»¥å‡ºç°åœ¨ä»»ä½•åœ°æ–¹ã€‚è¿™ç§æ¶æ„å‡è®¾ä½¿å¾—ä¸¤ä¸ªå…³é”®åˆ›æ–°å¾—ä»¥å®ç°ï¼Œä»è€Œæé«˜äº†ç©ºé—´ç»“æ„æ•°æ®çš„æ•ˆç‡ã€‚å‚æ•°å…±äº«å…è®¸ç›¸åŒçš„ç‰¹å¾æ£€æµ‹å™¨åº”ç”¨äºä¸åŒçš„ç©ºé—´ä½ç½®ï¼Œå°†å‚æ•°ä»æ•°ç™¾ä¸‡å‡å°‘åˆ°æ•°åƒï¼ŒåŒæ—¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚å±€éƒ¨è¿æ¥æ€§å°†è¿æ¥é™åˆ¶åœ¨ç©ºé—´ç›¸é‚»åŒºåŸŸï¼Œåæ˜ äº†ç©ºé—´é‚»è¿‘ä¸ç‰¹å¾ç›¸å…³æ€§çš„è§è§£ã€‚
- en: '***Convolutional Neural Networks (CNNs)*** are neural architectures that exploit
    *spatial structure* through *local connectivity* and *parameter sharing*, using
    *learnable filters* to build *hierarchical representations* with substantially
    fewer parameters than fully-connected networks.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰*** æ˜¯ä¸€ç§åˆ©ç”¨*å±€éƒ¨è¿æ¥*å’Œ*å‚æ•°å…±äº«*é€šè¿‡*å¯å­¦ä¹ æ»¤æ³¢å™¨*æ„å»ºå…·æœ‰æ¯”å…¨è¿æ¥ç½‘ç»œå°‘å¾—å¤šçš„å‚æ•°çš„*å±‚æ¬¡è¡¨ç¤º*çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚'
- en: 'These architectural innovations represent a trade-off in deep learning design:
    sacrificing the theoretical generality of MLPs for practical efficiency gains
    when data exhibits known structure. While MLPs treat each input element independently,
    CNNs exploit spatial relationships to achieve computational savings and improved
    performance on vision tasks.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å»ºç­‘åˆ›æ–°ä»£è¡¨äº†æ·±åº¦å­¦ä¹ è®¾è®¡ä¸­çš„æƒè¡¡ï¼šå½“æ•°æ®è¡¨ç°å‡ºå·²çŸ¥ç»“æ„æ—¶ï¼Œä¸ºäº†å®ç°å®é™…æ•ˆç‡çš„æå‡è€Œç‰ºç‰²äº†MLPsçš„ç†è®ºé€šç”¨æ€§ã€‚è™½ç„¶MLPsç‹¬ç«‹å¤„ç†æ¯ä¸ªè¾“å…¥å…ƒç´ ï¼Œä½†CNNsé€šè¿‡åˆ©ç”¨ç©ºé—´å…³ç³»æ¥å®ç°è®¡ç®—èŠ‚çœå’Œæå‡åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
- en: Pattern Processing Needs
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å¼å¤„ç†éœ€æ±‚
- en: 'Spatial pattern processing addresses scenarios where the relationship between
    data points depends on their relative positions or proximity. Consider processing
    a natural image: a pixelâ€™s relationship with its neighbors is important for detecting
    edges, textures, and shapes. These local patterns then combine hierarchically
    to form more complex features: edges form shapes, shapes form objects, and objects
    form scenes.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç©ºé—´æ¨¡å¼å¤„ç†è§£å†³æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ä¾èµ–äºå®ƒä»¬çš„ç›¸å¯¹ä½ç½®æˆ–é‚»è¿‘æ€§çš„åœºæ™¯ã€‚è€ƒè™‘å¤„ç†è‡ªç„¶å›¾åƒï¼šåƒç´ ä¸å…¶é‚»å±…çš„å…³ç³»å¯¹äºæ£€æµ‹è¾¹ç¼˜ã€çº¹ç†å’Œå½¢çŠ¶å¾ˆé‡è¦ã€‚è¿™äº›å±€éƒ¨æ¨¡å¼éšåä»¥åˆ†å±‚çš„æ–¹å¼ç»„åˆï¼Œå½¢æˆæ›´å¤æ‚çš„åŠŸèƒ½ï¼šè¾¹ç¼˜å½¢æˆå½¢çŠ¶ï¼Œå½¢çŠ¶å½¢æˆå¯¹è±¡ï¼Œå¯¹è±¡å½¢æˆåœºæ™¯ã€‚
- en: This hierarchical spatial pattern processing appears across many domains. In
    computer vision, local pixel patterns form edges and textures that combine into
    recognizable objects. Speech processing relies on patterns across nearby time
    segments to identify phonemes and words. Sensor networks analyze correlations
    between physically proximate sensors to understand environmental patterns. Medical
    imaging depends on recognizing tissue patterns that indicate biological structures.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åˆ†å±‚ç©ºé—´æ¨¡å¼å¤„ç†åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å‡ºç°ã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå±€éƒ¨åƒç´ æ¨¡å¼å½¢æˆè¾¹ç¼˜å’Œçº¹ç†ï¼Œè¿™äº›çº¹ç†ç»„åˆæˆå¯è¯†åˆ«çš„å¯¹è±¡ã€‚è¯­éŸ³å¤„ç†ä¾èµ–äºé‚»è¿‘æ—¶é—´æ®µçš„æ¨¡å¼æ¥è¯†åˆ«éŸ³ç´ å’Œå•è¯ã€‚ä¼ æ„Ÿå™¨ç½‘ç»œåˆ†æç‰©ç†é‚»è¿‘ä¼ æ„Ÿå™¨ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥ç†è§£ç¯å¢ƒæ¨¡å¼ã€‚åŒ»å­¦æˆåƒä¾èµ–äºè¯†åˆ«ç»„ç»‡æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼æŒ‡ç¤ºç”Ÿç‰©ç»“æ„ã€‚
- en: 'Focusing on image processing to illustrate these principles, if we want to
    detect a cat in an image, certain spatial patterns must be recognized: the triangular
    shape of ears, the round contours of the face, the texture of fur. Importantly,
    these patterns maintain their meaning regardless of where they appear in the image.
    A cat is still a cat whether it appears in the top-left or bottom-right corner.
    This indicates two key requirements for spatial pattern processing: the ability
    to detect local patterns and the ability to recognize these patterns regardless
    of their position[7](#fn7). [FigureÂ 4.2](ch010.xhtml#fig-cnn-spatial-processing)
    shows convolutional neural networks achieving this through hierarchical feature
    extraction, where simple patterns compose into increasingly complex representations
    at successive layers.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“æ³¨äºå›¾åƒå¤„ç†æ¥é˜è¿°è¿™äº›åŸåˆ™ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨å›¾åƒä¸­æ£€æµ‹ä¸€åªçŒ«ï¼Œå¿…é¡»è¯†åˆ«æŸäº›ç©ºé—´æ¨¡å¼ï¼šè€³æœµçš„ä¸‰è§’å½¢å½¢çŠ¶ã€è„¸éƒ¨åœ†æ¶¦çš„è½®å»“ã€æ¯›å‘çš„çº¹ç†ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ¨¡å¼æ— è®ºå‡ºç°åœ¨å›¾åƒçš„å“ªä¸ªä½ç½®éƒ½ä¿æŒå…¶æ„ä¹‰ã€‚çŒ«æ— è®ºå‡ºç°åœ¨å·¦ä¸Šè§’è¿˜æ˜¯å³ä¸‹è§’ï¼Œä»ç„¶æ˜¯çŒ«ã€‚è¿™è¡¨æ˜ç©ºé—´æ¨¡å¼å¤„ç†æœ‰ä¸¤ä¸ªå…³é”®è¦æ±‚ï¼šæ£€æµ‹å±€éƒ¨æ¨¡å¼çš„èƒ½åŠ›ä»¥åŠè¯†åˆ«è¿™äº›æ¨¡å¼è€Œä¸è€ƒè™‘å®ƒä»¬ä½ç½®çš„èƒ½åŠ›[7](#fn7)ã€‚[å›¾4.2](ch010.xhtml#fig-cnn-spatial-processing)å±•ç¤ºäº†å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡åˆ†å±‚ç‰¹å¾æå–å®ç°è¿™ä¸€ç‚¹ï¼Œç®€å•æ¨¡å¼åœ¨è¿ç»­å±‚ä¸­ç»„åˆæˆè¶Šæ¥è¶Šå¤æ‚çš„è¡¨ç¤ºã€‚
- en: '![](../media/file55.svg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file55.svg)'
- en: 'FigureÂ 4.2: **Spatial Feature Extraction**: Convolutional neural networks identify
    patterns independent of their location in an image by applying learnable filters
    across the input, enabling robust object recognition. These filters detect local
    features, and their repeated application across the image creates translation
    invariance, the ability to recognize a pattern regardless of its position.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.2ï¼š**ç©ºé—´ç‰¹å¾æå–**ï¼šå·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡åœ¨æ•´ä¸ªè¾“å…¥ä¸Šåº”ç”¨å¯å­¦ä¹ çš„è¿‡æ»¤å™¨æ¥è¯†åˆ«å›¾åƒä¸­ç‹¬ç«‹äºå…¶ä½ç½®çš„å›¾æ¡ˆï¼Œä»è€Œå®ç°é²æ£’çš„å¯¹è±¡è¯†åˆ«ã€‚è¿™äº›è¿‡æ»¤å™¨æ£€æµ‹å±€éƒ¨ç‰¹å¾ï¼Œå¹¶åœ¨å›¾åƒä¸Šé‡å¤åº”ç”¨è¿™äº›ç‰¹å¾åˆ›å»ºå¹³ç§»ä¸å˜æ€§ï¼Œå³æ— è®ºå›¾æ¡ˆçš„ä½ç½®å¦‚ä½•éƒ½èƒ½è¯†åˆ«å›¾æ¡ˆçš„èƒ½åŠ›ã€‚
- en: 'This leads us to the convolutional neural network architecture (CNN), pioneered
    by Yann LeCun[8](#fn8) and Y. LeCun et al. ([1989](ch058.xhtml#ref-lecun1989backpropagation)).
    CNNs achieve this through several key innovations: parameter sharing[9](#fn9),
    local connectivity, and translation invariance[10](#fn10).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼•å¯¼æˆ‘ä»¬åˆ°å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆCNNï¼‰ï¼Œç”±Yann LeCun[8](#fn8)å’ŒY. LeCunç­‰äºº[1989](ch058.xhtml#ref-lecun1989backpropagation)å¼€åˆ›ã€‚CNNé€šè¿‡å‡ ä¸ªå…³é”®åˆ›æ–°å®ç°è¿™ä¸€ç‚¹ï¼šå‚æ•°å…±äº«[9](#fn9)ã€å±€éƒ¨è¿æ¥å’Œå¹³ç§»ä¸å˜æ€§[10](#fn10)ã€‚
- en: Algorithmic Structure
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•ç»“æ„
- en: 'The core operation in a CNN can be expressed mathematically as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CNNä¸­çš„æ ¸å¿ƒæ“ä½œå¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºä¸ºï¼š
- en: <semantics><mrow><msubsup><mi>ğ‡</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><munder><mo>âˆ‘</mo><mrow><mi>d</mi><mi>i</mi></mrow></munder><munder><mo>âˆ‘</mo><mrow><mi>d</mi><mi>j</mi></mrow></munder><munder><mo>âˆ‘</mo><mi>c</mi></munder><msubsup><mi>ğ–</mi><mrow><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><msubsup><mi>ğ‡</mi><mrow><mi>i</mi><mo>+</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>j</mi><mo>+</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true"
    form="postfix">)</mo></mrow></msubsup><mo>+</mo><msubsup><mi>ğ›</mi><mi>k</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{H}^{(l)}_{i,j,k}
    = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c}
    + \mathbf{b}^{(l)}_k\right)</annotation></semantics>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation describes how CNNs process spatial data. <semantics><msubsup><mi>ğ‡</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation
    encoding="application/x-tex">\mathbf{H}^{(l)}_{i,j,k}</annotation></semantics>
    is the output at spatial position <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics>
    in channel <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    of layer <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>.
    The triple sum iterates over the filter dimensions: <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics>
    scans the spatial filter size, and <semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics>
    covers input channels. <semantics><msubsup><mi>ğ–</mi><mrow><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation
    encoding="application/x-tex">\mathbf{W}^{(l)}_{di,dj,c,k}</annotation></semantics>
    represents the filter weights, capturing local spatial patterns. Unlike MLPs that
    connect all inputs to outputs, CNNs only connect local spatial neighborhoods.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹æè¿°äº†CNNå¦‚ä½•å¤„ç†ç©ºé—´æ•°æ®ã€‚ <semantics><msubsup><mi>ğ‡</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation
    encoding="application/x-tex">\mathbf{H}^{(l)}_{i,j,k}</annotation></semantics>
    æ˜¯åœ¨å±‚ <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>
    çš„é€šé“ <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    ä¸­ç©ºé—´ä½ç½® <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics>
    çš„è¾“å‡ºã€‚ä¸‰é‡æ±‚å’Œéå†æ»¤æ³¢å™¨ç»´åº¦ï¼š<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics>
    æ‰«æç©ºé—´æ»¤æ³¢å™¨å¤§å°ï¼Œè€Œ <semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics>
    è¦†ç›–è¾“å…¥é€šé“ã€‚ <semantics><msubsup><mi>ğ–</mi><mrow><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>k</mi></mrow><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation
    encoding="application/x-tex">\mathbf{W}^{(l)}_{di,dj,c,k}</annotation></semantics>
    è¡¨ç¤ºæ»¤æ³¢å™¨æƒé‡ï¼Œæ•æ‰å±€éƒ¨ç©ºé—´æ¨¡å¼ã€‚ä¸å°†æ‰€æœ‰è¾“å…¥è¿æ¥åˆ°è¾“å‡ºçš„MLPsä¸åŒï¼ŒCNNåªè¿æ¥å±€éƒ¨ç©ºé—´é‚»åŸŸã€‚
- en: 'Breaking down the notation further, <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics>
    corresponds to spatial positions, <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    indexes output channels, <semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics>
    indexes input channels, and <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics>
    spans the local receptive field[11](#fn11). Unlike the dense matrix multiplication
    of MLPs, this operation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥åˆ†è§£ç¬¦å·ï¼Œ<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics>
    å¯¹åº”ç©ºé—´ä½ç½®ï¼Œ<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    æŒ‡çš„æ˜¯è¾“å‡ºé€šé“ï¼Œ<semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics>
    æŒ‡çš„æ˜¯è¾“å…¥é€šé“ï¼Œè€Œ <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>i</mi><mo>,</mo><mi>d</mi><mi>j</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(di,dj)</annotation></semantics>
    è¦†ç›–äº†å±€éƒ¨æ„Ÿå—é‡[11](#fn11)ã€‚ä¸MLPsçš„å¯†é›†çŸ©é˜µä¹˜æ³•ä¸åŒï¼Œè¿™ä¸ªæ“ä½œï¼š
- en: Processes local neighborhoods (typically <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> or <semantics><mrow><mn>5</mn><mo>Ã—</mo><mn>5</mn></mrow><annotation
    encoding="application/x-tex">5\times 5</annotation></semantics>)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤„ç†å±€éƒ¨é‚»åŸŸï¼ˆé€šå¸¸æ˜¯ <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> æˆ– <semantics><mrow><mn>5</mn><mo>Ã—</mo><mn>5</mn></mrow><annotation
    encoding="application/x-tex">5\times 5</annotation></semantics>ï¼‰
- en: Reuses the same weights at each spatial position
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªç©ºé—´ä½ç½®é‡å¤ä½¿ç”¨ç›¸åŒçš„æƒé‡
- en: Maintains spatial structure in its output
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å…¶è¾“å‡ºä¸­ä¿æŒç©ºé—´ç»“æ„
- en: To illustrate this process concretely, consider the MNIST digit classification
    task with <semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> grayscale images.
    Each convolutional layer applies a set of filters (e.g., <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>) that slide across
    the image, computing local weighted sums. If we use 32 filters with padding to
    preserve dimensions, the layer produces a <semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn><mo>Ã—</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">28\times 28\times 32</annotation></semantics> output,
    where each spatial position contains 32 different feature measurements of its
    local neighborhood. This contrasts sharply with the Multi-Layer Perceptron (MLP)
    approach, where the entire image is flattened into a 784-dimensional vector before
    processing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…·ä½“è¯´æ˜è¿™ä¸€è¿‡ç¨‹ï¼Œè€ƒè™‘MNISTæ•°å­—åˆ†ç±»ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ä½¿ç”¨<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>ç°åº¦å›¾åƒã€‚æ¯ä¸ªå·ç§¯å±‚åº”ç”¨ä¸€ç»„è¿‡æ»¤å™¨ï¼ˆä¾‹å¦‚ï¼Œ<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>ï¼‰ï¼Œè¿™äº›è¿‡æ»¤å™¨åœ¨å›¾åƒä¸Šæ»‘åŠ¨ï¼Œè®¡ç®—å±€éƒ¨åŠ æƒæ±‚å’Œã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨32ä¸ªè¿‡æ»¤å™¨å¹¶ä½¿ç”¨å¡«å……æ¥ä¿æŒç»´åº¦ï¼Œè¯¥å±‚å°†äº§ç”Ÿä¸€ä¸ª<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn><mo>Ã—</mo><mn>32</mn></mrow><annotation
    encoding="application/x-tex">28\times 28\times 32</annotation></semantics>è¾“å‡ºï¼Œå…¶ä¸­æ¯ä¸ªç©ºé—´ä½ç½®åŒ…å«å…¶å±€éƒ¨é‚»åŸŸçš„32ä¸ªä¸åŒç‰¹å¾æµ‹é‡ã€‚è¿™ä¸å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ï¼Œåœ¨å¤„ç†ä¹‹å‰ï¼Œæ•´ä¸ªå›¾åƒè¢«å±•å¹³æˆä¸€ä¸ª784ç»´å‘é‡ã€‚
- en: This algorithmic structure directly implements the requirements for spatial
    pattern processing, creating distinct computational patterns that influence system
    design. Unlike MLPs, convolutional networks preserve spatial locality, leveraging
    the hierarchical feature extraction principles established above. These properties
    drive architectural optimizations in AI accelerators, where operations such as
    data reuse, tiling, and parallel filter computation are important for performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ç»“æ„ç›´æ¥å®ç°äº†ç©ºé—´æ¨¡å¼å¤„ç†çš„éœ€æ±‚ï¼Œåˆ›å»ºäº†å½±å“ç³»ç»Ÿè®¾è®¡çš„ç‹¬ç‰¹è®¡ç®—æ¨¡å¼ã€‚ä¸MLPsä¸åŒï¼Œå·ç§¯ç½‘ç»œä¿ç•™äº†ç©ºé—´å±€éƒ¨æ€§ï¼Œåˆ©ç”¨ä¸Šè¿°å»ºç«‹çš„åˆ†å±‚ç‰¹å¾æå–åŸåˆ™ã€‚è¿™äº›ç‰¹æ€§æ¨åŠ¨äº†äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ä¸­çš„æ¶æ„ä¼˜åŒ–ï¼Œå…¶ä¸­æ•°æ®é‡ç”¨ã€ç“¦ç‰‡åŒ–å’Œå¹¶è¡Œæ»¤æ³¢è®¡ç®—ç­‰æ“ä½œå¯¹äºæ€§èƒ½è‡³å…³é‡è¦ã€‚
- en: '**Mathematical Background**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•°å­¦èƒŒæ™¯**'
- en: Group theory provides the mathematical framework for understanding symmetries
    and transformations in data. Translation equivariance means that shifting an input
    produces a correspondingly shifted outputâ€”a key property that enables CNNs to
    recognize patterns regardless of position.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç¾¤è®ºä¸ºç†è§£æ•°æ®ä¸­çš„å¯¹ç§°æ€§å’Œå˜æ¢æä¾›äº†æ•°å­¦æ¡†æ¶ã€‚å¹³ç§»ç­‰å˜æ€§æ„å‘³ç€è¾“å…¥çš„å¹³ç§»ä¼šäº§ç”Ÿç›¸åº”å¹³ç§»çš„è¾“å‡ºâ€”â€”è¿™æ˜¯CNNèƒ½å¤Ÿè¯†åˆ«ä½ç½®æ— å…³æ¨¡å¼çš„å…³é”®å±æ€§ã€‚
- en: 'Group theory provides the framework for understanding CNN effectiveness[12](#fn12),
    which provides a mathematical framework for understanding symmetries in data.
    Translation invariance emerges because convolution is equivariant with respect
    to the translation groupâ€”if we shift the input image, the output feature maps
    shift by the same amount. Mathematically, if <semantics><msub><mi>T</mi><mi>v</mi></msub><annotation
    encoding="application/x-tex">T_v</annotation></semantics> represents translation
    by vector <semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics>,
    then a convolutional layer <semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>
    satisfies: <semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>T</mi><mi>v</mi></msub><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>T</mi><mi>v</mi></msub><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">f(T_v x) = T_v f(x)</annotation></semantics>. This
    equivariance property allows CNNs to learn features that generalize across spatial
    locations.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç¾¤è®ºä¸ºç†è§£å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æœ‰æ•ˆæ€§[12](#fn12)æä¾›äº†æ¡†æ¶ï¼Œè¿™ä¸ºç†è§£æ•°æ®ä¸­çš„å¯¹ç§°æ€§æä¾›äº†æ•°å­¦æ¡†æ¶ã€‚å¹³ç§»ä¸å˜æ€§å‡ºç°æ˜¯å› ä¸ºå·ç§¯ä¸å¹³ç§»ç¾¤ç­‰å˜â€”â€”å¦‚æœæˆ‘ä»¬ç§»åŠ¨è¾“å…¥å›¾åƒï¼Œè¾“å‡ºç‰¹å¾å›¾ä¹Ÿä¼šä»¥ç›¸åŒçš„é‡ç§»åŠ¨ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œå¦‚æœ<semantics><msub><mi>T</mi><mi>v</mi></msub><annotation
    encoding="application/x-tex">T_v</annotation></semantics>è¡¨ç¤ºç”±å‘é‡<semantics><mi>v</mi><annotation
    encoding="application/x-tex">v</annotation></semantics>è¿›è¡Œçš„å¹³ç§»ï¼Œé‚£ä¹ˆä¸€ä¸ªå·ç§¯å±‚<semantics><mi>f</mi><annotation
    encoding="application/x-tex">f</annotation></semantics>æ»¡è¶³ï¼š<semantics><mrow><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>T</mi><mi>v</mi></msub><mi>x</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>T</mi><mi>v</mi></msub><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">f(T_v x) = T_v f(x)</annotation></semantics>ã€‚è¿™ç§ç­‰å˜æ€§å±æ€§ä½¿å¾—CNNèƒ½å¤Ÿå­¦ä¹ è·¨ç©ºé—´ä½ç½®çš„æ³›åŒ–ç‰¹å¾ã€‚
- en: 'The choice of convolution reflects deeper principles about inductive bias[13](#fn13)
    in neural architecture design. By restricting connectivity to local neighborhoods
    and sharing parameters across spatial positions, CNNs encode prior knowledge about
    the structure of visual data: that important features are local and translation-invariant.
    This architectural constraint reduces the hypothesis space[14](#fn14) that the
    network must search, enabling more efficient learning from limited data compared
    to fully connected networks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯çš„é€‰æ‹©åæ˜ äº†å…³äºç¥ç»ç½‘ç»œè®¾è®¡ä¸­å½’çº³åç½®[13](#fn13)çš„æ›´æ·±å±‚åŸåˆ™ã€‚é€šè¿‡é™åˆ¶è¿æ¥åˆ°å±€éƒ¨é‚»åŸŸå¹¶åœ¨ç©ºé—´ä½ç½®é—´å…±äº«å‚æ•°ï¼ŒCNNç¼–ç äº†å…³äºè§†è§‰æ•°æ®ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ï¼šé‡è¦çš„ç‰¹å¾æ˜¯å±€éƒ¨çš„å’Œå¹³ç§»ä¸å˜çš„ã€‚è¿™ç§æ¶æ„çº¦æŸå‡å°‘äº†ç½‘ç»œå¿…é¡»æœç´¢çš„å‡è®¾ç©ºé—´[14](#fn14)ï¼Œä¸å…¨è¿æ¥ç½‘ç»œç›¸æ¯”ï¼Œè¿™ä½¿å¾—ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ›´åŠ é«˜æ•ˆã€‚
- en: 'CNNs naturally implement hierarchical representation learning through their
    layered structure. Early layers detect low-level features like edges and textures
    with small receptive fields, while deeper layers combine these into increasingly
    complex patterns with larger receptive fields. This hierarchical organization
    mirrors the structure of the visual cortex and enables CNNs to build compositional
    representations: complex objects are represented as compositions of simpler parts.
    The mathematical foundation for this emerges from the fact that stacking convolutional
    layers creates a tree-like dependency structure, where each deep neuron depends
    on an exponentially large set of input pixels, enabling efficient representation
    of hierarchical patterns.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: CNNé€šè¿‡å…¶åˆ†å±‚ç»“æ„è‡ªç„¶åœ°å®ç°äº†å±‚æ¬¡åŒ–è¡¨ç¤ºå­¦ä¹ ã€‚æ—©æœŸå±‚æ£€æµ‹å…·æœ‰å°æ„Ÿå—é‡çš„ä½çº§ç‰¹å¾ï¼Œå¦‚è¾¹ç¼˜å’Œçº¹ç†ï¼Œè€Œæ·±å±‚å±‚å°†è¿™äº›ç‰¹å¾ç»„åˆæˆå…·æœ‰æ›´å¤§æ„Ÿå—é‡çš„è¶Šæ¥è¶Šå¤æ‚çš„æ¨¡å¼ã€‚è¿™ç§å±‚æ¬¡åŒ–ç»„ç»‡åæ˜ äº†è§†è§‰çš®å±‚çš„ç»“æ„ï¼Œä½¿å¾—CNNèƒ½å¤Ÿæ„å»ºç»„åˆè¡¨ç¤ºï¼šå¤æ‚å¯¹è±¡è¢«è¡¨ç¤ºä¸ºæ›´ç®€å•éƒ¨åˆ†çš„ç»„åˆã€‚è¿™ç§æ•°å­¦åŸºç¡€çš„æ¥æºæ˜¯ï¼Œå †å å·ç§¯å±‚åˆ›å»ºäº†ä¸€ä¸ªæ ‘çŠ¶ä¾èµ–ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªæ·±å±‚ç¥ç»å…ƒä¾èµ–äºä¸€ä¸ªæŒ‡æ•°çº§å¤§çš„è¾“å…¥åƒç´ é›†ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨ç¤ºå±‚æ¬¡åŒ–æ¨¡å¼ã€‚
- en: Architectural Characteristics
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å»ºç­‘ç‰¹æ€§
- en: Parameter sharing dramatically reduces complexity compared to MLPs by reusing
    the same filters across spatial locations. This sharing embodies the assumption
    that useful features (such as edges or textures) can appear anywhere in an image,
    making the same feature detector valuable across all spatial positions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸MLPç›¸æ¯”ï¼Œå‚æ•°å…±äº«é€šè¿‡åœ¨ç©ºé—´ä½ç½®é—´é‡ç”¨ç›¸åŒçš„è¿‡æ»¤å™¨å¤§å¤§å‡å°‘äº†å¤æ‚æ€§ã€‚è¿™ç§å…±äº«ä½“ç°äº†è¿™æ ·çš„å‡è®¾ï¼šæœ‰ç”¨çš„ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜æˆ–çº¹ç†ï¼‰å¯ä»¥å‡ºç°åœ¨å›¾åƒçš„ä»»ä½•åœ°æ–¹ï¼Œä½¿å¾—ç›¸åŒçš„ç‰¹å¾æ£€æµ‹å™¨åœ¨æ‰€æœ‰ç©ºé—´ä½ç½®éƒ½æœ‰ä»·å€¼ã€‚
- en: The architectural efficiency of CNNs enables further optimization through specialized
    techniques. Depthwise separable convolutions decompose standard convolutions into
    depthwise and pointwise operations, reducing computation by 8-9Ã— for typical mobile
    deployments. Channel pruning eliminates entire feature maps based on importance
    metrics, achieving 40-50% FLOPs reduction with <1% accuracy loss. These optimization
    strategies build on spatial locality principles, with [ChapterÂ 10](ch016.xhtml#sec-model-optimizations)
    exploring hardware-specific implementations and [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration)
    detailing how modern processors exploit convolutionâ€™s inherent data reuse patterns.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: CNNsçš„æ¶æ„æ•ˆç‡ä½¿å¾—å¯ä»¥é€šè¿‡ä¸“ç”¨æŠ€æœ¯è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚æ·±åº¦å¯åˆ†ç¦»å·ç§¯å°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºæ·±åº¦å’Œç‚¹æ“ä½œï¼Œå¯¹äºå…¸å‹çš„ç§»åŠ¨éƒ¨ç½²ï¼Œè®¡ç®—é‡å‡å°‘äº†8-9å€ã€‚é€šé“å‰ªææ ¹æ®é‡è¦æ€§æŒ‡æ ‡æ¶ˆé™¤æ•´ä¸ªç‰¹å¾å›¾ï¼Œåœ¨å°äº1%çš„ç²¾åº¦æŸå¤±ä¸‹å®ç°äº†40-50%çš„FLOPså‡å°‘ã€‚è¿™äº›ä¼˜åŒ–ç­–ç•¥å»ºç«‹åœ¨ç©ºé—´å±€éƒ¨æ€§åŸç†ä¹‹ä¸Šï¼Œ[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)æ¢è®¨äº†ç‰¹å®šç¡¬ä»¶çš„å®ç°ï¼Œ[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)è¯¦ç»†è¯´æ˜äº†ç°ä»£å¤„ç†å™¨å¦‚ä½•åˆ©ç”¨å·ç§¯å›ºæœ‰çš„æ•°æ®é‡ç”¨æ¨¡å¼ã€‚
- en: As illustrated in [FigureÂ 4.3](ch010.xhtml#fig-cnn), convolution operations
    involve sliding a small filter over the input image to generate a feature map[15](#fn15).
    This process captures local structures while maintaining translation invariance.
    For an interactive visual exploration of convolutional networks, the [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
    project provides an insightful demonstration of how these networks are constructed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾4.3](ch010.xhtml#fig-cnn)æ‰€ç¤ºï¼Œå·ç§¯æ“ä½œæ¶‰åŠåœ¨è¾“å…¥å›¾åƒä¸Šæ»‘åŠ¨ä¸€ä¸ªå°æ»¤æ³¢å™¨ä»¥ç”Ÿæˆç‰¹å¾å›¾[15](#fn15)ã€‚æ­¤è¿‡ç¨‹æ•æ‰å±€éƒ¨ç»“æ„åŒæ—¶ä¿æŒå¹³ç§»ä¸å˜æ€§ã€‚å¯¹äºå·ç§¯ç½‘ç»œçš„äº¤äº’å¼è§†è§‰æ¢ç´¢ï¼Œ[CNN
    Explainer](https://poloclub.github.io/cnn-explainer/)é¡¹ç›®æä¾›äº†ä¸€ä¸ªæœ‰è§åœ°çš„æ¼”ç¤ºï¼Œå±•ç¤ºäº†è¿™äº›ç½‘ç»œæ˜¯å¦‚ä½•æ„å»ºçš„ã€‚
- en: '![](../media/file56.svg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file56.svg)'
- en: 'FigureÂ 4.3: The convolution operation processes input data through localized
    feature extraction using filters that slide across the image to identify patterns
    regardless of their position.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.3ï¼šå·ç§¯æ“ä½œé€šè¿‡ä½¿ç”¨åœ¨å›¾åƒä¸Šæ»‘åŠ¨çš„æ»¤æ³¢å™¨è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–æ¥å¤„ç†è¾“å…¥æ•°æ®ï¼Œä»¥è¯†åˆ«ä½ç½®æ— å…³çš„æ¨¡å¼ã€‚
- en: Computational Mapping
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æ˜ å°„
- en: Convolution operations create computational patterns different from MLP dense
    matrix multiplication. This translation from mathematical operations to implementation
    details reveals distinct computational characteristics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ“ä½œåˆ›å»ºçš„è®¡ç®—æ¨¡å¼ä¸MLPå¯†é›†çŸ©é˜µä¹˜æ³•ä¸åŒã€‚è¿™ç§ä»æ•°å­¦è¿ç®—åˆ°å®ç°ç»†èŠ‚çš„è½¬æ¢æ­ç¤ºäº†ç‹¬ç‰¹çš„è®¡ç®—ç‰¹æ€§ã€‚
- en: The first implementation, `conv_layer_spatial` (shown in [ListingÂ 4.3](ch010.xhtml#lst-conv_layer_spatial)),
    uses high-level convolution operations to express the computation concisely. This
    is typical in deep learning frameworks, where optimized libraries handle the underlying
    complexity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ç§å®ç°ï¼Œ`conv_layer_spatial`ï¼ˆå¦‚[åˆ—è¡¨4.3](ch010.xhtml#lst-conv_layer_spatial)æ‰€ç¤ºï¼‰ï¼Œä½¿ç”¨é«˜çº§å·ç§¯æ“ä½œç®€æ´åœ°è¡¨è¾¾è®¡ç®—ã€‚è¿™åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¾ˆå…¸å‹ï¼Œå…¶ä¸­ä¼˜åŒ–çš„åº“å¤„ç†åº•å±‚å¤æ‚æ€§ã€‚
- en: 'ListingÂ 4.3: This hierarchical approach processes input data through feature
    extraction using a convolution operation that combines a kernel and bias before
    applying an activation function.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.3ï¼šè¿™ç§åˆ†å±‚æ–¹æ³•é€šè¿‡ä½¿ç”¨ç»“åˆæ ¸å’Œåç½®çš„å·ç§¯æ“ä½œè¿›è¡Œç‰¹å¾æå–æ¥å¤„ç†è¾“å…¥æ•°æ®ï¼Œç„¶ååœ¨åº”ç”¨æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The bridge between the logical model and physical execution becomes critical
    for understanding CNN system requirements. While the high-level convolution operation
    appears as a simple sliding window computation, the hardware must orchestrate
    complex data movement patterns and exploit spatial locality for efficiency.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘æ¨¡å‹ä¸ç‰©ç†æ‰§è¡Œä¹‹é—´çš„æ¡¥æ¢å¯¹äºç†è§£CNNç³»ç»Ÿéœ€æ±‚è‡³å…³é‡è¦ã€‚è™½ç„¶é«˜çº§å·ç§¯æ“ä½œçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªç®€å•çš„æ»‘åŠ¨çª—å£è®¡ç®—ï¼Œä½†ç¡¬ä»¶å¿…é¡»ç¼–æ’å¤æ‚çš„æ•°æ®ç§»åŠ¨æ¨¡å¼å¹¶åˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§ä»¥æé«˜æ•ˆç‡ã€‚
- en: 'The second implementation, conv_layer_compute (see [ListingÂ 4.4](ch010.xhtml#lst-conv_layer_compute)),
    reveals the actual computational pattern: nested loops that process each spatial
    position, applying the same filter weights to local regions of the input. These
    seven nested loops expose the true nature of convolutionâ€™s computational structure
    and the optimization opportunities it creates.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç§å®ç°ï¼Œconv_layer_computeï¼ˆè§[åˆ—è¡¨4.4](ch010.xhtml#lst-conv_layer_compute)ï¼‰ï¼Œæ­ç¤ºäº†å®é™…çš„è®¡ç®—æ¨¡å¼ï¼šåµŒå¥—å¾ªç¯å¤„ç†æ¯ä¸ªç©ºé—´ä½ç½®ï¼Œå°†ç›¸åŒçš„æ»¤æ³¢å™¨æƒé‡åº”ç”¨äºè¾“å…¥çš„å±€éƒ¨åŒºåŸŸã€‚è¿™ä¸ƒä¸ªåµŒå¥—å¾ªç¯æ­ç¤ºäº†å·ç§¯è®¡ç®—ç»“æ„çš„çœŸæ­£æœ¬è´¨ä»¥åŠå®ƒæ‰€åˆ›é€ çš„ä¼˜åŒ–æœºä¼šã€‚
- en: 'ListingÂ 4.4: **Nested Loops**: Convolutional layers process input through multiple
    nested loops that handle batched images, spatial dimensions, output channels,
    kernel windows, and input features, revealing the detailed computational structure
    of convolution operations.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.4ï¼š**åµŒå¥—å¾ªç¯**ï¼šå·ç§¯å±‚é€šè¿‡å¤šä¸ªåµŒå¥—å¾ªç¯å¤„ç†æ‰¹é‡å›¾åƒã€ç©ºé—´ç»´åº¦ã€è¾“å‡ºé€šé“ã€æ ¸çª—å£å’Œè¾“å…¥ç‰¹å¾ï¼Œæ­ç¤ºäº†å·ç§¯æ“ä½œçš„è¯¦ç»†è®¡ç®—ç»“æ„ã€‚
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The seven nested loops reveal different aspects of the computation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ƒå±‚åµŒå¥—å¾ªç¯æ­ç¤ºäº†è®¡ç®—çš„å„ä¸ªæ–¹é¢ï¼š
- en: 'Outer loops (1-3) manage position: which image and where in the image'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤–å±‚å¾ªç¯ï¼ˆ1-3ï¼‰ç®¡ç†ä½ç½®ï¼šå“ªä¸ªå›¾åƒä»¥åŠå›¾åƒä¸­çš„ä½ç½®
- en: 'Middle loop (4) handles output features: computing different learned patterns'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸­é—´å¾ªç¯ï¼ˆ4ï¼‰å¤„ç†è¾“å‡ºç‰¹å¾ï¼šè®¡ç®—ä¸åŒçš„å­¦ä¹ æ¨¡å¼
- en: 'Inner loops (5-7) perform the actual convolution: sliding the kernel window'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å±‚å¾ªç¯ï¼ˆ5-7ï¼‰æ‰§è¡Œå®é™…çš„å·ç§¯æ“ä½œï¼šæ»‘åŠ¨æ ¸çª—å£
- en: Examining this process in detail, the outer two loops (`for y` and `for x`)
    traverse each spatial position in the output feature map (for the MNIST example,
    this traverses all <semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> positions).
    At each position, values are computed for each output channel (`for k` loop),
    representing different learned features or patternsâ€”the 32 different feature detectors.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¦ç»†è€ƒå¯Ÿæ­¤è¿‡ç¨‹ï¼Œå¤–ä¸¤å±‚å¾ªç¯ï¼ˆ`for y`å’Œ`for x`ï¼‰éå†è¾“å‡ºç‰¹å¾å›¾ä¸­çš„æ¯ä¸ªç©ºé—´ä½ç½®ï¼ˆå¯¹äºMNISTç¤ºä¾‹ï¼Œè¿™éå†äº†æ‰€æœ‰<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>ä½ç½®ï¼‰ã€‚åœ¨æ¯ä¸ªä½ç½®ï¼Œè®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„å€¼ï¼ˆ`for
    k`å¾ªç¯ï¼‰ï¼Œä»£è¡¨ä¸åŒçš„å­¦ä¹ ç‰¹å¾æˆ–æ¨¡å¼â€”â€”32ä¸ªä¸åŒçš„ç‰¹å¾æ£€æµ‹å™¨ã€‚
- en: The inner three loops implement the actual convolution operation at each position.
    For each output value, we process a local <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> region of the
    input (the `dy` and `dx` loops) across all input channels (`for c` loop). This
    creates a sliding window effect, where the same <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filter moves across
    the image, performing multiply-accumulates between the filter weights and the
    local input values. Unlike the MLPâ€™s global connectivity, this local processing
    pattern means each output value depends only on a small neighborhood of the input.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å±‚ä¸‰ä¸ªå¾ªç¯åœ¨æ¯ä¸ªä½ç½®å®ç°å®é™…çš„å·ç§¯æ“ä½œã€‚å¯¹äºæ¯ä¸ªè¾“å‡ºå€¼ï¼Œæˆ‘ä»¬å¤„ç†è¾“å…¥çš„å±€éƒ¨<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>åŒºåŸŸï¼ˆ`dy`å’Œ`dx`å¾ªç¯ï¼‰ä»¥åŠæ‰€æœ‰è¾“å…¥é€šé“ï¼ˆ`for
    c`å¾ªç¯ï¼‰ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªæ»‘åŠ¨çª—å£æ•ˆæœï¼Œå…¶ä¸­ç›¸åŒçš„<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>æ»¤æ³¢å™¨åœ¨å›¾åƒä¸Šç§»åŠ¨ï¼Œæ‰§è¡Œæ»¤æ³¢å™¨æƒé‡å’Œå±€éƒ¨è¾“å…¥å€¼ä¹‹é—´çš„ä¹˜åŠ æ“ä½œã€‚ä¸MLPçš„å…¨å±€è¿æ¥æ€§ä¸åŒï¼Œè¿™ç§å±€éƒ¨å¤„ç†æ¨¡å¼æ„å‘³ç€æ¯ä¸ªè¾“å‡ºå€¼åªä¾èµ–äºè¾“å…¥çš„å°é‚»åŸŸã€‚
- en: For our MNIST example with <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filters and 32
    output channels, each output position requires only 9 multiply-accumulate operations
    per input channel, compared to the 784 operations needed in our MLP layer. This
    operation must be repeated for every spatial position <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>28</mn><mo>Ã—</mo><mn>28</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(28\times 28)</annotation></semantics> and every
    output channel (32).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„MNISTç¤ºä¾‹ï¼Œä½¿ç”¨<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>æ»¤æ³¢å™¨å’Œ32ä¸ªè¾“å‡ºé€šé“ï¼Œæ¯ä¸ªè¾“å‡ºä½ç½®åªéœ€è¦æ¯ä¸ªè¾“å…¥é€šé“9æ¬¡ä¹˜åŠ æ“ä½œï¼Œè€Œæˆ‘ä»¬çš„MLPå±‚éœ€è¦784æ¬¡æ“ä½œã€‚è¿™ä¸ªæ“ä½œå¿…é¡»å¯¹æ¯ä¸ªç©ºé—´ä½ç½®<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>28</mn><mo>Ã—</mo><mn>28</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times
    28)</annotation></semantics>å’Œæ¯ä¸ªè¾“å‡ºé€šé“ï¼ˆ32ï¼‰é‡å¤è¿›è¡Œã€‚
- en: While using fewer operations per output, the spatial structure creates different
    patterns of memory access and computation that systems must handle. These patterns
    influence system design, creating both challenges and opportunities for optimization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ¯ä¸ªè¾“å‡ºä½¿ç”¨çš„æ“ä½œè¾ƒå°‘ï¼Œä½†ç©ºé—´ç»“æ„åˆ›å»ºäº†ä¸åŒçš„å†…å­˜è®¿é—®å’Œè®¡ç®—æ¨¡å¼ï¼Œç³»ç»Ÿå¿…é¡»å¤„ç†ã€‚è¿™äº›æ¨¡å¼å½±å“ç³»ç»Ÿè®¾è®¡ï¼Œæ—¢å¸¦æ¥äº†ä¼˜åŒ–æŒ‘æˆ˜ï¼Œä¹Ÿå¸¦æ¥äº†ä¼˜åŒ–æœºä¼šã€‚
- en: System Implications
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: CNNs exhibit distinctive system-level patterns that differ significantly from
    MLP dense connectivity across all three analysis dimensions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: CNNåœ¨ä¸‰ä¸ªåˆ†æç»´åº¦ä¸Šæ˜¾ç¤ºå‡ºä¸MLPå¯†é›†è¿æ¥æ˜¾è‘—ä¸åŒçš„ç³»ç»Ÿçº§æ¨¡å¼ã€‚
- en: Memory Requirements
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜éœ€æ±‚
- en: 'For convolutional layers, memory requirements center around two key components:
    filter weights and feature maps. Unlike MLPs that require storing full connection
    matrices, CNNs use small, reusable filters. For a typical CNN processing 224Ã—224
    ImageNet images, a convolutional layer with 64 filters of size <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> requires storing
    only 576 weight parameters <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>Ã—</mo><mn>3</mn><mo>Ã—</mo><mn>64</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times
    3\times 64)</annotation></semantics>, dramatically less than the millions of weights
    needed for equivalent fully-connected processing. The system must store feature
    maps for all spatial positions, creating a different memory demand. A 224Ã—224
    input with 64 output channels requires storing 3.2 million activation values (224Ã—224Ã—64).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå·ç§¯å±‚ï¼Œå†…å­˜éœ€æ±‚ä¸»è¦é›†ä¸­åœ¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ä¸Šï¼šæ»¤æ³¢å™¨æƒé‡å’Œç‰¹å¾å›¾ã€‚ä¸éœ€è¦å­˜å‚¨å®Œæ•´è¿æ¥çŸ©é˜µçš„MLPä¸åŒï¼ŒCNNä½¿ç”¨å°å‹ã€å¯é‡ç”¨çš„æ»¤æ³¢å™¨ã€‚å¯¹äºä¸€ä¸ªå…¸å‹çš„CNNå¤„ç†224Ã—224çš„ImageNetå›¾åƒï¼Œä¸€ä¸ªåŒ…å«64ä¸ª<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>å¤§å°æ»¤æ³¢å™¨çš„å·ç§¯å±‚åªéœ€è¦å­˜å‚¨576ä¸ªæƒé‡å‚æ•°<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo>Ã—</mo><mn>3</mn><mo>Ã—</mo><mn>64</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times
    3\times 64)</annotation></semantics>ï¼Œè¿™æ¯”ç­‰æ•ˆçš„å…¨è¿æ¥å¤„ç†æ‰€éœ€çš„æ•°ç™¾ä¸‡ä¸ªæƒé‡å°‘å¾—å¤šã€‚ç³»ç»Ÿå¿…é¡»å­˜å‚¨æ‰€æœ‰ç©ºé—´ä½ç½®çš„ç‰¹å¾å›¾ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„å†…å­˜éœ€æ±‚ã€‚ä¸€ä¸ª224Ã—224çš„è¾“å…¥å’Œ64ä¸ªè¾“å‡ºé€šé“éœ€è¦å­˜å‚¨320ä¸‡ä¸ªæ¿€æ´»å€¼ï¼ˆ224Ã—224Ã—64ï¼‰ã€‚
- en: These memory access patterns suggest opportunities for optimization through
    weight reuse and careful feature map management. Processors optimize these spatial
    patterns by caching filter weights for reuse across positions while streaming
    feature map data. Frameworks implement spatial optimizations through specialized
    memory layouts that enable filter reuse and spatial locality in feature map access.
    CPUs and GPUs approach this differently. CPUs use their cache hierarchy to keep
    frequently used filters resident, while GPUs employ specialized memory architectures
    designed for the spatial access patterns of image processing. The detailed architecture
    design principles for these specialized processors are covered in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å­˜è®¿é—®æ¨¡å¼è¡¨æ˜ï¼Œé€šè¿‡æƒé‡é‡ç”¨å’Œä»”ç»†çš„ç‰¹å¾å›¾ç®¡ç†å¯ä»¥å®ç°ä¼˜åŒ–çš„æœºä¼šã€‚å¤„ç†å™¨é€šè¿‡ç¼“å­˜ç”¨äºè·¨ä½ç½®é‡ç”¨çš„æ»¤æ³¢å™¨æƒé‡æ¥ä¼˜åŒ–è¿™äº›ç©ºé—´æ¨¡å¼ï¼ŒåŒæ—¶æµå¼ä¼ è¾“ç‰¹å¾å›¾æ•°æ®ã€‚æ¡†æ¶é€šè¿‡ä¸“é—¨çš„å†…å­˜å¸ƒå±€å®ç°ç©ºé—´ä¼˜åŒ–ï¼Œè¿™äº›å¸ƒå±€èƒ½å¤Ÿå®ç°æ»¤æ³¢å™¨é‡ç”¨å’Œç‰¹å¾å›¾è®¿é—®çš„ç©ºé—´å±€éƒ¨æ€§ã€‚CPUå’ŒGPUé‡‡ç”¨ä¸åŒçš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚CPUä½¿ç”¨å…¶ç¼“å­˜å±‚æ¬¡ç»“æ„æ¥ä¿æŒå¸¸ç”¨æ»¤æ³¢å™¨çš„é©»ç•™ï¼Œè€ŒGPUåˆ™é‡‡ç”¨ä¸“ä¸ºå›¾åƒå¤„ç†çš„ç©ºé—´è®¿é—®æ¨¡å¼è®¾è®¡çš„ä¸“ç”¨å†…å­˜æ¶æ„ã€‚è¿™äº›ä¸“ç”¨å¤„ç†å™¨çš„è¯¦ç»†æ¶æ„è®¾è®¡åŸåˆ™åœ¨[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­æœ‰æ‰€ä»‹ç»ã€‚
- en: Computation Needs
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—éœ€æ±‚
- en: The core computation in CNNs involves repeatedly applying small filters across
    spatial positions. Each output value requires a local multiply-accumulate operation
    over the filter region. For ImageNet processing with <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filters and 64
    output channels, computing one spatial position involves 576 multiply-accumulates
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>Ã—</mo><mn>3</mn><mo>Ã—</mo><mn>64</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times
    3\times 64)</annotation></semantics>, and this must be repeated for all 50,176
    spatial positions (224Ã—224). While each individual computation involves fewer
    operations than an MLP layer, the total computational load remains large due to
    spatial repetition.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CNNsçš„æ ¸å¿ƒè®¡ç®—æ¶‰åŠåœ¨ç©ºé—´ä½ç½®ä¸Šåå¤åº”ç”¨å°å‹æ»¤æ³¢å™¨ã€‚æ¯ä¸ªè¾“å‡ºå€¼éƒ½éœ€è¦åœ¨æ»¤æ³¢å™¨åŒºåŸŸå†…è¿›è¡Œå±€éƒ¨ä¹˜ç´¯åŠ æ“ä½œã€‚å¯¹äºä½¿ç”¨<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>æ»¤æ³¢å™¨å’Œ64ä¸ªè¾“å‡ºé€šé“çš„ImageNetå¤„ç†ï¼Œè®¡ç®—ä¸€ä¸ªç©ºé—´ä½ç½®æ¶‰åŠ576æ¬¡ä¹˜ç´¯åŠ <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>3</mn><mo>Ã—</mo><mn>3</mn><mo>Ã—</mo><mn>64</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3\times
    3\times 64)</annotation></semantics>ï¼Œå¹¶ä¸”è¿™å¿…é¡»åœ¨æ‰€æœ‰50,176ä¸ªç©ºé—´ä½ç½®ï¼ˆ224Ã—224ï¼‰ä¸Šé‡å¤è¿›è¡Œã€‚è™½ç„¶æ¯ä¸ªå•ç‹¬çš„è®¡ç®—æ¶‰åŠçš„è¿ç®—æ¯”MLPå±‚å°‘ï¼Œä½†ç”±äºç©ºé—´é‡å¤ï¼Œæ€»çš„è®¡ç®—è´Ÿè½½ä»ç„¶å¾ˆå¤§ã€‚
- en: This computational pattern presents different optimization opportunities than
    MLPs. The regular, repeated nature of convolution operations enables efficient
    hardware utilization through structured parallelism. Modern processors exploit
    this pattern in various ways. CPUs leverage SIMD instructions[16](#fn16) to process
    multiple filter positions simultaneously, while GPUs parallelize computation across
    spatial positions and channels. The model optimization techniques that further
    reduce these computational demands, including specialized convolution optimizations
    and sparsity patterns, are detailed in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¡ç®—æ¨¡å¼æ¯”MLPsæä¾›äº†ä¸åŒçš„ä¼˜åŒ–æœºä¼šã€‚å·ç§¯æ“ä½œçš„è§„åˆ™ã€é‡å¤æ€§è´¨é€šè¿‡ç»“æ„åŒ–å¹¶è¡ŒåŒ–å®ç°äº†é«˜æ•ˆçš„ç¡¬ä»¶åˆ©ç”¨ã€‚ç°ä»£å¤„ç†å™¨ä»¥å„ç§æ–¹å¼åˆ©ç”¨è¿™ç§æ¨¡å¼ã€‚CPUåˆ©ç”¨SIMDæŒ‡ä»¤[16](#fn16)åŒæ—¶å¤„ç†å¤šä¸ªè¿‡æ»¤å™¨ä½ç½®ï¼Œè€ŒGPUåœ¨ç©ºé—´ä½ç½®å’Œé€šé“ä¸Šå¹¶è¡ŒåŒ–è®¡ç®—ã€‚åŒ…æ‹¬ä¸“é—¨çš„å·ç§¯ä¼˜åŒ–å’Œç¨€ç–æ¨¡å¼åœ¨å†…çš„è¿›ä¸€æ­¥é™ä½è¿™äº›è®¡ç®—éœ€æ±‚çš„æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)ä¸­è¯¦ç»†è¯´æ˜ã€‚
- en: Data Movement
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨
- en: 'The sliding window pattern of convolutions creates a distinctive data movement
    profile. Unlike MLPs where each weight is used once per forward pass, CNN filter
    weights are reused many times as the filter slides across spatial positions. For
    ImageNet processing, each <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filter weight
    is reused 50,176 times (once for each position in the 224Ã—224 feature map). This
    creates a different challenge: the system must stream input features through the
    computation unit while keeping filter weights stable.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯çš„æ»‘åŠ¨çª—å£æ¨¡å¼åˆ›å»ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„æ•°æ®ç§»åŠ¨é…ç½®æ–‡ä»¶ã€‚ä¸MLPsä¸­æ¯ä¸ªæƒé‡åœ¨æ¯ä¸ªå‰å‘ä¼ é€’ä¸­åªä½¿ç”¨ä¸€æ¬¡ä¸åŒï¼ŒCNNçš„è¿‡æ»¤å™¨æƒé‡åœ¨è¿‡æ»¤å™¨åœ¨ç©ºé—´ä½ç½®ä¸Šæ»‘åŠ¨æ—¶è¢«å¤šæ¬¡é‡ç”¨ã€‚å¯¹äºImageNetå¤„ç†ï¼Œæ¯ä¸ª<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>è¿‡æ»¤å™¨æƒé‡è¢«é‡ç”¨50,176æ¬¡ï¼ˆåœ¨224Ã—224ç‰¹å¾å›¾ä¸­çš„æ¯ä¸ªä½ç½®éƒ½ä½¿ç”¨ä¸€æ¬¡ï¼‰ã€‚è¿™å¸¦æ¥äº†ä¸åŒçš„æŒ‘æˆ˜ï¼šç³»ç»Ÿå¿…é¡»åœ¨ä¿æŒè¿‡æ»¤å™¨æƒé‡ç¨³å®šçš„åŒæ—¶ï¼Œå°†è¾“å…¥ç‰¹å¾æµç»è®¡ç®—å•å…ƒã€‚
- en: The predictable spatial access pattern enables strategic data movement optimizations.
    Different architectures handle this movement pattern through specialized mechanisms.
    CPUs maintain frequently used filter weights in cache while streaming through
    input features. GPUs employ memory architectures optimized for spatial locality
    and provide hardware support for efficient sliding window operations. Deep learning
    frameworks orchestrate these movements by organizing computations to maximize
    filter weight reuse and minimize redundant feature map accesses.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¯é¢„æµ‹çš„ç©ºé—´è®¿é—®æ¨¡å¼ä½¿å¾—æ•°æ®ç§»åŠ¨ä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚ä¸åŒçš„æ¶æ„é€šè¿‡ä¸“é—¨çš„æœºåˆ¶æ¥å¤„ç†è¿™ç§ç§»åŠ¨æ¨¡å¼ã€‚CPUåœ¨æµç»è¾“å…¥ç‰¹å¾çš„åŒæ—¶ï¼Œå°†é¢‘ç¹ä½¿ç”¨çš„è¿‡æ»¤å™¨æƒé‡ä¿æŒåœ¨ç¼“å­˜ä¸­ã€‚GPUé‡‡ç”¨é’ˆå¯¹ç©ºé—´å±€éƒ¨æ€§ä¼˜åŒ–çš„å†…å­˜æ¶æ„ï¼Œå¹¶ä¸ºé«˜æ•ˆçš„æ»‘åŠ¨çª—å£æ“ä½œæä¾›ç¡¬ä»¶æ”¯æŒã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶é€šè¿‡ç»„ç»‡è®¡ç®—æ¥æœ€å¤§åŒ–è¿‡æ»¤å™¨æƒé‡çš„é‡ç”¨å¹¶æœ€å°åŒ–å†—ä½™çš„ç‰¹å¾å›¾è®¿é—®ï¼Œä»è€Œåè°ƒè¿™äº›ç§»åŠ¨ã€‚
- en: 'RNNs: Sequential Pattern Processing'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNNsï¼šåºåˆ—æ¨¡å¼å¤„ç†
- en: Convolutional Neural Networks achieved efficiency gains by exploiting spatial
    locality, yet their architectural assumptions fail when patterns depend on temporal
    order rather than spatial proximity. While CNNs excel at recognizing "what" is
    present in data through shared feature detectors, they cannot capture "when" events
    occur or how they relate across time. This limitation manifests in domains such
    as natural language processing, where word meaning depends on sentential context,
    and time-series analysis, where future values depend on historical patterns.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡åˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§å®ç°äº†æ•ˆç‡æå‡ï¼Œä½†å½“æ¨¡å¼ä¾èµ–äºæ—¶é—´é¡ºåºè€Œä¸æ˜¯ç©ºé—´é‚»è¿‘æ€§æ—¶ï¼Œå…¶æ¶æ„å‡è®¾å°±ä¼šå¤±è´¥ã€‚è™½ç„¶CNNåœ¨é€šè¿‡å…±äº«ç‰¹å¾æ£€æµ‹å™¨è¯†åˆ«æ•°æ®ä¸­çš„â€œä»€ä¹ˆâ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬æ— æ³•æ•æ‰â€œä½•æ—¶â€äº‹ä»¶å‘ç”Ÿæˆ–å®ƒä»¬éšæ—¶é—´å¦‚ä½•ç›¸å…³ã€‚è¿™ç§é™åˆ¶åœ¨è¯¸å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºæ¥ï¼Œå…¶ä¸­è¯ä¹‰å–å†³äºå¥å­ä¸Šä¸‹æ–‡ï¼Œä»¥åŠåœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œæœªæ¥çš„å€¼ä¾èµ–äºå†å²æ¨¡å¼ã€‚
- en: 'Sequential data presents a challenge distinct from spatial processing: patterns
    can span arbitrary temporal distances, rendering fixed-size kernels ineffective.
    While spatial convolution leverages the principle that nearby pixels are typically
    related, temporal relationships operate differently. Important connections may
    span hundreds or thousands of time steps with no correlation to proximity. Traditional
    feedforward architectures, including CNNs, process each input independently and
    cannot maintain the temporal context necessary for these long-range dependencies.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: åºåˆ—æ•°æ®æå‡ºäº†ä¸ç©ºé—´å¤„ç†ä¸åŒçš„æŒ‘æˆ˜ï¼šæ¨¡å¼å¯ä»¥è·¨è¶Šä»»æ„çš„æ—¶é—´è·ç¦»ï¼Œä½¿å¾—å›ºå®šå¤§å°çš„æ ¸æ— æ•ˆã€‚è™½ç„¶ç©ºé—´å·ç§¯åˆ©ç”¨äº†é‚»è¿‘åƒç´ é€šå¸¸ç›¸å…³çš„åŸç†ï¼Œä½†æ—¶é—´å…³ç³»è¿ä½œæ–¹å¼ä¸åŒã€‚é‡è¦çš„è¿æ¥å¯èƒ½è·¨è¶Šæ•°ç™¾æˆ–æ•°åƒä¸ªæ—¶é—´æ­¥ï¼Œä¸é‚»è¿‘æ€§æ²¡æœ‰ç›¸å…³æ€§ã€‚ä¼ ç»Ÿçš„å‰é¦ˆæ¶æ„ï¼ŒåŒ…æ‹¬CNNï¼Œç‹¬ç«‹å¤„ç†æ¯ä¸ªè¾“å…¥ï¼Œæ— æ³•ç»´æŒè¿™äº›é•¿è·ç¦»ä¾èµ–æ‰€éœ€çš„æ—¶åºä¸Šä¸‹æ–‡ã€‚
- en: 'Recurrent Neural Networks address this architectural limitation ([Elman 1990](ch058.xhtml#ref-elman1990finding);
    [Hochreiter and Schmidhuber 1997](ch058.xhtml#ref-hochreiter1997long)) by embodying
    a temporal inductive bias: they assume sequential dependence, where the order
    of information matters and the past influences the present. This architectural
    assumption guides the introduction of memory as a component of the computational
    model. Rather than processing inputs in isolation, RNNs maintain an internal state
    that propagates information from previous time steps, enabling the network to
    condition its current output on historical context. This architecture embodies
    another trade-off: while CNNs sacrifice theoretical generality for spatial efficiency,
    RNNs introduce computational dependencies that challenge parallel execution in
    exchange for temporal processing capabilities.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œé€šè¿‡ä½“ç°æ—¶é—´å½’çº³åå·®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ˆ[Elman 1990](ch058.xhtml#ref-elman1990finding)ï¼›[Hochreiterå’ŒSchmidhuber
    1997](ch058.xhtml#ref-hochreiter1997long)ï¼‰ï¼šå®ƒä»¬å‡è®¾åºåˆ—ä¾èµ–æ€§ï¼Œå³ä¿¡æ¯çš„é¡ºåºå¾ˆé‡è¦ï¼Œè¿‡å»å½±å“ç°åœ¨ã€‚è¿™ä¸ªæ¶æ„å‡è®¾æŒ‡å¯¼äº†å°†è®°å¿†ä½œä¸ºè®¡ç®—æ¨¡å‹ç»„ä»¶çš„å¼•å…¥ã€‚RNNsä¸æ˜¯ç‹¬ç«‹å¤„ç†è¾“å…¥ï¼Œè€Œæ˜¯ç»´æŒä¸€ä¸ªå†…éƒ¨çŠ¶æ€ï¼Œå°†ä¿¡æ¯ä»å…ˆå‰çš„æ—¶é—´æ­¥ä¼ æ’­å‡ºå»ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿæ ¹æ®å†å²ä¸Šä¸‹æ–‡æ¡ä»¶åŒ–å…¶å½“å‰è¾“å‡ºã€‚è¿™ç§æ¶æ„ä½“ç°äº†ä¸€ç§å¦ä¸€ç§æƒè¡¡ï¼šè™½ç„¶CNNç‰ºç‰²äº†ç†è®ºä¸Šçš„é€šç”¨æ€§ä»¥æ¢å–ç©ºé—´æ•ˆç‡ï¼Œä½†RNNå¼•å…¥äº†è®¡ç®—ä¾èµ–æ€§ï¼Œè¿™æŒ‘æˆ˜äº†å¹¶è¡Œæ‰§è¡Œï¼Œä»¥æ¢å–æ—¶åºå¤„ç†èƒ½åŠ›ã€‚
- en: '***Recurrent Neural Networks (RNNs)*** are sequential neural architectures
    that maintain *internal memory state* across time steps through *recurrent connections*,
    enabling *variable-length sequence processing* at the cost of *sequential computation*
    that prevents parallelization.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¾ªç¯ç¥ç»ç½‘ç»œ (RNNs)*** æ˜¯ä¸€ç§åºåˆ—ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé€šè¿‡**å¾ªç¯è¿æ¥**åœ¨æ—¶é—´æ­¥é•¿ä¸­ç»´æŒ*å†…éƒ¨è®°å¿†çŠ¶æ€*ï¼Œä»¥*åºåˆ—è®¡ç®—*ä¸ºä»£ä»·å®ç°*å¯å˜é•¿åº¦åºåˆ—å¤„ç†*ã€‚'
- en: '**Coverage Note**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦†ç›–è¯´æ˜**'
- en: This section covers of RNNs, emphasizing their core contributions to sequential
    processing and the architectural principles that influenced modern attention mechanisms.
    While RNNs introduced critical conceptsâ€”memory states, temporal dependencies,
    and sequential computationâ€”contemporary practice increasingly favors attention-based
    architectures for sequence modeling. We focus on foundational principles rather
    than extensive implementation variants, dedicating significant depth to the attention
    mechanisms and Transformers ([SectionÂ 4.5](ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d))
    that have largely superseded RNNs in production systems while building directly
    on the insights gained from recurrent architectures.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æ¶µç›–äº†RNNsï¼Œå¼ºè°ƒäº†å®ƒä»¬å¯¹åºåˆ—å¤„ç†çš„æ ¸å¿ƒè´¡çŒ®ä»¥åŠå½±å“ç°ä»£æ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„åŸåˆ™ã€‚è™½ç„¶RNNså¼•å…¥äº†å…³é”®æ¦‚å¿µâ€”â€”è®°å¿†çŠ¶æ€ã€æ—¶åºä¾èµ–å’Œåºåˆ—è®¡ç®—â€”â€”ä½†å½“ä»£å®è·µè¶Šæ¥è¶Šå€¾å‘äºåŸºäºæ³¨æ„åŠ›çš„æ¶æ„è¿›è¡Œåºåˆ—å»ºæ¨¡ã€‚æˆ‘ä»¬ä¸“æ³¨äºåŸºç¡€åŸåˆ™è€Œä¸æ˜¯å¹¿æ³›çš„å®ç°å˜ä½“ï¼Œå¯¹æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformerï¼ˆ[ç¬¬4.5èŠ‚](ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d)ï¼‰è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œè¿™äº›æœºåˆ¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå·²ç»å–ä»£äº†RNNsåœ¨ç”Ÿäº§ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ŒåŒæ—¶ç›´æ¥åŸºäºå¾ªç¯æ¶æ„çš„è§è§£ã€‚
- en: Pattern Processing Needs
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å¼å¤„ç†éœ€æ±‚
- en: Sequential pattern processing addresses scenarios where current input interpretation
    depends on preceding information. In natural language processing, word meaning
    often depends heavily on previous words in the sentence. Context determines interpretation,
    as evidenced by the varying meanings of words based on surrounding terms. Similarly,
    in speech recognition, phoneme interpretation depends on surrounding sounds, while
    financial forecasting requires understanding historical data patterns.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åºåˆ—æ¨¡å¼å¤„ç†è§£å†³å½“å‰è¾“å…¥è§£é‡Šä¾èµ–äºå…ˆå‰ä¿¡æ¯çš„æƒ…å†µã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œè¯ä¹‰å¾€å¾€ä¸¥é‡ä¾èµ–äºå¥å­ä¸­çš„å…ˆå‰å•è¯ã€‚ä¸Šä¸‹æ–‡å†³å®šäº†è§£é‡Šï¼Œæ­£å¦‚æ ¹æ®å‘¨å›´æœ¯è¯­çš„ä¸åŒï¼Œå•è¯çš„å¤šç§å«ä¹‰æ‰€è¯æ˜çš„é‚£æ ·ã€‚åŒæ ·ï¼Œåœ¨è¯­éŸ³è¯†åˆ«ä¸­ï¼ŒéŸ³ç´ è§£é‡Šä¾èµ–äºå‘¨å›´çš„å£°éŸ³ï¼Œè€Œé‡‘èé¢„æµ‹éœ€è¦ç†è§£å†å²æ•°æ®æ¨¡å¼ã€‚
- en: The challenge in sequential processing lies in maintaining and updating relevant
    context over time. Human text comprehension does not restart with each word; rather,
    a running understanding evolves as new information is processed. Similarly, time-series
    data processing encounters patterns spanning different timescales, from immediate
    dependencies to long-term trends. This necessitates an architecture capable of
    both maintaining state over time and updating it based on new inputs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åºåˆ—å¤„ç†ä¸­çš„æŒ‘æˆ˜åœ¨äºéšç€æ—¶é—´çš„æ¨ç§»ç»´æŠ¤å’Œæ›´æ–°ç›¸å…³ä¸Šä¸‹æ–‡ã€‚äººç±»çš„æ–‡æœ¬ç†è§£å¹¶ä¸æ˜¯ä»æ¯ä¸ªå•è¯å¼€å§‹é‡æ–°å¯åŠ¨ï¼›ç›¸åï¼Œéšç€æ–°ä¿¡æ¯çš„å¤„ç†ï¼Œä¸€ä¸ªæŒç»­çš„ç†è§£åœ¨ä¸æ–­å‘å±•ã€‚åŒæ ·ï¼Œæ—¶é—´åºåˆ—æ•°æ®å¤„ç†ä¼šé‡åˆ°è·¨è¶Šä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å¼ï¼Œä»å³æ—¶ä¾èµ–å…³ç³»åˆ°é•¿æœŸè¶‹åŠ¿ã€‚è¿™éœ€è¦ä¸€ç§èƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»ä¿æŒçŠ¶æ€å¹¶æ ¹æ®æ–°è¾“å…¥æ›´æ–°çŠ¶æ€çš„æ¶æ„ã€‚
- en: 'These requirements translate into specific architectural demands: the system
    must maintain internal state to capture temporal context, update this state based
    on new inputs, and learn which historical information is relevant for current
    predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential
    processing must accommodate variable-length sequences while maintaining computational
    efficiency. These requirements culminate in the recurrent neural network (RNN)
    architecture.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›éœ€æ±‚è½¬åŒ–ä¸ºç‰¹å®šçš„æ¶æ„è¦æ±‚ï¼šç³»ç»Ÿå¿…é¡»ç»´æŠ¤å†…éƒ¨çŠ¶æ€ä»¥æ•æ‰æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®æ–°è¾“å…¥æ›´æ–°æ­¤çŠ¶æ€ï¼Œå¹¶å­¦ä¹ å“ªäº›å†å²ä¿¡æ¯å¯¹å½“å‰é¢„æµ‹æ˜¯ç›¸å…³çš„ã€‚ä¸å¤„ç†å›ºå®šå¤§å°è¾“å…¥çš„MLPså’ŒCNNsä¸åŒï¼Œåºåˆ—å¤„ç†å¿…é¡»é€‚åº”å¯å˜é•¿åº¦çš„åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¿™äº›éœ€æ±‚æœ€ç»ˆå¯¼è‡´äº†å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¶æ„çš„å‡ºç°ã€‚
- en: Algorithmic Structure
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•ç»“æ„
- en: RNNs address sequential processing through recurrent connections, distinguishing
    them from MLPs and CNNs. Rather than merely mapping inputs to outputs, RNNs maintain
    an internal state updated at each time step, creating a memory mechanism that
    propagates information forward in time. This temporal dependency modeling capability
    was first explored by Elman ([1990](ch058.xhtml#ref-elman1990finding)), who demonstrated
    RNN capacity to identify structure in time-dependent data. Basic RNNs suffer from
    the vanishing gradient problem[17](#fn17), constraining their ability to learn
    long-term dependencies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: RNNsé€šè¿‡å¾ªç¯è¿æ¥å¤„ç†åºåˆ—å¤„ç†ï¼Œä½¿å…¶ä¸MLPså’ŒCNNsåŒºåˆ†å¼€æ¥ã€‚RNNsä¸ä»…ä»…æ˜¯å°†è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºï¼Œè€Œæ˜¯åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ›´æ–°å†…éƒ¨çŠ¶æ€ï¼Œåˆ›å»ºä¸€ç§è®°å¿†æœºåˆ¶ï¼Œå°†ä¿¡æ¯å‘å‰ä¼ æ’­ã€‚è¿™ç§æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡èƒ½åŠ›æœ€åˆç”±Elman
    ([1990](ch058.xhtml#ref-elman1990finding)) æ¢ç´¢ï¼Œä»–å±•ç¤ºäº†RNNè¯†åˆ«æ—¶é—´ä¾èµ–æ•°æ®ç»“æ„çš„èƒ½åŠ›ã€‚åŸºæœ¬çš„RNNså—åˆ°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜[17](#fn17)çš„å›°æ‰°ï¼Œé™åˆ¶äº†å®ƒä»¬å­¦ä¹ é•¿æœŸä¾èµ–çš„èƒ½åŠ›ã€‚
- en: 'The core operation in a basic RNN can be expressed mathematically as: <semantics><mrow><msub><mi>ğ¡</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ğ–</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>ğ¡</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ğ–</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>ğ±</mi><mi>t</mi></msub><mo>+</mo><msub><mi>ğ›</mi><mi>h</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{h}_t
    = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)</annotation></semantics>
    where <semantics><msub><mi>ğ¡</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{h}_t</annotation></semantics>
    denotes the hidden state at time <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>,
    <semantics><msub><mi>ğ±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics>
    denotes the input at time <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>,
    <semantics><msub><mi>ğ–</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">\mathbf{W}_{hh}</annotation></semantics> contains
    the recurrent weights, and <semantics><msub><mi>ğ–</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">\mathbf{W}_{xh}</annotation></semantics> contains
    the input weights, as illustrated in the unfolded network structure in [FigureÂ 4.4](ch010.xhtml#fig-rnn).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬RNNçš„æ ¸å¿ƒæ“ä½œå¯ä»¥ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼š<semantics><mrow><msub><mi>ğ¡</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ğ–</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>ğ¡</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ğ–</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>ğ±</mi><mi>t</mi></msub><mo>+</mo><msub><mi>ğ›</mi><mi>h</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{h}_t
    = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)</annotation></semantics>
    å…¶ä¸­ <semantics><msub><mi>ğ¡</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{h}_t</annotation></semantics>
    è¡¨ç¤ºæ—¶é—´ <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>
    çš„éšè—çŠ¶æ€ï¼Œ<semantics><msub><mi>ğ±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics>
    è¡¨ç¤ºæ—¶é—´ <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>
    çš„è¾“å…¥ï¼Œ<semantics><msub><mi>ğ–</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">\mathbf{W}_{hh}</annotation></semantics> åŒ…å«å¾ªç¯æƒé‡ï¼Œè€Œ
    <semantics><msub><mi>ğ–</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">\mathbf{W}_{xh}</annotation></semantics> åŒ…å«è¾“å…¥æƒé‡ï¼Œå¦‚å›¾[å›¾4.4](ch010.xhtml#fig-rnn)ä¸­çš„å±•å¼€ç½‘ç»œç»“æ„æ‰€ç¤ºã€‚
- en: In word sequence processing, each word may be represented as a 100-dimensional
    vector (<semantics><msub><mi>ğ±</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics>),
    with a hidden state of 128 dimensions (<semantics><msub><mi>ğ¡</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\mathbf{h}_t</annotation></semantics>). At each time
    step, the network combines the current input with its previous state to update
    its sequential understanding, establishing a memory mechanism capable of capturing
    patterns across time steps.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯åºåˆ—å¤„ç†ä¸­ï¼Œæ¯ä¸ªè¯å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ª100ç»´å‘é‡ï¼ˆ<semantics><msub><mi>ğ±</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\mathbf{x}_t</annotation></semantics>ï¼‰ï¼Œå…·æœ‰128ç»´çš„éšè—çŠ¶æ€ï¼ˆ<semantics><msub><mi>ğ¡</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\mathbf{h}_t</annotation></semantics>ï¼‰ã€‚åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç½‘ç»œå°†å½“å‰è¾“å…¥ä¸å…¶å…ˆå‰çŠ¶æ€ç»“åˆä»¥æ›´æ–°å…¶åºåˆ—ç†è§£ï¼Œå»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿæ•æ‰æ—¶é—´æ­¥ä¹‹é—´æ¨¡å¼çš„è®°å¿†æœºåˆ¶ã€‚
- en: This recurrent structure fulfills sequential processing requirements through
    connections that maintain internal state and propagate information forward in
    time. Rather than processing all inputs independently, RNNs process sequential
    data by iteratively updating a hidden state based on the current input and the
    previous hidden state, as depicted in [FigureÂ 4.4](ch010.xhtml#fig-rnn). This
    architecture suits tasks including language modeling, speech recognition, and
    time-series forecasting.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å¾ªç¯ç»“æ„é€šè¿‡ä¿æŒå†…éƒ¨çŠ¶æ€å¹¶å‘å‰ä¼ æ’­ä¿¡æ¯æ¥æ»¡è¶³åºåˆ—å¤„ç†éœ€æ±‚ã€‚RNNsä¸æ˜¯ç‹¬ç«‹å¤„ç†æ‰€æœ‰è¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£æ›´æ–°ä¸€ä¸ªåŸºäºå½“å‰è¾“å…¥å’Œå…ˆå‰éšè—çŠ¶æ€çš„éšè—çŠ¶æ€æ¥å¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚å›¾[å›¾4.4](ch010.xhtml#fig-rnn)æ‰€ç¤ºã€‚è¿™ç§æ¶æ„é€‚ç”¨äºåŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€è¯­éŸ³è¯†åˆ«å’Œæ—¶é—´åºåˆ—é¢„æµ‹åœ¨å†…çš„ä»»åŠ¡ã€‚
- en: 'RNNs implement a recursive algorithm where each time stepâ€™s function call depends
    on the result of the previous call. Analogous to recursive functions that maintain
    state through the call stack, RNNs maintain state through their hidden vectors.
    The mathematical formula <semantics><mrow><msub><mi>ğ¡</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ğ¡</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ğ±</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}_t
    = f(\mathbf{h}_{t-1}, \mathbf{x}_t)</annotation></semantics> directly parallels
    recursive function definitions where `f(n) = g(f(n-1), input(n))`. This correspondence
    explains RNN capacity to handle variable-length sequences: just as recursive algorithms
    process lists of arbitrary length by applying the same function recursively, RNNs
    process sequences of any length by applying the same recurrent computation.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: RNNå®ç°äº†ä¸€ç§é€’å½’ç®—æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„å‡½æ•°è°ƒç”¨ä¾èµ–äºå‰ä¸€ä¸ªè°ƒç”¨çš„ç»“æœã€‚ç±»ä¼¼äºé€šè¿‡è°ƒç”¨æ ˆç»´æŠ¤çŠ¶æ€çš„é€’å½’å‡½æ•°ï¼ŒRNNé€šè¿‡å…¶éšè—å‘é‡æ¥ç»´æŠ¤çŠ¶æ€ã€‚æ•°å­¦å…¬å¼<semantics><mrow><msub><mi>ğ¡</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ğ¡</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ğ±</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{h}_t
    = f(\mathbf{h}_{t-1}, \mathbf{x}_t)</annotation></semantics>ç›´æ¥å¯¹åº”äºé€’å½’å‡½æ•°å®šä¹‰ï¼Œå…¶ä¸­`f(n)
    = g(f(n-1), input(n))`ã€‚è¿™ç§å¯¹åº”å…³ç³»è§£é‡Šäº†RNNå¤„ç†å¯å˜é•¿åº¦åºåˆ—çš„èƒ½åŠ›ï¼šæ­£å¦‚é€’å½’ç®—æ³•é€šè¿‡é€’å½’åº”ç”¨ç›¸åŒçš„å‡½æ•°æ¥å¤„ç†ä»»æ„é•¿åº¦çš„åˆ—è¡¨ä¸€æ ·ï¼ŒRNNé€šè¿‡åº”ç”¨ç›¸åŒçš„å¾ªç¯è®¡ç®—æ¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ã€‚
- en: Efficiency and Optimization
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•ˆç‡å’Œä¼˜åŒ–
- en: Sequential processing creates computational bottlenecks but enables unique efficiency
    characteristics for memory usage. RNNs achieve constant memory overhead for hidden
    state storage regardless of sequence length, making them extremely memory-efficient
    for long sequences. While Transformers require O(nÂ²) memory for sequence length
    n, RNNs maintain fixed memory usage, enabling processing of sequences thousands
    of steps long on modest hardware.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºåºå¤„ç†ä¼šåˆ›å»ºè®¡ç®—ç“¶é¢ˆï¼Œä½†ä¸ºå†…å­˜ä½¿ç”¨æä¾›äº†ç‹¬ç‰¹çš„æ•ˆç‡ç‰¹æ€§ã€‚RNNåœ¨éšè—çŠ¶æ€å­˜å‚¨æ–¹é¢å®ç°æ’å®šçš„å†…å­˜å¼€é”€ï¼Œæ— è®ºåºåˆ—é•¿åº¦å¦‚ä½•ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å¤„ç†é•¿åºåˆ—æ—¶æä¸ºå†…å­˜é«˜æ•ˆã€‚è€ŒTransformerséœ€è¦O(nÂ²)çš„å†…å­˜æ¥å¤„ç†é•¿åº¦ä¸ºnçš„åºåˆ—ï¼ŒRNNåˆ™ä¿æŒå›ºå®šçš„å†…å­˜ä½¿ç”¨ï¼Œä½¿å¾—åœ¨é€‚åº¦ç¡¬ä»¶ä¸Šå¤„ç†æ•°åƒæ­¥é•¿çš„åºåˆ—æˆä¸ºå¯èƒ½ã€‚
- en: Structured pruning of hidden-to-hidden connections can achieve 10x speedup while
    maintaining sequence modeling capability. The recurrent weight matrix <semantics><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">W_{hh}</annotation></semantics> typically dominates
    parameter count for large hidden states, but magnitude-based pruning reveals that
    70-80% of these connections contribute minimally to temporal dependencies. Block-structured
    pruning maintains computational efficiency while enabling significant model compression.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—åˆ°éšè—è¿æ¥çš„ç»“æ„åŒ–å‰ªæå¯ä»¥å®ç°10å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒåºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚å¾ªç¯æƒé‡çŸ©é˜µ<semantics><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><annotation
    encoding="application/x-tex">W_{hh}</annotation></semantics>é€šå¸¸å å¤§éšè—çŠ¶æ€å‚æ•°æ•°é‡çš„ä¸»å¯¼åœ°ä½ï¼Œä½†åŸºäºå¹…åº¦çš„å‰ªæè¡¨æ˜ï¼Œå…¶ä¸­70-80%çš„è¿æ¥å¯¹æ—¶é—´ä¾èµ–æ€§çš„è´¡çŒ®å¾ˆå°ã€‚å—ç»“æ„åŒ–å‰ªæåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ¨¡å‹å‹ç¼©ã€‚
- en: Sequential operations accumulate quantization errors, requiring careful quantization
    point placement and gradient scaling for stable low-precision training. Unlike
    feedforward networks where quantization errors remain localized, RNN errors propagate
    through time, making INT8 quantization more challenging. Per-timestep quantization
    schemes and careful handling of hidden state precision are required for maintaining
    accuracy in quantized RNN deployments.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºåºæ“ä½œä¼šç´¯ç§¯é‡åŒ–è¯¯å·®ï¼Œéœ€è¦ä»”ç»†æ”¾ç½®é‡åŒ–ç‚¹å’Œè¿›è¡Œæ¢¯åº¦ç¼©æ”¾ä»¥å®ç°ç¨³å®šçš„ä½ç²¾åº¦è®­ç»ƒã€‚ä¸é‡åŒ–è¯¯å·®ä¿æŒå±€éƒ¨åŒ–çš„å‰é¦ˆç½‘ç»œä¸åŒï¼ŒRNNè¯¯å·®ä¼šéšæ—¶é—´ä¼ æ’­ï¼Œä½¿å¾—INT8é‡åŒ–æ›´å…·æŒ‘æˆ˜æ€§ã€‚éœ€è¦é‡‡ç”¨æ¯æ—¶é—´æ­¥é‡åŒ–æ–¹æ¡ˆå’Œä»”ç»†å¤„ç†éšè—çŠ¶æ€ç²¾åº¦ï¼Œä»¥åœ¨é‡åŒ–RNNéƒ¨ç½²ä¸­ä¿æŒå‡†ç¡®æ€§ã€‚
- en: '![](../media/file57.svg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file57.svg)'
- en: 'FigureÂ 4.4: **Recurrent Neural Network Unfolding**: Rnns process sequential
    data by maintaining a hidden state that incorporates information from previous
    time steps through this diagram. the unfolded structure explicitly represents
    the temporal dependencies modeled by the recurrent weights, enabling the network
    to learn patterns across variable-length sequences.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.4ï¼š**å¾ªç¯ç¥ç»ç½‘ç»œå±•å¼€**ï¼šRNNé€šè¿‡ç»´æŠ¤ä¸€ä¸ªåŒ…å«å…ˆå‰æ—¶é—´æ­¥ä¿¡æ¯çš„çŠ¶æ€æ¥å¤„ç†é¡ºåºæ•°æ®ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚å±•å¼€çš„ç»“æ„æ˜ç¡®è¡¨ç¤ºäº†ç”±å¾ªç¯æƒé‡å»ºæ¨¡çš„æ—¶é—´ä¾èµ–æ€§ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¯å˜é•¿åº¦åºåˆ—ä¸­çš„æ¨¡å¼ã€‚
- en: Computational Mapping
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—æ˜ å°„
- en: RNN sequential processing creates computational patterns different from both
    MLPs and CNNs, extending the architectural diversity discussed in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de).
    This implementation approach shows temporal dependencies translating into specific
    computational requirements.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: RNNé¡ºåºå¤„ç†åˆ›å»ºçš„è®¡ç®—æ¨¡å¼ä¸MLPså’ŒCNNséƒ½ä¸åŒï¼Œæ‰©å±•äº†[ç¬¬4.1èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)ä¸­è®¨è®ºçš„æ¶æ„å¤šæ ·æ€§ã€‚è¿™ç§å®ç°æ–¹æ³•è¡¨æ˜æ—¶é—´ä¾èµ–æ€§è½¬åŒ–ä¸ºç‰¹å®šçš„è®¡ç®—éœ€æ±‚ã€‚
- en: 'As shown in [ListingÂ 4.5](ch010.xhtml#lst-rnn_layer_step), the `rnn_layer_step`
    function shows the operation using high-level matrix operations found in deep
    learning frameworks. It handles a single time step, taking the current input `x_t`
    and previous hidden state `h_prev`, along with two weight matrices: `W_hh` for
    hidden-to-hidden connections and `W_xh` for input-to-hidden connections. Through
    matrix multiplication operations (`matmul`), it merges the previous state and
    current input to generate the next hidden state.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[åˆ—è¡¨4.5](ch010.xhtml#lst-rnn_layer_step)æ‰€ç¤ºï¼Œ`rnn_layer_step`å‡½æ•°å±•ç¤ºäº†åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æ‰¾åˆ°çš„é«˜çº§çŸ©é˜µè¿ç®—çš„ä½¿ç”¨ã€‚å®ƒå¤„ç†å•ä¸ªæ—¶é—´æ­¥ï¼Œæ¥å—å½“å‰è¾“å…¥`x_t`å’Œå‰ä¸€ä¸ªéšè—çŠ¶æ€`h_prev`ï¼Œä»¥åŠä¸¤ä¸ªæƒé‡çŸ©é˜µï¼š`W_hh`ç”¨äºéšè—åˆ°éšè—è¿æ¥ï¼Œ`W_xh`ç”¨äºè¾“å…¥åˆ°éšè—è¿æ¥ã€‚é€šè¿‡çŸ©é˜µä¹˜æ³•è¿ç®—(`matmul`)ï¼Œå®ƒå°†å…ˆå‰çŠ¶æ€å’Œå½“å‰è¾“å…¥åˆå¹¶ä»¥ç”Ÿæˆä¸‹ä¸€ä¸ªéšè—çŠ¶æ€ã€‚
- en: 'ListingÂ 4.5: **RNN Layer Step**: Neural networks process sequential data through
    transformations that integrate current inputs and past states.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.5ï¼š**RNNå±‚æ­¥éª¤**ï¼šç¥ç»ç½‘ç»œé€šè¿‡æ•´åˆå½“å‰è¾“å…¥å’Œè¿‡å»çŠ¶æ€çš„è½¬æ¢æ¥å¤„ç†é¡ºåºæ•°æ®ã€‚
- en: '[PRE4]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Understanding RNN system implications requires examining how the elegant mathematical
    abstraction translates into hardware execution patterns. The simple recurrence
    relation `h_t = tanh(W_hh h_{t-1} + W_xh x_t + b)` conceals a computational structure
    that creates unique challenges: sequential dependencies that prevent parallelization,
    memory access patterns that differ from feedforward networks, and state management
    requirements that affect system design.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£RNNç³»ç»Ÿçš„å½±å“éœ€è¦æ£€æŸ¥ä¼˜é›…çš„æ•°å­¦æŠ½è±¡å¦‚ä½•è½¬åŒ–ä¸ºç¡¬ä»¶æ‰§è¡Œæ¨¡å¼ã€‚ç®€å•çš„é€’å½’å…³ç³»`h_t = tanh(W_hh h_{t-1} + W_xh x_t
    + b)`éšè—äº†ä¸€ä¸ªåˆ›å»ºç‹¬ç‰¹æŒ‘æˆ˜çš„è®¡ç®—ç»“æ„ï¼šé˜²æ­¢å¹¶è¡ŒåŒ–çš„é¡ºåºä¾èµ–æ€§ã€ä¸å‰é¦ˆç½‘ç»œä¸åŒçš„å†…å­˜è®¿é—®æ¨¡å¼ï¼Œä»¥åŠå½±å“ç³»ç»Ÿè®¾è®¡çš„çŠ¶æ€ç®¡ç†éœ€æ±‚ã€‚
- en: The detailed implementation ([ListingÂ 4.6](ch010.xhtml#lst-rnn_layer_compute))
    reveals the computational reality beneath the mathematical abstraction. The nested
    loop structure exposes how sequential processing creates both limitations and
    opportunities in system optimization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¦ç»†å®ç°([åˆ—è¡¨4.6](ch010.xhtml#lst-rnn_layer_compute))æ­ç¤ºäº†æ•°å­¦æŠ½è±¡èƒŒåçš„è®¡ç®—ç°å®ã€‚åµŒå¥—å¾ªç¯ç»“æ„æ­ç¤ºäº†é¡ºåºå¤„ç†å¦‚ä½•åœ¨ç³»ç»Ÿä¼˜åŒ–ä¸­åˆ›é€ é™åˆ¶å’Œæœºä¼šã€‚
- en: 'ListingÂ 4.6: **Recurrent Layer Computation**: Computes the hidden state at
    each time step through sequential transformations involving previous states and
    current inputs.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.6ï¼š**å¾ªç¯å±‚è®¡ç®—**ï¼šé€šè¿‡æ¶‰åŠå…ˆå‰çŠ¶æ€å’Œå½“å‰è¾“å…¥çš„é¡ºåºè½¬æ¢æ¥è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The nested loops in `rnn_layer_compute` expose the core computational pattern
    of RNNs (see [ListingÂ 4.6](ch010.xhtml#lst-rnn_layer_compute)). Loop 1 processes
    each sequence in the batch independently, allowing for batch-level parallelism.
    Within each batch item, Loop 2 computes how the previous hidden state influences
    the next state through the recurrent weights `W_hh`. Loop 3 then incorporates
    new information from the current input through the input weights `W_xh`. Finally,
    Loop 4 adds biases and applies the activation function to produce the new hidden
    state.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_layer_compute`ä¸­çš„åµŒå¥—å¾ªç¯æ­ç¤ºäº†RNNsçš„æ ¸å¿ƒè®¡ç®—æ¨¡å¼ï¼ˆå‚è§[åˆ—è¡¨4.6](ch010.xhtml#lst-rnn_layer_compute)ï¼‰ã€‚å¾ªç¯1ç‹¬ç«‹å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªåºåˆ—ï¼Œä»è€Œå…è®¸æ‰¹å¤„ç†çº§åˆ«çš„å¹¶è¡Œæ€§ã€‚åœ¨æ¯ä¸ªæ‰¹æ¬¡é¡¹ä¸­ï¼Œå¾ªç¯2é€šè¿‡å¾ªç¯æƒé‡`W_hh`è®¡ç®—å‰ä¸€ä¸ªéšè—çŠ¶æ€å¦‚ä½•å½±å“ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ç„¶åï¼Œå¾ªç¯3é€šè¿‡è¾“å…¥æƒé‡`W_xh`å°†å½“å‰è¾“å…¥çš„æ–°ä¿¡æ¯çº³å…¥ã€‚æœ€åï¼Œå¾ªç¯4æ·»åŠ åå·®å¹¶åº”ç”¨æ¿€æ´»å‡½æ•°ä»¥äº§ç”Ÿæ–°çš„éšè—çŠ¶æ€ã€‚'
- en: 'For a sequence processing task with input dimension 100 and hidden state dimension
    128, each time step requires two matrix multiplications: one <semantics><mrow><mn>128</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times 128</annotation></semantics> for the recurrent
    connection and one <semantics><mrow><mn>100</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">100\times 128</annotation></semantics> for the input
    projection. While individual time steps can process in parallel across batch elements,
    the time steps themselves must process sequentially. This creates a unique computational
    pattern that systems must handle.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾“å…¥ç»´åº¦ä¸º100å’Œéšè—çŠ¶æ€ç»´åº¦ä¸º128çš„åºåˆ—å¤„ç†ä»»åŠ¡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥éœ€è¦ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•ï¼šä¸€æ¬¡æ˜¯ç”¨äºå¾ªç¯è¿æ¥çš„<semantics><mrow><mn>128</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times 128</annotation></semantics>ï¼Œå¦ä¸€æ¬¡æ˜¯ç”¨äºè¾“å…¥æŠ•å½±çš„<semantics><mrow><mn>100</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">100\times 128</annotation></semantics>ã€‚è™½ç„¶å•ä¸ªæ—¶é—´æ­¥å¯ä»¥åœ¨æ‰¹å¤„ç†å…ƒç´ ä¹‹é—´å¹¶è¡Œå¤„ç†ï¼Œä½†æ—¶é—´æ­¥æœ¬èº«å¿…é¡»æŒ‰é¡ºåºå¤„ç†ã€‚è¿™åˆ›å»ºäº†ä¸€ç§ç‹¬ç‰¹çš„è®¡ç®—æ¨¡å¼ï¼Œç³»ç»Ÿå¿…é¡»å¤„ç†ã€‚
- en: System Implications
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: Following the analytical framework established for MLPs, RNNs exhibit distinctive
    patterns in memory requirements, computation needs, and data movement that differ
    significantly from both dense and spatial processing architectures.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸ºMLPså»ºç«‹çš„è§£ææ¡†æ¶ä¹‹åï¼ŒRNNåœ¨å†…å­˜éœ€æ±‚ã€è®¡ç®—éœ€æ±‚å’Œæ•°æ®å¤„ç†æ–¹é¢è¡¨ç°å‡ºç‹¬ç‰¹çš„æ¨¡å¼ï¼Œä¸å¯†é›†å’Œç©ºé—´å¤„ç†æ¶æ„æœ‰æ˜¾è‘—å·®å¼‚ã€‚
- en: Memory Requirements
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å†…å­˜éœ€æ±‚
- en: RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden)
    along with the hidden state. For the example with input dimension 100 and hidden
    state dimension 128, this requires storing 12,800 weights for input projection
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>100</mn><mo>Ã—</mo><mn>128</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times
    128)</annotation></semantics> and 16,384 weights for recurrent connections <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>128</mn><mo>Ã—</mo><mn>128</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(128\times
    128)</annotation></semantics>. Unlike CNNs where weights are reused across spatial
    positions, RNN weights are reused across time steps. The system must maintain
    the hidden state, which constitutes a key factor in memory usage and access patterns.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: RNNéœ€è¦å­˜å‚¨ä¸¤ç»„æƒé‡ï¼ˆè¾“å…¥åˆ°éšè—å’Œéšè—åˆ°éšè—ï¼‰ä»¥åŠéšè—çŠ¶æ€ã€‚å¯¹äºè¾“å…¥ç»´åº¦ä¸º100å’Œéšè—çŠ¶æ€ç»´åº¦ä¸º128çš„ä¾‹å­ï¼Œè¿™éœ€è¦å­˜å‚¨12,800ä¸ªç”¨äºè¾“å…¥æŠ•å½±çš„æƒé‡<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>100</mn><mo>Ã—</mo><mn>128</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times
    128)</annotation></semantics>å’Œ16,384ä¸ªç”¨äºå¾ªç¯è¿æ¥çš„æƒé‡<semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>128</mn><mo>Ã—</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(128\times 128)</annotation></semantics>ã€‚ä¸CNNä¸­æƒé‡åœ¨ç©ºé—´ä½ç½®é—´é‡ç”¨ä¸åŒï¼ŒRNNçš„æƒé‡åœ¨æ—¶é—´æ­¥é—´é‡ç”¨ã€‚ç³»ç»Ÿå¿…é¡»ç»´æŠ¤éšè—çŠ¶æ€ï¼Œè¿™æ˜¯å†…å­˜ä½¿ç”¨å’Œè®¿é—®æ¨¡å¼çš„å…³é”®å› ç´ ã€‚
- en: These memory access patterns create a different profile from MLPs and CNNs.
    Processors optimize sequential patterns by maintaining weight matrices in cache
    while streaming through temporal elements. Frameworks optimize temporal processing
    by batching sequences and managing hidden state storage between time steps. CPUs
    and GPUs approach this through different strategies; CPUs leverage their cache
    hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures
    designed for maintaining state across sequential operations. The specialized hardware
    optimizations for sequential processing, including memory banking and pipeline
    architectures, are detailed in [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å­˜è®¿é—®æ¨¡å¼ä¸MLPså’ŒCNNsä¸åŒã€‚å¤„ç†å™¨é€šè¿‡åœ¨ç¼“å­˜ä¸­ä¿æŒæƒé‡çŸ©é˜µå¹¶æµè¿‡æ—¶é—´å…ƒç´ æ¥ä¼˜åŒ–é¡ºåºæ¨¡å¼ã€‚æ¡†æ¶é€šè¿‡æ‰¹å¤„ç†åºåˆ—å’Œç®¡ç†æ—¶é—´æ­¥ä¹‹é—´çš„éšè—çŠ¶æ€å­˜å‚¨æ¥ä¼˜åŒ–æ—¶é—´å¤„ç†ã€‚CPUå’ŒGPUé€šè¿‡ä¸åŒçš„ç­–ç•¥æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼›CPUåˆ©ç”¨å…¶ç¼“å­˜å±‚æ¬¡ç»“æ„æ¥å®ç°æƒé‡é‡ç”¨ï¼›åŒæ—¶ï¼ŒGPUä½¿ç”¨ä¸“ä¸ºåœ¨é¡ºåºæ“ä½œä¸­ç»´æŠ¤çŠ¶æ€è€Œè®¾è®¡çš„ä¸“ç”¨å†…å­˜æ¶æ„ã€‚å…³äºé¡ºåºå¤„ç†çš„ä¸“ç”¨ç¡¬ä»¶ä¼˜åŒ–ï¼ŒåŒ…æ‹¬å†…å­˜é“¶è¡Œå’Œæµæ°´çº¿æ¶æ„ï¼Œåœ¨[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­è¯¦ç»†è¯´æ˜ã€‚
- en: Computation Needs
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—éœ€æ±‚
- en: 'The core computation in RNNs involves repeatedly applying weight matrices across
    time steps. For each time step, we perform two matrix multiplications: one with
    the input weights and one with the recurrent weights. In our example, processing
    a single time step requires 12,800 multiply-accumulates for the input projection
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>100</mn><mo>Ã—</mo><mn>128</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times
    128)</annotation></semantics> and 16,384 multiply-accumulates for the recurrent
    connection <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>128</mn><mo>Ã—</mo><mn>128</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(128\times
    128)</annotation></semantics>.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: RNNçš„æ ¸å¿ƒè®¡ç®—æ¶‰åŠåœ¨æ—¶é—´æ­¥å†…åå¤åº”ç”¨æƒé‡çŸ©é˜µã€‚å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬æ‰§è¡Œä¸¤æ¬¡çŸ©é˜µä¹˜æ³•ï¼šä¸€æ¬¡æ˜¯ä¸è¾“å…¥æƒé‡ç›¸ä¹˜ï¼Œå¦ä¸€æ¬¡æ˜¯ä¸å¾ªç¯æƒé‡ç›¸ä¹˜ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¤„ç†å•ä¸ªæ—¶é—´æ­¥éœ€è¦12,800æ¬¡ä¹˜åŠ æ“ä½œç”¨äºè¾“å…¥æŠ•å½±<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mn>100</mn><mo>Ã—</mo><mn>128</mn><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(100\times
    128)</annotation></semantics>å’Œ16,384æ¬¡ä¹˜åŠ æ“ä½œç”¨äºå¾ªç¯è¿æ¥<semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>128</mn><mo>Ã—</mo><mn>128</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(128\times 128)</annotation></semantics>ã€‚
- en: 'This computational pattern differs from both MLPs and CNNs in a key way: while
    we can parallelize across batch elements, we cannot parallelize across time steps
    due to the sequential dependency. Each time step must wait for the previous stepâ€™s
    hidden state before it can begin computation. This creates a tension between the
    inherent sequential nature of the algorithm and the desire for parallel execution
    in modern hardware.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¡ç®—æ¨¡å¼åœ¨å…³é”®æ–¹é¢ä¸MLPå’ŒCNNä¸åŒï¼šè™½ç„¶æˆ‘ä»¬å¯ä»¥åœ¨æ‰¹å¤„ç†å…ƒç´ ä¹‹é—´å¹¶è¡ŒåŒ–ï¼Œä½†ç”±äºé¡ºåºä¾èµ–æ€§ï¼Œæˆ‘ä»¬ä¸èƒ½åœ¨æ—¶é—´æ­¥ä¹‹é—´å¹¶è¡ŒåŒ–ã€‚æ¯ä¸ªæ—¶é—´æ­¥å¿…é¡»åœ¨å¼€å§‹è®¡ç®—ä¹‹å‰ç­‰å¾…å‰ä¸€ä¸ªæ­¥éª¤çš„éšè—çŠ¶æ€ã€‚è¿™å°±åœ¨ç®—æ³•çš„å›ºæœ‰é¡ºåºæ€§è´¨å’Œç°ä»£ç¡¬ä»¶ä¸­å¹¶è¡Œæ‰§è¡Œçš„éœ€æ±‚ä¹‹é—´äº§ç”Ÿäº†ç´§å¼ å…³ç³»ã€‚
- en: Processors address sequential constraints through specialized approaches. CPUs
    pipeline operations within time steps while maintaining temporal ordering. GPUs
    batch multiple sequences together to maintain high throughput despite sequential
    dependencies. Software frameworks optimize this further by techniques like sequence
    packing and unrolling computations across multiple time steps when possible, enabling
    more efficient utilization of parallel processing resources while respecting the
    sequential constraints inherent in recurrent architectures.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å™¨é€šè¿‡ä¸“é—¨çš„æ–¹æ³•è§£å†³é¡ºåºçº¦æŸé—®é¢˜ã€‚CPUåœ¨æ—¶é—´æ­¥å†…å¯¹æ“ä½œè¿›è¡Œæµæ°´çº¿å¤„ç†ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´é¡ºåºã€‚GPUå°†å¤šä¸ªåºåˆ—æ‰¹é‡å¤„ç†åœ¨ä¸€èµ·ï¼Œä»¥ç»´æŒé«˜ååé‡ï¼Œå°½ç®¡å­˜åœ¨é¡ºåºä¾èµ–æ€§ã€‚è½¯ä»¶æ¡†æ¶é€šè¿‡åºåˆ—æ‰“åŒ…å’Œè·¨å¤šä¸ªæ—¶é—´æ­¥å±•å¼€è®¡ç®—ç­‰æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œåœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œä½¿å¹¶è¡Œå¤„ç†èµ„æºçš„åˆ©ç”¨æ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶å°Šé‡å¾ªç¯æ¶æ„ä¸­å›ºæœ‰çš„é¡ºåºçº¦æŸã€‚
- en: Data Movement
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨
- en: The sequential processing in RNNs creates a distinctive data movement pattern
    that differs from both MLPs and CNNs. While MLPs need each weight only once per
    forward pass and CNNs reuse weights across spatial positions, RNNs reuse their
    weights across time steps while requiring careful management of the hidden state
    data flow.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: RNNä¸­çš„é¡ºåºå¤„ç†åˆ›å»ºäº†ä¸€ç§ç‹¬ç‰¹çš„æ•°æ®ç§»åŠ¨æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼ä¸MLPå’ŒCNNéƒ½ä¸åŒã€‚è™½ç„¶MLPåœ¨æ¯æ¬¡å‰å‘ä¼ é€’ä¸­åªéœ€è¦æ¯ä¸ªæƒé‡ä¸€æ¬¡ï¼Œè€ŒCNNåœ¨ç©ºé—´ä½ç½®ä¸Šé‡å¤ä½¿ç”¨æƒé‡ï¼Œä½†RNNåœ¨æ—¶é—´æ­¥å†…é‡å¤ä½¿ç”¨å…¶æƒé‡ï¼ŒåŒæ—¶éœ€è¦ä»”ç»†ç®¡ç†éšè—çŠ¶æ€æ•°æ®æµã€‚
- en: 'For our example with a 128-dimensional hidden state, each time step must: load
    the previous hidden state (128 values), access both weight matrices (29,184 total
    weights from both input and recurrent connections), and store the new hidden state
    (128 values). This pattern repeats for every element in the sequence. Unlike CNNs
    where we can predict and prefetch data based on spatial patterns, RNN data movement
    is driven by temporal dependencies.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œå…·æœ‰128ç»´éšè—çŠ¶æ€ï¼Œæ¯ä¸ªæ—¶é—´æ­¥å¿…é¡»ï¼šåŠ è½½å‰ä¸€ä¸ªéšè—çŠ¶æ€ï¼ˆ128ä¸ªå€¼ï¼‰ï¼Œè®¿é—®ä¸¤ä¸ªæƒé‡çŸ©é˜µï¼ˆæ¥è‡ªè¾“å…¥å’Œå¾ªç¯è¿æ¥çš„æ€»æƒé‡ä¸º29,184ï¼‰ï¼Œå¹¶å­˜å‚¨æ–°çš„éšè—çŠ¶æ€ï¼ˆ128ä¸ªå€¼ï¼‰ã€‚è¿™ç§æ¨¡å¼åœ¨åºåˆ—çš„æ¯ä¸ªå…ƒç´ ä¸Šé‡å¤ã€‚ä¸CNNä¸åŒï¼Œæˆ‘ä»¬æ— æ³•æ ¹æ®ç©ºé—´æ¨¡å¼é¢„æµ‹å’Œé¢„å–æ•°æ®ï¼ŒRNNçš„æ•°æ®ç§»åŠ¨æ˜¯ç”±æ—¶é—´ä¾èµ–æ€§é©±åŠ¨çš„ã€‚
- en: Different architectures handle this sequential data movement through specialized
    mechanisms. CPUs maintain weight matrices in cache while streaming through sequence
    elements and managing hidden state updates. GPUs employ memory architectures optimized
    for maintaining state information across sequential operations while processing
    multiple sequences in parallel. Deep learning frameworks orchestrate these movements
    by managing data transfers between time steps and optimizing batch operations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ¶æ„é€šè¿‡ä¸“é—¨çš„æœºåˆ¶å¤„ç†è¿™ç§é¡ºåºæ•°æ®ç§»åŠ¨ã€‚CPUåœ¨é€šè¿‡åºåˆ—å…ƒç´ å¹¶ç®¡ç†éšè—çŠ¶æ€æ›´æ–°æ—¶ï¼Œåœ¨ç¼“å­˜ä¸­ç»´æŠ¤æƒé‡çŸ©é˜µã€‚GPUé‡‡ç”¨ä¼˜åŒ–äº†çš„çŠ¶æ€ä¿¡æ¯ç»´æŠ¤çš„å†…å­˜æ¶æ„ï¼Œåœ¨å¹¶è¡Œå¤„ç†å¤šä¸ªåºåˆ—çš„åŒæ—¶ï¼Œå¤„ç†é¡ºåºæ“ä½œã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶é€šè¿‡ç®¡ç†æ—¶é—´æ­¥ä¹‹é—´çš„æ•°æ®ä¼ è¾“å’Œä¼˜åŒ–æ‰¹é‡æ“ä½œæ¥åè°ƒè¿™äº›ç§»åŠ¨ã€‚
- en: 'While RNNs established concepts for sequential processing, their architectural
    constraints create bottlenecks: sequential dependencies prevent parallelization
    across time steps, fixed-capacity hidden states create information bottlenecks
    for long sequences, and temporal proximity assumptions break down when important
    relationships span distant positions. These limitations motivated the development
    of attention mechanisms, which eliminate sequential processing constraints through
    dynamic, content-dependent connectivity. The following section examines how attention
    mechanisms address each of these RNN limitations while introducing new computational
    challenges. This extensive treatment reflects attention mechanismsâ€™ dominance
    in modern ML systems and their fundamental reimagining of sequential pattern processing.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶RNNä¸ºé¡ºåºå¤„ç†å»ºç«‹äº†æ¦‚å¿µï¼Œä½†å®ƒä»¬çš„æ¶æ„çº¦æŸåˆ›é€ äº†ç“¶é¢ˆï¼šé¡ºåºä¾èµ–æ€§é˜»æ­¢äº†æ—¶é—´æ­¥ä¹‹é—´çš„å¹¶è¡ŒåŒ–ï¼Œå›ºå®šå®¹é‡çš„éšè—çŠ¶æ€ä¸ºé•¿åºåˆ—åˆ›é€ äº†ä¿¡æ¯ç“¶é¢ˆï¼Œå¹¶ä¸”å½“é‡è¦å…³ç³»è·¨è¶Šé¥è¿œä½ç½®æ—¶ï¼Œæ—¶é—´é‚»è¿‘æ€§å‡è®¾å°±ä¼šå´©æºƒã€‚è¿™äº›é™åˆ¶ä¿ƒä½¿æ³¨æ„åŠ›æœºåˆ¶çš„å‘å±•ï¼Œé€šè¿‡åŠ¨æ€ã€å†…å®¹ä¾èµ–çš„è¿æ¥æ¶ˆé™¤é¡ºåºå¤„ç†çº¦æŸã€‚ä¸‹ä¸€èŠ‚å°†æ¢è®¨æ³¨æ„åŠ›æœºåˆ¶å¦‚ä½•è§£å†³è¿™äº›RNNé™åˆ¶ï¼ŒåŒæ—¶å¼•å…¥æ–°çš„è®¡ç®—æŒ‘æˆ˜ã€‚è¿™ç§å¹¿æ³›çš„å¤„ç†åæ˜ äº†æ³¨æ„åŠ›æœºåˆ¶åœ¨ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„ä¸»å¯¼åœ°ä½ä»¥åŠå®ƒä»¬å¯¹é¡ºåºæ¨¡å¼å¤„ç†çš„æ ¹æœ¬æ€§é‡æ–°æ„æƒ³ã€‚
- en: 'Attention Mechanisms: Dynamic Pattern Processing'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ï¼šåŠ¨æ€æ¨¡å¼å¤„ç†
- en: Recurrent Neural Networks successfully introduced memory to handle sequential
    dependencies, but their fixed sequential processing creates limitations. RNNs
    process information in temporal order, making it difficult to capture relationships
    between distant elements and impossible to parallelize computation across sequence
    positions. More critically, RNNs assume that temporal proximity correlates with
    importanceâ€”that nearby words or time steps are more relevant than distant ones.
    This assumption breaks down in many real-world scenarios.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œæˆåŠŸåœ°å¼•å…¥äº†è®°å¿†æ¥å¤„ç†é¡ºåºä¾èµ–æ€§ï¼Œä½†å®ƒä»¬çš„å›ºå®šé¡ºåºå¤„ç†åˆ›é€ äº†é™åˆ¶ã€‚RNNæŒ‰æ—¶é—´é¡ºåºå¤„ç†ä¿¡æ¯ï¼Œè¿™ä½¿å¾—éš¾ä»¥æ•æ‰è¿œè·ç¦»å…ƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”æ— æ³•åœ¨åºåˆ—ä½ç½®ä¹‹é—´å¹¶è¡ŒåŒ–è®¡ç®—ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒRNNå‡è®¾æ—¶é—´é‚»è¿‘æ€§ä¸é‡è¦æ€§ç›¸å…³â€”â€”å³é™„è¿‘çš„å•è¯æˆ–æ—¶é—´æ­¥æ¯”è¿œå¤„çš„æ›´ç›¸å…³ã€‚è¿™ç§å‡è®¾åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­éƒ½ä¼šå´©æºƒã€‚
- en: Consider the sentence "The cat, which was sitting by the window overlooking
    the garden, was sleeping." Here, "cat" and "sleeping" are separated by multiple
    intervening words, yet they form the core subject-predicate relationship. RNN
    architectures would process all the intervening elements sequentially, potentially
    losing this crucial connection in their fixed-capacity hidden state. This limitation
    revealed the need for architectures that could identify and weight relationships
    based on content rather than position.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹å¥å­ï¼šâ€œThe cat, which was sitting by the window overlooking the garden, was
    sleeping.â€ åœ¨è¿™é‡Œï¼Œâ€œcatâ€å’Œâ€œsleepingâ€è¢«å¤šä¸ªé—´éš”è¯åˆ†å¼€ï¼Œä½†å®ƒä»¬æ„æˆäº†æ ¸å¿ƒçš„ä¸»è°“å…³ç³»ã€‚RNNæ¶æ„ä¼šæŒ‰é¡ºåºå¤„ç†æ‰€æœ‰é—´éš”å…ƒç´ ï¼Œå¯èƒ½ä¼šåœ¨å®ƒä»¬çš„å›ºå®šå®¹é‡éšè—çŠ¶æ€ä¸­ä¸¢å¤±è¿™ä¸ªå…³é”®è”ç³»ã€‚è¿™ç§é™åˆ¶æ­ç¤ºäº†éœ€è¦èƒ½å¤Ÿæ ¹æ®å†…å®¹è€Œä¸æ˜¯ä½ç½®è¯†åˆ«å’ŒåŠ æƒå…³ç³»çš„æ¶æ„ã€‚
- en: Attention mechanisms emerged as the solution to this architectural constraint
    ([Bahdanau, Cho, and Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)) by introducing
    dynamic connectivity patterns that adapt based on input content. Rather than processing
    elements in predetermined order with fixed relationships, attention mechanisms
    compute the relevance between all pairs of elements and weight their interactions
    accordingly. This represents a shift from structural constraints to learned, data-dependent
    processing patterns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¼•å…¥æ ¹æ®è¾“å…¥å†…å®¹è‡ªé€‚åº”çš„åŠ¨æ€è¿æ¥æ¨¡å¼ï¼Œæˆä¸ºè§£å†³è¿™ç§æ¶æ„çº¦æŸçš„è§£å†³æ–¹æ¡ˆï¼ˆ[Bahdanau, Cho, and Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)ï¼‰ã€‚ä¸æŒ‰é¢„å®šé¡ºåºå¤„ç†å…·æœ‰å›ºå®šå…³ç³»çš„å…ƒç´ ä¸åŒï¼Œæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ‰€æœ‰å…ƒç´ å¯¹ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶ç›¸åº”åœ°åŠ æƒå®ƒä»¬çš„äº¤äº’ã€‚è¿™ä»£è¡¨äº†ä¸€ç§ä»ç»“æ„çº¦æŸåˆ°å­¦ä¹ ä¾èµ–çš„æ•°æ®å¤„ç†æ¨¡å¼çš„è½¬å˜ã€‚
- en: '***Attention Mechanisms*** are neural components that compute *content-dependent
    relationships* between sequence elements through *query-key-value operations*,
    enabling *selective focus* on relevant information and *long-range dependencies*
    without positional constraints.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›æœºåˆ¶**æ˜¯è®¡ç®—åºåˆ—å…ƒç´ ä¹‹é—´**å†…å®¹ç›¸å…³å…³ç³»**çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼Œé€šè¿‡**æŸ¥è¯¢-é”®-å€¼æ“ä½œ**ï¼Œå®ç°**é€‰æ‹©æ€§å…³æ³¨**ç›¸å…³ä¿¡æ¯å’Œ**é•¿è·ç¦»ä¾èµ–**ï¼Œè€Œä¸å—ä½ç½®çº¦æŸã€‚'
- en: While attention mechanisms were initially used as components within recurrent
    architectures, the Transformer architecture ([Vaswani et al. 2017](ch058.xhtml#ref-vaswani2017attention))
    demonstrated that attention alone could entirely replace sequential processing,
    creating a new architectural paradigm.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ³¨æ„åŠ›æœºåˆ¶æœ€åˆè¢«ç”¨ä½œå¾ªç¯æ¶æ„ä¸­çš„ç»„ä»¶ï¼Œä½†Transformeræ¶æ„([Vaswaniç­‰äºº2017](ch058.xhtml#ref-vaswani2017attention))è¯æ˜äº†ä»…æ³¨æ„åŠ›æœ¬èº«å°±å¯ä»¥å®Œå…¨å–ä»£é¡ºåºå¤„ç†ï¼Œåˆ›é€ äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ã€‚
- en: '***Transformers*** are neural architectures based entirely on *attention mechanisms*,
    using *multi-head self-attention* and *position encodings* to process sequences
    in *parallel* rather than sequentially, enabling efficient training and inference
    at scale.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer**æ˜¯åŸºäº**æ³¨æ„åŠ›æœºåˆ¶**çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨**å¤šå¤´è‡ªæ³¨æ„åŠ›**å’Œ**ä½ç½®ç¼–ç **æ¥å¹¶è¡Œå¤„ç†åºåˆ—ï¼Œè€Œä¸æ˜¯é¡ºåºå¤„ç†ï¼Œä»è€Œå®ç°å¤§è§„æ¨¡çš„æ•ˆç‡å’Œæ¨ç†ã€‚'
- en: Pattern Processing Needs
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨¡å¼å¤„ç†éœ€æ±‚
- en: 'Dynamic pattern processing addresses scenarios where relationships between
    elements are not fixed by architecture but instead emerge from content. Language
    translation exemplifies this challenge: when translating â€œthe bank by the river,â€
    understanding â€œbankâ€ requires attending to â€œriver,â€ but in â€œthe bank approved
    the loan,â€ the important relationship is with â€œapprovedâ€ and â€œloan.â€ Unlike RNNs
    that process information sequentially or CNNs that use fixed spatial patterns,
    an architecture is required that can dynamically determine which relationships
    matter.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€æ¨¡å¼å¤„ç†è§£å†³çš„æ˜¯å…ƒç´ ä¹‹é—´çš„å…³ç³»ä¸æ˜¯ç”±æ¶æ„å›ºå®šï¼Œè€Œæ˜¯ä»å†…å®¹ä¸­äº§ç”Ÿçš„åœºæ™¯ã€‚è¯­è¨€ç¿»è¯‘å°±æ˜¯è¿™ç§æŒ‘æˆ˜çš„ä¾‹è¯ï¼šåœ¨ç¿»è¯‘â€œæ²³è¾¹çš„é“¶è¡Œâ€æ—¶ï¼Œç†è§£â€œé“¶è¡Œâ€éœ€è¦å…³æ³¨â€œæ²³â€ï¼Œä½†åœ¨â€œé“¶è¡Œæ‰¹å‡†äº†è´·æ¬¾â€ä¸­ï¼Œé‡è¦çš„å…³ç³»æ˜¯ä¸â€œæ‰¹å‡†â€å’Œâ€œè´·æ¬¾â€ã€‚ä¸æŒ‰é¡ºåºå¤„ç†ä¿¡æ¯çš„RNNæˆ–ä½¿ç”¨å›ºå®šç©ºé—´æ¨¡å¼çš„CNNä¸åŒï¼Œéœ€è¦ä¸€ä¸ªèƒ½å¤ŸåŠ¨æ€ç¡®å®šå“ªäº›å…³ç³»é‡è¦çš„æ¶æ„ã€‚
- en: Expanding beyond language, this requirement for dynamic processing appears across
    many domains. In protein structure prediction, interactions between amino acids
    depend on their chemical properties and spatial arrangements. In graph analysis,
    node relationships vary based on graph structure and node features. In document
    analysis, connections between different sections depend on semantic content rather
    than just proximity.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…è¶Šè¯­è¨€ï¼Œè¿™ç§å¯¹åŠ¨æ€å¤„ç†çš„éœ€æ±‚å‡ºç°åœ¨è®¸å¤šé¢†åŸŸã€‚åœ¨è›‹ç™½è´¨ç»“æ„é¢„æµ‹ä¸­ï¼Œæ°¨åŸºé…¸ä¹‹é—´çš„ç›¸äº’ä½œç”¨å–å†³äºå®ƒä»¬çš„åŒ–å­¦æ€§è´¨å’Œç©ºé—´æ’åˆ—ã€‚åœ¨å›¾åˆ†æä¸­ï¼ŒèŠ‚ç‚¹å…³ç³»æ ¹æ®å›¾ç»“æ„å’ŒèŠ‚ç‚¹ç‰¹å¾è€Œå˜åŒ–ã€‚åœ¨æ–‡æ¡£åˆ†æä¸­ï¼Œä¸åŒéƒ¨åˆ†ä¹‹é—´çš„è¿æ¥å–å†³äºè¯­ä¹‰å†…å®¹ï¼Œè€Œä¸ä»…ä»…æ˜¯é‚»è¿‘æ€§ã€‚
- en: Synthesizing these requirements, dynamic processing demands specific capabilities
    from our processing architecture. The system must compute relationships between
    all pairs of elements, weigh these relationships based on content, and use these
    weights to selectively combine information. Unlike previous architectures with
    fixed connectivity patterns, dynamic processing requires the flexibility to modify
    its computation graph based on the input itself. These capabilities naturally
    lead us to the attention mechanism, which serves as the foundation for the Transformer
    architecture examined in detail in the following sections. [FigureÂ 4.5](ch010.xhtml#fig-transformer-attention-visualized)
    shows attention enabling this dynamic information flow.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç»¼åˆè¿™äº›éœ€æ±‚ï¼ŒåŠ¨æ€å¤„ç†è¦æ±‚æˆ‘ä»¬çš„å¤„ç†æ¶æ„å…·å¤‡ç‰¹å®šçš„èƒ½åŠ›ã€‚ç³»ç»Ÿå¿…é¡»è®¡ç®—æ‰€æœ‰å…ƒç´ å¯¹ä¹‹é—´çš„å…³ç³»ï¼Œæ ¹æ®å†…å®¹æƒè¡¡è¿™äº›å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›æƒé‡æ¥é€‰æ‹©æ€§åœ°ç»„åˆä¿¡æ¯ã€‚ä¸å…·æœ‰å›ºå®šè¿æ¥æ¨¡å¼çš„å‰ä¸€ä»£æ¶æ„ä¸åŒï¼ŒåŠ¨æ€å¤„ç†éœ€è¦æ ¹æ®è¾“å…¥æœ¬èº«ä¿®æ”¹å…¶è®¡ç®—å›¾çš„èƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›è‡ªç„¶å¼•å¯¼æˆ‘ä»¬åˆ°æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒæ˜¯ä»¥ä¸‹ç« èŠ‚ä¸­è¯¦ç»†æ¢è®¨çš„Transformeræ¶æ„çš„åŸºç¡€ã€‚[å›¾4.5](ch010.xhtml#fig-transformer-attention-visualized)æ˜¾ç¤ºäº†æ³¨æ„åŠ›å¦‚ä½•å®ç°è¿™ç§åŠ¨æ€ä¿¡æ¯æµã€‚
- en: '![](../media/file58.svg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file58.svg)'
- en: 'FigureÂ 4.5: **Attention Weights**: Transformer attention mechanisms dynamically
    assess relationships between subwords, assigning higher weights to more relevant
    connections within a sequence and enabling the model to focus on key information.
    These learned weights, visualized as connection strengths, reveal how the model
    attends to different parts of the input when processing language.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.5ï¼š**æ³¨æ„åŠ›æƒé‡**ï¼šTransformerçš„æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è¯„ä¼°å­è¯ä¹‹é—´çš„å…³ç³»ï¼Œä¸ºåºåˆ—ä¸­æ›´ç›¸å…³çš„è¿æ¥åˆ†é…æ›´é«˜çš„æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨å…³é”®ä¿¡æ¯ã€‚è¿™äº›å­¦ä¹ åˆ°çš„æƒé‡ï¼Œä»¥è¿æ¥å¼ºåº¦å¯è§†åŒ–ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†è¯­è¨€æ—¶å¦‚ä½•å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚
- en: Basic Attention Mechanism
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶
- en: Attention mechanisms represent a shift from fixed architectural connections
    to dynamic, content-based interactions between sequence elements. This section
    explores the mathematical foundations of attention, examining how query-key-value
    operations enable flexible pattern processing. We analyze the computational requirements,
    memory access patterns, and system implications that make attention both powerful
    and computationally demanding.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ä»£è¡¨äº†ä»å›ºå®šæ¶æ„è¿æ¥åˆ°åºåˆ—å…ƒç´ ä¹‹é—´åŠ¨æ€ã€åŸºäºå†…å®¹çš„äº¤äº’çš„è½¬å˜ã€‚æœ¬èŠ‚æ¢è®¨äº†æ³¨æ„åŠ›çš„æ•°å­¦åŸºç¡€ï¼Œæ¢è®¨äº†æŸ¥è¯¢-é”®-å€¼æ“ä½œå¦‚ä½•å®ç°çµæ´»çš„æ¨¡å¼å¤„ç†ã€‚æˆ‘ä»¬åˆ†æäº†è®¡ç®—éœ€æ±‚ã€å†…å­˜è®¿é—®æ¨¡å¼å’Œç³»ç»Ÿå½±å“ï¼Œè¿™äº›ä½¿å¾—æ³¨æ„åŠ›æ—¢å¼ºå¤§åˆè®¡ç®—å¯†é›†ã€‚
- en: Algorithmic Structure
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç®—æ³•ç»“æ„
- en: 'Attention mechanisms form the foundation of dynamic pattern processing by computing
    weighted connections between elements based on their content ([Bahdanau, Cho,
    and Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)). This approach processes
    relationships that are not fixed by architecture but instead emerge from the data
    itself. At the core of an attention mechanism lies an operation that can be expressed
    mathematically as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡æ ¹æ®å…¶å†…å®¹è®¡ç®—å…ƒç´ ä¹‹é—´çš„åŠ æƒè¿æ¥ï¼Œå½¢æˆäº†åŠ¨æ€æ¨¡å¼å¤„ç†çš„åŸºç¡€ï¼ˆ[Bahdanau, Cho, å’Œ Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)ï¼‰ã€‚è¿™ç§æ–¹æ³•å¤„ç†çš„å…³ç³»ä¸æ˜¯ç”±æ¶æ„å›ºå®šçš„ï¼Œè€Œæ˜¯ä»æ•°æ®æœ¬èº«ä¸­äº§ç”Ÿçš„ã€‚æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯ä»¥ç”¨æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºçš„æ“ä½œï¼š
- en: <semantics><mrow><mtext mathvariant="normal">Attention</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>ğ</mi><mo>,</mo><mi>ğŠ</mi><mo>,</mo><mi>ğ•</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>ğ</mi><msup><mi>ğŠ</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>ğ•</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(\mathbf{Q},
    \mathbf{K}, \mathbf{V}) = \text{softmax} \left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}</annotation></semantics>
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: <semantics><mrow><mtext mathvariant="normal">æ³¨æ„åŠ›</mtext><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>ğ</mi><mo>,</mo><mi>ğŠ</mi><mo>,</mo><mi>ğ•</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>ğ</mi><msup><mi>ğŠ</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>ğ•</mi></mrow> <annotation encoding="application/x-tex">\text{Attention}(\mathbf{Q},
    \mathbf{K}, \mathbf{V}) = \text{softmax} \left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}</annotation></semantics>
- en: This equation shows scaled dot-product attention. <semantics><mi>ğ</mi><annotation
    encoding="application/x-tex">\mathbf{Q}</annotation></semantics> (queries) and
    <semantics><mi>ğŠ</mi><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics>
    (keys) are matrix-multiplied to compute similarity scores, divided by <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation
    encoding="application/x-tex">\sqrt{d_k}</annotation></semantics> (key dimension)
    for numerical stability, then normalized with softmax[18](#fn18) to get attention
    weights. These weights are applied to <semantics><mi>ğ•</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics>
    (values) to produce the output. The result is a weighted combination where each
    position receives information from all relevant positions based on content similarity.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹å±•ç¤ºäº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€‚ <semantics><mi>ğ</mi><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics>ï¼ˆæŸ¥è¯¢ï¼‰å’Œ
    <semantics><mi>ğŠ</mi><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics>ï¼ˆé”®ï¼‰é€šè¿‡çŸ©é˜µä¹˜æ³•è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œé™¤ä»¥
    <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics>ï¼ˆé”®ç»´åº¦ï¼‰ä»¥å®ç°æ•°å€¼ç¨³å®šæ€§ï¼Œç„¶åé€šè¿‡softmax[18](#fn18)è¿›è¡Œå½’ä¸€åŒ–ä»¥è·å¾—æ³¨æ„åŠ›æƒé‡ã€‚è¿™äº›æƒé‡åº”ç”¨äº
    <semantics><mi>ğ•</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics>ï¼ˆå€¼ï¼‰ä»¥äº§ç”Ÿè¾“å‡ºã€‚ç»“æœæ˜¯åŠ æƒç»„åˆï¼Œå…¶ä¸­æ¯ä¸ªä½ç½®æ ¹æ®å†…å®¹ç›¸ä¼¼æ€§ä»æ‰€æœ‰ç›¸å…³ä½ç½®æ¥æ”¶ä¿¡æ¯ã€‚
- en: In this equation, <semantics><mi>ğ</mi><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics>
    (queries), <semantics><mi>ğŠ</mi><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics>
    (keys), and <semantics><mi>ğ•</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics>
    (values)[19](#fn19) represent learned projections of the input. For a sequence
    of length <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    with dimension <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>,
    this operation creates an <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation
    encoding="application/x-tex">N\times N</annotation></semantics> attention matrix,
    determining how each position should attend to all others.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼Œ<semantics><mi>ğ</mi><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics>ï¼ˆæŸ¥è¯¢ï¼‰ã€<semantics><mi>ğŠ</mi><annotation
    encoding="application/x-tex">\mathbf{K}</annotation></semantics>ï¼ˆé”®ï¼‰å’Œ<semantics><mi>ğ•</mi><annotation
    encoding="application/x-tex">\mathbf{V}</annotation></semantics>ï¼ˆå€¼ï¼‰[19](#fn19)ä»£è¡¨è¾“å…¥çš„å­¦ä¹ æŠ•å½±ã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>ä¸”ç»´åº¦ä¸º<semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics>çš„åºåˆ—ï¼Œè¿™ä¸ªæ“ä½œåˆ›å»ºäº†ä¸€ä¸ª<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation
    encoding="application/x-tex">N\times N</annotation></semantics>æ³¨æ„åŠ›çŸ©é˜µï¼Œç¡®å®šæ¯ä¸ªä½ç½®åº”è¯¥å¦‚ä½•å…³æ³¨æ‰€æœ‰å…¶ä»–ä½ç½®ã€‚
- en: The attention operation involves several key steps. First, it computes query,
    key, and value projections for each position in the sequence. Next, it generates
    an <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N\times
    N</annotation></semantics> attention matrix through query-key interactions. These
    steps are illustrated in [FigureÂ 4.6](ch010.xhtml#fig-attention). Finally, it
    uses these attention weights to combine value vectors, producing the output.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ“ä½œæ¶‰åŠå‡ ä¸ªå…³é”®æ­¥éª¤ã€‚é¦–å…ˆï¼Œå®ƒä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼æŠ•å½±ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒé€šè¿‡æŸ¥è¯¢-é”®äº¤äº’ç”Ÿæˆä¸€ä¸ª<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation
    encoding="application/x-tex">N\times N</annotation></semantics>æ³¨æ„åŠ›çŸ©é˜µã€‚è¿™äº›æ­¥éª¤åœ¨å›¾4.6ä¸­å¾—åˆ°äº†è¯´æ˜ã€‚æœ€åï¼Œå®ƒä½¿ç”¨è¿™äº›æ³¨æ„åŠ›æƒé‡æ¥ç»„åˆå€¼å‘é‡ï¼Œç”Ÿæˆè¾“å‡ºã€‚
- en: '![](../media/file59.svg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file59.svg)'
- en: 'FigureÂ 4.6: **Query-Key-Value Interaction**: Transformer attention mechanisms
    dynamically weigh input sequence elements by computing relationships between queries,
    keys, and values, enabling the model to focus on relevant information. these projections
    facilitate the creation of an attention matrix that determines the contribution
    of each value vector to the final output, effectively capturing contextual dependencies
    within the sequence. Source: [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.6ï¼š**æŸ¥è¯¢-é”®-å€¼äº¤äº’**ï¼šTransformeræ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼ä¹‹é—´çš„å…³ç³»ï¼ŒåŠ¨æ€åœ°æƒè¡¡è¾“å…¥åºåˆ—å…ƒç´ ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ç›¸å…³ä¿¡æ¯ã€‚è¿™äº›æŠ•å½±æœ‰åŠ©äºåˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼Œè¯¥çŸ©é˜µå†³å®šäº†æ¯ä¸ªå€¼å‘é‡å¯¹æœ€ç»ˆè¾“å‡ºçš„è´¡çŒ®ï¼Œæœ‰æ•ˆåœ°æ•æ‰åºåˆ—ä¸­çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚æ¥æºï¼š[Transformerè§£é‡Šå™¨](https://poloclub.GitHub.io/transformer-explainer/).
- en: The key is that, unlike the fixed weight matrices found in previous architectures,
    as shown in [FigureÂ 4.7](ch010.xhtml#fig-attention-weightcalc), these attention
    weights are computed dynamically for each input. This allows the model to adapt
    its processing based on the dynamic content at hand.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äºï¼Œä¸ä¹‹å‰æ¶æ„ä¸­å‘ç°çš„å›ºå®šæƒé‡çŸ©é˜µä¸åŒï¼Œå¦‚å›¾4.7æ‰€ç¤ºï¼Œè¿™äº›æ³¨æ„åŠ›æƒé‡æ˜¯é’ˆå¯¹æ¯ä¸ªè¾“å…¥åŠ¨æ€è®¡ç®—çš„ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®åŠ¨æ€å†…å®¹è°ƒæ•´å…¶å¤„ç†æ–¹å¼ã€‚
- en: '![](../media/file60.svg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file60.svg)'
- en: 'FigureÂ 4.7: **Dynamic Attention Weights**: Transformer models calculate attention
    weights dynamically based on the relationships between query, key, and value vectors,
    allowing the model to focus on relevant parts of the input sequence for each processing
    step. this contrasts with fixed-weight architectures and enables adaptive pattern
    processing for handling variable-length inputs and complex dependencies. Source:
    [transformer explainer](HTTPS://poloclub.GitHub.io/transformer-explainer/).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.7ï¼š**åŠ¨æ€æ³¨æ„åŠ›æƒé‡**ï¼šTransformeræ¨¡å‹æ ¹æ®æŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡ä¹‹é—´çš„å…³ç³»åŠ¨æ€è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¯ä¸ªå¤„ç†æ­¥éª¤ä¸­å…³æ³¨è¾“å…¥åºåˆ—çš„ç›¸å…³éƒ¨åˆ†ã€‚è¿™ä¸å›ºå®šæƒé‡æ¶æ„å½¢æˆå¯¹æ¯”ï¼Œå¹¶å…è®¸è‡ªé€‚åº”æ¨¡å¼å¤„ç†ä»¥å¤„ç†å˜é•¿è¾“å…¥å’Œå¤æ‚ä¾èµ–å…³ç³»ã€‚æ¥æºï¼š[Transformerè§£é‡Šå™¨](https://poloclub.GitHub.io/transformer-explainer/).
- en: Computational Mapping
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ˜ å°„
- en: Attention mechanisms create computational patterns that differ significantly
    from previous architectures. The implementation approach shown in [ListingÂ 4.7](ch010.xhtml#lst-attention_layer_compute)
    shows dynamic connectivity translating into specific computational requirements.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶åˆ›å»ºçš„è®¡ç®—æ¨¡å¼ä¸ä¹‹å‰çš„æ¶æ„æœ‰æ˜¾è‘—å·®å¼‚ã€‚åœ¨ [åˆ—è¡¨ 4.7](ch010.xhtml#lst-attention_layer_compute)
    ä¸­å±•ç¤ºçš„å®ç°æ–¹æ³•è¡¨æ˜ï¼ŒåŠ¨æ€è¿æ¥æ€§è½¬åŒ–ä¸ºç‰¹å®šçš„è®¡ç®—éœ€æ±‚ã€‚
- en: 'ListingÂ 4.7: **Attention Mechanism**: Transformer models compute attention
    through query-key-value interactions, enabling dynamic focus across input sequences
    for improved language understanding.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 4.7ï¼š**æ³¨æ„åŠ›æœºåˆ¶**ï¼šTransformer æ¨¡å‹é€šè¿‡æŸ¥è¯¢-é”®-å€¼äº¤äº’æ¥è®¡ç®—æ³¨æ„åŠ›ï¼Œä»è€Œåœ¨è¾“å…¥åºåˆ—ä¸Šå®ç°åŠ¨æ€å…³æ³¨ï¼Œä»¥æ”¹å–„è¯­è¨€ç†è§£ã€‚
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The translation from attentionâ€™s mathematical elegance to hardware execution
    reveals the computational price of dynamic connectivity. While the attention equation
    `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V` appears as a straightforward matrix operation,
    the physical implementation requires orchestrating quadratic numbers of pairwise
    computations that create different system demands than previous architectures.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ³¨æ„åŠ›æ•°å­¦ä¸Šçš„ä¼˜é›…æ€§è½¬æ¢ä¸ºç¡¬ä»¶æ‰§è¡Œï¼Œæ­ç¤ºäº†åŠ¨æ€è¿æ¥æ€§çš„è®¡ç®—æˆæœ¬ã€‚è™½ç„¶æ³¨æ„åŠ›æ–¹ç¨‹ `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`
    çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªç®€å•çš„çŸ©é˜µè¿ç®—ï¼Œä½†ç‰©ç†å®ç°éœ€è¦åè°ƒæˆåƒä¸Šä¸‡çš„æˆå¯¹è®¡ç®—ï¼Œè¿™æ¯”ä¹‹å‰çš„æ¶æ„äº§ç”Ÿäº†ä¸åŒçš„ç³»ç»Ÿéœ€æ±‚ã€‚
- en: The nested loops in `attention_layer_compute` expose attentionâ€™s true computational
    signature (see [ListingÂ 4.7](ch010.xhtml#lst-attention_layer_compute)). The first
    loop processes each sequence in the batch independently. The second and third
    loops compute attention scores between all pairs of positions, creating the quadratic
    computation pattern that makes attention both powerful and computationally demanding.
    The fourth loop uses these attention weights to combine values from all positions,
    completing the dynamic connectivity pattern that defines attention mechanisms.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`attention_layer_compute` ä¸­çš„åµŒå¥—å¾ªç¯æ­ç¤ºäº†æ³¨æ„åŠ›çš„çœŸå®è®¡ç®—ç‰¹å¾ï¼ˆå‚è§ [åˆ—è¡¨ 4.7](ch010.xhtml#lst-attention_layer_compute)ï¼‰ã€‚ç¬¬ä¸€ä¸ªå¾ªç¯ç‹¬ç«‹å¤„ç†æ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªåºåˆ—ã€‚ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªå¾ªç¯è®¡ç®—æ‰€æœ‰ä½ç½®å¯¹ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œå½¢æˆäº†ä½¿æ³¨æ„åŠ›æ—¢å¼ºå¤§åˆè®¡ç®—å¯†é›†çš„äºŒæ¬¡è®¡ç®—æ¨¡å¼ã€‚ç¬¬å››ä¸ªå¾ªç¯ä½¿ç”¨è¿™äº›æ³¨æ„åŠ›æƒé‡æ¥ç»„åˆæ‰€æœ‰ä½ç½®çš„ä»·å€¼ï¼Œå®Œæˆå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶çš„åŠ¨æ€è¿æ¥æ¨¡å¼ã€‚'
- en: System Implications
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: Attention mechanisms exhibit distinctive system-level patterns that differ from
    previous architectures through their dynamic connectivity requirements.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å…¶åŠ¨æ€è¿æ¥æ€§éœ€æ±‚å±•ç°å‡ºä¸ä¹‹å‰æ¶æ„ä¸åŒçš„ç³»ç»Ÿçº§æ¨¡å¼ã€‚
- en: Memory Requirements
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: å†…å­˜éœ€æ±‚
- en: In terms of memory requirements, attention mechanisms necessitate storage for
    attention weights, key-query-value projections, and intermediate feature representations.
    For a sequence length <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    and dimension d, each attention layer must store an <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation
    encoding="application/x-tex">N\times N</annotation></semantics> attention weight
    matrix for each sequence in the batch, three sets of projection matrices for queries,
    keys, and values (each sized <semantics><mrow><mi>d</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d\times d</annotation></semantics>), and input and
    output feature maps of size <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">N\times d</annotation></semantics>. The dynamic generation
    of attention weights for every input creates a memory access pattern where intermediate
    attention weights become a significant factor in memory usage.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†…å­˜éœ€æ±‚æ–¹é¢ï¼Œæ³¨æ„åŠ›æœºåˆ¶éœ€è¦å­˜å‚¨æ³¨æ„åŠ›æƒé‡ã€é”®-æŸ¥è¯¢-å€¼æŠ•å½±å’Œä¸­é—´ç‰¹å¾è¡¨ç¤ºã€‚å¯¹äºä¸€ä¸ªé•¿åº¦å°äº `<semantics><mi>N</mi><annotation
    encoding="application/x-tex">N</annotation></semantics>` çš„åºåˆ—å’Œç»´åº¦ dï¼Œæ¯ä¸ªæ³¨æ„åŠ›å±‚å¿…é¡»ä¸ºæ‰¹å¤„ç†ä¸­çš„æ¯ä¸ªåºåˆ—å­˜å‚¨ä¸€ä¸ª
    `<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N\times
    N</annotation></semantics>` çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œä¸‰ç»„æŠ•å½±çŸ©é˜µç”¨äºæŸ¥è¯¢ã€é”®å’Œå€¼ï¼ˆæ¯ä¸ªå¤§å°ä¸º `<semantics><mrow><mi>d</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">d\times d</annotation></semantics>`ï¼‰ï¼Œä»¥åŠå¤§å°ä¸º `<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">N\times d</annotation></semantics>` çš„è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾ã€‚ä¸ºæ¯ä¸ªè¾“å…¥åŠ¨æ€ç”Ÿæˆæ³¨æ„åŠ›æƒé‡åˆ›å»ºäº†ä¸€ä¸ªå†…å­˜è®¿é—®æ¨¡å¼ï¼Œå…¶ä¸­ä¸­é—´æ³¨æ„åŠ›æƒé‡æˆä¸ºå†…å­˜ä½¿ç”¨çš„ä¸€ä¸ªé‡è¦å› ç´ ã€‚
- en: Computation Needs
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: è®¡ç®—éœ€æ±‚
- en: 'Computation needs in attention mechanisms center around two main phases: generating
    attention weights and applying them to values. For each attention layer, the system
    performs many multiply-accumulate operations across multiple computational stages.
    The query-key interactions alone require <semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">N\times N\times d</annotation></semantics> multiply-accumulates,
    with an equal number needed for applying attention weights to values. Additional
    computations are required for the projection matrices and softmax operations.
    This computational pattern differs from previous architectures due to its quadratic
    scaling with sequence length and the need to perform fresh computations for each
    input.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è®¡ç®—éœ€æ±‚é›†ä¸­åœ¨ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šç”Ÿæˆæ³¨æ„åŠ›æƒé‡å¹¶å°†å®ƒä»¬åº”ç”¨äºå€¼ã€‚å¯¹äºæ¯ä¸ªæ³¨æ„åŠ›å±‚ï¼Œç³»ç»Ÿåœ¨å¤šä¸ªè®¡ç®—é˜¶æ®µæ‰§è¡Œè®¸å¤šä¹˜åŠ æ“ä½œã€‚ä»…æŸ¥è¯¢-é”®äº¤äº’å°±éœ€è¦<semantics><mrow><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi></mrow><annotation
    encoding="application/x-tex">N\times N\times d</annotation></semantics>æ¬¡ä¹˜åŠ æ“ä½œï¼Œåº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼ä¹Ÿéœ€è¦ç›¸åŒæ•°é‡çš„æ“ä½œã€‚è¿˜éœ€è¦é¢å¤–çš„è®¡ç®—æ¥å¤„ç†æŠ•å½±çŸ©é˜µå’Œsoftmaxæ“ä½œã€‚è¿™ç§è®¡ç®—æ¨¡å¼ä¸ä¹‹å‰çš„æ¶æ„ä¸åŒï¼Œå› ä¸ºå®ƒä¸åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡å…³ç³»ï¼Œå¹¶ä¸”éœ€è¦å¯¹æ¯ä¸ªè¾“å…¥è¿›è¡Œæ–°é²œçš„è®¡ç®—ã€‚
- en: Data Movement
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨
- en: Data movement in attention mechanisms presents unique challenges. Each attention
    operation involves projecting and moving query, key, and value vectors for each
    position, storing and accessing the full attention weight matrix, and coordinating
    the movement of value vectors during the weighted combination phase. This creates
    a data movement pattern where intermediate attention weights become a major factor
    in system bandwidth requirements. Unlike the more predictable access patterns
    of CNNs or the sequential access of RNNs, attention operations require frequent
    movement of dynamically computed weights across the memory hierarchy.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ•°æ®ç§»åŠ¨æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æ¯ä¸ªæ³¨æ„åŠ›æ“ä½œéƒ½æ¶‰åŠå¯¹æ¯ä¸ªä½ç½®çš„æŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡è¿›è¡ŒæŠ•å½±å’Œç§»åŠ¨ï¼Œå­˜å‚¨å’Œè®¿é—®å®Œæ•´çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œå¹¶åœ¨åŠ æƒç»„åˆé˜¶æ®µåè°ƒå€¼å‘é‡çš„ç§»åŠ¨ã€‚è¿™åˆ›é€ äº†ä¸€ç§æ•°æ®ç§»åŠ¨æ¨¡å¼ï¼Œå…¶ä¸­ä¸­é—´æ³¨æ„åŠ›æƒé‡æˆä¸ºç³»ç»Ÿå¸¦å®½éœ€æ±‚çš„ä¸»è¦å› ç´ ã€‚ä¸CNNçš„æ›´å¯é¢„æµ‹çš„è®¿é—®æ¨¡å¼æˆ–RNNçš„é¡ºåºè®¿é—®ä¸åŒï¼Œæ³¨æ„åŠ›æ“ä½œéœ€è¦é¢‘ç¹åœ°åœ¨å†…å­˜å±‚æ¬¡ç»“æ„ä¸­ç§»åŠ¨åŠ¨æ€è®¡ç®—çš„æƒé‡ã€‚
- en: These distinctive characteristics of attention mechanisms in terms of memory,
    computation, and data movement have significant implications for system design
    and optimization, setting the stage for the development of more advanced architectures
    like Transformers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶åœ¨å†…å­˜ã€è®¡ç®—å’Œæ•°æ®ç§»åŠ¨æ–¹é¢çš„ç‹¬ç‰¹ç‰¹æ€§å¯¹ç³»ç»Ÿè®¾è®¡å’Œä¼˜åŒ–å…·æœ‰é‡å¤§å½±å“ï¼Œä¸ºæ›´é«˜çº§æ¶æ„å¦‚ä¼ è¾“å™¨çš„å¼€å‘å¥ å®šäº†åŸºç¡€ã€‚
- en: 'Transformers: Attention-Only Architecture'
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼ è¾“å™¨ï¼šä»…æ³¨æ„åŠ›æ¶æ„
- en: 'While attention mechanisms introduced the concept of dynamic pattern processing,
    they were initially applied as additions to existing architectures, particularly
    RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from
    the fundamental limitations of recurrent architectures: sequential processing
    constraints that prevented efficient parallelization and difficulties with very
    long sequences. The breakthrough insight was recognizing that attention mechanisms
    alone could replace both convolutional and recurrent processing entirely.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥äº†åŠ¨æ€æ¨¡å¼å¤„ç†çš„æ¦‚å¿µæ—¶ï¼Œå®ƒä»¬æœ€åˆè¢«ä½œä¸ºç°æœ‰æ¶æ„çš„è¡¥å……åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ä¸­ç”¨äºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ã€‚è¿™ç§æ··åˆæ–¹æ³•ä»ç„¶å—åˆ°å¾ªç¯æ¶æ„çš„åŸºæœ¬é™åˆ¶ï¼šåºåˆ—å¤„ç†çº¦æŸé˜»æ­¢äº†æœ‰æ•ˆçš„å¹¶è¡ŒåŒ–ï¼Œä»¥åŠå¤„ç†éå¸¸é•¿åºåˆ—çš„å›°éš¾ã€‚çªç ´æ€§çš„æ´å¯Ÿæ˜¯è®¤è¯†åˆ°ï¼Œä»…æ³¨æ„åŠ›æœºåˆ¶æœ¬èº«å°±å¯ä»¥å®Œå…¨å–ä»£å·ç§¯å’Œå¾ªç¯å¤„ç†ã€‚
- en: 'Transformers, introduced in the landmark "Attention is All You Need" paper[20](#fn20)
    by Vaswani et al. ([2017](ch058.xhtml#ref-vaswani2017attention)), embody a revolutionary
    inductive bias: **they assume no prior structure but allow the model to learn
    all pairwise relationships dynamically based on content**. This architectural
    assumption represents the culmination of the architectural evolution detailed
    in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)
    by eliminating all structural constraints in favor of pure content-dependent processing.
    Rather than adding attention to RNNs, Transformers built the entire architecture
    around attention mechanisms, introducing self-attention as the primary computational
    pattern. This architectural decision traded the parameter efficiency of CNNs and
    the sequential coherence of RNNs for maximum flexibility and parallelizability.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani ç­‰äººåœ¨é‡Œç¨‹ç¢‘å¼çš„è®ºæ–‡ "Attention is All You Need"[20](#fn20) ä¸­ä»‹ç»äº† Transformersï¼Œè¯¥è®ºæ–‡å‘è¡¨äº
    [2017](ch058.xhtml#ref-vaswani2017attention)ã€‚è¯¥è®ºæ–‡ä½“ç°äº†ä¸€ç§é©å‘½æ€§çš„å½’çº³åå·®ï¼š**å®ƒä»¬å‡è®¾æ²¡æœ‰å…ˆéªŒç»“æ„ï¼Œä½†å…è®¸æ¨¡å‹æ ¹æ®å†…å®¹åŠ¨æ€åœ°å­¦ä¹ æ‰€æœ‰æˆå¯¹å…³ç³»**ã€‚è¿™ç§æ¶æ„å‡è®¾ä»£è¡¨äº†åœ¨
    [ç¬¬ 4.1 èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)
    ä¸­è¯¦ç»†é˜è¿°çš„æ¶æ„æ¼”å˜çš„æœ€ç»ˆæˆæœï¼Œé€šè¿‡æ¶ˆé™¤æ‰€æœ‰ç»“æ„çº¦æŸï¼Œä»¥çº¯å†…å®¹ä¾èµ–å¤„ç†ä¸ºä»£ä»·ã€‚Transformers ä¸æ˜¯å‘ RNNs æ·»åŠ æ³¨æ„åŠ›ï¼Œè€Œæ˜¯å›´ç»•æ³¨æ„åŠ›æœºåˆ¶æ„å»ºäº†æ•´ä¸ªæ¶æ„ï¼Œå¼•å…¥è‡ªæ³¨æ„åŠ›ä½œä¸ºä¸»è¦çš„è®¡ç®—æ¨¡å¼ã€‚è¿™ä¸€æ¶æ„å†³ç­–ä»¥
    CNNs çš„å‚æ•°æ•ˆç‡å’Œ RNNs çš„åºåˆ—ä¸€è‡´æ€§ä¸ºä»£ä»·ï¼Œæ¢å–äº†æœ€å¤§çš„çµæ´»æ€§å’Œå¹¶è¡Œæ€§ã€‚
- en: 'This represents the final step in our architectural journey: from MLPs that
    connected everything to everything, to CNNs that connected locally, to RNNs that
    connected sequentially, to Transformers that connect dynamically based on learned
    content relationships. Each evolution sacrificed constraints for capabilities,
    with Transformers achieving maximum expressivity at the computational cost established
    in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»£è¡¨äº†æˆ‘ä»¬æ¶æ„æ—…ç¨‹çš„æœ€ç»ˆä¸€æ­¥ï¼šä»è¿æ¥ä¸€åˆ‡åˆ°ä¸€åˆ‡çš„ MLPsï¼Œåˆ°è¿æ¥å±€éƒ¨çš„ CNNsï¼Œå†åˆ°æŒ‰é¡ºåºè¿æ¥çš„ RNNsï¼Œæœ€ååˆ°åŸºäºå­¦ä¹ åˆ°çš„å†…å®¹å…³ç³»åŠ¨æ€è¿æ¥çš„
    Transformersã€‚æ¯ä¸€æ¬¡æ¼”å˜éƒ½ç‰ºç‰²äº†çº¦æŸä»¥æ¢å–èƒ½åŠ›ï¼ŒTransformers åœ¨ [ç¬¬ 4.1 èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)
    ä¸­ç¡®ç«‹çš„è®¡ç®—æˆæœ¬ä¸‹å®ç°äº†æœ€å¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- en: Algorithmic Structure
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç®—æ³•ç»“æ„
- en: The key innovation in Transformers lies in their use of self-attention layers.
    In the self-attention mechanism used by Transformers, the Query, Key, and Value
    vectors are all derived from the same input sequence. This is the key distinction
    from earlier attention mechanisms where the query might come from a decoder while
    the keys and values came from an encoder. By making all components self-referential,
    self-attention allows the model to weigh the importance of different positions
    within the same sequence when encoding each position. For instance, in processing
    the sentence â€œThe animal didnâ€™t cross the street because it was too wide,â€ self-attention
    allows the model to link â€œitâ€ with â€œstreet,â€ capturing long-range dependencies
    that are challenging for traditional sequential models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers çš„å…³é”®åˆ›æ–°åœ¨äºå®ƒä»¬ä½¿ç”¨äº†è‡ªæ³¨æ„åŠ›å±‚ã€‚åœ¨ Transformers æ‰€ä½¿ç”¨çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒæŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰å‘é‡éƒ½æ¥è‡ªç›¸åŒçš„è¾“å…¥åºåˆ—ã€‚è¿™ä¸æ—©æœŸæ³¨æ„åŠ›æœºåˆ¶çš„å…³é”®åŒºåˆ«åœ¨äºï¼ŒæŸ¥è¯¢å¯èƒ½æ¥è‡ªè§£ç å™¨ï¼Œè€Œé”®å’Œå€¼æ¥è‡ªç¼–ç å™¨ã€‚é€šè¿‡ä½¿æ‰€æœ‰ç»„ä»¶éƒ½è‡ªå‚ç…§ï¼Œè‡ªæ³¨æ„åŠ›å…è®¸æ¨¡å‹åœ¨ç¼–ç æ¯ä¸ªä½ç½®æ—¶ï¼Œæ ¹æ®å†…å®¹åŠ¨æ€åœ°æƒè¡¡åŒä¸€åºåˆ—ä¸­ä¸åŒä½ç½®çš„é‡è¦æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤„ç†å¥å­â€œåŠ¨ç‰©æ²¡æœ‰è¿‡è¡—ï¼Œå› ä¸ºå®ƒå¤ªå®½äº†â€æ—¶ï¼Œè‡ªæ³¨æ„åŠ›å…è®¸æ¨¡å‹å°†â€œå®ƒâ€ä¸â€œè¡—é“â€è”ç³»èµ·æ¥ï¼Œæ•æ‰åˆ°ä¼ ç»Ÿåºåˆ—æ¨¡å‹éš¾ä»¥å¤„ç†çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚
- en: 'The self-attention mechanism can be expressed mathematically in a form similar
    to the basic attention mechanism: <semantics><mrow><mtext mathvariant="normal">SelfAttention</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext
    mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğ</mi></msub></mrow><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğŠ</mi></msub></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğ•</mi></msub></mrow></mrow>
    <annotation encoding="application/x-tex">\text{SelfAttention}(\mathbf{X}) = \text{softmax}
    \left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}</annotation></semantics>'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç”¨ç±»ä¼¼äºåŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶çš„å½¢å¼è¿›è¡Œæ•°å­¦è¡¨è¾¾ï¼š<semantics><mrow><mtext mathvariant="normal">SelfAttention</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext
    mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğ</mi></msub></mrow><msup><mrow><mo
    stretchy="true" form="prefix">(</mo><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğŠ</mi></msub></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo
    stretchy="true" form="postfix">)</mo></mrow><mrow><mi>ğ—</mi><msub><mi>ğ–</mi><mi>ğ•</mi></msub></mrow></mrow>
    <annotation encoding="application/x-tex">\text{SelfAttention}(\mathbf{X}) = \text{softmax}
    \left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}</annotation></semantics>
- en: Here, <semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>
    is the input sequence, and <semantics><msub><mi>ğ–</mi><mi>ğ</mi></msub><annotation
    encoding="application/x-tex">\mathbf{W_Q}</annotation></semantics>, <semantics><msub><mi>ğ–</mi><mi>ğŠ</mi></msub><annotation
    encoding="application/x-tex">\mathbf{W_K}</annotation></semantics>, and <semantics><msub><mi>ğ–</mi><mi>ğ•</mi></msub><annotation
    encoding="application/x-tex">\mathbf{W_V}</annotation></semantics> are learned
    weight matrices for queries, keys, and values respectively. This formulation highlights
    how self-attention derives all its components from the same input, creating a
    dynamic, content-dependent processing pattern.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ<semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>
    æ˜¯è¾“å…¥åºåˆ—ï¼Œè€Œ <semantics><msub><mi>ğ–</mi><mi>ğ</mi></msub><annotation encoding="application/x-tex">\mathbf{W_Q}</annotation></semantics>ã€<semantics><msub><mi>ğ–</mi><mi>ğŠ</mi></msub><annotation
    encoding="application/x-tex">\mathbf{W_K}</annotation></semantics> å’Œ <semantics><msub><mi>ğ–</mi><mi>ğ•</mi></msub><annotation
    encoding="application/x-tex">\mathbf{W_V}</annotation></semantics> åˆ†åˆ«æ˜¯ç”¨äºæŸ¥è¯¢ã€é”®å’Œå€¼çš„å­¦åˆ°çš„æƒé‡çŸ©é˜µã€‚è¿™ç§å…¬å¼çªå‡ºäº†è‡ªæ³¨æ„åŠ›å¦‚ä½•ä»ç›¸åŒçš„è¾“å…¥ä¸­æ¨å¯¼å‡ºæ‰€æœ‰ç»„ä»¶ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªåŠ¨æ€çš„ã€å†…å®¹ç›¸å…³çš„å¤„ç†æ¨¡å¼ã€‚
- en: Building on this foundation, Transformers employ multi-head attention, which
    extends the self-attention mechanism by running multiple attention functions in
    parallel. Each â€œheadâ€ involves a separate set of query/key/value projections that
    can focus on different aspects of the input, allowing the model to jointly attend
    to information from different representation subspaces. This multi-head structure
    provides the model with a richer representational capability, enabling it to capture
    various types of relationships within the data simultaneously.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªåŸºç¡€ä¸Šï¼ŒTransformers ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Œé€šè¿‡å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å‡½æ•°æ¥æ‰©å±•è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚æ¯ä¸ªâ€œå¤´â€éƒ½æ¶‰åŠä¸€ç»„ç‹¬ç«‹çš„æŸ¥è¯¢/é”®/å€¼æŠ•å½±ï¼Œå¯ä»¥å…³æ³¨è¾“å…¥çš„ä¸åŒæ–¹é¢ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨æ¥è‡ªä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ã€‚è¿™ç§å¤šå¤´ç»“æ„ä¸ºæ¨¡å‹æä¾›äº†æ›´ä¸°å¯Œçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶æ•æ‰æ•°æ®ä¸­çš„å„ç§ç±»å‹çš„å…³ç³»ã€‚
- en: 'The mathematical formulation for multi-head attention is: <semantics><mrow><mtext
    mathvariant="normal">MultiHead</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ</mi><mo>,</mo><mi>ğŠ</mi><mo>,</mo><mi>ğ•</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Concat</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="normal">head</mtext><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mtext
    mathvariant="normal">head</mtext><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>ğ–</mi><mi>O</mi></msup></mrow>
    <annotation encoding="application/x-tex">\text{MultiHead}(\mathbf{Q}, \mathbf{K},
    \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O</annotation></semantics>
    where each attention head is computed as: <semantics><mrow><msub><mtext mathvariant="normal">head</mtext><mi>i</mi></msub><mo>=</mo><mtext
    mathvariant="normal">Attention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>Q</mi></msubsup><mo>,</mo><mi>ğŠ</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>K</mi></msubsup><mo>,</mo><mi>ğ•</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>V</mi></msubsup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{head}_i
    = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)</annotation></semantics>'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›çš„æ•°å­¦å…¬å¼ä¸ºï¼š<semantics><mrow><mtext mathvariant="normal">MultiHead</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>ğ</mi><mo>,</mo><mi>ğŠ</mi><mo>,</mo><mi>ğ•</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">Concat</mtext><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mtext mathvariant="normal">head</mtext><mn>1</mn></msub><mo>,</mo><mi>â€¦</mi><mo>,</mo><msub><mtext
    mathvariant="normal">head</mtext><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>ğ–</mi><mi>O</mi></msup></mrow>
    <annotation encoding="application/x-tex">\text{MultiHead}(\mathbf{Q}, \mathbf{K},
    \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O</annotation></semantics>
    å…¶ä¸­æ¯ä¸ªæ³¨æ„åŠ›å¤´æ˜¯æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¡ç®—çš„ï¼š<semantics><mrow><msub><mtext mathvariant="normal">head</mtext><mi>i</mi></msub><mo>=</mo><mtext
    mathvariant="normal">Attention</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>Q</mi></msubsup><mo>,</mo><mi>ğŠ</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>K</mi></msubsup><mo>,</mo><mi>ğ•</mi><msubsup><mi>ğ–</mi><mi>i</mi><mi>V</mi></msubsup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{head}_i
    = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)</annotation></semantics>
- en: A critical component in both self-attention and multi-head attention is the
    scaling factor <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation
    encoding="application/x-tex">\sqrt{d_k}</annotation></semantics>, which serves
    an important mathematical purpose. This factor prevents the dot products from
    growing too large, which would push the softmax function into regions with extremely
    small gradients. For queries and keys of dimension <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">d_k</annotation></semantics>, their dot product has
    variance <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>,
    so dividing by <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation
    encoding="application/x-tex">\sqrt{d_k}</annotation></semantics> normalizes the
    variance to 1, maintaining stable gradients and enabling effective learning.[21](#fn21)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªæ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œä¸€ä¸ªå…³é”®ç»„ä»¶æ˜¯ç¼©æ”¾å› å­ <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation
    encoding="application/x-tex">\sqrt{d_k}</annotation></semantics>ï¼Œå®ƒå…·æœ‰é‡è¦çš„æ•°å­¦ä½œç”¨ã€‚è¿™ä¸ªå› å­é˜²æ­¢ç‚¹ç§¯å˜å¾—è¿‡å¤§ï¼Œè¿™ä¼šå°†softmaxå‡½æ•°æ¨å‘æ¢¯åº¦æå°çš„åŒºåŸŸã€‚å¯¹äºç»´åº¦ä¸º
    <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>
    çš„æŸ¥è¯¢å’Œé”®ï¼Œå®ƒä»¬çš„ç‚¹ç§¯å…·æœ‰æ–¹å·® <semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding="application/x-tex">d_k</annotation></semantics>ï¼Œå› æ­¤é™¤ä»¥
    <semantics><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics>
    å°†æ–¹å·®å½’ä¸€åŒ–åˆ°1ï¼Œä¿æŒç¨³å®šçš„æ¢¯åº¦å¹¶å®ç°æœ‰æ•ˆçš„å­¦ä¹ ã€‚[21](#fn21)
- en: Beyond the mathematical mechanics, attention mechanisms can be understood conceptually
    as implementing a form of content-addressable memory system. Like hash tables
    that retrieve values based on key matching, attention computes similarity between
    a query and all available keys, then retrieves a weighted combination of corresponding
    values. The dot product similarity `QÂ·K` functions like a hash function that measures
    how well each key matches the query. The softmax normalization ensures the weights
    sum to 1, implementing a probabilistic retrieval mechanism. This connection explains
    why attention proves effective for tasks requiring flexible information retrievalâ€”it
    provides a differentiable approximation to database lookup operations.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'é™¤äº†æ•°å­¦æœºåˆ¶ä¹‹å¤–ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ä»æ¦‚å¿µä¸Šç†è§£ä¸ºå®ç°äº†å†…å®¹å¯å¯»å€å†…å­˜ç³»ç»Ÿçš„ä¸€ç§å½¢å¼ã€‚ç±»ä¼¼äºåŸºäºé”®åŒ¹é…æ£€ç´¢å€¼çš„å“ˆå¸Œè¡¨ï¼Œæ³¨æ„åŠ›è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰å¯ç”¨é”®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç„¶åæ£€ç´¢ç›¸åº”å€¼çš„åŠ æƒç»„åˆã€‚ç‚¹ç§¯ç›¸ä¼¼åº¦`QÂ·K`å°±åƒä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼Œå®ƒè¡¡é‡æ¯ä¸ªé”®ä¸æŸ¥è¯¢åŒ¹é…çš„ç¨‹åº¦ã€‚softmaxå½’ä¸€åŒ–ç¡®ä¿æƒé‡ä¹‹å’Œä¸º1ï¼Œå®ç°äº†æ¦‚ç‡æ£€ç´¢æœºåˆ¶ã€‚è¿™ç§è”ç³»è§£é‡Šäº†ä¸ºä»€ä¹ˆæ³¨æ„åŠ›å¯¹äºéœ€è¦çµæ´»ä¿¡æ¯æ£€ç´¢çš„ä»»åŠ¡æ˜¯æœ‰æ•ˆçš„â€”â€”å®ƒæä¾›äº†å¯¹æ•°æ®åº“æŸ¥æ‰¾æ“ä½œçš„å¾®åˆ†è¿‘ä¼¼ã€‚ '
- en: 'From an information-theoretic perspective, attention mechanisms implement optimal
    information aggregation under uncertainty. The attention weights represent uncertainty
    about which parts of the input contain relevant information for the current processing
    step. The softmax operation implements a maximum entropy principle: among all
    possible ways to distribute attention across input positions, softmax selects
    the distribution with maximum entropy subject to the constraint that similarity
    scores determine relative importance ([Cover and Thomas 2001](ch058.xhtml#ref-cover2006elements)).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¿¡æ¯è®ºçš„è§’åº¦æ¥çœ‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸ç¡®å®šæ€§ä¸‹å®ç°äº†æœ€ä¼˜çš„ä¿¡æ¯èšåˆã€‚æ³¨æ„åŠ›æƒé‡ä»£è¡¨äº†å…³äºå“ªäº›è¾“å…¥éƒ¨åˆ†åŒ…å«å½“å‰å¤„ç†æ­¥éª¤çš„ç›¸å…³ä¿¡æ¯çš„ä¸ç¡®å®šæ€§ã€‚softmaxæ“ä½œå®ç°äº†æœ€å¤§ç†µåŸç†ï¼šåœ¨æ‰€æœ‰å¯èƒ½çš„å°†æ³¨æ„åŠ›åˆ†é…åˆ°è¾“å…¥ä½ç½®çš„æ–¹å¼ä¸­ï¼Œsoftmaxé€‰æ‹©å…·æœ‰æœ€å¤§ç†µçš„åˆ†å¸ƒï¼ŒåŒæ—¶æ»¡è¶³ç›¸ä¼¼åº¦åˆ†æ•°å†³å®šç›¸å¯¹é‡è¦æ€§çš„çº¦æŸï¼ˆ[Coverå’ŒThomas
    2001](ch058.xhtml#ref-cover2006elements)ï¼‰ã€‚
- en: Efficiency and Optimization
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•ˆç‡å’Œä¼˜åŒ–
- en: Attention mechanisms are highly redundant, with many heads learning similar
    patterns. Head pruning and low-rank attention factorization can reduce computation
    by 50-80% with careful implementation. Analysis of large Transformer models reveals
    that most attention heads fall into a few common patterns (positional, syntactic,
    semantic), suggesting that explicit architectural specialization could replace
    learned redundancy.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶é«˜åº¦å†—ä½™ï¼Œè®¸å¤šå¤´éƒ¨å­¦ä¹ ç›¸ä¼¼çš„æ¨¡å¼ã€‚é€šè¿‡å¤´éƒ¨å‰ªæå’Œä½ç§©æ³¨æ„åŠ›å› å­åˆ†è§£ï¼Œå¯ä»¥åœ¨è°¨æ…å®æ–½çš„æƒ…å†µä¸‹å°†è®¡ç®—é‡å‡å°‘50-80%ã€‚å¯¹å¤§å‹Transformeræ¨¡å‹çš„åˆ†æè¡¨æ˜ï¼Œå¤§å¤šæ•°æ³¨æ„åŠ›å¤´éƒ¨éƒ½è½å…¥å‡ ç§å¸¸è§çš„æ¨¡å¼ï¼ˆä½ç½®ã€å¥æ³•ã€è¯­ä¹‰ï¼‰ï¼Œè¿™è¡¨æ˜æ˜¾å¼çš„æ¶æ„ä¸“ä¸šåŒ–å¯ä»¥æ›¿ä»£å­¦ä¹ åˆ°çš„å†—ä½™ã€‚
- en: Attention operations are particularly sensitive to quantization due to the softmax
    operation and the quadratic number of attention scores. Separate quantization
    schemes for Q, K, V projections and careful handling of softmax operations are
    required for stable quantization. Post-training INT8 quantization typically achieves
    2-3% accuracy loss, while INT4 quantization requires more sophisticated quantization-aware
    training approaches.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ“ä½œç”±äºsoftmaxæ“ä½œå’Œæ³¨æ„åŠ›åˆ†æ•°çš„äºŒæ¬¡æ•°é‡ï¼Œå¯¹é‡åŒ–ç‰¹åˆ«æ•æ„Ÿã€‚éœ€è¦ä¸ºQã€Kã€VæŠ•å½±åˆ¶å®šå•ç‹¬çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶ä»”ç»†å¤„ç†softmaxæ“ä½œï¼Œä»¥ç¡®ä¿ç¨³å®šçš„é‡åŒ–ã€‚è®­ç»ƒåINT8é‡åŒ–é€šå¸¸ä¼šå¯¼è‡´2-3%çš„ç²¾åº¦æŸå¤±ï¼Œè€ŒINT4é‡åŒ–åˆ™éœ€è¦æ›´å¤æ‚çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ–¹æ³•ã€‚
- en: The quadratic scaling with sequence length creates efficiency limitations. Sparse
    attention patterns (such as local windows, strided patterns, or learned sparsity)
    can reduce complexity from O(nÂ²) to O(n log n) or O(n) while maintaining most
    modeling capability. Linear attention approximations trade some expressive power
    for linear scaling, enabling processing of much longer sequences on limited hardware.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åºåˆ—é•¿åº¦çš„äºŒæ¬¡ç¼©æ”¾åˆ›é€ äº†æ•ˆç‡é™åˆ¶ã€‚ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼ˆå¦‚å±€éƒ¨çª—å£ã€æ­¥è¿›æ¨¡å¼æˆ–å­¦ä¹ åˆ°çš„ç¨€ç–æ€§ï¼‰å¯ä»¥å°†å¤æ‚åº¦ä»O(nÂ²)é™ä½åˆ°O(n log n)æˆ–O(n)ï¼ŒåŒæ—¶ä¿æŒå¤§å¤šæ•°å»ºæ¨¡èƒ½åŠ›ã€‚çº¿æ€§æ³¨æ„åŠ›è¿‘ä¼¼ä»¥ç‰ºç‰²ä¸€äº›è¡¨è¾¾èƒ½åŠ›ä¸ºä»£ä»·ï¼Œå®ç°äº†çº¿æ€§ç¼©æ”¾ï¼Œä½¿å¾—åœ¨æœ‰é™çš„ç¡¬ä»¶ä¸Šèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„åºåˆ—ã€‚
- en: 'This information-theoretic interpretation reveals why attention is so effective
    for selective processing. The mechanism automatically balances two competing objectives:
    focusing on the most relevant information (minimizing entropy) while maintaining
    sufficient breadth to avoid missing important details (maximizing entropy). The
    attention pattern emerges as the optimal trade-off between these objectives, explaining
    why transformers can effectively handle long sequences and complex dependencies.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¿¡æ¯è®ºè§£é‡Šæ­ç¤ºäº†ä¸ºä»€ä¹ˆæ³¨æ„åŠ›å¯¹äºé€‰æ‹©æ€§å¤„ç†å¦‚æ­¤æœ‰æ•ˆã€‚è¯¥æœºåˆ¶è‡ªåŠ¨å¹³è¡¡ä¸¤ä¸ªç›¸äº’ç«äº‰çš„ç›®æ ‡ï¼šå…³æ³¨æœ€ç›¸å…³çš„ä¿¡æ¯ï¼ˆæœ€å°åŒ–ç†µï¼‰åŒæ—¶ä¿æŒè¶³å¤Ÿçš„å¹¿åº¦ä»¥é¿å…é—æ¼é‡è¦ç»†èŠ‚ï¼ˆæœ€å¤§åŒ–ç†µï¼‰ã€‚æ³¨æ„åŠ›æ¨¡å¼ä½œä¸ºè¿™äº›ç›®æ ‡ä¹‹é—´çš„æœ€ä½³æƒè¡¡å‡ºç°ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆå˜æ¢å™¨å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†é•¿åºåˆ—å’Œå¤æ‚ä¾èµ–å…³ç³»ã€‚
- en: Self-attention learns dynamic activation patterns across the input sequence.
    Unlike CNNs which apply fixed filters or RNNs which use fixed recurrence patterns,
    attention learns which elements should activate together based on their content.
    This creates a form of adaptive connectivity where the effective network topology
    changes for each input. Recent research has shown that attention heads in trained
    models often specialize in detecting specific linguistic or semantic patterns
    ([Clark et al. 2019](ch058.xhtml#ref-clark2019what)), suggesting that the mechanism
    naturally discovers interpretable structural regularities in data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›å­¦ä¹ è¾“å…¥åºåˆ—ä¸­çš„åŠ¨æ€æ¿€æ´»æ¨¡å¼ã€‚ä¸ä½¿ç”¨å›ºå®šæ»¤æ³¢å™¨çš„CNNæˆ–ä½¿ç”¨å›ºå®šé€’å½’æ¨¡å¼çš„RNNä¸åŒï¼Œæ³¨æ„åŠ›æ ¹æ®å…¶å†…å®¹å­¦ä¹ å“ªäº›å…ƒç´ åº”è¯¥ä¸€èµ·æ¿€æ´»ã€‚è¿™åˆ›é€ äº†ä¸€ç§è‡ªé€‚åº”è¿æ¥å½¢å¼ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥çš„æœ‰æ•ˆç½‘ç»œæ‹“æ‰‘éƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒæ¨¡å‹ä¸­çš„æ³¨æ„åŠ›å¤´é€šå¸¸ä¸“é—¨ç”¨äºæ£€æµ‹ç‰¹å®šçš„è¯­è¨€æˆ–è¯­ä¹‰æ¨¡å¼ï¼ˆ[Clarkç­‰äºº2019](ch058.xhtml#ref-clark2019what)ï¼‰ï¼Œè¿™è¡¨æ˜è¯¥æœºåˆ¶è‡ªç„¶åœ°å‘ç°äº†æ•°æ®ä¸­çš„å¯è§£é‡Šç»“æ„è§„å¾‹ã€‚
- en: The Transformer architecture leverages this self-attention mechanism within
    a broader structure that typically includes feed-forward layers, layer normalization,
    and residual connections (see [FigureÂ 4.8](ch010.xhtml#fig-transformer)). This
    combination allows Transformers to process input sequences in parallel, capturing
    complex dependencies without the need for sequential computation. As a result,
    Transformers have demonstrated significant effectiveness across a wide range of
    tasks, from natural language processing to computer vision, transforming deep
    learning architectures across domains.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨æ¶æ„åˆ©ç”¨è¿™ç§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æ›´å¹¿æ³›çš„ç»“æ„ä¸­ï¼Œé€šå¸¸åŒ…æ‹¬å‰é¦ˆå±‚ã€å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ï¼ˆå‚è§[å›¾4.8](ch010.xhtml#fig-transformer)ï¼‰ã€‚è¿™ç§ç»„åˆå…è®¸å˜æ¢å™¨å¹¶è¡Œå¤„ç†è¾“å…¥åºåˆ—ï¼Œæ•æ‰å¤æ‚ä¾èµ–å…³ç³»ï¼Œè€Œæ— éœ€è¿›è¡Œé¡ºåºè®¡ç®—ã€‚å› æ­¤ï¼Œå˜æ¢å™¨åœ¨ä»è‡ªç„¶è¯­è¨€å¤„ç†åˆ°è®¡ç®—æœºè§†è§‰çš„å¹¿æ³›ä»»åŠ¡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œè·¨è¶Šäº†ä¸åŒé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚
- en: '![](../media/file61.svg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file61.svg)'
- en: 'FigureÂ 4.8: **Attention Head**: Neural networks compute attention through query-key-value
    interactions, enabling dynamic focus across subwords for improved sentence understanding.
    Source: Attention Is All You Need.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.8ï¼š**æ³¨æ„åŠ›å¤´**ï¼šç¥ç»ç½‘ç»œé€šè¿‡æŸ¥è¯¢-é”®-å€¼äº¤äº’è®¡ç®—æ³¨æ„åŠ›ï¼Œä½¿å­è¯èƒ½å¤ŸåŠ¨æ€èšç„¦ï¼Œä»è€Œæé«˜å¥å­ç†è§£èƒ½åŠ›ã€‚æ¥æºï¼šAttention Is All You
    Need.
- en: Computational Mapping
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ˜ å°„
- en: 'While Transformer self-attention builds upon the basic attention mechanism,
    it introduces distinct computational patterns that set it apart. To understand
    these patterns, we must examine the typical implementation of self-attention in
    Transformers (see [ListingÂ 4.8](ch010.xhtml#lst-self_attention_layer)):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å˜æ¢å™¨è‡ªæ³¨æ„åŠ›å»ºç«‹åœ¨åŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶ä¹‹ä¸Šï¼Œä½†å®ƒå¼•å…¥äº†ç‹¬ç‰¹çš„è®¡ç®—æ¨¡å¼ï¼Œä½¿å…¶ä¸ä¼—ä¸åŒã€‚ä¸ºäº†ç†è§£è¿™äº›æ¨¡å¼ï¼Œæˆ‘ä»¬å¿…é¡»æ£€æŸ¥å˜æ¢å™¨ä¸­è‡ªæ³¨æ„åŠ›çš„å…¸å‹å®ç°ï¼ˆå‚è§[åˆ—è¡¨4.8](ch010.xhtml#lst-self_attention_layer)ï¼‰ï¼š
- en: 'ListingÂ 4.8: **Self-Attention Mechanism**: Transformer models compute attention
    through query-key-value interactions, enabling dynamic focus across input sequences
    for improved language understanding.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨4.8ï¼š**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå˜æ¢å™¨æ¨¡å‹é€šè¿‡æŸ¥è¯¢-é”®-å€¼äº¤äº’è®¡ç®—æ³¨æ„åŠ›ï¼Œä½¿è¾“å…¥åºåˆ—èƒ½å¤ŸåŠ¨æ€èšç„¦ï¼Œä»è€Œæé«˜è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
- en: '[PRE7]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: System Implications
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿå½±å“
- en: This implementation reveals key computational characteristics that apply to
    basic attention mechanisms, with Transformer self-attention representing a specific
    case. First, self-attention enables parallel processing across all positions in
    the sequence. This is evident in the matrix multiplications that compute `Q`,
    `K`, and `V` simultaneously for all positions. Unlike recurrent architectures
    that process inputs sequentially, this parallel nature allows for more efficient
    computation, especially on modern hardware designed for parallel operations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§å®ç°æ­ç¤ºäº†é€‚ç”¨äºåŸºæœ¬æ³¨æ„åŠ›æœºåˆ¶çš„å…³é”®è®¡ç®—ç‰¹æ€§ï¼Œå…¶ä¸­Transformerè‡ªæ³¨æ„åŠ›ä»£è¡¨äº†ä¸€ä¸ªå…·ä½“æ¡ˆä¾‹ã€‚é¦–å…ˆï¼Œè‡ªæ³¨æ„åŠ›ä½¿å¾—åœ¨æ•´ä¸ªåºåˆ—çš„æ‰€æœ‰ä½ç½®ä¸Šå®ç°å¹¶è¡Œå¤„ç†ã€‚è¿™åœ¨åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„`Q`ã€`K`å’Œ`V`çš„çŸ©é˜µä¹˜æ³•ä¸­è¡¨ç°å¾—éå¸¸æ˜æ˜¾ã€‚ä¸æŒ‰é¡ºåºå¤„ç†è¾“å…¥çš„å¾ªç¯æ¶æ„ä¸åŒï¼Œè¿™ç§å¹¶è¡Œæ€§è´¨å…è®¸æ›´æœ‰æ•ˆçš„è®¡ç®—ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ºå¹¶è¡Œæ“ä½œè®¾è®¡çš„ç°ä»£ç¡¬ä»¶ä¸Šã€‚
- en: Second, the attention score computation results in a matrix of size `(seq_len
    Ã— seq_len)`, leading to quadratic complexity with respect to sequence length.
    This quadratic relationship becomes a significant computational bottleneck when
    processing long sequences, a challenge that has spurred research into more efficient
    attention mechanisms.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒï¼Œæ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ç»“æœæ˜¯ä¸€ä¸ªå¤§å°ä¸º `(seq_len Ã— seq_len)` çš„çŸ©é˜µï¼Œå¯¼è‡´åºåˆ—é•¿åº¦ä¸Šçš„äºŒæ¬¡å¤æ‚åº¦ã€‚å½“å¤„ç†é•¿åºåˆ—æ—¶ï¼Œè¿™ç§äºŒæ¬¡å…³ç³»æˆä¸ºä¸€ä¸ªæ˜¾è‘—çš„è®¡ç®—ç“¶é¢ˆï¼Œè¿™ä¹Ÿä¿ƒä½¿äººä»¬ç ”ç©¶æ›´æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: Third, the multi-head attention mechanism effectively runs multiple self-attention
    operations in parallel, each with its own set of learned projections. While this
    increases the computational load linearly with the number of heads, it allows
    the model to capture different types of relationships within the same input, enhancing
    the modelâ€™s representational power.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆåœ°å¹¶è¡Œè¿è¡Œå¤šä¸ªè‡ªæ³¨æ„åŠ›æ“ä½œï¼Œæ¯ä¸ªæ“ä½œéƒ½æœ‰å…¶è‡ªå·±çš„å­¦ä¹ æŠ•å½±é›†ã€‚è™½ç„¶è¿™ä¼šéšç€å¤´æ•°çš„å¢åŠ è€Œçº¿æ€§å¢åŠ è®¡ç®—è´Ÿè½½ï¼Œä½†å®ƒå…è®¸æ¨¡å‹æ•æ‰åˆ°ç›¸åŒè¾“å…¥ä¸­çš„ä¸åŒç±»å‹çš„å…³ç³»ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
- en: Fourth, the core computations in self-attention are dominated by large matrix
    multiplications. For a sequence of length <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    and embedding dimension <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>,
    the main operations involve matrices of sizes <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(N\times d)</annotation></semantics>, <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>d</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="true"
    form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(d\times
    d)</annotation></semantics>, and <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times
    N)</annotation></semantics>. These intensive matrix operations are well-suited
    for acceleration on specialized hardware like GPUs, but they also contribute significantly
    to the overall computational cost of the model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬å››ï¼Œè‡ªæ³¨æ„åŠ›ä¸­çš„æ ¸å¿ƒè®¡ç®—ä¸»è¦ç”±å¤§çŸ©é˜µä¹˜æ³•ç»„æˆã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º <semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>
    çš„åºåˆ—å’ŒåµŒå…¥ç»´åº¦ <semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>ï¼Œä¸»è¦æ“ä½œæ¶‰åŠå¤§å°ä¸º
    <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times
    d)</annotation></semantics>ã€<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>Ã—</mo><mi>d</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(d\times
    d)</annotation></semantics> å’Œ <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times
    N)</annotation></semantics> çš„çŸ©é˜µã€‚è¿™äº›å¯†é›†çš„çŸ©é˜µè¿ç®—éå¸¸é€‚åˆåœ¨ä¸“é—¨çš„ç¡¬ä»¶ï¼ˆå¦‚GPUï¼‰ä¸ŠåŠ é€Ÿï¼Œä½†å®ƒä»¬ä¹Ÿæ˜¾è‘—å¢åŠ äº†æ¨¡å‹çš„æ•´ä½“è®¡ç®—æˆæœ¬ã€‚
- en: Finally, self-attention generates memory-intensive intermediate results. The
    attention weights matrix <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times
    N)</annotation></semantics> and the intermediate results for each attention head
    create large memory requirements, especially for long sequences. This can pose
    challenges for deployment on memory-constrained devices and necessitates careful
    memory management in implementations.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè‡ªæ³¨æ„åŠ›ç”Ÿæˆäº†å†…å­˜å¯†é›†çš„ä¸­é—´ç»“æœã€‚æ³¨æ„åŠ›æƒé‡çŸ©é˜µ<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>N</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(N\times
    N)</annotation></semantics>ä»¥åŠæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ä¸­é—´ç»“æœåˆ›å»ºäº†å¤§é‡çš„å†…å­˜éœ€æ±‚ï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿åºåˆ—ã€‚è¿™å¯èƒ½åœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶æå‡ºæŒ‘æˆ˜ï¼Œå¹¶éœ€è¦åœ¨å®ç°ä¸­ä»”ç»†ç®¡ç†å†…å­˜ã€‚
- en: These computational patterns create a unique profile for Transformer self-attention,
    distinct from previous architectures. The parallel nature of the computations
    makes Transformers well-suited for modern parallel processing hardware, but the
    quadratic complexity with sequence length poses challenges for processing long
    sequences. As a result, much research has focused on developing optimization techniques,
    such as sparse attention patterns or low-rank approximations, to address these
    challenges. Each of these optimizations presents its own trade-offs between computational
    efficiency and model expressiveness, a balance that must be carefully considered
    in practical applications.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è®¡ç®—æ¨¡å¼ä¸ºTransformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åˆ›é€ äº†ä¸€ä¸ªç‹¬ç‰¹çš„è½®å»“ï¼Œä¸ä¹‹å‰çš„æ¶æ„ä¸åŒã€‚è®¡ç®—çš„å¹¶è¡Œæ€§ä½¿å¾—Transformeréå¸¸é€‚åˆç°ä»£å¹¶è¡Œå¤„ç†ç¡¬ä»¶ï¼Œä½†åºåˆ—é•¿åº¦çš„äºŒæ¬¡å¤æ‚æ€§å¯¹å¤„ç†é•¿åºåˆ—æå‡ºäº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œè®¸å¤šç ”ç©¶éƒ½é›†ä¸­åœ¨å¼€å‘ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼æˆ–ä½ç§©è¿‘ä¼¼ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æ¯ç§ä¼˜åŒ–éƒ½åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¹‹é—´æä¾›äº†è‡ªå·±çš„æƒè¡¡ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¿…é¡»ä»”ç»†è€ƒè™‘ã€‚
- en: This examination of four distinct architectural families reveals both their
    individual characteristics and their collective evolution. Rather than viewing
    these architectures in isolation, a deeper understanding emerges when we consider
    how they relate to each other and build upon shared foundations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å››ç§ä¸åŒæ¶æ„å®¶æ—çš„è€ƒå¯Ÿæ­ç¤ºäº†å®ƒä»¬çš„ä¸ªä½“ç‰¹å¾å’Œé›†ä½“æ¼”å˜ã€‚ä¸å…¶å°†è¿™äº›æ¶æ„å­¤ç«‹çœ‹å¾…ï¼Œä¸å¦‚å½“æˆ‘ä»¬è€ƒè™‘å®ƒä»¬ä¹‹é—´çš„å…³ç³»ä»¥åŠå®ƒä»¬å¦‚ä½•å»ºç«‹åœ¨å…±åŒåŸºç¡€ä¸Šæ—¶ï¼Œä¼šè·å¾—æ›´æ·±å…¥çš„ç†è§£ã€‚
- en: Architectural Building Blocks
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»ºç­‘æ¨¡å—
- en: Having examined four major architectural familiesâ€”MLPs, CNNs, RNNs, and Transformersâ€”each
    with distinct computational characteristics and system implications, a unifying
    perspective emerges. Deep learning architectures, while presented as distinct
    approaches in previous sections, are better understood as compositions of building
    blocks that evolved over time. Like complex LEGO structures built from basic bricks,
    modern neural networks combine and iterate on core computational patterns that
    emerged through decades of research ([Yann LeCun, Bengio, and Hinton 2015](ch058.xhtml#ref-lecun2015deep)).
    Each architectural innovation introduced new building blocks while discovering
    novel applications of existing ones.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒå¯Ÿäº†å››ä¸ªä¸»è¦æ¶æ„å®¶æ—â€”â€”MLPã€CNNã€RNNå’ŒTransformerâ€”â€”æ¯ä¸ªéƒ½å…·æœ‰ç‹¬ç‰¹çš„è®¡ç®—ç‰¹æ€§å’Œç³»ç»Ÿå½±å“åï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§’å‡ºç°äº†ã€‚è™½ç„¶åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæ·±åº¦å­¦ä¹ æ¶æ„è¢«ä½œä¸ºä¸åŒçš„æ–¹æ³•æå‡ºï¼Œä½†å®ƒä»¬æ›´å¥½åœ°è¢«ç†è§£ä¸ºéšç€æ—¶é—´çš„æ¨ç§»è€Œæ¼”å˜çš„æ¨¡å—ç»„åˆã€‚å°±åƒç”±åŸºæœ¬ç –å—æ„å»ºçš„å¤æ‚ä¹é«˜ç»“æ„ä¸€æ ·ï¼Œç°ä»£ç¥ç»ç½‘ç»œç»“åˆå¹¶è¿­ä»£äº†æ•°åå¹´æ¥ç ”ç©¶äº§ç”Ÿçš„æ ¸å¿ƒè®¡ç®—æ¨¡å¼([æ¨Â·å‹’å…‹æ–‡ã€æœ¬å‰å¥¥å’Œè¾›é¡¿
    2015](ch058.xhtml#ref-lecun2015deep))ã€‚æ¯ä¸€æ¬¡æ¶æ„åˆ›æ–°éƒ½å¼•å…¥äº†æ–°çš„æ¨¡å—ï¼ŒåŒæ—¶å‘ç°äº†ç°æœ‰æ¨¡å—çš„æ–°åº”ç”¨ã€‚
- en: These building blocks and their evolution illuminate modern architectural design.
    The simple perceptron ([Rosenblatt 1958](ch058.xhtml#ref-rosenblatt1958perceptron))
    evolved into multi-layer networks ([Rumelhart, Hinton, and Williams 1986](ch058.xhtml#ref-rumelhart1986learning)),
    which subsequently spawned specialized patterns for spatial and sequential processing.
    Each advancement preserved useful elements from predecessors while introducing
    new computational primitives. Contemporary architectures, such as Transformers,
    represent carefully engineered combinations of these building blocks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å—åŠå…¶æ¼”å˜è¿‡ç¨‹ç…§äº®äº†ç°ä»£å»ºç­‘è®¾è®¡ã€‚ç®€å•çš„æ„ŸçŸ¥å™¨([ç½—æ£®å¸ƒæ‹‰ç‰¹ 1958](ch058.xhtml#ref-rosenblatt1958perceptron))æ¼”å˜ä¸ºå¤šå±‚ç½‘ç»œ([é²æ¢…å°”å“ˆç‰¹ã€è¾›é¡¿å’Œå¨å»‰å§†æ–¯
    1986](ch058.xhtml#ref-rumelhart1986learning))ï¼Œéšååˆäº§ç”Ÿäº†ä¸“é—¨ç”¨äºç©ºé—´å’Œåºåˆ—å¤„ç†çš„æ¨¡å¼ã€‚æ¯ä¸€æ¬¡è¿›æ­¥éƒ½ä¿ç•™äº†å‰è¾ˆçš„æœ‰ç”¨å…ƒç´ ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°çš„è®¡ç®—åŸè¯­ã€‚å½“ä»£æ¶æ„ï¼Œå¦‚Transformerï¼Œä»£è¡¨äº†è¿™äº›æ¨¡å—ç²¾å¿ƒè®¾è®¡çš„ç»„åˆã€‚
- en: This progression reveals both the evolution of neural networks and the discovery
    and refinement of core computational patterns that remain relevant. Building on
    the architectural progression outlined in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de),
    each new architecture introduces distinct computational demands and system-level
    challenges.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€è¿›å±•æ­ç¤ºäº†ç¥ç»ç½‘ç»œçš„å‘å±•ä»¥åŠæ ¸å¿ƒè®¡ç®—æ¨¡å¼çš„å‘ç°å’Œæ”¹è¿›ï¼Œè¿™äº›æ¨¡å¼ä»ç„¶ç›¸å…³ã€‚åœ¨[ç¬¬4.1èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)ä¸­æ¦‚è¿°çš„æ¶æ„è¿›å±•çš„åŸºç¡€ä¸Šï¼Œæ¯ä¸ªæ–°çš„æ¶æ„éƒ½å¼•å…¥äº†ç‹¬ç‰¹çš„è®¡ç®—éœ€æ±‚å’Œç³»ç»Ÿçº§æŒ‘æˆ˜ã€‚
- en: '[TableÂ 4.1](ch010.xhtml#tbl-dl-evolution) summarizes this evolution, highlighting
    the key primitives and system focus for each era of deep learning development.
    This table captures the major shifts in deep learning architecture design and
    corresponding changes in system-level considerations. The progression spans from
    early dense matrix operations optimized for CPUs, through convolutions leveraging
    GPU acceleration and sequential operations necessitating sophisticated memory
    hierarchies, to the current era of attention mechanisms requiring flexible accelerators
    and high-bandwidth memory.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨4.1](ch010.xhtml#tbl-dl-evolution)æ€»ç»“äº†è¿™ä¸€æ¼”å˜ï¼Œçªå‡ºäº†æ¯ä¸ªæ·±åº¦å­¦ä¹ å‘å±•é˜¶æ®µçš„å…³é”®åŸè¯­å’Œç³»ç»Ÿå…³æ³¨ç‚¹ã€‚è¿™å¼ è¡¨æ•æ‰äº†æ·±åº¦å­¦ä¹ æ¶æ„è®¾è®¡çš„ä¸»è¦è½¬å˜ä»¥åŠç³»ç»Ÿçº§è€ƒè™‘çš„ç›¸åº”å˜åŒ–ã€‚è¿™ä¸€è¿›å±•æ¶µç›–äº†ä»é’ˆå¯¹CPUä¼˜åŒ–çš„æ—©æœŸå¯†é›†çŸ©é˜µæ“ä½œï¼Œé€šè¿‡åˆ©ç”¨GPUåŠ é€Ÿçš„å·ç§¯å’Œéœ€è¦å¤æ‚å†…å­˜å±‚æ¬¡ç»“æ„çš„é¡ºåºæ“ä½œï¼Œåˆ°å½“å‰éœ€è¦çµæ´»åŠ é€Ÿå™¨å’Œé«˜é€Ÿå†…å­˜çš„æ³¨æ„åŠ›æœºåˆ¶æ—¶ä»£ã€‚'
- en: 'TableÂ 4.1: **Deep Learning Evolution**: Neural network architectures have progressed
    from simple, fully connected layers to complex models leveraging specialized hardware
    and addressing sequential data dependencies. This table maps architectural eras
    to key computational primitives and corresponding system-level optimizations,
    revealing a historical trend toward increased parallelism and memory bandwidth
    requirements.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.1ï¼š**æ·±åº¦å­¦ä¹ æ¼”å˜**ï¼šç¥ç»ç½‘ç»œæ¶æ„å·²ä»ç®€å•çš„å…¨è¿æ¥å±‚å‘å±•åˆ°åˆ©ç”¨ä¸“ç”¨ç¡¬ä»¶å’Œè§£å†³é¡ºåºæ•°æ®ä¾èµ–çš„å¤æ‚æ¨¡å‹ã€‚è¿™å¼ è¡¨å°†æ¶æ„æ—¶ä»£æ˜ å°„åˆ°å…³é”®è®¡ç®—åŸè¯­å’Œç›¸åº”çš„ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œæ­ç¤ºäº†ä¸€ä¸ªå†å²è¶‹åŠ¿ï¼Œå³å‘æ›´é«˜çš„å¹¶è¡Œæ€§å’Œå†…å­˜å¸¦å®½éœ€æ±‚å‘å±•ã€‚
- en: '| **Era** | **Dominant Architecture** | **Key Primitives** | **System Focus**
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| **æ—¶ä»£** | **ä¸»å¯¼æ¶æ„** | **å…³é”®åŸè¯­** | **ç³»ç»Ÿå…³æ³¨ç‚¹** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Early NN** | MLP | Dense Matrix Ops | CPU optimization |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **æ—©æœŸç¥ç»ç½‘ç»œ** | MLP | ç¨ å¯†çŸ©é˜µæ“ä½œ | CPUä¼˜åŒ– |'
- en: '| **CNN Revolution** | CNN | Convolutions | GPU acceleration |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **CNNé©å‘½** | CNN | å·ç§¯ | GPUåŠ é€Ÿ |'
- en: '| **Sequence Modeling** | RNN | Sequential Ops | Memory hierarchies |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| **åºåˆ—å»ºæ¨¡** | RNN | åºåˆ—æ“ä½œ | å†…å­˜å±‚æ¬¡ç»“æ„ |'
- en: '| **Attention Era** | Transformer | Attention, Dynamic Compute | Flexible accelerators,
    High-bandwidth memory |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| **æ³¨æ„åŠ›æ—¶ä»£** | Transformer | æ³¨æ„åŠ›ï¼ŒåŠ¨æ€è®¡ç®— | çµæ´»çš„åŠ é€Ÿå™¨ï¼Œé«˜é€Ÿå†…å­˜ |'
- en: Examination of these building blocks shows primitives evolving and combining
    to create increasingly powerful neural network architectures.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹è¿™äº›æ„å»ºå—çš„è€ƒå¯Ÿæ˜¾ç¤ºï¼ŒåŸè¯­åœ¨æ¼”å˜å’Œç»„åˆä¸­åˆ›é€ è¶Šæ¥è¶Šå¼ºå¤§çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚
- en: Evolution from Perceptron to Multi-Layer Networks
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»æ„ŸçŸ¥å™¨åˆ°å¤šå±‚ç½‘ç»œçš„æ¼”å˜
- en: 'While we examined MLPs in [SectionÂ 4.2](ch010.xhtml#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f)
    as a mechanism for dense pattern processing, here we focus on how they established
    building blocks that appear throughout deep learning. The evolution from perceptron
    to MLP introduced several key concepts: the power of layer stacking, the importance
    of non-linear transformations, and the basic feedforward computation pattern.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬åœ¨[ç¬¬4.2èŠ‚](ch010.xhtml#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f)ä¸­è€ƒå¯Ÿäº†MLPä½œä¸ºå¯†é›†æ¨¡å¼å¤„ç†æœºåˆ¶çš„æœºåˆ¶ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯å®ƒä»¬å¦‚ä½•å»ºç«‹äº†è´¯ç©¿æ·±åº¦å­¦ä¹ çš„æ„å»ºå—ã€‚ä»æ„ŸçŸ¥å™¨åˆ°MLPçš„æ¼”å˜å¼•å…¥äº†å‡ ä¸ªå…³é”®æ¦‚å¿µï¼šå±‚å †å çš„åŠ›é‡ã€éçº¿æ€§è½¬æ¢çš„é‡è¦æ€§ä»¥åŠåŸºæœ¬çš„æ­£å‘è®¡ç®—æ¨¡å¼ã€‚
- en: The introduction of hidden layers between input and output created a template
    for feature transformation that appears in virtually every modern architecture.
    Even in sophisticated networks like Transformers, we find MLP-style feedforward
    layers performing feature processing. The concept of transforming data through
    successive non-linear layers has become a paradigm that transcends specific architecture
    types.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å¼•å…¥éšè—å±‚ï¼Œä¸ºå‡ ä¹æ¯ä¸ªç°ä»£æ¶æ„ä¸­å‡ºç°çš„ç‰¹å¾è½¬æ¢åˆ›å»ºäº†ä¸€ä¸ªæ¨¡æ¿ã€‚å³ä½¿åœ¨åƒTransformerè¿™æ ·çš„å¤æ‚ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½æ‰¾åˆ°æ‰§è¡Œç‰¹å¾å¤„ç†çš„MLPé£æ ¼çš„å‰é¦ˆå±‚ã€‚é€šè¿‡è¿ç»­çš„éçº¿æ€§å±‚è½¬æ¢æ•°æ®çš„æ¦‚å¿µå·²ç»æˆä¸ºè¶…è¶Šç‰¹å®šæ¶æ„ç±»å‹çš„èŒƒå¼ã€‚
- en: Most significantly, the development of MLPs established the backpropagation
    algorithm[22](#fn22), which to this day remains the cornerstone of neural network
    optimization. This key contribution has enabled the development of deep architectures
    and influenced how later architectures would be designed to maintain gradient
    flow.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€é‡è¦çš„æ˜¯ï¼ŒMLPsçš„å‘å±•ç¡®ç«‹äº†åå‘ä¼ æ’­ç®—æ³•[22](#fn22)ï¼Œè¿™ä¸€ç®—æ³•è‡³ä»Šä»ç„¶æ˜¯ç¥ç»ç½‘ç»œä¼˜åŒ–çš„åŸºçŸ³ã€‚è¿™ä¸€å…³é”®è´¡çŒ®ä½¿å¾—æ·±åº¦æ¶æ„çš„å‘å±•æˆä¸ºå¯èƒ½ï¼Œå¹¶å½±å“äº†åç»­æ¶æ„çš„è®¾è®¡ï¼Œä»¥ä¿æŒæ¢¯åº¦æµã€‚
- en: These building blocks, layered feature transformation, non-linear activation,
    and gradient-based learning, set the foundation for more specialized architectures.
    Subsequent innovations often focused on structuring these basic components in
    new ways rather than replacing them entirely.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ„å»ºå—ï¼ŒåŒ…æ‹¬åˆ†å±‚ç‰¹å¾è½¬æ¢ã€éçº¿æ€§æ¿€æ´»å’ŒåŸºäºæ¢¯åº¦çš„å­¦ä¹ ï¼Œä¸ºæ›´ä¸“ä¸šçš„æ¶æ„å¥ å®šäº†åŸºç¡€ã€‚åç»­çš„åˆ›æ–°é€šå¸¸é›†ä¸­åœ¨ä»¥æ–°çš„æ–¹å¼æ„å»ºè¿™äº›åŸºæœ¬ç»„ä»¶ï¼Œè€Œä¸æ˜¯å®Œå…¨å–ä»£å®ƒä»¬ã€‚
- en: Evolution from Dense to Spatial Processing
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»å¯†é›†å¤„ç†åˆ°ç©ºé—´å¤„ç†çš„æ¼”å˜
- en: The development of CNNs marked an architectural innovation, specifically the
    realization that we could specialize the dense connectivity of MLPs for spatial
    patterns. While retaining the core concept of layer-wise processing, CNNs introduced
    several building blocks that would influence all future architectures.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: CNNsçš„å‘å±•æ ‡å¿—ç€æ¶æ„åˆ›æ–°ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬èƒ½å¤Ÿä¸“é—¨åŒ–MLPsçš„å¯†é›†è¿æ¥ä»¥å¤„ç†ç©ºé—´æ¨¡å¼çš„è®¤è¯†ã€‚åœ¨ä¿ç•™å±‚çŠ¶å¤„ç†çš„æ ¸å¿ƒæ¦‚å¿µçš„åŒæ—¶ï¼ŒCNNså¼•å…¥äº†å‡ ä¸ªå°†å½±å“æ‰€æœ‰æœªæ¥æ¶æ„çš„æ„å»ºå—ã€‚
- en: The first key innovation was the concept of parameter sharing. Unlike MLPs where
    each connection had its own weight, CNNs showed how the same parameters could
    be reused across different parts of the input. This not only made the networks
    more efficient but introduced the powerful idea that architectural structure could
    encode useful priors about the data ([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient)).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯å‚æ•°å…±äº«çš„æ¦‚å¿µã€‚ä¸æ¯ä¸ªè¿æ¥éƒ½æœ‰è‡ªå·±çš„æƒé‡ä¸åŒï¼ŒCNNså±•ç¤ºäº†ç›¸åŒçš„å‚æ•°å¯ä»¥åœ¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†é‡å¤ä½¿ç”¨ã€‚è¿™ä¸ä»…ä½¿ç½‘ç»œæ›´é«˜æ•ˆï¼Œè¿˜å¼•å…¥äº†å¼ºå¤§çš„æ€æƒ³ï¼Œå³æ¶æ„ç»“æ„å¯ä»¥ç¼–ç å…³äºæ•°æ®çš„æœ‰ç”¨å…ˆéªŒä¿¡æ¯
    ([Lecun et al. 1998](ch058.xhtml#ref-lecun1998gradient))ã€‚
- en: Perhaps even more influential was the introduction of skip connections through
    ResNets[23](#fn23) ([K. He et al. 2015](ch058.xhtml#ref-he2016deep)). Originally
    they were designed to help train very deep CNNs, skip connections have become
    a building block that appears in virtually every modern architecture. They showed
    how direct paths through the network could help gradient flow and information
    propagation, a concept now central to Transformer designs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æ›´å…·å½±å“åŠ›çš„æ˜¯é€šè¿‡ResNets[23](#fn23) ([K. He et al. 2015](ch058.xhtml#ref-he2016deep))å¼•å…¥çš„è·³è·ƒè¿æ¥ã€‚æœ€åˆå®ƒä»¬è¢«è®¾è®¡ç”¨æ¥å¸®åŠ©è®­ç»ƒéå¸¸æ·±çš„CNNsï¼Œè·³è·ƒè¿æ¥å·²ç»æˆä¸ºå‡ ä¹æ¯ä¸ªç°ä»£æ¶æ„ä¸­å‡ºç°çš„æ„å»ºå—ã€‚å®ƒä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç½‘ç»œä¸­çš„ç›´æ¥è·¯å¾„å¸®åŠ©æ¢¯åº¦æµå’Œä¿¡æ¯ä¼ æ’­ï¼Œè¿™ä¸€æ¦‚å¿µç°åœ¨å·²æˆä¸ºTransformerè®¾è®¡ä¸­çš„æ ¸å¿ƒã€‚
- en: CNNs also introduced batch normalization, a technique for stabilizing neural
    network optimization by normalizing intermediate features ([Ioffe and Szegedy
    2015a](ch058.xhtml#ref-ioffe2015batch)). This concept of feature normalization,
    while originating in CNNs, evolved into layer normalization and is now a key component
    in modern architectures.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: CNNsè¿˜å¼•å…¥äº†æ‰¹é‡å½’ä¸€åŒ–ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å½’ä¸€åŒ–ä¸­é—´ç‰¹å¾æ¥ç¨³å®šç¥ç»ç½‘ç»œä¼˜åŒ–çš„æŠ€æœ¯ ([Ioffe and Szegedy 2015a](ch058.xhtml#ref-ioffe2015batch))ã€‚è¿™ç§ç‰¹å¾å½’ä¸€åŒ–çš„æ¦‚å¿µï¼Œè™½ç„¶èµ·æºäºCNNsï¼Œä½†æ¼”å˜æˆäº†å±‚å½’ä¸€åŒ–ï¼Œç°åœ¨å·²æˆä¸ºç°ä»£æ¶æ„çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚
- en: These innovations, such as parameter sharing, skip connections, and normalization,
    transcended their origins in spatial processing to become essential building blocks
    in the deep learning toolkit.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åˆ›æ–°ï¼Œå¦‚å‚æ•°å…±äº«ã€è·³è·ƒè¿æ¥å’Œå½’ä¸€åŒ–ï¼Œè¶…è¶Šäº†å®ƒä»¬åœ¨ç©ºé—´å¤„ç†ä¸­çš„èµ·æºï¼Œæˆä¸ºæ·±åº¦å­¦ä¹ å·¥å…·ç®±ä¸­çš„åŸºæœ¬æ„å»ºå—ã€‚
- en: Evolution of Sequence Processing
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åºåˆ—å¤„ç†çš„å‘å±•
- en: While CNNs specialized MLPs for spatial patterns, sequence models adapted neural
    networks for temporal dependencies. RNNs introduced the concept of maintaining
    and updating state, a building block that influenced how networks could process
    sequential information, ([Elman 1990](ch058.xhtml#ref-elman1990finding)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: å½“CNNsä¸“é—¨ç”¨äºç©ºé—´æ¨¡å¼çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰æ—¶ï¼Œåºåˆ—æ¨¡å‹é€‚åº”äº†ç¥ç»ç½‘ç»œä»¥å¤„ç†æ—¶é—´ä¾èµ–æ€§ã€‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰å¼•å…¥äº†ç»´æŒå’Œæ›´æ–°çŠ¶æ€çš„æ¦‚å¿µï¼Œè¿™æ˜¯å½±å“ç½‘ç»œå¦‚ä½•å¤„ç†åºåˆ—ä¿¡æ¯çš„ä¸€ä¸ªæ„å»ºå—ï¼Œ([Elman
    1990](ch058.xhtml#ref-elman1990finding))ã€‚
- en: The development of LSTMs[24](#fn24) and GRUs[25](#fn25) brought sophisticated
    gating mechanisms to neural networks ([Hochreiter and Schmidhuber 1997](ch058.xhtml#ref-hochreiter1997long);
    [Cho et al. 2014](ch058.xhtml#ref-cho2014properties)). These gates, themselves
    small MLPs, showed how simple feedforward computations could be composed to control
    information flow. This concept of using neural networks to modulate other neural
    networks became a recurring pattern in architecture design.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM[24](#fn24)å’ŒGRU[25](#fn25)çš„å‘å±•å°†å¤æ‚çš„é—¨æ§æœºåˆ¶å¼•å…¥äº†ç¥ç»ç½‘ç»œï¼ˆ[Hochreiter å’Œ Schmidhuber 1997](ch058.xhtml#ref-hochreiter1997long)ï¼›[Cho
    ç­‰äºº 2014](ch058.xhtml#ref-cho2014properties)ï¼‰ã€‚è¿™äº›é—¨æ§æœºåˆ¶ï¼Œæœ¬èº«æ˜¯å°å‹MLPï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç®€å•çš„å‰é¦ˆè®¡ç®—æ¥ç»„åˆä»¥æ§åˆ¶ä¿¡æ¯æµã€‚è¿™ç§ä½¿ç”¨ç¥ç»ç½‘ç»œè°ƒèŠ‚å…¶ä»–ç¥ç»ç½‘ç»œçš„æ¦‚å¿µæˆä¸ºæ¶æ„è®¾è®¡ä¸­çš„å¸¸è§æ¨¡å¼ã€‚
- en: Perhaps most significantly, sequence models demonstrated the power of adaptive
    computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how
    networks could process variable-length inputs by reusing weights over time. This
    insight, that architectural patterns could adapt to input structure, laid groundwork
    for more flexible architectures.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœ€æ˜¾è‘—çš„æ˜¯ï¼Œåºåˆ—æ¨¡å‹å±•ç¤ºäº†è‡ªé€‚åº”è®¡ç®—è·¯å¾„çš„åŠ›é‡ã€‚ä¸MLPså’ŒCNNsçš„å›ºå®šæ¨¡å¼ä¸åŒï¼ŒRNNså±•ç¤ºäº†ç½‘ç»œå¦‚ä½•é€šè¿‡åœ¨æ—¶é—´ä¸Šé‡å¤ä½¿ç”¨æƒé‡æ¥å¤„ç†å¯å˜é•¿åº¦çš„è¾“å…¥ã€‚è¿™ä¸€æ´å¯Ÿï¼Œå³æ¶æ„æ¨¡å¼å¯ä»¥é€‚åº”è¾“å…¥ç»“æ„ï¼Œä¸ºæ›´çµæ´»çš„æ¶æ„å¥ å®šäº†åŸºç¡€ã€‚
- en: Sequence models also popularized the concept of attention through encoder-decoder
    architectures ([Bahdanau, Cho, and Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)).
    Initially introduced as an improvement to machine translation, attention mechanisms
    showed how networks could learn to dynamically focus on relevant information.
    This building block would later become the foundation of Transformer architectures.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: åºåˆ—æ¨¡å‹é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ˆ[Bahdanau, Cho, å’Œ Bengio 2014](ch058.xhtml#ref-bahdanau2014neural)ï¼‰ä¹Ÿæ™®åŠäº†æ³¨æ„åŠ›æ¦‚å¿µã€‚æœ€åˆä½œä¸ºæœºå™¨ç¿»è¯‘çš„æ”¹è¿›è€Œå¼•å…¥ï¼Œæ³¨æ„åŠ›æœºåˆ¶å±•ç¤ºäº†ç½‘ç»œå¦‚ä½•å­¦ä¹ åŠ¨æ€å…³æ³¨ç›¸å…³ä¿¡æ¯ã€‚è¿™ä¸ªæ„å»ºå—åæ¥æˆä¸ºTransformeræ¶æ„çš„åŸºç¡€ã€‚
- en: 'Modern Architectures: Synthesis and Unification'
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç°ä»£æ¶æ„ï¼šç»¼åˆä¸ç»Ÿä¸€
- en: 'Modern architectures, particularly Transformers, represent a sophisticated
    synthesis of these fundamental building blocks. Rather than introducing entirely
    new patterns, they innovate through strategic combination and refinement of existing
    components. The Transformer architecture exemplifies this approach: at its core,
    MLP-style feedforward networks process features between attention layers. The
    attention mechanism itself builds on sequence model concepts while eliminating
    recurrent connections, instead employing position embeddings inspired by CNN intuitions.
    The architecture extensively utilizes skip connections (see [FigureÂ 4.9](ch010.xhtml#fig-example-skip-connection)),
    inherited from ResNets, while layer normalization, evolved from CNN batch normalization,
    stabilizes optimization ([Ba, Kiros, and Hinton 2016](ch058.xhtml#ref-ba2016layer)).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ¶æ„ï¼Œå°¤å…¶æ˜¯Transformerï¼Œä»£è¡¨äº†è¿™äº›åŸºæœ¬æ„å»ºå—çš„é«˜çº§ç»¼åˆã€‚å®ƒä»¬å¹¶éå¼•å…¥å…¨æ–°çš„æ¨¡å¼ï¼Œè€Œæ˜¯é€šè¿‡æˆ˜ç•¥æ€§åœ°ç»„åˆå’Œæ”¹è¿›ç°æœ‰ç»„ä»¶è¿›è¡Œåˆ›æ–°ã€‚Transformeræ¶æ„å°±æ˜¯è¿™ä¸€æ–¹æ³•çš„ä¾‹è¯ï¼šåœ¨å…¶æ ¸å¿ƒï¼ŒMLPé£æ ¼çš„å‰é¦ˆç½‘ç»œåœ¨æ³¨æ„åŠ›å±‚ä¹‹é—´å¤„ç†ç‰¹å¾ã€‚æ³¨æ„åŠ›æœºåˆ¶æœ¬èº«å»ºç«‹åœ¨åºåˆ—æ¨¡å‹æ¦‚å¿µä¹‹ä¸Šï¼ŒåŒæ—¶æ¶ˆé™¤äº†å¾ªç¯è¿æ¥ï¼Œè½¬è€Œé‡‡ç”¨å—CNNç›´è§‰å¯å‘çš„ä½ç½®åµŒå…¥ã€‚è¯¥æ¶æ„å¹¿æ³›åˆ©ç”¨è·³è·ƒè¿æ¥ï¼ˆè§[å›¾4.9](ch010.xhtml#fig-example-skip-connection)ï¼‰ï¼Œè¿™äº›è¿æ¥æ˜¯ä»ResNetsç»§æ‰¿è€Œæ¥çš„ï¼Œè€Œå±‚å½’ä¸€åŒ–ï¼ˆä»CNNæ‰¹å½’ä¸€åŒ–æ¼”å˜è€Œæ¥ï¼‰åˆ™ç¨³å®šäº†ä¼˜åŒ–è¿‡ç¨‹ï¼ˆ[Ba,
    Kiros, å’Œ Hinton 2016](ch058.xhtml#ref-ba2016layer)ï¼‰ã€‚
- en: '![](../media/file62.svg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file62.svg)'
- en: 'FigureÂ 4.9: **Residual Connection**: Skip connections add the input of a layer
    to its output, enabling gradients to flow directly through the network and mitigating
    the vanishing gradient problem in deep architectures. This allows training of
    significantly deeper networks, as seen in resnets and adopted in modern transformer
    architectures to improve optimization and performance.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.9ï¼š**æ®‹å·®è¿æ¥**ï¼šè·³è·ƒè¿æ¥å°†å±‚çš„è¾“å…¥æ·»åŠ åˆ°å…¶è¾“å‡ºä¸­ï¼Œä½¿å¾—æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡ç½‘ç»œæµåŠ¨ï¼Œç¼“è§£äº†æ·±å±‚æ¶æ„ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚è¿™å…è®¸è®­ç»ƒæ·±åº¦æ›´å¤§çš„ç½‘ç»œï¼Œæ­£å¦‚åœ¨resnetsä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œå¹¶è¢«ç°ä»£Transformeræ¶æ„é‡‡ç”¨ä»¥æ”¹å–„ä¼˜åŒ–å’Œæ€§èƒ½ã€‚
- en: This composition of building blocks creates emergent capabilities exceeding
    the sum of individual components. The self-attention mechanism, while building
    on previous attention concepts, enables novel forms of dynamic pattern processing.
    The arrangement of these componentsâ€”attention followed by feedforward layers,
    with skip connections and normalizationâ€”has proven sufficiently effective to become
    a template for new architectures.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ„å»ºå—çš„ç»„åˆåˆ›é€ äº†è¶…è¶Šå•ä¸ªç»„ä»¶æ€»å’Œçš„æ¶Œç°èƒ½åŠ›ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å…ˆå‰æ³¨æ„åŠ›æ¦‚å¿µçš„åŸºç¡€ä¸Šï¼Œå®ç°äº†æ–°å‹åŠ¨æ€æ¨¡å¼å¤„ç†ã€‚è¿™äº›ç»„ä»¶çš„æ’åˆ—â€”â€”æ³¨æ„åŠ›æœºåˆ¶åè·Ÿå‰é¦ˆå±‚ï¼Œå¸¦æœ‰è·³è¿‡è¿æ¥å’Œå½’ä¸€åŒ–â€”â€”å·²è¢«è¯æ˜è¶³å¤Ÿæœ‰æ•ˆï¼Œæˆä¸ºæ–°æ¶æ„çš„æ¨¡æ¿ã€‚
- en: Recent innovations in vision and language models follow this pattern of recombining
    building blocks. Vision Transformers[26](#fn26) adapt the Transformer architecture
    to images while maintaining its essential components ([Dosovitskiy et al. 2021](ch058.xhtml#ref-dosovitskiy2021image)).
    Large language models scale up these patterns while introducing refinements like
    grouped-query attention or sliding window attention, yet still rely on the core
    building blocks established through this architectural evolution ([T. Brown et
    al. 2020](ch058.xhtml#ref-brown2020language)). These modern architectural innovations
    demonstrate the principles of efficient scaling covered in [ChapterÂ 9](ch015.xhtml#sec-efficient-ai),
    while their practical implementation challenges and optimizations are explored
    in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰å’Œè¯­è¨€æ¨¡å‹ä¸­çš„æœ€è¿‘åˆ›æ–°éµå¾ªè¿™ç§é‡æ–°ç»„åˆæ„å»ºå—çš„æ¨¡å¼ã€‚è§†è§‰Transformer[26](#fn26)å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒï¼ŒåŒæ—¶ä¿æŒå…¶åŸºæœ¬ç»„ä»¶([Dosovitskiyç­‰äºº2021](ch058.xhtml#ref-dosovitskiy2021image))ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•äº†è¿™äº›æ¨¡å¼ï¼ŒåŒæ—¶å¼•å…¥äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æˆ–æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ç­‰æ”¹è¿›ï¼Œä½†ä»ç„¶ä¾èµ–äºé€šè¿‡è¿™ç§æ¶æ„æ¼”å˜å»ºç«‹çš„æ ¸å¿ƒç†å¿µ([T.
    Brownç­‰äºº2020](ch058.xhtml#ref-brown2020language))ã€‚è¿™äº›ç°ä»£æ¶æ„åˆ›æ–°å±•ç¤ºäº†[ç¬¬9ç« ](ch015.xhtml#sec-efficient-ai)ä¸­æ¶µç›–çš„é«˜æ•ˆæ‰©å±•åŸåˆ™ï¼Œè€Œå…¶å®é™…å®æ–½æŒ‘æˆ˜å’Œä¼˜åŒ–å°†åœ¨[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)ä¸­æ¢è®¨ã€‚
- en: 'The following comparison of primitive utilization across different neural network
    architectures shows modern architectures synthesizing and innovating upon previous
    approaches:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢å¯¹ä¸åŒç¥ç»ç½‘ç»œæ¶æ„ä¸­åŸå§‹åˆ©ç”¨ç‡çš„æ¯”è¾ƒæ˜¾ç¤ºäº†ç°ä»£æ¶æ„åœ¨å…ˆå‰æ–¹æ³•çš„åŸºç¡€ä¸Šç»¼åˆå’Œåˆ›æ–°ï¼š
- en: 'TableÂ 4.2: **Primitive Utilization**: Neural network architectures differ in
    their core computational and memory access patterns, impacting hardware requirements
    and efficiency. Transformers uniquely combine matrix multiplication with attention
    mechanisms, resulting in random memory access and data movement patterns distinct
    from sequential rnns or strided cnns.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.2ï¼š**åŸå§‹åˆ©ç”¨ç‡**ï¼šç¥ç»ç½‘ç»œæ¶æ„åœ¨æ ¸å¿ƒè®¡ç®—å’Œå†…å­˜è®¿é—®æ¨¡å¼ä¸Šæœ‰æ‰€ä¸åŒï¼Œè¿™å½±å“äº†ç¡¬ä»¶éœ€æ±‚å’Œæ•ˆç‡ã€‚Transformerç‹¬ç‰¹åœ°å°†çŸ©é˜µä¹˜æ³•ä¸æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œå¯¼è‡´éšæœºçš„å†…å­˜è®¿é—®å’Œæ•°æ®ç§»åŠ¨æ¨¡å¼ï¼Œä¸é¡ºåºçš„RNNæˆ–æ­¥è¿›CNNä¸åŒã€‚
- en: '| **Primitive Type** | **MLP** | **CNN** | **RNN** | **Transformer** |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **åŸå§‹ç±»å‹** | **MLP** | **CNN** | **RNN** | **Transformer** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Computational** | Matrix Multiplication | Convolution (Matrix Mult.) |
    Matrix Mult. + State Update | Matrix Mult. + Attention |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **è®¡ç®—** | çŸ©é˜µä¹˜æ³• | å·ç§¯ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ | çŸ©é˜µä¹˜æ³• + çŠ¶æ€æ›´æ–° | çŸ©é˜µä¹˜æ³• + æ³¨æ„åŠ› |'
- en: '| **Memory Access** | Sequential | Strided | Sequential + Random | Random (Attention)
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜è®¿é—®** | é¡ºåº | æ­¥è¿› | é¡ºåº + éšæœº | éšæœºï¼ˆæ³¨æ„åŠ›ï¼‰ |'
- en: '| **Data Movement** | Broadcast | Sliding Window | Sequential | Broadcast +
    Gather |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **æ•°æ®ç§»åŠ¨** | å¹¿æ’­ | æ»‘åŠ¨çª—å£ | é¡ºåº | å¹¿æ’­ + æ”¶é›† |'
- en: As shown in [TableÂ 4.2](ch010.xhtml#tbl-primitive-comparison), Transformers
    combine elements from previous architectures while introducing new patterns. They
    retain the core matrix multiplication operations common to all architectures but
    introduce a more complex memory access pattern with their attention mechanism.
    Their data movement patterns blend the broadcast operations of MLPs with the gather
    operations reminiscent of more dynamic architectures.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[è¡¨4.2](ch010.xhtml#tbl-primitive-comparison)æ‰€ç¤ºï¼ŒTransformerç»“åˆäº†å…ˆå‰æ¶æ„çš„å…ƒç´ ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°çš„æ¨¡å¼ã€‚å®ƒä»¬ä¿ç•™äº†æ‰€æœ‰æ¶æ„å…±æœ‰çš„æ ¸å¿ƒçŸ©é˜µä¹˜æ³•æ“ä½œï¼Œä½†é€šè¿‡å…¶æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥äº†æ›´å¤æ‚çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚å®ƒä»¬çš„æ•°æ®ç§»åŠ¨æ¨¡å¼èåˆäº†MLPçš„å¹¿æ’­æ“ä½œå’Œç±»ä¼¼æ›´åŠ¨æ€æ¶æ„çš„æ”¶é›†æ“ä½œã€‚
- en: This synthesis of primitives in Transformers shows modern architectures innovating
    by recombining and refining existing building blocks from the architectural progression
    established in [SectionÂ 4.1](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de),
    rather than inventing entirely new computational paradigms. This evolutionary
    process guides the development of future architectures and helps design of efficient
    systems to support them.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åœ¨Transformersä¸­å¯¹åŸè¯­çš„åˆæˆè¡¨æ˜ï¼Œç°ä»£æ¶æ„é€šè¿‡é‡æ–°ç»„åˆå’Œæ”¹è¿›åœ¨[ç¬¬4.1èŠ‚](ch010.xhtml#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de)ä¸­ç¡®ç«‹çš„ç°æœ‰æ„å»ºå—ï¼Œè€Œä¸æ˜¯å‘æ˜å…¨æ–°çš„è®¡ç®—èŒƒå¼ï¼Œæ¥è¿›è¡Œåˆ›æ–°ã€‚è¿™ä¸€è¿›åŒ–è¿‡ç¨‹æŒ‡å¯¼äº†æœªæ¥æ¶æ„çš„å‘å±•ï¼Œå¹¶æœ‰åŠ©äºè®¾è®¡æ”¯æŒå®ƒä»¬çš„æ•ˆç‡ç³»ç»Ÿã€‚
- en: System-Level Building Blocks
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿçº§æ„å»ºå—
- en: Examination of different deep learning architectures enables distillation of
    their system requirements into primitives that underpin both hardware and software
    implementations. These primitives represent operations that cannot be decomposed
    further while maintaining their essential characteristics. Just as complex molecules
    are built from basic atoms, sophisticated neural networks are constructed from
    these operations.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¸åŒæ·±åº¦å­¦ä¹ æ¶æ„çš„è€ƒå¯Ÿä½¿å¾—å¯ä»¥å°†å®ƒä»¬çš„ç³»ç»Ÿéœ€æ±‚æç‚¼ä¸ºæ”¯æ’‘ç¡¬ä»¶å’Œè½¯ä»¶å®ç°çš„åŸè¯­ã€‚è¿™äº›åŸè¯­ä»£è¡¨äº†ä¸€äº›æ“ä½œï¼Œåœ¨ä¿æŒå…¶åŸºæœ¬ç‰¹å¾çš„åŒæ—¶æ— æ³•è¿›ä¸€æ­¥åˆ†è§£ã€‚æ­£å¦‚å¤æ‚åˆ†å­æ˜¯ç”±åŸºæœ¬åŸå­æ„å»ºè€Œæˆï¼Œå¤æ‚çš„ç¥ç»ç½‘ç»œä¹Ÿæ˜¯ç”±è¿™äº›æ“ä½œæ„å»ºè€Œæˆã€‚
- en: Core Computational Primitives
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒè®¡ç®—åŸè¯­
- en: 'Three operations serve as the building blocks for all deep learning computations:
    matrix multiplication, sliding window operations, and dynamic computation. These
    operations are primitive because they cannot be further decomposed without losing
    their essential computational properties and efficiency characteristics.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ç§æ“ä½œæ˜¯æ‰€æœ‰æ·±åº¦å­¦ä¹ è®¡ç®—çš„åŸºç¡€æ„å»ºå—ï¼šçŸ©é˜µä¹˜æ³•ã€æ»‘åŠ¨çª—å£æ“ä½œå’ŒåŠ¨æ€è®¡ç®—ã€‚è¿™äº›æ“ä½œæ˜¯åŸè¯­ï¼Œå› ä¸ºå¦‚æœä¸å¤±å»å…¶åŸºæœ¬çš„è®¡ç®—å±æ€§å’Œæ•ˆç‡ç‰¹æ€§ï¼Œå°±æ— æ³•è¿›ä¸€æ­¥åˆ†è§£ã€‚
- en: 'Matrix multiplication represents the basic form of transforming sets of features.
    When we multiply a matrix of inputs by a matrix of weights, weâ€™re computing weighted
    combinations, which is the core operation of neural networks. For example, in
    our MNIST network, each 784-dimensional input vector multiplies with a <semantics><mrow><mn>784</mn><mo>Ã—</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">784\times 100</annotation></semantics> weight matrix.
    This pattern appears everywhere: MLPs use it directly for layer computations,
    CNNs reshape convolutions into matrix multiplications (turning a <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> convolution into
    a matrix operation, as illustrated in [FigureÂ 4.10](ch010.xhtml#fig-im2col-diagram)),
    and Transformers use it extensively in their attention mechanisms.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µä¹˜æ³•ä»£è¡¨äº†ç‰¹å¾é›†è½¬æ¢çš„åŸºæœ¬å½¢å¼ã€‚å½“æˆ‘ä»¬ç”¨ä¸€ä¸ªè¾“å…¥çŸ©é˜µä¹˜ä»¥ä¸€ä¸ªæƒé‡çŸ©é˜µæ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨è®¡ç®—åŠ æƒç»„åˆï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ“ä½œã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„MNISTç½‘ç»œä¸­ï¼Œæ¯ä¸ª784ç»´è¾“å…¥å‘é‡ä¸ä¸€ä¸ª<semantics><mrow><mn>784</mn><mo>Ã—</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">784\times 100</annotation></semantics>æƒé‡çŸ©é˜µç›¸ä¹˜ã€‚è¿™ç§æ¨¡å¼æ— å¤„ä¸åœ¨ï¼šMLPsç›´æ¥ç”¨å®ƒè¿›è¡Œå±‚è®¡ç®—ï¼ŒCNNså°†å·ç§¯é‡å¡‘ä¸ºçŸ©é˜µä¹˜æ³•ï¼ˆå°†ä¸€ä¸ª<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>å·ç§¯è½¬æ¢ä¸ºçŸ©é˜µæ“ä½œï¼Œå¦‚å›¾[å›¾4.10](ch010.xhtml#fig-im2col-diagram)æ‰€ç¤ºï¼‰ï¼Œå¹¶ä¸”Transformersåœ¨å…¶æ³¨æ„åŠ›æœºåˆ¶ä¸­å¹¿æ³›ä½¿ç”¨å®ƒã€‚
- en: Computational Building Blocks
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ„å»ºå—
- en: Modern neural networks operate through three computational patterns that appear
    across all architectures. These patterns explain how different architectures achieve
    their computational goals and why certain hardware optimizations are effective.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£ç¥ç»ç½‘ç»œé€šè¿‡ä¸‰ç§è®¡ç®—æ¨¡å¼åœ¨æ‰€æœ‰æ¶æ„ä¸­è¿è¡Œã€‚è¿™äº›æ¨¡å¼è§£é‡Šäº†ä¸åŒæ¶æ„å¦‚ä½•å®ç°å…¶è®¡ç®—ç›®æ ‡ï¼Œä»¥åŠä¸ºä»€ä¹ˆæŸäº›ç¡¬ä»¶ä¼˜åŒ–æ˜¯æœ‰æ•ˆçš„ã€‚
- en: The detailed analysis of sparse computation patterns, including structured and
    unstructured sparsity, hardware-aware optimization strategies, and algorithm-hardware
    co-design principles, is addressed in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations)
    and [ChapterÂ 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç¨€ç–è®¡ç®—æ¨¡å¼çš„è¯¦ç»†åˆ†æï¼ŒåŒ…æ‹¬ç»“æ„åŒ–å’Œéç»“æ„åŒ–ç¨€ç–æ€§ã€ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–ç­–ç•¥å’Œç®—æ³•-ç¡¬ä»¶ååŒè®¾è®¡åŸåˆ™ï¼Œåœ¨[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)å’Œ[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ä¸­è®¨è®ºã€‚
- en: '![](../media/file63.svg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file63.svg)'
- en: 'FigureÂ 4.10: **Convolution as Matrix Multiplication**: Reshaping convolutional
    layers into matrix multiplications using the `im2col` technique, enables efficient
    computation using optimized BLAS libraries and allows for parallel processing
    on standard hardware. This transformation is crucial for accelerating cnns and
    forms the basis for implementing convolutions on diverse platforms.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.10ï¼š**å·ç§¯ä½œä¸ºçŸ©é˜µä¹˜æ³•**ï¼šä½¿ç”¨`im2col`æŠ€æœ¯å°†å·ç§¯å±‚é‡å¡‘ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨ä¼˜åŒ–çš„BLASåº“è¿›è¡Œè®¡ç®—ï¼Œå¹¶å…è®¸åœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šè¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚è¿™ç§è½¬æ¢å¯¹äºåŠ é€Ÿå·ç§¯ç¥ç»ç½‘ç»œè‡³å…³é‡è¦ï¼Œå¹¶æ„æˆäº†åœ¨å¤šç§å¹³å°ä¸Šå®ç°å·ç§¯çš„åŸºç¡€ã€‚
- en: The im2col[27](#fn27) (image to column) technique, developed by Intel in the
    1990s, accomplishes matrix reshaping by unfolding overlapping image patches into
    columns of a matrix, as illustrated in [FigureÂ 4.10](ch010.xhtml#fig-im2col-diagram).
    Each sliding window position in the convolution becomes a column in the transformed
    matrix, while the filter kernels are arranged as rows. This allows the convolution
    operation to be expressed as a standard GEMM (General Matrix Multiply) operation.
    The transformation trades memory consumptionâ€”duplicating data where windows overlapâ€”for
    computational efficiency, enabling CNNs to leverage decades of BLAS optimizations
    and achieving 5-10x speedups on CPUs. In modern systems, these matrix multiplications
    map to specific hardware and software implementations. Hardware accelerators provide
    specialized tensor cores that can perform thousands of multiply-accumulates in
    parallel; NVIDIAâ€™s A100 tensor cores can achieve up to 312 TFLOPS for mixed-precision
    (TF32) workloads, or 156 TFLOPS for FP32 through massive parallelization of these
    operations. Software frameworks like PyTorch and TensorFlow automatically map
    these high-level operations to optimized matrix libraries (NVIDIA [cuBLAS](https://developer.nvidia.com/cublas),
    Intel [MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve))
    that exploit these hardware capabilities.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: im2col[27](#fn27)ï¼ˆå›¾åƒåˆ°åˆ—ï¼‰æŠ€æœ¯ï¼Œç”±è‹±ç‰¹å°”åœ¨20ä¸–çºª90å¹´ä»£å¼€å‘ï¼Œé€šè¿‡å±•å¼€é‡å çš„å›¾åƒå—åˆ°çŸ©é˜µçš„åˆ—ä¸­æ¥å®ç°çŸ©é˜µé‡å¡‘ï¼Œå¦‚å›¾[å›¾4.10](ch010.xhtml#fig-im2col-diagram)æ‰€ç¤ºã€‚å·ç§¯ä¸­çš„æ¯ä¸ªæ»‘åŠ¨çª—å£ä½ç½®åœ¨è½¬æ¢çŸ©é˜µä¸­æˆä¸ºä¸€åˆ—ï¼Œè€Œæ»¤æ³¢å™¨æ ¸åˆ™æŒ‰è¡Œæ’åˆ—ã€‚è¿™ä½¿å¾—å·ç§¯æ“ä½œå¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªæ ‡å‡†çš„GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰æ“ä½œã€‚è¿™ç§è½¬æ¢ä»¥å†…å­˜æ¶ˆè€—â€”â€”åœ¨çª—å£é‡å å¤„å¤åˆ¶æ•°æ®â€”â€”ä¸ºä»£ä»·ï¼Œæ¢å–è®¡ç®—æ•ˆç‡ï¼Œä½¿å¾—å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥åˆ©ç”¨æ•°åå¹´çš„BLASä¼˜åŒ–ï¼Œå¹¶åœ¨CPUä¸Šå®ç°5-10å€çš„åŠ é€Ÿã€‚åœ¨ç°ä»£ç³»ç»Ÿä¸­ï¼Œè¿™äº›çŸ©é˜µä¹˜æ³•æ˜ å°„åˆ°ç‰¹å®šçš„ç¡¬ä»¶å’Œè½¯ä»¶å®ç°ã€‚ç¡¬ä»¶åŠ é€Ÿå™¨æä¾›ä¸“é—¨çš„å¼ é‡æ ¸å¿ƒï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œæ•°åƒæ¬¡ä¹˜åŠ æ“ä½œï¼›NVIDIAçš„A100å¼ é‡æ ¸å¿ƒå¯ä»¥é’ˆå¯¹æ··åˆç²¾åº¦ï¼ˆTF32ï¼‰å·¥ä½œè´Ÿè½½è¾¾åˆ°é«˜è¾¾312
    TFLOPSï¼Œæˆ–é€šè¿‡è¿™äº›æ“ä½œçš„å·¨å¤§å¹¶è¡ŒåŒ–å®ç°156 TFLOPSçš„FP32ã€‚åƒPyTorchå’ŒTensorFlowè¿™æ ·çš„è½¯ä»¶æ¡†æ¶è‡ªåŠ¨å°†è¿™äº›é«˜çº§æ“ä½œæ˜ å°„åˆ°ä¼˜åŒ–çš„çŸ©é˜µåº“ï¼ˆNVIDIA
    [cuBLAS](https://developer.nvidia.com/cublas)ï¼ŒIntel [MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve)ï¼‰ï¼Œè¿™äº›åº“åˆ©ç”¨äº†è¿™äº›ç¡¬ä»¶èƒ½åŠ›ã€‚
- en: Sliding window operations compute local relationships by applying the same operation
    to chunks of data. In CNNs processing MNIST images, a <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> convolution filter
    slides across the <semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics> input, requiring
    <semantics><mrow><mn>26</mn><mo>Ã—</mo><mn>26</mn></mrow><annotation encoding="application/x-tex">26\times
    26</annotation></semantics> windows of computation, assuming a stride size of
    1\. Modern hardware accelerators implement this through specialized memory access
    patterns and data buffering schemes that optimize data reuse. For example, Googleâ€™s
    TPU uses a <semantics><mrow><mn>128</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times 128</annotation></semantics> systolic array[28](#fn28)
    where data flows systematically through processing elements, allowing each input
    value to be reused across multiple computations without accessing memory.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: æ»‘åŠ¨çª—å£æ“ä½œé€šè¿‡å°†ç›¸åŒçš„æ“ä½œåº”ç”¨äºæ•°æ®å—æ¥è®¡ç®—å±€éƒ¨å…³ç³»ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†MNISTå›¾åƒæ—¶ï¼Œä¸€ä¸ª<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>å·ç§¯æ»¤æ³¢å™¨åœ¨<semantics><mrow><mn>28</mn><mo>Ã—</mo><mn>28</mn></mrow><annotation
    encoding="application/x-tex">28\times 28</annotation></semantics>è¾“å…¥ä¸Šæ»‘åŠ¨ï¼Œéœ€è¦<semantics><mrow><mn>26</mn><mo>Ã—</mo><mn>26</mn></mrow><annotation
    encoding="application/x-tex">26\times 26</annotation></semantics>ä¸ªè®¡ç®—çª—å£ï¼Œå‡è®¾æ­¥é•¿å¤§å°ä¸º1ã€‚ç°ä»£ç¡¬ä»¶åŠ é€Ÿå™¨é€šè¿‡ä¸“é—¨çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œæ•°æ®ç¼“å†²æ–¹æ¡ˆæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¿™äº›æ–¹æ¡ˆä¼˜åŒ–äº†æ•°æ®é‡ç”¨ã€‚ä¾‹å¦‚ï¼Œè°·æ­Œçš„TPUä½¿ç”¨ä¸€ä¸ª<semantics><mrow><mn>128</mn><mo>Ã—</mo><mn>128</mn></mrow><annotation
    encoding="application/x-tex">128\times 128</annotation></semantics>çš„æ”¶ç¼©é˜µåˆ—[28](#fn28)ï¼Œå…¶ä¸­æ•°æ®æœ‰ç³»ç»Ÿåœ°é€šè¿‡å¤„ç†å•å…ƒæµåŠ¨ï¼Œå…è®¸æ¯ä¸ªè¾“å…¥å€¼åœ¨å¤šä¸ªè®¡ç®—ä¸­é‡ç”¨è€Œä¸è®¿é—®å†…å­˜ã€‚
- en: 'Dynamic computation, where the operation itself depends on the input data,
    emerged prominently with attention mechanisms but represents a capability needed
    for adaptive processing. In Transformer attention, each query dynamically determines
    its interaction weights with all keys; for a sequence of length 512, 512 different
    weight patterns must be computed on the fly. Unlike fixed patterns where the computation
    graph is known in advance, dynamic computation requires runtime decisions. This
    creates specific implementation challenges: hardware must provide flexible data
    routing (modern GPUs employ dynamic scheduling) and support variable computation
    patterns, while software frameworks require efficient mechanisms for handling
    data-dependent execution paths (PyTorchâ€™s dynamic computation graphs, TensorFlowâ€™s
    dynamic control flow).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€è®¡ç®—ï¼Œå…¶ä¸­æ“ä½œæœ¬èº«å–å†³äºè¾“å…¥æ•°æ®ï¼Œéšç€å…³æ³¨æœºåˆ¶çš„å‡ºç°è€Œå˜å¾—çªå‡ºï¼Œä½†ä»£è¡¨äº†è‡ªé€‚åº”å¤„ç†æ‰€éœ€çš„èƒ½åŠ›ã€‚åœ¨Transformerå…³æ³¨ä¸­ï¼Œæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€ç¡®å®šå…¶ä¸æ‰€æœ‰é”®çš„äº¤äº’æƒé‡ï¼›å¯¹äºé•¿åº¦ä¸º512çš„åºåˆ—ï¼Œå¿…é¡»åœ¨è¿è¡Œæ—¶åŠ¨æ€è®¡ç®—512ç§ä¸åŒçš„æƒé‡æ¨¡å¼ã€‚ä¸é¢„å…ˆçŸ¥é“è®¡ç®—å›¾çš„å›ºå®šæ¨¡å¼ä¸åŒï¼ŒåŠ¨æ€è®¡ç®—éœ€è¦è¿è¡Œæ—¶å†³ç­–ã€‚è¿™åˆ›é€ äº†ç‰¹å®šçš„å®ç°æŒ‘æˆ˜ï¼šç¡¬ä»¶å¿…é¡»æä¾›çµæ´»çš„æ•°æ®è·¯ç”±ï¼ˆç°ä»£GPUé‡‡ç”¨åŠ¨æ€è°ƒåº¦ï¼‰å¹¶æ”¯æŒå¯å˜çš„è®¡ç®—æ¨¡å¼ï¼Œè€Œè½¯ä»¶æ¡†æ¶éœ€è¦é«˜æ•ˆçš„æœºåˆ¶æ¥å¤„ç†æ•°æ®ç›¸å…³çš„æ‰§è¡Œè·¯å¾„ï¼ˆPyTorchçš„åŠ¨æ€è®¡ç®—å›¾ï¼ŒTensorFlowçš„åŠ¨æ€æ§åˆ¶æµï¼‰ã€‚
- en: 'These primitives combine in sophisticated ways in modern architectures. A Transformer
    layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix
    multiplications for feature projections (<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics> operations
    implemented through tensor cores), may employ sliding windows for efficient attention
    over long sequences (using specialized memory access patterns for local regions),
    and requires dynamic computation for attention weights (computing <semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics> attention
    patterns at runtime). The way these primitives interact creates specific demands
    on system design, ranging from memory hierarchy organization to computation scheduling.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸè¯­åœ¨ç°ä»£æ¶æ„ä¸­ä»¥å¤æ‚çš„æ–¹å¼ç»„åˆã€‚ä¸€ä¸ªå¤„ç†512ä¸ªæ ‡è®°åºåˆ—çš„Transformerå±‚æ¸…æ¥šåœ°å±•ç¤ºäº†è¿™ä¸€ç‚¹ï¼šå®ƒä½¿ç”¨çŸ©é˜µä¹˜æ³•è¿›è¡Œç‰¹å¾æŠ•å½±ï¼ˆ<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics>æ“ä½œï¼Œé€šè¿‡å¼ é‡æ ¸å¿ƒå®ç°ï¼‰ï¼Œå¯èƒ½ä½¿ç”¨æ»‘åŠ¨çª—å£å¯¹é•¿åºåˆ—è¿›è¡Œé«˜æ•ˆçš„å…³æ³¨ï¼ˆä½¿ç”¨é’ˆå¯¹å±€éƒ¨åŒºåŸŸçš„ä¸“ç”¨å†…å­˜è®¿é—®æ¨¡å¼ï¼‰ï¼Œå¹¶ä¸”éœ€è¦åŠ¨æ€è®¡ç®—å…³æ³¨æƒé‡ï¼ˆåœ¨è¿è¡Œæ—¶è®¡ç®—<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics>å…³æ³¨æ¨¡å¼ï¼‰ã€‚è¿™äº›åŸè¯­ä¹‹é—´çš„äº¤äº’æ–¹å¼å¯¹ç³»ç»Ÿè®¾è®¡æå‡ºäº†å…·ä½“è¦æ±‚ï¼Œä»å†…å­˜å±‚æ¬¡ç»“æ„ç»„ç»‡åˆ°è®¡ç®—è°ƒåº¦ã€‚
- en: The building blocks weâ€™ve discussed help explain why certain hardware features
    exist (like tensor cores for matrix multiplication) and why software frameworks
    organize computations in particular ways (like batching similar operations together).
    As we move from computational primitives to consider memory access and data movement
    patterns, recognizing how these operations shape the demands placed on memory
    systems becomes essential and data transfer mechanisms. The way computational
    primitives are implemented and combined has direct implications for how data needs
    to be stored, accessed, and moved within the system.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€è®¨è®ºçš„æ„å»ºå—æœ‰åŠ©äºè§£é‡Šä¸ºä»€ä¹ˆæŸäº›ç¡¬ä»¶ç‰¹æ€§å­˜åœ¨ï¼ˆå¦‚ç”¨äºçŸ©é˜µä¹˜æ³•çš„å¼ é‡æ ¸å¿ƒï¼‰ä»¥åŠä¸ºä»€ä¹ˆè½¯ä»¶æ¡†æ¶ä»¥ç‰¹å®šæ–¹å¼ç»„ç»‡è®¡ç®—ï¼ˆå¦‚å°†ç±»ä¼¼çš„æ“ä½œä¸€èµ·æ‰¹å¤„ç†ï¼‰ã€‚å½“æˆ‘ä»¬ä»è®¡ç®—åŸè¯­è½¬å‘è€ƒè™‘å†…å­˜è®¿é—®å’Œæ•°æ®ç§»åŠ¨æ¨¡å¼æ—¶ï¼Œè®¤è¯†åˆ°è¿™äº›æ“ä½œå¦‚ä½•å¡‘é€ å¯¹å†…å­˜ç³»ç»Ÿå’Œæ•°æ®ä¼ è¾“æœºåˆ¶çš„ç‰¹å®šè¦æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚è®¡ç®—åŸè¯­çš„å®æ–½å’Œç»„åˆæ–¹å¼å¯¹æ•°æ®åœ¨ç³»ç»Ÿå†…éƒ¨å­˜å‚¨ã€è®¿é—®å’Œç§»åŠ¨çš„éœ€æ±‚æœ‰ç›´æ¥å½±å“ã€‚
- en: Memory Access Primitives
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å†…å­˜è®¿é—®åŸè¯­
- en: The efficiency of deep learning models depends heavily on memory access and
    management. Memory access often constitutes the primary bottleneck in modern ML
    systems; even though a matrix multiplication unit may be capable of performing
    thousands of operations per cycle, it will remain idle if data is not available
    at the requisite time. For example, accessing data from DRAM typically requires
    hundreds of cycles, while on-chip computation requires only a few cycles.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ•ˆç‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå†…å­˜è®¿é—®å’Œç®¡ç†ã€‚å†…å­˜è®¿é—®é€šå¸¸æ˜¯ç°ä»£æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„ä¸»è¦ç“¶é¢ˆï¼›å³ä½¿çŸ©é˜µä¹˜æ³•å•å…ƒå¯èƒ½èƒ½å¤Ÿåœ¨æ¯ä¸ªå‘¨æœŸæ‰§è¡Œæ•°åƒæ¬¡æ“ä½œï¼Œå¦‚æœæ²¡æœ‰åœ¨æ‰€éœ€æ—¶é—´æä¾›æ•°æ®ï¼Œå®ƒå°†ä¿æŒç©ºé—²ã€‚ä¾‹å¦‚ï¼Œä»DRAMè®¿é—®æ•°æ®é€šå¸¸éœ€è¦æ•°ç™¾ä¸ªå‘¨æœŸï¼Œè€Œç‰‡ä¸Šè®¡ç®—åªéœ€è¦å‡ ä¸ªå‘¨æœŸã€‚
- en: 'Three memory access patterns dominate in deep learning architectures: sequential
    access, strided access, and random access. Each pattern creates different demands
    on the memory system and offers different opportunities for optimization.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ æ¶æ„ä¸­ï¼Œä¸‰ç§å†…å­˜è®¿é—®æ¨¡å¼å ä¸»å¯¼åœ°ä½ï¼šé¡ºåºè®¿é—®ã€æ­¥é•¿è®¿é—®å’Œéšæœºè®¿é—®ã€‚æ¯ç§æ¨¡å¼éƒ½å¯¹å†…å­˜ç³»ç»Ÿæå‡ºäº†ä¸åŒçš„è¦æ±‚ï¼Œå¹¶æä¾›äº†ä¸åŒçš„ä¼˜åŒ–æœºä¼šã€‚
- en: 'Sequential access is the simplest and most efficient pattern. Consider an MLP
    performing matrix multiplication with a batch of MNIST images: it needs to access
    both the <semantics><mrow><mn>784</mn><mo>Ã—</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">784\times 100</annotation></semantics> weight matrix
    and the input vectors sequentially. This pattern maps well to modern memory systems;
    DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s
    in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming
    data. Software frameworks optimize for this by ensuring data is laid out contiguously
    in memory and aligning data to cache line boundaries.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºåºè®¿é—®æ˜¯æœ€ç®€å•ä¸”æœ€æœ‰æ•ˆçš„æ¨¡å¼ã€‚è€ƒè™‘ä¸€ä¸ªæ‰§è¡ŒMNISTå›¾åƒæ‰¹å¤„ç†çŸ©é˜µä¹˜æ³•çš„MLPï¼šå®ƒéœ€è¦é¡ºåºè®¿é—®784Ã—100çš„æƒé‡çŸ©é˜µå’Œè¾“å…¥å‘é‡ã€‚è¿™ç§æ¨¡å¼å¾ˆå¥½åœ°æ˜ å°„åˆ°ç°ä»£å†…å­˜ç³»ç»Ÿï¼›DRAMå¯ä»¥åœ¨çªå‘æ¨¡å¼ä¸‹è¿›è¡Œé¡ºåºè¯»å–ï¼ˆåœ¨ç°ä»£GPUä¸­è¾¾åˆ°é«˜è¾¾400
    GB/sï¼‰ï¼Œç¡¬ä»¶é¢„å–å™¨å¯ä»¥æœ‰æ•ˆåœ°é¢„æµ‹å’Œè·å–å³å°†åˆ°æ¥çš„æ•°æ®ã€‚è½¯ä»¶æ¡†æ¶é€šè¿‡ç¡®ä¿æ•°æ®åœ¨å†…å­˜ä¸­è¿ç»­æ’åˆ—å¹¶å¯¹é½åˆ°ç¼“å­˜è¡Œè¾¹ç•Œæ¥ä¼˜åŒ–è¿™ä¸€ç‚¹ã€‚
- en: Strided access appears prominently in CNNs, where each output position needs
    to access a window of input values at regular intervals. For a CNN processing
    MNIST images with <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filters, each
    output position requires accessing 9 input values with a stride matching the input
    width. While less efficient than sequential access, hardware supports this through
    pattern-aware caching strategies and specialized memory controllers. Software
    frameworks often transform these strided patterns into sequential access through
    data layout reorganization, where the im2col transformation in deep learning frameworks
    converts convolutionâ€™s strided access into efficient matrix multiplications.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¥é•¿è®¿é—®åœ¨CNNä¸­å°¤ä¸ºçªå‡ºï¼Œå…¶ä¸­æ¯ä¸ªè¾“å‡ºä½ç½®éœ€è¦ä»¥å›ºå®šé—´éš”è®¿é—®è¾“å…¥å€¼çš„ä¸€ä¸ªçª—å£ã€‚å¯¹äºä¸€ä¸ªå¤„ç†MNISTå›¾åƒçš„CNNï¼Œä½¿ç”¨3Ã—3çš„è¿‡æ»¤å™¨ï¼Œæ¯ä¸ªè¾“å‡ºä½ç½®éœ€è¦ä»¥ä¸è¾“å…¥å®½åº¦åŒ¹é…çš„æ­¥é•¿è®¿é—®9ä¸ªè¾“å…¥å€¼ã€‚è™½ç„¶ä¸å¦‚é¡ºåºè®¿é—®é«˜æ•ˆï¼Œä½†ç¡¬ä»¶é€šè¿‡æ¨¡å¼æ„ŸçŸ¥çš„ç¼“å­˜ç­–ç•¥å’Œä¸“é—¨çš„å†…å­˜æ§åˆ¶å™¨æ¥æ”¯æŒè¿™ä¸€ç‚¹ã€‚è½¯ä»¶æ¡†æ¶é€šå¸¸é€šè¿‡æ•°æ®å¸ƒå±€é‡ç»„å°†è¿™äº›æ­¥é•¿æ¨¡å¼è½¬æ¢ä¸ºé¡ºåºè®¿é—®ï¼Œå…¶ä¸­æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„im2colè½¬æ¢å°†å·ç§¯çš„æ­¥é•¿è®¿é—®è½¬æ¢ä¸ºé«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•ã€‚
- en: Random access poses the greatest challenge for system efficiency. In a Transformer
    processing a sequence of 512 tokens, each attention operation potentially needs
    to access any position in the sequence, creating unpredictable memory access patterns.
    Random access can severely impact performance through cache misses (potentially
    causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems
    address this through large cache hierarchies (modern GPUs have several MB of L2
    cache) and sophisticated prefetching strategies, while software frameworks employ
    techniques like attention pattern pruning to reduce random access requirements.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºè®¿é—®å¯¹ç³»ç»Ÿæ•ˆç‡æå‡ºäº†æœ€å¤§çš„æŒ‘æˆ˜ã€‚åœ¨ä¸€ä¸ªå¤„ç†512ä¸ªæ ‡è®°åºåˆ—çš„Transformerä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ“ä½œå¯èƒ½éœ€è¦è®¿é—®åºåˆ—ä¸­çš„ä»»ä½•ä½ç½®ï¼Œä»è€Œåˆ›å»ºä¸å¯é¢„æµ‹çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚éšæœºè®¿é—®å¯èƒ½ä¼šé€šè¿‡ç¼“å­˜æœªå‘½ä¸­ï¼ˆå¯èƒ½æ¯æ¬¡è®¿é—®é€ æˆ100+ä¸ªå‘¨æœŸå»¶è¿Ÿï¼‰å’Œä¸å¯é¢„æµ‹çš„å†…å­˜å»¶è¿Ÿä¸¥é‡å½±å“æ€§èƒ½ã€‚ç³»ç»Ÿé€šè¿‡å¤§å‹ç¼“å­˜å±‚æ¬¡ç»“æ„ï¼ˆç°ä»£GPUå…·æœ‰å‡ ä¸ªMBçš„L2ç¼“å­˜ï¼‰å’Œå¤æ‚çš„é¢„å–ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè€Œè½¯ä»¶æ¡†æ¶åˆ™é‡‡ç”¨è¯¸å¦‚æ³¨æ„åŠ›æ¨¡å¼å‰ªæç­‰æŠ€æœ¯æ¥å‡å°‘éšæœºè®¿é—®éœ€æ±‚ã€‚
- en: These different memory access patterns contribute to the overall memory requirements
    of each architecture. To illustrate this, [TableÂ 4.3](ch010.xhtml#tbl-arch-complexity)
    compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¸åŒçš„å†…å­˜è®¿é—®æ¨¡å¼å¯¹æ¯ä¸ªæ¶æ„çš„æ•´ä½“å†…å­˜éœ€æ±‚åšå‡ºäº†è´¡çŒ®ã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œ[è¡¨4.3](ch010.xhtml#tbl-arch-complexity)æ¯”è¾ƒäº†MLPsã€CNNsã€RNNså’ŒTransformersçš„å†…å­˜å¤æ‚åº¦ã€‚
- en: 'TableÂ 4.3: **Memory Access Complexity**: Different neural network architectures
    exhibit varying memory access patterns and storage requirements, impacting system
    performance and scalability. Parameter storage scales with input dependency and
    model size, while activation storage represents a significant runtime cost, particularly
    for sequence-based models where rnns offer a parameter efficiency advantage when
    sequence length exceeds hidden state size (<semantics><mrow><mi>n</mi><mo>></mo><mi>h</mi></mrow><annotation
    encoding="application/x-tex">n > h</annotation></semantics>).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.3ï¼š**å†…å­˜è®¿é—®å¤æ‚åº¦**ï¼šä¸åŒçš„ç¥ç»ç½‘ç»œæ¶æ„è¡¨ç°å‡ºä¸åŒçš„å†…å­˜è®¿é—®æ¨¡å¼å’Œå­˜å‚¨éœ€æ±‚ï¼Œè¿™ä¼šå½±å“ç³»ç»Ÿæ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚å‚æ•°å­˜å‚¨éšç€è¾“å…¥ä¾èµ–å’Œæ¨¡å‹å¤§å°è€Œæ‰©å±•ï¼Œè€Œæ¿€æ´»å­˜å‚¨ä»£è¡¨äº†æ˜¾è‘—çš„è¿è¡Œæ—¶æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨åºåˆ—æ¨¡å‹ä¸­ï¼Œå½“åºåˆ—é•¿åº¦è¶…è¿‡éšè—çŠ¶æ€å¤§å°æ—¶ï¼ˆ<semantics><mrow><mi>n</mi><mo>></mo><mi>h</mi></mrow><annotation
    encoding="application/x-tex">n > h</annotation></semantics>ï¼‰ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨å‚æ•°æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚
- en: '| **Architecture** | **Input Dependency** | **Parameter Storage** | **Activation
    Storage** | **Scaling Behavior** |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| **æ¶æ„** | **è¾“å…¥ä¾èµ–** | **å‚æ•°å­˜å‚¨** | **æ¿€æ´»å­˜å‚¨** | **ç¼©æ”¾è¡Œä¸º** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **MLP** | Linear | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>W</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N
    \times W)</annotation></semantics> | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(B \times W)</annotation></semantics> | Predictable
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| **MLP** | çº¿æ€§ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>W</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N
    \times W)</annotation></semantics> | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(B \times W)</annotation></semantics> | å¯é¢„æµ‹çš„ |'
- en: '| **CNN** | Constant | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>K</mi><mo>Ã—</mo><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(K \times C)</annotation></semantics> | <semantics><mrow><mi>O</mi><mo
    stretchy="false" form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><msub><mi>H</mi><mtext
    mathvariant="normal">img</mtext></msub></mrow><annotation encoding="application/x-tex">O(B\times
    H_{\text{img}}</annotation></semantics> <semantics><mrow><mi>Ã—</mi><msub><mi>W</mi><mtext
    mathvariant="normal">img</mtext></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">\times W_{\text{img}})</annotation></semantics> |
    Efficient |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| **CNN** | å¸¸æ•° | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>K</mi><mo>Ã—</mo><mi>C</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(K
    \times C)</annotation></semantics> | <semantics><mrow><mi>O</mi><mo stretchy="false"
    form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><msub><mi>H</mi><mtext mathvariant="normal">img</mtext></msub></mrow><annotation
    encoding="application/x-tex">O(B\times H_{\text{img}}</annotation></semantics>
    <semantics><mrow><mi>Ã—</mi><msub><mi>W</mi><mtext mathvariant="normal">img</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\times
    W_{\text{img}})</annotation></semantics> | é«˜æ•ˆçš„ |'
- en: '| **RNN** | Linear | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2)</annotation></semantics>
    | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><mi>T</mi><mo>Ã—</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B
    \times T \times h)</annotation></semantics> | Challenging |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| **RNN** | çº¿æ€§ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2)</annotation></semantics>
    | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><mi>T</mi><mo>Ã—</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B
    \times T \times h)</annotation></semantics> | å…·æŒ‘æˆ˜æ€§ |'
- en: '| **Transformer** | Quadratic | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(N \times d)</annotation></semantics> | <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><msup><mi>N</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B
    \times N^2)</annotation></semantics> | Problematic |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| **Transformer** | äºŒæ¬¡ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>N</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(N \times d)</annotation></semantics> | <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>B</mi><mo>Ã—</mo><msup><mi>N</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(B
    \times N^2)</annotation></semantics> | å­˜åœ¨é—®é¢˜ |'
- en: 'Where:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:
    Input or sequence size'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics>:
    è¾“å…¥æˆ–åºåˆ—å¤§å°'
- en: '<semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>:
    Layer width'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>:
    å±‚å®½åº¦'
- en: '<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>:
    Batch size'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>:
    æ‰¹å¤„ç†å¤§å°'
- en: '<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>:
    Kernel size'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>:
    æ ¸å¤§å°'
- en: '<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>:
    Number of channels'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>:
    é€šé“æ•°'
- en: '<semantics><msub><mi>H</mi><mtext mathvariant="normal">img</mtext></msub><annotation
    encoding="application/x-tex">H_{\text{img}}</annotation></semantics>: Height of
    input feature map (CNN)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><msub><mi>H</mi><mtext mathvariant="normal">img</mtext></msub><annotation
    encoding="application/x-tex">H_{\text{img}}</annotation></semantics>: è¾“å…¥ç‰¹å¾å›¾çš„é«˜åº¦ï¼ˆCNNï¼‰'
- en: '<semantics><msub><mi>W</mi><mtext mathvariant="normal">img</mtext></msub><annotation
    encoding="application/x-tex">W_{\text{img}}</annotation></semantics>: Width of
    input feature map (CNN)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><msub><mi>W</mi><mtext mathvariant="normal">img</mtext></msub><annotation
    encoding="application/x-tex">W_{\text{img}}</annotation></semantics>: è¾“å…¥ç‰¹å¾å›¾çš„å®½åº¦ï¼ˆCNNï¼‰'
- en: '<semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics>:
    Hidden state size (RNN)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics>:
    éšè—çŠ¶æ€å¤§å°ï¼ˆRNNï¼‰'
- en: '<semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>:
    Sequence length'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>:
    åºåˆ—é•¿åº¦'
- en: '<semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>:
    Model dimensionality'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>:
    æ¨¡å‹ç»´åº¦'
- en: '[TableÂ 4.3](ch010.xhtml#tbl-arch-complexity) reveals how memory requirements
    scale with different architectural choices. The quadratic scaling of activation
    storage in Transformers, for instance, highlights the need for large memory capacities
    and efficient memory management in systems designed for Transformer-based workloads.
    In contrast, CNNs exhibit more favorable memory scaling due to their parameter
    sharing and localized processing. These memory access patterns complement the
    computational scaling behaviors examined later in [TableÂ 4.6](ch010.xhtml#tbl-computational-complexity),
    completing the picture of each architectureâ€™s resource requirements. These memory
    complexity considerations inform system-level design decisions, such as choosing
    memory hierarchy configurations and developing memory optimization strategies.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨4.3](ch010.xhtml#tbl-arch-complexity) å±•ç¤ºäº†å†…å­˜éœ€æ±‚å¦‚ä½•éšç€ä¸åŒçš„æ¶æ„é€‰æ‹©è€Œå˜åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨ Transformer
    ä¸­çš„æ¿€æ´»å­˜å‚¨çš„äºŒæ¬¡æ‰©å±•çªå‡ºäº†ä¸ºåŸºäº Transformer çš„å·¥ä½œè´Ÿè½½è®¾è®¡çš„ç³»ç»Ÿä¸­éœ€è¦å¤§å†…å­˜å®¹é‡å’Œé«˜æ•ˆå†…å­˜ç®¡ç†çš„å¿…è¦æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”±äºå‚æ•°å…±äº«å’Œå±€éƒ¨å¤„ç†ï¼ŒCNN
    æ˜¾ç¤ºå‡ºæ›´æœ‰åˆ©çš„å†…å­˜æ‰©å±•ã€‚è¿™äº›å†…å­˜è®¿é—®æ¨¡å¼è¡¥å……äº†éšååœ¨ [è¡¨4.6](ch010.xhtml#tbl-computational-complexity) ä¸­è€ƒå¯Ÿçš„è®¡ç®—æ‰©å±•è¡Œä¸ºï¼Œå®Œæˆäº†æ¯ä¸ªæ¶æ„èµ„æºéœ€æ±‚çš„å›¾æ™¯ã€‚è¿™äº›å†…å­˜å¤æ‚åº¦è€ƒè™‘å› ç´ ä¸ºç³»ç»Ÿçº§è®¾è®¡å†³ç­–æä¾›äº†ä¿¡æ¯ï¼Œä¾‹å¦‚é€‰æ‹©å†…å­˜å±‚æ¬¡ç»“æ„é…ç½®å’Œå¼€å‘å†…å­˜ä¼˜åŒ–ç­–ç•¥ã€‚'
- en: The impact of these patterns becomes clear when we consider data reuse opportunities.
    In CNNs, each input pixel participates in multiple convolution windows (typically
    9 times for a <semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics> filter), making
    effective data reuse necessary for performance. Modern GPUs provide multi-level
    cache hierarchies (L1, L2, shared memory) to capture this reuse, while software
    techniques like loop tiling ensure data remains in cache once loaded.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è€ƒè™‘æ•°æ®é‡ç”¨æœºä¼šæ—¶ï¼Œè¿™äº›æ¨¡å¼çš„å½±å“å˜å¾—æ˜æ˜¾ã€‚åœ¨CNNä¸­ï¼Œæ¯ä¸ªè¾“å…¥åƒç´ å‚ä¸å¤šä¸ªå·ç§¯çª—å£ï¼ˆé€šå¸¸ä¸º<semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation
    encoding="application/x-tex">3\times 3</annotation></semantics>æ»¤æ³¢å™¨ï¼‰ï¼Œè¿™ä½¿å¾—æœ‰æ•ˆçš„æ•°æ®é‡ç”¨å¯¹äºæ€§èƒ½è‡³å…³é‡è¦ã€‚ç°ä»£GPUæä¾›å¤šçº§ç¼“å­˜å±‚æ¬¡ç»“æ„ï¼ˆL1ã€L2ã€å…±äº«å†…å­˜ï¼‰æ¥æ•è·è¿™ç§é‡ç”¨ï¼Œè€Œè½¯ä»¶æŠ€æœ¯å¦‚å¾ªç¯åˆ†å—ç¡®ä¿æ•°æ®ä¸€æ—¦åŠ è½½å°±ä¿æŒåœ¨ç¼“å­˜ä¸­ã€‚
- en: Working set size, the amount of data needed simultaneously for computation,
    varies dramatically across architectures. An MLP layer processing MNIST images
    might need only a few hundred KB (weights plus activations), while a Transformer
    processing long sequences can require several MB just for storing attention patterns.
    These differences directly influence hardware design choices, like the balance
    between compute units and on-chip memory, and software optimizations like activation
    checkpointing or attention approximation techniques.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥ä½œé›†å¤§å°ï¼Œå³åŒæ—¶ç”¨äºè®¡ç®—æ‰€éœ€çš„æ•°æ®é‡ï¼Œåœ¨æ¶æ„ä¹‹é—´å·®å¼‚å¾ˆå¤§ã€‚å¤„ç†MNISTå›¾åƒçš„MLPå±‚å¯èƒ½åªéœ€è¦å‡ ç™¾KBï¼ˆæƒé‡åŠ æ¿€æ´»ï¼‰ï¼Œè€Œå¤„ç†é•¿åºåˆ—çš„Transformerå¯èƒ½åªéœ€è¦å‡ ä¸ªMBæ¥å­˜å‚¨æ³¨æ„åŠ›æ¨¡å¼ã€‚è¿™äº›å·®å¼‚ç›´æ¥å½±å“ç¡¬ä»¶è®¾è®¡é€‰æ‹©ï¼Œå¦‚è®¡ç®—å•å…ƒå’Œç‰‡ä¸Šå†…å­˜ä¹‹é—´çš„å¹³è¡¡ï¼Œä»¥åŠè½¯ä»¶ä¼˜åŒ–ï¼Œå¦‚æ¿€æ´»æ£€æŸ¥ç‚¹æˆ–æ³¨æ„åŠ›è¿‘ä¼¼æŠ€æœ¯ã€‚
- en: Understanding these memory access patterns is essential as architectures evolve.
    The shift from CNNs to Transformers, for instance, has driven the development
    of hardware with larger on-chip memories and more sophisticated caching strategies
    to handle increased working sets and more dynamic access patterns. Future architectures
    will likely continue to be shaped by their memory access characteristics as much
    as their computational requirements.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¶æ„çš„å‘å±•ï¼Œç†è§£è¿™äº›å†…å­˜è®¿é—®æ¨¡å¼è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œä»CNNåˆ°Transformerçš„è½¬å˜æ¨åŠ¨äº†å…·æœ‰æ›´å¤§ç‰‡ä¸Šå†…å­˜å’Œæ›´å¤æ‚ç¼“å­˜ç­–ç•¥çš„ç¡¬ä»¶çš„å‘å±•ï¼Œä»¥å¤„ç†æ›´å¤§çš„å·¥ä½œé›†å’Œæ›´åŠ¨æ€çš„è®¿é—®æ¨¡å¼ã€‚æœªæ¥çš„æ¶æ„å¯èƒ½ä¼šç»§ç»­ç”±å®ƒä»¬çš„å†…å­˜è®¿é—®ç‰¹æ€§ä»¥åŠå®ƒä»¬çš„è®¡ç®—éœ€æ±‚æ¥å¡‘é€ ã€‚
- en: Data Movement Primitives
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨åŸè¯­
- en: While computational and memory access patterns define what operations occur
    where, data movement primitives characterize how information flows through the
    system. These patterns are key because data movement often consumes more time
    and energy than computation itself, as moving data from off-chip memory typically
    requires 100-1000<semantics><mi>Ã—</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    more energy than performing a floating-point operation.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è®¡ç®—å’Œå†…å­˜è®¿é—®æ¨¡å¼å®šä¹‰äº†æ“ä½œå‘ç”Ÿåœ¨ä½•å¤„æ—¶ï¼Œæ•°æ®ç§»åŠ¨åŸè¯­æè¿°äº†ä¿¡æ¯å¦‚ä½•é€šè¿‡ç³»ç»ŸæµåŠ¨ã€‚è¿™äº›æ¨¡å¼è‡³å…³é‡è¦ï¼Œå› ä¸ºæ•°æ®ç§»åŠ¨é€šå¸¸æ¯”è®¡ç®—æœ¬èº«æ¶ˆè€—æ›´å¤šçš„æ—¶é—´å’Œèƒ½é‡ï¼Œå°†æ•°æ®ä»ç‰‡å¤–å†…å­˜ç§»åŠ¨åˆ°èŠ¯ç‰‡é€šå¸¸éœ€è¦æ¯”æ‰§è¡Œæµ®ç‚¹è¿ç®—å¤š100-1000å€ï¼ˆä½¿ç”¨LaTeXè¯­æ³•è¡¨ç¤ºï¼‰çš„èƒ½é‡ã€‚
- en: 'Four data movement patterns are prevalent in deep learning architectures: broadcast,
    scatter, gather, and reduction. [FigureÂ 4.11](ch010.xhtml#fig-collective-comm)
    illustrates these patterns and their relationships. Broadcast operations send
    the same data to multiple destinations simultaneously. In matrix multiplication
    with batch size 32, each weight must be broadcast to process different inputs
    in parallel. Modern hardware supports this through specialized interconnects,
    NVIDIA GPUs provide hardware multicast capabilities, achieving up to 600 GB/s
    broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks
    optimize broadcasts by restructuring computations (like matrix tiling) to maximize
    data reuse.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å­¦ä¹ æ¶æ„ä¸­ï¼Œå››ç§æ•°æ®ç§»åŠ¨æ¨¡å¼å¾ˆå¸¸è§ï¼šå¹¿æ’­ã€æ•£å°„ã€æ”¶é›†å’Œå½’çº¦ã€‚[å›¾4.11](ch010.xhtml#fig-collective-comm)å±•ç¤ºäº†è¿™äº›æ¨¡å¼åŠå…¶å…³ç³»ã€‚å¹¿æ’­æ“ä½œåŒæ—¶å°†ç›¸åŒçš„æ•°æ®å‘é€åˆ°å¤šä¸ªç›®çš„åœ°ã€‚åœ¨æ‰¹é‡å¤§å°ä¸º32çš„çŸ©é˜µä¹˜æ³•ä¸­ï¼Œæ¯ä¸ªæƒé‡éƒ½å¿…é¡»è¿›è¡Œå¹¿æ’­ä»¥å¹¶è¡Œå¤„ç†ä¸åŒçš„è¾“å…¥ã€‚ç°ä»£ç¡¬ä»¶é€šè¿‡ä¸“ç”¨äº’è¿æ”¯æŒè¿™ä¸€ç‚¹ï¼ŒNVIDIA
    GPUæä¾›ç¡¬ä»¶å¤šæ’­åŠŸèƒ½ï¼Œå®ç°é«˜è¾¾600 GB/sçš„å¹¿æ’­å¸¦å®½ï¼Œè€ŒTPUä½¿ç”¨ä¸“ç”¨çš„å¹¿æ’­æ€»çº¿ã€‚è½¯ä»¶æ¡†æ¶é€šè¿‡é‡æ„è®¡ç®—ï¼ˆå¦‚çŸ©é˜µåˆ†å—ï¼‰æ¥ä¼˜åŒ–å¹¿æ’­ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®é‡ç”¨ã€‚
- en: '![](../media/file64.svg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../media/file64.svg)'
- en: 'FigureÂ 4.11: **Collective Communication Patterns**: Deep learning training
    and inference frequently require data exchange between processing units; this
    figure outlines four core patterns (broadcast, scatter, gather, and reduction)
    that define how data moves within a distributed system and impact overall performance.
    Understanding these patterns enables optimization of data movement, critical because
    communication costs often dominate computation in modern machine learning workloads.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.11ï¼š**é›†ä½“é€šä¿¡æ¨¡å¼**ï¼šæ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç†é€šå¸¸éœ€è¦åœ¨å¤„ç†å•å…ƒä¹‹é—´è¿›è¡Œæ•°æ®äº¤æ¢ï¼›æ­¤å›¾æ¦‚è¿°äº†å››ä¸ªæ ¸å¿ƒæ¨¡å¼ï¼ˆå¹¿æ’­ã€æ•£åˆ—ã€èšåˆå’Œå‡å°‘ï¼‰ï¼Œè¿™äº›æ¨¡å¼å®šä¹‰äº†æ•°æ®å¦‚ä½•åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç§»åŠ¨ä»¥åŠå¦‚ä½•å½±å“æ•´ä½“æ€§èƒ½ã€‚ç†è§£è¿™äº›æ¨¡å¼èƒ½å¤Ÿä¼˜åŒ–æ•°æ®ç§»åŠ¨ï¼Œè¿™åœ¨ç°ä»£æœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºé€šä¿¡æˆæœ¬é€šå¸¸å ä¸»å¯¼åœ°ä½ã€‚
- en: Scatter operations distribute different elements to different destinations.
    When parallelizing a <semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics> matrix multiplication
    across GPU cores, each core receives a subset of the computation. This parallelization
    is important for performance but challenging, as memory conflicts and load imbalance,
    can reduce efficiency by 50% or more. Hardware provides flexible interconnects
    (like NVIDIAâ€™s NVLink offering 600 GB/s bi-directional bandwidth), while software
    frameworks employ sophisticated work distribution algorithms to maintain high
    utilization.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: æ•£åˆ—æ“ä½œå°†ä¸åŒçš„å…ƒç´ åˆ†é…åˆ°ä¸åŒçš„ç›®çš„åœ°ã€‚å½“å°†<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics>çŸ©é˜µä¹˜æ³•å¹¶è¡ŒåŒ–åˆ°GPUæ ¸å¿ƒæ—¶ï¼Œæ¯ä¸ªæ ¸å¿ƒéƒ½ä¼šæ¥æ”¶åˆ°è®¡ç®—çš„ä¸€éƒ¨åˆ†ã€‚è¿™ç§å¹¶è¡ŒåŒ–å¯¹äºæ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå†…å­˜å†²çªå’Œè´Ÿè½½ä¸å¹³è¡¡å¯èƒ½ä¼šé™ä½æ•ˆç‡50%æˆ–æ›´å¤šã€‚ç¡¬ä»¶æä¾›äº†çµæ´»çš„äº’è¿ï¼ˆå¦‚NVIDIAçš„NVLinkæä¾›600
    GB/sçš„åŒå‘å¸¦å®½ï¼‰ï¼Œè€Œè½¯ä»¶æ¡†æ¶é‡‡ç”¨å¤æ‚çš„å·¥ä½œåˆ†é…ç®—æ³•ä»¥ä¿æŒé«˜åˆ©ç”¨ç‡ã€‚
- en: Gather operations collect data from multiple sources. In Transformer attention
    with sequence length 512, each query must gather information from 512 different
    key-value pairs. These irregular access patterns are challenging, random gathering
    can be <semantics><mrow><mn>10</mn><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics>
    slower than sequential access. Hardware supports this through high-bandwidth interconnects
    and large caches, while software frameworks employ techniques like attention pattern
    pruning to reduce gathering overhead.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: èšåˆæ“ä½œä»å¤šä¸ªæ¥æºæ”¶é›†æ•°æ®ã€‚åœ¨åºåˆ—é•¿åº¦ä¸º512çš„Transformeræ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªæŸ¥è¯¢å¿…é¡»ä»512ä¸ªä¸åŒçš„é”®å€¼å¯¹ä¸­æ”¶é›†ä¿¡æ¯ã€‚è¿™äº›ä¸è§„åˆ™çš„è®¿é—®æ¨¡å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéšæœºæ”¶é›†å¯èƒ½æ¯”é¡ºåºè®¿é—®æ…¢<semantics><mrow><mn>10</mn><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics>ã€‚ç¡¬ä»¶é€šè¿‡é«˜å¸¦å®½äº’è¿å’Œå¤§å‹ç¼“å­˜æ¥æ”¯æŒè¿™ä¸€ç‚¹ï¼Œè€Œè½¯ä»¶æ¡†æ¶é‡‡ç”¨è¯¸å¦‚æ³¨æ„åŠ›æ¨¡å¼å‰ªæç­‰æŠ€æœ¯æ¥å‡å°‘æ”¶é›†å¼€é”€ã€‚
- en: Reduction operations combine multiple values into a single result through operations
    like summation. When computing attention scores in Transformers or layer outputs
    in MLPs, efficient reduction is essential. Hardware implements tree-structured
    reduction networks (reducing latency from <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(n)</annotation></semantics> to <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mo>log</mo><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\log
    n)</annotation></semantics>), while software frameworks use optimized parallel
    reduction algorithms that can achieve near-theoretical peak performance.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘æ“ä½œé€šè¿‡æ±‚å’Œç­‰æ“ä½œå°†å¤šä¸ªå€¼åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„ç»“æœã€‚åœ¨è®¡ç®—Transformerä¸­çš„æ³¨æ„åŠ›åˆ†æ•°æˆ–MLPä¸­çš„å±‚è¾“å‡ºæ—¶ï¼Œé«˜æ•ˆçš„å‡å°‘æ“ä½œè‡³å…³é‡è¦ã€‚ç¡¬ä»¶å®ç°äº†æ ‘çŠ¶ç»“æ„çš„å‡å°‘ç½‘ç»œï¼ˆå°†å»¶è¿Ÿä»<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(n)</annotation></semantics>å‡å°‘åˆ°<semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mo>log</mo><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\log
    n)</annotation></semantics>ï¼‰ï¼Œè€Œè½¯ä»¶æ¡†æ¶ä½¿ç”¨ä¼˜åŒ–çš„å¹¶è¡Œå‡å°‘ç®—æ³•ï¼Œå¯ä»¥å®ç°æ¥è¿‘ç†è®ºå³°å€¼æ€§èƒ½ã€‚
- en: 'These patterns combine in sophisticated ways. A Transformer attention operation
    with sequence length 512 and batch size 32 involves:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å¼ä»¥å¤æ‚çš„æ–¹å¼ç»“åˆåœ¨ä¸€èµ·ã€‚åºåˆ—é•¿åº¦ä¸º512ä¸”æ‰¹å¤§å°ä¸º32çš„Transformeræ³¨æ„åŠ›æ“ä½œæ¶‰åŠï¼š
- en: Broadcasting query vectors (<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">512\times 64</annotation></semantics> elements)
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¿æ’­æŸ¥è¯¢å‘é‡ï¼ˆ<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">512\times 64</annotation></semantics>ä¸ªå…ƒç´ ï¼‰
- en: Gathering relevant keys and values (<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">512\times 512\times 64</annotation></semantics> elements)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¶é›†ç›¸å…³é”®å’Œå€¼ï¼ˆ<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn><mo>Ã—</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">512\times 512\times 64</annotation></semantics>å…ƒç´ ï¼‰
- en: Reducing attention scores (<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics> elements per
    sequence)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é™ä½æ³¨æ„åŠ›åˆ†æ•°ï¼ˆ<semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>512</mn></mrow><annotation
    encoding="application/x-tex">512\times 512</annotation></semantics>å…ƒç´ æ¯ä¸ªåºåˆ—ï¼‰
- en: The evolution from CNNs to Transformers has increased reliance on gather and
    reduction operations, driving hardware innovations like more flexible interconnects
    and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[29](#fn29)),
    efficient data movement becomes increasingly critical, leading to innovations
    like near-memory processing and sophisticated data flow optimizations.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ä»CNNåˆ°Transformerçš„æ¼”å˜å¢åŠ äº†å¯¹æ”¶é›†å’Œå½’çº¦æ“ä½œçš„éœ€æ±‚ï¼Œæ¨åŠ¨äº†æ›´çµæ´»äº’è¿å’Œæ›´å¤§ç‰‡ä¸Šå†…å­˜ç­‰ç¡¬ä»¶åˆ›æ–°çš„äº§ç”Ÿã€‚éšç€æ¨¡å‹çš„å¢é•¿ï¼ˆä¸€äº›ç°åœ¨è¶…è¿‡1000äº¿å‚æ•°[29](#fn29)ï¼‰ï¼Œé«˜æ•ˆæ•°æ®ç§»åŠ¨å˜å¾—è¶Šæ¥è¶Šå…³é”®ï¼Œå¯¼è‡´äº†è¿‘å†…å­˜å¤„ç†å’Œå¤æ‚æ•°æ®æµä¼˜åŒ–ç­‰åˆ›æ–°ã€‚
- en: System Design Impact
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿè®¾è®¡å½±å“
- en: The computational, memory access, and data movement primitives weâ€™ve explored
    form the foundational requirements that shape the design of systems for deep learning.
    The way these primitives influence hardware design, create common bottlenecks,
    and drive trade-offs is important for developing efficient and effective ML systems.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€æ¢è®¨çš„è®¡ç®—ã€å†…å­˜è®¿é—®å’Œæ•°æ®ç§»åŠ¨åŸè¯­æ„æˆäº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿè®¾è®¡çš„æ ¹æœ¬è¦æ±‚ã€‚è¿™äº›åŸè¯­å¦‚ä½•å½±å“ç¡¬ä»¶è®¾è®¡ã€åˆ›é€ å…±åŒç“¶é¢ˆå¹¶æ¨åŠ¨æƒè¡¡ï¼Œå¯¹äºå¼€å‘é«˜æ•ˆæœ‰æ•ˆçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚
- en: One of the most significant impacts of these primitives on system design is
    the push towards specialized hardware. The prevalence of matrix multiplications
    and convolutions in deep learning has led to the development of tensor processing
    units (TPUs)[30](#fn30) and tensor cores in GPUs, which are specifically designed
    to perform these operations efficiently.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸè¯­å¯¹ç³»ç»Ÿè®¾è®¡æœ€æ˜¾è‘—çš„å½±å“ä¹‹ä¸€æ˜¯æ¨åŠ¨ä¸“ç”¨ç¡¬ä»¶çš„å‘å±•ã€‚æ·±åº¦å­¦ä¹ ä¸­çŸ©é˜µä¹˜æ³•å’Œå·ç§¯çš„æ™®éå­˜åœ¨å¯¼è‡´äº†å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUsï¼‰[30](#fn30) å’ŒGPUä¸­çš„å¼ é‡æ ¸å¿ƒçš„å‘å±•ï¼Œè¿™äº›å•å…ƒä¸“é—¨è®¾è®¡ç”¨äºé«˜æ•ˆæ‰§è¡Œè¿™äº›æ“ä½œã€‚
- en: Memory systems have also been profoundly influenced by the demands of deep learning
    primitives. The need to support both sequential and random access patterns efficiently
    has driven the development of sophisticated memory hierarchies. High-bandwidth
    memory (HBM)[31](#fn31) has become common in AI accelerators to support the massive
    data movement requirements, especially for operations like attention mechanisms
    in Transformers. On-chip memory hierarchies have grown in complexity, with multiple
    levels of caching and scratchpad memories[32](#fn32) to support the diverse working
    set sizes of different neural network layers.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜å‚¨ç³»ç»Ÿä¹Ÿå—åˆ°äº†æ·±åº¦å­¦ä¹ åŸè¯­éœ€æ±‚çš„æ·±è¿œå½±å“ã€‚ä¸ºäº†é«˜æ•ˆåœ°æ”¯æŒé¡ºåºå’Œéšæœºè®¿é—®æ¨¡å¼ï¼Œæ¨åŠ¨äº†å¤æ‚å†…å­˜å±‚æ¬¡ç»“æ„çš„å‘å±•ã€‚é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰[31](#fn31)
    å·²åœ¨äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ä¸­å˜å¾—å¸¸è§ï¼Œä»¥æ”¯æŒå¤§é‡æ•°æ®ç§»åŠ¨éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯å¯¹äºTransformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ç­‰æ“ä½œã€‚ç‰‡ä¸Šå†…å­˜å±‚æ¬¡ç»“æ„å˜å¾—æ›´åŠ å¤æ‚ï¼Œå…·æœ‰å¤šçº§ç¼“å­˜å’Œå¿«é€Ÿå­˜å‚¨å™¨[32](#fn32)ï¼Œä»¥æ”¯æŒä¸åŒç¥ç»ç½‘ç»œå±‚çš„ä¸åŒå·¥ä½œé›†å¤§å°ã€‚
- en: The data movement primitives have particularly influenced the design of interconnects
    and on-chip networks. The need to support efficient broadcasts, gathers, and reductions
    has led to the development of more flexible and higher-bandwidth interconnects.
    Some AI chips now feature specialized networks-on-chip designed to accelerate
    common data movement patterns in neural networks.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç§»åŠ¨åŸè¯­ç‰¹åˆ«å½±å“äº†äº’è¿å’Œç‰‡ä¸Šç½‘ç»œçš„è®¾è®¡ã€‚æ”¯æŒé«˜æ•ˆå¹¿æ’­ã€æ”¶é›†å’Œå½’çº¦çš„éœ€æ±‚å¯¼è‡´äº†æ›´çµæ´»å’Œæ›´é«˜å¸¦å®½çš„äº’è¿çš„å‘å±•ã€‚ä¸€äº›äººå·¥æ™ºèƒ½èŠ¯ç‰‡ç°åœ¨å…·æœ‰ä¸“é—¨è®¾è®¡çš„ç‰‡ä¸Šç½‘ç»œï¼Œæ—¨åœ¨åŠ é€Ÿç¥ç»ç½‘ç»œä¸­çš„å¸¸è§æ•°æ®ç§»åŠ¨æ¨¡å¼ã€‚
- en: '[TableÂ 4.4](ch010.xhtml#tbl-sys-design-implications) summarizes the system
    implications of these primitives:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨4.4](ch010.xhtml#tbl-sys-design-implications)æ€»ç»“äº†è¿™äº›åŸè¯­å¯¹ç³»ç»Ÿçš„å«ä¹‰ï¼š'
- en: 'TableÂ 4.4: **Primitive-Hardware Co-Design**: Efficient machine learning systems
    require tight integration between algorithmic primitives and underlying hardware;
    this table maps common primitives to specific hardware accelerations and software
    optimizations, highlighting key challenges in their implementation. Specialized
    hardware, such as tensor cores and datapaths, address the computational demands
    of primitives like matrix multiplication and sliding windows, while software techniques
    like batching and dynamic graph execution further enhance performance.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4.4ï¼š**åŸè¯­-ç¡¬ä»¶ååŒè®¾è®¡**ï¼šé«˜æ•ˆçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿéœ€è¦ç®—æ³•åŸè¯­ä¸åº•å±‚ç¡¬ä»¶ä¹‹é—´çš„ç´§å¯†é›†æˆï¼›æ­¤è¡¨å°†å¸¸è§çš„åŸè¯­æ˜ å°„åˆ°ç‰¹å®šçš„ç¡¬ä»¶åŠ é€Ÿå’Œè½¯ä»¶ä¼˜åŒ–ï¼Œçªå‡ºäº†å…¶å®ç°ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚ä¸“ç”¨ç¡¬ä»¶ï¼Œå¦‚å¼ é‡æ ¸å¿ƒå’Œæ•°æ®è·¯å¾„ï¼Œè§£å†³äº†çŸ©é˜µä¹˜æ³•å’Œæ»‘åŠ¨çª—å£ç­‰åŸè¯­çš„è®¡ç®—éœ€æ±‚ï¼Œè€Œæ‰¹å¤„ç†å’ŒåŠ¨æ€å›¾æ‰§è¡Œç­‰è½¯ä»¶æŠ€æœ¯è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚
- en: '| **Primitive** | **Hardware Impact** | **Software Optimization** | **Key Challenges**
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| **åŸè¯­** | **ç¡¬ä»¶å½±å“** | **è½¯ä»¶ä¼˜åŒ–** | **å…³é”®æŒ‘æˆ˜** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Matrix Multiplication** | Tensor Cores | Batching, GEMM libraries | Parallelization,
    precision |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| **çŸ©é˜µä¹˜æ³•** | å¼ é‡æ ¸å¿ƒ | æ‰¹å¤„ç†ï¼ŒGEMM åº“ | å¹¶è¡ŒåŒ–ï¼Œç²¾åº¦ |'
- en: '| **Sliding Window** | Specialized datapaths | Data layout optimization | Stride
    handling |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| **æ»‘åŠ¨çª—å£** | ä¸“ç”¨æ•°æ®è·¯å¾„ | æ•°æ®å¸ƒå±€ä¼˜åŒ– | æ­¥é•¿å¤„ç† |'
- en: '| **Dynamic Computation** | Flexible routing | Dynamic graph execution | Load
    balancing |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **åŠ¨æ€è®¡ç®—** | çµæ´»è·¯ç”± | åŠ¨æ€å›¾æ‰§è¡Œ | è´Ÿè½½å‡è¡¡ |'
- en: '| **Sequential Access** | Burst mode DRAM | Contiguous allocation | Access
    latency |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| **é¡ºåºè®¿é—®** | çˆ†å‘æ¨¡å¼ DRAM | è¿ç»­åˆ†é… | è®¿é—®å»¶è¿Ÿ |'
- en: '| **Random Access** | Large caches | Memory-aware scheduling | Cache misses
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **éšæœºè®¿é—®** | å¤§å®¹é‡ç¼“å­˜ | å†…å­˜æ„ŸçŸ¥è°ƒåº¦ | ç¼“å­˜æœªå‘½ä¸­ |'
- en: '| **Broadcast** | Specialized interconnects | Operation fusion | Bandwidth
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **å¹¿æ’­** | ä¸“ç”¨äº’è¿ | æ“ä½œèåˆ | å¸¦å®½ |'
- en: '| **Gather/Scatter** | High-bandwidth memory | Work distribution | Load balancing
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| **æ”¶é›†/åˆ†æ•£** | é«˜å¸¦å®½å†…å­˜ | å·¥ä½œåˆ†é… | è´Ÿè½½å‡è¡¡ |'
- en: Despite these advancements, several bottlenecks persist in deep learning models.
    Memory bandwidth often remains a key limitation, particularly for models with
    large working sets or those that require frequent random access. The energy cost
    of data movement, especially between off-chip memory and processing units, continues
    to be a significant concern. For large-scale models, the communication overhead
    in distributed training can become a bottleneck, limiting scaling efficiency.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å–å¾—äº†è¿™äº›è¿›æ­¥ï¼Œä½†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä»å­˜åœ¨å‡ ä¸ªç“¶é¢ˆã€‚å†…å­˜å¸¦å®½é€šå¸¸ä»ç„¶æ˜¯å…³é”®é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¤§å·¥ä½œé›†æˆ–éœ€è¦é¢‘ç¹éšæœºè®¿é—®çš„æ¨¡å‹ã€‚æ•°æ®ç§»åŠ¨çš„èƒ½é‡æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨ç‰‡å¤–å†…å­˜å’Œå¤„ç†å•å…ƒä¹‹é—´ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§é—®é¢˜ã€‚å¯¹äºå¤§è§„æ¨¡æ¨¡å‹ï¼Œåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„é€šä¿¡å¼€é”€å¯èƒ½æˆä¸ºç“¶é¢ˆï¼Œé™åˆ¶æ‰©å±•æ•ˆç‡ã€‚
- en: Energy Consumption Analysis Across Architectures
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ¶æ„é—´çš„èƒ½è€—åˆ†æ
- en: Energy consumption patterns vary dramatically across neural network architectures,
    with implications for both datacenter deployment and edge computing scenarios.
    Each architectural pattern exhibits distinct energy characteristics that inform
    deployment decisions and optimization strategies.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„çš„èƒ½è€—æ¨¡å¼å·®å¼‚å¾ˆå¤§ï¼Œå¯¹æ•°æ®ä¸­å¿ƒéƒ¨ç½²å’Œè¾¹ç¼˜è®¡ç®—åœºæ™¯éƒ½æœ‰å½±å“ã€‚æ¯ç§æ¶æ„æ¨¡å¼éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„èƒ½è€—ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æŒ‡å¯¼éƒ¨ç½²å†³ç­–å’Œä¼˜åŒ–ç­–ç•¥ã€‚
- en: Dense matrix operations in MLPs achieve excellent arithmetic intensity[33](#fn33)
    (computation per data movement) but consume significant absolute energy. Each
    multiply-accumulate operation consumes approximately 4.6pJ, while data movement
    from DRAM costs 640pJ per 32-bit value. For typical MLP inference, 70-80% of energy
    goes to data movement rather than computation, making memory bandwidth optimization
    critical for energy efficiency.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ MLP ä¸­çš„å¯†é›†çŸ©é˜µæ“ä½œå®ç°äº†å‡ºè‰²çš„ç®—æœ¯å¼ºåº¦[33](#fn33)ï¼ˆæ¯æ•°æ®ç§»åŠ¨çš„è®¡ç®—é‡ï¼‰ï¼Œä½†æ¶ˆè€—äº†æ˜¾è‘—çš„ç»å¯¹èƒ½é‡ã€‚æ¯æ¬¡ä¹˜åŠ æ“ä½œæ¶ˆè€—å¤§çº¦ 4.6pJï¼Œè€Œæ•°æ®ä»
    DRAM ç§»åŠ¨åˆ° 32 ä½å€¼éœ€è¦ 640pJã€‚å¯¹äºå…¸å‹çš„ MLP æ¨ç†ï¼Œ70-80% çš„èƒ½é‡ç”¨äºæ•°æ®ç§»åŠ¨è€Œä¸æ˜¯è®¡ç®—ï¼Œè¿™ä½¿å¾—å†…å­˜å¸¦å®½ä¼˜åŒ–å¯¹äºèƒ½æ•ˆè‡³å…³é‡è¦ã€‚
- en: Convolutional operations reduce energy consumption through data reuse but exhibit
    variable efficiency depending on implementation. Im2col-based convolution implementations
    trade memory for simplicity, often doubling memory requirements and energy consumption.
    Direct convolution implementations achieve 3-5x better energy efficiency by eliminating
    redundant data movement, particularly for larger kernel sizes.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯æ“ä½œé€šè¿‡æ•°æ®é‡ç”¨å‡å°‘èƒ½è€—ï¼Œä½†æ•ˆç‡å› å®ç°æ–¹å¼è€Œå¼‚ã€‚åŸºäº Im2col çš„å·ç§¯å®ç°ä»¥ç®€å•æ€§æ¢å–å†…å­˜ï¼Œé€šå¸¸åŠ å€å†…å­˜éœ€æ±‚å’Œèƒ½è€—ã€‚ç›´æ¥å·ç§¯å®ç°é€šè¿‡æ¶ˆé™¤å†—ä½™æ•°æ®ç§»åŠ¨ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå¤§çš„å†…æ ¸å°ºå¯¸ä¸‹ï¼Œå®ç°äº†
    3-5 å€æ›´å¥½çš„èƒ½æ•ˆã€‚
- en: Sequential processing in RNNs creates energy efficiency opportunities through
    temporal data reuse. The constant memory footprint of RNN hidden states enables
    aggressive caching strategies, reducing DRAM access energy by 80-90% for long
    sequences. The sequential dependencies limit parallelization opportunities, often
    resulting in suboptimal hardware utilization and higher energy per operation.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: RNNsä¸­çš„é¡ºåºå¤„ç†é€šè¿‡æ—¶é—´æ•°æ®çš„é‡å¤ä½¿ç”¨åˆ›é€ äº†èƒ½æºæ•ˆç‡çš„æœºä¼šã€‚RNNéšè—çŠ¶æ€çš„æ’å®šå†…å­˜è¶³è¿¹ä½¿å¾—å¯ä»¥é‡‡ç”¨ç§¯æçš„ç¼“å­˜ç­–ç•¥ï¼Œå¯¹äºé•¿åºåˆ—ï¼Œå¯ä»¥å‡å°‘80-90%çš„DRAMè®¿é—®èƒ½æºã€‚é¡ºåºä¾èµ–æ€§é™åˆ¶äº†å¹¶è¡ŒåŒ–æœºä¼šï¼Œé€šå¸¸å¯¼è‡´ç¡¬ä»¶åˆ©ç”¨ç‡ä½ä¸‹å’Œæ¯æ“ä½œæ›´é«˜çš„èƒ½è€—ã€‚
- en: Attention mechanisms in Transformers exhibit the highest energy consumption
    per operation due to quadratic scaling and complex data movement patterns. Self-attention
    operations consume 2-3x more energy per FLOP than standard matrix multiplication
    due to irregular memory access patterns and the need to store attention matrices.
    This energy cost scales quadratically with sequence length, making long-sequence
    processing energy-prohibitive without architectural modifications.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºäºŒæ¬¡æ‰©å±•å’Œå¤æ‚çš„æ•°æ®ç§»åŠ¨æ¨¡å¼ï¼ŒTransformersä¸­çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨æ¯æ“ä½œä¸­è¡¨ç°å‡ºæœ€é«˜çš„èƒ½è€—ã€‚ç”±äºä¸è§„åˆ™çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œéœ€è¦å­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µï¼Œè‡ªæ³¨æ„åŠ›æ“ä½œæ¯FLOPæ¯”æ ‡å‡†çŸ©é˜µä¹˜æ³•æ¶ˆè€—2-3å€çš„èƒ½æºã€‚è¿™ç§èƒ½æºæˆæœ¬ä¸åºåˆ—é•¿åº¦æˆäºŒæ¬¡æ–¹å…³ç³»ï¼Œä½¿å¾—æ²¡æœ‰æ¶æ„ä¿®æ”¹çš„é•¿åºåˆ—å¤„ç†æˆä¸ºèƒ½æºç¦æ­¢çš„ã€‚
- en: System designers must navigate trade-offs in supporting different primitives,
    each with unique characteristics that influence system design and performance.
    For example, optimizing for the dense matrix operations common in MLPs and CNNs
    might come at the cost of flexibility needed for the more dynamic computations
    in attention mechanisms. Supporting large working sets for Transformers might
    require sacrificing energy efficiency.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿè®¾è®¡è€…å¿…é¡»åœ¨ä¸åŒåŸè¯­çš„æ”¯æŒä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œæ¯ä¸ªåŸè¯­éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§ä¼šå½±å“ç³»ç»Ÿè®¾è®¡å’Œæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä¸ºMLPå’ŒCNNä¸­å¸¸è§çš„å¯†é›†çŸ©é˜µè¿ç®—è¿›è¡Œä¼˜åŒ–å¯èƒ½ä¼šä»¥ç‰ºç‰²æ³¨æ„åŠ›æœºåˆ¶ä¸­æ›´åŠ¨æ€è®¡ç®—æ‰€éœ€çš„çµæ´»æ€§ä¸ºä»£ä»·ã€‚æ”¯æŒTransformersçš„å¤§å·¥ä½œé›†å¯èƒ½éœ€è¦ç‰ºç‰²èƒ½æºæ•ˆç‡ã€‚
- en: Balancing these trade-offs requires consideration of the target workloads and
    deployment scenarios. Understanding the nature of each primitive guides the development
    of both hardware and software optimizations in ML systems, allowing designers
    to make informed decisions about system architecture and resource allocation.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³è¡¡è¿™äº›æƒè¡¡éœ€è¦è€ƒè™‘ç›®æ ‡å·¥ä½œè´Ÿè½½å’Œéƒ¨ç½²åœºæ™¯ã€‚ç†è§£æ¯ä¸ªåŸè¯­çš„æœ¬è´¨æŒ‡å¯¼äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­ç¡¬ä»¶å’Œè½¯ä»¶ä¼˜åŒ–çš„å¼€å‘ï¼Œä½¿å¾—è®¾è®¡è€…èƒ½å¤Ÿå°±ç³»ç»Ÿæ¶æ„å’Œèµ„æºåˆ†é…åšå‡ºæ˜æ™ºçš„å†³å®šã€‚
- en: 'The analysis of architectural patterns, computational primitives, and system
    implications establishes the foundation for addressing a practical challenge:
    how do engineers systematically choose the right architecture for their specific
    problem? The diversity of neural network architectures, each optimized for different
    data patterns and computational constraints, requires a structured approach to
    architecture selection. This selection process must consider not only algorithmic
    performance but also deployment constraints covered in [ChapterÂ 2](ch008.xhtml#sec-ml-systems)
    and operational efficiency requirements detailed in [ChapterÂ 13](ch019.xhtml#sec-ml-operations).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¶æ„æ¨¡å¼ã€è®¡ç®—åŸè¯­å’Œç³»ç»Ÿå½±å“çš„åˆ†æä¸ºè§£å†³ä¸€ä¸ªå®é™…æŒ‘æˆ˜å¥ å®šäº†åŸºç¡€ï¼šå·¥ç¨‹å¸ˆå¦‚ä½•ç³»ç»Ÿåœ°é€‰æ‹©é€‚åˆä»–ä»¬ç‰¹å®šé—®é¢˜çš„æ­£ç¡®æ¶æ„ï¼Ÿç¥ç»ç½‘ç»œæ¶æ„çš„å¤šæ ·æ€§ï¼Œæ¯ä¸ªæ¶æ„éƒ½é’ˆå¯¹ä¸åŒçš„æ•°æ®æ¨¡å¼å’Œè®¡ç®—çº¦æŸè¿›è¡Œäº†ä¼˜åŒ–ï¼Œéœ€è¦ä¸€ç§ç»“æ„åŒ–çš„æ¶æ„é€‰æ‹©æ–¹æ³•ã€‚è¿™ä¸ªé€‰æ‹©è¿‡ç¨‹ä¸ä»…è¦è€ƒè™‘ç®—æ³•æ€§èƒ½ï¼Œè¿˜è¦è€ƒè™‘[ç¬¬2ç« ](ch008.xhtml#sec-ml-systems)ä¸­æ¶µç›–çš„éƒ¨ç½²çº¦æŸå’Œ[ç¬¬13ç« ](ch019.xhtml#sec-ml-operations)ä¸­è¯¦ç»†è¯´æ˜çš„æ“ä½œæ•ˆç‡è¦æ±‚ã€‚
- en: Architecture Selection Framework
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¶æ„é€‰æ‹©æ¡†æ¶
- en: 'The exploration of neural network architectures, from dense MLPs to dynamic
    Transformers, demonstrates how each design embodies specific assumptions about
    data structure and computational patterns. MLPs assume arbitrary feature relationships,
    CNNs exploit spatial locality, RNNs capture temporal dependencies, and Transformers
    model complex relational patterns. For practitioners facing real-world problems,
    a question emerges: how to systematically select the appropriate architecture
    for a specific use case?'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å¯†é›†MLPåˆ°åŠ¨æ€Transformersçš„ç¥ç»ç½‘ç»œæ¶æ„çš„æ¢ç´¢å±•ç¤ºäº†æ¯ä¸ªè®¾è®¡å¦‚ä½•ä½“ç°å¯¹æ•°æ®ç»“æ„å’Œè®¡ç®—æ¨¡å¼çš„å…·ä½“å‡è®¾ã€‚MLPå‡è®¾ä»»æ„ç‰¹å¾å…³ç³»ï¼ŒCNNåˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§ï¼ŒRNNæ•è·æ—¶é—´ä¾èµ–æ€§ï¼Œè€ŒTransformersæ¨¡æ‹Ÿå¤æ‚çš„å…³è”æ¨¡å¼ã€‚å¯¹äºé¢ä¸´ç°å®ä¸–ç•Œé—®é¢˜çš„ä»ä¸šè€…æ¥è¯´ï¼Œä¸€ä¸ªé—®é¢˜å‡ºç°äº†ï¼šå¦‚ä½•ç³»ç»Ÿåœ°é€‰æ‹©é€‚ç”¨äºç‰¹å®šç”¨ä¾‹çš„é€‚å½“æ¶æ„ï¼Ÿ
- en: 'The diversity of available architectures overwhelms practitioners, when each
    claims superiority for different scenarios. Successful architecture selection
    requires understanding principles rather than following trends: matching data
    characteristics to architectural strengths, evaluating computational constraints
    against system capabilities, and balancing accuracy requirements with deployment
    realities.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨æ¶æ„çš„å¤šæ ·æ€§è®©ä»ä¸šè€…æ„Ÿåˆ°ä¸çŸ¥æ‰€æªï¼Œå› ä¸ºæ¯ç§æ¶æ„éƒ½å£°ç§°é€‚ç”¨äºä¸åŒçš„åœºæ™¯ã€‚æˆåŠŸçš„æ¶æ„é€‰æ‹©éœ€è¦ç†è§£åŸåˆ™è€Œä¸æ˜¯è·Ÿéšè¶‹åŠ¿ï¼šå°†æ•°æ®ç‰¹æ€§ä¸æ¶æ„ä¼˜åŠ¿ç›¸åŒ¹é…ï¼Œè¯„ä¼°è®¡ç®—çº¦æŸä¸ç³»ç»Ÿèƒ½åŠ›ï¼Œä»¥åŠå¹³è¡¡ç²¾åº¦è¦æ±‚ä¸éƒ¨ç½²ç°å®ã€‚
- en: This systematic approach to architecture selection draws upon the computational
    patterns and system implications explored in the preceding analysis. By understanding
    how different architectures process information and their corresponding resource
    requirements, engineers can make informed decisions that align with both problem
    requirements and practical constraints. The framework integrates principles from
    efficient AI design [ChapterÂ 9](ch015.xhtml#sec-efficient-ai) with practical deployment
    considerations as discussed in ML operations [ChapterÂ 13](ch019.xhtml#sec-ml-operations).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¶æ„é€‰æ‹©çš„ç³»ç»Ÿæ€§æ–¹æ³•å€Ÿé‰´äº†å‰è¿°åˆ†æä¸­æ¢è®¨çš„è®¡ç®—æ¨¡å¼å’Œç³»ç»Ÿå½±å“ã€‚é€šè¿‡ç†è§£ä¸åŒæ¶æ„å¦‚ä½•å¤„ç†ä¿¡æ¯å’Œç›¸åº”çš„èµ„æºéœ€æ±‚ï¼Œå·¥ç¨‹å¸ˆå¯ä»¥åšå‡ºç¬¦åˆé—®é¢˜éœ€æ±‚å’Œå®é™…çº¦æŸçš„æ˜æ™ºå†³ç­–ã€‚è¯¥æ¡†æ¶å°†é«˜æ•ˆAIè®¾è®¡[ç¬¬9ç« ](ch015.xhtml#sec-efficient-ai)çš„åŸåˆ™ä¸MLæ“ä½œ[ç¬¬13ç« ](ch019.xhtml#sec-ml-operations)ä¸­è®¨è®ºçš„å®é™…éƒ¨ç½²è€ƒè™‘å› ç´ ç›¸ç»“åˆã€‚
- en: Data-to-Architecture Mapping
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®åˆ°æ¶æ„æ˜ å°„
- en: 'The first step in systematic architecture selection involves understanding
    how different data types align with architectural strengths. Each neural network
    architecture evolved to address specific patterns in data: MLPs handle arbitrary
    relationships in tabular data, CNNs exploit spatial locality in images, RNNs capture
    temporal dependencies in sequences, and Transformers model complex relational
    patterns where any element might influence any other.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿæ€§æ¶æ„é€‰æ‹©çš„ç¬¬ä¸€ä¸ªæ­¥éª¤æ¶‰åŠç†è§£ä¸åŒæ•°æ®ç±»å‹å¦‚ä½•ä¸æ¶æ„ä¼˜åŠ¿ç›¸åŒ¹é…ã€‚æ¯ä¸ªç¥ç»ç½‘ç»œæ¶æ„éƒ½æ˜¯ä¸ºäº†è§£å†³æ•°æ®ä¸­çš„ç‰¹å®šæ¨¡å¼è€Œæ¼”åŒ–çš„ï¼šMLPså¤„ç†è¡¨æ ¼æ•°æ®ä¸­çš„ä»»æ„å…³ç³»ï¼ŒCNNsåˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´å±€éƒ¨æ€§ï¼ŒRNNsæ•æ‰åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–æ€§ï¼Œè€ŒTransformersåˆ™æ¨¡æ‹Ÿä»»ä½•å…ƒç´ å¯èƒ½å½±å“ä»»ä½•å…¶ä»–å…ƒç´ çš„å¤æ‚å…³ç³»æ¨¡å¼ã€‚
- en: This alignment is not coincidental. It reflects computational trade-offs. Architectures
    that match data characteristics can leverage natural structure for efficiency,
    while mismatched architectures must work against their design assumptions, leading
    to poor performance or excessive resource consumption.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŒ¹é…å¹¶éå¶ç„¶ã€‚å®ƒåæ˜ äº†è®¡ç®—æƒè¡¡ã€‚ä¸æ•°æ®ç‰¹æ€§åŒ¹é…çš„æ¶æ„å¯ä»¥åˆ©ç”¨è‡ªç„¶ç»“æ„æé«˜æ•ˆç‡ï¼Œè€Œä¸è®¾è®¡å‡è®¾ä¸åŒ¹é…çš„æ¶æ„å¿…é¡»ä¸ä¹‹å¯¹æŠ—ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³æˆ–èµ„æºæ¶ˆè€—è¿‡å¤šã€‚
- en: '[TableÂ 4.5](ch010.xhtml#tbl-architecture-selection) provides a systematic framework
    for matching data characteristics to appropriate architectures:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨4.5](ch010.xhtml#tbl-architecture-selection) æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œç”¨äºåŒ¹é…æ•°æ®ç‰¹æ€§åˆ°é€‚å½“çš„æ¶æ„ï¼š'
- en: 'TableÂ 4.5: **Architecture Selection Framework**: Systematic matching of data
    characteristics to neural network architectures based on computational requirements
    and pattern types.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.5ï¼š**æ¶æ„é€‰æ‹©æ¡†æ¶**ï¼šåŸºäºè®¡ç®—éœ€æ±‚å’Œæ¨¡å¼ç±»å‹ï¼Œç³»ç»Ÿåœ°åŒ¹é…æ•°æ®ç‰¹æ€§ä¸ç¥ç»ç½‘ç»œæ¶æ„ã€‚
- en: '| **Architecture** | **Data Type** | **Key Characteristics** | **Example Applications**
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| **æ¶æ„** | **æ•°æ®ç±»å‹** | **å…³é”®ç‰¹æ€§** | **ç¤ºä¾‹åº”ç”¨** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **MLPs** | Tabular/Structured | â€¢ No spatial/temporal â€¢Â ArbitraryÂ relationships
    â€¢Â DenseÂ connectivity | â€¢ Financial modeling â€¢Â MedicalÂ measurements â€¢Â Recommendation
    systems |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| **MLPs** | è¡¨æ ¼/ç»“æ„åŒ– | â€¢ æ— ç©ºé—´/æ—¶é—´ â€¢ ä»»æ„å…³ç³» â€¢ å¯†é›†è¿æ¥ | â€¢ é‡‘èå»ºæ¨¡ â€¢ åŒ»å­¦æµ‹é‡ â€¢ æ¨èç³»ç»Ÿ |'
- en: '| **CNNs** | Spatial/Grid-like | â€¢ Local patterns â€¢Â TranslationÂ equivariance
    â€¢Â ParameterÂ sharing | â€¢ Image recognition â€¢Â 2DÂ sensorÂ data â€¢Â SignalÂ processing
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNs**ï¼‰ | ç©ºé—´/ç½‘æ ¼çŠ¶ | â€¢ å±€éƒ¨æ¨¡å¼ â€¢ å¹³ç§»ç­‰å˜æ€§ â€¢ å‚æ•°å…±äº« | â€¢ å›¾åƒè¯†åˆ« â€¢ 2D ä¼ æ„Ÿå™¨æ•°æ® â€¢ ä¿¡å·å¤„ç†
    |'
- en: '| **RNNs** | Sequential/Temporal | â€¢ Temporal dependencies â€¢Â VariableÂ length
    â€¢Â MemoryÂ across time | â€¢ Time series forecasting â€¢Â Simple language tasks â€¢Â Speech
    recognition |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNs**ï¼‰ | åºåˆ—/æ—¶é—´ | â€¢ æ—¶é—´ä¾èµ–æ€§ â€¢ å˜é•¿ â€¢ æ—¶é—´ä¸Šçš„è®°å¿† | â€¢ æ—¶é—´åºåˆ—é¢„æµ‹ â€¢ ç®€å•è¯­è¨€ä»»åŠ¡ â€¢ è¯­éŸ³è¯†åˆ«
    |'
- en: '| **Transformers** | Complex Relational | â€¢ Long-range dependencies â€¢Â AttentionÂ mechanisms
    â€¢Â Dynamic relationships | â€¢ Language understanding â€¢Â Machine translation â€¢Â Complex
    reasoning tasks |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| **Transformers** | å¤æ‚å…³ç³» | â€¢ é•¿è·ç¦»ä¾èµ–æ€§ â€¢ æ³¨æ„åŠ›æœºåˆ¶ â€¢ åŠ¨æ€å…³ç³» | â€¢ è¯­è¨€ç†è§£ â€¢ æœºå™¨ç¿»è¯‘ â€¢ å¤æ‚æ¨ç†ä»»åŠ¡
    |'
- en: While data characteristics guide initial architecture selection, computational
    constraints often determine final feasibility. Understanding the scaling behavior
    of each architecture enables realistic resource planning and deployment decisions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ•°æ®ç‰¹æ€§æŒ‡å¯¼äº†åˆå§‹æ¶æ„é€‰æ‹©ï¼Œä½†è®¡ç®—çº¦æŸé€šå¸¸å†³å®šäº†æœ€ç»ˆçš„å¯è¡Œæ€§ã€‚ç†è§£æ¯ç§æ¶æ„çš„æ‰©å±•è¡Œä¸ºå¯ä»¥ä¿ƒè¿›åˆç†çš„èµ„æºè§„åˆ’å’Œéƒ¨ç½²å†³ç­–ã€‚
- en: Computational Complexity Considerations
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—å¤æ‚æ€§è€ƒè™‘
- en: Architecture selection must account for computational and memory trade-offs
    that determine deployment feasibility. Each architecture exhibits distinct scaling
    behaviors that create different bottlenecks as problem size increases. Understanding
    these patterns enables realistic resource planning and prevents costly architectural
    mismatches during deployment.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶æ„é€‰æ‹©å¿…é¡»è€ƒè™‘åˆ°è®¡ç®—å’Œå†…å­˜æƒè¡¡ï¼Œè¿™äº›æƒè¡¡å†³å®šäº†éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚æ¯ç§æ¶æ„éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„æ‰©å±•è¡Œä¸ºï¼Œéšç€é—®é¢˜è§„æ¨¡çš„å¢åŠ ï¼Œä¼šå½¢æˆä¸åŒçš„ç“¶é¢ˆã€‚ç†è§£è¿™äº›æ¨¡å¼å¯ä»¥ä¿ƒè¿›åˆç†çš„èµ„æºè§„åˆ’ï¼Œå¹¶åœ¨éƒ¨ç½²æœŸé—´é¿å…æ˜‚è´µçš„æ¶æ„ä¸åŒ¹é…ã€‚
- en: The computational profile of each architecture reflects its underlying design
    philosophy. Dense architectures like MLPs prioritize representational capacity
    through full connectivity, while structured architectures like CNNs achieve efficiency
    through parameter sharing and locality assumptions. Sequential architectures like
    RNNs trade parallelization for memory efficiency, while attention-based architectures
    like Transformers exchange memory for computational flexibility. For completeness,
    we examine these same architectures from both computational scaling and memory
    access perspectives (see [TableÂ 4.3](ch010.xhtml#tbl-arch-complexity)), as each
    viewpoint reveals different optimization opportunities and system design considerations.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç§æ¶æ„çš„è®¡ç®—ç‰¹å¾åæ˜ äº†å…¶èƒŒåçš„è®¾è®¡ç†å¿µã€‚å¯†é›†æ¶æ„å¦‚MLPé€šè¿‡å…¨è¿æ¥ä¼˜å…ˆè€ƒè™‘è¡¨ç¤ºèƒ½åŠ›ï¼Œè€Œç»“æ„åŒ–æ¶æ„å¦‚CNNé€šè¿‡å‚æ•°å…±äº«å’Œå±€éƒ¨æ€§å‡è®¾å®ç°æ•ˆç‡ã€‚é¡ºåºæ¶æ„å¦‚RNNä»¥å†…å­˜æ•ˆç‡ä¸ºä»£ä»·æ¢å–å¹¶è¡ŒåŒ–ï¼Œè€ŒåŸºäºæ³¨æ„åŠ›çš„æ¶æ„å¦‚Transformersåˆ™ä»¥å†…å­˜æ¢å–è®¡ç®—çµæ´»æ€§ã€‚ä¸ºäº†å…¨é¢æ€§ï¼Œæˆ‘ä»¬ä»è®¡ç®—æ‰©å±•å’Œå†…å­˜è®¿é—®ä¸¤ä¸ªè§’åº¦ï¼ˆå‚è§[è¡¨4.3](ch010.xhtml#tbl-arch-complexity)ï¼‰è€ƒå¯Ÿäº†è¿™äº›ç›¸åŒçš„æ¶æ„ï¼Œå› ä¸ºæ¯ä¸ªè§†è§’éƒ½æ­ç¤ºäº†ä¸åŒçš„ä¼˜åŒ–æœºä¼šå’Œç³»ç»Ÿè®¾è®¡è€ƒè™‘ã€‚
- en: '[TableÂ 4.6](ch010.xhtml#tbl-computational-complexity) summarizes the key computational
    characteristics of each architecture:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¡¨4.6](ch010.xhtml#tbl-computational-complexity)æ€»ç»“äº†æ¯ç§æ¶æ„çš„å…³é”®è®¡ç®—ç‰¹æ€§ï¼š'
- en: 'TableÂ 4.6: **Computational Complexity Comparison**: Scaling behaviors and resource
    requirements for major neural network architectures. Variables: <semantics><mi>d</mi><annotation
    encoding="application/x-tex">d</annotation></semantics> = dimension, <semantics><mi>h</mi><annotation
    encoding="application/x-tex">h</annotation></semantics> = hidden size, <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> = kernel size, <semantics><mi>c</mi><annotation
    encoding="application/x-tex">c</annotation></semantics>Â =Â channels, <semantics><mrow><mi>H</mi><mo>,</mo><mi>W</mi></mrow><annotation
    encoding="application/x-tex">H,W</annotation></semantics> = spatial dimensions,
    <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    = time steps, <semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    = sequence length, <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    = batch size.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.6ï¼š**è®¡ç®—å¤æ‚æ€§æ¯”è¾ƒ**ï¼šä¸»è¦ç¥ç»ç½‘ç»œæ¶æ„çš„æ‰©å±•è¡Œä¸ºå’Œèµ„æºéœ€æ±‚ã€‚å˜é‡ï¼š<semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics>
    = ç»´åº¦ï¼Œ<semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics>
    = éšè—å±‚å¤§å°ï¼Œ<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    = æ ¸å¤§å°ï¼Œ<semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics>
    = é€šé“æ•°ï¼Œ<semantics><mrow><mi>H</mi><mo>,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H,W</annotation></semantics>
    = ç©ºé—´ç»´åº¦ï¼Œ<semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    = æ—¶é—´æ­¥é•¿ï¼Œ<semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>
    = åºåˆ—é•¿åº¦ï¼Œ<semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>
    = æ‰¹å¤„ç†å¤§å°ã€‚
- en: '| **Architecture** | **Parameters** | **Forward Pass** | **Memory** | **Parallelization**
    |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| **æ¶æ„** | **å‚æ•°** | **æ­£å‘ä¼ æ’­** | **å†…å­˜** | **å¹¶è¡ŒåŒ–** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **MLPs** | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics>
    perÂ layer | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics>
    perÂ layer | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics>
    weights <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>Ã—</mo><mi>b</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d\times
    b)</annotation></semantics> activations | Excellent Matrix ops parallel |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **MLPs** | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics>
    æ¯å±‚ | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>d</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">O(d_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>d</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">d_{\text{out}})</annotation></semantics>
    æ¯å±‚ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics>
    æƒé‡ <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mo>Ã—</mo><mi>b</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d\times
    b)</annotation></semantics> æ¿€æ´» | ä¼˜ç§€çš„çŸ©é˜µæ“ä½œå¹¶è¡Œ |'
- en: '| **CNNs** | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">O(k^2\times</annotation></semantics> <semantics><mrow><msub><mi>c</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics>
    per layer | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>H</mi><mo>Ã—</mo><mi>W</mi><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">O(H\times W\times</annotation></semantics> <semantics><mrow><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">k^2\times</annotation></semantics> <semantics><mrow><msub><mi>c</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics>
    | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mo>Ã—</mo><mi>W</mi><mo>Ã—</mo><mi>c</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(H\times
    W\times c)</annotation></semantics> features <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo><msup><mi>c</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(k^2\times
    c^2)</annotation></semantics> weights | Good Spatial independence |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| **CNNs** | <semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">O(k^2\times</annotation></semantics> <semantics><mrow><msub><mi>c</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics>
    æ¯å±‚ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>H</mi><mo>Ã—</mo><mi>W</mi><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">O(H\times W\times</annotation></semantics> <semantics><mrow><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo></mrow><annotation
    encoding="application/x-tex">k^2\times</annotation></semantics> <semantics><mrow><msub><mi>c</mi><mtext
    mathvariant="normal">in</mtext></msub><mo>Ã—</mo></mrow><annotation encoding="application/x-tex">c_{\text{in}}\times</annotation></semantics>
    <semantics><mrow><msub><mi>c</mi><mtext mathvariant="normal">out</mtext></msub><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">c_{\text{out}})</annotation></semantics>
    | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mo>Ã—</mo><mi>W</mi><mo>Ã—</mo><mi>c</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(H\times
    W\times c)</annotation></semantics> ç‰¹å¾ | <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>k</mi><mn>2</mn></msup><mo>Ã—</mo><msup><mi>c</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(k^2\times
    c^2)</annotation></semantics> æƒé‡ | è‰¯å¥½çš„ç©ºé—´ç‹¬ç«‹æ€§ |'
- en: '| **RNNs** | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mi>h</mi><mo>Ã—</mo><mi>d</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2+h\times
    d)</annotation></semantics> total | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>T</mi><mo>Ã—</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T\times
    h^2)</annotation></semantics> for <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    timeÂ steps | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h)</annotation></semantics>
    hidden state (constant) | Poor Sequential deps |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| **RNNs** | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mi>h</mi><mo>Ã—</mo><mi>d</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h^2+h\times
    d)</annotation></semantics> æ€»è®¡ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>T</mi><mo>Ã—</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(T\times
    h^2)</annotation></semantics> å¯¹äº <semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics>
    æ—¶é—´æ­¥ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(h)</annotation></semantics>
    éšè—çŠ¶æ€ï¼ˆå¸¸æ•°ï¼‰ | è¾ƒå·®çš„åºåˆ—ä¾èµ–æ€§ |'
- en: '| **Transformers** | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics>
    projections <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>Ã—</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2\times
    h)</annotation></semantics> multi-head | <semantics><mrow><mi>O</mi><mo stretchy="false"
    form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>Ã—</mo><mi>d</mi><mo>+</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">O(n^2\times d+n</annotation></semantics> <semantics><mrow><mi>Ã—</mi><mi>d</mi><mi>Â²</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\times
    dÂ²)</annotation></semantics> per layer | <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics>
    attention <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>Ã—</mo><mi>d</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n\times
    d)</annotation></semantics> sequences | Excellent (positions) Limited by memory
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| **Transformers** | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2)</annotation></semantics>
    æŠ•å½± <semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>Ã—</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(d^2\times
    h)</annotation></semantics> å¤šå¤´ | <semantics><mrow><mi>O</mi><mo stretchy="false"
    form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>Ã—</mo><mi>d</mi><mo>+</mo><mi>n</mi></mrow><annotation
    encoding="application/x-tex">O(n^2\times d+n</annotation></semantics> <semantics><mrow><mi>Ã—</mi><mi>d</mi><mi>Â²</mi><mo
    stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\times
    dÂ²)</annotation></semantics> æ¯å±‚ | <semantics><mrow><mi>O</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">O(n^2)</annotation></semantics> æ³¨æ„åŠ› <semantics><mrow><mi>O</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>n</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n\times
    d)</annotation></semantics> åºåˆ— | ä¼˜ç§€ï¼ˆä½ç½®ï¼‰å—é™äºå†…å­˜ |'
- en: Scalability and Production Considerations
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯æ‰©å±•æ€§å’Œç”Ÿäº§è€ƒè™‘å› ç´ 
- en: Production deployment introduces constraints beyond algorithmic performance,
    including latency requirements, memory limitations, energy budgets, and fault
    tolerance needs. Each architecture exhibits distinct production characteristics
    that determine real-world feasibility.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿäº§éƒ¨ç½²å¼•å…¥äº†è¶…å‡ºç®—æ³•æ€§èƒ½çš„çº¦æŸï¼ŒåŒ…æ‹¬å»¶è¿Ÿè¦æ±‚ã€å†…å­˜é™åˆ¶ã€èƒ½æºé¢„ç®—å’Œå®¹é”™éœ€æ±‚ã€‚æ¯ç§æ¶æ„éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„ç”Ÿäº§ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å†³å®šäº†å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚
- en: MLPs and CNNs scale well across multiple devices through data parallelism, achieving
    near-linear speedups with proper batch size scaling. RNNs face parallelization
    challenges due to sequential dependencies, requiring pipeline parallelism or other
    specialized techniques. Transformers achieve excellent parallelization across
    sequence positions but suffer from quadratic memory scaling that limits batch
    sizes and effective utilization.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs å’Œ CNNs é€šè¿‡æ•°æ®å¹¶è¡Œæ€§åœ¨å¤šä¸ªè®¾å¤‡ä¸Šå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œé€šè¿‡é€‚å½“çš„æ‰¹é‡å¤§å°ç¼©æ”¾å®ç°æ¥è¿‘çº¿æ€§çš„åŠ é€Ÿã€‚RNNs ç”±äºåºåˆ—ä¾èµ–æ€§è€Œé¢ä¸´å¹¶è¡ŒåŒ–æŒ‘æˆ˜ï¼Œéœ€è¦ç®¡é“å¹¶è¡Œæˆ–å…¶ä»–ä¸“ç”¨æŠ€æœ¯ã€‚Transformers
    åœ¨åºåˆ—ä½ç½®ä¸Šå®ç°äº†å‡ºè‰²çš„å¹¶è¡ŒåŒ–ï¼Œä½†å—åˆ°äºŒæ¬¡å†…å­˜ç¼©æ”¾çš„é™åˆ¶ï¼Œè¿™é™åˆ¶äº†æ‰¹é‡å¤§å°å’Œæœ‰æ•ˆåˆ©ç”¨ç‡ã€‚
- en: MLPs provide predictable latency proportional to layer size, making them suitable
    for real-time applications with strict SLA requirements. CNNs exhibit variable
    latency depending on implementation strategy and hardware capabilities, with optimized
    implementations achieving sub-millisecond inference. RNNs create latency dependencies
    on sequence length, making them challenging for interactive applications. Transformers
    provide excellent throughput for batch processing but struggle with single-inference
    latency due to attention overhead.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs æä¾›ä¸å±‚å¤§å°æˆæ¯”ä¾‹çš„å¯é¢„æµ‹å»¶è¿Ÿï¼Œè¿™ä½¿å¾—å®ƒä»¬é€‚åˆå…·æœ‰ä¸¥æ ¼ SLA è¦æ±‚çš„å®æ—¶åº”ç”¨ã€‚CNNs çš„å»¶è¿Ÿæ ¹æ®å®ç°ç­–ç•¥å’Œç¡¬ä»¶èƒ½åŠ›è€Œå˜åŒ–ï¼Œä¼˜åŒ–å®ç°å¯ä»¥è¾¾åˆ°äºšæ¯«ç§’çº§çš„æ¨ç†ã€‚RNNs
    ä¼šæ ¹æ®åºåˆ—é•¿åº¦åˆ›å»ºå»¶è¿Ÿä¾èµ–ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨äº¤äº’å¼åº”ç”¨ä¸­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚Transformers ä¸ºæ‰¹é‡å¤„ç†æä¾›äº†å‡ºè‰²çš„ååé‡ï¼Œä½†ç”±äºæ³¨æ„åŠ›å¼€é”€ï¼Œåœ¨å•æ¬¡æ¨ç†å»¶è¿Ÿæ–¹é¢å­˜åœ¨å›°éš¾ã€‚
- en: Memory requirements vary significantly across architectures in production environments.
    MLPs require fixed memory proportional to model size, enabling straightforward
    capacity planning. CNNs need variable memory for feature maps that scales with
    input resolution, requiring dynamic memory management for variable-size inputs.
    RNNs maintain constant memory for hidden states but may require unbounded memory
    for very long sequences. Transformers face quadratic memory growth that creates
    hard limits on sequence length in production.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä¸åŒæ¶æ„çš„å†…å­˜éœ€æ±‚å·®å¼‚å¾ˆå¤§ã€‚MLPséœ€è¦ä¸æ¨¡å‹å¤§å°æˆæ¯”ä¾‹çš„å›ºå®šå†…å­˜ï¼Œè¿™ä½¿å¾—å®¹é‡è§„åˆ’å˜å¾—ç®€å•ã€‚CNNséœ€è¦å¯å˜å†…å­˜æ¥å­˜å‚¨ç‰¹å¾å›¾ï¼Œå…¶å¤§å°ä¸è¾“å…¥åˆ†è¾¨ç‡æˆæ¯”ä¾‹ï¼Œå¯¹äºå¯å˜å¤§å°çš„è¾“å…¥éœ€è¦åŠ¨æ€å†…å­˜ç®¡ç†ã€‚RNNsç»´æŠ¤éšè—çŠ¶æ€çš„å¸¸é‡å†…å­˜ï¼Œä½†å¯¹äºéå¸¸é•¿çš„åºåˆ—å¯èƒ½éœ€è¦æ— ç•Œçš„å†…å­˜ã€‚Transformersé¢ä¸´äºŒæ¬¡å†…å­˜å¢é•¿ï¼Œè¿™åœ¨ç”Ÿäº§ä¸­ä¸ºåºåˆ—é•¿åº¦è®¾ç½®äº†ç¡¬æ€§é™åˆ¶ã€‚
- en: Fault tolerance and recovery characteristics differ significantly between architectures.
    MLPs and CNNs exhibit stateless computation that enables straightforward checkpointing
    and recovery. RNNs maintain temporal state that complicates distributed training
    and failure recovery procedures. Transformers combine stateless computation with
    massive memory requirements, making checkpoint sizes a practical concern for large
    models.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: å®¹é”™å’Œæ¢å¤ç‰¹æ€§åœ¨æ¶æ„ä¹‹é—´å·®å¼‚æ˜¾è‘—ã€‚MLPså’ŒCNNsè¡¨ç°å‡ºæ— çŠ¶æ€è®¡ç®—ï¼Œè¿™ä½¿å¾—ç®€å•çš„æ£€æŸ¥ç‚¹å’Œæ¢å¤æˆä¸ºå¯èƒ½ã€‚RNNsç»´æŠ¤æ—¶é—´çŠ¶æ€ï¼Œè¿™ä½¿åˆ†å¸ƒå¼è®­ç»ƒå’Œæ•…éšœæ¢å¤è¿‡ç¨‹å¤æ‚åŒ–ã€‚Transformersç»“åˆäº†æ— çŠ¶æ€è®¡ç®—å’Œå·¨å¤§çš„å†…å­˜éœ€æ±‚ï¼Œä½¿å¾—æ£€æŸ¥ç‚¹å¤§å°æˆä¸ºå¤§å‹æ¨¡å‹çš„ä¸€ä¸ªå®é™…å…³æ³¨ç‚¹ã€‚
- en: Hardware mapping efficiency varies considerably across architectural patterns.
    Modern MLPs achieve 80-90% of peak hardware performance on specialized tensor
    units. CNNs reach 60-75% efficiency depending on layer configuration and memory
    hierarchy design. RNNs typically achieve 30-50% of peak performance due to sequential
    constraints and irregular memory access patterns. Transformers achieve 70-85%
    efficiency for large batch sizes but drop significantly for small batches due
    to attention overhead.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶æ˜ å°„æ•ˆç‡åœ¨æ¶æ„æ¨¡å¼ä¹‹é—´å·®å¼‚å¾ˆå¤§ã€‚ç°ä»£MLPsåœ¨ä¸“é—¨çš„å¼ é‡å•å…ƒä¸Šå®ç°äº†80-90%çš„å³°å€¼ç¡¬ä»¶æ€§èƒ½ã€‚CNNsçš„æ•ˆç‡å–å†³äºå±‚é…ç½®å’Œå†…å­˜å±‚æ¬¡ç»“æ„è®¾è®¡ï¼Œé€šå¸¸åœ¨60-75%ä¹‹é—´ã€‚RNNsç”±äºé¡ºåºçº¦æŸå’Œä¸è§„åˆ™çš„å†…å­˜è®¿é—®æ¨¡å¼ï¼Œé€šå¸¸åªèƒ½è¾¾åˆ°å³°å€¼æ€§èƒ½çš„30-50%ã€‚Transformerså¯¹äºå¤§æ‰¹æ¬¡å¤§å°å¯ä»¥è¾¾åˆ°70-85%çš„æ•ˆç‡ï¼Œä½†å¯¹äºå°æ‰¹æ¬¡ç”±äºæ³¨æ„åŠ›å¼€é”€è€Œæ˜¾è‘—ä¸‹é™ã€‚
- en: Hardware Mapping and Optimization Strategies
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶æ˜ å°„å’Œä¼˜åŒ–ç­–ç•¥
- en: Different architectural patterns require distinct optimization strategies for
    efficient hardware mapping. Understanding these patterns enables systematic performance
    tuning and hardware selection decisions.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ¶æ„æ¨¡å¼éœ€è¦ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥æ¥å®ç°é«˜æ•ˆçš„ç¡¬ä»¶æ˜ å°„ã€‚ç†è§£è¿™äº›æ¨¡å¼èƒ½å¤Ÿå®ç°ç³»ç»Ÿæ€§çš„æ€§èƒ½è°ƒæ•´å’Œç¡¬ä»¶é€‰æ‹©å†³ç­–ã€‚
- en: 'Dense matrix operations in MLPs map naturally to tensor processing units and
    GPU tensor cores. These operations benefit from several key optimizations: matrix
    tiling to fit cache hierarchies, mixed-precision computation to double throughput,
    and operation fusion to reduce memory traffic. Optimal tile sizes depend on cache
    hierarchy, typically 64x64 for L1 cache and 256x256 for L2, while tensor cores
    achieve peak efficiency with specific dimension multiples such as 16x16 blocks
    for Volta architecture.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: MLPsä¸­çš„å¯†é›†çŸ©é˜µè¿ç®—è‡ªç„¶æ˜ å°„åˆ°å¼ é‡å¤„ç†å•å…ƒå’ŒGPUå¼ é‡æ ¸å¿ƒã€‚è¿™äº›æ“ä½œå—ç›Šäºå‡ ä¸ªå…³é”®ä¼˜åŒ–ï¼šçŸ©é˜µåˆ†å—ä»¥é€‚åº”ç¼“å­˜å±‚æ¬¡ç»“æ„ï¼Œæ··åˆç²¾åº¦è®¡ç®—ä»¥åŠ å€ååé‡ï¼Œä»¥åŠæ“ä½œèåˆä»¥å‡å°‘å†…å­˜æµé‡ã€‚æœ€ä¼˜çš„åˆ†å—å¤§å°å–å†³äºç¼“å­˜å±‚æ¬¡ç»“æ„ï¼Œé€šå¸¸æ˜¯64x64ç”¨äºL1ç¼“å­˜å’Œ256x256ç”¨äºL2ï¼Œè€Œå¼ é‡æ ¸å¿ƒé€šè¿‡ç‰¹å®šçš„ç»´åº¦å€æ•°è¾¾åˆ°å³°å€¼æ•ˆç‡ï¼Œä¾‹å¦‚Voltaæ¶æ„ä¸­çš„16x16å—ã€‚
- en: CNNs benefit from specialized convolution algorithms and data layout optimizations
    that differ significantly from dense matrix operations. Im2col transformations
    convert convolutions to matrix multiplication but double memory usage. Winograd
    algorithms reduce arithmetic complexity by 2.25x for 3x3 convolutions at the cost
    of numerical stability. Direct convolution with custom kernels achieves optimal
    memory efficiency but requires architecture-specific tuning.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: CNNså¾—ç›Šäºä¸“é—¨çš„å·ç§¯ç®—æ³•å’Œæ•°æ®å¸ƒå±€ä¼˜åŒ–ï¼Œè¿™äº›ä¼˜åŒ–ä¸å¯†é›†çŸ©é˜µè¿ç®—æœ‰æ˜¾è‘—å·®å¼‚ã€‚Im2colå˜æ¢å°†å·ç§¯è½¬æ¢ä¸ºçŸ©é˜µä¹˜æ³•ï¼Œä½†ä¼šåŠ å€å†…å­˜ä½¿ç”¨ã€‚Winogradç®—æ³•é€šè¿‡é™ä½3x3å·ç§¯çš„ç®—æœ¯å¤æ‚åº¦2.25å€æ¥æé«˜æ•ˆç‡ï¼Œä½†ä»¥æ•°å€¼ç¨³å®šæ€§ä¸ºä»£ä»·ã€‚ä½¿ç”¨è‡ªå®šä¹‰å†…æ ¸çš„ç›´æ¥å·ç§¯å®ç°äº†æœ€ä¼˜çš„å†…å­˜æ•ˆç‡ï¼Œä½†éœ€è¦é’ˆå¯¹ç‰¹å®šæ¶æ„è¿›è¡Œè°ƒæ•´ã€‚
- en: RNNs require different optimization approaches due to their temporal dependencies.
    Loop unrolling reduces control overhead but increases memory usage. State vectorization
    enables SIMD operations across multiple sequences. Wavefront parallelization exploits
    independence across timesteps for bidirectional processing.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: RNN ç”±äºå…¶æ—¶é—´ä¾èµ–æ€§ï¼Œéœ€è¦ä¸åŒçš„ä¼˜åŒ–æ–¹æ³•ã€‚å¾ªç¯å±•å¼€å‡å°‘äº†æ§åˆ¶å¼€é”€ï¼Œä½†å¢åŠ äº†å†…å­˜ä½¿ç”¨ã€‚çŠ¶æ€å‘é‡åŒ–ä½¿å¾—å¯ä»¥åœ¨å¤šä¸ªåºåˆ—ä¸Šæ‰§è¡Œ SIMD æ“ä½œã€‚æ³¢å‰å¹¶è¡ŒåŒ–åˆ©ç”¨æ—¶é—´æ­¥ä¹‹é—´çš„ç‹¬ç«‹æ€§è¿›è¡ŒåŒå‘å¤„ç†ã€‚
- en: Transformers demand specialized attention optimizations due to their quadratic
    complexity. FlashAttention algorithms reduce memory usage from O(nÂ²) to O(n) through
    online softmax computation and gradient recomputation. Sparse attention patterns
    including local, strided, and random approaches maintain modeling capability while
    reducing complexity. Multi-query attention shares key and value projections across
    heads, reducing memory bandwidth by 30-50%.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer ç”±äºå…¶äºŒæ¬¡æ–¹å¤æ‚åº¦ï¼Œéœ€è¦ä¸“é—¨çš„æ³¨æ„åŠ›ä¼˜åŒ–ã€‚FlashAttention ç®—æ³•é€šè¿‡åœ¨çº¿ softmax è®¡ç®—å’Œæ¢¯åº¦é‡è®¡ç®—ï¼Œå°†å†…å­˜ä½¿ç”¨ä»
    O(nÂ²) é™ä½åˆ° O(n)ã€‚åŒ…æ‹¬å±€éƒ¨ã€æ­¥è¿›å’Œéšæœºæ–¹æ³•åœ¨å†…çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼åœ¨é™ä½å¤æ‚åº¦çš„åŒæ—¶ä¿æŒå»ºæ¨¡èƒ½åŠ›ã€‚å¤šæŸ¥è¯¢æ³¨æ„åŠ›åœ¨å¤´éƒ¨ä¹‹é—´å…±äº«é”®å’Œå€¼æŠ•å½±ï¼Œå°†å†…å­˜å¸¦å®½é™ä½äº†
    30-50%ã€‚
- en: Multi-Layer Perceptrons represent the most straightforward computational pattern,
    with costs dominated by matrix multiplications. The dense connectivity that enables
    MLPs to model arbitrary relationships comes at the price of quadratic parameter
    growth with layer width. Each neuron connects to every neuron in the previous
    layer, creating large parameter counts that grow quadratically with network width.
    The computation is dominated by matrix-vector products, which are highly optimized
    on modern hardware. Matrix operations are inherently parallel and map efficiently
    to GPU architectures, with each output neuron computed independently. The optimization
    techniques for reducing these parameter counts, including pruning and low-rank
    approximations specifically targeting dense layers, are covered in [ChapterÂ 10](ch016.xhtml#sec-model-optimizations).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ä»£è¡¨äº†æœ€ç›´æ¥çš„è®¡ç®—æ¨¡å¼ï¼Œå…¶æˆæœ¬ä¸»è¦ç”±çŸ©é˜µä¹˜æ³•å†³å®šã€‚ä½¿ MLP èƒ½å¤Ÿå»ºæ¨¡ä»»æ„å…³ç³»çš„å¯†é›†è¿æ¥ä»¥äºŒæ¬¡æ–¹å‚æ•°å¢é•¿ä¸ºä»£ä»·ã€‚æ¯ä¸ªç¥ç»å…ƒéƒ½ä¸å‰ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒç›¸è¿ï¼Œåˆ›å»ºäº†å¤§é‡çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°éšç€ç½‘ç»œå®½åº¦çš„å¢åŠ è€ŒäºŒæ¬¡æ–¹å¢é•¿ã€‚è®¡ç®—ä¸»è¦ç”±çŸ©é˜µ-å‘é‡ä¹˜æ³•ä¸»å¯¼ï¼Œè¿™åœ¨ç°ä»£ç¡¬ä»¶ä¸Šå¾—åˆ°äº†é«˜åº¦ä¼˜åŒ–ã€‚çŸ©é˜µæ“ä½œæœ¬è´¨ä¸Šæ˜¯å¹¶è¡Œçš„ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æ˜ å°„åˆ°
    GPU æ¶æ„ï¼Œæ¯ä¸ªè¾“å‡ºç¥ç»å…ƒéƒ½æ˜¯ç‹¬ç«‹è®¡ç®—çš„ã€‚é™ä½è¿™äº›å‚æ•°æ•°é‡çš„ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¿®å‰ªå’Œä½ç§©è¿‘ä¼¼ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¯†é›†å±‚çš„ï¼Œåœ¨[ç¬¬ 10 ç« ](ch016.xhtml#sec-model-optimizations)ä¸­è¿›è¡Œäº†ä»‹ç»ã€‚
- en: Convolutional Neural Networks achieve computational efficiency through parameter
    sharing and spatial locality, but their costs scale with both spatial dimensions
    and channel depth. The convolution operationâ€™s computational intensity depends
    heavily on kernel size and feature map resolution. Parameter sharing across spatial
    locations dramatically reduces memory compared to equivalent MLPs, while computational
    cost grows linearly with image resolution and quadratically with kernel size.
    Feature map memory dominates usage and becomes prohibitive for high-resolution
    inputs. Spatial independence enables parallel processing across different spatial
    locations and channels, though memory bandwidth often becomes the limiting factor.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡å‚æ•°å…±äº«å’Œç©ºé—´å±€éƒ¨æ€§å®ç°è®¡ç®—æ•ˆç‡ï¼Œä½†å…¶æˆæœ¬éšç€ç©ºé—´ç»´åº¦å’Œé€šé“æ·±åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚å·ç§¯æ“ä½œçš„è®¡ç®—å¼ºåº¦å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå†…æ ¸å¤§å°å’Œç‰¹å¾å›¾åˆ†è¾¨ç‡ã€‚ä¸ç­‰æ•ˆçš„
    MLP ç›¸æ¯”ï¼Œè·¨ç©ºé—´ä½ç½®çš„å‚æ•°å…±äº«å¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œè€Œè®¡ç®—æˆæœ¬éšç€å›¾åƒåˆ†è¾¨ç‡çº¿æ€§å¢é•¿ï¼Œéšç€å†…æ ¸å¤§å°äºŒæ¬¡æ–¹å¢é•¿ã€‚ç‰¹å¾å›¾å†…å­˜çš„ä½¿ç”¨å ä¸»å¯¼åœ°ä½ï¼Œå¯¹äºé«˜åˆ†è¾¨ç‡è¾“å…¥å˜å¾—éš¾ä»¥æ‰¿å—ã€‚ç©ºé—´ç‹¬ç«‹æ€§ä½¿å¾—å¯ä»¥åœ¨ä¸åŒçš„ç©ºé—´ä½ç½®å’Œé€šé“ä¹‹é—´è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œå°½ç®¡å†…å­˜å¸¦å®½ç»å¸¸æˆä¸ºé™åˆ¶å› ç´ ã€‚
- en: Recurrent Neural Networks optimize for memory efficiency at the cost of parallelization.
    Their sequential nature creates computational bottlenecks but enables processing
    of variable-length sequences with constant memory overhead. The hidden-to-hidden
    connections (<semantics><msup><mi>h</mi><mn>2</mn></msup><annotation encoding="application/x-tex">h^2</annotation></semantics>
    term) dominate parameter count for large hidden states. Sequential dependencies
    prevent parallel processing across time, making RNNs inherently slower than feedforward
    alternatives. Their constant memory usage for hidden state storage makes RNNs
    memory-efficient for long sequences, with this efficiency coming at the cost of
    computational speed.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œåœ¨ç‰ºç‰²å¹¶è¡ŒåŒ–çš„ä»£ä»·ä¸‹ä¼˜åŒ–å†…å­˜æ•ˆç‡ã€‚å®ƒä»¬çš„é¡ºåºç‰¹æ€§åˆ›é€ äº†è®¡ç®—ç“¶é¢ˆï¼Œä½†å…è®¸ä»¥æ’å®šçš„å†…å­˜å¼€é”€å¤„ç†å¯å˜é•¿åº¦çš„åºåˆ—ã€‚å¯¹äºå¤§éšçŠ¶æ€ï¼Œéšè—åˆ°éšè—çš„è¿æ¥ï¼ˆ<semantics><msup><mi>h</mi><mn>2</mn></msup><annotation
    encoding="application/x-tex">h^2</annotation></semantics>é¡¹ï¼‰å å‚æ•°æ•°é‡çš„ä¸»å¯¼åœ°ä½ã€‚é¡ºåºä¾èµ–æ€§é˜»æ­¢äº†æ—¶é—´ä¸Šçš„å¹¶è¡Œå¤„ç†ï¼Œä½¿å¾—å¾ªç¯ç¥ç»ç½‘ç»œåœ¨æœ¬è´¨ä¸Šæ¯”å‰é¦ˆæ›¿ä»£æ–¹æ¡ˆæ…¢ã€‚å®ƒä»¬å¯¹éšè—çŠ¶æ€å­˜å‚¨çš„æ’å®šå†…å­˜ä½¿ç”¨ä½¿å¾—å¾ªç¯ç¥ç»ç½‘ç»œåœ¨é•¿åºåˆ—ä¸­å†…å­˜æ•ˆç‡é«˜ï¼Œä½†è¿™ç§æ•ˆç‡æ˜¯ä»¥è®¡ç®—é€Ÿåº¦ä¸ºä»£ä»·çš„ã€‚
- en: Transformers achieve maximum flexibility through attention mechanisms but pay
    a steep price in memory usage. Their quadratic scaling with sequence length creates
    limits on the sequences they can process. Parameter count scales with model dimension
    but remains independent of sequence length. The <semantics><msup><mi>n</mi><mn>2</mn></msup><annotation
    encoding="application/x-tex">n^2</annotation></semantics> term from attention
    computation dominates for long sequences, while the <semantics><mrow><mi>n</mi><mo>Ã—</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation
    encoding="application/x-tex">n \times d^2</annotation></semantics> term from feed-forward
    layers dominates for short sequences. Attention matrices create the primary memory
    bottleneck, as each attention head must store pairwise similarities between all
    sequence positions, leading to prohibitive memory usage for long sequences. While
    parallelization is excellent across sequence positions and attention heads, the
    quadratic memory requirement often forces smaller batch sizes, limiting effective
    parallelization.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerå®ç°äº†æœ€å¤§çš„çµæ´»æ€§ï¼Œä½†ä»˜å‡ºäº†é«˜æ˜‚çš„å†…å­˜ä½¿ç”¨ä»£ä»·ã€‚å®ƒä»¬ä¸åºåˆ—é•¿åº¦çš„äºŒæ¬¡å…³ç³»é™åˆ¶äº†å®ƒä»¬å¯ä»¥å¤„ç†çš„åºåˆ—é•¿åº¦ã€‚å‚æ•°æ•°é‡ä¸æ¨¡å‹ç»´åº¦æˆæ¯”ä¾‹ï¼Œä½†ä¸åºåˆ—é•¿åº¦æ— å…³ã€‚å¯¹äºé•¿åºåˆ—ï¼Œæ³¨æ„åŠ›è®¡ç®—ä¸­çš„<semantics><msup><mi>n</mi><mn>2</mn></msup><annotation
    encoding="application/x-tex">n^2</annotation></semantics>é¡¹å ä¸»å¯¼åœ°ä½ï¼Œè€Œæ¥è‡ªå‰é¦ˆå±‚çš„<semantics><mrow><mi>n</mi><mo>Ã—</mo><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation
    encoding="application/x-tex">n \times d^2</annotation></semantics>é¡¹åœ¨çŸ­åºåˆ—ä¸­å ä¸»å¯¼åœ°ä½ã€‚æ³¨æ„åŠ›çŸ©é˜µåˆ›å»ºäº†ä¸»è¦çš„å†…å­˜ç“¶é¢ˆï¼Œå› ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´å¿…é¡»å­˜å‚¨æ‰€æœ‰åºåˆ—ä½ç½®ä¹‹é—´çš„æˆå¯¹ç›¸ä¼¼æ€§ï¼Œå¯¼è‡´é•¿åºåˆ—çš„å†…å­˜ä½¿ç”¨ä¸å¯æ‰¿å—ã€‚è™½ç„¶åºåˆ—ä½ç½®å’Œæ³¨æ„åŠ›å¤´ä¹‹é—´çš„å¹¶è¡ŒåŒ–éå¸¸å‡ºè‰²ï¼Œä½†äºŒæ¬¡å†…å­˜éœ€æ±‚é€šå¸¸è¿«ä½¿æ‰¹é‡å¤§å°æ›´å°ï¼Œé™åˆ¶äº†æœ‰æ•ˆçš„å¹¶è¡ŒåŒ–ã€‚
- en: These complexity patterns define optimal domains for each architecture. MLPs
    excel when parameter efficiency is not critical, CNNs dominate for moderate-resolution
    spatial data, RNNs remain viable for very long sequences where memory is constrained,
    and Transformers excel for complex relational tasks where their computational
    cost justifies their computational cost through superior performance.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¤æ‚æ€§æ¨¡å¼å®šä¹‰äº†æ¯ä¸ªæ¶æ„çš„æœ€ä½³é¢†åŸŸã€‚å½“å‚æ•°æ•ˆç‡ä¸æ˜¯å…³é”®å› ç´ æ—¶ï¼ŒMLPsè¡¨ç°å“è¶Šï¼›å¯¹äºä¸­ç­‰åˆ†è¾¨ç‡çš„ç©ºé–“æ•°æ®ï¼ŒCNNså ä¸»å¯¼åœ°ä½ï¼›RNNsåœ¨å†…å­˜å—é™çš„éå¸¸é•¿åºåˆ—ä¸­ä»ç„¶å¯è¡Œï¼›è€ŒTransformeråœ¨å¤æ‚å…³ç³»ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå…¶è®¡ç®—æˆæœ¬é€šè¿‡ä¼˜è¶Šçš„æ€§èƒ½å¾—åˆ°è¯æ˜ã€‚
- en: Architectural Comparison Summary
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å»ºç­‘æ¯”è¾ƒæ€»ç»“
- en: The systematic analysis of each architectural family reveals distinct computational
    signatures that determine their suitability for different deployment scenarios.
    [TableÂ 4.7](ch010.xhtml#tbl-architecture-comparison) provides a quantitative comparison
    across key systems metrics, enabling engineers to make informed trade-offs between
    model capability and computational constraints.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªæ¶æ„å®¶æ—çš„ç³»ç»Ÿåˆ†ææ­ç¤ºäº†ä¸åŒçš„è®¡ç®—ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å†³å®šäº†å®ƒä»¬åœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚[è¡¨4.7](ch010.xhtml#tbl-architecture-comparison)æä¾›äº†å…³é”®ç³»ç»ŸæŒ‡æ ‡çš„å®šé‡æ¯”è¾ƒï¼Œä½¿å·¥ç¨‹å¸ˆèƒ½å¤Ÿåœ¨æ¨¡å‹èƒ½åŠ›å’Œè®¡ç®—çº¦æŸä¹‹é—´åšå‡ºæ˜æ™ºçš„æƒè¡¡ã€‚
- en: 'TableÂ 4.7: **Quantitative Architecture Comparison**: Computational complexity
    analysis across four major neural network architectures. Parameters scale with
    network dimensions (N=neurons, M=inputs, K=kernel size, C=channels, D=depth, H=hidden
    size, T=time steps, d=model dimension). Memory requirements reflect peak activation
    storage during training. Parallelism indicates amenability to parallel computation.
    Key bottlenecks represent primary performance limiting factors in typical deployments.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.7ï¼š**å®šé‡æ¶æ„æ¯”è¾ƒ**ï¼šå¯¹å››ä¸ªä¸»è¦ç¥ç»ç½‘ç»œæ¶æ„çš„è®¡ç®—å¤æ‚æ€§åˆ†æã€‚å‚æ•°éšç½‘ç»œç»´åº¦ç¼©æ”¾ï¼ˆN=ç¥ç»å…ƒï¼ŒM=è¾“å…¥ï¼ŒK=æ ¸å¤§å°ï¼ŒC=é€šé“ï¼ŒD=æ·±åº¦ï¼ŒH=éšè—å±‚å¤§å°ï¼ŒT=æ—¶é—´æ­¥é•¿ï¼Œd=æ¨¡å‹ç»´åº¦ï¼‰ã€‚å†…å­˜éœ€æ±‚åæ˜ è®­ç»ƒæœŸé—´çš„å³°å€¼æ¿€æ´»å­˜å‚¨ã€‚å¹¶è¡Œæ€§è¡¨ç¤ºé€‚åˆå¹¶è¡Œè®¡ç®—ã€‚å…³é”®ç“¶é¢ˆä»£è¡¨å…¸å‹éƒ¨ç½²ä¸­çš„ä¸»è¦æ€§èƒ½é™åˆ¶å› ç´ ã€‚
- en: '| **Metric** | **MLP** | **CNN** | **RNN** | **Transformer** |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| **æŒ‡æ ‡** | **MLP** | **CNN** | **RNN** | **Transformer** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Parameters** | O(NÃ—M) | O(KÂ²Ã—CÃ—D) | O(HÂ²) | O(NÃ—dÂ²) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| **å‚æ•°** | O(NÃ—M) | O(KÂ²Ã—CÃ—D) | O(HÂ²) | O(NÃ—dÂ²) |'
- en: '| **FLOPs/Sample** | O(NÃ—M) | O(KÂ²Ã—HÃ—WÃ—C) | O(TÃ—HÂ²) | O(NÂ²Ã—d) |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| **ï¼ˆæ¯æ ·æœ¬FLOPsï¼‰** | O(NÃ—M) | O(KÂ²Ã—HÃ—WÃ—C) | O(TÃ—HÂ²) | O(NÂ²Ã—d) |'
- en: '| **Memory** | O(BÃ—M) | O(BÃ—HÃ—WÃ—C) | O(BÃ—TÃ—H) | O(BÃ—NÂ²) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| **å†…å­˜** | O(BÃ—M) | O(BÃ—HÃ—WÃ—C) | O(BÃ—TÃ—H) | O(BÃ—NÂ²) |'
- en: '| **(Activations)** |  |  |  |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| **ï¼ˆæ¿€æ´»ï¼‰** |  |  |  |  |'
- en: '| **Parallelism** | High | High | Low (Sequential) | High |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| **å¹¶è¡Œæ€§** | é«˜ | é«˜ | ä½ï¼ˆé¡ºåºï¼‰ | é«˜ |'
- en: '| **Key Bottleneck** | Memory BW | Memory BW | Sequential Dep. | Memory (NÂ²)
    |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| **å…³é”®ç“¶é¢ˆ** | å†…å­˜å¸¦å®½ | å†…å­˜å¸¦å®½ | é¡ºåºä¾èµ– | å†…å­˜ï¼ˆNÂ²ï¼‰ |'
- en: This quantitative framework enables systematic architecture selection by explicitly
    revealing the scaling behaviors that determine computational feasibility. MLPs
    and CNNs achieve high parallelism but face memory bandwidth constraints as model
    size grows. RNNs maintain constant memory usage but sacrifice parallelism for
    sequential processing. Transformers achieve maximum expressivity but face quadratic
    memory scaling that limits sequence length. Understanding these trade-offs proves
    essential for matching architectural choices to deployment constraints.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®šé‡æ¡†æ¶é€šè¿‡æ˜ç¡®æ­ç¤ºå†³å®šè®¡ç®—å¯è¡Œæ€§çš„ç¼©æ”¾è¡Œä¸ºï¼Œä½¿ç³»ç»Ÿæ€§çš„æ¶æ„é€‰æ‹©æˆä¸ºå¯èƒ½ã€‚MLPå’ŒCNNå®ç°é«˜å¹¶è¡Œæ€§ï¼Œä½†éšç€æ¨¡å‹å¤§å°çš„å¢é•¿é¢ä¸´å†…å­˜å¸¦å®½é™åˆ¶ã€‚RNNä¿æŒæ’å®šçš„å†…å­˜ä½¿ç”¨ï¼Œä½†ä¸ºäº†é¡ºåºå¤„ç†ç‰ºç‰²äº†å¹¶è¡Œæ€§ã€‚Transformerå®ç°æœ€å¤§è¡¨è¾¾èƒ½åŠ›ï¼Œä½†é¢ä¸´äºŒæ¬¡å†…å­˜ç¼©æ”¾ï¼Œé™åˆ¶äº†åºåˆ—é•¿åº¦ã€‚ç†è§£è¿™äº›æƒè¡¡å¯¹äºå°†æ¶æ„é€‰æ‹©ä¸éƒ¨ç½²é™åˆ¶ç›¸åŒ¹é…è‡³å…³é‡è¦ã€‚
- en: Decision Framework
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å†³ç­–æ¡†æ¶
- en: 'Effective architecture selection requires balancing multiple competing factors:
    data characteristics, computational resources, performance requirements, and deployment
    constraints. While data patterns provide initial guidance and complexity analysis
    establishes feasibility bounds, final architectural choices often involve nuanced
    trade-offs demanding systematic evaluation.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆçš„æ¶æ„é€‰æ‹©éœ€è¦å¹³è¡¡å¤šä¸ªç›¸äº’ç«äº‰çš„å› ç´ ï¼šæ•°æ®ç‰¹å¾ã€è®¡ç®—èµ„æºã€æ€§èƒ½è¦æ±‚å’Œéƒ¨ç½²é™åˆ¶ã€‚è™½ç„¶æ•°æ®æ¨¡å¼æä¾›åˆå§‹æŒ‡å¯¼ï¼Œå¤æ‚æ€§åˆ†æå»ºç«‹å¯è¡Œæ€§ç•Œé™ï¼Œä½†æœ€ç»ˆçš„æ¶æ„é€‰æ‹©é€šå¸¸æ¶‰åŠç»†å¾®çš„æƒè¡¡ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„è¯„ä¼°ã€‚
- en: '![](../media/file65.svg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file65.svg)'
- en: 'FigureÂ 4.12: **Architecture Selection Decision Framework**: A systematic flowchart
    for choosing neural network architectures based on data characteristics and deployment
    constraints. The process begins with data type identification (text/sequences/images/tabular)
    to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then
    iteratively evaluates memory budget, computational cost, inference speed, accuracy
    targets, and hardware compatibility.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.12ï¼š**æ¶æ„é€‰æ‹©å†³ç­–æ¡†æ¶**ï¼šåŸºäºæ•°æ®ç‰¹å¾å’Œéƒ¨ç½²é™åˆ¶çš„ç³»ç»Ÿæµç¨‹å›¾ï¼Œç”¨äºé€‰æ‹©ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥è¿‡ç¨‹ä»æ•°æ®ç±»å‹è¯†åˆ«ï¼ˆæ–‡æœ¬/åºåˆ—/å›¾åƒ/è¡¨æ ¼ï¼‰å¼€å§‹ï¼Œä»¥é€‰æ‹©åˆå§‹æ¶æ„å€™é€‰è€…ï¼ˆTransformer/RNN/CNN/MLPsï¼‰ï¼Œç„¶åè¿­ä»£è¯„ä¼°å†…å­˜é¢„ç®—ã€è®¡ç®—æˆæœ¬ã€æ¨ç†é€Ÿåº¦ã€å‡†ç¡®åº¦ç›®æ ‡å’Œç¡¬ä»¶å…¼å®¹æ€§ã€‚
- en: '[FigureÂ 4.12](ch010.xhtml#fig-dnn-fm-framework) provides a structured approach
    to architecture selection decisions, ensuring consideration of all relevant factors
    while avoiding common pitfalls such as selection based on novelty or perceived
    sophistication. The decision flowchart guides systematic architecture selection
    by first matching data characteristics to architectural strengths, then validating
    against practical constraints. The process is inherently iterativeâ€”resource limitations
    or performance gaps often necessitate reconsidering earlier choices.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾4.12](ch010.xhtml#fig-dnn-fm-framework) æä¾›äº†ä¸€ç§ç»“æ„åŒ–çš„æ¶æ„é€‰æ‹©å†³ç­–æ–¹æ³•ï¼Œç¡®ä¿è€ƒè™‘æ‰€æœ‰ç›¸å…³å› ç´ ï¼ŒåŒæ—¶é¿å…åŸºäºæ–°é¢–æ€§æˆ–æ„ŸçŸ¥å¤æ‚æ€§çš„å¸¸è§é™·é˜±ã€‚å†³ç­–æµç¨‹å›¾é€šè¿‡é¦–å…ˆå°†æ•°æ®ç‰¹å¾ä¸æ¶æ„ä¼˜åŠ¿ç›¸åŒ¹é…ï¼Œç„¶åéªŒè¯å®é™…é™åˆ¶ï¼Œæ¥æŒ‡å¯¼ç³»ç»Ÿæ€§çš„æ¶æ„é€‰æ‹©ã€‚è¯¥è¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯è¿­ä»£çš„â€”â€”èµ„æºé™åˆ¶æˆ–æ€§èƒ½å·®è·é€šå¸¸éœ€è¦é‡æ–°è€ƒè™‘æ—©æœŸé€‰æ‹©ã€‚'
- en: 'This framework applies through four key steps. First, data analysis: pattern
    types in data provide the strongest initial signal. Spatial data naturally aligns
    with CNNs, sequential data with RNNs. Second, progressive constraint validation:
    each constraint check (memory, computational budget, inference speed) acts as
    a filter. Failing any constraint necessitates either scaling down the current
    architecture or considering a fundamentally different approach.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¡†æ¶é€šè¿‡å››ä¸ªå…³é”®æ­¥éª¤åº”ç”¨ã€‚é¦–å…ˆï¼Œæ•°æ®åˆ†æï¼šæ•°æ®ä¸­çš„æ¨¡å¼ç±»å‹æä¾›äº†æœ€å¼ºçš„åˆå§‹ä¿¡å·ã€‚ç©ºé—´æ•°æ®è‡ªç„¶ä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ç›¸åŒ¹é…ï¼Œåºåˆ—æ•°æ®ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ç›¸åŒ¹é…ã€‚å…¶æ¬¡ï¼Œæ¸è¿›å¼çº¦æŸéªŒè¯ï¼šæ¯ä¸ªçº¦æŸæ£€æŸ¥ï¼ˆå†…å­˜ã€è®¡ç®—é¢„ç®—ã€æ¨ç†é€Ÿåº¦ï¼‰éƒ½å……å½“ä¸€ä¸ªè¿‡æ»¤å™¨ã€‚ä»»ä½•çº¦æŸå¤±è´¥éƒ½éœ€è¦é™ä½å½“å‰æ¶æ„çš„è§„æ¨¡æˆ–è€ƒè™‘ä¸€ä¸ªæ ¹æœ¬ä¸åŒçš„æ–¹æ³•ã€‚
- en: Third, iterative trade-off handling when accuracy targets remain unmet. Additional
    model capacity may be required, necessitating a return to constraint checking.
    If deployment hardware cannot support the chosen architecture, reconsidering the
    entire architectural approach may be necessary. Fourth, anticipate multiple iterations,
    as real projects typically cycle through this framework several times before achieving
    optimal balance between data fit, computational feasibility, and deployment requirements.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ï¼Œå½“å‡†ç¡®æ€§ç›®æ ‡æœªè¾¾æˆæ—¶ï¼Œè¿›è¡Œè¿­ä»£æƒè¡¡å¤„ç†ã€‚å¯èƒ½éœ€è¦é¢å¤–çš„æ¨¡å‹å®¹é‡ï¼Œè¿™éœ€è¦å›åˆ°çº¦æŸæ£€æŸ¥ã€‚å¦‚æœéƒ¨ç½²ç¡¬ä»¶æ— æ³•æ”¯æŒæ‰€é€‰æ¶æ„ï¼Œå¯èƒ½éœ€è¦é‡æ–°è€ƒè™‘æ•´ä¸ªæ¶æ„æ–¹æ³•ã€‚ç¬¬å››ï¼Œé¢„æœŸå¤šæ¬¡è¿­ä»£ï¼Œå› ä¸ºçœŸå®é¡¹ç›®é€šå¸¸åœ¨å®ç°æ•°æ®æ‹Ÿåˆã€è®¡ç®—å¯è¡Œæ€§å’Œéƒ¨ç½²è¦æ±‚ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ä¹‹å‰ï¼Œä¼šå¤šæ¬¡å¾ªç¯é€šè¿‡æ­¤æ¡†æ¶ã€‚
- en: This systematic approach prevents architecture selection based solely on novelty
    or perceived sophistication, ensuring alignment of choices with both problem requirements
    and system capabilities.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç³»ç»Ÿæ–¹æ³•é˜²æ­¢ä»…åŸºäºæ–°é¢–æ€§æˆ–æ„ŸçŸ¥åˆ°çš„å¤æ‚æ€§é€‰æ‹©æ¶æ„ï¼Œç¡®ä¿é€‰æ‹©ä¸é—®é¢˜è¦æ±‚å’Œç³»ç»Ÿèƒ½åŠ›ç›¸ä¸€è‡´ã€‚
- en: 'Unified Framework: Inductive Biases'
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»Ÿä¸€æ¡†æ¶ï¼šå½’çº³åå·®
- en: 'The architectural diversity exploredâ€”from MLPs to Transformersâ€”share a unified
    theoretical framework: each architecture embodies specific inductive biases that
    constrain the hypothesis space and guide learning toward solutions appropriate
    for different data types and problem structures.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢ç´¢çš„æ¶æ„å¤šæ ·æ€§â€”â€”ä»å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰åˆ°è½¬æ¢å™¨ï¼ˆTransformersï¼‰â€”â€”å…±äº«ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼šæ¯ä¸ªæ¶æ„ä½“ç°ç‰¹å®šçš„å½’çº³åå·®ï¼Œè¿™äº›åå·®é™åˆ¶äº†å‡è®¾ç©ºé—´å¹¶æŒ‡å¯¼å­¦ä¹ ï¼Œä»¥é€‚åº”ä¸åŒçš„æ•°æ®ç±»å‹å’Œé—®é¢˜ç»“æ„ã€‚
- en: Different architectures form a hierarchy of decreasing inductive bias. CNNs
    exhibit the strongest inductive biases through local connectivity, parameter sharing,
    and translation equivariance. These constraints dramatically reduce the parameter
    space while limiting flexibility to spatial data with local structure. RNNs demonstrate
    moderate inductive bias through sequential processing and shared temporal weights.
    The hidden state mechanism assumes that past information influences current processing,
    rendering them appropriate for temporal sequences.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ¶æ„å½¢æˆäº†ä¸€ä¸ªé€’å‡çš„å½’çº³åå·®å±‚æ¬¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰é€šè¿‡å±€éƒ¨è¿æ¥ã€å‚æ•°å…±äº«å’Œå¹³ç§»ç­‰å˜æ€§è¡¨ç°å‡ºæœ€å¼ºçš„å½’çº³åå·®ã€‚è¿™äº›çº¦æŸå¤§å¤§å‡å°‘äº†å‚æ•°ç©ºé—´ï¼ŒåŒæ—¶é™åˆ¶äº†å±€éƒ¨ç»“æ„çš„ç©ºé—´æ•°æ®çš„çµæ´»æ€§ã€‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰é€šè¿‡åºåˆ—å¤„ç†å’Œå…±äº«æ—¶é—´æƒé‡è¡¨ç°å‡ºé€‚åº¦çš„å½’çº³åå·®ã€‚éšè—çŠ¶æ€æœºåˆ¶å‡å®šè¿‡å»çš„ä¿¡æ¯ä¼šå½±å“å½“å‰çš„å¤„ç†ï¼Œè¿™ä½¿å¾—å®ƒä»¬é€‚ç”¨äºæ—¶é—´åºåˆ—ã€‚
- en: MLPs maintain minimal architectural bias beyond layer-wise processing. Dense
    connectivity allows modeling arbitrary relationships but requires more data to
    learn structure that other architectures encode explicitly. Transformers represent
    adaptive inductive bias through learned attention patterns. The architecture can
    dynamically adjust its inductive bias based on the data, combining flexibility
    with the ability to discover relevant structural regularities.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰åœ¨å±‚å†…å¤„ç†ä¹‹å¤–ä¿æŒæœ€å°çš„æ¶æ„åå·®ã€‚å¯†é›†è¿æ¥å…è®¸å»ºæ¨¡ä»»æ„å…³ç³»ï¼Œä½†éœ€è¦æ›´å¤šæ•°æ®æ¥å­¦ä¹ å…¶ä»–æ¶æ„æ˜ç¡®ç¼–ç çš„ç»“æ„ã€‚è½¬æ¢å™¨é€šè¿‡å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æ¨¡å¼è¡¨ç¤ºè‡ªé€‚åº”å½’çº³åå·®ã€‚è¯¥æ¶æ„å¯ä»¥æ ¹æ®æ•°æ®åŠ¨æ€è°ƒæ•´å…¶å½’çº³åå·®ï¼Œç»“åˆçµæ´»æ€§ä»¥åŠå‘ç°ç›¸å…³ç»“æ„è§„å¾‹çš„èƒ½åŠ›ã€‚
- en: All successful architectures implement forms of hierarchical representation
    learning, but through different mechanisms. CNNs build spatial hierarchies through
    progressive receptive field expansion, applying the spatial pattern processing
    framework detailed in [SectionÂ 4.3](ch010.xhtml#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff).
    RNNs build temporal hierarchies through hidden state evolution, extending the
    sequential processing approach from [SectionÂ 4.4](ch010.xhtml#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14).
    Transformers build content-dependent hierarchies through multi-head attention,
    applying the dynamic pattern processing mechanisms described in [SectionÂ 4.5](ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æˆåŠŸçš„æ¶æ„éƒ½å®ç°äº†æŸç§å½¢å¼çš„åˆ†å±‚è¡¨ç¤ºå­¦ä¹ ï¼Œä½†é€šè¿‡ä¸åŒçš„æœºåˆ¶ã€‚CNNé€šè¿‡é€æ­¥æ‰©å±•æ„Ÿå—é‡æ„å»ºç©ºé—´å±‚æ¬¡ç»“æ„ï¼Œåº”ç”¨äº†[ç¬¬4.3èŠ‚](ch010.xhtml#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff)ä¸­è¯¦ç»†æè¿°çš„ç©ºé—´æ¨¡å¼å¤„ç†æ¡†æ¶ã€‚RNNé€šè¿‡éšè—çŠ¶æ€æ¼”å˜æ„å»ºæ—¶é—´å±‚æ¬¡ç»“æ„ï¼Œæ‰©å±•äº†[ç¬¬4.4èŠ‚](ch010.xhtml#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14)ä¸­çš„é¡ºåºå¤„ç†æ–¹æ³•ã€‚è½¬æ¢å™¨é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æ„å»ºå†…å®¹ç›¸å…³çš„å±‚æ¬¡ç»“æ„ï¼Œåº”ç”¨äº†[ç¬¬4.5èŠ‚](ch010.xhtml#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d)ä¸­æè¿°çš„åŠ¨æ€æ¨¡å¼å¤„ç†æœºåˆ¶ã€‚
- en: 'This hierarchical organization reflects a principle: complex patterns can be
    efficiently represented through composition of simpler components. The success
    of deep learning stems from the discovery that gradient-based optimization can
    effectively learn these compositional structures when provided with appropriate
    architectural inductive biases.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åˆ†å±‚ç»„ç»‡åæ˜ äº†ä¸€ä¸ªåŸåˆ™ï¼šå¤æ‚çš„æ¨¡å¼å¯ä»¥é€šè¿‡ç®€å•ç»„ä»¶çš„ç»„åˆæœ‰æ•ˆåœ°è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ æˆåŠŸçš„åŸå› åœ¨äºå‘ç°ï¼Œå½“æä¾›é€‚å½“çš„æ¶æ„å½’çº³åå·®æ—¶ï¼ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ è¿™äº›ç»„åˆç»“æ„ã€‚
- en: 'The theoretical insights about representation learning have direct implications
    for systems engineering. Hierarchical representations require computational patterns
    that can efficiently compose lower-level features into higher-level abstractions.
    This drives system design decisions:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¡¨ç¤ºå­¦ä¹ çš„ç†è®ºæ´å¯Ÿå¯¹ç³»ç»Ÿå·¥ç¨‹æœ‰ç›´æ¥å½±å“ã€‚åˆ†å±‚è¡¨ç¤ºéœ€è¦èƒ½å¤Ÿé«˜æ•ˆåœ°å°†è¾ƒä½çº§åˆ«çš„ç‰¹å¾ç»„åˆæˆé«˜çº§æŠ½è±¡çš„è®¡ç®—æ¨¡å¼ã€‚è¿™æ¨åŠ¨äº†ç³»ç»Ÿè®¾è®¡å†³ç­–ï¼š
- en: Memory hierarchies must align with representational hierarchies to minimize
    data movement costs
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­˜å‚¨å±‚æ¬¡ç»“æ„å¿…é¡»ä¸è¡¨ç¤ºå±‚æ¬¡ç»“æ„å¯¹é½ä»¥æœ€å°åŒ–æ•°æ®ç§»åŠ¨æˆæœ¬
- en: Parallelization strategies must respect the dependency structure of hierarchical
    computation
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–ç­–ç•¥å¿…é¡»å°Šé‡åˆ†å±‚è®¡ç®—çš„ä¾èµ–ç»“æ„
- en: Hardware accelerators must efficiently support the matrix operations that implement
    feature composition
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶åŠ é€Ÿå™¨å¿…é¡»é«˜æ•ˆåœ°æ”¯æŒå®ç°ç‰¹å¾ç»„åˆçš„çŸ©é˜µè¿ç®—
- en: Software frameworks must provide abstractions that enable efficient hierarchical
    computation across diverse architectures
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¯ä»¶æ¡†æ¶å¿…é¡»æä¾›æŠ½è±¡ï¼Œä»¥å®ç°è·¨ä¸åŒæ¶æ„çš„é«˜æ•ˆåˆ†å±‚è®¡ç®—
- en: Understanding architectures as embodying different inductive biases helps explain
    both their strengths and their systems requirements, providing a principled foundation
    for architecture selection and system optimization decisions.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¶æ„è§†ä¸ºä½“ç°ä¸åŒçš„å½’çº³åå·®æœ‰åŠ©äºè§£é‡Šå®ƒä»¬çš„ä¼˜ç‚¹å’Œç³»ç»Ÿéœ€æ±‚ï¼Œä¸ºæ¶æ„é€‰æ‹©å’Œç³»ç»Ÿä¼˜åŒ–å†³ç­–æä¾›åŸåˆ™æ€§çš„åŸºç¡€ã€‚
- en: Fallacies and Pitfalls
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°¬è¯¯ä¸é™·é˜±
- en: Neural network architectures represent specialized computational structures
    designed for different data types and problem domains, which creates common misconceptions
    about their selection and deployment. The rich variety of architectural patternsâ€”from
    dense networks to transformersâ€”often leads engineers to make choices based on
    novelty or perceived sophistication rather than task-specific requirements and
    computational constraints.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„ä»£è¡¨ä¸ºä¸åŒæ•°æ®ç±»å‹å’Œé—®é¢˜åŸŸè®¾è®¡çš„ä¸“ç”¨è®¡ç®—ç»“æ„ï¼Œè¿™å¯¼è‡´äº†å…³äºå®ƒä»¬é€‰æ‹©å’Œéƒ¨ç½²çš„å¸¸è§è¯¯è§£ã€‚ä»å¯†é›†ç½‘ç»œåˆ°è½¬æ¢å™¨ç­‰ä¸°å¯Œçš„æ¶æ„æ¨¡å¼å¾€å¾€å¯¼è‡´å·¥ç¨‹å¸ˆåŸºäºæ–°é¢–æ€§æˆ–æ„ŸçŸ¥çš„å¤æ‚æ€§è€Œéç‰¹å®šä»»åŠ¡çš„è¦æ±‚å’Œè®¡ç®—çº¦æŸåšå‡ºé€‰æ‹©ã€‚
- en: '**Fallacy:** *More complex architectures always perform better than simpler
    ones.*'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**è°¬è¯¯**ï¼š*æ›´å¤æ‚çš„æ¶æ„æ€»æ˜¯æ¯”ç®€å•çš„æ¶æ„è¡¨ç°æ›´å¥½*ã€‚'
- en: This misconception prompts teams to immediately adopt transformer-based models
    or elaborate architectures without understanding their requirements. While sophisticated
    architectures such as transformers excel at complex tasks requiring long-range
    dependencies, they require significantly more computational resources and memory.
    For numerous problems, particularly those with limited data or clear structural
    patterns, simpler architectures such as MLPs or CNNs achieve comparable accuracy
    with significantly less computational overhead. Architecture selection should
    correspond to problem complexity rather than defaulting to the most advanced option.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¯¯è§£ä¿ƒä½¿å›¢é˜Ÿç«‹å³é‡‡ç”¨åŸºäºtransformerçš„æ¨¡å‹æˆ–å¤æ‚çš„æ¶æ„ï¼Œè€Œä¸äº†è§£å…¶éœ€æ±‚ã€‚è™½ç„¶åƒtransformerè¿™æ ·çš„å¤æ‚æ¶æ„åœ¨å¤„ç†éœ€è¦é•¿è·ç¦»ä¾èµ–çš„å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬éœ€è¦æ˜¾è‘—æ›´å¤šçš„è®¡ç®—èµ„æºå’Œå†…å­˜ã€‚å¯¹äºè®¸å¤šé—®é¢˜ï¼Œå°¤å…¶æ˜¯é‚£äº›æ•°æ®æœ‰é™æˆ–å…·æœ‰æ˜æ˜¾ç»“æ„æ¨¡å¼çš„é—®é¢˜ï¼Œç®€å•çš„æ¶æ„ï¼ˆå¦‚MLPsæˆ–CNNsï¼‰åœ¨æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¾¾åˆ°ç›¸å½“çš„å‡†ç¡®åº¦ã€‚æ¶æ„é€‰æ‹©åº”ä¸é—®é¢˜å¤æ‚æ€§ç›¸å¯¹åº”ï¼Œè€Œä¸æ˜¯é»˜è®¤é€‰æ‹©æœ€å…ˆè¿›çš„é€‰æ‹©ã€‚
- en: '**Pitfall:** *Ignoring the computational implications of architectural choices
    during model selection.*'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š** *åœ¨æ¨¡å‹é€‰æ‹©è¿‡ç¨‹ä¸­å¿½è§†äº†æ¶æ„é€‰æ‹©çš„è®¡ç®—å½±å“ã€‚*'
- en: Many practitioners select architectures based solely on accuracy metrics from
    academic papers without considering computational requirements. A CNNâ€™s spatial
    locality assumptions might deliver excellent accuracy for image tasks but require
    specialized memory access patterns. Similarly, RNNsâ€™ sequential dependencies create
    serialization bottlenecks that limit parallelization opportunities. This oversight
    leads to deployment failures when models cannot meet latency requirements or exceed
    memory constraints in production environments.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šä»ä¸šè€…ä»…æ ¹æ®å­¦æœ¯è®ºæ–‡ä¸­çš„å‡†ç¡®åº¦æŒ‡æ ‡é€‰æ‹©æ¶æ„ï¼Œè€Œæ²¡æœ‰è€ƒè™‘è®¡ç®—éœ€æ±‚ã€‚CNNçš„ç©ºé—´å±€éƒ¨æ€§å‡è®¾å¯èƒ½ä¸ºå›¾åƒä»»åŠ¡æä¾›å‡ºè‰²çš„å‡†ç¡®åº¦ï¼Œä½†éœ€è¦ç‰¹å®šçš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚åŒæ ·ï¼ŒRNNçš„åºåˆ—ä¾èµ–æ€§åˆ›å»ºäº†åºåˆ—ç“¶é¢ˆï¼Œé™åˆ¶äº†å¹¶è¡ŒåŒ–æœºä¼šã€‚è¿™ç§ç–å¿½å¯¼è‡´æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ— æ³•æ»¡è¶³å»¶è¿Ÿè¦æ±‚æˆ–è¶…å‡ºå†…å­˜é™åˆ¶æ—¶éƒ¨ç½²å¤±è´¥ã€‚
- en: '**Fallacy:** *Architecture performance is independent of hardware characteristics.*'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '**è°¬è¯¯ï¼š** *æ¶æ„æ€§èƒ½ä¸ç¡¬ä»¶ç‰¹æ€§æ— å…³ã€‚*'
- en: 'This belief assumes that all architectures perform equally well across different
    hardware platforms. In reality, different architectures exploit different hardware
    features: CNNs benefit from specialized tensor cores, MLPs leverage high-bandwidth
    memory, and RNNs require efficient sequential processing capabilities. A model
    that achieves optimal performance on GPUs might perform poorly on mobile devices
    or embedded processors. Understanding hardware-architecture alignment is crucial
    for effective deployment strategies.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¿¡å¿µå‡è®¾æ‰€æœ‰æ¶æ„åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šè¡¨ç°éƒ½ä¸€æ ·å¥½ã€‚å®é™…ä¸Šï¼Œä¸åŒçš„æ¶æ„åˆ©ç”¨ä¸åŒçš„ç¡¬ä»¶ç‰¹æ€§ï¼šCNNå—ç›Šäºä¸“é—¨çš„å¼ é‡æ ¸å¿ƒï¼ŒMLPsåˆ©ç”¨é«˜å¸¦å®½å†…å­˜ï¼Œè€ŒRNNéœ€è¦é«˜æ•ˆçš„åºåˆ—å¤„ç†èƒ½åŠ›ã€‚åœ¨GPUä¸Šå®ç°æœ€ä½³æ€§èƒ½çš„æ¨¡å‹å¯èƒ½åœ¨ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼å¤„ç†å™¨ä¸Šè¡¨ç°ä¸ä½³ã€‚ç†è§£ç¡¬ä»¶-æ¶æ„å¯¹é½å¯¹äºæœ‰æ•ˆçš„éƒ¨ç½²ç­–ç•¥è‡³å…³é‡è¦ã€‚
- en: '**Pitfall:** *Mixing architectural patterns without understanding their interaction
    effects.*'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š** *åœ¨ä¸äº†è§£å…¶äº¤äº’æ•ˆåº”çš„æƒ…å†µä¸‹æ··åˆæ¶æ„æ¨¡å¼ã€‚*'
- en: Combining different architectural components (such as adding attention layers
    to CNNs or using skip connections in RNNs) can create unexpected computational
    bottlenecks. Each architectural pattern exhibits distinct memory access patterns
    and computational characteristics. Naive combinations may eliminate the performance
    benefits of individual components or create memory bandwidth conflicts. Successful
    hybrid architectures require careful analysis of how different patterns interact
    at the system level.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸åŒçš„æ¶æ„ç»„ä»¶ï¼ˆä¾‹å¦‚å‘CNNæ·»åŠ æ³¨æ„åŠ›å±‚æˆ–åœ¨RNNä¸­ä½¿ç”¨è·³è¿‡è¿æ¥ï¼‰å¯èƒ½ä¼šäº§ç”Ÿæ„å¤–çš„è®¡ç®—ç“¶é¢ˆã€‚æ¯ç§æ¶æ„æ¨¡å¼éƒ½è¡¨ç°å‡ºç‹¬ç‰¹çš„å†…å­˜è®¿é—®æ¨¡å¼å’Œè®¡ç®—ç‰¹æ€§ã€‚ç®€å•çš„ç»„åˆå¯èƒ½ä¼šæ¶ˆé™¤å•ä¸ªç»„ä»¶çš„æ€§èƒ½ä¼˜åŠ¿æˆ–äº§ç”Ÿå†…å­˜å¸¦å®½å†²çªã€‚æˆåŠŸçš„æ··åˆæ¶æ„éœ€è¦ä»”ç»†åˆ†æä¸åŒæ¨¡å¼åœ¨ç³»ç»Ÿå±‚é¢çš„äº¤äº’ã€‚
- en: '**Pitfall:** *Designing architectures without considering the full hardware-software
    co-design implications across the deployment pipeline.*'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '**é™·é˜±ï¼š** *åœ¨è®¾è®¡æ¶æ„æ—¶æ²¡æœ‰è€ƒè™‘éƒ¨ç½²ç®¡é“ä¸­å…¨ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡çš„å…¨éƒ¨å½±å“ã€‚*'
- en: Many architecture decisions optimize for high-end GPU performance without considering
    the complete system lifecycle from development through deployment. An architecture
    designed for large-scale compute clusters may be poorly suited for edge deployment
    due to memory constraints, lack of specialized compute units, or limited parallelization
    capabilities. Similarly, architectures optimized for inference latency might sacrifice
    development efficiency, leading to longer development cycles and higher computational
    costs. Effective architecture selection requires analyzing the entire system stack
    including compute infrastructure, model compilation and optimization tools, target
    deployment hardware, and operational constraints. The choice between CNN depth
    and width, transformer head configurations, or activation functions has cascading
    effects on memory bandwidth utilization, cache efficiency, and numerical precision
    requirements that must be considered holistically rather than in isolation.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæ¶æ„å†³ç­–ä¼˜åŒ–é«˜ç«¯GPUæ€§èƒ½ï¼Œè€Œæ²¡æœ‰è€ƒè™‘ä»å¼€å‘åˆ°éƒ¨ç½²çš„æ•´ä¸ªç³»ç»Ÿç”Ÿå‘½å‘¨æœŸã€‚ä¸ºå¤§è§„æ¨¡è®¡ç®—é›†ç¾¤è®¾è®¡çš„æ¶æ„å¯èƒ½ç”±äºå†…å­˜é™åˆ¶ã€ç¼ºä¹ä¸“ç”¨è®¡ç®—å•å…ƒæˆ–æœ‰é™çš„å¹¶è¡ŒåŒ–èƒ½åŠ›ï¼Œä¸é€‚åˆè¾¹ç¼˜éƒ¨ç½²ã€‚åŒæ ·ï¼Œé’ˆå¯¹æ¨ç†å»¶è¿Ÿä¼˜åŒ–çš„æ¶æ„å¯èƒ½ä¼šç‰ºç‰²å¼€å‘æ•ˆç‡ï¼Œå¯¼è‡´æ›´é•¿çš„å¼€å‘å‘¨æœŸå’Œæ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚æœ‰æ•ˆçš„æ¶æ„é€‰æ‹©éœ€è¦åˆ†ææ•´ä¸ªç³»ç»Ÿå †æ ˆï¼ŒåŒ…æ‹¬è®¡ç®—åŸºç¡€è®¾æ–½ã€æ¨¡å‹ç¼–è¯‘å’Œä¼˜åŒ–å·¥å…·ã€ç›®æ ‡éƒ¨ç½²ç¡¬ä»¶å’Œæ“ä½œçº¦æŸã€‚åœ¨CNNæ·±åº¦å’Œå®½åº¦ã€Transformerå¤´é…ç½®æˆ–æ¿€æ´»å‡½æ•°ä¹‹é—´çš„é€‰æ‹©ä¼šå¯¹å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ã€ç¼“å­˜æ•ˆç‡å’Œæ•°å€¼ç²¾åº¦è¦æ±‚äº§ç”Ÿçº§è”æ•ˆåº”ï¼Œè¿™äº›éƒ½éœ€è¦æ•´ä½“è€ƒè™‘ï¼Œè€Œä¸æ˜¯å­¤ç«‹åœ°è€ƒè™‘ã€‚
- en: Summary
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Neural network architectures form specialized computational structures tailored
    to process different types of data and solve distinct classes of problems. Multi-Layer
    Perceptrons handle tabular data through dense connections, convolutional networks
    exploit spatial locality in images, and recurrent networks process sequential
    information. Each architecture embodies specific assumptions about data structure
    and computational patterns. Modern transformer architectures unify many of these
    concepts through attention mechanisms that dynamically route information based
    on relevance rather than fixed connectivity patterns.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œæ¶æ„å½¢æˆäº†ä¸“é—¨çš„è®¡ç®—ç»“æ„ï¼Œé’ˆå¯¹å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®å’Œè§£å†³ä¸åŒç±»åˆ«çš„é—®é¢˜ã€‚å¤šå±‚æ„ŸçŸ¥å™¨é€šè¿‡å¯†é›†è¿æ¥å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œå·ç§¯ç½‘ç»œåˆ©ç”¨å›¾åƒä¸­çš„ç©ºé—´å±€éƒ¨æ€§ï¼Œå¾ªç¯ç½‘ç»œå¤„ç†åºåˆ—ä¿¡æ¯ã€‚æ¯ä¸ªæ¶æ„éƒ½ä½“ç°äº†å¯¹æ•°æ®ç»“æ„å’Œè®¡ç®—æ¨¡å¼çš„å…·ä½“å‡è®¾ã€‚ç°ä»£Transformeræ¶æ„é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç»Ÿä¸€äº†è®¸å¤šè¿™äº›æ¦‚å¿µï¼Œè¯¥æœºåˆ¶æ ¹æ®ç›¸å…³æ€§è€Œä¸æ˜¯å›ºå®šçš„è¿æ¥æ¨¡å¼åŠ¨æ€è·¯ç”±ä¿¡æ¯ã€‚
- en: Despite their apparent diversity, these architectures share fundamental computational
    primitives that recur across different designs. Matrix multiplication operations
    form the computational core, whether in dense layers, convolutions, or attention
    mechanisms. Memory access patterns vary significantly between architectures, with
    some requiring sliding window operations for local processing while others demand
    global information aggregation. Dynamic computation patterns in attention mechanisms
    create data-dependent execution flows that challenge traditional optimization
    approaches.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›æ¶æ„åœ¨è¡¨é¢ä¸Šçœ‹èµ·æ¥å¤šæ ·åŒ–ï¼Œä½†å®ƒä»¬å…±äº«åŸºæœ¬çš„è®¡ç®—åŸè¯­ï¼Œè¿™äº›åŸè¯­åœ¨ä¸åŒè®¾è®¡ä¸­åå¤å‡ºç°ã€‚çŸ©é˜µä¹˜æ³•æ“ä½œæ„æˆäº†è®¡ç®—æ ¸å¿ƒï¼Œæ— è®ºæ˜¯åœ¨å¯†é›†å±‚ã€å·ç§¯è¿˜æ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚å†…å­˜è®¿é—®æ¨¡å¼åœ¨æ¶æ„ä¹‹é—´å·®å¼‚å¾ˆå¤§ï¼Œä¸€äº›æ¶æ„éœ€è¦æ»‘åŠ¨çª—å£æ“ä½œè¿›è¡Œå±€éƒ¨å¤„ç†ï¼Œè€Œå¦ä¸€äº›æ¶æ„åˆ™è¦æ±‚å…¨å±€ä¿¡æ¯èšåˆã€‚æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„åŠ¨æ€è®¡ç®—æ¨¡å¼åˆ›å»ºäº†æ•°æ®ä¾èµ–çš„æ‰§è¡Œæµç¨‹ï¼Œè¿™å¯¹ä¼ ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•æ„æˆäº†æŒ‘æˆ˜ã€‚
- en: '**Key Takeaways**'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…³é”®è¦ç‚¹**'
- en: 'Different architectures embody specific assumptions about data structure: MLPs
    for tabular data, CNNs for spatial relationships, RNNs for sequences, Transformers
    for flexible attention'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸åŒçš„æ¶æ„å¯¹æ•°æ®ç»“æ„æœ‰ç‰¹å®šçš„å‡è®¾ï¼šMLPsç”¨äºè¡¨æ ¼æ•°æ®ï¼ŒCNNsç”¨äºç©ºé—´å…³ç³»ï¼ŒRNNsç”¨äºåºåˆ—ï¼ŒTransformersç”¨äºçµæ´»çš„æ³¨æ„åŠ›
- en: Shared computational primitives including matrix operations, sliding windows,
    and dynamic routing form the foundation across diverse architectures
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬çŸ©é˜µè¿ç®—ã€æ»‘åŠ¨çª—å£å’ŒåŠ¨æ€è·¯ç”±åœ¨å†…çš„å…±äº«è®¡ç®—åŸè¯­æ„æˆäº†è·¨ä¸åŒæ¶æ„çš„åŸºç¡€
- en: Memory access patterns and data movement requirements vary significantly between
    architectures, directly impacting system performance and optimization strategies
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†…å­˜è®¿é—®æ¨¡å¼å’Œæ•°æ®å¤„ç†éœ€æ±‚åœ¨æ¶æ„ä¹‹é—´å·®å¼‚å¾ˆå¤§ï¼Œè¿™ç›´æ¥å½±å“ç³»ç»Ÿæ€§èƒ½å’Œä¼˜åŒ–ç­–ç•¥
- en: Understanding the mapping between algorithmic intent and system implementation
    enables effective performance optimization and hardware selection
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£ç®—æ³•æ„å›¾ä¸ç³»ç»Ÿå®ç°ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„æ€§èƒ½ä¼˜åŒ–å’Œç¡¬ä»¶é€‰æ‹©
- en: The architectural foundations established in this chapterâ€”computational patterns,
    memory access characteristics, and data movement primitivesâ€”directly inform the
    design of specialized hardware and optimization strategies explored in subsequent
    chapters. Understanding that CNNs exhibit spatial locality enables the development
    of systolic arrays optimized for convolution operations ([ChapterÂ 11](ch017.xhtml#sec-ai-acceleration)).
    Recognizing that Transformers demand quadratic memory scaling motivates attention-specific
    optimizations such as FlashAttention and sparse attention patterns ([ChapterÂ 10](ch016.xhtml#sec-model-optimizations)).
    The progression from architectural understanding to hardware design to algorithmic
    optimization represents a systematic approach to ML systems engineering.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­å»ºç«‹çš„å»ºç­‘å­¦åŸºç¡€â€”â€”è®¡ç®—æ¨¡å¼ã€å†…å­˜è®¿é—®ç‰¹æ€§å’Œæ•°æ®ç§»åŠ¨åŸè¯­â€”â€”ç›´æ¥æŒ‡å¯¼äº†åç»­ç« èŠ‚ä¸­æ¢è®¨çš„ä¸“ç”¨ç¡¬ä»¶å’Œä¼˜åŒ–ç­–ç•¥çš„è®¾è®¡ã€‚ç†è§£CNNè¡¨ç°å‡ºç©ºé—´å±€éƒ¨æ€§ä½¿å¾—å¯ä»¥å¼€å‘é’ˆå¯¹å·ç§¯æ“ä½œçš„è°æŒ¯é˜µåˆ—ä¼˜åŒ–ï¼ˆ[ç¬¬11ç« ](ch017.xhtml#sec-ai-acceleration)ï¼‰ã€‚è®¤è¯†åˆ°Transformeréœ€è¦äºŒæ¬¡æ–¹å†…å­˜æ‰©å±•ä¿ƒä½¿äº†å¦‚FlashAttentionå’Œç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ç­‰æ³¨æ„åŠ›ç‰¹å®šä¼˜åŒ–ï¼ˆ[ç¬¬10ç« ](ch016.xhtml#sec-model-optimizations)ï¼‰ã€‚ä»æ¶æ„ç†è§£åˆ°ç¡¬ä»¶è®¾è®¡å†åˆ°ç®—æ³•ä¼˜åŒ–çš„å‘å±•è¿‡ç¨‹ä»£è¡¨äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿå·¥ç¨‹çš„ç³»ç»Ÿæ–¹æ³•ã€‚
- en: As architectures become more dynamic and sophisticated, the relationship between
    algorithmic innovation and systems optimization becomes increasingly critical
    for achieving practical performance gains in real-world deployments. The operational
    challenges of deploying and maintaining these sophisticated architectures in production
    environments are addressed in [ChapterÂ 13](ch019.xhtml#sec-ml-operations), while
    the broader implications for sustainable AI development, including energy efficiency
    considerations stemming from architectural choices, are explored in [ChapterÂ 18](ch024.xhtml#sec-sustainable-ai).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¶æ„å˜å¾—æ›´åŠ åŠ¨æ€å’Œå¤æ‚ï¼Œç®—æ³•åˆ›æ–°ä¸ç³»ç»Ÿä¼˜åŒ–ä¹‹é—´çš„å…³ç³»å¯¹äºåœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­å®ç°å®é™…æ€§èƒ½æå‡å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚åœ¨[ç¬¬13ç« ](ch019.xhtml#sec-ml-operations)ä¸­è®¨è®ºäº†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å’Œç»´æŠ¤è¿™äº›å¤æ‚æ¶æ„çš„æ“ä½œæŒ‘æˆ˜ï¼Œè€Œåœ¨[ç¬¬18ç« ](ch024.xhtml#sec-sustainable-ai)ä¸­æ¢è®¨äº†å¯æŒç»­äººå·¥æ™ºèƒ½å‘å±•çš„æ›´å¹¿æ³›å½±å“ï¼ŒåŒ…æ‹¬ç”±æ¶æ„é€‰æ‹©å¼•èµ·çš„èƒ½æºæ•ˆç‡è€ƒè™‘ã€‚
- en: '* * *'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
