["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as stats                                   # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # matrix scatter plots\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom sklearn.naive_bayes import GaussianNB                    # naive Bayes model and prediction\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.metrics import classification_report             # classification report\nfrom sklearn.metrics import confusion_matrix                  # confusion matrix\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nbinary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),\n                     interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),\n                     interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,cat,label,cmap): \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels = 50) # plot the predictions\n    for i in range(len(cat)):\n        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], \n                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors=\"black\",label=label[i])\n\n    plt.title(title)                                          # add the labels\n    plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()\n\ndef visualize_model_prob(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,title,):# plots the data points and the prediction probabilities \n    n_classes = 10\n    cmap = plt.cm.inferno\n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n\n    z_min = 0.0; z_max = 1.0\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z1 = Z[:,0].reshape(xx.shape); Z2 = Z[:,1].reshape(xx.shape)\n\n    plt.subplot(121)\n    cs1 = plt.contourf(xx, yy, Z1, cmap=plt.cm.YlOrBr,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=plt.cm.Greys,norm=None,vmin=z_min,vmax=z_max,\n                     alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n    plt.title(title + ' Probability of Low Production')\n    plt.xlabel(xfeature.name)\n    plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(cs1, orientation = 'vertical')\n    cbar.set_label('Probability', rotation=270, labelpad=20)\n\n    plt.subplot(122)\n    cs2 = plt.contourf(xx, yy, Z2, cmap=plt.cm.YlOrBr,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=plt.cm.Greys,norm=None,vmin=z_min,vmax=z_max,\n                     alpha=0.8,linewidths=0.3, edgecolors=\"black\")\n    plt.title(title + ' Probability of High Production')\n    plt.xlabel(xfeature.name)\n    plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(cs2, orientation = 'vertical')\n    cbar.set_label('Probability', rotation=270, labelpad=20)\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(r\"C:\\Users\\pm27995\\Downloads\")                      # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 1\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})   \nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1000.0; ymax = 9000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\nycname = 'c' + yname\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname]) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprod_trunc = 4200                                             # criteria for low and high production truncation\ny['cProduction'] = np.where(y['Production']>=prod_trunc, 1, 0) # conditional statement assign a new feature \n```", "```py\ny.head(n=5)                                                   # preview the first n rows of the DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('           Training DataFrame                      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 21                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.ylim([0,11]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.ylim([0,11]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'lightsteelblue',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nplt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='D',color = 'lightgreen',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\nplt.scatter(df_test[df_test[yname]>prod_trunc][Xname[0]],df_test[df_test[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'lightsteelblue',alpha = 0.8,edgecolor = 'black',zorder=1)\nplt.scatter(df_test[df_test[yname]<prod_trunc][Xname[0]],df_test[df_test[yname]<prod_trunc][Xname[1]],s=80,\n            marker='D',color = 'lightgreen',alpha = 0.8,edgecolor = 'black',zorder=1)\n\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplot(224)                                              # predictor feature #2 histogram\n_,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 1.0,\n                     edgecolor='white',color='white',density=False,zorder=2)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train',zorder=10)\n_,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 1.0,\n                     edgecolor='white',color='white',density=False,zorder=2)\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test',zorder=10)\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.vlines(prod_trunc,0,100,color='black',ls='--')\nplt.xlabel(ylabelunit); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); add_grid(); plt.title(ylabel)\nplt.xlim([ymin,ymax]); plt.ylim([0,11]); plt.legend(loc='upper right')  \nplt.fill_between([ymin,prod_trunc],[0,0],[100,100],color='lightgreen',zorder=1)\nplt.fill_between([prod_trunc,ymax],[0,0],[100,100],color='lightsteelblue',zorder=1)\nplt.annotate('Low Production',[2400,9],color='black',zorder=100); plt.annotate('High Production',[4500,9],color='black',zorder=100)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.6, wspace=0.2, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)\nplt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nplt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\n\nplt.scatter(df_test[df_test[yname]>prod_trunc][Xname[0]],df_test[df_test[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)\nplt.scatter(df_test[df_test[yname]<prod_trunc][Xname[0]],df_test[df_test[yname]<prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)\n\nplt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\nplt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Testing Truncated ' + yname + ' vs. ' + Xname[1] + ' and ' + Xname[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1]); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npriors = (0.5,0.5)   # naive prior \n```", "```py\npriors = (0.95,0.05)                                            # set the prior probabilities of low and high production \n```", "```py\ngnb = GaussianNB(priors = priors)                             # instantiate the Gaussian naive Bayes model\nGaussianNB_fit = gnb.fit(X_train,y_train[ycname])             # train with the training data \n```", "```py\ny_pred = GaussianNB_fit.predict(np.c_[X_test['Por'],X_test['Brittle']]) # predict over the testing data\n\nplt.subplot(111)\nhigh = plt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nlow = plt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\n\nplt.scatter(df_test[y_pred == 0][Xname[0]]-0.19,df_test[y_pred == 0][Xname[1]],s=80,lw=2,\n             marker='<',c = df_test[y_pred == 0][ycname].values,alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap)\nplt.scatter(df_test[y_pred == 1][Xname[0]]-0.19,df_test[y_pred == 1][Xname[1]],s=80,lw=2,\n             marker='<',c = df_test[y_pred == 1][ycname].values,alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap)\n\nplt.scatter(df_test[y_pred == 0][Xname[0]]+0.19,df_test[y_pred == 0][Xname[1]],s=80,lw=2,\n             marker='>',c = y_pred[y_pred == 0],alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap,vmin=0,vmax=1)\nplt.scatter(df_test[y_pred == 1][Xname[0]]+0.19,df_test[y_pred == 1][Xname[1]],s=80,lw=2,\n             marker='>',c = y_pred[y_pred == 1],alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap,vmin=0,vmax=1)\n\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=10,marker='o',c = 'black',alpha = 0.8,edgecolor = 'black',zorder=100)\n\ntrain = plt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train') # for legend\ntruth = plt.scatter([-999],[-999],s=80,marker='<',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Truth')\npredicted = plt.scatter([-999],[-999],s=80,marker='>',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Predicted')\n\nplt.legend([low,high,train,truth,predicted],['Low Production','High Production','Train','Test Truth','Test Predicted'],loc = 'upper right')\nplt.title('Training and Testing Truncated ' + yname + ' vs. ' + Xname[1] + ' and ' + Xname[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1]); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nclassification_report(truth, predicted)                # build a classification report to check our classification model \n```", "```py\n* recall - the ratio of true positives divided by all cases of the category in the testing dataset\n\n* precision - the ratio of true positives divided by all positives (true positives + false positives)\n\n* f1-score - the harmonic mean of recall and precision\n\n* support - the number of samples of each category in the testing data \n```", "```py\nreport = classification_report(y_test[ycname].values, y_pred, labels=[0,1],output_dict=True)\nprecision = [report['0']['precision'],report['1']['precision']]\nrecall = [report['0']['recall'],report['1']['recall']]\nf1score = [report['0']['f1-score'],report['1']['f1-score']]\n\nplt.subplot(121)                                              # plot classification report\nplt.bar([-0.125,0.875],precision,width=0.25,color=['darkgrey','gold'],edgecolor='black')\nplt.bar([0.125,1.125],recall,width=0.25,color=['darkgrey','gold'],edgecolor='black')\n\nplt.ylim([0.,1.0]); plt.xlim([-0.5,1.5]); add_grid() \nax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())\nplt.annotate('Precision',[-0.135,0.5],rotation=90.0); plt.annotate('Recall',[0.115,0.5],rotation=90.0)\nplt.annotate('Precision',[0.865,0.5],rotation=90.0); plt.annotate('Recall',[1.115,0.5],rotation=90.0)\nplt.plot([-0.125,0.125],[f1score[0],f1score[0]],color='black')\nplt.annotate('f1-score',[-0.08,f1score[0]-0.035])\nplt.plot([0.875,1.125],[f1score[1],f1score[1]],color='black')\nplt.annotate('f1-score',[0.92,f1score[1]-0.035])\nplt.ylabel('Classification Metric'); plt.title('Classification Report, ' + str(ylabel) + ' from ' + str(Xlabel[0]) + ' and ' + str(Xname[1]))\nx_ticks = [0, 1]; x_labels = ['Low Production', 'High Production']; plt.xticks(x_ticks,x_labels)\n\nconfusion_mat = confusion_matrix(y_test[ycname].values, y_pred)\n\nplt.subplot(122)                                              # plot confusion matrix\nsns.heatmap(confusion_mat,annot=False,fmt=\"d\",annot_kws={\"color\": \"black\",\"size\":15},cbar=False,\n            xticklabels=['Low Production','High Production'], yticklabels=['Low Production','High Production',],cmap=cmap)\nplt.xlabel('Predicted Labels', fontsize=12)\nplt.ylabel('True Labels', fontsize=12)\nplt.title('Confusion Matrix, ' + str(ylabel) + ' from ' + str(Xlabel[0]) + ' and ' + str(Xname[1]), fontsize=14)\nplt.annotate(str(round(confusion_mat[0,0]/len(df_test)*100,2)) + '%',[0.5,0.45],size=15,color='black',ha='center')\nplt.annotate(str(round(confusion_mat[0,1]/len(df_test)*100,2)) + '%',[1.5,0.45],size=15,color='white',ha='center')\nplt.annotate(str(round(confusion_mat[1,0]/len(df_test)*100,2)) + '%',[0.5,1.45],size=15,color='white',ha='center')\nplt.annotate(str(round(confusion_mat[1,1]/len(df_test)*100,2)) + '%',[1.5,1.45],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[0,0]) + ' of ' + str(len(df_test)),[0.5,0.55],size=15,color='black',ha='center')\nplt.annotate(str(confusion_mat[0,1]) + ' of ' + str(len(df_test)),[1.5,0.55],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[1,0]) + ' of ' + str(len(df_test)),[0.5,1.55],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[1,1]) + ' of ' + str(len(df_test)),[1.5,1.55],size=15,color='white',ha='center')\nplt.plot([0,2,2,0,0],[0,0,2,2,0],color='black'); plt.plot([0,2],[1,1],color='black'); plt.plot([1,1],[0,2],color='black')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nvisualize_model(GaussianNB_fit,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[ycname],0.0,1.0,\n                'Training Data and Naive Bayes Model',[0,1],['Low Production','High Production'],binary_cmap)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nvisualize_model_prob(GaussianNB_fit,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],\n                     'Training Data and Naive Bayes ')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as stats                                   # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter,NullLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # matrix scatter plots\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom sklearn.naive_bayes import GaussianNB                    # naive Bayes model and prediction\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.metrics import classification_report             # classification report\nfrom sklearn.metrics import confusion_matrix                  # confusion matrix\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nbinary_cmap = ListedColormap(['grey', 'gold'])                # custom binary categorical colormap\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # supress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),\n                     interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),\n                     interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,title,cat,label,cmap): \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels = 50) # plot the predictions\n    for i in range(len(cat)):\n        im = plt.scatter(xfeature[response==cat[i]],yfeature[response==cat[i]],s=None,c=response[response==cat[i]], \n                    marker=None, cmap=cmap, norm=None,vmin=z_min,vmax=z_max,alpha=0.8,linewidths=0.3, edgecolors=\"black\",label=label[i])\n\n    plt.title(title)                                          # add the labels\n    plt.xlabel(xfeature.name); plt.ylabel(yfeature.name)\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max]); add_grid()\n\ndef visualize_model_prob(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,title,):# plots the data points and the prediction probabilities \n    n_classes = 10\n    cmap = plt.cm.inferno\n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n\n    z_min = 0.0; z_max = 1.0\n    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z1 = Z[:,0].reshape(xx.shape); Z2 = Z[:,1].reshape(xx.shape)\n\n    plt.subplot(121)\n    cs1 = plt.contourf(xx, yy, Z1, cmap=plt.cm.YlOrBr,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=plt.cm.Greys,norm=None,vmin=z_min,vmax=z_max,\n                     alpha=0.8, linewidths=0.3, edgecolors=\"black\")\n    plt.title(title + ' Probability of Low Production')\n    plt.xlabel(xfeature.name)\n    plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(cs1, orientation = 'vertical')\n    cbar.set_label('Probability', rotation=270, labelpad=20)\n\n    plt.subplot(122)\n    cs2 = plt.contourf(xx, yy, Z2, cmap=plt.cm.YlOrBr,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=plt.cm.Greys,norm=None,vmin=z_min,vmax=z_max,\n                     alpha=0.8,linewidths=0.3, edgecolors=\"black\")\n    plt.title(title + ' Probability of High Production')\n    plt.xlabel(xfeature.name)\n    plt.ylabel(yfeature.name)\n    cbar = plt.colorbar(cs2, orientation = 'vertical')\n    cbar.set_label('Probability', rotation=270, labelpad=20)\n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(r\"C:\\Users\\pm27995\\Downloads\")                      # set the working directory \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 1\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})   \nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1000.0; ymax = 9000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\nycname = 'c' + yname\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname]) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nprod_trunc = 4200                                             # criteria for low and high production truncation\ny['cProduction'] = np.where(y['Production']>=prod_trunc, 1, 0) # conditional statement assign a new feature \n```", "```py\ny.head(n=5)                                                   # preview the first n rows of the DataFrame \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=73073) # train and test split\ndf_train = pd.concat([X_train,y_train],axis=1)                # make one train DataFrame with both X and y (remove all other features)\ndf_test = pd.concat([X_test,y_test],axis=1)                   # make one testin DataFrame with both X and y (remove all other features) \n```", "```py\nprint('           Training DataFrame                      Testing DataFrame')\ndisplay_sidebyside(df_train,df_test)                          # custom function for side-by-side DataFrame display \n```", "```py\n Training DataFrame                      Testing DataFrame \n```", "```py\nnbins = 21                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[0]); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.ylim([0,11]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq1,_,_ = plt.hist(x=df_train[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nfreq2,_,_ = plt.hist(x=df_test[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test')\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title(Xlabel[1]); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.ylim([0,11]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df_train[Xname[0]],df_train[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=40,marker='o',color = 'red',alpha = 0.8,edgecolor = 'black',zorder=10,label='Test')\nplt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'lightsteelblue',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nplt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='D',color = 'lightgreen',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\nplt.scatter(df_test[df_test[yname]>prod_trunc][Xname[0]],df_test[df_test[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'lightsteelblue',alpha = 0.8,edgecolor = 'black',zorder=1)\nplt.scatter(df_test[df_test[yname]<prod_trunc][Xname[0]],df_test[df_test[yname]<prod_trunc][Xname[1]],s=80,\n            marker='D',color = 'lightgreen',alpha = 0.8,edgecolor = 'black',zorder=1)\n\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplot(224)                                              # predictor feature #2 histogram\n_,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 1.0,\n                     edgecolor='white',color='white',density=False,zorder=2)\nfreq1,_,_ = plt.hist(x=df_train[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train',zorder=10)\n_,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 1.0,\n                     edgecolor='white',color='white',density=False,zorder=2)\nfreq2,_,_ = plt.hist(x=df_test[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='red',density=False,label='Test',zorder=10)\nmax_freq = max(freq1.max()*1.10,freq2.max()*1.10)\nplt.vlines(prod_trunc,0,100,color='black',ls='--')\nplt.xlabel(ylabelunit); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); add_grid(); plt.title(ylabel)\nplt.xlim([ymin,ymax]); plt.ylim([0,11]); plt.legend(loc='upper right')  \nplt.fill_between([ymin,prod_trunc],[0,0],[100,100],color='lightgreen',zorder=1)\nplt.fill_between([prod_trunc,ymax],[0,0],[100,100],color='lightsteelblue',zorder=1)\nplt.annotate('Low Production',[2400,9],color='black',zorder=100); plt.annotate('High Production',[4500,9],color='black',zorder=100)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.6, wspace=0.2, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)\nplt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nplt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\n\nplt.scatter(df_test[df_test[yname]>prod_trunc][Xname[0]],df_test[df_test[yname]>prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,)\nplt.scatter(df_test[df_test[yname]<prod_trunc][Xname[0]],df_test[df_test[yname]<prod_trunc][Xname[1]],s=80,\n            marker='s',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,)\n\nplt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train')\nplt.scatter([-999],[-999],s=80,marker='s',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Test')\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Testing Truncated ' + yname + ' vs. ' + Xname[1] + ' and ' + Xname[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1]); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\npriors = (0.5,0.5)   # naive prior \n```", "```py\npriors = (0.95,0.05)                                            # set the prior probabilities of low and high production \n```", "```py\ngnb = GaussianNB(priors = priors)                             # instantiate the Gaussian naive Bayes model\nGaussianNB_fit = gnb.fit(X_train,y_train[ycname])             # train with the training data \n```", "```py\ny_pred = GaussianNB_fit.predict(np.c_[X_test['Por'],X_test['Brittle']]) # predict over the testing data\n\nplt.subplot(111)\nhigh = plt.scatter(df_train[df_train[yname]>prod_trunc][Xname[0]],df_train[df_train[yname]>prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'gold',alpha = 0.8,edgecolor = 'black',zorder=1,label='High Production')\nlow = plt.scatter(df_train[df_train[yname]<prod_trunc][Xname[0]],df_train[df_train[yname]<prod_trunc][Xname[1]],s=80,\n            marker='o',color = 'darkgrey',alpha = 0.8,edgecolor = 'black',zorder=1,label='Low Production')\n\nplt.scatter(df_test[y_pred == 0][Xname[0]]-0.19,df_test[y_pred == 0][Xname[1]],s=80,lw=2,\n             marker='<',c = df_test[y_pred == 0][ycname].values,alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap)\nplt.scatter(df_test[y_pred == 1][Xname[0]]-0.19,df_test[y_pred == 1][Xname[1]],s=80,lw=2,\n             marker='<',c = df_test[y_pred == 1][ycname].values,alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap)\n\nplt.scatter(df_test[y_pred == 0][Xname[0]]+0.19,df_test[y_pred == 0][Xname[1]],s=80,lw=2,\n             marker='>',c = y_pred[y_pred == 0],alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap,vmin=0,vmax=1)\nplt.scatter(df_test[y_pred == 1][Xname[0]]+0.19,df_test[y_pred == 1][Xname[1]],s=80,lw=2,\n             marker='>',c = y_pred[y_pred == 1],alpha = 0.8,edgecolor = 'black',zorder=1,cmap=binary_cmap,vmin=0,vmax=1)\n\nplt.scatter(df_test[Xname[0]],df_test[Xname[1]],s=10,marker='o',c = 'black',alpha = 0.8,edgecolor = 'black',zorder=100)\n\ntrain = plt.scatter([-999],[-999],s=80,marker='o',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Train') # for legend\ntruth = plt.scatter([-999],[-999],s=80,marker='<',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Truth')\npredicted = plt.scatter([-999],[-999],s=80,marker='>',color = 'white',alpha = 0.8,edgecolor = 'black',zorder=1,label='Predicted')\n\nplt.legend([low,high,train,truth,predicted],['Low Production','High Production','Train','Test Truth','Test Predicted'],loc = 'upper right')\nplt.title('Training and Testing Truncated ' + yname + ' vs. ' + Xname[1] + ' and ' + Xname[0])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1]); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nclassification_report(truth, predicted)                # build a classification report to check our classification model \n```", "```py\n* recall - the ratio of true positives divided by all cases of the category in the testing dataset\n\n* precision - the ratio of true positives divided by all positives (true positives + false positives)\n\n* f1-score - the harmonic mean of recall and precision\n\n* support - the number of samples of each category in the testing data \n```", "```py\nreport = classification_report(y_test[ycname].values, y_pred, labels=[0,1],output_dict=True)\nprecision = [report['0']['precision'],report['1']['precision']]\nrecall = [report['0']['recall'],report['1']['recall']]\nf1score = [report['0']['f1-score'],report['1']['f1-score']]\n\nplt.subplot(121)                                              # plot classification report\nplt.bar([-0.125,0.875],precision,width=0.25,color=['darkgrey','gold'],edgecolor='black')\nplt.bar([0.125,1.125],recall,width=0.25,color=['darkgrey','gold'],edgecolor='black')\n\nplt.ylim([0.,1.0]); plt.xlim([-0.5,1.5]); add_grid() \nax = plt.gca(); ax.xaxis.set_minor_locator(NullLocator())\nplt.annotate('Precision',[-0.135,0.5],rotation=90.0); plt.annotate('Recall',[0.115,0.5],rotation=90.0)\nplt.annotate('Precision',[0.865,0.5],rotation=90.0); plt.annotate('Recall',[1.115,0.5],rotation=90.0)\nplt.plot([-0.125,0.125],[f1score[0],f1score[0]],color='black')\nplt.annotate('f1-score',[-0.08,f1score[0]-0.035])\nplt.plot([0.875,1.125],[f1score[1],f1score[1]],color='black')\nplt.annotate('f1-score',[0.92,f1score[1]-0.035])\nplt.ylabel('Classification Metric'); plt.title('Classification Report, ' + str(ylabel) + ' from ' + str(Xlabel[0]) + ' and ' + str(Xname[1]))\nx_ticks = [0, 1]; x_labels = ['Low Production', 'High Production']; plt.xticks(x_ticks,x_labels)\n\nconfusion_mat = confusion_matrix(y_test[ycname].values, y_pred)\n\nplt.subplot(122)                                              # plot confusion matrix\nsns.heatmap(confusion_mat,annot=False,fmt=\"d\",annot_kws={\"color\": \"black\",\"size\":15},cbar=False,\n            xticklabels=['Low Production','High Production'], yticklabels=['Low Production','High Production',],cmap=cmap)\nplt.xlabel('Predicted Labels', fontsize=12)\nplt.ylabel('True Labels', fontsize=12)\nplt.title('Confusion Matrix, ' + str(ylabel) + ' from ' + str(Xlabel[0]) + ' and ' + str(Xname[1]), fontsize=14)\nplt.annotate(str(round(confusion_mat[0,0]/len(df_test)*100,2)) + '%',[0.5,0.45],size=15,color='black',ha='center')\nplt.annotate(str(round(confusion_mat[0,1]/len(df_test)*100,2)) + '%',[1.5,0.45],size=15,color='white',ha='center')\nplt.annotate(str(round(confusion_mat[1,0]/len(df_test)*100,2)) + '%',[0.5,1.45],size=15,color='white',ha='center')\nplt.annotate(str(round(confusion_mat[1,1]/len(df_test)*100,2)) + '%',[1.5,1.45],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[0,0]) + ' of ' + str(len(df_test)),[0.5,0.55],size=15,color='black',ha='center')\nplt.annotate(str(confusion_mat[0,1]) + ' of ' + str(len(df_test)),[1.5,0.55],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[1,0]) + ' of ' + str(len(df_test)),[0.5,1.55],size=15,color='white',ha='center')\nplt.annotate(str(confusion_mat[1,1]) + ' of ' + str(len(df_test)),[1.5,1.55],size=15,color='white',ha='center')\nplt.plot([0,2,2,0,0],[0,0,2,2,0],color='black'); plt.plot([0,2],[1,1],color='black'); plt.plot([1,1],[0,2],color='black')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nvisualize_model(GaussianNB_fit,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[ycname],0.0,1.0,\n                'Training Data and Naive Bayes Model',[0,1],['Low Production','High Production'],binary_cmap)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nvisualize_model_prob(GaussianNB_fit,X_train[Xname[0]],Xmin[0],Xmax[0],X_train[Xname[1]],Xmin[1],Xmax[1],y_train[yname],\n                     'Training Data and Naive Bayes ')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2); plt.show() \n```"]