["```py\n# Fast GELU approximation (used in practice)\nGELU(x) ≈ 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x³)))\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Initialize Adam optimizer with model parameters\n# and learning rate\noptimizer = optim.Adam(\n    model.parameters(), lr=0.001, betas=(0.9, 0.999)\n)\nloss_function = nn.CrossEntropyLoss()\n\n# Standard training loop implementing the four-step optimization cycle\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        # Step 1: Clear accumulated gradients from previous iteration\n        optimizer.zero_grad()\n\n        # Step 2: Forward pass - compute model predictions\n        predictions = model(data)\n        loss = loss_function(predictions, targets)\n\n        # Step 3: Backward pass - compute gradients via\n        # automatic differentiation\n        loss.backward()\n\n        # Step 4: Parameter update - apply Adam optimization equations\n        optimizer.step()\n```", "```py\n# Mathematical operations implemented by optimizer.step() for Adam\n# These computations happen automatically within the framework\n\n# Adam hyperparameters (typically β₁=0.9, β₂=0.999, ε=1e-8)\nbeta_1, beta_2, epsilon = 0.9, 0.999, 1e-8\nlearning_rate = 0.001\n\n# For each parameter tensor in the model:\nfor param in model.parameters():\n    if param.grad is not None:\n        grad = param.grad.data  # Current gradient\n\n        # Step 1: Update biased first moment estimate\n        # (momentum)\n        # m_t = β₁ * m_{t-1} + (1-β₁) * ∇L(θₜ)\n        momentum_buffer = (\n            beta_1 * momentum_buffer + (1 - beta_1) * grad\n        )\n\n        # Step 2: Update biased second moment estimate\n        # (squared gradients)\n        # v_t = β₂ * v_{t-1} + (1-β₂) * (∇L(θₜ))²\n        variance_buffer = beta_2 * variance_buffer + (\n            1 - beta_2\n        ) * grad.pow(2)\n\n        # Step 3: Compute bias-corrected estimates\n        momentum_corrected = momentum_buffer / (\n            1 - beta_1**step_count\n        )\n        variance_corrected = variance_buffer / (\n            1 - beta_2**step_count\n        )\n\n        # Step 4: Apply parameter update\n        # θ_{t+1} = θₜ - α * m_t / (√v_t + ε)\n        param.data -= (\n            learning_rate\n            * momentum_corrected\n            / (variance_corrected.sqrt() + epsilon)\n        )\n```", "```py\nimport torch\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport math\n\n# Initialize optimizer with initial learning rate\noptimizer = optim.Adam(\n    model.parameters(), lr=0.001, weight_decay=1e-4\n)\n\n# Configure cosine annealing scheduler\n# T_max: number of epochs for one complete cosine cycle\n# eta_min: minimum learning rate (default: 0)\nscheduler = lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=100,  # Complete cycle over 100 epochs\n    eta_min=1e-6,  # Minimum learning rate\n)\n\n# Training loop with integrated learning rate scheduling\nfor epoch in range(num_epochs):\n    # Track learning rate for monitoring\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    print(f\"Epoch {epoch}: Learning Rate = {current_lr:.6f}\")\n\n    # Standard training loop\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predictions = model(data)\n        loss = loss_function(predictions, targets)\n        loss.backward()\n        optimizer.step()\n\n    # Update learning rate at end of epoch\n    # Implements: lr = eta_min + (eta_max - eta_min) * (1 + cos(π * epoch / T_max)) / 2\n    scheduler.step()\n```", "```py\nloss.backward()  # Compute gradients\noptimizer.step()  # Update parameters\n```", "```py\nloader = DataLoader(\n    dataset, batch_size=32, num_workers=4, prefetch_factor=2\n)\n```", "```py\noptimizer.zero_grad()\nfor step in range(4):  # Accumulation steps\n    micro_batch = next(dataloader)  # 16 samples\n    loss = model(micro_batch) / 4  # Scale loss\n    loss.backward()  # Accumulate gradients\n# Now gradients represent 64 samples\nall_reduce(gradients)  # Sync across 8 GPUs\noptimizer.step()  # Update with effective batch=512\n```", "```py\n# Simple data parallelism - framework handles gradient synchronization\nmodel = torch.nn.DataParallel(model)\n# Training loop remains unchanged - framework automatically:\n# 1\\. Splits batch across GPUs\n# 2\\. Replicates model on each device\n# 3\\. Gathers gradients and averages them\n# 4\\. Broadcasts updated parameters\n```", "```py\n# Production distributed training - explicit control over communication\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\")  # NCCL for GPU communication\nmodel = torch.nn.parallel.DistributedDataParallel(model)\n# Framework now uses optimized AllReduce instead of parameter server\n```", "```py\n# Manual model parallelism - explicit device placement\nclass ModelParallelNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers_gpu0 = nn.Sequential(...).to(\"cuda:0\")\n        self.layers_gpu1 = nn.Sequential(...).to(\"cuda:1\")\n\n    def forward(self, x):\n        x = self.layers_gpu0(x.to(\"cuda:0\"))\n        x = self.layers_gpu1(\n            x.to(\"cuda:1\")\n        )  # Cross-GPU data transfer\n        return x\n```", "```py\n# Framework-provided collective operations\ndist.all_reduce(tensor)  # Gradient averaging across all devices\ndist.broadcast(tensor, src=0)  # Parameter broadcasting from master\ndist.all_gather(\n    tensor_list, tensor\n)  # Collecting tensors from all devices\n```"]