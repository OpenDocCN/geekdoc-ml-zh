- en: On-Device Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设备上学习
- en: '*DALL·E 3 Prompt: Drawing of a smartphone with its internal components exposed,
    revealing diverse miniature engineers of different genders and skin tones actively
    working on the ML model. The engineers, including men, women, and non-binary individuals,
    are tuning parameters, repairing connections, and enhancing the network on the
    fly. Data flows into the ML model, being processed in real-time, and generating
    output inferences.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E 3 提示：一部智能手机的内部组件暴露的绘图，展示了不同性别和肤色的微型工程师正在积极工作在机器学习模型上。这些工程师包括男性、女性和非二元个体，他们正在调整参数、修复连接和即时增强网络。数据流入机器学习模型，实时处理，并生成输出推断。*'
- en: '![](../media/file219.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file219.png)'
- en: Purpose
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的
- en: '*Why does on-device learning represent the most fundamental architectural shift
    in machine learning systems since the separation of training and inference, and
    what makes this capability essential for the future of intelligent systems?*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么在设备上学习代表了自训练和推理分离以来机器学习系统中最基本的架构转变，是什么使得这种能力对智能系统的未来至关重要？*'
- en: 'On-device learning dismantles the assumption governing machine learning architecture
    for decades: the separation between where models are trained and where they operate.
    This redefines what systems can become by enabling continuous adaptation in the
    real world rather than static deployment of pre-trained models. The shift from
    centralized training to distributed, adaptive learning transforms systems from
    passive inference engines into intelligent agents capable of personalization,
    privacy preservation, and autonomous improvement in disconnected environments.
    This architectural revolution becomes essential as AI systems move beyond controlled
    data centers into unpredictable environments where pre-training cannot anticipate
    every scenario or deployment condition. Understanding on-device learning principles
    enables engineers to design systems that break free from static model limitations,
    creating adaptive intelligence that learns and evolves at the point of human interaction.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上学习打破了数十年来主导机器学习架构的假设：模型训练和操作之间的分离。这通过在现实世界中实现持续适应而不是静态部署预训练模型来重新定义系统可以成为什么。从集中式训练到分布式、自适应学习的转变将系统从被动的推理引擎转变为能够在断开连接的环境中实现个性化、隐私保护和自主改进的智能代理。随着人工智能系统超越受控数据中心进入不可预测的环境，预训练无法预见每个场景或部署条件，这种架构革命变得至关重要。理解在设备上学习的原则使工程师能够设计突破静态模型限制的系统，在人类交互点创建能够学习和演化的自适应智能。
- en: '**Learning Objectives**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: Distinguish on-device learning from centralized training approaches by comparing
    computational distribution, data locality, and coordination mechanisms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过比较计算分布、数据局部性和协调机制来区分在设备上学习与集中式训练方法
- en: Identify key motivational drivers (personalization, latency, privacy, infrastructure
    efficiency) and evaluate when on-device learning is appropriate versus alternative
    approaches
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别关键的动力驱动因素（个性化、延迟、隐私、基础设施效率）并评估在设备上学习何时比其他方法更合适
- en: Analyze how training amplifies resource constraints compared to inference, quantifying
    memory (3-5<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>),
    computational (2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>),
    and energy overhead impacts on system design
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析与推理相比，训练如何放大资源限制，量化内存（3-5<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>）、计算（2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>）和能量开销对系统设计的影响
- en: Evaluate adaptation strategies including weight freezing, residual updates,
    and sparse updates by comparing their resource consumption, expressivity, and
    suitability for different device classes
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过比较资源消耗、表达性和对不同设备类别的适用性来评估包括权重冻结、残差更新和稀疏更新在内的适应策略
- en: Examine data efficiency techniques for learning with limited local datasets,
    including few-shot learning, experience replay, and data compression methods
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查在有限本地数据集上进行学习的效率技术，包括少样本学习、经验重放和数据压缩方法
- en: Apply federated learning protocols to coordinate privacy-preserving model updates
    across heterogeneous device populations while managing communication efficiency
    and convergence challenges
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用联邦学习协议来协调异构设备群体中的隐私保护模型更新，同时管理通信效率和收敛挑战
- en: Design on-device learning systems that integrate thermal management, memory
    hierarchy optimization, and power budgeting to maintain acceptable user experience
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计集成热管理、内存层次优化和电源预算的设备学习系统，以保持可接受的用户体验
- en: Implement practical deployment strategies that address MLOps integration challenges
    including device-aware pipelines, distributed monitoring, and heterogeneous update
    coordination
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施解决MLOps集成挑战的实用部署策略，包括设备感知管道、分布式监控和异构更新协调
- en: Distributed Learning Paradigm Shift
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式学习范式转变
- en: Operational frameworks ([Chapter 13](ch019.xhtml#sec-ml-operations)) establish
    the foundation for managing machine learning systems at scale through centralized
    orchestration, monitoring, and deployment pipelines. These frameworks assume controlled
    cloud environments where computational resources are abundant, network connectivity
    is reliable, and system behavior is predictable. However, as machine learning
    systems increasingly move beyond data centers to edge devices, these fundamental
    assumptions begin to break down.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 操作框架（[第13章](ch019.xhtml#sec-ml-operations)）通过集中编排、监控和部署管道，为大规模管理机器学习系统奠定基础。这些框架假设在受控的云环境中，计算资源丰富，网络连接可靠，系统行为可预测。然而，随着机器学习系统越来越多地超出数据中心，移动到边缘设备，这些基本假设开始崩溃。
- en: A smartphone learning to predict user text input, a smart home device adapting
    to household routines, or an autonomous vehicle updating its perception models
    based on local driving conditions exemplify scenarios where traditional centralized
    training approaches prove inadequate. The smartphone encounters linguistic patterns
    unique to individual users that were not present in global training data. The
    smart home device must adapt to seasonal changes and family dynamics that vary
    dramatically across households. The autonomous vehicle faces local road conditions,
    weather patterns, and traffic behaviors that differ from its original training
    environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一部学习预测用户文本输入的手机、一个适应家庭日常生活的智能家居设备，或一辆根据当地驾驶条件更新感知模型的自动驾驶汽车，都是传统集中式训练方法证明不足的场景。智能手机遇到了全球训练数据中不存在的、属于个别用户的独特语言模式。智能家居设备必须适应季节变化和家庭动态，这些变化在各个家庭之间差异很大。自动驾驶汽车面临着与原始训练环境不同的当地道路条件、天气模式和交通行为。
- en: These scenarios exemplify on-device learning, where models must train and adapt
    directly on the devices where they operate[1](#fn1). This paradigm transforms
    machine learning from a centralized discipline to a distributed ecosystem where
    learning occurs across millions of heterogeneous devices, each operating under
    unique constraints and local conditions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些场景是设备学习的例子，其中模型必须在它们运行的设备上直接进行训练和适应[1](#fn1)。这种范式将机器学习从集中学科转变为分布式生态系统，学习发生在数百万种异构设备上，每个设备都处于独特的约束和本地条件下。
- en: 'The transition to on-device learning introduces fundamental tension in machine
    learning systems design. While cloud-based architectures leverage abundant computational
    resources and controlled operational environments, edge devices must function
    within severely constrained resource envelopes characterized by limited memory
    capacity, restricted computational throughput, constrained energy budgets, and
    intermittent network connectivity. These constraints that make on-device learning
    technically challenging simultaneously enable its most significant advantages:
    personalized adaptation through localized data processing, privacy preservation
    through data locality, and operational autonomy through independence from centralized
    infrastructure.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 向设备学习过渡在机器学习系统设计中引入了基本紧张关系。虽然基于云的架构利用了丰富的计算资源和受控的操作环境，但边缘设备必须在严重受限的资源范围内运行，这些资源包括有限的内存容量、受限的计算吞吐量、受限的能源预算和间歇性的网络连接。这些使设备学习在技术上具有挑战性的约束同时使其最显著的优势得以实现：通过本地数据处理实现个性化适应，通过数据本地化保护隐私，以及通过独立于集中基础设施实现操作自主性。
- en: This chapter examines the theoretical foundations and practical methodologies
    necessary to navigate this architectural tension. Building on computational efficiency
    principles ([Chapter 9](ch015.xhtml#sec-efficient-ai)) and operational frameworks
    ([Chapter 13](ch019.xhtml#sec-ml-operations)), we investigate the specialized
    algorithmic techniques, architectural design patterns, and system-level principles
    that enable effective learning under extreme resource constraints. The challenge
    extends beyond conventional optimization of training algorithms, requiring reconceptualization
    of the entire machine learning pipeline for deployment environments where traditional
    computational assumptions fail.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了导航这种架构紧张关系所需的理论基础和实践方法。基于计算效率原则（[第9章](ch015.xhtml#sec-efficient-ai)）和运营框架（[第13章](ch019.xhtml#sec-ml-operations)），我们研究了在极端资源约束下实现有效学习的专用算法技术、架构设计模式和系统级原则。挑战不仅超越了传统训练算法的优化，还要求对整个机器学习管道进行重新概念化，以适应传统计算假设失效的部署环境。
- en: '***On-Device Learning*** is the local training or adaptation of machine learning
    models directly on deployed hardware without server connectivity, enabling *personalization*,
    *privacy preservation*, and *autonomous operation* under severe resource constraints.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**设备端学习**是指在无服务器连接的情况下，直接在部署的硬件上对机器学习模型进行本地训练或适配。这种模式在资源受限的条件下，能够实现*个性化*、*隐私保护*和*自主操作*。'
- en: The implications of this paradigm extend far beyond technical optimization,
    challenging established assumptions regarding machine learning system development,
    deployment, and maintenance lifecycles. Models transition from following predictable
    versioning patterns to exhibiting continuous divergence and adaptation trajectories.
    Performance evaluation methodologies shift from centralized monitoring dashboards
    to distributed assessment across heterogeneous user populations. Privacy preservation
    evolves from a regulatory compliance consideration to a core architectural requirement
    that shapes system design decisions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种范式的影响远远超出了技术优化，挑战了关于机器学习系统开发、部署和维护生命周期的既定假设。模型从遵循可预测的版本模式转变为表现出连续的分歧和适应轨迹。性能评估方法从集中的监控仪表板转变为跨异构用户群体的分布式评估。隐私保护从监管合规的考虑转变为塑造系统设计决策的核心架构要求。
- en: Understanding these systemic implications requires examining both the compelling
    motivations driving organizational adoption of on-device learning and the substantial
    technical challenges that must be addressed. This analysis establishes the theoretical
    foundations and practical methodologies required to architect systems capable
    of effective learning at the network edge while operating within stringent constraints.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些系统性的影响需要考察推动组织采用设备端学习的强大动机以及必须解决的重大技术挑战。这种分析建立了构建能够在网络边缘有效学习的同时，在严格约束下运行的系统的理论基础和实践方法。
- en: Motivations and Benefits
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机与益处
- en: Machine learning systems have traditionally relied on centralized training pipelines,
    where models are developed and refined using large, curated datasets and powerful
    cloud-based infrastructure ([Jeffrey Dean and Ghemawat 2008](ch058.xhtml#ref-dean2012large)).
    Once trained, these models are deployed to client devices for inference, creating
    a clear separation between the training and deployment phases. While this architectural
    separation has served most use cases well, it imposes significant limitations
    in modern applications where local data is dynamic, private, or highly personalized.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习系统通常依赖于集中的训练流程，其中模型通过使用大型、精心挑选的数据集和强大的云基础设施进行开发和优化（[Jeffrey Dean 和 Ghemawat
    2008](ch058.xhtml#ref-dean2012large)）。一旦训练完成，这些模型就会被部署到客户端设备上进行推理，从而在训练和部署阶段之间形成清晰的分离。虽然这种架构分离在大多数用例中都表现良好，但在现代应用中，本地数据是动态的、私密的或高度个性化的，它带来了显著的限制。
- en: On-device learning challenges this established model by enabling systems to
    train or adapt directly on the device, without relying on constant connectivity
    to the cloud. This shift represents more than a technological advancement, it
    reflects changing application requirements and user expectations that demand responsive,
    personalized, and privacy-preserving machine learning systems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习通过允许系统在设备上直接进行训练或自适应，而不依赖于持续连接到云端，挑战了这一既定模型。这种转变不仅仅代表技术进步，它反映了不断变化的应用需求和用户期望，这些需求和期望要求响应性、个性化且保护隐私的人工智能系统。
- en: Consider a smartphone keyboard adapting to a user’s unique vocabulary and typing
    patterns. To personalize predictions, the system must perform gradient updates
    on a compact language model using locally observed text input. A single gradient
    update for even a minimal language model requires 50-100 MB of memory for activations
    and optimizer state. Modern smartphones typically allocate 200-300 MB to background
    applications like keyboards (varies by OS and device generation). This razor-thin
    margin, where a single training step consumes 25% of available memory, exemplifies
    the central engineering challenge of on-device learning. The system must achieve
    meaningful personalization while operating within constraints so severe that traditional
    training approaches become architecturally infeasible. This quantitative reality
    drives the need for specialized techniques that make adaptation possible within
    extreme resource limitations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个智能手机键盘适应用户的独特词汇和打字模式。为了个性化预测，系统必须在本地观察到的文本输入上对紧凑的语言模型执行梯度更新。即使是对于最小的语言模型，单个梯度更新也需要50-100
    MB的内存用于激活和优化器状态。现代智能手机通常为键盘等后台应用程序分配200-300 MB（根据操作系统和设备代系而异）。这个极薄的内存边界，其中单个训练步骤消耗了可用内存的25%，体现了设备上学习的核心工程挑战。系统必须在如此严重的限制下实现有意义的个性化，而传统的训练方法在架构上变得不可行。这种定量现实推动了需要专门技术，这些技术使得在极端资源限制下实现自适应成为可能。
- en: On-Device Learning Benefits
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备上学习的好处
- en: Understanding the driving forces behind on-device learning adoption requires
    examining the inherent limitations of traditional centralized approaches. Traditional
    machine learning systems rely on a clear division of labor between model training
    and inference. Training is performed in centralized environments with access to
    high-performance compute resources and large-scale datasets. Once trained, models
    are distributed to client devices, where they operate in a static inference-only
    mode.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 理解设备上学习采用背后的驱动因素需要审视传统集中式方法固有的局限性。传统的机器学习系统依赖于模型训练和推理之间明确的劳动分工。训练在可访问高性能计算资源和大规模数据集的集中环境中进行。一旦训练完成，模型被分发到客户端设备，在那里它们以静态的推理模式运行。
- en: While this centralized paradigm has proven effective in many deployments, it
    introduces fundamental limitations in scenarios where data is user-specific, behavior
    is dynamic, or connectivity is intermittent. These limitations become particularly
    acute as machine learning moves beyond controlled environments into real-world
    applications with diverse user populations and deployment contexts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种集中式范式在许多部署中已被证明是有效的，但它引入了在数据是用户特定、行为是动态的或连接是间歇性的场景中的基本局限性。随着机器学习超越受控环境进入具有多样化用户群体和部署背景的实际情况，这些局限性变得尤为突出。
- en: 'On-device learning addresses these limitations by enabling deployed devices
    to perform model adaptation using locally available data. On-device learning is
    not merely an efficiency optimization; it serves as a cornerstone of building
    trustworthy AI systems, opening Part IV: Trustworthy Systems. By keeping data
    local, it provides a powerful foundation for privacy. By adapting to individual
    users, it enhances fairness and utility. By enabling offline operation, it improves
    robustness against network failures and infrastructure dependencies. This chapter
    explores the engineering required to build these trustworthy, adaptive systems.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习通过允许部署的设备使用本地数据执行模型自适应来克服这些局限性。设备上学习不仅仅是效率优化；它是构建可信赖人工智能系统的基石，开启第四部分：可信赖的系统。通过保持数据本地化，它为隐私提供了强大的基础。通过适应单个用户，它增强了公平性和实用性。通过支持离线操作，它提高了对网络故障和基础设施依赖的鲁棒性。本章探讨了构建这些可信赖、自适应系统所需的工程。
- en: 'This shift from centralized to decentralized learning is motivated by four
    key considerations that reflect both technological capabilities and changing application
    requirements: personalization, latency and availability, privacy, and infrastructure
    efficiency ([T. Li et al. 2020](ch058.xhtml#ref-li2020federated)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从集中式到分布式学习的转变是由四个关键考虑因素驱动的，这些因素反映了技术能力和不断变化的应用需求：个性化、延迟和可用性、隐私和基础设施效率 ([T.
    Li et al. 2020](ch058.xhtml#ref-li2020federated))。
- en: Personalization represents the most compelling motivation, as deployed models
    often encounter usage patterns and data distributions that differ substantially
    from their training environments. Local adaptation allows models to refine behavior
    in response to user-specific data, capturing linguistic preferences, physiological
    baselines, sensor characteristics, or environmental conditions. This capability
    proves essential in applications with high inter-user variability, where a single
    global model cannot serve all users effectively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化代表了最具吸引力的动机，因为部署的模型经常遇到与它们的训练环境有显著差异的使用模式和数据分布。本地适应允许模型根据用户特定的数据来细化行为，捕捉语言偏好、生理基线、传感器特征或环境条件。这种能力在用户间差异大的应用中证明是至关重要的，在这些应用中，一个全局模型无法有效地服务于所有用户。
- en: Latency and availability constraints provide additional justification for local
    learning. In edge computing scenarios, connectivity to centralized infrastructure
    may be unreliable, delayed, or intentionally limited to preserve bandwidth or
    reduce energy consumption. On-device learning enables autonomous improvement of
    models even in fully offline or delay-sensitive contexts, where round-trip updates
    to the cloud are architecturally infeasible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟和可用性限制为本地学习提供了额外的理由。在边缘计算场景中，连接到集中式基础设施可能不可靠、延迟或有意限制以保留带宽或减少能耗。设备上的学习允许模型在完全离线或对延迟敏感的上下文中自主改进，即使往返云端的更新在架构上不可行。
- en: Privacy considerations provide a third compelling driver. Many modern applications
    involve sensitive or regulated data, including biometric measurements, typed input,
    location traces, or health information. Local learning mitigates privacy concerns
    by keeping raw data on the device and operating within privacy-preserving boundaries,
    potentially aiding adherence to regulations such as GDPR[2](#fn2), HIPAA ([Tomes
    1996](ch058.xhtml#ref-hipaa1996health)), or region-specific data sovereignty laws.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私考虑提供了一个第三大推动力。许多现代应用涉及敏感或受监管的数据，包括生物识别测量、输入文本、位置跟踪或健康信息。本地学习通过在设备上保留原始数据并在隐私保护边界内操作来减轻隐私担忧，可能有助于遵守GDPR[2](#fn2)、HIPAA
    ([Tomes 1996](ch058.xhtml#ref-hipaa1996health))或特定区域的数据主权法律。
- en: Infrastructure efficiency provides economic motivation for distributed learning
    approaches. Centralized training pipelines require substantial backend infrastructure
    to collect, store, and process user data from potentially millions of devices.
    By shifting learning to the edge, systems reduce communication costs and distribute
    training workloads across the deployment fleet, relieving pressure on centralized
    resources while improving scalability.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施效率为分布式学习方法提供了经济动机。集中式训练管道需要大量的后端基础设施来收集、存储和处理来自数百万设备的用户数据。通过将学习转移到边缘，系统降低了通信成本，并将训练工作负载分布在部署舰队中，减轻了集中资源的压力，同时提高了可扩展性。
- en: Alternative Approaches and Decision Criteria
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替代方法和决策标准
- en: On-device learning represents a significant engineering investment with inherent
    complexity that may not be justified by the benefits. Before committing to this
    approach, teams should carefully evaluate whether simpler alternatives can achieve
    comparable results with lower operational overhead. Understanding when not to
    implement on-device learning is as important as understanding its benefits, as
    premature adoption can introduce unnecessary complexity without proportional value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上的学习代表了一个重大的工程投资，其固有的复杂性可能无法由其带来的好处来证明。在承诺采用这种方法之前，团队应仔细评估是否更简单的替代方案可以以更低的运营成本实现类似的结果。了解何时不实施设备上的学习与了解其好处一样重要，因为过早采用可能会在不成比例的价值下引入不必要的复杂性。
- en: 'Several alternative approaches often suffice for personalization and adaptation
    requirements without local training complexity:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 几种替代方法通常足以满足个性化与适应需求，而不需要本地训练的复杂性：
- en: '**Feature-based Personalization**: Provides effective customization by storing
    user preferences, interaction history, and behavioral features locally. Rather
    than adapting model weights, the system feeds these stored features into a static
    model to achieve personalization. News recommendation systems exemplify this approach
    by storing user topic preferences and reading patterns locally, then combining
    these features with a centralized content model to provide personalized recommendations
    without model updates.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于特征的个性化**：通过本地存储用户偏好、交互历史和行为特征来实现有效的定制。系统不是调整模型权重，而是将这些存储的特征输入到静态模型中，以实现个性化。新闻推荐系统通过本地存储用户主题偏好和阅读模式，然后将这些特征与集中式内容模型结合，提供个性化的推荐，而无需更新模型。'
- en: '**Cloud-based Fine-tuning with Privacy Controls**: Enables personalization
    through centralized adaptation with appropriate privacy safeguards. User data
    is processed in batches during off-peak hours using privacy-preserving techniques
    such as differential privacy[3](#fn3) or federated analytics. This approach often
    achieves superior accuracy compared to resource-constrained on-device updates
    while maintaining acceptable privacy properties for many applications.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于云的微调与隐私控制**：通过集中式适应实现个性化，同时采用适当的隐私保护措施。用户数据在非高峰时段批量处理，使用差分隐私[3](#fn3)或联邦分析等隐私保护技术。这种方法通常比资源受限的设备更新具有更高的准确性，同时保持许多应用的可接受的隐私属性。'
- en: '**User-specific Lookup Tables**: Combine global models with personalized retrieval
    mechanisms. The system maintains a lightweight, user-specific lookup table for
    frequently accessed patterns while using a shared global model for generalization.
    This hybrid approach provides personalization benefits with minimal computational
    and storage overhead.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户特定的查找表**：结合全局模型与个性化检索机制。系统维护一个轻量级的、针对特定用户的查找表，用于频繁访问的模式，同时使用共享的全局模型进行泛化。这种混合方法提供了个性化优势，同时计算和存储开销最小。'
- en: The decision to implement on-device learning should be driven by quantifiable
    requirements that preclude these simpler alternatives. True data privacy constraints
    that legally prohibit cloud processing, genuine network limitations that prevent
    reliable connectivity, quantitative latency budgets that preclude cloud round-trips,
    or demonstrable performance improvements that justify the operational complexity
    represent legitimate drivers for on-device learning adoption.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实施设备学习的决策应受可量化的需求驱动，这些需求排除了这些更简单的替代方案。真正的数据隐私限制，法律禁止云处理，真正的网络限制，防止可靠连接，定量延迟预算，禁止云往返，或可证明的性能改进，证明操作复杂性的合理性，这些是设备学习采用的有效驱动因素。
- en: For applications with critical timing requirements (camera processing under
    33 ms, voice response under 500 ms, AR/VR motion-to-photon latency under 20 ms,
    or safety-critical control under 10 ms), network round-trip times (typically 50-200 ms)
    make cloud-based alternatives architecturally infeasible. In such scenarios, on-device
    learning becomes necessary regardless of complexity considerations. Teams should
    thoroughly evaluate simpler solutions before committing to the significant engineering
    investment that on-device learning requires.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有关键时间要求的（例如，33 ms以下的相机处理，500 ms以下的语音响应，20 ms以下的AR/VR运动到光子延迟，或10 ms以下的安全关键控制），网络往返时间（通常为50-200 ms）使得基于云的替代方案在架构上不可行。在这种情况下，无论考虑复杂性的因素如何，设备学习都成为必要。团队在承诺设备学习所需的重大工程投资之前，应彻底评估更简单的解决方案。
- en: 'These motivations are grounded in the broader concept of knowledge transfer,
    where a pretrained model transfers useful representations to a new task or domain.
    This foundational principle makes on-device learning both feasible and effective,
    enabling sophisticated adaptation with minimal local resources. As depicted in
    [Figure 14.1](ch020.xhtml#fig-transfer-conceptual), knowledge transfer can occur
    between closely related tasks (e.g., playing different board games or musical
    instruments), or across domains that share structure (e.g., from riding a bicycle
    to driving a scooter). In the context of on-device learning, this means leveraging
    a model pretrained in the cloud and adapting it efficiently to a new context using
    only local data and limited updates. The figure highlights the key idea: pretrained
    knowledge allows fast adaptation without relearning from scratch, even when the
    new task diverges in input modality or goal.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些动机根植于更广泛的知识迁移概念，其中预训练模型将有用的表示迁移到新的任务或领域。这一基本原理使得设备上的学习既可行又有效，允许在最小本地资源下进行复杂的适应。如图14.1[图14.1](ch020.xhtml#fig-transfer-conceptual)所示，知识迁移可以发生在紧密相关的任务之间（例如，玩不同的桌面游戏或乐器），或者跨越具有相似结构的领域（例如，从骑自行车到骑滑板）。在设备上学习的背景下，这意味着利用在云端预训练的模型，并仅使用本地数据和有限的更新来高效地将其适应到新的环境中。该图强调了关键思想：预训练的知识允许快速适应，无需从头开始重新学习，即使新任务在输入模式或目标上有所偏离。
- en: '![](../media/file220.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file220.png)'
- en: 'Figure 14.1: Knowledge Transfer: Pretrained models accelerate learning on new
    tasks by leveraging existing representations, as seen by adapting skills between
    related board games or musical instruments. This transfer extends across domains
    like bicycle riding and scooter operation, where shared underlying structures
    allow efficient adaptation with limited new data.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：知识迁移：预训练模型通过利用现有表示来加速新任务的学习，例如通过在相关桌面游戏或乐器之间调整技能。这种迁移扩展到自行车骑行和滑板操作等跨领域，其中共享的底层结构允许在有限的新数据下进行高效的调整。
- en: This conceptual shift, enabled by transfer learning and adaptation, enables
    real-world on-device applications. Whether adapting a language model for personal
    typing preferences, adjusting gesture recognition to individual movement patterns,
    or recalibrating a sensor model in changing environments, on-device learning allows
    systems to remain responsive, efficient, and user-aligned over time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种由迁移学习和适应能力带来的概念转变，使得实际设备上的应用成为可能。无论是调整语言模型以适应个人打字偏好，调整手势识别以适应个人运动模式，还是在不断变化的环境中重新校准传感器模型，设备上的学习允许系统在时间上保持响应性、高效性和与用户的对齐。
- en: Real-World Application Domains
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际应用领域
- en: Building on these established motivations (personalization, latency, privacy,
    and infrastructure efficiency), real-world deployments demonstrate the practical
    impact of on-device learning across diverse application domains. These domains
    span consumer technologies, healthcare, industrial systems, and embedded applications,
    each showcasing scenarios where the benefits outlined above become essential for
    effective machine learning deployment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在这些既定动机（个性化、延迟、隐私和基础设施效率）的基础上，实际部署展示了设备上学习在众多应用领域中的实际影响。这些领域包括消费技术、医疗保健、工业系统和嵌入式应用，每个领域都展示了上述好处对于有效机器学习部署成为关键的场景。
- en: Mobile input prediction represents the most mature and widely deployed example
    of on-device learning. In systems such as smartphone keyboards, predictive text
    and autocorrect features benefit substantially from continuous local adaptation.
    User typing patterns are highly personalized and evolve dynamically, making centralized
    static models insufficient for optimal user experience. On-device learning allows
    language models to fine-tune their predictions directly on the device, achieving
    personalization while maintaining data locality.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 移动输入预测代表了在设备上学习的最成熟和最广泛部署的例子。在智能手机键盘等系统中，预测文本和自动纠错功能从持续的本地调整中受益良多。用户的打字模式高度个性化且动态变化，使得集中式的静态模型不足以提供最佳用户体验。设备上的学习允许语言模型直接在设备上微调其预测，实现个性化同时保持数据本地性。
- en: For instance, Google’s Gboard employs federated learning to improve shared models
    across a large population of users while keeping raw data local to each device
    ([Hard et al. 2018](ch058.xhtml#ref-hard2018federated))[4](#fn4).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Google的Gboard使用联邦学习来改善大量用户之间的共享模型，同时保持每个设备的原始数据本地化([Hard等人2018](ch058.xhtml#ref-hard2018federated))[4](#fn4)。
- en: 'As shown in [Figure 14.2](ch020.xhtml#fig-ondevice-gboard), different prediction
    strategies illustrate how local adaptation operates in real time: next-word prediction
    (NWP) suggests likely continuations based on prior text, while Smart Compose uses
    on-the-fly rescoring to offer dynamic completions, demonstrating the sophistication
    of local inference mechanisms.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图14.2](ch020.xhtml#fig-ondevice-gboard)所示，不同的预测策略展示了本地适应在实时操作中的工作方式：下一词预测（NWP）基于先前的文本提出可能的延续，而智能完成则使用即时重新评分提供动态补全，展示了本地推理机制的复杂性。
- en: '![](../media/file221.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file221.png)'
- en: 'Figure 14.2: On-Device Prediction Strategies: Gboard employs both next-word
    prediction and smart compose with on-the-fly rescoring to adapt to user typing
    patterns locally, enhancing personalization and preserving privacy. These techniques
    demonstrate how machine learning models can refine predictions in real time without
    transmitting data to a central server, enabling efficient and private mobile input
    experiences.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：设备上的预测策略：Gboard同时使用下一词预测和智能完成以及即时重新评分来适应用户的本地打字模式，增强个性化并保护隐私。这些技术展示了机器学习模型如何在实时中细化预测，而不需要将数据传输到中央服务器，从而实现高效且私密的移动输入体验。
- en: Building on the consumer applications, wearable and health monitoring devices
    present equally compelling use cases with additional regulatory constraints. These
    systems rely on real-time data from accelerometers, heart rate sensors, and electrodermal
    activity monitors to track user health and fitness. Physiological baselines vary
    dramatically between individuals, creating a personalization challenge that static
    models cannot address effectively. On-device learning allows models to adapt to
    these individual baselines over time, substantially improving the accuracy of
    activity recognition, stress detection, and sleep staging while meeting regulatory
    requirements for data localization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费应用的基础上，可穿戴设备和健康监测设备提供了同样有吸引力的用例，并增加了额外的监管约束。这些系统依赖于加速度计、心率传感器和皮肤电活动监测器的实时数据来跟踪用户健康和健身。生理基线在个体之间差异很大，这为静态模型无法有效解决的个性化挑战。设备上的学习允许模型随着时间的推移适应这些个体基线，显著提高活动识别、压力检测和睡眠阶段的准确性，同时满足数据本地化的监管要求。
- en: Voice interaction technologies present another important application domain
    with unique acoustic challenges. Wake-word detection[5](#fn5) and voice interfaces
    in devices such as smart speakers and earbuds must recognize voice commands quickly
    and accurately, even in noisy or dynamic acoustic environments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 语音交互技术呈现了另一个重要的应用领域，具有独特的声学挑战。唤醒词检测[5](#fn5)以及智能音箱和耳机等设备中的语音界面必须快速准确地识别语音命令，即使在嘈杂或动态的声学环境中也是如此。
- en: 'These systems face strict latency requirements: voice interfaces must maintain
    end-to-end response times under 500 ms to preserve natural conversation flow,
    with wake-word detection requiring sub-100 ms response times to avoid user frustration.
    Local training allows models to adapt to the user’s unique voice profile and changing
    ambient context, reducing false positives and missed detections while meeting
    these demanding performance constraints. This adaptation proves particularly valuable
    in far-field audio settings, where microphone configurations and room acoustics
    vary dramatically across deployments.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统面临严格的延迟要求：语音界面必须保持端到端响应时间低于500毫秒，以保持自然对话流程，唤醒词检测需要低于100毫秒的响应时间，以避免用户感到沮丧。本地训练允许模型适应用户的独特语音特征和不断变化的周围环境，在满足这些性能约束的同时，减少误报和漏检。这种适应在远场音频设置中尤其有价值，因为麦克风配置和房间声学在部署中差异很大。
- en: Beyond consumer applications, industrial IoT and remote monitoring systems demonstrate
    the value of on-device learning in resource-constrained environments. In applications
    such as agricultural sensing, pipeline monitoring, or environmental surveillance,
    connectivity to centralized infrastructure may be limited, expensive, or entirely
    unavailable. On-device learning allows these systems to detect anomalies, adjust
    thresholds, or adapt to seasonal trends without continuous communication with
    the cloud. This capability proves necessary for maintaining autonomy and reliability
    in edge-deployed sensor networks, where system downtime or missed detections can
    have significant economic or safety consequences.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了消费类应用之外，工业物联网和远程监控系统展示了在资源受限环境中设备上学习的价值。在农业传感、管道监控或环境监控等应用中，连接到集中式基础设施可能有限、昂贵或完全不可用。设备上的学习允许这些系统检测异常、调整阈值或适应季节性趋势，而无需与云持续通信。这种能力对于保持边缘部署的传感器网络的自主性和可靠性至关重要，因为系统停机或漏检可能产生重大的经济或安全后果。
- en: The most demanding applications emerge in embedded computer vision systems,
    including those in robotics, AR/VR, and smart cameras, which combine complex visual
    processing with extreme timing constraints. Camera applications must process frames
    within 33 ms to maintain 30 FPS real-time performance, while AR/VR systems demand
    motion-to-photon latencies under 20 ms to prevent nausea and maintain immersion.
    Safety-critical control systems require even tighter bounds, typically under 10 ms,
    where delayed decisions can have severe consequences. These systems operate in
    novel or rapidly changing environments that differ substantially from their original
    training conditions. On-device adaptation allows models to recalibrate to new
    lighting conditions, object appearances, or motion patterns while meeting these
    critical latency budgets that fundamentally drive the architectural decision between
    on-device versus cloud-based processing.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最具挑战性的应用出现在嵌入式计算机视觉系统中，包括机器人、AR/VR和智能摄像头等，这些系统将复杂的视觉处理与极端的时序约束相结合。摄像头应用必须在33毫秒内处理帧以保持30
    FPS的实时性能，而AR/VR系统则要求运动到光子的延迟低于20毫秒，以防止恶心并保持沉浸感。安全关键的控制系统需要更严格的界限，通常低于10毫秒，因为延迟的决策可能产生严重的后果。这些系统在新的或快速变化的环境中运行，与它们的原始训练条件有显著差异。设备上的自适应允许模型重新校准以适应新的光照条件、物体外观或运动模式，同时满足这些关键的延迟预算，这些预算从根本上推动了设备上与基于云的处理之间的架构决策。
- en: 'Each domain reveals a common pattern: deployment environments introduce variation
    and context-specific requirements that cannot be anticipated during centralized
    training. These applications demonstrate how the motivational drivers (personalization,
    latency, privacy, and infrastructure efficiency) manifest as concrete engineering
    constraints. Mobile keyboards face memory limitations for storing user-specific
    patterns, wearable devices encounter energy budgets that restrict training frequency,
    voice interfaces must meet sub-100 ms latency requirements that preclude cloud
    coordination, and industrial IoT systems operate in network-constrained environments
    that demand autonomous adaptation. This pattern illuminates the fundamental design
    requirement shaping all subsequent technical decisions: learning must be performed
    efficiently, privately, and reliably under significant resource constraints that
    we examine through constraint analysis ([Section 14.3](ch020.xhtml#sec-ondevice-learning-design-constraints-c776)),
    adaptation techniques ([Section 14.4](ch020.xhtml#sec-ondevice-learning-model-adaptation-6a82)),
    and federated coordination ([Section 14.6](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每个领域都揭示了一个共同的模式：部署环境引入了变化和特定上下文的要求，这些要求在集中式训练期间无法预料。这些应用展示了动机驱动因素（个性化、延迟、隐私和基础设施效率）如何表现为具体的工程约束。移动键盘面临存储用户特定模式的内存限制，可穿戴设备遇到限制训练频率的能量预算，语音界面必须满足低于100毫秒的延迟要求，这排除了云协调，而工业物联网系统在网络受限的环境中运行，需要自主适应。这种模式揭示了塑造所有后续技术决策的基本设计要求：学习必须在重大资源约束下高效、私密和可靠地进行，这些约束通过约束分析（[第14.3节](ch020.xhtml#sec-ondevice-learning-design-constraints-c776)）、自适应技术（[第14.4节](ch020.xhtml#sec-ondevice-learning-model-adaptation-6a82)）和联邦协调（[第14.6节](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e)）来考察。
- en: 'Architectural Trade-offs: Centralized vs. Decentralized Training'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建筑权衡：集中式与去中心化训练
- en: These applications demonstrate the practical value of on-device learning across
    diverse domains. Building on this foundation, we now examine how on-device learning
    differs from traditional ML architectures, revealing a complete reimagining of
    the training lifecycle that extends far beyond simple deployment choices.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用展示了在多个领域内设备端学习的实际价值。在此基础上，我们现在探讨设备端学习与传统机器学习架构的不同之处，揭示了训练生命周期的全面重新构想，这远远超出了简单的部署选择。
- en: Understanding the shift that on-device learning represents requires examining
    how traditional machine learning systems are structured and where their limitations
    become apparent. Most machine learning systems today follow a centralized learning
    paradigm that has served the field well but increasingly shows strain under modern
    deployment requirements. Models are trained in data centers using large-scale,
    curated datasets aggregated from many sources. Once trained, these models are
    deployed to client devices in a static form, where they perform inference without
    further modification. Updates to model parameters, either to incorporate new data
    or to improve generalization, are handled periodically through offline retraining,
    often using newly collected or labeled data sent back from the field.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解设备端学习所代表的转变，需要检查传统机器学习系统的结构以及它们的局限性在哪里变得明显。如今，大多数机器学习系统遵循一个集中式学习范式，这个范式为该领域服务得很好，但随着现代部署要求的增加，它越来越显示出压力。模型在数据中心使用大规模、精心挑选的数据集进行训练，这些数据集来自许多来源。一旦训练完成，这些模型就以静态形式部署到客户端设备上，在那里它们执行推理而无需进一步修改。模型参数的更新，无论是为了整合新数据还是为了提高泛化能力，通常通过离线重新训练来处理，这通常使用从现场收集或标记的数据来完成。
- en: 'This established centralized model offers numerous proven advantages: high-performance
    computing infrastructure, access to diverse data distributions, and robust debugging
    and validation pipelines. It also depends on several assumptions that may not
    hold in modern deployment scenarios: reliable data transfer, trust in data custodianship,
    and infrastructure capable of managing global updates across device fleets. As
    machine learning is deployed into increasingly diverse and distributed environments,
    the limitations of this approach become more apparent and often prohibitive.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种既定的集中式模型提供了许多经过验证的优势：高性能计算基础设施、访问多样化的数据分布，以及强大的调试和验证管道。它还依赖于几个在现代部署场景中可能不成立的假设：可靠的数据传输、对数据保管人的信任，以及能够管理跨设备编队的全球更新的基础设施。随着机器学习被部署到越来越多样化、分布式的环境中，这种方法的优势变得更加明显，但同时也往往具有阻碍性。
- en: In contrast to this centralized approach, on-device learning embraces an inherently
    decentralized paradigm that challenges many traditional assumptions. Each device
    maintains its own copy of a model and adapts it locally using data that is typically
    unavailable to centralized infrastructure. Training occurs on-device, often asynchronously
    and under varying resource conditions that change based on device usage patterns,
    battery levels, and thermal states. Data never leaves the device, reducing privacy
    exposure but also complicating coordination between devices. Devices may differ
    dramatically in their hardware capabilities, runtime environments, and patterns
    of use, making the learning process heterogeneous and difficult to standardize.
    These hardware variations create significant system design challenges.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种集中式方法相反，设备端学习拥抱了一种固有的去中心化范式，挑战了许多传统假设。每个设备都维护自己模型的副本，并使用通常无法提供给集中式基础设施的数据在本地对其进行调整。训练在设备上发生，通常是异步的，并在基于设备使用模式、电池水平和热状态变化的资源条件下进行。数据永远不会离开设备，这减少了隐私暴露，但也使得设备间的协调变得更加复杂。设备在硬件能力、运行环境和使用模式上可能存在显著差异，使得学习过程异构且难以标准化。这些硬件变化为系统设计带来了重大挑战。
- en: This decentralized architecture introduces a new class of systems challenges
    that extend well beyond traditional machine learning concerns. Devices may operate
    with different versions of the model, leading to inconsistencies in behavior across
    the deployment fleet. Evaluation and validation become significantly more complex,
    as there is no central point from which to measure performance across all devices
    ([McMahan et al. 2017c](ch058.xhtml#ref-mcmahan2017communication)). Model updates
    must be carefully managed to prevent degradation, and safety guarantees become
    substantially harder to enforce in the absence of centralized testing and validation
    infrastructure.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种去中心化架构引入了一类新的系统挑战，这些挑战远远超出了传统机器学习关注的范畴。设备可能运行不同版本的模型，导致部署集群中行为的不一致性。评估和验证变得更加复杂，因为没有中央点可以衡量所有设备的性能([McMahan
    等人 2017c](ch058.xhtml#ref-mcmahan2017communication))。必须谨慎管理模型更新以防止退化，并且在缺乏集中测试和验证基础设施的情况下，安全保证变得更加难以执行。
- en: Managing thousands of heterogeneous edge devices exceeds typical distributed
    systems complexity. Device heterogeneity extends beyond hardware differences to
    include varying operating system versions, security patches, network configurations,
    and power management policies. At any given time, 20-40% of devices are offline
    ([Bonawitz et al. 2019](ch058.xhtml#ref-bonawitz2019towards)), while others have
    been disconnected for weeks or months, creating persistent coordination challenges.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 管理数千个异构边缘设备超出了典型分布式系统的复杂性。设备异质性不仅包括硬件差异，还包括不同的操作系统版本、安全补丁、网络配置和电源管理策略。在任何给定时间，20-40%
    的设备处于离线状态([Bonawitz 等人 2019](ch058.xhtml#ref-bonawitz2019towards))，而其他设备可能已经断开连接数周或数月，从而造成持续的协调挑战。
- en: 'When disconnected devices reconnect, they require state reconciliation to avoid
    version conflicts. Update verification becomes critical as devices can silently
    fail to apply updates or report success while running outdated models. Robust
    systems implement multi-stage verification: cryptographic signatures confirm update
    integrity, functional tests validate model behavior, and telemetry confirms deployment
    success. Rollback strategies must handle partial deployments where some devices
    received updates while others remain on previous versions, requiring sophisticated
    orchestration to maintain system consistency during failure recovery.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当断开连接的设备重新连接时，它们需要状态协调以避免版本冲突。更新验证变得至关重要，因为设备可能静默地未能应用更新或报告成功，同时运行着过时的模型。健壮的系统实施多阶段验证：加密签名确认更新完整性，功能测试验证模型行为，遥测确认部署成功。回滚策略必须处理部分部署的情况，其中一些设备已接收更新，而其他设备仍停留在之前的版本，这需要复杂的编排来在故障恢复期间保持系统一致性。
- en: These challenges require different approaches to system design and operational
    management compared to centralized ML systems, building on the distributed systems
    principles from [Chapter 13](ch019.xhtml#sec-ml-operations) while introducing
    edge-specific complexities.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战需要与集中式机器学习系统相比不同的系统设计和运营管理方法，基于第 13 章中分布式系统原则，同时引入边缘特定的复杂性。
- en: Despite these challenges, decentralization introduces opportunities that often
    justify the additional complexity. It allows for deep personalization without
    centralized oversight, supports robust learning in disconnected or bandwidth-limited
    environments, and reduces the operational cost and infrastructure burden for model
    updates. Realizing these benefits raises questions of how to effectively coordinate
    learning across devices, whether through periodic synchronization, federated aggregation,
    or hybrid approaches that balance local and global objectives.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，去中心化引入了经常足以证明额外复杂性的机会。它允许在没有集中监管的情况下进行深度个性化，支持在断开连接或带宽受限的环境中稳健学习，并降低模型更新的运营成本和基础设施负担。实现这些好处引发了如何有效协调设备间学习的问题，无论是通过定期同步、联邦聚合还是平衡本地和全局目标的混合方法。
- en: 'The move from centralized to decentralized learning represents more than a
    shift in deployment architecture. It reshapes the entire design space for machine
    learning systems, requiring new approaches to model architecture, training algorithms,
    data management, and system validation. In centralized training, data is aggregated
    from many sources and processed in large-scale data centers, where models are
    trained, validated, and then deployed in a static form to edge devices. In contrast,
    on-device learning introduces a fundamentally different paradigm: models are updated
    directly on client devices using local data, often asynchronously and under diverse
    hardware conditions. This architectural transformation introduces coordination
    challenges while enabling autonomous local adaptation, requiring careful consideration
    of validation, system reliability, and update orchestration across heterogeneous
    device populations.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从集中式学习到分布式学习的转变不仅仅代表了部署架构的转移。它重塑了机器学习系统的整个设计空间，需要新的方法来处理模型架构、训练算法、数据管理和系统验证。在集中式训练中，数据从许多来源汇总并在大型数据中心进行处理，在那里模型被训练、验证，然后以静态形式部署到边缘设备。相比之下，设备学习引入了一种根本不同的范式：模型直接在客户端设备上使用本地数据更新，通常是非同步的，并在不同的硬件条件下进行。这种架构转型引入了协调挑战，同时实现了自主的本地适应，需要仔细考虑验证、系统可靠性和跨异构设备群体的更新编排。
- en: On-device learning responds to the limitations of centralized machine learning
    workflows. The transformation from centralized to decentralized learning creates
    three distinct operational phases, each with different characteristics and challenges.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设备学习是对集中式机器学习工作流程局限性的回应。从集中式到分布式学习的转变创造了三个不同的操作阶段，每个阶段都有不同的特征和挑战。
- en: The traditional centralized paradigm begins with cloud-based training on aggregated
    data, followed by static model deployment to client devices. This approach works
    well when data collection is feasible, network connectivity is reliable, and a
    single global model can serve all users effectively. However, it breaks down when
    data becomes personalized, privacy-sensitive, or collected in environments with
    limited connectivity.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的集中式范式从基于云的聚合数据训练开始，然后是静态模型部署到客户端设备。当数据收集可行、网络连接可靠且单个全局模型可以有效地为所有用户服务时，这种方法效果良好。然而，当数据变得个性化、对隐私敏感或在有限连接的环境中收集时，它就会崩溃。
- en: Once deployed, local differences begin to emerge as each device encounters its
    own unique data distribution. Devices collect data that reflects individual user
    patterns, environmental conditions, and usage contexts. This data is often non-IID
    (non-independent and identically distributed)[6](#fn6) and noisy, requiring local
    model adaptation to maintain performance. This transition marks the shift from
    global generalization to local specialization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署，随着每个设备遇到其独特的数据分布，本地差异开始出现。设备收集的数据反映了个人用户模式、环境条件和使用上下文。这些数据通常是非-IID（非独立同分布）[6](#fn6)且噪声大的，需要本地模型适应以保持性能。这一转变标志着从全局泛化到本地专业化的转变。
- en: The final phase introduces federated coordination, where devices periodically
    synchronize their local adaptations through aggregated model updates rather than
    raw data sharing. This enables privacy-preserving global refinement while maintaining
    the benefits of local personalization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个阶段引入了联邦协调，设备通过汇总模型更新而不是原始数据共享来定期同步其本地适应。这既实现了隐私保护的全局优化，又保持了本地个性化的好处。
- en: These three distinct phases (centralized training, local adaptation, and federated
    coordination) represent an architectural evolution that reshapes every aspect
    of the machine learning lifecycle. [Figure 14.3](ch020.xhtml#fig-centralized-vs-decentralized)
    illustrates how data flow, computational distribution, and coordination mechanisms
    differ across these phases, highlighting the increasing complexity but also the
    enhanced capabilities that emerge at each stage. Understanding this progression
    helps frame the challenges that on-device learning systems must address.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个不同的阶段（集中式训练、本地适应和联邦协调）代表了架构的演变，重塑了机器学习生命周期的各个方面。[图14.3](ch020.xhtml#fig-centralized-vs-decentralized)展示了在这些阶段中数据流、计算分布和协调机制的不同，突出了每个阶段出现的日益复杂的复杂性以及增强的能力。理解这一进展有助于界定设备学习系统必须解决的挑战。
- en: '![](../media/file222.svg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file222.svg)'
- en: 'Figure 14.3: The evolution from centralized cloud training (region A) through
    local device adaptation (region B) to federated coordination (region C) represents
    a fundamental shift in machine learning architecture. Each phase introduces distinct
    operational characteristics, from uniform global models to personalized local
    adaptations to coordinated distributed learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：从集中式云训练（区域A）通过本地设备适应（区域B）到联邦协调（区域C）的演变代表了机器学习架构的根本性转变。每个阶段都引入了独特的操作特性，从统一的全球模型到个性化的本地适应，再到协调的分布式学习。
- en: Design Constraints
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计约束
- en: 'Part III established efficiency principles that shape all machine learning
    systems. [Chapter 9](ch015.xhtml#sec-efficient-ai) introduced three efficiency
    dimensions (algorithmic, compute, and data efficiency) and revealed through scaling
    laws why brute-force approaches hit fundamental limits. [Chapter 10](ch016.xhtml#sec-model-optimizations)
    developed compression techniques including quantization, pruning, and knowledge
    distillation that enable deployment on resource-constrained devices. [Chapter 11](ch017.xhtml#sec-ai-acceleration)
    characterized edge hardware capabilities from microcontrollers to mobile accelerators,
    as detailed in the hardware discussions. These chapters focused primarily on inference
    workloads: running pre-trained models efficiently.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分确立了塑造所有机器学习系统的效率原则。[第9章](ch015.xhtml#sec-efficient-ai)介绍了三个效率维度（算法效率、计算效率和数据效率），并通过扩展定律揭示了为什么蛮力方法会遇到基本限制。[第10章](ch016.xhtml#sec-model-optimizations)开发了包括量化、剪枝和知识蒸馏在内的压缩技术，这些技术使得在资源受限的设备上部署成为可能。[第11章](ch017.xhtml#sec-ai-acceleration)描述了从微控制器到移动加速器的边缘硬件能力，如硬件讨论中详细所述。这些章节主要关注推理工作负载：高效地运行预训练模型。
- en: On-device learning operates under these same efficiency constraints but with
    training-specific amplifications that make optimization dramatically more challenging.
    Where inference requires a single forward pass through the network, training demands
    forward propagation, gradient computation through backpropagation, and weight
    updates, increasing memory requirements by 3-5<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> and computational
    costs by 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>.
    The model compression techniques that enable efficient inference become baseline
    requirements rather than optimizations, as training within edge device constraints
    would be impossible without aggressive compression.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上学习遵循相同的效率约束，但具有针对训练特定的放大，这使得优化变得更加具有挑战性。在推理阶段，网络只需要单次正向传递，而在训练阶段，则需要正向传播、通过反向传播进行梯度计算和权重更新，这增加了3-5<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的内存需求以及2-3<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的计算成本。能够实现高效推理的模型压缩技术，从优化变成了基本要求，因为没有激进的压缩，在边缘设备上的训练将是不可能的。
- en: Given the established motivations for on-device learning, we now examine the
    fundamental engineering challenges that shape its implementation. Enabling learning
    on the device requires completely rethinking conventional assumptions about where
    and how machine learning systems operate. In centralized environments, models
    are trained with access to extensive compute infrastructure, large and curated
    datasets, and generous memory and energy budgets. At the edge, none of these assumptions
    hold, creating a fundamentally different design space.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到设备上学习的既定动机，我们现在来探讨塑造其实施的根本性工程挑战。在设备上实现学习需要完全重新思考关于机器学习系统运行位置和方式的传统假设。在集中式环境中，模型是在访问广泛的计算基础设施、大量精选的数据集以及慷慨的内存和能源预算的情况下进行训练的。在边缘，这些假设都不成立，从而创造了一个根本不同的设计空间。
- en: 'On-device learning constraints fall into three critical dimensions that parallel
    but extend the efficiency framework from Part III: model compression requirements
    (extending algorithmic efficiency), sparse and non-uniform data characteristics
    (extending data efficiency), and severely limited computational resources (extending
    compute efficiency). These three dimensions form an interconnected constraint
    space that defines the feasible region for on-device learning systems, with each
    dimension imposing distinct limitations that influence algorithmic choices, system
    architecture, and deployment strategies.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习约束分为三个关键维度，这些维度与第三部分中的效率框架平行但扩展：模型压缩需求（扩展算法效率）、稀疏和非均匀数据特征（扩展数据效率），以及严重受限的计算资源（扩展计算效率）。这三个维度形成一个相互关联的约束空间，定义了设备上学习系统的可行区域，每个维度都施加独特的限制，影响算法选择、系统架构和部署策略。
- en: Quantifying Training Overhead on Edge Devices
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘设备上训练开销的量化
- en: The transition from inference-only deployment to on-device training creates
    multiplicative rather than additive complexity. These constraints interact and
    amplify each other in ways that reshape system design requirements, building on
    the resource optimization principles from [Chapter 9](ch015.xhtml#sec-efficient-ai)
    while introducing new challenges specific to distributed learning environments.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从仅推理部署到设备上训练的转变，产生了乘法而非加法复杂性。这些约束相互作用并相互放大，以重塑系统设计要求，基于第9章的资源优化原则，同时引入了特定于分布式学习环境的新挑战。
- en: The efficiency constraints introduced in Part III apply to both inference and
    training, but training amplifies each constraint dimension by 3-10<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>. [Table 14.1](ch020.xhtml#tbl-training-amplification)
    quantifies how training workloads intensify the challenges established in [Chapter 9](ch015.xhtml#sec-efficient-ai),
    [Chapter 10](ch016.xhtml#sec-model-optimizations), and [Chapter 11](ch017.xhtml#sec-ai-acceleration).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分引入的效率约束适用于推理和训练，但训练将每个约束维度放大了3-10倍。[表14.1](ch020.xhtml#tbl-training-amplification)量化了训练工作负载如何加剧第9章（ch015.xhtml#sec-efficient-ai）、第10章（ch016.xhtml#sec-model-optimizations）和第11章（ch017.xhtml#sec-ai-acceleration）中确立的挑战。
- en: These amplifications explain why simply applying Part III optimization techniques
    to training workloads proves insufficient. Each constraint category shapes on-device
    learning system design, requiring approaches that build on but extend beyond the
    inference-focused methods from earlier chapters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些放大解释了为什么仅仅将第三部分优化技术应用于训练工作负载证明是不够的。每个约束类别都会影响设备上学习系统设计，需要基于但超越早期章节中推理方法的方法。
- en: 'Table 14.1: **Training Amplifies Inference Constraints**: On-device learning
    operates under the same efficiency constraints as inference (Part III) but with
    training-specific amplifications that make optimization dramatically more challenging.
    This table quantifies how each constraint dimension intensifies when transitioning
    from running pre-trained models to adapting them locally. Amplification factors
    assume standard backpropagation without optimizations like gradient checkpointing.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.1：**训练放大推理约束**：设备上学习在相同的效率约束下运行，但具有特定于训练的放大，这使得优化变得极具挑战性。本表量化了从运行预训练模型到本地调整模型时，每个约束维度如何加剧。放大系数假设标准反向传播，不包含梯度检查点等优化。
- en: '| **Constraint Dimension** | **Inference (Part III)** | **Training Amplification**
    | **Impact on Design** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **约束维度** | **推理（第三部分）** | **训练放大** | **对设计的影响** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Memory Footprint** | Model weights + single activation map | Weights +
    full activation cache + gradients + optimizer state | 3-5<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> increase; forces
    aggressive compression |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **内存占用** | 模型权重 + 单个激活图 | 权重 + 完整激活缓存 + 梯度 + 优化器状态 | 增加3-5倍；迫使进行激进压缩 |'
- en: '| **Compute Operations** | Forward pass only | Forward + backward + weight
    update | 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    increase; limits model complexity |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **计算操作** | 仅前向传递 | 前向 + 反向 + 权重更新 | 增加2-3倍；限制模型复杂性 |'
- en: '| **Memory Bandwidth** | Sequential weight reads | Bidirectional data flow
    for gradients | 5-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    increase; creates bottlenecks |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **内存带宽** | 顺序权重读取 | 梯度双向数据流 | 增加5-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>；造成瓶颈
    |'
- en: '| **Energy per Sample** | Single inference operation | Multiple gradient steps
    with convergence | 10-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    increase; requires opportunistic scheduling |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **每样本能耗** | 单个推理操作 | 多个梯度步骤直至收敛 | 增加10-50<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>；需要机会性调度 |'
- en: '| **Data Requirements** | Pre-collected, curated datasets | Sparse, noisy,
    streaming local data | Necessitates sample-efficient methods |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **数据需求** | 预收集、整理的数据集 | 稀疏、噪声、流式本地数据 | 需要样本高效的方法 |'
- en: '| **Hardware Utilization** | Optimized for forward passes | Different access
    patterns for backprop | Inference accelerators may not help training |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **硬件利用率** | 优化正向传播 | 反向传播的不同访问模式 | 推理加速器可能对训练无帮助 |'
- en: '[Figure 14.4](ch020.xhtml#fig-ondevice-pretraining) illustrates a pipeline
    that combines offline pre-training with online adaptive learning on resource-constrained
    IoT devices. The system first undergoes meta-training with generic data. During
    deployment, device-specific constraints such as data availability, compute, and
    memory shape the adaptation strategy by ranking and selecting layers and channels
    to update. This allows efficient on-device learning within limited resource envelopes.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.4](ch020.xhtml#fig-ondevice-pretraining)展示了将离线预训练与资源受限的物联网设备上的在线自适应学习相结合的管道。系统首先使用通用数据进行元训练。在部署期间，设备特定的约束（如数据可用性、计算和内存）通过排名和选择要更新的层和通道来塑造适应策略。这允许在有限的资源范围内进行高效的设备学习。'
- en: '![](../media/file223.svg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file223.svg)'
- en: 'Figure 14.4: Resource-constrained devices use a two-stage learning process:
    offline pre-training establishes initial model weights, followed by online adaptation
    that selectively updates layers based on available data, compute, and memory.
    This approach balances model performance with the practical limitations of edge
    deployment, enabling continuous learning in real-world environments.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：资源受限的设备使用两阶段学习过程：离线预训练建立初始模型权重，随后是在线自适应，根据可用的数据、计算和内存选择性地更新层。这种方法在模型性能与边缘部署的实际限制之间取得平衡，使现实世界环境中的持续学习成为可能。
- en: Model Constraints
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型约束
- en: The first dimension of on-device learning constraints centers on the model itself.
    Its structure, size, and computational requirements determine deployment feasibility.
    The structure and size of the machine learning model directly influence whether
    on-device training is even possible, let alone practical. Unlike cloud-deployed
    models that can span billions of parameters and rely on multi-gigabyte memory
    budgets, models intended for on-device learning must conform to tight constraints
    on memory, storage, and computational complexity. These constraints apply not
    only at inference time, but become even more restrictive during training, where
    additional resources are needed for gradient computation, parameter updates, and
    optimizer state management.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 设备学习约束的第一维度集中在模型本身。其结构、大小和计算需求决定了部署的可行性。机器学习模型的结构和大小直接影响设备上训练是否可行，更不用说是否实用。与可以跨越数十亿参数并依赖于多吉字节内存预算的云部署模型不同，旨在设备上学习的模型必须符合对内存、存储和计算复杂性的严格限制。这些限制不仅适用于推理时间，在训练期间变得更加严格，因为需要额外的资源来进行梯度计算、参数更新和优化器状态管理。
- en: The scale of these constraints becomes apparent when examining specific examples
    across the device spectrum. The MobileNetV2 architecture, commonly used in mobile
    vision tasks, requires approximately 14 MB of storage in its standard configuration.
    While this memory requirement is entirely feasible for modern smartphones with
    gigabytes of available RAM, it far exceeds the memory available on embedded microcontrollers
    such as the Arduino Nano 33 BLE Sense[7](#fn7), which provides only 256 KB of
    SRAM and 1 MB of flash storage. This dramatic difference in available resources
    necessitates aggressive model compression techniques. In such severely constrained
    platforms, even a single layer of a typical convolutional neural network may exceed
    available RAM during training due to the need to store intermediate feature maps
    and gradient information.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查设备谱系中的具体示例时，这些约束的范围变得明显。MobileNetV2 架构，常用于移动视觉任务，在其标准配置下需要大约 14 MB 的存储空间。虽然这种内存需求对于具有数GB可用RAM的现代智能手机来说是完全可行的，但它远远超过了Arduino
    Nano 33 BLE Sense[7](#fn7)等嵌入式微控制器上可用的内存，后者仅提供256 KB的SRAM和1 MB的闪存存储。这种可用资源的巨大差异需要采取激进的模型压缩技术。在这样的严重受限平台上，即使是典型的卷积神经网络的单层，在训练过程中也可能因为需要存储中间特征图和梯度信息而超出可用的RAM。
- en: Beyond static storage requirements, the training process itself dramatically
    expands the effective memory footprint, creating an additional layer of constraint.
    Standard backpropagation requires caching activations for each layer during the
    forward pass, which are then reused during gradient computation in the backward
    pass. As established in the amplification analysis above, this activation caching
    multiplies memory requirements compared to inference-only deployment. For a seemingly
    modest 10-layer convolutional model processing <semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">64 \times 64</annotation></semantics> images, the
    required memory may exceed 1 to 2 MB, well beyond the SRAM capacity of most embedded
    systems and highlighting the fundamental tension between model expressiveness
    and resource availability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了静态存储需求之外，训练过程本身也会显著扩大有效内存占用，从而增加了一层额外的约束。标准的反向传播需要在正向传递期间缓存每一层的激活，然后在反向传递的梯度计算中重新使用这些激活。如上所述的放大分析所确立的，这种激活缓存将内存需求与仅推理部署相比增加了。对于一个看似朴素的10层卷积模型处理<semantics><mrow><mn>64</mn><mo>×</mo><mn>64</mn></mrow><annotation
    encoding="application/x-tex">64 \times 64</annotation></semantics>图像，所需的内存可能超过1到2
    MB，远远超出大多数嵌入式系统的SRAM容量，突显了模型表达能力和资源可用性之间的基本矛盾。
- en: Compounding these memory constraints, model complexity directly affects runtime
    energy consumption and thermal limits, introducing additional practical barriers
    to deployment. In systems such as smartwatches or battery-powered wearables, sustained
    model training can rapidly deplete energy reserves or trigger thermal throttling
    that degrades performance. Training a full model using floating-point operations
    on these devices is often infeasible from an energy perspective, even when memory
    constraints are satisfied. These practical limitations have motivated the development
    of ultra-lightweight model variants, such as MLPerf Tiny benchmark networks ([C.
    Banbury et al. 2021](ch058.xhtml#ref-banbury2021mlperf)), which fit within 100–200
    KB and can be adapted using only partial gradient updates. These specialized models
    employ aggressive quantization and pruning strategies to achieve such compact
    representations while maintaining sufficient expressiveness for meaningful adaptation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内存约束的叠加，模型复杂性直接影响运行时能耗和热限，引入了额外的实际部署障碍。在智能手表或电池供电的可穿戴设备等系统中，持续的模型训练会迅速耗尽能源储备或触发性能下降的热管理。在这些设备上使用浮点运算进行完整模型的训练，从能源角度来看通常是不切实际的，即使内存约束得到满足。这些实际限制促使开发了超轻量级模型变体，如MLPerf
    Tiny基准网络([C. Banbury等. 2021](ch058.xhtml#ref-banbury2021mlperf))，这些网络的大小在100-200
    KB之间，并且可以通过仅使用部分梯度更新来适应。这些专用模型采用激进的量化和剪枝策略，在保持足够表达性以进行有意义适应的同时，实现了如此紧凑的表示。
- en: 'The practical implications of battery and thermal constraints extend beyond
    just limiting training duration. Mobile devices must carefully balance training
    opportunities with user experience. Aggressive on-device training can cause noticeable
    device heating and rapid battery drain, leading to user dissatisfaction and potential
    app uninstalls. Modern smartphones typically limit sustained processing to 2-3 W
    for ML workloads to prevent thermal discomfort, though they can burst to 5-10 W
    for brief periods before thermal throttling kicks in. Training even modest models
    can easily exceed these sustainable power limits. This reality necessitates intelligent
    scheduling strategies: training during charging periods when thermal dissipation
    is improved, utilizing low-power cores for gradient computation when possible,
    and implementing thermal-aware duty cycling that pauses training when temperature
    thresholds are exceeded. Some systems even leverage device usage patterns, scheduling
    intensive adaptation only during overnight charging when the device is idle and
    connected to power.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 电池和热限制的实用影响不仅限于限制训练时间。移动设备必须仔细平衡训练机会与用户体验。激进的设备端训练可能导致设备明显发热和快速耗电，导致用户不满甚至可能卸载应用程序。现代智能手机通常将
    ML 工作负载的持续处理功率限制在 2-3 W，以防止热不适，尽管在热限制启动之前，它们可以短暂地达到 5-10 W。即使是训练简单的模型也可能轻易超过这些可持续的功率限制。这一现实需要智能的调度策略：在充电期间进行训练，以改善热散失，在可能的情况下使用低功耗核心进行梯度计算，并在温度阈值超过时实施热感知的轮换，暂停训练。一些系统甚至利用设备使用模式，仅在设备空闲且连接电源的夜间充电期间进行密集的适应调度。
- en: Given these multifaceted constraints, the model architecture itself must be
    fundamentally designed with on-device learning capabilities in mind from the outset.
    Many conventional architectures, such as large transformers or deep convolutional
    networks, are simply not viable for on-device adaptation due to their inherent
    size and computational complexity. Instead, specialized lightweight architectures
    such as MobileNets[8](#fn8), SqueezeNet ([Iandola et al. 2016](ch058.xhtml#ref-iandola2016squeezenet)),
    and EfficientNet ([Tan and Le 2019a](ch058.xhtml#ref-tan2019efficientnet)) have
    been developed specifically for resource-constrained environments. These architectures
    leverage efficiency principles and architectural optimizations, rethinking how
    neural networks can be structured. These specialized models employ techniques
    such as depthwise separable convolutions[9](#fn9), bottleneck layers, and aggressive
    quantization to dramatically reduce memory and compute requirements while maintaining
    sufficient performance for practical applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些多方面的限制，模型架构本身必须从一开始就从根本上设计，以考虑设备端学习的能力。许多传统的架构，如大型转换器或深度卷积网络，由于它们固有的规模和计算复杂性，在设备端适应方面根本不可行。相反，专门开发了一些轻量级架构，如MobileNets[8](#fn8)、SqueezeNet
    ([Iandola et al. 2016](ch058.xhtml#ref-iandola2016squeezenet)) 和 EfficientNet
    ([Tan and Le 2019a](ch058.xhtml#ref-tan2019efficientnet))，这些架构专门针对资源受限的环境。这些架构利用效率原则和架构优化，重新思考了神经网络的结构。这些专用模型采用深度可分离卷积[9](#fn9)、瓶颈层和激进的量化等技术，在显著降低内存和计算需求的同时，保持了足够的应用性能。
- en: These architectures are often designed to be modular, allowing for easy adaptation
    and fine-tuning. For example, MobileNets ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets))
    can be configured with different width multipliers and resolution settings to
    balance performance and resource usage. Concretely, MobileNetV2 with α=1.0 requires
    3.4 M parameters (13.6 MB in FP32), but with α=0.5 this drops to 0.7 M parameters
    (2.8 MB), enabling deployment on devices with just 4 MB available RAM. This flexibility
    is important for on-device learning, where the model must adapt to the specific
    constraints of the deployment environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构通常设计为模块化，便于适应和微调。例如，MobileNets ([A. G. Howard et al. 2017](ch058.xhtml#ref-howard2017mobilenets))
    可以配置不同的宽度乘数和分辨率设置，以平衡性能和资源使用。具体来说，α=1.0 的 MobileNetV2 需要 3.4 M 个参数（FP32 格式下为 13.6 MB），但
    α=0.5 时参数减少到 0.7 M（2.8 MB），使得设备仅有的 4 MB 可用 RAM 也能部署。这种灵活性对于设备端学习非常重要，因为模型必须适应部署环境的特定限制。
- en: While model architecture determines the memory and computational baseline for
    on-device learning, the characteristics of available training data introduce equally
    fundamental limitations that shape every aspect of the learning process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型架构决定了设备上学习的内存和计算基线，但可用训练数据的特征引入了同样根本的限制，这些限制塑造了学习过程的各个方面。
- en: Data Constraints
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据限制
- en: The second dimension of on-device learning constraints centers on data availability
    and quality. The nature of data available to on-device ML systems differs dramatically
    from the large, curated, and centrally managed datasets used in cloud-based training.
    At the edge, data is locally collected, temporally sparse, and often unstructured
    or unlabeled, creating a different learning environment. These characteristics
    introduce multifaceted challenges in volume, quality, and statistical distribution,
    all of which directly affect the reliability and generalizability of learning
    on the device.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习限制的第二个维度集中在数据可用性和质量上。设备上机器学习系统可用的数据性质与在云训练中使用的庞大、精心策划和集中管理的数据集大相径庭。在边缘，数据是本地收集的，时间上稀疏的，通常是非结构化或未标记的，创造了一个不同的学习环境。这些特征在数量、质量和统计分布方面引入了多方面的挑战，所有这些都直接影响设备上学习的可靠性和泛化能力。
- en: Data volume represents the first major constraint, severely limited by both
    storage constraints and the sporadic nature of user interaction. For example,
    a smart fitness tracker may collect motion data only during physical activity,
    generating relatively few labeled samples per day. If a user wears the device
    for just 30 minutes of exercise, only a few hundred data points might be available
    for training, compared to the thousands or millions typically required for effective
    supervised learning in controlled environments. This scarcity changes the learning
    paradigm from data-rich to data-efficient algorithms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量代表第一个主要限制因素，受到存储限制和用户交互的偶然性质的双重限制。例如，一个智能健身追踪器可能仅在身体活动期间收集运动数据，每天产生的标记样本相对较少。如果用户仅佩戴设备进行30分钟的锻炼，可能只有几百个数据点可用于训练，与在受控环境中进行有效的监督学习通常所需的数千或数百万数据点相比。这种稀缺性改变了学习范式，从数据丰富的算法转变为数据高效的算法。
- en: 'Beyond volume limitations, on-device data is frequently non-IID (non-independent
    and identically distributed) ([Y. Zhao et al. 2018](ch058.xhtml#ref-zhao2018federated)),
    creating statistical challenges that cloud-based systems rarely encounter. This
    heterogeneity manifests across multiple dimensions: user behavior patterns, environmental
    conditions, linguistic preferences, and usage contexts. A voice assistant deployed
    across households encounters dramatic variation in accents, languages, speaking
    styles, and command patterns. Similarly, smartphone keyboards adapt to individual
    typing patterns, autocorrect preferences, and multilingual usage that varies dramatically
    between users. This data heterogeneity complicates both model convergence and
    the design of update mechanisms that must generalize across devices while maintaining
    personalization.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了体积限制之外，设备上的数据通常是非独立同分布的（non-IID）([Y. Zhao et al. 2018](ch058.xhtml#ref-zhao2018federated))，这为云系统很少遇到的统计挑战。这种异质性在多个维度上表现出来：用户行为模式、环境条件、语言偏好和使用上下文。在多个家庭中部署的语音助手会遇到口音、语言、说话方式和命令模式的大幅变化。同样，智能手机键盘会适应个人的打字模式、自动纠错偏好和多语言使用，这些在用户之间差异很大。这种数据异质性既复杂了模型的收敛，也复杂了必须跨设备泛化同时保持个性化的更新机制的设计。
- en: Compounding these distribution challenges, label scarcity presents an additional
    critical obstacle that severely limits traditional learning approaches. Most edge-collected
    data is unlabeled by default, requiring systems to learn from weak or implicit
    supervision signals. In a smartphone camera, for instance, the device may capture
    thousands of images throughout the day, but only a few are associated with meaningful
    user actions (e.g., tagging, favoriting, or sharing), which could serve as implicit
    labels. In many applications, including detecting anomalies in sensor data and
    adapting gesture recognition models, explicit labels may be entirely unavailable,
    making traditional supervised learning infeasible without developing alternative
    methods for weak supervision or unsupervised adaptation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分布挑战的叠加，标签稀缺又提出了一个额外的关键障碍，严重限制了传统的学习方法。大多数边缘收集的数据默认情况下是无标签的，需要系统从弱或隐式监督信号中学习。例如，在智能手机摄像头中，设备可能一天中捕获成千上万张图片，但只有少数与有意义的用户行为（例如，标记、收藏或分享）相关，这些可以作为隐式标签。在许多应用中，包括检测传感器数据中的异常和调整手势识别模型，可能完全无法获得显式标签，这使得在没有开发弱监督或无监督适应的替代方法的情况下，传统的监督学习变得不可行。
- en: Data quality issues add another layer of complexity to the on-device learning
    challenge. Noise and variability further degrade the already limited data available
    for training. Embedded systems such as environmental sensors or automotive ECUs
    may experience fluctuations in sensor calibration, environmental interference,
    or mechanical wear, leading to corrupted or drifting input signals over time.
    Without centralized validation systems to detect and filter these errors, they
    may silently degrade learning performance, creating a reliability challenge that
    cloud-based systems can more easily address through data preprocessing pipelines.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量问题给设备上的学习挑战增加了另一层复杂性。噪声和可变性进一步降低了用于训练的有限数据。嵌入式系统，如环境传感器或汽车ECU，可能会经历传感器校准、环境干扰或机械磨损的波动，导致随时间推移输入信号损坏或漂移。没有集中式验证系统来检测和过滤这些错误，它们可能会默默地降低学习性能，创造一个可靠性挑战，云系统可以通过数据预处理管道更容易地解决。
- en: Finally, data privacy and security concerns impose the most restrictive constraints
    of all, often making data sharing architecturally impossible rather than merely
    undesirable. Sensitive information, such as health data, personal communications,
    or user behavioral patterns, must be protected from unauthorized access under
    legal and ethical requirements. This constraint often completely precludes the
    use of traditional data-sharing methods, such as uploading raw data to a central
    server for training. Instead, on-device learning must rely on sophisticated techniques
    that enable local adaptation without ever exposing sensitive information, changing
    how learning systems can be designed and validated.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据隐私和安全问题对所有约束施加了最严格的限制，通常使得数据共享在架构上变得不可能，而不仅仅是不可取。敏感信息，如健康数据、个人通讯或用户行为模式，必须在法律和伦理要求下受到保护，防止未经授权的访问。这种约束通常完全排除了使用传统数据共享方法，例如将原始数据上传到中央服务器进行训练。相反，设备上的学习必须依赖于复杂的技巧，这些技巧能够实现本地适应，而不会暴露任何敏感信息，从而改变了学习系统的设计和验证方式。
- en: Compute Constraints
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算约束
- en: '[Chapter 11](ch017.xhtml#sec-ai-acceleration) characterized the edge hardware
    landscape that provides computational substrate for machine learning: microcontrollers
    like STM32F4 and ESP32 at the most constrained end, mobile-class processors with
    dedicated AI accelerators (Apple Neural Engine, Qualcomm Hexagon, Google Tensor)
    in the middle, and high-capability edge devices at the upper end. That chapter
    focused on inference capabilities—the computational throughput, memory bandwidth,
    and energy efficiency achievable when executing pre-trained models.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[第11章](ch017.xhtml#sec-ai-acceleration)描述了为机器学习提供计算基础的边缘硬件景观：在最受限的端点是微控制器如STM32F4和ESP32，中间是具有专用AI加速器（苹果神经网络引擎、高通Hexagon、谷歌Tensor）的移动级处理器，在高端是高能力边缘设备。该章节重点介绍了推理能力——在执行预训练模型时可以达到的计算吞吐量、内存带宽和能效。'
- en: Training workloads exhibit fundamentally different computational characteristics
    that reshape hardware utilization patterns. Building on the edge hardware landscape
    characterized in [Chapter 11](ch017.xhtml#sec-ai-acceleration), from microcontrollers
    to mobile AI accelerators, on-device learning must operate within severely constrained
    computational envelopes that differ dramatically from cloud-based training infrastructure
    by factors of hundreds or thousands in raw computational capacity.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作负载展现出根本不同的计算特性，这改变了硬件利用模式。基于第11章中描述的边缘硬件景观，从微控制器到移动AI加速器，设备上的学习必须在严重受限的计算范围内运行，这与基于云的训练基础设施在原始计算能力上相差数百或数千倍。
- en: 'The key difference: backpropagation requires significantly higher memory bandwidth
    than inference due to gradient computation and activation caching, weight updates
    create write-heavy access patterns unlike inference’s read-only operations, and
    optimizer state management demands additional memory allocation that inference
    never encounters. These training-specific demands mean hardware perfectly adequate
    for inference may prove entirely inadequate for adaptation, even when updating
    only a small parameter subset.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 关键区别在于：反向传播由于梯度计算和激活缓存需要显著更高的内存带宽，权重更新会创建不同于推理的只读操作的写密集型访问模式，而优化器状态管理需要额外的内存分配，这是推理从未遇到的。这些特定的训练需求意味着对于推理完全足够的硬件可能证明在适应方面完全不足，即使只是更新一个小参数子集。
- en: At the most constrained end of the spectrum, devices such as the STM32F4[10](#fn10)
    or ESP32[11](#fn11) microcontrollers offer only a few hundred kilobytes of SRAM
    and completely lack hardware support for floating-point operations ([Warden and
    Situnayake 2020](ch058.xhtml#ref-lai2020tinyml)). These extreme constraints represent
    the fundamental limitations of edge hardware ([Chapter 11](ch017.xhtml#sec-ai-acceleration)).
    Such severe limitations preclude the use of conventional deep learning libraries
    and require models to be meticulously designed for integer arithmetic and minimal
    runtime memory allocation. In these environments, even apparently simple models
    require highly specialized techniques, including quantization-aware training[12](#fn12)
    and selective parameter updates, to execute training loops without exceeding memory
    or power budgets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在最受限的一端，STM32F4[10](#fn10) 或 ESP32[11](#fn11) 等微控制器仅提供几百千字节的SRAM，并且完全缺乏对浮点操作的硬件支持
    ([Warden and Situnayake 2020](ch058.xhtml#ref-lai2020tinyml))。这些极端限制代表了边缘硬件的基本限制
    ([Chapter 11](ch017.xhtml#sec-ai-acceleration))。如此严重的限制排除了使用传统的深度学习库，并要求模型必须精心设计以适应整数算术和最小的运行时内存分配。在这些环境中，即使是看似简单的模型也需要高度专业化的技术，包括量化感知训练[12](#fn12)
    和选择性参数更新，以在不超过内存或功耗预算的情况下执行训练循环。
- en: 'The practical implications are stark: while the STM32F4 microcontroller can
    run a simple linear regression model with a few hundred parameters, training even
    a small convolutional neural network would immediately exceed its memory capacity.
    In these severely constrained environments, training is often limited to simple
    algorithms such as stochastic gradient descent (SGD)[13](#fn13) or <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>-means clustering, which
    can be implemented using integer arithmetic and minimal memory overhead, representing
    a fundamental departure from modern machine learning practice.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实际影响是明显的：虽然STM32F4微控制器可以运行一个带有几百个参数的简单线性回归模型，但训练甚至一个小的卷积神经网络会立即超出其内存容量。在这些严重受限的环境中，训练通常仅限于简单的算法，如随机梯度下降（SGD）[13](#fn13)
    或 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>-means聚类，这些算法可以使用整数算术和最小的内存开销实现，这代表了与现代机器学习实践的根本性转变。
- en: Moving up the computational hierarchy, mobile-class hardware represents a significant
    improvement but still operates under substantial constraints. Platforms including
    the Qualcomm Snapdragon, Apple Neural Engine[14](#fn14), and Google Tensor SoC[15](#fn15)
    provide significantly more compute power than microcontrollers, often featuring
    dedicated AI accelerators and optimized support for 8-bit or mixed-precision[16](#fn16)
    matrix operations. These accelerators, their capabilities, and their programming
    models are detailed in [Chapter 11](ch017.xhtml#sec-ai-acceleration). While these
    platforms can support more sophisticated training routines, including full backpropagation
    over compact models, they still fall dramatically short of the computational throughput
    and memory bandwidth available in centralized data centers. For instance, training
    a lightweight transformer[17](#fn17) on a smartphone is technically feasible but
    must be tightly bounded in both time and energy consumption to avoid degrading
    the user experience, highlighting the persistent tension between learning capabilities
    and practical deployment constraints.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算层次结构中向上移动，移动级硬件代表了一个显著的改进，但仍然受到大量约束。包括高通骁龙、苹果神经网络引擎[14](#fn14)和谷歌Tensor SoC[15](#fn15)在内的平台，比微控制器提供了显著更多的计算能力，通常具有专门的AI加速器和针对8位或混合精度[16](#fn16)矩阵操作的优化支持。这些加速器、它们的性能和它们的编程模型在[第11章](ch017.xhtml#sec-ai-acceleration)中详细说明。虽然这些平台可以支持更复杂的训练程序，包括对紧凑模型的完整反向传播，但它们在计算吞吐量和内存带宽方面仍然远远落后于集中式数据中心。例如，在智能手机上训练轻量级转换器[17](#fn17)在技术上可行，但必须在时间和能耗上严格限制，以避免降低用户体验，突显了学习能力和实际部署约束之间的持续紧张关系。
- en: These computational limitations become especially acute in real-time or battery-operated
    systems, as demonstrated in camera processing requirements, where specific latency
    budgets create hard architectural constraints. Camera applications processing
    at 30 FPS cannot exceed 33 ms per frame, voice interfaces require rapid response
    times for natural interaction, AR/VR systems demand sub-20 ms motion-to-photon
    latency to prevent user discomfort, and safety-critical control systems must respond
    within 10 ms to ensure operational safety. These quantitative constraints determine
    whether on-device learning is feasible or whether cloud-based alternatives become
    architecturally necessary. In a smartphone-based speech recognizer, on-device
    adaptation must seamlessly coexist with primary inference workloads without interfering
    with response latency or system responsiveness. Similarly, in wearable medical
    monitors, training must occur opportunistically during carefully managed windows—typically
    during periods of low activity or charging—to preserve battery life and avoid
    thermal management issues.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算限制在实时或电池供电系统中尤为突出，正如在相机处理需求中所展示的，特定的延迟预算创造了硬性架构约束。以30帧每秒（FPS）处理的相机应用，每帧不能超过33毫秒（ms），语音界面需要快速响应时间以实现自然交互，增强现实/虚拟现实（AR/VR）系统需要低于20毫秒的运动到光子（motion-to-photon）延迟以避免用户不适，而安全关键的控制系统必须在10毫秒内响应以确保操作安全。这些定量约束决定了设备上学习是否可行，或者基于云的替代方案在架构上是否必要。在基于智能手机的语音识别器中，设备上的自适应必须与主要推理工作负载无缝共存，而不干扰响应延迟或系统响应性。同样，在可穿戴医疗监测器中，训练必须在精心管理的时间窗口内偶然发生——通常在低活动期或充电期间——以保护电池寿命并避免热管理问题。
- en: 'Beyond raw computational capacity, the architectural implications of these
    hardware constraints extend into fundamental system design choices. Training operations
    exhibit fundamentally different memory access patterns than inference workloads:
    backpropagation requires 3-5<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    higher memory bandwidth due to gradient computation and activation caching, creating
    bottlenecks that pure computational metrics don’t capture. Modern edge accelerators
    attempt to address these challenges through increasingly specialized hardware
    features. Adaptive precision datapaths allow dynamic switching between INT4 for
    forward passes and FP16 for gradient computation, optimizing both accuracy and
    efficiency within power budgets. Sparse computation units accelerate selective
    parameter updates by skipping zero gradients—a capability critical for efficient
    bias-only and LoRA adaptations. Near-memory compute architectures[18](#fn18) reduce
    data movement costs by performing gradient updates directly adjacent to weight
    storage, addressing the memory bandwidth bottleneck. However, most current edge
    accelerators remain fundamentally optimized for inference workloads, creating
    significant hardware-software co-design opportunities for future generations of
    on-device training accelerators specifically designed to handle the unique demands
    of local adaptation.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始的计算能力之外，这些硬件限制在系统设计选择上的影响延伸到了根本性的系统设计。训练操作与推理工作负载在内存访问模式上存在根本性的不同：由于梯度计算和激活缓存，反向传播需要3-5倍更高的内存带宽，这创造了纯计算指标无法捕捉的瓶颈。现代边缘加速器试图通过越来越专业的硬件特性来应对这些挑战。自适应精度数据路径允许在正向传递使用INT4和梯度计算使用FP16之间动态切换，在能源预算内优化准确性和效率。稀疏计算单元通过跳过零梯度来加速选择性参数更新，这对于高效的偏差仅适应和LoRA适应是关键能力。近内存计算架构[18](#fn18)通过在权重存储附近直接执行梯度更新来降低数据移动成本，解决内存带宽瓶颈。然而，大多数当前的边缘加速器仍然在本质上针对推理工作负载进行优化，为未来一代专门设计用于处理本地适应独特需求的设备上训练加速器创造了显著的硬件-软件协同设计机会。
- en: Edge Hardware Integration Challenges
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘硬件集成挑战
- en: 'Beyond the individual constraints of models, data, and computation, on-device
    learning systems must navigate the complex interactions between these elements
    and the underlying physics of mobile computing: power dissipation, thermal limits,
    and energy budgets. These physical constraints are not mere engineering details;
    they are fundamental design drivers that determine the entire feasible space of
    on-device learning algorithms. Understanding these quantitative constraints enables
    informed design decisions that balance learning capabilities with long-term system
    sustainability and user acceptance.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 超越模型、数据和计算的个人限制，设备上的学习系统必须应对这些元素与移动计算底层物理（如功耗、热限制和能源预算）之间的复杂交互。这些物理限制不仅仅是工程细节，它们是基本的设计驱动因素，决定了设备上学习算法的整个可行空间。理解这些定量限制能够使设计决策更加明智，在平衡学习能力和长期系统可持续性以及用户接受度之间取得平衡。
- en: Energy and Thermal Constraint Analysis
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 能源和热限制分析
- en: Energy and thermal management represent perhaps the most challenging aspect
    of on-device learning system design, as they directly impact user experience and
    device longevity. Mobile devices operate under strict power budgets that fundamentally
    determine feasible model complexity and training schedules. The thermal design
    power (TDP) of mobile processors creates hard constraints that shape every aspect
    of on-device learning strategies. Modern smartphones typically maintain sustained
    processing at 2-3 W for ML workloads to prevent thermal discomfort, but can burst
    to 5-10 W for brief periods before thermal throttling dramatically reduces performance
    by 50% or more. This thermal cycling behavior forces training algorithms to operate
    in carefully managed burst modes, utilizing peak performance for only 10-30 seconds
    before backing off to sustainable power levels, a constraint that fundamentally
    changes how training algorithms must be designed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 能量和热管理可能是设备上学习系统设计中最具挑战性的方面，因为它们直接影响用户体验和设备寿命。移动设备在严格的电源预算下运行，这从根本上决定了可行的模型复杂度和训练计划。移动处理器的热设计功耗（TDP）为设备上学习的各个方面创造了硬约束。现代智能手机通常将ML工作负载的持续处理能力维持在2-3 W，以防止热不适，但在热管理显著降低性能50%或更多之前，可以短暂地爆发出5-10 W。这种热循环行为迫使训练算法在精心管理的高峰模式下运行，仅在10-30秒内利用峰值性能，然后降低到可持续的功率水平，这一约束从根本上改变了训练算法的设计方式。
- en: The mobile power budget hierarchy reveals the tight constraints under which
    on-device learning must operate. Smartphone sustained processing is limited to
    2-3 W to prevent user-noticeable heating and maintain acceptable battery life
    throughout the day. Peak training burst mode can reach 10 W, but this power level
    is sustainable for only 10-30 seconds before thermal throttling kicks in to protect
    the hardware. Dedicated neural processing units consume 0.5-2 W for AI workloads,
    offering optimized power efficiency compared to general-purpose processors. CPU-based
    AI processing requires 3-5 W and demands aggressive thermal management with duty
    cycling to prevent overheating, making it the least power-efficient option for
    sustained on-device learning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 移动电源预算层次结构揭示了设备上学习必须运行的严格约束。智能手机的持续处理能力限制在2-3 W，以防止用户察觉到发热并保持一整天的可接受电池寿命。峰值训练突发模式可以达到10 W，但这种功率水平只能持续10-30秒，然后热管理启动以保护硬件。专用神经网络单元用于AI工作负载，消耗0.5-2 W，与通用处理器相比，提供优化的功耗效率。基于CPU的AI处理需要3-5 W，并需要积极的温度管理以及轮换工作以防止过热，使其成为持续设备上学习最不节能的选项。
- en: The power consumption characteristics of training workloads create additional
    layers of constraint that extend beyond simple computational capacity. Power consumption
    scales superlinearly with model size and training complexity, with training operations
    consuming 10-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    more power than equivalent inference workloads due to the substantial computational
    overhead of gradient computation (consuming 40-70% of training power), weight
    updates (20-30%), and dramatically increased data movement between memory hierarchies
    (10-30%). To maintain acceptable user experience, mobile devices typically budget
    only 500-1000 mW for sustained ML training, effectively limiting practical training
    sessions to 10-100 minutes daily under normal usage patterns. This severe power
    constraint fundamentally shifts the design priority from maximizing computational
    throughput to optimizing power efficiency, requiring careful co-optimization of
    algorithms and hardware utilization patterns.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作负载的功耗特性创造了额外的约束层，这些约束层超出了简单的计算能力。功耗随模型大小和训练复杂度的超线性增长，训练操作消耗的功率比等效推理工作负载多10-50<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>，这是由于梯度计算（消耗40-70%的训练功率）、权重更新（20-30%）和内存层次结构之间数据移动的显著增加（10-30%）的大量计算开销。为了保持可接受的用户体验，移动设备通常只为持续ML训练预留500-1000 mW的预算，实际上将实际训练时间限制在每天正常使用模式下的10-100分钟。这种严重的功耗约束从根本上改变了设计优先级，从最大化计算吞吐量转向优化功耗效率，需要仔细协同优化算法和硬件利用模式。
- en: The thermal management challenges extend far beyond simple power limits, creating
    complex dynamic constraints that vary with environmental conditions and usage
    patterns. Training workloads generate localized heat that can trigger protective
    throttling in specific processor cores or accelerator units, often in unpredictable
    ways that depend on ambient temperature and device design. Modern mobile SoCs
    implement sophisticated thermal management systems, including dynamic voltage
    and frequency scaling (DVFS)[19](#fn19), core migration between efficiency and
    performance clusters, and selective shutdown of non-essential processing units.
    Successfully deployed on-device learning systems must intimately integrate with
    these thermal management frameworks, intelligently scheduling training bursts
    during optimal thermal windows and gracefully degrading performance when thermal
    limits are approached, rather than simply failing or causing user-visible performance
    problems.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 热管理挑战远远超出了简单的功率限制，创造了复杂且随环境条件和使用模式变化的动态约束。训练工作负载会产生局部热量，这可能会在特定的处理器核心或加速器单元中触发保护性降频，通常是以不可预测的方式，这取决于环境温度和设备设计。现代移动系统级芯片（SoC）实现了复杂的热管理系统，包括动态电压和频率缩放（DVFS）[19](#fn19)、在效率集群和性能集群之间的核心迁移，以及选择性关闭非关键处理单元。成功部署在设备上的学习系统必须与这些热管理框架紧密集成，智能地安排在最佳热窗口期间进行训练爆发，并在接近热限制时优雅地降低性能，而不是简单地失败或导致用户可见的性能问题。
- en: Memory Hierarchy Optimization
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存层次结构优化
- en: Complementing the thermal and power challenges, memory hierarchy constraints
    create another fundamental bottleneck that shapes on-device learning system design.
    As established in the constraint amplification analysis above, these limitations
    affect both static model storage and the dynamic memory requirements during training,
    often pushing systems beyond their practical limits.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 补充热能和功率挑战，内存层次结构的约束为设备上的学习系统设计创造了另一个基本瓶颈。如上所述的约束放大分析所确立的，这些限制影响静态模型存储和训练期间的动态内存需求，通常会将系统推向其实际极限之外。
- en: The device memory hierarchy spans several orders of magnitude across different
    device classes, each presenting distinct constraints for on-device learning. The
    iPhone 15 Pro provides 8 GB total system memory, but only approximately 2-4 GB
    remains available for application workloads after accounting for operating system
    requirements and background processes. Budget Android devices operate with 4 GB
    total system memory, leaving just 1-2 GB available for ML workloads after OS overhead
    consumes significant resources. IoT embedded systems provide 64 MB-1 GB total
    memory that must be shared between system tasks and application data, creating
    severe constraints for any learning algorithms. Microcontrollers offer only 256 KB-2 MB
    SRAM, requiring extreme optimization and careful memory management that fundamentally
    limits the complexity of models that can adapt on such platforms.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存层次结构跨越不同设备类别，数量级跨度很大，每个类别都为设备上的学习提供了独特的约束。iPhone 15 Pro提供了总共8 GB的系统内存，但在考虑到操作系统需求和后台进程后，只有大约2-4
    GB可用于应用工作负载。预算型安卓设备运行时拥有总共4 GB的系统内存，在操作系统开销消耗了大量资源后，仅剩下1-2 GB可用于机器学习工作负载。物联网嵌入式系统提供64
    MB-1 GB的总内存，这些内存必须在系统任务和应用数据之间共享，为任何学习算法创造了严重的约束。微控制器仅提供256 KB-2 MB的SRAM，这要求极端优化和仔细的内存管理，从根本上限制了可以在这些平台上适应的模型复杂性。
- en: 'The memory expansion during training creates particularly acute challenges
    that often determine system feasibility. Standard backpropagation requires caching
    intermediate activations for each layer during the forward pass, which are then
    reused during gradient computation in the backward pass, creating substantial
    memory overhead. A MobileNetV2 model requiring just 14 MB for inference balloons
    to 50-70 MB during training, often exceeding the available memory budget on many
    mobile devices and making training impossible without aggressive optimization.
    This dramatic expansion necessitates sophisticated model compression techniques
    that must compound multiplicatively: INT8 quantization provides <semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics> memory reduction,
    structured pruning achieves <semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics> parameter reduction,
    and knowledge distillation enables <semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">5\times</annotation></semantics> model size reduction
    while maintaining accuracy within 2-5% of the original model. These techniques
    must be carefully combined to achieve the aggressive compression ratios required
    for practical deployment.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的内存扩展带来了特别严峻的挑战，这些挑战往往决定了系统的可行性。标准的反向传播需要在正向传递过程中为每一层缓存中间激活，然后在反向传递的梯度计算中重新使用，从而产生大量的内存开销。一个仅需要14
    MB进行推理的MobileNetV2模型在训练过程中膨胀到50-70 MB，通常超过了许多移动设备上的可用内存预算，使得没有积极的优化就无法进行训练。这种戏剧性的膨胀需要复杂的模型压缩技术，这些技术必须以乘法方式组合：INT8量化提供<semantics><mrow><mn>4</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">4\times</annotation></semantics>的内存减少，结构化剪枝实现<semantics><mrow><mn>10</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">10\times</annotation></semantics>的参数减少，而知识蒸馏使得模型大小减少<semantics><mrow><mn>5</mn><mo>×</mo></mrow><annotation
    encoding="application/x-tex">5\times</annotation></semantics>，同时保持准确率在原始模型的2-5%以内。这些技术必须谨慎组合，以实现实际部署所需的激进压缩比率。
- en: Given these memory constraints, cache optimization becomes absolutely critical
    for achieving acceptable performance with constrained memory pools. Modern mobile
    SoCs feature complex memory hierarchies with L1 cache (32-64 KB), L2 cache (1-8 MB),
    and system memory (4-16 GB) that exhibit 10-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> latency differences
    between levels, creating severe performance cliffs when working sets exceed cache
    capacity. Training workloads that exceed cache capacity face dramatic performance
    degradation due to memory bandwidth bottlenecks that can slow training by orders
    of magnitude. Successful on-device learning systems must carefully design data
    access patterns to maximize cache hit rates, often requiring specialized memory
    layouts that group related parameters for spatial locality, carefully sized mini-batches
    that fit entirely within cache constraints, and sophisticated gradient accumulation
    strategies that minimize expensive memory bus traffic.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些内存限制下，缓存优化对于在受限的内存池中实现可接受的性能变得绝对关键。现代移动SoC具有复杂的内存层次结构，包括L1缓存（32-64 KB）、L2缓存（1-8
    MB）和系统内存（4-16 GB），不同层级之间表现出10-100倍的延迟差异，当工作集超过缓存容量时，会形成严重的性能悬崖。超出缓存容量的训练工作负载由于内存带宽瓶颈而面临显著的性能下降，这可能导致训练速度降低几个数量级。成功的设备端学习系统必须精心设计数据访问模式以最大化缓存命中率，通常需要专门的内存布局来分组相关参数以实现空间局部性，精心设计的迷你批次以完全符合缓存限制，以及复杂的梯度累积策略以最小化昂贵的内存总线流量。
- en: The memory bandwidth limitations become particularly acute during training.
    While inference workloads primarily read model weights sequentially, training
    requires bidirectional data flow for gradient computation and weight updates.
    This increased memory traffic can saturate the memory subsystem, creating bottlenecks
    that limit training throughput regardless of computational capacity. Advanced
    implementations employ techniques such as gradient checkpointing[20](#fn20) to
    trade computation for memory, and mixed-precision training to reduce bandwidth
    requirements while maintaining numerical stability.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，内存带宽限制变得尤为突出。虽然推理工作负载主要按顺序读取模型权重，但训练需要双向数据流来进行梯度计算和权重更新。这种增加的内存流量可能会饱和内存子系统，形成瓶颈，限制训练吞吐量，无论计算能力如何。高级实现采用梯度检查点[20](#fn20)等技术以计算换取内存，以及混合精度训练以降低带宽需求同时保持数值稳定性。
- en: Mobile AI Accelerator Optimization
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 移动AI加速器优化
- en: Different mobile platforms provide distinct acceleration capabilities that determine
    not only achievable model complexity but also feasible learning paradigms. The
    architectural differences between these accelerators fundamentally shape the design
    space for on-device training algorithms, influencing everything from numerical
    precision choices to gradient computation strategies.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的移动平台提供了不同的加速能力，这些能力不仅决定了可实现的模型复杂度，还决定了可行的学习范式。这些加速器之间的架构差异从根本上塑造了设备上训练算法的设计空间，影响着从数值精度选择到梯度计算策略的各个方面。
- en: 'Current generation mobile accelerators demonstrate remarkable diversity in
    their capabilities and optimization focus. Apple’s Neural Engine in the A17 Pro
    delivers 35 TOPS peak performance specialized for 8-bit and 16-bit operations,
    optimized primarily for CoreML inference patterns with limited training support,
    making it ideal for inference-heavy adaptation techniques. Qualcomm’s Hexagon
    DSP in the Snapdragon 8 Gen 3 achieves 45 TOPS with flexible precision support
    and programmable vector units, enabling mixed-precision training workflows that
    can adapt precision dynamically based on training phase and memory constraints.
    Google’s Tensor TPU in the Pixel 8 is optimized specifically for TensorFlow Lite
    operations with strong INT8 performance and tight integration with federated learning
    frameworks, reflecting Google’s strategic focus on distributed learning scenarios.
    The energy efficiency comparison reveals why dedicated neural processing units
    are essential: NPUs achieve 1-5 TOPS per watt versus general-purpose CPUs at just
    0.1-0.2 TOPS per watt, representing a 5-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    efficiency advantage that makes the difference between feasible and infeasible
    on-device training.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当前一代的移动加速器在能力和优化重点上表现出显著的多样性。苹果的A17 Pro中的Neural Engine提供了35 TOPS的峰值性能，专门针对8位和16位操作，主要针对CoreML推理模式进行优化，对训练的支持有限，使其成为推理密集型适应技术的理想选择。高通的Snapdragon
    8 Gen 3中的Hexagon DSP通过灵活的精度支持和可编程向量单元实现了45 TOPS，能够实现混合精度训练工作流程，可以根据训练阶段和内存约束动态调整精度。谷歌的Pixel
    8中的Tensor TPU针对TensorFlow Lite操作进行了优化，具有强大的INT8性能，并与联邦学习框架紧密集成，反映了谷歌对分布式学习场景的战略关注。能效比较揭示了为什么专用神经网络单元是必不可少的：NPUs在每瓦特实现1-5
    TOPS，而通用CPU在每瓦特仅实现0.1-0.2 TOPS，这代表了5-50<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的效率优势，这在设备上训练的可行性与不可行性之间划定了界限。
- en: These accelerators determine not just raw performance but feasible learning
    paradigms and algorithmic approaches. Apple’s Neural Engine excels at fixed-precision
    inference workloads but provides limited support for the dynamic precision requirements
    of gradient computation, making it more suitable for inference-heavy adaptation
    techniques like few-shot learning. Qualcomm’s Hexagon DSP offers greater training
    flexibility through its programmable vector units and support for mixed-precision
    arithmetic, enabling more sophisticated on-device training including full backpropagation
    on compact models. Google’s Tensor TPU integrates tightly with federated learning
    frameworks and provides optimized communication primitives for distributed training
    scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些加速器不仅决定了原始性能，还决定了可行的学习范式和算法方法。苹果的神经网络引擎在固定精度推理工作负载方面表现出色，但为梯度计算的动态精度需求提供了有限的支持，使其更适合于推理密集型适应技术，如少样本学习。高通的Hexagon
    DSP通过其可编程向量单元和对混合精度算术的支持提供了更大的训练灵活性，使得在紧凑模型上实现更复杂的设备上训练成为可能，包括完整的反向传播。谷歌的Tensor
    TPU与联邦学习框架紧密集成，并为分布式训练场景提供了优化的通信原语。
- en: 'The architectural implications extend beyond computational throughput to memory
    access patterns and data flow optimization. Training workloads exhibit fundamentally
    different characteristics than inference: gradient computation requires significantly
    higher memory bandwidth due to the amplification effects discussed above, weight
    updates create write-heavy access patterns, and optimizer state management demands
    additional memory allocation. Modern edge accelerators are beginning to address
    these challenges through specialized hardware features including adaptive precision
    datapaths that dynamically switch between INT4 for forward passes and FP16 for
    gradient computation, sparse computation units that accelerate selective parameter
    updates by skipping zero gradients, and near-memory compute architectures that
    reduce data movement costs by performing gradient updates directly adjacent to
    weight storage.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的影响不仅限于计算吞吐量，还包括内存访问模式和数据处理优化。训练工作负载与推理具有根本不同的特性：由于上述讨论的放大效应，梯度计算需要显著更高的内存带宽，权重更新创建出写密集型的访问模式，而优化器状态管理需要额外的内存分配。现代边缘加速器正通过专用硬件特性来应对这些挑战，包括自适应精度数据路径，该路径可以动态地在INT4（用于正向传递）和FP16（用于梯度计算）之间切换，稀疏计算单元通过跳过零梯度来加速选择性参数更新，以及近内存计算架构，通过在权重存储附近直接执行梯度更新来降低数据移动成本。
- en: However, most current edge accelerators remain primarily optimized for inference
    workloads, creating a significant hardware-software co-design opportunity. Future
    on-device training accelerators will need to efficiently handle the unique demands
    of local adaptation, including support for dynamic precision scaling, efficient
    gradient accumulation, and specialized memory hierarchies optimized for the bidirectional
    data flow patterns characteristic of training workloads. Architecture selection
    influences everything from model quantization strategies and gradient computation
    approaches to federated communication protocols and thermal management policies.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前大多数边缘加速器主要针对推理工作负载进行优化，这为硬件和软件协同设计提供了巨大的机会。未来的设备上训练加速器需要高效地处理本地适应的独特需求，包括支持动态精度缩放、高效的梯度累积以及针对训练工作负载双向数据流模式优化的专用内存层次结构。架构选择影响着从模型量化策略和梯度计算方法到联邦通信协议和热管理策略的各个方面。
- en: Holistic Resource Management Strategies
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全面的资源管理策略
- en: The constraint analysis above reveals three fundamental challenge categories
    that define the on-device learning design space. Each constraint category directly
    drives a corresponding solution pillar, creating a systematic engineering approach
    to this complex systems problem. The constraint-to-solution mapping follows naturally
    from understanding how specific limitations necessitate particular technical responses.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上述约束分析揭示了三个基本挑战类别，这些类别定义了设备上学习设计空间。每个约束类别直接驱动相应的解决方案支柱，形成了一种系统化的工程方法来应对这个复杂系统问题。约束到解决方案的映射自然地来自于理解特定限制如何需要特定的技术响应。
- en: The resource amplification effects—where training increases memory requirements
    by 3-10<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>,
    computational costs by 2-3<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>,
    and energy consumption proportionally—directly necessitate Model Adaptation approaches.
    When traditional training becomes impossible due to resource constraints, systems
    must fundamentally reduce the scope of parameter updates while preserving learning
    capability.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 资源放大效应——训练使内存需求增加3-10倍，计算成本增加2-3倍，能耗成比例增加——直接需要模型适应方法。当由于资源限制而无法进行传统训练时，系统必须在保持学习能力的同时，从根本上减少参数更新的范围。
- en: The information scarcity constraints—limited local datasets, non-IID distributions,
    privacy restrictions on data sharing, and minimal supervision—directly drive Data
    Efficiency solutions. When conventional data-hungry approaches fail due to insufficient
    local information, systems must extract maximum learning signal from minimal examples.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 信息稀缺约束——局部数据集有限、非独立同分布分布、数据共享的隐私限制和最小监督——直接推动了数据效率解决方案。当由于局部信息不足而导致传统数据密集型方法失败时，系统必须从最少的示例中提取最大的学习信号。
- en: The coordination challenges—device heterogeneity, intermittent connectivity,
    distributed validation complexity, and scalability requirements—directly motivate
    Federated Coordination mechanisms. When isolated on-device learning limits collective
    intelligence, systems must enable privacy-preserving collaboration across device
    populations.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 协调挑战——设备异质性、间歇性连接、分布式验证复杂性和可扩展性要求——直接推动了联邦协调机制。当设备上的独立学习限制了集体智慧时，系统必须能够在设备群体之间实现隐私保护的协作。
- en: This constraint-to-solution mapping, illustrated in [Table 14.2](ch020.xhtml#tbl-constraint-solution-mapping),
    creates a systematic engineering framework where each pillar addresses specific
    aspects of the deployment challenge while integrating with the others. Rather
    than viewing these as independent techniques, successful on-device learning systems
    orchestrate all three approaches to create coherent adaptive systems that operate
    effectively within edge constraints.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表14.2](ch020.xhtml#tbl-constraint-solution-mapping)所示，这种约束到解决方案的映射创建了一个系统性的工程框架，其中每个支柱解决部署挑战的特定方面，同时与其他部分相整合。而不是将这些视为独立的技术，成功的设备上学习系统编排所有三种方法，以创建在边缘约束内有效运行的协调一致的自适应系统。
- en: 'Table 14.2: **Constraint-Solution Mapping**: The three fundamental constraint
    categories in on-device learning each drive corresponding solution approaches
    through direct necessity.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.2：**约束-解决方案映射**：设备上学习的三个基本约束类别通过直接必要性驱动相应的解决方案。
- en: '| **Constraint Category** | **Key Challenges** | **Solution Approach** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **约束类别** | **关键挑战** | **解决方案** |'
- en: '| --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Resource Amplification** | • Training workloads (3-10× memory) • Memory limitations
    • Power constraints | **Model Adaptation** • Parameter-efficient updates • Selective
    layer fine-tuning • Low-rank adaptations |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **资源放大** | • 训练工作负载（3-10倍内存） • 内存限制 • 功率限制 | **模型适应** • 参数高效更新 • 选择性层微调 •
    低秩适应 |'
- en: '| **Information Scarcity** | • Limited local datasets • Non-IID distributions
    • Privacy restrictions | **Data Efficiency**• Few-shot learning• Meta-learning
    • Transfer learning |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| **信息稀缺** | • 局部数据集有限 • 非独立同分布分布 • 数据共享的隐私限制 | **数据效率**• 少样本学习• 元学习 • 迁移学习
    |'
- en: '| **Coordination Challenges** | • Device heterogeneity • Intermittent connectivity
    • Distributed validation | **Federated Coordination** • Privacy-preserving aggregation
    • Robust communication protocols • Asynchronous participation |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **协调挑战** | • 设备异质性 • 间歇性连接 • 分布式验证 | **联邦协调** • 隐私保护聚合 • 坚固的通信协议 • 异步参与 |'
- en: The subsequent sections examine each solution pillar systematically, building
    on the optimization principles from [Chapter 10](ch016.xhtml#sec-model-optimizations)
    and the distributed systems frameworks from [Chapter 13](ch019.xhtml#sec-ml-operations).
    Each pillar provides essential capabilities that the others cannot deliver alone,
    but their integration creates systems capable of meaningful adaptation within
    the severe constraints of edge deployment environments.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节将系统地检查每个解决方案支柱，基于第10章（[第10章](ch016.xhtml#sec-model-optimizations)）中的优化原则和第13章（[第13章](ch019.xhtml#sec-ml-operations)）中的分布式系统框架。每个支柱都提供了其他单个支柱无法提供的必要功能，但它们的集成创造了能够在边缘部署环境的严格约束下进行有意义适应的系统。
- en: Model Adaptation
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型适应
- en: 'The computational and memory constraints outlined above create a challenging
    environment for model training, but they also reveal clear solution pathways when
    approached systematically. Model adaptation represents the first pillar of on-device
    learning systems engineering: reducing the scope of parameter updates to make
    training feasible within edge constraints while maintaining sufficient model expressivity
    for meaningful personalization.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的计算和内存限制为模型训练创造了一个具有挑战性的环境，但它们也揭示了当系统性地处理时，清晰的解决方案路径。模型适应代表了设备端学习系统工程的第一个支柱：通过减少参数更新的范围，在边缘约束内使训练可行，同时保持足够模型的表达性以实现有意义的个性化。
- en: 'The engineering challenge centers on navigating a fundamental trade-off space:
    adaptation expressivity versus resource consumption. At one extreme, updating
    all parameters provides maximum flexibility but exceeds edge device capabilities.
    At the other extreme, no adaptation preserves resources but fails to capture user-specific
    patterns. Effective on-device learning systems must operate in the middle ground,
    selecting adaptation strategies based on three key engineering criteria.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 工程挑战集中在导航一个基本权衡空间：适应表达性与资源消耗。在一种极端情况下，更新所有参数提供最大灵活性，但超出了边缘设备的处理能力。在另一种极端情况下，不进行适应可以保留资源，但无法捕捉到用户特定的模式。有效的设备端学习系统必须在中间地带运行，根据三个关键工程标准选择适应策略。
- en: First, available memory, compute, and energy determine which adaptation approaches
    are feasible. A smartwatch with 1 MB RAM requires fundamentally different strategies
    than a smartphone with 8 GB. Second, the degree of user-specific variation drives
    adaptation complexity needs. Simple preference learning may require only bias
    updates, while complex domain shifts demand more sophisticated approaches. Third,
    adaptation techniques must integrate with existing inference pipelines, federated
    coordination protocols, and operational monitoring systems established in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可用的内存、计算和能源决定了哪些适应方法是可行的。具有1MB RAM的智能手表需要与具有8GB RAM的手机采用完全不同的策略。其次，用户特定差异的程度驱动了适应复杂性的需求。简单的偏好学习可能只需要偏差更新，而复杂的领域迁移则需要更复杂的方法。第三，适应技术必须与第13章（[第13章](ch019.xhtml#sec-ml-operations)）中建立的现有推理管道、联邦协调协议和运营监控系统相集成。
- en: This systems perspective guides the selection and combination of techniques
    starting with lightweight approaches ([Section 14.4.1](ch020.xhtml#sec-ondevice-learning-weight-freezing-3407))
    and progressing to more sophisticated methods ([Section 14.4.3](ch020.xhtml#sec-ondevice-learning-sparse-updates-879b)),
    moving from lightweight bias-only approaches through progressively more expressive
    but resource-intensive methods. Each technique represents a different point in
    the engineering trade-off space rather than an isolated algorithmic solution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统视角指导了从轻量级方法（[第14.4.1节](ch020.xhtml#sec-ondevice-learning-weight-freezing-3407)）开始的选择和组合，并逐步过渡到更复杂的方法（[第14.4.3节](ch020.xhtml#sec-ondevice-learning-sparse-updates-879b)），从仅包含偏差的轻量级方法逐步过渡到更具表达性但资源密集型的方法。每种技术代表工程权衡空间中的不同点，而不是一个孤立的算法解决方案。
- en: 'Building on the compression techniques from [Chapter 10](ch016.xhtml#sec-model-optimizations),
    on-device learning transforms compression from a one-time optimization into an
    ongoing constraint. The central insight driving all model adaptation approaches
    is that complete model retraining is neither necessary nor feasible for on-device
    learning scenarios. Instead, systems can strategically leverage pre-trained representations
    and adapt only the minimal parameter subset required to capture local variations,
    operating on the principle: preserve what works globally, adapt what matters locally.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[第10章](ch016.xhtml#sec-model-optimizations)中的压缩技术，设备上的学习将压缩从一次性优化转变为持续约束。驱动所有模型调整方法的中心洞察是，对于设备上的学习场景，完全重新训练模型既不必要也不可行。相反，系统可以战略性地利用预训练表示，并仅适应捕获局部变化所需的最小参数子集，遵循的原则是：保留全局有效的内容，适应局部重要的事项。
- en: This section systematically examines three complementary adaptation strategies,
    each specifically designed to address different device constraint profiles and
    application requirements. Weight freezing addresses severe memory limitations
    by updating only bias terms or final layers, enabling learning even on severely
    constrained microcontrollers that would otherwise lack the resources for any form
    of adaptation. Structured updates use low-rank and residual adaptations to balance
    model expressiveness with computational efficiency, enabling more sophisticated
    learning than bias-only approaches while maintaining manageable resource requirements.
    Sparse updates enable selective parameter modification based on gradient importance
    or layer criticality, concentrating learning capacity on the most impactful parameters
    while leaving less important weights frozen.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本节系统地检查了三种互补的调整策略，每种策略都是专门为解决不同的设备约束配置和应用需求而设计的。权重冻结通过仅更新偏置项或最终层来应对严重的内存限制，即使在其他情况下缺乏任何形式适应资源的严重受限的微控制器上也能实现学习。结构化更新使用低秩和残差调整来平衡模型的表达能力和计算效率，在保持可管理资源需求的同时，允许比仅偏置方法更复杂的学习。稀疏更新允许根据梯度重要性或层关键性进行选择性的参数修改，将学习能力集中在最有影响的参数上，而将不那么重要的权重冻结。
- en: These approaches build on established architectural principles while strategically
    applying optimization strategies to the unique challenges of edge deployment.
    Each technique represents a carefully considered point in the fundamental accuracy-efficiency
    tradeoff space, enabling practical deployment across the full spectrum of edge
    hardware capabilities—from ultra-constrained microcontrollers to capable mobile
    processors.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在既定的架构原则基础上，战略性地应用优化策略来解决边缘部署的独特挑战。每种技术都代表了在基本精度-效率权衡空间中经过仔细考虑的点，使得在边缘硬件能力的全范围内实现实用部署成为可能——从超受限的微控制器到功能强大的移动处理器。
- en: Weight Freezing
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重冻结
- en: 'The most straightforward approach to making on-device learning feasible is
    to dramatically reduce the number of parameters that require updating. One of
    the simplest and most effective strategies for achieving this reduction is to
    freeze the majority of a model’s parameters and adapt only a carefully chosen
    minimal subset. The most widely used approach within this family is bias-only
    adaptation, in which all weights are held fixed and only the bias terms (typically
    scalar offsets applied after linear or convolutional layers) are updated during
    training. This simple constraint creates significant benefits: it reduces the
    number of trainable parameters (often by 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>), simplifies memory
    management during backpropagation, and helps mitigate overfitting when training
    data is sparse or noisy.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使设备上的学习成为可能的最直接方法之一是显著减少需要更新的参数数量。实现这种减少的最简单和最有效的策略之一是冻结模型的大部分参数，并仅适应精心选择的极小子集。在这个家族中最广泛使用的方法是仅偏置调整，其中所有权重都保持固定，仅在训练期间更新偏置项（通常是线性或卷积层之后的标量偏移量）。这个简单的约束带来了显著的好处：它减少了可训练参数的数量（通常减少100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>），简化了反向传播期间的内存管理，并在训练数据稀疏或噪声时有助于减轻过拟合。
- en: 'Consider a standard neural network layer: <semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow>
    <annotation encoding="application/x-tex">y = W x + b</annotation></semantics>
    where <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics>
    is the weight matrix, <semantics><mrow><mi>b</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">b \in \mathbb{R}^m</annotation></semantics> is the
    bias vector, and <semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation
    encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics> is the
    input. In full training, gradients are computed for both <semantics><mi>W</mi><annotation
    encoding="application/x-tex">W</annotation></semantics> and <semantics><mi>b</mi><annotation
    encoding="application/x-tex">b</annotation></semantics>. In bias-only adaptation,
    we constrain: <semantics><mrow><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>W</mi></mrow></mfrac><mo>=</mo><mn>0</mn><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac><mo>≠</mo><mn>0</mn></mrow>
    <annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial
    W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0</annotation></semantics>
    so that only the bias is updated via gradient descent: <semantics><mrow><mi>b</mi><mo>←</mo><mi>b</mi><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">b \leftarrow b - \eta \frac{\partial
    \mathcal{L}}{\partial b}</annotation></semantics>'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个标准的神经网络层：<semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow>
    <annotation encoding="application/x-tex">y = W x + b</annotation></semantics>
    其中 <semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics>
    是权重矩阵，<semantics><mrow><mi>b</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation
    encoding="application/x-tex">b \in \mathbb{R}^m</annotation></semantics> 是偏置向量，<semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation
    encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics> 是输入。在完整训练中，计算了<semantics><mi>W</mi><annotation
    encoding="application/x-tex">W</annotation></semantics>和<semantics><mi>b</mi><annotation
    encoding="application/x-tex">b</annotation></semantics>的梯度。在仅偏置适应中，我们约束：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>W</mi></mrow></mfrac><mo>=</mo><mn>0</mn><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac><mo>≠</mo><mn>0</mn></mrow>
    <annotation encoding="application/x-tex">\frac{\partial \mathcal{L}}{\partial
    W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0</annotation></semantics>
    因此，只有偏置通过梯度下降进行更新：<semantics><mrow><mi>b</mi><mo>←</mo><mi>b</mi><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">b \leftarrow b - \eta \frac{\partial
    \mathcal{L}}{\partial b}</annotation></semantics>
- en: This reduces the number of stored gradients and optimizer states, enabling training
    to proceed under memory-constrained conditions. On embedded devices that lack
    floating-point units, this reduction enables on-device learning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这减少了存储的梯度数和优化器状态数，使得在内存受限条件下也能进行训练。在缺少浮点单元的嵌入式设备上，这种减少使得设备上的学习成为可能。
- en: The code snippet in [Listing 14.1](ch020.xhtml#lst-bias-adaptation) demonstrates
    how to implement bias-only adaptation in PyTorch.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表14.1](ch020.xhtml#lst-bias-adaptation)中的代码片段展示了如何在PyTorch中实现仅偏置适应。'
- en: 'Listing 14.1: **Bias-Only Adaptation**: Freezes model parameters except for
    biases to reduce memory usage and allow on-device learning.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1：**仅偏置适应**：冻结模型参数中的偏置以减少内存使用，并允许设备上的学习。
- en: '[PRE0]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This pattern ensures that only bias terms participate in the backward pass and
    optimizer update, simplifying the training process while maintaining adaptation
    capability. It proves valuable when adapting pretrained models to user-specific
    or device-local data where the core representations remain relevant but require
    calibration.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式确保只有偏置项参与反向传播和优化器更新，简化了训练过程同时保持适应能力。当将预训练模型适应于特定用户或设备本地数据时，其中核心表示仍然相关但需要校准，这非常有价值。
- en: The practical effectiveness of this approach is demonstrated by TinyTL, a framework
    explicitly designed to enable efficient adaptation of deep neural networks on
    microcontrollers and other severely memory-limited platforms. Rather than updating
    all network parameters during training (impossible on such constrained devices),
    TinyTL strategically freezes both the convolutional weights and the batch normalization
    statistics, training only the bias terms and, in some cases, lightweight residual
    components. This architectural constraint creates a profound shift in memory requirements
    during backpropagation, since the largest memory consumers (intermediate activations)
    no longer need to be stored for gradient computation across frozen layers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TinyTL 的实际有效性通过 TinyTL 框架得到证明，该框架专门设计用于在微控制器和其他严重内存受限平台上高效地适应深度神经网络。在训练期间，TinyTL
    并不是更新所有网络参数（在如此受限的设备上不可能实现），而是战略性地冻结卷积权重和批量归一化统计信息，仅训练偏置项以及在某些情况下轻量级的残差组件。这种架构限制在反向传播期间对内存需求产生了深远的影响，因为最大的内存消耗者（中间激活）不再需要存储以进行冻结层之间的梯度计算。
- en: The architectural impact of this approach becomes clear when comparing standard
    training with the TinyTL approach. [Figure 14.5](ch020.xhtml#fig-tinytl-architecture)
    illustrates the fundamental differences between a conventional model and the TinyTL
    approach to on-device adaptation. Given the edge device memory constraints established
    earlier, the TinyTL approach fundamentally changes the memory equation by eliminating
    the need to store activations for frozen layers, making adaptation possible within
    the severe memory constraints of edge devices.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当将标准训练与 TinyTL 方法进行比较时，这种方法的架构影响变得清晰。[图 14.5](ch020.xhtml#fig-tinytl-architecture)
    展示了传统模型与 TinyTL 方法在设备上适应的根本区别。考虑到之前建立的边缘设备内存限制，TinyTL 方法通过消除存储冻结层激活的需求，从根本上改变了内存方程，使得在边缘设备的严重内存限制下进行适应成为可能。
- en: '![](../media/file224.svg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file224.svg)'
- en: 'Figure 14.5: TinyTL reduces on-device training costs by freezing convolutional
    weights and batch normalization, updating only bias terms and lightweight residual
    connections to minimize memory usage during backpropagation. This approach allows
    deployment of deep neural networks on resource-constrained edge devices with limited
    SRAM, facilitating efficient model adaptation without requiring full parameter
    updates.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：TinyTL 通过冻结卷积权重和批量归一化，仅更新偏置项和轻量级残差连接来降低设备上训练的成本。这种方法允许在资源受限的边缘设备上部署深度神经网络，这些设备具有有限的
    SRAM，从而在不要求完整参数更新的情况下实现高效的模型适应。
- en: In contrast, the TinyTL architecture freezes all weights and updates only the
    bias terms inserted after convolutional layers. These bias modules are lightweight
    and require minimal memory, enabling efficient training with a drastically reduced
    memory footprint. The frozen convolutional layers act as a fixed feature extractor,
    and only the trainable bias components are involved in adaptation. By avoiding
    storage of full activation maps and limiting the number of updated parameters,
    TinyTL allows on-device training under severe resource constraints.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，TinyTL 架构冻结了所有权重，并且只更新在卷积层之后插入的偏置项。这些偏置模块轻量级且占用内存最小，使得在显著减少内存占用的情况下实现高效训练成为可能。冻结的卷积层充当固定的特征提取器，并且只有可训练的偏置组件参与适应。通过避免存储完整的激活图并限制更新参数的数量，TinyTL
    允许在设备上在严重资源限制下进行训练。
- en: Because the base model remains unchanged, TinyTL assumes that the pretrained
    features are sufficiently expressive for downstream tasks. The bias terms allow
    for minor but meaningful shifts in model behavior, particularly for personalization
    tasks. When domain shift is more significant, TinyTL can optionally incorporate
    small residual adapters to improve expressivity, all while preserving the system’s
    tight memory and energy profile.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基础模型保持不变，TinyTL 假设预训练的特征对于下游任务来说足够表达。偏置项允许模型行为发生微小但有意义的变化，尤其是在个性化任务中。当领域变化更为显著时，TinyTL
    可以选择性地结合小的残差适配器来提高表达能力，同时保持系统的紧凑内存和能耗配置文件。
- en: These design choices allow TinyTL to reduce training memory usage by 10×. For
    instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated
    parameters from over 3 million to fewer than 50,000[21](#fn21). Combined with
    quantization, this allows local adaptation on devices with only a few hundred
    kilobytes of memory—making on-device learning truly feasible in constrained environments.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设计选择使得TinyTL能够将训练内存使用量减少10倍。例如，使用TinyTL对MobileNetV2模型进行自适应可以将更新参数的数量从超过300万个减少到少于50,000个[21](#fn21)。结合量化，这允许在只有几百千字节内存的设备上进行本地自适应——使得在受限环境中进行设备学习真正可行。
- en: Structured Parameter Updates
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化参数更新
- en: While weight freezing provides computational efficiency and clear memory bounds,
    it severely limits model expressivity by constraining adaptation to a small parameter
    subset. When bias-only updates prove insufficient for capturing complex domain
    shifts or user-specific patterns, residual and low-rank techniques provide increased
    adaptation capability while maintaining computational tractability. These approaches
    represent a middle ground between the extreme efficiency of weight freezing and
    the full expressivity of unrestricted fine-tuning.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然权重冻结提供了计算效率和清晰的内存界限，但它通过限制自适应到一个小参数子集，严重限制了模型的表达能力。当仅偏置更新不足以捕捉复杂的领域变化或用户特定的模式时，残差和低秩技术提供了增强的自适应能力，同时保持了计算上的可处理性。这些方法代表了权重冻结的极端效率和无限制微调的完全表达能力之间的中间地带。
- en: Rather than modifying existing parameters, these methods extend frozen models
    by adding trainable components—residual adaptation modules ([Houlsby et al. 2019](ch058.xhtml#ref-houlsby2019parameter))
    or low-rank parameterizations ([E. J. Hu et al. 2021](ch058.xhtml#ref-hu2021lora))—that
    provide controlled increases in model capacity. This architectural approach enables
    more sophisticated adaptation while preserving the computational benefits that
    make on-device learning feasible.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与修改现有参数不同，这些方法通过添加可训练组件——残差自适应模块([Houlsby等人，2019](ch058.xhtml#ref-houlsby2019parameter))或低秩参数化([E.
    J. Hu等人，2021](ch058.xhtml#ref-hu2021lora))——来扩展冻结模型，这些组件提供了受控的模型容量增加。这种架构方法使得更复杂的自适应成为可能，同时保持了使设备学习可行的计算优势。
- en: These methods extend a frozen model by adding trainable layers, which are typically
    small and computationally inexpensive, that allow the network to respond to new
    data. The main body of the network remains fixed, while only the added components
    are optimized. This modularity makes the approach well-suited for on-device adaptation
    in constrained settings, where small updates must deliver meaningful changes.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法通过添加可训练层来扩展冻结模型，这些层通常很小且计算成本低，使得网络能够对新数据进行响应。网络的主要部分保持不变，而只有添加的组件被优化。这种模块化使得该方法非常适合在受限环境中进行设备上的自适应，在这种环境中，必须提供能够带来有意义变化的小更新。
- en: Adapter-Based Adaptation
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于适配器的自适应
- en: 'A common implementation involves inserting adapters, which are small residual
    bottleneck layers, between existing layers in a pretrained model. Consider a hidden
    representation <semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics>
    passed between layers. A residual adapter introduces a transformation: <semantics><mrow><mi>h</mi><mi>′</mi><mo>=</mo><mi>h</mi><mo>+</mo><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">h'' = h + A(h)</annotation></semantics>
    where <semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(\cdot)</annotation></semantics>
    is a trainable function, typically composed of two linear layers with a nonlinearity:
    <semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mi>σ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mn>1</mn></msub><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A(h)
    = W_2 \, \sigma(W_1 h)</annotation></semantics> with <semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W_1 \in \mathbb{R}^{r \times d}</annotation></semantics>
    and <semantics><mrow><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W_2 \in \mathbb{R}^{d \times r}</annotation></semantics>,
    where <semantics><mrow><mi>r</mi><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">r
    \ll d</annotation></semantics>. This bottleneck design ensures that only a small
    number of parameters are introduced per layer.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的实现方法是在预训练模型中的现有层之间插入适配器，这些适配器是小的残差瓶颈层。考虑在层之间传递的隐藏表示<semantics><mi>h</mi><annotation
    encoding="application/x-tex">h</annotation></semantics>。一个残差适配器引入了一种转换：<semantics><mrow><mi>h</mi><mi>′</mi><mo>=</mo><mi>h</mi><mo>+</mo><mi>A</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">h' = h + A(h)</annotation></semantics>
    其中 <semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(\cdot)</annotation></semantics>
    是一个可训练的函数，通常由两个带有非线性函数的线性层组成：<semantics><mrow><mi>A</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>h</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mi>σ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>W</mi><mn>1</mn></msub><mi>h</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">A(h)
    = W_2 \, \sigma(W_1 h)</annotation></semantics> 其中 <semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W_1 \in \mathbb{R}^{r \times d}</annotation></semantics>
    和 <semantics><mrow><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">W_2 \in \mathbb{R}^{d \times r}</annotation></semantics>，其中
    <semantics><mrow><mi>r</mi><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">r
    \ll d</annotation></semantics>。这种瓶颈设计确保每个层只引入少量参数。
- en: The adapters act as learnable perturbations on top of a frozen backbone. Because
    they are small and sparsely applied, they add negligible memory overhead, yet
    they allow the model to shift its predictions in response to new inputs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器在冻结的骨干网络上充当可学习的扰动。由于它们体积小且应用稀疏，因此它们几乎不会增加内存开销，但它们允许模型根据新的输入调整其预测。
- en: Low-Rank Techniques
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩技术
- en: 'Another efficient strategy is to constrain weight updates themselves to a low-rank
    structure. Rather than updating a full matrix <semantics><mi>W</mi><annotation
    encoding="application/x-tex">W</annotation></semantics>, we approximate the update
    as: <semantics><mrow><mi>Δ</mi><mi>W</mi><mo>≈</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow>
    <annotation encoding="application/x-tex">\Delta W \approx U V^\top</annotation></semantics>
    where <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times r}</annotation></semantics>
    and <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{n \times r}</annotation></semantics>,
    with <semantics><mrow><mi>r</mi><mo>≪</mo><mo>min</mo><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">r \ll \min(m,n)</annotation></semantics>. This reduces
    the number of trainable parameters from <semantics><mrow><mi>m</mi><mi>n</mi></mrow><annotation
    encoding="application/x-tex">mn</annotation></semantics> to <semantics><mrow><mi>r</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mo>+</mo><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(m
    + n)</annotation></semantics>.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种高效策略是将权重更新本身约束到低秩结构。而不是更新完整的矩阵 <semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics>，我们将更新近似为：<semantics><mrow><mi>Δ</mi><mi>W</mi><mo>≈</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow>
    <annotation encoding="application/x-tex">\Delta W \approx U V^\top</annotation></semantics>
    其中 <semantics><mrow><mi>U</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">U \in \mathbb{R}^{m \times r}</annotation></semantics>
    和 <semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation
    encoding="application/x-tex">V \in \mathbb{R}^{n \times r}</annotation></semantics>，其中
    <semantics><mrow><mi>r</mi><mo>≪</mo><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r
    \ll \min(m,n)</annotation></semantics>。这减少了可训练参数的数量，从 <semantics><mrow><mi>m</mi><mi>n</mi></mrow><annotation
    encoding="application/x-tex">mn</annotation></semantics> 减少到 <semantics><mrow><mi>r</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>m</mi><mo>+</mo><mi>n</mi><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(m
    + n)</annotation></semantics>。
- en: 'The mathematical intuition behind this decomposition connects to fundamental
    linear algebra principles: any matrix can be expressed as a sum of rank-one matrices
    through singular value decomposition. By constraining our updates to low rank
    (typically <semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r
    = 4</annotation></semantics> to <semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics>),
    we capture the most significant modes of variation while reducing parameters.
    For a typical transformer layer with dimensions <semantics><mrow><mn>768</mn><mo>×</mo><mn>768</mn></mrow><annotation
    encoding="application/x-tex">768 \times 768</annotation></semantics>, full fine-tuning
    requires updating 589,824 parameters. With rank-4 decomposition, we update only
    <semantics><mrow><mn>768</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>6</mn><mo>,</mo><mn>144</mn></mrow><annotation
    encoding="application/x-tex">768 \times 4 \times 2 = 6,144</annotation></semantics>
    parameters, a 96% reduction, while empirically retaining 85-90% of the adaptation
    quality.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解背后的数学直觉与基本的线性代数原理相联系：任何矩阵都可以通过奇异值分解表示为秩为1的矩阵之和。通过将我们的更新约束到低秩（通常是 <semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation
    encoding="application/x-tex">r = 4</annotation></semantics> 到 <semantics><mn>16</mn><annotation
    encoding="application/x-tex">16</annotation></semantics>），我们捕捉到最重要的变化模式，同时减少参数。对于一个典型的维度为
    <semantics><mrow><mn>768</mn><mo>×</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">768
    \times 768</annotation></semantics> 的transformer层，全微调需要更新589,824个参数。使用秩-4分解，我们只更新
    <semantics><mrow><mn>768</mn><mo>×</mo><mn>4</mn><mo>×</mo><mn>2</mn><mo>=</mo><mn>6</mn><mo>,</mo><mn>144</mn></mrow><annotation
    encoding="application/x-tex">768 \times 4 \times 2 = 6,144</annotation></semantics>
    个参数，减少了96%，同时在经验上保留了85-90%的适应质量。
- en: 'During adaptation, the new weight is computed as: <semantics><mrow><msub><mi>W</mi><mtext
    mathvariant="normal">adapted</mtext></msub><mo>=</mo><msub><mi>W</mi><mtext mathvariant="normal">frozen</mtext></msub><mo>+</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow>
    <annotation encoding="application/x-tex">W_{\text{adapted}} = W_{\text{frozen}}
    + U V^\top</annotation></semantics>'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在自适应过程中，新的权重计算如下：<semantics><mrow><msub><mi>W</mi><mtext mathvariant="normal">adapted</mtext></msub><mo>=</mo><msub><mi>W</mi><mtext
    mathvariant="normal">frozen</mtext></msub><mo>+</mo><mi>U</mi><msup><mi>V</mi><mi>⊤</mi></msup></mrow>
    <annotation encoding="application/x-tex">W_{\text{adapted}} = W_{\text{frozen}}
    + U V^\top</annotation></semantics>
- en: This formulation is commonly used in LoRA (Low-Rank Adaptation)[22](#fn22) techniques,
    originally developed for transformer models ([E. J. Hu et al. 2021](ch058.xhtml#ref-hu2021lora))
    but broadly applicable across architectures. From a systems engineering perspective,
    LoRA addresses critical connectivity and resource trade-offs in on-device learning
    deployment.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式在LoRA（低秩自适应）[22](#fn22)技术中常用，最初是为transformer模型开发的（[E. J. Hu等，2021](ch058.xhtml#ref-hu2021lora)），但适用于各种架构。从系统工程的角度来看，LoRA解决了设备学习中部署的关键连接性和资源权衡问题。
- en: Consider a mobile deployment where a 7B parameter language model requires 14 GB
    for full fine-tuning—impossible on typical smartphones with 6-8 GB total memory.
    LoRA with rank-16 reduces this to ~100 MB of trainable parameters (0.7% of original),
    enabling local adaptation within mobile memory constraints.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个移动部署场景，其中7B参数的语言模型需要14GB进行完全微调——这在典型的6-8GB总内存的智能手机上是不可能的。使用16秩的LoRA可以将可训练参数减少到约100MB（原始的0.7%），从而在移动内存限制内实现本地自适应。
- en: LoRA’s efficiency becomes critical in intermittent connectivity scenarios. A
    full model update over cellular networks would require 14 GB download (potential
    cost $140+ in mobile data charges), while LoRA adapter updates are typically 10-50 MB.
    For periodic federated coordination, devices can synchronize LoRA adapters in
    under 30 seconds on 3G networks, compared to hours for full model transfers. This
    enables practical federated learning even with poor network conditions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在间歇性连接场景中，LoRA的效率变得至关重要。通过蜂窝网络进行完整模型更新需要14GB的下载（潜在成本140+美元的移动数据费用），而LoRA适配器更新通常为10-50MB。对于定期的联邦协调，设备可以在3G网络上在30秒内同步LoRA适配器，而完整模型传输可能需要数小时。这即使在网络条件较差的情况下也使得实用的联邦学习成为可能。
- en: Systems typically deploy different LoRA configurations based on device capabilities—flagship
    phones use rank-32 adapters for higher expressivity, mid-range devices use rank-16
    for balanced performance, and budget devices use rank-8 to stay within 2 GB memory
    limits. Low-rank updates can be implemented efficiently on edge devices, particularly
    when <semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics>
    and <semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics>
    are small and fixed-point representations are supported ([Listing 14.2](ch020.xhtml#lst-lowrank-adapter)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通常根据设备能力部署不同的LoRA配置——旗舰手机使用32秩适配器以实现更高的表达能力，中端设备使用16秩以实现平衡的性能，而预算设备使用8秩以保持在2GB内存限制内。低秩更新可以在边缘设备上高效实现，尤其是在<semantics><mi>U</mi><annotation
    encoding="application/x-tex">U</annotation></semantics>和<semantics><mi>V</mi><annotation
    encoding="application/x-tex">V</annotation></semantics>很小且支持定点表示的情况下（[列表14.2](ch020.xhtml#lst-lowrank-adapter)）。
- en: 'Listing 14.2: **Low-Rank Adapter**: The code implements a low-rank adapter
    module by approximating weight updates using matrices (u) and (v), reducing parameter
    count while enabling efficient model adaptation on edge devices.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.2：**低秩适配器**：该代码通过使用矩阵（u）和（v）近似权重更新来实现低秩适配器模块，减少参数数量，同时使边缘设备上的高效模型自适应成为可能。
- en: '[PRE1]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This adapter adds a small residual transformation to a frozen layer. When inserted
    into a larger model, only the adapter parameters are trained.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此适配器向一个冻结层添加了一个小的残差变换。当将其插入到更大的模型中时，只有适配器参数会被训练。
- en: Edge Personalization
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边缘个性化
- en: Adapters are useful when a global model is deployed to many devices and must
    adapt to device-specific input distributions. In smartphone camera pipelines,
    environmental lighting, user preferences, or lens distortion vary between users
    ([Rebuffi, Bilen, and Vedaldi 2017](ch058.xhtml#ref-rebuffi2017learning)). A shared
    model can be frozen and fine-tuned per-device using a few residual modules, allowing
    lightweight personalization without risking catastrophic forgetting. In voice-based
    systems, adapter modules have been shown to reduce word error rates in personalized
    speech recognition without retraining the full acoustic model. They also allow
    easy rollback or switching between user-specific versions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当全局模型部署到多个设备上并且必须适应特定于设备的输入分布时，适配器很有用。在智能手机摄像头管道中，环境光照、用户偏好或镜头畸变在不同用户之间会有所不同（[Rebuffi、Bilen
    和 Vedaldi 2017](ch058.xhtml#ref-rebuffi2017learning)）。可以通过使用几个残差模块来冻结共享模型并针对每个设备进行微调，从而实现轻量级个性化，而不会导致灾难性遗忘。在基于语音的系统中，适配器模块已被证明可以降低个性化语音识别中的词错误率，而无需重新训练完整的声学模型。它们还允许轻松回滚或在不同用户特定版本之间切换。
- en: Performance vs. Resource Trade-offs
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 性能与资源权衡
- en: Residual and low-rank updates strike a balance between expressivity and efficiency.
    Compared to bias-only learning, they can model more substantial deviations from
    the pretrained task. However, they require more memory and compute for training
    and inference.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 残差和低秩更新在表达性和效率之间取得了平衡。与仅使用偏差的学习相比，它们可以模拟从预训练任务中更显著的偏差。然而，它们在训练和推理过程中需要更多的内存和计算资源。
- en: When considering residual and low-rank updates for on-device learning, several
    important tradeoffs emerge. First, these methods consistently demonstrate superior
    adaptation quality compared to bias-only approaches, particularly when deployed
    in scenarios involving significant distribution shifts from the original training
    data ([Quiñonero-Candela et al. 2008](ch058.xhtml#ref-quinonero2009dataset)).
    This improved adaptability stems from their increased parameter capacity and ability
    to learn more complex transformations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑设备上学习的残差和低秩更新时，出现了几个重要的权衡。首先，与仅使用偏差的方法相比，这些方法在适应性质量上始终表现出优越性，尤其是在部署在涉及从原始训练数据到显著分布变化的场景中（[Quiñonero-Candela
    等人 2008](ch058.xhtml#ref-quinonero2009dataset)）。这种改进的适应性源于它们增加的参数容量和能够学习更复杂变换的能力。
- en: This enhanced adaptability comes at a cost. The introduction of additional layers
    or parameters inevitably increases both memory requirements and computational
    latency during forward and backward passes. While these increases are modest compared
    to full model training, they must be considered when deploying to resource-constrained
    devices.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这种增强的适应性是有代价的。引入额外的层或参数不可避免地会增加前向和反向传递期间的内存需求和计算延迟。虽然与完整模型训练相比，这些增加是微不足道的，但在部署到资源受限的设备时必须考虑这一点。
- en: Implementing these adaptation techniques requires system-level support for dynamic
    computation graphs and the ability to selectively inject trainable parameters.
    Not all deployment environments or inference engines support such capabilities
    out of the box.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些适应技术需要系统级支持动态计算图和选择性地注入可训练参数的能力。并非所有部署环境或推理引擎都默认支持这些功能。
- en: Residual adaptation techniques have proven valuable in mobile and edge computing
    scenarios where devices have sufficient computational resources. Modern smartphones
    and tablets can accommodate these adaptations while maintaining acceptable performance
    characteristics. This makes residual adaptation a practical choice for applications
    requiring personalization without the overhead of full model retraining.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 残差适应技术在移动和边缘计算场景中已被证明是有价值的，在这些场景中，设备有足够的计算资源。现代智能手机和平板电脑可以适应这些更新，同时保持可接受的性能特征。这使得残差适应成为需要个性化但无需重新训练完整模型开销的应用的实际选择。
- en: Sparse Updates
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏更新
- en: As we progress from bias-only updates through low-rank adaptations to more sophisticated
    techniques, sparse updates represent the most advanced approach in our model adaptation
    hierarchy. While the previous techniques add new parameters or restrict updates
    to specific subsets, sparse updates dynamically identify which existing parameters
    provide the greatest adaptation benefit for each specific task or user. This approach
    maximizes adaptation expressivity while maintaining the computational efficiency
    essential for edge deployment.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们从仅偏置更新通过低秩自适应到更复杂的技术，稀疏更新代表了我们模型自适应层次中最先进的方法。虽然前一种技术添加了新参数或限制更新到特定子集，但稀疏更新动态地识别出为每个特定任务或用户提供最大自适应益处的现有参数。这种方法在保持边缘部署所需的计算效率的同时，最大化了自适应的表达能力。
- en: Even when adaptation is restricted to a small number of parameters through the
    techniques discussed above, training remains resource-intensive on constrained
    devices. Sparse updates address this challenge by selectively updating only task-relevant
    subsets of model parameters, rather than modifying entire networks or introducing
    new modules. This approach, known as task-adaptive sparse updating ([X. Zhang,
    Song, and Tao 2020](ch058.xhtml#ref-zhang2020efficient)), represents the culmination
    of principled parameter selection strategies.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 即使通过上述技术将自适应限制在少量参数上，在受限制的设备上进行训练仍然资源密集。稀疏更新通过仅选择性地更新与任务相关的模型参数子集，而不是修改整个网络或引入新模块，来解决这一挑战。这种方法被称为任务自适应稀疏更新([X.
    Zhang, Song, and Tao 2020](ch058.xhtml#ref-zhang2020efficient))，代表了原则性参数选择策略的顶峰。
- en: The key insight is that not all layers of a deep model contribute equally to
    performance gains on a new task or dataset. If we can identify a *minimal subset
    of parameters* that are most impactful for adaptation, we can train only those,
    reducing memory and compute costs while still achieving meaningful personalization.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的洞察是，深度模型的不是所有层都对新的任务或数据集的性能提升做出同等贡献。如果我们能识别出对自适应影响最大的*最小参数子集*，我们就可以只训练这些参数，从而在降低内存和计算成本的同时，仍然实现有意义的个性化。
- en: Sparse Update Design
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏更新设计
- en: 'Let a neural network be defined by parameters <semantics><mrow><mi>θ</mi><mo>=</mo><mo
    stretchy="false" form="prefix">{</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>L</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\theta
    = \{\theta_1, \theta_2, \ldots, \theta_L\}</annotation></semantics> across <semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics> layers. In standard fine-tuning,
    we compute gradients and perform updates on all parameters: <semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo><mrow><mtext
    mathvariant="normal">for</mtext></mrow> <mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi></mrow>
    <annotation encoding="application/x-tex">\theta_i \leftarrow \theta_i - \eta \frac{\partial
    \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L</annotation></semantics>'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 设一个神经网络由参数<semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>L</mi></msub><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\theta
    = \{\theta_1, \theta_2, \ldots, \theta_L\}</annotation></semantics>跨越<semantics><mi>L</mi><annotation
    encoding="application/x-tex">L</annotation></semantics>层定义。在标准微调中，我们计算所有参数的梯度并进行更新：<semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo><mrow><mtext
    mathvariant="normal">for</mtext></mrow> <mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi></mrow>
    <annotation encoding="application/x-tex">\theta_i \leftarrow \theta_i - \eta \frac{\partial
    \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L</annotation></semantics>
- en: 'In task-adaptive sparse updates, we select a small subset <semantics><mrow><mi>𝒮</mi><mo>⊂</mo><mo
    stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo
    stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{S}
    \subset \{1, \ldots, L\}</annotation></semantics> such that only parameters in
    <semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>
    are updated: <semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow>
    <mi>i</mi><mo>∈</mo><mi>𝒮</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd columnalign="left"
    style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">\theta_i \leftarrow \begin{cases} \theta_i
    - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, & \text{if } i \in \mathcal{S}
    \\ \theta_i, & \text{otherwise} \end{cases}</annotation></semantics>'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '在任务自适应稀疏更新中，我们选择一个小的子集<semantics><mrow><mi>𝒮</mi><mo>⊂</mo><mo stretchy="false"
    form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo stretchy="false"
    form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{S}
    \subset \{1, \ldots, L\}</annotation></semantics>，使得只有<semantics><mi>𝒮</mi><annotation
    encoding="application/x-tex">\mathcal{S}</annotation></semantics>中的参数被更新：<semantics><mrow><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><mrow><mo
    stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>θ</mi><mi>i</mi></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi>∂</mi><mi>ℒ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo></mtd><mtd
    columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow>
    <mi>i</mi><mo>∈</mo><mi>𝒮</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align:
    left"><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd columnalign="left"
    style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow>
    <annotation encoding="application/x-tex">\theta_i \leftarrow \begin{cases} \theta_i
    - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, & \text{if } i \in \mathcal{S}
    \\ \theta_i, & \text{otherwise} \end{cases}</annotation></semantics>'
- en: The challenge lies in selecting the optimal subset <semantics><mi>𝒮</mi><annotation
    encoding="application/x-tex">\mathcal{S}</annotation></semantics> given memory
    and compute constraints.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于在内存和计算约束下选择最优子集<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>。
- en: Layer Selection
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层选择
- en: 'A principled strategy for selecting <semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>
    is to use contribution analysis—an empirical method that estimates how much each
    layer contributes to downstream performance improvement. For example, one can
    measure the marginal gain from updating each layer independently:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 选择<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>的一个原则性策略是使用贡献分析——这是一种经验方法，用于估计每个层对下游性能改进的贡献程度。例如，可以独立地测量更新每个层的边际增益：
- en: Freeze the entire model.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结整个模型。
- en: Unfreeze one candidate layer.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻一个候选层。
- en: Finetune briefly and evaluate improvement in validation accuracy.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简短微调并评估验证准确性的改进。
- en: Rank layers by performance gain per unit cost (e.g., per KB of trainable memory).
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按每单位成本（例如，每KB的可训练内存）的性能增益对层进行排名。
- en: This layer-wise profiling yields a ranking from which <semantics><mi>𝒮</mi><annotation
    encoding="application/x-tex">\mathcal{S}</annotation></semantics> can be constructed
    subject to a memory budget.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这种层级分析产生了一个排名，根据这个排名可以在内存预算的限制下构建<semantics><mi>𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics>。
- en: A concrete example is TinyTrain, a method designed to allow rapid adaptation
    on-device ([C. Deng, Zhang, and Wu 2022](ch058.xhtml#ref-deng2022tinytrain)).
    TinyTrain pretrains a model along with meta-gradients that capture which layers
    are most sensitive to new tasks. At runtime, the system dynamically selects layers
    to update based on task characteristics and available resources.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具体的例子是TinyTrain，这是一种旨在允许设备上快速适应的方法（[C. Deng, Zhang, and Wu 2022](ch058.xhtml#ref-deng2022tinytrain)）。TinyTrain在预训练模型的同时，还预训练了元梯度，以捕捉哪些层对新的任务最为敏感。在运行时，系统根据任务特性和可用资源动态选择要更新的层。
- en: Selective Layer Update Implementation
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择性层更新实现
- en: This pattern can be extended with profiling logic to select layers based on
    contribution scores or hardware profiles, as shown in [Listing 14.3](ch020.xhtml#lst-selective-update).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式可以通过分析逻辑扩展，根据贡献分数或硬件配置文件选择层，如列表14.3所示（ch020.xhtml#lst-selective-update）。
- en: 'Listing 14.3: **Selective Layer Updating**: This technique allows fine-tuning
    specific layers of a pre-trained model while keeping others frozen, optimizing
    computational resources for targeted improvements. *Source: PyTorch Documentation*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.3：**选择性层更新**：这项技术允许在保持其他层冻结的同时微调预训练模型的特定层，从而优化计算资源以实现针对性的改进。*来源：PyTorch文档*
- en: '[PRE2]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TinyTrain Personalization
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TinyTrain个性化
- en: Consider a scenario where a user wears an augmented reality headset that performs
    real-time object recognition. As lighting and environments shift, the system must
    adapt to maintain accuracy—but training must occur during brief idle periods or
    while charging.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，用户佩戴一个执行实时物体识别的增强现实头盔。随着光照和环境的变化，系统必须适应以保持准确性——但训练必须在短暂的空闲期间或充电时进行。
- en: 'TinyTrain allows this by using meta-training during offline preparation: the
    model learns not only to perform the task, but also which parameters are most
    important to adapt. Then, at deployment, the device performs task-adaptive sparse
    updates, modifying only a few layers that are most relevant for its current environment.
    This keeps adaptation fast, energy-efficient, and memory-aware.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: TinyTrain通过在离线准备期间使用元训练来实现这一点：模型不仅学习执行任务，还学习哪些参数对于适应最为重要。然后，在部署时，设备执行针对当前环境的任务自适应稀疏更新，仅修改与当前环境最相关的少数层。这保持了适应的快速性、节能性和内存感知性。
- en: Adaptation Strategy Trade-offs
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 适应策略权衡
- en: Task-adaptive sparse updates introduce several important system-level considerations
    that must be carefully balanced. First, the overhead of contribution analysis,
    although primarily incurred during pretraining or initial profiling, represents
    a non-trivial computational cost. This overhead is typically acceptable since
    it occurs offline, but it must be factored into the overall system design and
    deployment pipeline.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 任务自适应稀疏更新引入了几个重要的系统级考虑因素，必须仔细平衡。首先，贡献分析的开销，尽管主要发生在预训练或初始分析期间，代表了一个非微不足道的计算成本。这种开销通常是可以接受的，因为它发生在离线时，但它必须纳入整体系统设计和部署流程中。
- en: Second, the stability of the adaptation process becomes important when working
    with sparse updates. If too few parameters are selected for updating, the model
    may underfit the target distribution, failing to capture important local variations.
    This suggests the need for careful validation of the selected parameter subset
    before deployment, potentially incorporating minimum thresholds for adaptation
    capacity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当使用稀疏更新时，适应过程的不稳定性变得重要。如果选定的参数太少，模型可能无法很好地拟合目标分布，无法捕捉到重要的局部变化。这表明在部署前需要对选定的参数子集进行仔细验证，可能需要结合适应能力的最小阈值。
- en: Third, the selection of updatable parameters must account for hardware-specific
    characteristics of the target platform. Beyond just considering gradient magnitudes,
    the system must evaluate the actual execution cost of updating specific layers
    on the deployed hardware. Some parameters might show high contribution scores
    but prove expensive to update on certain architectures, requiring a more nuanced
    selection strategy that balances statistical utility with runtime efficiency.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，选择可更新参数时必须考虑目标平台的特定硬件特性。除了考虑梯度幅度之外，系统必须评估在部署硬件上更新特定层的实际执行成本。一些参数可能显示出高贡献分数，但在某些架构上更新成本高昂，需要一种更细致的选择策略，以平衡统计效用和运行时效率。
- en: Despite these tradeoffs, task-adaptive sparse updates provide a powerful mechanism
    to scale adaptation to diverse deployment contexts, from microcontrollers to mobile
    devices ([Levy et al. 2023](ch058.xhtml#ref-diao2023sparse)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些权衡，任务自适应稀疏更新提供了一种强大的机制，可以将适应扩展到各种部署环境，从微控制器到移动设备（[Levy等人2023](ch058.xhtml#ref-diao2023sparse)）。
- en: Adaptation Strategy Comparison
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 适应策略比较
- en: Each adaptation strategy for on-device learning offers a distinct balance between
    expressivity, resource efficiency, and implementation complexity. Understanding
    these tradeoffs is important when designing systems for diverse deployment targets—from
    ultra-low-power microcontrollers to feature-rich mobile processors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 每种设备学习自适应策略都提供了表达性、资源效率和实现复杂度之间的独特平衡。在设计针对各种部署目标的系统时，理解这些权衡很重要——从超低功耗微控制器到功能丰富的移动处理器。
- en: Bias-only adaptation is the most lightweight approach, updating only scalar
    offsets in each layer while freezing all other parameters. This significantly
    reduces memory requirements and computational burden, making it suitable for devices
    with tight memory and energy budgets. However, its limited expressivity means
    it is best suited to applications where the pretrained model already captures
    most of the relevant task features and only minor local calibration is required.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 仅偏差的自适应是最轻量级的方法，它只更新每层的标量偏移量，同时冻结所有其他参数。这显著降低了内存需求和计算负担，使其适用于内存和能源预算紧张的设备。然而，由于其表达能力有限，它最适合于预训练模型已经捕捉到大多数相关任务特征，并且只需要进行少量局部校准的应用。
- en: Residual adaptation, often implemented via adapter modules, introduces a small
    number of trainable parameters into the frozen backbone of a neural network. This
    allows for greater flexibility than bias-only updates, while still maintaining
    control over the adaptation cost. Because the backbone remains fixed, training
    can be performed efficiently and safely under constrained conditions. This method
    supports modular personalization across tasks and users, making it a favorable
    choice for mobile settings where moderate adaptation capacity is needed.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余自适应，通常通过适配器模块实现，向冻结的神经网络主干引入少量可训练参数。这比仅偏差更新提供了更大的灵活性，同时仍然保持对自适应成本的掌控。由于主干保持固定，训练可以在受限条件下高效且安全地进行。这种方法支持跨任务和用户的模块化个性化，使其成为需要适度自适应能力的移动环境中的优选选择。
- en: Task-adaptive sparse updates offer the greatest potential for task-specific
    finetuning by selectively updating only a subset of layers or parameters based
    on their contribution to downstream performance. While this method allows expressive
    local adaptation, it requires a mechanism for layer selection, through profiling,
    contribution analysis, or meta-training, which introduces additional complexity.
    Nonetheless, when deployed carefully, it allows for dynamic tradeoffs between
    accuracy and efficiency, particularly in systems that experience large domain
    shifts or evolving input conditions.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 任务自适应稀疏更新通过选择性地仅更新对下游性能有贡献的子集层或参数，为特定任务的微调提供了最大的潜力。虽然这种方法允许表达性的局部自适应，但它需要一个层选择机制，通过配置文件、贡献分析或元训练来实现，这引入了额外的复杂性。然而，当谨慎部署时，它允许在准确性和效率之间进行动态权衡，尤其是在经历大规模领域转移或输入条件演变的系统中。
- en: 'These three approaches form a spectrum of tradeoffs. Their relative suitability
    depends on application domain, available hardware, latency constraints, and expected
    distribution shift. [Table 14.3](ch020.xhtml#tbl-adaptation-strategies) summarizes
    their characteristics:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法形成了一个权衡的谱系。它们的相对适用性取决于应用领域、可用硬件、延迟约束和预期的分布变化。[表14.3](ch020.xhtml#tbl-adaptation-strategies)总结了它们的特征：
- en: 'Table 14.3: **Adaptation Strategy Trade-Offs**: Table entries characterize
    three approaches to model adaptation—bias-only updates, selective layer updates,
    and full finetuning—by quantifying their impact on trainable parameters, memory
    overhead, expressivity, suitability for different use cases, and system requirements.
    These characteristics reveal the inherent trade-offs between model flexibility,
    computational cost, and performance when deploying machine learning systems in
    dynamic environments.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.3：**自适应策略权衡**：表条目通过量化其对可训练参数、内存开销、表达能力、对不同用例的适用性和系统要求的影响，来描述模型自适应的三种方法——仅偏差更新、选择性层更新和完全微调。这些特征揭示了在动态环境中部署机器学习系统时，模型灵活性、计算成本和性能之间的固有权衡。
- en: '| **Technique** | **Trainable Parameters** | **Memory Overhead** | **Expressivity**
    | **Use Case Suitability** | **System Requirements** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **可训练参数** | **内存开销** | **表达能力** | **适用用例** | **系统要求** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Bias-Only Updates** | Bias terms only | Minimal | Low | Simple personalization;
    low variance | Extreme memory/compute limits |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **仅偏置更新** | 仅偏置项 | 最小 | 低 | 简单个性化；低方差 | 极端内存/计算限制 |'
- en: '| **Residual Adapters** | Adapter modules | Moderate | Moderate to High | User-specific
    tuning on mobile | Mobile-class SoCs with runtime support |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **残差适配器** | 适配器模块 | 中等 | 中等到高 | 移动设备上的用户特定调整 | 带运行时支持的移动级SoC |'
- en: '| **Sparse Layer Updates** | Selective parameter subsets | Variable | High
    (task-adaptive) | Real-time adaptation; domain shift | Requires profiling or meta-training
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **稀疏层更新** | 选择性参数子集 | 变量 | 高（任务自适应） | 实时适应；领域迁移 | 需要配置文件或元训练 |'
- en: Data Efficiency
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据效率
- en: 'Having established resource-efficient adaptation through model techniques,
    we encounter the second pillar of on-device learning systems engineering: maximizing
    learning signal from severely constrained data. This represents a fundamental
    shift from the data-abundant environments assumed by traditional ML systems to
    the information-scarce reality of edge deployment.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模型技术建立了资源高效的适应机制后，我们遇到了设备上学习系统工程的第二个支柱：从严重受限的数据中最大化学习信号。这代表了从传统机器学习系统所假设的数据丰富环境到边缘部署信息稀缺现实的基本转变。
- en: 'The systems engineering challenge centers on a critical trade-off: data collection
    cost versus adaptation quality. Edge devices face severe data acquisition constraints
    that reshape learning system design in ways not encountered in centralized training.
    Understanding and navigating these constraints requires systematic analysis of
    four interconnected engineering dimensions.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 系统工程挑战集中在关键权衡上：数据收集成本与适应质量。边缘设备面临严重的数据获取约束，这以在集中式训练中未遇到的方式重塑了学习系统设计。理解和导航这些约束需要系统地分析四个相互关联的工程维度。
- en: First, every data point has acquisition costs in terms of user friction, energy
    consumption, storage overhead, and privacy risk. A voice assistant learning from
    audio samples must balance improvement potential against battery drain and user
    comfort with always-on recording. Second, limited data collection capacity forces
    systems to choose between broad coverage and deep examples. A mobile keyboard
    can collect many shallow typing patterns or fewer detailed interaction sequences,
    each strategy implying different learning approaches. Third, some applications
    demand rapid learning from minimal examples (emergency response scenarios), while
    others can accumulate data over time (user preference learning). This temporal
    dimension drives fundamental architectural choices. Fourth, data efficiency techniques
    must integrate with the model adaptation approaches from [Section 14.4](ch020.xhtml#sec-ondevice-learning-model-adaptation-6a82),
    federated coordination ([Section 14.6](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e)),
    and the operational monitoring established in [Chapter 13](ch019.xhtml#sec-ml-operations).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，每个数据点都有获取成本，包括用户摩擦、能耗、存储开销和隐私风险。一个从音频样本学习的语音助手必须平衡改进潜力与电池消耗和用户对始终开启录音的舒适度。其次，有限的数据收集能力迫使系统在广泛覆盖和深入示例之间做出选择。一个移动键盘可以收集许多浅层打字模式或较少的详细交互序列，每种策略都意味着不同的学习方法。第三，某些应用需要从最小示例中快速学习（紧急响应场景），而其他应用可以随着时间的推移积累数据（用户偏好学习）。这个时间维度驱动了基本架构选择。第四，数据效率技术必须与[第14.4节](ch020.xhtml#sec-ondevice-learning-model-adaptation-6a82)中的模型适应方法、联邦协调([第14.6节](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e))以及[第13章](ch019.xhtml#sec-ml-operations)中建立的运营监控相结合。
- en: These engineering constraints create a systematic trade-off space where different
    data efficiency approaches serve different combinations of constraints. Rather
    than choosing a single technique, successful on-device learning systems typically
    combine multiple approaches, each addressing specific aspects of the data scarcity
    challenge.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工程约束创建了一个系统性的权衡空间，其中不同的数据效率方法服务于不同组合的约束。成功的设备上学习系统通常结合多种方法，每种方法都针对数据稀缺挑战的特定方面。
- en: This section examines four complementary data efficiency strategies that address
    different facets of the data scarcity challenge. Few-shot learning enables adaptation
    from minimal labeled examples, allowing systems to personalize based on just a
    handful of user-provided samples rather than requiring extensive training datasets.
    Streaming updates accommodate data that arrives incrementally over time, enabling
    continuous adaptation as devices encounter new patterns during normal operation
    without needing to collect and store large batches. Experience replay maximizes
    learning from limited data through intelligent reuse, replaying important examples
    multiple times to extract maximum learning signal from scarce training data. Data
    compression reduces memory requirements while preserving learning signals, enabling
    systems to maintain replay buffers and training histories within the tight memory
    constraints of edge devices.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了四种互补的数据效率策略，这些策略针对数据稀缺挑战的不同方面。少样本学习允许从最少的标记示例中进行适应，使系统可以根据用户提供的少量样本进行个性化，而不是需要大量的训练数据集。流式更新适应随着时间的推移逐渐到达的数据，使设备在正常操作中遇到新模式时能够持续适应，而无需收集和存储大量批次。经验重放通过智能重用最大化从有限数据中的学习，多次重放重要示例以从稀缺的训练数据中提取最大的学习信号。数据压缩在保留学习信号的同时减少内存需求，使系统能够在边缘设备的严格内存约束内维护重放缓冲区和训练历史。
- en: Each technique addresses different aspects of the data constraint problem, enabling
    robust learning even when traditional supervised learning would fail.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 每种技术都针对数据约束问题的不同方面，即使在传统监督学习失败的情况下也能实现稳健的学习。
- en: Few-Shot Learning and Data Streaming
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 少样本学习和数据流
- en: 'In conventional machine learning workflows, effective training typically requires
    large labeled datasets, carefully curated and preprocessed to ensure sufficient
    diversity and balance. On-device learning, by contrast, must often proceed from
    only a handful of local examples—collected passively through user interaction
    or ambient sensing, and rarely labeled in a supervised fashion. These constraints
    motivate two complementary adaptation strategies: few-shot learning, in which
    models generalize from a small, static set of examples, and streaming adaptation,
    where updates occur continuously as data arrives.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习工作流程中，有效的训练通常需要大量的标记数据集，这些数据集经过精心策划和预处理，以确保足够的多样性和平衡。相比之下，设备上的学习通常必须从少量本地示例开始——这些示例通过用户交互或环境感知被动收集，并且很少以监督方式标记。这些限制促使两种互补的适应策略：少样本学习，其中模型从一个小而静态的示例集中泛化，以及流式适应，其中更新随着数据的到达而持续进行。
- en: 'Few-shot adaptation is particularly relevant when the device observes a small
    number of labeled or weakly labeled instances for a new task or user condition
    ([Yaqing Wang et al. 2020](ch058.xhtml#ref-wang2020generalizing)). In such settings,
    it is often infeasible to perform full finetuning of all model parameters without
    overfitting. Instead, methods such as bias-only updates, adapter modules, or prototype-based
    classification are employed to make use of limited data while minimizing capacity
    for memorization. Let <semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false"
    form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation
    encoding="application/x-tex">D = \{(x_i, y_i)\}_{i=1}^K</annotation></semantics>
    denote a <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>-shot
    dataset of labeled examples collected on-device. The goal is to update the model
    parameters <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    to improve task performance under constraints such as:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本适应在设备观察少量标记或弱标记实例的新任务或用户条件下尤其相关 ([Yaqing Wang 等人 2020](ch058.xhtml#ref-wang2020generalizing))。在这种情况下，在不过度拟合的情况下进行所有模型参数的全量微调往往是不可行的。相反，采用如仅更新偏差、适配器模块或基于原型的分类等方法来利用有限的数据，同时最小化记忆能力。用
    <semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation
    encoding="application/x-tex">D = \{(x_i, y_i)\}_{i=1}^K</annotation></semantics>
    表示在设备上收集的标记示例的 <semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>
    个少样本数据集。目标是更新模型参数 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    以在以下约束条件下提高任务性能：
- en: 'Limited number of gradient steps: <semantics><mrow><mi>T</mi><mo>≪</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">T \ll 100</annotation></semantics>'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的梯度步数：<semantics><mrow><mi>T</mi><mo>≪</mo><mn>100</mn></mrow><annotation
    encoding="application/x-tex">T \ll 100</annotation></semantics>
- en: 'Constrained memory footprint: <semantics><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">updated</mtext></msub><mo stretchy="false" form="postfix">∥</mo><mo>≪</mo><mo
    stretchy="false" form="postfix">∥</mo><mi>θ</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation
    encoding="application/x-tex">\|\theta_{\text{updated}}\| \ll \|\theta\|</annotation></semantics>'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制内存占用：<semantics><mrow><mo stretchy="false" form="postfix">∥</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">updated</mtext></msub><mo stretchy="false" form="postfix">∥</mo><mo>≪</mo><mo
    stretchy="false" form="postfix">∥</mo><mi>θ</mi><mo stretchy="false" form="postfix">∥</mo></mrow><annotation
    encoding="application/x-tex">\|\theta_{\text{updated}}\| \ll \|\theta\|</annotation></semantics>
- en: Preservation of prior task knowledge (to avoid catastrophic forgetting)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持先前任务知识（以避免灾难性遗忘）
- en: Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation
    in a real-world, on-device deployment ([Warden 2018](ch058.xhtml#ref-warden2018speech)).
    These models are used to detect fixed phrases, including phrases like “Hey Siri”[23](#fn23)
    or “OK Google”, with low latency and high reliability. A typical KWS model consists
    of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network
    that transforms input audio into an embedding space) followed by a lightweight
    classifier. In commercial systems, the encoder is trained centrally using thousands
    of hours of labeled speech across multiple languages and speakers. However, supporting
    custom wake words (e.g., “Hey Jarvis”) or adapting to underrepresented accents
    and dialects is often infeasible via centralized training due to data scarcity
    and privacy concerns.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词检测 (KWS) 系统是少样本适应在现实世界、设备部署中的具体例子 ([Warden 2018](ch058.xhtml#ref-warden2018speech))。这些模型用于检测固定短语，包括“Hey
    Siri”[23](#fn23) 或“OK Google”等短语，具有低延迟和高可靠性。一个典型的 KWS 模型由一个预训练的声学编码器（例如，一个小型卷积或循环网络，将输入音频转换为嵌入空间）和一个轻量级分类器组成。在商业系统中，编码器使用跨多种语言和说话人的数千小时标记语音进行集中训练。然而，由于数据稀缺和隐私问题，支持自定义唤醒词（例如，“Hey
    Jarvis”）或适应代表性不足的口音和方言通常通过集中训练是不可行的。
- en: Few-shot adaptation solves this problem by finetuning only the output classifier
    or a small subset of parameters, including bias terms, using just a few example
    utterances collected directly on the device. For example, a user might provide
    5–10 recordings of their custom wake word. These samples are then used to update
    the model locally, while the main encoder remains frozen to preserve generalization
    and reduce memory overhead. This allows personalization without requiring additional
    labeled data or transmitting private audio to the cloud.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本自适应通过仅微调输出分类器或一小部分参数（包括偏差项），使用仅从设备上收集的几个示例语句来解决此问题。例如，用户可能提供5-10个自定义唤醒词的录音。然后，这些样本用于在本地更新模型，同时主编码器保持冻结以保持泛化并减少内存开销。这允许个性化，而无需额外的标记数据或向云端传输私人音频。
- en: Such an approach is not only computationally efficient, but also aligned with
    privacy-preserving design principles. Because only the output layer is updated,
    often involving a simple gradient step or prototype computation, the total memory
    footprint and runtime compute are compatible with mobile-class devices or even
    microcontrollers. This makes KWS a canonical case study for few-shot learning
    at the edge, where the system must operate under tight constraints while delivering
    user-specific performance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅计算效率高，而且与保护隐私的设计原则相一致。因为只有输出层被更新，通常涉及简单的梯度步骤或原型计算，总的内存占用和运行时计算与移动设备或甚至微控制器兼容。这使得关键词唤醒（KWS）成为边缘少样本学习的典型案例研究，在该研究中，系统必须在严格的约束下运行，同时提供用户特定的性能。
- en: 'Beyond static few-shot learning, many on-device scenarios benefit from streaming
    adaptation, where models must learn incrementally as new data arrives ([Hayes
    et al. 2020](ch058.xhtml#ref-hayes2020remind)). Streaming adaptation generalizes
    this idea to continuous, asynchronous settings where data arrives incrementally
    over time. Let <semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mi>t</mi></msub><msubsup><mo
    stretchy="false" form="postfix">}</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></msubsup></mrow><annotation
    encoding="application/x-tex">\{x_t\}_{t=1}^{\infty}</annotation></semantics> represent
    a stream of observations. In streaming settings, the model must update itself
    after observing each new input, typically without access to prior data, and under
    bounded memory and compute. The model update can be written generically as: <semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><msub><mi>η</mi><mi>t</mi></msub><mi>∇</mi><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)</annotation></semantics>
    where <semantics><msub><mi>η</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\eta_t</annotation></semantics>
    is the learning rate at time <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>.
    This form of adaptation is sensitive to noise and drift in the input distribution,
    and thus often incorporates mechanisms such as learning rate decay, meta-learned
    initialization, or update gating to improve stability.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 除了静态少样本学习之外，许多设备上的场景还可以从流式自适应中受益，其中模型必须随着新数据的到来逐步学习（[Hayes等人2020](ch058.xhtml#ref-hayes2020remind)）。流式自适应将这一想法推广到连续、异步的设置中，其中数据随时间逐步到达。用<semantics><mrow><mo
    stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mi>t</mi></msub><msubsup><mo
    stretchy="false" form="postfix">}</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>∞</mi></msubsup></mrow><annotation
    encoding="application/x-tex">\{x_t\}_{t=1}^{\infty}</annotation></semantics>表示一个观察流。在流式设置中，模型必须在观察每个新输入后更新自己，通常没有访问先前数据，并且内存和计算有限。模型更新可以写成通用形式：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><msub><mi>η</mi><mi>t</mi></msub><mi>∇</mi><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\theta_{t+1}
    = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)</annotation></semantics>
    其中 <semantics><msub><mi>η</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\eta_t</annotation></semantics>
    是时间 <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>
    的学习率。这种自适应形式对输入分布中的噪声和漂移敏感，因此通常采用学习率衰减、元学习初始化或更新门控等机制来提高稳定性。
- en: Aside from KWS, practical examples of these strategies abound. In wearable health
    devices, a model that classifies physical activities may begin with a generic
    classifier and adapt to user-specific motion patterns using only a few labeled
    activity segments. In smart assistants, user voice profiles are fine-tuned over
    time using ongoing speech input, even when explicit supervision is unavailable.
    In such cases, local feedback, including correction, repetition, or downstream
    task success, can serve as implicit signals to guide learning.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关键词语音识别（KWS）之外，这些策略的实际例子比比皆是。在可穿戴健康设备中，一个用于分类身体活动的模型可能从一个通用分类器开始，仅使用少量标记的活动片段来适应用户的特定运动模式。在智能助手中，用户语音配置文件会随着时间的推移通过持续的语音输入进行微调，即使没有明确监督的情况下也是如此。在这种情况下，包括纠正、重复或下游任务成功在内的本地反馈可以作为指导学习的隐含信号。
- en: Few-shot and streaming adaptation highlight the shift from traditional training
    pipelines to data-efficient, real-time learning under uncertainty. They form a
    foundation for more advanced memory and replay strategies, which we turn to next.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本和流式适应突显了从传统训练流程向在不确定性下的数据高效、实时学习的转变。它们构成了更高级记忆和重放策略的基础，我们将在下一部分进行探讨。
- en: Experience Replay
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验重放
- en: Experience replay addresses the challenge of catastrophic forgetting—where learning
    new tasks causes models to forget previously learned information—in continuous
    learning scenarios by maintaining a buffer of representative examples from previous
    learning episodes. This technique, originally developed for reinforcement learning
    ([Mnih et al. 2015](ch058.xhtml#ref-mnih2015human)), proves essential in on-device
    learning where sequential data streams can cause models to overfit to recent examples.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 经验重放通过维护先前学习阶段中代表性示例的缓冲区，在连续学习场景中解决了灾难性遗忘的挑战——即学习新任务导致模型忘记先前学习的信息。这种技术最初是为强化学习开发的（[Mnih
    等人 2015](ch058.xhtml#ref-mnih2015human)），在设备端学习中证明至关重要，因为顺序数据流可能导致模型过度拟合到最近的示例。
- en: Unlike server-side replay strategies that rely on large datasets and extensive
    compute, on-device replay must operate with extremely limited capacity, often
    with tens or hundreds of samples, and must avoid interfering with user experience
    ([Rolnick et al. 2019](ch058.xhtml#ref-rolnick2019experience)). Buffers may store
    only compressed features or distilled summaries, and updates must occur opportunistically
    (e.g., during idle cycles or charging). These system-level constraints reshape
    how replay is implemented and evaluated in the context of embedded ML.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于大量数据集和广泛计算资源的服务器端重放策略不同，设备端重放必须在极其有限的容量下运行，通常只有几十或几百个样本，并且必须避免干扰用户体验（[Rolnick
    等人 2019](ch058.xhtml#ref-rolnick2019experience)）。缓冲区可能只存储压缩特征或精炼摘要，并且更新必须具有机会性（例如，在空闲周期或充电期间）。这些系统级约束重塑了在嵌入式机器学习背景下重放的实施和评估方式。
- en: 'Let <semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>
    represent a memory buffer that retains a fixed-size subset of training examples.
    At time step <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>,
    the model receives a new data point <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x_t,
    y_t)</annotation></semantics> and appends it to <semantics><mi>ℳ</mi><annotation
    encoding="application/x-tex">\mathcal{M}</annotation></semantics>. A replay-based
    update then samples a batch <semantics><mrow><mo stretchy="false" form="prefix">{</mo><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><annotation
    encoding="application/x-tex">\{(x_i, y_i)\}_{i=1}^{k}</annotation></semantics>
    from <semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>
    and applies a gradient step: <semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>ℒ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta
    \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]</annotation></semantics>
    where <semantics><msub><mi>θ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\theta_t</annotation></semantics>
    are the model parameters, <semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics>
    is the learning rate, and <semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics>
    is the loss function. Over time, this replay mechanism allows the model to reinforce
    prior knowledge while incorporating new information.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 用<semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>表示一个内存缓冲区，该缓冲区保留训练示例的固定大小子集。在时间步<semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics>，模型接收一个新的数据点<semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x_t,
    y_t)</annotation></semantics>并将其添加到<semantics><mi>ℳ</mi><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics>中。然后，基于重放的自适应更新从<semantics><mi>ℳ</mi><annotation
    encoding="application/x-tex">\mathcal{M}</annotation></semantics>中采样一个批次<semantics><mrow><mo
    stretchy="false" form="prefix">{</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup></mrow><annotation
    encoding="application/x-tex">\{(x_i, y_i)\}_{i=1}^{k}</annotation></semantics>，并应用梯度步：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mrow><mo
    stretchy="true" form="prefix">[</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>ℒ</mi><mrow><mo
    stretchy="true" form="postfix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>;</mo><msub><mi>θ</mi><mi>t</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta
    \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]</annotation></semantics>其中<semantics><msub><mi>θ</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\theta_t</annotation></semantics>是模型参数，<semantics><mi>η</mi><annotation
    encoding="application/x-tex">\eta</annotation></semantics>是学习率，<semantics><mi>ℒ</mi><annotation
    encoding="application/x-tex">\mathcal{L}</annotation></semantics>是损失函数。随着时间的推移，这种重放机制允许模型在结合新信息的同时加强先前知识。
- en: A practical on-device implementation might use a ring buffer to store a small
    set of compressed feature vectors rather than full input examples. The pseudocode
    as shown in [Listing 14.4](ch020.xhtml#lst-replay-buffer) illustrates a minimal
    replay buffer designed for constrained environments.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实用的设备上实现可能使用环形缓冲区来存储一小组压缩的特征向量，而不是完整的输入示例。如[列表14.4](ch020.xhtml#lst-replay-buffer)所示，伪代码说明了为受限环境设计的最小重放缓冲区。
- en: 'Listing 14.4: **Replay Buffer**: Implements a circular storage mechanism for
    efficient memory management in constrained environments. This approach allows
    models to efficiently retain and sample from recent data points, balancing the
    need to use historical information while incorporating new insights.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.4：**重放缓冲区**：实现了一种循环存储机制，用于在受限环境中进行高效的内存管理。这种方法允许模型有效地保留和从最近的数据点中采样，在利用历史信息的同时，结合新的见解。
- en: '[PRE3]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This implementation maintains a fixed-capacity cyclic buffer, storing compressed
    representations (e.g., last-layer embeddings) and associated labels. Such buffers
    are useful for replaying adaptation updates without violating memory or energy
    budgets.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现维护一个固定容量的循环缓冲区，存储压缩表示（例如，最后一层的嵌入）和相关标签。此类缓冲区对于在不违反内存或能耗预算的情况下重放适应更新非常有用。
- en: In TinyML applications[24](#fn24), experience replay has been applied to problems
    such as gesture recognition, where devices must continuously improve predictions
    while observing a small number of events per day. Instead of training directly
    on the streaming data, the device stores representative feature vectors from recent
    gestures and uses them to finetune classification boundaries periodically. Similarly,
    in on-device keyword spotting, replaying past utterances can improve wake-word
    detection accuracy without the need to transmit audio data off-device.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在TinyML应用[24](#fn24)中，经验重放已被应用于诸如手势识别等问题，在这些问题中，设备必须持续改进预测，同时每天观察少量事件。而不是直接在流数据上训练，设备存储最近手势的代表性特征向量，并定期使用它们来微调分类边界。同样，在设备端关键词检测中，重放过去的语音可以改善唤醒词检测的准确性，而无需将音频数据传输到设备外。
- en: While experience replay improves stability in data-sparse or non-stationary
    environments, it introduces several tradeoffs. Storing raw inputs may breach privacy
    constraints or exceed storage budgets, especially in vision and audio applications.
    Replaying from feature vectors reduces memory usage but may limit the richness
    of gradients for upstream layers. Write cycles to persistent flash memory, which
    are frequently necessary for long-term storage on embedded devices, can also raise
    wear-leveling concerns. These constraints require careful co-design of memory
    usage policies, replay frequency, and feature selection strategies, particularly
    in continuous deployment scenarios.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然经验重放提高了数据稀疏或非平稳环境中的稳定性，但它引入了几个权衡。存储原始输入可能会违反隐私限制或超出存储预算，尤其是在视觉和音频应用中。从特征向量重放可以减少内存使用，但可能会限制上游层的梯度丰富性。对持久性闪存的写周期，这对于嵌入式设备上的长期存储通常是必要的，也可能引起磨损均衡问题。这些限制要求仔细协同设计内存使用策略、重放频率和特征选择策略，尤其是在持续部署场景中。
- en: Data Compression
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据压缩
- en: In many on-device learning scenarios, the raw training data may be too large,
    noisy, or redundant to store and process effectively. This motivates the use of
    compressed data representations, where the original inputs are transformed into
    lower-dimensional embeddings or compact encodings that preserve salient information
    while minimizing memory and compute costs.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多设备端学习场景中，原始的训练数据可能太大、噪声过多或冗余，难以有效存储和处理。这促使人们使用压缩数据表示，将原始输入转换为低维嵌入或紧凑编码，在最小化内存和计算成本的同时保留显著信息。
- en: Compressed representations serve two complementary goals. First, they reduce
    the footprint of stored data, allowing devices to maintain longer histories or
    replay buffers under tight memory budgets ([Sanh et al. 2019](ch058.xhtml#ref-sanh2019distilbert)).
    Second, they simplify the learning task by projecting raw inputs into more structured
    feature spaces, often learned via pretraining or meta-learning, in which efficient
    adaptation is possible with minimal supervision.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩表示实现了两个互补的目标。首先，它们减少了存储数据的占用空间，使得设备能够在紧张的内存预算下维持更长的历史记录或重放缓冲区（[Sanh等人2019](ch058.xhtml#ref-sanh2019distilbert)）。其次，通过将原始输入投影到更结构化的特征空间，简化了学习任务，这些特征空间通常通过预训练或元学习获得，其中在最小监督下可以进行有效的适应。
- en: One common approach is to encode data points using a pretrained feature extractor
    and discard the original high-dimensional input. For example, an image <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> might be passed through
    a CNN to produce an embedding vector <semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_i
    = f(x_i)</annotation></semantics>, where <semantics><mrow><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">f(\cdot)</annotation></semantics> is a fixed feature
    encoder. This embedding captures visual structure (e.g., shape, texture, or spatial
    layout) in a compact representation, usually ranging from 64 to 512 dimensions,
    suitable for lightweight downstream adaptation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是使用预训练的特征提取器对数据点进行编码，并丢弃原始的高维输入。例如，一个图像 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation
    encoding="application/x-tex">x_i</annotation></semantics> 可能会通过一个卷积神经网络（CNN）来生成一个嵌入向量
    <semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_i
    = f(x_i)</annotation></semantics>，其中 <semantics><mrow><mi>f</mi><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>⋅</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">f(\cdot)</annotation></semantics> 是一个固定的特征编码器。这个嵌入以紧凑的表示捕捉视觉结构（例如，形状、纹理或空间布局），通常在64到512维之间，适合轻量级下游适应。
- en: 'Mathematically, training can proceed over compressed samples <semantics><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(z_i,
    y_i)</annotation></semantics> using a lightweight decoder or projection head.
    Let <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    represent the trainable parameters of this decoder model, which is typically a
    small neural network that maps from compressed representations to output predictions.
    As each example is presented, the model parameters are updated using gradient
    descent: <semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>ℒ</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>g</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i;
    \theta), y_i\big)</annotation></semantics> Here:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，训练可以通过压缩样本进行，使用轻量级的解码器或投影头处理 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(z_i,
    y_i)</annotation></semantics>。让 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    代表这个解码模型的可训练参数，这通常是一个小型神经网络，它将压缩表示映射到输出预测。每当呈现一个示例时，模型参数都会通过梯度下降进行更新：<semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>η</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>ℒ</mi><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><mi>g</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo
    minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow> <annotation
    encoding="application/x-tex">\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i;
    \theta), y_i\big)</annotation></semantics> 这里：
- en: <semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics>
    is the compressed representation of the <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>-th
    input,
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics>
    是第 <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>
    个输入的压缩表示，
- en: <semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics>
    is the corresponding label or supervision signal,
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics>
    是相应的标签或监督信号，
- en: <semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(z_i;
    \theta)</annotation></semantics> is the decoder’s prediction,
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>;</mo><mi>θ</mi><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(z_i;
    \theta)</annotation></semantics> 是解码器的预测，
- en: <semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics>
    is the loss function measuring prediction error,
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>ℒ</mi><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics>
    是衡量预测误差的损失函数，
- en: <semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics>
    is the learning rate, and
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics>
    是学习率，并且
- en: <semantics><msub><mi>∇</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\nabla_\theta</annotation></semantics>
    denotes the gradient with respect to the parameters <semantics><mi>θ</mi><annotation
    encoding="application/x-tex">\theta</annotation></semantics>.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <semantics><msub><mi>∇</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\nabla_\theta</annotation></semantics>
    表示相对于参数 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    的梯度。
- en: This formulation highlights how only a compact decoder model, which has the
    parameter set <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>,
    needs to be trained, making the learning process feasible even when memory and
    compute are limited.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式突出了为什么只需要训练一个紧凑的解码器模型，其参数集为 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>，这使得即使在内存和计算有限的情况下，学习过程也是可行的。
- en: Advanced approaches extend beyond fixed encoders by learning discrete or sparse
    dictionaries that represent data using low-rank or sparse coefficient matrices.
    A dataset of sensor traces can be factorized as <semantics><mrow><mi>X</mi><mo>≈</mo><mi>D</mi><mi>C</mi></mrow><annotation
    encoding="application/x-tex">X \approx DC</annotation></semantics>, where <semantics><mi>D</mi><annotation
    encoding="application/x-tex">D</annotation></semantics> is a dictionary of basis
    patterns and <semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics>
    is a block-sparse coefficient matrix indicating which patterns are active in each
    example. By updating only a small number of dictionary atoms or coefficients,
    the model adapts with minimal overhead.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 高级方法通过学习表示数据使用低秩或稀疏系数矩阵的离散或稀疏字典，超越了固定编码器。一个传感器轨迹数据集可以被分解为 <semantics><mrow><mi>X</mi><mo>≈</mo><mi>D</mi><mi>C</mi></mrow><annotation
    encoding="application/x-tex">X \approx DC</annotation></semantics>，其中 <semantics><mi>D</mi><annotation
    encoding="application/x-tex">D</annotation></semantics> 是基模式字典，而 <semantics><mi>C</mi><annotation
    encoding="application/x-tex">C</annotation></semantics> 是指示每个示例中哪些模式是活跃的块稀疏系数矩阵。通过仅更新少量字典原子或系数，模型以最小的开销进行适应。
- en: Compressed representations prove useful in privacy-sensitive settings, as they
    allow raw data to be discarded or obfuscated after encoding. Compression acts
    as an implicit regularizer, smoothing the learning process and mitigating overfitting
    when only a few training examples are available.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐私敏感的环境中，压缩表示被证明是有用的，因为它们允许在编码后丢弃或模糊原始数据。压缩作为一种隐式正则化器，在只有少量训练示例可用时，可以平滑学习过程并减轻过拟合。
- en: In practice, these strategies have been applied in domains such as keyword spotting,
    where raw audio signals are first transformed into Mel-frequency cepstral coefficients
    (MFCCs)[25](#fn25)—a compact, lossy representation of the power spectrum of speech.
    These MFCC vectors serve as compressed inputs for downstream models, enabling
    local adaptation using only a few kilobytes of memory.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些策略已经应用于诸如关键词检测等域，其中原始音频信号首先被转换为梅尔频率倒谱系数（MFCCs）[25](#fn25)——这是语音功率谱的紧凑、有损表示。这些MFCC向量作为压缩输入用于下游模型，仅使用几KB的内存即可实现局部自适应。
- en: Data Efficiency Strategy Comparison
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据效率策略比较
- en: The techniques introduced in this section (few-shot learning, experience replay,
    and compressed data representations) offer strategies for adapting models on-device
    when data is scarce or streaming. They operate under different assumptions and
    constraints, and their effectiveness depends on system-level factors such as memory
    capacity, data availability, task structure, and privacy requirements.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍的技术（少量样本学习、经验回放和压缩数据表示）为在数据稀缺或流式传输时在设备上适应模型提供了策略。它们在不同的假设和约束下运行，其有效性取决于系统级因素，如内存容量、数据可用性、任务结构和隐私要求。
- en: Few-shot adaptation excels when a small but informative set of labeled examples
    is available, particularly when personalization or rapid task-specific tuning
    is required. It minimizes compute and data needs, but its effectiveness depends
    on the quality of pretrained representations and the alignment between the initial
    model and the local task.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当有少量但信息丰富的标记示例集可用时，少量样本适应表现出色，尤其是在需要个性化或快速任务特定调整时。它最小化了计算和数据需求，但它的有效性取决于预训练表示的质量以及初始模型与本地任务之间的对齐。
- en: Experience replay addresses continual adaptation by mitigating forgetting and
    improving stability, especially in non-stationary environments. It allows reuse
    of past data, but requires memory to store examples and compute cycles for periodic
    updates. Replay buffers may also raise privacy or longevity concerns, especially
    on devices with limited storage or flash write cycles.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放通过减轻遗忘和改善稳定性来处理持续适应，尤其是在非平稳环境中。它允许重用过去的数据，但需要存储示例的内存和周期性更新的计算周期。重放缓冲区也可能引起隐私或长期存储的担忧，尤其是在存储空间有限或闪存写入周期有限的设备上。
- en: 'Compressed data representations reduce the footprint of learning by transforming
    raw data into compact feature spaces. This approach supports longer retention
    of experience and efficient finetuning, particularly when only lightweight heads
    are trainable. Compression can introduce information loss, and fixed encoders
    may fail to capture task-relevant variability if they are not well-aligned with
    deployment conditions. [Table 14.4](ch020.xhtml#tbl-ondevice-techniques) summarizes
    key tradeoffs:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩数据表示通过将原始数据转换为紧凑的特征空间来减少学习占用的空间。这种方法支持更长时间的经验保留和高效的微调，尤其是在只有轻量级头部可训练的情况下。压缩可能会引入信息损失，如果固定编码器没有与部署条件良好对齐，它们可能无法捕捉到与任务相关的变化。[表14.4](ch020.xhtml#tbl-ondevice-techniques)总结了关键权衡：
- en: 'Table 14.4: **On-Device Learning Trade-Offs**: Few-shot adaptation balances
    data efficiency with model personalization by leveraging small labeled datasets,
    but requires careful consideration of memory and compute constraints for deployment
    on resource-limited devices. The table summarizes key considerations for selecting
    appropriate on-device learning techniques based on application requirements and
    available resources.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.4：**设备上学习权衡**：少量样本适应通过利用小型标记数据集在数据效率和模型个性化之间取得平衡，但需要在资源受限的设备上部署时仔细考虑内存和计算约束。该表总结了根据应用需求和可用资源选择适当的设备上学习技术时的关键考虑因素。
- en: '| **Technique** | **Data Requirements** | **Memory/Compute Overhead** | **Use
    Case Fit** |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **数据需求** | **内存/计算开销** | **适用场景** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Few-Shot Adaptation** | Small labeled set (K-shots) | Low | Personalization,
    quick on-device finetuning |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **少量样本适应** | 小型标记集（K次射击） | 低 | 个性化，快速设备微调 |'
- en: '| **Experience Replay** | Streaming data | Moderate (buffer & update) | Non-stationary
    data, stability under drift |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **经验回放** | 流数据 | 中等（缓冲区与更新） | 非平稳数据，漂移下的稳定性 |'
- en: '| **Compressed** **Representations** | Unlabeled or encoded data | Low to Moderate
    | Memory-limited devices, privacy-sensitive contexts |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **压缩表示** | 未标记或编码数据 | 低到中等 | 内存受限设备，隐私敏感环境 |'
- en: In practice, these methods are not mutually exclusive. Many real-world systems
    combine them to achieve robust, efficient adaptation. For example, a keyword spotting
    system may use compressed audio features (e.g., MFCCs), finetune a few parameters
    from a small support set, and maintain a replay buffer of past embeddings for
    continual refinement.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些方法不是相互排斥的。许多现实世界的系统将它们结合起来，以实现稳健、高效的适应。例如，一个关键词检测系统可能使用压缩音频特征（例如，MFCCs），从一个小支持集中微调少量参数，并维护过去嵌入的重放缓冲区以进行持续优化。
- en: 'Together, these strategies embody the core challenge of on-device learning:
    achieving reliable model improvement under persistent constraints on data, compute,
    and memory.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略共同体现了设备学习的核心挑战：在数据、计算和内存的持续约束下实现可靠的模型改进。
- en: Federated Learning
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习
- en: The individual device techniques examined above—from bias-only updates to sophisticated
    adapter modules—create powerful personalization capabilities but reveal a fundamental
    limitation when deployed at scale. While each device can adapt effectively to
    local conditions, these isolated improvements cannot benefit the broader device
    population. Valuable insights about model robustness, adaptation strategies, and
    failure modes remain trapped on individual devices, losing the collective intelligence
    that makes centralized training effective.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 上述单个设备技术——从仅偏差更新到复杂的适配器模块——创造了强大的个性化能力，但在规模部署时揭示了根本性的限制。虽然每个设备可以有效地适应本地条件，但这些孤立的改进不能惠及更广泛的设备群体。关于模型鲁棒性、适应策略和故障模式的宝贵见解仍然被困在单个设备上，失去了使集中式训练有效的集体智慧。
- en: This limitation becomes apparent in scenarios requiring both personalization
    and population-scale learning. The model adaptation and data efficiency techniques
    enable individual devices to learn effectively within resource constraints, but
    they also reveal a fundamental coordination challenge that emerges when sophisticated
    local learning meets the realities of distributed deployment.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这种限制在需要个人化和人口规模学习的场景中变得明显。模型适应和数据效率技术使单个设备能够在资源约束下有效地学习，但它们也揭示了一个基本的协调挑战，当复杂的本地学习遇到分布式部署的现实时出现。
- en: 'Consider a voice assistant deployed to 10 million homes. Each device adapts
    locally to its user’s voice, accent, and vocabulary. Device A learns that “data”
    is pronounced /ˈdeɪtə/, Device B learns /ˈdætə/. Device C encounters the rare
    phrase “machine learning” frequently (tech household), while Device D never sees
    it (non-tech household). After six months of local adaptation:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个部署在1000万户家庭的语音助手。每个设备都根据其用户的语音、口音和词汇进行本地适应。设备A了解到“数据”发音为/ˈdeɪtə/，设备B了解到/ˈdætə/。设备C经常遇到罕见的短语“机器学习”（技术家庭），而设备D从未见过（非技术家庭）。经过六个月的本地适应：
- en: Each device excels at its specific user’s patterns but only its patterns
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个设备擅长其特定用户的模式，但只有这些模式
- en: Rare vocabulary gets learned on some devices, forgotten on others
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罕见词汇在某些设备上学习，在其他设备上遗忘
- en: Local biases accumulate without correction from broader population
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地偏差在没有更广泛的群体纠正的情况下积累
- en: Valuable insights discovered on one device benefit no others
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个设备上发现的宝贵见解对其他设备没有好处
- en: Individual on-device learning, while powerful, faces fundamental limitations
    when devices operate in isolation. Each device observes only a narrow slice of
    the full data distribution, limiting generalization. Device capabilities vary
    dramatically, creating learning imbalances across the population. Valuable insights
    learned on one device cannot benefit others, reducing overall system intelligence.
    Without coordination, models may diverge or degrade over time due to local biases.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单个设备上的学习非常强大，但当设备在孤立状态下运行时，它面临着根本性的限制。每个设备只能观察到整个数据分布的一个狭窄切片，限制了泛化。设备的能力差异很大，在人口中造成了学习不平衡。在一个设备上学习的宝贵见解不能惠及其他设备，降低了整体系统智能。没有协调，模型可能会随着时间的推移而发散或退化，这是由于本地偏差造成的。
- en: Federated learning emerges as the solution to distributed coordination constraints.
    It enables privacy-preserving collaboration where devices contribute to collective
    intelligence without sharing raw data. Rather than viewing individual device learning
    and coordinated learning as separate paradigms, federated learning represents
    the natural evolution when on-device systems deploy at scale. This approach transforms
    the constraint of data locality from a limitation into a privacy feature, allowing
    systems to learn from population-scale data while keeping individual information
    secure.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习成为解决分布式协调约束的解决方案。它实现了隐私保护的合作，其中设备在不共享原始数据的情况下为集体智慧做出贡献。与其将单个设备学习和协调学习视为不同的范例，联邦学习代表了当设备在规模上部署时的自然演变。这种方法将数据本地的约束从限制转变为隐私特性，允许系统从人口规模的数据中学习，同时保持个人信息的安全。
- en: The privacy requirements here directly connect to security and privacy principles
    that become crucial in production deployments. Rather than viewing individual
    device learning and coordinated learning as separate paradigms, federated learning
    represents the natural evolution of on-device systems when deployed at scale.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的隐私要求直接关联到安全和隐私原则，这些原则在生产部署中变得至关重要。与其将单个设备学习和协调学习视为不同的范例，不如说联邦学习是在大规模部署时设备系统的自然演变。
- en: '***Federated Learning*** is a decentralized training approach in which distributed
    devices collaboratively train a *shared model* using *local data* while exchanging
    only *model updates*, preserving *privacy* through data localization.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习**是一种去中心化的训练方法，在这种方法中，分布式设备通过使用本地数据协同训练一个**共享模型**，而只交换**模型更新**，通过数据本地化来保护**隐私**。'
- en: To better understand the role of federated learning, it is useful to contrast
    it with other learning paradigms. [Figure 14.6](ch020.xhtml#fig-learning-paradigms)
    illustrates the distinction between offline learning, on-device learning, and
    federated learning. In traditional offline learning, all data is collected and
    processed centrally. The model is trained in the cloud using curated datasets
    and is then deployed to edge devices without further adaptation. In contrast,
    on-device learning allows local model adaptation using data generated on the device
    itself, supporting personalization but in isolation—without sharing insights across
    users. Federated learning bridges these two extremes by enabling localized training
    while coordinating updates globally. It retains data privacy by keeping raw data
    local, yet benefits from distributed model improvements by aggregating updates
    from many devices.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解联邦学习的作用，将其与其他学习范例进行对比是有用的。[图14.6](ch020.xhtml#fig-learning-paradigms)展示了离线学习、设备学习和联邦学习之间的区别。在传统的离线学习中，所有数据都集中收集和处理。模型在云中使用精选的数据集进行训练，然后部署到边缘设备而无需进一步适应。相比之下，设备学习允许使用设备本身生成的数据进行本地模型适应，支持个性化但孤立——不与用户共享见解。联邦学习通过允许本地训练同时协调全球更新来弥合这两个极端。它通过保持原始数据本地化来保留数据隐私，同时通过聚合来自许多设备的更新来从分布式模型改进中受益。
- en: '![](../media/file225.svg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file225.svg)'
- en: 'Figure 14.6: Federated learning balances data privacy with collective model
    improvement by coordinating local training across distributed devices, unlike
    offline learning’s centralized approach or on-device learning’s isolated adaptation.
    This figure contrasts how each paradigm handles data location and model update
    strategies, revealing the trade-offs between personalization, data security, and
    global knowledge sharing.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：联邦学习通过协调分布式设备上的本地训练，在数据隐私和集体模型改进之间取得平衡，与离线学习的集中式方法或设备学习的孤立适应方法不同。此图对比了每种范例如何处理数据位置和模型更新策略，揭示了个性化、数据安全和全球知识共享之间的权衡。
- en: This section explores the principles and practical considerations of federated
    learning in the context of mobile and embedded systems. It begins by outlining
    the canonical FL protocols and their system implications. It then discusses device
    participation constraints, communication-efficient update mechanisms, and strategies
    for personalized learning. Throughout, the emphasis remains on how federated methods
    can extend the reach of on-device learning by enabling distributed model training
    across diverse and resource-constrained hardware platforms.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了在移动和嵌入式系统背景下联邦学习的原理和实际考虑。它首先概述了标准的FL协议及其系统影响。然后讨论设备参与约束、通信高效的更新机制以及个性化学习的策略。在整个过程中，重点始终是如何通过使分布式模型训练跨越各种资源和受限的硬件平台来扩展设备学习的范围。
- en: Privacy-Preserving Collaborative Learning
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私保护协同学习
- en: Federated learning (FL) is a decentralized paradigm for training machine learning
    models across a population of devices without transferring raw data to a central
    server ([McMahan et al. 2017c](ch058.xhtml#ref-mcmahan2017communication)). Unlike
    traditional centralized training pipelines, which require aggregating all training
    data in a single location, federated learning distributes the training process
    itself. Each participating device computes updates based on its local data and
    contributes to a global model through an aggregation protocol, typically coordinated
    by a central server. This shift in training architecture aligns closely with the
    needs of mobile, edge, and embedded systems, where privacy, communication cost,
    and system heterogeneity impose significant constraints on centralized approaches.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）是一种去中心化的范式，用于在设备群体中训练机器学习模型，而不需要将原始数据传输到中央服务器（[McMahan等人2017c](ch058.xhtml#ref-mcmahan2017communication)）。与传统集中式训练流程不同，后者需要在单个位置聚合所有训练数据，联邦学习将训练过程本身进行分布。每个参与设备根据其本地数据计算更新，并通过聚合协议向全局模型做出贡献，通常由中央服务器协调。这种训练架构的转变与移动、边缘和嵌入式系统的需求紧密一致，在这些系统中，隐私、通信成本和系统异构性对集中式方法施加了重大限制。
- en: As demonstrated across the application domains discussed earlier—from Gboard’s
    keyboard personalization to wearable health monitoring to voice interfaces—federated
    learning bridges the gap between model improvement and the system-level constraints
    established throughout this chapter. It enables the personalization, privacy,
    and connectivity benefits motivating on-device learning while addressing the resource
    constraints through coordinated but distributed training. However, these benefits
    introduce new challenges including client variability, communication efficiency,
    and non-IID data distributions that require specialized protocols and coordination
    mechanisms.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的应用领域所示——从Gboard的键盘个性化到可穿戴健康监测再到语音界面——联邦学习弥合了模型改进与本章中建立的系统级约束之间的差距。它实现了设备学习的个性化、隐私和连接性优势，同时通过协调但分布的训练来应对资源约束。然而，这些优势也引入了新的挑战，包括客户端的可变性、通信效率和非-IID数据分布，这些都需要专门的协议和协调机制。
- en: Building on this foundation, the remainder of this section explores the key
    techniques and tradeoffs that define federated learning in on-device settings,
    examining the core learning protocols that govern coordination across devices
    and investigating strategies for scheduling, communication efficiency, and personalization.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，本节剩余部分探讨了定义设备上联邦学习的核心技术和权衡，检查了治理跨设备协调的核心学习协议，并研究了调度、通信效率和个性化的策略。
- en: Learning Protocols
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习协议
- en: Federated learning protocols define the rules and mechanisms by which devices
    collaborate to train a shared model. These protocols govern how local updates
    are computed, aggregated, and communicated, as well as how devices participate
    in the training process. The choice of protocol has significant implications for
    system performance, communication overhead, and model convergence.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习协议定义了设备协作训练共享模型的规则和机制。这些协议规定了如何计算、聚合和通信本地更新，以及设备如何参与训练过程。协议的选择对系统性能、通信开销和模型收敛有重大影响。
- en: In this section, we outline the core components of federated learning protocols,
    including local training, aggregation methods, and communication strategies. We
    also discuss the tradeoffs associated with different approaches and their implications
    for on-device ML systems.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了联邦学习协议的核心组件，包括本地训练、聚合方法和通信策略。我们还讨论了不同方法相关的权衡及其对设备上机器学习系统的影响。
- en: Local Training
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地训练
- en: 'Local training refers to the process by which individual devices compute model
    updates based on their local data. This step is critical in federated learning,
    as it allows devices to adapt the shared model to their specific contexts without
    transferring raw data. The local training process involves the following steps:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本地训练是指单个设备根据其本地数据计算模型更新的过程。在联邦学习中，这一步骤至关重要，因为它允许设备在不传输原始数据的情况下，将共享模型适应其特定环境。本地训练过程包括以下步骤：
- en: '**Model Initialization**: Each device initializes its local model parameters,
    often by downloading the latest global model from the server.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型初始化**：每个设备都会初始化其本地模型参数，通常是通过从服务器下载最新的全局模型来实现的。'
- en: '**Local Data Sampling**: The device samples a subset of its local data for
    training. This data may be non-IID, meaning that it may not be uniformly distributed
    across devices.'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**本地数据采样**：设备为其本地数据采样一个子集进行训练。这些数据可能是非-IID的，这意味着它们可能不在设备之间均匀分布。'
- en: '**Local Training**: The device performs a number of training iterations on
    its local data, updating the model parameters based on the computed gradients.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**本地训练**：设备在其本地数据上执行多次训练迭代，根据计算出的梯度更新模型参数。'
- en: '**Model Update**: After local training, the device computes a model update
    (e.g., the difference between the updated and initial parameters) and prepares
    to send it to the server.'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型更新**：在本地训练后，设备计算模型更新（例如，更新后的参数与初始参数之间的差异）并准备将其发送到服务器。'
- en: '**Communication**: The device transmits the model update to the server, typically
    using a secure communication channel to protect user privacy.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通信**：设备通过通常使用安全通信通道来保护用户隐私的方式将模型更新传输到服务器。'
- en: '**Model Aggregation**: The server aggregates the updates from multiple devices
    to produce a new global model, which is then distributed back to the participating
    devices.'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型聚合**：服务器从多个设备聚合更新以生成一个新的全局模型，然后将其分发给参与设备。'
- en: This process is repeated iteratively, with devices periodically downloading
    the latest global model and performing local training. The frequency of these
    updates can vary based on system constraints, device availability, and communication
    costs.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程会迭代重复，设备定期下载最新的全局模型并进行本地训练。这些更新的频率可以根据系统约束、设备可用性和通信成本而变化。
- en: Federated Aggregation Protocols
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 联邦聚合协议
- en: At the heart of federated learning is a coordination mechanism that allows many
    devices, each having access to only a small, local dataset, to collaboratively
    train a shared model. This is achieved through a protocol where client devices
    perform local training and transmit model updates to a central server. The server
    aggregates these updates to refine a global model, which is then redistributed
    to clients for the next training round. This cyclical procedure decouples the
    learning process from centralized data collection, making it well-suited to the
    mobile and edge environments characterized throughout this chapter where user
    data is private, bandwidth is constrained, and device participation is sporadic.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习的核心是一个协调机制，它允许许多设备，每个设备只能访问一个小型本地数据集，共同训练一个共享模型。这是通过一个协议实现的，其中客户端设备在其本地数据上执行本地训练并将模型更新传输到中央服务器。服务器聚合这些更新以细化全局模型，然后将其重新分发给客户端进行下一轮训练。这种循环过程将学习过程与集中式数据收集解耦，非常适合本章中描述的移动和边缘环境，在这些环境中用户数据是私密的，带宽受限，设备参与是间歇性的。
- en: The most widely used baseline for this process is Federated Averaging (FedAvg)[26](#fn26),
    which has become a canonical algorithm for federated learning ([McMahan et al.
    2017c](ch058.xhtml#ref-mcmahan2017communication)). In FedAvg, each device trains
    its local copy of the model using stochastic gradient descent (SGD) on its private
    data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程最广泛使用的基线是联邦平均（FedAvg）[26](#fn26)，它已成为联邦学习的标准算法（[McMahan 等人 2017c](ch058.xhtml#ref-mcmahan2017communication)）。在
    FedAvg 中，每个设备使用其私有数据上的随机梯度下降（SGD）训练其模型的本地副本。
- en: 'Formally, let <semantics><msub><mi>𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_k</annotation></semantics>
    denote the local dataset on client <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>,
    and let <semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation
    encoding="application/x-tex">\theta_k^t</annotation></semantics> be the parameters
    of the model on client <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    at round <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>.
    Each client performs <semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>
    steps of SGD on its local data, yielding an update <semantics><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation
    encoding="application/x-tex">\theta_k^{t+1}</annotation></semantics>. The central
    server then aggregates these updates as: <semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n}
    \theta_k^{t+1}</annotation></semantics> where <semantics><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>𝒟</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">n_k
    = |\mathcal{D}_k|</annotation></semantics> is the number of samples on device
    <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>,
    <semantics><mrow><mi>n</mi><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">n = \sum_k n_k</annotation></semantics> is the total
    number of samples across participating clients, and <semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics> is the number of active
    devices in the current round.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，用<semantics><msub><mi>𝒟</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_k</annotation></semantics>表示客户端<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>上的本地数据集，用<semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation
    encoding="application/x-tex">\theta_k^t</annotation></semantics>表示客户端<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>在第<semantics><mi>t</mi><annotation
    encoding="application/x-tex">t</annotation></semantics>轮次的模型参数。每个客户端在其本地数据上执行<semantics><mi>E</mi><annotation
    encoding="application/x-tex">E</annotation></semantics>步SGD，得到一个更新<semantics><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><annotation
    encoding="application/x-tex">\theta_k^{t+1}</annotation></semantics>。然后中心服务器将这些更新汇总为：<semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msubsup><mi>θ</mi><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n}
    \theta_k^{t+1}</annotation></semantics> 其中 <semantics><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>=</mo><mrow><mo
    stretchy="true" form="prefix">|</mo><msub><mi>𝒟</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">n_k
    = |\mathcal{D}_k|</annotation></semantics> 是设备<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>上的样本数量，<semantics><mrow><mi>n</mi><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">n = \sum_k n_k</annotation></semantics> 是参与客户端的总样本数量，<semantics><mi>K</mi><annotation
    encoding="application/x-tex">K</annotation></semantics>是当前轮次中活跃设备的数量。
- en: 'This cyclical coordination protocol forms the foundation of federated learning,
    as illustrated in [Figure 14.7](ch020.xhtml#fig-federated-averaging-cycle) that
    clarifies the core FedAvg process:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这种循环协调协议构成了联邦学习的基础，如图[图14.7](ch020.xhtml#fig-federated-averaging-cycle)所示，该图阐明了核心FedAvg过程：
- en: '![](../media/file226.svg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file226.svg)'
- en: 'Figure 14.7: **Federated Averaging Cycle**: The four-step coordination protocol
    that enables distributed training while preserving data privacy. (1) Server distributes
    global model to participating clients, (2) Clients train locally on their private
    data using multiple SGD steps, (3) Clients send updated model weights (not raw
    data) back to the server, (4) Server performs weighted averaging of client updates
    to create new global model.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：**联邦平均周期**：四个步骤的协调协议，在保持数据隐私的同时实现分布式训练。（1）服务器将全局模型分发到参与客户端，（2）客户端使用多个SGD步骤在其私有数据上本地训练，（3）客户端将更新的模型权重（而非原始数据）发送回服务器，（4）服务器对客户端的更新进行加权平均，以创建新的全局模型。
- en: 'This basic structure introduces a number of design choices and tradeoffs. The
    number of local steps <semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>
    impacts the balance between computation and communication: larger <semantics><mi>E</mi><annotation
    encoding="application/x-tex">E</annotation></semantics> reduces communication
    frequency but risks divergence if local data distributions vary too much. The
    selection of participating clients affects convergence stability and fairness.
    In real-world deployments, not all devices are available at all times, and hardware
    capabilities may differ substantially, requiring robust participation scheduling
    and failure tolerance.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本结构引入了许多设计选择和权衡。本地步骤的数量<semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics>影响计算和通信之间的平衡：较大的<semantics><mi>E</mi><annotation
    encoding="application/x-tex">E</annotation></semantics>会减少通信频率，但如果本地数据分布差异太大，则存在发散的风险。参与客户端的选择影响收敛稳定性和公平性。在实际部署中，并非所有设备在所有时间都可用，硬件能力可能差异很大，需要强大的参与调度和容错能力。
- en: Client Scheduling
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户端调度
- en: Federated learning operates under the assumption that clients, devices, which
    hold local data, periodically become available for participation in training rounds.
    In real-world systems, client availability is intermittent and variable. Devices
    may be turned off, disconnected from power, lacking network access, or otherwise
    unable to participate at any given time. As a result, client scheduling plays
    a central role in the effectiveness and efficiency of distributed learning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习基于这样的假设：持有本地数据的客户端、设备，会定期可用以参与训练轮次。在现实世界的系统中，客户端的可用性是间歇性和变化的。设备可能会关闭，断开电源，缺乏网络访问，或者在其他情况下无法在任何给定时间参与。因此，客户端调度在分布式学习的有效性和效率中起着核心作用。
- en: At a baseline level, federated ML systems define eligibility criteria for participation.
    Devices must meet minimum requirements such as being plugged in, connected to
    Wi-Fi, and idle, to avoid interfering with user experience or depleting battery
    resources. These criteria determine which subset of the total population is considered
    “available” for any given training round.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本水平上，联邦机器学习系统定义了参与资格标准。设备必须满足最低要求，如插电、连接到Wi-Fi和空闲，以避免干扰用户体验或耗尽电池资源。这些标准决定了在任意训练轮次中被认为是“可用”的总人口子集。
- en: Beyond these operational filters, devices also differ in their hardware capabilities,
    data availability, and network conditions. Some smartphones contain many recent
    examples relevant to the current task, while others have outdated or irrelevant
    data. Network bandwidth and upload speed may vary widely depending on geography
    and carrier infrastructure. As a result, selecting clients at random can lead
    to poor coverage of the underlying data distribution and unstable model convergence.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些操作过滤器之外，设备在硬件能力、数据可用性和网络条件方面也存在差异。一些智能手机包含许多与当前任务相关的最新示例，而其他设备则包含过时或不相关的数据。网络带宽和上传速度可能因地理位置和运营商基础设施而大相径庭。因此，随机选择客户端可能导致对底层数据分布的覆盖不足和模型收敛不稳定。
- en: 'Availability-driven selection introduces participation bias: clients with favorable
    conditions, including frequent charging, high-end hardware, and consistent connectivity,
    are more likely to participate repeatedly, while others are systematically underrepresented.
    This can skew the resulting model toward behaviors and preferences of a privileged
    subset of the population, raising both fairness and generalization concerns.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性驱动的选择引入了参与偏差：具有有利条件（包括频繁充电、高端硬件和稳定的连接）的客户端更有可能反复参与，而其他客户端则可能系统地代表性不足。这可能导致生成的模型偏向于特权子集的行为和偏好，从而引发公平性和泛化问题。
- en: 'The severity of participation bias becomes apparent when examining real deployment
    statistics. Studies of federated learning deployments show that the most active
    10% of devices can contribute to over 50% of training rounds, while the bottom
    50% of devices may never participate at all. This creates a feedback loop: models
    become increasingly optimized for users with high-end devices and stable connectivity,
    potentially degrading performance for resource-constrained users who need adaptation
    the most. A keyboard prediction model might become biased toward the typing patterns
    of users with flagship phones who charge overnight, missing important linguistic
    variations from users with budget devices or irregular charging patterns.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查实际部署统计时，参与偏差的严重性变得明显。联邦学习的部署研究表明，最活跃的10%的设备可以贡献超过50%的训练轮次，而底部的50%的设备可能根本不参与。这形成了一个反馈循环：模型变得越来越优化，以适应高端设备和稳定连接的用户，这可能会降低最需要适应的资源受限用户的性能。一个键盘预测模型可能会偏向于使用旗舰手机并夜间充电的用户的手指打字模式，从而忽略了来自预算设备或非规律充电模式用户的语言变化。
- en: To address these challenges, systems must balance scheduling efficiency with
    client diversity. A key approach involves using stratified or quota-based sampling
    to ensure representative client participation across different groups. Some systems
    implement “fairness budgets” that track cumulative participation and actively
    prioritize underrepresented devices when they become available. Others use importance
    sampling techniques to reweight contributions based on estimated population statistics
    rather than raw participation rates. For instance, asynchronous buffer-based techniques
    allow participating clients to contribute model updates independently, without
    requiring synchronized coordination in every round ([Nguyen et al. 2021](ch058.xhtml#ref-fedbuff)).
    This model has been extended to incorporate staleness awareness ([Rodio and Neglia
    2024](ch058.xhtml#ref-fedstale)) and fairness mechanisms ([J. Ma et al. 2024](ch058.xhtml#ref-fedstaleweight)),
    preventing bias from over-active clients who might otherwise dominate the training
    process.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，系统必须在调度效率和客户多样性之间取得平衡。一个关键的方法是采用分层或配额制的抽样，以确保不同群体中代表性的客户参与。一些系统实施了“公平预算”，以跟踪累积参与情况，并在可用时积极优先考虑代表性不足的设备。其他系统使用重要性抽样技术，根据估计的群体统计数据重新加权贡献，而不是根据原始参与率。例如，基于异步缓冲区的技术允许参与客户独立贡献模型更新，而不需要在每一轮中都进行同步协调（[Nguyen
    等人 2021](ch058.xhtml#ref-fedbuff)）。该模型已被扩展以包含陈旧度感知（[Rodio 和 Neglia 2024](ch058.xhtml#ref-fedstale)）和公平机制（[J.
    Ma 等人 2024](ch058.xhtml#ref-fedstaleweight)），防止过度活跃的客户在训练过程中可能产生的偏差。
- en: To address these challenges, federated ML systems implement adaptive client
    selection strategies. These include prioritizing clients with underrepresented
    data types, targeting geographies or demographics that are less frequently sampled,
    and using historical participation data to enforce fairness constraints. Systems
    incorporate predictive modeling to anticipate future client availability or success
    rates, improving training throughput.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，联邦机器学习系统实施了自适应客户选择策略。这包括优先考虑代表性不足的数据类型的客户，针对较少采样的地理或人口统计特征，以及使用历史参与数据来强制执行公平约束。系统采用预测建模来预测未来客户的可用性或成功率，从而提高训练吞吐量。
- en: Selected clients perform one or more local training steps on their private data
    and transmit their model updates to a central server. These updates are aggregated
    to form a new global model. Typically, this aggregation is weighted, where the
    contributions of each client are scaled, for example, by the number of local examples
    used during training, before averaging. This ensures that clients with more representative
    or larger datasets exert proportional influence on the global model.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 被选中的客户在其私有数据上执行一个或多个本地训练步骤，并将他们的模型更新传输到中央服务器。这些更新被汇总形成一个新的全局模型。通常，这种汇总是加权的，其中每个客户的贡献按比例缩放，例如，在训练期间使用的本地示例数量，然后再进行平均。这确保了具有更代表性或更大数据集的客户对全局模型产生成比例的影响。
- en: These scheduling decisions directly impact system performance. They affect convergence
    rate, model generalization, energy consumption, and overall user experience. Poor
    scheduling can result in excessive stragglers, overfitting to narrow client segments,
    or wasted computation. As a result, client scheduling is not merely a logistical
    concern; it is a core component of system design in federated learning, demanding
    both algorithmic insight and infrastructure-level coordination.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调度决策直接影响系统性能。它们影响收敛速度、模型泛化、能耗和整体用户体验。不良的调度可能导致过多的延迟者、过度拟合到狭窄的客户端段或浪费计算。因此，客户端调度不仅仅是物流问题；它是联邦学习系统设计中一个核心组件，需要算法洞察和基础设施级别的协调。
- en: Bandwidth-Aware Update Compression
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带宽感知更新压缩
- en: One of the principal bottlenecks in federated ML systems is the cost of communication
    between edge clients and the central server. Transmitting full model weights or
    gradients after every training round can overwhelm bandwidth and energy budgets,
    particularly for mobile or embedded devices operating over constrained wireless
    links[27](#fn27). To address this, a range of techniques have been developed to
    reduce communication overhead while preserving learning efficacy.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦机器学习系统中的一个主要瓶颈是边缘客户端和中央服务器之间通信的成本。在每轮训练后传输完整的模型权重或梯度可能会耗尽带宽和能源预算，尤其是对于在受限无线链路上运行的移动或嵌入式设备[27](#fn27)。为了解决这个问题，已经开发了一系列技术来减少通信开销，同时保持学习效率。
- en: 'These techniques fall into three primary categories: model compression, selective
    update sharing, and architectural partitioning.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术主要分为三个类别：模型压缩、选择性更新共享和架构分区。
- en: Model compression methods aim to reduce the size of transmitted updates through
    quantization[28](#fn28), sparsification, or subsampling. Instead of sending full-precision
    gradients, a client transmits 8-bit quantized updates or communicates only the
    top-<semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    gradient elements[29](#fn29) with highest magnitude.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩方法旨在通过量化[28](#fn28)、稀疏化或子采样来减少传输更新的大小。客户端不是发送全精度梯度，而是传输8位量化更新或仅通信具有最高幅度的前<semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>梯度元素[29](#fn29)。
- en: Selective update sharing further reduces communication by transmitting only
    subsets of model parameters or updates. In layer-wise selective sharing, clients
    update only certain layers, typically the final classifier or adapter modules,
    while keeping the majority of the backbone frozen. This reduces both upload cost
    and the risk of overfitting shared representations to non-representative client
    data.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性更新共享通过仅传输模型参数或更新的子集进一步减少通信。在分层选择性共享中，客户端仅更新某些层，通常是最终的分类器或适配器模块，同时保持大多数骨干层冻结。这减少了上传成本和将共享表示过度拟合到非代表性客户端数据的风险。
- en: Split models and architectural partitioning divide the model into a shared global
    component and a private local component. Clients train and maintain their private
    modules independently while synchronizing only the shared parts with the server.
    This allows for user-specific personalization with minimal communication and privacy
    leakage.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 分割模型和架构分区将模型分为共享的全局组件和私有的本地组件。客户端独立训练和维护其私有模块，同时仅与服务器同步共享部分。这允许进行用户特定的个性化，同时最小化通信和隐私泄露。
- en: 'All of these approaches operate within the context of a federated aggregation
    protocol. A standard baseline for aggregation is Federated Averaging (FedAvg),
    in which the server updates the global model by computing a weighted average of
    the client updates received in a given round. Let <semantics><msub><mi>𝒦</mi><mi>t</mi></msub><annotation
    encoding="application/x-tex">\mathcal{K}_t</annotation></semantics> denote the
    set of participating clients in round <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>,
    and let <semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation
    encoding="application/x-tex">\theta_k^t</annotation></semantics> represent the
    locally updated model parameters from client <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>. The server computes the
    new global model <semantics><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation
    encoding="application/x-tex">\theta^{t+1}</annotation></semantics> as: <semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub></mfrac><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k \in \mathcal{K}_t}
    \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t</annotation></semantics>'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都在联邦聚合协议的背景下运行。聚合的标准基线是联邦平均（FedAvg），其中服务器通过计算在给定轮次中接收到的客户端更新的加权平均值来更新全局模型。令
    <semantics><msub><mi>𝒦</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{K}_t</annotation></semantics>
    表示第 <semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics>
    轮次中参与的客户端集合，并令 <semantics><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup><annotation
    encoding="application/x-tex">\theta_k^t</annotation></semantics> 表示客户端 <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics> 的本地更新模型参数。服务器计算新的全局模型
    <semantics><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation
    encoding="application/x-tex">\theta^{t+1}</annotation></semantics> 如下： <semantics><mrow><msup><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub></mfrac><msubsup><mi>θ</mi><mi>k</mi><mi>t</mi></msubsup></mrow>
    <annotation encoding="application/x-tex">\theta^{t+1} = \sum_{k \in \mathcal{K}_t}
    \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t</annotation></semantics>
- en: Here, <semantics><msub><mi>n</mi><mi>k</mi></msub><annotation encoding="application/x-tex">n_k</annotation></semantics>
    is the number of local training examples at client <semantics><mi>k</mi><annotation
    encoding="application/x-tex">k</annotation></semantics>, and <semantics><mrow><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k</annotation></semantics>
    is the total number of training examples across all participating clients. This
    data-weighted aggregation ensures that clients with more training data exert a
    proportionally larger influence on the global model, while also accounting for
    partial participation and heterogeneous data volumes.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<semantics><msub><mi>n</mi><mi>k</mi></msub><annotation encoding="application/x-tex">n_k</annotation></semantics>
    表示客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    的本地训练样本数量，而 <semantics><mrow><msub><mi>n</mi><msub><mi>𝒦</mi><mi>t</mi></msub></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><msub><mi>𝒦</mi><mi>t</mi></msub></mrow></msub><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation
    encoding="application/x-tex">n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k</annotation></semantics>
    表示所有参与客户端的总训练样本数量。这种基于数据权重的聚合确保了拥有更多训练数据的客户端对全局模型产生成比例更大的影响，同时也考虑了部分参与和异构数据量。
- en: However, communication-efficient updates can introduce tradeoffs. Compression
    may degrade gradient fidelity, selective updates can limit model capacity, and
    split architectures may complicate coordination. As a result, effective federated
    learning requires careful balancing of bandwidth constraints, privacy concerns,
    and convergence dynamics—a balance that depends heavily on the capabilities and
    variability of the client population.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通信高效的更新可能会引入权衡。压缩可能会降低梯度精度，选择性更新可能会限制模型容量，而分割架构可能会复杂化协调。因此，有效的联邦学习需要仔细平衡带宽限制、隐私关注和收敛动态——这种平衡在很大程度上取决于客户端群体的能力和多样性。
- en: Federated Personalization
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 联邦个性化
- en: While compression and communication strategies improve scalability, they do
    not address a important limitation of the global federated learning paradigm—its
    inability to capture user-specific variation. In real-world deployments, devices
    often observe distinct and heterogeneous data distributions. A one-size-fits-all
    global model may underperform when applied uniformly across diverse users. This
    motivates the need for personalized federated learning, where local models are
    adapted to user-specific data without compromising the benefits of global coordination.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然压缩和通信策略提高了可扩展性，但它们并没有解决全球联邦学习范式的一个重要限制——无法捕捉用户特定的变化。在实际部署中，设备通常会观察到不同且异构的数据分布。当统一应用于不同用户时，一个通用的全局模型可能会表现不佳。这促使了个性化联邦学习的需求，在这种学习中，本地模型会根据用户特定的数据进行调整，同时不损害全球协调的好处。
- en: 'Let <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\theta_k</annotation></semantics>
    denote the model parameters on client <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>,
    and <semantics><msub><mi>θ</mi><mtext mathvariant="normal">global</mtext></msub><annotation
    encoding="application/x-tex">\theta_{\text{global}}</annotation></semantics> the
    aggregated global model. Traditional FL seeks to minimize a global objective:
    <semantics><mrow><munder><mo>min</mo><mi>θ</mi></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>k</mi></msub><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)</annotation></semantics>
    where <semantics><mrow><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_k(\theta)</annotation></semantics> is
    the local loss on client <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>,
    and <semantics><msub><mi>w</mi><mi>k</mi></msub><annotation encoding="application/x-tex">w_k</annotation></semantics>
    is a weighting factor (e.g., proportional to local dataset size). However, this
    formulation assumes that a single model <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    can serve all users well. In practice, local loss landscapes <semantics><msub><mi>ℒ</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">\mathcal{L}_k</annotation></semantics> often differ
    significantly across clients, reflecting non-IID data distributions and varying
    task requirements.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 令 <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\theta_k</annotation></semantics>
    表示客户端 <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    上的模型参数，以及 <semantics><msub><mi>θ</mi><mtext mathvariant="normal">global</mtext></msub><annotation
    encoding="application/x-tex">\theta_{\text{global}}</annotation></semantics> 表示汇总的全局模型。传统的联邦学习旨在最小化全局目标：<semantics><mrow><munder><mo>min</mo><mi>θ</mi></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>w</mi><mi>k</mi></msub><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow>
    <annotation encoding="application/x-tex">\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)</annotation></semantics>
    其中 <semantics><mrow><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo stretchy="true"
    form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation
    encoding="application/x-tex">\mathcal{L}_k(\theta)</annotation></semantics> 是客户端
    <semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics>
    上的局部损失，而 <semantics><msub><mi>w</mi><mi>k</mi></msub><annotation encoding="application/x-tex">w_k</annotation></semantics>
    是一个权重因子（例如，与本地数据集大小成比例）。然而，这种公式假设单个模型 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>
    可以很好地服务于所有用户。在实践中，局部损失景观 <semantics><msub><mi>ℒ</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">\mathcal{L}_k</annotation></semantics> 在客户端之间往往差异很大，反映了非独立同分布的数据分布和不同的任务需求。
- en: 'Personalization modifies this objective to allow each client to maintain its
    own adapted parameters <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">\theta_k</annotation></semantics>, optimized with
    respect to both the global model and local data: <semantics><mrow><munder><mo>min</mo><mrow><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>K</mi></msub></mrow></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo>⋅</mo><mi>ℛ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo>,</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">global</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\min_{\theta_1,
    \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot
    \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)</annotation></semantics>'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化修改了这个目标，允许每个客户端维护自己的适应参数 <semantics><msub><mi>θ</mi><mi>k</mi></msub><annotation
    encoding="application/x-tex">\theta_k</annotation></semantics>，这些参数与全局模型和本地数据都进行了优化：<semantics><mrow><munder><mo>min</mo><mrow><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>K</mi></msub></mrow></munder><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>ℒ</mi><mi>k</mi></msub><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="true"
    form="postfix">)</mo></mrow><mo>+</mo><mi>λ</mi><mo>⋅</mo><mi>ℛ</mi><mrow><mo
    stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mi>k</mi></msub><mo>,</mo><msub><mi>θ</mi><mtext
    mathvariant="normal">global</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo
    stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\min_{\theta_1,
    \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot
    \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)</annotation></semantics>
- en: Here, <semantics><mi>ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics>
    is a regularization term that penalizes deviation from the global model, and <semantics><mi>λ</mi><annotation
    encoding="application/x-tex">\lambda</annotation></semantics> controls the strength
    of this penalty. This formulation allows local models to deviate as needed, while
    still benefiting from global coordination.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<semantics><mi>ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics>
    是一个正则化项，它惩罚与全局模型的偏差，而 <semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics>
    控制这种惩罚的强度。这种公式允许局部模型根据需要偏离，同时仍然从全局协调中受益。
- en: Real-world use cases illustrate the importance of this approach. Consider a
    wearable health monitor that tracks physiological signals to classify physical
    activities. While a global model may perform reasonably well across the population,
    individual users exhibit unique motion patterns, gait signatures, or sensor placements.
    Personalized finetuning of the final classification layer or low-rank adapters
    allows improved accuracy, particularly for rare or user-specific classes.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用案例说明了这种方法的重要性。考虑一个可穿戴的健康监测器，它通过追踪生理信号来分类身体活动。虽然全局模型可能在整个人群中表现良好，但个别用户表现出独特的运动模式、步态特征或传感器放置。对最终分类层或低秩适配器的个性化微调可以提升准确性，尤其是在处理罕见或特定用户类别时。
- en: Several personalization strategies have emerged to address the tradeoffs between
    compute overhead, privacy, and adaptation speed. One widely used approach is local
    finetuning, in which each client downloads the latest global model and performs
    a small number of gradient steps using its private data. While this method is
    simple and preserves privacy, it may yield suboptimal results when the global
    model is poorly aligned with the client’s data distribution or when the local
    dataset is extremely limited.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了几种个性化策略来解决计算开销、隐私和适应速度之间的权衡。一种广泛使用的方法是本地微调，其中每个客户端下载最新的全局模型，并使用其私有数据执行少量梯度步骤。虽然这种方法简单且保护隐私，但当全局模型与客户端的数据分布不匹配或本地数据集极其有限时，它可能产生次优结果。
- en: Another effective technique involves personalization layers, where the model
    is partitioned into a shared backbone and a lightweight, client-specific head—typically
    the final classification layer ([Arivazhagan et al. 2019](ch058.xhtml#ref-arivazhagan2019federated)).
    Only the head is updated on-device, significantly reducing memory usage and training
    time. This approach is particularly well-suited for scenarios in which the primary
    variation across clients lies in output categories or decision boundaries.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有效技术涉及个性化层，其中模型被划分为共享骨干和轻量级、客户端特定的头部——通常是最终的分类层([Arivazhagan et al. 2019](ch058.xhtml#ref-arivazhagan2019federated))。仅在设备上更新头部，显著减少了内存使用和训练时间。这种方法特别适合于客户端之间主要差异在于输出类别或决策边界的情况。
- en: Clustered federated learning offers an alternative by grouping clients according
    to similarities in their data or performance characteristics, and training separate
    models for each cluster. This strategy can enhance accuracy within homogeneous
    subpopulations but introduces additional system complexity and may require exchanging
    metadata to determine group membership.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 集群联邦学习通过根据客户端数据或性能特征的相似性对客户端进行分组，并为每个集群训练单独的模型，提供了一种替代方案。这种策略可以在同质子群体中提高准确性，但引入了额外的系统复杂性，可能需要交换元数据以确定群体成员资格。
- en: Finally, meta-learning approaches, such as Model-Agnostic Meta-Learning (MAML),
    aim to produce a global model initialization that can be quickly adapted to new
    tasks with just a few local updates ([Finn, Abbeel, and Levine 2017](ch058.xhtml#ref-finn2017model)).
    This technique is especially useful when clients have limited data or operate
    in environments with frequent distributional shifts. Each of these strategies
    reflects a different point in the tradeoff space. These strategies vary in their
    system implications, including compute overhead, privacy guarantees, and adaptation
    latency. [Table 14.5](ch020.xhtml#tbl-personalization-strategies) summarizes the
    tradeoffs.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，元学习方法，如模型无关元学习（MAML），旨在产生一个全局模型初始化，该初始化可以通过仅进行少量本地更新快速适应新任务([Finn, Abbeel,
    and Levine 2017](ch058.xhtml#ref-finn2017model))。当客户端数据有限或在频繁分布变化的环境中操作时，这项技术特别有用。这些策略反映了权衡空间中的不同点。这些策略在系统影响方面有所不同，包括计算开销、隐私保证和适应延迟。[表14.5](ch020.xhtml#tbl-personalization-strategies)总结了权衡。
- en: 'Table 14.5: **Personalization Trade-Offs**: Federated learning strategies balance
    personalization with system costs, impacting compute overhead, privacy preservation,
    and adaptation speed for diverse client populations. This table summarizes how
    local finetuning, clustered learning, and meta-learning each navigate this trade-off
    space, enabling tailored models while considering practical deployment constraints.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.5：**个性化权衡**：联邦学习策略在个性化与系统成本之间取得平衡，影响计算开销、隐私保护和针对不同客户端群体的适应速度。本表总结了本地微调、集群学习和元学习如何各自在这个权衡空间中导航，在考虑实际部署约束的同时实现定制模型。
- en: '| **Strategy** | **Personalization Mechanism** | **Compute Overhead** | **Privacy**
    **Preservation** | **Adaptation Speed** |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| **策略** | **个性化机制** | **计算开销** | **隐私保护** | **适应速度** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Local Finetuning** | Gradient descent on local loss post-aggregation |
    Low to Moderate | High (no data sharing) | Fast (few steps) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **本地微调** | 在聚合后的本地损失上进行梯度下降 | 低到中等 | 高（无数据共享） | 快速（少量步骤） |'
- en: '| **Personalization Layers** | Split model: shared base + user-specific head
    | Moderate | High | Fast (train small head) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| **个性化层** | 分割模型：共享基础 + 用户特定头部 | 中等 | 高 | 快速（训练小型头部） |'
- en: '| **Clustered FL** | Group clients by data similarity, train per group | Moderate
    to High | Medium (group metadata) | Medium |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **集群联邦学习** | 根据数据相似性分组客户端，按组训练 | 中等到高 | 中等（组元数据） | 中等 |'
- en: '| **Meta-Learning** | Train for fast adaptation across tasks/devices | High
    (meta-objective) | High | Very Fast (few-shot) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **元学习** | 训练以快速适应不同任务/设备 | 高（元目标） | 高 | 非常快（少样本） |'
- en: Selecting the appropriate personalization method depends on deployment constraints,
    data characteristics, and the desired balance between accuracy, privacy, and computational
    efficiency. In practice, hybrid approaches that combine elements of multiple strategies,
    including local finetuning atop a personalized head, are often employed to achieve
    robust performance across heterogeneous devices.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当个性化方法取决于部署约束、数据特征以及所需在准确性、隐私和计算效率之间的平衡。在实践中，通常采用结合多种策略的混合方法，包括在个性化头部之上进行本地微调，以在异构设备上实现稳健的性能。
- en: Federated Privacy
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 联邦隐私
- en: While federated learning is often motivated by privacy concerns, as it involves
    keeping raw data localized instead of transmitting it to a central server, the
    paradigm introduces its own set of security and privacy risks. Although devices
    do not share their raw data, the transmitted model updates (such as gradients
    or weight changes) can inadvertently leak information about the underlying private
    data. Techniques such as model inversion attacks and membership inference attacks
    demonstrate that adversaries may partially reconstruct or infer properties of
    local datasets by analyzing these updates.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然联邦学习通常是由隐私关注所驱动，因为它涉及将原始数据本地化而不是传输到中央服务器，但这种范式引入了自己的一套安全和隐私风险。尽管设备不会共享它们的原始数据，但传输的模型更新（如梯度或权重变化）可能会无意中泄露关于底层私有数据的详细信息。模型反演攻击和成员推断攻击等技术表明，攻击者可能通过分析这些更新部分重建或推断本地数据集的属性。
- en: To mitigate such risks, modern federated ML systems commonly employ protective
    measures. Secure Aggregation protocols ensure that individual model updates are
    encrypted and aggregated in a way that the server only observes the combined result,
    not any individual client’s contribution. Differential Privacy[30](#fn30) techniques
    inject carefully calibrated noise into updates to mathematically bound the information
    that can be inferred about any single client’s data.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些风险，现代联邦机器学习系统通常采用保护措施。安全聚合协议确保单个模型更新被加密并以一种方式聚合，使得服务器只能观察到综合结果，而不是任何单个客户端的贡献。差分隐私[30](#fn30)技术将精心校准的噪声注入到更新中，以数学上限制可以推断关于任何单个客户端数据的任何信息。
- en: While these techniques enhance privacy, they introduce additional system complexity
    and tradeoffs between model utility, communication cost, and robustness. A deeper
    exploration of these attacks, defenses, and their implications requires dedicated
    coverage of security principles in distributed ML systems.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些技术增强了隐私性，但它们引入了额外的系统复杂性，并在模型效用、通信成本和鲁棒性之间引入了权衡。对这些攻击、防御及其影响的更深入探索需要专门覆盖分布式机器学习系统中的安全原则。
- en: Large-Scale Device Orchestration
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大规模设备编排
- en: Federated learning transforms machine learning into a massive distributed systems
    challenge that extends far beyond traditional algorithmic considerations. Coordinating
    thousands or millions of heterogeneous devices with intermittent connectivity
    requires sophisticated distributed systems protocols that handle Byzantine failures,
    network partitions, and communication efficiency at unprecedented scale. These
    challenges fundamentally differ from the controlled environments of data center
    distributed training, where high-bandwidth networks and reliable infrastructure
    enable straightforward coordination protocols.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习将机器学习转化为一个巨大的分布式系统挑战，这远远超出了传统的算法考虑。协调数千或数百万具有间歇性连接的异构设备需要复杂的分布式系统协议，这些协议能够处理拜占庭故障、网络分区和前所未有的通信效率。这些挑战与数据中心分布式训练的受控环境根本不同，在那里高带宽网络和可靠的基础设施使得简单的协调协议变得简单。
- en: Network and Bandwidth Optimization
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络和带宽优化
- en: The communication bottleneck represents the primary scalability constraint in
    federated learning systems. Understanding the quantitative transfer requirements
    enables principled design decisions about model architectures, update compression
    strategies, and client participation policies that determine system viability.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 通信瓶颈是联邦学习系统中主要的可扩展性约束。理解定量传输需求使得关于模型架构、更新压缩策略和确定系统可行性的客户端参与策略的设计决策具有原则性。
- en: The federated communication hierarchy reveals the severe bandwidth constraints
    under which distributed learning must operate. Full model synchronization requires
    10-500 MB per training round for typical deep learning models—prohibitive for
    mobile networks with limited upload bandwidth that averages just 5-50 Mbps in
    practice. Gradient compression achieves 10-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> reduction through
    quantization (reducing FP32 to INT8), sparsification (transmitting only non-zero
    gradients), and selective gradient transmission (sending only the most significant
    updates). Practical deployments demand even more aggressive 100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics> compression ratios,
    reducing 100 MB models to manageable 100 KB-1 MB updates that mobile devices can
    transmit within reasonable timeframes and without exhausting data plans. Communication
    frequency introduces a critical trade-off between model update freshness—more
    frequent updates enable faster adaptation to changing conditions—and network efficiency
    constraints that limit sustainable bandwidth consumption.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦通信层次结构揭示了分布式学习必须运行的严重带宽限制。对于典型的深度学习模型，每次训练轮次需要10-500 MB的全模型同步——这在实际中对于平均上传带宽仅为5-50
    Mbps的有限上传带宽的移动网络来说是不可行的。梯度压缩通过量化（将FP32减少到INT8）、稀疏化（仅传输非零梯度）和选择性梯度传输（仅发送最重要的更新）实现了10-100<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的压缩。实际部署需要更加激进的100-1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>压缩比率，将100 MB的模型压缩到可管理的100
    KB-1 MB更新，这样移动设备可以在合理的时间内传输，而不会耗尽数据计划。通信频率在模型更新新鲜度和网络效率限制之间引入了一个关键的权衡——更频繁的更新能够更快地适应变化条件，但同时也限制了可持续的带宽消耗。
- en: 'Network infrastructure constraints directly impact participation rates and
    overall system viability. Modern 4G networks typically provide upload speeds ranging
    from 5-50 Mbps under optimal conditions (with significant geographic and carrier
    variation), meaning an 8 MB model update requires 1.3-13 seconds of sustained
    transmission. However, real-world mobile networks exhibit extreme variability:
    rural areas may experience 1 Mbps upload speeds while urban 5G deployments enable
    100+ Mbps. This 100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    variance in network capability necessitates adaptive communication strategies
    that optimize for lowest-common-denominator connectivity while enabling high-capability
    devices to contribute more effectively.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 网络基础设施的限制直接影响到参与率和整个系统的可行性。现代4G网络在最佳条件下通常提供5-50 Mbps的上传速度（存在显著的地理和运营商差异），这意味着8
    MB的模型更新需要1.3-13秒的持续传输。然而，现实世界的移动网络表现出极大的可变性：农村地区可能只有1 Mbps的上传速度，而城市5G部署则能实现100+
    Mbps。这种100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>的网络能力差异需要自适应的通信策略，以优化最低共同连接性，同时使高能力设备能够更有效地做出贡献。
- en: 'The relationship between communication requirements and participation rates
    exhibits sharp threshold effects. Empirical studies demonstrate that federated
    learning systems requiring model transfers exceeding 10 MB achieve less than 10%
    sustained client participation, while systems maintaining updates below 1 MB can
    sustain 40-60% participation rates across diverse mobile populations. This communication
    efficiency directly translates to model quality improvements: higher participation
    rates provide better statistical diversity and more robust gradient estimates
    for global model updates.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 通信需求与参与率之间的关系表现出明显的阈值效应。实证研究表明，需要超过10 MB模型转移的联邦学习系统，其持续客户端参与率低于10%，而保持更新低于1
    MB的系统可以在多样化的移动人群中维持40-60%的参与率。这种通信效率直接转化为模型质量的提升：更高的参与率提供了更好的统计多样性和更稳健的全局模型更新梯度估计。
- en: Advanced compression techniques become essential for practical deployment. Gradient
    quantization reduces precision from FP32 to INT8 or even binary representations,
    achieving 4-32<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    compression with minimal accuracy loss. Sparsification techniques transmit only
    the largest gradient components, leveraging the natural sparsity in neural network
    updates. Top-k gradient selection further reduces communication by transmitting
    only the most significant parameter updates, while error accumulation ensures
    that small gradients are not permanently lost.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 高级压缩技术对于实际部署变得至关重要。梯度量化将精度从FP32降低到INT8甚至二进制表示，实现了4-32<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的压缩，同时最小化精度损失。稀疏化技术只传输最大的梯度分量，利用神经网络更新中的自然稀疏性。Top-k梯度选择进一步减少通信，只传输最重要的参数更新，而误差累积确保小梯度不会永久丢失。
- en: Asynchronous Device Synchronization
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异步设备同步
- en: Federated learning operates at the complex intersection of distributed systems
    and machine learning, inheriting fundamental challenges from both domains while
    introducing unique complications that arise from the mobile, heterogeneous, and
    unreliable nature of edge devices.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习在分布式系统和机器学习的复杂交叉点上运行，既继承了这两个领域的根本挑战，又引入了由边缘设备的移动性、异构性和不可靠性带来的独特复杂性。
- en: Federated learning must contend with Byzantine fault tolerance requirements
    that extend beyond typical distributed systems challenges. Device failures occur
    frequently as clients crash, lose power, or disconnect during training rounds
    due to battery depletion or network connectivity issues—far more common than server
    failures in traditional distributed training. Malicious updates present security
    concerns as adversarial clients can provide corrupted gradients deliberately designed
    to degrade global model performance or extract private information from the aggregation
    process. Robust aggregation protocols implementing Byzantine-resilient averaging
    ensure system reliability despite the presence of compromised or unreliable participants,
    though these protocols introduce significant computational overhead. Consensus
    mechanisms must coordinate millions of unreliable participants without the overhead
    of traditional distributed consensus protocols like Paxos or Raft, which were
    designed for small clusters of reliable servers.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习必须应对比典型分布式系统挑战更广泛的拜占庭容错要求。设备故障频繁发生，因为客户端在训练轮次中崩溃、断电或断开连接，这比传统分布式训练中的服务器故障更为常见。恶意更新带来了安全担忧，因为敌对客户端可以故意提供损坏的梯度，这些梯度旨在降低全局模型性能或从聚合过程中提取私人信息。实现拜占庭容错平均的鲁棒聚合协议确保了系统可靠性，尽管这些协议引入了显著的计算开销。共识机制必须在没有传统分布式共识协议（如Paxos或Raft）开销的情况下协调数百万不可靠的参与者，这些协议是为小型可靠服务器集群设计的。
- en: Network partitions pose particularly acute challenges for federated coordination
    protocols. Unlike traditional distributed systems operating within reliable data
    center networks, federated learning must gracefully handle prolonged client disconnection
    events where devices may remain offline for hours or days while traveling, in
    poor coverage areas, or simply powered down. Asynchronous coordination protocols
    enable continued training progress despite missing participants, but must carefully
    balance staleness (accepting potentially outdated contributions) against freshness
    (prioritizing recent but potentially sparse updates).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 网络分区对联邦协调协议提出了特别严峻的挑战。与传统在可靠数据中心网络中运行的分布式系统不同，联邦学习必须优雅地处理长时间客户端断开事件，在这些事件中，设备可能因旅行、在覆盖较差的区域或简单断电而离线数小时或数天。异步协调协议能够在缺少参与者的情况下继续训练进度，但必须仔细平衡陈旧性（接受可能过时的贡献）与新鲜性（优先处理最近但可能稀疏的更新）。
- en: Fault recovery and resilience strategies form an essential layer of federated
    learning infrastructure. Checkpoint synchronization through periodic global model
    snapshots enables recovery from server failures and provides rollback points when
    corrupted training rounds are detected, though checkpointing large models across
    millions of devices introduces substantial storage and communication overhead.
    Partial update handling ensures systems gracefully handle incomplete training
    rounds when significant subsets of clients fail or disconnect mid-training, requiring
    careful weighting strategies to prevent bias toward more reliable device cohorts.
    State reconciliation protocols enable clients rejoining after extended offline
    periods—potentially days or weeks—to efficiently resynchronize with the current
    global model while minimizing communication overhead that could overwhelm bandwidth-constrained
    devices. Dynamic load balancing addresses uneven client availability patterns
    that create computational hotspots, requiring intelligent load redistribution
    across available participants to maintain training throughput despite time-varying
    participation rates.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 故障恢复和弹性策略是联邦学习基础设施的一个基本层。通过定期全局模型快照进行检查点同步，可以在服务器故障时进行恢复，并在检测到损坏的训练轮次时提供回滚点，尽管在数百万设备上检查点同步大型模型会引入大量的存储和通信开销。部分更新处理确保系统在大量客户端在训练过程中失败或断开连接时能够优雅地处理不完整的训练轮次，需要仔细的加权策略来防止偏向更可靠的设备群体。状态协调协议使客户端在长时间离线后（可能是几天或几周）能够高效地与当前全局模型重新同步，同时最小化可能压倒带宽受限设备的通信开销。动态负载均衡解决不均匀的客户端可用性模式，这些模式会创建计算热点，需要智能地在可用的参与者之间重新分配负载，以维持训练吞吐量，尽管参与率随时间变化。
- en: The asynchronous nature of federated coordination introduces additional complexity
    in maintaining training convergence guarantees. Traditional synchronous training
    assumes all participants complete each round, but federated systems must handle
    stragglers and dropouts gracefully. Techniques such as FedAsync[31](#fn31) enable
    asynchronous aggregation where the server continuously updates the global model
    as client updates arrive, while bounded staleness mechanisms prevent extremely
    outdated updates from corrupting recent progress.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦协调的异步性质在保持训练收敛保证方面引入了额外的复杂性。传统的同步训练假设所有参与者都完成每一轮，但联邦系统必须优雅地处理落后者和退出者。例如，FedAsync[31](#fn31)这样的技术可以实现异步聚合，其中服务器在客户端更新到达时持续更新全局模型，而有限的陈旧机制防止过时的更新破坏最近的进展。
- en: Managing Million-Device Heterogeneity
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 管理百万设备异构性
- en: 'Real-world federated learning deployments exhibit extreme heterogeneity across
    multiple dimensions simultaneously: hardware capabilities, network conditions,
    data distributions, and availability patterns. This multi-dimensional heterogeneity
    fundamentally challenges traditional distributed machine learning assumptions
    about homogeneous participants operating under similar conditions.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界的联邦学习部署在多个维度上同时表现出极端的异构性：硬件能力、网络条件、数据分布和可用性模式。这种多维异构性从根本上挑战了传统分布式机器学习关于在相似条件下操作的同质参与者假设。
- en: Real-world federated learning deployments face multi-dimensional device heterogeneity
    that creates extreme variation across every system dimension. Computational variation
    spans 1000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    differences in processing power between flagship smartphones running at 35 TOPS
    and IoT microcontrollers operating at just 0.03 TOPS, fundamentally limiting what
    models can train on different device tiers. Memory constraints exhibit even more
    dramatic 100-10,000<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    differences in available RAM across device categories, ranging from 256KB on microcontrollers
    to 16 GB on premium smartphones, determining whether devices can perform any local
    training at all or must rely purely on inference. Energy limitations force training
    sessions to be carefully scheduled around charging patterns, thermal constraints,
    and battery preservation requirements, with mobile devices typically limiting
    ML workloads to 500-1000 mW sustained power consumption. Network diversity introduces
    orders-of-magnitude performance differences as WiFi, 4G, 5G, and satellite connectivity
    exhibit vastly different bandwidth (ranging from 1 Mbps to 1 Gbps), latency (10 ms
    to 600 ms), and reliability characteristics that determine feasible update frequencies
    and compression requirements.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的联邦学习部署面临着多维度的设备异质性，这导致每个系统维度都存在极端的差异性。计算差异跨越了1000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的差异，这是在35 TOPS运行的旗舰智能手机和仅运行在0.03
    TOPS的物联网微控制器之间的处理能力差异，从根本上限制了可以在不同设备层级上训练的模型。内存限制在设备类别之间表现出更为显著的100-10,000<semantics><mi>×</mi><annotation
    encoding="application/x-tex">\times</annotation></semantics>的差异，从微控制器上的256KB到高端智能手机上的16
    GB不等，这决定了设备是否可以进行任何本地训练，或者必须完全依赖推理。能量限制迫使训练会话必须围绕充电模式、热约束和电池保护要求进行仔细安排，移动设备通常将机器学习工作负载限制在500-1000 mW的持续功耗。网络多样性引入了数量级的性能差异，因为WiFi、4G、5G和卫星连接表现出截然不同的带宽（从1
    Mbps到1 Gbps）、延迟（10 ms到600 ms）和可靠性特征，这些特征决定了可行的更新频率和压缩要求。
- en: Adaptive coordination protocols address this heterogeneity through sophisticated
    tiered participation strategies that optimize resource utilization across the
    device spectrum. High-capability devices such as flagship smartphones can perform
    complex local training with large batch sizes and multiple epochs, while resource-constrained
    IoT devices contribute through lightweight updates, specialized subtasks, or even
    simple data aggregation. This creates a natural computational hierarchy where
    powerful devices act as “super-peers” performing disproportionate computation,
    while edge devices contribute specialized local knowledge and coverage.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应协调协议通过复杂的分层参与策略解决这种异质性，优化了整个设备谱系中的资源利用率。高能力设备，如旗舰智能手机，可以执行带有大批量数据和多个时代的复杂本地训练，而资源受限的物联网设备则通过轻量级更新、专用子任务或甚至简单的数据聚合来做出贡献。这创造了一个自然的计算层次结构，其中强大的设备充当“超级对等节点”，执行不成比例的计算，而边缘设备则贡献专门的本地知识和覆盖范围。
- en: The scale challenges extend far beyond device heterogeneity to fundamental coordination
    overhead limitations. Traditional distributed consensus algorithms such as Raft
    or PBFT are designed for dozens of nodes in controlled environments, but federated
    learning requires coordination among millions of participants across unreliable
    networks. This necessitates hierarchical coordination architectures where regional
    aggregation servers reduce communication overhead by performing local consensus
    before contributing to global aggregation. Edge computing infrastructure provides
    natural hierarchical coordination points, enabling federated learning systems
    to leverage existing content delivery networks (CDNs) and mobile edge computing
    (MEC) deployments for efficient gradient aggregation.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 规模挑战远远超出了设备异质性，扩展到了基本的协调开销限制。传统的分布式共识算法，如Raft或PBFT，是为受控环境中的数十个节点设计的，但联邦学习需要在不可靠网络上协调数百万参与者。这需要分层协调架构，其中区域聚合服务器通过在贡献全球聚合之前执行本地共识来减少通信开销。边缘计算基础设施提供了自然的分层协调点，使联邦学习系统能够利用现有的内容分发网络（CDNs）和移动边缘计算（MEC）部署，以实现高效的梯度聚合。
- en: 'Modern federated systems implement sophisticated client selection strategies
    that balance statistical diversity with practical constraints. Random sampling
    ensures unbiased representation but may select many low-capability devices, while
    capability-based selection improves training efficiency but risks statistical
    bias. Hybrid approaches use stratified sampling across device tiers, ensuring
    both statistical representativeness and computational efficiency. These selection
    strategies must also consider temporal patterns: office workers’ devices may be
    available during specific hours, while IoT sensors provide continuous but limited
    computational resources.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现代联邦系统实施复杂的客户端选择策略，在统计多样性和实际约束之间取得平衡。随机抽样确保了无偏代表性，但可能会选择许多低能力设备，而基于能力的选择可以提高训练效率，但存在统计偏差的风险。混合方法在设备层级之间使用分层抽样，确保了统计代表性以及计算效率。这些选择策略还必须考虑时间模式：办公室工作人员的设备可能在特定时间段内可用，而物联网传感器提供连续但有限的计算资源。
- en: Production Integration
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产集成
- en: The theoretical foundation established earlier—model adaptation strategies,
    data efficiency techniques, and federated coordination algorithms—provides the
    building blocks for on-device learning systems. However, translating these individual
    components into production-ready systems requires addressing integration challenges
    that cut across all constraint dimensions simultaneously.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 之前建立的理论基础——模型适应性策略、数据效率技术和联邦协调算法——为设备上学习系统提供了构建块。然而，将这些单个组件转换为生产就绪系统需要解决跨越所有约束维度的集成挑战。
- en: Real-world deployment introduces systemic complexity that exceeds the sum of
    individual techniques. Model adaptation, data efficiency, and federated coordination
    must work together seamlessly rather than as independent optimizations. Different
    learning strategies have varying computational and memory profiles that must be
    coordinated within overall device budgets. Training, inference, and communication
    must be scheduled carefully to avoid interference with user experience and system
    stability. Unlike centralized systems with observable training loops, on-device
    learning requires distributed validation and failure detection mechanisms that
    operate across heterogeneous device populations.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的部署引入了超过单个技术总和的系统复杂性。模型适应性、数据效率和联邦协调必须无缝协作，而不是作为独立的优化。不同的学习策略具有不同的计算和内存配置文件，必须在整体设备预算内进行协调。训练、推理和通信必须仔细安排，以避免干扰用户体验和系统稳定性。与具有可观察训练循环的集中式系统不同，设备上学习需要分布式验证和故障检测机制，这些机制在异构设备群体中运行。
- en: This transition from theory to practice requires systematic engineering approaches
    that balance competing constraints while maintaining system reliability. Successful
    on-device learning deployments depend not on individual algorithmic improvements
    but on holistic system designs that orchestrate multiple techniques within operational
    constraints. The subsequent sections examine how production systems address these
    integration challenges through principled design patterns, operational practices,
    and monitoring strategies that enable scalable, reliable on-device learning deployment.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从理论到实践的转变需要系统性的工程方法，在平衡相互竞争的约束条件的同时保持系统可靠性。成功的设备上学习部署不依赖于单个算法的改进，而依赖于整体系统设计，该设计在操作约束内协调多种技术。接下来的部分将探讨生产系统如何通过原则性的设计模式、操作实践和监控策略来应对这些集成挑战，这些策略使得可扩展、可靠的设备上学习部署成为可能。
- en: MLOps Integration Challenges
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLOps集成挑战
- en: Integrating on-device learning into existing MLOps workflows requires extending
    the operational frameworks established in [Chapter 13](ch019.xhtml#sec-ml-operations)
    to handle distributed training, heterogeneous devices, and privacy-preserving
    coordination. The continuous integration pipelines, model versioning systems,
    and monitoring infrastructure discussed in the preceding chapter provide essential
    foundations, but must be adapted to address unique edge deployment challenges.
    Standard MLOps pipelines assume centralized data access, controlled deployment
    environments, and unified monitoring capabilities that do not directly apply to
    edge learning scenarios, requiring new approaches to the technical debt management
    and operational excellence principles established earlier.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 将设备上学习集成到现有的MLOps工作流程中需要扩展[第13章](ch019.xhtml#sec-ml-operations)中建立的运营框架，以处理分布式训练、异构设备和隐私保护协调。前一章中讨论的持续集成管道、模型版本控制系统和监控基础设施提供了基本基础，但必须适应以解决独特的边缘部署挑战。标准MLOps管道假设集中式数据访问、受控部署环境和统一的监控能力，这些能力不直接适用于边缘学习场景，需要新的方法来管理早期建立的债务管理和技术卓越原则。
- en: Deployment Pipeline Transformations
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署流程转换
- en: 'Traditional MLOps deployment pipelines from [Chapter 13](ch019.xhtml#sec-ml-operations)
    follow a standardized CI/CD process: model training, validation, staging, and
    production deployment of a single model artifact to uniform infrastructure. On-device
    learning requires device-aware deployment pipelines that distribute different
    adaptation strategies across heterogeneous device tiers. Microcontrollers receive
    bias-only updates, mid-range phones use LoRA adapters, and flagship devices perform
    selective layer updates. The deployment artifact evolves from a static model file
    to a collection of adaptation policies, initial model weights, and device-specific
    optimization configurations.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 传统MLOps部署流程来自[第13章](ch019.xhtml#sec-ml-operations)，遵循标准化的CI/CD流程：单个模型实体的模型训练、验证、预部署和生产部署到统一的基础设施。设备上学习需要设备感知的部署流程，该流程在不同异构设备层级上分配不同的适应策略。微控制器接收仅偏置的更新，中端手机使用LoRA适配器，而旗舰设备执行选择性层更新。部署实体从静态模型文件演变为一系列适应策略、初始模型权重和设备特定的优化配置。
- en: This architectural shift necessitates extending traditional deployment pipelines
    with device capability detection, strategy selection logic, and tiered deployment
    orchestration that maintains the reliability guarantees of conventional MLOps
    while accommodating unprecedented deployment diversity.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构转变需要扩展传统部署流程，包括设备能力检测、策略选择逻辑和分层部署编排，以保持传统MLOps的可靠性保证，同时适应前所未有的部署多样性。
- en: This transformation introduces new complexity in version management. While centralized
    systems maintain a single model version, on-device learning systems must simultaneously
    track multiple versioning dimensions. The pre-trained backbone distributed to
    all devices represents the base model version, which serves as the foundation
    for all local adaptations. Different update mechanisms deployed per device class
    constitute adaptation strategies, varying from simple bias adjustments on microcontrollers
    to full layer fine-tuning on flagship devices. Local model states naturally diverge
    from the base as devices encounter unique data distributions, creating device-specific
    checkpoints that reflect individual adaptation histories. Finally, federated learning
    rounds that periodically synchronize device populations establish aggregation
    epochs, marking discrete points where distributed knowledge converges into updated
    global models. Successful deployments implement tiered versioning schemes where
    base models evolve slowly—typically through monthly updates—while local adaptations
    occur continuously, creating a hierarchical version space rather than the linear
    version history familiar from traditional deployments.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转变在版本管理中引入了新的复杂性。虽然集中式系统维护单个模型版本，但设备上的学习系统必须同时跟踪多个版本维度。分发到所有设备的预训练骨干代表基础模型版本，它作为所有本地适应的基础。针对每个设备类别部署的不同更新机制构成了适应策略，从微控制器上的简单偏差调整到旗舰设备上的完整层微调。随着设备遇到独特的数据分布，本地模型状态自然会与基础模型分离，创建反映个别适应历史的设备特定检查点。最后，定期同步设备群体的联邦学习轮次建立了聚合时期，标志着分布式知识汇聚到更新后的全局模型的离散点。成功的部署实施分层版本方案，其中基础模型缓慢演变——通常通过每月更新——而本地适应则持续进行，从而创建了一个类似于传统部署中熟悉的线性版本历史的高级版本空间。
- en: Monitoring System Evolution
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监控系统演变
- en: '[Chapter 13](ch019.xhtml#sec-ml-operations) established monitoring practices
    that aggregate metrics from centralized inference servers. On-device learning
    monitoring must operate within fundamentally different constraints that reshape
    how systems observe, measure, and respond to model behavior across distributed
    device populations.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13章](ch019.xhtml#sec-ml-operations) 建立了从集中式推理服务器聚合指标的性能监控实践。设备上的学习监控必须在本质上不同的约束条件下运行，这些约束条件重塑了系统如何观察、测量以及对分布式设备群体中的模型行为做出反应的方式。'
- en: Privacy-preserving telemetry represents the first fundamental departure from
    traditional monitoring. Collecting performance metrics without compromising user
    privacy requires federated analytics where devices share only aggregate statistics
    or differentially private summaries. Systems cannot simply log individual predictions
    or training samples as centralized systems do. Instead, devices report distribution
    summaries such as mean accuracy and confidence histograms rather than per-example
    metrics. All reported statistics must include differential privacy guarantees
    that bound information leakage through carefully calibrated noise addition. Secure
    aggregation protocols prevent the server from observing individual device contributions,
    ensuring that even the aggregation process itself cannot reconstruct private information
    from any single device’s data.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护遥测代表了与传统监控的第一个根本性区别。在不损害用户隐私的情况下收集性能指标需要联邦分析，其中设备仅共享汇总统计信息或差分隐私摘要。系统不能像集中式系统那样简单地记录单个预测或训练样本。相反，设备报告分布摘要，如平均准确性和置信度直方图，而不是每个实例的指标。所有报告的统计数据都必须包含差分隐私保证，这些保证通过精心校准的噪声添加来限制信息泄露。安全聚合协议防止服务器观察到单个设备的贡献，确保即使聚合过程本身也无法从任何单个设备的数据中重建隐私信息。
- en: Drift detection presents additional challenges without access to ground truth
    labels. Traditional monitoring compares model predictions against labeled validation
    sets maintained on centralized infrastructure. On-device systems must detect drift
    using only local signals available during deployment. Confidence calibration tracks
    whether predicted probabilities match empirical frequencies, detecting degradation
    when the model’s confidence estimates become poorly calibrated to actual outcomes.
    Input distribution monitoring detects when feature distributions shift from training
    data through statistical techniques that require no labels. Task performance proxies
    leverage implicit feedback such as user corrections or task abandonment as quality
    signals that indicate when the model fails to meet user needs. Shadow baseline
    comparison runs a frozen base model alongside the adapted model to measure divergence,
    flagging cases where local adaptation degrades rather than improves performance
    relative to the known-good baseline.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移检测在没有访问到地面实况标签的情况下提出了额外的挑战。传统的监控将模型预测与维护在集中式基础设施上的标记验证集进行比较。设备上的系统必须在部署期间仅使用本地信号来检测漂移。置信度校准跟踪预测概率是否与经验频率相匹配，当模型的置信度估计与实际结果校准不良时，检测到退化。输入分布监控通过不需要标签的统计技术检测特征分布从训练数据中发生的变化。任务性能代理利用隐式反馈，如用户更正或任务放弃，作为质量信号，指示模型未能满足用户需求。阴影基线比较在适配模型旁边运行冻结的基础模型，以测量偏差，标记出本地适应相对于已知良好的基线性能下降而不是提高的情况。
- en: 'Heterogeneous performance tracking addresses a third critical challenge: global
    averages mask critical failures when device populations exhibit high variance.
    Monitoring systems must segment performance across multiple dimensions to identify
    systematic issues that affect specific device cohorts. Capability-based performance
    gaps reveal when flagship devices achieve substantially better results than budget
    devices, indicating that adaptation strategies may need adjustment for resource-constrained
    hardware. Regional bias issues surface when models perform well in some geographic
    markets but poorly in others, potentially reflecting data distribution shifts
    or cultural factors not captured during initial training. Temporal patterns emerge
    when performance degrades for devices running stale base models that have not
    received recent updates from federated aggregation. Participation inequality becomes
    visible when comparing devices that adapt frequently against those that rarely
    participate in training, revealing potential fairness issues in how learning benefits
    are distributed across the user population.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 异构性能跟踪解决了一个第三大关键挑战：当设备群体表现出高方差时，全局平均值会掩盖关键故障。监控系统必须在多个维度上分割性能，以识别影响特定设备群体的系统性问题。基于能力的性能差距表明，旗舰设备与预算设备相比取得了显著更好的结果，这表明针对资源受限的硬件，适应性策略可能需要调整。当模型在某些地理市场上表现良好而在其他市场上表现不佳时，区域偏差问题就会显现出来，这可能反映了数据分布的变化或文化因素，这些因素在初始训练期间没有被捕捉到。当运行过时的基础模型且未从联邦聚合中接收最近更新的设备性能下降时，就会出现时间模式。当比较经常适应的设备与很少参与训练的设备时，参与不平等变得明显，这揭示了学习收益在用户群体中分配的潜在公平性问题。
- en: Continuous Training Orchestration
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 持续训练编排
- en: Traditional continuous training covered in [Chapter 13](ch019.xhtml#sec-ml-operations)
    executes scheduled retraining jobs on centralized infrastructure with predictable
    resource availability and coordinated execution. On-device learning transforms
    this into continuous distributed training where millions of devices train independently
    without global synchronization, creating orchestration challenges that require
    fundamentally different coordination strategies.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13章中介绍的传统的持续训练在具有可预测资源可用性和协调执行的集中式基础设施上执行计划中的重新训练作业。设备上的学习将此转变为持续分布式训练，其中数百万台设备独立训练，无需全局同步，从而产生了需要根本不同的协调策略的编排挑战。
- en: Asynchronous device coordination represents the first major departure from centralized
    training. Millions of devices train independently on their local data, but the
    orchestration system cannot rely on synchronized participation. Only 20-40% of
    devices are typically available in any training round due to network connectivity
    limitations, battery constraints, and varying usage patterns. The system must
    exhibit straggler tolerance, ensuring that slow devices on limited hardware or
    poor network connections cannot block faster devices from progressing with their
    local adaptations. Devices often operate on different base model versions simultaneously,
    creating version skew that the aggregation protocol must handle gracefully without
    forcing all devices to maintain identical model states. State reconciliation becomes
    necessary when devices reconnect after extended offline periods—potentially days
    or weeks—requiring the system to integrate their accumulated local adaptations
    despite having missed multiple federated aggregation rounds.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 异步设备协调代表了从集中式训练的第一个重大转变。数百万台设备在其本地数据上独立训练，但编排系统不能依赖于同步参与。由于网络连接限制、电池约束和不同的使用模式，通常只有20-40%的设备在任何一轮训练中可用。系统必须表现出对落后者的容忍度，确保在有限硬件或较差网络连接上的慢速设备不会阻碍快速设备进行其本地适应。设备通常同时运行在不同的基础模型版本上，这会创建版本偏差，聚合协议必须优雅地处理，而无需强制所有设备保持相同的模型状态。当设备在长时间离线后重新连接时——可能是几天或几周——状态协调变得必要，需要系统整合它们累积的本地适应，尽管它们已经错过了多次联邦聚合轮次。
- en: Resource-aware scheduling ensures that training respects both device constraints
    and user experience. Orchestration policies implement opportunistic training windows
    that execute adaptation only when the device is idle, charging, and connected
    to WiFi, avoiding interference with active user tasks or consuming metered cellular
    data. Thermal budgets suspend training when device temperature exceeds manufacturer-specified
    thresholds, preventing user discomfort and hardware damage from sustained computational
    loads. Battery preservation policies limit training energy consumption to less
    than 5% of battery capacity per day, ensuring that on-device learning does not
    noticeably impact device runtime from the user’s perspective. Network-aware communication
    compresses model updates aggressively when devices must use metered connections,
    trading computational overhead for reduced bandwidth consumption to minimize user
    data charges.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 资源感知调度确保训练过程既尊重设备限制，也考虑用户体验。编排策略实施机会主义训练窗口，仅在设备空闲、充电且连接到WiFi时执行适应，避免干扰活跃用户任务或消耗计费蜂窝数据。当设备温度超过制造商指定的阈值时，热预算会暂停训练，防止因持续的计算负载导致用户不适和硬件损坏。电池保护策略将训练能耗限制在每天不超过电池容量的5%，确保设备上的学习不会明显影响用户视角下的设备运行时间。网络感知通信在设备必须使用计费连接时，会积极压缩模型更新，以减少带宽消耗，从而降低用户数据费用，以换取计算开销的降低。
- en: Convergence assessment without global visibility poses the final orchestration
    challenge. Traditional training monitors loss curves on centralized validation
    sets, providing clear signals about training progress and convergence. Distributed
    training must assess convergence through indirect signals aggregated across the
    device population. Federated evaluation aggregates validation metrics from devices
    that maintain local held-out sets, providing approximate measures of global model
    quality despite incomplete device participation. Update magnitude tracking monitors
    how much local gradients change the global model in each aggregation round, with
    diminishing update sizes signaling potential convergence. Participation diversity
    ensures broad device representation in aggregated updates, preventing convergence
    metrics from reflecting only a narrow subset of the deployment environment. Temporal
    consistency detects when model improvements plateau across multiple aggregation
    rounds, indicating that the current adaptation strategy has exhausted its potential
    gains and may require adjustment.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 没有全局可见性的收敛评估构成了最终的编排挑战。传统的训练监控在集中式验证集上损失曲线，提供关于训练进度和收敛的明确信号。分布式训练必须通过跨设备群体汇总的间接信号来评估收敛。联邦评估从维护本地保留集的设备中汇总验证指标，尽管设备参与不完全，但提供对全局模型质量的近似度量。更新幅度跟踪监控每个聚合轮次中本地梯度对全局模型的影响程度，更新大小的减少表明可能已收敛。参与多样性确保在汇总更新中广泛代表设备，防止收敛指标仅反映部署环境的狭窄子集。时间一致性检测模型改进在多个聚合轮次中达到平台期，表明当前适应性策略已耗尽其潜在收益，可能需要调整。
- en: Validation Strategy Adaptation
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证策略适应性
- en: The validation approaches from [Chapter 13](ch019.xhtml#sec-ml-operations) assume
    access to held-out test sets and centralized evaluation infrastructure where model
    quality can be measured directly against known ground truth. On-device learning
    requires distributed validation that respects privacy and resource constraints
    while still providing reliable quality signals across heterogeneous device populations.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章中的验证方法假设可以访问保留的测试集和集中式评估基础设施，可以直接将模型质量与已知的真实情况进行比较。设备上的学习需要分布式验证，在尊重隐私和资源限制的同时，仍能为异构设备群体提供可靠的质量信号。
- en: Shadow model evaluation provides the primary validation mechanism by maintaining
    multiple model variants on each device and comparing their behavior. Devices simultaneously
    run a baseline shadow model—a frozen copy of the last known-good base model that
    provides a stable reference point—alongside the current locally-adapted version
    that reflects recent on-device training. Many systems also maintain the latest
    federated aggregation result as a global model variant, enabling comparison between
    individual device adaptations and the collective knowledge aggregated from the
    entire device population. By comparing predictions across these variants on incoming
    data streams, systems detect when local adaptation degrades performance relative
    to established baselines. This comparison occurs continuously during normal operation,
    requiring no additional labeled validation data. When the adapted model consistently
    underperforms the baseline shadow, the system triggers automatic rollback to the
    known-good version, preventing performance degradation from persisting in production.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 影子模型评估通过在每个设备上维护多个模型变体并比较其行为来提供主要的验证机制。设备同时运行一个基线影子模型——最后已知良好基模型的冻结副本，提供一个稳定的参考点——以及当前本地适应的版本，该版本反映了最近的设备上训练。许多系统还维护最新的联邦聚合结果作为全局模型变体，使个人设备适应与从整个设备群体汇总的集体知识之间的比较成为可能。通过比较这些变体在传入数据流中的预测，系统检测到本地适应相对于既定的基线性能下降。这种比较在正常操作期间持续进行，无需额外的标记验证数据。当适应的模型持续低于基线影子模型时，系统触发自动回滚到已知良好的版本，防止性能下降在生产中持续存在。
- en: Confidence-based quality gates provide an additional validation signal when
    labeled validation data is unavailable. Without ground truth labels, systems use
    prediction confidence as a quality proxy that correlates with model performance.
    Well-calibrated models should exhibit high confidence on in-distribution samples
    that resemble their training data, with confidence scores that accurately reflect
    the probability of correct predictions. Confidence drops indicate either distributional
    shift—where input data no longer matches training distributions—or model degradation
    from problematic local adaptations. Threshold-based gating implements this validation
    mechanism by continuously monitoring average prediction confidence and suspending
    adaptation when confidence falls below baseline levels established during initial
    deployment. This approach catches many failure modes without requiring labeled
    validation data, though it cannot detect all performance issues since overconfident
    but incorrect predictions can maintain high confidence scores.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 基于置信度的质量门在标签验证数据不可用的情况下提供额外的验证信号。在没有地面真实标签的情况下，系统使用预测置信度作为质量代理，这与模型性能相关。校准良好的模型应在与其训练数据相似的分布样本上表现出高置信度，置信度得分应准确反映正确预测的概率。置信度下降表明可能是分布偏移——输入数据不再匹配训练分布——或者模型因问题性的局部适应而退化。基于阈值的门控通过持续监控平均预测置信度，并在置信度低于初始部署期间建立的基线水平时暂停适应来实现这种验证机制。这种方法可以捕捉许多故障模式，而无需标记的验证数据，尽管它不能检测到所有性能问题，因为过于自信但错误的预测可以保持高置信度得分。
- en: Federated A/B testing enables validation of new adaptation strategies or model
    architectures across distributed device populations. To validate proposed changes,
    systems implement distributed experiments that randomly assign devices to treatment
    and control groups while maintaining statistical balance across device tiers and
    usage patterns. Both groups collect federated metrics using privacy-preserving
    aggregation protocols that prevent individual device data from being exposed while
    enabling population-level comparisons. The system compares adaptation success
    rates—measuring how frequently local adaptations improve over baseline models—along
    with convergence speed that indicates how quickly devices reach optimal performance,
    and final performance metrics that reflect ultimate model quality after adaptation
    completes. Successful strategies demonstrating clear improvements in treatment
    groups are rolled out gradually across the device population, starting with small
    percentages and expanding only after confirming that benefits generalize beyond
    the experimental cohort.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦A/B测试允许在分布式设备群体中验证新的适应策略或模型架构。为了验证所提出的更改，系统实施分布式实验，随机将设备分配到治疗组和对照组，同时保持设备层级和用法模式之间的统计平衡。两组都使用隐私保护聚合协议收集联邦度量，这些协议防止个人设备数据泄露，同时允许进行人口水平比较。系统比较适应成功率——衡量局部适应如何频繁地超过基线模型——以及收敛速度，这表明设备达到最佳性能的速度有多快，以及最终性能指标，这些指标反映了适应完成后模型的最终质量。在治疗组中显示出明显改进的成功策略将逐步推广到设备群体中，从小的百分比开始，只有在确认好处可以推广到实验群体之外后才会扩大。
- en: These operational transformations necessitate new tooling and infrastructure
    that systematically extends traditional MLOps practices from [Chapter 13](ch019.xhtml#sec-ml-operations).
    The CI/CD pipelines, monitoring dashboards, A/B testing frameworks, and incident
    response procedures established for centralized deployments form the foundation
    for on-device learning operations. The federated learning protocols ([Section 14.6](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e))
    provide coordination mechanisms for distributed training, while monitoring challenges
    ([Section 14.9.3](ch020.xhtml#sec-ondevice-learning-distributed-system-observability-2270))
    address observability gaps created by decentralized adaptation.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作转换需要新的工具和基础设施，这些工具和基础设施系统地扩展了传统的MLOps实践，从[第13章](ch019.xhtml#sec-ml-operations)开始。为集中部署建立的CI/CD管道、监控仪表板、A/B测试框架和事件响应程序构成了设备学习操作的基础。联邦学习协议([第14.6节](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e))提供了分布式训练的协调机制，而监控挑战([第14.9.3节](ch020.xhtml#sec-ondevice-learning-distributed-system-observability-2270))则解决了由去中心化适应产生的可观察性差距。
- en: Successful on-device learning deployments build upon proven MLOps methodologies
    while adapting them to the unique challenges of distributed, heterogeneous learning
    environments. This evolutionary approach ensures operational reliability while
    enabling the benefits of edge learning.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的设备上学习部署建立在经过验证的MLOps方法之上，同时适应了分布式、异构学习环境的独特挑战。这种进化方法确保了操作可靠性，同时实现了边缘学习的益处。
- en: Bio-Inspired Learning Efficiency
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生物启发学习效率
- en: The constraints of on-device learning mirror fundamental challenges solved by
    biological intelligence systems, offering theoretical insights into efficient
    learning design. Understanding these connections enables principled approaches
    to resource-constrained machine learning that leverage billions of years of evolutionary
    optimization.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习的约束反映了生物智能系统解决的基本挑战，为高效学习设计提供了理论见解。理解这些联系使得能够采用基于原理的方法来利用数十亿年的进化优化进行资源受限的机器学习。
- en: Learning from Biological Neural Efficiency
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从生物神经效率中学习
- en: The human brain operates at approximately 20 watts while continuously learning
    from limited supervision—precisely the efficiency target for on-device learning
    systems[32](#fn32). This remarkable efficiency emerges from several architectural
    principles that directly inform edge learning design, demonstrating what is theoretically
    achievable with highly optimized learning systems.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑在持续从有限的监督中学习的同时，以大约20瓦的功率运行——这正是设备上学习系统的效率目标[32](#fn32)。这种非凡的效率源于几个直接影响边缘学习设计的架构原则，展示了理论上高度优化的学习系统可以达到什么水平。
- en: The brain’s efficiency characteristics reveal multiple dimensions of optimization
    that on-device systems should target. From a power perspective, the brain consumes
    just 20 W total, with approximately 10 W dedicated to active learning and memory
    consolidation—an energy budget comparable to what mobile devices can sustainably
    allocate to on-device learning during charging periods. Memory efficiency comes
    from sparse, distributed representations where only 1-2% of neurons activate simultaneously
    during any cognitive task, dramatically reducing the computational and storage
    requirements compared to dense neural networks. Learning efficiency manifests
    through few-shot learning capabilities that enable adaptation from single exposures,
    along with continuous adaptation mechanisms that avoid catastrophic forgetting
    when integrating new knowledge. Hierarchical processing organizes information
    across multiple scales, from low-level sensory inputs to high-level abstract reasoning,
    enabling efficient reuse of learned features across different tasks and contexts.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑的效率特性揭示了设备上系统应针对的多个优化维度。从功耗角度来看，大脑总共消耗20 W的功率，其中大约10 W用于主动学习和记忆巩固——这种能量预算与移动设备在充电期间可持续分配给设备上学习的能量相当。内存效率来源于稀疏、分布式的表示，在任何认知任务中，只有1-2%的神经元同时激活，与密集神经网络相比，大大减少了计算和存储需求。学习效率通过少样本学习能力体现，这种能力使得可以从单一暴露中适应，以及通过连续适应机制，在整合新知识时避免灾难性遗忘。分层处理在多个尺度上组织信息，从低级感官输入到高级抽象推理，使得在不同任务和环境中高效重用学习到的特征。
- en: Biological learning exhibits several features that on-device systems must replicate
    to achieve similar efficiency. Sparse representations ensure efficient use of
    limited neural resources—only a tiny fraction of brain neurons fire during any
    cognitive task. This sparsity directly parallels the selective parameter updates
    and pruned architectures essential for mobile deployment. Event-driven processing
    minimizes energy consumption by activating computation only when sensory input
    changes, analogous to opportunistic training during device idle periods.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学习表现出几个特征，设备上系统必须复制以实现类似的效率。稀疏表示确保了有限的神经资源的有效利用——在任何认知任务中，只有大脑神经元的一小部分会激活。这种稀疏性直接对应于移动部署中必不可少的精选参数更新和剪枝架构。事件驱动处理通过仅在感官输入变化时激活计算来最小化能耗，类似于设备空闲期间的机会训练。
- en: Unlabeled Data Exploitation Strategies
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 未标记数据利用策略
- en: 'Mobile devices continuously collect rich sensor streams ideal for self-supervised
    learning: visual data from cameras, temporal patterns from accelerometers, spatial
    patterns from GPS, and interaction patterns from touchscreen usage. This abundant
    unlabeled data enables sophisticated representation learning without external
    supervision.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备持续收集丰富的传感器流，非常适合自监督学习：来自摄像头的视觉数据、来自加速度计的时间模式、来自GPS的空间模式以及来自触摸屏使用的交互模式。这大量的未标记数据使得在没有外部监督的情况下进行复杂的表示学习成为可能。
- en: The scale of sensor data generation on mobile devices creates unprecedented
    opportunities for self-supervised learning. Visual streams from cameras operating
    at 30 frames per second provide approximately 2.6 million frames daily, offering
    abundant data for contrastive learning approaches that learn visual representations
    by comparing augmented versions of the same image[33](#fn33). Motion data from
    accelerometers sampling at 100 Hz generates 8.6 million data points daily, capturing
    temporal patterns suitable for learning representations of human activities and
    device movement. Location traces from GPS sensors enable spatial representation
    learning and behavioral prediction by capturing movement patterns and frequently
    visited locations without requiring explicit labels. Interaction patterns from
    touch events, typing dynamics, and app usage sequences create rich behavioral
    embeddings that reveal user preferences and habits, enabling personalized model
    adaptation without manual annotation.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备上传感器数据生成的规模为自监督学习创造了前所未有的机遇。每秒30帧的摄像头视觉流每天提供约260万帧图像，为通过比较相同图像的增强版本来学习视觉表示的对比学习方法提供了大量数据[33](#fn33)。以100 Hz采样率的加速度计生成的运动数据每天产生860万个数据点，捕捉适合学习人类活动和设备运动表示的时间模式。来自GPS传感器的位置轨迹通过捕捉运动模式和频繁访问的位置，无需显式标签，实现空间表示学习和行为预测。来自触摸事件、打字动态和应用使用序列的交互模式创建了丰富的行为嵌入，揭示了用户偏好和习惯，从而实现无需手动标注的个性化模型适应。
- en: Contrastive learning from temporal correlations offers particularly promising
    opportunities for leveraging this sensor data. Consecutive frames from mobile
    cameras naturally provide positive pairs for visual representation learning—images
    captured milliseconds apart typically show the same scene from slightly different
    perspectives—while augmentation techniques such as color jittering and random
    cropping create negative examples. Audio streams from microphones enable self-supervised
    speech representation learning through masking and prediction tasks, where the
    model learns to predict masked portions of audio spectrograms. Even device orientation
    and motion data can be used for self-supervised pretraining of activity recognition
    models, learning representations that capture the temporal structure of human
    movement without requiring labeled activity annotations.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间相关性中进行对比学习为利用这些传感器数据提供了特别有前景的机遇。移动摄像头的连续帧自然为视觉表示学习提供正对，毫秒之差捕获的图像通常是从略微不同的角度看到的同一场景，而诸如颜色抖动和随机裁剪等增强技术则创建了负例。麦克风中的音频流通过掩码和预测任务实现自监督语音表示学习，其中模型学习预测音频频谱图的掩码部分。甚至设备方向和运动数据也可以用于活动识别模型的自我监督预训练，学习捕捉人类运动时间结构的表示，而无需标记的活动注释。
- en: 'The biological inspiration extends to continual learning without forgetting.
    Brains continuously integrate new experiences while retaining decades of memories
    through mechanisms like synaptic consolidation and replay. On-device systems must
    implement analogous mechanisms: elastic weight consolidation prevents catastrophic
    forgetting by protecting weights important for previous tasks, experience replay
    maintains stability during adaptation by interleaving new training with replayed
    examples from previous tasks, and progressive neural architectures expand model
    capacity as new tasks emerge rather than forcing all knowledge into fixed-capacity
    networks.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 生物启发的思想扩展到持续学习而不遗忘。大脑通过突触巩固和重放等机制持续整合新的经验，同时保留数十年的记忆。设备上的系统必须实现类似机制：弹性权重巩固通过保护对先前任务重要的权重来防止灾难性遗忘，经验重放通过交替新训练和先前任务的回放示例来维持适应期间的稳定性，而渐进式神经网络架构随着新任务的涌现而扩展模型容量，而不是将所有知识都强制纳入固定容量的网络。
- en: Lifelong Adaptation Without Forgetting
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 永续适应而不遗忘
- en: 'Real-world on-device deployment demands continual adaptation to changing environments,
    user behavior, and task requirements. This presents the fundamental challenge
    of the stability-plasticity tradeoff: models must remain stable enough to preserve
    existing knowledge while plastic enough to learn new patterns.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界的设备端部署需要不断适应不断变化的环境、用户行为和任务需求。这提出了稳定性-塑性权衡的基本挑战：模型必须保持足够的稳定性以保留现有知识，同时足够塑性以学习新模式。
- en: Continual learning on edge devices faces several interconnected challenges that
    compound the difficulty of distributed adaptation. Catastrophic forgetting occurs
    when new learning overwrites previously acquired knowledge, causing models to
    lose performance on earlier tasks as they adapt to new ones—a particularly severe
    problem when devices cannot access historical training data. Task interference
    emerges when multiple learning objectives compete for limited model capacity,
    forcing difficult tradeoffs between different capabilities that the model must
    maintain simultaneously. Data distribution shift manifests as deployment environments
    differ significantly from training conditions, requiring models to adapt to new
    patterns while maintaining performance on the original distribution. Resource
    constraints fundamentally limit the available solutions, as limited memory prevents
    storing all historical data for replay-based approaches that work well in centralized
    settings but exceed edge device capabilities.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上进行持续学习面临几个相互关联的挑战，这些挑战加剧了分布式适应的难度。当新的学习覆盖了之前获得的知识时，会发生灾难性遗忘，导致模型在适应新任务时性能下降，尤其是在设备无法访问历史训练数据的情况下这是一个特别严重的问题。当多个学习目标争夺有限的模型容量时，会出现任务干扰，迫使模型在必须同时保持的不同能力之间做出艰难的权衡。数据分布变化表现为部署环境与训练条件显著不同，要求模型适应新模式的同时保持对原始分布的性能。
- en: Meta-learning approaches address these challenges by learning learning algorithms
    themselves rather than just learning specific tasks. Model-Agnostic Meta-Learning
    (MAML) trains models to quickly adapt to new tasks with minimal data—exactly the
    capability required for personalized on-device adaptation where collecting large
    user-specific datasets is impractical. Few-shot learning techniques enable rapid
    specialization from small user-specific datasets, allowing models to personalize
    based on just a handful of examples while maintaining general capabilities learned
    during pretraining.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习方法通过学习学习算法本身来解决这些挑战，而不仅仅是学习特定任务。模型无关元学习（MAML）训练模型以使用最少的数据快速适应新任务——这正是个性化设备端适应所必需的，在收集大型用户特定数据集不切实际的情况下。少样本学习技术允许从小的用户特定数据集中快速专业化，使模型能够仅基于少量示例进行个性化，同时保持预训练期间学习到的通用能力。
- en: 'The theoretical foundation suggests that optimal on-device learning systems
    will combine sparse representations, self-supervised pretraining on sensor data,
    and meta-learning for rapid adaptation. These principles directly influence practical
    system design: sparse model architectures reduce memory and compute requirements,
    self-supervised objectives utilize abundant unlabeled sensor data, and meta-learning
    enables efficient personalization from limited user interactions.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 理论基础表明，最优的设备端学习系统将结合稀疏表示、基于传感器数据的自监督预训练以及元学习以实现快速适应。这些原则直接影响实际系统设计：稀疏模型架构降低内存和计算需求，自监督目标利用丰富的未标记传感器数据，而元学习则使模型能够从有限的用户交互中实现高效个性化。
- en: A key principle in building practical systems is to minimize the adaptation
    footprint. Full-model fine-tuning is typically infeasible on edge platforms, instead,
    localized update strategies, including bias-only optimization, residual adapters,
    and lightweight task-specific heads, should be prioritized. These approaches allow
    model specialization under resource constraints while mitigating the risks of
    overfitting or instability.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 构建实用系统的关键原则是尽量减少适应足迹。在边缘平台上进行全模型微调通常不可行，因此，应优先考虑局部更新策略，包括仅优化偏差、残差适配器和轻量级任务特定头部。这些方法允许在资源受限的情况下实现模型专业化，同时减轻过拟合或不稳定的风险。
- en: The feasibility of lightweight adaptation depends importantly on the strength
    of offline pretraining ([Bommasani et al. 2021](ch058.xhtml#ref-bommasani2021opportunities)).
    Pretrained models should encapsulate generalizable feature representations that
    allow efficient adaptation from limited local data. Shifting the burden of feature
    extraction to centralized training reduces the complexity and energy cost of on-device
    updates, while improving convergence stability in data-sparse environments.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级适应的可行性在很大程度上取决于离线预训练的强度 ([Bommasani 等人 2021](ch058.xhtml#ref-bommasani2021opportunities))。预训练模型应封装可泛化的特征表示，以便从有限的本地数据中高效地适应。将特征提取的负担转移到集中式训练可以降低设备更新的复杂性和能耗，同时在数据稀疏环境中提高收敛稳定性。
- en: Even when adaptation is lightweight, opportunistic scheduling remains important
    to preserve system responsiveness and user experience. Local updates should be
    deferred to periods when the device is idle, connected to external power, and
    operating on a reliable network. Such policies minimize the impact of background
    training on latency, battery consumption, and thermal performance.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 即使适应是轻量级的，机会性调度仍然很重要，以保持系统响应性和用户体验。本地更新应推迟到设备空闲、连接到外部电源并运行在可靠网络上的时段。此类策略最小化了背景训练对延迟、电池消耗和热性能的影响。
- en: The sensitivity of local training artifacts necessitates careful data security
    measures. Replay buffers, support sets, adaptation logs, and model update metadata
    must be protected against unauthorized access or tampering. Lightweight encryption
    or hardware-backed secure storage can mitigate these risks without imposing prohibitive
    resource costs on edge platforms.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 本地训练工件敏感性需要谨慎的数据安全措施。重放缓冲区、支持集、适应日志和模型更新元数据必须受到未经授权的访问或篡改的保护。轻量级加密或基于硬件的安全存储可以减轻这些风险，而不会对边缘平台施加过高的资源成本。
- en: However, security measures alone do not guarantee model robustness. As models
    adapt locally, monitoring adaptation dynamics becomes important. Lightweight validation
    techniques, including confidence scoring, drift detection heuristics, and shadow
    model evaluation, can help identify divergence early, enabling systems to trigger
    rollback mechanisms before severe degradation occurs ([Gama et al. 2014](ch058.xhtml#ref-gama2014survey)).
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅靠安全措施并不能保证模型的鲁棒性。随着模型在本地适应，监控适应动态变得重要。轻量级验证技术，包括置信度评分、漂移检测启发式方法和影子模型评估，可以帮助早期识别偏差，使系统在发生严重退化之前触发回滚机制
    ([Gama 等人 2014](ch058.xhtml#ref-gama2014survey))。
- en: Robust rollback procedures depend on retaining trusted model checkpoints. Every
    deployment should preserve a known-good baseline version of the model that can
    be restored if adaptation leads to unacceptable behavior. This principle is especially
    important in safety-important and regulated domains, where failure recovery must
    be provable and rapid.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 顽强的回滚程序依赖于保留可信的模型检查点。每次部署都应该保留一个已知的良好基线版本，以便在适应导致不可接受的行为时可以恢复。这一原则在安全重要和受监管的领域尤为重要，在这些领域，故障恢复必须是可证明且快速的。
- en: In decentralized or federated learning contexts, communication efficiency becomes
    a first-order design constraint. Compression techniques such as quantized gradient
    updates, sparsified parameter sets, and selective model transmission must be employed
    to allow scalable coordination across large, heterogeneous fleets of devices without
    overwhelming bandwidth or energy budgets ([Konečný et al. 2016](ch058.xhtml#ref-konevcny2016federated)).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在去中心化或联邦学习环境中，通信效率成为一级设计约束。必须采用压缩技术，如量化梯度更新、稀疏参数集和选择性模型传输，以允许在大规模、异构设备群之间进行可扩展的协调，而不会耗尽带宽或能源预算
    ([Konečný 等人 2016](ch058.xhtml#ref-konevcny2016federated))。
- en: When personalization is required, systems should aim for localized adaptation
    wherever possible. Restricting updates to lightweight components, including final
    classification heads or modular adapters, constrains the risk of catastrophic
    forgetting, reduces memory overhead, and accelerates adaptation without destabilizing
    core model representations.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要个性化时，系统应尽可能实现本地适应。将更新限制在轻量级组件中，包括最终的分类头或模块化适配器，可以限制灾难性遗忘的风险，减少内存开销，并加速适应而不会破坏核心模型表示的稳定性。
- en: Finally, throughout the system lifecycle, privacy and compliance requirements
    must be architected into adaptation pipelines. Mechanisms to support user consent,
    data minimization, retention limits, and the right to erasure must be considered
    core aspects of model design, not post-hoc adjustments. Meeting regulatory obligations
    at scale demands that on-device learning workflows align inherently with principles
    of auditable autonomy.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在整个系统生命周期中，隐私和合规性要求必须被构建到适应管道中。支持用户同意、数据最小化、保留限制和删除权的机制必须被视为模型设计的基本方面，而不是事后调整。在规模上满足监管义务要求设备上学习工作流程与可审计的自主性原则内在一致。
- en: The flowchart in [Figure 14.8](ch020.xhtml#fig-odl-design-flow) summarizes key
    decision points in designing practical, scalable, and resilient on-device ML systems.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.8](ch020.xhtml#fig-odl-design-flow)中的流程图总结了设计实用、可扩展和弹性设备上机器学习系统时的关键决策点。'
- en: '![](../media/file227.svg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file227.svg)'
- en: 'Figure 14.8: This flowchart guides the systematic development of practical
    on-device ML systems by outlining key decision points related to data management,
    model selection, and privacy considerations throughout the system lifecycle. Integrating
    privacy and compliance requirements—such as user consent and data minimization—into
    the design process ensures auditable autonomy and scalable deployment of on-device
    intelligence.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：此流程图通过概述与数据管理、模型选择和隐私考虑相关的关键决策点，指导实用设备上机器学习系统的系统化开发。将隐私和合规性要求（如用户同意和数据最小化）集成到设计过程中确保了可审计的自主性和设备智能的可扩展部署。
- en: Systems Integration for Production Deployment
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产部署的系统集成
- en: Real-world on-device learning systems achieve effectiveness by systematically
    combining all three solution pillars rather than relying on isolated techniques.
    This integration requires careful systems engineering to manage interactions,
    resolve conflicts, and optimize the overall system performance within deployment
    constraints.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的设备上学习系统通过系统地结合所有三个解决方案支柱来实现有效性，而不是依赖于孤立的技术。这种集成需要仔细的系统工程来管理交互、解决冲突，并在部署约束下优化整体系统性能。
- en: Consider a production voice assistant deployment across 50 million heterogeneous
    devices. The system architecture demonstrates systematic integration across three
    complementary layers that work together to enable effective learning under diverse
    constraints.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在一个覆盖5000万台异构设备的生产级语音助手部署。该系统架构展示了在三个互补层之间的系统化集成，这三个层共同工作以在多种约束条件下实现有效的学习。
- en: The model adaptation layer stratifies techniques by device capability, matching
    sophistication to available resources. Flagship phones representing the top 20%
    of the deployment use LoRA rank-32 adapters that enable sophisticated voice pattern
    learning through high-dimensional parameter updates. Mid-tier devices comprising
    60% of the fleet employ rank-16 adapters that balance adaptation expressiveness
    with the tighter memory constraints typical of mainstream smartphones. Budget
    devices making up the remaining 20% rely on bias-only updates that stay comfortably
    within 1 GB memory limits while still enabling basic personalization.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 模型适应层根据设备能力分层技术，将复杂性匹配到可用资源。代表部署中前20%的旗舰手机使用LoRA rank-32适配器，通过高维参数更新实现复杂的语音模式学习。占车队60%的中端设备采用rank-16适配器，在适应表达性和主流智能手机典型的更紧的内存限制之间取得平衡。构成剩余20%的预算设备依赖于仅偏置更新的方式，保持在1
    GB内存限制内，同时仍然能够实现基本个性化。
- en: The data efficiency layer implements adaptive strategies across the entire device
    population while respecting individual resource constraints. All devices implement
    experience replay, but with device-appropriate buffer sizes—10 MB on budget devices
    versus 100 MB on flagship models—ensuring that memory-constrained devices can
    still benefit from replay-based learning. Few-shot learning enables rapid adaptation
    to new users within their first 10 interactions, reducing the cold-start problem
    that plagues systems requiring extensive training data. Streaming updates accommodate
    continuous voice pattern evolution as users’ speaking styles naturally change
    over time or as they use the assistant in new acoustic environments.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 数据效率层在整个设备群体中实施自适应策略，同时尊重个体资源限制。所有设备都实现经验重放，但具有设备适当的缓冲区大小——预算设备上为10 MB，而旗舰机型上为100
    MB，确保内存受限的设备仍能从基于重放的学习中受益。少样本学习使用户在前10次交互中快速适应新用户，减少了困扰需要大量训练数据的系统的冷启动问题。流式更新适应用户说话风格随时间自然变化或在新声学环境中使用助手时的连续语音模式演变。
- en: The federated coordination layer orchestrates privacy-preserving collaboration
    across the device population. Devices participate in federated training rounds
    opportunistically based on connectivity status and battery level, ensuring that
    coordination does not degrade user experience. LoRA adapters aggregate efficiently
    with just 50 MB per update compared to 14 GB for full model synchronization, making
    federated learning practical over mobile networks. Privacy-preserving aggregation
    protocols ensure that individual voice patterns never leave devices while still
    enabling population-scale improvements in accent recognition and language understanding
    that benefit all users.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦协调层在设备群体中协调隐私保护的合作。设备根据连接状态和电池水平有选择地参与联邦训练轮次，确保协调不会降低用户体验。LoRA适配器只需50 MB的更新即可高效聚合，而全模型同步需要14
    GB，这使得联邦学习在移动网络上变得可行。隐私保护聚合协议确保个人语音模式永远不会离开设备，同时仍能实现口音识别和语言理解方面的种群规模改进，使所有用户受益。
- en: 'Effective systems integration requires adherence to key engineering principles
    that ensure robust operation across heterogeneous device populations:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的系统集成需要遵守关键工程原则，以确保在异构设备群体中稳健运行：
- en: '**Hierarchical Capability Matching**: Deploy more sophisticated techniques
    on capable devices while ensuring basic functionality across the device spectrum.
    Never assume uniform capabilities.'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分层能力匹配**：在具备能力的设备上部署更复杂的技巧，同时确保在整个设备范围内保持基本功能。永远不要假设能力统一。'
- en: '**Graceful Degradation**: Systems must operate effectively when individual
    components fail. Poor connectivity should not prevent local adaptation; low battery
    should trigger minimal adaptation modes.'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优雅降级**：系统必须在单个组件失败时有效运行。不良的连接性不应阻止本地适应；低电量应触发最小适应模式。'
- en: '**Conflict Resolution**: Model adaptation and data efficiency techniques can
    conflict (limited memory vs buffer size). Systematic resource allocation prevents
    these conflicts through predefined priority hierarchies.'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**冲突解决**：模型适应和数据效率技术可能存在冲突（内存限制与缓冲区大小）。通过预定义的优先级层次结构进行系统资源分配可以防止这些冲突。'
- en: '**Performance Validation**: Integration creates emergent behaviors that individual
    techniques don’t exhibit. Systems require comprehensive testing across device
    combinations and network conditions.'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能验证**：集成会产生个体技术所不具备的涌现行为。系统需要在设备组合和网络条件下进行全面测试。'
- en: This integrated approach transforms on-device learning from a collection of
    techniques into a coherent systems capability that provides robust personalization
    within real-world deployment constraints.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成方法将设备上的学习从一系列技术转变为一个连贯的系统能力，在现实世界的部署限制内提供稳健的个性化。
- en: Persistent Technical and Operational Challenges
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续的技术和操作挑战
- en: The solution techniques explored above—model adaptation, data efficiency, and
    federated coordination—address many fundamental constraints of on-device learning
    but also reveal persistent challenges that emerge from their interaction in real-world
    deployments. These challenges represent the current frontiers of on-device learning
    research and highlight areas where the techniques discussed earlier reach their
    limits or create new operational complexities. Understanding these challenges
    provides critical context for evaluating when on-device learning approaches are
    appropriate and where alternative strategies may be necessary.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 上文探讨的解决方案技术——模型适应性、数据效率和联邦协调——解决了设备上学习的许多基本约束，但也揭示了从现实部署中产生的持续挑战。这些挑战代表了设备上学习研究的前沿，并突出了先前讨论的技术达到其极限或创造新的操作复杂性的领域。理解这些挑战为评估何时采用设备上学习方法以及何时可能需要替代策略提供了关键背景。
- en: Unlike conventional centralized systems, where training occurs in controlled
    environments with uniform hardware and curated datasets, edge systems must contend
    with heterogeneity in devices, fragmentation in data, and the absence of centralized
    validation infrastructure. These factors give rise to new systems-level tradeoffs
    that test the boundaries of the adaptation strategies, data efficiency methods,
    and coordination mechanisms we have examined.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 与在具有统一硬件和精选数据集的受控环境中进行训练的传统集中式系统不同，边缘系统必须应对设备异构性、数据碎片化和缺乏集中式验证基础设施。这些因素产生了新的系统级权衡，考验了我们考察的适应性策略、数据效率方法和协调机制的边界。
- en: Device and Data Heterogeneity Management
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备和数据异构性管理
- en: 'Federated and on-device ML systems must operate across a vast and diverse ecosystem
    of devices, ranging from smartphones and wearables to IoT sensors and microcontrollers.
    This heterogeneity spans multiple dimensions: hardware capabilities, software
    stacks, network connectivity, and power availability. Unlike cloud-based systems,
    where environments can be standardized and controlled, edge deployments encounter
    a wide distribution of system configurations and constraints. These variations
    introduce significant complexity in algorithm design, resource scheduling, and
    model deployment.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦和设备上的机器学习系统必须在从智能手机和可穿戴设备到物联网传感器和微控制器的广泛且多样化的设备生态系统中运行。这种异构性跨越多个维度：硬件能力、软件堆栈、网络连接和电力可用性。与可以标准化和控制的云基础系统不同，边缘部署会遇到广泛的系统配置和约束。这些变化在算法设计、资源调度和模型部署方面引入了显著的复杂性。
- en: At the hardware level, devices differ in terms of memory capacity, processor
    architecture (e.g., ARM Cortex-M vs. A-series)[34](#fn34), instruction set support
    (e.g., availability of SIMD or floating-point units), and the presence or absence
    of AI accelerators. Some clients may possess powerful NPUs capable of running
    small training loops, while others may rely solely on low-frequency CPUs with
    minimal RAM. These differences affect the feasible size of models, the choice
    of training algorithm, and the frequency of updates.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件层面，设备在内存容量、处理器架构（例如，ARM Cortex-M与A系列[34](#fn34)）、指令集支持（例如，SIMD或浮点单元的可用性）以及是否存在AI加速器方面存在差异。一些客户端可能拥有能够运行小型训练循环的强大NPU，而其他客户端可能仅依赖低频CPU和最小内存。这些差异影响了模型的可实现大小、训练算法的选择以及更新的频率。
- en: Software heterogeneity compounds the challenge. Devices may run different versions
    of operating systems, kernel-level drivers, and runtime libraries. Some environments
    support optimized ML runtimes like TensorFlow Lite[35](#fn35) Micro or ONNX Runtime
    Mobile, while others rely on custom inference stacks or restricted APIs. These
    discrepancies can lead to subtle inconsistencies in behavior, especially when
    models are compiled differently or when floating-point precision varies across
    platforms.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 软件异构性加剧了挑战。设备可能运行不同的操作系统版本、内核级驱动程序和运行时库。一些环境支持优化的机器学习运行时，如TensorFlow Lite[35](#fn35)
    Micro或ONNX Runtime Mobile，而其他环境则依赖于定制的推理堆栈或受限的API。这些差异可能导致行为上的微妙不一致，尤其是在模型编译不同或平台间浮点精度不同的情况下。
- en: In addition to computational heterogeneity, devices exhibit variation in connectivity
    and uptime. Some are intermittently connected, plugged in only occasionally, or
    operate under strict bandwidth constraints. Others may have continuous power and
    reliable networking, but still prioritize user-facing responsiveness over background
    learning. These differences complicate the orchestration of coordinated learning
    and the scheduling of updates.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算异构性之外，设备在连接性和运行时间上表现出差异。有些设备是间歇性连接的，偶尔插电，或在严格的带宽限制下运行。其他设备可能具有连续的电源和可靠的联网，但仍然优先考虑用户界面的响应性，而不是后台学习。这些差异使得协调学习和更新调度复杂化。
- en: Finally, system fragmentation affects reproducibility and testing. With such
    a wide range of execution environments, it is difficult to ensure consistent model
    behavior or to debug failures reliably. This makes monitoring, validation, and
    rollback mechanisms more important—but also more difficult to implement uniformly
    across the fleet.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，系统碎片化影响了可重复性和测试。在如此广泛的执行环境中，很难确保模型行为的一致性或可靠地调试故障。这使得监控、验证和回滚机制变得更加重要——但也更难以在所有设备上统一实施。
- en: Consider a federated learning deployment for mobile keyboards. A high-end smartphone
    might feature 8 GB of RAM, a dedicated AI accelerator, and continuous Wi-Fi access.
    In contrast, a budget device may have just 2 GB of RAM, no hardware acceleration,
    and rely on intermittent mobile data. These disparities influence how long training
    runs can proceed, how frequently models can be updated, and even whether training
    is feasible at all. To support such a range, the system must dynamically adjust
    training schedules, model formats, and compression strategies—ensuring equitable
    model improvement across users while respecting each device’s limitations.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个用于移动键盘的联邦学习部署。高端智能手机可能配备8 GB的RAM、专门的AI加速器和连续的Wi-Fi接入。相比之下，预算设备可能只有2 GB的RAM、没有硬件加速，并依赖于间歇性的移动数据。这些差异影响了训练运行的时间长度、模型更新的频率，甚至是否可以进行训练。为了支持这种范围，系统必须动态调整训练计划、模型格式和压缩策略——确保用户之间模型改进的公平性，同时尊重每个设备的限制。
- en: Non-IID Data Distribution Challenges
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非IID数据分布挑战
- en: In centralized machine learning, data can be aggregated, shuffled, and curated
    to approximate independent and identically distributed (IID) samples—a key assumption
    underlying many learning algorithms. On-device and federated learning systems
    fundamentally challenge this assumption, requiring algorithms that can handle
    highly fragmented and non-IID data across diverse devices and contexts.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在集中式机器学习中，数据可以被聚合、打乱和整理，以近似独立同分布（IID）样本——这是许多学习算法背后的关键假设。设备端和联邦学习系统从根本上挑战了这个假设，需要能够处理跨不同设备和环境的高度碎片化和非IID数据的算法。
- en: The statistical implications of this fragmentation create cascading challenges
    throughout the learning process. Gradients computed on different devices may conflict,
    slowing convergence or destabilizing training. Local updates risk overfitting
    to individual client idiosyncrasies, reducing performance when aggregated globally.
    The diversity of data across clients also complicates evaluation, as no single
    test set can represent the true deployment distribution.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 这种碎片化的统计影响在整个学习过程中造成了级联挑战。在不同设备上计算的梯度可能冲突，减缓收敛或使训练不稳定。局部更新可能会过度拟合到单个客户端的独特性，当全局聚合时降低性能。客户端间数据的多样性也使得评估复杂化，因为没有任何单个测试集可以代表真实的部署分布。
- en: These challenges necessitate robust algorithms that can handle heterogeneity
    and imbalanced participation. Techniques such as personalization layers, importance
    weighting, and adaptive aggregation schemes provide partial solutions, but the
    optimal approach varies with application context and the specific nature of data
    fragmentation. As established in [Section 14.3.3](ch020.xhtml#sec-ondevice-learning-data-constraints-303e),
    this statistical heterogeneity represents one of the core challenges distinguishing
    on-device learning from traditional centralized approaches.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战需要能够处理异构性和不平衡参与的鲁棒算法。如个性化层、重要性加权、自适应聚合方案等技术提供了部分解决方案，但最佳方法随着应用上下文和数据碎片化的具体性质而变化。如[第14.3.3节](ch020.xhtml#sec-ondevice-learning-data-constraints-303e)所述，这种统计异构性代表了区分设备端学习与传统集中式方法的核心挑战之一。
- en: Distributed System Observability
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式系统可观察性
- en: The monitoring and observability frameworks from [Chapter 13](ch019.xhtml#sec-ml-operations)
    must be fundamentally reimagined for distributed edge environments. Traditional
    centralized monitoring approaches that rely on unified data collection and real-time
    visibility become impractical when devices operate intermittently connected and
    data cannot be centralized. The drift detection and performance monitoring techniques
    established in MLOps provide conceptual foundations, but require adaptation to
    handle the distributed, privacy-preserving nature of on-device learning systems.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[第13章](ch019.xhtml#sec-ml-operations)的监控和可观察性框架必须为分布式边缘环境进行根本性的重新构想。当设备间歇性连接且数据无法集中时，依赖于统一数据收集和实时可见性的传统集中式监控方法变得不切实际。MLOps中建立的漂移检测和性能监控技术提供了概念基础，但需要适应以处理设备上学习系统的分布式、隐私保护特性。
- en: Unlike centralized machine learning systems, where model updates can be continuously
    evaluated against held-out validation sets, on-device learning introduces a core
    shift in visibility and observability. Once deployed, models operate in highly
    diverse and often disconnected environments, where internal updates may proceed
    without external monitoring. This creates significant challenges for ensuring
    that model adaptation is both beneficial and safe.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 与可以持续对保留的验证集进行评估的集中式机器学习系统不同，设备上学习引入了可见性和可观察性的核心转变。一旦部署，模型在高度多样化和通常断开连接的环境中运行，内部更新可能在没有外部监控的情况下进行。这为确保模型适应既有益又安全带来了重大挑战。
- en: A core difficulty lies in the absence of centralized validation data. In traditional
    workflows, models are trained and evaluated using curated datasets that serve
    as proxies for deployment conditions. On-device learners, by contrast, adapt in
    response to local inputs, which are rarely labeled and may not be systematically
    collected. As a result, the quality and direction of updates, whether they enhance
    generalization or cause drift, are difficult to assess without interfering with
    the user experience or violating privacy constraints.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 核心困难在于缺乏集中的验证数据。在传统的工作流程中，模型使用作为部署条件代理的精选数据集进行训练和评估。相比之下，设备上学习者根据本地输入进行适应，这些输入很少标记，并且可能没有系统地收集。因此，更新质量和方向，无论是增强泛化还是导致漂移，都难以评估，而不会干扰用户体验或违反隐私限制。
- en: The risk of model drift is especially pronounced in streaming settings, where
    continual adaptation may cause a slow degradation in performance. For instance,
    a voice recognition model that adapts too aggressively to background noise may
    eventually overfit to transient acoustic conditions, reducing accuracy on the
    target task. Without visibility into the evolution of model parameters or outputs,
    such degradations can remain undetected until they become severe.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式设置中，模型漂移的风险尤为明显，持续的适应可能导致性能缓慢下降。例如，一个对背景噪声适应过于激进的语音识别模型最终可能会过度拟合到瞬时的声学条件，降低目标任务的准确性。如果没有对模型参数或输出的演变有可见性，这种退化可能直到变得严重才被发现。
- en: Mitigating this problem requires mechanisms for on-device validation and update
    gating. One approach is to interleave adaptation steps with lightweight performance
    checks—using proxy objectives or self-supervised signals to approximate model
    confidence ([Y. Deng, Mokhtari, and Ozdaglar 2021](ch058.xhtml#ref-deng2021adaptive)).
    For example, a keyword spotting system might track detection confidence across
    recent utterances and suspend updates if confidence consistently drops below a
    threshold. Alternatively, shadow evaluation can be employed, where multiple model
    variants are maintained on the device and evaluated in parallel on incoming data
    streams, allowing the system to compare the adapted model’s behavior against a
    stable baseline.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这个问题需要设备上验证和更新门控的机制。一种方法是将适应步骤与轻量级性能检查交替进行——使用代理目标或自监督信号来近似模型置信度 ([Y. Deng,
    Mokhtari, and Ozdaglar 2021](ch058.xhtml#ref-deng2021adaptive))。例如，一个关键词检测系统可能会跟踪最近话语中的检测置信度，并在置信度持续低于阈值时暂停更新。或者，可以采用影子评估，在设备上维护多个模型变体，并在传入的数据流上并行评估，使系统能够将适应后的模型行为与稳定的基线进行比较。
- en: Another strategy involves periodic checkpointing and rollback, where snapshots
    of the model state are saved before adaptation. If subsequent performance degrades,
    as determined by downstream metrics or user feedback, the system can revert to
    a known good state. This approach has been used in health monitoring devices,
    where incorrect predictions could lead to user distrust or safety concerns. However,
    it introduces storage and compute overhead, especially in memory-constrained environments.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略涉及周期性检查点和回滚，即在适应之前保存模型状态快照。如果后续性能下降，如通过下游指标或用户反馈确定，系统可以回滚到已知良好的状态。这种方法已在健康监测设备中使用，错误的预测可能导致用户不信任或安全担忧。然而，它引入了存储和计算开销，尤其是在内存受限的环境中。
- en: In some cases, federated validation offers a partial solution. Devices can share
    anonymized model updates or summary statistics with a central server, which aggregates
    them across users to identify global patterns of drift or failure. While this
    preserves some degree of privacy, it introduces communication overhead and may
    not capture rare or user-specific failures.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，联邦验证提供了一种部分解决方案。设备可以与中央服务器共享匿名化的模型更新或汇总统计信息，该服务器汇总用户数据以识别全局漂移或故障模式。虽然这保留了一定程度的隐私，但它引入了通信开销，并且可能无法捕捉到罕见或特定于用户的故障。
- en: Ultimately, update monitoring and validation in on-device learning require a
    rethinking of traditional evaluation practices. Instead of centralized test sets,
    systems must rely on implicit signals, runtime feedback, and conservative adaptation
    policies to ensure robustness. The absence of global observability is not merely
    a technical limitation—it reflects a deeper systems challenge in aligning local
    adaptation with global reliability.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在设备上学习中的更新监控和验证需要重新思考传统的评估实践。与传统集中式测试集不同，系统必须依赖隐式信号、运行时反馈和保守的适应策略来确保鲁棒性。缺乏全局可观察性不仅仅是技术限制——它反映了更深层次的系统挑战，即在局部适应与全局可靠性之间进行协调。
- en: Performance Evaluation in Dynamic Environments
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态环境中的性能评估
- en: '[Chapter 12](ch018.xhtml#sec-benchmarking-ai) established systematic approaches
    for measuring ML system performance: inference latency, throughput, energy efficiency,
    and accuracy metrics. These benchmarking methodologies provide foundations for
    characterizing model performance, but they were designed for static inference
    workloads. On-device learning requires extending these metrics to capture adaptation
    quality and training efficiency through training-specific benchmarks.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '[第12章](ch018.xhtml#sec-benchmarking-ai)建立了测量机器学习系统性能的系统方法：推理延迟、吞吐量、能效和准确度指标。这些基准测试方法为表征模型性能提供了基础，但它们是为静态推理工作负载设计的。设备上的学习需要扩展这些指标，通过特定的训练基准来捕捉适应质量和训练效率。'
- en: Beyond the inference metrics from [Chapter 12](ch018.xhtml#sec-benchmarking-ai),
    adaptive systems require specialized training metrics that capture learning efficiency
    under edge constraints. Adaptation efficiency measures accuracy improvement per
    training sample consumed, quantified as the slope of the learning curve under
    resource constraints—a system achieving 2% accuracy gain per 100 training samples
    demonstrates higher adaptation efficiency than one requiring 500 samples for the
    same improvement, directly translating to faster personalization and reduced data
    collection requirements. Memory-constrained convergence evaluates the validation
    loss achieved within specified RAM budgets, such as “convergence within 512 KB
    training footprint,” capturing how effectively systems learn given fixed memory
    allocations—critical for comparing adaptation strategies across device classes
    from microcontrollers to smartphones. Energy-per-update quantifies millijoules
    consumed per gradient update, a metric critical for battery-powered devices where
    training energy directly impacts user experience—mobile devices typically budget
    500-1000 mW for sustained ML workloads, translating to just 1.8-3.6 joules per
    hour of adaptation before noticeably affecting battery life. Time-to-adaptation
    measures wall-clock time from receiving new data to achieving measurable improvement,
    accounting for opportunistic scheduling constraints that defer training to idle
    periods—this metric captures real-world adaptation speed including waiting for
    device idleness, charging status, and thermal headroom rather than just raw computational
    throughput.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 除了第12章中提到的推理指标，自适应系统需要专门的训练指标来捕捉在边缘约束下的学习效率。适应效率衡量的是每个训练样本消耗后的准确度提升，量化为资源约束下的学习曲线斜率——一个每100个训练样本实现2%准确度提升的系统比需要500个样本才能达到相同提升的系统具有更高的适应效率，这直接转化为更快的个性化服务和减少数据收集需求。内存约束下的收敛评估在指定的RAM预算内实现的验证损失，例如“在512
    KB训练足迹内收敛”，捕捉了系统在固定内存分配下的学习效率——这对于比较从微控制器到智能手机的设备类别之间的适应策略至关重要。每更新一次能耗量量化了每次梯度更新所消耗的毫焦耳，这是一个对于电池供电设备至关重要的指标，因为训练能耗直接影响用户体验——移动设备通常为持续ML工作负载预留500-1000
    mW，这意味着每小时适应所需的能量仅为1.8-3.6焦耳，在明显影响电池寿命之前。
- en: Evaluating whether local adaptation actually improves over global models requires
    personalization gain metrics that justify the overhead of on-device learning.
    Per-user performance delta measures accuracy improvement for the adapted model
    versus the global baseline on user-specific holdout data—systems should demonstrate
    statistically significant improvements, typically exceeding 2% accuracy gains,
    to justify the computational overhead, energy consumption, and complexity that
    adaptation introduces. Personalization-privacy tradeoff quantifies accuracy gain
    per unit of local data exposure, measuring the value extracted from privacy-sensitive
    information—this metric helps assess whether adaptation benefits outweigh the
    privacy costs of retaining user data locally, particularly important for applications
    handling sensitive information like health data or personal communications. Catastrophic
    forgetting rate measures degradation on the original task as the model adapts
    to local distributions through retention testing—acceptable forgetting rates depend
    on the application domain but typically should remain below 5% accuracy loss on
    original tasks to ensure that personalization does not come at the expense of
    the model’s general capabilities.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 评估局部适应是否真正优于全局模型需要个性化增益指标来证明设备上学习的开销。每个用户的性能变化衡量的是适应模型与全局基线在用户特定保留数据上的准确度提升——系统应显示出具有统计学意义的改进，通常超过2%的准确度提升，以证明计算开销、能耗和适应引入的复杂性是合理的。个性化-隐私权衡量化了每单位本地数据暴露的准确度提升，衡量从隐私敏感信息中提取的价值——这个指标有助于评估适应带来的好处是否超过了保留本地用户数据的隐私成本，这对于处理敏感信息如健康数据或个人通信的应用尤为重要。灾难性遗忘率衡量模型适应局部分布时原始任务的退化，通过保留测试来衡量——可接受的遗忘率取决于应用领域，但通常应保持在原始任务上5%以下准确度损失，以确保个性化不会以牺牲模型的一般能力为代价。
- en: When devices coordinate through federated learning ([Section 14.6](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e)),
    federated coordination cost metrics become critical for assessing system viability.
    Communication efficiency measures model accuracy improvement per byte transmitted,
    capturing the effectiveness of gradient compression and selective update strategies—modern
    federated systems achieve 10-100<semantics><mi>×</mi><annotation encoding="application/x-tex">\times</annotation></semantics>
    compression through quantization and sparsification techniques while maintaining
    95% or more of uncompressed accuracy, making the difference between practical
    and impractical mobile deployment. Stragglers impact quantifies convergence delay
    caused by slow or unreliable devices, measured as the difference in convergence
    time with versus without participation filters—effective straggler mitigation
    through asynchronous aggregation and selective participation reduces convergence
    time by 30-50% compared to synchronous approaches that wait for all devices. Aggregation
    quality evaluates global model performance as a function of device participation
    rate, revealing minimum viable participation thresholds below which federated
    learning fails to converge effectively—most federated systems require 10-20% device
    participation per round to maintain stable convergence, establishing clear requirements
    for client selection and availability management strategies.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 当设备通过联邦学习（[第14.6节](ch020.xhtml#sec-ondevice-learning-federated-learning-6e7e)）进行协调时，联邦协调成本指标对于评估系统可行性变得至关重要。通信效率衡量每传输一个字节模型精度提升的程度，捕捉梯度压缩和选择性更新策略的有效性——现代联邦系统通过量化和稀疏化技术实现了10-100倍的压缩，同时保持95%或以上的未压缩精度，这是实际和不可行的移动部署之间的区别。落后者影响量化了由慢速或不稳定设备引起的收敛延迟，测量为有与无参与滤波器的收敛时间差异——通过异步聚合和选择性参与的有效落后者缓解将收敛时间减少了30-50%，与同步方法相比，同步方法等待所有设备。聚合质量评估全局模型性能作为设备参与率的函数，揭示了联邦学习无法有效收敛的最小可行参与阈值——大多数联邦系统每轮需要10-20%的设备参与以维持稳定的收敛，为客户端选择和可用性管理策略确立了明确的要求。
- en: 'These training-specific benchmarks complement the inference metrics from [Chapter 12](ch018.xhtml#sec-benchmarking-ai),
    creating complete performance characterization for adaptive systems. Practical
    benchmarking must measure both dimensions: a system that achieves fast inference
    but slow adaptation, or efficient adaptation but poor final accuracy, fails to
    meet real-world requirements. The integration of inference and training benchmarks
    enables holistic evaluation of on-device learning systems across their full operational
    lifecycle.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 这些针对训练的基准测试补充了第12章（ch018.xhtml#sec-benchmarking-ai）中的推理指标，为自适应系统提供了完整的性能描述。实际的基准测试必须衡量这两个维度：一个系统如果实现了快速的推理但适应缓慢，或者高效的适应但最终精度差，则无法满足现实世界的需求。推理和训练基准的集成使得可以对设备端学习系统在其整个操作生命周期内进行全面的评估。
- en: Resource Management
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源管理
- en: On-device learning introduces resource contention modes absent in conventional
    inference-only deployments. Many edge devices are provisioned to run pretrained
    models efficiently but are rarely designed with training workloads in mind. Local
    adaptation therefore competes for scarce resources, including compute cycles,
    memory bandwidth, energy, and thermal headroom, with other system processes and
    user-facing applications.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 设备端学习引入了传统仅推理部署中不存在的资源竞争模式。许多边缘设备被配置为高效运行预训练模型，但很少考虑训练工作负载。因此，本地适应性与其他系统进程和面向用户的程序竞争有限的资源，包括计算周期、内存带宽、能源和热头房。
- en: The most direct constraint is compute availability. Training involves additional
    forward and backward passes through the model, which can exceed the cost of inference.
    Even when only a small subset of parameters is updated, for instance, in bias-only
    or head-only adaptation, backpropagation must still traverse the relevant layers,
    triggering increased instruction counts and memory traffic. On devices with shared
    compute units (e.g., mobile SoCs or embedded CPUs), this demand can delay interactive
    tasks, reduce frame rates, or impair sensor processing.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的约束是计算可用性。训练涉及通过模型进行额外的正向和反向传递，这可能会超过推理的成本。即使只有一小部分参数被更新，例如在仅偏置或仅头部适应的情况下，反向传播仍然需要遍历相关层，从而触发指令计数和内存流量的增加。在具有共享计算单元的设备（例如，移动SoC或嵌入式CPU）上，这种需求可能会延迟交互式任务，降低帧率或损害传感器处理。
- en: Energy consumption compounds this problem. Adaptation typically involves sustained
    computation over multiple input samples, which taxes battery-powered systems and
    may lead to rapid energy depletion. For instance, performing a single epoch of
    adaptation on a microcontroller-class device can consume several millijoules[36](#fn36)—an
    appreciable fraction of the energy budget for a duty-cycled system operating on
    harvested power. This necessitates careful scheduling, such that learning occurs
    only during idle periods, when energy reserves are high and user latency constraints
    are relaxed.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 能量消耗加剧了这个问题。适应通常涉及在多个输入样本上的持续计算，这会消耗电池供电系统的能量，并可能导致能量迅速耗尽。例如，在微控制器级设备上执行单个适应周期可能会消耗几毫焦耳[36](#fn36)——这对于一个基于收集能量的分时系统来说是一个相当大的能量预算部分。这需要仔细的调度，以确保学习仅在空闲期间进行，此时能量储备充足，用户延迟约束放松。
- en: From a memory perspective, training incurs higher peak usage than inference,
    due to the need to cache intermediate activations[37](#fn37), gradients, and optimizer
    state ([Ji Lin et al. 2020](ch058.xhtml#ref-lin2020mcunet)).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 从内存的角度来看，由于需要缓存中间激活[37](#fn37)、梯度以及优化器状态([Ji Lin等人 2020](ch058.xhtml#ref-lin2020mcunet))，训练产生的峰值使用率高于推理。
- en: These resource demands must also be balanced against quality of service (QoS)
    goals. Users expect edge devices to respond reliably and consistently, regardless
    of whether learning is occurring in the background. Any observable degradation,
    including dropped audio in a wake-word detector or lag in a wearable display,
    can erode user trust. These system reliability concerns parallel the operational
    challenges discussed in [Chapter 13](ch019.xhtml#sec-ml-operations). As such,
    many systems adopt opportunistic learning policies, where adaptation is suspended
    during foreground activity and resumed only when system load is low.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源需求也必须与服务质量（QoS）目标相平衡。用户期望边缘设备能够可靠且一致地响应，无论学习是否在后台进行。任何可观察到的退化，包括唤醒词检测器中的音频丢失或可穿戴显示器中的延迟，都可能侵蚀用户的信任。这些系统可靠性问题与第13章中讨论的操作挑战相平行。因此，许多系统采用机会性学习策略，即在后台活动期间暂停适应，仅在系统负载低时恢复。
- en: 'In some deployments, adaptation is further gated by cost constraints imposed
    by networked infrastructure. For instance, devices may offload portions of the
    learning workload to nearby gateways or cloudlets, introducing bandwidth and communication
    trade-offs. These hybrid models raise additional questions of task placement and
    scheduling: should the update occur locally, or be deferred until a high-throughput
    link is available?'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些部署中，适应还受到网络基础设施施加的成本约束的进一步限制。例如，设备可能将学习工作负载的一部分卸载到附近的网关或云小点，这引入了带宽和通信权衡。这些混合模型提出了关于任务放置和调度的额外问题：更新应该本地发生，还是推迟到高吞吐量链路可用时？
- en: In summary, the cost of on-device learning is not solely measured in FLOPs or
    memory usage. It manifests as a complex interplay of system load, user experience,
    energy availability, and infrastructure capacity. Addressing these challenges
    requires co-design across algorithmic, runtime, and hardware layers, ensuring
    that adaptation remains unobtrusive, efficient, and sustainable under real-world
    constraints.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，设备上学习的成本并不仅仅以FLOPs或内存使用量来衡量。它表现为系统负载、用户体验、能量可用性和基础设施容量之间复杂交互的结果。解决这些挑战需要算法、运行时和硬件层的协同设计，确保适应在现实世界约束下保持不引人注目、高效和可持续。
- en: Identifying and Preventing System Failures
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别和预防系统故障
- en: Understanding potential failure modes in on-device learning helps prevent costly
    deployment mistakes. Based on documented challenges in federated learning research
    ([Kairouz et al. 2021](ch058.xhtml#ref-kairouz2021advances)) and known risks in
    adaptive systems, several categories of failures warrant careful consideration.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 理解设备上学习的潜在故障模式有助于防止昂贵的部署错误。基于联邦学习研究中的记录挑战（[Kairouz 等人 2021](ch058.xhtml#ref-kairouz2021advances)）和自适应系统中已知的风险，几个类别的故障需要仔细考虑。
- en: The most fundamental risk in on-device learning is unbounded adaptation drift,
    where continuous learning without constraints causes models to gradually diverge
    from their intended behavior. Consider a hypothetical keyboard prediction system
    that learns from all user inputs including corrections—it might begin incorporating
    typos as valid suggestions, leading to progressively degraded predictions. This
    risk becomes acute in health monitoring applications where gradual changes in
    user baselines could be learned as “normal,” potentially causing the system to
    miss important anomalies that would have been detected by a static model. The
    insidious nature of this drift is that it occurs slowly and locally, making detection
    difficult without proper monitoring infrastructure.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习最基本的风险是无界适应漂移，即无约束的持续学习导致模型逐渐偏离其预期行为。考虑一个假设的键盘预测系统，它从所有用户输入包括纠正中学习——它可能会开始将错误输入作为有效建议纳入，导致预测质量逐渐下降。在健康监测应用中，这种风险变得尤为严重，因为用户基线中的渐进变化可能会被学习为“正常”，可能导致系统错过静态模型能够检测到的重要异常。这种漂移的隐蔽性在于它缓慢且局部地发生，没有适当的监控基础设施很难检测到。
- en: 'Beyond individual device drift, federated learning systems face the challenge
    of participation bias amplification at the population level. Devices with reliable
    power and connectivity participate more frequently in federated rounds ([T. Li
    et al. 2020](ch058.xhtml#ref-li2020federated)). This uneven participation creates
    scenarios where models become increasingly optimized for users with high-end devices
    while performance degrades for those with limited resources. The resulting feedback
    loop exacerbates digital inequality: better-served users receive increasingly
    better models, while underserved populations experience declining performance,
    reducing their engagement and further diminishing their representation in training
    rounds ([J. Wang et al. 2021](ch058.xhtml#ref-wang2021field)). These fairness
    and bias amplification concerns highlight the ethical implications of distributed
    learning systems.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 除了个别设备漂移之外，联邦学习系统在人口层面上面临着参与偏差放大的挑战。具有可靠电源和连接性的设备更频繁地参与联邦轮次（[T. Li 等人 2020](ch058.xhtml#ref-li2020federated)）。这种不均匀的参与创造了模型越来越优化高端设备用户而性能对资源有限的用户下降的场景。由此产生的反馈循环加剧了数字不平等：服务较好的用户获得越来越好的模型，而服务不足的人群则经历性能下降，减少他们的参与，进一步减少他们在训练轮次中的代表性（[J.
    Wang 等人 2021](ch058.xhtml#ref-wang2021field)）。这些公平性和偏差放大问题突出了分布式学习系统的伦理影响。
- en: 'These systematic biases interact with data quality issues to create autocorrection
    feedback loops, particularly in text-based applications. When systems cannot distinguish
    between intended inputs and corrections, they may develop unexpected behaviors.
    Frequently corrected domain-specific terminology might be incorrectly learned
    as errors, leading to inappropriate suggestions in professional contexts. This
    problem compounds the drift issue: not only do models adapt to individual quirks,
    but they may also learn from their own mistakes when users accept autocorrections
    without realizing the system is learning from these interactions.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统性偏差与数据质量问题相互作用，形成自纠正反馈循环，尤其是在基于文本的应用中。当系统无法区分预期输入和纠正时，它们可能会表现出意外的行为。经常被纠正的特定领域术语可能会被错误地学习为错误，导致在专业环境中提出不恰当的建议。这个问题加剧了漂移问题：不仅模型适应了个人的怪癖，而且当用户接受自动纠正而没有意识到系统正在从这些交互中学习时，它们也可能从自己的错误中学习。
- en: The interconnected nature of these failure modes, from individual drift to population
    bias to data quality degradation, underscores the importance of implementing comprehensive
    safety mechanisms. Successful deployments require bounded adaptation ranges to
    prevent unbounded drift, stratified sampling to address participation bias, careful
    data filtering to avoid learning from corrections as ground truth, and shadow
    evaluation against static baselines to detect degradation. While specific production
    incidents are rarely publicized due to competitive and privacy concerns, the research
    community has identified these patterns as critical areas requiring systematic
    mitigation strategies ([T. Li et al. 2020](ch058.xhtml#ref-li2020federated); [Kairouz
    et al. 2021](ch058.xhtml#ref-kairouz2021advances)).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这些故障模式之间的相互关联性，从个体漂移到群体偏差再到数据质量下降，强调了实施全面安全机制的重要性。成功的部署需要有限的适应范围以防止无限制的漂移，分层抽样以解决参与偏差，仔细的数据过滤以避免从校正中学习作为真实情况，以及与静态基线的影子评估以检测退化。尽管由于竞争和隐私问题，具体的生产事件很少公开，但研究界已将这些模式识别为需要系统缓解策略的关键领域([T.
    Li 等人 2020](ch058.xhtml#ref-li2020federated); [Kairouz 等人 2021](ch058.xhtml#ref-kairouz2021advances))。
- en: Production Deployment Risk Assessment
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产部署风险评估
- en: The deployment of adaptive models on edge devices introduces challenges that
    extend beyond technical feasibility. In domains where compliance, auditability,
    and regulatory approval are necessary, including healthcare, finance, and safety-important
    systems, on-device learning poses a core tension between system autonomy and control.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上部署自适应模型引入了超出技术可行性的挑战。在需要合规性、可审计性和监管批准的领域，包括医疗保健、金融和安全性重要的系统，设备上的学习在系统自主性和控制之间构成了核心紧张关系。
- en: In traditional machine learning pipelines, all model updates are centrally managed,
    versioned, and validated. The training data, model checkpoints, and evaluation
    metrics are typically recorded in reproducible workflows that support traceability.
    When learning occurs on the device itself, however, this visibility is lost. Each
    device may independently evolve its model parameters, influenced by unique local
    data streams that are never observed by the developer or system maintainer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习管道中，所有模型更新都是集中管理、版本控制和验证的。训练数据、模型检查点和评估指标通常记录在可重现的工作流程中，支持可追溯性。然而，当学习发生在设备本身时，这种可见性就会丧失。每个设备可能独立地发展其模型参数，受到开发者或系统维护者从未观察到的独特本地数据流的影响。
- en: This autonomy creates a validation gap. Without access to the input data or
    the exact update trajectory, it becomes difficult to verify that the learned model
    still adheres to its original specification or performance guarantees. This is
    especially problematic in regulated industries, where certification depends on
    demonstrating that a system behaves consistently across defined operational boundaries.
    A device that updates itself in response to real-world usage may drift outside
    those bounds, triggering compliance violations without any external signal.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自主性产生了验证差距。没有访问输入数据或确切更新轨迹，很难验证学习到的模型是否仍然遵循其原始规范或性能保证。这在受监管的行业中尤其成问题，因为认证取决于证明系统在定义的操作边界内表现一致。一个响应现实世界使用而自行更新的设备可能会超出这些边界，在没有外部信号的情况下触发合规违规。
- en: The lack of centralized oversight complicates rollback and failure recovery.
    If a model update degrades performance, it may not be immediately detectable,
    particularly in offline scenarios or systems without telemetry. By the time failure
    is observed, the system’s internal state may have diverged significantly from
    any known checkpoint, making diagnosis and recovery more complex than in static
    deployments. This necessitates robust safety mechanisms, such as conservative
    update thresholds, rollback caches, or dual-model architectures that retain a
    verified baseline.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏集中监管使得回滚和故障恢复变得复杂。如果模型更新降低了性能，可能不会立即被发现，尤其是在离线场景或没有遥测功能的系统中。当观察到故障时，系统的内部状态可能已经与任何已知的检查点有显著差异，这使得诊断和恢复比在静态部署中更为复杂。这需要强大的安全机制，例如保守的更新阈值、回滚缓存或保留经过验证基线的双模型架构。
- en: In addition to compliance challenges, on-device learning introduces new security
    vulnerabilities. Because model adaptation occurs locally and relies on device-specific,
    potentially untrusted data streams, adversaries may attempt to manipulate the
    learning process by tampering with stored data, such as replay buffers, or by
    injecting poisoned examples during adaptation, to degrade model performance or
    introduce vulnerabilities. Any locally stored adaptation data, such as feature
    embeddings or few-shot examples, must be secured against unauthorized access to
    prevent unintended information leakage.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 除了合规挑战之外，设备上的学习还引入了新的安全漏洞。由于模型适应是本地发生的，并且依赖于特定于设备的、可能不可信的数据流，攻击者可能会通过篡改存储的数据（如重放缓冲区）或在适应过程中注入毒化示例来操纵学习过程，以降低模型性能或引入漏洞。任何本地存储的适应数据，如特征嵌入或少样本示例，都必须得到保护，防止未经授权的访问，以防止意外信息泄露。
- en: 'Maintaining model integrity over time is particularly difficult in decentralized
    settings, where central monitoring and validation are limited. Autonomous updates
    could, without external visibility, cause models to drift into unsafe or biased
    states. These risks are compounded by compliance obligations such as the GDPR’s
    right to erasure: if user data subtly influences a model through adaptation, tracking
    and reversing that influence becomes complex.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在去中心化环境中维护模型完整性尤其困难，因为中央监控和验证有限。自主更新可能在没有外部可见性的情况下导致模型漂移到不安全或偏颇的状态。这些风险通过合规义务（如GDPR的删除权）进一步加剧：如果用户数据通过适应微妙地影响模型，跟踪和逆转这种影响变得复杂。
- en: The security and integrity of self-adapting models, particularly at the edge,
    pose important open challenges. A comprehensive treatment of these threats and
    corresponding mitigation strategies requires specialized security frameworks for
    distributed ML systems.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应模型，尤其是在边缘，的安全性和完整性提出了重要的开放挑战。对这些威胁和相应的缓解策略的全面处理需要为分布式机器学习系统提供专门的网络安全框架。
- en: Privacy regulations also interact with on-device learning in nontrivial ways.
    While local adaptation can reduce the need to transmit sensitive data, it may
    still require storage and processing of personal information, including sensor
    traces or behavioral logs, on the device itself. These privacy considerations
    require careful attention to security frameworks and regulatory compliance. Depending
    on jurisdiction, this may invoke additional requirements for data retention, user
    consent, and auditability. Systems must be designed to satisfy these requirements
    without compromising adaptation effectiveness, which often involves encrypting
    stored data, enforcing retention limits, or implementing user-controlled reset
    mechanisms.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私法规也与设备上的学习以非平凡的方式相互作用。虽然本地适应可以减少传输敏感数据的需要，但它可能仍然需要在设备本身上存储和处理个人信息，包括传感器轨迹或行为日志。这些隐私考虑需要仔细关注安全框架和法规合规。根据司法管辖区，这可能会引发对数据保留、用户同意和可审计性的额外要求。系统必须设计得满足这些要求，而不损害适应的有效性，这通常涉及加密存储数据、实施保留限制或实施用户控制的重置机制。
- en: Lastly, the emergence of edge learning raises open questions about accountability
    and liability ([Brakerski et al. 2022](ch058.xhtml#ref-brakerski2022federated)).
    When a model adapts autonomously, who is responsible for its behavior? If an adapted
    model makes a faulty decision, such as misdiagnosing a health condition or misinterpreting
    a voice command, the root cause may lie in local data drift, poor initialization,
    or insufficient safeguards. Without standardized mechanisms for capturing and
    analyzing these failure modes, responsibility may be difficult to assign, and
    regulatory approval harder to obtain.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，边缘学习的出现提出了关于问责制和责任（[Brakerski等人2022](ch058.xhtml#ref-brakerski2022federated)）的开放性问题。当模型自主适应时，谁对其行为负责？如果适应后的模型做出了错误的决定，例如误诊健康状况或误解语音命令，其根本原因可能在于本地数据漂移、初始化不良或安全措施不足。如果没有标准化的机制来捕捉和分析这些故障模式，责任可能难以分配，监管批准也更难获得。
- en: Addressing these deployment and compliance risks requires new tooling, protocols,
    and design practices that support auditable autonomy—the ability of a system to
    adapt in place while still satisfying external requirements for traceability,
    reproducibility, and user protection. As on-device learning becomes more prevalent,
    these challenges will become central to both system architecture and governance
    frameworks.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些部署和合规风险需要新的工具、协议和设计实践，以支持可审计的自主性——系统在满足外部对可追溯性、可重复性和用户保护的要求的同时，能够就地适应。随着设备端学习变得更加普遍，这些挑战将成为系统架构和治理框架的核心。
- en: Engineering Challenge Synthesis
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工程挑战综合
- en: Designing on-device ML systems involves navigating a complex landscape of technical
    and practical constraints. While localized adaptation allows personalization,
    privacy, and responsiveness, it also introduces a range of challenges that span
    hardware heterogeneity, data fragmentation, observability, and regulatory compliance.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 设计设备端机器学习系统涉及在技术和实践约束的复杂景观中导航。虽然局部适应允许个性化、隐私和响应性，但它也引入了一系列挑战，这些挑战跨越了硬件异构性、数据碎片化、可观察性和法规遵从性。
- en: System heterogeneity complicates deployment and optimization by introducing
    variation in compute, memory, and runtime environments. Non-IID data distributions
    challenge learning stability and generalization, especially when models are trained
    on-device without access to global context. The absence of centralized monitoring
    makes it difficult to validate updates or detect performance regressions, and
    training activity must often compete with core device functionality for energy
    and compute. Finally, post-deployment learning introduces complications in model
    governance, from auditability and rollback to privacy assurance.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 系统异构性通过引入计算、内存和运行时环境的变化，使部署和优化复杂化。非独立同分布数据分布挑战学习稳定性和泛化能力，尤其是在模型在设备上训练且无法访问全局上下文的情况下。缺乏集中监控使得验证更新或检测性能退步变得困难，并且训练活动必须经常与核心设备功能竞争能源和计算资源。最后，部署后的学习在模型治理方面引入了复杂性，从可审计性和回滚到隐私保证。
- en: These challenges are not isolated—they interact in ways that influence the viability
    of different adaptation strategies. [Table 14.6](ch020.xhtml#tbl-ondevice-challenges)
    summarizes the primary challenges and their implications for ML systems deployed
    at the edge.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战并非孤立存在——它们以影响不同适应策略可行性的方式相互作用。[表14.6](ch020.xhtml#tbl-ondevice-challenges)总结了主要挑战及其对边缘部署的机器学习系统的影响。
- en: 'Table 14.6: **On-Device Learning Challenges**: System heterogeneity, non-IID
    data, and limited resources introduce unique challenges for deploying and adapting
    machine learning models on edge devices, impacting portability, stability, and
    governance. The table details root causes of these challenges and their system-level
    implications, highlighting trade-offs between model performance and resource constraints.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.6：**设备端学习挑战**：系统异构性、非独立同分布数据以及有限的资源为在边缘设备上部署和适应机器学习模型带来了独特的挑战，影响了可移植性、稳定性和治理。该表详细说明了这些挑战的根本原因及其系统级影响，突出了模型性能与资源约束之间的权衡。
- en: '| **Challenge** | **Root Cause** | **System-Level Implications** |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| **挑战** | **根本原因** | **系统级影响** |'
- en: '| --- | --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **System Heterogeneity** | Diverse hardware, software, and toolchains | Limits
    portability; requires platform-specific tuning |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| **系统异构性** | 多样化的硬件、软件和工具链 | 限制了可移植性；需要针对特定平台进行调整'
- en: '| **Non-IID and Fragmented Data** | Localized, user-specific data distributions
    | Hinders generalization; increases risk of drift |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| **非独立同分布和碎片化数据** | 本地化、用户特定的数据分布 | 阻碍了泛化；增加了漂移风险'
- en: '| **Limited Observability and Feedback** | No centralized testing or logging
    | Makes update validation and debugging difficult |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| **有限的可观察性和反馈** | 没有集中的测试或日志记录 | 使得更新验证和调试困难'
- en: '| **Resource Contention and Scheduling** | Competing demands for memory, compute,
    and battery | Requires dynamic scheduling and budget-aware learning |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| **资源竞争和调度** | 内存、计算和电池的竞争需求 | 需要动态调度和预算感知学习'
- en: '| **Deployment and Compliance Risk** | Learning continues post-deployment |
    Complicates model versioning, auditing, and rollback |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| **部署和合规风险** | 部署后继续学习 | 复杂化模型版本控制、审计和回滚'
- en: Foundations for Robust AI Systems
  id: totrans-538
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稳健人工智能系统的基础
- en: The operational challenges and failure modes explored in the preceding sections
    reveal vulnerabilities that extend beyond deployment concerns into fundamental
    system reliability. When models adapt autonomously across millions of heterogeneous
    devices, three categories of threats emerge that traditional centralized training
    never encounters.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 前几节探讨的操作挑战和故障模式揭示了超出部署担忧的系统基本可靠性方面的漏洞。当模型在数百万个异构设备上自主适应时，会出现三种传统集中式训练从未遇到的威胁类别。
- en: First, unlike centralized systems where failures are localized and observable
    (as discussed in [Chapter 13](ch019.xhtml#sec-ml-operations)), on-device learning
    creates scenarios where local failures can propagate silently across device populations.
    A corrupted adaptation on one device, if aggregated through federated learning,
    can poison the global model. Hardware faults that would trigger errors in centralized
    infrastructure may silently corrupt gradients on edge devices with minimal error
    detection capabilities.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 第一，与故障局部化和可观察的集中式系统（如第13章[sec-ml-operations]所述）不同，设备上的学习创造了本地故障可以无声传播到设备群体中的场景。如果一个设备上的损坏适应通过联邦学习聚合，可能会毒害全局模型。在集中式基础设施中会触发错误的硬件故障可能在边缘设备上静默地损坏梯度，而检测错误的能力最小。
- en: Second, the federated coordination mechanisms that enable collaborative learning
    also create new attack surfaces. Adversarial clients can inject poisoned gradients[38](#fn38)
    designed to degrade global model performance. Model inversion attacks can extract
    private information from shared updates despite aggregation. The distributed nature
    of on-device learning makes these attacks both easier to execute (compromising
    client devices) and harder to detect (no centralized validation).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，使协作学习成为可能的联邦协调机制也创造了新的攻击面。敌对客户端可以注入旨在降低全局模型性能的有毒梯度[38](#fn38)。即使聚合，模型反演攻击也可以从共享更新中提取私有信息。设备学习分布的特性使得这些攻击既容易执行（损害客户端设备）又难以检测（没有集中式验证）。
- en: Third, on-device systems must handle distribution shifts and environmental changes
    without access to labeled validation data. Models may confidently drift into failure
    modes, adapting to local biases or temporary anomalies. The non-IID data distributions
    across devices mean that local drift on individual devices may not trigger global
    alarms, allowing silent degradation.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，设备上的系统必须在无法访问标记验证数据的情况下处理分布变化和环境变化。模型可能会自信地漂移到故障模式，适应局部偏差或暂时异常。设备间的非独立同分布数据分布意味着单个设备上的局部漂移可能不会触发全局警报，从而允许无声退化。
- en: These reliability threats demand systematic approaches that ensure on-device
    learning systems remain robust despite autonomous adaptation, malicious manipulation,
    and environmental uncertainty. [Chapter 16](ch022.xhtml#sec-robust-ai) examines
    these challenges comprehensively, establishing principles for fault-tolerant AI
    systems that can maintain reliability despite hardware faults, adversarial attacks,
    and distribution shifts. The techniques developed there—Byzantine-resilient aggregation,
    adversarial training, and drift detection—become essential components of production-ready
    on-device learning systems rather than optional enhancements.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可靠性威胁需要系统性的方法来确保设备上的学习系统在自主适应、恶意操纵和环境不确定性下保持鲁棒性。[第16章](ch022.xhtml#sec-robust-ai)全面考察了这些挑战，确立了容错AI系统的原则，这些系统能够在硬件故障、对抗性攻击和数据分布变化的情况下保持可靠性。在那里开发的拜占庭容错聚合、对抗性训练和漂移检测技术成为生产就绪的设备学习系统的基本组成部分，而不是可选的增强功能。
- en: The privacy-preserving aspects of these robustness mechanisms, including secure
    aggregation and differential privacy, connect directly to [Chapter 15](ch021.xhtml#sec-security-privacy),
    which establishes the cryptographic foundations and privacy guarantees necessary
    for deploying self-learning systems at scale while maintaining user trust and
    regulatory compliance.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这些鲁棒性机制的隐私保护方面，包括安全聚合和差分隐私，直接关联到[第15章](ch021.xhtml#sec-security-privacy)，该章节建立了部署大规模自学习系统所需的密码学基础和隐私保障，同时保持用户信任和合规性。
- en: Fallacies and Pitfalls
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谬误和陷阱
- en: On-device learning operates in a fundamentally different environment from cloud-based
    training, with severe resource constraints and privacy requirements that challenge
    traditional machine learning assumptions. The appeal of local adaptation and privacy
    preservation can obscure the significant technical limitations and implementation
    challenges that determine whether on-device learning provides net benefits over
    simpler alternatives.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上的学习在本质上与基于云的训练环境不同，面临着严重的资源限制和隐私要求，这挑战了传统的机器学习假设。本地适应和隐私保护的吸引力可能会掩盖决定设备学习是否比简单替代方案提供净收益的重大技术限制和实施挑战。
- en: '**Fallacy:** *On-device learning provides the same adaptation capabilities
    as cloud-based training.*'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *设备学习提供了与基于云的训练相同的适应能力。*'
- en: This misconception leads teams to expect that local learning can achieve the
    same model improvements as centralized training with abundant computational resources.
    On-device learning operates under severe constraints including limited memory,
    restricted computational power, and minimal energy budgets that fundamentally
    limit adaptation capabilities. Local datasets are typically small, biased, and
    non-representative, making it impossible to achieve the same generalization performance
    as centralized training. Effective on-device learning requires accepting these
    limitations and designing adaptation strategies that provide meaningful improvements
    within practical constraints rather than attempting to replicate cloud-scale learning
    capabilities. This necessitates an efficiency-first mindset and careful optimization
    techniques.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 这种误解导致团队期望本地学习能够实现与集中训练相同的模型改进，而集中训练拥有丰富的计算资源。设备学习在严重的限制下运行，包括有限的内存、受限的计算能力和最小的能源预算，这些从根本上限制了适应能力。本地数据集通常是小的、有偏见的和非代表性的，这使得无法实现与集中训练相同的泛化性能。有效的设备学习需要接受这些限制，并设计适应策略，在实用限制内提供有意义的改进，而不是试图复制云规模的学习能力。这需要一种以效率为先的思维方式和仔细的优化技术。
- en: '**Pitfall:** *Assuming that federated learning automatically preserves privacy
    without additional safeguards.*'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *假设联邦学习自动保护隐私，无需额外的安全措施。*'
- en: Many practitioners believe that keeping data on local devices inherently provides
    privacy protection without considering the information that can be inferred from
    model updates. Gradient and parameter updates can leak significant information
    about local training data through various inference attacks. Device participation
    patterns, update frequencies, and model convergence behaviors can reveal sensitive
    information about users and their activities. True privacy preservation requires
    additional mechanisms like differential privacy (mathematical guarantees that
    individual data points cannot be inferred from model outputs), secure aggregation
    protocols that prevent parameter inspection, and careful communication protocols
    rather than relying solely on data locality.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 许多从业者认为，在本地设备上保留数据本质上提供了隐私保护，而没有考虑到可以从模型更新中推断出的信息。梯度更新和参数更新可以通过各种推理攻击泄露关于本地训练数据的重大信息。设备参与模式、更新频率和模型收敛行为可以揭示关于用户及其活动的敏感信息。真正的隐私保护需要额外的机制，如差分隐私（数学上保证单个数据点不能从模型输出中推断出来）、防止参数检查的安全聚合协议以及仔细的通信协议，而不是仅仅依赖于数据本地性。
- en: '**Fallacy:** *Resource-constrained adaptation always produces better personalized
    models than generic models.*'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '**谬误：** *资源受限的适应总是比通用模型产生更好的个性化模型。*'
- en: This belief assumes that any local adaptation is beneficial regardless of the
    quality or quantity of local data available. On-device learning with insufficient,
    noisy, or biased local data can actually degrade model performance compared to
    well-trained generic models. Small datasets may not provide enough signal for
    meaningful learning, while adaptation to local noise can harm generalization.
    Effective on-device learning systems must include mechanisms to detect when local
    adaptation is beneficial and fall back to generic models when local data is inadequate
    for reliable learning.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信念假设，无论可用的本地数据的质量或数量如何，任何本地适应都是有益的。在设备上使用不足、噪声或偏颇的本地数据进行学习实际上可能会降低模型性能，与训练良好的通用模型相比。小数据集可能无法提供足够的信息进行有意义的学习，而适应本地噪声可能会损害泛化能力。有效的设备学习系统必须包括机制来检测何时本地适应是有益的，并在本地数据不足以进行可靠学习时回退到通用模型。
- en: '**Pitfall:** *Ignoring the heterogeneity challenges across different device
    types and capabilities.*'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *忽视不同设备类型和能力之间的异质性挑战。*'
- en: Teams often design on-device learning systems assuming uniform hardware capabilities
    across deployment devices. Real-world deployments span diverse hardware with varying
    computational power, memory capacity, energy constraints, and networking capabilities.
    A learning algorithm that works well on high-end smartphones may fail catastrophically
    on resource-constrained IoT devices[39](#fn39).
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 团队通常在设计设备上学习系统时假设部署设备具有统一的硬件能力。现实世界的部署涵盖了具有不同计算能力、内存容量、能源限制和网络能力的各种硬件。一个在高端智能手机上表现良好的学习算法可能在资源受限的物联网设备上失败得非常严重[39](#fn39)。
- en: '**Pitfall:** *Underestimating the complexity of orchestrating learning across
    distributed edge systems.*'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '**陷阱：** *低估在分布式边缘系统中协调学习的复杂性。*'
- en: Many teams focus on individual device optimization without considering the system-level
    challenges of coordinating learning across thousands or millions of edge devices.
    Edge systems orchestration must handle intermittent connectivity, varying power
    states, different time zones, and unpredictable device availability patterns that
    create complex scheduling and synchronization challenges. Device clustering, federated
    rounds coordination, model versioning across diverse deployment contexts, and
    handling partial participation from unreliable devices require sophisticated infrastructure
    beyond simple aggregation servers. Additionally, real-world edge deployments involve
    multiple stakeholders with different incentives, security requirements, and operational
    procedures that must be balanced against learning objectives. Effective edge learning
    systems require robust orchestration frameworks that can maintain system coherence
    despite constant device churn, network partitions, and operational disruptions.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 许多团队专注于单个设备的优化，而没有考虑到协调数千或数百万边缘设备进行学习的系统级挑战。边缘系统编排必须处理间歇性连接、不同的电源状态、不同的时区和不可预测的设备可用性模式，这些都创造了复杂的调度和同步挑战。设备聚类、联邦轮次协调、跨不同部署环境中的模型版本控制以及处理不可靠设备的部分参与需要超越简单聚合服务器的复杂基础设施。此外，现实世界的边缘部署涉及多个利益相关者，他们有不同的激励措施、安全要求和操作程序，这些必须与学习目标相平衡。有效的边缘学习系统需要能够维持系统连贯性的强大编排框架，即使在设备不断更替、网络分区和操作中断的情况下也能保持。
- en: Summary
  id: totrans-557
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: On-device learning represents a fundamental shift from static, centralized training
    to dynamic, local adaptation directly on deployment devices. This paradigm enables
    machine learning systems to personalize experiences while preserving privacy,
    reduce network dependencies, and respond rapidly to changing local conditions.
    Success requires integrating optimization principles, understanding hardware constraints,
    and applying sound operational practices. The transition from traditional cloud-based
    training to edge-based learning requires overcoming severe computational, memory,
    and energy constraints that fundamentally reshape how models are designed and
    adapted.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习代表了从静态、集中式训练到动态、本地部署设备的直接适应的根本转变。这种范式使机器学习系统能够个性化体验同时保护隐私，减少网络依赖，并快速响应本地条件的变化。成功需要整合优化原则、理解硬件限制并应用合理的操作实践。从传统的基于云的训练到基于边缘的学习的转变需要克服严重的计算、内存和能源限制，这从根本上改变了模型的设计和适应方式。
- en: The technical strategies that enable practical on-device learning span multiple
    dimensions of system design. Adaptation techniques range from lightweight bias-only
    updates to selective parameter tuning, each offering different tradeoffs between
    expressivity and resource efficiency. Data efficiency becomes paramount when learning
    from limited local examples, driving innovations in few-shot learning[40](#fn40),
    streaming adaptation, and memory-based replay mechanisms[41](#fn41).
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 能够实现设备上实际学习的专业技术策略涵盖了系统设计的多个维度。适应技术从轻量级的仅更新偏差到选择性的参数调整不等，每种方法都提供了在表达性和资源效率之间的不同权衡。当从有限的本地示例中进行学习时，数据效率变得至关重要，这推动了少量样本学习[40](#fn40)、流式适应和基于内存的重放机制[41](#fn41)的创新。
- en: '**Key Takeaways**'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: On-device learning shifts machine learning from static deployment to dynamic
    local adaptation, enabling personalization while preserving privacy
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备上学习将机器学习从静态部署转变为动态本地适应，在保留隐私的同时实现个性化
- en: 'Resource constraints drive specialized techniques: bias-only updates, adapter
    modules, sparse parameter updates, and compressed data representations'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源限制推动了专用技术的出现：仅更新偏差、适配器模块、稀疏参数更新和压缩数据表示
- en: Federated learning coordinates distributed training across heterogeneous devices
    while maintaining privacy and handling non-IID data distributions
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 联邦学习协调异构设备上的分布式训练，同时保持隐私并处理非-IID数据分布
- en: Success requires co-designing algorithms with hardware constraints, balancing
    adaptation capability against memory, energy, and computational limitations
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功需要与硬件约束协同设计算法，在适应能力与内存、能源和计算限制之间取得平衡
- en: Real-world applications demonstrate both the potential and challenges of on-device
    learning, from keyword spotting systems that adapt to user voices to recommendation
    engines that personalize without transmitting user data. As machine learning expands
    into mobile, embedded, and wearable environments, the ability to learn locally
    while maintaining efficiency and reliability becomes essential for next-generation
    intelligent systems that operate seamlessly across diverse deployment contexts.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界的应用展示了设备上学习的潜力和挑战，从适应用户声音的关键词检测系统到不传输用户数据的个性化推荐引擎。随着机器学习扩展到移动、嵌入式和可穿戴环境，在保持效率和可靠性的同时本地学习的能力对于无缝运行于各种部署环境中的下一代智能系统变得至关重要。
- en: The distributed nature of on-device learning introduces new vulnerabilities
    that extend beyond individual device constraints. The very capabilities that make
    these systems powerful—learning from user data, adapting to local patterns, coordinating
    across devices—also create new attack surfaces and privacy risks. These adaptive
    systems must not only function correctly but also protect sensitive user information
    and defend against adversarial manipulation. Security and privacy frameworks ([Chapter 15](ch021.xhtml#sec-security-privacy))
    address these critical concerns, showing how to protect on-device learning systems
    from both privacy breaches and adversarial attacks. Subsequently, the robust AI
    principles ([Chapter 16](ch022.xhtml#sec-robust-ai)) extend these protections
    to encompass system-wide reliability challenges including hardware failures and
    software faults, while ML Operations ([Chapter 13](ch019.xhtml#sec-ml-operations))
    provides the comprehensive framework for deploying and maintaining these complex
    adaptive systems in production.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上学习的分布式特性引入了新的漏洞，这些漏洞不仅超越了单个设备限制。正是这些系统强大的能力——从用户数据中学习、适应本地模式、跨设备协调——也创造了新的攻击面和隐私风险。这些自适应系统不仅必须正确运行，还必须保护敏感用户信息并防御对抗性操纵。安全和隐私框架（[第15章](ch021.xhtml#sec-security-privacy)）解决这些关键问题，展示了如何保护设备上学习系统免受隐私泄露和对抗性攻击。随后，鲁棒人工智能原则（[第16章](ch022.xhtml#sec-robust-ai)）将这些保护扩展到涵盖系统级可靠性挑战，包括硬件故障和软件错误，而机器学习操作（[第13章](ch019.xhtml#sec-ml-operations)）提供了部署和维护这些复杂自适应系统的全面框架。
- en: '* * *'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
