<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.2.4 Other</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.2.4 Other</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html">https://huyenchip.com/ml-interviews-book/contents/8.2.4-other.html</a></blockquote>
                                
                                
<ol>
<li>[M] An autoencoder is a neural network that learns to copy its input to its output. When would this be useful?</li>
<li>Self-attention.<ol>
<li>[E] What’s the motivation for self-attention?</li>
<li>[E] Why would you choose a self-attention architecture over RNNs or CNNs?</li>
<li>[M] Why would you need multi-headed attention instead of just one head for attention?</li>
<li>[M] How would changing the number of heads in multi-headed attention affect the model’s performance?</li>
</ol>
</li>
<li>Transfer learning<ol>
<li>[E] You want to build a classifier to predict sentiment in tweets but you have very little labeled data (say 1000). What do you do?</li>
<li>[M] What’s gradual unfreezing? How might it help with transfer learning?</li>
</ol>
</li>
<li>Bayesian methods.<ol>
<li>[M] How do Bayesian methods differ from the mainstream deep learning approach?</li>
<li>[M] How are the pros and cons of Bayesian neural networks compared to the mainstream neural networks?</li>
<li>[M] Why do we say that Bayesian neural networks are natural ensembles?</li>
</ol>
</li>
<li>GANs.<ol>
<li>[E] What do GANs converge to?</li>
<li>[M] Why are GANs so hard to train?</li>
</ol>
</li>
</ol>

                                
                                    
</body>
</html>