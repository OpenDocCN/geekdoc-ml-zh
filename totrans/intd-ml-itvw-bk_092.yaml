- en: '8.1.1 Overview: Basic algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html](https://huyenchip.com/ml-interviews-book/contents/8.1.1-overview:-basic-algorithm.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 8.1.1.1 k-nearest neighbor (k-NN)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: k-NN is a non-parametric method used for classification and regression. Given
    an object, the algorithm’s output is computed from its k closest training examples
    in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: In k-NN classification, each object is classified into the class most common
    among its k nearest neighbors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In k-NN regression, each object’s value is calculated as the average of the
    values of its k nearest neighbors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applications**: anomaly detection, search, recommender system'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.2 k-means clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: k-means clustering aims to partition observations into k clusters in which each
    observation belongs to the cluster with the nearest mean. k-means minimizes within-cluster
    variances (squared Euclidean distances), but not regular Euclidean distances.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm doesn’t guarantee convergence to the global optimum. The result
    may depend on the initial clusters. As the algorithm is usually fast, it is common
    to run it multiple times with different starting conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is computationally difficult (NP-hard); however, efficient heuristic
    algorithms converge quickly to a local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm has a loose relationship to the k-nearest neighbor classifier.
    After obtaining clusters using k-means clustering, we can classify new data into
    those clusters by applying the 1-nearest neighbor classifier to the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications**: Vector quantization for signal processing (where k-means
    clustering was originally developed), cluster analysis, feature learning, topic
    modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.3 EM (expectation-maximization) algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: EM algorithm is an iterative method to find maximum likelihood (MLE) or maximum
    a posteriori (MAP) estimates of parameters. It’s useful when the model depends
    on unobserved latent variables and equations can’t be solved directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iteration alternates between performing:'
  prefs: []
  type: TYPE_NORMAL
- en: an expectation (E) step, which creates a function for the expectation of the
    log-likelihood evaluated using the current estimate for the parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a maximization (M) step, which computes parameters maximizing the expected log-likelihood
    found on the E step. These parameter-estimates are then used to determine the
    distribution of the latent variables in the next E step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EM algorithm is guaranteed to return a local optimum of the sample likelihood
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Gaussian mixture models (GMM)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications**: Data clustering, collaborative filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.4 Tree-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '**Decision tree** is a tree-based method that goes from observations about
    an object (represented in the branches) to conclusions about its target value
    (represented in the leaves). At its core, decision trees are nest if-else conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: In **classification trees**, the target value is discrete and each leaf represents
    a class. In **regression trees**, the target value is continuous and each leaf
    represents the mean of the target values of all objects that end up with that
    leaf.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are easy to interpret and can be used to visualize decisions.
    However, they are overfit to the data they are trained on -- small changes to
    the training set can result in significantly different tree structures, which
    lead to significantly different outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.5 Bagging and boosting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Bagging and boosting are two popular ensembling methods commonly used with tree-based
    algorithms that can also be used for other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.5.1 Bagging
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bagging, shortened for **b**ootstrap **agg**regat**ing**, is designed to improve
    the stability and accuracy of ML algorithms. It reduces variance and helps to
    avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Given a dataset, instead of training one classifier on the entire dataset, you
    sample **with** replacement to create different datasets, called bootstraps, and
    train a classification or regression model on each of these bootstraps. Sampling
    with replacement ensures each bootstrap is independent of its peers.
  prefs: []
  type: TYPE_NORMAL
- en: If the problem is classification, the final prediction is decided by the majority
    vote of all models. For example, if 10 classifiers vote SPAM and 6 models vote
    NOT SPAM, the final prediction is SPAM.
  prefs: []
  type: TYPE_NORMAL
- en: If the problem is regression, the final prediction is the average of all models’
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging generally improves unstable methods, such as neural networks, classification
    and regression trees, and subset selection in linear regression. However, it can
    mildly degrade the performance of stable methods such as k-nearest neighbors^([2](#fn_2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](../Images/5ad61a7b073fce0fb5f0bfa41c3585bc.png "image_tooltip")'
  prefs: []
  type: TYPE_IMG
- en: Illustration by [Sirakorn](https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg)
  prefs: []
  type: TYPE_NORMAL
- en: A **random forest** is an example of bagging. A random forest is a collection
    of decision trees constructed by both **bagging** and **feature randomness**,
    each tree can pick only from a random subset of features to use.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its ensembling nature, random forests correct for decision trees’ overfitting
    to their training set.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applications**: Random forests are among the most widely used machine learning
    algorithms in the real world. They are used in banking for fraud detection, medicine
    for disease prediction, stock market analysis, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on random forests, see [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)
    by Tony Yiu.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.5.2 Boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Boosting is a family of iterative ensemble algorithms that convert weak learners
    to strong ones. Each learner in this ensemble is trained on the same set of samples
    but the samples are weighted differently among iterations. Thus, future weak learners
    focus more on the examples that previous weak learners misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: You start by training the first weak classifier on the original dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Samples are reweighted based on how well the first classifier classifies them,
    e.g. misclassified samples are given higher weight.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the second classifier on this reweighted dataset. Your ensemble now consists
    of the first and the second classifiers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Samples are weighted based on how well the ensemble classifies them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the third classifier on this reweighted dataset. Add the third classifier
    to the ensemble.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for as many iterations as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Form the final strong classifier as a weighted combination of the existing classifiers
    -- classifiers with smaller training errors have higher weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Boosting](../Images/e5e31b937fa248d597f8d95cffc55f4b.png "image_tooltip")'
  prefs: []
  type: TYPE_IMG
- en: Illustration by [Sirakorn](https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg)
  prefs: []
  type: TYPE_NORMAL
- en: An example of a boosting algorithm is Gradient Boosting Machine which produces
    a prediction model typically from weak decision trees. It builds the model in
    a stage-wise fashion as other boosting methods do, and it generalizes them by
    allowing optimization of an arbitrary differentiable loss function.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost, a variant of GBM, used to be [the algorithm of choice for many winning
    teams of machine learning competitions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
    It’s been used in a wide range of tasks from classification, ranking, to the discovery
    of the Higgs Boson^([3](#fn_3)). However, many teams have been opting for [LightGBM](https://github.com/microsoft/LightGBM),
    a distributed gradient boosting framework that allows parallel learning which
    generally allows faster training on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1.6 Kernel methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In machine learning, kernel methods are a class of algorithms for pattern analysis,
    whose best-known member is the support vector machine (SVM). The general task
    of pattern analysis is to find and study general types of relations (for example
    clusters, rankings, principal components, correlations, classifications) in datasets.
    For many algorithms that solve these tasks, the data in raw representation have
    to be explicitly transformed into feature vector representations via a user-specified
    feature map: in contrast, kernel methods require only a user-specified kernel,
    i.e., a similarity function over pairs of data points in raw representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel methods owe their name to the use of kernel functions, which enable them
    to operate in a high-dimensional, implicit feature space without ever computing
    the coordinates of the data in that space, but rather by simply computing the
    inner products between the images of all pairs of data in the feature space. This
    operation is often computationally cheaper than the explicit computation of the
    coordinates. This approach is called the "kernel trick".[1] Kernel functions have
    been introduced for sequence data, graphs, text, images, as well as vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithms capable of operating with kernels include the kernel perceptron,
    support vector machines (SVM), Gaussian processes, principal components analysis
    (PCA), canonical correlation analysis, ridge regression, spectral clustering,
    linear adaptive filters, and many others. Any linear model can be turned into
    a non-linear model by applying the kernel trick to the model: replacing its features
    (predictors) with a kernel function.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*This book was created by [Chip Huyen](https://huyenchip.com) with the help
    of wonderful friends. For feedback, errata, and suggestions, the author can be
    reached [here](https://huyenchip.com/communication/). Copyright ©2021 Chip Huyen.*'
  prefs: []
  type: TYPE_NORMAL
