["```py\nimport geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\nimport geostatspy.geostats as geostats                        # GSLIB methods convert to Python \nimport geostatspy\nprint('GeostatsPy version: ' + str(geostatspy.__version__)) \n```", "```py\nGeostatsPy version: 0.0.71 \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nimport numpy as np                                            # ndarrays for gridded data\nimport pandas as pd                                           # DataFrames for tabular data\nfrom sklearn import preprocessing                             # remove encoding error\nfrom sklearn.feature_selection import RFE                     # for recursive feature selection\nfrom sklearn.feature_selection import mutual_info_regression  # mutual information\nfrom sklearn.linear_model import LinearRegression             # linear regression model\nfrom sklearn.ensemble import RandomForestRegressor            # model-based feature importance\nfrom sklearn import metrics                                   # measures to check our models\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor # variance inflation factor\nimport os                                                     # set working directory, run executables\nimport math                                                   # basic math operations\nimport random                                                 # for random numbers\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport matplotlib.ticker as mtick                             # control tick label formatting\nimport seaborn as sns                                         # for matrix scatter plots\nfrom scipy import stats                                       # summary statistics\nimport numpy.linalg as linalg                                 # for linear algebra\nimport scipy.spatial as sp                                    # for fast nearest neighbor search\nimport scipy.signal as signal                                 # kernel for moving window calculation\nfrom numba import jit                                         # for numerical speed up\nfrom statsmodels.stats.weightstats import DescrStatsW\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map \n```", "```py\nimport sys\n#!{sys.executable} -m pip install shap\nimport shap\nshap.initjs() \n```", "```py\nmy_colormap = plt.cm.get_cmap('RdBu_r', 256)                  # make a custom colormap\nnewcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space\nwhite = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)\n#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)\n#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)\nnewcolors[76:180, :] = white                                  # mask all correlations less than abs(0.4)\nsignif = ListedColormap(newcolors)                            # assign as listed colormap\n\nmy_colormap = plt.cm.get_cmap('inferno', 256)                 # make a custom colormap\nnewcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space\nwhite = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)\n#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)\nnewcolors[0:12, :] = white                                    # mask all correlations less than abs(0.6)\n#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)\nsign1 = ListedColormap(newcolors)                             # assign as listed colormap \n```", "```py\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal)\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,mpred-0.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,mpred-0.5]); add_grid();\n    plt.xticks(rotation=270.0)\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n    plt.xticks(rotation=270.0)\n\ndef partial_corr(C):                                          # partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net\n    C = np.asarray(C)\n    p = C.shape[1]\n    P_corr = np.zeros((p, p), dtype=float)\n    for i in range(p):\n        P_corr[i, i] = 1\n        for j in range(i+1, p):\n            idx = np.ones(p, dtype=bool)\n            idx[i] = False\n            idx[j] = False\n            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n            res_j = C[:, j] - C[:, idx].dot( beta_i)\n            res_i = C[:, i] - C[:, idx].dot(beta_j)\n            corr = stats.pearsonr(res_i, res_j)[0]\n            P_corr[i, j] = corr\n            P_corr[j, i] = corr\n    return P_corr\n\ndef semipartial_corr(C):                                      # Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation\n\n    C = np.asarray(C)\n    p = C.shape[1]\n    P_corr = np.zeros((p, p), dtype=float)\n    for i in range(p):\n        P_corr[i, i] = 1\n        for j in range(i+1, p):\n            idx = np.ones(p, dtype=bool)\n            idx[i] = False\n            idx[j] = False\n            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n            res_j = C[:, j] - C[:, idx].dot( beta_i)\n            res_i = C[:, i] \n            corr = stats.pearsonr(res_i, res_j)[0]\n            P_corr[i, j] = corr\n            P_corr[j, i] = corr\n    return P_corr\n\ndef mutual_matrix(df,features):                               # calculate mutual information matrix\n    mutual = np.zeros([len(features),len(features)])\n    for i, ifeature in enumerate(features):\n        for j, jfeature in enumerate(features):\n            if i != j:\n                mutual[i,j] = mutual_info_regression(df.iloc[:,i].values.reshape(-1, 1),np.ravel(df.iloc[:,j].values))[0]\n    mutual /= np.max(mutual) \n    for i, ifeature in enumerate(features):\n        mutual[i,i] = 1.0\n    return mutual\n\ndef mutual_information_objective(x,y):                        # modified from MRMR loss function, Ixy - average(Ixx)\n    mutual_information_quotient = []\n    for i, icol in enumerate(x.columns):\n        Vx = mutual_info_regression(x.iloc[:,i].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1)))\n        Ixx_mat = []\n        for m, mcol in enumerate(x.columns):\n            if i != m:\n                Ixx_mat.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,i].values.reshape(-1, 1))))\n        Wx = np.average(Ixx_mat)\n        mutual_information_quotient.append(Vx/Wx)\n    mutual_information_quotient  = np.asarray(mutual_information_quotient).reshape(-1)\n    return mutual_information_quotient\n\ndef delta_mutual_information_quotient(x,y):                   # standard mutual information quotient\n    delta_mutual_information_quotient = []               \n\n    Ixy = []\n    for m, mcol in enumerate(x.columns):\n        Ixy.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n    Vs = np.average(Ixy)\n    Ixx = []\n    for m, mcol in enumerate(x.columns):\n        for n, ncol in enumerate(x.columns):\n            Ixx.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))\n    Ws = np.average(Ixx) \n\n    for i, icol in enumerate(x.columns):          \n        Ixy_s = []                                          \n        for m, mcol in enumerate(x.columns):\n            if m != i:\n                Ixy_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n        Vs_s = np.average(Ixy_s)\n        Ixx_s = []\n        for m, mcol in enumerate(x.columns):\n            if m != i:\n                for n, ncol in enumerate(x.columns):\n                    if n != i:\n                        Ixx_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))                  \n        Ws_s = np.average(Ixx_s)\n        delta_mutual_information_quotient.append((Vs/Ws)-(Vs_s/Ws_s))\n\n    delta_mutual_information_quotient  = np.asarray(delta_mutual_information_quotient).reshape(-1)  \n    return delta_mutual_information_quotient\n\ndef weighted_avg_and_std(values, weights):                    # calculate weighted statistics (Eric O Lebigot, stack overflow)\n    average = np.average(values, weights=weights)\n    variance = np.average((values-average)**2, weights=weights)\n    return (average, math.sqrt(variance))\n\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():                                               # add major and minor gridlines\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"d:/PGE383\")                                   # set the working directory \n```", "```py\nidata = 0\nif idata == 0:\n    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n\n    response = 'Prod'                                             # specify the response feature\n    x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames\n    Y = df.loc[:,response]\n\n    features = x.columns.values.tolist() + [Y.name]               # store the names of the features\n    pred = x.columns.values.tolist()\n    resp = Y.name\n\n    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    Ymin = 500.0; Ymax = 9000.0\n\n    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n    resplabel = 'Normalized Initial Production (MCFPD)'\n\n    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n                 'Total Organic Carbon','Vitrinite Reflectance']\n    resptitle = 'Normalized Initial Production'\n\n    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\n    featuretitle = predtitle + [resptitle]\n\n    m = len(pred) + 1\n    mpred = len(pred)\n\n# elif idata == 1:\n#     names = {'Porosity':'Por'}\n\n#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n#     df = df.rename(columns=names)\n#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; \n#     df.drop('Unnamed: 0',axis=1,inplace=True) \n\n#     features = df.columns.values.tolist()                          # store the names of the features\n\n#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n#               'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\nelif idata == 2:  \n    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n\n    response = 'CumulativeOil'                                             # specify the response feature\n    x = df.copy(deep = True); x = x.drop(['Well_ID','X','Y',response],axis='columns') # make predictor and response DataFrames\n    Y = df.loc[:,response]\n\n    features = x.columns.values.tolist() + [Y.name]               # store the names of the features\n    pred = x.columns.values.tolist()\n    resp = Y.name\n\n    xmin = [1.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [75.0,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n    Ymin = 0.0; Ymax = 3000.0\n\n    predlabel = ['Well (ID)','X (m)','Y (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] \n    resplabel = 'Cumulative Production (MSTB)'\n\n    predtitle = ['Well','X','Y','Porosity','Permeability','Acoustic Impedance',\n              'Density (g/cm^3)','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus'] \n    resptitle = 'Cumulative Production'\n\n    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\n    featuretitle = predtitle + [resptitle]\n\n    m = len(pred) + 1\n    mpred = len(pred) \n```", "```py\n---------------------------------------------------------------------------\nSSLCertVerificationError  Traceback (most recent call last)\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1317, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)\n  1316 try:\n-> 1317     h.request(req.get_method(), req.selector, req.data, headers,\n  1318               encode_chunked=req.has_header('Transfer-encoding'))\n  1319 except OSError as err: # timeout error\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1230, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)\n  1229  \"\"\"Send a complete request to the server.\"\"\"\n-> 1230 self._send_request(method, url, body, headers, encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1276, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)\n  1275     body = _encode(body, 'body')\n-> 1276 self.endheaders(body, encode_chunked=encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1225, in HTTPConnection.endheaders(self, message_body, encode_chunked)\n  1224     raise CannotSendHeader()\n-> 1225 self._send_output(message_body, encode_chunked=encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1004, in HTTPConnection._send_output(self, message_body, encode_chunked)\n  1003 del self._buffer[:]\n-> 1004 self.send(msg)\n  1006 if message_body is not None:\n  1007 \n  1008     # create a consistent interface to message_body\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:944, in HTTPConnection.send(self, data)\n  943 if self.auto_open:\n--> 944     self.connect()\n  945 else:\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1399, in HTTPSConnection.connect(self)\n  1397     server_hostname = self.host\n-> 1399 self.sock = self._context.wrap_socket(self.sock,\n  1400                                       server_hostname=server_hostname)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:500, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\n  494 def wrap_socket(self, sock, server_side=False,\n  495                 do_handshake_on_connect=True,\n  496                 suppress_ragged_eofs=True,\n  497                 server_hostname=None, session=None):\n  498     # SSLSocket class handles server_hostname encoding before it calls\n  499     # ctx._wrap_socket()\n--> 500     return self.sslsocket_class._create(\n  501         sock=sock,\n  502         server_side=server_side,\n  503         do_handshake_on_connect=do_handshake_on_connect,\n  504         suppress_ragged_eofs=suppress_ragged_eofs,\n  505         server_hostname=server_hostname,\n  506         context=self,\n  507         session=session\n  508     )\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:1040, in SSLSocket._create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\n  1039             raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\n-> 1040         self.do_handshake()\n  1041 except (OSError, ValueError):\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:1309, in SSLSocket.do_handshake(self, block)\n  1308         self.settimeout(None)\n-> 1309     self._sslobj.do_handshake()\n  1310 finally:\n\nSSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nURLError  Traceback (most recent call last)\nCell In[7], line 3\n  1 idata = 0\n  2 if idata == 0:\n----> 3     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n  5     response = 'Prod'                                             # specify the response feature\n  6     x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  899 kwds_defaults = _refine_defaults_read(\n  900     dialect,\n  901     delimiter,\n   (...)\n  908     dtype_backend=dtype_backend,\n  909 )\n  910 kwds.update(kwds_defaults)\n--> 912 return _read(filepath_or_buffer, kwds)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577, in _read(filepath_or_buffer, kwds)\n  574 _validate_names(kwds.get(\"names\", None))\n  576 # Create the parser.\n--> 577 parser = TextFileReader(filepath_or_buffer, **kwds)\n  579 if chunksize or iterator:\n  580     return parser\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407, in TextFileReader.__init__(self, f, engine, **kwds)\n  1404     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n  1406 self.handles: IOHandles | None = None\n-> 1407 self._engine = self._make_engine(f, self.engine)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661, in TextFileReader._make_engine(self, f, engine)\n  1659     if \"b\" not in mode:\n  1660         mode += \"b\"\n-> 1661 self.handles = get_handle(\n  1662     f,\n  1663     mode,\n  1664     encoding=self.options.get(\"encoding\", None),\n  1665     compression=self.options.get(\"compression\", None),\n  1666     memory_map=self.options.get(\"memory_map\", False),\n  1667     is_text=is_text,\n  1668     errors=self.options.get(\"encoding_errors\", \"strict\"),\n  1669     storage_options=self.options.get(\"storage_options\", None),\n  1670 )\n  1671 assert self.handles is not None\n  1672 f = self.handles.handle\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:716, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n  713     codecs.lookup_error(errors)\n  715 # open URLs\n--> 716 ioargs = _get_filepath_or_buffer(\n  717     path_or_buf,\n  718     encoding=encoding,\n  719     compression=compression,\n  720     mode=mode,\n  721     storage_options=storage_options,\n  722 )\n  724 handle = ioargs.filepath_or_buffer\n  725 handles: list[BaseBuffer]\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:368, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n  366 # assuming storage_options is to be interpreted as headers\n  367 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--> 368 with urlopen(req_info) as req:\n  369     content_encoding = req.headers.get(\"Content-Encoding\", None)\n  370     if content_encoding == \"gzip\":\n  371         # Override compression based on Content-Encoding header\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:270, in urlopen(*args, **kwargs)\n  264  \"\"\"\n  265 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n  266 the stdlib.\n  267 \"\"\"\n  268 import urllib.request\n--> 270 return urllib.request.urlopen(*args, **kwargs)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:222, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n  220 else:\n  221     opener = _opener\n--> 222 return opener.open(url, data, timeout)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n  522     req = meth(req)\n  524 sys.audit('urllib.Request', req.full_url, req.data, req.headers, req.get_method())\n--> 525 response = self._open(req, data)\n  527 # post-process response\n  528 meth_name = protocol+\"_response\"\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:542, in OpenerDirector._open(self, req, data)\n  539     return result\n  541 protocol = req.type\n--> 542 result = self._call_chain(self.handle_open, protocol, protocol +\n  543                           '_open', req)\n  544 if result:\n  545     return result\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:502, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n  500 for handler in handlers:\n  501     func = getattr(handler, meth_name)\n--> 502     result = func(*args)\n  503     if result is not None:\n  504         return result\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1360, in HTTPSHandler.https_open(self, req)\n  1359 def https_open(self, req):\n-> 1360     return self.do_open(http.client.HTTPSConnection, req,\n  1361         context=self._context, check_hostname=self._check_hostname)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1320, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)\n  1317         h.request(req.get_method(), req.selector, req.data, headers,\n  1318                   encode_chunked=req.has_header('Transfer-encoding'))\n  1319     except OSError as err: # timeout error\n-> 1320         raise URLError(err)\n  1321     r = h.getresponse()\n  1322 except:\n\nURLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)> \n```", "```py\nPormin = np.min(df['Por'].values)                             # extract ndarray of data table column\nPormax = np.max(df['Por'].values)                             # and calculate min and max \n```", "```py\ndf.head(n=13)                                                 # we could also use this command for a table preview \n```", "```py\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\nplt.subplot(111)\n(df.isnull().sum()/len(df)).plot(kind = 'bar')                # calculate DataFrame with percentage missing by feature\nplt.xlabel('Feature'); plt.ylabel('Percentage of Missing Values'); plt.title('Data Completeness'); plt.ylim([0.0,1.0])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.2); add_grid(); plt.show() \n```", "```py\nproportion_NaN = 0.1                                    # proportion of values in DataFrame to remove\n\nremove = np.random.random(df.shape) < proportion_NaN    # make the boolean array for removal\nprint('Fraction of removed values in mask ndarray = ' + str(round(remove.sum()/remove.size,3)) + '.')\n\ndf_mask = df.mask(remove)                               # make a new DataFrame with specified proportion removed \n```", "```py\ndf_temp = df.copy(deep=True)                                  # make a deep copy of the DataFrame\ndf_bool = df_temp.isnull()                                    # true is value, false if NaN\n#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column\nheat = sns.heatmap(df_bool, cmap=['r','w'], annot=False, fmt='.0f',cbar=False,linecolor='black',linewidth=0.1) # make the binary heat map, no bins\nheat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=8)\nheat.set_yticklabels(heat.get_yticklabels(), rotation=0, fontsize=8)\nheat.set_title('Data Completeness Heatmap',fontsize=16); heat.set_xlabel('Feature',fontsize=12); heat.set_ylabel('Sample (Index)',fontsize=12)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.8, top=1.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.dropna(axis=0,how='any',inplace=True)                      # likewise deletion \n```", "```py\ndf.describe().transpose()                                     # DataFrame summary statistics \n```", "```py\nnbins = 20                                                    # number of histogram bins\nfor i, feature in enumerate(features):                        # plot histograms with central tendency and P10 and P90 labeled\n    plt.subplot(4,3,i+1)\n    y,_,_ = plt.hist(x=df[feature],weights=None,bins=nbins,alpha = 0.8,edgecolor='black',color='darkorange',density=True)\n    # histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')\n    plt.xlabel(feature); plt.ylabel('Frequency'); plt.ylim([0.0,y.max()*1.10]); plt.title(featuretitle[i]); add_grid() \n    # if feature == resp: \n    #     plt.xlim([Ymin,Ymax]) \n    # else:\n    #     plt.xlim([xmin[i],xmax[i]]) \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=4.1, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response \n```", "```py\ncovariance = df.iloc[:,df.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp\ncov_matrix = df.iloc[:,df.columns.get_indexer(features)].cov()\nplt.subplot(121)\nplot_corr(cov_matrix,'Covariance Matrix',4000.0,0.1)          # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features') \n\nplt.subplot(122)\nfeature_rank_plot(features,covariance,-20000.0,20000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\ndf.iloc[:,1:8].corr() \n```", "```py\ncorrelation = df.iloc[:,df.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp\ncorr_matrix = df.iloc[:,df.columns.get_indexer(features)].corr()\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nstats.spearmanr(df.iloc[:,1:8]) \n```", "```py\nrank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the rank correlation coefficient\nrank_matrix = pd.DataFrame(rank_correlation,columns=corr_matrix.columns)\nrank_correlation = rank_correlation[:,len(features)-1][:len(features)]\nrank_correlation_pval = rank_correlation_pval[:,len(pred)-1][:len(features)]\nprint(\"\\nRank Correlation p-value:\\n\"); print(rank_correlation_pval)\n\nplt.subplot(121)\nplot_corr(rank_matrix,'Rank Correlation Matrix',1.0,0.5)      # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nRank Correlation p-value:\n\n[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04\n 2.11367755e-06 0.00000000e+00 3.29170847e-04] \n```", "```py\nplt.subplot(121)                                              # plot correlation matrix with significance colormap\ndiff = corr_matrix.values - rank_matrix.values\ndiff_matrix = pd.DataFrame(diff,columns=corr_matrix.columns)\nplot_corr(diff_matrix,'Correlation - Rank Correlation',0.1,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\ncorr_diff = correlation - rank_correlation\n\nplt.subplot(122)\nfeature_rank_plot(features,corr_diff,-0.20,0.20,0.0,'Correlation Coefficient - Rank Correlation Coefficient','Correlation Diffference',0.1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npartial_correlation = partial_corr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the partial correlation coefficients\npartial_matrix = pd.DataFrame(partial_correlation,columns=corr_matrix.columns)\npartial_correlation = partial_correlation[:,len(features)-1][:len(features)] # extract a single row and remove production with itself \n\nplt.subplot(121)\nplot_corr(partial_matrix,'Partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsemipartial_correlation = semipartial_corr(df.iloc[:,df.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients\nsemipartial_matrix = pd.DataFrame(semipartial_correlation,columns=corr_matrix.columns)\nsemipartial_correlation = semipartial_correlation[:,len(features)-1][:len(features)]    # extract a single row and remove production with itself\n\nplt.subplot(121)\nplot_corr(semipartial_matrix,'Semi-partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\n# plt.subplot(151)\n# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)\n\nplt.subplot(131)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplot(132)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(133)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\n# plt.subplot(155)\n# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2); plt.show() \n```", "```py\n# dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 \n# dfS['Well'] = df['Well'].values\n# dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)\n# dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)\n# dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)\n# dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)\n# dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)\n# dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)\n# dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)\n# dfS.head() \n```", "```py\ndfS = pd.DataFrame()                                          # Gaussian transform, standardization to a mean of 0 and variance of 1 \n\nfor feature in features:\n    dfS[feature],d1,d2 = geostats.nscore(df,feature)\n\ndfS.head() \n```", "```py\ndfS.describe()                                                # check the summary statistics \n```", "```py\npairgrid = sns.PairGrid(dfS) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nstand_covariance = dfS.iloc[:,dfS.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)]\nstand_correlation = dfS.iloc[:,dfS.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)]\n\nstand_rank_correlation, stand_rank_correlation_pval = stats.spearmanr(dfS.iloc[:,dfS.columns.get_indexer(features)])\nstand_rank_correlation = stand_rank_correlation[:,len(features)-1][:len(features)]\nstand_partial_correlation = partial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)]) # calculate the partial correlation coefficients\nstand_partial_correlation = stand_partial_correlation[:,len(features)-1][:len(features)]\nstand_semipartial_correlation = semipartial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients\nstand_semipartial_correlation = stand_semipartial_correlation[:,len(features)-1][:len(features)] \n```", "```py\n# plt.subplot(2,5,1)\n# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)\n\nplt.subplot(2,3,1)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplot(2,3,2)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(2,3,3)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\n# plt.subplot(2,5,5)\n# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\n# plt.subplot(2,5,6)\n# feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)\n\nplt.subplot(2,3,4)\nfeature_rank_plot(features,stand_correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation of Standardized',0.5)\n\nplt.subplot(2,3,5)\nfeature_rank_plot(features,stand_rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation of Standardized',0.5)\n\nplt.subplot(2,3,6)\nfeature_rank_plot(features,stand_partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation of Standardized',0.5)\n\n# plt.subplot(2,5,10)\n# feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\ndf['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') \n```", "```py\nx = df[['Por','Perm','AI','Brittle','TOC','VR']]\nx_stand = (x - x.mean()) / (x.std()) \n```", "```py\nx = pd.concat([df['tProd'],x_stand.iloc[:,0:6]],axis=1) \n```", "```py\nx = pd.melt(x,id_vars=\"tProd\",var_name=\"Predictors\",value_name='Standardized_Value') \n```", "```py\nthreshold = 2000.0\n\ndf['tProd'] = np.where(df[resp]>=threshold, 'High', 'Low')       # make a high and low production categorical feature\n\nx_temp = df[pred]\nx_temp_stand = (x_temp - x_temp.mean()) / (x_temp.std())      # standardization by feature\nx_temp = pd.concat([df['tProd'],x_temp_stand.iloc[:,0:len(pred)]],axis=1) # add the production categorical feature to the DataFrame\nx_temp = pd.melt(x_temp,id_vars=\"tProd\",var_name=\"Predictor Feature\",value_name='Standardized Predictor Feature') # unpivot the DataFrame\n\nplt.subplot(111)\nsns.violinplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp,split=True, inner=\"quart\", palette=\"Set2\")\nplt.xticks(rotation=90); plt.title('Conditional Distributions by Production')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(111)\nsns.boxplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp)\nplt.xticks(rotation=90)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)\nplt.show()\n\ndf = df.drop(['tProd'], axis = 1) \n```", "```py\nvif_values = []\nfor i in range(df[pred].values.shape[1]):\n    vif_values.append(variance_inflation_factor(df[pred].values, i))\n\nvif_values = np.asarray(vif_values)\nindices = np.argsort(vif_values)[::-1]                  # find indices for descending order\n\nplt.subplot(111)                                        # plot the feature importance \nplt.title(\"Variance Inflation Factor\")\nplt.bar(range(df[pred].values.shape[1]), vif_values[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha=0.6, align=\"center\")\nplt.xticks(np.linspace(0,len(pred)-1,len(pred)), np.array(pred)[indices].tolist(),rotation=90); \n\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.yscale('log');\nplt.xlabel('Predictor Feature'); plt.ylabel('Variance Inflation Factor')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nreg = LinearRegression()                                      # instantiate a linear regression model \nreg.fit(df[pred],df[resp])                                    # train the model\nb = reg.coef_\n\nplt.subplot(111)\nfeature_rank_plot(pred,b,-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,r'Linear Regression Slope, $b_1$',0.5) \n```", "```py\nreg = LinearRegression()\nreg.fit(dfS[pred],dfS[resp])\nbeta = reg.coef_\n\nplt.subplot(111)\nfeature_rank_plot(pred,beta,-1.0,1.0,0.0,r'Feature Ranking, $\\beta$ Coefficients with ' + resp,r'Standardized Linear Regression Slope, $b_1$',0.5) \n```", "```py\n# Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization\nlab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y) # this removes an encoding error \n\nrandom_forest = RandomForestRegressor()                 # instantiate the random forest \nrandom_forest = random_forest.fit(x,np.ravel(Y_encoded)) # fit the random forest\nimportance_rank = random_forest.feature_importances_    # extract the expected feature importances\n\nimportance_rank_stand = importance_rank/np.max(importance_rank)                          # calculate relative mutual information\n\nstd = np.std([tree.feature_importances_ for tree in random_forest.estimators_],axis=0) # calculate stdev over trees\nindices = np.argsort(importance_rank)[::-1]             # find indices for descending order\n\nplt.subplot(111)                                        # plot the feature importance \nplt.title(\"Random Forest-based Feature importances\")\nplt.bar(range(x.shape[1]), importance_rank[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha = 0.6, yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5]); \nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.ylim([0.,1.0])\nplt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nplt.subplot(231)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(232)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\nplt.subplot(234)\nfeature_rank_plot(pred,b[0:len(pred)],-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,'B Coefficients',0.5)\n\nplt.subplot(235)\nfeature_rank_plot(pred,beta[0:len(pred)],-1.0,1.0,0.0,'Feature Ranking, Beta Coefficients with ' + resp,'Beta Coefficients',0.5)\n\nplt.subplot(236)\nfeature_rank_plot(pred,importance_rank_stand,0.0,1.0,0.0,'Feature Ranking, Feature Importance with ' + resp,'Standardized Feature Importance',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nindices = np.argsort(importances)[::-1] \n```", "```py\nx_df = df.loc[:,pred]                            # separate DataFrames for predictor and response features\ny_df = df.loc[:,resp]\n\nmi = mutual_info_regression(x_df,np.ravel(y_df))              # calculate mutual information\nmi /= np.max(mi)                                        # calculate relative mutual information\n\nindices = np.argsort(mi)[::-1]                          # find indices for descending order\n\nprint(\"Feature ranking:\")                               # write out the feature importances\nfor f in range(x.shape[1]):\n    print(\"%d. feature %s = %f\" % (f + 1, x.columns[indices][f], mi[indices[f]]))\n\nplt.subplot(111)                                        # plot the relative mutual information \nplt.title(\"Mutual Information\")\nplt.bar(range(x.shape[1]), mi[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha=0.6,align=\"center\")\nplt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.ylim([0,1.3])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.xlabel('Predictor Feature'); plt.ylabel('Mutual Information')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nFeature ranking:\n1\\. feature Por = 1.000000\n2\\. feature Perm = 0.345842\n3\\. feature TOC = 0.272418\n4\\. feature Brittle = 0.073310\n5\\. feature AI = 0.059024\n6\\. feature VR = 0.000000 \n```", "```py\nobj_mutual = mutual_information_objective(x_df,y_df)\nindices_obj = np.argsort(obj_mutual)[::-1]              # find indices for descending order\n\nplt.subplot(111)                                        # plot the relative mutual information \nplt.title(\"One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection\")\nplt.bar(range(x.shape[1]), obj_mutual[indices_obj],\n       color=\"darkorange\",alpha = 0.6, align=\"center\",edgecolor=\"black\")\nplt.xticks(range(x.shape[1]), x.columns[indices_obj],rotation=90)\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\ndelta_mutual_information = delta_mutual_information_quotient(x_df,y_df)\n\nindices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order\n\nplt.subplot(111)                                              # plot the relative mutual information \nplt.title(\"Delta Mutual Information Quotient\")\nplt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],\n       color=\"darkorange\",alpha = 0.6,align=\"center\",edgecolor = 'black')\nplt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nplt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')\nfor i, feature in enumerate(x.columns):\n    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))\nplt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')\nplt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)\nplt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); \nplt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')\nplt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)\nplt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nrfe_linear = RFE(LinearRegression(),n_features_to_select=1,verbose=0) # set up RFE linear regression model\ndf['const'] = np.ones(len(df))                                # let's add one's for the constant term\nrfe_linear = rfe_linear.fit(df[pred].values,np.ravel(df[resp])) # recursive elimination\ndfS = df.drop('const',axis = 1)                               # remove the ones\nprint('Recursive Feature Elimination: Multilinear Regression')\nfor i in range(0,len(pred)):\n    print('Rank #' + str(i+1) + ' ' + pred[rfe_linear.ranking_[i]-1]) \n```", "```py\nRecursive Feature Elimination: Multilinear Regression\nRank #1 Brittle\nRank #2 TOC\nRank #3 AI\nRank #4 VR\nRank #5 Por\nRank #6 Perm \n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nimport warnings\nwarnings.filterwarnings('ignore')            \nimport geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\nrfe_rf = RFE(RandomForestRegressor(max_depth=3),n_features_to_select=1,verbose=0) # set up RFE linear regression model\ndf['const'] = np.ones(len(df))                                # let's add one's for the constant term\n\nlab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y)\n\nrfe_rf = rfe_rf.fit(x,np.ravel(Y_encoded))                    # recursive elimination\ndfS = df.drop('const',axis = 1)                               # remove the ones\nprint('Recursive Feature Elimination: Random Forest Regression')\nfor i in range(0,len(pred)):\n    print('Rank #' + str(i+1) + ' ' + pred[rfe_rf.ranking_[i]-1]) \n```", "```py\nRecursive Feature Elimination: Random Forest Regression\nRank #1 Por\nRank #2 VR\nRank #3 Brittle\nRank #4 Perm\nRank #5 TOC\nRank #6 AI \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\n\n# #Underfit random forest\nmax_leaf_nodes = 2\nnum_tree = 10\nmax_features = 2\n\n#Overfit random forest\nmax_leaf_nodes = 50\nnum_tree = 1\nmax_features = 6\n\n# #Good random forest\nmax_leaf_nodes = 5\nnum_tree = 300\nmax_features = 2\n\nrfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)\nrfr.fit(X = x, y = Y)\n\nY_hat = predict_train = rfr.predict(x)\n\nMSE = metrics.mean_squared_error(Y,Y_hat)\nVar_Explained = metrics.explained_variance_score(Y,Y_hat)\nprint('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))\n\nimportances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature\nstd = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)\nindices = np.argsort(importances)[::-1].tolist()\n\nplt.subplot(121)\nplt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\nplt.xlim(0,7000); plt.ylim(0,7000)\nplt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)\n\nplt.subplot(122)\nplt.title(\"Feature Importances\")\nplt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color=\"darkorange\", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align=\"center\")\n#plt.xticks(range(X.shape[1]), indices)\nplt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nMean Squared Error on Training =  428100.87 , Variance Explained = 0.82 \n```", "```py\nbackground = shap.sample(x,nsamples=50,random_state=73073) \nmodel_explainer = shap.TreeExplainer(rfr)\nshap_values = model_explainer.shap_values(background) # global Shapley Measures \n```", "```py\nshap_values.shape \n```", "```py\n(50, 6) \n```", "```py\nnback = 7\n\nresp_avg  = np.average(Y_hat)\nyhat = rfr.predict(background.iloc[[nback]])\n\ncurrent = resp_avg\n\nplt.subplot(111)\n\nplt.plot([current,current],[0,0.3],color='black',lw=2,zorder=1)\nplt.plot([current-2,current],[0.2,0.3],color='black',lw=2,zorder=1)\nplt.plot([current,current+2],[0.3,0.2],color='black',lw=2,zorder=1)\nfor i in range(len(pred)+1):\n    plt.scatter(current,i+0.5,color='grey',edgecolor='black',zorder=10)\n    if i < len(pred):\n        if shap_values[nback,i] > 0.0:\n            color = 'red'\n        else:\n            color = 'blue'\n        plt.plot([current,current + shap_values[nback,i]],[i+1,i+1],color=color,lw=2,zorder=1)\n        plt.plot([current,current],[i+0.6,i+1],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]],[i+1,i+1.3],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i]-2,current + shap_values[nback,i]],[i+1.2,i+1.3],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]+2],[i+1.3,i+1.2],color=color,lw=2,zorder=1)\n        if shap_values[nback,i] > 0.0:\n            plt.annotate('+ ' + str(np.round(shap_values[nback,i],0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')\n        else:\n            plt.annotate('- ' + str(np.round(abs(shap_values[nback,i]),0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')\n        current = current + shap_values[nback,i]\n\nplt.plot([current,current],[i+0.7,i+1],color='black',lw=2,zorder=1)\nplt.plot([current-2,current],[i+0.9,i+1],color='black',lw=2,zorder=1)\nplt.plot([current,current+2],[i+1,i+0.9],color='black',lw=2,zorder=1)\n\nplt.plot([resp_avg,resp_avg],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)\nplt.plot([yhat,yhat],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)\nplt.annotate('Response Feature, Training Average',[resp_avg-8,1.0],rotation=90.0)\nplt.annotate('Model Prediction',[yhat-8,1.0],rotation=90.0)\n\nplt.yticks(ticks=np.arange(len(pred)+2), labels=[r'None / $\\overline{y}$'] + pred + [r'$\\hat{y}=f(X)$'])\nadd_grid(); plt.ylim([-0.5,len(pred)+1.5])\nplt.xlabel('Production (MCFPD)'); plt.ylabel('Feature'); plt.title('Local Shapley Values, Background Index: ' + str(nback))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nshap.force_plot(model_explainer.expected_value,shap_values,background,out_names = ['Production'],feature_names=pred,) \n```", "```py\nshap.force_plot(model_explainer.expected_value,shap_values[nback],background.iloc[[nback]],show=False,feature_names = pred) \n```", "```py\nplt.subplot(131)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background, plot_type=\"bar\",color = \"darkorange\",cmap = plt.cm.inferno)\nplt.ylabel('Predictor Features')\n\nplt.subplot(132)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,cmap = plt.cm.inferno)\n\nplt.subplot(133)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,plot_type = \"violin\")\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2)\nplt.show() \n```", "```py\nimport geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\nimport geostatspy.geostats as geostats                        # GSLIB methods convert to Python \nimport geostatspy\nprint('GeostatsPy version: ' + str(geostatspy.__version__)) \n```", "```py\nGeostatsPy version: 0.0.71 \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nimport numpy as np                                            # ndarrays for gridded data\nimport pandas as pd                                           # DataFrames for tabular data\nfrom sklearn import preprocessing                             # remove encoding error\nfrom sklearn.feature_selection import RFE                     # for recursive feature selection\nfrom sklearn.feature_selection import mutual_info_regression  # mutual information\nfrom sklearn.linear_model import LinearRegression             # linear regression model\nfrom sklearn.ensemble import RandomForestRegressor            # model-based feature importance\nfrom sklearn import metrics                                   # measures to check our models\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor # variance inflation factor\nimport os                                                     # set working directory, run executables\nimport math                                                   # basic math operations\nimport random                                                 # for random numbers\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport matplotlib.ticker as mtick                             # control tick label formatting\nimport seaborn as sns                                         # for matrix scatter plots\nfrom scipy import stats                                       # summary statistics\nimport numpy.linalg as linalg                                 # for linear algebra\nimport scipy.spatial as sp                                    # for fast nearest neighbor search\nimport scipy.signal as signal                                 # kernel for moving window calculation\nfrom numba import jit                                         # for numerical speed up\nfrom statsmodels.stats.weightstats import DescrStatsW\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map \n```", "```py\nimport sys\n#!{sys.executable} -m pip install shap\nimport shap\nshap.initjs() \n```", "```py\nmy_colormap = plt.cm.get_cmap('RdBu_r', 256)                  # make a custom colormap\nnewcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space\nwhite = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)\n#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)\n#newcolors[56:200, :] = white                                 # mask all correlations less than abs(0.6)\nnewcolors[76:180, :] = white                                  # mask all correlations less than abs(0.4)\nsignif = ListedColormap(newcolors)                            # assign as listed colormap\n\nmy_colormap = plt.cm.get_cmap('inferno', 256)                 # make a custom colormap\nnewcolors = my_colormap(np.linspace(0, 1, 256))               # define colormap space\nwhite = np.array([250/256, 250/256, 250/256, 1])              # define white color (4 channel)\n#newcolors[26:230, :] = white                                 # mask all correlations less than abs(0.8)\nnewcolors[0:12, :] = white                                    # mask all correlations less than abs(0.6)\n#newcolors[86:170, :] = white                                 # mask all correlations less than abs(0.4)\nsign1 = ListedColormap(newcolors)                             # assign as listed colormap \n```", "```py\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal)\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,mpred-0.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,mpred-0.5]); add_grid();\n    plt.xticks(rotation=270.0)\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n    plt.xticks(rotation=270.0)\n\ndef partial_corr(C):                                          # partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net\n    C = np.asarray(C)\n    p = C.shape[1]\n    P_corr = np.zeros((p, p), dtype=float)\n    for i in range(p):\n        P_corr[i, i] = 1\n        for j in range(i+1, p):\n            idx = np.ones(p, dtype=bool)\n            idx[i] = False\n            idx[j] = False\n            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n            res_j = C[:, j] - C[:, idx].dot( beta_i)\n            res_i = C[:, i] - C[:, idx].dot(beta_j)\n            corr = stats.pearsonr(res_i, res_j)[0]\n            P_corr[i, j] = corr\n            P_corr[j, i] = corr\n    return P_corr\n\ndef semipartial_corr(C):                                      # Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation\n\n    C = np.asarray(C)\n    p = C.shape[1]\n    P_corr = np.zeros((p, p), dtype=float)\n    for i in range(p):\n        P_corr[i, i] = 1\n        for j in range(i+1, p):\n            idx = np.ones(p, dtype=bool)\n            idx[i] = False\n            idx[j] = False\n            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n            res_j = C[:, j] - C[:, idx].dot( beta_i)\n            res_i = C[:, i] \n            corr = stats.pearsonr(res_i, res_j)[0]\n            P_corr[i, j] = corr\n            P_corr[j, i] = corr\n    return P_corr\n\ndef mutual_matrix(df,features):                               # calculate mutual information matrix\n    mutual = np.zeros([len(features),len(features)])\n    for i, ifeature in enumerate(features):\n        for j, jfeature in enumerate(features):\n            if i != j:\n                mutual[i,j] = mutual_info_regression(df.iloc[:,i].values.reshape(-1, 1),np.ravel(df.iloc[:,j].values))[0]\n    mutual /= np.max(mutual) \n    for i, ifeature in enumerate(features):\n        mutual[i,i] = 1.0\n    return mutual\n\ndef mutual_information_objective(x,y):                        # modified from MRMR loss function, Ixy - average(Ixx)\n    mutual_information_quotient = []\n    for i, icol in enumerate(x.columns):\n        Vx = mutual_info_regression(x.iloc[:,i].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1)))\n        Ixx_mat = []\n        for m, mcol in enumerate(x.columns):\n            if i != m:\n                Ixx_mat.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,i].values.reshape(-1, 1))))\n        Wx = np.average(Ixx_mat)\n        mutual_information_quotient.append(Vx/Wx)\n    mutual_information_quotient  = np.asarray(mutual_information_quotient).reshape(-1)\n    return mutual_information_quotient\n\ndef delta_mutual_information_quotient(x,y):                   # standard mutual information quotient\n    delta_mutual_information_quotient = []               \n\n    Ixy = []\n    for m, mcol in enumerate(x.columns):\n        Ixy.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n    Vs = np.average(Ixy)\n    Ixx = []\n    for m, mcol in enumerate(x.columns):\n        for n, ncol in enumerate(x.columns):\n            Ixx.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))\n    Ws = np.average(Ixx) \n\n    for i, icol in enumerate(x.columns):          \n        Ixy_s = []                                          \n        for m, mcol in enumerate(x.columns):\n            if m != i:\n                Ixy_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n        Vs_s = np.average(Ixy_s)\n        Ixx_s = []\n        for m, mcol in enumerate(x.columns):\n            if m != i:\n                for n, ncol in enumerate(x.columns):\n                    if n != i:\n                        Ixx_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))                  \n        Ws_s = np.average(Ixx_s)\n        delta_mutual_information_quotient.append((Vs/Ws)-(Vs_s/Ws_s))\n\n    delta_mutual_information_quotient  = np.asarray(delta_mutual_information_quotient).reshape(-1)  \n    return delta_mutual_information_quotient\n\ndef weighted_avg_and_std(values, weights):                    # calculate weighted statistics (Eric O Lebigot, stack overflow)\n    average = np.average(values, weights=weights)\n    variance = np.average((values-average)**2, weights=weights)\n    return (average, math.sqrt(variance))\n\ndef weighted_percentile(data, weights, perc):                 # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n    ix = np.argsort(data)\n    data = data[ix] \n    weights = weights[ix] \n    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n    return np.interp(perc, cdf, data)\n\ndef histogram_bounds(values,weights,color):                   # add uncertainty bounds to a histogram \n    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n    plt.plot([avg,avg],[0.0,45],color = color)\n    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')\n\ndef add_grid():                                               # add major and minor gridlines\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"d:/PGE383\")                                   # set the working directory \n```", "```py\nidata = 0\nif idata == 0:\n    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n\n    response = 'Prod'                                             # specify the response feature\n    x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames\n    Y = df.loc[:,response]\n\n    features = x.columns.values.tolist() + [Y.name]               # store the names of the features\n    pred = x.columns.values.tolist()\n    resp = Y.name\n\n    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    Ymin = 500.0; Ymax = 9000.0\n\n    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n    resplabel = 'Normalized Initial Production (MCFPD)'\n\n    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n                 'Total Organic Carbon','Vitrinite Reflectance']\n    resptitle = 'Normalized Initial Production'\n\n    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\n    featuretitle = predtitle + [resptitle]\n\n    m = len(pred) + 1\n    mpred = len(pred)\n\n# elif idata == 1:\n#     names = {'Porosity':'Por'}\n\n#     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n#     df = df.rename(columns=names)\n#     df['Por'] = df['Por'] * 100.0; df['AI'] = df['AI'] / 1000.0; \n#     df.drop('Unnamed: 0',axis=1,inplace=True) \n\n#     features = df.columns.values.tolist()                          # store the names of the features\n\n#     xmin = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n#     flabel = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n#               'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n#     ftitle = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n#               'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\nelif idata == 2:  \n    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n\n    response = 'CumulativeOil'                                             # specify the response feature\n    x = df.copy(deep = True); x = x.drop(['Well_ID','X','Y',response],axis='columns') # make predictor and response DataFrames\n    Y = df.loc[:,response]\n\n    features = x.columns.values.tolist() + [Y.name]               # store the names of the features\n    pred = x.columns.values.tolist()\n    resp = Y.name\n\n    xmin = [1.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax = [75.0,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n    Ymin = 0.0; Ymax = 3000.0\n\n    predlabel = ['Well (ID)','X (m)','Y (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] \n    resplabel = 'Cumulative Production (MSTB)'\n\n    predtitle = ['Well','X','Y','Porosity','Permeability','Acoustic Impedance',\n              'Density (g/cm^3)','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus'] \n    resptitle = 'Cumulative Production'\n\n    featurelabel = predlabel + [resplabel]                        # make feature labels and titles for concise code\n    featuretitle = predtitle + [resptitle]\n\n    m = len(pred) + 1\n    mpred = len(pred) \n```", "```py\n---------------------------------------------------------------------------\nSSLCertVerificationError  Traceback (most recent call last)\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1317, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)\n  1316 try:\n-> 1317     h.request(req.get_method(), req.selector, req.data, headers,\n  1318               encode_chunked=req.has_header('Transfer-encoding'))\n  1319 except OSError as err: # timeout error\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1230, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)\n  1229  \"\"\"Send a complete request to the server.\"\"\"\n-> 1230 self._send_request(method, url, body, headers, encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1276, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)\n  1275     body = _encode(body, 'body')\n-> 1276 self.endheaders(body, encode_chunked=encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1225, in HTTPConnection.endheaders(self, message_body, encode_chunked)\n  1224     raise CannotSendHeader()\n-> 1225 self._send_output(message_body, encode_chunked=encode_chunked)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1004, in HTTPConnection._send_output(self, message_body, encode_chunked)\n  1003 del self._buffer[:]\n-> 1004 self.send(msg)\n  1006 if message_body is not None:\n  1007 \n  1008     # create a consistent interface to message_body\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:944, in HTTPConnection.send(self, data)\n  943 if self.auto_open:\n--> 944     self.connect()\n  945 else:\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\http\\client.py:1399, in HTTPSConnection.connect(self)\n  1397     server_hostname = self.host\n-> 1399 self.sock = self._context.wrap_socket(self.sock,\n  1400                                       server_hostname=server_hostname)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:500, in SSLContext.wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\n  494 def wrap_socket(self, sock, server_side=False,\n  495                 do_handshake_on_connect=True,\n  496                 suppress_ragged_eofs=True,\n  497                 server_hostname=None, session=None):\n  498     # SSLSocket class handles server_hostname encoding before it calls\n  499     # ctx._wrap_socket()\n--> 500     return self.sslsocket_class._create(\n  501         sock=sock,\n  502         server_side=server_side,\n  503         do_handshake_on_connect=do_handshake_on_connect,\n  504         suppress_ragged_eofs=suppress_ragged_eofs,\n  505         server_hostname=server_hostname,\n  506         context=self,\n  507         session=session\n  508     )\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:1040, in SSLSocket._create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\n  1039             raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\n-> 1040         self.do_handshake()\n  1041 except (OSError, ValueError):\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\ssl.py:1309, in SSLSocket.do_handshake(self, block)\n  1308         self.settimeout(None)\n-> 1309     self._sslobj.do_handshake()\n  1310 finally:\n\nSSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n\nDuring handling of the above exception, another exception occurred:\n\nURLError  Traceback (most recent call last)\nCell In[7], line 3\n  1 idata = 0\n  2 if idata == 0:\n----> 3     df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n  5     response = 'Prod'                                             # specify the response feature\n  6     x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns') # make predictor and response DataFrames\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  899 kwds_defaults = _refine_defaults_read(\n  900     dialect,\n  901     delimiter,\n   (...)\n  908     dtype_backend=dtype_backend,\n  909 )\n  910 kwds.update(kwds_defaults)\n--> 912 return _read(filepath_or_buffer, kwds)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577, in _read(filepath_or_buffer, kwds)\n  574 _validate_names(kwds.get(\"names\", None))\n  576 # Create the parser.\n--> 577 parser = TextFileReader(filepath_or_buffer, **kwds)\n  579 if chunksize or iterator:\n  580     return parser\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407, in TextFileReader.__init__(self, f, engine, **kwds)\n  1404     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n  1406 self.handles: IOHandles | None = None\n-> 1407 self._engine = self._make_engine(f, self.engine)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661, in TextFileReader._make_engine(self, f, engine)\n  1659     if \"b\" not in mode:\n  1660         mode += \"b\"\n-> 1661 self.handles = get_handle(\n  1662     f,\n  1663     mode,\n  1664     encoding=self.options.get(\"encoding\", None),\n  1665     compression=self.options.get(\"compression\", None),\n  1666     memory_map=self.options.get(\"memory_map\", False),\n  1667     is_text=is_text,\n  1668     errors=self.options.get(\"encoding_errors\", \"strict\"),\n  1669     storage_options=self.options.get(\"storage_options\", None),\n  1670 )\n  1671 assert self.handles is not None\n  1672 f = self.handles.handle\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:716, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n  713     codecs.lookup_error(errors)\n  715 # open URLs\n--> 716 ioargs = _get_filepath_or_buffer(\n  717     path_or_buf,\n  718     encoding=encoding,\n  719     compression=compression,\n  720     mode=mode,\n  721     storage_options=storage_options,\n  722 )\n  724 handle = ioargs.filepath_or_buffer\n  725 handles: list[BaseBuffer]\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:368, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n  366 # assuming storage_options is to be interpreted as headers\n  367 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--> 368 with urlopen(req_info) as req:\n  369     content_encoding = req.headers.get(\"Content-Encoding\", None)\n  370     if content_encoding == \"gzip\":\n  371         # Override compression based on Content-Encoding header\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:270, in urlopen(*args, **kwargs)\n  264  \"\"\"\n  265 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n  266 the stdlib.\n  267 \"\"\"\n  268 import urllib.request\n--> 270 return urllib.request.urlopen(*args, **kwargs)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:222, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n  220 else:\n  221     opener = _opener\n--> 222 return opener.open(url, data, timeout)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n  522     req = meth(req)\n  524 sys.audit('urllib.Request', req.full_url, req.data, req.headers, req.get_method())\n--> 525 response = self._open(req, data)\n  527 # post-process response\n  528 meth_name = protocol+\"_response\"\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:542, in OpenerDirector._open(self, req, data)\n  539     return result\n  541 protocol = req.type\n--> 542 result = self._call_chain(self.handle_open, protocol, protocol +\n  543                           '_open', req)\n  544 if result:\n  545     return result\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:502, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n  500 for handler in handlers:\n  501     func = getattr(handler, meth_name)\n--> 502     result = func(*args)\n  503     if result is not None:\n  504         return result\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1360, in HTTPSHandler.https_open(self, req)\n  1359 def https_open(self, req):\n-> 1360     return self.do_open(http.client.HTTPSConnection, req,\n  1361         context=self._context, check_hostname=self._check_hostname)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\urllib\\request.py:1320, in AbstractHTTPHandler.do_open(self, http_class, req, **http_conn_args)\n  1317         h.request(req.get_method(), req.selector, req.data, headers,\n  1318                   encode_chunked=req.has_header('Transfer-encoding'))\n  1319     except OSError as err: # timeout error\n-> 1320         raise URLError(err)\n  1321     r = h.getresponse()\n  1322 except:\n\nURLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)> \n```", "```py\nPormin = np.min(df['Por'].values)                             # extract ndarray of data table column\nPormax = np.max(df['Por'].values)                             # and calculate min and max \n```", "```py\ndf.head(n=13)                                                 # we could also use this command for a table preview \n```", "```py\ndf.describe().transpose()                                     # calculate summary statistics for the data \n```", "```py\nplt.subplot(111)\n(df.isnull().sum()/len(df)).plot(kind = 'bar')                # calculate DataFrame with percentage missing by feature\nplt.xlabel('Feature'); plt.ylabel('Percentage of Missing Values'); plt.title('Data Completeness'); plt.ylim([0.0,1.0])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.2); add_grid(); plt.show() \n```", "```py\nproportion_NaN = 0.1                                    # proportion of values in DataFrame to remove\n\nremove = np.random.random(df.shape) < proportion_NaN    # make the boolean array for removal\nprint('Fraction of removed values in mask ndarray = ' + str(round(remove.sum()/remove.size,3)) + '.')\n\ndf_mask = df.mask(remove)                               # make a new DataFrame with specified proportion removed \n```", "```py\ndf_temp = df.copy(deep=True)                                  # make a deep copy of the DataFrame\ndf_bool = df_temp.isnull()                                    # true is value, false if NaN\n#df_bool = df_bool.set_index(df_temp.pop('UWI'))              # set the index / feature for the heat map y column\nheat = sns.heatmap(df_bool, cmap=['r','w'], annot=False, fmt='.0f',cbar=False,linecolor='black',linewidth=0.1) # make the binary heat map, no bins\nheat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=8)\nheat.set_yticklabels(heat.get_yticklabels(), rotation=0, fontsize=8)\nheat.set_title('Data Completeness Heatmap',fontsize=16); heat.set_xlabel('Feature',fontsize=12); heat.set_ylabel('Sample (Index)',fontsize=12)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.8, top=1.6, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.dropna(axis=0,how='any',inplace=True)                      # likewise deletion \n```", "```py\ndf.dropna(axis=0,how='any',inplace=True)                      # likewise deletion \n```", "```py\ndf.describe().transpose()                                     # DataFrame summary statistics \n```", "```py\nnbins = 20                                                    # number of histogram bins\nfor i, feature in enumerate(features):                        # plot histograms with central tendency and P10 and P90 labeled\n    plt.subplot(4,3,i+1)\n    y,_,_ = plt.hist(x=df[feature],weights=None,bins=nbins,alpha = 0.8,edgecolor='black',color='darkorange',density=True)\n    # histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')\n    plt.xlabel(feature); plt.ylabel('Frequency'); plt.ylim([0.0,y.max()*1.10]); plt.title(featuretitle[i]); add_grid() \n    # if feature == resp: \n    #     plt.xlim([Ymin,Ymax]) \n    # else:\n    #     plt.xlim([xmin[i],xmax[i]]) \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=4.1, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response \n```", "```py\ncovariance = df.iloc[:,df.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp\ncov_matrix = df.iloc[:,df.columns.get_indexer(features)].cov()\nplt.subplot(121)\nplot_corr(cov_matrix,'Covariance Matrix',4000.0,0.1)          # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features') \n\nplt.subplot(122)\nfeature_rank_plot(features,covariance,-20000.0,20000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\ndf.iloc[:,1:8].corr() \n```", "```py\ncorrelation = df.iloc[:,df.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)] # calculate covariance matrix and slice for only pred - resp\ncorr_matrix = df.iloc[:,df.columns.get_indexer(features)].corr()\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.5)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nstats.spearmanr(df.iloc[:,1:8]) \n```", "```py\nrank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the rank correlation coefficient\nrank_matrix = pd.DataFrame(rank_correlation,columns=corr_matrix.columns)\nrank_correlation = rank_correlation[:,len(features)-1][:len(features)]\nrank_correlation_pval = rank_correlation_pval[:,len(pred)-1][:len(features)]\nprint(\"\\nRank Correlation p-value:\\n\"); print(rank_correlation_pval)\n\nplt.subplot(121)\nplot_corr(rank_matrix,'Rank Correlation Matrix',1.0,0.5)      # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nRank Correlation p-value:\n\n[2.43279911e-02 1.34135205e-01 1.18844068e-10 2.71646948e-04\n 2.11367755e-06 0.00000000e+00 3.29170847e-04] \n```", "```py\nplt.subplot(121)                                              # plot correlation matrix with significance colormap\ndiff = corr_matrix.values - rank_matrix.values\ndiff_matrix = pd.DataFrame(diff,columns=corr_matrix.columns)\nplot_corr(diff_matrix,'Correlation - Rank Correlation',0.1,0.1) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\ncorr_diff = correlation - rank_correlation\n\nplt.subplot(122)\nfeature_rank_plot(features,corr_diff,-0.20,0.20,0.0,'Correlation Coefficient - Rank Correlation Coefficient','Correlation Diffference',0.1)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npartial_correlation = partial_corr(df.iloc[:,df.columns.get_indexer(features)]) # calculate the partial correlation coefficients\npartial_matrix = pd.DataFrame(partial_correlation,columns=corr_matrix.columns)\npartial_correlation = partial_correlation[:,len(features)-1][:len(features)] # extract a single row and remove production with itself \n\nplt.subplot(121)\nplot_corr(partial_matrix,'Partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nsemipartial_correlation = semipartial_corr(df.iloc[:,df.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients\nsemipartial_matrix = pd.DataFrame(semipartial_correlation,columns=corr_matrix.columns)\nsemipartial_correlation = semipartial_correlation[:,len(features)-1][:len(features)]    # extract a single row and remove production with itself\n\nplt.subplot(121)\nplot_corr(semipartial_matrix,'Semi-partial Correlation Matrix',1.0,0.5) # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\n# plt.subplot(151)\n# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.1)\n\nplt.subplot(131)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplot(132)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(133)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\n# plt.subplot(155)\n# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2); plt.show() \n```", "```py\n# dfS = pd.DataFrame()                                        # affine correction, standardization to a mean of 0 and variance of 1 \n# dfS['Well'] = df['Well'].values\n# dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)\n# dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)\n# dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)\n# dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)\n# dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)\n# dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)\n# dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)\n# dfS.head() \n```", "```py\ndfS = pd.DataFrame()                                          # Gaussian transform, standardization to a mean of 0 and variance of 1 \n\nfor feature in features:\n    dfS[feature],d1,d2 = geostats.nscore(df,feature)\n\ndfS.head() \n```", "```py\ndfS.describe()                                                # check the summary statistics \n```", "```py\npairgrid = sns.PairGrid(dfS) # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              shade = False, shade_lowest = False, alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nstand_covariance = dfS.iloc[:,dfS.columns.get_indexer(features)].cov().iloc[len(features)-1,:len(features)]\nstand_correlation = dfS.iloc[:,dfS.columns.get_indexer(features)].corr().iloc[len(features)-1,:len(features)]\n\nstand_rank_correlation, stand_rank_correlation_pval = stats.spearmanr(dfS.iloc[:,dfS.columns.get_indexer(features)])\nstand_rank_correlation = stand_rank_correlation[:,len(features)-1][:len(features)]\nstand_partial_correlation = partial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)]) # calculate the partial correlation coefficients\nstand_partial_correlation = stand_partial_correlation[:,len(features)-1][:len(features)]\nstand_semipartial_correlation = semipartial_corr(dfS.iloc[:,dfS.columns.get_indexer(features)])    # calculate the semi-partial correlation coefficients\nstand_semipartial_correlation = stand_semipartial_correlation[:,len(features)-1][:len(features)] \n```", "```py\n# plt.subplot(2,5,1)\n# feature_rank_plot(features,covariance,-5000.0,5000.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance',0.5)\n\nplt.subplot(2,3,1)\nfeature_rank_plot(features,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation',0.5)\n\nplt.subplot(2,3,2)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(2,3,3)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\n# plt.subplot(2,5,5)\n# feature_rank_plot(features,semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation',0.5)\n\n# plt.subplot(2,5,6)\n# feature_rank_plot(features,stand_covariance,-1.0,1.0,0.0,'Feature Ranking, Covariance with ' + resp,'Covariance of Standardized',0.5)\n\nplt.subplot(2,3,4)\nfeature_rank_plot(features,stand_correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + resp,'Correlation of Standardized',0.5)\n\nplt.subplot(2,3,5)\nfeature_rank_plot(features,stand_rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation of Standardized',0.5)\n\nplt.subplot(2,3,6)\nfeature_rank_plot(features,stand_partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation of Standardized',0.5)\n\n# plt.subplot(2,5,10)\n# feature_rank_plot(features,stand_semipartial_correlation,-1.0,1.0,0.0,'Feature Ranking, Semipartial Correlation with ' + resp,'Semipartial Correlation of Standardized',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.3, hspace=0.3); plt.show() \n```", "```py\ndf['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') \n```", "```py\nx = df[['Por','Perm','AI','Brittle','TOC','VR']]\nx_stand = (x - x.mean()) / (x.std()) \n```", "```py\nx = pd.concat([df['tProd'],x_stand.iloc[:,0:6]],axis=1) \n```", "```py\nx = pd.melt(x,id_vars=\"tProd\",var_name=\"Predictors\",value_name='Standardized_Value') \n```", "```py\nthreshold = 2000.0\n\ndf['tProd'] = np.where(df[resp]>=threshold, 'High', 'Low')       # make a high and low production categorical feature\n\nx_temp = df[pred]\nx_temp_stand = (x_temp - x_temp.mean()) / (x_temp.std())      # standardization by feature\nx_temp = pd.concat([df['tProd'],x_temp_stand.iloc[:,0:len(pred)]],axis=1) # add the production categorical feature to the DataFrame\nx_temp = pd.melt(x_temp,id_vars=\"tProd\",var_name=\"Predictor Feature\",value_name='Standardized Predictor Feature') # unpivot the DataFrame\n\nplt.subplot(111)\nsns.violinplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp,split=True, inner=\"quart\", palette=\"Set2\")\nplt.xticks(rotation=90); plt.title('Conditional Distributions by Production')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.subplot(111)\nsns.boxplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp)\nplt.xticks(rotation=90)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)\nplt.show()\n\ndf = df.drop(['tProd'], axis = 1) \n```", "```py\nvif_values = []\nfor i in range(df[pred].values.shape[1]):\n    vif_values.append(variance_inflation_factor(df[pred].values, i))\n\nvif_values = np.asarray(vif_values)\nindices = np.argsort(vif_values)[::-1]                  # find indices for descending order\n\nplt.subplot(111)                                        # plot the feature importance \nplt.title(\"Variance Inflation Factor\")\nplt.bar(range(df[pred].values.shape[1]), vif_values[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha=0.6, align=\"center\")\nplt.xticks(np.linspace(0,len(pred)-1,len(pred)), np.array(pred)[indices].tolist(),rotation=90); \n\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.yscale('log');\nplt.xlabel('Predictor Feature'); plt.ylabel('Variance Inflation Factor')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nreg = LinearRegression()                                      # instantiate a linear regression model \nreg.fit(df[pred],df[resp])                                    # train the model\nb = reg.coef_\n\nplt.subplot(111)\nfeature_rank_plot(pred,b,-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,r'Linear Regression Slope, $b_1$',0.5) \n```", "```py\nreg = LinearRegression()\nreg.fit(dfS[pred],dfS[resp])\nbeta = reg.coef_\n\nplt.subplot(111)\nfeature_rank_plot(pred,beta,-1.0,1.0,0.0,r'Feature Ranking, $\\beta$ Coefficients with ' + resp,r'Standardized Linear Regression Slope, $b_1$',0.5) \n```", "```py\n# Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization\nlab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y) # this removes an encoding error \n\nrandom_forest = RandomForestRegressor()                 # instantiate the random forest \nrandom_forest = random_forest.fit(x,np.ravel(Y_encoded)) # fit the random forest\nimportance_rank = random_forest.feature_importances_    # extract the expected feature importances\n\nimportance_rank_stand = importance_rank/np.max(importance_rank)                          # calculate relative mutual information\n\nstd = np.std([tree.feature_importances_ for tree in random_forest.estimators_],axis=0) # calculate stdev over trees\nindices = np.argsort(importance_rank)[::-1]             # find indices for descending order\n\nplt.subplot(111)                                        # plot the feature importance \nplt.title(\"Random Forest-based Feature importances\")\nplt.bar(range(x.shape[1]), importance_rank[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha = 0.6, yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5]); \nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.ylim([0.,1.0])\nplt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nplt.subplot(231)\nfeature_rank_plot(features,rank_correlation,-1.0,1.0,0.0,'Feature Ranking, Rank Correlation with ' + resp,'Rank Correlation',0.5)\n\nplt.subplot(232)\nfeature_rank_plot(features,partial_correlation,-1.0,1.0,0.0,'Feature Ranking, Partial Correlation with ' + resp,'Partial Correlation',0.5)\n\nplt.subplot(234)\nfeature_rank_plot(pred,b[0:len(pred)],-1000.0,1000.0,0.0,'Feature Ranking, B Coefficients with ' + resp,'B Coefficients',0.5)\n\nplt.subplot(235)\nfeature_rank_plot(pred,beta[0:len(pred)],-1.0,1.0,0.0,'Feature Ranking, Beta Coefficients with ' + resp,'Beta Coefficients',0.5)\n\nplt.subplot(236)\nfeature_rank_plot(pred,importance_rank_stand,0.0,1.0,0.0,'Feature Ranking, Feature Importance with ' + resp,'Standardized Feature Importance',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nindices = np.argsort(importances)[::-1] \n```", "```py\nx_df = df.loc[:,pred]                            # separate DataFrames for predictor and response features\ny_df = df.loc[:,resp]\n\nmi = mutual_info_regression(x_df,np.ravel(y_df))              # calculate mutual information\nmi /= np.max(mi)                                        # calculate relative mutual information\n\nindices = np.argsort(mi)[::-1]                          # find indices for descending order\n\nprint(\"Feature ranking:\")                               # write out the feature importances\nfor f in range(x.shape[1]):\n    print(\"%d. feature %s = %f\" % (f + 1, x.columns[indices][f], mi[indices[f]]))\n\nplt.subplot(111)                                        # plot the relative mutual information \nplt.title(\"Mutual Information\")\nplt.bar(range(x.shape[1]), mi[indices],edgecolor = 'black',\n       color=\"darkorange\",alpha=0.6,align=\"center\")\nplt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.ylim([0,1.3])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.xlabel('Predictor Feature'); plt.ylabel('Mutual Information')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nFeature ranking:\n1\\. feature Por = 1.000000\n2\\. feature Perm = 0.345842\n3\\. feature TOC = 0.272418\n4\\. feature Brittle = 0.073310\n5\\. feature AI = 0.059024\n6\\. feature VR = 0.000000 \n```", "```py\nobj_mutual = mutual_information_objective(x_df,y_df)\nindices_obj = np.argsort(obj_mutual)[::-1]              # find indices for descending order\n\nplt.subplot(111)                                        # plot the relative mutual information \nplt.title(\"One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection\")\nplt.bar(range(x.shape[1]), obj_mutual[indices_obj],\n       color=\"darkorange\",alpha = 0.6, align=\"center\",edgecolor=\"black\")\nplt.xticks(range(x.shape[1]), x.columns[indices_obj],rotation=90)\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.xlim([-0.5, x.shape[1]-0.5]); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\ndelta_mutual_information = delta_mutual_information_quotient(x_df,y_df)\n\nindices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order\n\nplt.subplot(111)                                              # plot the relative mutual information \nplt.title(\"Delta Mutual Information Quotient\")\nplt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],\n       color=\"darkorange\",alpha = 0.6,align=\"center\",edgecolor = 'black')\nplt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nplt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')\nfor i, feature in enumerate(x.columns):\n    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))\nplt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')\nplt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)\nplt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); \nplt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')\nplt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)\nplt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndelta_mutual_information = delta_mutual_information_quotient(x_df,y_df)\n\nindices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1] # find indices for descending order\n\nplt.subplot(111)                                              # plot the relative mutual information \nplt.title(\"Delta Mutual Information Quotient\")\nplt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],\n       color=\"darkorange\",alpha = 0.6,align=\"center\",edgecolor = 'black')\nplt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)\nplt.xlim([-0.5, x.shape[1]-0.5])\nplt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\nplt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\nplt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\nplt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\nplt.show() \n```", "```py\nplt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='black',edgecolor='black')\nfor i, feature in enumerate(x.columns):\n    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.2,stats.rankdata(-vif_values)[i]+0.1))\nplt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')\nplt.xlim(0,len(pred)+0.1); plt.ylim(0,len(pred)+0.1)\nplt.plot([2,len(pred)],[0,len(pred)-2],color='black',alpha=0.5,ls='--'); \nplt.plot([0,len(pred)-2],[2,len(pred)],color='black',alpha=0.5,ls='--')\nplt.fill_between([0,len(pred)-2], [2,len(pred)], [len(pred),len(pred)], color='coral',alpha=0.2,zorder=1)\nplt.fill_between([2,len(pred)], [0,len(pred)-2], [0,0], color='dodgerblue',alpha=0.2,zorder=1)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nrfe_linear = RFE(LinearRegression(),n_features_to_select=1,verbose=0) # set up RFE linear regression model\ndf['const'] = np.ones(len(df))                                # let's add one's for the constant term\nrfe_linear = rfe_linear.fit(df[pred].values,np.ravel(df[resp])) # recursive elimination\ndfS = df.drop('const',axis = 1)                               # remove the ones\nprint('Recursive Feature Elimination: Multilinear Regression')\nfor i in range(0,len(pred)):\n    print('Rank #' + str(i+1) + ' ' + pred[rfe_linear.ranking_[i]-1]) \n```", "```py\nRecursive Feature Elimination: Multilinear Regression\nRank #1 Brittle\nRank #2 TOC\nRank #3 AI\nRank #4 VR\nRank #5 Por\nRank #6 Perm \n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nimport warnings\nwarnings.filterwarnings('ignore')            \nimport geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\nrfe_rf = RFE(RandomForestRegressor(max_depth=3),n_features_to_select=1,verbose=0) # set up RFE linear regression model\ndf['const'] = np.ones(len(df))                                # let's add one's for the constant term\n\nlab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y)\n\nrfe_rf = rfe_rf.fit(x,np.ravel(Y_encoded))                    # recursive elimination\ndfS = df.drop('const',axis = 1)                               # remove the ones\nprint('Recursive Feature Elimination: Random Forest Regression')\nfor i in range(0,len(pred)):\n    print('Rank #' + str(i+1) + ' ' + pred[rfe_rf.ranking_[i]-1]) \n```", "```py\nRecursive Feature Elimination: Random Forest Regression\nRank #1 Por\nRank #2 VR\nRank #3 Brittle\nRank #4 Perm\nRank #5 TOC\nRank #6 AI \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\n\n# #Underfit random forest\nmax_leaf_nodes = 2\nnum_tree = 10\nmax_features = 2\n\n#Overfit random forest\nmax_leaf_nodes = 50\nnum_tree = 1\nmax_features = 6\n\n# #Good random forest\nmax_leaf_nodes = 5\nnum_tree = 300\nmax_features = 2\n\nrfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)\nrfr.fit(X = x, y = Y)\n\nY_hat = predict_train = rfr.predict(x)\n\nMSE = metrics.mean_squared_error(Y,Y_hat)\nVar_Explained = metrics.explained_variance_score(Y,Y_hat)\nprint('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))\n\nimportances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature\nstd = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)\nindices = np.argsort(importances)[::-1].tolist()\n\nplt.subplot(121)\nplt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\nplt.xlim(0,7000); plt.ylim(0,7000)\nplt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)\n\nplt.subplot(122)\nplt.title(\"Feature Importances\")\nplt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color=\"darkorange\", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align=\"center\")\n#plt.xticks(range(X.shape[1]), indices)\nplt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nMean Squared Error on Training =  428100.87 , Variance Explained = 0.82 \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\n\n# #Underfit random forest\nmax_leaf_nodes = 2\nnum_tree = 10\nmax_features = 2\n\n#Overfit random forest\nmax_leaf_nodes = 50\nnum_tree = 1\nmax_features = 6\n\n# #Good random forest\nmax_leaf_nodes = 5\nnum_tree = 300\nmax_features = 2\n\nrfr = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=num_tree, max_features=max_features)\nrfr.fit(X = x, y = Y)\n\nY_hat = predict_train = rfr.predict(x)\n\nMSE = metrics.mean_squared_error(Y,Y_hat)\nVar_Explained = metrics.explained_variance_score(Y,Y_hat)\nprint('Mean Squared Error on Training = ', round(MSE,2),', Variance Explained =', round(Var_Explained,2))\n\nimportances = rfr.feature_importances_               # expected (global) importance over the forest fore each predictor feature\nstd = np.std([rfr.feature_importances_ for tree in rfr.estimators_],axis=0)\nindices = np.argsort(importances)[::-1].tolist()\n\nplt.subplot(121)\nplt.scatter(Y,Y_hat,s=None, c='darkorange',marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.3, edgecolors=\"black\")\nplt.title('Random Forest Model'); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\nplt.xlim(0,7000); plt.ylim(0,7000)\nplt.arrow(0,0,7000,7000,width=0.02,color='black',head_length=0.0,head_width=0.0)\n\nplt.subplot(122)\nplt.title(\"Feature Importances\")\nplt.bar([pred[i] for i in indices],rfr.feature_importances_[indices],color=\"darkorange\", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align=\"center\")\n#plt.xticks(range(X.shape[1]), indices)\nplt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nMean Squared Error on Training =  428100.87 , Variance Explained = 0.82 \n```", "```py\nbackground = shap.sample(x,nsamples=50,random_state=73073) \nmodel_explainer = shap.TreeExplainer(rfr)\nshap_values = model_explainer.shap_values(background) # global Shapley Measures \n```", "```py\nshap_values.shape \n```", "```py\n(50, 6) \n```", "```py\nnback = 7\n\nresp_avg  = np.average(Y_hat)\nyhat = rfr.predict(background.iloc[[nback]])\n\ncurrent = resp_avg\n\nplt.subplot(111)\n\nplt.plot([current,current],[0,0.3],color='black',lw=2,zorder=1)\nplt.plot([current-2,current],[0.2,0.3],color='black',lw=2,zorder=1)\nplt.plot([current,current+2],[0.3,0.2],color='black',lw=2,zorder=1)\nfor i in range(len(pred)+1):\n    plt.scatter(current,i+0.5,color='grey',edgecolor='black',zorder=10)\n    if i < len(pred):\n        if shap_values[nback,i] > 0.0:\n            color = 'red'\n        else:\n            color = 'blue'\n        plt.plot([current,current + shap_values[nback,i]],[i+1,i+1],color=color,lw=2,zorder=1)\n        plt.plot([current,current],[i+0.6,i+1],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]],[i+1,i+1.3],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i]-2,current + shap_values[nback,i]],[i+1.2,i+1.3],color=color,lw=2,zorder=1)\n        plt.plot([current + shap_values[nback,i],current + shap_values[nback,i]+2],[i+1.3,i+1.2],color=color,lw=2,zorder=1)\n        if shap_values[nback,i] > 0.0:\n            plt.annotate('+ ' + str(np.round(shap_values[nback,i],0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')\n        else:\n            plt.annotate('- ' + str(np.round(abs(shap_values[nback,i]),0)),[current + shap_values[nback,i]*0.5,i+1.1],ha='center')\n        current = current + shap_values[nback,i]\n\nplt.plot([current,current],[i+0.7,i+1],color='black',lw=2,zorder=1)\nplt.plot([current-2,current],[i+0.9,i+1],color='black',lw=2,zorder=1)\nplt.plot([current,current+2],[i+1,i+0.9],color='black',lw=2,zorder=1)\n\nplt.plot([resp_avg,resp_avg],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)\nplt.plot([yhat,yhat],[-0.5,len(pred)+1.5],color='black',ls='--',zorder=1)\nplt.annotate('Response Feature, Training Average',[resp_avg-8,1.0],rotation=90.0)\nplt.annotate('Model Prediction',[yhat-8,1.0],rotation=90.0)\n\nplt.yticks(ticks=np.arange(len(pred)+2), labels=[r'None / $\\overline{y}$'] + pred + [r'$\\hat{y}=f(X)$'])\nadd_grid(); plt.ylim([-0.5,len(pred)+1.5])\nplt.xlabel('Production (MCFPD)'); plt.ylabel('Feature'); plt.title('Local Shapley Values, Background Index: ' + str(nback))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nshap.force_plot(model_explainer.expected_value,shap_values,background,out_names = ['Production'],feature_names=pred,) \n```", "```py\nshap.force_plot(model_explainer.expected_value,shap_values[nback],background.iloc[[nback]],show=False,feature_names = pred) \n```", "```py\nplt.subplot(131)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background, plot_type=\"bar\",color = \"darkorange\",cmap = plt.cm.inferno)\nplt.ylabel('Predictor Features')\n\nplt.subplot(132)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,cmap = plt.cm.inferno)\n\nplt.subplot(133)\nshap.summary_plot(show=False,feature_names = pred, shap_values = shap_values, features = background,plot_type = \"violin\")\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2)\nplt.show() \n```"]