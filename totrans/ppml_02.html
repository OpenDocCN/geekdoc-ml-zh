<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Chapter 1 What Is This Book About?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Chapter 1 What Is This Book About?</h1>
<blockquote>原文：<a href="https://ppml.dev/intro.html">https://ppml.dev/intro.html</a></blockquote>
<div id="intro" class="section level1 hasAnchor" number="1">

<p>The modern practice of data analysis is shaped by the convergence of many disciplines, each with its own history:
information theory, computer science, optimisation, probability and statistics among them. Machine learning and data
science can be considered their latest incarnations, inheriting the mantle of what used to be called “data analytics”.
Software engineering should be considered as a crucial addition to this list. Why do we need it to implement
modern data analysis efficiently and effectively?</p>
<div id="intro-ml" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Machine Learning<a href="intro.html#intro-ml" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>There are many definitions of machine learning. Broadly speaking, it is a discipline that aims to create computer
systems and algorithms that can learn a structured representation of reality without (or with less) human supervision in
order to interact with it <span class="citation">(Russell and Norvig <a href="#ref-norvig" role="doc-biblioref">2009</a>)</span>. At one end of the spectrum, we can take this to be a narrow version of artificial
general intelligence in which we want our computer systems to learn intellectual tasks independently and to generalise
them to new problems, much like a human being would. At the other end, we can view machine learning as the ability to
learn probabilistic models that provide a simplified representation of a specific phenomenon to perform a specific task
<span class="citation">(Ghahramani <a href="#ref-zoubin" role="doc-biblioref">2015</a>)</span> such as predicting an outcome of interest (supervised learning) or finding meaningful patterns in the data
(unsupervised learning). Somewhere in between these two extremes lie expert systems <span class="citation">(Castillo, Gutiérrez, and Hadi <a href="#ref-castillo" role="doc-biblioref">1997</a>)</span>, which “capture the
ability to think and reason about as an expert would in a particular domain” and can provide “a meaningful answer to a
less than fully specified question.”</p>
<p>Broadly speaking, in order to do this:</p>
<ol style="list-style-type: decimal">
<li>We need a working model of the world that describes the task and its context in a way that a computer can understand.</li>
<li>We need a goal: how do we measure the performance of the model? Because that is what we optimise for! Usually, it is
the ability to predict new events.</li>
<li>We encode our knowledge of the world, drawing information from training data, experts or both.</li>
<li>The computer system uses the model as a proxy of reality and, as new inputs come in, to perform inference and decide
if and how to perform the assigned task.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:data-vs-modeller"/>
<img src="../Images/47b1fcf345d2602e495775c4af2bab7a.png" alt="Different approaches to data analysis grouped by their sources of information: the data or the assumptions made by the modeller." width="80%" data-original-src="https://ppml.dev/chapter01/figures/data-vs-modeller.svg"/>
<p class="caption">
Figure 1.1: Different approaches to data analysis grouped by their sources of information: the data or the assumptions made by the modeller.
</p>
</div>
<p>The exact form these elements take will depend on the domain we are trying to represent and on the model we will use to
represent it. Machine learning is, at its core, a collection of models and algorithms from optimisation, statistics,
probability and information theory that deal with abstract problems: from simple linear regression models <span class="citation">(Weisberg <a href="#ref-weisberg" role="doc-biblioref">2014</a>)</span>,
to Bayesian networks <span class="citation">(Scutari and Denis <a href="#ref-scutari" role="doc-biblioref">2021</a>)</span>, to more complex models such as deep neural networks <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow" role="doc-biblioref">2016</a>)</span> and Gaussian
processes <span class="citation">(Rasmussen and Williams <a href="#ref-gaussianproc" role="doc-biblioref">2006</a>)</span>. These algorithms can be applied to a variety of domains from healthcare <span class="citation">(van der Schaar et al. <a href="#ref-mihaela" role="doc-biblioref">2021</a>)</span> to natural
language processing <span class="citation">(Aggarwal <a href="#ref-mlfortext" role="doc-biblioref">2018</a>)</span> and computer vision <span class="citation">(Voulodimos et al. <a href="#ref-vision" role="doc-biblioref">2018</a>)</span>, with some combinations of algorithms and domains working
out better than others.</p>
<p>In classical statistics (Figure <a href="intro.html#fig:data-vs-modeller">1.1</a>, bottom right), analysing data required the modeller to
specify the probabilistic model generating them in order to draw inferences from a limited number of data points. Such
models would necessarily have a simple structure for two reasons: because the modeller had to manually interpret their
properties and their output, and because of the lack of any substantial computing power to estimate their parameters.
This approach would put all the burden on the modeller: most of the utility that could be had from the model would come
from the ability of the modeller to distil whatever he was modelling into simple mathematics and to incorporate any
available prior information into the model structure. The result is the emphasis on closed-form results, low-order
approximations and asymptotics that characterises the earlier part of modern statistics.</p>
<p>There are, however, many phenomena that cannot be feasibly studied in this fashion. Firstly, there are limits to a human
modeller’s ability to encode complex behaviour when manually structuring models. These limits can easily be exceeded by
phenomena involving large numbers of variables or by non-linear patterns of interactions between variables that are not
very regular or known in advance. Secondly, there may not be enough information available to even attempt to structure a
probabilistic model. Thirdly, limiting our choice of models to those that can be written in closed form to allow the
modeller to fit, interpret and use them manually, without a significant use of computing power, does not necessarily
ensure that those models are easy to interpret. For instance, there are many documented pitfalls in interpreting
logistic regression <span class="citation">(Mood <a href="#ref-mood" role="doc-biblioref">2010</a>; Ranganathan, Pramesh, and Aggarwal <a href="#ref-pitfalls" role="doc-biblioref">2017</a>)</span>, which is arguably the simplest way to implement classification.</p>
<p>Classical applications of Bayesian statistics (Figure <a href="intro.html#fig:data-vs-modeller">1.1</a>, top right) address some of these
limitations. The modeller still has to structure a model covering both the data and any prior beliefs on their
behaviour, but the posterior may be estimated algorithmically using Markov Chain Monte Carlo (MCMC).</p>
<p>In contrast <span class="citation">(Breiman <a href="#ref-two-cultures" role="doc-biblioref">2001</a><a href="#ref-two-cultures" role="doc-biblioref">b</a>)</span>, algorithmic approaches shift the burden from the modeller to data collection and computer
software (Figure <a href="intro.html#fig:data-vs-modeller">1.1</a>, top left). The modeller’s role in constructing the probabilistic model is
limited, and is largely replaced by a computer system sifting through large amounts of data: hence the name “machine
learning”. The structure of the model is learned from the data, with few limitations in what it may look like. Neural
networks and Gaussian processes are universal approximators, for instance. Almost all the information comes from the
data, instead of being prior information that is mediated by the modeller, which is why machine learning approaches are
data-hungry.</p>
</div>
<div id="intro-ds" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Data Science<a href="intro.html#intro-ds" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Data science is similarly data-driven (Figure <a href="intro.html#fig:data-vs-modeller">1.1</a>, top left), but focuses on extracting insights
from raw data and presenting them graphically to support principled decision making. Kenett and Redman <span class="citation">(Kenett and Redman <a href="#ref-kenett" role="doc-biblioref">2019</a>)</span>
describe it as follows: “the real work of data scientists involves helping people make better decisions on the important
issues in the near term and building stronger organizations in the long term”. It requires strong involvement from the
data scientist in all areas of business, shifting the focus from computer systems to people. Nevertheless, data
scientists use statistical and machine learning models as the means to obtain those insights.</p>
<p>Compared to classical statistics, when data are abundant (Big Data! <span class="citation">(Katal, Wazid, and Goudar <a href="#ref-bigdata" role="doc-biblioref">2013</a>)</span>) we do not really need to construct their
generating process from prior knowledge. The data contain enough information for us to “let them speak for themselves”
and obtain useful insights, which are what we are mainly interested in. Of course, prior information from experts is
still useful: models that incorporate it tend to be better at producing insights that can be acted upon.</p>
<p>As a result, data science puts a strong focus on the quality of the data, which is often problematic when dealing with
data aggregated from multiple sources (data fusion) or with non-tabular data (natural language processing and computer
vision). Often, data are poorly defined, simply wrong or ultimately irrelevant for the purpose they were collected for.
Expert knowledge is crucial to assess them, to integrate them and to fix them if possible. Machine learning is widely
applied to both text and images as well, but focused mostly on modelling their hidden structure until recently, when
explainability became a hot topic <span class="citation">(see, for instance, Li et al. <a href="#ref-nlp-viz" role="doc-biblioref">2016</a>; Simonyan, Vedaldi, and Zisserman <a href="#ref-cv-viz" role="doc-biblioref">2014</a>)</span>.</p>
<p>Computer systems are key to data science, albeit with a different role than in machine learning. Storing and accessing
large amounts of data, exploring them interactively, building the software pipelines that analyse them, handling the
resulting spiky workloads: these are all tasks that require a sophisticated use of both hardware and software.</p>
</div>
<div id="intro-sweng" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Software Engineering<a href="intro.html#intro-sweng" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>Software engineering is the systematic application of sound engineering principles to all phases of the software life
cycle: design, development, maintenance, testing and evaluation <span class="citation">(van Vliet <a href="#ref-vanvliet" role="doc-biblioref">2008</a>)</span>. Its central tenet is mastering the
complexity inherent to developing large pieces of software that are reliable and efficient; that are usable and can be
evolved over time; and that can be developed and maintained in a viable way in terms of both cost and effort <span class="citation">(Ousterhout <a href="#ref-philo" role="doc-biblioref">2018</a>)</span>.</p>
<p>Early definitions of software engineering suggested that we should treat it as if it were a traditional engineering
discipline like, say, civil engineering. The result is the <em>waterfall model</em> <span class="citation">(Royce <a href="#ref-waterfall" role="doc-biblioref">1987</a>)</span>, which lays out software
development as a sequence of steps starting from collecting requirements and finishing with the deployment of the
finished product. Modern practices recognise, however, that this model is flawed in several ways. Firstly, civil
engineering arises from and is bound by the laws of physics, whereas we make up our own world with its own rules when we
develop software. These rules will change over time as our understanding of the problem space evolves; the laws of
physics do not. Secondly, the task the software is meant to perform will change over time, and our working definition of
that task will change as well. Civil engineering mostly deals with well-defined problems that stay well-defined for the
duration of the project. Finally, modifying a large building after its construction is completed is very difficult,
but we routinely do that with software. Most of the overall effort in the software lifetime is usually in maintaining
and evolving it.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:software-life-cycle"/>
<img src="../Images/f18887b0ce6e7fa7f6992fea404e8480.png" alt="A schematic view of the phases of the software development life-cycle." width="80%" data-original-src="https://ppml.dev/chapter01/figures/software-life-cycle.svg"/>
<p class="caption">
Figure 1.2: A schematic view of the phases of the software development life-cycle.
</p>
</div>
<p>Current software engineering practices take the opposite view that software development is an open-ended (“software is
never done”), iterative (the “software life-cycle”) process: this is the core of the “Agile Manifesto” <span class="citation">(Beck et al. <a href="#ref-agile" role="doc-biblioref">2001</a>)</span>. At a
high level, it is organised as shown in Figure <a href="intro.html#fig:software-life-cycle">1.2</a>: a perpetual cycle of planning, analysis,
design, implementation, testing and maintenance. The design of the software is heavily influenced by the domain it
operates in <span class="citation">(domain-driven development, Evans <a href="#ref-domain-driven" role="doc-biblioref">2003</a>)</span>. It uses tests <span class="citation">(test-driven development, Beck <a href="#ref-tdd" role="doc-biblioref">2002</a>)</span>, refactoring
<span class="citation">(Fowler <a href="#ref-refactoring" role="doc-biblioref">2018</a>)</span> and continuous integration <span class="citation">(Duvall, Matyas, and Glover <a href="#ref-cicd" role="doc-biblioref">2007</a>)</span> to incorporate new features, to fix bugs in a timely manner and to
keep the code “in shape”. Admittedly, all of these approaches have been touted as silver bullets to the point they have
become buzzwords, and their practical implementation has often distorted them to the point of making software
development worse. However, the key ideas of agile have merit, and we will discuss and apply them in moderation in this
book. They are well suited to structure the development of machine learning pipelines, which are built on a combination
of mutable models and input data.</p>
</div>
<div id="intro-together" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> How Do They Go Together?<a href="intro.html#intro-together" class="anchor-section" aria-label="Anchor link to header"/></h2>
<p>The centrality of computing in machine learning and data science makes software engineering practices essential in
modern data analysis: most of the work is done by computer systems, which are powered by software.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Encoding the data, storing and retrieving them efficiently, implementing machine learning models,
tying them together and with other systems: each of these tasks is complex enough that only sound engineering practices
can ensure the overall correctness of what we are doing. This is true, in different ways, for both academic research and
industry applications. As Kenett and Redman <span class="citation">(Kenett and Redman <a href="#ref-kenett" role="doc-biblioref">2019</a>)</span> put it, using a car analogy:</p>
<blockquote>
<p>“If data is the new oil, technology is the new engine. The engine powers the car and, without technological
advancements, a data- and analytics-led transformation would not be possible. Technologies include databases,
communications systems and protocols, applications that support the storage and processing of data, and the raw
computing horsepower (much of it now in the cloud) to drive it all.”</p>
</blockquote>
<p>In academia, there is a widespread belief that the software implementations of novel methods can be treated as “one-off
scripts”. “We only need to run it once to write this paper, there is no point in refactoring and re-engineering it.” is
a depressingly common sentiment. As is not sharing code to “stay ahead of the competition”. However, research and
application papers using machine learning rely crucially on the quality of the software they use because:</p>
<ol style="list-style-type: decimal">
<li>The models themselves are often black boxes whose mathematical behaviour is not completely understood (Section
<a href="troubleshooting-code.html#model-problems">9.2</a>).</li>
<li>The data are complex enough that even experts in the domains they come from struggle to completely explain them
(Section <a href="troubleshooting-code.html#data-problems">9.1</a>).</li>
</ol>
<p>If we do not understand both the data and the models completely, it becomes very difficult to spot problems in the
software we use to work on them: unexpected behaviour arising from software bugs may be mistaken for a peculiarity in
either of them. It is then crucial that we minimise the chances of this happening by applying all the best engineering
practices we have at our disposal. Present and past failures to do so have led to a widespread “reproducibility crisis”
in fields as diverse as drug research <span class="citation">(Prinz, Schlange, and Asadullah <a href="#ref-repro3" role="doc-biblioref">2011</a>, 20–25% reproducible)</span>, comparative psychology <span class="citation">(Stevens <a href="#ref-repro4" role="doc-biblioref">2017</a>, 36%
reproducible)</span>, finance <span class="citation">(Chang and Li <a href="#ref-repro5" role="doc-biblioref">2015</a>, 43% reproducible)</span> and computational neuroscience <span class="citation">(Miłkowski, Hensel, and Hohol <a href="#ref-repro6" role="doc-biblioref">2018</a>, only 12% of papers provide
both data and code)</span>. Machine learning and artificial intelligence research is in a similarly sorry state: that “when the
original authors provided assistance to the reproducers, 85% of results were successfully reproduced, compared to 4%
when the authors didn’t respond” <span class="citation">(Pineau et al. <a href="#ref-repro7" role="doc-biblioref">2021</a>)</span> <em>does</em> suggest that there is margin for improvement. Fortunately, in recent
years scientists have widely accepted this is a problem <span class="citation">(Nature <a href="#ref-repro1" role="doc-biblioref">2016</a>)</span>, and the machine learning community has reached some
consensus on how to tackle it <span class="citation">(Tatman, VanderPlas, and Dane <a href="#ref-repro2" role="doc-biblioref">2018</a>)</span>.</p>
<p>In industry, poor engineering leads to lower practical and computational performance and a quick accumulation of
technical debt <span class="citation">(Sculley et al. <a href="#ref-hidden-debt" role="doc-biblioref">2015</a>, and Section <a href="design-code.html#technical-debt">5.2</a>)</span>. Badly engineered data may not contain the information
we are looking for in a usable form; models that are not well packaged may be slow to deploy and difficult to roll back;
data may contain biases or may change over time in ways that make models fail silently; or the machine learning software
may become an inscrutable black box whose outputs are impossible to explain, making troubleshooting impossible.</p>
<p>To conclude, we believe that solid machine learning applications and research rest on three pillars:</p>
<ol style="list-style-type: decimal">
<li>The foundations of machine learning (mathematics, probability, computer science), which provide guarantees that the
models work.</li>
<li>Software engineering, which provides guarantees that the implementations of the models work (effectively and
efficiently).</li>
<li>The quality of the data in terms of features, size, fairness, and in how they were collected.</li>
</ol>
<p>In this book, we will concentrate on the software engineering aspect, touching briefly on some aspects of the data. We
will not discuss the theoretical or methodological aspects of machine learning, which are better covered in the huge
amount of specialised literature published to date <span class="citation">(such as Hastie, Tibshirani, and Friedman <a href="#ref-elemstatlearn" role="doc-biblioref">2009</a>; Russell and Norvig <a href="#ref-norvig" role="doc-biblioref">2009</a>; Goodfellow, Bengio, and Courville <a href="#ref-goodfellow" role="doc-biblioref">2016</a>; Gelman et al. <a href="#ref-gelman" role="doc-biblioref">2013</a>; Rasmussen and Williams <a href="#ref-gaussianproc" role="doc-biblioref">2006</a> and
many others)</span>.</p>
<!-- vim: set synmaxcol=600 textwidth=120 colorcolumn=120 spell wrap number: -->

</div>
</div>



    
</body>
</html>