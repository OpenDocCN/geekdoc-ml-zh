["```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.tree import DecisionTreeRegressor                # decision tree method\nfrom sklearn.ensemble import BaggingRegressor                 # bagging tree method\nfrom sklearn.ensemble import RandomForestRegressor            # random forest method\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,clabel,xlabel,ylabel,title):# plots the data points and model \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    im = plt.imshow(Z,interpolation = None,aspect = \"auto\",extent = [x_min,x_max,y_min,y_max], vmin = z_min, vmax = z_max,cmap = cmap)\n    if (type(xfeature) != str) & (type(yfeature) != str):\n        plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=1.0, \n                    linewidths=0.6, edgecolors=\"white\")\n        plt.xlabel(xlabel); plt.ylabel(ylabel)\n    else:\n        plt.xlabel(xlabel); plt.ylabel(ylabel)\n    plt.title(title)                                          # add the labels\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max])\n    cbar = plt.colorbar(im, orientation = 'vertical')         # add the color bar\n    cbar.set_label(clabel, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    return Z\n\ndef visualize_grid(Z,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,clabel,xlabel,ylabel,title,):# plots the data points and the decision tree \n    n_classes = 10\n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),\n                     np.arange(y_min, y_max, yplot_step))\n    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=cmap,norm=None,vmin=z_min,vmax=z_max,alpha=1.0, \n                     linewidths=0.6, edgecolors=\"white\")\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    cbar = plt.colorbar(im, orientation = 'vertical')\n    cbar.set_label(clabel, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_model(model,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree \n    oob_indices = np.setdiff1d(np.arange(len(y)), model.estimators_samples_)\n    oob_y_hat = model.oob_prediction_[oob_indices]\n    oob_y = y.values[oob_indices]\n    MSE_oob = metrics.mean_squared_error(oob_y,oob_y_hat)\n    plt.scatter(oob_y,oob_y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_model_OOB_MSE(model,y,ymin,ymax,ylabel,title):      # OOB MSE and cross plot over multiple bagged trees, checks for unestimated \n    oob_y_hat = model.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; \n    MSE_oob = metrics.mean_squared_error(oob_y,oob_y_hat)\n    plt.scatter(oob_y,oob_y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_grid(grid,xmin,xmax,ymin,ymax,xfeature,yfeature,response,zmin,zmax,title): # plots the estimated vs. the actual \n    if grid.ndim != 2:\n        raise ValueError(\"Prediction array must be 2D\")\n    ny, nx = grid.shape\n    xstep = (xmax - xmin)/nx; ystep = (ymax-ymin)/ny \n    predict_train = feature_sample(grid, xmin, ymin, xstep, ystep, xfeature, yfeature, 'sample')\n    MSE = metrics.mean_squared_error(response,predict_train)\n    plt.scatter(response,predict_train,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(zmin,zmax); plt.ylim(zmin,zmax)\n    plt.plot([zmin,zmax],[zmin,zmax],color='black'); add_grid()\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    # plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500]) # not technically OOB MSE\n\ndef feature_sample(array, xmin, ymin, xstep, ystep, df_x, df_y, name): # sampling predictions from a feature space grid \n    if array.ndim != 2:\n        raise ValueError(\"Array must be 2D\")\n    if len(df_x) != len(df_y):\n        raise ValueError(\"x and y feature arrays must have equal lengths\")   \n    ny, nx = array.shape\n    df = pd.DataFrame()\n    v = []\n    nsamp = len(df_x)\n    for isamp in range(nsamp):\n        x = df_x.iloc[isamp]\n        y = df_y.iloc[isamp]\n        iy = min(ny - int((y - ymin) / ystep) - 1, ny - 1)\n        ix = min(int((x - xmin) / xstep), nx - 1)\n        v.append(array[iy, ix])\n    df[name] = v\n    return df    \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1500.0; ymax = 7000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.head(n=5)                                                  # check the loaded DataFrame \n```", "```py\ndf.describe(percentiles=[0.1,0.9])                            # check DataFrame summary statistics \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq,_,_ = plt.hist(x=df[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq,_,_ = plt.hist(x=df[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df[Xname[0]],df[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplot(224)                                              # predictor feature #2 histogram\nfreq,_,_ = plt.hist(x=df[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(ylabelunit); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right') \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X[Xname[0]],X[Xname[1]],s=None, c=y[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nseed = 73073\nmax_depth = 100\nmin_samples_leaf = 2 \n```", "```py\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) \n```", "```py\nnum_trees = 100\nseed = 73073 \n```", "```py\nbagging_model = BaggingRegressor(base_estimator=regressor, n_estimators=num_trees, random_state=seed) \n```", "```py\nbagging_model.fit(X = predictors, y = response) \n```", "```py\nmax_depth = 100; min_samples_leaf = 2                         # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nnum_tree = 1                                                  # use only a single tree for this demonstration\nseeds = [73073, 73074, 73075, 73076, 73077, 73078]\nbagging_models = []; oob_MSE = []; score = []; pred = []\n\nindex = 1\nfor seed in seeds:                                            # visualize models over random number seeds\n    bagging_models.append(BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,\n                                           bootstrap=True,n_jobs = 4))\n    bagging_models[index-1].fit(X = X, y = y)\n    oob_MSE.append(bagging_models[index-1].oob_score_)\n    plt.subplot(2,3,index)\n    bag_X1 = X.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),0]\n    bag_X2 = X.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),1]\n    bag_y = y.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),0]\n    pred.append(visualize_model(bagging_models[index-1],bag_X1,Xmin[0],Xmax[0],bag_X2,Xmin[1],Xmax[1],bag_y,ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Bootstrap Data and Decision Tree #' + str(index) + ' '))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.4, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor index in range(0,len(seeds)):                             # check models over random number seeds\n    plt.subplot(2,3,index+1)\n    check_model(bagging_models[index],y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions Decision Tree #' +  str(index+1))\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.6, top=2.0, wspace=0.3, hspace=0.3) \n```", "```py\nZ = pred[0] \nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    if index == 1:\n        Z = pred[index-1]\n    else:\n        Z = Z + pred[index-1]                                 # calculate the average response over 3 trees\n    index = index + 1\n\nZ = Z / len(seeds)                                            # grid pixel-wise average of the 6 bootstrapped decision trees\n\nplt.subplot(121)                                              # plot predictions over predictor feature space\nvisualize_grid(Z,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,ylabelunit,\n               Xlabelunit[0],Xlabelunit[1],'All Data and Average of 6 Bootstrapped Trees')\n\nplt.subplot(122)                                              # check model predictions vs. testing dataset\ncheck_grid(Z,Xmin[0],Xmax[0],Xmin[1],Xmax[1],df[Xname[0]],df[Xname[1]],df[yname],ymin,ymax,'Model Check - Average of 6 Bootstrapped Trees')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.3, hspace=0.2) \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nseed = 73073;\nnum_trees = [1,3,5,10,30,500]                                 # number of trees averaged for each estimator\n\nbagging_models_ntrees = []; oob_MSE = []; pred = []\n\nindex = 1\nfor num_tree in num_trees:                                    # visualize the models over number of trees\n    bagging_models_ntrees.append(BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,n_jobs = 4))\n    bagging_models_ntrees[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(bagging_models_ntrees[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Bagging with ' + str(num_tree) + ' Trees'))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.4, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                    # check models over number of trees\n    plt.subplot(2,3,index)\n    check_model_OOB_MSE(bagging_models_ntrees[index-1],y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions with ' + \n                        str(num_trees[index-1]) + ' Decision Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.6, top=2.0, wspace=0.3, hspace=0.3) \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\nntree_list = []; MSE_oob_list = []\nfor num_tree in np.arange(1,350,50):                          # check OOB MSE over number of trees\n    bagg_tree = BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,n_jobs = 4).fit(X = X, y = y)\n    oob_y_hat = bagg_tree.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Number of Bagged Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Bagged Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nseeds = [73083, 73084, 73085]                                 # number of random number seeds\nnum_trees = [1,5,30,100]                                      # number of trees averaged for each estimator\nbagging_models_ntrees_seeds = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                    # loop over number of trees\n    for seed in seeds:                                        # loop over number of random number seeds\n        bagg_tree = BaggingRegressor(base_estimator=regressor, n_estimators=num_tree, \n                                     random_state=seed, oob_score = True, n_jobs = 4).fit(X = X, y = y)\n        oob_y_hat = bagg_tree.oob_prediction_\n        oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n        bagging_models_ntrees_seeds.append(bagg_tree)\n        MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n        plt.subplot(4,3,index)\n        visualize_model(bagging_models_ntrees_seeds[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Tree Model - ' + str(num_tree) + ' Tree(s)')\n        index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.0, wspace=0.35, hspace=0.3); plt.show() \n```", "```py\nmax_features = 'sqrt' \n```", "```py\nseed = 73073 \n```", "```py\nmax_depth = 5 \n```", "```py\nnum_tree = 300 \n```", "```py\nmax_features = 1 \n```", "```py\nmy_first_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features) \n```", "```py\nmy_first_forest.fit(X = predictors, y = response) \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\nmax_depth = 7\nnum_tree = 300\nmax_features = 1\n\nmy_first_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features,\n                                       oob_score=True,bootstrap=True)\n\nmy_first_forest.fit(X = X, y = y)                             # train the model with training data \n\nplt.subplot(121)                                              # predict with the model over the predictor feature space and visualize\nvisualize_model(my_first_forest,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(my_first_forest,y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions with Random Forest')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 1                                                 # set the random forest hyperparameters\nnum_tree = 1\nmax_features = 1\nsimple_forest = []\n\nseeds = [73103,73104,73105,73106,73107,73108]                 # set the random number seeds\n\nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n    simple_forest[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)                                    # predict with the model over the predictor feature space and visualize\n    visualize_model(simple_forest[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.3) \n```", "```py\nmax_depth = 1                                                 # set the random forest hyperparameters\nnum_tree = 1\nmax_features = 2\nsimple_forest = []\n\nseeds = [73103,73104,73105,73106,73107,73108]                 # random number seeds \n\nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n    simple_forest[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)                                    # predict with the model over the predictor feature space and visualize\n    visualize_model(simple_forest[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\nmax_depth = 7\nnum_tree = 500\nmax_features = 1\n\nbig_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features,\n                                   oob_score = True,bootstrap=True,n_jobs = 4)\n\nbig_forest.fit(X = X, y = y)\n\nplt.subplot(121)                                              # predict with the model over the predictor feature space and visualize\nvisualize_model(big_forest,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(big_forest,y,ymin,ymax,ylabelunit,'Model Check Random Forest Model')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2) \n```", "```py\nimportances = big_forest.feature_importances_ \n```", "```py\nimportances = big_forest.feature_importances_                 # expected (global) importance over the forest fore each predictor feature\nstd = np.std([tree.feature_importances_ for tree in big_forest.estimators_],axis=0) # retrieve importance by tree\nindices = np.argsort(importances)[::-1]                       # sort in descending feature importance\nfeatures = ['Porosity','Brittleness']                         # names or predictor features\n\nplt.subplot(121)\nplt.title(\"Random Forest Feature Importances\")\nplt.bar(features, importances[indices],color=\"darkorange\", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align=\"center\")\nplt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance'); add_grid()\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(big_forest,y,ymin,ymax,ylabelunit,'Model Check Random Forest Model for Feature Importance')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 5                                                 # set the random forest hyperparameters\nnum_trees = np.arange(1,100,2)\nmax_features = 1\ntrained_forests = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                     # loop over number of trees in our random forest\n    trained_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=int(num_tree),\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n    index = index + 1\n\nplt.subplot(121)\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Number of Random Forest Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Random Forest Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(ntree_list),max(ntree_list)])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depths = np.linspace(1,20,20)                             # set the tree maximum tree depths to consider\n\nnum_tree = 60                                                 # set the random forest hyperparameters\nmax_features = 1\ntrained_forests = []\nMSE_oob_list = []; max_depth_list = []\n\nindex = 1\nfor max_depth in max_depths:                                  # loop over tree depths \n    trained_forest = RandomForestRegressor(max_depth=int(max_depth), random_state=seed,n_estimators=num_tree,\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); max_depth_list.append(max_depth)\n    index = index + 1\n\nplt.subplot(121)                                                # plot OOB MSE vs. maximum tree depth\nplt.scatter(max_depth_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(max_depth_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Tree Maximum Depth'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Tree Maximum Depth')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(max_depth_list),max(max_depth_list)])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\n\npipe_forest = Pipeline([                                      # the machine learning workflow as a pipeline object\n    ('forest', RandomForestRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'forest__max_leaf_nodes': np.arange(2,100,2,dtype = int),\n}\n\ntuned_forest = GridSearchCV(pipe_forest,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             refit = True)\ntuned_forest.fit(X,y)                                         # fit model with tuned hyperparameters to all the data\n\nprint('Tuned hyperparameter: max_leaf_nodes = ' + str(tuned_forest.best_params_))\n\nestimate = tuned_forest.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + \n      str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nTuned hyperparameter: max_leaf_nodes = {'forest__max_leaf_nodes': 64}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 2001.2 MCFPD \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nmax_leaf_nodes = 30                                           # set the random forest hyperparameters\nnum_trees = np.arange(1,100,2)\nmax_features = max(1,int(len(xname)/3.0))                     # use the m/3 for regression rule\ntrained_forests = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                    # loop over number of trees in our random forest\n    trained_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=int(num_tree),\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n    index = index + 1\n\ntune_num_trees = ntree_list[np.argmin(MSE_oob_list)]          # get the number of trees that minimizes the OOB error\n\nplt.subplot(121)\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.plot([tune_num_trees,tune_num_trees],[min(MSE_oob_list),max(MSE_oob_list)],color='red',ls='--')\nplt.xlabel('Number of Random Forest Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Random Forest Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(ntree_list),max(ntree_list)])\n\ntuned_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=tune_num_trees,\n            oob_score = False,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n\ny_hat = tuned_forest.predict(X)                               # predict over all samples\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Random Forest, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\n%matplotlib inline                                         \nsuppress_warnings = True\nimport os                                                     # to set current working directory \nimport math                                                   # square root operator\nimport numpy as np                                            # arrays and matrix math\nimport scipy.stats as st                                      # statistical methods\nimport pandas as pd                                           # DataFrames\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator,AutoMinorLocator,FuncFormatter) # control of axes ticks\nfrom matplotlib.colors import ListedColormap                  # custom color maps\nimport seaborn as sns                                         # for matrix scatter plots\nfrom sklearn.tree import DecisionTreeRegressor                # decision tree method\nfrom sklearn.ensemble import BaggingRegressor                 # bagging tree method\nfrom sklearn.ensemble import RandomForestRegressor            # random forest method\nfrom sklearn.tree import _tree                                # for accessing tree information\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.preprocessing import StandardScaler              # standardize the features\nfrom sklearn.tree import export_graphviz                      # graphical visualization of trees\nfrom sklearn.model_selection import (cross_val_score,train_test_split,GridSearchCV,KFold) # model tuning\nfrom sklearn.pipeline import (Pipeline,make_pipeline)         # machine learning modeling pipeline\nfrom sklearn import metrics                                   # measures to check our models\nfrom sklearn.model_selection import cross_val_score           # multi-processor K-fold crossvalidation\nfrom sklearn.model_selection import train_test_split          # train and test split\nfrom IPython.display import display, HTML                     # custom displays\ncmap = plt.cm.inferno                                         # default color bar, no bias and friendly for color vision defeciency\nplt.rc('axes', axisbelow=True)                                # grid behind plotting elements\nif suppress_warnings == True:  \n    import warnings                                           # suppress any warnings for this demonstration\n    warnings.filterwarnings('ignore') \nseed = 13                                                     # random number seed for workflow repeatability \n```", "```py\ndef comma_format(x, pos):\n    return f'{int(x):,}'\n\ndef feature_rank_plot(pred,metric,mmin,mmax,nominal,title,ylabel,mask): # feature ranking plot\n    mpred = len(pred); mask_low = nominal-mask*(nominal-mmin); mask_high = nominal+mask*(mmax-nominal); m = len(pred) + 1\n    plt.plot(pred,metric,color='black',zorder=20)\n    plt.scatter(pred,metric,marker='o',s=10,color='black',zorder=100)\n    plt.plot([-0.5,m-1.5],[0.0,0.0],'r--',linewidth = 1.0,zorder=1)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric < nominal),interpolate=True,color='dodgerblue',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.zeros(mpred),metric,where=(metric > nominal),interpolate=True,color='lightcoral',alpha=0.3)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_low),metric,where=(metric < mask_low),interpolate=True,color='blue',alpha=0.8,zorder=10)\n    plt.fill_between(np.arange(0,mpred,1),np.full(mpred,mask_high),metric,where=(metric > mask_high),interpolate=True,color='red',alpha=0.8,zorder=10)  \n    plt.xlabel('Predictor Features'); plt.ylabel(ylabel); plt.title(title)\n    plt.ylim(mmin,mmax); plt.xlim([-0.5,m-1.5]); add_grid();\n    return\n\ndef plot_corr(corr_matrix,title,limits,mask):                 # plots a graphical correlation matrix \n    my_colormap = plt.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    white_low = int(128 - mask*128); white_high = int(128+mask*128)\n    newcolors[white_low:white_high, :] = white                # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = corr_matrix.shape[0]\n    im = plt.matshow(corr_matrix,fignum=0,vmin = -1.0*limits, vmax = limits,cmap = newcmp)\n    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns); ax = plt.gca()\n    ax.xaxis.set_label_position('bottom'); ax.xaxis.tick_bottom()\n    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title(title)\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef visualize_model(model,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,clabel,xlabel,ylabel,title):# plots the data points and model \n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step), # set up the mesh\n                     np.arange(y_min, y_max, yplot_step))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])          # predict with our trained model over the mesh\n    Z = Z.reshape(xx.shape)\n    im = plt.imshow(Z,interpolation = None,aspect = \"auto\",extent = [x_min,x_max,y_min,y_max], vmin = z_min, vmax = z_max,cmap = cmap)\n    if (type(xfeature) != str) & (type(yfeature) != str):\n        plt.scatter(xfeature,yfeature,s=None, c=response, marker=None, cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=1.0, \n                    linewidths=0.6, edgecolors=\"white\")\n        plt.xlabel(xlabel); plt.ylabel(ylabel)\n    else:\n        plt.xlabel(xlabel); plt.ylabel(ylabel)\n    plt.title(title)                                          # add the labels\n    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max])\n    cbar = plt.colorbar(im, orientation = 'vertical')         # add the color bar\n    cbar.set_label(clabel, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n    return Z\n\ndef visualize_grid(Z,xfeature,x_min,x_max,yfeature,y_min,y_max,response,z_min,z_max,clabel,xlabel,ylabel,title,):# plots the data points and the decision tree \n    n_classes = 10\n    xplot_step = (x_max - x_min)/300.0; yplot_step = (y_max - y_min)/300.0 # resolution of the model visualization\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, xplot_step),\n                     np.arange(y_min, y_max, yplot_step))\n    cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n\n    im = plt.scatter(xfeature,yfeature,s=None,c=response,marker=None,cmap=cmap,norm=None,vmin=z_min,vmax=z_max,alpha=1.0, \n                     linewidths=0.6, edgecolors=\"white\")\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    cbar = plt.colorbar(im, orientation = 'vertical')\n    cbar.set_label(clabel, rotation=270, labelpad=20)\n    cbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_model(model,y,ymin,ymax,ylabel,title): # get OOB MSE and cross plot a decision tree \n    oob_indices = np.setdiff1d(np.arange(len(y)), model.estimators_samples_)\n    oob_y_hat = model.oob_prediction_[oob_indices]\n    oob_y = y.values[oob_indices]\n    MSE_oob = metrics.mean_squared_error(oob_y,oob_y_hat)\n    plt.scatter(oob_y,oob_y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_model_OOB_MSE(model,y,ymin,ymax,ylabel,title):      # OOB MSE and cross plot over multiple bagged trees, checks for unestimated \n    oob_y_hat = model.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; \n    MSE_oob = metrics.mean_squared_error(oob_y,oob_y_hat)\n    plt.scatter(oob_y,oob_y_hat,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(ymin,ymax); plt.ylim(ymin,ymax)\n    plt.plot([ymin,ymax],[ymin,ymax],color='black'); add_grid()\n    plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500])\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n\ndef check_grid(grid,xmin,xmax,ymin,ymax,xfeature,yfeature,response,zmin,zmax,title): # plots the estimated vs. the actual \n    if grid.ndim != 2:\n        raise ValueError(\"Prediction array must be 2D\")\n    ny, nx = grid.shape\n    xstep = (xmax - xmin)/nx; ystep = (ymax-ymin)/ny \n    predict_train = feature_sample(grid, xmin, ymin, xstep, ystep, xfeature, yfeature, 'sample')\n    MSE = metrics.mean_squared_error(response,predict_train)\n    plt.scatter(response,predict_train,s=None, c='darkorange',marker=None,cmap=cmap,norm=None,vmin=None,vmax=None,alpha=0.8, \n                linewidths=1.0, edgecolors=\"black\")\n    plt.title(title); plt.xlabel('Truth: ' + str(ylabel)); plt.ylabel('Estimated: ' + str(ylabel))\n    plt.xlim(zmin,zmax); plt.ylim(zmin,zmax)\n    plt.plot([zmin,zmax],[zmin,zmax],color='black'); add_grid()\n    plt.gca().xaxis.set_major_formatter(FuncFormatter(comma_format))\n    plt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format))\n    # plt.annotate('Out of Bag MSE: ' + str(f'{(np.round(MSE_oob,2)):,.0f}'),[4500,2500]) # not technically OOB MSE\n\ndef feature_sample(array, xmin, ymin, xstep, ystep, df_x, df_y, name): # sampling predictions from a feature space grid \n    if array.ndim != 2:\n        raise ValueError(\"Array must be 2D\")\n    if len(df_x) != len(df_y):\n        raise ValueError(\"x and y feature arrays must have equal lengths\")   \n    ny, nx = array.shape\n    df = pd.DataFrame()\n    v = []\n    nsamp = len(df_x)\n    for isamp in range(nsamp):\n        x = df_x.iloc[isamp]\n        y = df_y.iloc[isamp]\n        iy = min(ny - int((y - ymin) / ystep) - 1, ny - 1)\n        ix = min(int((x - xmin) / xstep), nx - 1)\n        v.append(array[iy, ix])\n    df[name] = v\n    return df    \n\ndef display_sidebyside(*args):                                # display DataFrames side-by-side (ChatGPT 4.0 generated Spet, 2024)\n    html_str = ''\n    for df in args:\n        html_str += df.head().to_html()  # Using .head() for the first few rows\n    display(HTML(f'<div style=\"display: flex;\">{html_str}</div>')) \n```", "```py\n#os.chdir(\"c:/PGE383\")                                        # set the working directory \n```", "```py\nimport pandas as pd \n```", "```py\npd.read_csv() \n```", "```py\ndf = pd.read_csv(\"unconv_MV.csv\") \n```", "```py\ndf = df.sample(frac=.30, random_state = 73073); \ndf = df.reset_index() \n```", "```py\nadd_error = True                                              # add random error to the response feature\nstd_error = 500                                               # standard deviation of random error, for demonstration only\nidata = 2\n\nif idata == 1:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the data from my github repo\n    df_load = df_load.sample(frac=.30, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n\nelif idata == 2:\n    df_load = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v5.csv\") # load the data \n    df_load = df_load.sample(frac=.70, random_state = seed); df_load = df_load.reset_index() # extract 30% random to reduce the number of data\n    df_load = df_load.rename(columns={\"Prod\": \"Production\"})\n\nyname = 'Production'; Xname = ['Por','Brittle']               # specify the predictor features (x2) and response feature (x1)\nXmin = [5.0,0.0]; Xmax = [25.0,100.0]                         # set minimums and maximums for visualization \nymin = 1500.0; ymax = 7000.0\nXlabel = ['Porosity','Brittleness']; ylabel = 'Production'    # specify the feature labels for plotting\nXunit = ['%','%']; yunit = 'MCFPD'\nXlabelunit = [Xlabel[0] + ' (' + Xunit[0] + ')',Xlabel[1] + ' (' + Xunit[1] + ')']\nylabelunit = ylabel + ' (' + yunit + ')'\n\nif add_error == True:                                         # method to add error\n    np.random.seed(seed=seed)                                 # set random number seed\n    df_load[yname] = df_load[yname] + np.random.normal(loc = 0.0,scale=std_error,size=len(df_load)) # add noise\n    values = df_load._get_numeric_data(); values[values < 0] = 0   # set negative to 0 in a shallow copy ndarray\n\ny = pd.DataFrame(df_load[yname])                              # extract selected features as X and y DataFrames\nX = df_load[Xname]\ndf = pd.concat([X,y],axis=1)                                  # make one DataFrame with both X and y (remove all other features) \n```", "```py\ncorr_matrix = df.corr()\ncorrelation = corr_matrix.iloc[:,-1].values[:-1]\n\nplt.subplot(121)\nplot_corr(corr_matrix,'Correlation Matrix',1.0,0.1)           # using our correlation matrix visualization function\nplt.xlabel('Features'); plt.ylabel('Features')\n\nplt.subplot(122)\nfeature_rank_plot(Xname,correlation,-1.0,1.0,0.0,'Feature Ranking, Correlation with ' + yname,'Correlation',0.5)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npairgrid = sns.PairGrid(df,vars=Xname+[yname])                # matrix scatter plots\npairgrid = pairgrid.map_upper(plt.scatter, color = 'darkorange', edgecolor = 'black', alpha = 0.8, s = 10)\npairgrid = pairgrid.map_diag(plt.hist, bins = 20, color = 'darkorange',alpha = 0.8, edgecolor = 'k')# Map a density plot to the lower triangle\npairgrid = pairgrid.map_lower(sns.kdeplot, cmap = plt.cm.inferno, \n                              alpha = 1.0, n_levels = 10)\npairgrid.add_legend()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.9, top=0.9, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\ndf.head(n=5)                                                  # check the loaded DataFrame \n```", "```py\ndf.describe(percentiles=[0.1,0.9])                            # check DataFrame summary statistics \n```", "```py\nnbins = 20                                                    # number of histogram bins\n\nplt.subplot(221)                                              # predictor feature #1 histogram\nfreq,_,_ = plt.hist(x=df[Xname[0]],weights=None,bins=np.linspace(Xmin[0],Xmax[0],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(Xlabelunit[0]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Density'); add_grid()  \nplt.xlim([Xmin[0],Xmax[0]]); plt.legend(loc='upper right')   \n\nplt.subplot(222)                                              # predictor feature #2 histogram\nfreq,_,_ = plt.hist(x=df[Xname[1]],weights=None,bins=np.linspace(Xmin[1],Xmax[1],nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(Xlabelunit[1]); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([Xmin[1],Xmax[1]]); plt.legend(loc='upper right')   \n\nplt.subplot(223)                                              # predictor features #1 and #2 scatter plot\nplt.scatter(df[Xname[0]],df[Xname[1]],s=40,marker='o',color = 'darkorange',alpha = 0.8,edgecolor = 'black',zorder=10,label='Train')\nplt.title(Xlabel[0] + ' vs ' +  Xlabel[1])\nplt.xlabel(Xlabelunit[0]); plt.ylabel(Xlabelunit[1])\nplt.legend(); add_grid(); plt.xlim([Xmin[0],Xmax[0]]); plt.ylim([Xmin[1],Xmax[1]])\n\nplt.subplot(224)                                              # predictor feature #2 histogram\nfreq,_,_ = plt.hist(x=df[yname],weights=None,bins=np.linspace(ymin,ymax,nbins),alpha = 0.6,\n                     edgecolor='black',color='darkorange',density=False,label='Train')\nmax_freq = max(freq)*1.10\nplt.xlabel(ylabelunit); plt.ylabel('Frequency'); plt.ylim([0.0,max_freq]); plt.title('Porosity'); add_grid()  \nplt.xlim([ymin,ymax]); plt.legend(loc='upper right') \n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=1.6, wspace=0.3, hspace=0.25)\n#plt.savefig('Test.pdf', dpi=600, bbox_inches = 'tight',format='pdf') \nplt.show() \n```", "```py\nplt.subplot(111)                                              # visualize the train and test data in predictor feature space\nim = plt.scatter(X[Xname[0]],X[Xname[1]],s=None, c=y[yname], marker='o', cmap=cmap, \n    norm=None, vmin=ymin, vmax=ymax, alpha=0.8, linewidths=0.3, edgecolors=\"black\", label = 'Train')\nplt.title('Training ' + ylabel + ' vs. ' + Xlabel[1] + ' and ' + Xlabel[0]); \nplt.xlabel(Xlabel[0] + ' (' + Xunit[0] + ')'); plt.ylabel(Xlabel[1] + ' (' + Xunit[1] + ')')\nplt.xlim(Xmin[0],Xmax[0]); plt.ylim(Xmin[1],Xmax[1]); plt.legend(loc = 'upper right'); add_grid()\ncbar = plt.colorbar(im, orientation = 'vertical')\ncbar.set_label(ylabel + ' (' + yunit + ')', rotation=270, labelpad=20)\ncbar.ax.yaxis.set_major_formatter(FuncFormatter(comma_format))\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nseed = 73073\nmax_depth = 100\nmin_samples_leaf = 2 \n```", "```py\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) \n```", "```py\nnum_trees = 100\nseed = 73073 \n```", "```py\nbagging_model = BaggingRegressor(base_estimator=regressor, n_estimators=num_trees, random_state=seed) \n```", "```py\nbagging_model.fit(X = predictors, y = response) \n```", "```py\nmax_depth = 100; min_samples_leaf = 2                         # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nnum_tree = 1                                                  # use only a single tree for this demonstration\nseeds = [73073, 73074, 73075, 73076, 73077, 73078]\nbagging_models = []; oob_MSE = []; score = []; pred = []\n\nindex = 1\nfor seed in seeds:                                            # visualize models over random number seeds\n    bagging_models.append(BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,\n                                           bootstrap=True,n_jobs = 4))\n    bagging_models[index-1].fit(X = X, y = y)\n    oob_MSE.append(bagging_models[index-1].oob_score_)\n    plt.subplot(2,3,index)\n    bag_X1 = X.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),0]\n    bag_X2 = X.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),1]\n    bag_y = y.iloc[np.asarray(bagging_models[index-1].estimators_samples_[0]),0]\n    pred.append(visualize_model(bagging_models[index-1],bag_X1,Xmin[0],Xmax[0],bag_X2,Xmin[1],Xmax[1],bag_y,ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Bootstrap Data and Decision Tree #' + str(index) + ' '))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.4, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor index in range(0,len(seeds)):                             # check models over random number seeds\n    plt.subplot(2,3,index+1)\n    check_model(bagging_models[index],y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions Decision Tree #' +  str(index+1))\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.6, top=2.0, wspace=0.3, hspace=0.3) \n```", "```py\nZ = pred[0] \nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    if index == 1:\n        Z = pred[index-1]\n    else:\n        Z = Z + pred[index-1]                                 # calculate the average response over 3 trees\n    index = index + 1\n\nZ = Z / len(seeds)                                            # grid pixel-wise average of the 6 bootstrapped decision trees\n\nplt.subplot(121)                                              # plot predictions over predictor feature space\nvisualize_grid(Z,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,ylabelunit,\n               Xlabelunit[0],Xlabelunit[1],'All Data and Average of 6 Bootstrapped Trees')\n\nplt.subplot(122)                                              # check model predictions vs. testing dataset\ncheck_grid(Z,Xmin[0],Xmax[0],Xmin[1],Xmax[1],df[Xname[0]],df[Xname[1]],df[yname],ymin,ymax,'Model Check - Average of 6 Bootstrapped Trees')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.3, hspace=0.2) \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nseed = 73073;\nnum_trees = [1,3,5,10,30,500]                                 # number of trees averaged for each estimator\n\nbagging_models_ntrees = []; oob_MSE = []; pred = []\n\nindex = 1\nfor num_tree in num_trees:                                    # visualize the models over number of trees\n    bagging_models_ntrees.append(BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,n_jobs = 4))\n    bagging_models_ntrees[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)\n    pred.append(visualize_model(bagging_models_ntrees[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Bagging with ' + str(num_tree) + ' Trees'))\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.4, wspace=0.4, hspace=0.3) \n```", "```py\nindex = 1\nfor num_tree in num_trees:                                    # check models over number of trees\n    plt.subplot(2,3,index)\n    check_model_OOB_MSE(bagging_models_ntrees[index-1],y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions with ' + \n                        str(num_trees[index-1]) + ' Decision Trees')\n    index = index + 1\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.6, top=2.0, wspace=0.3, hspace=0.3) \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\nntree_list = []; MSE_oob_list = []\nfor num_tree in np.arange(1,350,50):                          # check OOB MSE over number of trees\n    bagg_tree = BaggingRegressor(base_estimator=regressor,n_estimators=num_tree,random_state=seed,oob_score = True,n_jobs = 4).fit(X = X, y = y)\n    oob_y_hat = bagg_tree.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Number of Bagged Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Bagged Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 3; min_samples_leaf = 5                           # set for a complicated tree\n\nregressor = DecisionTreeRegressor(max_depth = max_depth, min_samples_leaf = min_samples_leaf) # instantiate a decision tree\n\nseeds = [73083, 73084, 73085]                                 # number of random number seeds\nnum_trees = [1,5,30,100]                                      # number of trees averaged for each estimator\nbagging_models_ntrees_seeds = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                    # loop over number of trees\n    for seed in seeds:                                        # loop over number of random number seeds\n        bagg_tree = BaggingRegressor(base_estimator=regressor, n_estimators=num_tree, \n                                     random_state=seed, oob_score = True, n_jobs = 4).fit(X = X, y = y)\n        oob_y_hat = bagg_tree.oob_prediction_\n        oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n        bagging_models_ntrees_seeds.append(bagg_tree)\n        MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n        plt.subplot(4,3,index)\n        visualize_model(bagging_models_ntrees_seeds[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Tree Model - ' + str(num_tree) + ' Tree(s)')\n        index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=4.0, wspace=0.35, hspace=0.3); plt.show() \n```", "```py\nmax_features = 'sqrt' \n```", "```py\nseed = 73073 \n```", "```py\nmax_depth = 5 \n```", "```py\nnum_tree = 300 \n```", "```py\nmax_features = 1 \n```", "```py\nmy_first_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features) \n```", "```py\nmy_first_forest.fit(X = predictors, y = response) \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\nmax_depth = 7\nnum_tree = 300\nmax_features = 1\n\nmy_first_forest = RandomForestRegressor(max_depth=max_depth,random_state=seed,n_estimators=num_tree,max_features=max_features,\n                                       oob_score=True,bootstrap=True)\n\nmy_first_forest.fit(X = X, y = y)                             # train the model with training data \n\nplt.subplot(121)                                              # predict with the model over the predictor feature space and visualize\nvisualize_model(my_first_forest,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(my_first_forest,y,ymin,ymax,ylabelunit,'Out-of-Bag Predictions with Random Forest')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 1                                                 # set the random forest hyperparameters\nnum_tree = 1\nmax_features = 1\nsimple_forest = []\n\nseeds = [73103,73104,73105,73106,73107,73108]                 # set the random number seeds\n\nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n    simple_forest[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)                                    # predict with the model over the predictor feature space and visualize\n    visualize_model(simple_forest[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.2, hspace=0.3) \n```", "```py\nmax_depth = 1                                                 # set the random forest hyperparameters\nnum_tree = 1\nmax_features = 2\nsimple_forest = []\n\nseeds = [73103,73104,73105,73106,73107,73108]                 # random number seeds \n\nindex = 1\nfor seed in seeds:                                            # loop over random number seeds\n    simple_forest.append(RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features))\n    simple_forest[index-1].fit(X = X, y = y)\n    plt.subplot(2,3,index)                                    # predict with the model over the predictor feature space and visualize\n    visualize_model(simple_forest[index-1],df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n    index = index + 1\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.0, wspace=0.4, hspace=0.3) \n```", "```py\nseed = 73093                                                  # set the random forest hyperparameters\nmax_depth = 7\nnum_tree = 500\nmax_features = 1\n\nbig_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=num_tree, max_features=max_features,\n                                   oob_score = True,bootstrap=True,n_jobs = 4)\n\nbig_forest.fit(X = X, y = y)\n\nplt.subplot(121)                                              # predict with the model over the predictor feature space and visualize\nvisualize_model(big_forest,df[Xname[0]],Xmin[0],Xmax[0],df[Xname[1]],Xmin[1],Xmax[1],df[yname],ymin,ymax,\n                ylabelunit,Xlabelunit[0],Xlabelunit[1],'Training Data and Random Forest Model')\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(big_forest,y,ymin,ymax,ylabelunit,'Model Check Random Forest Model')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.4, hspace=0.2) \n```", "```py\nimportances = big_forest.feature_importances_ \n```", "```py\nimportances = big_forest.feature_importances_                 # expected (global) importance over the forest fore each predictor feature\nstd = np.std([tree.feature_importances_ for tree in big_forest.estimators_],axis=0) # retrieve importance by tree\nindices = np.argsort(importances)[::-1]                       # sort in descending feature importance\nfeatures = ['Porosity','Brittleness']                         # names or predictor features\n\nplt.subplot(121)\nplt.title(\"Random Forest Feature Importances\")\nplt.bar(features, importances[indices],color=\"darkorange\", alpha = 0.8, edgecolor = 'black', yerr=std[indices], align=\"center\")\nplt.ylim(0,1), plt.xlabel('Predictor Features'); plt.ylabel('Feature Importance'); add_grid()\n\nplt.subplot(122)                                              # perform cross validation with withheld testing data\ncheck_model_OOB_MSE(big_forest,y,ymin,ymax,ylabelunit,'Model Check Random Forest Model for Feature Importance')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.6, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depth = 5                                                 # set the random forest hyperparameters\nnum_trees = np.arange(1,100,2)\nmax_features = 1\ntrained_forests = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                     # loop over number of trees in our random forest\n    trained_forest = RandomForestRegressor(max_depth=max_depth, random_state=seed,n_estimators=int(num_tree),\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n    index = index + 1\n\nplt.subplot(121)\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Number of Random Forest Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Random Forest Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(ntree_list),max(ntree_list)])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nmax_depths = np.linspace(1,20,20)                             # set the tree maximum tree depths to consider\n\nnum_tree = 60                                                 # set the random forest hyperparameters\nmax_features = 1\ntrained_forests = []\nMSE_oob_list = []; max_depth_list = []\n\nindex = 1\nfor max_depth in max_depths:                                  # loop over tree depths \n    trained_forest = RandomForestRegressor(max_depth=int(max_depth), random_state=seed,n_estimators=num_tree,\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); max_depth_list.append(max_depth)\n    index = index + 1\n\nplt.subplot(121)                                                # plot OOB MSE vs. maximum tree depth\nplt.scatter(max_depth_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(max_depth_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.xlabel('Tree Maximum Depth'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Tree Maximum Depth')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(max_depth_list),max(max_depth_list)])\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=0.8, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nx1 = 0.25; x2 = 0.3                                           # predictor values for the prediction\n\npipe_forest = Pipeline([                                      # the machine learning workflow as a pipeline object\n    ('forest', RandomForestRegressor())\n])\n\nparams = {                                                    # the machine learning workflow method's parameters to search\n    'forest__max_leaf_nodes': np.arange(2,100,2,dtype = int),\n}\n\ntuned_forest = GridSearchCV(pipe_forest,params,scoring = 'neg_mean_squared_error', # hyperparameter tuning w. grid search k-fold cross validation \n                             refit = True)\ntuned_forest.fit(X,y)                                         # fit model with tuned hyperparameters to all the data\n\nprint('Tuned hyperparameter: max_leaf_nodes = ' + str(tuned_forest.best_params_))\n\nestimate = tuned_forest.predict([[x1,x2]])[0]                 # make a prediction (no tuning shown)\nprint('Estimated ' + ylabel + ' for ' + Xlabel[0] + ' = ' + str(x1) + ' and ' + Xlabel[1] + ' = ' + str(x2)  + ' is ' + \n      str(round(estimate,1)) + ' ' + yunit) # print results \n```", "```py\nTuned hyperparameter: max_leaf_nodes = {'forest__max_leaf_nodes': 64}\nEstimated Production for Porosity = 0.25 and Brittleness = 0.3 is 2001.2 MCFPD \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nmax_leaf_nodes = 30                                           # set the random forest hyperparameters\nnum_trees = np.arange(1,100,2)\nmax_features = max(1,int(len(xname)/3.0))                     # use the m/3 for regression rule\ntrained_forests = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                    # loop over number of trees in our random forest\n    trained_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=int(num_tree),\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n    index = index + 1\n\ntune_num_trees = ntree_list[np.argmin(MSE_oob_list)]          # get the number of trees that minimizes the OOB error\n\nplt.subplot(121)\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.plot([tune_num_trees,tune_num_trees],[min(MSE_oob_list),max(MSE_oob_list)],color='red',ls='--')\nplt.xlabel('Number of Random Forest Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Random Forest Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(ntree_list),max(ntree_list)])\n\ntuned_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=tune_num_trees,\n            oob_score = False,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n\ny_hat = tuned_forest.predict(X)                               # predict over all samples\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Random Forest, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well'],axis=1,inplace=True)                 # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n    ymin_new = 0.0; ymax_new = 10000.0\n    xlabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ylabel_new = 'Production (MCFPD)'\n\n    xtitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0','Facies'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n    df_new['Por'] = df_new['Por'] * 100.0; df_new['AI'] = df_new['AI'] / 1000.0\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [4.0,0.0]; xmax_new = [19.0,500.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 1.60; ymax_new = 6.20\n\n    xlabel_new = ['Porosity (fraction)','Permeability (mD)'] # set the names for plotting\n\n    ylabel_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    xtitle_new = ['Porosity','Permeability']\n\n    ytitle_new = 'Acoustic Impedance (kg/m2s*10^6)'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xname = features[:-1]\n    yname = [features[-1]]\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    ymin_new = 0.0; ymax_new = 1600.0\n\n    xlabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ylabel_new = 'Production (Mbbl)'\n\n    xtitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\n    ytitle_new = 'Production'\n\n    y = pd.DataFrame(df_new[yname])                              # extract selected features as X and y DataFrames \n    X = df_new[xname]\n\ndf_new.head(n=13) \n```", "```py\nmax_leaf_nodes = 30                                           # set the random forest hyperparameters\nnum_trees = np.arange(1,100,2)\nmax_features = max(1,int(len(xname)/3.0))                     # use the m/3 for regression rule\ntrained_forests = []\nMSE_oob_list = []; ntree_list = []\n\nindex = 1\nfor num_tree in num_trees:                                    # loop over number of trees in our random forest\n    trained_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=int(num_tree),\n            oob_score = True,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n    trained_forests.append(trained_forest)\n    oob_y_hat = trained_forest.oob_prediction_\n    oob_y = y[oob_y_hat > 0.0]; oob_y_hat = oob_y_hat[oob_y_hat > 0.0]; # remove if not estimated\n    MSE_oob_list.append(metrics.mean_squared_error(oob_y,oob_y_hat)); ntree_list.append(num_tree)\n    index = index + 1\n\ntune_num_trees = ntree_list[np.argmin(MSE_oob_list)]          # get the number of trees that minimizes the OOB error\n\nplt.subplot(121)\nplt.scatter(ntree_list,MSE_oob_list,color='darkorange',edgecolor='black',alpha=0.8,s=30,zorder=10)\nplt.plot(ntree_list,MSE_oob_list,color='black',ls='--',zorder=1)\nplt.plot([tune_num_trees,tune_num_trees],[min(MSE_oob_list),max(MSE_oob_list)],color='red',ls='--')\nplt.xlabel('Number of Random Forest Trees'); plt.ylabel('Mean Square Error'); plt.title('Out-of-Bag Mean Square Error vs Number of Random Forest Trees')\nplt.gca().yaxis.set_major_formatter(FuncFormatter(comma_format)); add_grid(); plt.xlim([min(ntree_list),max(ntree_list)])\n\ntuned_forest = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=seed,n_estimators=tune_num_trees,\n            oob_score = False,bootstrap=True,max_features=max_features).fit(X = X, y = y)\n\ny_hat = tuned_forest.predict(X)                               # predict over all samples\n\nplt.subplot(122)\nplt.scatter(y,y_hat,color='green',edgecolor='black') # cross validation plot\nplt.plot([ymin_new,ymax_new],[ymin_new,ymax_new],color='black',zorder=-1)\nplt.xlim(ymin_new,ymax_new); plt.ylim(ymin_new,ymax_new); add_grid() \nplt.xlabel('Truth: ' + ylabel_new); plt.ylabel('Estimate: ' + ylabel_new)\nplt.title('Tuned Random Forest, Cross Validation')\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```"]