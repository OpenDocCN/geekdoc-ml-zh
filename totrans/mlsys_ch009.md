# 深度学习入门

*DALL·E 3 提示：一个在干净白色背景上分为两半的矩形插图。左侧详细且多彩地描绘了一个生物神经网络，展示了相互连接的神经元、发光的突触和树突。右侧展示了一个时尚现代的人工神经网络，由相互连接的节点和边组成的网格表示，类似于数字电路。两边的过渡既明显又和谐，每半部分清楚地说明了其各自的主题：左侧是生物的，右侧是人工的.*

![图片](img/file32.png)

## 目的

*为什么深度学习系统工程师需要对神经网络操作有深入数学理解，而不是将它们视为黑盒组件？*

现代深度学习系统依赖于神经网络作为其核心计算引擎，但成功的工程需要理解控制其行为的数学。神经网络数学决定了内存需求、计算复杂度和优化景观，这些直接影响系统设计决策。如果不能掌握梯度流、激活函数和反向传播机制等概念，工程师无法预测系统行为、诊断训练失败或优化资源分配。每个数学运算都对应特定的硬件要求：矩阵乘法需要每秒数 GB 的内存带宽，而激活函数的选择决定了移动处理器的兼容性。理解这些操作将神经网络从不可见组件转变为可预测、可工程化的系统。

**学习目标**

+   跟踪人工智能从基于规则的系统到神经网络的演变，并确定驱动工程挑战

+   分析神经网络操作（矩阵乘法、激活、梯度）及其硬件影响

+   通过选择适当的层配置、激活函数和连接模式，根据计算约束和任务需求设计神经网络架构

+   通过多层网络实现前向传播，计算加权总和并应用激活函数将原始输入转换为层次特征表示

+   执行反向传播算法来计算梯度并更新网络权重，展示预测误差如何通过网络层反向传播

+   比较训练和推理操作阶段，分析它们不同的计算需求、资源需求和针对不同部署场景的优化策略

+   评估损失函数和优化算法，解释这些选择如何影响训练动态、收敛行为和最终模型性能

+   评估深度学习管道以识别计算瓶颈和优化机会

## 深度学习系统工程基础

考虑到看似简单的任务，比如在照片中识别猫。使用传统的编程方法，你需要编写显式规则：寻找三角形的耳朵，检查是否有胡须，验证是否有四条腿，检查毛皮图案，并处理无数的光照、角度、姿势和品种的变化。每个边缘情况都需要额外的规则，从而创建越来越复杂的决策树，但仍然会在遇到意外变化时失败。这种限制，即无法手动编码复杂现实世界问题的所有模式，推动了从基于规则的编程到机器学习的演变。

深度学习代表了这一演变的顶峰，通过直接从数百万只猫和非猫图像中学习来解决猫识别问题。我们不是编写规则，而是提供示例，让系统自动发现模式。这种从显式编程到学习表示的转变对我们设计和工程计算系统的方式产生了影响。

深度学习系统提出了一个工程挑战，这使得它们与传统软件区别开来。虽然传统系统执行基于显式规则的确定性算法，但深度学习系统通过数学过程学习数据表示来运行。这种转变要求工程师理解这些系统背后的数学操作，以便于它们的设计、实施和维护。

这种数学复杂性的工程影响非常重要。当生产系统表现出降低的性能特征时，传统的调试方法证明是不够的。性能异常可能源于优化过程中的梯度不稳定性 1，激活计算中的数值精度限制，或者张量操作固有的内存访问模式 2。没有基础数学素养，系统工程师无法有效地区分实现失败和算法约束，准确预测计算资源需求，或系统地优化由底层数学操作产生的性能瓶颈。

**深度学习**是机器学习的一个子领域，它使用具有多层**神经网络**来自动从数据中学习层次化表示，从而消除了**显式特征工程**的需求。

深度学习通过解决限制早期方法的局限性，已成为现代人工智能的主导方法。虽然基于规则的系统需要详尽的手动指定决策路径，而传统的机器学习技术需要特征工程专业知识，但神经网络架构可以直接从原始数据中发现模式表示。这种能力使得以前被认为难以处理的应用成为可能，尽管它引入了需要重新考虑系统架构设计原则的计算复杂性。如图 3.1 图所示，神经网络在机器学习和人工智能更广泛层次结构中构成了一个基础组成部分。

![图片](img/file33.png)

图 3.1：**AI 层次结构**：神经网络通过在大数据集中建模模式，成为机器学习和人工智能领域深度学习的一个核心组成部分。机器学习算法使系统能够作为更广泛人工智能领域的一部分从数据中学习。

向神经网络架构的转变代表了一种超越算法演变的转变，需要重新概念化系统设计方法。神经网络通过大量并行矩阵运算执行计算，这些运算与专用硬件架构配合良好。这些系统通过迭代优化过程学习，这些过程产生独特的内存访问模式并施加严格的数值精度要求。推理的计算特征与训练阶段大不相同，需要为每种操作模式制定不同的优化策略。

本章建立了有效工程神经网络系统所需的数学素养。我们不是将这些架构视为不透明的抽象，而是检查决定系统行为和性能的数学运算。我们研究生物神经网络过程如何启发人工神经元模型，分析单个神经元如何组成复杂的网络拓扑，以及这些网络如何通过数学优化获取知识。每个概念都直接与实际系统工程考虑相关：理解矩阵乘法运算可以阐明内存带宽需求，理解梯度计算机制可以解释数值精度约束，而识别优化动态可以指导资源分配决策。

我们首先考察人工智能方法是如何从基于规则的编程发展到自适应学习系统的。然后，我们研究启发人工神经元模型的生物神经网络过程，建立控制神经网络操作的数学框架，并分析使这些系统能够从复杂数据集中提取模式的优化过程。在整个探索过程中，我们关注每个数学原理的系统工程意义，构建设计、实施和优化生产规模深度学习系统所需的理论基础。

在完成本章学习后，学生将不再将神经网络视为晦涩的算法结构，而是将其视为可工程化的计算系统，其数学运算为其实际实施和操作部署提供直接指导。

## 机器学习范式的演变

为了理解为什么深度学习成为需要专门计算基础设施的主导方法，我们考察了人工智能方法随时间的发展。当前的 AI 时代代表了从基于规则的编程通过经典机器学习到现代神经网络的最新进化阶段。理解这一进展揭示了每种方法是如何建立在前辈的基础上并解决其局限性的。

### 传统基于规则的编程局限性

传统编程要求开发者明确定义规则，告诉计算机如何处理输入并产生输出。考虑一个简单的游戏，如 Breakout[3]，如图 3.2[2]所示。程序需要对每个交互进行明确的规则定义：当球击中砖块时，代码必须指定砖块应该被移除，球的运动方向应该反转。虽然这种方法对于具有明确物理和有限状态的游戏是有效的，但它展示了基于规则系统的局限性。

![图片](img/file34.svg)

图 3.2：**基于规则的系统**：传统编程依赖于明确定义的规则来将输入映射到输出，这限制了其在复杂或不确定环境中的适应性，因为必须预测并编码所有可能的场景。这种方法与深度学习形成对比，在深度学习中，系统从数据中学习模式，而不是依赖于预编程的逻辑。

不仅仅局限于单个应用，这种基于规则的范式扩展到了所有传统编程，如图 3.3[1]所示。程序需要同时处理规则和输入数据以产生输出。早期的人工智能研究探讨了这种方法是否可以通过编码足够的规则来捕捉智能行为，从而扩展到解决复杂问题。

![图片](img/file35.svg)

图 3.3：**基于规则的编程**：传统程序通过明确定义的规则操作数据，构成了早期人工智能系统的基础，但缺乏现代机器学习方法的适应性。这种方法与深度学习形成对比，在深度学习中，系统从示例中推断规则，而不是依赖于预编程的逻辑。

尽管它们表面上看起来很简单，但在复杂的现实世界任务中，基于规则的局限性变得明显。识别人类活动（图 3.4）说明了这一挑战：将低于 4 英里/小时的速度归类为行走似乎很简单，直到现实世界的复杂性出现。速度变化、活动之间的转换和边界情况都需要额外的规则，从而产生难以管理的决策树。计算机视觉任务加剧了这些困难：检测猫需要关于耳朵、胡须和身体形状的规则，同时还要考虑观察角度、光照、遮挡和自然变化。早期系统只在具有明确约束的受控环境中取得了成功。

![图片](img/file36.png)

图 3.4：**基于规则的编程**：传统程序依赖于明确定义的规则来操作数据，构成了早期人工智能系统的基础，但在复杂任务中缺乏适应性。

认识到这些局限性，20 世纪 70 年代和 80 年代人工智能研究中的知识工程方法试图系统地创建规则。专家系统 4 将领域知识编码为明确规则，在具有明确参数的特定领域显示出希望，但在人类自然执行的任务，如物体识别、语音理解或自然语言解释方面却遇到了困难。这些局限性凸显了一个挑战：许多智能行为的方面依赖于难以用基于规则的显式表示的隐式知识。

### 经典机器学习

面对基于规则的系统的可扩展性障碍，研究人员开始探索可以从数据中学习的方法。机器学习提供了一个有希望的方向：而不是为每种情况编写规则，研究人员可以编写识别示例中模式的程序。然而，这些方法的成功仍然在很大程度上依赖于人类洞察力来定义相关模式，这个过程被称为特征工程。

这种方法引入了特征工程：将原始数据转换为学习算法可以揭示模式的表示。方向梯度直方图（HOG）方法（Dalal 和 Triggs，未注明日期)5（图 3.5）是这种方法的例证，它识别亮度急剧变化的边缘，将图像划分为单元格，并测量每个单元格内的边缘方向。这把原始像素转换成对光照变化和微小位置变化具有鲁棒性的形状描述符。

![图片](img/file37.png)

图 3.5：**HOG 方法**：通过识别图像中的边缘来创建梯度直方图，将像素值转换为形状描述符，这些描述符对光照变化不变。

补充方法如 SIFT (Lowe 1999)6（尺度不变特征变换）和 Gabor 滤波器 7 捕捉了不同的视觉模式——SIFT 检测出在不同尺度和方向变化中保持稳定的特征点，而 Gabor 滤波器识别纹理和频率。每个都编码了关于视觉模式识别的领域专业知识。

这些工程努力在 2000 年代推动了计算机视觉的进步。系统现在能够以一定的鲁棒性识别对象，以应对现实世界的各种变化，从而在人脸检测、行人检测和物体识别等领域得到应用。尽管取得了这些成功，但这种方法也有局限性。专家需要为每个新问题仔细设计特征提取器，而结果的特征可能会错过在设计时未预料到的重要模式。

### 深度学习：自动模式发现

神经网络代表了我们在计算机上解决问题的方法的一次转变，确立了一种新的编程方法，它从数据中学习而不是遵循明确的规则。这种转变在考虑诸如计算机视觉等任务时尤其明显，特别是识别图像中的对象。

深度学习通过直接从原始数据中学习而有所不同。如我们之前在图 3.3 中看到的，传统的编程需要规则和数据作为输入来产生答案。机器学习颠倒了这种关系，如图 3.6 所示。我们不是编写规则，而是提供示例（数据）及其正确答案，以自动发现潜在的规则。这种转变消除了人类指定哪些模式重要的需求。

![图片](img/file38.svg)

图 3.6：**数据驱动规则发现**：深度学习模型直接从数据中学习模式和关系，消除了手动指定规则的需求，并能够从原始输入中自动提取特征。这与传统编程不同，在传统编程中，规则和数据都是生成输出的必要条件，以及与经典机器学习不同，在经典机器学习中，规则是从标记数据中推断出来的。

通过这个自动化过程，系统从示例中发现了这些模式。当展示数百万张猫的图片时，系统学会识别越来越复杂的视觉模式，从简单的边缘到更复杂的组合，这些组合构成了猫的特征。这与人眼视觉系统的运作方式相平行，从基本的视觉元素到复杂物体构建理解。

基于这种分层学习原理，深度网络学习分层表示，其中复杂模式从简单模式中产生。每一层学习越来越抽象的特征：边缘 → 形状 → 对象 → 概念。更深的网络可以用多项式级更多的参数表达指数级更多的函数，这就是为什么“深度”在理论上很重要的原因。组合性原则解释了为什么深度学习有效：复杂的现实世界模式通常具有与网络表示偏差相匹配的分层结构。

这种分层结构带来了一种优势：与传统方法中性能达到平台期不同，深度学习模型随着额外数据的增加（识别更多变化）和计算的进行（发现更细微的模式）而持续改进。这种可扩展性推动了性能的显著提升。图像识别的准确性从 2012 年的 74%提高到了今天的 95%以上 8。

神经网络的性能遵循可预测的缩放关系，这些关系直接影响系统设计。这些缩放定律解释了为什么现代人工智能系统优先考虑更大的模型而不是更长的训练时间：GPT-4 的参数比 GPT-1 多约 1000 倍，但训练时间却相似。因此，内存带宽和存储容量成为主要的限制因素，而不是原始的计算能力。这些缩放定律的详细数学公式及其定量分析在第八章中有所介绍，而第十章探讨了它们的实际应用。

除了性能提升之外，这种方法对人工智能系统的构建也有影响。深度学习直接从原始数据中学习的能力消除了手动特征工程的需求，同时也带来了新的要求。需要高级基础设施来处理大量数据集，强大的计算机来处理这些数据，以及专门的硬件来高效地执行复杂的数学计算。深度学习的计算需求推动了专用计算机芯片的发展，这些芯片针对这些计算进行了优化。

实证证据强烈支持这些说法。深度学习在计算机视觉中的成功证明了当给予足够的数据和计算时，这种方法如何超越传统方法。这种模式在许多领域重复出现，从语音识别到游戏，确立了深度学习作为人工智能变革性方法的地位。

然而，这种转变也伴随着权衡：深度学习的计算需求重塑了系统需求。了解这些需求为以下关于神经网络的技术细节提供了背景。

### 计算基础设施需求

从传统编程到深度学习的转变不仅代表了我们在解决问题方式上的转变，而且代表了计算系统需求的变化，这直接影响着机器学习系统设计的各个方面。当我们考虑机器学习系统的完整范围时，这种转变变得尤为重要，从大规模云部署到资源受限的 Tiny ML 设备。

传统程序遵循可预测的模式。它们执行顺序指令，以常规模式访问内存，并以理解良好的方式使用计算资源。一个典型的基于规则的图像处理系统可能会系统地扫描像素，应用固定操作，具有适度和可预测的计算和内存需求。这些特征使得传统程序在不同计算平台上的部署相对简单。

表 3.1：**系统资源演变**：编程范式从顺序计算转向具有特征工程的有序并行，最终转向深度学习中的大规模矩阵运算和复杂内存层次结构。此表阐明了深度学习如何从根本上改变系统需求，与传统编程和具有工程特征的机器学习相比，影响计算和内存访问模式。

| **系统方面** | **传统编程** | **具有特征的机器学习** | **深度学习** |
| --- | --- | --- | --- |
| **计算** | 顺序、可预测的路径 | 结构化并行操作 | 大规模矩阵并行 |
| **内存访问** | 小型、可预测的模式 | 中型、批量导向 | 大型、复杂分层模式 |
| **数据移动** | 简单的输入/输出流 | 结构化批量处理 | 交叉系统间的密集移动 |
| **硬件需求** | 以 CPU 为中心 | 带有向量单元的 CPU | 专用加速器 |
| **资源扩展** | 固定需求 | 与数据大小成线性关系 | 与复杂性成指数关系 |

随着我们向数据驱动方法迈进，具有工程特征的经典机器学习引入了新的复杂性。特征提取算法需要更密集的计算和结构化数据移动。例如，前面讨论过的 HOG 特征提取器需要多次遍历图像数据，计算梯度并构建直方图。虽然这增加了计算需求和内存复杂性，但资源需求仍然可预测且可扩展。

然而，深度学习在多个维度上重塑了系统需求，如表 3.1 所示。理解这些演变变化很重要，因为差异以多种方式显现，影响整个机器学习系统范围。

#### 并行矩阵运算模式

当比较这些方法时，计算范式转变立即变得明显。传统的程序遵循顺序逻辑流程。相比之下，深度学习需要在矩阵上进行大规模并行操作。这种转变解释了为什么专为顺序处理设计的传统 CPU 在神经网络计算中效率低下。

这种并行计算模型创造了新的瓶颈。基本挑战是内存墙：虽然可以通过添加更多处理单元来增加计算能力，但为这些单元提供内存带宽的增长并不那么有利 9。现代加速器通过具有多个缓存级别和专门内存架构的分层内存系统来解决这个问题，这些架构能够实现数据重用。关键洞察是保持数据靠近其处理位置——在更快、更小的缓存中，而不是在较慢、较大的主内存中——可以显著提高性能。

这些内存层次结构的挑战解释了为什么神经网络加速器专注于最大化数据重用。成功的架构不是反复从慢速的主内存中获取相同的权重，而是将频繁访问的数据保存在快速的本地存储中，并仔细安排操作以最小化数据移动。这些内存系统及其性能特性的详细定量分析在第十一章中有所介绍。

并行处理的需求推动了专用硬件架构的采用，从强大的云 GPU 到专用移动处理器再到 Tiny ML 加速器。第十一章 sec-ai-acceleration 中探讨了特定硬件架构及其在机器学习工作负载中的权衡。

#### 分层内存架构

内存需求带来了另一个转变。传统的程序通常保持较小的、固定的内存占用。相比之下，深度学习模型必须管理复杂内存层次结构中的参数。内存带宽往往成为主要的性能瓶颈，给资源受限的系统带来挑战。

这种内存密集型特性为神经网络计算创造了独特的性能瓶颈。矩阵乘法——神经网络的核心操作——通常是内存带宽限制的，而不是计算限制的 10。基本问题是处理器可以比从内存中获取数据更快地执行计算。每个权重都必须从内存中加载以执行乘法，如果内存系统不能快速提供数据，计算单元就会空闲等待值到来。这种计算能力和内存带宽之间的不平衡解释了为什么仅仅增加更多的处理单元并不能成比例地提高性能。

GPU 通过更高的内存带宽和巨大的并行性来应对这一挑战，实现了比传统 CPU 更好的利用率。然而，根本的约束仍然存在：神经网络中的能耗主要由数据移动而非计算主导。将数据从主内存移动到处理单元比实际的数学运算消耗更多的能量。这种能量层次结构解释了为什么专用处理器专注于减少数据移动的技术，使数据更接近处理的地方。

这种基本的内存-计算权衡在不同部署场景中表现出不同的特点。云服务器可以承担更多的内存和电力以最大化吞吐量，而移动设备必须仔细优化以在严格的电力预算内运行。训练系统即使在更高的能源成本下也优先考虑计算吞吐量，而推理系统则强调能源效率。这些不同的限制驱使着机器学习系统范围内的不同优化策略，从内存丰富的云部署到高度优化的 Tiny ML 实现。

第十章（第十章）详细介绍了内存优化策略，如量化剪枝，而第十一章（第十一章）探讨了硬件架构及其内存系统。

#### 分布式计算需求

研究人员发现，深度学习改变了系统的扩展方式以及效率的重要性。传统的程序具有相对固定的资源需求，具有可预测的性能特征。随着复杂性的增加，深度学习模型可以消耗指数级更多的资源。这种模型能力与资源消耗之间的关系使得系统效率成为一个关注点。第九章涵盖了优化这种关系的技巧，包括在保持模型性能的同时减少计算需求的方法。

将算法概念与硬件现实联系起来变得至关重要。虽然传统的程序可以相对直接地映射到标准计算机架构，但深度学习需要仔细考虑：

+   如何有效地将矩阵运算映射到物理硬件上（第十一章涵盖了针对特定硬件的优化策略）

+   最小化跨内存层次结构数据移动的方法

+   平衡计算能力与资源限制的方法（第九章探讨了扩展定律和效率权衡）

+   优化算法和系统级效率的技术（第十章提供了模型压缩技术）

这些转变解释了为什么深度学习激发了整个计算堆栈的创新。从专用硬件加速器到新的内存架构，再到复杂的软件框架，深度学习的需求持续重塑计算机系统设计。

在确立了从基于规则的系统到神经网络的历史演变以及这一演变所要求的计算基础设施之后，我们现在考察这些系统的根本灵感。神经网络计算答案的起点不是硅和软件，而是生物学——具体来说，是我们大脑中的神经网络，这些神经网络启发了现代人工智能系统所依赖的人工神经网络。

## 从生物学到硅

在考察了编程方法如何从规则发展到数据驱动学习，以及这种演变如何推动我们今天看到的计算基础设施需求之后，我们现在转向一个问题：这些神经网络实际上在计算什么？答案不是从硅开始，而是从生物学开始。

我们刚刚考察的巨大计算需求（专用处理器、分层内存系统、高带宽数据移动）都源于一个简单的灵感：生物神经元。了解自然界如何用 20 瓦的功率解决信息处理问题，揭示了人工神经网络系统的潜力和挑战。当我们考察生物神经元及其人工对应物时，注意寻找一个模式：我们选择实现或近似的每个生物特征都会产生特定的计算需求，将树突和突触模型直接与刚刚讨论的处理能力和内存带宽需求联系起来。

本节通过考察三个关键转换来连接生物灵感和系统实现：生物神经元如何启发人工神经元设计，神经网络原理如何转化为数学运算，以及这些运算如何推动我们之前概述的系统需求。到最后，你将理解为什么即使实现简化的神经网络计算也需要现代机器学习系统所要求的专用硬件基础设施。

### 生物神经网络处理原理

从系统角度来看，生物神经网络为我们刚刚讨论的计算挑战提供了解决方案：它们实现了大规模并行处理、高效的内存使用和自适应学习，同时消耗最少的能量。四个来自生物智能的关键原则直接影响了人工神经网络的设计：

**自适应学习**：大脑根据经验持续修改神经网络连接，通过与环境的互动来细化响应。这种生物能力启发了机器学习的核心原则：从数据中改进，而不是遵循固定的、预先编程的规则。

**并行处理**：大脑同时处理大量信息，不同区域专注于特定功能，同时协同工作。这种分布式、并行架构与传统顺序计算形成对比，并影响了现代人工智能系统设计。

**模式识别**：生物系统在识别复杂、嘈杂数据中的模式方面表现出色——在人群中识别面孔，在嘈杂环境中理解语音，从部分信息中识别物体。这种能力启发了计算机视觉和语音识别的应用，尽管人工系统仍在努力匹配大脑的效率。

**能源效率**：生物系统以非凡的能源效率进行处理。人脑的 20 瓦功率消耗 11 形成了一个显著的效率差距，人工系统仍在努力弥合这一差距。理解和复制这种效率在第十八章中通过环境影响分析和能源效率优化策略进行了探讨。

这些生物学原理为人工神经网络提出了关键要求：简单处理单元整合多个输入，可调节的连接强度，基于输入阈值的非线性激活，并行处理架构，以及通过连接强度修改进行学习。以下章节将探讨我们如何将这些生物学洞察转化为数学运算和硅基实现。

这些生物学原理塑造了人工智能中的两种方法。第一种试图直接模仿神经网络的结构和功能，创建了结构上类似于生物网络的的人工神经网络。第二种采取更抽象的方法，将生物学原理适应于在计算机硬件约束下高效工作，而不必精确复制生物结构。

要理解这两种方法在实际中的工作原理，我们首先必须考察使神经网络计算成为可能的基本单元：单个神经元。通过理解生物神经元如何处理信息，我们就可以看到这一过程如何转化为驱动人工神经网络的数学运算。

### 生物神经元结构

将这些高级原理转化为实际实现需要考察生物学信息处理的基本单元：神经元。这个细胞构建块为其人工对应物提供了蓝图，并揭示了复杂神经网络如何从简单的协同工作组件中产生。

在生物系统中，神经元（或细胞）代表神经系统的基本功能单元。理解其结构对于将人工系统与之类比至关重要。图 3.7 展示了生物神经元的结构。

![图片](img/file39.png)

图 3.7：**生物神经元映射**：人工神经元从其生物对应物中抽象出关键功能，在树突接收加权输入，在细胞体中求和，并通过轴突产生输出，类似于人工神经网络中的激活函数。这种抽象使得能够构建复杂的人工神经网络，能够进行复杂的信息处理。来源：geeksforgeeks。

生物神经元由几个关键组件组成。中心部分是细胞体，或称为细胞核，其中包含细胞核并执行细胞的基本生命过程。从细胞体延伸出分支状结构，称为树突，它们作为接收器接收来自其他神经元的信号。神经元之间的连接发生在突触 12，它们调节传输信号的强度。最后，一个细长的突起称为轴突，将电脉冲从细胞体传导到其他神经元。

将这些结构组件整合，神经元的功能如下：树突作为接收器，从其他神经元收集输入信号。这些连接处的突触调节每个信号的强度，决定每个输入的影响程度。细胞体将这些加权信号综合起来，并决定是否触发输出信号。如果触发，轴突将此信号传输到其他神经元。

生物神经元的每个元素在人工系统中都有一个计算上的类似物，反映了自然界中发现的原理，如学习、适应性和效率。为了更好地理解生物智能如何影响人工系统，表 3.2 捕捉了生物神经元和人工神经元组件之间的映射。这应该与图 3.7 一起查看，以获得完整的图景。它们共同展示了从生物到人工神经元的映射。

表 3.2：**神经元对应关系**：生物神经元通过类似组件启发人工神经元的设计——树突对应输入（接收信号），突触对应权重（调节连接强度），细胞体对应净输入，轴突对应输出——为智能计算建模奠定基础。此表阐明了生物神经元的哪些关键功能被抽象并在人工神经网络中实现，从而实现学习和信息处理。

| **生物神经元** | **人工神经元** |
| --- | --- |
| **细胞** | 神经元/节点 |
| **树突** | 输入 |
| **突触** | 权重 |
| **细胞体** | 净输入 |
| **轴突** | 输出 |

理解这些对应关系对于掌握人工系统如何近似生物智能至关重要。每个组件通过不同的机制执行类似的功能，对人工神经网络具有特定的含义。

1.  **细胞 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics> 神经元/节点**：人工神经元或节点作为基本的计算单元，反映了生物系统中细胞的作用。

1.  **树突 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics> 输入**：生物神经元的树突接收来自其他神经元的信号，这与输入如何进入人工神经元的方式类似。它们充当信号接收器，就像收集信息的天线一样。

1.  **突触 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics> 权重**：突触调节神经元之间连接的强度，这与人工神经元中的权重直接相似。这些权重是可以调整的，通过控制每个输入的影响程度，使得学习与优化可以在时间上得到实现。

1.  **胞体 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics> 净输入**：人工神经元中的净输入将加权输入相加以确定激活，这与生物神经元中胞体整合信号的方式相似。

1.  **轴突 <semantics><mo>↔</mo><annotation encoding="application/x-tex">\longleftrightarrow</annotation></semantics> 输出**：人工神经元的输出将处理过的信息传递给后续的网络层，这与轴突将信号传递给其他神经元的方式非常相似。

这种映射展示了人工神经网络如何简化并抽象生物过程，同时保留其基本的计算原理。理解单个神经元只是开始。神经网络的真实力量来自于这些基本单元如何在更大的系统中协同工作。

从系统工程的角度来看，这种从生物到人工的翻译揭示了为什么神经网络具有如此高的计算要求。每个简单的生物过程都映射到密集的数学运算，这些运算必须并行执行数百万或数十亿次。

### 人工神经网络设计原则

从生物灵感到实际实施的桥梁，将生物原理转换为人工计算需要深刻理解生物神经网络在细胞和网络层面上的有效性，以及为什么在硅中复制这些能力会带来如此重大的系统挑战。大脑通过在数十亿个神经元之间进行分布式计算来处理信息，每个神经元的运行速度相对于硅晶体管来说相对较慢。尽管存在速度限制，但大脑的并行架构能够实现对复杂感官输入、决策和行为控制的复杂实时处理。

尽管在速度上存在明显劣势，但这种计算效率源于大脑的基本组织原则。每个神经元作为一个简单的处理单元，整合来自数千个其他神经元的输入，并根据这个综合输入是否超过阈值产生一个二进制输出信号。神经元之间的连接强度，通过突触介导，会通过经验不断修改。这种突触可塑性构成了生物神经网络学习和适应的基础。

在人工系统中复制生物效率需要克服基本权衡。虽然大脑仅用 20 瓦（如前所述）就能实现非凡的效率，但与之相当的的人工神经网络则需要数量级更多的电力。例如，大型语言模型在训练期间可能消耗兆瓦电力，在推理期间消耗千瓦电力——比大脑的电力多出数千到数百万倍。这种显著的效率差距推动了工程领域对专用硬件、量化技术和架构创新的关注。

从这些组织洞察中汲取灵感，生物系统提出了人工神经网络中所需的一些关键计算元素：

+   集成多个输入的简单处理单元

+   单元之间可调节的连接强度

+   基于输入阈值的非线性激活

+   并行处理架构

+   通过修改连接强度进行学习

现在的问题变成了：我们如何将这些抽象的生物原理转化为计算机可以执行的具体数学操作？

### 神经概念数学翻译

将生物洞察力转化为实际系统，我们面临着在数字系统的刚性框架内捕捉神经计算本质的挑战。正如我们在神经元模型分析中确立的（参见表 3.2），人工神经元将生物过程简化为三个关键操作：加权输入处理（突触强度）、求和（信号整合）和激活函数（基于阈值的触发）。

表 3.3 提供了一个系统性的视角，展示了这些生物特征如何映射到它们的计算对应物，揭示了数字神经实现的可能性与局限性。

表 3.3：**生物-计算类比**：人工神经元抽象了生物神经网络系统的关键原则，将神经元触发映射到激活函数，突触强度映射到加权连接，将信号整合映射到求和操作——为数字神经实现奠定了基础。生物系统中的分布式记忆和并行处理在权重矩阵和并发计算中找到计算对应物，突显了这种抽象的强大和局限性。

| **生物特征** | **计算翻译** |
| --- | --- |
| **神经元激发** | 激活函数 |
| **突触强度** | 加权连接 |
| **信号整合** | 求和操作 |
| **分布式内存** | 权重矩阵 |
| **并行处理** | 并发计算 |

使用前面概述的生物到人工映射原理，这种数学抽象保留了关键的计算原理，同时实现了高效的数字实现。权重、求和和激活操作直接对应于我们在神经元对应分析中确定的突触强度、信号整合和阈值激发机制。

这种抽象具有计算成本。在生物学中轻松完成的事情，在人工系统中需要密集的数学计算。正如在记忆系统部分所讨论的，这些操作由于内存带宽限制而产生了显著的计算需求。

人工神经网络中的记忆与生物系统有着显著不同的形式。虽然生物记忆分布在突触连接和神经网络模式中，但人工网络将信息存储在离散的权重和参数中。这种架构差异反映了当前计算硬件的限制，其中内存和处理在物理上是分离的，而不是像生物系统那样集成。尽管这些实现方式不同，但人工神经网络在模式识别和学习方面实现了类似的功能能力。

大脑的巨大并行性在人工实现中是一个挑战。虽然生物神经网络通过数亿个同时工作的神经元处理信息，但人工系统通过如 GPU 和张量处理单元等专用硬件来近似这种并行性。这些设备有效地计算构成人工神经网络数学基础矩阵运算，以不同于生物系统的规模和粒度实现并行处理。

### 硬件和软件需求

神经原理的计算翻译产生了来自生物和人工实现之间关键差异的基础设施需求，这直接塑造了系统设计。

表 3.4 展示了每个计算元素如何驱动特定的系统需求。这种映射显示了在计算翻译中做出的选择如何直接影响实施所需的硬件和系统架构。

表 3.4：**计算需求**：人工神经网络的设计直接转化为特定的系统需求；例如，高效的激活函数需要快速的非线性操作单元，而大规模的权重存储需求需要高带宽的内存访问。理解这种映射有助于指导硬件和系统架构的选择，以有效地实施人工智能。

| **计算元素** | **系统要求** |
| --- | --- |
| **激活函数** | 快速非线性操作单元 |
| **权重操作** | 高带宽内存访问 |
| **并行计算** | 专用并行处理器 |
| **权重存储** | 大规模内存系统 |
| **学习算法** | 梯度计算硬件 |

存储架构代表了一个关键需求，由生物系统和人工系统处理内存的关键差异所驱动。在生物系统中，记忆和处理是内在集成的——突触既存储连接强度又处理信号。然而，人工系统必须在处理单元和内存之间保持清晰的分离。这需要既有高容量存储来存储数百万或数十亿个连接权重，又有高带宽路径来快速在存储和处理单元之间移动这些数据。这种数据移动的效率通常成为生物系统不面临的临界瓶颈。

学习过程本身对人工系统提出了独特的要求。虽然生物网络通过局部化学过程修改突触强度，但人工网络必须在整个网络中协调权重更新。这会在训练期间产生计算和内存需求，因为系统不仅必须存储当前权重，还必须为梯度和中途计算保留空间。反向传播错误信号的需求，没有真正的生物类似物，使得系统架构复杂化。确保这些大型模型和保护敏感训练数据引入了复杂的要求，这些要求在第十六章中得到了解决。

能效成为最后一个关键需求，突显了生物和人工实现之间可能的最鲜明对比。人脑惊人的能效，大约 20 瓦特，与人工神经网络的大量电力需求形成鲜明对比。当前系统通常需要数量级更多的能量来实现类似的功能。这一差距推动了更高效硬件架构的研究，并对神经网络的实际部署产生了深远影响，尤其是在资源受限的环境，如移动设备或边缘计算系统中。这一能源消耗的环境影响和可持续 AI 发展的策略在第十八章中进行了探讨。

这些系统需求直接推动了我们在构建机器学习系统时所做的架构选择，从第十一章（ch017.xhtml#sec-ai-acceleration）中涵盖的专用硬件加速器到第八章（ch014.xhtml#sec-ai-training）中讨论的分布式训练系统。理解这些需求存在的原因，即根植于生物计算和人工计算之间的关键差异，对于做出关于系统设计和优化的明智决策至关重要。

### 神经网络计算的发展

我们可以通过硬件和算法的进步来欣赏深度学习领域是如何演变以应对这些挑战的。这一旅程始于 20 世纪 50 年代的早期人工神经网络，以感知器（Rosenblatt 1958）13 的引入为标志。虽然这些早期系统在概念上具有突破性，但它们受到了其时代计算能力的严重限制，主要是缺乏处理能力和内存容量的大型计算机，这些能力对于复杂网络来说是必需的。

20 世纪 80 年代反向传播算法的发展（Rumelhart, Hinton, and Williams 1986）是一个理论上的突破 14，并为训练多层网络提供了一种系统性的方法。该算法的计算需求远远超过了可用的硬件能力。即使是训练一个适度的网络也可能需要数周时间，这使得实验和实际应用变得具有挑战性。算法需求与硬件能力之间的这种不匹配导致了对神经网络兴趣的下降期。

![图片](img/file40.png)

图 3.8：**计算增长**：计算能力的指数级增长——最初从 1952 年到 2010 年以 1.4 倍的速度增长，然后从 2012 年到 2022 年加速到每 3.4 个月翻一番——使得深度学习模型的扩展成为可能。这种趋势，加上 2015 年后大型模型 10 个月翻一番的周期，直接解决了训练复杂神经网络的历史瓶颈，并推动了该领域的近期进展。来源：(Sardanelli et al. 2023)。

虽然我们在前面的章节中已经建立了深度学习的技术基础，但这个术语本身在 2010 年代获得了显著的关注，这与计算能力的显著进步和数据可访问性的提高相吻合。该领域呈指数级增长，如图图 3.8 所示。该图表揭示了两个显著的趋势：以每秒浮点运算次数（FLOPS）衡量的计算能力最初从 1952 年到 2010 年遵循了<semantics><mrow><mn>1.4</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.4\times</annotation></semantics>的改进模式，然后从 2012 年到 2022 年加速到 3.4 个月的翻倍周期。也许更引人注目的是，2015 年到 2022 年间大规模模型的出现（在图中未明确显示或容易看到），其增长速度比一般趋势快 2 到 3 个数量级，遵循了激进的 10 个月翻倍周期。

进化趋势是由三个维度的并行进步驱动的：数据可用性、算法创新和计算基础设施。这三个因素在良性循环中相互加强，至今仍在推动该领域的进步。如图图 3.9 所示，更强大的计算基础设施使得处理更大的数据集成为可能。更大的数据集推动了算法创新。更好的算法需要更复杂的计算系统。

![图片](img/file41.svg)

图 3.9

数据革命改变了神经网络所能实现的可能性。互联网和数字设备的兴起为训练数据提供了前所未有的访问。图像分享平台提供了数百万个标记图像。数字文本集合实现了大规模的语言处理。传感器网络和物联网设备产生了连续的实时数据流。这种数据丰富性为神经网络有效地学习复杂模式提供了所需的原始材料。

算法创新使得有效地使用这些数据成为可能。新的网络初始化方法和控制学习率的方法使训练更加稳定。防止过拟合 15 的技术使模型能够更好地泛化到新的数据。研究人员发现，神经网络性能与模型大小、计算和数据量呈可预测的比例关系，导致越来越雄心勃勃的架构。

计算基础设施的发展以满足这些不断增长的需求。在硬件方面，图形处理单元（GPU）提供了高效神经网络计算所需的并行处理能力。像 TPU16（Norman P. Jouppi 等人，2017d）这样的专用 AI 加速器进一步提升了性能。高带宽内存系统和快速互连解决了数据移动挑战。同样重要的是软件的进步——使构建和训练网络更容易的框架和库 17，能够实现大规模训练的分布式计算系统，以及优化模型部署的工具。

数据可用性、算法创新和计算基础设施的融合为现代深度学习奠定了基础。构建有效的机器学习系统需要理解驱动基础设施需求的计算操作。简单的数学运算，当扩展到数百万个参数和数十亿个训练示例时，创造了塑造这一演变的巨大计算需求。

## 神经网络基础

在追踪了神经网络从生物启发到历史里程碑再到现代系统的演变之后，我们现在将重点从“为什么深度学习成功”转移到“神经网络实际上是如何计算的”。本节发展了机器学习系统工程所必需的数学和架构基础。

我们采取自下而上的方法，从简单到复杂：执行加权求和的单个神经元 → 组织并行计算的层 → 将原始输入转换为预测的完整网络。每个概念都引入了数学原理及其系统影响。在阅读时，请注意每个看似简单的操作——这里的点积，那里的激活函数——如何累积成我们之前讨论的计算需求：数百万个参数需要数吉字节内存，数十亿个操作需要专用硬件，大量数据集需要分布式训练。

神经架构的最新发展和建立在这些基础之上的新兴范例在第二十章中进行了探讨。目前，我们建立所有神经网络共有的基础概念，从简单的分类器到大型语言模型。

### 网络架构基础

神经网络的架构决定了信息如何从输入流向输出，通过系统流动。虽然现代网络可以非常复杂，但它们都建立在几个关键的组织原则之上，这些原则直接影响系统设计。理解这些原则对于实现神经网络和欣赏为什么它们需要我们讨论的计算基础设施至关重要。

为了将这些概念具体化，我们将在本节中使用手写数字识别作为例子——具体来说，是来自 MNIST 数据集（Lecun 等人，1998）的图像分类任务。这个看似简单的任务揭示了神经网络的所有基本原理，同时为更复杂的应用提供了直观感受。

**任务**：给定一个 28×28 像素的手写数字灰度图像，将其分类为十个数字之一（0-9）。

**输入表示**：每个图像包含 784 个像素（28×28），值从 0（白色）到 255（黑色）。我们通过除以 255 将这些值归一化到[0,1]的范围内。当输入到神经网络时，这 784 个值形成我们的输入向量 <semantics><mrow><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>784</mn></msup></mrow><annotation encoding="application/x-tex">\mathbf{x} \in \mathbb{R}^{784}</annotation></semantics>。

**输出表示**：网络产生 10 个值，每个可能的数字一个。这些值代表网络对输入图像包含每个数字的置信度。置信度最高的数字成为预测结果。

**为什么选择这个例子**：MNIST 数据集足够小，可以完全理解（784 个输入，一个简单网络大约有 10 万个参数），但又足够大，具有现实性。这个任务直观易懂——每个人都知道“识别一个手写的 7”是什么意思——使其成为学习可以扩展到更大问题的神经网络原理的理想选择。

**网络架构预览**：一个典型的 MNIST 分类器可能使用：784 个输入神经元（每个像素一个）→ 128 个隐藏神经元 → 64 个隐藏神经元 → 10 个输出神经元（每个数字类别一个）。随着我们开发概念，我们将参考这个特定的架构。

驱动实际系统设计，每个架构选择——从神经元如何连接到层如何组织——都创造了必须高效映射到硬件的特定计算模式。这种网络架构与计算需求之间的映射对于构建可扩展的机器学习系统至关重要。

#### 非线性激活函数

所有神经网络架构的核心都是一个基本构建块：人工神经元或感知器，它实现了之前建立的生物到人工的翻译原则。从系统角度来看，理解感知器的数学操作至关重要，因为这些简单的操作在网络上重复数百万次时，就形成了我们之前讨论的计算瓶颈。

考虑我们的 MNIST 数字识别任务。28×28 图像中的每个像素都成为我们网络的输入。第一隐藏层中的一个神经元可能学会检测特定的模式——比如在数字“1”或“7”中出现的垂直边缘。这个神经元必须以某种方式将所有 784 个像素值组合成一个输出，以指示其模式是否存在。

感知机通过加权求和来完成这项任务。它接受多个输入 <semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, ..., x_n</annotation></semantics>（在我们的例子中，<semantics><mrow><mi>n</mi><mo>=</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">n=784</annotation></semantics>个像素值），每个输入代表分析对象的一个特征。对于数字识别，这些特征仅仅是原始像素强度，而对于其他任务，它们可能是预测房价的房屋特征或预测歌曲流行度的歌曲属性。

这个乘法过程揭示了看似简单的操作背后的计算复杂性。从计算的角度来看，每个输入都需要在内存中存储并在处理过程中检索。当在深度网络中的数百万个神经元上乘法时，这些内存访问模式成为主要的性能瓶颈。这就是为什么我们之前讨论的内存层次结构和带宽考虑对神经网络性能如此关键的原因。

理解这个加权求和过程后，感知机可以被配置为执行回归或分类任务。对于回归，实际数值输出 <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics> 被使用。对于分类，输出取决于 <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics> 是否超过某个阈值。如果 <semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics> 超过这个阈值，感知机可能会输出一个类别（例如，“是”），如果没有，则输出另一个类别（例如，“否”）。

![](img/file42.svg)

图 3.10：**加权输入求和**：感知机计算多个输入的加权总和，这些输入代表特征值，并将结果传递给激活函数以产生输出。每个输入 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics> 在聚合之前被乘以相应的权重 <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">w_{ij}</annotation></semantics>，这构成了从数据中学习复杂模式的基础。使用此图。

通过可视化这些数学概念，图 3.10 展示了感知器的基本构建块，它是更复杂神经网络的基础。超越单个单元的扩展，感知器层协同工作，每一层的输出作为下一层的输入。这种层次结构创建了能够处理越来越复杂任务的深度学习模型，从图像识别到自然语言处理。

拆解计算机制，每个输入 <semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics> 都有一个相应的权重 <semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">w_{ij}</annotation></semantics>，感知器只是将每个输入与其匹配的权重相乘。中间输出 <semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics> 是作为输入的加权总和来计算的：<semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">z = \sum (x_i \cdot w_{ij})</annotation></semantics>

这种数学表达式的表面简单性掩盖了其计算复杂性。当扩展到数百万个神经元和数十亿个参数时，这些内存访问模式成为神经网络计算中的主要性能瓶颈。

为了增强模型的灵活性，在这个中间计算中，添加了一个偏置项 <semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics>，允许模型通过上下移动线性输出函数来更好地拟合数据。因此，感知器计算出的中间线性组合包括偏置项变为：<semantics><mrow><mi>z</mi><mo>=</mo><mo>∑</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow> <annotation encoding="application/x-tex">z = \sum (x_i \cdot w_{ij}) + b</annotation></semantics>

这个数学公式直接决定了我们之前讨论的硬件需求。求和操作需要累加器单元，乘法操作需要高吞吐量的算术单元，而内存访问则需要高带宽的内存系统。理解数学运算与硬件需求之间的这种联系对于设计高效的机器学习系统至关重要。

除了线性变换之外，激活函数是非线性变换的关键，它通过将线性加权求和转换为非线性输出，使神经网络能够学习复杂模式。没有激活函数，多层线性层会塌缩成一个单一的线性变换，严重限制了网络的表达能力。图 3.11 展示了最常用的四种激活函数及其特征形状。

![](img/file43.svg)

图 3.11：**常见激活函数**：神经网络依赖于非线性激活函数来近似复杂关系。每个函数都表现出独特的特征：sigmoid 将输入映射到<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,1)</annotation></semantics>，具有平滑的梯度，tanh 提供以零为中心的输出<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1)</annotation></semantics>，ReLU 通过对于负输入输出零来引入稀疏性，softmax 将 logits 转换为概率分布。这些不同的行为使得网络能够学习不同类型的模式和关系。

激活函数的选择对学习效果和计算效率有深远的影响。理解每个函数的数学特性对于设计有效的神经网络至关重要。最常用的激活函数包括：

##### Sigmoid

Sigmoid 函数将任何输入值映射到 0 和 1 之间的有界范围：<semantics><mrow><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mrow> <annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}}</annotation></semantics>

这种 S 形曲线（如图 3.11（ch009.xhtml#fig-activation-functions）左上角所示）产生的输出可以解释为概率，这使得 sigmoid 特别适用于二元分类任务。对于非常大的正输入，函数趋近于 1；对于非常大的负输入，它趋近于 0。sigmoid 平滑、连续的特性使其在所有地方都是可微的，这对于基于梯度的学习是必要的。

然而，sigmoid 函数有一个显著的局限性：对于绝对值较大的输入（远离零），梯度变得极其小——这种现象称为**梯度消失问题**18。在反向传播过程中，这些小的梯度在层之间相乘，导致早期层的梯度呈指数级减小。这实际上阻止了深度网络的学习，因为权重更新变得微不足道。

Sigmoid 的输出不是零中心（所有输出都是正数）。这种不对称性可能导致优化过程中的权重更新效率低下，因为连接到 sigmoid 单元的权重的梯度都将具有相同的符号。

##### 双曲正切函数

双曲正切函数通过将输入映射到范围 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1, 1)</annotation></semantics> 来解决 sigmoid 的零中心限制： <semantics><mrow><mo>tanh</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}</annotation></semantics>

如 图 3.11（右上角）所示，tanh 产生一个类似于 sigmoid 的 S 形曲线，但中心在零点。负输入映射到负输出，而正输入映射到正输出。这种对称性有助于在训练期间平衡梯度流动，通常比 sigmoid 导致更快的收敛。

与 sigmoid 类似，tanh 在任何地方都是平滑且可微的。它对于具有大数值的输入仍然受到梯度消失问题的困扰。当函数饱和（接近 -1 或 1）时，梯度变得非常小。尽管存在这种局限性，但 tanh 的零中心输出使其在许多架构中比 sigmoid 更适合隐藏层，特别是在循环神经网络中，保持时间步之间的激活平衡非常重要。

##### ReLU

矩形线性单元（ReLU）通过提供一个简单的解决方案来解决梯度消失问题（Nair 和 Hinton 2010)19：<semantics><mrow><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mi>x</mi></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>></mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>0</mn></mtd><mtd columnalign="left" style="text-align: left"><mrow><mtext mathvariant="normal">if</mtext></mrow> <mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow> <annotation encoding="application/x-tex">\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}</annotation></semantics>

图 3.11（左下角）显示了 ReLU 的特征形状：对于正输入是直线，对于负输入是零。这种简单性提供了几个优点：

**梯度流**：对于正输入，ReLU 的梯度正好是 1，允许梯度在网络中不变地流动。这防止了 sigmoid 和 tanh 在深层架构中遇到的梯度消失问题。

**稀疏性**：通过将所有负激活设置为零，ReLU 在网络上引入了自然稀疏性。通常，ReLU 网络中的大约 50%的神经元对于任何给定的输入都会输出零。这种稀疏性有助于减少过拟合，并使网络更具可解释性。

**计算效率**：与需要昂贵的指数计算的 sigmoid 和 tanh 函数不同，ReLU 通过简单的比较和条件操作进行计算：`output = (input > 0) ? input : 0`。这种简单性转化为更快的计算和更低的能耗，这对于在资源受限的设备上部署尤为重要。

ReLU 并非没有缺点。**死亡 ReLU 问题**发生在神经元“卡住”并输出零的情况下。如果一个神经元的权重更新使得其加权输入始终为负，该神经元将输出零，并在反向传播期间贡献零梯度。这个神经元实际上变得非功能性，并且永远无法恢复。仔细初始化和学习率选择有助于减轻这个问题。

##### Softmax

与之前独立作用于每个值的激活函数不同，softmax 同时考虑所有值以产生一个概率分布：<semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}</annotation></semantics>

对于一个包含<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>个值（通常称为 logits）的向量，softmax 将其转换为<semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics>个概率，这些概率之和为 1。图 3.11（右下角）显示了 softmax 输出的一个组成部分；在实践中，softmax 处理整个向量，其中每个元素的输出都取决于所有输入值。

Softmax 几乎仅用于多类分类问题的输出层。通过将任意实值 logits 转换为概率，softmax 使网络能够在多个类别中表达信心。概率最高的类别成为预测类别。指数函数确保较大的 logits 获得不成比例的高概率，当网络有信心时，这会在类别之间创建清晰的区分。

输入 logits 与输出概率之间的数学关系是可微分的，这使得在训练期间梯度可以通过 softmax 反向传播。当与交叉熵损失（在第八章中讨论）结合使用时，softmax 产生特别干净的梯度表达式，有效地指导学习。

**系统视角：激活函数与硬件**

**为什么 ReLU 在实践中的统治地位**：除了避免梯度消失等数学优势之外，ReLU 的硬件效率解释了其广泛的应用。计算<semantics><mrow><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics>只需要一个比较操作，而 sigmoid 和 tanh 需要计算指数——这些操作在时间和能量上都要昂贵得多。这种计算简单性意味着 ReLU 可以在任何处理器上更快地执行，并且消耗的电量显著减少，这对于电池供电设备来说是一个关键考虑因素。激活函数的计算和硬件影响，包括性能基准和现代加速器的实现策略，将在第八章 Chapter 8 中探讨。

![](img/file44.svg)

图 3.12：**非线性激活**：神经网络通过将输入的加权和应用非线性激活函数来模拟复杂关系，从而能够表示非线性决策边界。这些函数通过点的排列转换输入值，从而具有学习复杂模式的能力，这些模式超出了线性组合的范围。来源：Medium，sachin kaushik。

如上所述的激活函数部分详细说明，这些非线性变换将线性输入和转换成非线性输出：<semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y} = \sigma(z)</annotation></semantics>

因此，感知器的最终输出，包括激活函数，可以表示为：

图 3.12 展示了数据表现出非线性模式，而线性方法无法充分模拟的例子，这说明了为什么前面讨论的非线性激活函数对于复杂模式识别是至关重要的。

泛化逼近定理 20 确立了具有激活函数的神经网络可以逼近任意函数。这一理论基础，结合上述讨论的 ReLU 和 sigmoid 等特定激活函数的计算和优化特性，解释了神经网络在复杂任务中的实际有效性。

将线性组合与激活函数相结合，完整的感知器计算如下：<semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>σ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>∑</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y} = \sigma\left(\sum (x_i \cdot w_{ij}) + b\right)</annotation></semantics>

#### 层和连接

虽然单个感知器可以模拟简单的决策，但神经网络的力量来自于将多个神经元组合成层。层是一组并行处理信息的神经元。层中的每个神经元独立地对相同的输入进行操作，但具有自己的一套权重和偏置，这使得层能够从相同的数据中学习不同的特征或模式。

在典型的神经网络中，我们以层次化的方式组织这些层：

1.  **输入层**：接收原始数据特征

1.  **隐藏层**：通过多个阶段处理和转换数据

1.  **输出层**：生成最终的预测或决策

图 3.13 展示了这种分层架构。当数据通过这些层流动时，每一层都会转换数据的表示，逐渐构建更复杂和抽象的特征。这种层次化处理赋予了深度神经网络学习复杂模式非凡的能力。

![](img/file45.svg)

图 3.13：**分层网络架构**：深度神经网络通过连续的层转换数据，使提取越来越复杂的特征和模式成为可能。每一层都对前一层的输出应用非线性转换，最终将原始输入映射到期望的输出。来源：brunellon。

#### 数据在网络层中的流动

当数据通过网络流动时，在每一层都会进行转换以提取有意义的模式。我们为单个神经元建立的加权求和和激活过程扩展到整个网络：每一层都对其所有神经元并行应用这些操作，一个层的输出成为下一层的输入。这创建了一个层次化的管道，其中早期层检测到的简单特征在深层层中组合成越来越复杂的模式——使神经网络能够从原始数据中学习复杂的表示。

### 参数和连接

神经网络的可学习参数主要由权重和偏置组成，它们共同决定了信息在网络中的流动方式以及如何对输入数据进行变换。本节探讨了这些参数在神经网络中的组织和结构。我们探讨了连接层的权重矩阵，定义网络拓扑的连接模式，提供变换灵活性的偏置项，以及实现高效计算的参数组织策略。

#### 权重矩阵

权重决定了输入如何强烈地影响神经元输出。在较大的网络中，这些权重会组织成矩阵，以便在层之间进行高效的计算。例如，在一个具有<semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics>个输入特征和<semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>个神经元的层中，权重形成一个矩阵<semantics><mrow><mi>𝐖</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{n \times m}</annotation></semantics>。这个矩阵中的每一列代表层中单个神经元的权重。这种组织方式使得网络能够同时处理多个输入，这对于有效地处理现实世界数据是一个基本特征。

回想一下，对于一个单个神经元，我们计算了<semantics><mrow><mi>z</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">z = \sum_{i=1}^n (x_i \cdot w_{ij}) + b</annotation></semantics>。当我们有一个具有<semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>个神经元的层时，我们可以分别计算每个神经元的输出，但矩阵运算提供了一种更有效的方法。而不是单独计算每个神经元，矩阵乘法使我们能够同时计算所有<semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics>个输出：<semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow> <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}</annotation></semantics>

这种矩阵组织不仅仅是数学上的便利，它反映了现代神经网络为了效率而实施的实现方式。每个权重<semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">w_{ij}</annotation></semantics>代表了输入特征<semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics>和层中神经元<semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics>之间连接的强度。

#### 网络连接架构

在最简单和最常见的情况下，一个层中的每个神经元都与前一层的每个神经元相连，形成了我们所说的“密集”或“全连接”层。这种模式意味着每个神经元都有机会从前一层的所有可用特征中学习。虽然本章专注于全连接层以建立基础原理，但替代的连接模式（在第四章第四章中探讨）可以通过根据问题特征限制连接来显著提高结构化数据的效率。

图 3.14 展示了这些层之间的密集连接。对于一个具有大小为<semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>3</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1, n_2, n_3)</annotation></semantics>的网络的权重矩阵将具有以下维度：

+   在第一层和第二层之间：<semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics>

+   在第二层和第三层之间：<semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>

![](img/file46.svg)

图 3.14：**全连接层**：多层感知器（MLPs）利用层之间的密集连接，使每个神经元能够整合前一层的所有神经元的信 息。定义这些连接的权重矩阵—<semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}</annotation></semantics> 和 <semantics><mrow><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><msub><mi>n</mi><mn>3</mn></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}</annotation></semantics>—决定了这些整合的强度，并有助于从输入数据中学习复杂的模式。来源：J. McCaffrey。

#### 偏置项

层中的每个神经元也都有一个相关的偏置项。虽然权重决定了输入的相对重要性，但偏置允许神经元移动其激活函数。这种移动对于学习至关重要，因为它为网络提供了适应更复杂模式的灵活性。

对于具有 <semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics> 个神经元的层，偏置项形成一个向量 <semantics><mrow><mi>𝐛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{b} \in \mathbb{R}^m</annotation></semantics>。当我们计算层的输出时，这个偏置向量被添加到输入的加权和：<semantics><mrow><mi>𝐳</mi><mo>=</mo><msup><mi>𝐱</mi><mi>T</mi></msup><mi>𝐖</mi><mo>+</mo><mi>𝐛</mi></mrow> <annotation encoding="application/x-tex">\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}</annotation></semantics>

偏置项 21 有效地允许每个神经元具有不同的“阈值”以激活，这使得网络更具表现力。

#### 权重和偏置存储组织

神经网络中权重和偏置的组织遵循一种系统模式。对于一个具有 <semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics> 层的网络，我们保持：

+   每个层的权重矩阵 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>

+   每个层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> 的偏置向量 <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics>

+   每个层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> 的激活函数 <semantics><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">f^{(l)}</annotation></semantics>

这给出了完整的层计算：<semantics><mrow><msup><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐡</mi><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></mrow></msup><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{h}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})</annotation></semantics> 其中 <semantics><msup><mi>𝐡</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{h}^{(l)}</annotation></semantics> 表示应用激活函数后的层的输出。

**检查点：神经网络架构基础**

在继续到网络拓扑和训练之前，验证你对我们已经涵盖的基础概念的理解：

**核心概念**：

**系统影响**：

**自测示例**：对于一个具有 784→100→10 层的数字识别网络，计算：(1)每个权重矩阵中的参数数量，(2)总参数数量，(3)在推理过程中存储的单个图像的激活。

*如果这些内容有任何不清楚的地方，请在继续之前回顾第 3.4 节（神经网络基础）、第 3.4.1.1 节（神经元和激活）或第 3.4.2 节（权重和偏差）。接下来的关于训练和优化的章节直接建立在这些基础上。*

### 架构设计

网络拓扑描述了单个神经元如何组织成层并连接形成完整的神经网络。构建直觉从 AI 历史中一个简单的问题开始，这个问题变得闻名 22。

考虑一个学习 XOR 函数的网络——这是一个需要非线性的经典问题。输入 <semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics> 和 <semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics> 可以是 0 或 1，当输入不同时 XOR 输出 1，当它们相同时输出 0。

**网络结构**：2 个输入 → 2 个隐藏神经元 → 1 个输出

**前向传递示例**：对于输入 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1, 0)</annotation></semantics>:

+   隐藏神经元 1：<semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>11</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>12</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_1 = \text{ReLU}(1 \cdot w_{11} + 0 \cdot w_{12} + b_1)</annotation></semantics>

+   隐藏神经元 2：<semantics><mrow><msub><mi>h</mi><mn>2</mn></msub><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>⋅</mo><msub><mi>w</mi><mn>21</mn></msub><mo>+</mo><mn>0</mn><mo>⋅</mo><msub><mi>w</mi><mn>22</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_2 = \text{ReLU}(1 \cdot w_{21} + 0 \cdot w_{22} + b_2)</annotation></semantics>

+   输出：<semantics><mrow><mi>y</mi><mo>=</mo><mtext mathvariant="normal">sigmoid</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>31</mn></msub><mo>+</mo><msub><mi>h</mi><mn>2</mn></msub><mo>⋅</mo><msub><mi>w</mi><mn>32</mn></msub><mo>+</mo><msub><mi>b</mi><mn>3</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y = \text{sigmoid}(h_1 \cdot w_{31} + h_2 \cdot w_{32} + b_3)</annotation></semantics>

这个简单的网络展示了隐藏层如何使学习非线性模式成为可能——这是单层无法实现的。

XOR 示例建立了基本的三层架构，但现实世界的网络需要系统地考虑设计约束和计算规模 23。使用 MNIST(Lecun et al. 1998)24 数据集识别手写数字说明了问题结构如何决定网络维度，同时隐藏层配置仍然是一个关键的设计决策。

#### 前馈网络架构

将三层架构应用于 MNIST 揭示了数据特性和任务需求如何限制网络设计。如图图 3.15<semantics><mtext mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics>所示，一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素的手写数字灰度图像必须通过输入、隐藏和输出层进行处理，以产生分类输出。

输入层的宽度直接由我们的数据格式决定。如图图 3.15<semantics><mtext mathvariant="normal">b)</mtext><annotation encoding="application/x-tex">\text{b)}</annotation></semantics>所示，对于一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素的图像，每个像素成为一个输入特征，需要 784 个输入神经元<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>28</mn><mo>×</mo><mn>28</mn><mo>=</mo><mn>784</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(28\times 28 = 784)</annotation></semantics>。我们可以将其视为一个 2D 像素网格，或者是一个 784 个值的扁平向量，其中每个值代表一个像素的强度。

输出层的结构由我们的任务需求决定。对于数字分类，我们使用 10 个输出神经元，每个可能的数字（0-9）一个。当呈现一个图像时，网络为每个输出神经元产生一个值，其中更高的值表示图像代表该特定数字的可能性更大。

在这些固定的输入和输出层之间，我们在设计隐藏层拓扑结构方面具有灵活性。隐藏层结构的选择，包括要使用的层数及其相应的宽度，是神经网络中的关键设计决策之一。增加额外的层增加了网络的深度，使其能够通过连续的转换学习更抽象的特征。每一层的宽度提供了在每个抽象层次上学习不同特征的能力。

![图片](img/file47.svg)

图 3.15: <semantics><mtext mathvariant="normal">a)</mtext><annotation encoding="application/x-tex">\text{a)}</annotation></semantics> 用于分类 MNIST 数字的神经网络拓扑，展示了如何处理一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素图像。左侧的图像显示了原始数字，并标注了尺寸。右侧的网络显示了每个像素如何连接到隐藏层，最终产生 10 个输出以进行数字分类。 <semantics><mtext mathvariant="normal">b)</mtext><annotation encoding="application/x-tex">\text{b)}</annotation></semantics> MNIST 网络拓扑的另一种可视化，展示了二维图像在网络处理之前被展平成一个 784 维向量。这种表示强调了空间数据是如何转换成适合神经网络处理格式的。

这些基本的拓扑选择对网络的性能和计算需求都有重大影响。每一层或神经元的增加都会增加在训练和推理过程中必须存储和计算的数量。然而，如果没有足够的深度或宽度，网络可能缺乏学习数据中复杂模式的能力。

#### 设计权衡：深度 vs 宽度 vs 性能

神经网络拓扑的设计集中在三个关键决策上：层数（深度）、每层的大小（宽度）以及这些层如何连接。每个选择都会影响网络的学习能力及其计算需求。

网络深度决定了可达到的抽象程度：堆叠的层通过连续的转换构建越来越复杂的特征。对于 MNIST，浅层检测边缘，中间层将边缘组合成笔画，深层组装成完整的数字模式。然而，增加深度会增加计算成本、训练难度（梯度消失）和架构复杂性，而不会带来保证的好处。

每一层的宽度，由它包含的神经元数量决定，控制了网络在每一阶段可以并行处理多少信息。更宽的层可以同时学习更多特征，但需要成比例更多的参数和计算。例如，如果隐藏层在我们的数字识别任务中处理边缘特征，其宽度决定了它可以同时检测多少不同的边缘模式。

在拓扑设计中的一个非常重要的考虑因素是总参数数。对于一个具有大小为<semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>n</mi><mi>L</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n_1, n_2, \ldots, n_L)</annotation></semantics>的层的网络，每一对相邻层<semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>和<semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l+1</annotation></semantics>需要<semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">n_l \times n_{l+1}</annotation></semantics>个权重参数，以及<semantics><msub><mi>n</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">n_{l+1}</annotation></semantics>个偏置参数。这些参数必须在内存中存储并在训练过程中更新，这使得参数数成为实际应用中的一个关键约束。

网络设计需要平衡学习容量、计算效率和训练可追踪性。虽然基本方法是将每个神经元连接到下一层的每个神经元（全连接），但这并不总是最有效的策略。全连接方法假设每个输入元素都可能与其他每个元素交互——然而，现实世界的数据很少表现出这种无约束的关系。

考虑 MNIST 示例：一个 28×28 的图像有 784 个像素，可以形成 306,936 个可能的像素对（<semantics><mfrac><mrow><mn>784</mn><mo>×</mo><mn>783</mn></mrow><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{784 \times 783}{2}</annotation></semantics>）。一个包含 100 个神经元的全连接第一层学习 78,400 个权重，实际上检查了每个可能的像素关系。相邻像素（形成数字的边缘）之间的交互比相对角上的像素更多。全连接层通过参数和计算学习像素（1,1）与像素（28,28）之间没有强烈的交互，这些关系我们可以通过结构编码。专门的架构（在第四章中探讨）通过根据问题结构限制连接来解决这个问题的不效率，通过利用空间局部性、时间顺序或其他特定领域的模式，以 10-100×更少的参数实现更优的结果。

信息在网络中的流动代表了另一个重要的考虑因素。虽然基本流程是从输入到输出，但一些网络设计包括额外的路径，如跳跃连接或残差连接。这些替代路径通过作为在需要时允许更直接信息流的捷径来促进训练并提高学习复杂模式的有效性，类似于人类大脑在物体识别过程中结合详细和一般印象的方式。

这些设计决策具有重大的实际意义，包括存储网络参数的内存使用、训练和推理过程中的计算成本、训练行为和收敛性，以及网络泛化到新示例的能力。这些权衡的最佳平衡在很大程度上取决于具体问题、可用的计算资源以及数据集特征。成功的网络设计需要对这些因素进行仔细考虑，同时考虑实际约束。

在我们建立了对网络架构的理解——神经元如何连接到层，层如何堆叠成网络，以及设计选择如何影响计算需求之后——我们现在可以解决核心问题：这些网络是如何学习的？架构提供了结构，但学习过程通过发现使准确预测成为可能的权重值，使这种结构变得生动。

**系统视角：架构决定部署可行性**

**从设计到部署**：每一个架构决策——层数、层宽、连接模式——直接决定了内存需求和计算成本。一个拥有 100 万个参数的网络，仅为了存储权重就需要大约 4MB 的内存，在考虑推理过程中的激活之前。随着模型变得更深更宽，它们的内存占用和计算需求呈平方增长，而不是线性增长。这种架构与资源需求之间的数学关系解释了为什么相同的架构模式不能在所有平台上统一部署。系统工程的洞察：架构设计必须从一开始就考虑目标部署约束，因为事后压缩只能部分恢复架构与资源不匹配的问题。

#### 层连接设计模式

神经网络可以通过层之间的不同连接模式进行结构化，每种模式都为学习和计算提供了独特的优势。理解这些模式可以提供对网络如何处理信息和从数据中学习表示的见解。

密集连接表示每个神经元都与后续层中的每个神经元相连的标准模式。在我们的 MNIST 示例中，将我们的 784 维输入层连接到 100 个神经元的隐藏层需要 78,400 个权重参数。这种完全连接使网络能够学习输入和输出之间的任意关系，但参数数量与层宽度的平方成正比。

稀疏连接模式在神经元层间连接的方式上引入了有意的限制。而不是保持所有可能的连接，神经元只连接到相邻层中的一部分神经元。这种方法从生物神经网络系统中汲取灵感，其中神经元通常只与有限数量的其他神经元形成连接。在我们的 MNIST 示例等视觉处理任务中，神经元可能只连接到表示附近像素的输入，反映了视觉特征的局部性质。

随着网络的加深，从输入到输出的路径变长，可能会使学习过程复杂化。跳跃连接通过在非相邻层之间添加直接路径来解决这个问题。这些连接提供了信息流的替代路径，补充了标准的层间逐层进展。在我们的数字识别示例中，跳跃连接可能允许后续层直接引用高级模式和原始像素值。

这些连接模式对神经网络的理沦能力和实际应用都有重大影响。密集连接以牺牲计算效率为代价最大化学习灵活性。稀疏连接可以减少计算需求，同时可能提高网络学习结构化模式的能力。跳跃连接有助于在更深的网络中保持有效的信息流。

#### 模型大小和计算复杂度

神经网络中参数（权重和偏置）的排列决定了其学习能力和计算需求。虽然拓扑结构定义了网络的结构，但参数的初始化和组织在学习和性能中起着至关重要的作用。

参数数量随着网络宽度和深度的增加而增长。对于我们的 MNIST 示例，考虑一个具有 784 维输入层、两个各有 100 个神经元的隐藏层和一个 10 个神经元的输出层的网络。第一层需要 78,400 个权重和 100 个偏置，第二层需要 10,000 个权重和 100 个偏置，输出层需要 1,000 个权重和 10 个偏置，总共 89,610 个参数。每个参数都必须在内存中存储并在学习过程中更新。

参数初始化对网络行为至关重要。将所有参数设置为零会导致同一层的神经元行为相同，从而阻止多样化的特征学习。相反，权重通常随机初始化，而偏差通常从小的常数值或甚至零开始。这些初始值的规模至关重要，因为过大或过小的值可能导致学习动态不佳。

参数的分布影响信息通过层的流动。在数字识别中，如果权重太小，重要的输入细节可能无法传播到后续层。如果太大，网络可能会放大噪声。偏差有助于调整每个神经元的激活阈值，使网络能够学习最佳决策边界。

不同的架构可能对参数组织施加特定的约束。一些在网络区域之间共享权重，以编码位置不变的模式识别。其他可能将某些权重限制为零，实现稀疏连接模式。

在我们理解了网络架构、神经元和参数之后，我们现在可以解决一个基本问题：这些随机初始化的参数如何变得有用？答案在于将网络从初始随机状态转换成能够做出准确预测的系统这一学习过程。

## 学习过程

神经网络通过在示例上进行训练的过程来学习执行任务。这个过程将网络从初始状态（正如我们刚才讨论的，权重随机初始化）转换到训练状态，在这些状态下，相同的权重编码了来自训练数据的具有意义的模式。理解这一过程对于深度学习模型的理论基础和实践实现都是至关重要的。

### 从标记示例中进行监督学习

建立在我们的建筑基础之上，神经网络训练的核心原则是从标记示例中进行监督学习。以我们的 MNIST 数字识别任务为例：我们有一个包含 60,000 个训练图像的数据集，每个图像都是一个<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素的灰度图像，并配对其正确的数字标签。网络必须通过预测和权重调整的迭代过程来学习这些图像与其对应数字之间的关系。确保训练数据的质量和完整性对于模型的成功至关重要，如第六章所述。

输入和输出之间的关系驱动了训练方法。训练作为一个循环进行，其中每个迭代涉及处理一个称为批次的训练示例子集 25。对于每个批次，网络执行几个关键操作：

+   通过网络层进行正向计算以生成预测

+   使用损失函数评估预测精度

+   基于预测误差计算权重调整

+   更新网络权重以改善未来的预测

将这种迭代方法形式化，这个过程可以用数学表达式表示。给定一个输入图像<semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics>及其真实标签<semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>，网络计算其预测：<semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\hat{y} = f(x; \theta)</annotation></semantics>其中<semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>表示神经网络函数，<semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics>表示所有可训练参数（权重和偏置，我们之前讨论过）。网络的误差通过损失函数<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>来衡量：<semantics><mrow><mtext mathvariant="normal">loss</mtext><mo>=</mo><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\text{loss} = L(\hat{y}, y)</annotation></semantics>

这种预测质量的量化成为学习的基础。这种误差测量通过称为“反向传播”的过程驱动网络参数的调整，我们将在后面详细探讨。

超越单个示例的缩放，在实践中，训练是在示例批次上进行的，而不是单个输入。对于 MNIST 数据集，每个训练迭代可能会同时处理 32、64 或 128 个图像。这种批量处理有两个目的：它通过并行处理使现代计算硬件得到有效利用，并通过在多个示例中平均误差来提供更稳定的参数更新。

这种基于批次的处理方法既提高了计算效率，又保证了训练的稳定性。训练周期会持续进行，直到网络达到足够的准确度或达到预定的迭代次数。在整个过程中，损失函数充当指南，其最小化表示网络性能的改善。正如在第十二章 Chapter 12 中讨论的那样，建立适当的指标和评估协议对于评估训练效果至关重要。

### 前向传递计算

正向传播，如图 3.16 图所示，是神经网络中的核心计算过程，其中输入数据通过网络层流动以生成预测。理解这个过程很重要，因为它既是网络推理的基础，也是训练的基础。我们通过我们的 MNIST 数字识别示例来考察正向传播是如何工作的。

![图片](img/file48.svg)

图 3.16：**前向传播过程**：神经网络通过在相互连接的层中按顺序应用加权求和和激活函数，将输入数据转换为预测，从而实现复杂模式识别。这种分层计算构成了在训练过程中进行推理和更新模型参数的基础。

当一个手写数字的图像进入我们的网络时，它将通过层进行一系列的转换。每个转换将加权输入与学习到的模式相结合，逐步提取相关特征。在我们的 MNIST 示例中，一个 28×28 像素的图像通过多个层进行处理，最终为每个可能的数字（0-9）生成概率。

该过程从输入层开始，其中每个像素的灰度值成为输入特征。对于 MNIST 来说，这意味着 784 个输入值（28×28=784），每个值在 0 到 1 之间归一化。然后这些值通过隐藏层向前传播，其中每个神经元根据其学习的权重组合其输入，并应用非线性激活函数。

从计算角度来看，每次通过我们的 MNIST 网络（784→128→64→10）的前向传递都需要大量的矩阵运算。仅第一层就每个样本执行近 10 万个乘加运算。当批量处理多个样本时，这些运算相应地相乘，需要仔细管理内存带宽和计算资源。专门的硬件如 GPU 可以通过并行处理高效地执行这些运算。

#### 单个层处理

通过神经网络的正向计算是系统性的，每一层都将其输入转换为越来越抽象的表示。在我们的 MNIST 网络中，这个过程发生在不同的阶段。

在每一层，计算涉及两个关键步骤：输入的线性变换后跟一个非线性激活。线性变换应用我们之前见过的相同的加权求和操作，但现在使用跟踪我们所在层级的符号：<semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}</annotation></semantics>

在这里，<semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics> 代表第 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics> 层的权重矩阵，<semantics><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{(l-1)}</annotation></semantics> 包含上一层的激活值（应用激活函数后的输出），而 <semantics><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(l)}</annotation></semantics> 是偏置向量。上标 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l)</annotation></semantics> 用于追踪每个参数属于哪一层。

在此线性变换之后，每一层应用一个非线性激活函数 <semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics>：<semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)} = f(\mathbf{Z}^{(l)})</annotation></semantics>

此过程在每个层级上重复，形成一个变换链：

输入 → 线性变换 → 激活 → 线性变换 → 激活 → … → 输出

在我们的 MNIST 示例中，像素值首先通过第一隐藏层的权重进行变换，将 784 维输入转换为中间表示。每个后续层进一步变换这一表示，最终产生一个 10 维输出向量，表示网络对每个可能的数字的置信度。

#### 矩阵乘法公式

完整的前向传播过程可以表示为函数的组合，每个函数代表一层变换。将这一过程数学化是基于 MNIST 示例的。

对于具有<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>层的网络，我们可以将完整的正向计算表示为：<semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>⋯</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo><mi>⋯</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(L)} = f^{(L)}\Big(\mathbf{W}^{(L)}f^{(L-1)}\Big(\mathbf{W}^{(L-1)}\cdots\big(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)})\big)\cdots + \mathbf{b}^{(L-1)}\Big) + \mathbf{b}^{(L)}\Big)</annotation></semantics>

虽然这个嵌套表达式可以捕捉到整个过程，但我们通常是一步一步地计算它：

1.  第一层：`<semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>𝐗</mi><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(1)} = f^{(1)}(\mathbf{Z}^{(1)})</annotation></semantics>` 

1.  隐藏层 `<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>=</mo><mn>2</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(l = 2,\ldots, L-1)</annotation></semantics>`: `<semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(l)} = f^{(l)}(\mathbf{Z}^{(l)})</annotation></semantics>`

1.  输出层: <semantics><mrow><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow> <annotation encoding="application/x-tex">\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)} + \mathbf{b}^{(L)}</annotation></semantics> <semantics><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">\mathbf{A}^{(L)} = f^{(L)}(\mathbf{Z}^{(L)})</annotation></semantics>

在我们的 MNIST 示例中，如果我们有一批<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>图像，这些操作的维度是：

+   输入 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics>: <semantics><mrow><mi>B</mi><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">B \times 784</annotation></semantics>

+   第一层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics>: <semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">n_1\times 784</annotation></semantics>

+   隐藏层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(l)}</annotation></semantics>: <semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>×</mo><msub><mi>n</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">n_l\times n_{l-1}</annotation></semantics>

+   输出层权重 <semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>L</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(L)}</annotation></semantics>: <semantics><mrow><mn>10</mn><mo>×</mo><msub><mi>n</mi><mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">10 \times n_{L-1}</annotation></semantics>

#### 步骤计算序列

理解这些数学运算如何转化为实际计算需要检查一批 MNIST 图像的前向传播过程。这个过程说明了数据如何从原始像素值转换为数字预测。

考虑一个包含 32 张图像的批次进入我们的网络。每张图像最初是一个 <semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics> 的像素值网格，我们将其展平成一个 784 维的向量。对于整个批次，这给我们一个大小为 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics> 的输入矩阵 <semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">32\times 784</annotation></semantics>，其中每一行代表一张图像。这些值通常被归一化，使其位于 0 和 1 之间。

每一层的转换过程如下：

+   **输入层处理**：网络接收我们的输入矩阵 <semantics><mi>𝐗</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics> <semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>32</mn><mo>×</mo><mn>784</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(32\times 784)</annotation></semantics> 并使用第一层的权重进行转换。如果我们的第一个隐藏层有 128 个神经元，<semantics><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{W}^{(1)}</annotation></semantics> 是一个 <semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">784\times 128</annotation></semantics> 的矩阵。得到的计算 <semantics><mrow><mi>𝐗</mi><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X}\mathbf{W}^{(1)}</annotation></semantics> 产生一个 <semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">32\times 128</annotation></semantics> 的矩阵。

+   **隐藏层转换**：这个矩阵中的每个元素然后都会加上相应的偏置并通过激活函数。例如，使用 ReLU 激活函数时，任何负值变为零，而正值保持不变。这种非线性转换使网络能够学习数据中的复杂模式。

+   **输出生成**：最后一层将其输入转换为 <semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">32\times 10</annotation></semantics> 矩阵，其中每一行包含 10 个值，对应于网络对每个可能数字的置信度分数。通常，这些分数会通过 softmax 函数转换为概率：<semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">digit</mtext></mrow> <mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msup><mi>e</mi><msub><mi>z</mi><mi>k</mi></msub></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10} e^{z_k}}</annotation></semantics>

对于批处理中的每一张图像，这会产生一个可能的数字的概率分布。概率最高的数字代表网络的预测。

#### 实现和优化考虑

前向传播的实现需要仔细关注几个影响计算效率和内存使用的实际方面。当处理大量数据或使用深度网络时，这些考虑变得尤为重要。

内存管理在前向传播期间起着重要作用。每个层的激活必须存储起来，以便在训练期间的逆传播中使用。对于我们的 MNIST 示例，批处理大小为 32，如果我们有三个大小为 128、256 和 128 的隐藏层，激活存储需求为：

+   第一隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> 个值

+   第二隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn><mo>=</mo><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">32\times 256 = 8,192</annotation></semantics> 个值

+   第三隐藏层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>128</mn><mo>=</mo><mn>4</mn><mo>,</mo><mn>096</mn></mrow><annotation encoding="application/x-tex">32\times 128 = 4,096</annotation></semantics> 个值

+   输出层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>320</mn></mrow><annotation encoding="application/x-tex">32\times 10 = 320</annotation></semantics> 个值

这会产生总共 16,704 个值，在训练过程中必须为每个批次保留在内存中。内存需求与批次大小成线性关系，对于更大的网络来说，内存需求变得相当大。

批处理引入了重要的权衡。更大的批次可以更有效地进行矩阵运算和更好的硬件利用率，但需要更多的内存。例如，将批次大小加倍到 64 将使激活的内存需求加倍。批次大小、内存使用和计算效率之间的关系指导了实际中批次大小的选择。

计算的组织也会影响性能。矩阵运算可以通过仔细的内存布局和专门的库进行优化。激活函数的选择会影响网络的学习能力以及计算效率，因为某些函数（如 ReLU）的计算量比其他函数（如 tanh 或 sigmoid）少。

神经网络的计算特性有利于并行处理架构。虽然传统的 CPU 可以执行这些操作，但专为并行计算设计的 GPU 可以在矩阵运算上实现显著的加速——通常比矩阵运算快 10-100 倍。专门的 AI 加速器通过降低精度算术、专门的内存架构以及针对神经网络计算模式的数据流优化等技术，实现了更高的效率。

能耗在硬件平台之间也有显著差异。CPU 提供灵活性，但每个操作的能耗更高。GPU 提供高吞吐量，但功耗也更高。专门的边缘加速器优化能耗效率，以更少的功率完成相同的计算——这对于移动和嵌入式部署来说是一个关键考虑因素。这种能耗差异源于基本的内存层次结构挑战，其中数据移动主导了计算成本。

这些考虑构成了理解神经网络系统需求的基础，我们将在第四章中更详细地探讨。

现在我们已经了解了神经网络如何通过正向传播处理输入以生成预测，一个关键问题随之出现：我们如何确定这些预测是否良好？答案在于损失函数，它为衡量预测质量提供了数学框架。

### 损失函数

神经网络通过衡量和最小化预测误差来学习。损失函数提供了量化这些误差的算法结构，作为指导学习过程的必要反馈机制。通过损失函数，我们可以将“做出良好预测”的抽象目标转化为具体的优化问题。

为了理解损失函数的作用，让我们继续使用我们的 MNIST 数字识别示例。当网络处理一个手写数字图像时，它输出十个数字，代表它对每个可能数字（0-9）的置信度。损失函数衡量这些预测与真实答案的偏差程度。例如，如果一个图像显示的是“7”，网络应该对数字“7”表现出高置信度，而对其他所有数字表现出低置信度。当网络的预测偏离这个目标时，损失函数会对网络进行惩罚。

考虑一个具体的例子：如果网络看到一个“7”的图像，并输出置信度：<semantics><mrow><mo stretchy="true" form="prefix" mathvariant="monospace">[</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.2</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.3</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo stretchy="true" form="postfix" mathvariant="monospace">]</mo></mrow> <annotation encoding="application/x-tex">\mathtt{[0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]}</annotation></semantics>

最高的置信度（0.3）分配给了数字“7”，但这个置信度相当低，表明预测存在不确定性。一个好的损失函数会在这里产生一个高的损失值，表明网络需要显著改进。相反，如果网络输出：<semantics><mrow><mo stretchy="true" form="prefix" mathvariant="monospace">[</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.9</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.0</mn><mo mathvariant="monospace">,</mo><mn mathvariant="monospace">0.1</mn><mo stretchy="true" form="postfix" mathvariant="monospace">]</mo></mrow> <annotation encoding="application/x-tex">\mathtt{[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]}</annotation></semantics>

损失函数应该产生一个较低的值，因为这样的预测更接近理想值。这说明了损失函数如何通过提供预测质量的反馈来指导网络改进。

#### 错误测量基础

损失函数衡量网络的预测与正确答案之间的距离。这种差异用一个单一的数字表示：较低的损失意味着预测更准确，而较高的损失则表明网络需要改进。在训练过程中，损失函数通过帮助网络调整其权重以做出更好的预测来指导网络。例如，在识别手写数字时，损失将惩罚对正确数字信心较低的预测。

从数学上讲，损失函数<semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics>接受两个输入：网络的预测<semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics>和真实值<semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics>。在我们的 MNIST 任务中，对于单个训练示例：<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">预测与真实值之间差异的度量</mtext></mrow> <annotation encoding="application/x-tex">L(\hat{y}, y) = \text{measure of discrepancy between prediction and truth}</annotation></semantics>

当使用数据批次进行训练时，我们通常计算批次中所有示例的平均损失：<semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)</annotation></semantics> 其中 <semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics> 是批次大小，并且 <semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\hat{y}_i, y_i)</annotation></semantics> 代表第 <semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics> 个示例的预测和真实值。

损失函数的选择取决于任务类型。对于我们的 MNIST 分类问题，我们需要一个能够：

1.  处理多个类别的概率分布

1.  提供有意义的梯度以进行学习

1.  有效地惩罚错误预测

1.  与批量处理具有良好的扩展性

#### 交叉熵和分类损失函数

对于像 MNIST 数字识别这样的分类任务，“交叉熵”(香农 1948)26 损失函数已经成为标准选择。这种损失函数特别适合于比较预测的概率分布与真实的类别标签。

对于单个数字图像，我们的网络输出的是十个可能数字的概率分布。我们将真实标签表示为一个 one-hot 向量，其中除了正确数字位置上的 1 之外，所有条目都是 0。例如，如果真实数字是“7”，标签将是：<semantics><mrow><mi>y</mi><mo>=</mo><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mo></mrow> <annotation encoding="application/x-tex">y = \big[0, 0, 0, 0, 0, 0, 0, 1, 0, 0\big]</annotation></semantics>

此例中的交叉熵损失为：<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mi>j</mi></msub><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y}, y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)</annotation></semantics> 其中 <semantics><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\hat{y}_j</annotation></semantics> 表示网络对数字 j 的预测概率。鉴于我们的独热编码，这简化为：<semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>c</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L(\hat{y}, y) = -\log(\hat{y}_c)</annotation></semantics> 其中 <semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics> 是正确类别的索引。这意味着损失只取决于对正确数字的预测概率——网络根据其对正确答案的信心程度受到惩罚。

例如，如果我们的网络预测以下概率为“7”的图像：

```py
Predicted: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1]
True: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
```

损失将是 <semantics><mrow><mi>−</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.8</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">-\log(0.8)</annotation></semantics>，这大约是 0.223。如果网络更有信心，并预测正确的数字为 0.9，则损失将降低到大约 0.105。

#### 批量损失计算方法

损失的实际计算涉及对数值稳定性和批量处理的考虑。当处理数据批次时，我们计算批次中所有示例的平均损失。

对于 B 个样本的批次，交叉熵损失变为：<semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></munderover><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L_{\text{batch}} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})</annotation></semantics>

高效计算这个损失需要仔细考虑数值精度。对非常小的概率取对数可能导致数值不稳定性。考虑一种情况，我们的网络预测正确类别的概率为 0.0001。直接计算<semantics><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.0001</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(0.0001)</annotation></semantics>可能会造成下溢或导致结果不精确。

为了解决这个问题，我们通常通过以下两个关键修改来实现损失计算：

1.  添加一个小的ε值以防止取零的对数：<semantics><mrow><mi>L</mi><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>+</mo><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow> <annotation encoding="application/x-tex">L = -\log(\hat{y} + \epsilon)</annotation></semantics>

1.  为了提高数值稳定性，应用 log-sum-exp 技巧：<semantics><mrow><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mo>exp</mo><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo>−</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mo>exp</mo><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mo><msub><mi>z</mi><mi>j</mi></msub><mo>−</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\text{softmax}(z_i) = \frac{\exp\big(z_i - \max(z)\big)}{\sum_j \exp\big(z_j - \max(z)\big)}</annotation></semantics>

对于我们的 MNIST 示例，批大小为 32，这意味着：

+   处理 32 组 10 个概率值

+   计算 32 个单独的损失值

+   平均这些值以产生最终的批损失

#### 对学习动态的影响

理解损失函数如何影响训练有助于解释深度学习模型中的关键实现决策。

在每次训练迭代中，损失值具有多重作用：

1.  性能指标：它量化了当前网络的准确性

1.  优化目标：其梯度指导权重更新

1.  收敛信号：其趋势表示训练进度

对于我们的 MNIST 分类器，在训练过程中监控损失可以揭示网络的学习轨迹。典型的模式可能显示：

+   初始高损失（<semantics><mrow><mo>∼</mo><mn>2.3</mn></mrow><annotation encoding="application/x-tex">\sim 2.3</annotation></semantics>，相当于在 10 个类别中的随机猜测）

+   在早期训练迭代中迅速下降

+   随着网络微调其预测，逐渐改进

+   最终稳定在较低的损失（<semantics><mrow><mo>∼</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\sim 0.1</annotation></semantics>，表示有信心做出正确的预测）

损失函数相对于网络输出的梯度提供了驱动反向传播的初始误差信号。对于交叉熵损失，这些梯度具有特别简单的形式：预测概率与真实概率之间的差异。这种数学特性使得交叉熵损失特别适合分类任务，因为它即使在预测非常错误的情况下也能提供强大的梯度。

损失函数的选择也会影响其他训练决策：

+   学习率选择（较大的损失梯度可能需要较小的学习率）

+   批大小（批间损失平均影响梯度稳定性）

+   优化算法行为

+   收敛标准

一旦我们通过损失函数量化了网络的预测误差，下一步关键步骤就是确定如何调整网络的权重以减少这些误差。这引出了反向传播，这是神经网络从错误中学习的一种机制。

### 梯度计算和反向传播

***反向传播*** 是一种算法，通过系统地应用链式法则反向通过网络层，高效地计算神经网络损失函数相对于所有参数的梯度。

反向传播，通常称为反向传播，是神经网络训练的算法基石，它通过基于梯度的优化系统地调整权重。

为了对这一复杂过程建立直观理解，可以通过工厂装配线类比来考虑“信用分配”问题。想象一个汽车工厂，其中车辆通过多个站点：站点 A 安装底盘，站点 B 添加发动机，站点 C 安装轮子，站点 D 进行最终装配。当生产线末端的质检员发现一辆有缺陷的汽车时，他们面临一个关键问题：哪个站点对问题贡献最大，每个站点应该如何调整其工艺？

解决方案从缺陷开始逆向工作。检查员首先检查最终装配（站点 D）并确定其工作如何影响质量问题。站点 D 随后查看它从站点 C 接收到的内容，并计算问题中有多少来自轮子与它自己的装配工作。这种反馈逆向流动：站点 C 检查站点 B 的发动机，站点 B 则审查站点 A 的底盘。每个站点都会收到一个与其工作对缺陷贡献成比例的“调整信号”。如果站点 B 的发动机安装是主要原因，它会收到一个强烈的信号来改变其工艺，而执行正确的站点则会收到较小或没有调整信号。

反向传播系统地解决了神经网络中的“信用分配”问题。输出层（如站点 D）接收关于出了什么错的直接反馈最多。它计算其从前一层的输入对错误的影响，并通过网络发送特定的调整信号。每一层都会根据其对预测错误的贡献接收相应的指导，并相应地调整其权重。这个过程确保每一层都能从错误中学习，最有责任感的连接会做出最大的调整。

在神经网络中，每一层就像装配线上的一个站点，反向传播确定每个连接对最终预测错误的贡献程度。这种从错误中学习的系统方法构成了神经网络通过经验改进的基础。

本节介绍了完整的优化框架，从梯度计算到实际训练实现的整个过程。

#### 反向传播算法步骤

当正向传播计算预测时，反向传播确定如何调整网络的权重以改进这些预测。为了理解这个过程，可以考虑我们的 MNIST 示例，其中网络预测一个“3”来表示一个“7”的图像。反向传播提供了一种系统的方法来调整网络中的权重，通过计算每个权重对错误的影响，使这种错误在未来发生的可能性降低。

这个过程从网络的输出层开始，我们比较预测的数字概率与真实标签。这个错误随后通过网络反向流动，每一层的权重根据其对最终预测的贡献接收一个更新信号。计算遵循微积分的链式法则，将权重与最终错误之间的复杂关系分解成可管理的步骤。

反向传播的数学基础为训练神经网络提供了理论基础，但实际实现需要复杂的软件框架。现代框架如 PyTorch 和 TensorFlow 实现了自动微分系统，可以自动处理梯度计算，消除了手动导数实现的需求。这些框架的系统工程方面，包括计算图和优化策略，在第七章中得到了全面介绍。

#### 错误信号传播

梯度通过神经网络的流动遵循与正向传播相反的路径。从输出层的损失开始，梯度反向传播，计算每一层，最终每个权重如何影响了最终的预测误差。

在我们的 MNIST 示例中，考虑当网络错误地将一个“7”分类为“3”时会发生什么。损失函数在输出层生成一个初始错误信号，本质上表明“7”的概率应该增加，而“3”的概率应该减少。然后，这个错误信号通过网络层反向传播。

对于具有 L 层的网络，梯度流可以用数学公式表示。在每一层 l，我们计算该层的输出如何影响最终损失：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l)}} = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial \mathbf{A}^{(l)}}</annotation></semantics>

这个计算过程通过网络向后级联，每一层的梯度依赖于它之前一层计算的梯度。这个过程揭示了每一层的转换是如何贡献到最终预测误差的。例如，如果早期层中的一些权重强烈影响了错误分类，它们将获得更大的梯度值，这表明需要更大幅度的调整。

这个过程在深层网络中面临挑战。随着梯度通过许多层向后流动，它们可能消失或爆炸。当梯度在许多层中反复相乘时，它们可以变得指数级小，特别是在 sigmoid 或 tanh 激活函数的情况下。这导致早期层学习非常缓慢或根本不学习，因为它们接收到的更新微乎其微。相反，如果梯度值始终大于 1，它们可以指数级增长，导致训练不稳定和权重更新破坏性。

#### 导数计算过程

梯度的实际计算涉及到在每一层计算几个偏导数。对于每一层，我们需要确定权重、偏差和激活的变化如何影响最终的损失。这些计算直接来自微积分的链式法则，但必须高效实现以适应实际的神经网络训练。

在每一层 <semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics>，我们计算三个主要的梯度分量：

1.  权重梯度：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><msup><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>T</mi></msup></mrow> <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T</annotation></semantics>

1.  偏差梯度：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>

1.  输入梯度（用于传播到前一层）：<semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐀</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><msup><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msup><mi>𝐙</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{A}^{(l-1)}} = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}</annotation></semantics>

在我们的 MNIST 示例中，考虑网络输出数字概率的最后一层。如果网络预测了“7”图像的<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mi>…</mi><mo>,</mo><mn>0.05</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0.1, 0.2, 0.5,\ldots, 0.05]</annotation></semantics>，梯度计算将：

1.  从这些概率中的误差开始

1.  计算权重调整如何影响这个误差

1.  将这些梯度反向传播以帮助调整早期层的权重

这些数学公式精确地描述了梯度计算，但系统突破的亮点在于框架如何自动实现这些计算。考虑一个简单的操作，如矩阵乘法后跟 ReLU 激活：`output = torch.relu(input @ weight)`。数学梯度涉及计算 ReLU 的导数（对于负输入为 0，对于正输入为 1）并应用矩阵乘法的链式法则。框架通过以下方式自动处理：

1.  在正向传递期间将操作记录在计算图中

1.  存储必要的中间值（用于梯度计算的预 ReLU 激活）

1.  自动为每个操作生成反向传递函数

1.  优化整个图中的内存使用和计算顺序

这种自动化将梯度计算从需要深厚数学专业知识且容易出错的手动过程转变为一种可靠的系统功能，它能够实现快速实验和部署。该框架在优化计算效率、内存使用和硬件利用率的同时确保正确性。

#### 计算实现细节

反向传播的实际实现需要仔细考虑计算资源和内存管理。这些实现细节对训练效率和可扩展性有重大影响。

在反向传播过程中的内存需求主要来自两个来源。首先，我们需要存储正向传播过程中的中间激活值，因为这些值是计算梯度所必需的。对于我们的 MNIST 网络，批大小为 32，每一层的激活值必须被维护：

+   输入层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">32\times 784</annotation></semantics> 个值 (~100KB 使用 32 位数字)

+   隐藏层 1：<semantics><mrow><mn>32</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">32\times 512</annotation></semantics> 个值 (~64KB)

+   隐藏层 2：<semantics><mrow><mn>32</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">32\times 256</annotation></semantics> 个值 (~32KB)

+   输出层：<semantics><mrow><mn>32</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">32\times 10</annotation></semantics> 个值 (~1.3KB)

其次，我们必须在反向传播过程中为每个参数存储梯度。对于我们的示例网络，该网络大约有 500,000 个参数，这需要几个兆字节的内存来存储梯度。像 Adam27 这样的高级优化器需要额外的内存来存储动量项，这大约将梯度存储需求翻倍。

内存带宽需求与模型大小和批大小成比例。每个训练步骤都需要加载所有参数、存储梯度和访问激活值——这会产生大量的内存流量。对于像我们的 MNIST 示例这样的适度网络，这种流量在典型的内存系统能力范围内仍然是可管理的。然而，随着模型变得更大，内存带宽可能成为一个重要的瓶颈，最大的模型可能需要专用的高带宽内存系统来维持训练效率。

其次，我们需要存储梯度本身的空间。对于每一层，我们必须维护与权重和偏置相似维度的梯度。以我们之前的例子，一个具有大小为 128、256 和 128 的隐藏层的网络为例，这意味着需要存储：

+   第一层梯度：<semantics><mrow><mn>784</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">784\times 128</annotation></semantics> 个值

+   第二层梯度：<semantics><mrow><mn>128</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">128\times 256</annotation></semantics> 个值

+   第三层梯度：<semantics><mrow><mn>256</mn><mo>×</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">256\times 128</annotation></semantics> 个值

+   输出层梯度：<semantics><mrow><mn>128</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">128\times 10</annotation></semantics> 个值

反向传播的计算模式遵循特定的顺序：

1.  在当前层计算梯度

1.  更新存储的梯度

1.  将错误信号传播到前一层

1.  重复直到达到输入层

对于批量处理，这些计算在批处理中的所有示例上同时执行，从而能够有效地使用矩阵运算和并行处理能力。

现代框架通过复杂的自动微分引擎处理这些计算。当你调用 PyTorch 中的 `loss.backward()` 时，框架会自动管理内存分配、操作调度以及在计算图上的梯度累积。系统跟踪哪些张量需要梯度，并在需要时通过梯度检查点优化内存使用，并安排操作以最大化硬件利用率。这种自动管理允许实践者专注于模型设计，而不是梯度计算实现的复杂细节。

### 权重更新和优化

训练神经网络需要通过迭代优化过程系统地调整权重和偏差，以最小化预测误差。在本节中，基于我们在生物到人工翻译中建立的计算基础，我们将探讨神经网络优化的核心机制，从基于梯度的参数更新到实际训练实现。

#### 参数更新算法

**梯度下降**是一种迭代优化算法，通过反复调整参数的方向来最小化*损失函数*，该方向由参数的*梯度*计算得出。

优化过程通过梯度下降 28 调整网络权重，这是一种系统方法，它实现了从我们的生物神经网络分析中推导出的学习原则。这个迭代过程计算每个权重对误差的贡献，并更新参数以减少损失，逐渐提高网络的预测能力。

基本更新规则结合了反向传播的梯度计算与参数调整：<semantics><mrow><msub><mi>θ</mi><mtext mathvariant="normal">new</mtext></msub><mo>=</mo><msub><mi>θ</mi><mtext mathvariant="normal">old</mtext></msub><mo>−</mo><mi>α</mi><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow> <annotation encoding="application/x-tex">\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta}L</annotation></semantics> 其中 <semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics> 代表任何网络参数（权重或偏差），<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics> 是学习率，而 <semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><mi>L</mi></mrow><annotation encoding="application/x-tex">\nabla_{\theta}L</annotation></semantics> 是通过反向传播计算出的梯度。

对于我们的 MNIST 示例，这意味着调整权重以提高数字分类的准确性。如果网络经常将“7”与“1”混淆，梯度下降将修改权重以更好地区分这些数字。学习率<semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics>29 控制调整幅度——过大值会导致超过最佳参数，而太小值会导致收敛缓慢。

尽管神经网络的损失地形高度非凸，具有多个局部最小值，但在实践中梯度下降可靠地找到有效的解决方案。涉及彩票假设（Frankle 和 Carbin 2018）、隐含偏差（Neyshabur 等人 2017）和过度参数化优势（Nakkiran 等人 2019）等概念的理论原因仍然是活跃的研究领域。对于实际的机器学习系统工程，关键洞察是具有适当的学习率、初始化和正则化的梯度下降可以持续训练神经网络以达到高性能。

#### 小批量梯度更新

神经网络在训练过程中通常同时处理多个示例，这种方法被称为小批量梯度下降。我们不是在更新每个单独的图像后更新权重，而是在更新之前计算一批示例的平均梯度。

对于大小为<semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics>的一批，损失梯度变为：<semantics><mrow><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mtext mathvariant="normal">batch</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>B</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi>∇</mi><mi>θ</mi></msub><msub><mi>L</mi><mi>i</mi></msub></mrow> <annotation encoding="application/x-tex">\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B \nabla_{\theta}L_i</annotation></semantics>

在我们的 MNIST 训练中，典型的批大小为 32，这意味着：

1.  通过正向传播处理 32 个图像

1.  计算所有 32 个预测的损失

1.  将所有 32 个示例的梯度平均

1.  使用这个平均梯度更新权重

**系统视角：批大小和硬件利用率**

**批量大小的权衡**：较大的批次可以提高硬件效率，因为矩阵运算可以以处理一个实例的计算成本处理多个实例。然而，批次中的每个实例都需要内存来存储其激活，这创造了一个基本的权衡：较大的批次更有效地使用硬件，但需要更多的内存。因此，可用内存成为批量大小的硬约束，反过来又影响硬件的利用效率。这种算法设计（批大小）与硬件能力（内存）之间的关系说明了为什么机器学习系统工程需要同时考虑这两个方面。

#### 迭代学习过程

完整的训练过程将前向传播、反向传播和权重更新组合成一个系统的训练循环。这个循环会重复，直到网络达到令人满意的性能或达到预定的迭代次数。

整个训练数据集的一次遍历称为一个 epoch30。对于 MNIST，有 60,000 个训练图像和 32 个批量大小的数据，每个 epoch 包含 1,875 个批次数迭代。训练循环结构如下：

1.  对于每个 epoch：

    +   打乱训练数据以防止学习顺序依赖的模式

    +   对于每个批次：

        +   执行前向传播

        +   计算损失

        +   执行反向传播

        +   使用梯度下降更新权重

    +   评估网络性能

在训练过程中，我们监控几个关键指标：

+   训练损失：最近批次的平均损失

+   验证准确率：在保留的测试数据上的性能

+   学习进度：网络改进的速度

对于我们的数字识别任务，我们可能会观察到网络在多个 epoch 的训练后，准确率从 10%（随机猜测）提高到超过 95%。

#### 收敛和稳定性考虑

成功实施神经网络训练需要关注几个关键的实际方面，这些方面对学习效果有显著影响。这些考虑因素架起了理论与实践之间的桥梁。

当机器学习模型学习到特定于*训练数据*的特定模式，而这些模式无法推广到*未见数据*时，就会发生**过拟合**，导致训练准确率高但测试性能差。

学习率选择可能是影响训练的最关键参数。对于我们的 MNIST 网络，学习率的选择极大地影响了训练动态。一个大的学习率 0.1 可能会导致不稳定的训练，其中损失振荡或爆炸，因为权重更新超出了最优值。相反，一个非常小的学习率 0.0001 可能会导致收敛极其缓慢，需要更多的 epoch 才能达到良好的性能。一个适中的学习率 0.01 通常在训练速度和稳定性之间提供了良好的平衡，使网络能够稳步进步，同时保持稳定的学习。

在训练过程中，收敛监控提供了关键反馈。随着训练的进行，我们通常观察到损失值稳定在某个特定值附近，表明网络正在接近局部最优。验证准确率也往往会达到平台期，这表明网络已经从数据中提取了大部分可学习的模式。训练和验证性能之间的差距可以揭示网络是否过拟合或对新示例泛化良好。在生产环境中监控模型的操作方面，包括检测模型退化性能漂移，这些方面在第十三章中得到了全面覆盖。

随着我们扩展神经网络训练，资源需求变得越来越重要。内存占用必须容纳模型参数和反向传播所需的中间计算。计算与批大小成线性关系，影响训练速度和硬件利用率。现代训练通常利用 GPU 加速，因此，对于实际实施来说，高效利用并行计算能力至关重要。

训练神经网络也带来了一些挑战。当网络对训练数据过于专业化时，就会发生过拟合，它在已见示例上表现良好，但在新示例上表现较差。梯度不稳定性可能表现为梯度消失或梯度爆炸，使得学习变得困难。批大小、可用内存和计算资源之间的相互作用通常需要仔细平衡，以在硬件约束内实现高效的训练。

**检查点：神经网络学习过程**

你现在已经完成了完整的训练周期——数学工具，它使得神经网络能够从数据中学习。在转向推理和部署之前，验证你的理解：

**正向传播：**

**损失函数：**

**反向传播：**

**优化：**

**完整的训练循环：**

**自测**：对于我们的 MNIST 网络（784→128→64→10），追踪在批大小为 32 的一次训练迭代中发生了什么：哪些矩阵相乘？什么被存储？需要多少内存？计算了哪些梯度？

*如果任何概念感觉不清楚，请回顾第 3.5.2 节（正向传播）、第 3.5.3 节（损失函数）、第 3.5.4 节（反向传播）或第 3.5.5 节（优化过程）。这些机制构成了理解训练与推理区别的下一个探索的基础。*

## 推理管道

在详细探讨了训练过程之后，我们现在转向神经网络的运营阶段。神经网络有两个不同的目的：在训练期间从数据中学习，在推理期间进行预测。虽然我们已经探讨了网络通过前向传播、反向传播和权重更新来学习的方式，但预测阶段的工作方式不同。在推理期间，网络使用其学习到的参数将输入转换为输出，而不需要学习机制。这个更简单的计算过程仍然需要仔细考虑数据如何通过网络流动以及系统资源如何被利用。理解预测阶段至关重要，因为它代表了神经网络如何实际部署来解决现实世界问题，从分类图像到生成文本预测。

### 生产部署和预测流程

神经网络的运营部署集中在推理上，这是使用训练好的模型对新数据进行预测的过程。与需要迭代参数更新和大量计算资源的训练不同，推理代表了在生产系统中提供价值的运营工作量。理解这两个阶段之间的基本差异对于设计高效的机器学习系统至关重要，因为每个阶段都对硬件、内存和软件架构提出了不同的要求。本节从系统性地比较训练开始，探讨将输入转换为预测的计算流程，来考察推理的核心特征。

这个阶段转换引入了一个关于模型适应性的重要约束，这对系统设计产生了重大影响。虽然训练好的模型通过学习到的统计模式在未见过的输入上表现出泛化能力，但学习到的参数在整个部署过程中保持固定。一旦训练结束，模型就应用其学习到的概率分布而不做修改。当运营数据分布与训练分布不同时，模型继续执行其固定的计算路径，而不考虑这种变化。考虑一个自动驾驶车辆感知系统：如果施工区域频率显著增加或部署中出现新的车辆配置，模型的响应将反映训练期间学习的统计模式，而不是适应演变的运营环境。机器学习系统适应能力并非来自运行时模型的修改，而是来自使用更新数据进行的系统重训练，这是一个在第八章中详细描述的故意工程过程。

#### 运营阶段差异

神经网络操作分为两个基本不同的阶段，这两个阶段对计算要求和系统约束产生了显著的不同。训练需要通过网络进行正向和反向传递来计算梯度并更新权重，而推理仅涉及正向传递计算。这种架构简化意味着在推理期间每个层只执行一组操作，使用学习到的权重将输入转换为输出，而不跟踪中间值以进行梯度计算，如图图 3.17 所示。

这些计算差异直接体现在硬件需求和部署策略中。训练集群通常使用高内存的 GPU31 和大量的冷却基础设施。推理部署优先考虑不同平台上的延迟和能效：移动设备使用低功耗的神经网络处理器（通常为 2-4W），边缘服务器部署专门的推理加速器 32，而云服务使用推理优化的实例，降低数值精度以提高吞吐量 33。每天处理数百万请求的生产推理系统需要复杂的基础设施，包括负载均衡、自动扩展和故障转移机制，这些在训练环境中通常是无需的。

![图片](img/file49.svg)

图 3.17：**推理与训练流程**：在推理过程中，神经网络仅利用学习到的权重进行正向传递计算，简化了数据流并降低了与训练相比的计算成本，因为训练需要正向和反向传递来更新权重。这种简化的流程使得训练好的模型能够高效地部署以进行实时预测。

参数冻结代表了训练和推理阶段之间另一个主要的区别。在训练过程中，权重和偏差持续更新以最小化损失函数。在推理过程中，这些参数保持固定，作为从训练数据中学习到的静态转换。这种参数冻结不仅简化了计算，还使得在训练期间无法实现的优化成为可能，例如权重量化或剪枝。

训练循环与推理传递之间的结构差异对系统设计有显著影响。训练在迭代循环中运行，通过多个批次的数据重复处理多个 epoch 来细化网络的参数。相比之下，推理通常只处理每个输入一次，通过单次正向传递生成预测。这种从迭代细化到单次预测的转变影响了我们为部署而构建系统的方式。

这些结构上的差异在训练和推理之间创造了显著不同的内存和计算需求。训练需要大量的内存来存储反向传播的中间激活、权重更新的梯度和优化状态。推理消除了这些内存密集型需求，只需要足够的内存来存储模型参数并计算单次前向传播。这种内存足迹的减少，加上更简单的计算模式，使得推理可以在从强大的服务器到资源受限的边缘设备更广泛的设备上高效运行。

通常情况下，训练阶段需要更多的计算资源和内存来进行学习，而推理过程则被优化以实现高效的预测。表 3.5 总结了训练和推理之间的关键差异。

表 3.5：**训练与推理对比**：神经网络从计算密集的训练阶段——需要带有更新参数的前向和反向传播——过渡到使用固定参数和仅前向传播的效率推理阶段。这种区别使得在资源受限的设备上部署成为可能，通过最小化预测期间的内存需求和计算负载。

| **方面** | **训练** | **推理** |
| --- | --- | --- |
| **计算流程** | 前向和反向传播，梯度计算 | 仅前向传播，直接从输入到输出 |
| **参数** | 持续更新的权重和偏差 | 固定/冻结的权重和偏差 |
| **处理模式** | 多个 epoch 的迭代循环 | 网络的单次遍历 |
| **内存需求** | 高 – 存储激活、梯度、优化器状态 | 低 – 仅存储模型参数和当前输入 |
| **计算需求** | 重 – 梯度更新，反向传播 | 较轻 – 仅矩阵乘法 |
| **硬件需求** | 用于高效训练的 GPU/专用硬件 | 可在更简单的设备上运行，包括移动/边缘设备 |

训练和推理阶段之间的这种鲜明对比突出了为什么系统架构在开发和部署环境之间往往存在显著差异。虽然训练需要大量的计算资源和专用硬件，但推理可以针对效率进行优化，并部署在更广泛的设备上。

训练和推理允许不同的架构优化。训练需要高精度的算术和反向传播计算，推动了具有灵活计算单元的专用硬件的采用。推理允许进行各种效率优化和利用更简单计算流的专用架构。这些差异解释了为什么专用推理处理器与通用训练硬件相比，可以实现更高的能效。

内存使用模式也差异很大：训练存储所有激活以进行反向传播（需要 2-3 倍更多的内存），而推理可以在使用后立即丢弃激活。

#### 端到端预测工作流程

在实际应用中实现神经网络需要完整的处理管道，这个管道不仅限于网络本身。这个管道，如图 3.18 所示，通过一系列不同的阶段将原始输入转换为有意义的输出，每个阶段对于系统的运行都是必不可少的。理解这个完整的管道对于深度学习系统的设计和部署提供了关键见解。

![图片](img/file50.svg)

图 3.18：**推理管道**：机器学习系统通过一系列连续的阶段将原始输入转换为最终输出——预处理、神经网络计算和后处理——每个阶段对于准确预测和部署都至关重要。这个管道强调了模型架构与实际应用所需的完整系统之间的区别。

从图中可以看出，深度学习系统作为混合架构运行，结合了传统的计算操作和神经网络计算。神经网络组件，专注于通过矩阵操作学习到的转换，只是更广泛的计算框架中的一个元素。这个框架包括输入数据的准备和网络输出的解释，这些过程主要依赖于传统的计算方法。

考虑数据如何在图 3.18 中通过管道流动：

1.  原始输入以原始形式到达，可能是图像、文本、传感器读数或其他数据类型

1.  预处理将这些输入转换为神经网络可消费的格式

1.  神经网络执行其学习到的转换

1.  网络的原始输出通常以数值形式出现

1.  后处理将这些输出转换为有意义的、可操作的结果

这个管道结构揭示了深度学习系统的一些关键特征。尽管神经网络在计算上非常复杂，但它仍然是一个更大系统中的组件。性能瓶颈可能出现在管道的任何阶段，而不仅仅是神经网络计算中。因此，系统优化必须考虑整个管道，而不仅仅是关注神经网络的操作。

这种架构的混合性质对系统实现有重大影响。虽然神经网络计算可能从专用硬件加速器中受益，但预处理和后处理操作通常在传统处理器上执行。这种计算在异构硬件资源上的分布是系统设计中的一个基本考虑因素。

### 数据预处理和归一化

预处理阶段将原始输入转换为适合神经网络计算格式的数据。虽然在理论讨论中经常被忽视，但这一阶段在现实世界数据和神经网络操作之间形成了一个关键的桥梁。以我们的 MNIST 数字识别示例来说：在我们之前设计的神经网络处理手写数字图像之前，它必须经过几个转换。手写数字的原始图像以各种格式、大小和像素值范围到达。例如，在图 3.19 中，我们可以看到数字的大小各不相同，甚至同一个人写的数字 6 也有不同的写法。

![图片](img/file51.png)

图 3.19：**手写数字的可变性**：现实世界的数据在风格、大小和方向上表现出显著的差异，这需要强大的预处理技术来保证可靠的机器学习性能。这些图像展示了数字识别的挑战，即使看似简单的输入也需要进行归一化和特征提取，才能被神经网络有效处理。来源：o. augereau。

预处理阶段通过传统的计算操作对这些输入进行标准化：

+   将图像缩放到所需的<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素尺寸，相机图像通常较大(r)。

+   像素值归一化从<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>255</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,255]</annotation></semantics>到<semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics>，大多数相机生成彩色图像。

+   将 2D 图像数组展平为 784 维向量，为神经网络做准备。

+   基本验证以确保数据完整性，确保网络预测正确。

预处理与神经网络计算的区别在于它依赖于传统的计算操作而不是学习到的转换。虽然神经网络通过训练学习识别数字，但预处理操作仍然是固定的、确定性的转换。这种区别对系统有重要的影响：预处理在传统的 CPU 上而不是在专门的神经网络硬件上运行，其性能特征遵循传统的计算模式。

预处理的有效性直接影响系统性能。不良的归一化可能导致准确性降低，不一致的缩放可能引入伪影，低效的实现可能造成瓶颈。理解这些影响有助于设计在现实条件下表现良好的鲁棒深度学习系统。

### 前向传递计算管道

推理阶段代表神经网络的操作状态，其中学习的参数被用来将输入转换为预测。与之前讨论的训练阶段不同，推理仅关注具有固定参数的前向计算。

#### 模型加载和设置

在处理任何输入之前，神经网络必须正确初始化以进行推理。这个初始化阶段包括将训练期间学习的模型参数加载到内存中。对于我们的 MNIST 数字识别网络，这意味着为每一层加载特定的权重矩阵和偏置向量。我们架构的确切内存需求如下：

+   第一隐藏层输入：

    +   权重矩阵：<semantics><mrow><mn>784</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>78</mn><mo>,</mo><mn>400</mn></mrow><annotation encoding="application/x-tex">784\times 100 = 78,400</annotation></semantics> 个参数

    +   偏置向量：100 个参数

+   第一到第二隐藏层：

    +   权重矩阵：<semantics><mrow><mn>100</mn><mo>×</mo><mn>100</mn><mo>=</mo><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">100\times 100 = 10,000</annotation></semantics> 个参数

    +   偏置向量：100 个参数

+   第二隐藏层到输出：

    +   权重矩阵：<semantics><mrow><mn>100</mn><mo>×</mo><mn>10</mn><mo>=</mo><mn>1</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">100\times 10 = 1,000</annotation></semantics> 个参数

    +   偏置向量：10 个参数

该架构的完整参数需求在下面的资源需求部分中详细说明。对于处理单个图像，这意味着为以下内容分配空间：

+   第一隐藏层激活值：100 个值

+   第二隐藏层激活值：100 个值

+   输出层激活值：10 个值

这种内存分配模式与训练阶段有显著差异，在训练阶段需要额外的内存用于梯度、优化器状态和反向传播计算。

在实际推理部署中，采用各种内存优化技术以减少资源需求同时保持可接受的准确性。系统可能将多个请求组合在一起以更好地利用硬件能力并满足响应时间要求。对于资源受限的部署，各种模型压缩方法有助于模型适应可用内存同时保持功能。

#### 推理前向传递执行

在推理过程中，数据通过网络的层使用初始化的参数传播。这个前向传播过程，虽然结构与训练时的对应过程相似，但具有不同的计算约束和优化。计算遵循从输入到输出的确定性路径，在每个层使用学习到的参数转换数据。

对于我们的 MNIST 数字识别网络，考虑每一层的精确计算。网络通过连续的转换处理一个表示为 784 维向量的预处理图像：

1.  第一隐藏层计算：

    +   输入转换：784 个输入与 78,400 个权重通过矩阵乘法结合

    +   线性计算：<semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mi>𝐱</mi><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{z}^{(1)} = \mathbf{x}\mathbf{W}^{(1)} + \mathbf{b}^{(1)}</annotation></semantics>

    +   激活：<semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)})</annotation></semantics>

    +   输出：100 维激活向量

1.  第二隐藏层计算：

    +   输入转换：100 个值与 10,000 个权重结合

    +   线性计算：<semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{z}^{(2)} = \mathbf{a}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}</annotation></semantics>

    +   激活函数: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">ReLU</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(2)} = \text{ReLU}(\mathbf{z}^{(2)})</annotation></semantics>

    +   输出: 100 维激活向量

1.  输出层计算:

    +   最终转换：100 个值与 1,000 个权重结合

    +   线性计算: <semantics><mrow><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><msup><mi>𝐖</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mi>𝐛</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{z}^{(3)} = \mathbf{a}^{(2)}\mathbf{W}^{(3)} + \mathbf{b}^{(3)}</annotation></semantics>

    +   激活函数: <semantics><mrow><msup><mi>𝐚</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mtext mathvariant="normal">softmax</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>𝐳</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{a}^{(3)} = \text{softmax}(\mathbf{z}^{(3)})</annotation></semantics>

    +   输出: 10 个概率值

表 3.6 展示了这些计算，虽然从数学上与训练时的正向传播相同，但显示了重要的操作差异：

表 3.6: **正向传播优化**：在推理过程中，神经网络通过仅保留当前层的激活并释放中间状态来优先考虑计算效率，这与训练时维护完整的激活历史以供反向传播不同。这种优化通过将资源集中在即时计算而不是梯度准备上，简化了输出生成过程。

| **特性** | **训练正向传播** | **推理正向传播** |
| --- | --- | --- |
| **激活存储** | 维护完整的激活历史以供反向传播 | 仅保留当前层的激活 |
| **内存模式** | 在正向传播过程中保留中间状态 | 层计算完成后释放内存 |
| **计算流程** | 结构化以准备梯度计算 | 优化以直接生成输出 |
| **资源配置文件** | 训练操作需要更高的内存要求 | 最小化内存占用以实现高效执行 |

这种简化的计算模式能够在保持网络学习能力的同时实现高效的推理。内存需求减少和计算流程简化使得推理特别适合部署在资源受限的环境中，如移动机器学习和微型机器学习。

#### 内存和计算资源

与训练相比，神经网络在推理过程中消耗的计算资源不同。在推理过程中，资源利用率主要集中于高效的正向传递计算和最小的内存开销。检查 MNIST 数字识别网络的特定要求揭示：

推理过程中的内存需求可以精确量化：

1.  静态内存（模型参数）：

    +   第一层：78,400 个权重 + 100 个偏置

    +   第二层：10,000 个权重 + 100 个偏置

    +   第三层：1,000 个权重 + 10 个偏置

    +   总计：89,610 个参数 (<semantics><mrow><mo>≈</mo><mn>358.44</mn></mrow><annotation encoding="application/x-tex">\approx 358.44</annotation></semantics> KB 在 32 位浮点精度下 34)

1.  动态内存（激活值）：

    +   第一层输出：100 个值

    +   第二层输出：100 个值

    +   第三层输出：10 个值

    +   总计：210 个值 (<semantics><mrow><mo>≈</mo><mn>0.84</mn></mrow><annotation encoding="application/x-tex">\approx 0.84</annotation></semantics> KB 在 32 位浮点精度下)

对于每个输入，计算需求遵循固定的模式：

+   第一层：78,400 次乘加操作

+   第二层：10,000 次乘加操作

+   输出层：1,000 次乘加操作

+   总计：每次推理 89,400 次乘加操作

这种资源配置文件与训练需求形成鲜明对比，训练时需要额外的梯度内存和反向传播的计算开销显著增加了资源需求。推理计算的预测性和简化的特性使得各种优化机会和高效的硬件利用成为可能。

#### 性能提升技术

推理计算的固定特性提供了在训练期间不可用的优化机会。一旦神经网络的参数被冻结，可预测的计算模式允许在内存使用和计算效率方面进行系统性的改进。

批处理大小选择是推理优化中的一个关键权衡。在训练过程中，为了稳定梯度计算，需要大批次，但推理提供了更多的灵活性。处理单个输入可以最小化延迟，使其对于需要即时响应的实时应用非常理想。然而，批处理可以通过更好地利用并行计算能力显著提高吞吐量，尤其是在 GPU 上。对于我们 MNIST 网络，考虑内存影响：处理单个图像需要存储 210 个激活值，而 32 个图像的批次需要 6,720 个激活值，但在并行硬件上可以快 32 倍处理图像。

推理过程中的内存管理可以比训练过程中更加高效。由于中间值仅用于正向计算，内存缓冲区可以仔细管理并重复使用。每一层的激活值只需存在到下一层计算完成即可。这使可能的情况下可以进行原地操作，从而减少总的内存占用。推理的固定性质允许进行精确的内存对齐和访问模式，这些模式针对底层硬件架构进行了优化。

在推理过程中，针对硬件的优化变得尤为重要。在 CPU 上，计算可以组织起来以最大化缓存利用率，并利用并行处理能力，即同时将相同的操作应用于多个数据元素。GPU 部署得益于优化的矩阵乘法例程和高效的内存传输模式。这些优化不仅超越了纯计算效率，还可以显著影响功耗和硬件利用率，这在实际部署中是关键因素。

推理的可预测性也使得像降低数值精度这样的优化成为可能。虽然训练通常需要全浮点精度以保持稳定的学习，但推理通常可以在保持可接受精度的同时使用较低的精度。对于我们 MNIST 网络，这样的优化可以显著减少内存占用，同时提高计算效率。

这些优化原则，虽然通过我们简单的 MNIST 前馈网络进行说明，但仅代表了神经网络优化的基础。更复杂的架构引入了额外的考虑和机会，包括专门为空间数据处理、顺序计算和基于注意力的计算模式设计的架构。这些架构变化及其优化在第四章、第十章和第九章中进行了探讨。

### 输出解释和决策制定

将神经网络输出转换为可操作的预测需要回归到传统的计算范式。正如预处理将现实世界数据连接到神经计算一样，后处理将神经输出重新连接到传统的计算系统。这完成了我们之前检查的混合计算管道，其中神经计算和传统计算操作协同工作以解决现实世界问题。

后处理的复杂性不仅限于简单的数学变换。现实世界的系统必须处理不确定性、验证输出，并与更大的计算系统集成。在我们的 MNIST 示例中，一个数字识别系统可能不仅需要最可能的数字，还需要置信度度量来确定何时需要人工干预。这引入了额外的计算步骤：置信度阈值、二级预测检查和错误处理逻辑，所有这些都在传统的计算框架中实现。

后处理的计算需求与神经网络推理大相径庭。虽然推理受益于并行处理和专用硬件，但后处理通常在传统的 CPU 上运行，遵循顺序逻辑模式。这种回归到传统的计算既带来了优势也带来了限制。操作比神经计算更灵活且更容易修改，但如果不仔细实现，它们可能会成为瓶颈。例如，为一批预测计算 softmax 概率需要与神经网络层的矩阵乘法不同的优化策略。

系统集成考虑通常主导后处理设计。输出格式必须与下游系统要求匹配，错误处理必须与更广泛系统协议保持一致，性能必须满足系统级约束。在一个完整的邮件分拣系统中，后处理阶段不仅必须识别数字，还必须将这些预测格式化为分拣机械，适当地处理不确定性情况，并保持与物理邮件流动速度相匹配的处理速度。

这种回归到传统的计算范式完成了深度学习系统的混合特性。正如预处理为神经计算准备现实世界数据一样，后处理将神经输出调整为现实世界的应用。理解这种混合特性，即神经计算与传统计算之间的相互作用，对于设计和实现有效的深度学习系统至关重要。

我们现在已经涵盖了神经网络的完整生命周期：从架构设计到训练动态再到推理部署。每个概念——神经元、层、前向传播、反向传播、损失函数、优化——都代表了一个拼图的一部分。但在实践中，这些部分是如何组合在一起的？以下检查点帮助您验证对这些组件如何整合到完整系统中的理解，之后我们将考察一个历史案例研究，该研究将这些原则在实际部署中付诸实践。

**检查点：完整的神经网络系统**

在检查这些概念如何在现实世界的部署中整合之前，验证您对完整神经网络生命周期的理解：

**跨阶段集成：**

**从训练到部署：**

**推理与部署：**

**系统集成：**

**端到端流程：**

**自我测试**：对于一个在生产中部署的 MNIST 数字分类器（784→128→64→10）：（1）解释为什么训练此模型需要约 12GB GPU 内存，而推理只需要约 400MB。（2）追踪一个数字图像从相机捕获到预处理、推理和后处理的最终预测。（3）识别在每秒处理 100 个图像的实时系统中可能出现的瓶颈。（4）描述您如何在生产中监控模型退化。

*以下案例研究展示了这些概念如何在大规模部署的生产系统中整合。注意观察架构选择、训练策略和部署约束如何结合以创建一个工作的机器学习系统。*

## 案例研究：美国邮政服务数字识别

我们从第一性原理探讨了神经网络——神经元如何计算，层如何转换数据，训练如何调整权重，以及推理如何做出预测。这些概念可能看起来很抽象，但它们都在第一个大规模神经网络部署中汇集在一起：美国邮政服务的手写数字识别系统。这个历史案例说明了我们所学的数学原理如何转化为实际工程决策、系统权衡和现实世界的性能约束。

神经网络的理沦基础在解决大规模现实世界问题的系统中得到了具体体现。20 世纪 90 年代部署的美国邮政服务手写数字识别系统就是这种从理论到实践的例证。这一早期的生产部署确立了现代机器学习系统中许多仍然相关的原则：稳健的预处理管道的重要性、自动化决策中置信度阈值的需要，以及在不断变化的现实世界条件下维持系统性能的挑战。虽然今天的系统在更强大的硬件上部署了更加复杂的架构，但研究这个基础案例研究揭示了本章早期建立的优化原则如何结合在一起创建生产系统——这些经验教训从 20 世纪 90 年代的邮件分类扩展到 2025 年的边缘人工智能部署。

### 邮件分类挑战

美国邮政服务（USPS）每天处理超过一亿件邮件，每件邮件都需要根据手写的 ZIP 代码进行准确的路线规划。在 20 世纪 90 年代初，人工操作员主要执行这项任务，使其成为世界上最大的手动数据录入操作之一。通过神经网络自动化这一过程代表了人工智能早期和成功的大规模部署，体现了许多神经计算的核心原则。

这个任务的复杂性变得显而易见：ZIP 代码识别系统必须处理在多种条件下捕获的手写数字图像——不同的书写风格、笔的类型、纸张颜色和环境因素（图 3.20）。它必须在毫秒内做出准确的预测，以保持邮件处理速度。识别错误可能导致邮件误投，造成重大延误和成本。这个现实世界的限制意味着系统不仅需要高精度，还需要可靠的预测置信度度量，以确定何时需要人工干预。

![图片](img/file52.jpg)

图 3.20：**手写数字的变异性**：现实世界中的手写数字在笔画宽度、倾斜度和字符形成方面存在显著差异，这对 USPS 使用的自动化识别系统等系统构成了挑战。这些示例说明了在光学字符识别（OCR）任务中实现高精度所需的鲁棒特征提取和模型泛化。

这个具有挑战性的环境提出了涵盖我们讨论的神经网络实现各个方面的要求，从生物启发到实际部署考虑。系统的成功或失败不仅取决于神经网络的准确性，还取决于从图像捕获到最终分类决策的整个流程。

### 工程流程和设计决策

USPS 数字识别系统的发展需要在每个阶段都进行仔细考虑，从数据收集到部署。这个过程说明了神经网络的理论原理如何转化为实际工程决策。

数据收集带来了第一个主要挑战。与受控的实验室环境不同，邮政设施需要处理具有极大多样性的邮件件。训练数据集必须捕捉这种多样性。不同年龄、教育背景和书写风格的人所写的数字只是挑战的一部分。信封的颜色和质感各不相同，图像是在不同的光照条件和方向下捕获的。这项广泛的数据收集工作后来为我们在示例中使用的 MNIST 数据库的创建做出了贡献。

网络架构设计需要平衡多个约束条件。虽然更深层的网络可能实现更高的准确性，但它们也会增加处理时间和计算需求。处理单个数字的<semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">28\times 28</annotation></semantics>像素图像需要在严格的时间限制内完成，同时还要在可用的硬件上可靠运行。网络必须在不同的条件下保持一致的准确性，从书写良好的数字到匆忙的涂鸦。

训练网络引入了额外的复杂性。系统不仅需要在测试数据集上实现高准确性，还需要在无尽多样的现实世界书写风格上实现高准确性。仔细的预处理将输入图像标准化，以考虑大小和方向的变化。数据增强技术增加了训练样本的多样性。团队在不同的人口群体中验证了性能，并在实际操作条件下进行了测试，以确保稳健的性能。

工程团队面临着一个关于置信度阈值的关键决策。将这些阈值设置得太高会将太多邮件件路由给人工操作员，从而违背了自动化的目的。将这些阈值设置得太低则可能存在投递错误的风险。解决方案是通过分析正确与错误预测的置信度分布得出的。这种分析确定了优化自动化率和错误率之间权衡的阈值，确保高效运行的同时保持可接受的准确性。

### 生产系统架构

跟随一封邮件通过 USPS 识别系统，可以说明我们讨论的概念如何整合成一个完整的解决方案。从物理邮件件到分拣信件的旅程展示了传统计算、神经网络推理和物理机械之间的相互作用。

当信封到达成像站时，过程开始。高速相机以每秒超过几件邮件（例如，10 件）的速度捕捉 ZIP 代码区域。此图像获取过程必须适应不断变化的信封颜色、书写风格和环境条件。尽管操作速度很快，但系统必须保持一致的图像质量，因为运动模糊和适当的照明带来了重大的工程挑战。

预处理将这些原始相机图像转换为适合神经网络分析的形式。系统必须定位 ZIP 代码区域，分割单个数字，并对每个数字图像进行归一化。这一阶段采用传统的计算机视觉技术：图像阈值化适应信封背景颜色，连通分量分析识别单个数字，尺寸归一化产生标准的 28×28 像素图像。速度仍然至关重要；这些操作必须在毫秒内完成，以保持吞吐量。

神经网络随后处理每个归一化的数字图像。经过训练的网络，具有我们之前详细说明的 89,610 个参数，执行正向传播以生成预测。每个数字通过两个各有 100 个神经元的隐藏层，最终产生十个输出值，代表数字的概率。这一推理过程虽然计算密集，但得益于我们在上一节中讨论的优化。

后处理将这些神经网络输出转换为分类决策。系统对每个数字预测应用置信度阈值。一个完整的 ZIP 代码需要所有五个数字都高度置信，一个不确定的数字会将整个邮件标记为需要人工审查。当置信度达到阈值时，系统将分类指令传输到机械系统，这些系统将物理地将邮件件引导到其适当的分类箱中。

整个管道在严格的时序约束下运行。从图像捕获到分类决策，处理必须在邮件件到达分类点之前完成。系统同时维护多个邮件件在各个管道阶段，需要计算和机械系统之间的仔细同步。这种实时操作说明了我们在推理和后处理中讨论的优化为什么在实际应用中变得至关重要。

### 性能结果和运营影响

基于神经网络的 ZIP 代码识别的实施改变了美国邮政服务（USPS）的邮件处理操作。到 2000 年，全国多个设施采用了这项技术，每天处理数百万件邮件。这一实际部署展示了神经网络系统在关键任务应用中的潜力和局限性。

性能指标揭示了验证许多基本原理的有趣模式。系统在清晰书写的数字上达到了最高的准确率，这些数字与训练数据中的类似。然而，性能因现实世界因素而显著变化。光照条件影响了预处理的有效性。不寻常的书写风格有时会混淆神经网络。环境振动也可能影响图像质量。这些挑战导致了物理系统和神经网络管道的持续改进。

经济影响是显著的。在自动化之前，手动分拣需要操作员以平均每秒处理一件的速度读取和输入 ZIP 代码。神经网络系统以十倍于此的速度处理物品，同时降低了劳动力成本和错误率。然而，该系统并没有完全消除人工操作员；他们的角色转变为处理不确定案例和维护系统性能。这种结合人工和人类智能的混合方法成为其他自动化项目的典范。

系统还揭示了在生产环境中部署神经网络的宝贵经验。训练数据质量至关重要；网络在训练集中代表良好的数字风格上表现最佳。定期重新训练有助于适应不断变化的书写风格。维护需要硬件专家和深度学习专家，引入了新的运营考虑。这些见解影响了神经网络在其他工业应用中的后续部署。

研究人员发现，这种实现展示了理论原则如何转化为实际约束。神经网络生物启发的数字识别提供了基础，但成功的部署需要仔细考虑系统级因素：处理速度、错误处理、维护需求和与现有基础设施的集成。这些经验教训继续为现代深度学习部署提供信息，其中类似的规模、可靠性和集成挑战仍然存在。

### 关键工程教训和设计原则

美国邮政服务 ZIP 代码识别系统展示了从生物启发到实际神经网络部署的历程。它展示了从预处理到推理再到后处理的神经网络基本原理如何结合解决现实世界问题。

系统的开发展示了理解理论基础和实践考虑的重要性。虽然生物视觉系统可以轻松处理手写数字，但将这种能力转化为人工系统需要仔细考虑网络架构、训练程序和系统集成。

这种早期大规模神经网络部署的成功帮助确立了我们现在认为是标准的许多实践：详尽训练数据的重要性、信心指标的需求、预处理和后处理的作用以及系统级优化的关键性。

USPS 系统所展示的原则——稳健的预处理、基于信心的决策以及人机混合工作流程——在现代部署中仍然是基础性的，尽管规模和复杂性发生了巨大变化。当时 USPS 部署的网络在专用硬件上以每秒 10 张的速度处理约 10 万个参数的图像，消耗 50-100W 的功率，而今天的移动设备部署的模型具有 1-1000 万个参数，每秒处理 30 多帧图像，用于实时视觉任务，在神经处理器上消耗的功率小于 2W。2025 年的边缘 AI 系统——从智能手机面部识别到自动驾驶车辆感知——面临着类似的挑战，即在准确性和计算约束之间取得平衡，但运行在远为紧张的功率预算（毫瓦与瓦特）和更严格的延迟要求（毫秒与数十毫秒）之下。核心的系统工程原则保持不变：理解数学运算使硬件-软件协同设计成为可能，预处理管道决定了对现实世界变化的鲁棒性，而信心阈值将需要人工判断的情况与自动化处理的情况分开。因此，这个历史案例研究不仅提供了历史背景，还为从云到边缘到微型设备的整个范围内现代机器学习系统部署的推理提供了一个模板。

## 深度学习与 AI 三角形

我们在本章中探讨的神经网络概念直接映射到控制所有深度学习系统的 AI 三角形框架。这种联系阐明了为什么深度学习需要对计算架构和系统设计原则进行如此根本性的重新思考。

**算法**：我们已涵盖的数学基础——前向传播、激活函数、反向传播和梯度下降——定义了深度学习系统的算法核心。我们做出的架构选择（层深度、神经元数量、连接模式）直接决定了计算复杂度、内存需求和训练动态。每个激活函数的选择，从 ReLU 的计算效率到 sigmoid 的饱和梯度，都代表了具有深远系统影响的算法决策。区分神经网络与经典方法的层次特征学习正是源于这些算法构建块，但成功在很大程度上取决于其他两个三角形组件。

**数据**：学习过程完全依赖于标记数据来计算损失函数并通过反向传播指导权重更新。我们的 MNIST 示例展示了数据质量、分布和规模如何直接决定网络性能——算法保持不变，但数据特征决定了学习是否成功。从手动特征工程到自动表示学习的转变并没有消除数据依赖性；它将挑战从设计特征转变为整理能够捕捉现实世界模式全部复杂性的数据集。数据预处理、增强和验证策略成为算法设计决策，这些决策塑造了整个学习过程。

**基础设施**：前向传播和反向传播所需的巨大矩阵乘法数量揭示了为什么专门的硬件基础设施对于深度学习成功变得至关重要。我们探讨的内存带宽限制、有利于 GPU 架构的并行计算模式以及训练与推理的不同计算需求，都源自我们研究的数学运算。从 CPU 到 GPU 再到专门的 AI 加速器的演变，直接响应了神经网络算法固有的计算模式。理解这些数学基础使工程师能够就硬件选择、内存层次结构设计和分布式训练策略做出明智的决策。

这三个组件的相互依存性通过我们章节的进展而显现：算法定义了必要的计算，数据决定了这些计算是否能够学习有意义的模式，基础设施决定了系统是否能够在规模上高效执行。神经网络之所以成功，并不是因为任何单个组件有所改进，而是因为这三个领域的进步是一致的——更复杂的算法、更大的数据集和专门的硬件产生了一种协同效应，从而改变了人工智能。

这种 AI 三角形的视角解释了为什么深度学习工程需要超越传统软件开发的系统思维。不考虑其他组件而优化任何单个组件会导致次优结果：最优雅的算法在没有高质量数据的情况下失败，最佳数据集在没有足够的计算基础设施的情况下仍然无法使用，最强大的硬件如果没有能够有效从数据中学习的算法，将一无所获。

## 谬误与陷阱

深度学习代表了从显式编程到从数据学习的范式转变，这产生了关于何时以及如何应用这些强大但复杂的系统的独特误解。神经网络数学基础和统计性质往往导致对其能力、局限性和适当用例的误解。

**谬误：** *神经网络是“黑盒”，无法理解或调试。*

尽管神经网络缺乏传统算法的显式规则基础透明度，但多种技术能够理解和调试其行为。激活可视化揭示了神经元对哪些模式做出响应，梯度分析显示了输入如何影响输出，而注意力机制突出了哪些特征影响决策。层级相关性传播追踪决策路径通过网络，而消融研究确定了关键组件。难以理解的感觉通常源于试图通过传统的编程范式而不是统计和可视化分析方法来理解神经网络。现代可解释性工具提供了对网络行为的洞察，尽管这确实不同于逐行代码调试。

**谬误：** *深度学习消除了对领域专业知识和仔细的特征工程的需求。*

自动特征学习的承诺导致了这样的误解，即深度学习可以独立于领域知识运作。实际上，成功的深度学习应用需要广泛的领域专业知识来设计适当的架构（例如，对于空间数据使用卷积层，对于序列使用循环结构），选择有意义的训练目标，创建代表性的数据集，并在特定情境下解释模型输出。USPS 数字识别系统之所以成功，正是因为它结合了邮政服务关于邮件处理、数字书写模式和操作限制的专业知识。领域知识指导着关于数据增强策略、验证指标和部署要求的关键决策，这些决策决定了实际应用的成功。

**陷阱：** *对于可以用更简单方法解决的问题使用复杂的深度学习模型。*

团队经常部署复杂的神经网络来完成可以用线性模型或决策树解决的问题，这引入了不必要的复杂性、计算成本和维护负担。一个训练时间只需毫秒的线性回归模型，在数据有限或关系真正线性时，可能比需要数小时训练的神经网络表现更好。在采用深度学习之前，使用简单模型建立基线性能。如果一个逻辑回归模型在分类任务上达到了 95%的准确率，那么神经网络带来的边际改进很少能证明增加的复杂性是合理的。将深度学习保留用于那些表现出层次结构模式、非线性关系或高维交互的问题，这些是简单模型无法捕捉的。

**陷阱：** *在不理解潜在数据分布的情况下训练神经网络。*

许多从业者将神经网络训练视为一种机械过程，通过标准架构输入数据，忽略了决定成功的关键数据特征。在不平衡数据集上训练的网络，除非通过重采样或损失加权来解决，否则在少数类别上表现将不佳。非平稳分布需要持续的重训练或自适应机制。异常值可能会主导梯度更新，防止收敛。USPS 系统在达到生产就绪性能之前，需要对数字频率分布、书写风格变化和图像质量因素进行仔细分析。成功的训练需要彻底的探索性数据分析、对统计特性的理解，以及在训练过程中持续监控数据质量指标。

**陷阱：** *假设研究级模型可以直接部署到生产系统中，而不考虑系统级因素。*

许多团队将模型开发视为与系统部署分开，当研究原型遇到生产限制时会导致失败。一个在干净数据集上实现出色准确率的神经网络，当与实时数据管道、遗留数据库或分布式服务基础设施集成时可能会失败。生产系统需要考虑延迟预算、内存限制、并发用户负载和容错机制，这些在研究环境中很少出现。从研究代码到生产系统的转变需要仔细关注数据预处理管道、模型序列化格式、服务基础设施的可扩展性和监测系统，以检测性能下降。成功的部署需要数据科学和系统工程团队早期合作，以将模型需求与操作限制相一致。

## 摘要

神经网络通过用从数据中学习模式的自适应系统取代基于规则的编程，改变了计算方法。在本章中探索的从生物到人工神经元映射的基础上，这些系统创建了处理复杂信息并通过经验提高性能的实用实现。

神经网络架构展示了层次化处理，其中每一层从原始数据中逐步提取更抽象的模式。训练通过迭代优化调整连接权重以最小化预测误差，而推理则将学习到的知识应用于对新数据的预测。这种学习阶段和应用阶段的分离，为计算资源、内存使用和处理延迟等系统设计部署策略创造了不同的系统需求。

本章通过全连接架构建立了数学和系统含义。在这里探索的多层感知器展示了通用函数逼近。只要有足够的神经元和适当的权重，这样的网络理论上可以学习任何连续函数。这种数学的普遍性伴随着计算成本。以我们的 MNIST 示例来说：一个 28×28 像素的图像包含 784 个输入值，一个全连接网络独立地处理每个像素，仅在第一层就学习到 61,400 个权重（784 个输入 × 100 个神经元）。相邻像素高度相关，而远距离像素很少交互。全连接架构在学习无关的长距离关系上消耗了计算资源。

**关键要点**

+   神经网络用从数据中通过分层处理架构发现的自适应模式替换了手工编码的规则。

+   全连接网络提供了通用逼近能力，但通过平等对待所有输入关系而牺牲了计算效率。

+   训练和推理代表了不同的操作阶段，具有不同的计算需求和系统设计要求。

+   完整的处理管道将传统计算与神经计算集成到预处理、推理和后处理阶段。

+   系统级考虑——从激活函数的选择到批量大小配置再到网络拓扑——直接决定了在云、边缘和微型设备上的部署可行性。

+   专用架构（CNNs、RNNs、Transformers）将问题结构编码到网络设计中，在完全连接的替代方案上实现了显著的效率提升。

实际问题表现出通用全连接网络无法有效利用的结构：图像具有空间局部性，文本具有序列依赖性，图具有关系模式，时间序列数据具有时间动态性。这种结构盲点造成了三个关键问题：计算浪费（学习不存在的关联关系）、数据效率低下（需要更多的训练示例来学习可以结构化编码的模式），以及可扩展性差（随着输入维度的增加，参数数量激增）。

下一章（第四章）通过引入直接将问题结构编码到网络设计中的专用架构来解决这些限制。卷积神经网络通过限制连接和权重共享，利用空间局部性进行视觉任务，通过更少的参数（10-100×）实现了最先进的性能。循环神经网络通过隐藏状态捕获序列数据的时间依赖性，尽管序列处理创造了并行化挑战。变换器通过注意力机制实现序列的并行处理，革命性地改变了自然语言处理，同时也引入了新的内存扩展挑战。

每一项架构创新都带来了系统工程的权衡，这些权衡直接建立在本章建立的基础之上。卷积层需要与全连接层不同的内存访问模式，循环网络面临不同的并行化约束，而注意力机制创造了新的计算瓶颈。数学运算仍然是我们在研究中学习的矩阵乘法和非线性激活，但它们的组织方式改变了系统需求。

理解这些专业架构代表着在机器学习系统工程中的自然下一步——将我们已掌握的前向传播、梯度下降和激活函数的原则应用于旨在计算效率和特定问题结构的架构中。随着我们探索如何构建不仅学习效果良好，而且在现实世界计算系统约束下也能学习的神经网络，从生物灵感到数学公式再到系统实现的过程持续进行。

* * *
