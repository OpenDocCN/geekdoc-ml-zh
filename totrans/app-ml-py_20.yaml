- en: Ridge Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_ridge_regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is a tutorial for / demonstration of **Ridge Regression**.
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lectures on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Machine Learning](https://youtu.be/zOUM_AnI1DQ?si=wzWdJ35qJ9n8O6Bl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression](https://youtu.be/0fzbyhWiP84)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ridge Regression](https://youtu.be/pMGO40yXZ5Y?si=ygJAheyX-v2BmSiR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LASSO Regression](https://youtu.be/cVFYhlCCI_8?si=NbwIDaZj30vxezn2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Norms](https://youtu.be/JmxGlrurQp0?si=vuF1TXDbZkyRC1j-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lectures are all part of my [Machine Learning Course](https://youtube.com/playlist?list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&si=XonjO2wHdXffMpeI)
    on YouTube with linked well-documented Python workflows and interactive dashboards.
    My goal is to share accessible, actionable, and repeatable educational content.
    If you want to know about my motivation, check out [Michael‚Äôs Story](https://michaelpyrcz.com/my-story).
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow, demonstration of ridge regression and comparison to
    linear regression for machine learning-based predictions. Why start with linear
    regression?
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is the simplest parametric predictive machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learn about training machine learning models with an analytical solution
    calculated from the derivative of training MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get‚Äôs us started with the concepts of loss functions and norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have access to analytics expressions for confidence intervals for model uncertainty
    and hypothesis tests for parameter significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why cover ridge regression next?
  prefs: []
  type: TYPE_NORMAL
- en: Some times linear regression is not simple enough and we actually need a simpler
    model!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce the concept of model regularization and hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs some basic details about predictive machine learning ridge regression
    models, let‚Äôs start with linear regression first and build to ridge regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d82dccdcda485413554e49d73e4d1fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/175e41e10e74a46b4a56258ccdfb94c0.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b25cdf17fb10f18b362f50ba655df92b.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With ridge regression we add a hyperparameter, \(\lambda\), to our minimization,
    with a shrinkage penalty term, \(\sum_{j=1}^m b_{\alpha}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: As a result, ridge regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: lambda does not include the intercept, \(b_0\).'
  prefs: []
  type: TYPE_NORMAL
- en: The \(\lambda\) is a hyperparameter that controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a291289b4f4b55ba9adbb7538140a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing error components and expected test square error for ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the solution approaches linear regression, there
    is no bias (relative to a linear model fit), but the model variance is likely
    higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases,
    the model becomes simpler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the model parameters \(b_1,\ldots,b_m\) shrink
    to 0.0 and the model predictions approaches the training data response feature
    mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a variety of ridge regression models for various \(\lamda\) values that
    will be calculated in the workflow below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f26df3583b9f434159b8bbb15cc292b.png)'
  prefs: []
  type: TYPE_IMG
- en: Ridge regression models with low to high \(\lambda\) hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: Train / Test Data Split for Cross Validation-based Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The available data is split into training and testing subsets.
  prefs: []
  type: TYPE_NORMAL
- en: in general 15-30% of the data is withheld from training to apply as testing
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: testing data selection should be fair, the same difficulty of predictions (offset/different
    from the training data) [Fair Train-test Split Paper](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    (Salazar et al., 2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are various other methods including train, validation and testing splits,
    and k-fold cross validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamentally, all these methods proceed by training models with training data
    and testing model accuracy over withheld testing data for a range of hyperparameters.
    Then the hyperparameters are selected that minimize error with the withheld testing
    dataset, this is hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: after hyperparameter tuning the model is retrained with all of the data with
    the selected hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the train, validation and testing approach then uses a 2nd withheld subset of
    the dataset to check the tuned model with data not applied for model training
    nor model tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following workflow we will use the train and test approach for hyperparameter
    tuning. Here‚Äôs more details and a summary of the related concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Model Parameter Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training data is applied to train the model parameters such that the model
    minimizes mismatch with the training data
  prefs: []
  type: TYPE_NORMAL
- en: it is common to use **mean square error** (known as a **L2 norm**) as a loss
    function summarizing the model mismatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**minimizing the loss function** for simple models an analytical solution may
    be available, but for most machine this requires an iterative optimization method
    to find the best model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs the derivation of the analytical solution, starting with the loss function
    with, least squares (left) and regularization (right) terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha
    = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m
    b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: let‚Äôs convert this into matrix notation for convenience,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = (ùë¶‚àíùëã\beta)^ùëá (ùë¶‚àíùëã\beta)+\lambda
    \beta^ùëá \beta \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\beta\) is a vector of the model parameters, \(y\) is a vector of response
    feature values, and \(X\) is a matrix of predictor feature values, both over the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: We can expand the quadratic term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = ùë¶‚àí2ùëã^ùëá ùë¶\beta+(ùëã^ùëá ùëã) \beta^ùëá
    \beta+\lambda \beta^ùëá \beta \]
  prefs: []
  type: TYPE_NORMAL
- en: We take the partial derivative with respect to the model parameters and set
    it equal to 0.0,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right)
    = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can further simplify with minor adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2(ùëã^ùëá ùëã) \beta + 2 \lambda \beta = 2ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Divide both sides by 2,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (ùëã^ùëá ùëã) \beta + \lambda \beta = ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Group the common terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (ùëã^ùëá ùëã+ \lambda I) \beta =ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I\) is an identity matrix. Now we can solve for our ridge regression
    parameters,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùõΩ=\left(ùëã^ùëá ùëã+\lambda I \right)^{-1} ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, \(ùëã^ùëá ùëã+\lambda I\) is generally invertible, so this is solvable.
  prefs: []
  type: TYPE_NORMAL
- en: This process is repeated over a range of model complexities specified by hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation of Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interpretation and motivation for ridge regression,
  prefs: []
  type: TYPE_NORMAL
- en: when \(ùëö \ge ùëõ\), linear regression is ill-posed problem, i.e., a problem with
    more than 1 solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for ill-posed problems we introduce some constraints or regularization to limit
    the solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3b29636566b2ebc1625773201be9a303.png)'
  prefs: []
  type: TYPE_IMG
- en: Ill-posed linear regression model (left), and well-posed with additional shrinkage
    constraint ridge regression (right).
  prefs: []
  type: TYPE_NORMAL
- en: Model Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The withheld testing data is retrieved and loss function (usually the **L2 norm**
    again) is calculated to summarize the error over the testing data
  prefs: []
  type: TYPE_NORMAL
- en: this is repeated over the range of specified hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the model complexity / hyperparameters that minimize the loss function / error
    summary in testing is selected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is known are model hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Model Overfit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More model complexity / flexibility than can be justified with the available
    data, data accuracy, frequency and coverage
  prefs: []
  type: TYPE_NORMAL
- en: Model explains ‚Äúidiosyncrasies‚Äù of the data, capturing data noise/error in the
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High accuracy in training, but low accuracy in testing / real-world use away
    from training data cases ‚Äì poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model error in testing is increasing while the model error in training
    is decreasing, this is an indicator of an overfit model.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a function to streamline the addition specified percentiles and
    major and minor gridlines to our plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time). Also, in this case make
    sure to place the required (see below) data file in this working directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This
    dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: density (\(g/cm^{3}\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (volume %)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, the dataset is synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Train-Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity we apply a random train-test split with the train_test_split
    function from scikit-learn package, model_selection module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the DataFrame is useful first check of the data.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: we have a custom function to preview the training and testing DataFrames side-by-side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 1.778580 | 11.426485 |'
  prefs: []
  type: TYPE_TB
- en: '| 101 | 2.410560 | 8.488544 |'
  prefs: []
  type: TYPE_TB
- en: '| 88 | 2.216014 | 10.133693 |'
  prefs: []
  type: TYPE_TB
- en: '| 79 | 1.631896 | 12.712326 |'
  prefs: []
  type: TYPE_TB
- en: '| 58 | 1.528019 | 16.129542 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 59 | 1.742534 | 15.380154 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.710628 |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 1.552713 | 14.131878 |'
  prefs: []
  type: TYPE_TB
- en: '| 92 | 1.762359 | 11.154896 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 1.885087 | 9.403056 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum
    in a nice data table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 78.000000 | 78.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 1.739027 | 12.501465 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 0.302510 | 3.428260 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 0.996736 | 3.276449 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.410560 | 21.660179 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 27.000000 | 27.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 1.734710 | 12.380796 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 0.247761 | 2.916045 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 1.067960 | 7.894595 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.119652 | 18.133771 |'
  prefs: []
  type: TYPE_TB
- en: Visualize the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs check the consistency and coverage of training and testing with histograms
    and scatter plots.
  prefs: []
  type: TYPE_NORMAL
- en: check to make sure the train and test data cover the range of possible feature
    combinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ensure we are not extrapolating beyond the training data with the testing cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png](../Images/019f46cd3338e55a8b84bcd57a899945.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first calculate the linear regression model. We use scikit learn and then
    extend the same workflow to ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: we are building a model, \(\phi = f(\rho)\), where \(\phi\) is porosity and
    \(\rho\) is density.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we could also say, we have ‚Äúporosity regressed on density‚Äù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our model has this specific equation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi = b_1 \times \rho + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png](../Images/5bc6600437501d419bf56af2e1dc727d.png)'
  prefs: []
  type: TYPE_IMG
- en: You may have noticed the additional reshape operation applied to the predictor
    feature in the predict function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is needed because scikit-learn assumes more than one predictor feature;
    therefore, expects a 2D array of samples (rows) and features (columns), but we
    have only a 1D vector.
  prefs: []
  type: TYPE_NORMAL
- en: the reshape operation turns the 1D vector into a 2D vector with only 1 column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Regression Model Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs run some quick model checks. Much more could be done, but I limit this
    for brevity here.
  prefs: []
  type: TYPE_NORMAL
- en: see the Linear Regression chapter for more information and checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png](../Images/208a933176542d3185b7f4f4c9a1bf61.png)'
  prefs: []
  type: TYPE_IMG
- en: Ridge Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs replace the scikit-learn linear regression method with the scikit-learn
    ridge regression method.
  prefs: []
  type: TYPE_NORMAL
- en: note, we must now set the \(\lambda\) hyperparameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in scikit-learn the hyperparameter(s) is(are) set with the instantiation of
    the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png](../Images/05ecf578788df580a4c1f1f25e9ffcb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs repeat the simple model checks that we applied with our linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png](../Images/2e6dac5a06cdc0e91c454e5758d2a7af.png)'
  prefs: []
  type: TYPE_IMG
- en: Interesting, we explained less variance and have a larger residual standard
    deviation (more error).
  prefs: []
  type: TYPE_NORMAL
- en: ridge regression for our arbitrarily selected hyperparameter, \(\lambda\), actually
    reduced both testing variance explained and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is not surprising, we are not actually tuning the hyperparameter to get
    the best model!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating the Lambda Hyperparameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs loop over multiple lambda values - from 0 to 100 and observe the change
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: training and testing, mean square error (MSE) and variance explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png](../Images/5a585689234bfbaf29513933357bc5cb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe from the first 8 cases above of the trained ridge regression
    model that increase in the \(\lambda\) hyperparameter decreases the slope of the
    linear fit.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs plot the mean squaure error and variance explained over train and test
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Recall, the variance explained, \(R^2\) , is given by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(SS_{\text{residual}}\) is the sum of squares of residuals (or errors),
    and \(SS_{\text{total}}\) is the total sum of squares (the variance of the observed
    data).
  prefs: []
  type: TYPE_NORMAL
- en: and the Mean Squared Error (MSE) is given by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual value, \(\hat{y}_i\) is the predicted value, and
    \(n\) is the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png](../Images/cebb95f748df4c664c9a127975a7c87b.png)'
  prefs: []
  type: TYPE_IMG
- en: We observe that as we increase the lambda parameter the variance explained decreases
    and the mean square error increases.
  prefs: []
  type: TYPE_NORMAL
- en: this makes sense as the data has a consistent linear trend and as the slope
    ‚Äòshrinks‚Äô to zero the error increases and the variance explained decreases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there could be other cases where the reduced slope actually performs better
    in testing. For example with sparse and noisy data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs explore the concept of model variance, an important part of machine
    learning accuracy in testing.
  prefs: []
  type: TYPE_NORMAL
- en: the sensitivity of the model to the specific training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the sensitivity to the training data, model variance
    decreases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs demonstrate this with this workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: loop over multiple lambda values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loop over multiple bootstrap samples of the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the ridge regression fit (slope)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the variance of these bootstrap results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WARNING: this will take several minutes to run'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png](../Images/42462583008a06affc490158f6f84d94.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is as expected, with increase in \(\lambda\) hyperparameter the sensitivity
    of the model to the training data is decreased.
  prefs: []
  type: TYPE_NORMAL
- en: k-fold Cross Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be useful to conduct a complete k-fold validation to evaluate the testing
    error vs. the hyperparameter lambda for model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: the following code is provided to do this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, with a single predictor feature we must reshape to a 2D array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We loop over 100 \(\lambda\) values from 0.01 to 100,000,
  prefs: []
  type: TYPE_NORMAL
- en: get the negative mean square error for each of the 4 k-folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then we take the average and apply the absolute value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why work with negative mean square error? Simple, to use the functionality in
    scikit-learn that optimizes by maximization and for consistency with other scores
    like \(r^2\) where larger values are better.
  prefs: []
  type: TYPE_NORMAL
- en: I find negative MSE confusing, so for plotting I use the absolute value to convert
    the values to strictly positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png](../Images/8eb67ef97a23db1eb0ad78518b9e99c2.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above we observe 3 phases,
  prefs: []
  type: TYPE_NORMAL
- en: left - the test MSE levels out at a minimum, the model has converged on linear
    regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: right - the test MSE levels out at a maximum, the model has converged on the
    predictor feature mean, i.e., model parameter slope is 0.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: center - transitional between both cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case we see that the linear regression model (\(\lambda = 0.0\)) is
    the best model! If ridge regression is the optimum model the test mean square
    error would minimize between linear regression and mean.
  prefs: []
  type: TYPE_NORMAL
- en: this commonly occurs for datasets with issues with noise, data paucity and high
    dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our model is the same as linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: could we create a situation where the best model is not linear regression? I.e.,
    were regularization is helpful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: yes, we can. Let‚Äôs remove most the samples to create data paucity and add a
    lot of noise!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admittedly, I iterated the random seeds for the sample and noise to get this
    result.
  prefs: []
  type: TYPE_NORMAL
- en: few data (low \(n\)) and high dimensionality (high \(m\)) will generally result
    in LASSO outperforming linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png](../Images/b011c5d0305db3ebe120ee6826faed3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of ridge regression. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs a simple workflow, demonstration of ridge regression and comparison to
    linear regression for machine learning-based predictions. Why start with linear
    regression?
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is the simplest parametric predictive machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learn about training machine learning models with an analytical solution
    calculated from the derivative of training MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get‚Äôs us started with the concepts of loss functions and norms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have access to analytics expressions for confidence intervals for model uncertainty
    and hypothesis tests for parameter significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why cover ridge regression next?
  prefs: []
  type: TYPE_NORMAL
- en: Some times linear regression is not simple enough and we actually need a simpler
    model!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce the concept of model regularization and hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs some basic details about predictive machine learning ridge regression
    models, let‚Äôs start with linear regression first and build to ridge regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression for prediction, let‚Äôs start by looking at a linear model fit
    to a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d82dccdcda485413554e49d73e4d1fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs start by defining some terms,
  prefs: []
  type: TYPE_NORMAL
- en: '**predictor feature** - an input feature for the prediction model, given we
    are only discussing linear regression and not multilinear regression we have only
    one predictor feature, \(x\). On out plots (including above) the predictor feature
    is on the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**response feature** - the output feature for the prediction model, in this
    case, \(y\). On our plots (including above) the response feature is on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, here are some key aspects of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric Model**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a parametric predictive machine learning model, we accept an a prior
    assumption of linearity and then gain a very low parametric representation that
    is easy to train without a onerous amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: the fit model is a simple weighted linear additive model based on all the available
    features, \(x_1,\ldots,x_m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the parametric model takes the form of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ y = \sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs the visualization of the linear model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/175e41e10e74a46b4a56258ccdfb94c0.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Squares**'
  prefs: []
  type: TYPE_NORMAL
- en: The analytical solution for the model parameters, \(b_1,\ldots,b_m,b_0\), is
    available for the L2 norm loss function, the errors are summed and squared known
    a least squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'we minimize the error, residual sum of squares (RSS) over the training data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ RSS = \sum_{i=1}^n \left(y_i - (\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0) \right)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual response feature values and \(\sum_{\alpha = 1}^m
    b_{\alpha} x_{\alpha} + b_0\) are the model predictions, over the \(\alpha = 1,\ldots,n\)
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a visualization of the L2 norm loss function, MSE,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b25cdf17fb10f18b362f50ba655df92b.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear model loss function, mean square error.
  prefs: []
  type: TYPE_NORMAL
- en: this may be simplified as the sum of square error over the training data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \begin{equation} \sum_{i=1}^n (\Delta y_i)^2 \end{equation}
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Delta y_i\) is actual response feature observation \(y_i\) minus the
    model prediction \(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha} + b_0\), over the
    \(i = 1,\ldots,n\) training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumptions**'
  prefs: []
  type: TYPE_NORMAL
- en: There are important assumption with our linear regression model,
  prefs: []
  type: TYPE_NORMAL
- en: '**Error-free** - predictor variables are error free, not random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity** - response is linear combination of feature(s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constant Variance** - error in response is constant over predictor(s) value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of Error** - error in response are uncorrelated with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No multicollinearity** - none of the features are redundant with other features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With ridge regression we add a hyperparameter, \(\lambda\), to our minimization,
    with a shrinkage penalty term, \(\sum_{j=1}^m b_{\alpha}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha = 1}^m b_{\alpha} x_{\alpha,i}
    + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: As a result, ridge regression training integrates two and often competing goals
    to find the model parameters,
  prefs: []
  type: TYPE_NORMAL
- en: find the model parameters that minimize the error with training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minimize the slope parameters towards zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: lambda does not include the intercept, \(b_0\).'
  prefs: []
  type: TYPE_NORMAL
- en: The \(\lambda\) is a hyperparameter that controls the degree of fit of the model
    and may be related to the model bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a291289b4f4b55ba9adbb7538140a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing error components and expected test square error for ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow 0\) the solution approaches linear regression, there
    is no bias (relative to a linear model fit), but the model variance is likely
    higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the model variance decreases and the model bias increases,
    the model becomes simpler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for \(\lambda \rightarrow \infty\) the model parameters \(b_1,\ldots,b_m\) shrink
    to 0.0 and the model predictions approaches the training data response feature
    mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs a variety of ridge regression models for various \(\lamda\) values that
    will be calculated in the workflow below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f26df3583b9f434159b8bbb15cc292b.png)'
  prefs: []
  type: TYPE_IMG
- en: Ridge regression models with low to high \(\lambda\) hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: Train / Test Data Split for Cross Validation-based Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The available data is split into training and testing subsets.
  prefs: []
  type: TYPE_NORMAL
- en: in general 15-30% of the data is withheld from training to apply as testing
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: testing data selection should be fair, the same difficulty of predictions (offset/different
    from the training data) [Fair Train-test Split Paper](https://www.sciencedirect.com/science/article/pii/S0920410521015023)
    (Salazar et al., 2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are various other methods including train, validation and testing splits,
    and k-fold cross validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamentally, all these methods proceed by training models with training data
    and testing model accuracy over withheld testing data for a range of hyperparameters.
    Then the hyperparameters are selected that minimize error with the withheld testing
    dataset, this is hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: after hyperparameter tuning the model is retrained with all of the data with
    the selected hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the train, validation and testing approach then uses a 2nd withheld subset of
    the dataset to check the tuned model with data not applied for model training
    nor model tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following workflow we will use the train and test approach for hyperparameter
    tuning. Here‚Äôs more details and a summary of the related concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Model Parameter Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training data is applied to train the model parameters such that the model
    minimizes mismatch with the training data
  prefs: []
  type: TYPE_NORMAL
- en: it is common to use **mean square error** (known as a **L2 norm**) as a loss
    function summarizing the model mismatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**minimizing the loss function** for simple models an analytical solution may
    be available, but for most machine this requires an iterative optimization method
    to find the best model parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs the derivation of the analytical solution, starting with the loss function
    with, least squares (left) and regularization (right) terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = \sum_{i=1}^n \left(y_i - \left(\sum_{\alpha
    = 1}^m b_{\alpha} x_{\alpha,i} + b_0 \right) \right)^2 + \lambda \sum_{j=1}^m
    b_{\alpha}^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: let‚Äôs convert this into matrix notation for convenience,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = (ùë¶‚àíùëã\beta)^ùëá (ùë¶‚àíùëã\beta)+\lambda
    \beta^ùëá \beta \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\beta\) is a vector of the model parameters, \(y\) is a vector of response
    feature values, and \(X\) is a matrix of predictor feature values, both over the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: We can expand the quadratic term,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left( \text{RSS} + \text{shrinkage} \right) = ùë¶‚àí2ùëã^ùëá ùë¶\beta+(ùëã^ùëá ùëã) \beta^ùëá
    \beta+\lambda \beta^ùëá \beta \]
  prefs: []
  type: TYPE_NORMAL
- en: We take the partial derivative with respect to the model parameters and set
    it equal to 0.0,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\partial}{\partial \beta} \left( \text{RSS} + \text{shrinkage} \right)
    = -2X^T y + 2(X^T X) \beta + 2\lambda \beta = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can further simplify with minor adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2(ùëã^ùëá ùëã) \beta + 2 \lambda \beta = 2ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Divide both sides by 2,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (ùëã^ùëá ùëã) \beta + \lambda \beta = ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Group the common terms,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (ùëã^ùëá ùëã+ \lambda I) \beta =ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I\) is an identity matrix. Now we can solve for our ridge regression
    parameters,
  prefs: []
  type: TYPE_NORMAL
- en: \[ ùõΩ=\left(ùëã^ùëá ùëã+\lambda I \right)^{-1} ùëã^ùëá ùë¶ \]
  prefs: []
  type: TYPE_NORMAL
- en: Note, \(ùëã^ùëá ùëã+\lambda I\) is generally invertible, so this is solvable.
  prefs: []
  type: TYPE_NORMAL
- en: This process is repeated over a range of model complexities specified by hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation of Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interpretation and motivation for ridge regression,
  prefs: []
  type: TYPE_NORMAL
- en: when \(ùëö \ge ùëõ\), linear regression is ill-posed problem, i.e., a problem with
    more than 1 solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for ill-posed problems we introduce some constraints or regularization to limit
    the solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/3b29636566b2ebc1625773201be9a303.png)'
  prefs: []
  type: TYPE_IMG
- en: Ill-posed linear regression model (left), and well-posed with additional shrinkage
    constraint ridge regression (right).
  prefs: []
  type: TYPE_NORMAL
- en: Model Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The withheld testing data is retrieved and loss function (usually the **L2 norm**
    again) is calculated to summarize the error over the testing data
  prefs: []
  type: TYPE_NORMAL
- en: this is repeated over the range of specified hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the model complexity / hyperparameters that minimize the loss function / error
    summary in testing is selected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is known are model hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Model Overfit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More model complexity / flexibility than can be justified with the available
    data, data accuracy, frequency and coverage
  prefs: []
  type: TYPE_NORMAL
- en: Model explains ‚Äúidiosyncrasies‚Äù of the data, capturing data noise/error in the
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High accuracy in training, but low accuracy in testing / real-world use away
    from training data cases ‚Äì poor ability of the model to generalize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model error in testing is increasing while the model error in training
    is decreasing, this is an indicator of an overfit model.
  prefs: []
  type: TYPE_NORMAL
- en: Load the Required Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also need some standard packages. These should have been installed with
    Anaconda 3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you get a package import error, you may have to first install some of these
    packages. This can usually be accomplished by opening up a command window on Windows
    and then typing ‚Äòpython -m pip install [package-name]‚Äô. More assistance is available
    with the respective package docs.
  prefs: []
  type: TYPE_NORMAL
- en: Declare Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs define a function to streamline the addition specified percentiles and
    major and minor gridlines to our plots.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Set the Working Directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always like to do this so I don‚Äôt lose files and to simplify subsequent read
    and writes (avoid including the full address each time). Also, in this case make
    sure to place the required (see below) data file in this working directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You will have to update the part in quotes with your own working directory and
    the format is different on a Mac (e.g. ‚Äú~/PGE‚Äù).
  prefs: []
  type: TYPE_NORMAL
- en: Loading Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here‚Äôs the command to load our comma delimited data file in to a Pandas‚Äô DataFrame
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs load the provided multivariate, spatial dataset ‚Äòunconv_MV.csv‚Äô. This
    dataset has variables from 1,000 unconventional wells including:'
  prefs: []
  type: TYPE_NORMAL
- en: density (\(g/cm^{3}\))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: porosity (volume %)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, the dataset is synthetic.
  prefs: []
  type: TYPE_NORMAL
- en: We load it with the pandas ‚Äòread_csv‚Äô function into a DataFrame we called ‚Äòmy_data‚Äô
    and then preview it to make sure it loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Train-Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity we apply a random train-test split with the train_test_split
    function from scikit-learn package, model_selection module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Visualize the DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizing the DataFrame is useful first check of the data.
  prefs: []
  type: TYPE_NORMAL
- en: many things can go wrong, e.g., we loaded the wrong data, all the features did
    not load, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can preview by utilizing the ‚Äòhead‚Äô DataFrame member function (with a nice
    and clean format, see below).
  prefs: []
  type: TYPE_NORMAL
- en: we have a custom function to preview the training and testing DataFrames side-by-side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 1.778580 | 11.426485 |'
  prefs: []
  type: TYPE_TB
- en: '| 101 | 2.410560 | 8.488544 |'
  prefs: []
  type: TYPE_TB
- en: '| 88 | 2.216014 | 10.133693 |'
  prefs: []
  type: TYPE_TB
- en: '| 79 | 1.631896 | 12.712326 |'
  prefs: []
  type: TYPE_TB
- en: '| 58 | 1.528019 | 16.129542 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 59 | 1.742534 | 15.380154 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.404932 | 13.710628 |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 1.552713 | 14.131878 |'
  prefs: []
  type: TYPE_TB
- en: '| 92 | 1.762359 | 11.154896 |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 1.885087 | 9.403056 |'
  prefs: []
  type: TYPE_TB
- en: Summary Statistics for Tabular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of efficient methods to calculate summary statistics from tabular
    data in DataFrames. The describe command provides count, mean, minimum, maximum
    in a nice data table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 78.000000 | 78.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 1.739027 | 12.501465 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 0.302510 | 3.428260 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 0.996736 | 3.276449 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.410560 | 21.660179 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Density | Porosity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 27.000000 | 27.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 1.734710 | 12.380796 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 0.247761 | 2.916045 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 1.067960 | 7.894595 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 2.119652 | 18.133771 |'
  prefs: []
  type: TYPE_TB
- en: Visualize the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs check the consistency and coverage of training and testing with histograms
    and scatter plots.
  prefs: []
  type: TYPE_NORMAL
- en: check to make sure the train and test data cover the range of possible feature
    combinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ensure we are not extrapolating beyond the training data with the testing cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/c4e4535126491e78be365e71b1c9a03a2d1b7af5ddc9027eed6eced1b9c42f72.png](../Images/019f46cd3338e55a8b84bcd57a899945.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first calculate the linear regression model. We use scikit learn and then
    extend the same workflow to ridge regression.
  prefs: []
  type: TYPE_NORMAL
- en: we are building a model, \(\phi = f(\rho)\), where \(\phi\) is porosity and
    \(\rho\) is density.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we could also say, we have ‚Äúporosity regressed on density‚Äù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our model has this specific equation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \phi = b_1 \times \rho + b_0 \]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/9068634e78d0710f8ba9632a527aa661f2ca3527b1121d2db728317558851841.png](../Images/5bc6600437501d419bf56af2e1dc727d.png)'
  prefs: []
  type: TYPE_IMG
- en: You may have noticed the additional reshape operation applied to the predictor
    feature in the predict function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This is needed because scikit-learn assumes more than one predictor feature;
    therefore, expects a 2D array of samples (rows) and features (columns), but we
    have only a 1D vector.
  prefs: []
  type: TYPE_NORMAL
- en: the reshape operation turns the 1D vector into a 2D vector with only 1 column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Regression Model Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs run some quick model checks. Much more could be done, but I limit this
    for brevity here.
  prefs: []
  type: TYPE_NORMAL
- en: see the Linear Regression chapter for more information and checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/d10062259023c26d588429dde18deab7d42abe383ae44ad7fa6d4c7b96ef620e.png](../Images/208a933176542d3185b7f4f4c9a1bf61.png)'
  prefs: []
  type: TYPE_IMG
- en: Ridge Regression Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs replace the scikit-learn linear regression method with the scikit-learn
    ridge regression method.
  prefs: []
  type: TYPE_NORMAL
- en: note, we must now set the \(\lambda\) hyperparameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in scikit-learn the hyperparameter(s) is(are) set with the instantiation of
    the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/61654e5169f1e8d42162279fae72b88a9d9108ce66bb01b9fd227b101c160daf.png](../Images/05ecf578788df580a4c1f1f25e9ffcb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs repeat the simple model checks that we applied with our linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/bd6d04e063283738954b9b88fa6ed77d11eeec140126dcf4052ef6d9523d2c48.png](../Images/2e6dac5a06cdc0e91c454e5758d2a7af.png)'
  prefs: []
  type: TYPE_IMG
- en: Interesting, we explained less variance and have a larger residual standard
    deviation (more error).
  prefs: []
  type: TYPE_NORMAL
- en: ridge regression for our arbitrarily selected hyperparameter, \(\lambda\), actually
    reduced both testing variance explained and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is not surprising, we are not actually tuning the hyperparameter to get
    the best model!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating the Lambda Hyperparameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs loop over multiple lambda values - from 0 to 100 and observe the change
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: training and testing, mean square error (MSE) and variance explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2a222870bfcb75734199aa94ba43bb3842ecda84cd1bf44eee6b1daf798f4199.png](../Images/5a585689234bfbaf29513933357bc5cb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe from the first 8 cases above of the trained ridge regression
    model that increase in the \(\lambda\) hyperparameter decreases the slope of the
    linear fit.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs plot the mean squaure error and variance explained over train and test
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Recall, the variance explained, \(R^2\) , is given by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(SS_{\text{residual}}\) is the sum of squares of residuals (or errors),
    and \(SS_{\text{total}}\) is the total sum of squares (the variance of the observed
    data).
  prefs: []
  type: TYPE_NORMAL
- en: and the Mean Squared Error (MSE) is given by,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(y_i\) is the actual value, \(\hat{y}_i\) is the predicted value, and
    \(n\) is the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/eaa10f0fe491c5af34e0480460db637c0b45df49bd3270493df83ecce6357f75.png](../Images/cebb95f748df4c664c9a127975a7c87b.png)'
  prefs: []
  type: TYPE_IMG
- en: We observe that as we increase the lambda parameter the variance explained decreases
    and the mean square error increases.
  prefs: []
  type: TYPE_NORMAL
- en: this makes sense as the data has a consistent linear trend and as the slope
    ‚Äòshrinks‚Äô to zero the error increases and the variance explained decreases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there could be other cases where the reduced slope actually performs better
    in testing. For example with sparse and noisy data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs explore the concept of model variance, an important part of machine
    learning accuracy in testing.
  prefs: []
  type: TYPE_NORMAL
- en: the sensitivity of the model to the specific training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as \(\lambda\) increases the sensitivity to the training data, model variance
    decreases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs demonstrate this with this workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: loop over multiple lambda values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loop over multiple bootstrap samples of the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the ridge regression fit (slope)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the variance of these bootstrap results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WARNING: this will take several minutes to run'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/5e13984f63c906b044e58c06c1851dc67ab928545f9bfb881f3e02ebb1002cf1.png](../Images/42462583008a06affc490158f6f84d94.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is as expected, with increase in \(\lambda\) hyperparameter the sensitivity
    of the model to the training data is decreased.
  prefs: []
  type: TYPE_NORMAL
- en: k-fold Cross Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be useful to conduct a complete k-fold validation to evaluate the testing
    error vs. the hyperparameter lambda for model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: the following code is provided to do this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: once again, with a single predictor feature we must reshape to a 2D array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We loop over 100 \(\lambda\) values from 0.01 to 100,000,
  prefs: []
  type: TYPE_NORMAL
- en: get the negative mean square error for each of the 4 k-folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then we take the average and apply the absolute value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why work with negative mean square error? Simple, to use the functionality in
    scikit-learn that optimizes by maximization and for consistency with other scores
    like \(r^2\) where larger values are better.
  prefs: []
  type: TYPE_NORMAL
- en: I find negative MSE confusing, so for plotting I use the absolute value to convert
    the values to strictly positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/2e65418d331d427736b40372288b8cec15d2e1bd38e995d14e8e51014c0e10d7.png](../Images/8eb67ef97a23db1eb0ad78518b9e99c2.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above we observe 3 phases,
  prefs: []
  type: TYPE_NORMAL
- en: left - the test MSE levels out at a minimum, the model has converged on linear
    regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: right - the test MSE levels out at a maximum, the model has converged on the
    predictor feature mean, i.e., model parameter slope is 0.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: center - transitional between both cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case we see that the linear regression model (\(\lambda = 0.0\)) is
    the best model! If ridge regression is the optimum model the test mean square
    error would minimize between linear regression and mean.
  prefs: []
  type: TYPE_NORMAL
- en: this commonly occurs for datasets with issues with noise, data paucity and high
    dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our model is the same as linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: could we create a situation where the best model is not linear regression? I.e.,
    were regularization is helpful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: yes, we can. Let‚Äôs remove most the samples to create data paucity and add a
    lot of noise!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admittedly, I iterated the random seeds for the sample and noise to get this
    result.
  prefs: []
  type: TYPE_NORMAL
- en: few data (low \(n\)) and high dimensionality (high \(m\)) will generally result
    in LASSO outperforming linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/ac7782f4fe7198210e3ac58ca9cbea5c3c4e0c30335c332fac08ce3ae63077a9.png](../Images/b011c5d0305db3ebe120ee6826faed3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of ridge regression. Much more could be done and
    discussed, I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PI is Professor John Foster)?
    My research combines data analytics, stochastic modeling and machine learning
    theory with practice to develop novel methods and workflows to add value. We are
    solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
