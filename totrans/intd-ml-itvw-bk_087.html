<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>7.1 Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>7.1 Basics</h1>
<blockquote>原文：<a href="https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html">https://huyenchip.com/ml-interviews-book/contents/7.1-basics.html</a></blockquote>
                                
                                
<ol>
<li>[E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and active learning.</li>
<li>Empirical risk minimization.<ol>
<li>[E] What’s the risk in empirical risk minimization?</li>
<li>[E] Why is it empirical?</li>
<li>[E] How do we minimize that risk?</li>
</ol>
</li>
<li>[E] Occam's razor states that when the simple explanation and complex explanation both work equally well, the simple explanation is usually correct.  How do we apply this principle in ML?</li>
<li>[E] What are the conditions that allowed deep learning to gain popularity in the last decade?</li>
<li>[M] If we have a wide NN and a deep NN with the same number of parameters, which one is more expressive and why?</li>
<li>[H] The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then why can’t a simple neural network reach an arbitrarily small positive error?</li>
<li>[E] What are saddle points and local minima? Which are thought to cause more problems for training large NNs?</li>
<li>Hyperparameters.<ol>
<li>[E] What are the differences between parameters and hyperparameters?</li>
<li>[E] Why is hyperparameter tuning important?</li>
<li>[M] Explain algorithm for tuning hyperparameters.</li>
</ol>
</li>
<li>Classification vs. regression.<ol>
<li>[E] What makes a classification problem different from a regression problem?</li>
<li>[E] Can a classification problem be turned into a regression problem and vice versa?</li>
</ol>
</li>
<li>Parametric vs. non-parametric methods.<ol>
<li>[E] What’s the difference between parametric methods and non-parametric methods? Give an example of each method.</li>
<li>[H] When should we use one and when should we use the other?</li>
</ol>
</li>
<li>[M] Why does ensembling independently trained models generally improve performance?</li>
<li>[M] Why does L1 regularization tend to lead to sparsity while L2 regularization pushes weights closer to 0?</li>
<li>[E] Why does an ML model’s performance degrade in production?</li>
<li>[M] What problems might we run into when deploying large machine learning models?</li>
<li>Your model performs really well on the test set but poorly in production.<ol>
<li>[M] What are your hypotheses about the causes?</li>
<li>[H] How do you validate whether your hypotheses are correct?</li>
<li>[M] Imagine your hypotheses about the causes are correct. What would you do to address them?</li>
</ol>
</li>
</ol>

                                
                                    
</body>
</html>