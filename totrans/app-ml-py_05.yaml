- en: Probability Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_probability.html](https://geostatsguy.github.io/MachineLearningDemos_Book/MachineLearning_probability.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael J. Pyrcz, Professor, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: '[Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter of e-book ‚ÄúApplied Machine Learning in Python: a Hands-on Guide with
    Code‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite this e-Book as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *Applied Machine Learning in Python: A Hands-on Guide with
    Code* [e-book]. Zenodo. doi:10.5281/zenodo.15169138 [![DOI](../Images/7e4ea662f44af1eae87e87ecbb962ff4.png)](https://doi.org/10.5281/zenodo.15169138)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflows in this book and more are available here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cite the MachineLearningDemos GitHub Repository as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pyrcz, M.J., 2024, *MachineLearningDemos: Python Machine Learning Demonstration
    Workflows Repository* (0.0.3) [Software]. Zenodo. DOI: 10.5281/zenodo.13835312\.
    GitHub repository: [GeostatsGuy/MachineLearningDemos](https://github.com/GeostatsGuy/MachineLearningDemos)
    [![DOI](../Images/4e3a59c17d684b06a170c4af84e0f631.png)](https://zenodo.org/doi/10.5281/zenodo.13835312)'
  prefs: []
  type: TYPE_NORMAL
- en: By Michael J. Pyrcz
  prefs: []
  type: TYPE_NORMAL
- en: ¬© Copyright 2024.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is a summary of **Probability Concepts** including essential concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation and Application to Model Uncertainty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to Calculate Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability Operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marginal, Conditional and Joint Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independence Checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian Updating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YouTube Lecture**: check out my lecture on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Probability and Statistics](https://youtu.be/jl14s8jvXcc?si=TA1YAG_LVWAXMeik)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I also have more comprehensive probability content from my Data Analytics and
    Geostatistics course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Probability](https://youtu.be/IGPayWv1BBM?si=K2zI0qBQV3FvJ9fM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Frequentist Probability](https://youtu.be/NSvyljWT4mw?si=c0HepAkQwLDx3TCb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bayesian Probability](https://youtu.be/Ppwfr8H177M?si=NYBOi8zTCAxJEpGl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Joint, Marginal, and Conditional Probability](https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Joint, Marginal, and Conditional Probability](https://youtu.be/kxjnPVyuuo8?si=FI3Tiu72Wc5Neunm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bayesian Coin Example](https://youtu.be/D1UKZGOYDOg?si=uFSAB0xLWsr80TYj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For convenience here‚Äôs a summary of the salient points.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why cover probability at the beginning of an e-book or course on machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Formulation** - many of our machine learning models are formulated
    with probability concepts, for example, naive Bayes classification is derived
    from Bayes‚Äô Theorem and Bayesian linear regression estimates the probability distributions
    for our model parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Cleaning and Preparation** - is 90% of any machine learning workflow
    and we cannot complete these steps without understanding our data distributions
    and statistics and all of these are based on probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss Functions and Optimization** - many of our machine learning models are
    trained through optimization of a loss function that relies on probability for
    stochastic steps or even directly in the loss function as is the case for maximum
    likelihood estimation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tuning Machine Learning Models** - machine learning model tuning to reduce
    model overfit is based on the concept of expected model performance in the presence
    of various uncertainty sources, and probability is the language of statistical
    expectation and uncertainty.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Machine Learning Model Choice** - You will make choices between frequentist
    and Bayesian predictive machine learning models and their differences are the
    result of two distinct approaches to calculate probability and build models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Real World Applications** - to make the best choices in machine learning
    workflows design and to use our results to support decisions we must integrate
    probability concepts Using our models in the real world.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us build our machine learning skills on a solid foundation of probability!
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs: []
  type: TYPE_NORMAL
- en: Probability is an essential prerequisite for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now we get started by defining probability and then we will be ready to talk
    about ways to calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: What is Probability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand what is probability consider Kolmogorov‚Äôs 3 axioms for probabilities,
    i.e., the rules that any measure of probability must honor,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53594a139b9058c07be33e4cff134ea4.png)'
  prefs: []
  type: TYPE_IMG
- en: Andrey Kolmogorov (1903 ‚Äì 1987), Soviet mathematician, photo taken 1972 and
    from image from https://culturemath.ens.fr/thematiques/biographie/life-and-work-kolmogorov).
  prefs: []
  type: TYPE_NORMAL
- en: '**non-negativity** - probability of an event is a non-negative number'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(ùê¥) \ge 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: imagine negative probability!
  prefs: []
  type: TYPE_NORMAL
- en: '**normalization** - probability of the entire sample space is one (unity),
    also known as probability closure'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: something happens!
  prefs: []
  type: TYPE_NORMAL
- en: '**additivity** - addition of probability of mutually exclusive events for unions'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: e.g., probability of \(A_1\) and \(A_2\) mutual exclusive events is, \(Prob(A_1
    + A_2) = Prob(A_1) + Prob(A_2)\)
  prefs: []
  type: TYPE_NORMAL
- en: note our concise notation, \(P(\cdot)\), for probability of event in the bracket
    occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a good place to start for valid probabilities, we will refine this later.
    Now we can ask,
  prefs: []
  type: TYPE_NORMAL
- en: How to Calculate Probability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are 3 probability perspectives that can be applied to calculate probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability by long-term frequencies** (Frequentist Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: probability as ratio of outcomes from an experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requires repeated observations from a controlled experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, you flip a coin 100 times and count the number of outcomes with
    heads \(n(\text{heads})\) and then calculate this ratio,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{heads}) = \frac{n(\text{heads})}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the frequentist approach to calculate probabilities. The experiment
    is the set of coin tosses.
  prefs: []
  type: TYPE_NORMAL
- en: one issue with frequentist probabilities is, can we now use this probability
    of heads outside the experiment? For example,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: on another day?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a different person tossing the coin?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a different coin?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability by physical tendencies or propensities** (Engineering Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: probability calculated from knowledge about the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we could know the probability of coin toss outcomes without the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is the engineering approach to probability, model the system and use this
    model, i.e., the physics of the system to calculate probability of outcomes
  prefs: []
  type: TYPE_NORMAL
- en: did you know that there is a \(\frac{1}{6,000}\) probability of a coin landing
    and staying upright on its edge? This could be lower depending of manner of tossing,
    coin thickness and characteristics of the surface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability by Degrees of belief** (Bayesian Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: first we specify the scientific concept of ‚Äúbelief‚Äù as your opinion based on
    all your knowledge and experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our model integrates our certainty about a result and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is very flexible, we can assign probability to any event, and includes
    a framework for updating with new information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if this is your approach, then you are using the Bayesian approach for probability.
    If not, before you dismiss this approach let me make a couple arguments,
  prefs: []
  type: TYPE_NORMAL
- en: you may be bothered by this idea of belief, as it may seem subjective compared
    to the probabilities objectively measured from a frequentist‚Äôs experiment but,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to use the frequentist probability you have to make the subjective decision
    to apply it outside the experiment, i.e., we need to go beyond a single coin!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: the Bayesian probability approach includes objective probabilities from experiments,
    but it also allows for integration of our expert knowledge
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A Warning about Calculating Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Statistics can be misused or even abused, leading to flawed conclusions and
    poor decision-making. When probabilities are calculated improperly or misinterpreted,
    it can result in significant consequences. Here are a few examples of what can
    go wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Insufficient sampling** - there are various rules of thumb about what constitutes
    a small sample size, i.e., too small for inference about the population parameters
    and too small to train a reliable prediction model. Some say 7, some say 30, indubitably
    there is a minimum sample size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: in general, our models will communicate increasing uncertainty as the number
    of data decreases, but at some point these models break!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: consider bootstrap, yes as the number of samples decrease the uncertainty in
    the statistic reported by bootstrap increases, but this uncertainty distribution
    is centered on the sample statistic! In other words, we need enough samples to
    have a reasonable estimate of the uncertainty model‚Äôs expectation in the first
    place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of course, there is minimum number of samples to complete a mathematical operation,
    for example principal components analysis can only calculate \(n-1\) principal
    components (where \(n\) is the number of samples) and we cannot fit a linear regression
    model to \(n=1\) samples. Best practice is to stay very far away (have many more
    samples) from any of these algorithm limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Biased sampling** - in general our probability calculations will not automatically
    debias the sample data. Any bias in the samples will pass bias through to the
    probabilities representing our uncertainty model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for example, a biased sample mean will bias simple kriging estimates away from
    data. Geostatistical simulation reproduces the entire input feature distribution,
    so any bias in any part of the distribution will be passed to the spatial models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, there is bad data. When we use Bayesian methods and rely on expert experience,
    that is not a license to say anything and defend it as belief. On the contrary,
    we must rigorously document and defend all our choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unskilled practice and a lack of rigor** - there are mistakes that can be
    made with probabilities and some of them are shockingly common.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: our first line of defense is to understand what our methods are doing under
    the hood! This will help us recognize logical inconsistencies as we build our
    workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our second line of defense is to check every step in our workflows. Like an
    accountant we must close the loops with all our probability calculations, a process
    that accountants call reconciliation. Like a software engineer we must unit test
    every operation to ensure we don‚Äôt introduce errors as we update our probability
    workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, if we perform primary feature cosimulation with the same random
    number seed as the secondary feature simulation we will introduce artificial correlation
    between the simulated primary and secondary features that will dramatically change
    the conditional and joint probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in another example, if we use information from the likelihood probabilities
    to inform the prior probabilities, we will significantly under-estimate the uncertainty.
    This error is a form of information leakage, descriptively we state it as ‚Äúdouble
    dipping‚Äù from the information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please remember these warnings as you proceed below and onward as you build
    your own data science workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Venn Diagrams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Venn Diagrams are a tool to communicate probability.
  prefs: []
  type: TYPE_NORMAL
- en: representing the possible events or outcomes (\(A, B,\ldots\)) of an experiment
    within the collection of all possible events or outcomes, the sample space (\(\Omega\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c212b1fc6b3c5da35b7df52769a0af42.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple example Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: What do we learn from this Venn diagram?
  prefs: []
  type: TYPE_NORMAL
- en: the relative size of regions for each event within is the relative probability
    of occurrence, probability of \(B\) is greater than probability of \(A\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the overlap over events is the probability of joint occurrence, there is 0.0
    probability of \(A\) and \(B\) occurring together
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us include a more practical Venn diagram to ensure this concept is not too
    abstract. Here‚Äôs a Venn diagram from 3,000 core samples with interpreted facies
  prefs: []
  type: TYPE_NORMAL
- en: the events are sandstone (Sm) and mudstone (Fm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/824ce0a24892a79f960068d34d629915.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing the facies assignments for 3,000 core samples.
  prefs: []
  type: TYPE_NORMAL
- en: What do we learn from this Venn diagram?
  prefs: []
  type: TYPE_NORMAL
- en: mudstone is more likely than sandstone over these core samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are a lot of samples that are neither sandstone nor mudstone, the white
    space within \(\Omega\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are samples that are interpreted as both sandstone and mudstone, i.e.,
    interbedded sandstone-mudstone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do not forget to draw and label the \(\Omega\) box or we can‚Äôt understand the
    relative probability of the events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can use any convenient shape to represent an event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Venn diagrams are an excellent tool to visualize probability, so
    we will use them to visualize and teach probability here.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now provide an extended definition for the frequentist approach for probability.
    A measure of the likelihood that an event will occur based on frequencies observed
    from an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For random experiments and well-defined settings, we calculate probability
    as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \lim_{n \to \infty} \frac{n(A)}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(n(A)\) = number of times event \(A\) occurred \(n\) = number of trails
  prefs: []
  type: TYPE_NORMAL
- en: we use limit notation above to indicate sufficient sampling and that the solution
    converges and improves accuracy as we introduce more samples from our experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example,
  prefs: []
  type: TYPE_NORMAL
- en: probability of drilling a dry hole, encountering sandstone, and exceeding a
    rock porosity of \(15\%\) at a location (\(\bf{u}_{\alpha}\)) based on historical
    results for drilling in this reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we walk-through various probability operations from the frequentist perspective,
    may I ask for patience from my Bayesian friends as we will later return to the
    Bayesian perspective for probability.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Union of Events** - for example, all outcomes in the sample space that belong
    to either event \(A\) or \(B\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the union of events operator, we use the word ‚Äúor‚Äù and the mathematics symbol,
    \(cup\), using set notation we state the samples in the union \(A\) or \(B\) as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cup B = \{x: x \in A \text{ or } x \in B\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for a union as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating the union \(P(A \cup B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01902ece9dac16a02139346ccccf2574.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing union probability operator for events \(A\) or \(B\).
    Note, the legend on the top right is included to clarify the plot but is not part
    of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Intersection of Events** - for example, all outcomes in the sample space
    that belong to both events \(A\) and \(B\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the intersection of events operator, we use the word ‚Äúand‚Äù and the mathematics
    symbol \(cap\), using set notation we state the samples in the intersection \(A\)
    and \(B\) as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for an intersection as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with the common probability shorthand as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: we will call this a joint probability later. Here is a Venn diagram illustrating
    the intersection \(P(A \cap B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55a85b2ac3f3804377f80585821678e0.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing intersection probability operator for events \(A\)
    and \(B\). Note, the legend on the top right is included to clarify the plot but
    is not part of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compliment of Events** - for example, all outcomes in the sample space that
    do not belong to event \(A\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the compliment of event(s) operator we use the word ‚Äúnot‚Äù and the mathematics
    symbol \(^c\), using set notation, we state the samples in the compliment of \(A\)
    as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A^c = \{x: x \notin A\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for a compliment as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A^c) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating the compliment \(P(A^c)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/507be2ebc15778352071fdbdd52d2a3c.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing probability compliment operator for events \(A\).
    Note, the legend on the top right is included to clarify the plot but is not part
    of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutually Exclusive Events** - for example, events do not intersect or do
    not have any common outcomes, \(A\) and \(B\) do not occur at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: or with the common probability shorthand as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating mutual exclusive events \(A\) and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c212b1fc6b3c5da35b7df52769a0af42.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\) as mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exhaustive, Mutually Exclusive Events** the sequence of events whose union
    is equal to the sample space, all-possible events (\(\Omega\)) and there is no
    intersection between the events:'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are exhaustive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ or } x \in B \} = \Omega \]'
  prefs: []
  type: TYPE_NORMAL
- en: and events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and for mutually exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating mutual exclusive, exhaustive events \(A\)
    and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02c7a7dba945af2e442593dbf746edb5.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\) as exhaustive, mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Combinations of Operators** - we can use these probability operators with
    any number of events to communicate complicated probability cases. For example,
    let us define these events,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event \(A\): oil present (\(A^c\): dry hole)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event \(B\): ùëÜùëö (\(B^c\): ùêπùëö)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event \(C\): porosity ‚â• 15% (\(C^c\): porosity < 15%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of dry hole with massive sandstone (ùëÜùëö) and porosity
    ‚â• 15%?
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A^c \cap B \cap C) = \frac{\text{Area}(A^c \cap B \cap C)}{\text{Area}(\Omega)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram representing this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe9499a30db96f497a94f9d209947cec.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing a more complicated case with 3 events, \(A, B, C\),
    with compliment and intersection probability operators.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints on Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined probability notation and probability operations, now
    we return the idea of permissible probability values expressed by Kolmogorov.
    We can now build on Kolmogorov‚Äôs probability axioms with this set of probability
    constraints,
  prefs: []
  type: TYPE_NORMAL
- en: Non-negativity, Normalization constraints include,
  prefs: []
  type: TYPE_NORMAL
- en: Probability is bounded,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0.0 \le P(A) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability must be between 0.0 and 1.0 (including 0.0 and 1.0)
  prefs: []
  type: TYPE_NORMAL
- en: Probability closure,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of any event is 1.0
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of A or not A is 1.0
  prefs: []
  type: TYPE_NORMAL
- en: Probability for null sets,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(\emptyset) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of nothing happens is zero
  prefs: []
  type: TYPE_NORMAL
- en: Now we use these probability concepts and notation to append more essential
    complicated probability concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Addition Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the probability for a union of events? Remember that union is an ‚Äúor‚Äù
    operation represented by the \(\cup\) notation. Consider,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: inspection of the previously shown Venn diagram indicates that we can not calculate
    the union , \(P(A \cup B)\), by just summing \(P(A)\) and \(P(B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55a85b2ac3f3804377f80585821678e0.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\), showing the intersection
    of \(A\) and \(B\) that will be double counted if we calculate the union and \(A\)
    or \(B\) as the sum of \(A\) and \(B\). Note, the legend on the top right is included
    to clarify the plot but is not part of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the probabilities \(P(A)\) and \(P(B)\) will double count the intersection,
    \(P(A \cap B)\), so we must subtract it,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A \cap B) \]
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there is a general expression for any number of events to calculate the
    probability of the union,
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm not going to include it here but suffice it to say that it is a book keeping
    nightmare given the combinatorial of intersections that can be counted too many
    times!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a much simpler case. If the events are mutually exclusive then there
    is no intersection, \(P(A,B) = 0.0\), and we can just sum the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: for the general case of mutually exclusive for any number of events,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A_i \cap A_j = \emptyset, \quad \forall \quad i \ne j \]
  prefs: []
  type: TYPE_NORMAL
- en: then we can write this general equation for the addition rule for any number
    of mutually exclusive events,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram showing 4 mutual exclusive events, \(A_1, A_2, A_3, A_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aecd4b1580232c0f55e753bad39f1ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing 4 events, \(A_1, A_2, A_3, A_4\), that are all mutually
    exclusive. We can calculate the probability of the union of these events as the
    sum of probabilities of each.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the probability of an event given another event has occurred? To discuss
    this, let us get more specific, what is the probability of event \(B\) given event
    \(A\) has occurred?
  prefs: []
  type: TYPE_NORMAL
- en: to express this we use the notation, \(P(B|A)\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we read this notation as, ‚Äúprobability of B given A‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an example of a conditional probability. We calculate conditional probability
    with this equation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A \cap B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: This may seem complicated, but we can easily visualize and understand this equation
    from our Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e6c3488677335aca9cd1bdaaa70f6bb.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate conditional probability we "shrink our universe" to the given condition.
    For the probability of $B$ given $A$ we shrink our Venn diagram to \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: after we shrink our universe, our \(\Omega\) is only event \(A\)! Now we can
    easily see that the conditional probability is simply the probability of the intersection
    of \(A\) and \(B\) divided by the probability of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b40d3731ffed4bc1939cfc97dfdce13.png)'
  prefs: []
  type: TYPE_IMG
- en: After we "shrink our universe" to the condition, we can see conditional probability
    equation clearly as the intersection of \(A\) and \(B\) divided by the probability
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: Now we introduce a couple examples to test our knowledge of conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability Example #1** - for this Venn diagram provide the
    conditional probabilities, \(P(A|B)\) and \(P(B|A)\), in terms of \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ca9cd3deee70b9c41f0865eb8f1bc9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram to check your understanding of conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: This is simple to solve, the answer is 0.0 for both conditional probability,
    because to calculate conditional probability the numerator is the probability,
    \(P(A,B)\), and this is 0.0 if there is no overlap in the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: also, since \(P(B,A) = P(B,A)\) the numerator is the same for both conditional
    probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B,A)}{P(B)} = \frac{0.0}{P(B)} = 0.0 \]\[ P(B|A) = \frac{P(A,B)}{P(A)}
    = \frac{0.0}{P(A)} = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we attempt a more challenging example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability Example #2** - for this Venn diagram provide the
    conditional probabilities, \(P(A|B)\) and \(P(B|A)\), in terms of \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97292e6e32b997818928d4ab73818c14.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram to check your understanding of conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: This one is more interesting. Let us look at each conditional probability one
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B,A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: notice that \(P(B,A) = P(B)\) since all \(B\) is inside \(A\), so we can substitute
    \(P(B)\) for \(P(B,A)\) above,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B)}{P(B)} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: this makes sense, given \(B\) then \(A\) must occur, the conditional probability
    is 1.0\. Now we calculate the other conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: just a reminder that \(P(B,A) = P(A,B)\) so we can use our previous result and
    substitute, \(P(B)\) for \(P(B,A)\) above.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} = \frac{P(B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: If you understand these two examples, then you have a good foundation in conditional
    probability,
  prefs: []
  type: TYPE_NORMAL
- en: and I can challenge you with a more complicated without conditional probabilities
    and learn an interesting general case!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: let us take this Venn diagram with 3 events (\(A, B, C\)) and all possible event
    intersections labeled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/63615b4387d8a1e2080ddc3fc29f841c.png)'
  prefs: []
  type: TYPE_IMG
- en: A more complicated case of conditional probability dealing with 3 events, \(A,
    B, C\).
  prefs: []
  type: TYPE_NORMAL
- en: now we calculate the conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(A \cap B} \]
  prefs: []
  type: TYPE_NORMAL
- en: Recalling the definition of conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A \cap B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: so we can reorder this to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{P(A \cap B)} = P(B|A) \cdot {P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: spoiler alert, later we will define this at the multiplication rule, but for
    now we can substitute this as the denominator for the conditional probability,
    \(P(C|B \cap A)\), to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(B|A) \cdot P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can reorder this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B \cap C) = P(C|B \cap A) \cdot P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: Do you see the pattern? We can calculate high order probability intersections
    as a sequential set, product sum of marginal probability and then growing conditional
    probabilities. Let me state it with words,
  prefs: []
  type: TYPE_NORMAL
- en: probability of \(A\), \(B\) and \(C\) is the probability of \(A\) times the
    probability of \(B\) given \(A\), times the probability of \(C\) given \(B\) and
    \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, we can generalize this for any number of events as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1 \cap A_2 \cap \ldots \cap A_n) = P(A_n|A_{n-1} \cap \ldots \cap A_1)
    \cdot P(A_{n-1}|A_{n-2} \cap \ldots \cap A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot
    P(A_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: or more compactly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1, A_2, \ldots , A_n) = P(A_n|A_{n-1}, \ldots , A_1) \cdot P(A_{n-1}|A_{n-2},
    \ldots , A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: Are we just showing off now? No! This example teaches us a lot about how conditional
    probabilities work.
  prefs: []
  type: TYPE_NORMAL
- en: also, this is exactly the concept used with sequential Gaussian simulation where
    we sample sequentially from conditional distributions instead of from the high
    order joint probability, an impossible task without the sequential approach!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are ready to summarize marginal, conditional and joint probabilities
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Marginal, Conditional and Joint Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we define marginal, conditional and joint probability, provide notation
    and discuss how to calculate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginal Probability** - probability of an event, irrespective of any other
    event,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A), P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities are calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \frac{n(A)}{n(\Omega)} \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities may be calculated from joint probabilities through the
    process of marginalization,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \int_{-\infty}^{\infty} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: where we integrate over all cases of the other events, \(B\), to remove them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given discrete cases of event \(B\) we can simply sum the probabilities over
    all cases of \(B\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \sum_{i=1}^{k_B} P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability** - probability of an event, given another event
    has already occurred,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \text{ given } B), P(B \text{ given } A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with compaction probability notation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B), P(B|A) \]
  prefs: []
  type: TYPE_NORMAL
- en: see the previous section for more information, but for symmetry we repeat the
    method to calculate conditional probability as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: of course, as we saw in our conditional probability examples above, we cannot
    switch the order of the events,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B|A) \ne P(A|B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint Probability** - probability of multiple events occurring together,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \text{ and } B), P(B \text{ and } A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with compaction probability notation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B), P(B \cap A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or even more compact as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B), P(B,A) \]
  prefs: []
  type: TYPE_NORMAL
- en: of course, the ordering does not matter for joint probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(B,A) \]
  prefs: []
  type: TYPE_NORMAL
- en: we calculate joint probabilities as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = \frac{n(A,B)}{n(\Omega)} \]
  prefs: []
  type: TYPE_NORMAL
- en: To clarify the concept of marginal, conditional and joint probability (and distributions),
    I have coded a set of interactive Python dashboards, [Interactive_Marginal_Joint_Conditional_Probability](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Marginal_Joint_Conditional_Probability.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: that provides a dataset and interactively calculates and compares marginal,
    conditional and joint probabilities and visualizes and compares the marginal and
    conditional distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ada0a5ff99a013044e2ec05654f7710a.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the interactive Python dashboards from my series on marginal, conditional
    and joint probability and distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Multiplication Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown above, we can calculate the joint probability of \(A\) and \(B\) as
    the product of the conditional probability of \(B\) given \(A\) with the marginal
    probability of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: and of course, we can also state,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cap A) = P(B,A) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: this is know as the multiplication rule, we use the multiplication rule to develop
    the definition of independence.
  prefs: []
  type: TYPE_NORMAL
- en: Independent Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the probability multiplication rule,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: now we ask the question, what is the conditional probability \(P(B|A)\) if events
    \(A\) and \(B\) are independent? Given independence between \(A\) and \(B\) we
    would expect that knowing about \(A\) will not impact the outcome of \(B\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: if we substitute this into \(P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)\) we get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(B) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: by the same logic we can show,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can summarize this as, events \(A\) and \(B\) are independent if and
    only if the following relations are true,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A \cap B) = P(A) \cdot P(B)\) - joint probability is the product of the
    marginal probabilities
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(A|B) = P(A)\) - conditional is the marginal probability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B|A) = P(B)\) - conditional probability is the marginal probability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any of these are violated, we can suspect that there exists some form of
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: we leave the significance of this result outside the scope for this chapter
    on probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: certainly, if we can assume independence this simplifies our data science workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence Example #1** - check independence for the following dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/222dd21803448a6193e3a86b018195b1.png)'
  prefs: []
  type: TYPE_IMG
- en: If event \(A_1\) is Fm facies in Middle Unit (blue circles) and event \(A_2\)
    is Sm facies in Bottom Unit (red circles), and joint cases of event \(A_1\) and
    \(A_2\) (black rectangles for joint \(A_1\) and \(A_2\) occur together) are \(A_1\)
    and \(A_2\) independent events?
  prefs: []
  type: TYPE_NORMAL
- en: To check for independence, we calculate the marginal and joint probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1) = \frac{5}{10} \quad P(A_2) = \frac{6}{10} \quad P(A_1,A_2) = \frac{2}{10}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: now we check one of the conditions for independence, as the joint probability
    as product of the marginal probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A_1 \cap A_2) = P(A_1) \cdot P(A_2)\) \(\rightarrow\) \(0.2 \ne 0.5 \cdot
    0.6\), \(\therefore\) we suspect a relationship between events \(A_1\) and \(A_2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we finally dive into the Bayesian approach to probability, a very flexible
    approach that can be applied to assign probability to anything, and includes a
    framework for updating with new information.
  prefs: []
  type: TYPE_NORMAL
- en: When I say that the Bayesian approach can assign probability to anything, I
    realize this needs some further explanation,
  prefs: []
  type: TYPE_NORMAL
- en: of course, remember my statements in the ‚ÄúA Warning about Calculating Probability‚Äù
    section above, there are times when we do not have enough data, and there are
    workflows that are incorrect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I mean that any type of information can be encoded into the Bayesian framework,
    including belief
  prefs: []
  type: TYPE_NORMAL
- en: Let us discuss this supported by a great example from the introduction chapter
    of Sivia (1996), ‚ÄúWhat is the Mass of Jupyter?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1f7bb75c53f50d876649588c61eed7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Jupyter image from New Horizons Long Range Imager (LORRI), taken at 57 million
    km on January 2007\. Image from https://en.wikipedia.org/wiki/Jupiter#/media/File:Jupiter_New_Horizons.jpg.
  prefs: []
  type: TYPE_NORMAL
- en: What would be frequentist and Bayesian approaches to calculate the probabilities
    for the uncertainty model of the mass of Jupiter?
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist perspective - calculate the cumulative distribution function by
    measuring the mass of enough Jupiter-like planets from many star systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian - form a prior probability and update with any available information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now to anyone saying, the frequentist approach is possible, remember the first
    exoplanets were discovered in 1995 by astronomers Michel Mayor and Didier Queloz.
  prefs: []
  type: TYPE_NORMAL
- en: not coincidentally, the first discovered exoplanet was a Jupyter-mass hot, gas
    giant, as these are easier to see with the Doppler-based wobble method, because
    near-to-star gas giants have greater pull on their star resulting in a relatively
    high amplitude and frequency that can be readily observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when Sivia was authoring his book, there were no known exoplanets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we didn‚Äôt have other Jupiter like planets to apply the frequentist approach,
    but we could use our knowledge about the formation of solar systems to formulate
    a prior model. Then we could use any available observations or measurements from
    Jupiter to update this prior. We could do something!
  prefs: []
  type: TYPE_NORMAL
- en: but I am getting ahead of myself, we must properly introduce the Bayesian approach
    to probability, by starting with Bayes‚Äô theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes‚Äô Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once again, here‚Äôs the multiplication rule,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cup A) = P(B,A) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: and it follows that we can also express it as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: If follows that,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cup A) = P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: therefore, we can substitute the right-hand sides of the multiplication rules
    above into this equality as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) \cdot P(B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: this is Bayes‚Äô theorem. We can make a simple modification to get the popular
    form of Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: To better explain Bayes‚Äô theorem, we replace the \(A\) with ‚ÄúModel‚Äù and \(B\)
    with ‚ÄúNew Data‚Äù, where ‚ÄúNew Data‚Äù is the new data that we are updating with.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{Model}|\text{New Data}) = \frac{P(\text{New Data}|\text{Model}) \cdot
    P(\text{Model})}{P(\text{New Data})} \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can see Bayesian updating as taking a model and updating it with new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Now we make some observations about Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: We are reversing conditional probabilities to get \(P(A|B)\) from \(P(B|A)\),
    this often comes in handy because we readily have access to one but not the other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The probabilities in Bayes‚Äô theorem are known as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B)\) - evidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(A)\) - prior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A)\) - likelihood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(A|B)\) - posterior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can substitute the probabilities with their labels,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Prior should have no information from likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the prior probability is estimated prior to the collection of the new information
    in the likelihood. If the prior and likelihood includes new information, then
    this is ‚Äúdouble accounting‚Äù of the new information!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evidence term is usually just a standardization to ensure probability closure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: evidence probability is often calculated with marginalization,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = \int_{A} P(B|A) \cdot P(A) dA \]
  prefs: []
  type: TYPE_NORMAL
- en: for the example or two mutually exclusive, exhaustive outcomes, \(A\) and \(A^c\),
    then this marginalization can be applied to calculate \(P(B)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c) \]
  prefs: []
  type: TYPE_NORMAL
- en: and by substitution we get this expanded but usually easier to calculate form
    of Bayes‚Äô theorem,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize this marginalization for any number of discrete, mutually
    exclusive, exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = \sum_{i=1}^{m} P(B|A_i) \cdot P(A_i), \quad \forall \quad i = 1,\ldots,m
    \]
  prefs: []
  type: TYPE_NORMAL
- en: in some cases, like Markov chain Monte Carlo with the Metropolis-Hastings algorithm,
    we eliminate the need for the evidence term because we calculate a probability
    ratio of the proposed step vs. the current state and the evidence probability
    cancels out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the prior is na√Øve then the posterior is equal to the likelihood. This is
    logical, if we know nothing prior to collecting the new data then we completely
    rely on the likelihood probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a na√Øve prior is a maximum uncertainty prior, like the uniform probability with
    all outcomes equal probable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: under the assumption of standard normal, global Gaussian distribution with a
    mean of 0.0 and standard deviation of 1.0, a na√Øve prior is the standard normal
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the likelihood probability is na√Øve then the posterior is equal to the prior.
    Once again, this is logical, if the new data provides no information, then we
    should continue to rely on the prior probability and the original model is not
    updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayesian Probability Example Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An efficient way to improve your understanding of Bayesian probability is through
    solving illustrative problems. Here‚Äôs a great group of problems,
  prefs: []
  type: TYPE_NORMAL
- en: you conduct a test and get a positive test result for something we can‚Äôt directly
    observe happening, but what is the probability of the thing actually happening
    given the positive test?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs some examples of tests that fit in this class of problems,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2624fe69f396e2d3677c62b7a46ec9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example cases of a tests, the underlying event is happening vs. a test that
    indicates the underlying event is happening.
  prefs: []
  type: TYPE_NORMAL
- en: and we can rewrite Bayes‚Äô theorem for the case of one of these tests,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02aa6aa53eb33b77c1b4114e43091536.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes' theorem rewritten for the case of tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Updating Example #1** - prior information at a site suggests a deepwater
    channel reservoir exists at a given location with a probability of 0.6\. We consider
    further investigation with a 3D seismic survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '3D seismic survey will indicate a channelized reservoir:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äê is present with 0.9 probability if it really is present
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äê is not present with a probability 0.7 if it really is not
  prefs: []
  type: TYPE_NORMAL
- en: We have our test, seismic survey, that will offer new data, and we have our
    prior information (prior to collecting the new seismic data). Now we define our
    events,
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) = the deepwater channel is present, then \(P(A)\) is the probability the
    thing is happening before collecting the new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(B\) = new seismic shows a deepwater channel, then \(P(B)\) is the probability
    of a positive test for the thing happening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It follows that the compliments are,
  prefs: []
  type: TYPE_NORMAL
- en: \(A^c\) = the deepwater channel not present, then \(P(A^c)\) is the probability
    the thing is not happening before collecting the new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(B^c\) = new seismic does not show a deepwater channel, then \(P(B^c)\) is
    the probability of a negative test for the thing happening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we write out Bayes‚Äô theorem, the regular and expanded evidence term by marginalization,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B|A)
    \cdot P(A) + P(B|A^c) \cdot P(A^c)} \]
  prefs: []
  type: TYPE_NORMAL
- en: What do we know? From the question above we have,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A) = 0.6\) - ‚Äúprior information at a site suggests a deepwater channel reservoir
    exists at a given location with a probability of 0.6‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A) = 0.9\) - ‚Äúis present with 0.9 probability if it really is present‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B^c|A^c) = 0.7\) - ‚Äúis not present with a probability 0.7 if it really is
    not‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we also need \(P(A^c)\) and \(P(B|A^c)\). We calculate these from closure
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A^c) = 1.0 - P(A) = 1.0 - 0.6 = 0.4\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A^c) = 1.0 - P(B^c|A^c) = 1.0 - 0.7 = 0.3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have all the information that we need to substitute,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.3 \cdot 0.4} = 0.82 \]
  prefs: []
  type: TYPE_NORMAL
- en: Given a positive test, seismic indicating a deepwater channel is present, we
    have posterior probability of 0.82.
  prefs: []
  type: TYPE_NORMAL
- en: Is seismic data useful? How do we access this?
  prefs: []
  type: TYPE_NORMAL
- en: consider the change from prior probability, 0.6, to posterior probability, 0.82,
    showing a reduction in uncertainty. By integrating the value and potential loss
    of this decision it is possible to now assign the value of information for seismic
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I leave decision analysis out of scope for this e-book, but it is a fascinating
    topic, and I recommend anyone involved in data science to learn at least the basics
    to ensure you add value by impacting the decision(s)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is quite instructive to conduct a sensitivity on this example problem to
    assess the behavior of Bayesian updating. You could download and run my [interactive
    Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: it is a custom dashboard to visualize Bayesian updating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cdc74f980ae59f1da85ef88b5b1e69d0.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard demonstrating Bayesian updating. Note, \(H\)
    represents the thing is happening (\(A\) in our example), and \(+\) represents
    a positive test and \(-\) represents a negative test (\(B\) and \(B^c\) respectively
    in our example).
  prefs: []
  type: TYPE_NORMAL
- en: We have improved the test precision, probability of seismic detecting a channel
    given it is present, \(P(B|A)\) from 0.9 to 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.99 \cdot 0.6}{0.99 \cdot 0.6 + 0.3 \cdot 0.4} = 0.83 \]
  prefs: []
  type: TYPE_NORMAL
- en: we have almost no change in the posterior probability. Now we have improved
    our ability to detect no channel given the channel is not present, \(P(B|A^c)\)
    from 0.3 to 0.03.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.03 \cdot 0.4} = 0.98 \]
  prefs: []
  type: TYPE_NORMAL
- en: we change the posterior probability from 0.82 to 0.98! What happened?
  prefs: []
  type: TYPE_NORMAL
- en: our test was being impacted by a relatively high probability of false positive,
    seismic showing a channel when there is no channel present, \(P(B|A^c) = 0.3\)
    paired with a relatively high rate of no channel present, \(P(A^c) = 0.4\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: now we switch gears and look at another illustrative example of Bayesian updating
    with machines on a production line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Updating Example #2** - You have 3 machines making the same product
    (imagine a large production line). They have different volumes and errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Note we assume the products are mutually exclusive (only come from a single
    machine), and exhaustive (all products come from one of the 3 machines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2bbde9fa70803137b7d119b3a7e9636c.png)'
  prefs: []
  type: TYPE_IMG
- en: Machines 1, 2 and 3, produce the same product that are then mixed in the assembly
    line. They each have different production rates (percentage of total and error
    rates).
  prefs: []
  type: TYPE_NORMAL
- en: We want to know given an error in an individual product, what is the probability
    that the product came from a specific machine? First, we define our variables,
  prefs: []
  type: TYPE_NORMAL
- en: \(Y\) = the product has an error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(X_1, X_2, X_3\) = the product came from machines 1, 2 or 3 respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our Bayesian formulation for probability of defective (product with error) product
    coming from a specific machine is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_i|Y) = \frac{P(Y|X_i) \cdot P(X_i)}{P(Y)} \quad \forall \quad i = 1,
    2, 3 \]
  prefs: []
  type: TYPE_NORMAL
- en: we will need to calculate our evidence term, P(Y) to solve this for any product.
    Also, we only must do this once, the evidence term is the same for all products.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply marginalization to accomplish this by substituting our variables
    into the previously introduced general equation as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = \sum_{i=1}^{m} P(Y|X_i) \cdot P(X_i), \quad \forall \quad i = 1,\ldots,3
    \]
  prefs: []
  type: TYPE_NORMAL
- en: our specific form for our problem is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = P(Y|X_1) \cdot P(X_1) + P(Y|X_2) \cdot P(X_2) + P(Y|X_3) \cdot P(X_3)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we substitute the probabilities from the problem statement as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = 0.2 \cdot 0.05 + 0.3 \cdot 0.03 + 0.5 \cdot 0.01 = 0.024 \]
  prefs: []
  type: TYPE_NORMAL
- en: Given this evidence probability, now we can calculate the posterior probabilities
    of the product coming from each machine given the product is defective,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_1|Y) = \frac{P(Y|X_1) \cdot P(X_1)}{P(Y)} = \frac{0.05 \cdot 0.2}{0.024}
    = 0.41 \]\[ P(X_2|Y) = \frac{P(Y|X_2) \cdot P(X_2)}{P(Y)} = \frac{0.03 \cdot 0.3}{0.024}
    = 0.38 \]\[ P(X_3|Y) = \frac{P(Y|X_3) \cdot P(X_3)}{P(Y)} = \frac{0.01 \cdot 0.5}{0.024}
    = 0.21 \]
  prefs: []
  type: TYPE_NORMAL
- en: Let us close the loop by checking closure, we expect these posterior probabilities
    to sum to 1.0 over all machines (once again assuming products are mutually exclusive,
    and exhaustive over the machines),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_1|Y) + P(X_2|Y) + P(X_3|Y) = 0.41 + 0.38 + 0.21 = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and we have closure.
  prefs: []
  type: TYPE_NORMAL
- en: These two examples are very helpful to understand Bayesian updating. In the
    classroom I get my students to calculate the additional posteriors such as the
    probability of the event happening given a negative test, etc.
  prefs: []
  type: TYPE_NORMAL
- en: my second dashboard in this Jupyter notebook [interactive Bayesian updating
    dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    includes these examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I invite you to play with Bayesian updating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/25502cc12d722a9c207d481f3da67a20.png)'
  prefs: []
  type: TYPE_IMG
- en: My more complete interactive Python dashboard demonstrating Bayesian updating
    for all possible posterior probabilities. Note, \(H\) represents the thing is
    happening \(A\) in our example, and \(+\) represents a positive test and \(-\)
    represents a negative test, \(B\) and \(B^c\) respectively in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Updating with Gaussian Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sivia (1996) provided an analytical approach for Bayesian updating under the
    assumption of a standard normal global distribution, Gaussian distributed with
    mean of 0.0 and variance of 1.0\. We calculate,
  prefs: []
  type: TYPE_NORMAL
- en: the mean of the posterior distribution from the prior and likelihood mean and
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mu_{\text{posterior}} = \frac{\mu_{\text{likelihood}} \cdot \sigma^2_{\text{prior}}
    + \mu_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}}
    \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0} \]
  prefs: []
  type: TYPE_NORMAL
- en: the variance of the posterior distribution from the prior and likelihood variances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma^2_{\text{posterior}} = \frac{\sigma^2_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0
    ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0
    \right]+1.0} \]
  prefs: []
  type: TYPE_NORMAL
- en: consistent with the multivariate Gaussian distribution this posterior is homoscedastic,
    the prior or likelihood means are not in the equation for posterior variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: my third dashboard in this Jupyter notebook [interactive Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    includes Bayesian updating with Gaussian distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1a20eb3b488e5034f7fbf4ed5cafb8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard demonstrating Bayesian updating under the assumption
    of standard normal global distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This example helps my students understand the concept of Bayesian updating for
    building uncertainty distributions. Here are some things to try out,
  prefs: []
  type: TYPE_NORMAL
- en: a na√Øve prior or likelihood and observe the posterior is the same as the likelihood
    or prior, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a very certain, low variance prior. Start with a large prior variance and as
    you reduce the variance, the posterior is pulled towards the prior. The influence
    of prior and likelihood is proportional to their relative certainty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a contradiction between prior and likelihood with very different means. You
    may be able to even introduce extrapolation, a low prior positive mean with higher
    positive likelihood mean can result in an even higher positive posterior mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of probability. Much more could be done and discussed,
    I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why cover probability at the beginning of an e-book or course on machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Formulation** - many of our machine learning models are formulated
    with probability concepts, for example, naive Bayes classification is derived
    from Bayes‚Äô Theorem and Bayesian linear regression estimates the probability distributions
    for our model parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Cleaning and Preparation** - is 90% of any machine learning workflow
    and we cannot complete these steps without understanding our data distributions
    and statistics and all of these are based on probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss Functions and Optimization** - many of our machine learning models are
    trained through optimization of a loss function that relies on probability for
    stochastic steps or even directly in the loss function as is the case for maximum
    likelihood estimation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tuning Machine Learning Models** - machine learning model tuning to reduce
    model overfit is based on the concept of expected model performance in the presence
    of various uncertainty sources, and probability is the language of statistical
    expectation and uncertainty.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Machine Learning Model Choice** - You will make choices between frequentist
    and Bayesian predictive machine learning models and their differences are the
    result of two distinct approaches to calculate probability and build models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Real World Applications** - to make the best choices in machine learning
    workflows design and to use our results to support decisions we must integrate
    probability concepts Using our models in the real world.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us build our machine learning skills on a solid foundation of probability!
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs: []
  type: TYPE_NORMAL
- en: Probability is an essential prerequisite for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Now we get started by defining probability and then we will be ready to talk
    about ways to calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: What is Probability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand what is probability consider Kolmogorov‚Äôs 3 axioms for probabilities,
    i.e., the rules that any measure of probability must honor,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53594a139b9058c07be33e4cff134ea4.png)'
  prefs: []
  type: TYPE_IMG
- en: Andrey Kolmogorov (1903 ‚Äì 1987), Soviet mathematician, photo taken 1972 and
    from image from https://culturemath.ens.fr/thematiques/biographie/life-and-work-kolmogorov).
  prefs: []
  type: TYPE_NORMAL
- en: '**non-negativity** - probability of an event is a non-negative number'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(ùê¥) \ge 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: imagine negative probability!
  prefs: []
  type: TYPE_NORMAL
- en: '**normalization** - probability of the entire sample space is one (unity),
    also known as probability closure'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: something happens!
  prefs: []
  type: TYPE_NORMAL
- en: '**additivity** - addition of probability of mutually exclusive events for unions'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P\left(‚ãÉ_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: e.g., probability of \(A_1\) and \(A_2\) mutual exclusive events is, \(Prob(A_1
    + A_2) = Prob(A_1) + Prob(A_2)\)
  prefs: []
  type: TYPE_NORMAL
- en: note our concise notation, \(P(\cdot)\), for probability of event in the bracket
    occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a good place to start for valid probabilities, we will refine this later.
    Now we can ask,
  prefs: []
  type: TYPE_NORMAL
- en: How to Calculate Probability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are 3 probability perspectives that can be applied to calculate probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability by long-term frequencies** (Frequentist Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: probability as ratio of outcomes from an experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: requires repeated observations from a controlled experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, you flip a coin 100 times and count the number of outcomes with
    heads \(n(\text{heads})\) and then calculate this ratio,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{heads}) = \frac{n(\text{heads})}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: this is the frequentist approach to calculate probabilities. The experiment
    is the set of coin tosses.
  prefs: []
  type: TYPE_NORMAL
- en: one issue with frequentist probabilities is, can we now use this probability
    of heads outside the experiment? For example,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: on another day?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a different person tossing the coin?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a different coin?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability by physical tendencies or propensities** (Engineering Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: probability calculated from knowledge about the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we could know the probability of coin toss outcomes without the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is the engineering approach to probability, model the system and use this
    model, i.e., the physics of the system to calculate probability of outcomes
  prefs: []
  type: TYPE_NORMAL
- en: did you know that there is a \(\frac{1}{6,000}\) probability of a coin landing
    and staying upright on its edge? This could be lower depending of manner of tossing,
    coin thickness and characteristics of the surface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability by Degrees of belief** (Bayesian Probability),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: first we specify the scientific concept of ‚Äúbelief‚Äù as your opinion based on
    all your knowledge and experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our model integrates our certainty about a result and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: this is very flexible, we can assign probability to any event, and includes
    a framework for updating with new information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if this is your approach, then you are using the Bayesian approach for probability.
    If not, before you dismiss this approach let me make a couple arguments,
  prefs: []
  type: TYPE_NORMAL
- en: you may be bothered by this idea of belief, as it may seem subjective compared
    to the probabilities objectively measured from a frequentist‚Äôs experiment but,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to use the frequentist probability you have to make the subjective decision
    to apply it outside the experiment, i.e., we need to go beyond a single coin!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: the Bayesian probability approach includes objective probabilities from experiments,
    but it also allows for integration of our expert knowledge
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A Warning about Calculating Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Statistics can be misused or even abused, leading to flawed conclusions and
    poor decision-making. When probabilities are calculated improperly or misinterpreted,
    it can result in significant consequences. Here are a few examples of what can
    go wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Insufficient sampling** - there are various rules of thumb about what constitutes
    a small sample size, i.e., too small for inference about the population parameters
    and too small to train a reliable prediction model. Some say 7, some say 30, indubitably
    there is a minimum sample size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: in general, our models will communicate increasing uncertainty as the number
    of data decreases, but at some point these models break!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: consider bootstrap, yes as the number of samples decrease the uncertainty in
    the statistic reported by bootstrap increases, but this uncertainty distribution
    is centered on the sample statistic! In other words, we need enough samples to
    have a reasonable estimate of the uncertainty model‚Äôs expectation in the first
    place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: of course, there is minimum number of samples to complete a mathematical operation,
    for example principal components analysis can only calculate \(n-1\) principal
    components (where \(n\) is the number of samples) and we cannot fit a linear regression
    model to \(n=1\) samples. Best practice is to stay very far away (have many more
    samples) from any of these algorithm limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Biased sampling** - in general our probability calculations will not automatically
    debias the sample data. Any bias in the samples will pass bias through to the
    probabilities representing our uncertainty model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for example, a biased sample mean will bias simple kriging estimates away from
    data. Geostatistical simulation reproduces the entire input feature distribution,
    so any bias in any part of the distribution will be passed to the spatial models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: also, there is bad data. When we use Bayesian methods and rely on expert experience,
    that is not a license to say anything and defend it as belief. On the contrary,
    we must rigorously document and defend all our choices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unskilled practice and a lack of rigor** - there are mistakes that can be
    made with probabilities and some of them are shockingly common.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: our first line of defense is to understand what our methods are doing under
    the hood! This will help us recognize logical inconsistencies as we build our
    workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our second line of defense is to check every step in our workflows. Like an
    accountant we must close the loops with all our probability calculations, a process
    that accountants call reconciliation. Like a software engineer we must unit test
    every operation to ensure we don‚Äôt introduce errors as we update our probability
    workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for example, if we perform primary feature cosimulation with the same random
    number seed as the secondary feature simulation we will introduce artificial correlation
    between the simulated primary and secondary features that will dramatically change
    the conditional and joint probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in another example, if we use information from the likelihood probabilities
    to inform the prior probabilities, we will significantly under-estimate the uncertainty.
    This error is a form of information leakage, descriptively we state it as ‚Äúdouble
    dipping‚Äù from the information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please remember these warnings as you proceed below and onward as you build
    your own data science workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Venn Diagrams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Venn Diagrams are a tool to communicate probability.
  prefs: []
  type: TYPE_NORMAL
- en: representing the possible events or outcomes (\(A, B,\ldots\)) of an experiment
    within the collection of all possible events or outcomes, the sample space (\(\Omega\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c212b1fc6b3c5da35b7df52769a0af42.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple example Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: What do we learn from this Venn diagram?
  prefs: []
  type: TYPE_NORMAL
- en: the relative size of regions for each event within is the relative probability
    of occurrence, probability of \(B\) is greater than probability of \(A\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the overlap over events is the probability of joint occurrence, there is 0.0
    probability of \(A\) and \(B\) occurring together
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us include a more practical Venn diagram to ensure this concept is not too
    abstract. Here‚Äôs a Venn diagram from 3,000 core samples with interpreted facies
  prefs: []
  type: TYPE_NORMAL
- en: the events are sandstone (Sm) and mudstone (Fm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/824ce0a24892a79f960068d34d629915.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing the facies assignments for 3,000 core samples.
  prefs: []
  type: TYPE_NORMAL
- en: What do we learn from this Venn diagram?
  prefs: []
  type: TYPE_NORMAL
- en: mudstone is more likely than sandstone over these core samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are a lot of samples that are neither sandstone nor mudstone, the white
    space within \(\Omega\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are samples that are interpreted as both sandstone and mudstone, i.e.,
    interbedded sandstone-mudstone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: do not forget to draw and label the \(\Omega\) box or we can‚Äôt understand the
    relative probability of the events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can use any convenient shape to represent an event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Venn diagrams are an excellent tool to visualize probability, so
    we will use them to visualize and teach probability here.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now provide an extended definition for the frequentist approach for probability.
    A measure of the likelihood that an event will occur based on frequencies observed
    from an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For random experiments and well-defined settings, we calculate probability
    as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \lim_{n \to \infty} \frac{n(A)}{n} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(n(A)\) = number of times event \(A\) occurred \(n\) = number of trails
  prefs: []
  type: TYPE_NORMAL
- en: we use limit notation above to indicate sufficient sampling and that the solution
    converges and improves accuracy as we introduce more samples from our experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example,
  prefs: []
  type: TYPE_NORMAL
- en: probability of drilling a dry hole, encountering sandstone, and exceeding a
    rock porosity of \(15\%\) at a location (\(\bf{u}_{\alpha}\)) based on historical
    results for drilling in this reservoir
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we walk-through various probability operations from the frequentist perspective,
    may I ask for patience from my Bayesian friends as we will later return to the
    Bayesian perspective for probability.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Union of Events** - for example, all outcomes in the sample space that belong
    to either event \(A\) or \(B\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the union of events operator, we use the word ‚Äúor‚Äù and the mathematics symbol,
    \(cup\), using set notation we state the samples in the union \(A\) or \(B\) as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cup B = \{x: x \in A \text{ or } x \in B\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for a union as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating the union \(P(A \cup B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01902ece9dac16a02139346ccccf2574.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing union probability operator for events \(A\) or \(B\).
    Note, the legend on the top right is included to clarify the plot but is not part
    of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Intersection of Events** - for example, all outcomes in the sample space
    that belong to both events \(A\) and \(B\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the intersection of events operator, we use the word ‚Äúand‚Äù and the mathematics
    symbol \(cap\), using set notation we state the samples in the intersection \(A\)
    and \(B\) as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for an intersection as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with the common probability shorthand as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: we will call this a joint probability later. Here is a Venn diagram illustrating
    the intersection \(P(A \cap B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55a85b2ac3f3804377f80585821678e0.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing intersection probability operator for events \(A\)
    and \(B\). Note, the legend on the top right is included to clarify the plot but
    is not part of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compliment of Events** - for example, all outcomes in the sample space that
    do not belong to event \(A\)'
  prefs: []
  type: TYPE_NORMAL
- en: for the compliment of event(s) operator we use the word ‚Äúnot‚Äù and the mathematics
    symbol \(^c\), using set notation, we state the samples in the compliment of \(A\)
    as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A^c = \{x: x \notin A\} \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for a compliment as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A^c) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating the compliment \(P(A^c)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/507be2ebc15778352071fdbdd52d2a3c.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing probability compliment operator for events \(A\).
    Note, the legend on the top right is included to clarify the plot but is not part
    of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutually Exclusive Events** - for example, events do not intersect or do
    not have any common outcomes, \(A\) and \(B\) do not occur at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ A \cap B = \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: or with the common probability shorthand as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating mutual exclusive events \(A\) and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c212b1fc6b3c5da35b7df52769a0af42.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\) as mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exhaustive, Mutually Exclusive Events** the sequence of events whose union
    is equal to the sample space, all-possible events (\(\Omega\)) and there is no
    intersection between the events:'
  prefs: []
  type: TYPE_NORMAL
- en: using set notation, we state events \(A\) and \(B\) are exhaustive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ or } x \in B \} = \Omega \]'
  prefs: []
  type: TYPE_NORMAL
- en: and events \(A\) and \(B\) are mutually exclusive as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\[ \{x: x \in A \text{ and } x \in B \} = \emptyset \]'
  prefs: []
  type: TYPE_NORMAL
- en: and the probability notation for exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and for mutually exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram illustrating mutual exclusive, exhaustive events \(A\)
    and \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02c7a7dba945af2e442593dbf746edb5.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\) as exhaustive, mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Combinations of Operators** - we can use these probability operators with
    any number of events to communicate complicated probability cases. For example,
    let us define these events,'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event \(A\): oil present (\(A^c\): dry hole)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event \(B\): ùëÜùëö (\(B^c\): ùêπùëö)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event \(C\): porosity ‚â• 15% (\(C^c\): porosity < 15%)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of dry hole with massive sandstone (ùëÜùëö) and porosity
    ‚â• 15%?
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A^c \cap B \cap C) = \frac{\text{Area}(A^c \cap B \cap C)}{\text{Area}(\Omega)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram representing this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe9499a30db96f497a94f9d209947cec.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing a more complicated case with 3 events, \(A, B, C\),
    with compliment and intersection probability operators.
  prefs: []
  type: TYPE_NORMAL
- en: Constraints on Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have defined probability notation and probability operations, now
    we return the idea of permissible probability values expressed by Kolmogorov.
    We can now build on Kolmogorov‚Äôs probability axioms with this set of probability
    constraints,
  prefs: []
  type: TYPE_NORMAL
- en: Non-negativity, Normalization constraints include,
  prefs: []
  type: TYPE_NORMAL
- en: Probability is bounded,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 0.0 \le P(A) \le 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability must be between 0.0 and 1.0 (including 0.0 and 1.0)
  prefs: []
  type: TYPE_NORMAL
- en: Probability closure,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(\Omega) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of any event is 1.0
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A) + P(A^c) = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of A or not A is 1.0
  prefs: []
  type: TYPE_NORMAL
- en: Probability for null sets,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(\emptyset) = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: probability of nothing happens is zero
  prefs: []
  type: TYPE_NORMAL
- en: Now we use these probability concepts and notation to append more essential
    complicated probability concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Addition Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the probability for a union of events? Remember that union is an ‚Äúor‚Äù
    operation represented by the \(\cup\) notation. Consider,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: inspection of the previously shown Venn diagram indicates that we can not calculate
    the union , \(P(A \cup B)\), by just summing \(P(A)\) and \(P(B)\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55a85b2ac3f3804377f80585821678e0.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing events \(A\) and \(B\), showing the intersection
    of \(A\) and \(B\) that will be double counted if we calculate the union and \(A\)
    or \(B\) as the sum of \(A\) and \(B\). Note, the legend on the top right is included
    to clarify the plot but is not part of the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the probabilities \(P(A)\) and \(P(B)\) will double count the intersection,
    \(P(A \cap B)\), so we must subtract it,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A) + P(B) - P(A \cap B) \]
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there is a general expression for any number of events to calculate the
    probability of the union,
  prefs: []
  type: TYPE_NORMAL
- en: I‚Äôm not going to include it here but suffice it to say that it is a book keeping
    nightmare given the combinatorial of intersections that can be counted too many
    times!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a much simpler case. If the events are mutually exclusive then there
    is no intersection, \(P(A,B) = 0.0\), and we can just sum the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: for the general case of mutually exclusive for any number of events,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A_i \cap A_j = \emptyset, \quad \forall \quad i \ne j \]
  prefs: []
  type: TYPE_NORMAL
- en: then we can write this general equation for the addition rule for any number
    of mutually exclusive events,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P\left( \bigcup_{i=1}^k A_i \right) = \sum_{i=1}^k P(A_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: Here‚Äôs a Venn diagram showing 4 mutual exclusive events, \(A_1, A_2, A_3, A_4\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aecd4b1580232c0f55e753bad39f1ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: A Venn diagram representing 4 events, \(A_1, A_2, A_3, A_4\), that are all mutually
    exclusive. We can calculate the probability of the union of these events as the
    sum of probabilities of each.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the probability of an event given another event has occurred? To discuss
    this, let us get more specific, what is the probability of event \(B\) given event
    \(A\) has occurred?
  prefs: []
  type: TYPE_NORMAL
- en: to express this we use the notation, \(P(B|A)\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we read this notation as, ‚Äúprobability of B given A‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an example of a conditional probability. We calculate conditional probability
    with this equation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A \cap B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: This may seem complicated, but we can easily visualize and understand this equation
    from our Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e6c3488677335aca9cd1bdaaa70f6bb.png)'
  prefs: []
  type: TYPE_IMG
- en: To calculate conditional probability we "shrink our universe" to the given condition.
    For the probability of $B$ given $A$ we shrink our Venn diagram to \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: after we shrink our universe, our \(\Omega\) is only event \(A\)! Now we can
    easily see that the conditional probability is simply the probability of the intersection
    of \(A\) and \(B\) divided by the probability of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b40d3731ffed4bc1939cfc97dfdce13.png)'
  prefs: []
  type: TYPE_IMG
- en: After we "shrink our universe" to the condition, we can see conditional probability
    equation clearly as the intersection of \(A\) and \(B\) divided by the probability
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: Now we introduce a couple examples to test our knowledge of conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability Example #1** - for this Venn diagram provide the
    conditional probabilities, \(P(A|B)\) and \(P(B|A)\), in terms of \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ca9cd3deee70b9c41f0865eb8f1bc9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram to check your understanding of conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: This is simple to solve, the answer is 0.0 for both conditional probability,
    because to calculate conditional probability the numerator is the probability,
    \(P(A,B)\), and this is 0.0 if there is no overlap in the Venn diagram.
  prefs: []
  type: TYPE_NORMAL
- en: also, since \(P(B,A) = P(B,A)\) the numerator is the same for both conditional
    probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B,A)}{P(B)} = \frac{0.0}{P(B)} = 0.0 \]\[ P(B|A) = \frac{P(A,B)}{P(A)}
    = \frac{0.0}{P(A)} = 0.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we attempt a more challenging example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability Example #2** - for this Venn diagram provide the
    conditional probabilities, \(P(A|B)\) and \(P(B|A)\), in terms of \(A\) and \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97292e6e32b997818928d4ab73818c14.png)'
  prefs: []
  type: TYPE_IMG
- en: Venn diagram to check your understanding of conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: This one is more interesting. Let us look at each conditional probability one
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B,A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: notice that \(P(B,A) = P(B)\) since all \(B\) is inside \(A\), so we can substitute
    \(P(B)\) for \(P(B,A)\) above,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B)}{P(B)} = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: this makes sense, given \(B\) then \(A\) must occur, the conditional probability
    is 1.0\. Now we calculate the other conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: just a reminder that \(P(B,A) = P(A,B)\) so we can use our previous result and
    substitute, \(P(B)\) for \(P(B,A)\) above.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} = \frac{P(B,A)}{P(A)} = \frac{P(B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: If you understand these two examples, then you have a good foundation in conditional
    probability,
  prefs: []
  type: TYPE_NORMAL
- en: and I can challenge you with a more complicated without conditional probabilities
    and learn an interesting general case!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: let us take this Venn diagram with 3 events (\(A, B, C\)) and all possible event
    intersections labeled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/63615b4387d8a1e2080ddc3fc29f841c.png)'
  prefs: []
  type: TYPE_IMG
- en: A more complicated case of conditional probability dealing with 3 events, \(A,
    B, C\).
  prefs: []
  type: TYPE_NORMAL
- en: now we calculate the conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(A \cap B} \]
  prefs: []
  type: TYPE_NORMAL
- en: Recalling the definition of conditional probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A \cap B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: so we can reorder this to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{P(A \cap B)} = P(B|A) \cdot {P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: spoiler alert, later we will define this at the multiplication rule, but for
    now we can substitute this as the denominator for the conditional probability,
    \(P(C|B \cap A)\), to get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(C|B \cap A) = \frac{P(A \cap B \cap C}{P(B|A) \cdot P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can reorder this as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B \cap C) = P(C|B \cap A) \cdot P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: Do you see the pattern? We can calculate high order probability intersections
    as a sequential set, product sum of marginal probability and then growing conditional
    probabilities. Let me state it with words,
  prefs: []
  type: TYPE_NORMAL
- en: probability of \(A\), \(B\) and \(C\) is the probability of \(A\) times the
    probability of \(B\) given \(A\), times the probability of \(C\) given \(B\) and
    \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, we can generalize this for any number of events as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1 \cap A_2 \cap \ldots \cap A_n) = P(A_n|A_{n-1} \cap \ldots \cap A_1)
    \cdot P(A_{n-1}|A_{n-2} \cap \ldots \cap A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot
    P(A_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: or more compactly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1, A_2, \ldots , A_n) = P(A_n|A_{n-1}, \ldots , A_1) \cdot P(A_{n-1}|A_{n-2},
    \ldots , A_1) \cdot \ldots \cdot P(A_2|A_1) \cdot P(A_1) \]
  prefs: []
  type: TYPE_NORMAL
- en: Are we just showing off now? No! This example teaches us a lot about how conditional
    probabilities work.
  prefs: []
  type: TYPE_NORMAL
- en: also, this is exactly the concept used with sequential Gaussian simulation where
    we sample sequentially from conditional distributions instead of from the high
    order joint probability, an impossible task without the sequential approach!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are ready to summarize marginal, conditional and joint probabilities
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Marginal, Conditional and Joint Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we define marginal, conditional and joint probability, provide notation
    and discuss how to calculate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Marginal Probability** - probability of an event, irrespective of any other
    event,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A), P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities are calculated as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \frac{n(A)}{n(\Omega)} \]
  prefs: []
  type: TYPE_NORMAL
- en: marginal probabilities may be calculated from joint probabilities through the
    process of marginalization,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \int_{-\infty}^{\infty} P(A,B) dB \]
  prefs: []
  type: TYPE_NORMAL
- en: where we integrate over all cases of the other events, \(B\), to remove them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: given discrete cases of event \(B\) we can simply sum the probabilities over
    all cases of \(B\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A) = \sum_{i=1}^{k_B} P(A,B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional Probability** - probability of an event, given another event
    has already occurred,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \text{ given } B), P(B \text{ given } A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with compaction probability notation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B), P(B|A) \]
  prefs: []
  type: TYPE_NORMAL
- en: see the previous section for more information, but for symmetry we repeat the
    method to calculate conditional probability as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B|A) = \frac{P(A,B)}{P(A)} \]
  prefs: []
  type: TYPE_NORMAL
- en: of course, as we saw in our conditional probability examples above, we cannot
    switch the order of the events,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B|A) \ne P(A|B) \]
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint Probability** - probability of multiple events occurring together,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \text{ and } B), P(B \text{ and } A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or with compaction probability notation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A \cap B), P(B \cap A) \]
  prefs: []
  type: TYPE_NORMAL
- en: or even more compact as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B), P(B,A) \]
  prefs: []
  type: TYPE_NORMAL
- en: of course, the ordering does not matter for joint probabilities,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = P(B,A) \]
  prefs: []
  type: TYPE_NORMAL
- en: we calculate joint probabilities as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A,B) = \frac{n(A,B)}{n(\Omega)} \]
  prefs: []
  type: TYPE_NORMAL
- en: To clarify the concept of marginal, conditional and joint probability (and distributions),
    I have coded a set of interactive Python dashboards, [Interactive_Marginal_Joint_Conditional_Probability](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Marginal_Joint_Conditional_Probability.ipynb),
  prefs: []
  type: TYPE_NORMAL
- en: that provides a dataset and interactively calculates and compares marginal,
    conditional and joint probabilities and visualizes and compares the marginal and
    conditional distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ada0a5ff99a013044e2ec05654f7710a.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the interactive Python dashboards from my series on marginal, conditional
    and joint probability and distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Multiplication Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown above, we can calculate the joint probability of \(A\) and \(B\) as
    the product of the conditional probability of \(B\) given \(A\) with the marginal
    probability of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: and of course, we can also state,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cap A) = P(B,A) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: this is know as the multiplication rule, we use the multiplication rule to develop
    the definition of independence.
  prefs: []
  type: TYPE_NORMAL
- en: Independent Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the probability multiplication rule,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: now we ask the question, what is the conditional probability \(P(B|A)\) if events
    \(A\) and \(B\) are independent? Given independence between \(A\) and \(B\) we
    would expect that knowing about \(A\) will not impact the outcome of \(B\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B|A) = P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: if we substitute this into \(P(A \cap B) = P(A,B) = P(B|A) \cdot P(A)\) we get,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cap B) = P(B) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: by the same logic we can show,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: Now we can summarize this as, events \(A\) and \(B\) are independent if and
    only if the following relations are true,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A \cap B) = P(A) \cdot P(B)\) - joint probability is the product of the
    marginal probabilities
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(A|B) = P(A)\) - conditional is the marginal probability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B|A) = P(B)\) - conditional probability is the marginal probability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any of these are violated, we can suspect that there exists some form of
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: we leave the significance of this result outside the scope for this chapter
    on probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: certainly, if we can assume independence this simplifies our data science workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence Example #1** - check independence for the following dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/222dd21803448a6193e3a86b018195b1.png)'
  prefs: []
  type: TYPE_IMG
- en: If event \(A_1\) is Fm facies in Middle Unit (blue circles) and event \(A_2\)
    is Sm facies in Bottom Unit (red circles), and joint cases of event \(A_1\) and
    \(A_2\) (black rectangles for joint \(A_1\) and \(A_2\) occur together) are \(A_1\)
    and \(A_2\) independent events?
  prefs: []
  type: TYPE_NORMAL
- en: To check for independence, we calculate the marginal and joint probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A_1) = \frac{5}{10} \quad P(A_2) = \frac{6}{10} \quad P(A_1,A_2) = \frac{2}{10}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: now we check one of the conditions for independence, as the joint probability
    as product of the marginal probabilities,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A_1 \cap A_2) = P(A_1) \cdot P(A_2)\) \(\rightarrow\) \(0.2 \ne 0.5 \cdot
    0.6\), \(\therefore\) we suspect a relationship between events \(A_1\) and \(A_2\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we finally dive into the Bayesian approach to probability, a very flexible
    approach that can be applied to assign probability to anything, and includes a
    framework for updating with new information.
  prefs: []
  type: TYPE_NORMAL
- en: When I say that the Bayesian approach can assign probability to anything, I
    realize this needs some further explanation,
  prefs: []
  type: TYPE_NORMAL
- en: of course, remember my statements in the ‚ÄúA Warning about Calculating Probability‚Äù
    section above, there are times when we do not have enough data, and there are
    workflows that are incorrect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I mean that any type of information can be encoded into the Bayesian framework,
    including belief
  prefs: []
  type: TYPE_NORMAL
- en: Let us discuss this supported by a great example from the introduction chapter
    of Sivia (1996), ‚ÄúWhat is the Mass of Jupyter?‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1f7bb75c53f50d876649588c61eed7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Jupyter image from New Horizons Long Range Imager (LORRI), taken at 57 million
    km on January 2007\. Image from https://en.wikipedia.org/wiki/Jupiter#/media/File:Jupiter_New_Horizons.jpg.
  prefs: []
  type: TYPE_NORMAL
- en: What would be frequentist and Bayesian approaches to calculate the probabilities
    for the uncertainty model of the mass of Jupiter?
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist perspective - calculate the cumulative distribution function by
    measuring the mass of enough Jupiter-like planets from many star systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian - form a prior probability and update with any available information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now to anyone saying, the frequentist approach is possible, remember the first
    exoplanets were discovered in 1995 by astronomers Michel Mayor and Didier Queloz.
  prefs: []
  type: TYPE_NORMAL
- en: not coincidentally, the first discovered exoplanet was a Jupyter-mass hot, gas
    giant, as these are easier to see with the Doppler-based wobble method, because
    near-to-star gas giants have greater pull on their star resulting in a relatively
    high amplitude and frequency that can be readily observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: when Sivia was authoring his book, there were no known exoplanets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we didn‚Äôt have other Jupiter like planets to apply the frequentist approach,
    but we could use our knowledge about the formation of solar systems to formulate
    a prior model. Then we could use any available observations or measurements from
    Jupiter to update this prior. We could do something!
  prefs: []
  type: TYPE_NORMAL
- en: but I am getting ahead of myself, we must properly introduce the Bayesian approach
    to probability, by starting with Bayes‚Äô theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes‚Äô Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once again, here‚Äôs the multiplication rule,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cup A) = P(B,A) = P(A|B) \cdot P(B) \]
  prefs: []
  type: TYPE_NORMAL
- en: and it follows that we can also express it as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A \cup B) = P(A,B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: If follows that,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(B \cup A) = P(A \cup B) \]
  prefs: []
  type: TYPE_NORMAL
- en: therefore, we can substitute the right-hand sides of the multiplication rules
    above into this equality as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) \cdot P(B) = P(B|A) \cdot P(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: this is Bayes‚Äô theorem. We can make a simple modification to get the popular
    form of Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} \]
  prefs: []
  type: TYPE_NORMAL
- en: To better explain Bayes‚Äô theorem, we replace the \(A\) with ‚ÄúModel‚Äù and \(B\)
    with ‚ÄúNew Data‚Äù, where ‚ÄúNew Data‚Äù is the new data that we are updating with.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(\text{Model}|\text{New Data}) = \frac{P(\text{New Data}|\text{Model}) \cdot
    P(\text{Model})}{P(\text{New Data})} \]
  prefs: []
  type: TYPE_NORMAL
- en: now we can see Bayesian updating as taking a model and updating it with new
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Now we make some observations about Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: We are reversing conditional probabilities to get \(P(A|B)\) from \(P(B|A)\),
    this often comes in handy because we readily have access to one but not the other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The probabilities in Bayes‚Äô theorem are known as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P(B)\) - evidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(A)\) - prior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A)\) - likelihood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(A|B)\) - posterior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can substitute the probabilities with their labels,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Prior should have no information from likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the prior probability is estimated prior to the collection of the new information
    in the likelihood. If the prior and likelihood includes new information, then
    this is ‚Äúdouble accounting‚Äù of the new information!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evidence term is usually just a standardization to ensure probability closure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: evidence probability is often calculated with marginalization,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = \int_{A} P(B|A) \cdot P(A) dA \]
  prefs: []
  type: TYPE_NORMAL
- en: for the example or two mutually exclusive, exhaustive outcomes, \(A\) and \(A^c\),
    then this marginalization can be applied to calculate \(P(B)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c) \]
  prefs: []
  type: TYPE_NORMAL
- en: and by substitution we get this expanded but usually easier to calculate form
    of Bayes‚Äô theorem,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we can generalize this marginalization for any number of discrete, mutually
    exclusive, exhaustive events as,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P(B) = \sum_{i=1}^{m} P(B|A_i) \cdot P(A_i), \quad \forall \quad i = 1,\ldots,m
    \]
  prefs: []
  type: TYPE_NORMAL
- en: in some cases, like Markov chain Monte Carlo with the Metropolis-Hastings algorithm,
    we eliminate the need for the evidence term because we calculate a probability
    ratio of the proposed step vs. the current state and the evidence probability
    cancels out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the prior is na√Øve then the posterior is equal to the likelihood. This is
    logical, if we know nothing prior to collecting the new data then we completely
    rely on the likelihood probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a na√Øve prior is a maximum uncertainty prior, like the uniform probability with
    all outcomes equal probable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: under the assumption of standard normal, global Gaussian distribution with a
    mean of 0.0 and standard deviation of 1.0, a na√Øve prior is the standard normal
    distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the likelihood probability is na√Øve then the posterior is equal to the prior.
    Once again, this is logical, if the new data provides no information, then we
    should continue to rely on the prior probability and the original model is not
    updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayesian Probability Example Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An efficient way to improve your understanding of Bayesian probability is through
    solving illustrative problems. Here‚Äôs a great group of problems,
  prefs: []
  type: TYPE_NORMAL
- en: you conduct a test and get a positive test result for something we can‚Äôt directly
    observe happening, but what is the probability of the thing actually happening
    given the positive test?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here‚Äôs some examples of tests that fit in this class of problems,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2624fe69f396e2d3677c62b7a46ec9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example cases of a tests, the underlying event is happening vs. a test that
    indicates the underlying event is happening.
  prefs: []
  type: TYPE_NORMAL
- en: and we can rewrite Bayes‚Äô theorem for the case of one of these tests,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02aa6aa53eb33b77c1b4114e43091536.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes' theorem rewritten for the case of tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Updating Example #1** - prior information at a site suggests a deepwater
    channel reservoir exists at a given location with a probability of 0.6\. We consider
    further investigation with a 3D seismic survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '3D seismic survey will indicate a channelized reservoir:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äê is present with 0.9 probability if it really is present
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äê is not present with a probability 0.7 if it really is not
  prefs: []
  type: TYPE_NORMAL
- en: We have our test, seismic survey, that will offer new data, and we have our
    prior information (prior to collecting the new seismic data). Now we define our
    events,
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) = the deepwater channel is present, then \(P(A)\) is the probability the
    thing is happening before collecting the new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(B\) = new seismic shows a deepwater channel, then \(P(B)\) is the probability
    of a positive test for the thing happening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It follows that the compliments are,
  prefs: []
  type: TYPE_NORMAL
- en: \(A^c\) = the deepwater channel not present, then \(P(A^c)\) is the probability
    the thing is not happening before collecting the new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(B^c\) = new seismic does not show a deepwater channel, then \(P(B^c)\) is
    the probability of a negative test for the thing happening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we write out Bayes‚Äô theorem, the regular and expanded evidence term by marginalization,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B|A)
    \cdot P(A) + P(B|A^c) \cdot P(A^c)} \]
  prefs: []
  type: TYPE_NORMAL
- en: What do we know? From the question above we have,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A) = 0.6\) - ‚Äúprior information at a site suggests a deepwater channel reservoir
    exists at a given location with a probability of 0.6‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A) = 0.9\) - ‚Äúis present with 0.9 probability if it really is present‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B^c|A^c) = 0.7\) - ‚Äúis not present with a probability 0.7 if it really is
    not‚Äù
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: but we also need \(P(A^c)\) and \(P(B|A^c)\). We calculate these from closure
    as,
  prefs: []
  type: TYPE_NORMAL
- en: \(P(A^c) = 1.0 - P(A) = 1.0 - 0.6 = 0.4\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P(B|A^c) = 1.0 - P(B^c|A^c) = 1.0 - 0.7 = 0.3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we have all the information that we need to substitute,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.3 \cdot 0.4} = 0.82 \]
  prefs: []
  type: TYPE_NORMAL
- en: Given a positive test, seismic indicating a deepwater channel is present, we
    have posterior probability of 0.82.
  prefs: []
  type: TYPE_NORMAL
- en: Is seismic data useful? How do we access this?
  prefs: []
  type: TYPE_NORMAL
- en: consider the change from prior probability, 0.6, to posterior probability, 0.82,
    showing a reduction in uncertainty. By integrating the value and potential loss
    of this decision it is possible to now assign the value of information for seismic
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I leave decision analysis out of scope for this e-book, but it is a fascinating
    topic, and I recommend anyone involved in data science to learn at least the basics
    to ensure you add value by impacting the decision(s)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is quite instructive to conduct a sensitivity on this example problem to
    assess the behavior of Bayesian updating. You could download and run my [interactive
    Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: it is a custom dashboard to visualize Bayesian updating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cdc74f980ae59f1da85ef88b5b1e69d0.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard demonstrating Bayesian updating. Note, \(H\)
    represents the thing is happening (\(A\) in our example), and \(+\) represents
    a positive test and \(-\) represents a negative test (\(B\) and \(B^c\) respectively
    in our example).
  prefs: []
  type: TYPE_NORMAL
- en: We have improved the test precision, probability of seismic detecting a channel
    given it is present, \(P(B|A)\) from 0.9 to 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.99 \cdot 0.6}{0.99 \cdot 0.6 + 0.3 \cdot 0.4} = 0.83 \]
  prefs: []
  type: TYPE_NORMAL
- en: we have almost no change in the posterior probability. Now we have improved
    our ability to detect no channel given the channel is not present, \(P(B|A^c)\)
    from 0.3 to 0.03.
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B|A) \cdot P(A) + P(B|A^c) \cdot P(A^c)}
    = \frac{0.9 \cdot 0.6}{0.9 \cdot 0.6 + 0.03 \cdot 0.4} = 0.98 \]
  prefs: []
  type: TYPE_NORMAL
- en: we change the posterior probability from 0.82 to 0.98! What happened?
  prefs: []
  type: TYPE_NORMAL
- en: our test was being impacted by a relatively high probability of false positive,
    seismic showing a channel when there is no channel present, \(P(B|A^c) = 0.3\)
    paired with a relatively high rate of no channel present, \(P(A^c) = 0.4\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: now we switch gears and look at another illustrative example of Bayesian updating
    with machines on a production line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bayesian Updating Example #2** - You have 3 machines making the same product
    (imagine a large production line). They have different volumes and errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Note we assume the products are mutually exclusive (only come from a single
    machine), and exhaustive (all products come from one of the 3 machines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2bbde9fa70803137b7d119b3a7e9636c.png)'
  prefs: []
  type: TYPE_IMG
- en: Machines 1, 2 and 3, produce the same product that are then mixed in the assembly
    line. They each have different production rates (percentage of total and error
    rates).
  prefs: []
  type: TYPE_NORMAL
- en: We want to know given an error in an individual product, what is the probability
    that the product came from a specific machine? First, we define our variables,
  prefs: []
  type: TYPE_NORMAL
- en: \(Y\) = the product has an error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(X_1, X_2, X_3\) = the product came from machines 1, 2 or 3 respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: our Bayesian formulation for probability of defective (product with error) product
    coming from a specific machine is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_i|Y) = \frac{P(Y|X_i) \cdot P(X_i)}{P(Y)} \quad \forall \quad i = 1,
    2, 3 \]
  prefs: []
  type: TYPE_NORMAL
- en: we will need to calculate our evidence term, P(Y) to solve this for any product.
    Also, we only must do this once, the evidence term is the same for all products.
  prefs: []
  type: TYPE_NORMAL
- en: We will apply marginalization to accomplish this by substituting our variables
    into the previously introduced general equation as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = \sum_{i=1}^{m} P(Y|X_i) \cdot P(X_i), \quad \forall \quad i = 1,\ldots,3
    \]
  prefs: []
  type: TYPE_NORMAL
- en: our specific form for our problem is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = P(Y|X_1) \cdot P(X_1) + P(Y|X_2) \cdot P(X_2) + P(Y|X_3) \cdot P(X_3)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: we substitute the probabilities from the problem statement as,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(Y) = 0.2 \cdot 0.05 + 0.3 \cdot 0.03 + 0.5 \cdot 0.01 = 0.024 \]
  prefs: []
  type: TYPE_NORMAL
- en: Given this evidence probability, now we can calculate the posterior probabilities
    of the product coming from each machine given the product is defective,
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_1|Y) = \frac{P(Y|X_1) \cdot P(X_1)}{P(Y)} = \frac{0.05 \cdot 0.2}{0.024}
    = 0.41 \]\[ P(X_2|Y) = \frac{P(Y|X_2) \cdot P(X_2)}{P(Y)} = \frac{0.03 \cdot 0.3}{0.024}
    = 0.38 \]\[ P(X_3|Y) = \frac{P(Y|X_3) \cdot P(X_3)}{P(Y)} = \frac{0.01 \cdot 0.5}{0.024}
    = 0.21 \]
  prefs: []
  type: TYPE_NORMAL
- en: Let us close the loop by checking closure, we expect these posterior probabilities
    to sum to 1.0 over all machines (once again assuming products are mutually exclusive,
    and exhaustive over the machines),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P(X_1|Y) + P(X_2|Y) + P(X_3|Y) = 0.41 + 0.38 + 0.21 = 1.0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and we have closure.
  prefs: []
  type: TYPE_NORMAL
- en: These two examples are very helpful to understand Bayesian updating. In the
    classroom I get my students to calculate the additional posteriors such as the
    probability of the event happening given a negative test, etc.
  prefs: []
  type: TYPE_NORMAL
- en: my second dashboard in this Jupyter notebook [interactive Bayesian updating
    dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    includes these examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I invite you to play with Bayesian updating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/25502cc12d722a9c207d481f3da67a20.png)'
  prefs: []
  type: TYPE_IMG
- en: My more complete interactive Python dashboard demonstrating Bayesian updating
    for all possible posterior probabilities. Note, \(H\) represents the thing is
    happening \(A\) in our example, and \(+\) represents a positive test and \(-\)
    represents a negative test, \(B\) and \(B^c\) respectively in our example.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Updating with Gaussian Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sivia (1996) provided an analytical approach for Bayesian updating under the
    assumption of a standard normal global distribution, Gaussian distributed with
    mean of 0.0 and variance of 1.0\. We calculate,
  prefs: []
  type: TYPE_NORMAL
- en: the mean of the posterior distribution from the prior and likelihood mean and
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mu_{\text{posterior}} = \frac{\mu_{\text{likelihood}} \cdot \sigma^2_{\text{prior}}
    + \mu_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0 ‚àí \sigma^2_{\text{likelihood}}
    \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0 \right]+1.0} \]
  prefs: []
  type: TYPE_NORMAL
- en: the variance of the posterior distribution from the prior and likelihood variances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \sigma^2_{\text{posterior}} = \frac{\sigma^2_{\text{prior}} \cdot \sigma^2_{\text{likelihood}}}{\left[1.0
    ‚àí \sigma^2_{\text{likelihood}} \right] \cdot \left[\sigma^2_{\text{prior}} ‚àí 1.0
    \right]+1.0} \]
  prefs: []
  type: TYPE_NORMAL
- en: consistent with the multivariate Gaussian distribution this posterior is homoscedastic,
    the prior or likelihood means are not in the equation for posterior variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: my third dashboard in this Jupyter notebook [interactive Bayesian updating dashboard](https://github.com/GeostatsGuy/DataScienceInteractivePython/blob/main/Interactive_Bayesian_Updating.ipynb)
    includes Bayesian updating with Gaussian distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1a20eb3b488e5034f7fbf4ed5cafb8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: My interactive Python dashboard demonstrating Bayesian updating under the assumption
    of standard normal global distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This example helps my students understand the concept of Bayesian updating for
    building uncertainty distributions. Here are some things to try out,
  prefs: []
  type: TYPE_NORMAL
- en: a na√Øve prior or likelihood and observe the posterior is the same as the likelihood
    or prior, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a very certain, low variance prior. Start with a large prior variance and as
    you reduce the variance, the posterior is pulled towards the prior. The influence
    of prior and likelihood is proportional to their relative certainty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a contradiction between prior and likelihood with very different means. You
    may be able to even introduce extrapolation, a low prior positive mean with higher
    positive likelihood mean can result in an even higher positive posterior mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This was a basic treatment of probability. Much more could be done and discussed,
    I have many more resources. Check out my [shared resource inventory](https://michaelpyrcz.com/my-resources)
    and the YouTube lecture links at the start of this chapter with resource links
    in the videos‚Äô descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is helpful,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/eb709b2c0a0c715da01ae0165efdf3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Professor Michael Pyrcz in his office on the 40 acres, campus of The University
    of Texas at Austin.
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz is a professor in the [Cockrell School of Engineering](https://cockrell.utexas.edu/faculty-directory/alphabetical/p),
    and the [Jackson School of Geosciences](https://www.jsg.utexas.edu/researcher/michael_pyrcz/),
    at [The University of Texas at Austin](https://www.utexas.edu/), where he researches
    and teaches subsurface, spatial data analytics, geostatistics, and machine learning.
    Michael is also,
  prefs: []
  type: TYPE_NORMAL
- en: the principal investigator of the [Energy Analytics](https://fri.cns.utexas.edu/energy-analytics)
    freshmen research initiative and a core faculty in the Machine Learn Laboratory
    in the College of Natural Sciences, The University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an associate editor for [Computers and Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences/about/editorial-board),
    and a board member for [Mathematical Geosciences](https://link.springer.com/journal/11004/editorial-board),
    the International Association for Mathematical Geosciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael has written over 70 [peer-reviewed publications](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en),
    a [Python package](https://pypi.org/project/geostatspy/) for spatial data analytics,
    co-authored a textbook on spatial data analytics, [Geostatistical Reservoir Modeling](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    and author of two recently released e-books, [Applied Geostatistics in Python:
    a Hands-on Guide with GeostatsPy](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    and [Applied Machine Learning in Python: a Hands-on Guide with Code](https://geostatsguy.github.io/MachineLearningDemos_Book/intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: All of Michael‚Äôs university lectures are available on his [YouTube Channel](https://www.youtube.com/@GeostatsGuyLectures)
    with links to 100s of Python interactive dashboards and well-documented workflows
    in over 40 repositories on his [GitHub account](https://github.com/GeostatsGuy),
    to support any interested students and working professionals with evergreen content.
    To find out more about Michael‚Äôs work and shared educational resources visit his
    Website.
  prefs: []
  type: TYPE_NORMAL
- en: Want to Work Together?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope this content is helpful to those that want to learn more about subsurface
    modeling, data analytics and machine learning. Students and working professionals
    are welcome to participate.
  prefs: []
  type: TYPE_NORMAL
- en: Want to invite me to visit your company for training, mentoring, project review,
    workflow design and / or consulting? I‚Äôd be happy to drop by and work with you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested in partnering, supporting my graduate student research or my Subsurface
    Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster,
    Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling
    and machine learning theory with practice to develop novel methods and workflows
    to add value. We are solving challenging subsurface problems!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can be reached at [mpyrcz@austin.utexas.edu](mailto:mpyrcz%40austin.utexas.edu).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôm always happy to discuss,
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael*'
  prefs: []
  type: TYPE_NORMAL
- en: Michael Pyrcz, Ph.D., P.Eng. Professor, Cockrell School of Engineering and The
    Jackson School of Geosciences, The University of Texas at Austin
  prefs: []
  type: TYPE_NORMAL
- en: 'More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy)
    | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao)
    | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446)
    | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig) | [Applied
    Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html)
    | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/)
    | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)'
  prefs: []
  type: TYPE_NORMAL
