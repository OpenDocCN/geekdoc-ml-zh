- en: Image Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类
- en: '![](../media/file539.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file539.png)'
- en: '*DALL·E prompt - 1950s style cartoon illustration based on a real image by
    Marcelo Rovai*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*DALL·E提示 - 基于Marcelo Rovai的真实图像的1950年代风格卡通插图*'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: We are increasingly facing an artificial intelligence (AI) revolution, where,
    as [Gartner](https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156)
    states, **Edge AI and Computer Vision** have a very high impact potential, and
    **it is for now**!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正日益面临一场人工智能（AI）革命，正如 [Gartner](https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156)
    所述，**边缘AI和计算机视觉**具有非常高的影响潜力，**而且目前确实是如此**！
- en: When we look into Machine Learning (ML) applied to vision, the first concept
    that greets us is **Image Classification**, a kind of ML’s *Hello World* that
    is both simple and profound!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究应用于视觉的机器学习（ML）时，首先遇到的概念是**图像分类**，这是一种ML的“Hello World”，既简单又深刻！
- en: The Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered
    around the[XIAO ESP32-S3 Sense](https://www.seeedstudio.com/xiao-series-page),
    featuring an integrated **OV3660** camera and SD card support. Those features
    make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision
    AI.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Seeed Studio XIAOML Kit提供了一套以[XIAO ESP32-S3 Sense](https://www.seeedstudio.com/xiao-series-page)为中心的全面硬件解决方案，集成了**OV3660**摄像头和SD卡支持。这些特性使XIAO
    ESP32S3 Sense成为探索TinyML视觉AI的绝佳起点。
- en: In this Lab, we will explore Image Classification using the non-code tool **SenseCraft
    AI** and explore a more detailed development with **Edge Impulse Studio** and
    **Arduino IDE**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验室中，我们将使用非代码工具**SenseCraft AI**探索图像分类，并使用**Edge Impulse Studio**和**Arduino
    IDE**进行更详细的发展。
- en: '**Learning Objectives**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习目标**'
- en: '**Deploy Pre-trained Models** using SenseCraft AI Studio for immediate computer
    vision applications'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用SenseCraft AI Studio部署预训练模型**，以实现即时的计算机视觉应用'
- en: '**Collect and Manage Image Datasets** for custom classification tasks with
    proper data organization'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集和管理图像数据集**，以适当的组织方式执行自定义分类任务'
- en: '**Train Custom Image Classification Models** using transfer learning with MobileNet
    V2 architecture'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用MobileNet V2架构通过迁移学习训练自定义图像分类模型**'
- en: '**Optimize Models for Edge Deployment** through quantization and memory-efficient
    preprocessing'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过量化内存高效的预处理优化模型**，以适应边缘部署'
- en: '**Implement Post-processing Pipelines,** including GPIO control and real-time
    inference integration'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现后处理管道**，包括GPIO控制和实时推理集成'
- en: '**Compare Development Approaches** between no-code and advanced ML platforms
    for embedded applications'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较无代码和高级ML平台在嵌入式应用中的开发方法**'
- en: Image Classification
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分类
- en: Image classification is a fundamental task in computer vision that involves
    categorizing entire images into one of several predefined classes. This process
    entails analyzing the visual content of an image and assigning it a label from
    a fixed set of categories based on the dominant object or scene it depicts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是计算机视觉中的一个基本任务，涉及将整个图像分类到几个预定义类别之一。这个过程包括分析图像的视觉内容，并根据它所描绘的主要对象或场景，从一组固定的类别中分配一个标签。
- en: Image classification is crucial in various applications, ranging from organizing
    and searching through large databases of images in digital libraries and social
    media platforms to enabling autonomous systems to comprehend their surroundings.
    Common architectures that have significantly advanced the field of image classification
    include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet.
    These models have demonstrated remarkable accuracy on challenging datasets, such
    as [ImageNet](https://www.image-net.org/index.php), by learning hierarchical representations
    of visual data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类在各种应用中至关重要，从组织和搜索数字图书馆和社交媒体平台中的大型图像数据库，到使自主系统理解其周围环境。在显著推进图像分类领域的常见架构包括卷积神经网络（CNNs），如AlexNet、VGGNet和ResNet。这些模型通过学习视觉数据分层表示，在诸如
    [ImageNet](https://www.image-net.org/index.php) 这样的挑战性数据集上展示了显著的准确性。
- en: As the cornerstone of many computer vision systems, image classification drives
    innovation, laying the groundwork for more complex tasks like object detection
    and image segmentation, and facilitating a deeper understanding of visual data
    across various industries. So, let’s start exploring the [Person Classification](https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public)
    model (“Person - No Person”), a ready-to-use computer vision application on the
    **[SenseCraft AI](https://sensecraft.seeed.cc/ai/device/local/32)**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作为许多计算机视觉系统的基石，图像分类推动了创新，为更复杂任务如目标检测和图像分割奠定了基础，并促进了跨各个行业对视觉数据的更深入理解。因此，让我们开始探索
    [人物分类](https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public)
    模型（“人物 - 非人物”），这是一个在 **[SenseCraft AI](https://sensecraft.seeed.cc/ai/device/local/32)**
    上可用的现成计算机视觉应用。
- en: '![](../media/file540.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file540.png)'
- en: Image Classification on the SenseCraft AI Workspace
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 SenseCraft AI 工作空间中进行图像分类
- en: Start by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected
    from the Expansion Board) to the computer via USB-C, and then open the [SenseCraft
    AI Workspace](https://sensecraft.seeed.cc/ai/device/local/32) to connect it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过 USB-C 将 XIAOML Kit（或仅将 XIAO ESP32S3 Sense 从扩展板断开）连接到电脑，然后打开 [SenseCraft
    AI 工作空间](https://sensecraft.seeed.cc/ai/device/local/32) 以连接它。
- en: '![](../media/file541.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file541.png)'
- en: 'Once connected, select the option `[Select Model...]` and enter in the search
    window: “*Person Classification*”. From the options available, select the one
    trained over the MobileNet V2 (passing the mouse over the models will open a pop-up
    window with its main characteristics).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 连接后，选择 `[选择模型...]` 选项，并在搜索窗口中输入：“*人物分类*”。从可用选项中选择在 MobileNet V2 上训练的模型（将鼠标移过模型将打开一个弹出窗口，显示其主要特性）。
- en: '![](../media/file542.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file542.png)'
- en: Click on the chosen model and confirm the deployment. A new firmware for the
    model should start uploading to our device.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 点击选定的模型并确认部署。模型的新固件应开始上传到我们的设备。
- en: Note that the percentage of models downloaded and firmware uploaded will be
    displayed. If not, try disconnecting the device, then reconnect it and press the
    boot button.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，将显示已下载的模型百分比和上传的固件。如果没有显示，请尝试断开设备，然后重新连接并按启动按钮。
- en: After the model is uploaded successfully, we can view the live feed from the
    XIAO camera and the classification result (`Person` or `Not a Person`) in the
    **Preview** area, along with the inference details displayed in the **Device Logger**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型成功上传后，我们可以在预览区域查看 XIAO 摄像头的实时视频流和分类结果（`人物` 或 `非人物`），以及 **设备日志** 中显示的推理细节。
- en: Note that we can also select our **Inference Frame Interval**, from “Real-Time”
    (Default) to 10 seconds, and the **Mode** (UART, I2C, etc) as the data is shared
    by the device (the default is UART via USB).
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，我们还可以选择我们的 **推理帧间隔**，从“实时”（默认）到 10 秒，以及 **模式**（UART、I2C 等），因为设备通过这些方式共享数据（默认是通过
    USB 的 UART）。
- en: '![](../media/file543.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file543.png)'
- en: At the Device Logger, we can see that the latency of the model is from 52 to
    78 ms for pre-processing and around 532ms for inference, which will give us a
    total time of a little less than 600ms, or about **1.7 Frames per second (FPS)**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备日志中，我们可以看到模型的前处理延迟为 52 到 78 毫秒，推理大约为 532ms，这将给我们总时间略小于 600ms，即大约 **每秒 1.7
    帧 (FPS)**。
- en: To run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V,
    resulting in a **power consumption of 830mW**.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 运行 Mobilenet V2 0.35 时，XIAO 的峰值电流为 160mA，在 5.23V 下，导致 **功耗为 830mW**。
- en: Post-Processing
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: An essential step in an Image Classification project pipeline is to define what
    we want to do with the inference result. So, imagine that we will use the XIAO
    to automatically turn on the room lights if a person is detected.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类项目管道中，一个重要的步骤是定义我们想要对推理结果做什么。所以，想象一下，我们将使用 XIAO 在检测到人物时自动打开房间灯光。
- en: '![](../media/file544.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file544.png)'
- en: With the SebseCraft AI, we can do it on the `Output -> GPIO` section. Click
    on the Add icon to trigger the action when event conditions are met. A pop-up
    window will open, where you can define the action to be taken. For example, if
    a person is detected with a confidence of more than 60% the internal `LED` should
    be ON. In a real scenario, a GPIO, for example, `D0`, `D1`, `D2`, `D11`, or `D12`,
    would be used to trigger a relay to turn on a light.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SebseCraft AI，我们可以在 `输出 -> GPIO` 部分完成这项工作。当满足事件条件时，点击添加图标以触发动作。将打开一个弹出窗口，您可以在其中定义要采取的动作。例如，如果检测到置信度超过
    60% 的人，内部 `LED` 应该打开。在实际场景中，可以使用 GPIO，例如 `D0`、`D1`、`D2`、`D11` 或 `D12` 来触发继电器打开灯光。
- en: '![](../media/file545.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file545.png)'
- en: Once confirmed, the created **Trigger Action** will be shown. Press `Send` to
    upload the command to the XIAO.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 确认后，创建的 **触发动作** 将会显示。按下 `发送` 按钮将命令上传到 XIAO。
- en: '![](../media/file546.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file546.png)'
- en: Now, pointing the XIAO at a person will make the internal LED go ON.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 XIAO 对准一个人，内部 LED 将会打开。
- en: '![](../media/file547.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file547.png)'
- en: We will explore more trigger actions and post-processing techniques further
    in this lab.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本实验中，我们将进一步探索更多的触发动作和后处理技术。
- en: An Image Classification Project
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分类项目
- en: Let’s create a simple Image Classification project using SenseCraft AI Studio.
    Below, we can see a typical machine learning pipeline that will be used in our
    project.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的图像分类项目，使用 SenseCraft AI Studio。下面，我们可以看到一个典型的机器学习流程，它将在我们的项目中使用。
- en: '![](../media/file548.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file548.png)'
- en: 'On SenseCraft AI Studio: Let’s open the tab [Training](https://sensecraft.seeed.cc/ai/training):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SenseCraft AI Studio 上：让我们打开 `[训练](https://sensecraft.seeed.cc/ai/training)`
    选项卡：
- en: '![](../media/file549.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file549.png)'
- en: The default is to train a `Classification` model with a WebCam if it is available.
    Let’s select the `XIAOESP32S3 Sense` instead. Pressing the green button `[Connect]`
    will cause a Pop-Up window to appear. Select the corresponding Port and press
    the blue button `[Connect]`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果有网络摄像头，将训练一个 `分类` 模型。让我们选择 `XIAOESP32S3 Sense`。按下绿色按钮 `[连接]` 将会弹出一个窗口。选择相应的端口并按下蓝色按钮
    `[连接]`。
- en: '![](../media/file550.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file550.png)'
- en: The image streamed from the Grove Vision AI V2 will be displayed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Grove Vision AI V2 流出的图像将会显示。
- en: The Goal
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标
- en: The first step, as we can see in the ML pipeline, is to define a goal. Let’s
    imagine that we have an industrial installation that should automatically sort
    wheels and boxes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，正如我们在机器学习流程中看到的那样，是定义一个目标。让我们想象我们有一个工业安装，它应该能够自动分类轮子和箱子。
- en: '![](../media/file551.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file551.png)'
- en: So, let’s simulate it, classifying, for example, a toy `box` and a toy `wheel`.
    We should also include a 3rd class of images, `background`, where there are no
    objects in the scene.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们模拟它，例如，对玩具 `箱子` 和玩具 `轮子` 进行分类。我们还应该包括一个第三类图像，`背景`，其中场景中没有对象。
- en: '![](../media/file552.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file552.png)'
- en: Data Collection
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Let’s create the classes, following, for example, an alphabetical order:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建类别，例如按照字母顺序：
- en: 'Class1: background'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一类：背景
- en: 'Class 2: box'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二类：箱子
- en: 'Class 3: wheel'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三类：轮子
- en: '![](../media/file553.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file553.png)'
- en: Select one of the classes and keep pressing the green button (`Hold to Record`)
    under the preview area. The collected images (and their counting) will appear
    on the Image Samples Screen. Carefully and slowly, move the camera to capture
    different angles of the object. To modify the position or interfere with the image,
    release the green button, rearrange the object, and then hold it again to resume
    the capture.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个类别，并在预览区域下方的绿色按钮（`按住以记录`）上持续按下。收集到的图像（及其计数）将出现在图像样本屏幕上。仔细且缓慢地移动相机以捕捉物体的不同角度。要修改位置或干扰图像，释放绿色按钮，重新排列物体，然后再次按住以继续捕获。
- en: '![](../media/file554.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file554.png)'
- en: After collecting the images, review them and delete any incorrect ones.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 收集图像后，检查它们并删除任何错误的图像。
- en: '![](../media/file555.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file555.png)'
- en: Collect around **50 images** from each class and go to Training Step.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别收集大约 **50 张图像** 并进入训练步骤。
- en: Note that it is possible to download the collected images to be used in another
    application, for example, with the Edge Impulse Studio.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，可以将收集到的图像下载到其他应用程序中使用，例如 Edge Impulse Studio。
- en: Training
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: Confirm if the correct device is selected (`XIAO ESP32S3 Sense`) and press `[Start
    Training]`
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 确认是否已选择正确的设备（`XIAO ESP32S3 Sense`）并按下 `[开始训练]`
- en: '![](../media/file556.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file556.png)'
- en: Test
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试
- en: After training, the inference result can be previewed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，可以预览推理结果。
- en: Note that the model is not running on the device. We are, in fact, only capturing
    the images with the device and performing a **live preview** using the training
    model, which is running in the Studio.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，模型并未在设备上运行。实际上，我们只是在设备上捕获图像，并使用在Studio中运行的训练模型进行**实时预览**。
- en: '![](../media/file557.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file557.png)'
- en: Now is the time to really deploy the model in the device.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是真正将模型部署到设备上的时候了。
- en: Deployment
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署
- en: Select the trained model and `XIAO ESP32S3 Sense` at the `Supported Devices`
    window. And press `[Deploy to device]`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在“支持设备”窗口中选择已训练的模型和`XIAO ESP32S3 Sense`。然后点击 `[部署到设备]`。
- en: '![](../media/file558.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file558.png)'
- en: The SeneCrafit AI will redirect us to the **Vision Workplace** tab. `Confirm`
    the deployment, select the Port, and `Connect` it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SenseCraft AI将带我们到**视觉工作区**选项卡。`确认`部署，选择端口，并`连接`。
- en: '![](../media/file559.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file559.png)'
- en: The model will be flashed into the device. After an automatic reset, the model
    will start running on the device. On the Device Logger, we can see that the inference
    has a **latency of approximately 426 ms**, plus a **pre-processing of around 110ms**,
    corresponding to a **frame rate of 1.8 frames per second (FPS)**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将被烧录到设备中。自动重置后，模型将在设备上开始运行。在设备日志中，我们可以看到推理的**延迟约为426毫秒**，加上**预处理大约110毫秒**，对应于**每秒1.8帧（FPS）**。
- en: Also, note that in **Settings**, it is possible to adjust the model’s confidence.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，在**设置**中，可以调整模型的置信度。
- en: '![](../media/file560.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file560.png)'
- en: To run the Image Classification Model, the XIAO ESP32S3 had a peak current of
    14mA at 5.23V, resulting in a **power consumption of 730mW**.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要运行图像分类模型，XIAO ESP32S3在5.23V下的峰值电流为14mA，导致**功耗为730mW**。
- en: As before, in the **Output –> GPIO**, we can turn the GPIOs or the Internal
    LED ON based on the detected class. For example, the LED will be turned ON when
    the wheel is detected.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在**输出 –> GPIO**中，我们可以根据检测到的类别打开GPIO或内部LED。例如，当检测到轮子时，LED将打开。
- en: '![](../media/file561.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file561.png)'
- en: Saving the Model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存模型
- en: 'It is possible to save the model in the SenseCraft AI Studio. The Studio will
    retain all our models for later deployment. For that, return to the `Training`
    tab and select the button `[Save to SenseCraft`]:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在SenseCraft AI Studio中保存模型。工作室将保留我们所有的模型以供以后部署。为此，返回到“训练”选项卡并选择按钮 `[保存到SenseCraft`]：
- en: '![](../media/file562.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file562.png)'
- en: Follow the instructions to enter the model’s name, description, image, and other
    details.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 按照说明输入模型的名称、描述、图像和其他详细信息。
- en: '![](../media/file563.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file563.png)'
- en: Note that the trained model (an Int8 MobileNet V2 with a size of 320KB) can
    be downloaded for further use or even analysis, for example, using [Netron](https://github.com/lutzroeder/netron).
    Note that the model uses images of size 224x224x3 as its Input Tensor. In the
    next step, we will use different hyperparameters on the Edge Impulse Studio.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练好的模型（一个大小为320KB的Int8 MobileNet V2）可以下载以供进一步使用或分析，例如使用[Netron](https://github.com/lutzroeder/netron)。注意，该模型使用大小为224x224x3的图像作为其输入张量。在下一步中，我们将在Edge
    Impulse Studio上使用不同的超参数。
- en: '![](../media/file564.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file564.png)'
- en: Also, the model can be deployed again to the device at any time. Automatically,
    the **Workspace** will be open on the SenseCraft AI.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型可以随时再次部署到设备上。自动地，**工作区**将在SenseCraft AI上打开。
- en: Image Classification Project from a Dataset
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据集开始的图像分类项目
- en: The primary objective of our project is to train a model and perform inference
    on the XIAO ESP32S3 Sense. For training, we should find some data **(in fact,
    tons of data!)**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的首要目标是训练一个模型并在XIAO ESP32S3 Sense上执行推理。为了训练，我们需要找到一些数据 **（实际上，是大量的数据！）**。
- en: '*But as we already know, first of all, we need a goal! What do we want to classify?*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*但我们已经知道，首先，我们需要一个目标！我们想要分类什么？*'
- en: With TinyML, a set of techniques associated with machine learning inference
    on embedded devices, we should limit the classification to three or four categories
    due to limitations (mainly memory). We can, for example, train the images captured
    for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TinyML，由于限制（主要是内存），我们应该将分类限制在三个或四个类别。例如，我们可以训练SenseCraft AI Studio中下载的Box与Wheel捕获的图像。
- en: Alternatively, we can use a completely new dataset, such as one that differentiates
    apples from bananas and potatoes, or other categories. If possible, try finding
    a specific dataset that includes images from those categories. [Kaggle fruit-and-vegetable-image-recognition](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)
    is a good start.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 或者，我们可以使用一个全新的数据集，例如区分苹果、香蕉和土豆或其他类别的数据集。如果可能的话，尝试找到一个包含这些类别图像的特定数据集。[Kaggle水果和蔬菜图像识别](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)是一个不错的起点。
- en: Let’s download the dataset captured in the previous section. Open the menu (3
    dots) on each of the captured classes and select `Export Data`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载上一节中捕获的数据集。在每个捕获的类别上打开菜单（3个点），然后选择“导出数据”。
- en: '![](../media/file565.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file565.png)'
- en: The dataset will be downloaded to the computer as a .ZIP file, with one file
    for each class. Save them in your working folder and unzip them. You should have
    three folders, one for each class.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集将以.ZIP文件的形式下载到计算机上，每个类别一个文件。将它们保存在您的工作文件夹中，并解压缩它们。您应该有三个文件夹，每个文件夹对应一个类别。
- en: '![](../media/file566.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file566.png)'
- en: Optionally, you can add some fresh images, using, for example, the code discussed
    in the setup lab.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可选地，您可以使用例如设置实验室中讨论的代码添加一些新的图片。
- en: Training the model with Edge Impulse Studio
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Edge Impulse Studio训练模型
- en: We will use the Edge Impulse Studio to train our model. [Edge Impulse](https://www.edgeimpulse.com/)
    is a leading development platform for machine learning on edge devices.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Edge Impulse Studio来训练我们的模型。[Edge Impulse](https://www.edgeimpulse.com/)是边缘设备上机器学习的主要开发平台。
- en: 'Enter your account credentials (or create a free account) at Edge Impulse.
    Next, create a new project:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在Edge Impulse输入您的账户凭证（或创建一个免费账户）。接下来，创建一个新的项目：
- en: Data Acquisition
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据获取
- en: Next, go to the **Data acquisition** section and there, select `+ Add data`.
    A pop-up window will appear. Select `UPLOAD DATA`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，转到**数据获取**部分，在那里选择`+ 添加数据`。将弹出一个弹出窗口。选择`上传数据`。
- en: '![](../media/file567.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file567.png)'
- en: After selection, a new Pop-Up window will appear, asking to update the data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 选择后，将弹出一个新的弹出窗口，要求更新数据。
- en: 'In Upload mode: `select a folder` and press `[Choose Files]`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上传模式中：`选择一个文件夹`并按 `[选择文件]`。
- en: Go to the folder that contains one of the classes and press `[Upload]`
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前往包含一个类别的文件夹，并按 `[上传]`。
- en: '![](../media/file568.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file568.png)'
- en: You will return automatically to the Upload data window.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将自动返回到上传数据窗口。
- en: Select `Automatically split between training and testing`
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择“自动在训练和测试之间分割”。
- en: And enter the label of the images that are in the folder.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件夹中输入图片的标签。
- en: Select `[Upload data]`
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 `[上传数据]`。
- en: At this point, the files will start to be uploaded, and after that, another
    Pop-Up window will appear asking if you are building an object detection project.
    Select `[no]`
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一点上，文件将开始上传，之后，将出现另一个弹出窗口询问您是否在构建一个目标检测项目。选择 `[no]`。
- en: '![](../media/file569.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file569.png)'
- en: Repeat the procedure for all classes. **Do not forget to change the label’s
    name**. If you forget and the images are uploaded, please note that they will
    be mixed in the Studio. Do not worry, you can manually move the data between classes
    further.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有类别重复此过程。**不要忘记更改标签的名称**。如果您忘记了，并且图像已上传，请注意它们将在工作室中混合。不要担心，您可以在之后手动在类别之间移动数据。
- en: Close the Upload Data window and return to the **Data acquisition** page. We
    can see that all dataset was uploaded. Note that on the upper panel, we can see
    that we have 158 items, all of which are balanced. Also, 19% of the images were
    left for testing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭上传数据窗口，返回到**数据获取**页面。我们可以看到所有数据集都已上传。注意，在上面的面板中，我们可以看到我们有158个条目，它们都是平衡的。此外，19%的图像被留作测试。
- en: '![](../media/file570.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file570.png)'
- en: Impulse Design
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 脉冲设计
- en: An impulse takes raw data (in this case, images), extracts features (resizes
    pictures), and then uses a learning block to classify new data.
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个脉冲会提取原始数据（在这种情况下，图像），提取特征（调整图片大小），然后使用学习块对新数据进行分类。
- en: Classifying images is the most common application of deep learning, but a substantial
    amount of data is required to accomplish this task. We have around 50 images for
    each category. Is this number enough? Not at all! We will need thousands of images
    to “teach” or “model” each class, allowing us to differentiate them. However,
    we can resolve this issue by retraining a previously trained model using thousands
    of images. We refer to this technique as **“Transfer Learning” (TL)**. With TL,
    we can fine-tune a pre-trained image classification model on our data, achieving
    good performance even with relatively small image datasets, as in our case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是深度学习最常见应用，但完成这项任务需要大量的数据。我们每个类别大约有50张图像。这个数量足够吗？根本不够！我们需要数千张图像来“教学”或“建模”每个类别，以便我们能够区分它们。然而，我们可以通过使用数千张图像重新训练先前训练的模型来解决这个问题。我们称这种技术为**“迁移学习”（TL）**。使用TL，我们可以在我们的数据上微调预训练的图像分类模型，即使是在相对较小的图像数据集上也能取得良好的性能。
- en: '![](../media/file571.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file571.png)'
- en: With TL, we can fine-tune a pre-trained image classification model on our data,
    performing well even with relatively small image datasets (our case).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TL，我们可以在我们的数据上微调预训练的图像分类模型，即使是在相对较小的图像数据集（我们的案例）上也能表现出色。
- en: So, starting from the raw images, we will resize them <semantics><mrow><mo stretchy="true"
    form="prefix">(</mo><mn>96</mn><mo>×</mo><mn>96</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation
    encoding="application/x-tex">(96\times 96)</annotation></semantics> Pixels are
    fed to our Transfer Learning block. Let’s create an Impulse.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从原始图像开始，我们将它们调整大小为<semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>96</mn><mo>×</mo><mn>96</mn><mo
    stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(96\times
    96)</annotation></semantics>像素，并将它们输入到我们的迁移学习模块。让我们创建一个脉冲。
- en: At this point, we can also define our target device to monitor our “budget”
    (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse,
    so let’s consider the Espressif ESP-EYE, which is similar but slower.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这一点上，我们也可以定义我们的目标设备以监控我们的“预算”（内存和延迟）。XIAO ESP32S3不是Edge Impulse的官方支持设备，因此让我们考虑Espressif
    ESP-EYE，它相似但速度较慢。
- en: '![](../media/file572.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file572.png)'
- en: Save the Impulse, as shown above, and go to the **Image** section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 保存上述脉冲，然后转到**图像**部分。
- en: Pre-processing (Feature Generation)
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理（特征生成）
- en: Besides resizing the images, we can convert them to grayscale or retain their
    original RGB color depth. Let’s select `[RGB]` in the `Image` section. Doing that,
    each data sample will have a dimension of 27,648 features (96x96x3). Pressing
    `[Save Parameters]` will open a new tab, `Generate Features`. Press the button
    `[Generate Features]`to generate the features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整图像大小外，我们还可以将它们转换为灰度或保留它们的原始RGB颜色深度。让我们在“图像”部分选择 `[RGB]`。这样做后，每个数据样本将具有27,648个特征维度（96x96x3）。按下
    `[Save Parameters]` 将打开一个新标签页，`Generate Features`。按下 `[Generate Features]` 按钮以生成特征。
- en: Model Design, Training, and Test
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型设计、训练和测试
- en: 'In 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html).
    In 2018, [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381),
    was launched, and, in 2019, the V3\. The Mobilinet is a family of general-purpose
    computer vision neural networks explicitly designed for mobile devices to support
    classification, detection, and other applications. MobileNets are small, low-latency,
    low-power models parameterized to meet the resource constraints of various use
    cases.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在2007年，谷歌推出了[MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html)。在2018年，发布了[MobileNetV2:
    Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)，然后在2019年推出了V3。MobileNet是一系列通用计算机视觉神经网络，专门为移动设备设计，以支持分类、检测和其他应用。MobileNets是小型、低延迟、低功耗的模型，参数化以满足各种用例的资源限制。'
- en: Although the base MobileNet architecture is already compact and has low latency,
    a specific use case or application may often require the model to be even smaller
    and faster. MobileNets introduce a straightforward parameter, **α** (alpha), called
    the width multiplier to construct these smaller, less computationally expensive
    models. The role of the width multiplier α is to thin a network uniformly at each
    layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基础MobileNet架构已经非常紧凑且具有低延迟，但特定的用例或应用可能经常需要模型更小、更快。MobileNets引入了一个简单的参数，**α**（alpha），称为宽度乘数，以构建这些更小、计算成本更低的模型。宽度乘数α的作用是在每一层均匀地瘦化网络。
- en: Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96
    and 160x160 images), with several different **α** values (from 0.05 to 1.0). For
    example, you will get the highest accuracy with V2, 160x160 images, and α=1.0\.
    Of course, there is a trade-off. The higher the accuracy, the more memory (around
    1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.
    The smaller footprint will be obtained at another extreme with MobileNet V1 and
    α=0.10 (around 53.2K RAM and 101K ROM).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Impulse Studio提供了MobileNet V1（96x96图像）和V2（96x96和160x160图像），以及几个不同的**α**值（从0.05到1.0）。例如，使用V2，160x160图像，α=1.0时，你将获得最高的准确率。当然，这里有一个权衡。准确率越高，运行模型所需的内存（大约1.3M
    RAM和2.6M ROM）就越多，这意味着更大的延迟。在另一个极端，使用MobileNet V1和α=0.10时，将获得更小的占用空间（大约53.2K RAM和101K
    ROM）。
- en: We will use the **MobileNet V2 0.35** as our base model (but a model with a
    greater alpha can be used here). The final layer of our model, preceding the output
    layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将使用**MobileNet V2 0.35**作为我们的基础模型（但也可以使用具有更大α值的模型）。我们的模型最终层，在输出层之前，将有16个神经元，并具有10%的dropout率以防止过拟合。
- en: Another necessary technique to use with deep learning is **data augmentation**.
    Data augmentation is a method that can help improve the accuracy of machine learning
    models by creating additional artificial data. A data augmentation system makes
    small, random changes to your training data during the training process (such
    as flipping, cropping, or rotating the images).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习一起使用的另一个必要技术是**数据增强**。数据增强是一种可以通过创建额外的合成数据来帮助提高机器学习模型准确率的方法。数据增强系统在训练过程中对训练数据进行小的、随机的更改（例如翻转、裁剪或旋转图像）。
- en: 'Under the hood, here you can see how Edge Impulse implements a data Augmentation
    policy on your data:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，你可以看到Edge Impulse是如何在数据上实现数据增强策略的：
- en: '[PRE0]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s us define the hyperparameters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义超参数：
- en: 'Epochs: 20,'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数：20
- en: 'Bach Size: 32'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小：32
- en: 'Learning Rate: 0.0005'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.0005
- en: 'Validation size: 20%'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证大小：20%
- en: 'And, so, we have as a training result:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的训练结果如下：
- en: '![](../media/file573.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file573.png)'
- en: The model profile predicts **233 KB of RAM and 546 KB of Flash**, indicating
    no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio
    indicates a **latency of around 1160 ms**, which is very high. However, this is
    to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6,
    and the ESP32S3 uses a newer and more powerful Xtensa LX7.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模型配置文件预测**233 KB的RAM和546 KB的闪存**，这表明Xiao ESP32S3没有问题，它有8 MB的PSRAM。此外，工作室表明**延迟约为1160
    ms**，这非常高。然而，这是可以预料的，因为我们正在使用ESP-EYE，其CPU是Extensa LX6，而ESP32S3使用的是更新、更强大的Xtensa
    LX7。
- en: With the test data, we also achieved 100% accuracy, even with a quantized INT8
    model. This result is not typical in real projects, but our project here is relatively
    simple, with two objects that are very distinctive from each other.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用测试数据，我们甚至实现了100%的准确率，即使是在量化INT8模型的情况下。这个结果在真实项目中并不典型，但我们的项目相对简单，只有两个彼此非常不同的对象。
- en: Model Deployment
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'We can deploy the trained model:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以部署训练好的模型：
- en: As `.TFLITE` to be used on the **SenseCraft AI**
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为`.TFLITE`用于**SenseCraft AI**
- en: As an `Arduino Library` in the **Edge Impulse Studio**.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为**Edge Impulse Studio**中的`Arduino库`。
- en: Let’s start with the SenseCraft, which is more straightforward and more intuitive.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从更简单、更直观的SenseCraft开始。
- en: Model Deployment on the SenseCraft AI
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在SenseCraft AI上的模型部署
- en: On the **Dashboard**, it is possible to download the trained model in several
    different formats. Let’s download `TensorFlow Lite (int8 quantized)`, which has
    a size of 623KB.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在**仪表板**上，你可以以几种不同的格式下载训练好的模型。让我们下载`TensorFlow Lite (int8量化)`，其大小为623KB。
- en: '![](../media/file574.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file574.png)'
- en: On **SenseCraft AI Studio**, go to the `Workspace` tab, select `XIAO ESP32S3`,
    the corresponding Port, and connect the device.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在**SenseCraft AI Studio**中，转到`工作区`标签，选择`XIAO ESP32S3`，相应的端口，并连接设备。
- en: 'You should see the last model that was uploaded to the device. Select the green
    button `[Upload Model]`. A pop-up window will prompt you to enter the model name,
    the model file, and the class names (**objects**). We should use labels in alphabetical
    order: `0: background`, `1: box`, and `2: wheel`, and then press `[Send]`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '你应该能看到最后上传到设备上的模型。选择绿色按钮 `[上传模型]`。一个弹出窗口将提示你输入模型名称、模型文件和类名（**对象**）。我们应该使用按字母顺序排列的标签：`0:
    background`，`1: box`，和`2: wheel`，然后按 `[发送]`。'
- en: '![](../media/file575.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file575.png)'
- en: After a few seconds, the model will be uploaded (“flashed”) to our device, and
    the camera image will appear in real-time on the **Preview** Sector. The Classification
    result will be displayed under the image preview. It is also possible to select
    the `Confidence Threshold` of your inference using the cursor on **Settings**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，模型将被上传（闪存）到我们的设备，相机图像将实时出现在 **预览** 区域。分类结果将在图像预览下方显示。您还可以使用 **设置** 中的光标选择推理的
    `置信度阈值`。
- en: On the **Device Logger**, we can view the Serial Monitor, where we can observe
    the latency, which is approximately 81 ms for pre-processing and 205 ms for inference,
    **corresponding to a frame rate of 3.4 frames per second (FPS)**, what is double
    of we got, training the model on SenseCraft, because we are working with smaller
    images (96x96 versus 224x224).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **设备日志** 中，我们可以查看串行监视器，其中可以观察到延迟，预处理大约为 81 毫秒，推理大约为 205 毫秒，**对应于每秒 3.4 帧（FPS）的帧率**，这是我们使用
    SenseCraft 训练模型时得到的两倍，因为我们正在处理更小的图像（96x96 与 224x224）。
- en: The total latency is around **4 times faster** than the estimation made in Edge
    Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an
    Xtensa LX7 CPU.
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总延迟大约比在 Xtensa LX6 CPU 上 Edge Impulse Studio 中的估计快 **4 倍**；现在我们在 Xtensa LX7
    CPU 上进行推理。
- en: '![](../media/file576.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file576.png)'
- en: Post-Processing
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后处理
- en: It is possible to obtain the output of a model inference, including Latency,
    Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This
    allows us to utilize the **XIAO ESP32S3 Sense as an AI sensor**. In other words,
    we can retrieve the model data using different communication protocols such as
    MQTT, UART, I2C, or SPI, depending on our project requirements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 可以获取模型推理的输出，包括延迟、类别 ID 和置信度，如 SenseCraft AI 的设备日志中所示。这使我们能够将 **XIAO ESP32S3
    感应器** 作为 AI 传感器使用。换句话说，我们可以根据项目需求使用不同的通信协议，如 MQTT、UART、I2C 或 SPI 来检索模型数据。
- en: The idea is similar to what we have done on the [Seeed Grove Vision AI V2 Image
    Classification Post-Processing Lab](https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-9610).
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个想法与我们之前在 [Seeed Grove Vision AI V2 图像分类后处理实验室](https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-9610)
    中所做的是类似的。
- en: Below is an example of a connection using the I2C bus.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用 I2C 总线的连接示例。
- en: Please refer to the [Seeed Studio Wiki](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)
    for more information.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 [Seeed Studio Wiki](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)
    获取更多信息。
- en: Model Deployment as an Arduino Library at EI Studio
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 EI Studio 中将模型部署为 Arduino 库
- en: 'On the **Deploy** section at Edge Impulse Studio, Select `Arduino library`,
    `TensorFlow Lite`, `Quantized(int8)`, and press `[Build]`. The trained model will
    be downloaded as a .zip Arduino library:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Edge Impulse Studio 的 **部署** 部分选择 `Arduino 库`、`TensorFlow Lite`、`量化(int8)`
    并按 `[构建]`。训练好的模型将以 .zip Arduino 库的形式下载：
- en: '![](../media/file577.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file577.png)'
- en: Open your Arduino IDE, and under **Sketch,** go to **Include Library** and **add
    .ZIP Library.** Next, select the file downloaded from Edge Impulse Studio and
    press `[Open]`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的 Arduino IDE，在 **草图** 下，转到 **包含库** 并 **添加 .ZIP 库**。然后，选择从 Edge Impulse Studio
    下载的文件并按 `[打开]`。
- en: '![](../media/file578.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file578.png)'
- en: 'Go to the Arduino IDE `Examples` and look for the project by its name (in this
    case: “Box_versus_Whell_…Interfering”. Open `esp32` -> `esp32_camera`. The sketch
    `esp32_camera.ino` will be downloaded to the IDE.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 Arduino IDE 的 `示例`，根据其名称查找项目（在本例中为：“Box_versus_Whell_…Interfering”。打开 `esp32`
    -> `esp32_camera`。`esp32_camera.ino` 草图将被下载到 IDE。
- en: 'This sketch was developed for the standard ESP32 and will not work with the
    XIAO ESP32S3 Sense. It should be modified. Let’s download the modified one from
    the project GitHub: [Image_class_XIAOML-Kit.ino](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/blob/main/XIAOML_Kit_code/image_class_XIAOML-Kit/image_class_XIAOML-Kit.ino).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个草图是为标准 ESP32 开发的，与 XIAO ESP32S3 感应器不兼容。需要进行修改。让我们从项目的 GitHub 下载修改后的版本：[Image_class_XIAOML-Kit.ino](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/blob/main/XIAOML_Kit_code/image_class_XIAOML-Kit/image_class_XIAOML-Kit.ino)。
- en: XIAO ESP32S3 Image Classification Code Explained
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: XIAO ESP32S3 图像分类代码解释
- en: The code captures images from the onboard camera, processes them, and classifies
    them (in this case, “Box”, “Wheel”, or “Background”) using the trained model on
    EI Studio. It runs continuously, performing real-time inference on the edge device.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从板上摄像头捕获图像，处理它们，并使用EI Studio上的训练模型对它们进行分类（在这种情况下，“Box”，“Wheel”或“Background”）。它持续运行，在边缘设备上执行实时推理。
- en: 'In short,:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：
- en: Camera → JPEG Image → RGB888 Conversion → Resize to 96x96 → Neural Network →
    Classification Results → Serial Output
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像头 → JPEG图像 → RGB888转换 → 调整大小到96x96 → 神经网络 → 分类结果 → 串行输出
- en: Key Components
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键组件
- en: '**Library Includes and Dependencies**'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**库包含和依赖项**'
- en: '[PRE1]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Edge Impulse Inference Library**: Contains our trained model and inference
    engine'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Edge Impulse推理库**：包含我们的训练模型和推理引擎'
- en: '**Image Processing**: Provides functions for image manipulation'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像处理**：提供图像操作功能'
- en: '**ESP Camera**: Hardware interface for the camera module'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ESP Camera**：摄像头模块的硬件接口'
- en: '**Camera Pin Configurations**'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摄像头引脚配置**'
- en: 'The XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660),
    which may have different pin configurations. The code defines three possible configurations:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: XIAO ESP32S3 Sense可以与不同的摄像头传感器（OV2640或OV3660）一起工作，这些传感器可能具有不同的引脚配置。代码定义了三种可能的配置：
- en: '[PRE2]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This flexibility allows the code to automatically try different pin mappings
    if the first one doesn’t work, making it more robust across different hardware
    revisions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性允许代码在第一个引脚映射不工作的情况下自动尝试不同的引脚映射，使其在不同硬件版本上更具鲁棒性。
- en: '**Memory Management Settings**'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**内存管理设置**'
- en: '[PRE3]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Frame Buffer Size**: Defines the raw image size (320x240 pixels)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**帧缓冲区大小**：定义原始图像大小（320x240像素）'
- en: '**Heap Allocation**: Uses dynamic memory allocation for flexibility'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆分配**：使用动态内存分配以提供灵活性'
- en: '**PSRAM Support**: The ESP32S3 has 8MB of PSRAM for storing large data like
    images'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PSRAM支持**：ESP32S3有8MB的PSRAM用于存储大型数据，如图像'
- en: '`setup()` - Initialization'
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`setup()` - 初始化'
- en: '[PRE4]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数：
- en: Initializes serial communication for debugging output
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化串行通信以进行调试输出
- en: Initializes the camera with automatic configuration detection
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自动配置检测初始化摄像头
- en: Waits 2 seconds before starting continuous inference
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始连续推理前等待2秒
- en: '`loop()` - Main Processing Loop'
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`loop()` - 主处理循环'
- en: 'The loop performs these steps continuously:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 循环持续执行以下步骤：
- en: '**Step 1: Memory Allocation**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步：内存分配**'
- en: '[PRE5]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Allocates memory for the image buffer, preferring PSRAM (faster external RAM)
    but falling back to regular heap if needed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为图像缓冲区分配内存，优先使用PSRAM（更快的外部RAM），如果需要则回退到常规堆。
- en: '**Step 2: Image Capture**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步：图像捕获**'
- en: '[PRE6]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Captures an image from the camera and stores it in the buffer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从摄像头捕获图像并将其存储在缓冲区中。
- en: '**Step 3: Run Inference**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步：运行推理**'
- en: '[PRE7]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Runs the machine learning model on the captured image.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕获的图像上运行机器学习模型。
- en: '**Step 4: Output Results**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4步：输出结果**'
- en: '[PRE8]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Prints the classification results showing confidence scores for each category.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 打印分类结果，显示每个类别的置信度分数。
- en: '`ei_camera_init()` - Smart Camera Initialization'
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`ei_camera_init()` - 智能摄像头初始化'
- en: 'This function implements an intelligent initialization sequence:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数实现了一个智能初始化序列：
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The function:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数：
- en: Tries multiple pin configurations
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试多种引脚配置
- en: Tests different clock frequencies (10MHz or 16MHz)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试不同的时钟频率（10MHz或16MHz）
- en: Attempts PSRAM first, then falls back to DRAM
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先尝试PSRAM，然后回退到DRAM
- en: Applies sensor-specific settings based on detected hardware
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据检测到的硬件应用传感器特定设置
- en: '`ei_camera_capture()` - Image Processing Pipeline'
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`ei_camera_capture()` - 图像处理管道'
- en: '[PRE10]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This function:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数：
- en: Captures a JPEG image from the camera
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从摄像头捕获JPEG图像
- en: Converts it to RGB888 format (required by the ML model)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其转换为RGB888格式（ML模型所需）
- en: Resizes the image to match the model’s input size (96x96 pixels)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像调整大小以匹配模型的输入大小（96x96像素）
- en: Inference
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: Upload the code to the XIAO ESP32S3 Sense.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将代码上传到XIAO ESP32S3 Sense。
- en: ⚠️ **Attention**
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⚠️ **注意**
- en: ''
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Xiao ESP32S3 **MUST** have the PSRAM enabled. You can check it on the Arduino
    IDE upper menu: `Tools`–> `PSRAM:OPI PSRAM`'
  id: totrans-235
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao ESP32S3 **必须**启用PSRAM。您可以在Arduino IDE的上级菜单中检查它：`工具`–> `PSRAM:OPI PSRAM`
- en: The Arduino Boards package (`esp32 by Espressif Systems`) should be **version
    2.017**. Do not update it
  id: totrans-236
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arduino板包（`esp32 by Espressif Systems`）应为**版本2.017**。请勿更新
- en: '![](../media/file579.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file579.png)'
- en: Open the Serial Monitor
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开串行监视器
- en: Point the camera at the objects, and check the result on the Serial Monitor.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将摄像头对准物体，并在串行监视器上检查结果。
- en: '![](../media/file580.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](../media/file580.png)'
- en: Post-Processing
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理
- en: In edge AI applications, the inference result is only as valuable as our ability
    to act upon it. While serial output provides detailed information for debugging
    and development, real-world deployments require immediate, human-readable feedback
    that doesn’t depend on external monitors or connections.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘人工智能应用中，推理结果的价值仅取决于我们对其采取行动的能力。虽然串行输出提供了详细的调试和开发信息，但现实世界的部署需要立即、人类可读的反馈，这种反馈不依赖于外部监视器或连接。
- en: The XIAOML Kit tiny 0.42” OLED display (72×40 pixels) serves as a crucial post-processing
    component that transforms raw ML inference results into immediate, human-readable
    feedback—displaying detected class names and confidence levels directly on the
    device, eliminating the need for external monitors and enabling truly standalone
    edge AI deployment in industrial, agricultural, or retail environments where instant
    visual confirmation of AI predictions is essential.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: XIAOML Kit 0.42” OLED 显示屏（72×40 像素）作为关键的后处理组件，将原始机器学习推理结果转换为即时、人类可读的反馈——直接在设备上显示检测到的类别名称和置信度，消除了对外部监视器的需求，并使工业、农业或零售环境中的独立边缘人工智能部署成为可能，在这些环境中，对人工智能预测的即时视觉确认至关重要。
- en: 'So, let’s modify the sketch to automatically adapt to the model trained on
    Edge Impulse by reading the class names and count directly from the model. The
    display will show abbreviated class names (3 letters) with larger fonts for better
    visibility on the tiny 72x40 pixel display. Download the code from the GitHub:
    [XIAOML-Kit-Img_Class_OLED_Gen](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/XIAOML-Kit-Img_Class_OLED_Gen).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们修改草图以自动适应在 Edge Impulse 上训练的模型，通过直接从模型中读取类别名称和计数。显示屏将显示缩写后的类别名称（3 个字母），字体较大，以便在小型
    72x40 像素显示屏上获得更好的可见性。从 GitHub 下载代码：[XIAOML-Kit-Img_Class_OLED_Gen](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/XIAOML-Kit-Img_Class_OLED_Gen)。
- en: 'Running the code, we can see the result:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码，我们可以看到结果：
- en: '![](../media/file581.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../media/file581.png)'
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image
    classification applications. Through this lab, we’ve explored two distinct development
    approaches that cater to different skill levels and project requirements.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: XIAO ESP32S3 Sense 是一个令人印象深刻的、灵活的平台，适用于图像分类应用。通过这个实验室，我们探索了两种不同的开发方法，以满足不同的技能水平和项目需求。
- en: The **SenseCraft AI Studio** provides an accessible entry point with its **no-code
    interface**, enabling rapid prototyping and deployment of pre-trained models like
    person detection. With real-time inference and integrated post-processing capabilities,
    it demonstrates how AI can be deployed without extensive programming or ML knowledge.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SenseCraft AI Studio** 提供了一个易于访问的入口，其 **无代码界面** 允许快速原型设计和部署预训练模型，如人体检测。通过实时推理和集成后处理功能，它展示了如何在不进行大量编程或机器学习知识的情况下部署人工智能。'
- en: For more advanced applications, **Edge Impulse Studio** offers comprehensive
    machine learning pipeline tools, including custom dataset management, transfer
    learning with several pre-trained models, such as MobileNet, and model optimization.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更高级的应用，**Edge Impulse Studio** 提供了全面的机器学习管道工具，包括自定义数据集管理、与多个预训练模型（如 MobileNet）的迁移学习以及模型优化。
- en: Key insights from this lab include the importance of image resolution trade-offs,
    the effectiveness of transfer learning for small datasets, and the practical considerations
    of edge AI deployment, such as power consumption and memory constraints.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验室的关键见解包括图像分辨率权衡的重要性、迁移学习在小数据集上的有效性以及边缘人工智能部署的实际考虑，例如功耗和内存限制。
- en: 'The Lab demonstrates fundamental TinyML principles that extend beyond this
    specific hardware: resource-constrained inference, real-time processing requirements,
    and the complete pipeline from data collection through model deployment to practical
    applications. With built-in post-processing capabilities including GPIO control
    and communication protocols, the XIAO serves as more than just an inference engine—it
    becomes a complete AI sensor platform.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验室展示了超越特定硬件的 TinyML 基本原理：资源受限的推理、实时处理需求以及从数据收集到模型部署再到实际应用的完整流程。内置的后处理功能包括
    GPIO 控制、通信协议等，XIAO 不仅是一个推理引擎，还成为了一个完整的 AI 传感器平台。
- en: This foundation in image classification prepares you for more complex computer
    vision tasks while showcasing how modern edge AI makes sophisticated computer
    vision accessible, cost-effective, and deployable in real-world embedded applications
    ranging from industrial automation to smart home systems.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图像分类的基础知识将为你准备更复杂的计算机视觉任务，同时展示了现代边缘人工智能如何使复杂的计算机视觉变得可访问、成本效益高，并且能够在从工业自动化到智能家居系统等现实世界的嵌入式应用中部署。
- en: Resources
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Getting Started with the XIAO ESP32S3](https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XIAO ESP32S3 入门指南](https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/)'
- en: '[SenseCraft AI Studio Home](https://sensecraft.seeed.cc/ai/home)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SenseCraft AI Studio 主页](https://sensecraft.seeed.cc/ai/home)'
- en: '[SenseCraft Vision Workspace](https://sensecraft.seeed.cc/ai/device/local/32)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SenseCraft 视觉工作空间](https://sensecraft.seeed.cc/ai/device/local/32)'
- en: '[Dataset example](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据集示例](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)'
- en: '[Edge Impulse Project](https://studio.edgeimpulse.com/public/757065/live)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse 项目](https://studio.edgeimpulse.com/public/757065/live)'
- en: '[XIAO as an AI Sensor](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XIAO 作为人工智能传感器](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)'
- en: '[Seeed Arduino SSCMA Library](https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Seeed Arduino SSCMA 库](https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA)'
- en: '[XIAOML Kit Code](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XIAOML 工具包代码](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code)'
