["```py\nwhere $X_i$ are original features and $X^s_i$ are transformed features. \n```", "```py\ngiven the features are standardized the matrix is a correlation matrix \n```", "```py\ngiven the features are standardized the matrix is a correlation matrix, \n```", "```py\nwe can reorder to, \n```", "```py\nwhere $I$ is an identity matrix. By Cramer‚Äôs rule, we have a solution if the determinant is 0, \n```", "```py\nfind the possible Eigenvalues, $\\lambda_ùõº$, and solve for eigenvectors, $ùíó_ùú∂, \\quad \\alpha=ùüè,\\ldots,ùíé$ \n```", "```py\nthat form a basis on which the data are projected for dimensionality reduction, \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nfrom sklearn.preprocessing import MinMaxScaler                # min/max normalization\nfrom sklearn.decomposition import PCA                         # PCA program from scikit learn (package for machine learning)\nfrom sklearn.preprocessing import StandardScaler              # standardize variables to mean of 0.0 and variance of 1.0\nimport numpy as np                                            # ndarrays for gridded data\nimport pandas as pd                                           # DataFrames for tabular data\nimport pandas.plotting as pd_plot                             # pandas plotting functions\nimport copy                                                   # for deep copies\nimport os                                                     # set working directory, run executables\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nimport matplotlib.ticker as mtick                             # control tick label formatting\nfrom matplotlib.ticker import PercentFormatter                # percentage axis label formatting\nimport seaborn as sns                                         # advanced plotting\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map\nseed = 42                                                     # random number seed \n```", "```py\ndef plot_corr(df,size=10):                                    # plots a graphical correlation matrix \n    from matplotlib.colors import ListedColormap              # make a custom colormap\n    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    newcolors[65:191, :] = white                              # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = len(df.columns)\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0,cmap = newcmp)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title('Correlation Matrix')\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef add_grid2(sub_plot):\n    sub_plot.grid(True, which='major',linewidth = 1.0); sub_plot.grid(True, which='minor',linewidth = 0.2) # add y grids\n    sub_plot.tick_params(which='major',length=7); sub_plot.tick_params(which='minor', length=4)\n    sub_plot.xaxis.set_minor_locator(AutoMinorLocator()); sub_plot.yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/Local\")                                 # set the working directory \n```", "```py\n#my_data = pd.read_csv(\"unconv_MV.csv\") \nmy_data = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the comma delimited data file\nmy_data = my_data.iloc[:,1:]                              # remove the well index \n```", "```py\nmy_data[:7]                                               # preview the first 7 rows of the dataframe \n```", "```py\nmy_data.describe().transpose()                            # calculate summary statistics for the data \n```", "```py\ndf.get_numerical_data() \n```", "```py\nnum = my_data._get_numeric_data()                         # get the numerical values\nnum[num < 0] = 0                                          # truncate negative values to 0.0\nmy_data.describe().transpose()                            # calculate summary statistics for the data \n```", "```py\ncorr_matrix = np.corrcoef(my_data, rowvar = False) \n```", "```py\ncorr_matrix = np.corrcoef(my_data, rowvar = False)\nprint(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals \n```", "```py\n[[ 1\\.    0.81 -0.51 -0.25  0.71  0.08  0.69]\n [ 0.81  1\\.   -0.32 -0.15  0.51  0.05  0.57]\n [-0.51 -0.32  1\\.    0.17 -0.55  0.49 -0.33]\n [-0.25 -0.15  0.17  1\\.   -0.24  0.3  -0.07]\n [ 0.71  0.51 -0.55 -0.24  1\\.    0.31  0.5 ]\n [ 0.08  0.05  0.49  0.3   0.31  1\\.    0.14]\n [ 0.69  0.57 -0.33 -0.07  0.5   0.14  1\\.  ]] \n```", "```py\nplot_corr(my_data,7)                                      # using our correlation matrix visualization function\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npd_plot.scatter_matrix(my_data) \n```", "```py\npd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nmy_data_por_perm = my_data.iloc[0:100,0:2]                # extract just por and logperm, 100 samples\nmy_data_por_perm.describe().transpose()                   # calculate summary statistics for the data \n```", "```py\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.hist(my_data_por_perm[\"Por\"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)\nax1.set_title('Porosity'); ax1.set_xlabel('Porosity (%)'); ax1.set_ylabel('Frequency'); add_grid2(ax1)\nax2.hist(my_data_por_perm[\"LogPerm\"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)\nax2.set_title('Log Transformed Permeability'); ax2.set_xlabel('Log[Permeability] (log(mD)'); add_grid2(ax2)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nplt.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"] \n```", "```py\nplt.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Log Transformed Permeability vs. Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Log(Permeability (Log(mD))'); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.7, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nfeatures = ['Por','LogPerm']\nx = my_data_por_perm.loc[:,features].values\nmu = np.mean(x, axis=0)\nsd = np.std(x, axis=0)\nx = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0\n\nprint(\"Original Mean Por\", np.round(mu[0],2), ', Original Mean LogPerm = ', np.round(mu[1],2)) \nprint(\"Original StDev Por\", np.round(sd[0],2), ', Original StDev LogPerm = ', np.round(sd[1],2)) \nprint('Mean Transformed Por =',np.round(np.mean(x[:,0]),2),', Mean Transformed LogPerm =',np.round(np.mean(x[:,1]),2))\nprint('Variance Transformed Por =',np.var(x[:,0]),', Variance Transformed LogPerm =',np.var(x[:,1])) \n```", "```py\nOriginal Mean Por 14.99 , Original Mean LogPerm =  1.39\nOriginal StDev Por 2.81 , Original StDev LogPerm =  0.39\nMean Transformed Por = 0.0 , Mean Transformed LogPerm = -0.0\nVariance Transformed Por = 1.0000000000000002 , Variance Transformed LogPerm = 1.0 \n```", "```py\ncov = np.cov(x,rowvar = False)\ncov \n```", "```py\narray([[1.01010101, 0.80087707],\n       [0.80087707, 1.01010101]]) \n```", "```py\ndfS = pd.DataFrame({'sPor': x[:,0], 'sLogPerm': x[:,1]})\nsns.jointplot(data=dfS,x='sPor',y='sLogPerm',marginal_kws=dict(bins=30),color='darkorange',edgecolor='black')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nn_components = 2\npca = PCA(n_components=n_components)\npca.fit(x) \n```", "```py\nn_components = 2\npca = PCA(n_components=n_components).fit(x) \n```", "```py\nprint(np.round(pca.components_,3))\nprint('First Principal Component = ' + str(np.round(pca.components_[0,:],3)))\nprint('Second Principal Component = ' + str(np.round(pca.components_[1,:],3))) \n```", "```py\n[[ 0.707  0.707]\n [ 0.707 -0.707]]\nFirst Principal Component = [0.707 0.707]\nSecond Principal Component = [ 0.707 -0.707] \n```", "```py\nprint('Variance explained by PC1 and PC2 =', np.round(pca.explained_variance_ratio_,3))\nprint('First Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[0],3)) + ' of the total variance.')\nprint('Second Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[1],3)) + ' of the total variance.') \n```", "```py\nVariance explained by PC1 and PC2 = [0.896 0.104]\nFirst Principal Component explains 0.896 of the total variance.\nSecond Principal Component explains 0.104 of the total variance. \n```", "```py\nf, (ax101, ax102, ax103) = plt.subplots(1, 3,figsize=(12,3))\nf.subplots_adjust(wspace=0.7)\n\nax101.scatter(x[:,0],x[:,1], s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax101.set_title('Standardized LogPerm vs. Por'); ax101.set_xlabel('Standardized Por'); ax101.set_ylabel('Standardized LogPerm')\nax101.set_xlim([-3,3]); ax101.set_ylim([-3,3]); add_grid2(ax101)\n\nx_trans = pca.transform(x)                                # calculate the principal component scores\nax102.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax102.set_title('Principal Component Scores'); ax102.set_xlabel('PC1'); ax102.set_ylabel('PC2')\nax102.set_xlim([-3,3]); ax102.set_ylim([-3,3]); add_grid2(ax102)\n\nx_reverse = pca.inverse_transform(x_trans)                        # reverse the principal component scores to standardized values\nax103.scatter(x_reverse[:,0],x_reverse[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax103.set_title('Reverse PCA'); ax103.set_xlabel('Standardized Por'); ax103.set_ylabel('Standardized LogPerm')\nax103.set_xlim([-3,3]); ax103.set_ylim([-3,3]); add_grid2(ax103)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nprint('Variance of the 2 features:')\nprint(np.var(x, axis = 0))\n\nprint('\\nTotal Variance from Original Features:')\nprint(np.sum(np.var(x, axis = 0)))\n\nprint('\\nVariance of the 2 principle components:')\nprint(np.round(np.var(x_trans, axis = 0),2))\n\nprint('\\nTotal Variance from Original Features:')\nprint(round(np.sum(np.var(x_trans, axis = 0)),2)) \n```", "```py\nVariance of the 2 features:\n[1\\. 1.]\n\nTotal Variance from Original Features:\n2.0\n\nVariance of the 2 principle components:\n[1.79 0.21]\n\nTotal Variance from Original Features:\n2.0 \n```", "```py\nprint('\\nCorrelation Matrix of the 2 original features components:')\nprint(np.round(np.corrcoef(x, rowvar = False),2))\n\nprint('\\nCorrelation Matrix of the 2 principle components\\' scores:')\nprint(np.round(np.corrcoef(x_trans, rowvar = False),2)) \n```", "```py\nCorrelation Matrix of the 2 original features components:\n[[1\\.   0.79]\n [0.79 1\\.  ]]\n\nCorrelation Matrix of the 2 principle components' scores:\n[[ 1\\. -0.]\n [-0\\.  1.]] \n```", "```py\nfrom numpy.linalg import eig\neigen_values,eigen_vectors = eig(cov)\nprint('Eigen Vectors:\\n' +  str(np.round(eigen_vectors,2)))\nprint('First Eigen Vector: ' + str(eigen_vectors[:,0]))\nprint('Second Eigen Vector: ' + str(eigen_vectors[:,1]))\nprint('Eigen Values:\\n' +  str(np.round(eigen_values,2)))\nPC = eigen_vectors.T.dot(x.T)\nplt.subplot(121)\nplt.scatter(PC[0,:],PC[1,:],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Principal Component Scores By-hand with numpy.linalg Eig Function'); plt.xlabel('PC1'); plt.ylabel('PC2')\nplt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()\n\nplt.subplot(122)\nplt.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Principal Component Scores with scikit-learn PCA'); plt.xlabel('PC1'); plt.ylabel('PC2')\nplt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nEigen Vectors:\n[[ 0.71 -0.71]\n [ 0.71  0.71]]\nFirst Eigen Vector: [0.70710678 0.70710678]\nSecond Eigen Vector: [-0.70710678  0.70710678]\nEigen Values:\n[1.81 0.21] \n```", "```py\nnComp = 1\nf, ((ax201, ax202, ax203), (ax206, ax205, ax204)) = plt.subplots(2, 3,figsize=(15,10))\n#f, ((ax201, ax202), (ax203, ax204), (ax205, ax206)) = plt.subplots(3, 2,figsize=(10,15))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax201.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax201.set_title('1\\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')\nax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)\n\nmu = np.mean(np.vstack((my_data_por_perm[\"Por\"].values,my_data_por_perm[\"LogPerm\"].values)), axis=1)\nsd = np.std(np.vstack((my_data_por_perm[\"Por\"].values,my_data_por_perm[\"LogPerm\"].values)), axis=1)\nx = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0\n\nax202.scatter(x[:,0],x[:,1], s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax202.set_title('2\\. Standardized LogPerm vs. Por'); ax202.set_xlabel('Standardized Por'); ax202.set_ylabel('Standardized LogPerm')\nax202.set_xlim([-3.5,3.5]); ax202.set_ylim([-3.5,3.5]); add_grid2(ax202)\n\nn_components = 2                                          # build principal component model with 2 components\npca = PCA(n_components=n_components)\npca.fit(x)\n\nx_trans = pca.transform(x)                                # calculate principal component scores\nax203.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax203.set_title('3\\. Principal Component Scores'); ax203.set_xlabel('PC1'); ax203.set_ylabel('PC2')\nax203.set_xlim([-3.5,3.5]); ax203.set_ylim([-3.5,3.5]); add_grid2(ax203)\n\nx_trans[:,1] = 0.0                                         # zero / remove the 2nd principal component \n\nax204.scatter(x_trans[:,0],x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax204.set_title('4\\. Only 1st Principal Component Scores'); ax204.set_xlabel('PC1'); ax204.set_ylabel('PC2')\nax204.set_xlim([-3.5,3.5]); ax204.set_ylim([-3.5,3.5]); add_grid2(ax204)\n\nxhat = pca.inverse_transform(x_trans)                             # reverse the principal component scores to standardized values\nax205.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax205.set_title('5\\. Reverse PCA'); ax205.set_xlabel('Standardized Por'); ax205.set_ylabel('Standardized LogPerm')\nax205.set_xlim([-3.5,3.5]); ax205.set_ylim([-3.5,3.5]); add_grid2(ax205)\n\nxhat = np.dot(pca.inverse_transform(x)[:,:nComp], pca.components_[:nComp,:])\nxhat = sd*xhat + mu                                       # remove the standardization\n\nax206.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"blue\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.6, linewidths=1.0, edgecolors=\"black\")\nax206.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax206.set_title('6\\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')\nax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)\n\nplt.show() \n```", "```py\nf, (ax201, ax206) = plt.subplots(1, 2,figsize=(10,6))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax201.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax201.set_title('1\\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')\nax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)\n\nax206.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax206.set_title('6\\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')\nax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)\nplt.show()\n\nvar_por = np.var(my_data_por_perm[\"Por\"]); var_por_hat = np.var(xhat[:,0]);\nvar_logperm = np.var(my_data_por_perm[\"LogPerm\"]); var_logperm_hat = np.var(xhat[:,1]);\nprint('Variance Por =',np.round(var_por,3),', Variance Reduced Dimensional Por =',np.round(var_por_hat,3),'Fraction = ',np.round(var_por_hat/var_por,3))\nprint('Variance LogPerm =',np.round(var_logperm,3),', Variance Reduced Dimensional LogPerm =',np.round(var_logperm_hat,3),'Fraction = ',np.round(var_logperm_hat/var_logperm,3))\nprint('Total Variance =',np.round(var_por + var_logperm,3), ', Total Variance Reduced Dimension =',np.round(var_por_hat+var_logperm_hat,3),'Fraction = ',np.round((var_por_hat+var_logperm_hat)/(var_por+var_logperm),3)) \n```", "```py\nVariance Por = 7.89 , Variance Reduced Dimensional Por = 7.073 Fraction =  0.896\nVariance LogPerm = 0.151 , Variance Reduced Dimensional LogPerm = 0.136 Fraction =  0.896\nTotal Variance = 8.041 , Total Variance Reduced Dimension = 7.208 Fraction =  0.896 \n```", "```py\nmy_data_f6 = my_data.iloc[0:500,0:6]                      # extract the 6 predictors, 500 samples \n```", "```py\nmy_data_f6.describe().transpose()                         # calculate summary statistics for the data \n```", "```py\ncorr_matrix = np.corrcoef(my_data_f6, rowvar = False)\nprint(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals \n```", "```py\n[[ 1\\.    0.79 -0.49 -0.25  0.71  0.12]\n [ 0.79  1\\.   -0.32 -0.13  0.48  0.04]\n [-0.49 -0.32  1\\.    0.14 -0.53  0.47]\n [-0.25 -0.13  0.14  1\\.   -0.24  0.24]\n [ 0.71  0.48 -0.53 -0.24  1\\.    0.35]\n [ 0.12  0.04  0.47  0.24  0.35  1\\.  ]] \n```", "```py\nfeatures = ['Por','LogPerm','AI','Brittle','TOC','VR']\nx_f6 = my_data_f6.loc[:,features].values\nmu_f6 = np.mean(x_f6, axis=0)\nsd_f6 = np.std(x_f6, axis=0)\nx_f6 = StandardScaler().fit_transform(x_f6)\n\nprint(\"Original Means\", features[:], np.round(mu_f6[:],2)) \nprint(\"Original StDevs\", features[:],np.round(sd_f6[:],2)) \nprint('Mean Transformed =',features[:],np.round(x.mean(axis=0),2))\nprint('Variance Transformed Por =',features[:],np.round(x.var(axis=0),2)) \n```", "```py\nOriginal Means ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [14.9   1.4   2.99 49.75  1\\.    1.99]\nOriginal StDevs ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [ 2.98  0.41  0.56 15.2   0.5   0.31]\nMean Transformed = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [0\\. 0.]\nVariance Transformed Por = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [1\\. 1.] \n```", "```py\nf, (ax6,ax7,ax8,ax9,ax10,ax11) = plt.subplots(1, 6, sharey=True, figsize=(15,2))\nax6.hist(x_f6[:,0], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax6.set_title('Std. Porosity'); ax6.set_xlim(-5,5)\nax7.hist(x_f6[:,1], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax7.set_title('Std. Log[Perm.]'); ax7.set_xlim(-5,5)\nax8.hist(x_f6[:,2], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax8.set_title('Std. Acoustic Imped.'); ax8.set_xlim(-5,5)\nax9.hist(x_f6[:,3], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax9.set_title('Std. Brittleness'); ax9.set_xlim(-5,5)\nax10.hist(x_f6[:,4], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax10.set_title('Std. Total Organic C'); ax10.set_xlim(-5,5)\nax11.hist(x_f6[:,5], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax11.set_title('Std. Vit. Reflectance'); ax11.set_xlim(-5,5)\nplt.show() \n```", "```py\nn_components = 6\npca_f6 = PCA(n_components=n_components)\npca_f6.fit(x_f6)\n\nprint(np.round(pca_f6.components_,3))                     # visualize the component loadings \n```", "```py\n[[ 0.558  0.476 -0.405 -0.211  0.504  0.01 ]\n [-0.117 -0.114 -0.432 -0.323 -0.229 -0.794]\n [-0.019 -0.124  0.384 -0.898  0.07   0.157]\n [-0.214 -0.674 -0.424 -0.006  0.526  0.21 ]\n [-0.784  0.522 -0.031 -0.046  0.331 -0.019]\n [ 0.12  -0.138  0.566  0.206  0.55  -0.549]] \n```", "```py\nprint('Variance explained by PC1 thru PC6 =', np.round(pca_f6.explained_variance_ratio_,3))\n\nf, (ax10, ax11) = plt.subplots(1, 2,figsize=(10,6))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax10.plot(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8)\nax10.scatter(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8,edgecolor='black')\nax10.set_xlabel('Principal Component'); ax10.set_ylabel('Variance Explained'); ax10.set_title('Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nyticks = mtick.FormatStrFormatter(fmt); ax10.set_xlim(1,6); ax10.set_ylim(0,100.0)\nax10.yaxis.set_major_formatter(yticks); add_grid2(ax10)\n\nax11.plot(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8)\nax11.scatter(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')\nax11.plot([1,6],[95,95], color='black',linestyle='dashed')\nax11.set_xlabel('Principal Component'); ax11.set_ylabel('Cumulative Variance Explained'); ax11.set_title('Cumulative Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nyticks = mtick.FormatStrFormatter(fmt); ax11.set_xlim(1,6); ax11.set_ylim(0,100.0); ax11.annotate('95% variance explained',[4.05,90])\nax11.yaxis.set_major_formatter(yticks); add_grid2(ax11)\n\nplt.show() \n```", "```py\nVariance explained by PC1 thru PC6 = [0.462 0.246 0.149 0.11  0.024 0.009] \n```", "```py\nprint('\\nCorrelation Matrix of the 6 original features components:')\nprint(np.round(np.corrcoef(x_f6, rowvar = False),2))\n\nprint('\\nCorrelation Matrix of the 6 principle components\\' scores:')\nprint(np.round(np.corrcoef(pca_f6.transform(x_f6), rowvar = False),2)) \n```", "```py\nCorrelation Matrix of the 6 original features components:\n[[ 1\\.    0.79 -0.49 -0.25  0.71  0.12]\n [ 0.79  1\\.   -0.32 -0.13  0.48  0.04]\n [-0.49 -0.32  1\\.    0.14 -0.53  0.47]\n [-0.25 -0.13  0.14  1\\.   -0.24  0.24]\n [ 0.71  0.48 -0.53 -0.24  1\\.    0.35]\n [ 0.12  0.04  0.47  0.24  0.35  1\\.  ]]\n\nCorrelation Matrix of the 6 principle components' scores:\n[[ 1\\.  0\\. -0\\.  0\\.  0\\. -0.]\n [ 0\\.  1\\. -0\\. -0\\. -0\\. -0.]\n [-0\\. -0\\.  1\\. -0\\. -0\\.  0.]\n [ 0\\. -0\\. -0\\.  1\\.  0\\.  0.]\n [ 0\\. -0\\. -0\\.  0\\.  1\\.  0.]\n [-0\\. -0\\.  0\\.  0\\.  0\\.  1.]] \n```", "```py\nnComp = 6\nxhat_6d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_6d = sd_f6*xhat_6d + mu_f6\n\nnComp = 5\nxhat_5d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_5d = sd_f6*xhat_5d + mu_f6\n\nnComp = 4\nxhat_4d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_4d = sd_f6*xhat_4d + mu_f6\n\nnComp = 3\nxhat_3d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_3d = sd_f6*xhat_3d + mu_f6\n\nnComp = 2\nxhat_2d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_2d = sd_f6*xhat_2d + mu_f6\n\nnComp = 1\nxhat_1d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_1d = sd_f6*xhat_1d + mu_f6\n\nf, (ax12, ax13, ax14, ax15, ax16, ax17, ax18) = plt.subplots(1, 7,figsize=(20,20))\nf.subplots_adjust(wspace=0.7)\n\nax12.scatter(my_data_f6[\"Por\"],my_data_f6[\"LogPerm\"],s=None, c=\"darkorange\",marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax12.set_title('Original Data'); ax12.set_xlabel('Por'); ax12.set_ylabel('LogPerm')\nax12.set_ylim(0.0,3.0); ax12.set_xlim(8,22); ax12.set_aspect(4.0); \n\nax13.scatter(xhat_1d[:,0],xhat_1d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax13.set_title('1 Principal Component'); ax13.set_xlabel('Por'); ax13.set_ylabel('LogPerm')\nax13.set_ylim(0.0,3.0); ax13.set_xlim(8,22); ax13.set_aspect(4.0)\n\nax14.scatter(xhat_2d[:,0],xhat_2d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax14.set_title('2 Principal Components'); ax14.set_xlabel('Por'); ax14.set_ylabel('LogPerm')\nax14.set_ylim(0.0,3.0); ax14.set_xlim(8,22); ax14.set_aspect(4.0)\n\nax15.scatter(xhat_3d[:,0],xhat_3d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax15.set_title('3 Principal Components'); ax15.set_xlabel('Por'); ax15.set_ylabel('LogPerm')\nax15.set_ylim(0.0,3.0); ax15.set_xlim(8,22); ax15.set_aspect(4.0)\n\nax16.scatter(xhat_4d[:,0],xhat_4d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax16.set_title('4 Principal Components'); ax16.set_xlabel('Por'); ax16.set_ylabel('LogPerm')\nax16.set_ylim(0.0,3.0); ax16.set_xlim(8,22); ax16.set_aspect(4.0)\n\nax17.scatter(xhat_5d[:,0],xhat_5d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax17.set_title('5 Principal Components'); ax17.set_xlabel('Por'); ax17.set_ylabel('LogPerm')\nax17.set_ylim(0.0,3.0); ax17.set_xlim(8,22); ax17.set_aspect(4.0)\n\nax18.scatter(xhat_6d[:,0],xhat_6d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax18.set_title('6 Principal Components'); ax18.set_xlabel('Por'); ax18.set_ylabel('LogPerm')\nax18.set_ylim(0.0,3.0); ax18.set_xlim(8,22); ax18.set_aspect(4.0)\n\nplt.show() \n```", "```py\nprint('1 Principal Component : Variance Por =',np.round(np.var(xhat_1d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_1d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('2 Principal Components: Variance Por =',np.round(np.var(xhat_2d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_2d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('3 Principal Components: Variance Por =',np.round(np.var(xhat_3d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_3d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('4 Principal Components: Variance Por =',np.round(np.var(xhat_4d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_4d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('5 Principal Components: Variance Por =',np.round(np.var(xhat_5d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_5d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('6 Principal Components: Variance Por =',np.round(np.var(xhat_6d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_6d[:,1])/(sd_f6[1]*sd_f6[1]),2)) \n```", "```py\n1 Principal Component : Variance Por = 0.86  Variance Log Perm =  0.63\n2 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.65\n3 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.66\n4 Principal Components: Variance Por = 0.91  Variance Log Perm =  0.96\n5 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0\n6 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0 \n```", "```py\ndf_1d = pd.DataFrame(data=xhat_1d,columns=features)   \ndf_2d = pd.DataFrame(data=xhat_2d,columns=features)\ndf_3d = pd.DataFrame(data=xhat_3d,columns=features)\ndf_4d = pd.DataFrame(data=xhat_4d,columns=features)\ndf_5d = pd.DataFrame(data=xhat_5d,columns=features)\ndf_6d = pd.DataFrame(data=xhat_6d,columns=features) \n```", "```py\nfig = plt.figure()\n\npd_plot.scatter_matrix(my_data_f6, alpha = 0.1,           # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Original Data')\n\npd_plot.scatter_matrix(df_1d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('1 Principal Component')\n\npd_plot.scatter_matrix(df_2d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('2 Principal Components')\n\npd_plot.scatter_matrix(df_3d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('3 Principal Components')\n\npd_plot.scatter_matrix(df_4d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('4 Principal Components')\n\npd_plot.scatter_matrix(df_5d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('5 Principal Components')\n\npd_plot.scatter_matrix(df_6d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('6 Principal Components')\n\nplt.show() \n```", "```py\n<Figure size 640x480 with 0 Axes> \n```", "```py\nx_rand = np.random.rand(10000,5); df_x_rand = pd.DataFrame(x_rand)\nprint('Variance of original features: ', np.round(np.var(x_rand, axis = 0),2))\nprint('Proportion of variance of original features: ', np.round(np.var(x_rand, axis = 0)/np.sum(np.var(x_rand, axis = 0)),2))\nprint('Correlation Matrix of original features:\\n'); print(np.round(np.cov(x_rand, rowvar = False),2)); print()\n\npd_plot.scatter_matrix(df_x_rand, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Original Features')\n\npca_rand = PCA(n_components=5)\npca_rand.fit(x_rand)\nprint('PCA Variance Explained ', np.round(pca_rand.explained_variance_ratio_,2))  \n\nscores_x_rand = pca_rand.transform(x_rand); df_scores_x_rand = pd.DataFrame(scores_x_rand)\n\nprint('\\nCorrelation Matrix of scores:\\n'); print(np.round(np.cov(scores_x_rand, rowvar = False),2)); print()\n\npd_plot.scatter_matrix(df_scores_x_rand, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Principal Component Scores') \n```", "```py\nVariance of original features:  [0.08 0.08 0.08 0.08 0.08]\nProportion of variance of original features:  [0.2 0.2 0.2 0.2 0.2]\nCorrelation Matrix of original features:\n\n[[ 0.08 -0\\.    0\\.   -0\\.    0\\.  ]\n [-0\\.    0.08  0\\.    0\\.   -0\\.  ]\n [ 0\\.    0\\.    0.08  0\\.   -0\\.  ]\n [-0\\.    0\\.    0\\.    0.08  0\\.  ]\n [ 0\\.   -0\\.   -0\\.    0\\.    0.08]] \n```", "```py\nPCA Variance Explained  [0.21 0.2  0.2  0.2  0.19]\n\nCorrelation Matrix of scores:\n\n[[ 0.09 -0\\.   -0\\.   -0\\.    0\\.  ]\n [-0\\.    0.08  0\\.   -0\\.   -0\\.  ]\n [-0\\.    0\\.    0.08  0\\.    0\\.  ]\n [-0\\.   -0\\.    0\\.    0.08  0\\.  ]\n [ 0\\.   -0\\.    0\\.    0\\.    0.08]] \n```", "```py\nText(0.5, 0.98, 'Principal Component Scores') \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\ndf_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features\ndf_new.head(n=13) \n```", "```py\nvar_explained = 0.95                                          # select the minimum variance explained\n\nn_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1\npca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA\npca_scores = pca_new.fit_transform(df_new.values)\n\ncumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance\n\nn_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance\n\ndf_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],\n            index=df_new.index)                               # project data to that many principal components\n\nsns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)\nplt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')\nplt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)\nplt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')\nplt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks\nplt.xticks(range(1, len(cumulative_variance) + 1))\nyticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) \nplt.annotate('95% variance explained',[4.05,90]); add_grid()\nplt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nsave_PCA = True                                        # save the imputed DataFrame?\n\nif save_PCA == True:\n    folder = r'C:\\Local'\n    file_name = r'dataframe_PCA.csv'\n\n    df_new_projected.to_csv(folder + \"/\" + file_name, index=False) \n```", "```py\n---------------------------------------------------------------------------\nOSError  Traceback (most recent call last)\nCell In[42], line 7\n  4 folder = r'C:\\Local'\n  5 file_name = r'dataframe_PCA.csv'\n----> 7 df_new_projected.to_csv(folder + \"/\" + file_name, index=False)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\core\\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\n  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n  3763 formatter = DataFrameFormatter(\n  3764     frame=df,\n  3765     header=header,\n   (...)\n  3769     decimal=decimal,\n  3770 )\n-> 3772 return DataFrameRenderer(formatter).to_csv(\n  3773     path_or_buf,\n  3774     lineterminator=lineterminator,\n  3775     sep=sep,\n  3776     encoding=encoding,\n  3777     errors=errors,\n  3778     compression=compression,\n  3779     quoting=quoting,\n  3780     columns=columns,\n  3781     index_label=index_label,\n  3782     mode=mode,\n  3783     chunksize=chunksize,\n  3784     quotechar=quotechar,\n  3785     date_format=date_format,\n  3786     doublequote=doublequote,\n  3787     escapechar=escapechar,\n  3788     storage_options=storage_options,\n  3789 )\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\n  1165     created_buffer = False\n  1167 csv_formatter = CSVFormatter(\n  1168     path_or_buf=path_or_buf,\n  1169     lineterminator=lineterminator,\n   (...)\n  1184     formatter=self.fmt,\n  1185 )\n-> 1186 csv_formatter.save()\n  1188 if created_buffer:\n  1189     assert isinstance(path_or_buf, StringIO)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240, in CSVFormatter.save(self)\n  236  \"\"\"\n  237 Create the writer & save.\n  238 \"\"\"\n  239 # apply compression and byte/text conversion\n--> 240 with get_handle(\n  241     self.filepath_or_buffer,\n  242     self.mode,\n  243     encoding=self.encoding,\n  244     errors=self.errors,\n  245     compression=self.compression,\n  246     storage_options=self.storage_options,\n  247 ) as handles:\n  248     # Note: self.encoding is irrelevant here\n  249     self.writer = csvlib.writer(\n  250         handles.handle,\n  251         lineterminator=self.lineterminator,\n   (...)\n  256         quotechar=self.quotechar,\n  257     )\n  259     self._save()\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n  735 # Only for write methods\n  736 if \"r\" not in mode and is_path:\n--> 737     check_parent_directory(str(handle))\n  739 if compression:\n  740     if compression != \"zstd\":\n  741         # compression libraries do not like an explicit text-mode\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:600, in check_parent_directory(path)\n  598 parent = Path(path).parent\n  599 if not parent.is_dir():\n--> 600     raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\n\nOSError: Cannot save file into a non-existent directory: 'C:\\Local' \n```", "```py\nwhere $X_i$ are original features and $X^s_i$ are transformed features. \n```", "```py\ngiven the features are standardized the matrix is a correlation matrix \n```", "```py\ngiven the features are standardized the matrix is a correlation matrix, \n```", "```py\nwe can reorder to, \n```", "```py\nwhere $I$ is an identity matrix. By Cramer‚Äôs rule, we have a solution if the determinant is 0, \n```", "```py\nfind the possible Eigenvalues, $\\lambda_ùõº$, and solve for eigenvectors, $ùíó_ùú∂, \\quad \\alpha=ùüè,\\ldots,ùíé$ \n```", "```py\nthat form a basis on which the data are projected for dimensionality reduction, \n```", "```py\nignore_warnings = True                                        # ignore warnings?\nfrom sklearn.preprocessing import MinMaxScaler                # min/max normalization\nfrom sklearn.decomposition import PCA                         # PCA program from scikit learn (package for machine learning)\nfrom sklearn.preprocessing import StandardScaler              # standardize variables to mean of 0.0 and variance of 1.0\nimport numpy as np                                            # ndarrays for gridded data\nimport pandas as pd                                           # DataFrames for tabular data\nimport pandas.plotting as pd_plot                             # pandas plotting functions\nimport copy                                                   # for deep copies\nimport os                                                     # set working directory, run executables\nimport matplotlib.pyplot as plt                               # for plotting\nfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\nimport matplotlib.ticker as mtick                             # control tick label formatting\nfrom matplotlib.ticker import PercentFormatter                # percentage axis label formatting\nimport seaborn as sns                                         # advanced plotting\nplt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\nif ignore_warnings == True:                                   \n    import warnings\n    warnings.filterwarnings('ignore')\ncmap = plt.cm.inferno                                         # color map\nseed = 42                                                     # random number seed \n```", "```py\ndef plot_corr(df,size=10):                                    # plots a graphical correlation matrix \n    from matplotlib.colors import ListedColormap              # make a custom colormap\n    my_colormap = plt.cm.get_cmap('RdBu_r', 256)          \n    newcolors = my_colormap(np.linspace(0, 1, 256))\n    white = np.array([256/256, 256/256, 256/256, 1])\n    newcolors[65:191, :] = white                              # mask all correlations less than abs(0.8)\n    newcmp = ListedColormap(newcolors)\n    m = len(df.columns)\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, size))\n    im = ax.matshow(corr,vmin = -1.0, vmax = 1.0,cmap = newcmp)\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    plt.yticks(range(len(corr.columns)), corr.columns);\n    plt.colorbar(im, orientation = 'vertical')\n    plt.title('Correlation Matrix')\n    for i in range(0,m):\n        plt.plot([i-0.5,i-0.5],[-0.5,m-0.5],color='black')\n        plt.plot([-0.5,m-0.5],[i-0.5,i-0.5],color='black')\n    plt.ylim([-0.5,m-0.5]); plt.xlim([-0.5,m-0.5])\n\ndef add_grid():\n    plt.gca().grid(True, which='major',linewidth = 1.0); plt.gca().grid(True, which='minor',linewidth = 0.2) # add y grids\n    plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n    plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n\ndef add_grid2(sub_plot):\n    sub_plot.grid(True, which='major',linewidth = 1.0); sub_plot.grid(True, which='minor',linewidth = 0.2) # add y grids\n    sub_plot.tick_params(which='major',length=7); sub_plot.tick_params(which='minor', length=4)\n    sub_plot.xaxis.set_minor_locator(AutoMinorLocator()); sub_plot.yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks \n```", "```py\n#os.chdir(\"c:/Local\")                                 # set the working directory \n```", "```py\n#my_data = pd.read_csv(\"unconv_MV.csv\") \nmy_data = pd.read_csv(r\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\") # load the comma delimited data file\nmy_data = my_data.iloc[:,1:]                              # remove the well index \n```", "```py\nmy_data[:7]                                               # preview the first 7 rows of the dataframe \n```", "```py\nmy_data.describe().transpose()                            # calculate summary statistics for the data \n```", "```py\ndf.get_numerical_data() \n```", "```py\nnum = my_data._get_numeric_data()                         # get the numerical values\nnum[num < 0] = 0                                          # truncate negative values to 0.0\nmy_data.describe().transpose()                            # calculate summary statistics for the data \n```", "```py\ncorr_matrix = np.corrcoef(my_data, rowvar = False) \n```", "```py\ncorr_matrix = np.corrcoef(my_data, rowvar = False)\nprint(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals \n```", "```py\n[[ 1\\.    0.81 -0.51 -0.25  0.71  0.08  0.69]\n [ 0.81  1\\.   -0.32 -0.15  0.51  0.05  0.57]\n [-0.51 -0.32  1\\.    0.17 -0.55  0.49 -0.33]\n [-0.25 -0.15  0.17  1\\.   -0.24  0.3  -0.07]\n [ 0.71  0.51 -0.55 -0.24  1\\.    0.31  0.5 ]\n [ 0.08  0.05  0.49  0.3   0.31  1\\.    0.14]\n [ 0.69  0.57 -0.33 -0.07  0.5   0.14  1\\.  ]] \n```", "```py\nplot_corr(my_data,7)                                      # using our correlation matrix visualization function\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\npd_plot.scatter_matrix(my_data) \n```", "```py\npd_plot.scatter_matrix(my_data, alpha = 0.1,              # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nmy_data_por_perm = my_data.iloc[0:100,0:2]                # extract just por and logperm, 100 samples\nmy_data_por_perm.describe().transpose()                   # calculate summary statistics for the data \n```", "```py\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.hist(my_data_por_perm[\"Por\"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)\nax1.set_title('Porosity'); ax1.set_xlabel('Porosity (%)'); ax1.set_ylabel('Frequency'); add_grid2(ax1)\nax2.hist(my_data_por_perm[\"LogPerm\"], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20)\nax2.set_title('Log Transformed Permeability'); ax2.set_xlabel('Log[Permeability] (log(mD)'); add_grid2(ax2)\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nplt.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"] \n```", "```py\nplt.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Log Transformed Permeability vs. Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Log(Permeability (Log(mD))'); add_grid()\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.7, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nfeatures = ['Por','LogPerm']\nx = my_data_por_perm.loc[:,features].values\nmu = np.mean(x, axis=0)\nsd = np.std(x, axis=0)\nx = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0\n\nprint(\"Original Mean Por\", np.round(mu[0],2), ', Original Mean LogPerm = ', np.round(mu[1],2)) \nprint(\"Original StDev Por\", np.round(sd[0],2), ', Original StDev LogPerm = ', np.round(sd[1],2)) \nprint('Mean Transformed Por =',np.round(np.mean(x[:,0]),2),', Mean Transformed LogPerm =',np.round(np.mean(x[:,1]),2))\nprint('Variance Transformed Por =',np.var(x[:,0]),', Variance Transformed LogPerm =',np.var(x[:,1])) \n```", "```py\nOriginal Mean Por 14.99 , Original Mean LogPerm =  1.39\nOriginal StDev Por 2.81 , Original StDev LogPerm =  0.39\nMean Transformed Por = 0.0 , Mean Transformed LogPerm = -0.0\nVariance Transformed Por = 1.0000000000000002 , Variance Transformed LogPerm = 1.0 \n```", "```py\ncov = np.cov(x,rowvar = False)\ncov \n```", "```py\narray([[1.01010101, 0.80087707],\n       [0.80087707, 1.01010101]]) \n```", "```py\ndfS = pd.DataFrame({'sPor': x[:,0], 'sLogPerm': x[:,1]})\nsns.jointplot(data=dfS,x='sPor',y='sLogPerm',marginal_kws=dict(bins=30),color='darkorange',edgecolor='black')\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nn_components = 2\npca = PCA(n_components=n_components)\npca.fit(x) \n```", "```py\nn_components = 2\npca = PCA(n_components=n_components).fit(x) \n```", "```py\nprint(np.round(pca.components_,3))\nprint('First Principal Component = ' + str(np.round(pca.components_[0,:],3)))\nprint('Second Principal Component = ' + str(np.round(pca.components_[1,:],3))) \n```", "```py\n[[ 0.707  0.707]\n [ 0.707 -0.707]]\nFirst Principal Component = [0.707 0.707]\nSecond Principal Component = [ 0.707 -0.707] \n```", "```py\nprint('Variance explained by PC1 and PC2 =', np.round(pca.explained_variance_ratio_,3))\nprint('First Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[0],3)) + ' of the total variance.')\nprint('Second Principal Component explains ' + str(np.round(pca.explained_variance_ratio_[1],3)) + ' of the total variance.') \n```", "```py\nVariance explained by PC1 and PC2 = [0.896 0.104]\nFirst Principal Component explains 0.896 of the total variance.\nSecond Principal Component explains 0.104 of the total variance. \n```", "```py\nf, (ax101, ax102, ax103) = plt.subplots(1, 3,figsize=(12,3))\nf.subplots_adjust(wspace=0.7)\n\nax101.scatter(x[:,0],x[:,1], s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax101.set_title('Standardized LogPerm vs. Por'); ax101.set_xlabel('Standardized Por'); ax101.set_ylabel('Standardized LogPerm')\nax101.set_xlim([-3,3]); ax101.set_ylim([-3,3]); add_grid2(ax101)\n\nx_trans = pca.transform(x)                                # calculate the principal component scores\nax102.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax102.set_title('Principal Component Scores'); ax102.set_xlabel('PC1'); ax102.set_ylabel('PC2')\nax102.set_xlim([-3,3]); ax102.set_ylim([-3,3]); add_grid2(ax102)\n\nx_reverse = pca.inverse_transform(x_trans)                        # reverse the principal component scores to standardized values\nax103.scatter(x_reverse[:,0],x_reverse[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax103.set_title('Reverse PCA'); ax103.set_xlabel('Standardized Por'); ax103.set_ylabel('Standardized LogPerm')\nax103.set_xlim([-3,3]); ax103.set_ylim([-3,3]); add_grid2(ax103)\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=0.8, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nprint('Variance of the 2 features:')\nprint(np.var(x, axis = 0))\n\nprint('\\nTotal Variance from Original Features:')\nprint(np.sum(np.var(x, axis = 0)))\n\nprint('\\nVariance of the 2 principle components:')\nprint(np.round(np.var(x_trans, axis = 0),2))\n\nprint('\\nTotal Variance from Original Features:')\nprint(round(np.sum(np.var(x_trans, axis = 0)),2)) \n```", "```py\nVariance of the 2 features:\n[1\\. 1.]\n\nTotal Variance from Original Features:\n2.0\n\nVariance of the 2 principle components:\n[1.79 0.21]\n\nTotal Variance from Original Features:\n2.0 \n```", "```py\nprint('\\nCorrelation Matrix of the 2 original features components:')\nprint(np.round(np.corrcoef(x, rowvar = False),2))\n\nprint('\\nCorrelation Matrix of the 2 principle components\\' scores:')\nprint(np.round(np.corrcoef(x_trans, rowvar = False),2)) \n```", "```py\nCorrelation Matrix of the 2 original features components:\n[[1\\.   0.79]\n [0.79 1\\.  ]]\n\nCorrelation Matrix of the 2 principle components' scores:\n[[ 1\\. -0.]\n [-0\\.  1.]] \n```", "```py\nfrom numpy.linalg import eig\neigen_values,eigen_vectors = eig(cov)\nprint('Eigen Vectors:\\n' +  str(np.round(eigen_vectors,2)))\nprint('First Eigen Vector: ' + str(eigen_vectors[:,0]))\nprint('Second Eigen Vector: ' + str(eigen_vectors[:,1]))\nprint('Eigen Values:\\n' +  str(np.round(eigen_values,2)))\nPC = eigen_vectors.T.dot(x.T)\nplt.subplot(121)\nplt.scatter(PC[0,:],PC[1,:],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Principal Component Scores By-hand with numpy.linalg Eig Function'); plt.xlabel('PC1'); plt.ylabel('PC2')\nplt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()\n\nplt.subplot(122)\nplt.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nplt.title('Principal Component Scores with scikit-learn PCA'); plt.xlabel('PC1'); plt.ylabel('PC2')\nplt.xlim([-3,3]); plt.ylim([-3,3]); add_grid()\n\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.3); plt.show() \n```", "```py\nEigen Vectors:\n[[ 0.71 -0.71]\n [ 0.71  0.71]]\nFirst Eigen Vector: [0.70710678 0.70710678]\nSecond Eigen Vector: [-0.70710678  0.70710678]\nEigen Values:\n[1.81 0.21] \n```", "```py\nnComp = 1\nf, ((ax201, ax202, ax203), (ax206, ax205, ax204)) = plt.subplots(2, 3,figsize=(15,10))\n#f, ((ax201, ax202), (ax203, ax204), (ax205, ax206)) = plt.subplots(3, 2,figsize=(10,15))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax201.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax201.set_title('1\\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')\nax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)\n\nmu = np.mean(np.vstack((my_data_por_perm[\"Por\"].values,my_data_por_perm[\"LogPerm\"].values)), axis=1)\nsd = np.std(np.vstack((my_data_por_perm[\"Por\"].values,my_data_por_perm[\"LogPerm\"].values)), axis=1)\nx = StandardScaler().fit_transform(x)                     # standardize the data features to mean = 0, var = 1.0\n\nax202.scatter(x[:,0],x[:,1], s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax202.set_title('2\\. Standardized LogPerm vs. Por'); ax202.set_xlabel('Standardized Por'); ax202.set_ylabel('Standardized LogPerm')\nax202.set_xlim([-3.5,3.5]); ax202.set_ylim([-3.5,3.5]); add_grid2(ax202)\n\nn_components = 2                                          # build principal component model with 2 components\npca = PCA(n_components=n_components)\npca.fit(x)\n\nx_trans = pca.transform(x)                                # calculate principal component scores\nax203.scatter(x_trans[:,0],-1*x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax203.set_title('3\\. Principal Component Scores'); ax203.set_xlabel('PC1'); ax203.set_ylabel('PC2')\nax203.set_xlim([-3.5,3.5]); ax203.set_ylim([-3.5,3.5]); add_grid2(ax203)\n\nx_trans[:,1] = 0.0                                         # zero / remove the 2nd principal component \n\nax204.scatter(x_trans[:,0],x_trans[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax204.set_title('4\\. Only 1st Principal Component Scores'); ax204.set_xlabel('PC1'); ax204.set_ylabel('PC2')\nax204.set_xlim([-3.5,3.5]); ax204.set_ylim([-3.5,3.5]); add_grid2(ax204)\n\nxhat = pca.inverse_transform(x_trans)                             # reverse the principal component scores to standardized values\nax205.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax205.set_title('5\\. Reverse PCA'); ax205.set_xlabel('Standardized Por'); ax205.set_ylabel('Standardized LogPerm')\nax205.set_xlim([-3.5,3.5]); ax205.set_ylim([-3.5,3.5]); add_grid2(ax205)\n\nxhat = np.dot(pca.inverse_transform(x)[:,:nComp], pca.components_[:nComp,:])\nxhat = sd*xhat + mu                                       # remove the standardization\n\nax206.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"blue\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.6, linewidths=1.0, edgecolors=\"black\")\nax206.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax206.set_title('6\\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')\nax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)\n\nplt.show() \n```", "```py\nf, (ax201, ax206) = plt.subplots(1, 2,figsize=(10,6))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax201.scatter(my_data_por_perm[\"Por\"],my_data_por_perm[\"LogPerm\"],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax201.set_title('1\\. LogPerm vs. Por'); ax201.set_xlabel('Por'); ax201.set_ylabel('LogPerm')\nax201.set_xlim([8,22]); ax201.set_ylim([0,2.5]); add_grid2(ax201)\n\nax206.scatter(xhat[:,0],xhat[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=1.0, edgecolors=\"black\")\nax206.set_title('6\\. De-standardized Reverse PCA'); ax206.set_xlabel('Por'); ax206.set_ylabel('LogPerm')\nax206.set_xlim([8,22]); ax206.set_ylim([0,2.5]); add_grid2(ax206)\nplt.show()\n\nvar_por = np.var(my_data_por_perm[\"Por\"]); var_por_hat = np.var(xhat[:,0]);\nvar_logperm = np.var(my_data_por_perm[\"LogPerm\"]); var_logperm_hat = np.var(xhat[:,1]);\nprint('Variance Por =',np.round(var_por,3),', Variance Reduced Dimensional Por =',np.round(var_por_hat,3),'Fraction = ',np.round(var_por_hat/var_por,3))\nprint('Variance LogPerm =',np.round(var_logperm,3),', Variance Reduced Dimensional LogPerm =',np.round(var_logperm_hat,3),'Fraction = ',np.round(var_logperm_hat/var_logperm,3))\nprint('Total Variance =',np.round(var_por + var_logperm,3), ', Total Variance Reduced Dimension =',np.round(var_por_hat+var_logperm_hat,3),'Fraction = ',np.round((var_por_hat+var_logperm_hat)/(var_por+var_logperm),3)) \n```", "```py\nVariance Por = 7.89 , Variance Reduced Dimensional Por = 7.073 Fraction =  0.896\nVariance LogPerm = 0.151 , Variance Reduced Dimensional LogPerm = 0.136 Fraction =  0.896\nTotal Variance = 8.041 , Total Variance Reduced Dimension = 7.208 Fraction =  0.896 \n```", "```py\nmy_data_f6 = my_data.iloc[0:500,0:6]                      # extract the 6 predictors, 500 samples \n```", "```py\nmy_data_f6.describe().transpose()                         # calculate summary statistics for the data \n```", "```py\ncorr_matrix = np.corrcoef(my_data_f6, rowvar = False)\nprint(np.around(corr_matrix,2))                           # print the correlation matrix to 2 decimals \n```", "```py\n[[ 1\\.    0.79 -0.49 -0.25  0.71  0.12]\n [ 0.79  1\\.   -0.32 -0.13  0.48  0.04]\n [-0.49 -0.32  1\\.    0.14 -0.53  0.47]\n [-0.25 -0.13  0.14  1\\.   -0.24  0.24]\n [ 0.71  0.48 -0.53 -0.24  1\\.    0.35]\n [ 0.12  0.04  0.47  0.24  0.35  1\\.  ]] \n```", "```py\nfeatures = ['Por','LogPerm','AI','Brittle','TOC','VR']\nx_f6 = my_data_f6.loc[:,features].values\nmu_f6 = np.mean(x_f6, axis=0)\nsd_f6 = np.std(x_f6, axis=0)\nx_f6 = StandardScaler().fit_transform(x_f6)\n\nprint(\"Original Means\", features[:], np.round(mu_f6[:],2)) \nprint(\"Original StDevs\", features[:],np.round(sd_f6[:],2)) \nprint('Mean Transformed =',features[:],np.round(x.mean(axis=0),2))\nprint('Variance Transformed Por =',features[:],np.round(x.var(axis=0),2)) \n```", "```py\nOriginal Means ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [14.9   1.4   2.99 49.75  1\\.    1.99]\nOriginal StDevs ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [ 2.98  0.41  0.56 15.2   0.5   0.31]\nMean Transformed = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [0\\. 0.]\nVariance Transformed Por = ['Por', 'LogPerm', 'AI', 'Brittle', 'TOC', 'VR'] [1\\. 1.] \n```", "```py\nf, (ax6,ax7,ax8,ax9,ax10,ax11) = plt.subplots(1, 6, sharey=True, figsize=(15,2))\nax6.hist(x_f6[:,0], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax6.set_title('Std. Porosity'); ax6.set_xlim(-5,5)\nax7.hist(x_f6[:,1], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax7.set_title('Std. Log[Perm.]'); ax7.set_xlim(-5,5)\nax8.hist(x_f6[:,2], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax8.set_title('Std. Acoustic Imped.'); ax8.set_xlim(-5,5)\nax9.hist(x_f6[:,3], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax9.set_title('Std. Brittleness'); ax9.set_xlim(-5,5)\nax10.hist(x_f6[:,4], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax10.set_title('Std. Total Organic C'); ax10.set_xlim(-5,5)\nax11.hist(x_f6[:,5], alpha = 0.8, color = 'darkorange', edgecolor = 'black', bins=20); ax11.set_title('Std. Vit. Reflectance'); ax11.set_xlim(-5,5)\nplt.show() \n```", "```py\nn_components = 6\npca_f6 = PCA(n_components=n_components)\npca_f6.fit(x_f6)\n\nprint(np.round(pca_f6.components_,3))                     # visualize the component loadings \n```", "```py\n[[ 0.558  0.476 -0.405 -0.211  0.504  0.01 ]\n [-0.117 -0.114 -0.432 -0.323 -0.229 -0.794]\n [-0.019 -0.124  0.384 -0.898  0.07   0.157]\n [-0.214 -0.674 -0.424 -0.006  0.526  0.21 ]\n [-0.784  0.522 -0.031 -0.046  0.331 -0.019]\n [ 0.12  -0.138  0.566  0.206  0.55  -0.549]] \n```", "```py\nprint('Variance explained by PC1 thru PC6 =', np.round(pca_f6.explained_variance_ratio_,3))\n\nf, (ax10, ax11) = plt.subplots(1, 2,figsize=(10,6))\nf.subplots_adjust(wspace=0.5,hspace = 0.3)\n\nax10.plot(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8)\nax10.scatter(np.arange(1,7,1),pca_f6.explained_variance_ratio_*100,color='darkorange',alpha=0.8,edgecolor='black')\nax10.set_xlabel('Principal Component'); ax10.set_ylabel('Variance Explained'); ax10.set_title('Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nyticks = mtick.FormatStrFormatter(fmt); ax10.set_xlim(1,6); ax10.set_ylim(0,100.0)\nax10.yaxis.set_major_formatter(yticks); add_grid2(ax10)\n\nax11.plot(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8)\nax11.scatter(np.arange(1,7,1),np.cumsum(pca_f6.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')\nax11.plot([1,6],[95,95], color='black',linestyle='dashed')\nax11.set_xlabel('Principal Component'); ax11.set_ylabel('Cumulative Variance Explained'); ax11.set_title('Cumulative Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nyticks = mtick.FormatStrFormatter(fmt); ax11.set_xlim(1,6); ax11.set_ylim(0,100.0); ax11.annotate('95% variance explained',[4.05,90])\nax11.yaxis.set_major_formatter(yticks); add_grid2(ax11)\n\nplt.show() \n```", "```py\nVariance explained by PC1 thru PC6 = [0.462 0.246 0.149 0.11  0.024 0.009] \n```", "```py\nprint('\\nCorrelation Matrix of the 6 original features components:')\nprint(np.round(np.corrcoef(x_f6, rowvar = False),2))\n\nprint('\\nCorrelation Matrix of the 6 principle components\\' scores:')\nprint(np.round(np.corrcoef(pca_f6.transform(x_f6), rowvar = False),2)) \n```", "```py\nCorrelation Matrix of the 6 original features components:\n[[ 1\\.    0.79 -0.49 -0.25  0.71  0.12]\n [ 0.79  1\\.   -0.32 -0.13  0.48  0.04]\n [-0.49 -0.32  1\\.    0.14 -0.53  0.47]\n [-0.25 -0.13  0.14  1\\.   -0.24  0.24]\n [ 0.71  0.48 -0.53 -0.24  1\\.    0.35]\n [ 0.12  0.04  0.47  0.24  0.35  1\\.  ]]\n\nCorrelation Matrix of the 6 principle components' scores:\n[[ 1\\.  0\\. -0\\.  0\\.  0\\. -0.]\n [ 0\\.  1\\. -0\\. -0\\. -0\\. -0.]\n [-0\\. -0\\.  1\\. -0\\. -0\\.  0.]\n [ 0\\. -0\\. -0\\.  1\\.  0\\.  0.]\n [ 0\\. -0\\. -0\\.  0\\.  1\\.  0.]\n [-0\\. -0\\.  0\\.  0\\.  0\\.  1.]] \n```", "```py\nnComp = 6\nxhat_6d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_6d = sd_f6*xhat_6d + mu_f6\n\nnComp = 5\nxhat_5d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_5d = sd_f6*xhat_5d + mu_f6\n\nnComp = 4\nxhat_4d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_4d = sd_f6*xhat_4d + mu_f6\n\nnComp = 3\nxhat_3d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_3d = sd_f6*xhat_3d + mu_f6\n\nnComp = 2\nxhat_2d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_2d = sd_f6*xhat_2d + mu_f6\n\nnComp = 1\nxhat_1d = np.dot(pca_f6.transform(x_f6)[:,:nComp], pca_f6.components_[:nComp,:])\nxhat_1d = sd_f6*xhat_1d + mu_f6\n\nf, (ax12, ax13, ax14, ax15, ax16, ax17, ax18) = plt.subplots(1, 7,figsize=(20,20))\nf.subplots_adjust(wspace=0.7)\n\nax12.scatter(my_data_f6[\"Por\"],my_data_f6[\"LogPerm\"],s=None, c=\"darkorange\",marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax12.set_title('Original Data'); ax12.set_xlabel('Por'); ax12.set_ylabel('LogPerm')\nax12.set_ylim(0.0,3.0); ax12.set_xlim(8,22); ax12.set_aspect(4.0); \n\nax13.scatter(xhat_1d[:,0],xhat_1d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax13.set_title('1 Principal Component'); ax13.set_xlabel('Por'); ax13.set_ylabel('LogPerm')\nax13.set_ylim(0.0,3.0); ax13.set_xlim(8,22); ax13.set_aspect(4.0)\n\nax14.scatter(xhat_2d[:,0],xhat_2d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax14.set_title('2 Principal Components'); ax14.set_xlabel('Por'); ax14.set_ylabel('LogPerm')\nax14.set_ylim(0.0,3.0); ax14.set_xlim(8,22); ax14.set_aspect(4.0)\n\nax15.scatter(xhat_3d[:,0],xhat_3d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax15.set_title('3 Principal Components'); ax15.set_xlabel('Por'); ax15.set_ylabel('LogPerm')\nax15.set_ylim(0.0,3.0); ax15.set_xlim(8,22); ax15.set_aspect(4.0)\n\nax16.scatter(xhat_4d[:,0],xhat_4d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax16.set_title('4 Principal Components'); ax16.set_xlabel('Por'); ax16.set_ylabel('LogPerm')\nax16.set_ylim(0.0,3.0); ax16.set_xlim(8,22); ax16.set_aspect(4.0)\n\nax17.scatter(xhat_5d[:,0],xhat_5d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax17.set_title('5 Principal Components'); ax17.set_xlabel('Por'); ax17.set_ylabel('LogPerm')\nax17.set_ylim(0.0,3.0); ax17.set_xlim(8,22); ax17.set_aspect(4.0)\n\nax18.scatter(xhat_6d[:,0],xhat_6d[:,1],s=None, c=\"darkorange\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=1.0, edgecolors=\"black\")\nax18.set_title('6 Principal Components'); ax18.set_xlabel('Por'); ax18.set_ylabel('LogPerm')\nax18.set_ylim(0.0,3.0); ax18.set_xlim(8,22); ax18.set_aspect(4.0)\n\nplt.show() \n```", "```py\nprint('1 Principal Component : Variance Por =',np.round(np.var(xhat_1d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_1d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('2 Principal Components: Variance Por =',np.round(np.var(xhat_2d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_2d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('3 Principal Components: Variance Por =',np.round(np.var(xhat_3d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_3d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('4 Principal Components: Variance Por =',np.round(np.var(xhat_4d[:,0])/(sd_f6[0]*sd_f6[0]),2),' Variance Log Perm = ',np.round(np.var(xhat_4d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('5 Principal Components: Variance Por =',np.round(np.var(xhat_5d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_5d[:,1])/(sd_f6[1]*sd_f6[1]),2))\n\nprint('6 Principal Components: Variance Por =',np.round(np.var(xhat_6d[:,0])/(sd_f6[0]*sd_f6[0]),2),'  Variance Log Perm = ',np.round(np.var(xhat_6d[:,1])/(sd_f6[1]*sd_f6[1]),2)) \n```", "```py\n1 Principal Component : Variance Por = 0.86  Variance Log Perm =  0.63\n2 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.65\n3 Principal Components: Variance Por = 0.88  Variance Log Perm =  0.66\n4 Principal Components: Variance Por = 0.91  Variance Log Perm =  0.96\n5 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0\n6 Principal Components: Variance Por = 1.0   Variance Log Perm =  1.0 \n```", "```py\ndf_1d = pd.DataFrame(data=xhat_1d,columns=features)   \ndf_2d = pd.DataFrame(data=xhat_2d,columns=features)\ndf_3d = pd.DataFrame(data=xhat_3d,columns=features)\ndf_4d = pd.DataFrame(data=xhat_4d,columns=features)\ndf_5d = pd.DataFrame(data=xhat_5d,columns=features)\ndf_6d = pd.DataFrame(data=xhat_6d,columns=features) \n```", "```py\nfig = plt.figure()\n\npd_plot.scatter_matrix(my_data_f6, alpha = 0.1,           # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Original Data')\n\npd_plot.scatter_matrix(df_1d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('1 Principal Component')\n\npd_plot.scatter_matrix(df_2d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('2 Principal Components')\n\npd_plot.scatter_matrix(df_3d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('3 Principal Components')\n\npd_plot.scatter_matrix(df_4d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('4 Principal Components')\n\npd_plot.scatter_matrix(df_5d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('5 Principal Components')\n\npd_plot.scatter_matrix(df_6d, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('6 Principal Components')\n\nplt.show() \n```", "```py\n<Figure size 640x480 with 0 Axes> \n```", "```py\nx_rand = np.random.rand(10000,5); df_x_rand = pd.DataFrame(x_rand)\nprint('Variance of original features: ', np.round(np.var(x_rand, axis = 0),2))\nprint('Proportion of variance of original features: ', np.round(np.var(x_rand, axis = 0)/np.sum(np.var(x_rand, axis = 0)),2))\nprint('Correlation Matrix of original features:\\n'); print(np.round(np.cov(x_rand, rowvar = False),2)); print()\n\npd_plot.scatter_matrix(df_x_rand, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Original Features')\n\npca_rand = PCA(n_components=5)\npca_rand.fit(x_rand)\nprint('PCA Variance Explained ', np.round(pca_rand.explained_variance_ratio_,2))  \n\nscores_x_rand = pca_rand.transform(x_rand); df_scores_x_rand = pd.DataFrame(scores_x_rand)\n\nprint('\\nCorrelation Matrix of scores:\\n'); print(np.round(np.cov(scores_x_rand, rowvar = False),2)); print()\n\npd_plot.scatter_matrix(df_scores_x_rand, alpha = 0.1,                # pandas matrix scatter plot\n    figsize=(6, 6),color = 'black', hist_kwds={'color':['grey']})\nplt.suptitle('Principal Component Scores') \n```", "```py\nVariance of original features:  [0.08 0.08 0.08 0.08 0.08]\nProportion of variance of original features:  [0.2 0.2 0.2 0.2 0.2]\nCorrelation Matrix of original features:\n\n[[ 0.08 -0\\.    0\\.   -0\\.    0\\.  ]\n [-0\\.    0.08  0\\.    0\\.   -0\\.  ]\n [ 0\\.    0\\.    0.08  0\\.   -0\\.  ]\n [-0\\.    0\\.    0\\.    0.08  0\\.  ]\n [ 0\\.   -0\\.   -0\\.    0\\.    0.08]] \n```", "```py\nPCA Variance Explained  [0.21 0.2  0.2  0.2  0.19]\n\nCorrelation Matrix of scores:\n\n[[ 0.09 -0\\.   -0\\.   -0\\.    0\\.  ]\n [-0\\.    0.08  0\\.   -0\\.   -0\\.  ]\n [-0\\.    0\\.    0.08  0\\.    0\\.  ]\n [-0\\.   -0\\.    0\\.    0.08  0\\.  ]\n [ 0\\.   -0\\.    0\\.    0\\.    0.08]] \n```", "```py\nText(0.5, 0.98, 'Principal Component Scores') \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\ndf_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features\ndf_new.head(n=13) \n```", "```py\nvar_explained = 0.95                                          # select the minimum variance explained\n\nn_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1\npca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA\npca_scores = pca_new.fit_transform(df_new.values)\n\ncumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance\n\nn_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance\n\ndf_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],\n            index=df_new.index)                               # project data to that many principal components\n\nsns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)\nplt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')\nplt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)\nplt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')\nplt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks\nplt.xticks(range(1, len(cumulative_variance) + 1))\nyticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) \nplt.annotate('95% variance explained',[4.05,90]); add_grid()\nplt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nsave_PCA = True                                        # save the imputed DataFrame?\n\nif save_PCA == True:\n    folder = r'C:\\Local'\n    file_name = r'dataframe_PCA.csv'\n\n    df_new_projected.to_csv(folder + \"/\" + file_name, index=False) \n```", "```py\n---------------------------------------------------------------------------\nOSError  Traceback (most recent call last)\nCell In[42], line 7\n  4 folder = r'C:\\Local'\n  5 file_name = r'dataframe_PCA.csv'\n----> 7 df_new_projected.to_csv(folder + \"/\" + file_name, index=False)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\core\\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\n  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n  3763 formatter = DataFrameFormatter(\n  3764     frame=df,\n  3765     header=header,\n   (...)\n  3769     decimal=decimal,\n  3770 )\n-> 3772 return DataFrameRenderer(formatter).to_csv(\n  3773     path_or_buf,\n  3774     lineterminator=lineterminator,\n  3775     sep=sep,\n  3776     encoding=encoding,\n  3777     errors=errors,\n  3778     compression=compression,\n  3779     quoting=quoting,\n  3780     columns=columns,\n  3781     index_label=index_label,\n  3782     mode=mode,\n  3783     chunksize=chunksize,\n  3784     quotechar=quotechar,\n  3785     date_format=date_format,\n  3786     doublequote=doublequote,\n  3787     escapechar=escapechar,\n  3788     storage_options=storage_options,\n  3789 )\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\n  1165     created_buffer = False\n  1167 csv_formatter = CSVFormatter(\n  1168     path_or_buf=path_or_buf,\n  1169     lineterminator=lineterminator,\n   (...)\n  1184     formatter=self.fmt,\n  1185 )\n-> 1186 csv_formatter.save()\n  1188 if created_buffer:\n  1189     assert isinstance(path_or_buf, StringIO)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240, in CSVFormatter.save(self)\n  236  \"\"\"\n  237 Create the writer & save.\n  238 \"\"\"\n  239 # apply compression and byte/text conversion\n--> 240 with get_handle(\n  241     self.filepath_or_buffer,\n  242     self.mode,\n  243     encoding=self.encoding,\n  244     errors=self.errors,\n  245     compression=self.compression,\n  246     storage_options=self.storage_options,\n  247 ) as handles:\n  248     # Note: self.encoding is irrelevant here\n  249     self.writer = csvlib.writer(\n  250         handles.handle,\n  251         lineterminator=self.lineterminator,\n   (...)\n  256         quotechar=self.quotechar,\n  257     )\n  259     self._save()\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n  735 # Only for write methods\n  736 if \"r\" not in mode and is_path:\n--> 737     check_parent_directory(str(handle))\n  739 if compression:\n  740     if compression != \"zstd\":\n  741         # compression libraries do not like an explicit text-mode\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:600, in check_parent_directory(path)\n  598 parent = Path(path).parent\n  599 if not parent.is_dir():\n--> 600     raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\n\nOSError: Cannot save file into a non-existent directory: 'C:\\Local' \n```", "```py\nidata = 0                                                     # select the dataset\n\nif idata == 0:\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well','Prod'],axis=1,inplace=True)          # remove well index and response feature\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax_new = [24.0,10.0,5.0,85.0,2.2,2.9] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)', # set the names for plotting\n             'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n\n    ftitle_new = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio', # set the units for plotting\n             'Total Organic Carbon','Vitrinite Reflectance']\n\nelif idata == 1:\n    names = {'Porosity':'Por'}\n\n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/12_sample_data.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['X','Y','Unnamed: 0'],axis=1,inplace=True)   # remove response feature\n    df_new = df_new.rename(columns=names)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [0.0,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [10000.0,10000.0,1.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\nelif idata == 2:  \n    df_new = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/res21_2D_wells.csv') # load data from Dr. Pyrcz's GitHub repository \n    df_new.drop(['Well_ID','X','Y','CumulativeOil'],axis=1,inplace=True) # remove Well Index, X and Y coordinates, and response feature\n    df_new = df_new.dropna(how='any',inplace=False)\n\n    features = df_new.columns.values.tolist()                 # store the names of the features\n\n    xmin_new = [1,0.0,0.0,4.0,0.0,6.5,1.4,1600.0,10.0,1300.0,1.6]; xmax_new = [73,10000.0,10000.0,19.0,500.0,8.3,3.6,6200.0,50.0,2000.0,12.0] # set the minimum and maximum values for plotting\n\n    flabel_new = ['Well (ID)','X (m)','Y (m)','Depth (m)','Porosity (fraction)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Facies (categorical)',\n              'Density (g/cm^3)','Compressible velocity (m/s)','Youngs modulus (GPa)', 'Shear velocity (m/s)', 'Shear modulus (GPa)'] # set the names for plotting\n\n    ftitle_new = ['Well','X','Y','Depth','Porosity','Permeability','Acoustic Impedance','Facies',\n              'Density','Compressible velocity','Youngs modulus', 'Shear velocity', 'Shear modulus']\n\ndf_new[df_new.columns] = MinMaxScaler().fit_transform(df_new) # min/max normalize all the features\ndf_new.head(n=13) \n```", "```py\nvar_explained = 0.95                                          # select the minimum variance explained\n\nn_components = min(len(df_new.columns),len(df_new)-1)         # max components is min of number of features or number of data - 1\npca_new = PCA(n_components=n_components).fit(df_new.values)   # calculate PCA\npca_scores = pca_new.fit_transform(df_new.values)\n\ncumulative_variance = np.cumsum(pca_new.explained_variance_ratio_) # calculate cumulative explained variance\n\nn_selected = np.argmax(cumulative_variance >= var_explained) + 1 # find number of components to retain 95% variance\n\ndf_new_projected = pd.DataFrame(pca_scores[:, :n_selected],columns=[f'PC{i+1}' for i in range(n_selected)],\n            index=df_new.index)                               # project data to that many principal components\n\nsns.pairplot(df_new_projected.iloc[:,:], plot_kws={'alpha':1.0,'s':50}, palette = 'colorblind', corner=True) # matrix scatter plot\nplt.subplots_adjust(left=0.0, bottom=0.0, right=0.6, top=0.7, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nplt.plot(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8)\nplt.scatter(np.arange(1,len(pca_new.explained_variance_ratio_)+1,1),np.cumsum(pca_new.explained_variance_ratio_*100),color='darkorange',alpha=0.8,edgecolor='black')\nplt.plot([1,len(df_new.columns)],[95,95], color='black',linestyle='dashed'); plt.plot([n_selected,n_selected],[0,100],color='red',zorder=-1)\nplt.annotate('Selected Number of Components = '+ str(n_selected),[n_selected,10],rotation=270,color='red')\nplt.xlabel('Principal Component'); plt.ylabel('Cumulative Variance Explained'); plt.title('Cumulative Variance Explained by Principal Component')\nfmt = '%.0f%%' # Format you want the ticks\nplt.xticks(range(1, len(cumulative_variance) + 1))\nyticks = mtick.FormatStrFormatter(fmt); plt.xlim(1,len(pca_new.explained_variance_ratio_)); plt.ylim(0,100.0) \nplt.annotate('95% variance explained',[4.05,90]); add_grid()\nplt.gca().yaxis.set_major_formatter(PercentFormatter(100.0))  # 1.0 = 100%\nplt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.0, wspace=0.2, hspace=0.2); plt.show() \n```", "```py\nsave_PCA = True                                        # save the imputed DataFrame?\n\nif save_PCA == True:\n    folder = r'C:\\Local'\n    file_name = r'dataframe_PCA.csv'\n\n    df_new_projected.to_csv(folder + \"/\" + file_name, index=False) \n```", "```py\n---------------------------------------------------------------------------\nOSError  Traceback (most recent call last)\nCell In[42], line 7\n  4 folder = r'C:\\Local'\n  5 file_name = r'dataframe_PCA.csv'\n----> 7 df_new_projected.to_csv(folder + \"/\" + file_name, index=False)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\core\\generic.py:3772, in NDFrame.to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\n  3761 df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n  3763 formatter = DataFrameFormatter(\n  3764     frame=df,\n  3765     header=header,\n   (...)\n  3769     decimal=decimal,\n  3770 )\n-> 3772 return DataFrameRenderer(formatter).to_csv(\n  3773     path_or_buf,\n  3774     lineterminator=lineterminator,\n  3775     sep=sep,\n  3776     encoding=encoding,\n  3777     errors=errors,\n  3778     compression=compression,\n  3779     quoting=quoting,\n  3780     columns=columns,\n  3781     index_label=index_label,\n  3782     mode=mode,\n  3783     chunksize=chunksize,\n  3784     quotechar=quotechar,\n  3785     date_format=date_format,\n  3786     doublequote=doublequote,\n  3787     escapechar=escapechar,\n  3788     storage_options=storage_options,\n  3789 )\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186, in DataFrameRenderer.to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\n  1165     created_buffer = False\n  1167 csv_formatter = CSVFormatter(\n  1168     path_or_buf=path_or_buf,\n  1169     lineterminator=lineterminator,\n   (...)\n  1184     formatter=self.fmt,\n  1185 )\n-> 1186 csv_formatter.save()\n  1188 if created_buffer:\n  1189     assert isinstance(path_or_buf, StringIO)\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240, in CSVFormatter.save(self)\n  236  \"\"\"\n  237 Create the writer & save.\n  238 \"\"\"\n  239 # apply compression and byte/text conversion\n--> 240 with get_handle(\n  241     self.filepath_or_buffer,\n  242     self.mode,\n  243     encoding=self.encoding,\n  244     errors=self.errors,\n  245     compression=self.compression,\n  246     storage_options=self.storage_options,\n  247 ) as handles:\n  248     # Note: self.encoding is irrelevant here\n  249     self.writer = csvlib.writer(\n  250         handles.handle,\n  251         lineterminator=self.lineterminator,\n   (...)\n  256         quotechar=self.quotechar,\n  257     )\n  259     self._save()\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:737, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n  735 # Only for write methods\n  736 if \"r\" not in mode and is_path:\n--> 737     check_parent_directory(str(handle))\n  739 if compression:\n  740     if compression != \"zstd\":\n  741         # compression libraries do not like an explicit text-mode\n\nFile C:\\ProgramData\\anaconda3\\envs\\MachineLearningBook\\lib\\site-packages\\pandas\\io\\common.py:600, in check_parent_directory(path)\n  598 parent = Path(path).parent\n  599 if not parent.is_dir():\n--> 600     raise OSError(rf\"Cannot save file into a non-existent directory: '{parent}'\")\n\nOSError: Cannot save file into a non-existent directory: 'C:\\Local' \n```"]